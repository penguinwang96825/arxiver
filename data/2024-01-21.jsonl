{"title": "ProLex: A Benchmark for Language Proficiency-oriented Lexical\n  Substitution", "abstract": "Lexical Substitution discovers appropriate substitutes for a given target\nword in a context sentence. However, the task fails to consider substitutes\nthat are of equal or higher proficiency than the target, an aspect that could\nbe beneficial for language learners looking to improve their writing. To bridge\nthis gap, we propose a new task, language proficiency-oriented lexical\nsubstitution. We also introduce ProLex, a novel benchmark designed to assess\nsystems' ability to generate not only appropriate substitutes but also\nsubstitutes that demonstrate better language proficiency. Besides the\nbenchmark, we propose models that can automatically perform the new task. We\nshow that our best model, a Llama2-13B model fine-tuned with task-specific\nsynthetic data, outperforms ChatGPT by an average of 3.2% in F-score and\nachieves comparable results with GPT-4 on ProLex.", "published": "2024-01-21 00:58:31", "link": "http://arxiv.org/abs/2401.11356v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Finding a Needle in the Adversarial Haystack: A Targeted Paraphrasing\n  Approach For Uncovering Edge Cases with Minimal Distribution Distortion", "abstract": "Adversarial attacks against language models(LMs) are a significant concern.\nIn particular, adversarial samples exploit the model's sensitivity to small\ninput changes. While these changes appear insignificant on the semantics of the\ninput sample, they result in significant decay in model performance. In this\npaper, we propose Targeted Paraphrasing via RL (TPRL), an approach to\nautomatically learn a policy to generate challenging samples that most likely\nimprove the model's performance. TPRL leverages FLAN T5, a language model, as a\ngenerator and employs a self learned policy using a proximal policy gradient to\ngenerate the adversarial examples automatically. TPRL's reward is based on the\nconfusion induced in the classifier, preserving the original text meaning\nthrough a Mutual Implication score. We demonstrate and evaluate TPRL's\neffectiveness in discovering natural adversarial attacks and improving model\nperformance through extensive experiments on four diverse NLP classification\ntasks via Automatic and Human evaluation. TPRL outperforms strong baselines,\nexhibits generalizability across classifiers and datasets, and combines the\nstrengths of language modeling and reinforcement learning to generate diverse\nand influential adversarial examples.", "published": "2024-01-21 02:25:29", "link": "http://arxiv.org/abs/2401.11373v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Majority or Minority: Data Imbalance Learning Method for Named Entity\n  Recognition", "abstract": "Data imbalance presents a significant challenge in various machine learning\n(ML) tasks, particularly named entity recognition (NER) within natural language\nprocessing (NLP). NER exhibits a data imbalance with a long-tail distribution,\nfeaturing numerous minority classes (i.e., entity classes) and a single\nmajority class (i.e., O-class). This imbalance leads to misclassifications of\nthe entity classes as the O-class. To tackle this issue, we propose a simple\nand effective learning method named majority or minority (MoM) learning. MoM\nlearning incorporates the loss computed only for samples whose ground truth is\nthe majority class into the loss of the conventional ML model. Evaluation\nexperiments on four NER datasets (Japanese and English) showed that MoM\nlearning improves prediction performance of the minority classes without\nsacrificing the performance of the majority class and is more effective than\nwidely known and state-of-the-art methods. We also evaluated MoM learning using\nframeworks as sequential labeling and machine reading comprehension, which are\ncommonly used in NER. Furthermore, MoM learning has achieved consistent\nperformance improvements regardless of language or framework.", "published": "2024-01-21 08:43:24", "link": "http://arxiv.org/abs/2401.11431v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linear Alignment: A Closed-form Solution for Aligning Human Preferences\n  without Tuning and Feedback", "abstract": "The success of AI assistants based on Language Models (LLMs) hinges on\nReinforcement Learning from Human Feedback (RLHF) to comprehend and align with\nuser intentions. However, traditional alignment algorithms, such as PPO, are\nhampered by complex annotation and training requirements. This reliance limits\nthe applicability of RLHF and hinders the development of professional\nassistants tailored to diverse human preferences. In this work, we introduce\n\\textit{Linear Alignment}, a novel algorithm that aligns language models with\nhuman preferences in one single inference step, eliminating the reliance on\ndata annotation and model training. Linear alignment incorporates a new\nparameterization for policy optimization under divergence constraints, which\nenables the extraction of optimal policy in a closed-form manner and\nfacilitates the direct estimation of the aligned response. Extensive\nexperiments on both general and personalized preference datasets demonstrate\nthat linear alignment significantly enhances the performance and efficiency of\nLLM alignment across diverse scenarios. Our code and dataset is published on\n\\url{https://github.com/Wizardcoast/Linear_Alignment.git}.", "published": "2024-01-21 10:46:23", "link": "http://arxiv.org/abs/2401.11458v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Over-Reasoning and Redundant Calculation of Large Language Models", "abstract": "Large language models (LLMs) can solve problems step-by-step. While this\nchain-of-thought (CoT) reasoning boosts LLMs' performance, it is unclear if\nLLMs \\textit{know} when to use CoT and whether those CoT are always necessary\nto answer the question. This paper shows that LLMs tend to generate redundant\ncalculations and reasoning on a manually constructed math QA dataset,\nGSM8K-Zero. GSM8K-Zero is constructed such that the questions can be answered\nwithout any calculations, but LLMs, including Llama-2 models and Claude-2, tend\nto generate lengthy and unnecessary calculations to answer the questions. We\nalso conduct experiments to explain why LLMs generate redundant calculations\nand reasonings. GSM8K-Zero is publicly available at\nhttps://github.com/d223302/Over-Reasoning-of-LLMs and\nhttps://huggingface.co/datasets/dcml0714/GSM8K-Zero.", "published": "2024-01-21 11:42:18", "link": "http://arxiv.org/abs/2401.11467v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robust Evaluation Measures for Evaluating Social Biases in Masked\n  Language Models", "abstract": "Many evaluation measures are used to evaluate social biases in masked\nlanguage models (MLMs). However, we find that these previously proposed\nevaluation measures are lacking robustness in scenarios with limited datasets.\nThis is because these measures are obtained by comparing the\npseudo-log-likelihood (PLL) scores of the stereotypical and anti-stereotypical\nsamples using an indicator function. The disadvantage is the limited mining of\nthe PLL score sets without capturing its distributional information. In this\npaper, we represent a PLL score set as a Gaussian distribution and use Kullback\nLeibler (KL) divergence and Jensen Shannon (JS) divergence to construct\nevaluation measures for the distributions of stereotypical and\nanti-stereotypical PLL scores. Experimental results on the publicly available\ndatasets StereoSet (SS) and CrowS-Pairs (CP) show that our proposed measures\nare significantly more robust and interpretable than those proposed previously.", "published": "2024-01-21 21:21:51", "link": "http://arxiv.org/abs/2401.11601v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Confidence Preservation Property in Knowledge Distillation Abstractions", "abstract": "Social media platforms prevent malicious activities by detecting harmful\ncontent of posts and comments. To that end, they employ large-scale deep neural\nnetwork language models for sentiment analysis and content understanding. Some\nmodels, like BERT, are complex, and have numerous parameters, which makes them\nexpensive to operate and maintain. To overcome these deficiencies, industry\nexperts employ a knowledge distillation compression technique, where a\ndistilled model is trained to reproduce the classification behavior of the\noriginal model. The distillation processes terminates when the distillation\nloss function reaches the stopping criteria. This function is mainly designed\nto ensure that the original and the distilled models exhibit alike\nclassification behaviors. However, besides classification accuracy, there are\nadditional properties of the original model that the distilled model should\npreserve to be considered as an appropriate abstraction. In this work, we\nexplore whether distilled TinyBERT models preserve confidence values of the\noriginal BERT models, and investigate how this confidence preservation property\ncould guide tuning hyperparameters of the distillation process.", "published": "2024-01-21 01:37:25", "link": "http://arxiv.org/abs/2401.11365v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Using Large Language Model for End-to-End Chinese ASR and NER", "abstract": "Mapping speech tokens to the same feature space as text tokens has become the\nparadigm for the integration of speech modality into decoder-only large\nlanguage models (LLMs). An alternative approach is to use an encoder-decoder\narchitecture that incorporates speech features through cross-attention. This\napproach, however, has received less attention in the literature. In this work,\nwe connect the Whisper encoder with ChatGLM3 and provide in-depth comparisons\nof these two approaches using Chinese automatic speech recognition (ASR) and\nname entity recognition (NER) tasks. We evaluate them not only by conventional\nmetrics like the F1 score but also by a novel fine-grained taxonomy of ASR-NER\nerrors. Our experiments reveal that encoder-decoder architecture outperforms\ndecoder-only architecture with a short context, while decoder-only architecture\nbenefits from a long context as it fully exploits all layers of the LLM. By\nusing LLM, we significantly reduced the entity omission errors and improved the\nentity ASR accuracy compared to the Conformer baseline. Additionally, we\nobtained a state-of-the-art (SOTA) F1 score of 0.805 on the AISHELL-NER test\nset by using chain-of-thought (CoT) NER which first infers long-form ASR\ntranscriptions and then predicts NER labels.", "published": "2024-01-21 03:15:05", "link": "http://arxiv.org/abs/2401.11382v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SEBERTNets: Sequence Enhanced BERT Networks for Event Entity Extraction\n  Tasks Oriented to the Finance Field", "abstract": "Event extraction lies at the cores of investment analysis and asset\nmanagement in the financial field, and thus has received much attention. The\n2019 China conference on knowledge graph and semantic computing (CCKS)\nchallenge sets up a evaluation competition for event entity extraction task\noriented to the finance field. In this task, we mainly focus on how to extract\nthe event entity accurately, and recall all the corresponding event entity\neffectively. In this paper, we propose a novel model, Sequence Enhanced BERT\nNetworks (SEBERTNets for short), which can inherit the advantages of the\nBERT,and while capturing sequence semantic information. In addition, motivated\nby recommendation system, we propose Hybrid Sequence Enhanced BERT Networks\n(HSEBERTNets for short), which uses a multi-channel recall method to recall all\nthe corresponding event entity. The experimental results show that, the F1\nscore of SEBERTNets is 0.905 in the first stage, and the F1 score of\nHSEBERTNets is 0.934 in the first stage, which demonstarate the effectiveness\nof our methods.", "published": "2024-01-21 06:10:03", "link": "http://arxiv.org/abs/2401.11408v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Reliable and Factual Response Generation: Detecting Unanswerable\n  Questions in Information-Seeking Conversations", "abstract": "Generative AI models face the challenge of hallucinations that can undermine\nusers' trust in such systems. We approach the problem of conversational\ninformation seeking as a two-step process, where relevant passages in a corpus\nare identified first and then summarized into a final system response. This way\nwe can automatically assess if the answer to the user's question is present in\nthe corpus. Specifically, our proposed method employs a sentence-level\nclassifier to detect if the answer is present, then aggregates these\npredictions on the passage level, and eventually across the top-ranked passages\nto arrive at a final answerability estimate. For training and evaluation, we\ndevelop a dataset based on the TREC CAsT benchmark that includes answerability\nlabels on the sentence, passage, and ranking levels. We demonstrate that our\nproposed method represents a strong baseline and outperforms a state-of-the-art\nLLM on the answerability prediction task.", "published": "2024-01-21 10:15:36", "link": "http://arxiv.org/abs/2401.11452v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Estimating the Usefulness of Clarifying Questions and Answers for\n  Conversational Search", "abstract": "While the body of research directed towards constructing and generating\nclarifying questions in mixed-initiative conversational search systems is vast,\nresearch aimed at processing and comprehending users' answers to such questions\nis scarce. To this end, we present a simple yet effective method for processing\nanswers to clarifying questions, moving away from previous work that simply\nappends answers to the original query and thus potentially degrades retrieval\nperformance. Specifically, we propose a classifier for assessing usefulness of\nthe prompted clarifying question and an answer given by the user. Useful\nquestions or answers are further appended to the conversation history and\npassed to a transformer-based query rewriting module. Results demonstrate\nsignificant improvements over strong non-mixed-initiative baselines.\nFurthermore, the proposed approach mitigates the performance drops when non\nuseful questions and answers are utilized.", "published": "2024-01-21 11:04:30", "link": "http://arxiv.org/abs/2401.11463v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Towards Better Inclusivity: A Diverse Tweet Corpus of English Varieties", "abstract": "The prevalence of social media presents a growing opportunity to collect and\nanalyse examples of English varieties. Whilst usage of these varieties was -\nand, in many cases, still is - used only in spoken contexts or hard-to-access\nprivate messages, social media sites like Twitter provide a platform for users\nto communicate informally in a scrapeable format. Notably, Indian English\n(Hinglish), Singaporean English (Singlish), and African-American English (AAE)\ncan be commonly found online. These varieties pose a challenge to existing\nnatural language processing (NLP) tools as they often differ orthographically\nand syntactically from standard English for which the majority of these tools\nare built. NLP models trained on standard English texts produced biased\noutcomes for users of underrepresented varieties. Some research has aimed to\novercome the inherent biases caused by unrepresentative data through techniques\nlike data augmentation or adjusting training models.\n  We aim to address the issue of bias at its root - the data itself. We curate\na dataset of tweets from countries with high proportions of underserved English\nvariety speakers, and propose an annotation framework of six categorical\nclassifications along a pseudo-spectrum that measures the degree of standard\nEnglish and that thereby indirectly aims to surface the manifestations of\nEnglish varieties in these tweets. Following best annotation practices, our\ngrowing corpus features 170,800 tweets taken from 7 countries, labeled by\nannotators who are from those countries and can communicate in\nregionally-dominant varieties of English. Our corpus highlights the accuracy\ndiscrepancies in pre-trained language identifiers between western English and\nnon-western (i.e., less standard) English varieties. We hope to contribute to\nthe growing literature identifying and reducing the implicit demographic\ndiscrepancies in NLP.", "published": "2024-01-21 13:18:20", "link": "http://arxiv.org/abs/2401.11487v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation", "abstract": "Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference.", "published": "2024-01-21 14:28:41", "link": "http://arxiv.org/abs/2401.11504v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CheX-GPT: Harnessing Large Language Models for Enhanced Chest X-ray\n  Report Labeling", "abstract": "Free-text radiology reports present a rich data source for various medical\ntasks, but effectively labeling these texts remains challenging. Traditional\nrule-based labeling methods fall short of capturing the nuances of diverse\nfree-text patterns. Moreover, models using expert-annotated data are limited by\ndata scarcity and pre-defined classes, impacting their performance, flexibility\nand scalability. To address these issues, our study offers three main\ncontributions: 1) We demonstrate the potential of GPT as an adept labeler using\ncarefully designed prompts. 2) Utilizing only the data labeled by GPT, we\ntrained a BERT-based labeler, CheX-GPT, which operates faster and more\nefficiently than its GPT counterpart. 3) To benchmark labeler performance, we\nintroduced a publicly available expert-annotated test set, MIMIC-500,\ncomprising 500 cases from the MIMIC validation set. Our findings demonstrate\nthat CheX-GPT not only excels in labeling accuracy over existing models, but\nalso showcases superior efficiency, flexibility, and scalability, supported by\nour introduction of the MIMIC-500 dataset for robust benchmarking. Code and\nmodels are available at https://github.com/Soombit-ai/CheXGPT.", "published": "2024-01-21 14:30:20", "link": "http://arxiv.org/abs/2401.11505v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Freely Long-Thinking Transformer (FraiLT)", "abstract": "Freely Long-Thinking Transformer (FraiLT) is an improved transformer model\ndesigned to enhance processing capabilities without scaling up size. It\nutilizes a recursive approach, iterating over a subset of layers multiple\ntimes, and introduces iteration encodings to maintain awareness across these\ncycles. Iteration encoding allows FraiLT to achieve the interpretive depth of\nlarger models in a compact form. When evaluated on a synthetic story dataset,\nFraiLT outperformed larger models, showcasing its ability to deliver\nhigh-quality performance while reducing memory demands. This model represents a\nstep forward towards more efficient and accessible language models.", "published": "2024-01-21 23:37:33", "link": "http://arxiv.org/abs/2401.11626v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Revolutionizing API Documentation through Summarization", "abstract": "This study tackles the challenges associated with interpreting Application\nProgramming Interface (API) documentation, an integral aspect of software\ndevelopment. Official API documentation, while essential, can be lengthy and\nchallenging to navigate, prompting developers to seek unofficial sources such\nas Stack Overflow. Leveraging the vast user-generated content on Stack\nOverflow, including code snippets and discussions, we employ BERTopic and\nextractive summarization to automatically generate concise and informative API\nsummaries. These summaries encompass key insights like general usage, common\ndeveloper issues, and potential solutions, sourced from the wealth of knowledge\non Stack Overflow. Software developers evaluate these summaries for\nperformance, coherence, and interoperability, providing valuable feedback on\nthe practicality of our approach.", "published": "2024-01-21 01:18:08", "link": "http://arxiv.org/abs/2401.11361v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Language Models as Hierarchy Encoders", "abstract": "Interpreting hierarchical structures latent in language is a key limitation\nof current language models (LMs). While previous research has implicitly\nleveraged these hierarchies to enhance LMs, approaches for their explicit\nencoding are yet to be explored. To address this, we introduce a novel approach\nto re-train transformer encoder-based LMs as Hierarchy Transformer encoders\n(HiTs), harnessing the expansive nature of hyperbolic space. Our method\nsituates the output embedding space of pre-trained LMs within a Poincar\\'e ball\nwith a curvature that adapts to the embedding dimension, followed by training\non hyperbolic clustering and centripetal losses. These losses are designed to\neffectively cluster related entities (input as texts) and organise them\nhierarchically. We evaluate HiTs against pre-trained LMs, standard fine-tuned\nLMs, and several hyperbolic embedding baselines, focusing on their capabilities\nin simulating transitive inference, predicting subsumptions, and transferring\nknowledge across hierarchies. The results demonstrate that HiTs consistently\noutperform all baselines in these tasks, underscoring the effectiveness and\ntransferability of our re-trained hierarchy encoders.", "published": "2024-01-21 02:29:12", "link": "http://arxiv.org/abs/2401.11374v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MedLM: Exploring Language Models for Medical Question Answering Systems", "abstract": "In the face of rapidly expanding online medical literature, automated systems\nfor aggregating and summarizing information are becoming increasingly crucial\nfor healthcare professionals and patients. Large Language Models (LLMs), with\ntheir advanced generative capabilities, have shown promise in various NLP\ntasks, and their potential in the healthcare domain, particularly for\nClosed-Book Generative QnA, is significant. However, the performance of these\nmodels in domain-specific tasks such as medical Q&A remains largely unexplored.\nThis study aims to fill this gap by comparing the performance of general and\nmedical-specific distilled LMs for medical Q&A. We aim to evaluate the\neffectiveness of fine-tuning domain-specific LMs and compare the performance of\ndifferent families of Language Models. The study will address critical\nquestions about these models' reliability, comparative performance, and\neffectiveness in the context of medical Q&A. The findings will provide valuable\ninsights into the suitability of different LMs for specific applications in the\nmedical domain.", "published": "2024-01-21 03:37:47", "link": "http://arxiv.org/abs/2401.11389v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MolTailor: Tailoring Chemical Molecular Representation to Specific Tasks\n  via Text Prompts", "abstract": "Deep learning is now widely used in drug discovery, providing significant\nacceleration and cost reduction. As the most fundamental building block,\nmolecular representation is essential for predicting molecular properties to\nenable various downstream applications. Most existing methods attempt to\nincorporate more information to learn better representations. However, not all\nfeatures are equally important for a specific task. Ignoring this would\npotentially compromise the training efficiency and predictive accuracy. To\naddress this issue, we propose a novel approach, which treats language models\nas an agent and molecular pretraining models as a knowledge base. The agent\naccentuates task-relevant features in the molecular representation by\nunderstanding the natural language description of the task, just as a tailor\ncustomizes clothes for clients. Thus, we call this approach MolTailor.\nEvaluations demonstrate MolTailor's superior performance over baselines,\nvalidating the efficacy of enhancing relevance for molecular representation\nlearning. This illustrates the potential of language model guided optimization\nto better exploit and unleash the capabilities of existing powerful molecular\nrepresentation methods. Our code is available at\nhttps://github.com/SCIR-HI/MolTailor.", "published": "2024-01-21 04:54:45", "link": "http://arxiv.org/abs/2401.11403v2", "categories": ["cs.LG", "cs.CL", "q-bio.BM"], "primary_category": "cs.LG"}
{"title": "In-context Learning with Retrieved Demonstrations for Language Models: A\n  Survey", "abstract": "Language models, especially pre-trained large language models, have showcased\nremarkable abilities as few-shot in-context learners (ICL), adept at adapting\nto new tasks with just a few demonstrations in the input context. However, the\nmodel's ability to perform ICL is sensitive to the choice of the few-shot\ndemonstrations. Instead of using a fixed set of demonstrations, one recent\ndevelopment is to retrieve demonstrations tailored to each input query. The\nimplementation of demonstration retrieval is relatively straightforward,\nleveraging existing databases and retrieval systems. This not only improves the\nefficiency and scalability of the learning process but also has been shown to\nreduce biases inherent in manual example selection. In light of the encouraging\nresults and growing research in ICL with retrieved demonstrations, we conduct\nan extensive review of studies in this area. In this survey, we discuss and\ncompare different design choices for retrieval models, retrieval training\nprocedures, and inference algorithms.", "published": "2024-01-21 23:34:42", "link": "http://arxiv.org/abs/2401.11624v5", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Text-to-Image Cross-Modal Generation: A Systematic Review", "abstract": "We review research on generating visual data from text from the angle of\n\"cross-modal generation.\" This point of view allows us to draw parallels\nbetween various methods geared towards working on input text and producing\nvisual output, without limiting the analysis to narrow sub-areas. It also\nresults in the identification of common templates in the field, which are then\ncompared and contrasted both within pools of similar methods and across lines\nof research. We provide a breakdown of text-to-image generation into various\nflavors of image-from-text methods, video-from-text methods, image editing,\nself-supervised and graph-based approaches. In this discussion, we focus on\nresearch papers published at 8 leading machine learning conferences in the\nyears 2016-2022, also incorporating a number of relevant papers not matching\nthe outlined search criteria. The conducted review suggests a significant\nincrease in the number of papers published in the area and highlights research\ngaps and potential lines of investigation. To our knowledge, this is the first\nreview to systematically look at text-to-image generation from the perspective\nof \"cross-modal generation.\"", "published": "2024-01-21 23:54:05", "link": "http://arxiv.org/abs/2401.11631v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Instructional Fingerprinting of Large Language Models", "abstract": "The exorbitant cost of training Large language models (LLMs) from scratch\nmakes it essential to fingerprint the models to protect intellectual property\nvia ownership authentication and to ensure downstream users and developers\ncomply with their license terms (e.g. restricting commercial use). In this\nstudy, we present a pilot study on LLM fingerprinting as a form of very\nlightweight instruction tuning. Model publisher specifies a confidential\nprivate key and implants it as an instruction backdoor that causes the LLM to\ngenerate specific text when the key is present. Results on 11 popularly-used\nLLMs showed that this approach is lightweight and does not affect the normal\nbehavior of the model. It also prevents publisher overclaim, maintains\nrobustness against fingerprint guessing and parameter-efficient training, and\nsupports multi-stage fingerprinting akin to MIT License. Code is available in\nhttps://cnut1648.github.io/Model-Fingerprint/.", "published": "2024-01-21 09:51:45", "link": "http://arxiv.org/abs/2401.12255v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Large Language Model based Multi-Agents: A Survey of Progress and\n  Challenges", "abstract": "Large Language Models (LLMs) have achieved remarkable success across a wide\narray of tasks. Due to the impressive planning and reasoning abilities of LLMs,\nthey have been used as autonomous agents to do many tasks automatically.\nRecently, based on the development of using one LLM as a single planning or\ndecision-making agent, LLM-based multi-agent systems have achieved considerable\nprogress in complex problem-solving and world simulation. To provide the\ncommunity with an overview of this dynamic field, we present this survey to\noffer an in-depth discussion on the essential aspects of multi-agent systems\nbased on LLMs, as well as the challenges. Our goal is for readers to gain\nsubstantial insights on the following questions: What domains and environments\ndo LLM-based multi-agents simulate? How are these agents profiled and how do\nthey communicate? What mechanisms contribute to the growth of agents'\ncapacities? For those interested in delving into this field of study, we also\nsummarize the commonly used datasets or benchmarks for them to have convenient\naccess. To keep researchers updated on the latest studies, we maintain an\nopen-source GitHub repository, dedicated to outlining the research on LLM-based\nmulti-agent systems.", "published": "2024-01-21 23:36:14", "link": "http://arxiv.org/abs/2402.01680v2", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Detection of Auditory Brainstem Response Peaks Using Image Processing\n  Techniques in Infants with Normal Hearing Sensitivity", "abstract": "Introduction: The auditory brainstem response (ABR) is measured to find the\nbrainstem-level peripheral auditory nerve system integrity in children having\nnormal hearing. The Auditory Evoked Potential (AEP) is generated using acoustic\nstimuli. Interpreting these waves requires competence to avoid misdiagnosing\nhearing problems. Automating ABR test labeling with computer vision may reduce\nhuman error. Method: The ABR test results of 26 children aged 1 to 20 months\nwith normal hearing in both ears were used. A new approach is suggested for\nautomatically calculating the peaks of waves of different intensities (in\ndecibels). The procedure entails acquiring wave images from an Audera device\nusing the Color Thresholder method, segmenting each wave as a single wave image\nusing the Image Region Analyzer application, converting all wave images into\nwaves using Image Processing (IP) techniques, and finally calculating the\nlatency of the peaks for each wave to be used by an audiologist for diagnosing\nthe disease. Findings: Image processing techniques were able to detect 1, 3,\nand 5 waves in the diagnosis field with accuracy (0.82), (0.98), and (0.98),\nrespectively, and its precision for waves 1, 3, and 5, were respectively\n(0.32), (0.97) and (0.87). This evaluation also worked well in the thresholding\npart and 82.7 % correctly detected the ABR waves. Conclusion: Our findings\nindicate that the audiology test battery suite can be made more accurate,\nquick, and error-free by using technology to automatically detect and label ABR\nwaves.", "published": "2024-01-21 14:29:33", "link": "http://arxiv.org/abs/2401.17317v1", "categories": ["q-bio.NC", "cs.CV", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "q-bio.NC"}
