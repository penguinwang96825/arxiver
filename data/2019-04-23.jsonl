{"title": "TextKD-GAN: Text Generation using KnowledgeDistillation and Generative\n  Adversarial Networks", "abstract": "Text generation is of particular interest in many NLP applications such as\nmachine translation, language modeling, and text summarization. Generative\nadversarial networks (GANs) achieved a remarkable success in high quality image\ngeneration in computer vision,and recently, GANs have gained lots of interest\nfrom the NLP community as well. However, achieving similar success in NLP would\nbe more challenging due to the discrete nature of text. In this work, we\nintroduce a method using knowledge distillation to effectively exploit GAN\nsetup for text generation. We demonstrate how autoencoders (AEs) can be used\nfor providing a continuous representation of sentences, which is a smooth\nrepresentation that assign non-zero probabilities to more than one word. We\ndistill this representation to train the generator to synthesize similar smooth\nrepresentations. We perform a number of experiments to validate our idea using\ndifferent datasets and show that our proposed approach yields better\nperformance in terms of the BLEU score and Jensen-Shannon distance (JSD)\nmeasure compared to traditional GAN-based text generation approaches without\npre-training.", "published": "2019-04-23 15:15:12", "link": "http://arxiv.org/abs/1905.01976v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Task Learning for Argumentation Mining", "abstract": "Multi-task learning has recently become a very active field in deep learning\nresearch. In contrast to learning a single task in isolation, multiple tasks\nare learned at the same time, thereby utilizing the training signal of related\ntasks to improve the performance on the respective machine learning tasks.\nRelated work shows various successes in different domains when applying this\nparadigm and this thesis extends the existing empirical results by evaluating\nmulti-task learning in four different scenarios: argumentation mining,\nepistemic segmentation, argumentation component segmentation, and\ngrapheme-to-phoneme conversion. We show that multi-task learning can, indeed,\nimprove the performance compared to single-task learning in all these\nscenarios, but may also hurt the performance. Therefore, we investigate the\nreasons for successful and less successful applications of this paradigm and\nfind that dataset properties such as entropy or the size of the label inventory\nare good indicators for a potential multi-task learning success and that\nmulti-task learning is particularly useful if the task at hand suffers from\ndata sparsity, i.e. a lack of training data. Moreover, multi-task learning is\nparticularly effective for long input sequences in our experiments. We have\nobserved this trend in all evaluated scenarios. Finally, we develop a highly\nconfigurable and extensible sequence tagging framework which supports\nmulti-task learning to conduct our empirical experiments and to aid future\nresearch regarding the multi-task learning paradigm and natural language\nprocessing.", "published": "2019-04-23 05:58:54", "link": "http://arxiv.org/abs/1904.10162v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Empirical Evaluation of Leveraging Named Entities for Arabic Sentiment\n  Analysis", "abstract": "Social media reflects the public attitudes towards specific events. Events\nare often related to persons, locations or organizations, the so-called Named\nEntities. This can define Named Entities as sentiment-bearing components. In\nthis paper, we dive beyond Named Entities recognition to the exploitation of\nsentiment-annotated Named Entities in Arabic sentiment analysis. Therefore, we\ndevelop an algorithm to detect the sentiment of Named Entities based on the\nmajority of attitudes towards them. This enabled tagging Named Entities with\nproper tags and, thus, including them in a sentiment analysis framework of two\nmodels: supervised and lexicon-based. Both models were applied on datasets of\nmulti-dialectal content. The results revealed that Named Entities have no\nconsiderable impact on the supervised model, while employing them in the\nlexicon-based model improved the classification performance and outperformed\nmost of the baseline systems.", "published": "2019-04-23 08:28:11", "link": "http://arxiv.org/abs/1904.10195v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GumDrop at the DISRPT2019 Shared Task: A Model Stacking Approach to\n  Discourse Unit Segmentation and Connective Detection", "abstract": "In this paper we present GumDrop, Georgetown University's entry at the DISRPT\n2019 Shared Task on automatic discourse unit segmentation and connective\ndetection. Our approach relies on model stacking, creating a heterogeneous\nensemble of classifiers, which feed into a metalearner for each final task. The\nsystem encompasses three trainable component stacks: one for sentence\nsplitting, one for discourse unit segmentation and one for connective\ndetection. The flexibility of each ensemble allows the system to generalize\nwell to datasets of different sizes and with varying levels of homogeneity.", "published": "2019-04-23 16:55:04", "link": "http://arxiv.org/abs/1904.10419v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "REVERIE: Remote Embodied Visual Referring Expression in Real Indoor\n  Environments", "abstract": "One of the long-term challenges of robotics is to enable robots to interact\nwith humans in the visual world via natural language, as humans are visual\nanimals that communicate through language. Overcoming this challenge requires\nthe ability to perform a wide variety of complex tasks in response to\nmultifarious instructions from humans. In the hope that it might drive progress\ntowards more flexible and powerful human interactions with robots, we propose a\ndataset of varied and complex robot tasks, described in natural language, in\nterms of objects visible in a large set of real images. Given an instruction,\nsuccess requires navigating through a previously-unseen environment to identify\nan object. This represents a practical challenge, but one that closely reflects\none of the core visual problems in robotics. Several state-of-the-art\nvision-and-language navigation, and referring-expression models are tested to\nverify the difficulty of this new task, but none of them show promising results\nbecause there are many fundamental differences between our task and previous\nones. A novel Interactive Navigator-Pointer model is also proposed that\nprovides a strong baseline on the task. The proposed model especially achieves\nthe best performance on the unseen test split, but still leaves substantial\nroom for improvement compared to the human performance.", "published": "2019-04-23 04:45:28", "link": "http://arxiv.org/abs/1904.10151v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Quaternion Knowledge Graph Embeddings", "abstract": "In this work, we move beyond the traditional complex-valued representations,\nintroducing more expressive hypercomplex representations to model entities and\nrelations for knowledge graph embeddings. More specifically, quaternion\nembeddings, hypercomplex-valued embeddings with three imaginary components, are\nutilized to represent entities. Relations are modelled as rotations in the\nquaternion space. The advantages of the proposed approach are: (1) Latent\ninter-dependencies (between all components) are aptly captured with Hamilton\nproduct, encouraging a more compact interaction between entities and relations;\n(2) Quaternions enable expressive rotation in four-dimensional space and have\nmore degree of freedom than rotation in complex plane; (3) The proposed\nframework is a generalization of ComplEx on hypercomplex space while offering\nbetter geometrical interpretations, concurrently satisfying the key desiderata\nof relational representation learning (i.e., modeling symmetry, anti-symmetry\nand inversion). Experimental results demonstrate that our method achieves\nstate-of-the-art performance on four well-established knowledge graph\ncompletion benchmarks.", "published": "2019-04-23 12:36:59", "link": "http://arxiv.org/abs/1904.10281v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Wasserstein-Fisher-Rao Document Distance", "abstract": "As a fundamental problem of natural language processing, it is important to\nmeasure the distance between different documents. Among the existing methods,\nthe Word Mover's Distance (WMD) has shown remarkable success in document\nsemantic matching for its clear physical insight as a parameter-free model.\nHowever, WMD is essentially based on the classical Wasserstein metric, thus it\noften fails to robustly represent the semantic similarity between texts of\ndifferent lengths. In this paper, we apply the newly developed\nWasserstein-Fisher-Rao (WFR) metric from unbalanced optimal transport theory to\nmeasure the distance between different documents. The proposed WFR document\ndistance maintains the great interpretability and simplicity as WMD. We\ndemonstrate that the WFR document distance has significant advantages when\ncomparing the texts of different lengths. In addition, an accelerated Sinkhorn\nbased algorithm with GPU implementation has been developed for the fast\ncomputation of WFR distances. The KNN classification results on eight datasets\nhave shown its clear improvement over WMD.", "published": "2019-04-23 13:11:40", "link": "http://arxiv.org/abs/1904.10294v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Fine-Grained Named Entity Recognition using ELMo and Wikidata", "abstract": "Fine-grained Named Entity Recognition is a task whereby we detect and\nclassify entity mentions to a large set of types. These types can span diverse\ndomains such as finance, healthcare, and politics. We observe that when the\ntype set spans several domains the accuracy of the entity detection becomes a\nlimitation for supervised learning models. The primary reason being the lack of\ndatasets where entity boundaries are properly annotated, whilst covering a\nlarge spectrum of entity types. Furthermore, many named entity systems suffer\nwhen considering the categorization of fine grained entity types. Our work\nattempts to address these issues, in part, by combining state-of-the-art deep\nlearning models (ELMo) with an expansive knowledge base (Wikidata). Using our\nframework, we cross-validate our model on the 112 fine-grained entity types\nbased on the hierarchy given from the Wiki(gold) dataset.", "published": "2019-04-23 19:18:26", "link": "http://arxiv.org/abs/1904.10503v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "End-to-End Spoken Language Translation", "abstract": "In this paper, we address the task of spoken language understanding. We\npresent a method for translating spoken sentences from one language into spoken\nsentences in another language. Given spectrogram-spectrogram pairs, our model\ncan be trained completely from scratch to translate unseen sentences. Our\nmethod consists of a pyramidal-bidirectional recurrent network combined with a\nconvolutional network to output sentence-level spectrograms in the target\nlanguage. Empirically, our model achieves competitive performance with\nstate-of-the-art methods on multiple languages and can generalize to unseen\nspeakers.", "published": "2019-04-23 05:27:12", "link": "http://arxiv.org/abs/1904.10760v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Natural Language Interactions in Autonomous Vehicles: Intent Detection\n  and Slot Filling from Passenger Utterances", "abstract": "Understanding passenger intents and extracting relevant slots are important\nbuilding blocks towards developing contextual dialogue systems for natural\ninteractions in autonomous vehicles (AV). In this work, we explored AMIE\n(Automated-vehicle Multi-modal In-cabin Experience), the in-cabin agent\nresponsible for handling certain passenger-vehicle interactions. When the\npassengers give instructions to AMIE, the agent should parse such commands\nproperly and trigger the appropriate functionality of the AV system. In our\ncurrent explorations, we focused on AMIE scenarios describing usages around\nsetting or changing the destination and route, updating driving behavior or\nspeed, finishing the trip and other use-cases to support various natural\ncommands. We collected a multi-modal in-cabin dataset with multi-turn dialogues\nbetween the passengers and AMIE using a Wizard-of-Oz scheme via a realistic\nscavenger hunt game activity. After exploring various recent Recurrent Neural\nNetworks (RNN) based techniques, we introduced our own hierarchical joint\nmodels to recognize passenger intents along with relevant slots associated with\nthe action to be performed in AV scenarios. Our experimental results\noutperformed certain competitive baselines and achieved overall F1 scores of\n0.91 for utterance-level intent detection and 0.96 for slot filling tasks. In\naddition, we conducted initial speech-to-text explorations by comparing\nintent/slot models trained and tested on human transcriptions versus noisy\nAutomatic Speech Recognition (ASR) outputs. Finally, we compared the results\nwith single passenger rides versus the rides with multiple passengers.", "published": "2019-04-23 19:13:51", "link": "http://arxiv.org/abs/1904.10500v1", "categories": ["cs.CL", "cs.HC", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Speech Emotion Recognition Using Multi-hop Attention Mechanism", "abstract": "In this paper, we are interested in exploiting textual and acoustic data of\nan utterance for the speech emotion classification task. The baseline approach\nmodels the information from audio and text independently using two deep neural\nnetworks (DNNs). The outputs from both the DNNs are then fused for\nclassification. As opposed to using knowledge from both the modalities\nseparately, we propose a framework to exploit acoustic information in tandem\nwith lexical data. The proposed framework uses two bi-directional long\nshort-term memory (BLSTM) for obtaining hidden representations of the\nutterance. Furthermore, we propose an attention mechanism, referred to as the\nmulti-hop, which is trained to automatically infer the correlation between the\nmodalities. The multi-hop attention first computes the relevant segments of the\ntextual data corresponding to the audio signal. The relevant textual data is\nthen applied to attend parts of the audio signal. To evaluate the performance\nof the proposed system, experiments are performed in the IEMOCAP dataset.\nExperimental results show that the proposed technique outperforms the\nstate-of-the-art system by 6.5% relative improvement in terms of weighted\naccuracy.", "published": "2019-04-23 13:09:21", "link": "http://arxiv.org/abs/1904.10788v2", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Acoustic scene classification using teacher-student learning with\n  soft-labels", "abstract": "Acoustic scene classification identifies an input segment into one of the\npre-defined classes using spectral information. The spectral information of\nacoustic scenes may not be mutually exclusive due to common acoustic properties\nacross different classes, such as babble noises included in both airports and\nshopping malls. However, conventional training procedure based on one-hot\nlabels does not consider the similarities between different acoustic scenes. We\nexploit teacher-student learning with the purpose to derive soft-labels that\nconsider common acoustic properties among different acoustic scenes. In\nteacher-student learning, the teacher network produces soft-labels, based on\nwhich the student network is trained. We investigate various methods to extract\nsoft-labels that better represent similarities across different scenes. Such\nattempts include extracting soft-labels from multiple audio segments that are\ndefined as an identical acoustic scene. Experimental results demonstrate the\npotential of our approach, showing a classification accuracy of 77.36 % on the\nDCASE 2018 task 1 validation set.", "published": "2019-04-23 03:42:20", "link": "http://arxiv.org/abs/1904.10135v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards joint sound scene and polyphonic sound event recognition", "abstract": "Acoustic Scene Classification (ASC) and Sound Event Detection (SED) are two\nseparate tasks in the field of computational sound scene analysis. In this\nwork, we present a new dataset with both sound scene and sound event labels and\nuse this to demonstrate a novel method for jointly classifying sound scenes and\nrecognizing sound events. We show that by taking a joint approach, learning is\nmore efficient and whilst improvements are still needed for sound event\ndetection, SED results are robust in a dataset where the sample distribution is\nskewed towards sound scenes.", "published": "2019-04-23 16:25:53", "link": "http://arxiv.org/abs/1904.10408v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Analogue Computer as a Voltage-Controlled Synthesiser", "abstract": "This paper re-appraises the role of analogue computers within electronic and\ncomputer music and provides some pointers to future areas of research. It\nbegins by introducing the idea of analogue computing and placing in the context\nof sound and music applications. This is followed by a brief examination of the\nclassic constituents of an analogue computer, contrasting these with the\ntypical modular voltage-controlled synthesiser. Two examples are presented,\nleading to a discussion on some parallels between these two technologies. This\nis followed by an examination of the current state-of-the-art in analogue\ncomputation and its prospects for applications in computer and electronic\nmusic.", "published": "2019-04-23 10:23:59", "link": "http://arxiv.org/abs/1904.10763v2", "categories": ["eess.AS", "cs.SD", "68U99", "J.5"], "primary_category": "eess.AS"}
{"title": "Replay attack detection with complementary high-resolution information\n  using end-to-end DNN for the ASVspoof 2019 Challenge", "abstract": "In this study, we concentrate on replacing the process of extracting\nhand-crafted acoustic feature with end-to-end DNN using complementary\nhigh-resolution spectrograms. As a result of advance in audio devices, typical\ncharacteristics of a replayed speech based on conventional knowledge alter or\ndiminish in unknown replay configurations. Thus, it has become increasingly\ndifficult to detect spoofed speech with a conventional knowledge-based\napproach. To detect unrevealed characteristics that reside in a replayed\nspeech, we directly input spectrograms into an end-to-end DNN without\nknowledge-based intervention. Explorations dealt in this study that\ndifferentiates from existing spectrogram-based systems are twofold:\ncomplementary information and high-resolution. Spectrograms with different\ninformation are explored, and it is shown that additional information such as\nthe phase information can be complementary. High-resolution spectrograms are\nemployed with the assumption that the difference between a bona-fide and a\nreplayed speech exists in the details. Additionally, to verify whether other\nfeatures are complementary to spectrograms, we also examine raw waveform and an\ni-vector based system. Experiments conducted on the ASVspoof 2019 physical\naccess challenge show promising results, where t-DCF and equal error rates are\n0.0570 and 2.45 % for the evaluation set, respectively.", "published": "2019-04-23 03:29:36", "link": "http://arxiv.org/abs/1904.10134v2", "categories": ["eess.AS", "cs.CR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Statistical Learning and Estimation of Piano Fingering", "abstract": "Automatic estimation of piano fingering is important for understanding the\ncomputational process of music performance and applicable to performance\nassistance and education systems. While a natural way to formulate the quality\nof fingerings is to construct models of the constraints/costs of performance,\nit is generally difficult to find appropriate parameter values for these\nmodels. Here we study an alternative data-driven approach based on statistical\nmodeling in which the appropriateness of a given fingering is described by\nprobabilities. Specifically, we construct two types of hidden Markov models\n(HMMs) and their higher-order extensions. We also study deep neural network\n(DNN)-based methods for comparison. Using a newly released dataset of fingering\nannotations, we conduct systematic evaluations of these models as well as a\nrepresentative constraint-based method. We find that the methods based on\nhigh-order HMMs outperform the other methods in terms of estimation accuracies.\nWe also quantitatively study individual difference of fingering and propose\nevaluation measures that can be used with multiple ground truth data. We\nconclude that the HMM-based methods are currently state of the art and generate\nacceptable fingerings in most parts and that they have certain limitations such\nas ignorance of phrase boundaries and interdependence of the two hands.", "published": "2019-04-23 10:25:31", "link": "http://arxiv.org/abs/1904.10237v2", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Harmonic-aligned Frame Mask Based on Non-stationary Gabor Transform with\n  Application to Content-dependent Speaker Comparison", "abstract": "We propose harmonic-aligned frame mask for speech signals using\nnon-stationary Gabor transform (NSGT). A frame mask operates on the transfer\ncoefficients of a signal and consequently converts the signal into a\ncounterpart signal. It depicts the difference between the two signals. In\npreceding studies, frame masks based on regular Gabor transform were applied to\nsingle-note instrumental sound analysis. This study extends the frame mask\napproach to speech signals. For voiced speech, the fundamental frequency is\nusually changing consecutively over time. We employ NSGT with pitch-dependent\nand therefore time-varying frequency resolution to attain harmonic alignment in\nthe transform domain and hence yield harmonic-aligned frame masks for speech\nsignals. We propose to apply the harmonic-aligned frame mask to\ncontent-dependent speaker comparison. Frame masks, computed from voiced signals\nof a same vowel but from different speakers, were utilized as similarity\nmeasures to compare and distinguish the speaker identities (SID). Results\nobtained with deep neural networks demonstrate that the proposed frame mask is\nvalid in representing speaker characteristics and shows a potential for SID\napplications in limited data scenarios.", "published": "2019-04-23 15:21:09", "link": "http://arxiv.org/abs/1904.10380v1", "categories": ["cs.SD", "eess.AS", "math.SP"], "primary_category": "cs.SD"}
{"title": "Latent Variable Algorithms for Multimodal Learning and Sensor Fusion", "abstract": "Multimodal learning has been lacking principled ways of combining information\nfrom different modalities and learning a low-dimensional manifold of meaningful\nrepresentations. We study multimodal learning and sensor fusion from a latent\nvariable perspective. We first present a regularized recurrent attention filter\nfor sensor fusion. This algorithm can dynamically combine information from\ndifferent types of sensors in a sequential decision making task. Each sensor is\nbonded with a modular neural network to maximize utility of its own\ninformation. A gating modular neural network dynamically generates a set of\nmixing weights for outputs from sensor networks by balancing utility of all\nsensors' information. We design a co-learning mechanism to encourage\nco-adaption and independent learning of each sensor at the same time, and\npropose a regularization based co-learning method. In the second part, we focus\non recovering the manifold of latent representation. We propose a co-learning\napproach using probabilistic graphical model which imposes a structural prior\non the generative model: multimodal variational RNN (MVRNN) model, and derive a\nvariational lower bound for its objective functions. In the third part, we\nextend the siamese structure to sensor fusion for robust acoustic event\ndetection. We perform experiments to investigate the latent representations\nthat are extracted; works will be done in the following months. Our experiments\nshow that the recurrent attention filter can dynamically combine different\nsensor inputs according to the information carried in the inputs. We consider\nMVRNN can identify latent representations that are useful for many downstream\ntasks such as speech synthesis, activity recognition, and control and planning.\nBoth algorithms are general frameworks which can be applied to other tasks\nwhere different types of sensors are jointly used for decision making.", "published": "2019-04-23 17:58:19", "link": "http://arxiv.org/abs/1904.10450v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
