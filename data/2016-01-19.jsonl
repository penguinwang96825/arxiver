{"title": "Modeling Coverage for Neural Machine Translation", "abstract": "Attention mechanism has enhanced state-of-the-art Neural Machine Translation\n(NMT) by jointly learning to align and translate. It tends to ignore past\nalignment information, however, which often leads to over-translation and\nunder-translation. To address this problem, we propose coverage-based NMT in\nthis paper. We maintain a coverage vector to keep track of the attention\nhistory. The coverage vector is fed to the attention model to help adjust\nfuture attention, which lets NMT system to consider more about untranslated\nsource words. Experiments show that the proposed approach significantly\nimproves both translation quality and alignment quality over standard\nattention-based NMT.", "published": "2016-01-19 07:09:38", "link": "http://arxiv.org/abs/1601.04811v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graded Entailment for Compositional Distributional Semantics", "abstract": "The categorical compositional distributional model of natural language\nprovides a conceptually motivated procedure to compute the meaning of\nsentences, given grammatical structure and the meanings of its words. This\napproach has outperformed other models in mainstream empirical language\nprocessing tasks. However, until recently it has lacked the crucial feature of\nlexical entailment -- as do other distributional models of meaning.\n  In this paper we solve the problem of entailment for categorical\ncompositional distributional semantics. Taking advantage of the abstract\ncategorical framework allows us to vary our choice of model. This enables the\nintroduction of a notion of entailment, exploiting ideas from the categorical\nsemantics of partial knowledge in quantum computation.\n  The new model of language uses density matrices, on which we introduce a\nnovel robust graded order capturing the entailment strength between concepts.\nThis graded measure emerges from a general framework for approximate\nentailment, induced by any commutative monoid. Quantum logic embeds in our\ngraded order.\n  Our main theorem shows that entailment strength lifts compositionally to the\nsentence level, giving a lower bound on sentence entailment. We describe the\nessential properties of graded entailment such as continuity, and provide a\nprocedure for calculating entailment strength.", "published": "2016-01-19 13:13:25", "link": "http://arxiv.org/abs/1601.04908v2", "categories": ["cs.CL", "cs.AI", "cs.LO", "math.CT", "quant-ph"], "primary_category": "cs.CL"}
