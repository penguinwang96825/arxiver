{"title": "Long Document Summarization with Top-down and Bottom-up Inference", "abstract": "Text summarization aims to condense long documents and retain key\ninformation. Critical to the success of a summarization model is the faithful\ninference of latent representations of words or tokens in the source documents.\nMost recent models infer the latent representations with a transformer encoder,\nwhich is purely bottom-up. Also, self-attention-based inference models face the\nchallenge of quadratic complexity with respect to sequence length. We propose a\nprincipled inference framework to improve summarization models on these two\naspects. Our framework assumes a hierarchical latent structure of a document\nwhere the top-level captures the long range dependency at a coarser time scale\nand the bottom token level preserves the details. Critically, this hierarchical\nstructure enables token representations to be updated in both a bottom-up and\ntop-down manner. In the bottom-up pass, token representations are inferred with\nlocal self-attention to leverage its efficiency. Top-down correction is then\napplied to allow tokens to capture long-range dependency. We demonstrate the\neffectiveness of the proposed framework on a diverse set of summarization\ndatasets, including narrative, conversational, scientific documents and news.\nOur model achieves (1) competitive or better performance on short documents\nwith higher memory and compute efficiency, compared to full attention\ntransformers, and (2) state-of-the-art performance on a wide range of long\ndocument summarization benchmarks, compared to recent efficient transformers.\nWe also show that our model can summarize an entire book and achieve\ncompetitive performance using $0.27\\%$ parameters (464M vs. 175B) and much less\ntraining data, compared to a recent GPT-3-based model. These results indicate\nthe general applicability and benefits of the proposed framework.", "published": "2022-03-15 01:24:51", "link": "http://arxiv.org/abs/2203.07586v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Procedural Text Understanding via Scene-Wise Evolution", "abstract": "Procedural text understanding requires machines to reason about entity states\nwithin the dynamical narratives. Current procedural text understanding\napproaches are commonly \\textbf{entity-wise}, which separately track each\nentity and independently predict different states of each entity. Such an\nentity-wise paradigm does not consider the interaction between entities and\ntheir states. In this paper, we propose a new \\textbf{scene-wise} paradigm for\nprocedural text understanding, which jointly tracks states of all entities in a\nscene-by-scene manner. Based on this paradigm, we propose \\textbf{S}cene\n\\textbf{G}raph \\textbf{R}easoner (\\textbf{SGR}), which introduces a series of\ndynamically evolving scene graphs to jointly formulate the evolution of\nentities, states and their associations throughout the narrative. In this way,\nthe deep interactions between all entities and states can be jointly captured\nand simultaneously derived from scene graphs. Experiments show that SGR not\nonly achieves the new state-of-the-art performance but also significantly\naccelerates the speed of reasoning.", "published": "2022-03-15 02:17:33", "link": "http://arxiv.org/abs/2203.07600v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Event Representation via Simultaneous Weakly Supervised\n  Contrastive Learning and Clustering", "abstract": "Representations of events described in text are important for various tasks.\nIn this work, we present SWCC: a Simultaneous Weakly supervised Contrastive\nlearning and Clustering framework for event representation learning. SWCC\nlearns event representations by making better use of co-occurrence information\nof events. Specifically, we introduce a weakly supervised contrastive learning\nmethod that allows us to consider multiple positives and multiple negatives,\nand a prototype-based clustering method that avoids semantically related events\nbeing pulled apart. For model training, SWCC learns representations by\nsimultaneously performing weakly supervised contrastive learning and\nprototype-based clustering. Experimental results show that SWCC outperforms\nother baselines on Hard Similarity and Transitive Sentence Similarity tasks. In\naddition, a thorough analysis of the prototype-based clustering method\ndemonstrates that the learned prototype vectors are able to implicitly capture\nvarious relations between events.", "published": "2022-03-15 04:12:00", "link": "http://arxiv.org/abs/2203.07633v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Keyphrase Extraction via Interpretable Neural Networks", "abstract": "Keyphrase extraction aims at automatically extracting a list of \"important\"\nphrases representing the key concepts in a document. Prior approaches for\nunsupervised keyphrase extraction resorted to heuristic notions of phrase\nimportance via embedding clustering or graph centrality, requiring extensive\ndomain expertise. Our work presents a simple alternative approach which defines\nkeyphrases as document phrases that are salient for predicting the topic of the\ndocument. To this end, we propose INSPECT -- an approach that uses\nself-explaining models for identifying influential keyphrases in a document by\nmeasuring the predictive impact of input phrases on the downstream task of the\ndocument topic classification. We show that this novel method not only\nalleviates the need for ad-hoc heuristics but also achieves state-of-the-art\nresults in unsupervised keyphrase extraction in four datasets across two\ndomains: scientific publications and news articles.", "published": "2022-03-15 04:30:47", "link": "http://arxiv.org/abs/2203.07640v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Synthetic Translations Improve Bitext Quality?", "abstract": "Synthetic translations have been used for a wide range of NLP tasks primarily\nas a means of data augmentation. This work explores, instead, how synthetic\ntranslations can be used to revise potentially imperfect reference translations\nin mined bitext. We find that synthetic samples can improve bitext quality\nwithout any additional bilingual supervision when they replace the originals\nbased on a semantic equivalence classifier that helps mitigate NMT noise. The\nimproved quality of the revised bitext is confirmed intrinsically via human\nevaluation and extrinsically through bilingual induction and MT tasks.", "published": "2022-03-15 04:36:29", "link": "http://arxiv.org/abs/2203.07643v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Long Sequence Encoding via Synchronization", "abstract": "Pre-trained Transformer models have achieved successes in a wide range of NLP\ntasks, but are inefficient when dealing with long input sequences. Existing\nstudies try to overcome this challenge via segmenting the long sequence\nfollowed by hierarchical encoding or post-hoc aggregation. We propose a\nsynchronization mechanism for hierarchical encoding. Our approach first\nidentifies anchor tokens across segments and groups them by their roles in the\noriginal input sequence. Then inside Transformer layer, anchor embeddings are\nsynchronized within their group via a self-attention module. Our approach is a\ngeneral framework with sufficient flexibility -- when adapted to a new task, it\nis easy to be enhanced with the task-specific anchor definitions. Experiments\non two representative tasks with different types of long input texts,\nNarrativeQA summary setting and wild multi-hop reasoning from HotpotQA,\ndemonstrate that our approach is able to improve the global information\nexchange among segments while maintaining efficiency.", "published": "2022-03-15 04:37:02", "link": "http://arxiv.org/abs/2203.07644v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do BERTs Learn to Use Browser User Interface? Exploring Multi-Step Tasks\n  with Unified Vision-and-Language BERTs", "abstract": "Pre-trained Transformers are good foundations for unified multi-task models\nowing to their task-agnostic representation. Pre-trained Transformers are often\ncombined with text-to-text framework to execute multiple tasks by a single\nmodel. Performing a task through a graphical user interface (GUI) is another\ncandidate to accommodate various tasks, including multi-step tasks with vision\nand language inputs. However, few papers combine pre-trained Transformers with\nperforming through GUI. To fill this gap, we explore a framework in which a\nmodel performs a task by manipulating the GUI implemented with web pages in\nmultiple steps. We develop task pages with and without page transitions and\npropose a BERT extension for the framework. We jointly trained our BERT\nextension with those task pages, and made the following observations. (1) The\nmodel learned to use both task pages with and without page transition. (2) In\nfour out of five tasks without page transitions, the model performs greater\nthan 75% of the performance of the original BERT, which does not use browsers.\n(3) The model did not generalize effectively on unseen tasks. These results\nsuggest that we can fine-tune BERTs to multi-step tasks through GUIs, and there\nis room for improvement in their generalizability. Code will be available\nonline.", "published": "2022-03-15 12:32:28", "link": "http://arxiv.org/abs/2203.07828v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph Pre-training for AMR Parsing and Generation", "abstract": "Abstract meaning representation (AMR) highlights the core semantic\ninformation of text in a graph structure. Recently, pre-trained language models\n(PLMs) have advanced tasks of AMR parsing and AMR-to-text generation,\nrespectively. However, PLMs are typically pre-trained on textual data, thus are\nsub-optimal for modeling structural knowledge. To this end, we investigate\ngraph self-supervised training to improve the structure awareness of PLMs over\nAMR graphs. In particular, we introduce two graph auto-encoding strategies for\ngraph-to-graph pre-training and four tasks to integrate text and graph\ninformation during pre-training. We further design a unified framework to\nbridge the gap between pre-training and fine-tuning tasks. Experiments on both\nAMR parsing and AMR-to-text generation show the superiority of our model. To\nour knowledge, we are the first to consider pre-training on semantic graphs.", "published": "2022-03-15 12:47:00", "link": "http://arxiv.org/abs/2203.07836v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Imputing Out-of-Vocabulary Embeddings with LOVE Makes Language Models\n  Robust with Little Cost", "abstract": "State-of-the-art NLP systems represent inputs with word embeddings, but these\nare brittle when faced with Out-of-Vocabulary (OOV) words. To address this\nissue, we follow the principle of mimick-like models to generate vectors for\nunseen words, by learning the behavior of pre-trained embeddings using only the\nsurface form of words. We present a simple contrastive learning framework,\nLOVE, which extends the word representation of an existing pre-trained language\nmodel (such as BERT), and makes it robust to OOV with few additional\nparameters. Extensive evaluations demonstrate that our lightweight model\nachieves similar or even better performances than prior competitors, both on\noriginal datasets and on corrupted variants. Moreover, it can be used in a\nplug-and-play fashion with FastText and BERT, where it significantly improves\ntheir robustness.", "published": "2022-03-15 13:11:07", "link": "http://arxiv.org/abs/2203.07860v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Extractive Opinion Summarization Using Sparse Coding", "abstract": "Opinion summarization is the task of automatically generating summaries that\nencapsulate information from multiple user reviews. We present Semantic\nAutoencoder (SemAE) to perform extractive opinion summarization in an\nunsupervised manner. SemAE uses dictionary learning to implicitly capture\nsemantic information from the review and learns a latent representation of each\nsentence over semantic units. A semantic unit is supposed to capture an\nabstract semantic concept. Our extractive summarization algorithm leverages the\nrepresentations to identify representative opinions among hundreds of reviews.\nSemAE is also able to perform controllable summarization to generate\naspect-specific summaries. We report strong performance on SPACE and AMAZON\ndatasets, and perform experiments to investigate the functioning of our model.\nOur code is publicly available at https://github.com/brcsomnath/SemAE.", "published": "2022-03-15 14:03:35", "link": "http://arxiv.org/abs/2203.07921v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modular and Parameter-Efficient Multimodal Fusion with Prompting", "abstract": "Recent research has made impressive progress in large-scale multimodal\npre-training. In the context of the rapid growth of model size, it is necessary\nto seek efficient and flexible methods other than finetuning. In this paper, we\npropose to use prompt vectors to align the modalities. Our method achieves\ncomparable performance to several other multimodal fusion methods in\nlow-resource settings. We further show that our method is modular and\nparameter-efficient for processing tasks involving two or more data modalities.", "published": "2022-03-15 16:50:15", "link": "http://arxiv.org/abs/2203.08055v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring the Impact of (Psycho-)Linguistic and Readability Features and\n  Their Spill Over Effects on the Prediction of Eye Movement Patterns", "abstract": "There is a growing interest in the combined use of NLP and machine learning\nmethods to predict gaze patterns during naturalistic reading. While promising\nresults have been obtained through the use of transformer-based language\nmodels, little work has been undertaken to relate the performance of such\nmodels to general text characteristics. In this paper we report on experiments\nwith two eye-tracking corpora of naturalistic reading and two language models\n(BERT and GPT-2). In all experiments, we test effects of a broad spectrum of\nfeatures for predicting human reading behavior that fall into five categories\n(syntactic complexity, lexical richness, register-based multiword combinations,\nreadability and psycholinguistic word properties). Our experiments show that\nboth the features included and the architecture of the transformer-based\nlanguage models play a role in predicting multiple eye-tracking measures during\nnaturalistic reading. We also report the results of experiments aimed at\ndetermining the relative importance of features from different groups using\nSP-LIME.", "published": "2022-03-15 17:13:45", "link": "http://arxiv.org/abs/2203.08085v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Representation Learning for Resource-Constrained Keyphrase Generation", "abstract": "State-of-the-art keyphrase generation methods generally depend on large\nannotated datasets, limiting their performance in domains with limited\nannotated data. To overcome this challenge, we design a data-oriented approach\nthat first identifies salient information using retrieval-based corpus-level\nstatistics, and then learns a task-specific intermediate representation based\non a pre-trained language model using large-scale unlabeled documents. We\nintroduce salient span recovery and salient span prediction as denoising\ntraining objectives that condense the intra-article and inter-article knowledge\nessential for keyphrase generation. Through experiments on multiple keyphrase\ngeneration benchmarks, we show the effectiveness of the proposed approach for\nfacilitating low-resource keyphrase generation and zero-shot domain adaptation.\nOur method especially benefits the generation of absent keyphrases, approaching\nthe performance of models trained with large training sets.", "published": "2022-03-15 17:48:04", "link": "http://arxiv.org/abs/2203.08118v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Differentiable Multi-Agent Actor-Critic for Multi-Step Radiology Report\n  Summarization", "abstract": "The IMPRESSIONS section of a radiology report about an imaging study is a\nsummary of the radiologist's reasoning and conclusions, and it also aids the\nreferring physician in confirming or excluding certain diagnoses. A cascade of\ntasks are required to automatically generate an abstractive summary of the\ntypical information-rich radiology report. These tasks include acquisition of\nsalient content from the report and generation of a concise, easily consumable\nIMPRESSIONS section. Prior research on radiology report summarization has\nfocused on single-step end-to-end models -- which subsume the task of salient\ncontent acquisition. To fully explore the cascade structure and explainability\nof radiology report summarization, we introduce two innovations. First, we\ndesign a two-step approach: extractive summarization followed by abstractive\nsummarization. Second, we additionally break down the extractive part into two\nindependent tasks: extraction of salient (1) sentences and (2) keywords.\nExperiments on English radiology reports from two clinical sites show our novel\napproach leads to a more precise summary compared to single-step and to\ntwo-step-with-single-extractive-process baselines with an overall improvement\nin F1 score Of 3-4%.", "published": "2022-03-15 21:18:09", "link": "http://arxiv.org/abs/2203.08257v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Non-neural Models Matter: A Re-evaluation of Neural Referring Expression\n  Generation Systems", "abstract": "In recent years, neural models have often outperformed rule-based and classic\nMachine Learning approaches in NLG. These classic approaches are now often\ndisregarded, for example when new neural models are evaluated. We argue that\nthey should not be overlooked, since, for some tasks, well-designed non-neural\napproaches achieve better performance than neural ones. In this paper, the task\nof generating referring expressions in linguistic context is used as an\nexample. We examined two very different English datasets (WEBNLG and WSJ), and\nevaluated each algorithm using both automatic and human evaluations. Overall,\nthe results of these evaluations suggest that rule-based systems with simple\nrule sets achieve on-par or better performance on both datasets compared to\nstate-of-the-art neural REG systems. In the case of the more realistic dataset,\nWSJ, a machine learning-based system with well-designed linguistic features\nperformed best. We hope that our work can encourage researchers to consider\nnon-neural models in future.", "published": "2022-03-15 21:47:25", "link": "http://arxiv.org/abs/2203.08274v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FastKASSIM: A Fast Tree Kernel-Based Syntactic Similarity Metric", "abstract": "Syntax is a fundamental component of language, yet few metrics have been\nemployed to capture syntactic similarity or coherence at the utterance- and\ndocument-level. The existing standard document-level syntactic similarity\nmetric is computationally expensive and performs inconsistently when faced with\nsyntactically dissimilar documents. To address these challenges, we present\nFastKASSIM, a metric for utterance- and document-level syntactic similarity\nwhich pairs and averages the most similar constituency parse trees between a\npair of documents based on tree kernels. FastKASSIM is more robust to syntactic\ndissimilarities and runs up to to 5.32 times faster than its predecessor over\ndocuments in the r/ChangeMyView corpus. FastKASSIM's improvements allow us to\nexamine hypotheses in two settings with large documents. We find that\nsyntactically similar arguments on r/ChangeMyView tend to be more persuasive,\nand that syntax is predictive of authorship attribution in the Australian High\nCourt Judgment corpus.", "published": "2022-03-15 22:33:26", "link": "http://arxiv.org/abs/2203.08299v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hyperdecoders: Instance-specific decoders for multi-task NLP", "abstract": "We investigate input-conditioned hypernetworks for multi-tasking in NLP,\ngenerating parameter-efficient adaptations for a decoder using a hypernetwork\nconditioned on the output of an encoder. This approach produces a unique\ndecoder adaptation for every input instance, allowing the network a larger\ndegree of flexibility than prior work that only produces one decoder adaptation\nper task. We apply our method to sequence classification tasks, extractive QA,\nand summarisation and find that it surpasses previous parameter efficient\nfine-tuning methods and often outperforms fully finetuning the underlying\nmodel. An analysis of the embeddings used by our hypernetwork shows that they\nare sensitive to output label and type, suggesting that our approach better\nmaps from encoder representations to output labels. Our code is publicly\navailable at https://github.com/allenai/hyperdecoders.", "published": "2022-03-15 22:39:53", "link": "http://arxiv.org/abs/2203.08304v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Generative Language Models for Zero-Shot Cross-Lingual\n  Event Argument Extraction", "abstract": "We present a study on leveraging multilingual pre-trained generative language\nmodels for zero-shot cross-lingual event argument extraction (EAE). By\nformulating EAE as a language generation task, our method effectively encodes\nevent structures and captures the dependencies between arguments. We design\nlanguage-agnostic templates to represent the event argument structures, which\nare compatible with any language, hence facilitating the cross-lingual\ntransfer. Our proposed model finetunes multilingual pre-trained generative\nlanguage models to generate sentences that fill in the language-agnostic\ntemplate with arguments extracted from the input passage. The model is trained\non source languages and is then directly applied to target languages for event\nargument extraction. Experiments demonstrate that the proposed model\noutperforms the current state-of-the-art models on zero-shot cross-lingual EAE.\nComprehensive studies and error analyses are presented to better understand the\nadvantages and the current limitations of using generative language models for\nzero-shot cross-lingual transfer EAE.", "published": "2022-03-15 23:00:32", "link": "http://arxiv.org/abs/2203.08308v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CARETS: A Consistency And Robustness Evaluative Test Suite for VQA", "abstract": "We introduce CARETS, a systematic test suite to measure consistency and\nrobustness of modern VQA models through a series of six fine-grained capability\ntests. In contrast to existing VQA test sets, CARETS features balanced question\ngeneration to create pairs of instances to test models, with each pair focusing\non a specific capability such as rephrasing, logical symmetry or image\nobfuscation. We evaluate six modern VQA systems on CARETS and identify several\nactionable weaknesses in model comprehension, especially with concepts such as\nnegation, disjunction, or hypernym invariance. Interestingly, even the most\nsophisticated models are sensitive to aspects such as swapping the order of\nterms in a conjunction or varying the number of answer choices mentioned in the\nquestion. We release CARETS to be used as an extensible tool for evaluating\nmulti-modal model robustness.", "published": "2022-03-15 03:01:03", "link": "http://arxiv.org/abs/2203.07613v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Do Language Models Plagiarize?", "abstract": "Past literature has illustrated that language models (LMs) often memorize\nparts of training instances and reproduce them in natural language generation\n(NLG) processes. However, it is unclear to what extent LMs \"reuse\" a training\ncorpus. For instance, models can generate paraphrased sentences that are\ncontextually similar to training samples. In this work, therefore, we study\nthree types of plagiarism (i.e., verbatim, paraphrase, and idea) among GPT-2\ngenerated texts, in comparison to its training data, and further analyze the\nplagiarism patterns of fine-tuned LMs with domain-specific corpora which are\nextensively used in practice. Our results suggest that (1) three types of\nplagiarism widely exist in LMs beyond memorization, (2) both size and decoding\nmethods of LMs are strongly associated with the degrees of plagiarism they\nexhibit, and (3) fine-tuned LMs' plagiarism patterns vary based on their corpus\nsimilarity and homogeneity. Given that a majority of LMs' training data is\nscraped from the Web without informing content owners, their reiteration of\nwords, phrases, and even core ideas from training sets into generated texts has\nethical implications. Their patterns are likely to exacerbate as both the size\nof LMs and their training data increase, raising concerns about\nindiscriminately pursuing larger models with larger training corpora.\nPlagiarized content can also contain individuals' personal and sensitive\ninformation. These findings overall cast doubt on the practicality of current\nLMs in mission-critical writing tasks and urge more discussions around the\nobserved phenomena. Data and source code are available at\nhttps://github.com/Brit7777/LM-plagiarism.", "published": "2022-03-15 03:11:11", "link": "http://arxiv.org/abs/2203.07618v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multilingual Mix: Example Interpolation Improves Multilingual Neural\n  Machine Translation", "abstract": "Multilingual neural machine translation models are trained to maximize the\nlikelihood of a mix of examples drawn from multiple language pairs. The\ndominant inductive bias applied to these models is a shared vocabulary and a\nshared set of parameters across languages; the inputs and labels corresponding\nto examples drawn from different language pairs might still reside in distinct\nsub-spaces. In this paper, we introduce multilingual crossover encoder-decoder\n(mXEncDec) to fuse language pairs at an instance level. Our approach\ninterpolates instances from different language pairs into joint `crossover\nexamples' in order to encourage sharing input and output spaces across\nlanguages. To ensure better fusion of examples in multilingual settings, we\npropose several techniques to improve example interpolation across dissimilar\nlanguages under heavy data imbalance. Experiments on a large-scale WMT\nmultilingual dataset demonstrate that our approach significantly improves\nquality on English-to-Many, Many-to-English and zero-shot translation tasks\n(from +0.5 BLEU up to +5.5 BLEU points). Results on code-switching sets\ndemonstrate the capability of our approach to improve model generalization to\nout-of-distribution multilingual examples. We also conduct qualitative and\nquantitative representation comparisons to analyze the advantages of our\napproach at the representation level.", "published": "2022-03-15 03:56:22", "link": "http://arxiv.org/abs/2203.07627v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Contrastive Learning of Sociopragmatic Meaning in Social Media", "abstract": "Recent progress in representation and contrastive learning in NLP has not\nwidely considered the class of \\textit{sociopragmatic meaning} (i.e., meaning\nin interaction within different language communities). To bridge this gap, we\npropose a novel framework for learning task-agnostic representations\ntransferable to a wide range of sociopragmatic tasks (e.g., emotion, hate\nspeech, humor, sarcasm). Our framework outperforms other contrastive learning\nframeworks for both in-domain and out-of-domain data, across both the general\nand few-shot settings. For example, compared to two popular pre-trained\nlanguage models, our method obtains an improvement of $11.66$ average $F_1$ on\n$16$ datasets when fine-tuned on only $20$ training samples per dataset.Our\ncode is available at: https://github.com/UBC-NLP/infodcl", "published": "2022-03-15 05:07:04", "link": "http://arxiv.org/abs/2203.07648v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Compressing Sentence Representation for Semantic Retrieval via\n  Homomorphic Projective Distillation", "abstract": "How to learn highly compact yet effective sentence representation?\nPre-trained language models have been effective in many NLP tasks. However,\nthese models are often huge and produce large sentence embeddings. Moreover,\nthere is a big performance gap between large and small models. In this paper,\nwe propose Homomorphic Projective Distillation (HPD) to learn compressed\nsentence embeddings. Our method augments a small Transformer encoder model with\nlearnable projection layers to produce compact representations while mimicking\na large pre-trained language model to retain the sentence representation\nquality. We evaluate our method with different model sizes on both semantic\ntextual similarity (STS) and semantic retrieval (SR) tasks. Experiments show\nthat our method achieves 2.7-4.5 points performance gain on STS tasks compared\nwith previous best representations of the same size. In SR tasks, our method\nimproves retrieval speed (8.2$\\times$) and memory usage (8.0$\\times$) compared\nwith state-of-the-art large models.", "published": "2022-03-15 07:05:43", "link": "http://arxiv.org/abs/2203.07687v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Evaluating BERT-based Pre-training Language Models for Detecting\n  Misinformation", "abstract": "It is challenging to control the quality of online information due to the\nlack of supervision over all the information posted online. Manual checking is\nalmost impossible given the vast number of posts made on online media and how\nquickly they spread. Therefore, there is a need for automated rumour detection\ntechniques to limit the adverse effects of spreading misinformation. Previous\nstudies mainly focused on finding and extracting the significant features of\ntext data. However, extracting features is time-consuming and not a highly\neffective process. This study proposes the BERT- based pre-trained language\nmodels to encode text data into vectors and utilise neural network models to\nclassify these vectors to detect misinformation. Furthermore, different\nlanguage models (LM) ' performance with different trainable parameters was\ncompared. The proposed technique is tested on different short and long text\ndatasets. The result of the proposed technique has been compared with the\nstate-of-the-art techniques on the same datasets. The results show that the\nproposed technique performs better than the state-of-the-art techniques. We\nalso tested the proposed technique by combining the datasets. The results\ndemonstrated that the large data training and testing size considerably\nimproves the technique's performance.", "published": "2022-03-15 08:54:36", "link": "http://arxiv.org/abs/2203.07731v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ViWOZ: A Multi-Domain Task-Oriented Dialogue Systems Dataset For\n  Low-resource Language", "abstract": "Most of the current task-oriented dialogue systems (ToD), despite having\ninteresting results, are designed for a handful of languages like Chinese and\nEnglish. Therefore, their performance in low-resource languages is still a\nsignificant problem due to the absence of a standard dataset and evaluation\npolicy. To address this problem, we proposed ViWOZ, a fully-annotated\nVietnamese task-oriented dialogue dataset. ViWOZ is the first multi-turn,\nmulti-domain tasked oriented dataset in Vietnamese, a low-resource language.\nThe dataset consists of a total of 5,000 dialogues, including 60,946 fully\nannotated utterances. Furthermore, we provide a comprehensive benchmark of both\nmodular and end-to-end models in low-resource language scenarios. With those\ncharacteristics, the ViWOZ dataset enables future studies on creating a\nmultilingual task-oriented dialogue system.", "published": "2022-03-15 09:22:04", "link": "http://arxiv.org/abs/2203.07742v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Ghost in the Machine has an American accent: value conflict in GPT-3", "abstract": "The alignment problem in the context of large language models must consider\nthe plurality of human values in our world. Whilst there are many resonant and\noverlapping values amongst the world's cultures, there are also many\nconflicting, yet equally valid, values. It is important to observe which\ncultural values a model exhibits, particularly when there is a value conflict\nbetween input prompts and generated outputs. We discuss how the co-creation of\nlanguage and cultural value impacts large language models (LLMs). We explore\nthe constitution of the training data for GPT-3 and compare that to the world's\nlanguage and internet access demographics, as well as to reported statistical\nprofiles of dominant values in some Nation-states. We stress tested GPT-3 with\na range of value-rich texts representing several languages and nations;\nincluding some with values orthogonal to dominant US public opinion as reported\nby the World Values Survey. We observed when values embedded in the input text\nwere mutated in the generated outputs and noted when these conflicting values\nwere more aligned with reported dominant US values. Our discussion of these\nresults uses a moral value pluralism (MVP) lens to better understand these\nvalue mutations. Finally, we provide recommendations for how our work may\ncontribute to other current work in the field.", "published": "2022-03-15 11:06:54", "link": "http://arxiv.org/abs/2203.07785v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SCD: Self-Contrastive Decorrelation for Sentence Embeddings", "abstract": "In this paper, we propose Self-Contrastive Decorrelation (SCD), a\nself-supervised approach. Given an input sentence, it optimizes a joint\nself-contrastive and decorrelation objective. Learning a representation is\nfacilitated by leveraging the contrast arising from the instantiation of\nstandard dropout at different rates. The proposed method is conceptually simple\nyet empirically powerful. It achieves comparable results with state-of-the-art\nmethods on multiple benchmarks without using contrastive pairs. This study\nopens up avenues for efficient self-supervised learning methods that are more\nrobust than current contrastive methods.", "published": "2022-03-15 13:02:10", "link": "http://arxiv.org/abs/2203.07847v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improved Multi-label Classification under Temporal Concept Drift:\n  Rethinking Group-Robust Algorithms in a Label-Wise Setting", "abstract": "In document classification for, e.g., legal and biomedical text, we often\ndeal with hundreds of classes, including very infrequent ones, as well as\ntemporal concept drift caused by the influence of real world events, e.g.,\npolicy changes, conflicts, or pandemics. Class imbalance and drift can\nsometimes be mitigated by resampling the training data to simulate (or\ncompensate for) a known target distribution, but what if the target\ndistribution is determined by unknown future events? Instead of simply\nresampling uniformly to hedge our bets, we focus on the underlying optimization\nalgorithms used to train such document classifiers and evaluate several\ngroup-robust optimization algorithms, initially proposed to mitigate\ngroup-level disparities. Reframing group-robust algorithms as adaptation\nalgorithms under concept drift, we find that Invariant Risk Minimization and\nSpectral Decoupling outperform sampling-based approaches to class imbalance and\nconcept drift, and lead to much better performance on minority classes. The\neffect is more pronounced the larger the label set.", "published": "2022-03-15 13:08:03", "link": "http://arxiv.org/abs/2203.07856v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "K-VQG: Knowledge-aware Visual Question Generation for Common-sense\n  Acquisition", "abstract": "Visual Question Generation (VQG) is a task to generate questions from images.\nWhen humans ask questions about an image, their goal is often to acquire some\nnew knowledge. However, existing studies on VQG have mainly addressed question\ngeneration from answers or question categories, overlooking the objectives of\nknowledge acquisition. To introduce a knowledge acquisition perspective into\nVQG, we constructed a novel knowledge-aware VQG dataset called K-VQG. This is\nthe first large, humanly annotated dataset in which questions regarding images\nare tied to structured knowledge. We also developed a new VQG model that can\nencode and use knowledge as the target for a question. The experiment results\nshow that our model outperforms existing models on the K-VQG dataset.", "published": "2022-03-15 13:38:10", "link": "http://arxiv.org/abs/2203.07890v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Gold Doesn't Always Glitter: Spectral Removal of Linear and Nonlinear\n  Guarded Attribute Information", "abstract": "We describe a simple and effective method (Spectral Attribute removaL; SAL)\nto remove private or guarded information from neural representations. Our\nmethod uses matrix decomposition to project the input representations into\ndirections with reduced covariance with the guarded information rather than\nmaximal covariance as factorization methods normally use. We begin with linear\ninformation removal and proceed to generalize our algorithm to the case of\nnonlinear information removal using kernels. Our experiments demonstrate that\nour algorithm retains better main task performance after removing the guarded\ninformation compared to previous work. In addition, our experiments demonstrate\nthat we need a relatively small amount of guarded attribute data to remove\ninformation about these attributes, which lowers the exposure to sensitive data\nand is more suitable for low-resource scenarios. Code is available at\nhttps://github.com/jasonshaoshun/SAL.", "published": "2022-03-15 13:40:22", "link": "http://arxiv.org/abs/2203.07893v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Signal in Noise: Exploring Meaning Encoded in Random Character Sequences\n  with Character-Aware Language Models", "abstract": "Natural language processing models learn word representations based on the\ndistributional hypothesis, which asserts that word context (e.g.,\nco-occurrence) correlates with meaning. We propose that $n$-grams composed of\nrandom character sequences, or $garble$, provide a novel context for studying\nword meaning both within and beyond extant language. In particular, randomly\ngenerated character $n$-grams lack meaning but contain primitive information\nbased on the distribution of characters they contain. By studying the\nembeddings of a large corpus of garble, extant language, and pseudowords using\nCharacterBERT, we identify an axis in the model's high-dimensional embedding\nspace that separates these classes of $n$-grams. Furthermore, we show that this\naxis relates to structure within extant language, including word\npart-of-speech, morphology, and concept concreteness. Thus, in contrast to\nstudies that are mainly limited to extant language, our work reveals that\nmeaning and primitive information are intrinsically linked.", "published": "2022-03-15 13:48:38", "link": "http://arxiv.org/abs/2203.07911v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Things not Written in Text: Exploring Spatial Commonsense from Visual\n  Signals", "abstract": "Spatial commonsense, the knowledge about spatial position and relationship\nbetween objects (like the relative size of a lion and a girl, and the position\nof a boy relative to a bicycle when cycling), is an important part of\ncommonsense knowledge. Although pretrained language models (PLMs) succeed in\nmany NLP tasks, they are shown to be ineffective in spatial commonsense\nreasoning. Starting from the observation that images are more likely to exhibit\nspatial commonsense than texts, we explore whether models with visual signals\nlearn more spatial commonsense than text-based PLMs. We propose a spatial\ncommonsense benchmark that focuses on the relative scales of objects, and the\npositional relationship between people and objects under different actions. We\nprobe PLMs and models with visual signals, including vision-language pretrained\nmodels and image synthesis models, on this benchmark, and find that image\nsynthesis models are more capable of learning accurate and consistent spatial\nknowledge than other models. The spatial knowledge from image synthesis models\nalso helps in natural language understanding tasks that require spatial\ncommonsense.", "published": "2022-03-15 17:02:30", "link": "http://arxiv.org/abs/2203.08075v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Data Contamination: From Memorization to Exploitation", "abstract": "Pretrained language models are typically trained on massive web-based\ndatasets, which are often \"contaminated\" with downstream test sets. It is not\nclear to what extent models exploit the contaminated data for downstream tasks.\nWe present a principled method to study this question. We pretrain BERT models\non joint corpora of Wikipedia and labeled downstream datasets, and fine-tune\nthem on the relevant task. Comparing performance between samples seen and\nunseen during pretraining enables us to define and quantify levels of\nmemorization and exploitation. Experiments with two models and three downstream\ntasks show that exploitation exists in some cases, but in others the models\nmemorize the contaminated data, but do not exploit it. We show that these two\nmeasures are affected by different factors such as the number of duplications\nof the contaminated data and the model size. Our results highlight the\nimportance of analyzing massive web-scale datasets to verify that progress in\nNLP is obtained by better language understanding and not better data\nexploitation.", "published": "2022-03-15 20:37:16", "link": "http://arxiv.org/abs/2203.08242v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Toward Improving Attentive Neural Networks in Legal Text Processing", "abstract": "In recent years, thanks to breakthroughs in neural network techniques\nespecially attentive deep learning models, natural language processing has made\nmany impressive achievements. However, automated legal word processing is still\na difficult branch of natural language processing. Legal sentences are often\nlong and contain complicated legal terminologies. Hence, models that work well\non general documents still face challenges in dealing with legal documents. We\nhave verified the existence of this problem with our experiments in this work.\nIn this dissertation, we selectively present the main achievements in improving\nattentive neural networks in automatic legal document processing. Language\nmodels tend to grow larger and larger, though, without expert knowledge, these\nmodels can still fail in domain adaptation, especially for specialized fields\nlike law.", "published": "2022-03-15 20:45:22", "link": "http://arxiv.org/abs/2203.08244v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Better Quality Estimation for Low Resource Corpus Mining", "abstract": "Quality Estimation (QE) models have the potential to change how we evaluate\nand maybe even train machine translation models. However, these models still\nlack the robustness to achieve general adoption. We show that State-of-the-art\nQE models, when tested in a Parallel Corpus Mining (PCM) setting, perform\nunexpectedly bad due to a lack of robustness to out-of-domain examples. We\npropose a combination of multitask training, data augmentation and contrastive\nlearning to achieve better and more robust QE performance. We show that our\nmethod improves QE performance significantly in the MLQE challenge and the\nrobustness of QE models when tested in the Parallel Corpus Mining setup. We\nincrease the accuracy in PCM by more than 0.80, making it on par with\nstate-of-the-art PCM methods that use millions of sentence pairs to train their\nmodels. In comparison, we use a thousand times less data, 7K parallel sentences\nin total, and propose a novel low resource PCM method.", "published": "2022-03-15 21:23:22", "link": "http://arxiv.org/abs/2203.08259v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TSM: Measuring the Enticement of Honeyfiles with Natural Language\n  Processing", "abstract": "Honeyfile deployment is a useful breach detection method in cyber deception\nthat can also inform defenders about the intent and interests of intruders and\nmalicious insiders. A key property of a honeyfile, enticement, is the extent to\nwhich the file can attract an intruder to interact with it. We introduce a\nnovel metric, Topic Semantic Matching (TSM), which uses topic modelling to\nrepresent files in the repository and semantic matching in an embedding vector\nspace to compare honeyfile text and topic words robustly. We also present a\nhoneyfile corpus created with different Natural Language Processing (NLP)\nmethods. Experiments show that TSM is effective in inter-corpus comparisons and\nis a promising tool to measure the enticement of honeyfiles. TSM is the first\nmeasure to use NLP techniques to quantify the enticement of honeyfile content\nthat compares the essential topical content of local contexts to honeyfiles and\nis robust to paraphrasing.", "published": "2022-03-15 01:07:51", "link": "http://arxiv.org/abs/2203.07580v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generalized but not Robust? Comparing the Effects of Data Modification\n  Methods on Out-of-Domain Generalization and Adversarial Robustness", "abstract": "Data modification, either via additional training datasets, data\naugmentation, debiasing, and dataset filtering, has been proposed as an\neffective solution for generalizing to out-of-domain (OOD) inputs, in both\nnatural language processing and computer vision literature. However, the effect\nof data modification on adversarial robustness remains unclear. In this work,\nwe conduct a comprehensive study of common data modification strategies and\nevaluate not only their in-domain and OOD performance, but also their\nadversarial robustness (AR). We also present results on a two-dimensional\nsynthetic dataset to visualize the effect of each method on the training\ndistribution. This work serves as an empirical study towards understanding the\nrelationship between generalizing to unseen domains and defending against\nadversarial perturbations. Our findings suggest that more data (either via\nadditional datasets or data augmentation) benefits both OOD accuracy and AR.\nHowever, data filtering (previously shown to improve OOD accuracy on natural\nlanguage inference) hurts OOD accuracy on other tasks such as question\nanswering and image classification. We provide insights from our experiments to\ninform future work in this direction.", "published": "2022-03-15 05:32:44", "link": "http://arxiv.org/abs/2203.07653v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Seamlessly Integrating Factual Information and Social Content with\n  Persuasive Dialogue", "abstract": "Complex conversation settings such as persuasion involve communicating\nchanges in attitude or behavior, so users' perspectives need to be addressed,\neven when not directly related to the topic. In this work, we contribute a\nnovel modular dialogue system framework that seamlessly integrates factual\ninformation and social content into persuasive dialogue. Our framework is\ngeneralizable to any dialogue tasks that have mixed social and task contents.\nWe conducted a study that compared user evaluations of our framework versus a\nbaseline end-to-end generation model. We found our framework was evaluated more\nfavorably in all dimensions including competence and friendliness, compared to\nthe end-to-end model which does not explicitly handle social content or factual\nquestions.", "published": "2022-03-15 05:38:34", "link": "http://arxiv.org/abs/2203.07657v3", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "One Agent To Rule Them All: Towards Multi-agent Conversational AI", "abstract": "The increasing volume of commercially available conversational agents (CAs)\non the market has resulted in users being burdened with learning and adopting\nmultiple agents to accomplish their tasks. Though prior work has explored\nsupporting a multitude of domains within the design of a single agent, the\ninteraction experience suffers due to the large action space of desired\ncapabilities. To address these problems, we introduce a new task BBAI:\nBlack-Box Agent Integration, focusing on combining the capabilities of multiple\nblack-box CAs at scale. We explore two techniques: question agent pairing and\nquestion response pairing aimed at resolving this task. Leveraging these\ntechniques, we design One For All (OFA), a scalable system that provides a\nunified interface to interact with multiple CAs. Additionally, we introduce\nMARS: Multi-Agent Response Selection, a new encoder model for question response\npairing that jointly encodes user question and agent response pairs. We\ndemonstrate that OFA is able to automatically and accurately integrate an\nensemble of commercially available CAs spanning disparate domains.\nSpecifically, using the MARS encoder we achieve the highest accuracy on our\nBBAI task, outperforming strong baselines.", "published": "2022-03-15 06:07:17", "link": "http://arxiv.org/abs/2203.07665v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "ReACC: A Retrieval-Augmented Code Completion Framework", "abstract": "Code completion, which aims to predict the following code token(s) according\nto the code context, can improve the productivity of software development.\nRecent work has proved that statistical language modeling with transformers can\ngreatly improve the performance in the code completion task via learning from\nlarge-scale source code datasets. However, current approaches focus only on\ncode context within the file or project, i.e. internal context. Our distinction\nis utilizing \"external\" context, inspired by human behaviors of copying from\nthe related code snippets when writing code. Specifically, we propose a\nretrieval-augmented code completion framework, leveraging both lexical copying\nand referring to code with similar semantics by retrieval. We adopt a\nstage-wise training approach that combines a source code retriever and an\nauto-regressive language model for programming language. We evaluate our\napproach in the code completion task in Python and Java programming languages,\nachieving a state-of-the-art performance on CodeXGLUE benchmark.", "published": "2022-03-15 08:25:08", "link": "http://arxiv.org/abs/2203.07722v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "UniSAr: A Unified Structure-Aware Autoregressive Language Model for\n  Text-to-SQL", "abstract": "Existing text-to-SQL semantic parsers are typically designed for particular\nsettings such as handling queries that span multiple tables, domains or turns\nwhich makes them ineffective when applied to different settings. We present\nUniSAr (Unified Structure-Aware Autoregressive Language Model), which benefits\nfrom directly using an off-the-shelf language model architecture and\ndemonstrates consistently high performance under different settings.\nSpecifically, UniSAr extends existing autoregressive language models to\nincorporate three non-invasive extensions to make them structure-aware: (1)\nadding structure mark to encode database schema, conversation context, and\ntheir relationships; (2) constrained decoding to decode well structured SQL for\na given database schema; and (3) SQL completion to complete potential missing\nJOIN relationships in SQL based on database schema. On seven well-known\ntext-to-SQL datasets covering multi-domain, multi-table and multi-turn, UniSAr\ndemonstrates highly comparable or better performance to the most advanced\nspecifically-designed text-to-SQL models. Importantly, our UniSAr is\nnon-invasive, such that other core model advances in text-to-SQL can also adopt\nour extensions to further enhance performance.", "published": "2022-03-15 11:02:55", "link": "http://arxiv.org/abs/2203.07781v2", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Does Corpus Quality Really Matter for Low-Resource Languages?", "abstract": "The vast majority of non-English corpora are derived from automatically\nfiltered versions of CommonCrawl. While prior work has identified major issues\non the quality of these datasets (Kreutzer et al., 2021), it is not clear how\nthis impacts downstream performance. Taking representation learning in Basque\nas a case study, we explore tailored crawling (manually identifying and\nscraping websites with high-quality content) as an alternative to filtering\nCommonCrawl. Our new corpus, called EusCrawl, is similar in size to the Basque\nportion of popular multilingual corpora like CC100 and mC4, yet it has a much\nhigher quality according to native annotators. For instance, 66% of documents\nare rated as high-quality for EusCrawl, in contrast with <33% for both mC4 and\nCC100. Nevertheless, we obtain similar results on downstream NLU tasks\nregardless of the corpus used for pre-training. Our work suggests that NLU\nperformance in low-resource languages is not primarily constrained by the\nquality of the data, and other factors like corpus size and domain coverage can\nplay a more important role.", "published": "2022-03-15 17:40:27", "link": "http://arxiv.org/abs/2203.08111v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Word Translation via Two-Stage Contrastive Learning", "abstract": "Word translation or bilingual lexicon induction (BLI) is a key cross-lingual\ntask, aiming to bridge the lexical gap between different languages. In this\nwork, we propose a robust and effective two-stage contrastive learning\nframework for the BLI task. At Stage C1, we propose to refine standard\ncross-lingual linear maps between static word embeddings (WEs) via a\ncontrastive learning objective; we also show how to integrate it into the\nself-learning procedure for even more refined cross-lingual maps. In Stage C2,\nwe conduct BLI-oriented contrastive fine-tuning of mBERT, unlocking its word\ntranslation capability. We also show that static WEs induced from the\n`C2-tuned' mBERT complement static WEs from Stage C1. Comprehensive experiments\non standard BLI datasets for diverse languages and different experimental\nsetups demonstrate substantial gains achieved by our framework. While the BLI\nmethod from Stage C1 already yields substantial gains over all state-of-the-art\nBLI methods in our comparison, even stronger improvements are met with the full\ntwo-stage framework: e.g., we report gains for 112/112 BLI setups, spanning 28\nlanguage pairs.", "published": "2022-03-15 22:51:22", "link": "http://arxiv.org/abs/2203.08307v5", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Training a Tokenizer for Free with Private Federated Learning", "abstract": "Federated learning with differential privacy, i.e. private federated learning\n(PFL), makes it possible to train models on private data distributed across\nusers' devices without harming privacy. PFL is efficient for models, such as\nneural networks, that have a fixed number of parameters, and thus a\nfixed-dimensional gradient vector. Such models include neural-net language\nmodels, but not tokenizers, the topic of this work. Training a tokenizer\nrequires frequencies of words from an unlimited vocabulary, and existing\nmethods for finding an unlimited vocabulary need a separate privacy budget.\n  A workaround is to train the tokenizer on publicly available data. However,\nin this paper we first show that a tokenizer trained on mismatched data results\nin worse model performance compared to a privacy-violating \"oracle\" tokenizer\nthat accesses user data, with perplexity increasing by 20%. We also show that\nsub-word tokenizers are better suited to the federated context than word-level\nones, since they can encode new words, though with more tokens per word.\n  Second, we propose a novel method to obtain a tokenizer without using any\nadditional privacy budget. During private federated learning of the language\nmodel, we sample from the model, train a new tokenizer on the sampled\nsequences, and update the model embeddings. We then continue private federated\nlearning, and obtain performance within 1% of the \"oracle\" tokenizer. Since\nthis process trains the tokenizer only indirectly on private data, we can use\nthe \"postprocessing guarantee\" of differential privacy and thus use no\nadditional privacy budget.", "published": "2022-03-15 14:29:39", "link": "http://arxiv.org/abs/2203.09943v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Evaluating the Text-to-SQL Capabilities of Large Language Models", "abstract": "We perform an empirical evaluation of Text-to-SQL capabilities of the Codex\nlanguage model. We find that, without any finetuning, Codex is a strong\nbaseline on the Spider benchmark; we also analyze the failure modes of Codex in\nthis setting. Furthermore, we demonstrate on the GeoQuery and Scholar\nbenchmarks that a small number of in-domain examples provided in the prompt\nenables Codex to perform better than state-of-the-art models finetuned on such\nfew-shot examples.", "published": "2022-03-15 17:23:53", "link": "http://arxiv.org/abs/2204.00498v1", "categories": ["cs.CL", "cs.DB", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Privacy-Preserving Speech Representation Learning using Vector\n  Quantization", "abstract": "With the popularity of virtual assistants (e.g., Siri, Alexa), the use of\nspeech recognition is now becoming more and more widespread.However, speech\nsignals contain a lot of sensitive information, such as the speaker's identity,\nwhich raises privacy concerns.The presented experiments show that the\nrepresentations extracted by the deep layers of speech recognition networks\ncontain speaker information.This paper aims to produce an anonymous\nrepresentation while preserving speech recognition performance.To this end, we\npropose to use vector quantization to constrain the representation space and\ninduce the network to suppress the speaker identity.The choice of the\nquantization dictionary size allows to configure the trade-off between utility\n(speech recognition) and privacy (speaker identity concealment).", "published": "2022-03-15 14:01:11", "link": "http://arxiv.org/abs/2203.09518v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.CR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "FB-MSTCN: A Full-Band Single-Channel Speech Enhancement Method Based on\n  Multi-Scale Temporal Convolutional Network", "abstract": "In recent years, deep learning-based approaches have significantly improved\nthe performance of single-channel speech enhancement. However, due to the\nlimitation of training data and computational complexity, real-time enhancement\nof full-band (48 kHz) speech signals is still very challenging. Because of the\nlow energy of spectral information in the high-frequency part, it is more\ndifficult to directly model and enhance the full-band spectrum using neural\nnetworks. To solve this problem, this paper proposes a two-stage real-time\nspeech enhancement model with extraction-interpolation mechanism for a\nfull-band signal. The 48 kHz full-band time-domain signal is divided into three\nsub-channels by extracting, and a two-stage processing scheme of `masking +\ncompensation' is proposed to enhance the signal in the complex domain. After\nthe two-stage enhancement, the enhanced full-band speech signal is restored by\ninterval interpolation. In the subjective listening and word accuracy test, our\nproposed model achieves superior performance and outperforms the baseline model\noverall by 0.59 MOS and 4.0% WAcc for the non-personalized speech denoising\ntask.", "published": "2022-03-15 06:55:55", "link": "http://arxiv.org/abs/2203.07684v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Investigating self-supervised learning for speech enhancement and\n  separation", "abstract": "Speech enhancement and separation are two fundamental tasks for robust speech\nprocessing. Speech enhancement suppresses background noise while speech\nseparation extracts target speech from interfering speakers. Despite a great\nnumber of supervised learning-based enhancement and separation methods having\nbeen proposed and achieving good performance, studies on applying\nself-supervised learning (SSL) to enhancement and separation are limited. In\nthis paper, we evaluate 13 SSL upstream methods on speech enhancement and\nseparation downstream tasks. Our experimental results on Voicebank-DEMAND and\nLibri2Mix show that some SSL representations consistently outperform baseline\nfeatures including the short-time Fourier transform (STFT) magnitude and log\nMel filterbank (FBANK). Furthermore, we analyze the factors that make existing\nSSL frameworks difficult to apply to speech enhancement and separation and\ndiscuss the representation properties desired for both tasks. Our study is\nincluded as the official speech enhancement and separation downstreams for\nSUPERB.", "published": "2022-03-15 14:43:02", "link": "http://arxiv.org/abs/2203.07960v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Text-free non-parallel many-to-many voice conversion using normalising\n  flows", "abstract": "Non-parallel voice conversion (VC) is typically achieved using lossy\nrepresentations of the source speech. However, ensuring only speaker identity\ninformation is dropped whilst all other information from the source speech is\nretained is a large challenge. This is particularly challenging in the scenario\nwhere at inference-time we have no knowledge of the text being read, i.e.,\ntext-free VC. To mitigate this, we investigate information-preserving VC\napproaches.\n  Normalising flows have gained attention for text-to-speech synthesis, however\nhave been under-explored for VC. Flows utilize invertible functions to learn\nthe likelihood of the data, thus provide a lossless encoding of speech. We\ninvestigate normalising flows for VC in both text-conditioned and text-free\nscenarios. Furthermore, for text-free VC we compare pre-trained and\njointly-learnt priors. Flow-based VC evaluations show no degradation between\ntext-free and text-conditioned VC, resulting in improvements over the\nstate-of-the-art. Also, joint-training of the prior is found to negatively\nimpact text-free VC quality.", "published": "2022-03-15 15:45:54", "link": "http://arxiv.org/abs/2203.08009v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Semi-FedSER: Semi-supervised Learning for Speech Emotion Recognition On\n  Federated Learning using Multiview Pseudo-Labeling", "abstract": "Speech Emotion Recognition (SER) application is frequently associated with\nprivacy concerns as it often acquires and transmits speech data at the\nclient-side to remote cloud platforms for further processing. These speech data\ncan reveal not only speech content and affective information but the speaker's\nidentity, demographic traits, and health status. Federated learning (FL) is a\ndistributed machine learning algorithm that coordinates clients to train a\nmodel collaboratively without sharing local data. This algorithm shows enormous\npotential for SER applications as sharing raw speech or speech features from a\nuser's device is vulnerable to privacy attacks. However, a major challenge in\nFL is limited availability of high-quality labeled data samples. In this work,\nwe propose a semi-supervised federated learning framework, Semi-FedSER, that\nutilizes both labeled and unlabeled data samples to address the challenge of\nlimited labeled data samples in FL. We show that our Semi-FedSER can generate\ndesired SER performance even when the local label rate l=20 using two SER\nbenchmark datasets: IEMOCAP and MSP-Improv.", "published": "2022-03-15 21:50:43", "link": "http://arxiv.org/abs/2203.08810v1", "categories": ["eess.AS", "cs.CR", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
