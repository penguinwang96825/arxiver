{"title": "Entity-Consistent End-to-end Task-Oriented Dialogue System with KB\n  Retriever", "abstract": "Querying the knowledge base (KB) has long been a challenge in the end-to-end\ntask-oriented dialogue system. Previous sequence-to-sequence (Seq2Seq) dialogue\ngeneration work treats the KB query as an attention over the entire KB, without\nthe guarantee that the generated entities are consistent with each other. In\nthis paper, we propose a novel framework which queries the KB in two steps to\nimprove the consistency of generated entities. In the first step, inspired by\nthe observation that a response can usually be supported by a single KB row, we\nintroduce a KB retrieval component which explicitly returns the most relevant\nKB row given a dialogue history. The retrieval result is further used to filter\nthe irrelevant entities in a Seq2Seq response generation model to improve the\nconsistency among the output entities. In the second step, we further perform\nthe attention mechanism to address the most correlated KB column. Two methods\nare proposed to make the training feasible without labeled retrieval data,\nwhich include distant supervision and Gumbel-Softmax technique. Experiments on\ntwo publicly available task oriented dialog datasets show the effectiveness of\nour model by outperforming the baseline systems and producing entity-consistent\nresponses.", "published": "2019-09-15 08:50:59", "link": "http://arxiv.org/abs/1909.06762v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual BERT Transformation for Zero-Shot Dependency Parsing", "abstract": "This paper investigates the problem of learning cross-lingual representations\nin a contextual space. We propose Cross-Lingual BERT Transformation (CLBT), a\nsimple and efficient approach to generate cross-lingual contextualized word\nembeddings based on publicly available pre-trained BERT models (Devlin et al.,\n2018). In this approach, a linear transformation is learned from contextual\nword alignments to align the contextualized embeddings independently trained in\ndifferent languages. We demonstrate the effectiveness of this approach on\nzero-shot cross-lingual transfer parsing. Experiments show that our embeddings\nsubstantially outperform the previous state-of-the-art that uses static\nembeddings. We further compare our approach with XLM (Lample and Conneau,\n2019), a recently proposed cross-lingual language model trained with massive\nparallel data, and achieve highly competitive results.", "published": "2019-09-15 10:33:17", "link": "http://arxiv.org/abs/1909.06775v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Query-Focused Scenario Construction", "abstract": "The news coverage of events often contains not one but multiple incompatible\naccounts of what happened. We develop a query-based system that extracts\ncompatible sets of events (scenarios) from such data, formulated as one-class\nclustering. Our system incrementally evaluates each event's compatibility with\nalready selected events, taking order into account. We use synthetic data\nconsisting of article mixtures for scalable training and evaluate our model on\na new human-curated dataset of scenarios about real-world news topics. Stronger\nneural network models and harder synthetic training settings are both important\nto achieve high performance, and our final scenario construction system\nsubstantially outperforms baselines based on prior work.", "published": "2019-09-15 20:25:22", "link": "http://arxiv.org/abs/1909.06877v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A simple discriminative training method for machine translation with\n  large-scale features", "abstract": "Margin infused relaxed algorithms (MIRAs) dominate model tuning in\nstatistical machine translation in the case of large scale features, but also\nthey are famous for the complexity in implementation. We introduce a new\nmethod, which regards an N-best list as a permutation and minimizes the\nPlackett-Luce loss of ground-truth permutations. Experiments with large-scale\nfeatures demonstrate that, the new method is more robust than MERT; though it\nis only matchable with MIRAs, it has a comparatively advantage, easier to\nimplement.", "published": "2019-09-15 18:18:35", "link": "http://arxiv.org/abs/1909.09491v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Language Adversarial Defense through Synonym Encoding", "abstract": "In the area of natural language processing, deep learning models are recently\nknown to be vulnerable to various types of adversarial perturbations, but\nrelatively few works are done on the defense side. Especially, there exists few\neffective defense method against the successful synonym substitution based\nattacks that preserve the syntactic structure and semantic information of the\noriginal text while fooling the deep learning models. We contribute in this\ndirection and propose a novel adversarial defense method called Synonym\nEncoding Method (SEM). Specifically, SEM inserts an encoder before the input\nlayer of the target model to map each cluster of synonyms to a unique encoding\nand trains the model to eliminate possible adversarial perturbations without\nmodifying the network architecture or adding extra data. Extensive experiments\ndemonstrate that SEM can effectively defend the current synonym substitution\nbased attacks and block the transferability of adversarial examples. SEM is\nalso easy and efficient to scale to large models and big datasets.", "published": "2019-09-15 03:35:18", "link": "http://arxiv.org/abs/1909.06723v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Emu: Enhancing Multilingual Sentence Embeddings with Semantic\n  Specialization", "abstract": "We present Emu, a system that semantically enhances multilingual sentence\nembeddings. Our framework fine-tunes pre-trained multilingual sentence\nembeddings using two main components: a semantic classifier and a language\ndiscriminator. The semantic classifier improves the semantic similarity of\nrelated sentences, whereas the language discriminator enhances the\nmultilinguality of the embeddings via multilingual adversarial training. Our\nexperimental results based on several language pairs show that our specialized\nembeddings outperform the state-of-the-art multilingual sentence embedding\nmodel on the task of cross-lingual intent classification using only monolingual\nlabeled data.", "published": "2019-09-15 04:31:21", "link": "http://arxiv.org/abs/1909.06731v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Rhyming Constraints using Structured Adversaries", "abstract": "Existing recurrent neural language models often fail to capture higher-level\nstructure present in text: for example, rhyming patterns present in poetry.\nMuch prior work on poetry generation uses manually defined constraints which\nare satisfied during decoding using either specialized decoding procedures or\nrejection sampling. The rhyming constraints themselves are typically not\nlearned by the generator. We propose an alternate approach that uses a\nstructured discriminator to learn a poetry generator that directly captures\nrhyming constraints in a generative adversarial setup. By causing the\ndiscriminator to compare poems based only on a learned similarity matrix of\npairs of line ending words, the proposed approach is able to successfully learn\nrhyming patterns in two different English poetry datasets (Sonnet and Limerick)\nwithout explicitly being provided with any phonetic information.", "published": "2019-09-15 05:58:09", "link": "http://arxiv.org/abs/1909.06743v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Many-to-Many Voice Conversion using Cycle-Consistent Variational\n  Autoencoder with Multiple Decoders", "abstract": "One of the obstacles in many-to-many voice conversion is the requirement of\nthe parallel training data, which contain pairs of utterances with the same\nlinguistic content spoken by different speakers. Since collecting such parallel\ndata is a highly expensive task, many works attempted to use non-parallel\ntraining data for many-to-many voice conversion. One of such approaches is\nusing the variational autoencoder (VAE). Though it can handle many-to-many\nvoice conversion without the parallel training, the VAE based voice conversion\nmethods suffer from low sound qualities of the converted speech. One of the\nmajor reasons is because the VAE learns only the self-reconstruction path. The\nconversion path is not trained at all. In this paper, we propose a cycle\nconsistency loss for VAE to explicitly learn the conversion path. In addition,\nwe propose to use multiple decoders to further improve the sound qualities of\nthe conventional VAE based voice conversion methods. The effectiveness of the\nproposed method is validated using objective and the subjective evaluations.", "published": "2019-09-15 14:03:40", "link": "http://arxiv.org/abs/1909.06805v4", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Automatically Extracting Challenge Sets for Non local Phenomena in\n  Neural Machine Translation", "abstract": "We show that the state of the art Transformer Machine Translation (MT) model\nis not biased towards monotonic reordering (unlike previous recurrent neural\nnetwork models), but that nevertheless, long-distance dependencies remain a\nchallenge for the model. Since most dependencies are short-distance, common\nevaluation metrics will be little influenced by how well systems perform on\nthem. We, therefore, propose an automatic approach for extracting challenge\nsets replete with long-distance dependencies and argue that evaluation using\nthis methodology provides a complementary perspective on system performance. To\nsupport our claim, we compile challenge sets for English-German and\nGerman-English, which are much larger than any previously released challenge\nset for MT. The extracted sets are large enough to allow reliable automatic\nevaluation, which makes the proposed approach a scalable and practical solution\nfor evaluating MT performance on the long-tail of syntactic phenomena.", "published": "2019-09-15 15:21:20", "link": "http://arxiv.org/abs/1909.06814v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Temporal Self-Attention Network for Medical Concept Embedding", "abstract": "In longitudinal electronic health records (EHRs), the event records of a\npatient are distributed over a long period of time and the temporal relations\nbetween the events reflect sufficient domain knowledge to benefit prediction\ntasks such as the rate of inpatient mortality. Medical concept embedding as a\nfeature extraction method that transforms a set of medical concepts with a\nspecific time stamp into a vector, which will be fed into a supervised learning\nalgorithm. The quality of the embedding significantly determines the learning\nperformance over the medical data. In this paper, we propose a medical concept\nembedding method based on applying a self-attention mechanism to represent each\nmedical concept. We propose a novel attention mechanism which captures the\ncontextual information and temporal relationships between medical concepts. A\nlight-weight neural net, \"Temporal Self-Attention Network (TeSAN)\", is then\nproposed to learn medical concept embedding based solely on the proposed\nattention mechanism. To test the effectiveness of our proposed methods, we have\nconducted clustering and prediction tasks on two public EHRs datasets comparing\nTeSAN against five state-of-the-art embedding methods. The experimental results\ndemonstrate that the proposed TeSAN model is superior to all the compared\nmethods. To the best of our knowledge, this work is the first to exploit\ntemporal self-attentive relations between medical events.", "published": "2019-09-15 21:20:09", "link": "http://arxiv.org/abs/1909.06886v1", "categories": ["cs.CL", "cs.LG", "68T30", "I.2.1"], "primary_category": "cs.CL"}
{"title": "Hint-Based Training for Non-Autoregressive Machine Translation", "abstract": "Due to the unparallelizable nature of the autoregressive factorization,\nAutoRegressive Translation (ART) models have to generate tokens sequentially\nduring decoding and thus suffer from high inference latency. Non-AutoRegressive\nTranslation (NART) models were proposed to reduce the inference time, but could\nonly achieve inferior translation accuracy. In this paper, we proposed a novel\napproach to leveraging the hints from hidden states and word alignments to help\nthe training of NART models. The results achieve significant improvement over\nprevious NART models for the WMT14 En-De and De-En datasets and are even\ncomparable to a strong LSTM-based ART baseline but one order of magnitude\nfaster in inference.", "published": "2019-09-15 01:39:59", "link": "http://arxiv.org/abs/1909.06708v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Quantum Structure in Cognition: Human Language as a Boson Gas of\n  Entangled Words", "abstract": "We model a piece of text of human language telling a story by means of the\nquantum structure describing a Bose gas in a state close to a Bose-Einstein\ncondensate near absolute zero temperature. For this we introduce energy levels\nfor the words (concepts) used in the story and we also introduce the new notion\nof 'cogniton' as the quantum of human thought. Words (concepts) are then\ncognitons in different energy states as it is the case for photons in different\nenergy states, or states of different radiative frequency, when the considered\nboson gas is that of the quanta of the electromagnetic field. We show that\nBose-Einstein statistics delivers a very good model for these pieces of texts\ntelling stories, both for short stories and for long stories of the size of\nnovels. We analyze an unexpected connection with Zipf's law in human language,\nthe Zipf ranking relating to the energy levels of the words, and the\nBose-Einstein graph coinciding with the Zipf graph. We investigate the issue of\n'identity and indistinguishability' from this new perspective and conjecture\nthat the way one can easily understand how two of 'the same concepts' are\n'absolutely identical and indistinguishable' in human language is also the way\nin which quantum particles are absolutely identical and indistinguishable in\nphysical reality, providing in this way new evidence for our conceptuality\ninterpretation of quantum theory.", "published": "2019-09-15 17:40:57", "link": "http://arxiv.org/abs/1909.06845v2", "categories": ["q-bio.NC", "cs.CL", "quant-ph"], "primary_category": "q-bio.NC"}
{"title": "Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with\n  Multi-lingual Language Representation Model", "abstract": "Because it is not feasible to collect training data for every language, there\nis a growing interest in cross-lingual transfer learning. In this paper, we\nsystematically explore zero-shot cross-lingual transfer learning on reading\ncomprehension tasks with a language representation model pre-trained on\nmulti-lingual corpus. The experimental results show that with pre-trained\nlanguage representation zero-shot learning is feasible, and translating the\nsource data into the target language is not necessary and even degrades the\nperformance. We further explore what does the model learn in zero-shot setting.", "published": "2019-09-15 10:33:05", "link": "http://arxiv.org/abs/1909.09587v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Induction and Reference of Entities in a Visual Story", "abstract": "We are enveloped by stories of visual interpretations in our everyday lives.\nThe way we narrate a story often comprises of two stages, which are, forming a\ncentral mind map of entities and then weaving a story around them. A\ncontributing factor to coherence is not just basing the story on these entities\nbut also, referring to them using appropriate terms to avoid repetition. In\nthis paper, we address these two stages of introducing the right entities at\nseemingly reasonable junctures and also referring them coherently in the\ncontext of visual storytelling. The building blocks of the central mind map,\nalso known as entity skeleton are entity chains including nominal and\ncoreference expressions. This entity skeleton is also represented in different\nlevels of abstractions to compose a generalized frame to weave the story. We\nbuild upon an encoder-decoder framework to penalize the model when the decoded\nstory does not adhere to this entity skeleton. We establish a strong baseline\nfor skeleton informed generation and then extend this to have the capability of\nmultitasking by predicting the skeleton in addition to generating the story.\nFinally, we build upon this model and propose a glocal hierarchical attention\nmodel that attends to the skeleton both at the sentence (local) and the story\n(global) levels. We observe that our proposed models outperform the baseline in\nterms of automatic evaluation metric, METEOR. We perform various analysis\ntargeted to evaluate the performance of our task of enforcing the entity\nskeleton such as the number and diversity of the entities generated. We also\nconduct human evaluation from which it is concluded that the visual stories\ngenerated by our model are preferred 82% of the times. In addition, we show\nthat our glocal hierarchical attention model improves coherence by introducing\nmore pronouns as required by the presence of nouns.", "published": "2019-09-15 01:09:01", "link": "http://arxiv.org/abs/1909.09699v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
