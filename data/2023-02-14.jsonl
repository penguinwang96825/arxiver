{"title": "The Stable Entropy Hypothesis and Entropy-Aware Decoding: An Analysis\n  and Algorithm for Robust Natural Language Generation", "abstract": "State-of-the-art language generation models can degenerate when applied to\nopen-ended generation problems such as text completion, story generation, or\ndialog modeling. This degeneration usually shows up in the form of incoherence,\nlack of vocabulary diversity, and self-repetition or copying from the context.\nIn this paper, we postulate that ``human-like'' generations usually lie in a\nnarrow and nearly flat entropy band, and violation of these entropy bounds\ncorrelates with degenerate behavior. Our experiments show that this stable\nnarrow entropy zone exists across models, tasks, and domains and confirm the\nhypothesis that violations of this zone correlate with degeneration. We then\nuse this insight to propose an entropy-aware decoding algorithm that respects\nthese entropy bounds resulting in less degenerate, more contextual, and\n\"human-like\" language generation in open-ended text generation settings.", "published": "2023-02-14 02:02:33", "link": "http://arxiv.org/abs/2302.06784v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BLIAM: Literature-based Data Synthesis for Synergistic Drug Combination\n  Prediction", "abstract": "Language models pre-trained on scientific literature corpora have\nsubstantially advanced scientific discovery by offering high-quality feature\nrepresentations for downstream applications. However, these features are often\nnot interpretable, and thus can reveal limited insights to domain experts.\nInstead of obtaining features from language models, we propose BLIAM, a\nliterature-based data synthesis approach to directly generate training data\npoints that are interpretable and model-agnostic to downstream applications.\nThe key idea of BLIAM is to create prompts using existing training data and\nthen use these prompts to synthesize new data points. BLIAM performs these two\nsteps iteratively as new data points will define more informative prompts and\nnew prompts will in turn synthesize more accurate data points. Notably,\nliterature-based data augmentation might introduce data leakage since labels of\ntest data points in downstream applications might have already been mentioned\nin the language model corpus. To prevent such leakage, we introduce GDSC-combo,\na large-scale drug combination discovery dataset that was published after the\nbiomedical language model was trained. We found that BLIAM substantially\noutperforms a non-augmented approach and manual prompting in this rigorous data\nsplit setting. BLIAM can be further used to synthesize data points for novel\ndrugs and cell lines that were not even measured in biomedical experiments. In\naddition to the promising prediction performance, the data points synthesized\nby BLIAM are interpretable and model-agnostic, enabling in silico augmentation\nfor in vitro experiments.", "published": "2023-02-14 06:48:52", "link": "http://arxiv.org/abs/2302.06860v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Few-shot learning approaches for classifying low resource domain\n  specific software requirements", "abstract": "With the advent of strong pre-trained natural language processing models like\nBERT, DeBERTa, MiniLM, T5, the data requirement for industries to fine-tune\nthese models to their niche use cases has drastically reduced (typically to a\nfew hundred annotated samples for achieving a reasonable performance). However,\nthe availability of even a few hundred annotated samples may not always be\nguaranteed in low resource domains like automotive, which often limits the\nusage of such deep learning models in an industrial setting. In this paper we\naim to address the challenge of fine-tuning such pre-trained models with only a\nfew annotated samples, also known as Few-shot learning. Our experiments focus\non evaluating the performance of a diverse set of algorithms and methodologies\nto achieve the task of classifying BOSCH automotive domain textual software\nrequirements into 3 categories, while utilizing only 15 annotated samples per\ncategory for fine-tuning. We find that while SciBERT and DeBERTa based models\ntend to be the most accurate at 15 training samples, their performance\nimprovement scales minimally as the number of annotated samples is increased to\n50 in comparison to Siamese and T5 based models.", "published": "2023-02-14 10:19:23", "link": "http://arxiv.org/abs/2302.06951v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Multi-source Active Learning for Natural Language\n  Inference", "abstract": "In recent years, active learning has been successfully applied to an array of\nNLP tasks. However, prior work often assumes that training and test data are\ndrawn from the same distribution. This is problematic, as in real-life settings\ndata may stem from several sources of varying relevance and quality. We show\nthat four popular active learning schemes fail to outperform random selection\nwhen applied to unlabelled pools comprised of multiple data sources on the task\nof natural language inference. We reveal that uncertainty-based strategies\nperform poorly due to the acquisition of collective outliers, i.e.,\nhard-to-learn instances that hamper learning and generalization. When outliers\nare removed, strategies are found to recover and outperform random baselines.\nIn further analysis, we find that collective outliers vary in form between\nsources, and show that hard-to-learn data is not always categorically harmful.\nLastly, we leverage dataset cartography to introduce difficulty-stratified\ntesting and find that different strategies are affected differently by example\nlearnability and difficulty.", "published": "2023-02-14 11:10:18", "link": "http://arxiv.org/abs/2302.06976v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AdapterSoup: Weight Averaging to Improve Generalization of Pretrained\n  Language Models", "abstract": "Pretrained language models (PLMs) are trained on massive corpora, but often\nneed to specialize to specific domains. A parameter-efficient adaptation method\nsuggests training an adapter for each domain on the task of language modeling.\nThis leads to good in-domain scores but can be impractical for domain- or\nresource-restricted settings. A solution is to use a related-domain adapter for\nthe novel domain at test time. In this paper, we introduce AdapterSoup, an\napproach that performs weight-space averaging of adapters trained on different\ndomains. Our approach is embarrassingly parallel: first, we train a set of\ndomain-specific adapters; then, for each novel domain, we determine which\nadapters should be averaged at test time. We present extensive experiments\nshowing that AdapterSoup consistently improves performance to new domains\nwithout extra training. We also explore weight averaging of adapters trained on\nthe same domain with different hyper-parameters, and show that it preserves the\nperformance of a PLM on new domains while obtaining strong in-domain results.\nWe explore various approaches for choosing which adapters to combine, such as\ntext clustering and semantic similarity. We find that using clustering leads to\nthe most competitive results on novel domains.", "published": "2023-02-14 13:09:23", "link": "http://arxiv.org/abs/2302.07027v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Complex Event Scenarios via Simple Entity-focused Questions", "abstract": "Event scenarios are often complex and involve multiple event sequences\nconnected through different entity participants. Exploring such complex\nscenarios requires an ability to branch through different sequences, something\nthat is difficult to achieve with standard event language modeling. To address\nthis, we propose a question-guided generation framework that models events in\ncomplex scenarios as answers to questions about participants. At any step in\nthe generation process, the framework uses the previously generated events as\ncontext, but generates the next event as an answer to one of three questions:\nwhat else a participant did, what else happened to a participant, or what else\nhappened. The participants and the questions themselves can be sampled or be\nprovided as input from a user, allowing for controllable exploration. Our\nempirical evaluation shows that this question-guided generation provides better\ncoverage of participants, diverse events within a domain, comparable\nperplexities for modeling event sequences, and more effective control for\ninteractive schema generation.", "published": "2023-02-14 15:48:56", "link": "http://arxiv.org/abs/2302.07139v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reveal the Unknown: Out-of-Knowledge-Base Mention Discovery with Entity\n  Linking", "abstract": "Discovering entity mentions that are out of a Knowledge Base (KB) from texts\nplays a critical role in KB maintenance, but has not yet been fully explored.\nThe current methods are mostly limited to the simple threshold-based approach\nand feature-based classification, and the datasets for evaluation are\nrelatively rare. We propose BLINKout, a new BERT-based Entity Linking (EL)\nmethod which can identify mentions that do not have corresponding KB entities\nby matching them to a special NIL entity. To better utilize BERT, we propose\nnew techniques including NIL entity representation and classification, with\nsynonym enhancement. We also apply KB Pruning and Versioning strategies to\nautomatically construct out-of-KB datasets from common in-KB EL datasets.\nResults on five datasets of clinical notes, biomedical publications, and\nWikipedia articles in various domains show the advantages of BLINKout over\nexisting methods to identify out-of-KB mentions for the medical ontologies,\nUMLS, SNOMED CT, and the general KB, WikiData.", "published": "2023-02-14 17:00:06", "link": "http://arxiv.org/abs/2302.07189v4", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Psycholinguistic Analysis of BERT's Representations of Compounds", "abstract": "This work studies the semantic representations learned by BERT for compounds,\nthat is, expressions such as sunlight or bodyguard. We build on recent studies\nthat explore semantic information in Transformers at the word level and test\nwhether BERT aligns with human semantic intuitions when dealing with\nexpressions (e.g., sunlight) whose overall meaning depends -- to a various\nextent -- on the semantics of the constituent words (sun, light). We leverage a\ndataset that includes human judgments on two psycholinguistic measures of\ncompound semantic analysis: lexeme meaning dominance (LMD; quantifying the\nweight of each constituent toward the compound meaning) and semantic\ntransparency (ST; evaluating the extent to which the compound meaning is\nrecoverable from the constituents' semantics). We show that BERT-based measures\nmoderately align with human intuitions, especially when using contextualized\nrepresentations, and that LMD is overall more predictable than ST. Contrary to\nthe results reported for 'standard' words, higher, more contextualized layers\nare the best at representing compound meaning. These findings shed new light on\nthe abilities of BERT in dealing with fine-grained semantic phenomena.\nMoreover, they can provide insights into how speakers represent compounds.", "published": "2023-02-14 18:23:15", "link": "http://arxiv.org/abs/2302.07232v1", "categories": ["cs.CL", "I.2.7; J.5"], "primary_category": "cs.CL"}
{"title": "TRESTLE: Toolkit for Reproducible Execution of Speech, Text and Language\n  Experiments", "abstract": "The evidence is growing that machine and deep learning methods can learn the\nsubtle differences between the language produced by people with various forms\nof cognitive impairment such as dementia and cognitively healthy individuals.\nValuable public data repositories such as TalkBank have made it possible for\nresearchers in the computational community to join forces and learn from each\nother to make significant advances in this area. However, due to variability in\napproaches and data selection strategies used by various researchers, results\nobtained by different groups have been difficult to compare directly. In this\npaper, we present TRESTLE (\\textbf{T}oolkit for \\textbf{R}eproducible\n\\textbf{E}xecution of \\textbf{S}peech \\textbf{T}ext and \\textbf{L}anguage\n\\textbf{E}xperiments), an open source platform that focuses on two datasets\nfrom the TalkBank repository with dementia detection as an illustrative domain.\nSuccessfully deployed in the hackallenge (Hackathon/Challenge) of the\nInternational Workshop on Health Intelligence at AAAI 2022, TRESTLE provides a\nprecise digital blueprint of the data pre-processing and selection strategies\nthat can be reused via TRESTLE by other researchers seeking comparable results\nwith their peers and current state-of-the-art (SOTA) approaches.", "published": "2023-02-14 20:07:31", "link": "http://arxiv.org/abs/2302.07322v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "READIN: A Chinese Multi-Task Benchmark with Realistic and Diverse Input\n  Noises", "abstract": "For many real-world applications, the user-generated inputs usually contain\nvarious noises due to speech recognition errors caused by linguistic\nvariations1 or typographical errors (typos). Thus, it is crucial to test model\nperformance on data with realistic input noises to ensure robustness and\nfairness. However, little study has been done to construct such benchmarks for\nChinese, where various language-specific input noises happen in the real world.\nIn order to fill this important gap, we construct READIN: a Chinese multi-task\nbenchmark with REalistic And Diverse Input Noises. READIN contains four diverse\ntasks and requests annotators to re-enter the original test data with two\ncommonly used Chinese input methods: Pinyin input and speech input. We designed\nour annotation pipeline to maximize diversity, for example by instructing the\nannotators to use diverse input method editors (IMEs) for keyboard noises and\nrecruiting speakers from diverse dialectical groups for speech noises. We\nexperiment with a series of strong pretrained language models as well as robust\ntraining methods, we find that these models often suffer significant\nperformance drops on READIN even with robustness methods like data\naugmentation. As the first large-scale attempt in creating a benchmark with\nnoises geared towards user-generated inputs, we believe that READIN serves as\nan important complement to existing Chinese NLP benchmarks. The source code and\ndataset can be obtained from https://github.com/thunlp/READIN.", "published": "2023-02-14 20:14:39", "link": "http://arxiv.org/abs/2302.07324v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meta-Learning Triplet Network with Adaptive Margins for Few-Shot Named\n  Entity Recognition", "abstract": "Meta-learning methods have been widely used in few-shot named entity\nrecognition (NER), especially prototype-based methods. However, the Other(O)\nclass is difficult to be represented by a prototype vector because there are\ngenerally a large number of samples in the class that have miscellaneous\nsemantics. To solve the problem, we propose MeTNet, which generates prototype\nvectors for entity types only but not O-class. We design an improved triplet\nnetwork to map samples and prototype vectors into a low-dimensional space that\nis easier to be classified and propose an adaptive margin for each entity type.\nThe margin plays as a radius and controls a region with adaptive size in the\nlow-dimensional space. Based on the regions, we propose a new inference\nprocedure to predict the label of a query instance. We conduct extensive\nexperiments in both in-domain and cross-domain settings to show the superiority\nof MeTNet over other state-of-the-art methods. In particular, we release a\nChinese few-shot NER dataset FEW-COMM extracted from a well-known e-commerce\nplatform. To the best of our knowledge, this is the first Chinese few-shot NER\ndataset. All the datasets and codes are provided at\nhttps://github.com/hccngu/MeTNet.", "published": "2023-02-14 15:10:26", "link": "http://arxiv.org/abs/2302.07739v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Role of Semantic Parsing in Understanding Procedural Text", "abstract": "In this paper, we investigate whether symbolic semantic representations,\nextracted from deep semantic parsers, can help reasoning over the states of\ninvolved entities in a procedural text. We consider a deep semantic\nparser~(TRIPS) and semantic role labeling as two sources of semantic parsing\nknowledge. First, we propose PROPOLIS, a symbolic parsing-based procedural\nreasoning framework. Second, we integrate semantic parsing information into\nstate-of-the-art neural models to conduct procedural reasoning. Our experiments\nindicate that explicitly incorporating such semantic knowledge improves\nprocedural understanding. This paper presents new metrics for evaluating\nprocedural reasoning tasks that clarify the challenges and identify differences\namong neural, symbolic, and integrated models.", "published": "2023-02-14 04:59:33", "link": "http://arxiv.org/abs/2302.06829v2", "categories": ["cs.CL", "cs.SC"], "primary_category": "cs.CL"}
{"title": "SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for\n  Classification in Low-Resource Domains", "abstract": "Prompting pre-trained language models leads to promising results across\nnatural language processing tasks but is less effective when applied in\nlow-resource domains, due to the domain gap between the pre-training data and\nthe downstream task. In this work, we bridge this gap with a novel and\nlightweight prompting methodology called SwitchPrompt for the adaptation of\nlanguage models trained on datasets from the general domain to diverse\nlow-resource domains. Using domain-specific keywords with a trainable gated\nprompt, SwitchPrompt offers domain-oriented prompting, that is, effective\nguidance on the target domains for general-domain language models. Our few-shot\nexperiments on three text classification benchmarks demonstrate the efficacy of\nthe general-domain pre-trained language models when used with SwitchPrompt.\nThey often even outperform their domain-specific counterparts trained with\nbaseline state-of-the-art prompting methods by up to 10.7% performance increase\nin accuracy. This result indicates that SwitchPrompt effectively reduces the\nneed for domain-specific language model pre-training.", "published": "2023-02-14 07:14:08", "link": "http://arxiv.org/abs/2302.06868v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploiting Summarization Data to Help Text Simplification", "abstract": "One of the major problems with text simplification is the lack of\nhigh-quality data. The sources of simplification datasets are limited to\nWikipedia and Newsela, restricting further development of this field. In this\npaper, we analyzed the similarity between text summarization and text\nsimplification and exploited summarization data to help simplify. First, we\nproposed an alignment algorithm to extract sentence pairs from summarization\ndatasets. Then, we designed four attributes to characterize the degree of\nsimplification and proposed a method to filter suitable pairs. We named these\npairs Sum4Simp (S4S). Next, we conducted human evaluations to show that S4S is\nhigh-quality and compared it with a real simplification dataset. Finally, we\nconducted experiments to illustrate that the S4S can improve the performance of\nseveral mainstream simplification models, especially in low-resource scenarios.", "published": "2023-02-14 15:32:04", "link": "http://arxiv.org/abs/2302.07124v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Friendly Face: Do Text-to-Image Systems Rely on Stereotypes when the\n  Input is Under-Specified?", "abstract": "As text-to-image systems continue to grow in popularity with the general\npublic, questions have arisen about bias and diversity in the generated images.\nHere, we investigate properties of images generated in response to prompts\nwhich are visually under-specified, but contain salient social attributes\n(e.g., 'a portrait of a threatening person' versus 'a portrait of a friendly\nperson'). Grounding our work in social cognition theory, we find that in many\ncases, images contain similar demographic biases to those reported in the\nstereotype literature. However, trends are inconsistent across different models\nand further investigation is warranted.", "published": "2023-02-14 16:11:06", "link": "http://arxiv.org/abs/2302.07159v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "ScatterShot: Interactive In-context Example Curation for Text\n  Transformation", "abstract": "The in-context learning capabilities of LLMs like GPT-3 allow annotators to\ncustomize an LLM to their specific tasks with a small number of examples.\nHowever, users tend to include only the most obvious patterns when crafting\nexamples, resulting in underspecified in-context functions that fall short on\nunseen cases. Further, it is hard to know when \"enough\" examples have been\nincluded even for known patterns. In this work, we present ScatterShot, an\ninteractive system for building high-quality demonstration sets for in-context\nlearning. ScatterShot iteratively slices unlabeled data into task-specific\npatterns, samples informative inputs from underexplored or not-yet-saturated\nslices in an active learning manner, and helps users label more efficiently\nwith the help of an LLM and the current example set. In simulation studies on\ntwo text perturbation scenarios, ScatterShot sampling improves the resulting\nfew-shot functions by 4-5 percentage points over random sampling, with less\nvariance as more examples are added. In a user study, ScatterShot greatly helps\nusers in covering different patterns in the input space and labeling in-context\nexamples more efficiently, resulting in better in-context learning and less\nuser effort.", "published": "2023-02-14 21:13:31", "link": "http://arxiv.org/abs/2302.07346v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "BiasTestGPT: Using ChatGPT for Social Bias Testing of Language Models", "abstract": "Pretrained Language Models (PLMs) harbor inherent social biases that can\nresult in harmful real-world implications. Such social biases are measured\nthrough the probability values that PLMs output for different social groups and\nattributes appearing in a set of test sentences. However, bias testing is\ncurrently cumbersome since the test sentences are generated either from a\nlimited set of manual templates or need expensive crowd-sourcing. We instead\npropose using ChatGPT for the controllable generation of test sentences, given\nany arbitrary user-specified combination of social groups and attributes\nappearing in the test sentences. When compared to template-based methods, our\napproach using ChatGPT for test sentence generation is superior in detecting\nsocial bias, especially in challenging settings such as intersectional biases.\nWe present an open-source comprehensive bias testing framework (BiasTestGPT),\nhosted on HuggingFace, that can be plugged into any open-source PLM for bias\ntesting. User testing with domain experts from various fields has shown their\ninterest in being able to test modern AI for social biases. Our tool has\nsignificantly improved their awareness of such biases in PLMs, proving to be\nlearnable and user-friendly. We thus enable seamless open-ended social bias\ntesting of PLMs by domain experts through an automatic large-scale generation\nof diverse test sentences for any combination of social categories and\nattributes.", "published": "2023-02-14 22:07:57", "link": "http://arxiv.org/abs/2302.07371v3", "categories": ["cs.CL", "cs.CY", "68T50", "I.2.7; J.5; K.4.1"], "primary_category": "cs.CL"}
{"title": "Adding Instructions during Pretraining: Effective Way of Controlling\n  Toxicity in Language Models", "abstract": "Pretrained large language models have become indispensable for solving\nvarious natural language processing (NLP) tasks. However, safely deploying them\nin real world applications is challenging because they generate toxic content.\nTo address this challenge, we propose two novel pretraining data augmentation\nstrategies that significantly reduce model toxicity without compromising its\nutility. Our two strategies are: (1) MEDA: adds raw toxicity score as meta-data\nto the pretraining samples, and (2) INST: adds instructions to those samples\nindicating their toxicity. Our results indicate that our best performing\nstrategy (INST) substantially reduces the toxicity probability up to 61% while\npreserving the accuracy on five benchmark NLP tasks as well as improving AUC\nscores on four bias detection tasks by 1.3%. We also demonstrate the\ngeneralizability of our techniques by scaling the number of training samples\nand the number of model parameters.", "published": "2023-02-14 23:00:42", "link": "http://arxiv.org/abs/2302.07388v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language Model Analysis for Ontology Subsumption Inference", "abstract": "Investigating whether pre-trained language models (LMs) can function as\nknowledge bases (KBs) has raised wide research interests recently. However,\nexisting works focus on simple, triple-based, relational KBs, but omit more\nsophisticated, logic-based, conceptualised KBs such as OWL ontologies. To\ninvestigate an LM's knowledge of ontologies, we propose OntoLAMA, a set of\ninference-based probing tasks and datasets from ontology subsumption axioms\ninvolving both atomic and complex concepts. We conduct extensive experiments on\nontologies of different domains and scales, and our results demonstrate that\nLMs encode relatively less background knowledge of Subsumption Inference (SI)\nthan traditional Natural Language Inference (NLI) but can improve on SI\nsignificantly when a small number of samples are given. We will open-source our\ncode and datasets.", "published": "2023-02-14 00:21:56", "link": "http://arxiv.org/abs/2302.06761v3", "categories": ["cs.CL", "cs.AI", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Learning gain differences between ChatGPT and human tutor generated\n  algebra hints", "abstract": "Large Language Models (LLMs), such as ChatGPT, are quickly advancing AI to\nthe frontiers of practical consumer use and leading industries to re-evaluate\nhow they allocate resources for content production. Authoring of open\neducational resources and hint content within adaptive tutoring systems is\nlabor intensive. Should LLMs like ChatGPT produce educational content on par\nwith human-authored content, the implications would be significant for further\nscaling of computer tutoring system approaches. In this paper, we conduct the\nfirst learning gain evaluation of ChatGPT by comparing the efficacy of its\nhints with hints authored by human tutors with 77 participants across two\nalgebra topic areas, Elementary Algebra and Intermediate Algebra. We find that\n70% of hints produced by ChatGPT passed our manual quality checks and that both\nhuman and ChatGPT conditions produced positive learning gains. However, gains\nwere only statistically significant for human tutor created hints. Learning\ngains from human-created hints were substantially and statistically\nsignificantly higher than ChatGPT hints in both topic areas, though ChatGPT\nparticipants in the Intermediate Algebra experiment were near ceiling and not\neven with the control at pre-test. We discuss the limitations of our study and\nsuggest several future directions for the field. Problem and hint content used\nin the experiment is provided for replicability.", "published": "2023-02-14 07:20:48", "link": "http://arxiv.org/abs/2302.06871v1", "categories": ["cs.CY", "cs.CL", "cs.HC"], "primary_category": "cs.CY"}
{"title": "AI Chat Assistants can Improve Conversations about Divisive Topics", "abstract": "A rapidly increasing amount of human conversation occurs online. But\ndivisiveness and conflict can fester in text-based interactions on social media\nplatforms, in messaging apps, and on other digital forums. Such toxicity\nincreases polarization and, importantly, corrodes the capacity of diverse\nsocieties to develop efficient solutions to complex social problems that impact\neveryone. Scholars and civil society groups promote interventions that can make\ninterpersonal conversations less divisive or more productive in offline\nsettings, but scaling these efforts to the amount of discourse that occurs\nonline is extremely challenging. We present results of a large-scale experiment\nthat demonstrates how online conversations about divisive topics can be\nimproved with artificial intelligence tools. Specifically, we employ a large\nlanguage model to make real-time, evidence-based recommendations intended to\nimprove participants' perception of feeling understood in conversations. We\nfind that these interventions improve the reported quality of the conversation,\nreduce political divisiveness, and improve the tone, without systematically\nchanging the content of the conversation or moving people's policy attitudes.\nThese findings have important implications for future research on social media,\npolitical deliberation, and the growing community of scholars interested in the\nplace of artificial intelligence within computational social science.", "published": "2023-02-14 06:42:09", "link": "http://arxiv.org/abs/2302.07268v5", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Generation of Highlights from Research Papers Using Pointer-Generator\n  Networks and SciBERT Embeddings", "abstract": "Nowadays many research articles are prefaced with research highlights to\nsummarize the main findings of the paper. Highlights not only help researchers\nprecisely and quickly identify the contributions of a paper, they also enhance\nthe discoverability of the article via search engines. We aim to automatically\nconstruct research highlights given certain segments of a research paper. We\nuse a pointer-generator network with coverage mechanism and a contextual\nembedding layer at the input that encodes the input tokens into SciBERT\nembeddings. We test our model on a benchmark dataset, CSPubSum, and also\npresent MixSub, a new multi-disciplinary corpus of papers for automatic\nresearch highlight generation. For both CSPubSum and MixSub, we have observed\nthat the proposed model achieves the best performance compared to related\nvariants and other models proposed in the literature. On the CSPubSum dataset,\nour model achieves the best performance when the input is only the abstract of\na paper as opposed to other segments of the paper. It produces ROUGE-1, ROUGE-2\nand ROUGE-L F1-scores of 38.26, 14.26 and 35.51, respectively, METEOR score of\n32.62, and BERTScore F1 of 86.65 which outperform all other baselines. On the\nnew MixSub dataset, where only the abstract is the input, our proposed model\n(when trained on the whole training corpus without distinguishing between the\nsubject categories) achieves ROUGE-1, ROUGE-2 and ROUGE-L F1-scores of 31.78,\n9.76 and 29.3, respectively, METEOR score of 24.00, and BERTScore F1 of 85.25.", "published": "2023-02-14 12:45:14", "link": "http://arxiv.org/abs/2302.07729v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Speaker-Independent Acoustic-to-Articulatory Speech Inversion", "abstract": "To build speech processing methods that can handle speech as naturally as\nhumans, researchers have explored multiple ways of building an invertible\nmapping from speech to an interpretable space. The articulatory space is a\npromising inversion target, since this space captures the mechanics of speech\nproduction. To this end, we build an acoustic-to-articulatory inversion (AAI)\nmodel that leverages self-supervision to generalize to unseen speakers. Our\napproach obtains 0.784 correlation on an electromagnetic articulography (EMA)\ndataset, improving the state-of-the-art by 12.5\\%. Additionally, we show the\ninterpretability of these representations through directly comparing the\nbehavior of estimated representations with speech production behavior. Finally,\nwe propose a resynthesis-based AAI evaluation metric that does not rely on\narticulatory labels, demonstrating its efficacy with an 18-speaker dataset.", "published": "2023-02-14 01:20:31", "link": "http://arxiv.org/abs/2302.06774v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-Source Contrastive Learning from Musical Audio", "abstract": "Contrastive learning constitutes an emerging branch of self-supervised\nlearning that leverages large amounts of unlabeled data, by learning a latent\nspace, where pairs of different views of the same sample are associated. In\nthis paper, we propose musical source association as a pair generation strategy\nin the context of contrastive music representation learning. To this end, we\nmodify COLA, a widely used contrastive learning audio framework, to learn to\nassociate a song excerpt with a stochastically selected and automatically\nextracted vocal or instrumental source. We further introduce a novel\nmodification to the contrastive loss to incorporate information about the\nexistence or absence of specific sources. Our experimental evaluation in three\ndifferent downstream tasks (music auto-tagging, instrument classification and\nmusic genre classification) using the publicly available Magna-Tag-A-Tune\n(MTAT) as a source dataset yields competitive results to existing literature\nmethods, as well as faster network convergence. The results also show that this\npre-training method can be steered towards specific features, according to the\nselected musical source, while also being dependent on the quality of the\nseparated sources.", "published": "2023-02-14 14:36:59", "link": "http://arxiv.org/abs/2302.07077v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A dataset for Audio-Visual Sound Event Detection in Movies", "abstract": "Audio event detection is a widely studied audio processing task, with\napplications ranging from self-driving cars to healthcare. In-the-wild datasets\nsuch as Audioset have propelled research in this field. However, many efforts\ntypically involve manual annotation and verification, which is expensive to\nperform at scale. Movies depict various real-life and fictional scenarios which\nmakes them a rich resource for mining a wide-range of audio events. In this\nwork, we present a dataset of audio events called Subtitle-Aligned Movie Sounds\n(SAM-S). We use publicly-available closed-caption transcripts to automatically\nmine over 110K audio events from 430 movies. We identify three dimensions to\ncategorize audio events: sound, source, quality, and present the steps involved\nto produce a final taxonomy of 245 sounds. We discuss the choices involved in\ngenerating the taxonomy, and also highlight the human-centered nature of sounds\nin our dataset. We establish a baseline performance for audio-only sound\nclassification of 34.76% mean average precision and show that incorporating\nvisual information can further improve the performance by about 5%. Data and\ncode are made available for research at\nhttps://github.com/usc-sail/mica-subtitle-aligned-movie-sounds", "published": "2023-02-14 19:55:39", "link": "http://arxiv.org/abs/2302.07315v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Detection and classification of vocal productions in large scale audio\n  recordings", "abstract": "We propose an automatic data processing pipeline to extract vocal productions\nfrom large-scale natural audio recordings and classify these vocal productions.\nThe pipeline is based on a deep neural network and adresses both issues\nsimultaneously. Though a series of computationel steps (windowing, creation of\na noise class, data augmentation, re-sampling, transfer learning, Bayesian\noptimisation), it automatically trains a neural network without requiring a\nlarge sample of labeled data and important computing resources. Our end-to-end\nmethodology can handle noisy recordings made under different recording\nconditions. We test it on two different natural audio data sets, one from a\ngroup of Guinea baboons recorded from a primate research center and one from\nhuman babies recorded at home. The pipeline trains a model on 72 and 77 minutes\nof labeled audio recordings, with an accuracy of 94.58% and 99.76%. It is then\nused to process 443 and 174 hours of natural continuous recordings and it\ncreates two new databases of 38.8 and 35.2 hours, respectively. We discuss the\nstrengths and limitations of this approach that can be applied to any massive\naudio recording.", "published": "2023-02-14 14:07:09", "link": "http://arxiv.org/abs/2302.07640v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.AP"], "primary_category": "cs.SD"}
{"title": "Synthesizing audio from tongue motion during speech using tagged MRI via\n  transformer", "abstract": "Investigating the relationship between internal tissue point motion of the\ntongue and oropharyngeal muscle deformation measured from tagged MRI and\nintelligible speech can aid in advancing speech motor control theories and\ndeveloping novel treatment methods for speech related-disorders. However,\nelucidating the relationship between these two sources of information is\nchallenging, due in part to the disparity in data structure between\nspatiotemporal motion fields (i.e., 4D motion fields) and one-dimensional audio\nwaveforms. In this work, we present an efficient encoder-decoder translation\nnetwork for exploring the predictive information inherent in 4D motion fields\nvia 2D spectrograms as a surrogate of the audio data. Specifically, our encoder\nis based on 3D convolutional spatial modeling and transformer-based temporal\nmodeling. The extracted features are processed by an asymmetric 2D convolution\ndecoder to generate spectrograms that correspond to 4D motion fields.\nFurthermore, we incorporate a generative adversarial training approach into our\nframework to further improve synthesis quality on our generated spectrograms.\nWe experiment on 63 paired motion field sequences and speech waveforms,\ndemonstrating that our framework enables the generation of clear audio\nwaveforms from a sequence of motion fields. Thus, our framework has the\npotential to improve our understanding of the relationship between these two\nmodalities and inform the development of treatments for speech disorders.", "published": "2023-02-14 17:27:55", "link": "http://arxiv.org/abs/2302.07203v1", "categories": ["eess.IV", "cs.CV", "cs.SD", "eess.AS", "eess.SP"], "primary_category": "eess.IV"}
