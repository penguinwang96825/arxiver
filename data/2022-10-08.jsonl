{"title": "Data-Efficiency with a Single GPU: An Exploration of Transfer Methods\n  for Small Language Models", "abstract": "Multi-task learning (MTL), instruction tuning, and prompting have recently\nbeen shown to improve the generalizability of large language models to new\ntasks. However, the benefits of such methods are less well-documented in\nsmaller language models, with some studies finding contradictory results. In\nthis work, we explore and isolate the effects of (i) model size, (ii) general\npurpose MTL, (iii) in-domain MTL, (iv) instruction tuning, and (v) few-shot\nfine-tuning for models with fewer than 500 million parameters. Our experiments\nin the zero-shot setting demonstrate that models gain 31% relative improvement,\non average, from general purpose MTL, with an additional 37.6% relative gain\nfrom in-domain MTL. Contradictory to prior works on large models, we find that\ninstruction tuning provides a modest 2% performance improvement for small\nmodels.", "published": "2022-10-08 01:45:22", "link": "http://arxiv.org/abs/2210.03871v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Don't Lose Yourself! Empathetic Response Generation via Explicit\n  Self-Other Awareness", "abstract": "As a critical step to achieve human-like chatbots, empathetic response\ngeneration has attained increasing interests. Previous attempts are incomplete\nand not sufficient enough to elicit empathy because they only focus on the\ninitial aspect of empathy to automatically mimic the feelings and thoughts of\nthe user via other-awareness. However, they ignore to maintain and take the own\nviews of the system into account, which is a crucial process to achieve the\nempathy called self-other awareness. To this end, we propose to generate\nEmpathetic response with explicit Self-Other Awareness (EmpSOA). Specifically,\nthree stages, self-other differentiation, self-other modulation and self-other\ngeneration, are devised to clearly maintain, regulate and inject the self-other\naware information into the process of empathetic response generation. Both\nautomatic and human evaluations on the benchmark dataset demonstrate the\nsuperiority of EmpSOA to generate more empathetic responses.", "published": "2022-10-08 02:26:09", "link": "http://arxiv.org/abs/2210.03884v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving End-to-End Text Image Translation From the Auxiliary Text\n  Translation Task", "abstract": "End-to-end text image translation (TIT), which aims at translating the source\nlanguage embedded in images to the target language, has attracted intensive\nattention in recent research. However, data sparsity limits the performance of\nend-to-end text image translation. Multi-task learning is a non-trivial way to\nalleviate this problem via exploring knowledge from complementary related\ntasks. In this paper, we propose a novel text translation enhanced text image\ntranslation, which trains the end-to-end model with text translation as an\nauxiliary task. By sharing model parameters and multi-task training, our model\nis able to take full advantage of easily-available large-scale text parallel\ncorpus. Extensive experimental results show our proposed method outperforms\nexisting end-to-end methods, and the joint multi-task learning with both text\ntranslation and recognition tasks achieves better results, proving translation\nand recognition auxiliary tasks are complementary.", "published": "2022-10-08 02:35:45", "link": "http://arxiv.org/abs/2210.03887v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ConstGCN: Constrained Transmission-based Graph Convolutional Networks\n  for Document-level Relation Extraction", "abstract": "Document-level relation extraction with graph neural networks faces a\nfundamental graph construction gap between training and inference - the golden\ngraph structure only available during training, which causes that most methods\nadopt heuristic or syntactic rules to construct a prior graph as a pseudo\nproxy. In this paper, we propose $\\textbf{ConstGCN}$, a novel graph\nconvolutional network which performs knowledge-based information propagation\nbetween entities along with all specific relation spaces without any prior\ngraph construction. Specifically, it updates the entity representation by\naggregating information from all other entities along with each relation space,\nthus modeling the relation-aware spatial information. To control the\ninformation flow passing through the indeterminate relation spaces, we propose\nto constrain the propagation using transmitting scores learned from the Noise\nContrastive Estimation between fact triples. Experimental results show that our\nmethod outperforms the previous state-of-the-art (SOTA) approaches on the DocRE\ndataset.", "published": "2022-10-08 07:36:04", "link": "http://arxiv.org/abs/2210.03949v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Non-Monotonic Latent Alignments for CTC-Based Non-Autoregressive Machine\n  Translation", "abstract": "Non-autoregressive translation (NAT) models are typically trained with the\ncross-entropy loss, which forces the model outputs to be aligned verbatim with\nthe target sentence and will highly penalize small shifts in word positions.\nLatent alignment models relax the explicit alignment by marginalizing out all\nmonotonic latent alignments with the CTC loss. However, they cannot handle\nnon-monotonic alignments, which is non-negligible as there is typically global\nword reordering in machine translation. In this work, we explore non-monotonic\nlatent alignments for NAT. We extend the alignment space to non-monotonic\nalignments to allow for the global word reordering and further consider all\nalignments that overlap with the target sentence. We non-monotonically match\nthe alignments to the target sentence and train the latent alignment model to\nmaximize the F1 score of non-monotonic matching. Extensive experiments on major\nWMT benchmarks show that our method substantially improves the translation\nperformance of CTC-based models. Our best model achieves 30.06 BLEU on WMT14\nEn-De with only one-iteration decoding, closing the gap between\nnon-autoregressive and autoregressive models.", "published": "2022-10-08 07:44:28", "link": "http://arxiv.org/abs/2210.03953v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SDA: Simple Discrete Augmentation for Contrastive Sentence\n  Representation Learning", "abstract": "Contrastive learning has recently achieved compelling performance in\nunsupervised sentence representation. As an essential element, data\naugmentation protocols, however, have not been well explored. The pioneering\nwork SimCSE resorting to a simple dropout mechanism (viewed as continuous\naugmentation) surprisingly dominates discrete augmentations such as cropping,\nword deletion, and synonym replacement as reported. To understand the\nunderlying rationales, we revisit existing approaches and attempt to\nhypothesize the desiderata of reasonable data augmentation methods: balance of\nsemantic consistency and expression diversity. We then develop three simple yet\neffective discrete sentence augmentation schemes: punctuation insertion, modal\nverbs, and double negation. They act as minimal noises at lexical level to\nproduce diverse forms of sentences. Furthermore, standard negation is\ncapitalized on to generate negative samples for alleviating feature suppression\ninvolved in contrastive learning. We experimented extensively with semantic\ntextual similarity on diverse datasets. The results support the superiority of\nthe proposed methods consistently. Our key code is available at\nhttps://github.com/Zhudongsheng75/SDA", "published": "2022-10-08 08:07:47", "link": "http://arxiv.org/abs/2210.03963v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bird-Eye Transformers for Text Generation Models", "abstract": "Transformers have become an indispensable module for text generation models\nsince their great success in machine translation. Previous works attribute\nthe~success of transformers to the query-key-value dot-product attention, which\nprovides a robust inductive bias by the fully connected token graphs. However,\nwe found that self-attention has a severe limitation. When predicting the\n(i+1)-th token, self-attention only takes the i-th token as an information\ncollector, and it tends to give a high attention weight to those tokens similar\nto itself. Therefore, most of the historical information that occurred before\nthe i-th token is not taken into consideration. Based on this observation, in\nthis paper, we propose a new architecture, called bird-eye transformer(BET),\nwhich goes one step further to improve the performance of transformers by\nreweighting self-attention to encourage it to focus more on important\nhistorical information. We have conducted experiments on multiple text\ngeneration tasks, including machine translation (2 datasets) and language\nmodels (3 datasets). These experimental~results show that our proposed model\nachieves a better performance than the baseline transformer architectures\non~all~datasets. The code is released at:\n\\url{https://sites.google.com/view/bet-transformer/home}.", "published": "2022-10-08 09:51:15", "link": "http://arxiv.org/abs/2210.03985v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generative Language Models for Paragraph-Level Question Generation", "abstract": "Powerful generative models have led to recent progress in question generation\n(QG). However, it is difficult to measure advances in QG research since there\nare no standardized resources that allow a uniform comparison among approaches.\nIn this paper, we introduce QG-Bench, a multilingual and multidomain benchmark\nfor QG that unifies existing question answering datasets by converting them to\na standard QG setting. It includes general-purpose datasets such as SQuAD for\nEnglish, datasets from ten domains and two styles, as well as datasets in eight\ndifferent languages. Using QG-Bench as a reference, we perform an extensive\nanalysis of the capabilities of language models for the task. First, we propose\nrobust QG baselines based on fine-tuning generative language models. Then, we\ncomplement automatic evaluation based on standard metrics with an extensive\nmanual evaluation, which in turn sheds light on the difficulty of evaluating QG\nmodels. Finally, we analyse both the domain adaptability of these models as\nwell as the effectiveness of multilingual models in languages other than\nEnglish. QG-Bench is released along with the fine-tuned models presented in the\npaper https://github.com/asahi417/lm-question-generation, which are also\navailable as a demo https://autoqg.net/.", "published": "2022-10-08 10:24:39", "link": "http://arxiv.org/abs/2210.03992v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ngram-OAXE: Phrase-Based Order-Agnostic Cross Entropy for\n  Non-Autoregressive Machine Translation", "abstract": "Recently, a new training oaxe loss has proven effective to ameliorate the\neffect of multimodality for non-autoregressive translation (NAT), which removes\nthe penalty of word order errors in the standard cross-entropy loss. Starting\nfrom the intuition that reordering generally occurs between phrases, we extend\noaxe by only allowing reordering between ngram phrases and still requiring a\nstrict match of word order within the phrases. Extensive experiments on NAT\nbenchmarks across language pairs and data scales demonstrate the effectiveness\nand universality of our approach. %Further analyses show that the proposed\nngram-oaxe alleviates the multimodality problem with a better modeling of\nphrase translation. Further analyses show that ngram-oaxe indeed improves the\ntranslation of ngram phrases, and produces more fluent translation with a\nbetter modeling of sentence structure.", "published": "2022-10-08 11:39:15", "link": "http://arxiv.org/abs/2210.03999v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EDU-level Extractive Summarization with Varying Summary Lengths", "abstract": "Extractive models usually formulate text summarization as extracting fixed\ntop-$k$ salient sentences from the document as a summary. Few works exploited\nextracting finer-grained Elementary Discourse Unit (EDU) with little analysis\nand justification for the extractive unit selection. Further, the selection\nstrategy of the fixed top-$k$ salient sentences fits the summarization need\npoorly, as the number of salient sentences in different documents varies and\ntherefore a common or best $k$ does not exist in reality. To fill these gaps,\nthis paper first conducts the comparison analysis of oracle summaries based on\nEDUs and sentences, which provides evidence from both theoretical and\nexperimental perspectives to justify and quantify that EDUs make summaries with\nhigher automatic evaluation scores than sentences. Then, considering this merit\nof EDUs, this paper further proposes an EDU-level extractive model with Varying\nsummary Lengths and develops the corresponding learning algorithm. EDU-VL\nlearns to encode and predict probabilities of EDUs in the document, generate\nmultiple candidate summaries with varying lengths based on various $k$ values,\nand encode and score candidate summaries, in an end-to-end training manner.\nFinally, EDU-VL is experimented on single and multi-document benchmark datasets\nand shows improved performances on ROUGE scores in comparison with\nstate-of-the-art extractive models, and further human evaluation suggests that\nEDU-constituent summaries maintain good grammaticality and readability.", "published": "2022-10-08 14:09:58", "link": "http://arxiv.org/abs/2210.04029v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Task-Adaptive Pretraining for Dialogue Response Selection", "abstract": "Recent advancements in dialogue response selection (DRS) are based on the\n\\textit{task-adaptive pre-training (TAP)} approach, by first initializing their\nmodel with BERT~\\cite{devlin-etal-2019-bert}, and adapt to dialogue data with\ndialogue-specific or fine-grained pre-training tasks. However, it is uncertain\nwhether BERT is the best initialization choice, or whether the proposed\ndialogue-specific fine-grained learning tasks are actually better than MLM+NSP.\nThis paper aims to verify assumptions made in previous works and understand the\nsource of improvements for DRS. We show that initializing with RoBERTa achieve\nsimilar performance as BERT, and MLM+NSP can outperform all previously proposed\nTAP tasks, during which we also contribute a new state-of-the-art on the Ubuntu\ncorpus. Additional analyses shows that the main source of improvements comes\nfrom the TAP step, and that the NSP task is crucial to DRS, different from\ncommon NLU tasks.", "published": "2022-10-08 17:58:49", "link": "http://arxiv.org/abs/2210.04073v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KALM: Knowledge-Aware Integration of Local, Document, and Global\n  Contexts for Long Document Understanding", "abstract": "With the advent of pretrained language models (LMs), increasing research\nefforts have been focusing on infusing commonsense and domain-specific\nknowledge to prepare LMs for downstream tasks. These works attempt to leverage\nknowledge graphs, the de facto standard of symbolic knowledge representation,\nalong with pretrained LMs. While existing approaches have leveraged external\nknowledge, it remains an open question how to jointly incorporate knowledge\ngraphs representing varying contexts, from local (e.g., sentence), to\ndocument-level, to global knowledge, to enable knowledge-rich exchange across\nthese contexts. Such rich contextualization can be especially beneficial for\nlong document understanding tasks since standard pretrained LMs are typically\nbounded by the input sequence length. In light of these challenges, we propose\nKALM, a Knowledge-Aware Language Model that jointly leverages knowledge in\nlocal, document-level, and global contexts for long document understanding.\nKALM first encodes long documents and knowledge graphs into the three\nknowledge-aware context representations. It then processes each context with\ncontext-specific layers, followed by a context fusion layer that facilitates\nknowledge exchange to derive an overarching document representation. Extensive\nexperiments demonstrate that KALM achieves state-of-the-art performance on six\nlong document understanding tasks and datasets. Further analyses reveal that\nthe three knowledge-aware contexts are complementary and they all contribute to\nmodel performance, while the importance and information exchange patterns of\ndifferent contexts vary with respect to different tasks and datasets.", "published": "2022-10-08 20:51:02", "link": "http://arxiv.org/abs/2210.04105v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BLAB Reporter: Automated journalism covering the Blue Amazon", "abstract": "This demo paper introduces the BLAB Reporter, a robot-journalist covering the\nBrazilian Blue Amazon. The Reporter is based on a pipeline architecture for\nNatural Language Generation; it offers daily reports, news summaries and\ncurious facts in Brazilian Portuguese. By collecting, storing and analysing\nstructured data from publicly available sources, the robot-journalist uses\ndomain knowledge to generate and publish texts in Twitter. Code and corpus are\npublicly available", "published": "2022-10-08 21:51:50", "link": "http://arxiv.org/abs/2210.06431v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "InfoCSE: Information-aggregated Contrastive Learning of Sentence\n  Embeddings", "abstract": "Contrastive learning has been extensively studied in sentence embedding\nlearning, which assumes that the embeddings of different views of the same\nsentence are closer. The constraint brought by this assumption is weak, and a\ngood sentence representation should also be able to reconstruct the original\nsentence fragments. Therefore, this paper proposes an information-aggregated\ncontrastive learning framework for learning unsupervised sentence embeddings,\ntermed InfoCSE. InfoCSE forces the representation of [CLS] positions to\naggregate denser sentence information by introducing an additional Masked\nlanguage model task and a well-designed network. We evaluate the proposed\nInfoCSE on several benchmark datasets w.r.t the semantic text similarity (STS)\ntask. Experimental results show that InfoCSE outperforms SimCSE by an average\nSpearman correlation of 2.60% on BERT-base, and 1.77% on BERT-large, achieving\nstate-of-the-art results among unsupervised sentence representation learning\nmethods. Our code are available at\nhttps://github.com/caskcsg/sentemb/tree/main/InfoCSE.", "published": "2022-10-08 15:53:19", "link": "http://arxiv.org/abs/2210.06432v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of\n  Large-Scale Pre-Trained Language Models", "abstract": "There are growing interests in adapting large-scale language models using\nparameter-efficient fine-tuning methods. However, accelerating the model itself\nand achieving better inference efficiency through model compression has not\nbeen thoroughly explored yet. Model compression could provide the benefits of\nreducing memory footprints, enabling low-precision computations, and ultimately\nachieving cost-effective inference. To combine parameter-efficient adaptation\nand model compression, we propose AlphaTuning consisting of post-training\nquantization of the pre-trained language model and fine-tuning only some parts\nof quantized parameters for a target task. Specifically, AlphaTuning works by\nemploying binary-coding quantization, which factorizes the full-precision\nparameters into binary parameters and a separate set of scaling factors. During\nthe adaptation phase, the binary values are frozen for all tasks, while the\nscaling factors are fine-tuned for the downstream task. We demonstrate that\nAlphaTuning, when applied to GPT-2 and OPT, performs competitively with full\nfine-tuning on a variety of downstream tasks while achieving >10x compression\nratio under 4-bit quantization and >1,000x reduction in the number of trainable\nparameters.", "published": "2022-10-08 00:36:00", "link": "http://arxiv.org/abs/2210.03858v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Short Text Pre-training with Extended Token Classification for\n  E-commerce Query Understanding", "abstract": "E-commerce query understanding is the process of inferring the shopping\nintent of customers by extracting semantic meaning from their search queries.\nThe recent progress of pre-trained masked language models (MLM) in natural\nlanguage processing is extremely attractive for developing effective query\nunderstanding models. Specifically, MLM learns contextual text embedding via\nrecovering the masked tokens in the sentences. Such a pre-training process\nrelies on the sufficient contextual information. It is, however, less effective\nfor search queries, which are usually short text. When applying masking to\nshort search queries, most contextual information is lost and the intent of the\nsearch queries may be changed. To mitigate the above issues for MLM\npre-training on search queries, we propose a novel pre-training task\nspecifically designed for short text, called Extended Token Classification\n(ETC). Instead of masking the input text, our approach extends the input by\ninserting tokens via a generator network, and trains a discriminator to\nidentify which tokens are inserted in the extended input. We conduct\nexperiments in an E-commerce store to demonstrate the effectiveness of ETC.", "published": "2022-10-08 04:50:04", "link": "http://arxiv.org/abs/2210.03915v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Detecting Label Errors in Token Classification Data", "abstract": "Mislabeled examples are a common issue in real-world data, particularly for\ntasks like token classification where many labels must be chosen on a\nfine-grained basis. Here we consider the task of finding sentences that contain\nlabel errors in token classification datasets. We study 11 different\nstraightforward methods that score tokens/sentences based on the predicted\nclass probabilities output by a (any) token classification model (trained via\nany procedure). In precision-recall evaluations based on real-world label\nerrors in entity recognition data from CoNLL-2003, we identify a simple and\neffective method that consistently detects those sentences containing label\nerrors when applied with different token classification models.", "published": "2022-10-08 05:14:22", "link": "http://arxiv.org/abs/2210.03920v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sparse Teachers Can Be Dense with Knowledge", "abstract": "Recent advances in distilling pretrained language models have discovered\nthat, besides the expressiveness of knowledge, the student-friendliness should\nbe taken into consideration to realize a truly knowledgable teacher. Based on a\npilot study, we find that over-parameterized teachers can produce expressive\nyet student-unfriendly knowledge and are thus limited in overall\nknowledgableness. To remove the parameters that result in\nstudent-unfriendliness, we propose a sparse teacher trick under the guidance of\nan overall knowledgable score for each teacher parameter. The knowledgable\nscore is essentially an interpolation of the expressiveness and\nstudent-friendliness scores. The aim is to ensure that the expressive\nparameters are retained while the student-unfriendly ones are removed.\nExtensive experiments on the GLUE benchmark show that the proposed sparse\nteachers can be dense with knowledge and lead to students with compelling\nperformance in comparison with a series of competitive baselines.", "published": "2022-10-08 05:25:34", "link": "http://arxiv.org/abs/2210.03923v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Fine-Grained Visual Understanding for Video Question Answering\n  via Decoupling Spatial-Temporal Modeling", "abstract": "While recent large-scale video-language pre-training made great progress in\nvideo question answering, the design of spatial modeling of video-language\nmodels is less fine-grained than that of image-language models; existing\npractices of temporal modeling also suffer from weak and noisy alignment\nbetween modalities. To learn fine-grained visual understanding, we decouple\nspatial-temporal modeling and propose a hybrid pipeline, Decoupled\nSpatial-Temporal Encoders, integrating an image- and a video-language encoder.\nThe former encodes spatial semantics from larger but sparsely sampled frames\nindependently of time, while the latter models temporal dynamics at lower\nspatial but higher temporal resolution. To help the video-language model learn\ntemporal relations for video QA, we propose a novel pre-training objective,\nTemporal Referring Modeling, which requires the model to identify temporal\npositions of events in video sequences. Extensive experiments demonstrate that\nour model outperforms previous work pre-trained on orders of magnitude larger\ndatasets.", "published": "2022-10-08 07:03:31", "link": "http://arxiv.org/abs/2210.03941v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Distilling Causal Effect from Miscellaneous Other-Class for Continual\n  Named Entity Recognition", "abstract": "Continual Learning for Named Entity Recognition (CL-NER) aims to learn a\ngrowing number of entity types over time from a stream of data. However, simply\nlearning Other-Class in the same way as new entity types amplifies the\ncatastrophic forgetting and leads to a substantial performance drop. The main\ncause behind this is that Other-Class samples usually contain old entity types,\nand the old knowledge in these Other-Class samples is not preserved properly.\nThanks to the causal inference, we identify that the forgetting is caused by\nthe missing causal effect from the old data. To this end, we propose a unified\ncausal framework to retrieve the causality from both new entity types and\nOther-Class. Furthermore, we apply curriculum learning to mitigate the impact\nof label noise and introduce a self-adaptive weight for balancing the causal\neffects between new entity types and Other-Class. Experimental results on three\nbenchmark datasets show that our method outperforms the state-of-the-art method\nby a large margin. Moreover, our method can be combined with the existing\nstate-of-the-art methods to improve the performance in CL-NER", "published": "2022-10-08 09:37:06", "link": "http://arxiv.org/abs/2210.03980v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Are All Steps Equally Important? Benchmarking Essentiality Detection of\n  Events", "abstract": "Natural language expresses events with varying granularities, where\ncoarse-grained events (goals) can be broken down into finer-grained event\nsequences (steps). A critical yet overlooked aspect of understanding event\nprocesses is recognizing that not all step events hold equal importance toward\nthe completion of a goal. In this paper, we address this gap by examining the\nextent to which current models comprehend the essentiality of step events in\nrelation to a goal event. Cognitive studies suggest that such capability\nenables machines to emulate human commonsense reasoning about preconditions and\nnecessary efforts of everyday tasks. We contribute a high-quality corpus of\n(goal, step) pairs gathered from the community guideline website WikiHow, with\nsteps manually annotated for their essentiality concerning the goal by experts.\nThe high inter-annotator agreement demonstrates that humans possess a\nconsistent understanding of event essentiality. However, after evaluating\nmultiple statistical and largescale pre-trained language models, we find that\nexisting approaches considerably underperform compared to humans. This\nobservation highlights the need for further exploration into this critical and\nchallenging task. The dataset and code are available at\nhttp://cogcomp.org/page/publication_view/1023.", "published": "2022-10-08 18:00:22", "link": "http://arxiv.org/abs/2210.04074v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Comparing Computational Architectures for Automated Journalism", "abstract": "The majority of NLG systems have been designed following either a\ntemplate-based or a pipeline-based architecture. Recent neural models for\ndata-to-text generation have been proposed with an end-to-end deep learning\nflavor, which handles non-linguistic input in natural language without explicit\nintermediary representations. This study compares the most often employed\nmethods for generating Brazilian Portuguese texts from structured data. Results\nsuggest that explicit intermediate steps in the generation process produce\nbetter texts than the ones generated by neural end-to-end architectures,\navoiding data hallucination while better generalizing to unseen inputs. Code\nand corpus are publicly available.", "published": "2022-10-08 21:20:52", "link": "http://arxiv.org/abs/2210.04107v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Number Theory Meets Linguistics: Modelling Noun Pluralisation Across\n  1497 Languages Using 2-adic Metrics", "abstract": "A simple machine learning model of pluralisation as a linear regression\nproblem minimising a p-adic metric substantially outperforms even the most\nrobust of Euclidean-space regressors on languages in the Indo-European,\nAustronesian, Trans New-Guinea, Sino-Tibetan, Nilo-Saharan, Oto-Meanguean and\nAtlantic-Congo language families. There is insufficient evidence to support\nmodelling distinct noun declensions as a p-adic neighbourhood even in\nIndo-European languages.", "published": "2022-10-08 09:37:43", "link": "http://arxiv.org/abs/2211.13124v1", "categories": ["cs.CL", "math.NT"], "primary_category": "cs.CL"}
{"title": "EgoTaskQA: Understanding Human Tasks in Egocentric Videos", "abstract": "Understanding human tasks through video observations is an essential\ncapability of intelligent agents. The challenges of such capability lie in the\ndifficulty of generating a detailed understanding of situated actions, their\neffects on object states (i.e., state changes), and their causal dependencies.\nThese challenges are further aggravated by the natural parallelism from\nmulti-tasking and partial observations in multi-agent collaboration. Most prior\nworks leverage action localization or future prediction as an indirect metric\nfor evaluating such task understanding from videos. To make a direct\nevaluation, we introduce the EgoTaskQA benchmark that provides a single home\nfor the crucial dimensions of task understanding through question-answering on\nreal-world egocentric videos. We meticulously design questions that target the\nunderstanding of (1) action dependencies and effects, (2) intents and goals,\nand (3) agents' beliefs about others. These questions are divided into four\ntypes, including descriptive (what status?), predictive (what will?),\nexplanatory (what caused?), and counterfactual (what if?) to provide diagnostic\nanalyses on spatial, temporal, and causal understandings of goal-oriented\ntasks. We evaluate state-of-the-art video reasoning models on our benchmark and\nshow their significant gaps between humans in understanding complex\ngoal-oriented egocentric videos. We hope this effort will drive the vision\ncommunity to move onward with goal-oriented video understanding and reasoning.", "published": "2022-10-08 05:49:05", "link": "http://arxiv.org/abs/2210.03929v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "KG-MTT-BERT: Knowledge Graph Enhanced BERT for Multi-Type Medical Text\n  Classification", "abstract": "Medical text learning has recently emerged as a promising area to improve\nhealthcare due to the wide adoption of electronic health record (EHR) systems.\nThe complexity of the medical text such as diverse length, mixed text types,\nand full of medical jargon, poses a great challenge for developing effective\ndeep learning models. BERT has presented state-of-the-art results in many NLP\ntasks, such as text classification and question answering. However, the\nstandalone BERT model cannot deal with the complexity of the medical text,\nespecially the lengthy clinical notes. Herein, we develop a new model called\nKG-MTT-BERT (Knowledge Graph Enhanced Multi-Type Text BERT) by extending the\nBERT model for long and multi-type text with the integration of the medical\nknowledge graph. Our model can outperform all baselines and other\nstate-of-the-art models in diagnosis-related group (DRG) classification, which\nrequires comprehensive medical text for accurate classification. We also\ndemonstrated that our model can effectively handle multi-type text and the\nintegration of medical knowledge graph can significantly improve the\nperformance.", "published": "2022-10-08 08:37:44", "link": "http://arxiv.org/abs/2210.03970v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhanced vectors for top-k document retrieval in Question Answering", "abstract": "Modern day applications, especially information retrieval webapps that\ninvolve \"search\" as their use cases are gradually moving towards \"answering\"\nmodules. Conversational chatbots which have been proved to be more engaging to\nusers, use Question Answering as their core. Since, precise answering is\ncomputationally expensive, several approaches have been developed to prefetch\nthe most relevant documents/passages from the database that contain the answer.\nWe propose a different approach that retrieves the evidence documents\nefficiently and accurately, making sure that the relevant document for a given\nuser query is not missed. We do so by assigning each document (or passage in\nour case), a unique identifier and using them to create dense vectors which can\nbe efficiently indexed. More precisely, we use the identifier to predict\nrandomly sampled context window words of the relevant question corresponding to\nthe passage along with the words of passage itself. This naturally embeds the\npassage identifier into the vector space in such a way that the embedding is\ncloser to the question without compromising he information content. This\napproach enables efficient creation of real-time query vectors in ~4\nmilliseconds.", "published": "2022-10-08 07:44:24", "link": "http://arxiv.org/abs/2210.10584v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.IR"}
{"title": "Semantic Representations of Mathematical Expressions in a Continuous\n  Vector Space", "abstract": "Mathematical notation makes up a large portion of STEM literature, yet\nfinding semantic representations for formulae remains a challenging problem.\nBecause mathematical notation is precise, and its meaning changes significantly\nwith small character shifts, the methods that work for natural text do not\nnecessarily work well for mathematical expressions. This work describes an\napproach for representing mathematical expressions in a continuous vector\nspace. We use the encoder of a sequence-to-sequence architecture, trained on\nvisually different but mathematically equivalent expressions, to generate\nvector representations (or embeddings). We compare this approach with a\nstructural approach that considers visual layout to embed an expression and\nshow that our proposed approach is better at capturing mathematical semantics.\nFinally, to expedite future research, we publish a corpus of equivalent\ntranscendental and algebraic expression pairs.", "published": "2022-10-08 22:33:39", "link": "http://arxiv.org/abs/2211.08142v3", "categories": ["cs.CL", "cs.AI", "cs.LO"], "primary_category": "cs.CL"}
{"title": "CoBERT: Self-Supervised Speech Representation Learning Through Code\n  Representation Learning", "abstract": "Speech is the surface form of a finite set of phonetic units, which can be\nrepresented by discrete codes. We propose the Code BERT (CoBERT) approach for\nself-supervised speech representation learning. The idea is to convert an\nutterance to a sequence of discrete codes, and perform code representation\nlearning, where we predict the code representations based on a masked view of\nthe original speech input. Unlike the prior self-distillation approaches of\nwhich the teacher and the student are of the same modality, our target model\npredicts representations from a different modality. CoBERT outperforms the most\nrecent state-of-the-art performance on the ASR task and brings significant\nimprovements on the SUPERB speech translation (ST) task. Our code and models\nare released at https://github.com/mct10/CoBERT.", "published": "2022-10-08 17:15:46", "link": "http://arxiv.org/abs/2210.04062v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
