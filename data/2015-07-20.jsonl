{"title": "How to Generate a Good Word Embedding?", "abstract": "We analyze three critical components of word embedding training: the model,\nthe corpus, and the training parameters. We systematize existing\nneural-network-based word embedding algorithms and compare them using the same\ncorpus. We evaluate each word embedding in three ways: analyzing its semantic\nproperties, using it as a feature for supervised tasks and using it to\ninitialize neural networks. We also provide several simple guidelines for\ntraining word embeddings. First, we discover that corpus domain is more\nimportant than corpus size. We recommend choosing a corpus in a suitable domain\nfor the desired task, after that, using a larger corpus yields better results.\nSecond, we find that faster models provide sufficient performance in most\ncases, and more complex models can be used if the training corpus is\nsufficiently large. Third, the early stopping metric for iterating should rely\non the development set of the desired task rather than the validation loss of\ntraining embedding.", "published": "2015-07-20 15:07:53", "link": "http://arxiv.org/abs/1507.05523v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Notes About a More Aware Dependency Parser", "abstract": "In this paper I explain the reasons that led me to research and conceive a\nnovel technology for dependency parsing, mixing together the strengths of\ndata-driven transition-based and constraint-based approaches. In particular I\nhighlight the problem to infer the reliability of the results of a data-driven\ntransition-based parser, which is extremely important for high-level processes\nthat expect to use correct parsing results. I then briefly introduce a number\nof notes about a new parser model I'm working on, capable to proceed with the\nanalysis in a \"more aware\" way, with a more \"robust\" concept of robustness.", "published": "2015-07-20 20:01:44", "link": "http://arxiv.org/abs/1507.05630v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
