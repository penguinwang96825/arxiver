{"title": "Type-enriched Hierarchical Contrastive Strategy for Fine-Grained Entity\n  Typing", "abstract": "Fine-grained entity typing (FET) aims to deduce specific semantic types of\nthe entity mentions in text. Modern methods for FET mainly focus on learning\nwhat a certain type looks like. And few works directly model the type\ndifferences, that is, let models know the extent that one type is different\nfrom others. To alleviate this problem, we propose a type-enriched hierarchical\ncontrastive strategy for FET. Our method can directly model the differences\nbetween hierarchical types and improve the ability to distinguish multi-grained\nsimilar types. On the one hand, we embed type into entity contexts to make type\ninformation directly perceptible. On the other hand, we design a constrained\ncontrastive strategy on the hierarchical structure to directly model the type\ndifferences, which can simultaneously perceive the distinguishability between\ntypes at different granularity. Experimental results on three benchmarks, BBN,\nOntoNotes, and FIGER show that our method achieves significant performance on\nFET by effectively modeling type differences.", "published": "2022-08-22 06:38:08", "link": "http://arxiv.org/abs/2208.10081v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recent Advances in Text-to-SQL: A Survey of What We Have and What We\n  Expect", "abstract": "Text-to-SQL has attracted attention from both the natural language processing\nand database communities because of its ability to convert the semantics in\nnatural language into SQL queries and its practical application in building\nnatural language interfaces to database systems. The major challenges in\ntext-to-SQL lie in encoding the meaning of natural utterances, decoding to SQL\nqueries, and translating the semantics between these two forms. These\nchallenges have been addressed to different extents by the recent advances.\nHowever, there is still a lack of comprehensive surveys for this task. To this\nend, we review recent progress on text-to-SQL for datasets, methods, and\nevaluation and provide this systematic survey, addressing the aforementioned\nchallenges and discussing potential future directions. We hope that this survey\ncan serve as quick access to existing work and motivate future research.", "published": "2022-08-22 07:18:23", "link": "http://arxiv.org/abs/2208.10099v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PANDA: Prompt Transfer Meets Knowledge Distillation for Efficient Model\n  Adaptation", "abstract": "Prompt Transfer (PoT) is a recently-proposed approach to improve\nprompt-tuning, by initializing the target prompt with the existing prompt\ntrained on similar source tasks. However, such a vanilla PoT approach usually\nachieves sub-optimal performance, as (i) the PoT is sensitive to the similarity\nof source-target pair and (ii) directly fine-tuning the prompt initialized with\nsource prompt on target task might lead to forgetting of the useful general\nknowledge learned from source task. To tackle these issues, we propose a new\nmetric to accurately predict the prompt transferability (regarding (i)), and a\nnovel PoT approach (namely PANDA) that leverages the knowledge distillation\ntechnique to alleviate the knowledge forgetting effectively (regarding (ii)).\nExtensive and systematic experiments on 189 combinations of 21 source and 9\ntarget datasets across 5 scales of PLMs demonstrate that: 1) our proposed\nmetric works well to predict the prompt transferability; 2) our PANDA\nconsistently outperforms the vanilla PoT approach by 2.3% average score (up to\n24.1%) among all tasks and model sizes; 3) with our PANDA approach,\nprompt-tuning can achieve competitive and even better performance than\nmodel-tuning in various PLM scales scenarios. We have publicly released our\ncode in https://github.com/WHU-ZQH/PANDA.", "published": "2022-08-22 09:14:14", "link": "http://arxiv.org/abs/2208.10160v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Locate Then Ask: Interpretable Stepwise Reasoning for Multi-hop Question\n  Answering", "abstract": "Multi-hop reasoning requires aggregating multiple documents to answer a\ncomplex question. Existing methods usually decompose the multi-hop question\ninto simpler single-hop questions to solve the problem for illustrating the\nexplainable reasoning process. However, they ignore grounding on the supporting\nfacts of each reasoning step, which tends to generate inaccurate\ndecompositions. In this paper, we propose an interpretable stepwise reasoning\nframework to incorporate both single-hop supporting sentence identification and\nsingle-hop question generation at each intermediate step, and utilize the\ninference of the current hop for the next until reasoning out the final result.\nWe employ a unified reader model for both intermediate hop reasoning and final\nhop inference and adopt joint optimization for more accurate and robust\nmulti-hop reasoning. We conduct experiments on two benchmark datasets HotpotQA\nand 2WikiMultiHopQA. The results show that our method can effectively boost\nperformance and also yields a better interpretable reasoning process without\ndecomposition supervision.", "published": "2022-08-22 13:24:25", "link": "http://arxiv.org/abs/2208.10297v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Novel Multi-Task Learning Approach for Context-Sensitive Compound Type\n  Identification in Sanskrit", "abstract": "The phenomenon of compounding is ubiquitous in Sanskrit. It serves for\nachieving brevity in expressing thoughts, while simultaneously enriching the\nlexical and structural formation of the language. In this work, we focus on the\nSanskrit Compound Type Identification (SaCTI) task, where we consider the\nproblem of identifying semantic relations between the components of a compound\nword. Earlier approaches solely rely on the lexical information obtained from\nthe components and ignore the most crucial contextual and syntactic information\nuseful for SaCTI. However, the SaCTI task is challenging primarily due to the\nimplicitly encoded context-sensitive semantic relation between the compound\ncomponents.\n  Thus, we propose a novel multi-task learning architecture which incorporates\nthe contextual information and enriches the complementary syntactic information\nusing morphological tagging and dependency parsing as two auxiliary tasks.\nExperiments on the benchmark datasets for SaCTI show 6.1 points (Accuracy) and\n7.7 points (F1-score) absolute gain compared to the state-of-the-art system.\nFurther, our multi-lingual experiments demonstrate the efficacy of the proposed\narchitecture in English and Marathi languages.The code and datasets are\npublicly available at https://github.com/ashishgupta2598/SaCTI", "published": "2022-08-22 13:41:51", "link": "http://arxiv.org/abs/2208.10310v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dialogue Term Extraction using Transfer Learning and Topological Data\n  Analysis", "abstract": "Goal oriented dialogue systems were originally designed as a natural language\ninterface to a fixed data-set of entities that users might inquire about,\nfurther described by domain, slots, and values. As we move towards adaptable\ndialogue systems where knowledge about domains, slots, and values may change,\nthere is an increasing need to automatically extract these terms from raw\ndialogues or related non-dialogue data on a large scale. In this paper, we take\nan important step in this direction by exploring different features that can\nenable systems to discover realizations of domains, slots, and values in\ndialogues in a purely data-driven fashion. The features that we examine stem\nfrom word embeddings, language modelling features, as well as topological\nfeatures of the word embedding space. To examine the utility of each feature\nset, we train a seed model based on the widely used MultiWOZ data-set. Then, we\napply this model to a different corpus, the Schema-Guided Dialogue data-set.\nOur method outperforms the previously proposed approach that relies solely on\nword embeddings. We also demonstrate that each of the features is responsible\nfor discovering different kinds of content. We believe our results warrant\nfurther research towards ontology induction, and continued harnessing of\ntopological data analysis for dialogue and natural language processing\nresearch.", "published": "2022-08-22 17:04:04", "link": "http://arxiv.org/abs/2208.10448v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Selection Collider Bias in Large Language Models", "abstract": "In this paper we motivate the causal mechanisms behind sample selection\ninduced collider bias (selection collider bias) that can cause Large Language\nModels (LLMs) to learn unconditional dependence between entities that are\nunconditionally independent in the real world. We show that selection collider\nbias can become amplified in underspecified learning tasks, and although\ndifficult to overcome, we describe a method to exploit the resulting spurious\ncorrelations for determination of when a model may be uncertain about its\nprediction. We demonstrate an uncertainty metric that matches human uncertainty\nin tasks with gender pronoun underspecification on an extended version of the\nWinogender Schemas evaluation set, and we provide an online demo where users\ncan apply our uncertainty metric to their own texts and models.", "published": "2022-08-22 05:38:15", "link": "http://arxiv.org/abs/2208.10063v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Repurposing Knowledge Graph Embeddings for Triple Representation via\n  Weak Supervision", "abstract": "The majority of knowledge graph embedding techniques treat entities and\npredicates as separate embedding matrices, using aggregation functions to build\na representation of the input triple. However, these aggregations are lossy,\ni.e. they do not capture the semantics of the original triples, such as\ninformation contained in the predicates. To combat these shortcomings, current\nmethods learn triple embeddings from scratch without utilizing entity and\npredicate embeddings from pre-trained models. In this paper, we design a novel\nfine-tuning approach for learning triple embeddings by creating weak\nsupervision signals from pre-trained knowledge graph embeddings. We develop a\nmethod for automatically sampling triples from a knowledge graph and estimating\ntheir pairwise similarities from pre-trained embedding models. These pairwise\nsimilarity scores are then fed to a Siamese-like neural architecture to\nfine-tune triple representations. We evaluate the proposed method on two widely\nstudied knowledge graphs and show consistent improvement over other\nstate-of-the-art triple embedding methods on triple classification and triple\nclustering tasks.", "published": "2022-08-22 14:07:08", "link": "http://arxiv.org/abs/2208.10328v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Inductive Knowledge Graph Reasoning for Multi-batch Emerging Entities", "abstract": "Over the years, reasoning over knowledge graphs (KGs), which aims to infer\nnew conclusions from known facts, has mostly focused on static KGs. The\nunceasing growth of knowledge in real life raises the necessity to enable the\ninductive reasoning ability on expanding KGs. Existing inductive work assumes\nthat new entities all emerge once in a batch, which oversimplifies the real\nscenario that new entities continually appear. This study dives into a more\nrealistic and challenging setting where new entities emerge in multiple\nbatches. We propose a walk-based inductive reasoning model to tackle the new\nsetting. Specifically, a graph convolutional network with adaptive relation\naggregation is designed to encode and update entities using their neighboring\nrelations. To capture the varying neighbor importance, we employ a query-aware\nfeedback attention mechanism during the aggregation. Furthermore, to alleviate\nthe sparse link problem of new entities, we propose a link augmentation\nstrategy to add trustworthy facts into KGs. We construct three new datasets for\nsimulating this multi-batch emergence scenario. The experimental results show\nthat our proposed model outperforms state-of-the-art embedding-based,\nwalk-based and rule-based models on inductive KG reasoning.", "published": "2022-08-22 14:59:19", "link": "http://arxiv.org/abs/2208.10378v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DP-Rewrite: Towards Reproducibility and Transparency in Differentially\n  Private Text Rewriting", "abstract": "Text rewriting with differential privacy (DP) provides concrete theoretical\nguarantees for protecting the privacy of individuals in textual documents. In\npractice, existing systems may lack the means to validate their\nprivacy-preserving claims, leading to problems of transparency and\nreproducibility. We introduce DP-Rewrite, an open-source framework for\ndifferentially private text rewriting which aims to solve these problems by\nbeing modular, extensible, and highly customizable. Our system incorporates a\nvariety of downstream datasets, models, pre-training procedures, and evaluation\nmetrics to provide a flexible way to lead and validate private text rewriting\nresearch. To demonstrate our software in practice, we provide a set of\nexperiments as a case study on the ADePT DP text rewriting system, detecting a\nprivacy leak in its pre-training approach. Our system is publicly available,\nand we hope that it will help the community to make DP text rewriting research\nmore accessible and transparent.", "published": "2022-08-22 15:38:16", "link": "http://arxiv.org/abs/2208.10400v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Image as a Foreign Language: BEiT Pretraining for All Vision and\n  Vision-Language Tasks", "abstract": "A big convergence of language, vision, and multimodal pretraining is\nemerging. In this work, we introduce a general-purpose multimodal foundation\nmodel BEiT-3, which achieves state-of-the-art transfer performance on both\nvision and vision-language tasks. Specifically, we advance the big convergence\nfrom three aspects: backbone architecture, pretraining task, and model scaling\nup. We introduce Multiway Transformers for general-purpose modeling, where the\nmodular architecture enables both deep fusion and modality-specific encoding.\nBased on the shared backbone, we perform masked \"language\" modeling on images\n(Imglish), texts (English), and image-text pairs (\"parallel sentences\") in a\nunified manner. Experimental results show that BEiT-3 obtains state-of-the-art\nperformance on object detection (COCO), semantic segmentation (ADE20K), image\nclassification (ImageNet), visual reasoning (NLVR2), visual question answering\n(VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K, COCO).", "published": "2022-08-22 16:55:04", "link": "http://arxiv.org/abs/2208.10442v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Revising Image-Text Retrieval via Multi-Modal Entailment", "abstract": "An outstanding image-text retrieval model depends on high-quality labeled\ndata. While the builders of existing image-text retrieval datasets strive to\nensure that the caption matches the linked image, they cannot prevent a caption\nfrom fitting other images. We observe that such a many-to-many matching\nphenomenon is quite common in the widely-used retrieval datasets, where one\ncaption can describe up to 178 images. These large matching-lost data not only\nconfuse the model in training but also weaken the evaluation accuracy. Inspired\nby visual and textual entailment tasks, we propose a multi-modal entailment\nclassifier to determine whether a sentence is entailed by an image plus its\nlinked captions. Subsequently, we revise the image-text retrieval datasets by\nadding these entailed captions as additional weak labels of an image and\ndevelop a universal variable learning rate strategy to teach a retrieval model\nto distinguish the entailed captions from other negative samples. In\nexperiments, we manually annotate an entailment-corrected image-text retrieval\ndataset for evaluation. The results demonstrate that the proposed entailment\nclassifier achieves about 78% accuracy and consistently improves the\nperformance of image-text retrieval baselines.", "published": "2022-08-22 07:58:54", "link": "http://arxiv.org/abs/2208.10126v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Review of Natural Language Processing in Pharmacology", "abstract": "Natural language processing (NLP) is an area of artificial intelligence that\napplies information technologies to process the human language, understand it\nto a certain degree, and use it in various applications. This area has rapidly\ndeveloped in the last few years and now employs modern variants of deep neural\nnetworks to extract relevant patterns from large text corpora. The main\nobjective of this work is to survey the recent use of NLP in the field of\npharmacology. As our work shows, NLP is a highly relevant information\nextraction and processing approach for pharmacology. It has been used\nextensively, from intelligent searches through thousands of medical documents\nto finding traces of adversarial drug interactions in social media. We split\nour coverage into five categories to survey modern NLP methodology, commonly\naddressed tasks, relevant textual data, knowledge bases, and useful programming\nlibraries. We split each of the five categories into appropriate subcategories,\ndescribe their main properties and ideas, and summarize them in a tabular form.\nThe resulting survey presents a comprehensive overview of the area, useful to\npractitioners and interested observers.", "published": "2022-08-22 12:10:27", "link": "http://arxiv.org/abs/2208.10228v2", "categories": ["cs.CL", "cs.LG", "q-bio.BM", "J.3; A.1"], "primary_category": "cs.CL"}
{"title": "The optimality of word lengths. Theoretical foundations and an empirical\n  study", "abstract": "Zipf's law of abbreviation, namely the tendency of more frequent words to be\nshorter, has been viewed as a manifestation of compression, i.e. the\nminimization of the length of forms -- a universal principle of natural\ncommunication. Although the claim that languages are optimized has become\ntrendy, attempts to measure the degree of optimization of languages have been\nrather scarce. Here we present two optimality scores that are dualy normalized,\nnamely, they are normalized with respect to both the minimum and the random\nbaseline. We analyze the theoretical and statistical pros and cons of these and\nother scores. Harnessing the best score, we quantify for the first time the\ndegree of optimality of word lengths in languages. This indicates that\nlanguages are optimized to 62 or 67 percent on average (depending on the\nsource) when word lengths are measured in characters, and to 65 percent on\naverage when word lengths are measured in time. In general, spoken word\ndurations are more optimized than written word lengths in characters. Our work\npaves the way to measure the degree of optimality of the vocalizations or\ngestures of other species, and to compare them against written, spoken, or\nsigned human languages.", "published": "2022-08-22 15:03:31", "link": "http://arxiv.org/abs/2208.10384v5", "categories": ["cs.CL", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "Generalizing Hate Speech Detection Using Multi-Task Learning: A Case\n  Study of Political Public Figures", "abstract": "Automatic identification of hateful and abusive content is vital in combating\nthe spread of harmful online content and its damaging effects. Most existing\nworks evaluate models by examining the generalization error on train-test\nsplits on hate speech datasets. These datasets often differ in their\ndefinitions and labeling criteria, leading to poor generalization performance\nwhen predicting across new domains and datasets. This work proposes a new\nMulti-task Learning (MTL) pipeline that trains simultaneously across multiple\nhate speech datasets to construct a more encompassing classification model.\nUsing a dataset-level leave-one-out evaluation (designating a dataset for\ntesting and jointly training on all others), we trial the MTL detection on new,\npreviously unseen datasets. Our results consistently outperform a large sample\nof existing work. We show strong results when examining the generalization\nerror in train-test splits and substantial improvements when predicting on\npreviously unseen datasets. Furthermore, we assemble a novel dataset, dubbed\nPubFigs, focusing on the problematic speech of American Public Political\nFigures. We crowdsource-label using Amazon MTurk more than $20,000$ tweets and\nmachine-label problematic speech in all the $305,235$ tweets in PubFigs. We\nfind that the abusive and hate tweeting mainly originates from right-leaning\nfigures and relates to six topics, including Islam, women, ethnicity, and\nimmigrants. We show that MTL builds embeddings that can simultaneously separate\nabusive from hate speech, and identify its topics.", "published": "2022-08-22 21:13:38", "link": "http://arxiv.org/abs/2208.10598v2", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Interpreting Embedding Spaces by Conceptualization", "abstract": "One of the main methods for computational interpretation of a text is mapping\nit into a vector in some embedding space. Such vectors can then be used for a\nvariety of textual processing tasks. Recently, most embedding spaces are a\nproduct of training large language models (LLMs). One major drawback of this\ntype of representation is their incomprehensibility to humans. Understanding\nthe embedding space is crucial for several important needs, including the need\nto debug the embedding method and compare it to alternatives, and the need to\ndetect biases hidden in the model. In this paper, we present a novel method of\nunderstanding embeddings by transforming a latent embedding space into a\ncomprehensible conceptual space. We present an algorithm for deriving a\nconceptual space with dynamic on-demand granularity. We devise a new evaluation\nmethod, using either human rater or LLM-based raters, to show that the\nconceptualized vectors indeed represent the semantics of the original latent\nones. We show the use of our method for various tasks, including comparing the\nsemantics of alternative models and tracing the layers of the LLM. The code is\navailable online\nhttps://github.com/adiSimhi/Interpreting-Embedding-Spaces-by-Conceptualization.", "published": "2022-08-22 15:32:17", "link": "http://arxiv.org/abs/2209.00445v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Concurrent Validity of Automatic Speech and Pause Measures During\n  Passage Reading in ALS", "abstract": "The analysis of speech measures in individuals with amyotrophic lateral\nsclerosis (ALS) can provide essential information for early diagnosis and\ntracking disease progression. However, current methods for extracting speech\nand pause features are manual or semi-automatic, which makes them\ntime-consuming and labour-intensive. The advent of speech-text alignment\nalgorithms provides an opportunity for inexpensive, automated, and accurate\nanalysis of speech measures in individuals with ALS. There is a need to\nvalidate speech and pause features calculated by these algorithms against\ncurrent gold standard methods. In this study, we extracted 8 speech/pause\nfeatures from 646 audio files of individuals with ALS and healthy controls\nperforming passage reading. Two pretrained forced alignment models - one using\ntransformers and another using a Gaussian mixture / hidden Markov architecture\n- were used for automatic feature extraction. The results were then validated\nagainst semi-automatic speech/pause analysis software, with further subgroup\nanalyses based on audio quality and disease severity. Features extracted using\ntransformer-based forced alignment had the highest agreement with gold\nstandards, including in terms of audio quality and disease severity. This study\nlays the groundwork for future intelligent diagnostic support systems for\nclinicians, and for novel methods of tracking disease progression remotely from\nhome.", "published": "2022-08-22 21:01:35", "link": "http://arxiv.org/abs/2208.10597v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Spatially Selective Active Noise Control Systems", "abstract": "Active noise control (ANC) systems are commonly designed to achieve maximal\nsound reduction regardless of the incident direction of the sound. When desired\nsound is present, the state-of-the-art methods add a separate system to\nreconstruct it. This can result in distortion and latency. In this work, we\npropose a multi-channel ANC system that only reduces sound from undesired\ndirections, and the system truly preserves the desired sound instead of\nreproducing it. The proposed algorithm imposes a spatial constraint on the\nhybrid ANC cost function to achieve spatial selectivity. Based on a six-channel\nmicrophone array on a pair of augmented eyeglasses, results show that the\nsystem minimized only noise coming from undesired directions. The control\nperformance could be maintained even when the array was heavily perturbed. The\nproposed algorithm was also compared with the existing methods in the\nliterature. Not only did the proposed system provide better noise reduction,\nbut it also required much less effort. The binaural localization cues did not\nneed to be reconstructed since the system preserved the physical sound wave\nfrom the desired source.", "published": "2022-08-22 01:15:02", "link": "http://arxiv.org/abs/2208.09997v2", "categories": ["eess.AS", "cs.SY", "eess.SY"], "primary_category": "eess.AS"}
{"title": "Multi-View Attention Transfer for Efficient Speech Enhancement", "abstract": "Recent deep learning models have achieved high performance in speech\nenhancement; however, it is still challenging to obtain a fast and\nlow-complexity model without significant performance degradation. Previous\nknowledge distillation studies on speech enhancement could not solve this\nproblem because their output distillation methods do not fit the speech\nenhancement task in some aspects. In this study, we propose multi-view\nattention transfer (MV-AT), a feature-based distillation, to obtain efficient\nspeech enhancement models in the time domain. Based on the multi-view features\nextraction model, MV-AT transfers multi-view knowledge of the teacher network\nto the student network without additional parameters. The experimental results\nshow that the proposed method consistently improved the performance of student\nmodels of various sizes on the Valentini and deep noise suppression (DNS)\ndatasets. MANNER-S-8.1GF with our proposed method, a lightweight model for\nefficient deployment, achieved 15.4x and 4.71x fewer parameters and\nfloating-point operations (FLOPs), respectively, compared to the baseline model\nwith similar performance.", "published": "2022-08-22 14:47:47", "link": "http://arxiv.org/abs/2208.10367v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DualVoice: Speech Interaction that Discriminates between Normal and\n  Whispered Voice Input", "abstract": "Interactions based on automatic speech recognition (ASR) have become widely\nused, with speech input being increasingly utilized to create documents.\nHowever, as there is no easy way to distinguish between commands being issued\nand text required to be input in speech, misrecognitions are difficult to\nidentify and correct, meaning that documents need to be manually edited and\ncorrected. The input of symbols and commands is also challenging because these\nmay be misrecognized as text letters. To address these problems, this study\nproposes a speech interaction method called DualVoice, by which commands can be\ninput in a whispered voice and letters in a normal voice. The proposed method\ndoes not require any specialized hardware other than a regular microphone,\nenabling a complete hands-free interaction. The method can be used in a wide\nrange of situations where speech recognition is already available, ranging from\ntext input to mobile/wearable computing. Two neural networks were designed in\nthis study, one for discriminating normal speech from whispered speech, and the\nsecond for recognizing whisper speech. A prototype of a text input system was\nthen developed to show how normal and whispered voice can be used in speech\ntext input. Other potential applications using DualVoice are also discussed.", "published": "2022-08-22 13:01:28", "link": "http://arxiv.org/abs/2208.10499v1", "categories": ["cs.HC", "cs.LG", "cs.SD", "eess.AS", "H.5.2; H.1.2"], "primary_category": "cs.HC"}
{"title": "Low-Level Physiological Implications of End-to-End Learning of Speech\n  Recognition", "abstract": "Current speech recognition architectures perform very well from the point of\nview of machine learning, hence user interaction. This suggests that they are\nemulating the human biological system well. We investigate whether the\ninference can be inverted to provide insights into that biological system; in\nparticular the hearing mechanism. Using SincNet, we confirm that end-to-end\nsystems do learn well known filterbank structures. However, we also show that\nwider band-width filters are important in the learned structure. Whilst some\nbenefits can be gained by initialising both narrow and wide-band filters,\nphysiological constraints suggest that such filters arise in mid-brain rather\nthan the cochlea. We show that standard machine learning architectures must be\nmodified to allow this process to be emulated neurally.", "published": "2022-08-22 13:10:36", "link": "http://arxiv.org/abs/2208.11700v1", "categories": ["q-bio.NC", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "q-bio.NC"}
{"title": "Examining Audio Communication Mechanisms for Supervising Fleets of\n  Agricultural Robots", "abstract": "Agriculture is facing a labor crisis, leading to increased interest in fleets\nof small, under-canopy robots (agbots) that can perform precise, targeted\nactions (e.g., crop scouting, weeding, fertilization), while being supervised\nby human operators remotely. However, farmers are not necessarily experts in\nrobotics technology and will not adopt technologies that add to their workload\nor do not provide an immediate payoff. In this work, we explore methods for\ncommunication between a remote human operator and multiple agbots and examine\nthe impact of audio communication on the operator's preferences and\nproductivity. We develop a simulation platform where agbots are deployed across\na field, randomly encounter failures, and call for help from the operator. As\nthe agbots report errors, various audio communication mechanisms are tested to\nconvey which robot failed and what type of failure occurs. The human is tasked\nwith verbally diagnosing the failure while completing a secondary task. A user\nstudy was conducted to test three audio communication methods: earcons,\nsingle-phrase commands, and full sentence communication. Each participant\ncompleted a survey to determine their preferences and each method's overall\neffectiveness. Our results suggest that the system using single phrases is the\nmost positively perceived by participants and may allow for the human to\ncomplete the secondary task more efficiently. The code is available at:\nhttps://github.com/akamboj2/Agbot-Sim.", "published": "2022-08-22 17:19:20", "link": "http://arxiv.org/abs/2208.10455v1", "categories": ["cs.RO", "cs.CY", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.RO"}
{"title": "Are disentangled representations all you need to build speaker\n  anonymization systems?", "abstract": "Speech signals contain a lot of sensitive information, such as the speaker's\nidentity, which raises privacy concerns when speech data get collected. Speaker\nanonymization aims to transform a speech signal to remove the source speaker's\nidentity while leaving the spoken content unchanged. Current methods perform\nthe transformation by relying on content/speaker disentanglement and voice\nconversion. Usually, an acoustic model from an automatic speech recognition\nsystem extracts the content representation while an x-vector system extracts\nthe speaker representation. Prior work has shown that the extracted features\nare not perfectly disentangled. This paper tackles how to improve features\ndisentanglement, and thus the converted anonymized speech. We propose enhancing\nthe disentanglement by removing speaker information from the acoustic model\nusing vector quantization. Evaluation done using the VoicePrivacy 2022 toolkit\nshowed that vector quantization helps conceal the original speaker identity\nwhile maintaining utility for speech recognition.", "published": "2022-08-22 07:51:47", "link": "http://arxiv.org/abs/2208.10497v3", "categories": ["cs.SD", "cs.AI", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Make Acoustic and Visual Cues Matter: CH-SIMS v2.0 Dataset and AV-Mixup\n  Consistent Module", "abstract": "Multimodal sentiment analysis (MSA), which supposes to improve text-based\nsentiment analysis with associated acoustic and visual modalities, is an\nemerging research area due to its potential applications in Human-Computer\nInteraction (HCI). However, the existing researches observe that the acoustic\nand visual modalities contribute much less than the textual modality, termed as\ntext-predominant. Under such circumstances, in this work, we emphasize making\nnon-verbal cues matter for the MSA task. Firstly, from the resource\nperspective, we present the CH-SIMS v2.0 dataset, an extension and enhancement\nof the CH-SIMS. Compared with the original dataset, the CH-SIMS v2.0 doubles\nits size with another 2121 refined video segments with both unimodal and\nmultimodal annotations and collects 10161 unlabelled raw video segments with\nrich acoustic and visual emotion-bearing context to highlight non-verbal cues\nfor sentiment prediction. Secondly, from the model perspective, benefiting from\nthe unimodal annotations and the unsupervised data in the CH-SIMS v2.0, the\nAcoustic Visual Mixup Consistent (AV-MC) framework is proposed. The designed\nmodality mixup module can be regarded as an augmentation, which mixes the\nacoustic and visual modalities from different videos. Through drawing\nunobserved multimodal context along with the text, the model can learn to be\naware of different non-verbal contexts for sentiment prediction. Our\nevaluations demonstrate that both CH-SIMS v2.0 and AV-MC framework enables\nfurther research for discovering emotion-bearing acoustic and visual cues and\npaves the path to interpretable end-to-end HCI applications for real-world\nscenarios.", "published": "2022-08-22 03:31:33", "link": "http://arxiv.org/abs/2209.02604v1", "categories": ["cs.MM", "cs.AI", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "The GENEA Challenge 2022: A large evaluation of data-driven co-speech\n  gesture generation", "abstract": "This paper reports on the second GENEA Challenge to benchmark data-driven\nautomatic co-speech gesture generation. Participating teams used the same\nspeech and motion dataset to build gesture-generation systems. Motion generated\nby all these systems was rendered to video using a standardised visualisation\npipeline and evaluated in several large, crowdsourced user studies. Unlike when\ncomparing different research papers, differences in results are here only due\nto differences between methods, enabling direct comparison between systems.\nThis year's dataset was based on 18 hours of full-body motion capture,\nincluding fingers, of different persons engaging in dyadic conversation. Ten\nteams participated in the challenge across two tiers: full-body and upper-body\ngesticulation. For each tier we evaluated both the human-likeness of the\ngesture motion and its appropriateness for the specific speech signal. Our\nevaluations decouple human-likeness from gesture appropriateness, which\npreviously was a major challenge in the field.\n  The evaluation results are a revolution, and a revelation. Some synthetic\nconditions are rated as significantly more human-like than human motion\ncapture. To the best of our knowledge, this has never been shown before on a\nhigh-fidelity avatar. On the other hand, all synthetic motion is found to be\nvastly less appropriate for the speech than the original motion-capture\nrecordings. Additional material is available via the project website at\nhttps://youngwoo-yoon.github.io/GENEAchallenge2022/", "published": "2022-08-22 16:55:02", "link": "http://arxiv.org/abs/2208.10441v1", "categories": ["cs.HC", "cs.GR", "cs.LG", "cs.MM", "cs.SD", "eess.AS", "I.3; I.2"], "primary_category": "cs.HC"}
