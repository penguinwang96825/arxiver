{"title": "Canonical Correlation Inference for Mapping Abstract Scenes to Text", "abstract": "We describe a technique for structured prediction, based on canonical\ncorrelation analysis. Our learning algorithm finds two projections for the\ninput and the output spaces that aim at projecting a given input and its\ncorrect output into points close to each other. We demonstrate our technique on\na language-vision problem, namely the problem of giving a textual description\nto an \"abstract scene\".", "published": "2016-08-09 12:26:19", "link": "http://arxiv.org/abs/1608.02784v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Language of Generalization", "abstract": "Language provides simple ways of communicating generalizable knowledge to\neach other (e.g., \"Birds fly\", \"John hikes\", \"Fire makes smoke\"). Though found\nin every language and emerging early in development, the language of\ngeneralization is philosophically puzzling and has resisted precise\nformalization. Here, we propose the first formal account of generalizations\nconveyed with language that makes quantitative predictions about human\nunderstanding. We test our model in three diverse domains: generalizations\nabout categories (generic language), events (habitual language), and causes\n(causal language). The model explains the gradience in human endorsement\nthrough the interplay between a simple truth-conditional semantic theory and\ndiverse beliefs about properties, formalized in a probabilistic model of\nlanguage understanding. This work opens the door to understanding precisely how\nabstract knowledge is learned from language.", "published": "2016-08-09 19:40:21", "link": "http://arxiv.org/abs/1608.02926v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Temporal Attention Model for Neural Machine Translation", "abstract": "Attention-based Neural Machine Translation (NMT) models suffer from attention\ndeficiency issues as has been observed in recent research. We propose a novel\nmechanism to address some of these limitations and improve the NMT attention.\nSpecifically, our approach memorizes the alignments temporally (within each\nsentence) and modulates the attention with the accumulated temporal memory, as\nthe decoder generates the candidate translation. We compare our approach\nagainst the baseline NMT model and two other related approaches that address\nthis issue either explicitly or implicitly. Large-scale experiments on two\nlanguage pairs show that our approach achieves better and robust gains over the\nbaseline and related NMT approaches. Our model further outperforms strong SMT\nbaselines in some settings even without using ensembles.", "published": "2016-08-09 19:42:14", "link": "http://arxiv.org/abs/1608.02927v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-task Domain Adaptation for Sequence Tagging", "abstract": "Many domain adaptation approaches rely on learning cross domain shared\nrepresentations to transfer the knowledge learned in one domain to other\ndomains. Traditional domain adaptation only considers adapting for one task. In\nthis paper, we explore multi-task representation learning under the domain\nadaptation scenario. We propose a neural network framework that supports domain\nadaptation for multiple tasks simultaneously, and learns shared representations\nthat better generalize for domain adaptation. We apply the proposed framework\nto domain adaptation for sequence tagging problems considering two tasks:\nChinese word segmentation and named entity recognition. Experiments show that\nmulti-task domain adaptation works better than disjoint domain adaptation for\neach task, and achieves the state-of-the-art results for both tasks in the\nsocial media domain.", "published": "2016-08-09 04:38:38", "link": "http://arxiv.org/abs/1608.02689v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TweeTime: A Minimally Supervised Method for Recognizing and Normalizing\n  Time Expressions in Twitter", "abstract": "We describe TweeTIME, a temporal tagger for recognizing and normalizing time\nexpressions in Twitter. Most previous work in social media analysis has to rely\non temporal resolvers that are designed for well-edited text, and therefore\nsuffer from the reduced performance due to domain mismatch. We present a\nminimally supervised method that learns from large quantities of unlabeled data\nand requires no hand-engineered rules or hand-annotated training corpora.\nTweeTIME achieves 0.68 F1 score on the end-to-end task of resolving date\nexpressions, outperforming a broad range of state-of-the-art systems.", "published": "2016-08-09 18:29:04", "link": "http://arxiv.org/abs/1608.02904v4", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Neural Generation of Regular Expressions from Natural Language with\n  Minimal Domain Knowledge", "abstract": "This paper explores the task of translating natural language queries into\nregular expressions which embody their meaning. In contrast to prior work, the\nproposed neural model does not utilize domain-specific crafting, learning to\ntranslate directly from a parallel corpus. To fully explore the potential of\nneural models, we propose a methodology for collecting a large corpus of\nregular expression, natural language pairs. Our resulting model achieves a\nperformance gain of 19.6% over previous state-of-the-art models.", "published": "2016-08-09 23:05:03", "link": "http://arxiv.org/abs/1608.03000v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mean Box Pooling: A Rich Image Representation and Output Embedding for\n  the Visual Madlibs Task", "abstract": "We present Mean Box Pooling, a novel visual representation that pools over\nCNN representations of a large number, highly overlapping object proposals. We\nshow that such representation together with nCCA, a successful multimodal\nembedding technique, achieves state-of-the-art performance on the Visual\nMadlibs task. Moreover, inspired by the nCCA's objective function, we extend\nclassical CNN+LSTM approach to train the network by directly maximizing the\nsimilarity between the internal representation of the deep learning\narchitecture and candidate answers. Again, such approach achieves a significant\nimprovement over the prior work that also uses CNN+LSTM approach on Visual\nMadlibs.", "published": "2016-08-09 08:24:02", "link": "http://arxiv.org/abs/1608.02717v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Towards cross-lingual distributed representations without parallel text\n  trained with adversarial autoencoders", "abstract": "Current approaches to learning vector representations of text that are\ncompatible between different languages usually require some amount of parallel\ntext, aligned at word, sentence or at least document level. We hypothesize\nhowever, that different natural languages share enough semantic structure that\nit should be possible, in principle, to learn compatible vector representations\njust by analyzing the monolingual distribution of words.\n  In order to evaluate this hypothesis, we propose a scheme to map word vectors\ntrained on a source language to vectors semantically compatible with word\nvectors trained on a target language using an adversarial autoencoder.\n  We present preliminary qualitative results and discuss possible future\ndevelopments of this technique, such as applications to cross-lingual sentence\nrepresentations.", "published": "2016-08-09 22:24:16", "link": "http://arxiv.org/abs/1608.02996v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
