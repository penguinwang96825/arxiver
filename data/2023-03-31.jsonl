{"title": "Design by Contract Framework for Quantum Software", "abstract": "To realize reliable quantum software, techniques to automatically ensure the\nquantum software's correctness have recently been investigated. However, they\nprimarily focus on fixed quantum circuits rather than the procedure of building\nquantum circuits. Despite being a common approach, the correctness of building\ncircuits using different parameters following the same procedure is not\nguaranteed. To this end, we propose a design-by-contract framework for quantum\nsoftware. Our framework provides a python-embedded language to write assertions\non the input and output states of all quantum circuits built by certain\nprocedures. Additionally, it provides a method to write assertions about the\nstatistical processing of measurement results to ensure the procedure's\ncorrectness for obtaining the final result. These assertions are automatically\nchecked using a quantum computer simulator. For evaluation, we implemented our\nframework and wrote assertions for some widely used quantum algorithms.\nConsequently, we found that our framework has sufficient expressive power to\nverify the whole procedure of quantum software.", "published": "2023-03-31 00:21:28", "link": "http://arxiv.org/abs/2303.17750v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WebQAmGaze: A Multilingual Webcam Eye-Tracking-While-Reading Dataset", "abstract": "We present WebQAmGaze, a multilingual low-cost eye-tracking-while-reading\ndataset, designed as the first webcam-based eye-tracking corpus of reading to\nsupport the development of explainable computational language processing\nmodels. WebQAmGaze includes webcam eye-tracking data from 600 participants of a\nwide age range naturally reading English, German, Spanish, and Turkish texts.\nEach participant performs two reading tasks composed of five texts each, a\nnormal reading and an information-seeking task, followed by a comprehension\nquestion. We compare the collected webcam data to high-quality eye-tracking\nrecordings. The results show a moderate to strong correlation between the eye\nmovement measures obtained with the webcam compared to those obtained with a\ncommercial eye-tracking device. When validating the data, we find that higher\nfixation duration on relevant text spans accurately indicates correctness when\nanswering the corresponding questions. This dataset advances webcam-based\nreading studies and opens avenues to low-cost and diverse data collection.\nWebQAmGaze is beneficial to learn about the cognitive processes behind\nquestion-answering and to apply these insights to computational models of\nlanguage understanding.", "published": "2023-03-31 08:18:30", "link": "http://arxiv.org/abs/2303.17876v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Selective Knowledge Distillation for Non-Autoregressive Neural Machine\n  Translation", "abstract": "Benefiting from the sequence-level knowledge distillation, the\nNon-Autoregressive Transformer (NAT) achieves great success in neural machine\ntranslation tasks. However, existing knowledge distillation has side effects,\nsuch as propagating errors from the teacher to NAT students, which may limit\nfurther improvements of NAT models and are rarely discussed in existing\nresearch. In this paper, we introduce selective knowledge distillation by\nintroducing an NAT evaluator to select NAT-friendly targets that are of high\nquality and easy to learn. In addition, we introduce a simple yet effective\nprogressive distillation method to boost NAT performance. Experiment results on\nmultiple WMT language directions and several representative NAT models show\nthat our approach can realize a flexible trade-off between the quality and\ncomplexity of training data for NAT models, achieving strong performances.\nFurther analysis shows that distilling only 5% of the raw translations can help\nan NAT outperform its counterpart trained on raw data by about 2.4 BLEU.", "published": "2023-03-31 09:16:13", "link": "http://arxiv.org/abs/2303.17910v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Cultural Transfer Learning for Chinese Offensive Language\n  Detection", "abstract": "Detecting offensive language is a challenging task. Generalizing across\ndifferent cultures and languages becomes even more challenging: besides\nlexical, syntactic and semantic differences, pragmatic aspects such as cultural\nnorms and sensitivities, which are particularly relevant in this context, vary\ngreatly. In this paper, we target Chinese offensive language detection and aim\nto investigate the impact of transfer learning using offensive language\ndetection data from different cultural backgrounds, specifically Korean and\nEnglish. We find that culture-specific biases in what is considered offensive\nnegatively impact the transferability of language models (LMs) and that LMs\ntrained on diverse cultural data are sensitive to different features in Chinese\noffensive language detection. In a few-shot learning scenario, however, our\nstudy shows promising prospects for non-English offensive language detection\nwith limited resources. Our findings highlight the importance of cross-cultural\ntransfer learning in improving offensive language detection and promoting\ninclusive digital spaces.", "published": "2023-03-31 09:50:07", "link": "http://arxiv.org/abs/2303.17927v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "JobHam-place with smart recommend job options and candidate filtering\n  options", "abstract": "Due to the increasing number of graduates, many applicants experience the\nsituation about finding a job, and employers experience difficulty filtering\njob applicants, which might negatively impact their effectiveness. However,\nmost job-hunting websites lack job recommendation and CV filtering or ranking\nfunctionality, which are not integrated into the system. Thus, a smart job\nhunter combined with the above functionality will be conducted in this project,\nwhich contains job recommendations, CV ranking and even a job dashboard for\nskills and job applicant functionality. Job recommendation and CV ranking\nstarts from the automatic keyword extraction and end with the Job/CV ranking\nalgorithm. Automatic keyword extraction is implemented by Job2Skill and the\nCV2Skill model based on Bert. Job2Skill consists of two components, text\nencoder and Gru-based layers, while CV2Skill is mainly based on Bert and\nfine-tunes the pre-trained model by the Resume- Entity dataset. Besides, to\nmatch skills from CV and job description and rank lists of jobs and candidates,\njob/CV ranking algorithms have been provided to compute the occurrence ratio of\nskill words based on TFIDF score and match ratio of the total skill numbers.\nBesides, some advanced features have been integrated into the website to\nimprove user experiences, such as the calendar and sweetalert2 plugin. And some\nbasic features to go through job application processes, such as job application\ntracking and interview arrangement.", "published": "2023-03-31 09:54:47", "link": "http://arxiv.org/abs/2303.17930v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Trimming Phonetic Alignments Improves the Inference of Sound\n  Correspondence Patterns from Multilingual Wordlists", "abstract": "Sound correspondence patterns form the basis of cognate detection and\nphonological reconstruction in historical language comparison. Methods for the\nautomatic inference of correspondence patterns from phonetically aligned\ncognate sets have been proposed, but their application to multilingual\nwordlists requires extremely well annotated datasets. Since annotation is\ntedious and time consuming, it would be desirable to find ways to improve\naligned cognate data automatically. Taking inspiration from trimming techniques\nin evolutionary biology, which improve alignments by excluding problematic\nsites, we propose a workflow that trims phonetic alignments in comparative\nlinguistics prior to the inference of correspondence patterns. Testing these\ntechniques on a large standardized collection of ten datasets with expert\nannotations from different language families, we find that the best trimming\ntechnique substantially improves the overall consistency of the alignments. The\nresults show a clear increase in the proportion of frequent correspondence\npatterns and words exhibiting regular cognate relations.", "published": "2023-03-31 09:55:48", "link": "http://arxiv.org/abs/2303.17932v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "$\\varepsilon$ K\u00da <MASK>: Integrating Yor\u00f9b\u00e1 cultural greetings\n  into machine translation", "abstract": "This paper investigates the performance of massively multilingual neural\nmachine translation (NMT) systems in translating Yor\\`ub\\'a greetings\n($\\varepsilon$ k\\'u [MASK]), which are a big part of Yor\\`ub\\'a language and\nculture, into English. To evaluate these models, we present IkiniYor\\`ub\\'a, a\nYor\\`ub\\'a-English translation dataset containing some Yor\\`ub\\'a greetings,\nand sample use cases. We analysed the performance of different multilingual NMT\nsystems including Google and NLLB and show that these models struggle to\naccurately translate Yor\\`ub\\'a greetings into English. In addition, we trained\na Yor\\`ub\\'a-English model by finetuning an existing NMT model on the training\nsplit of IkiniYor\\`ub\\'a and this achieved better performance when compared to\nthe pre-trained multilingual NMT models, although they were trained on a large\nvolume of data.", "published": "2023-03-31 11:16:20", "link": "http://arxiv.org/abs/2303.17972v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Multilingualism in Low-resource Neural Machine Translation\n  via Adversarial Learning", "abstract": "Generative Adversarial Networks (GAN) offer a promising approach for Neural\nMachine Translation (NMT). However, feeding multiple morphologically languages\ninto a single model during training reduces the NMT's performance. In GAN,\nsimilar to bilingual models, multilingual NMT only considers one reference\ntranslation for each sentence during model training. This single reference\ntranslation limits the GAN model from learning sufficient information about the\nsource sentence representation. Thus, in this article, we propose Denoising\nAdversarial Auto-encoder-based Sentence Interpolation (DAASI) approach to\nperform sentence interpolation by learning the intermediate latent\nrepresentation of the source and target sentences of multilingual language\npairs. Apart from latent representation, we also use the Wasserstein-GAN\napproach for the multilingual NMT model by incorporating the model generated\nsentences of multiple languages for reward computation. This computed reward\noptimizes the performance of the GAN-based multilingual model in an effective\nmanner. We demonstrate the experiments on low-resource language pairs and find\nthat our approach outperforms the existing state-of-the-art approaches for\nmultilingual NMT with a performance gain of up to 4 BLEU points. Moreover, we\nuse our trained model on zero-shot language pairs under an unsupervised\nscenario and show the robustness of the proposed approach.", "published": "2023-03-31 12:34:14", "link": "http://arxiv.org/abs/2303.18011v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating GPT-4 and ChatGPT on Japanese Medical Licensing Examinations", "abstract": "As large language models (LLMs) gain popularity among speakers of diverse\nlanguages, we believe that it is crucial to benchmark them to better understand\nmodel behaviors, failures, and limitations in languages beyond English. In this\nwork, we evaluate LLM APIs (ChatGPT, GPT-3, and GPT-4) on the Japanese national\nmedical licensing examinations from the past five years, including the current\nyear. Our team comprises native Japanese-speaking NLP researchers and a\npracticing cardiologist based in Japan. Our experiments show that GPT-4\noutperforms ChatGPT and GPT-3 and passes all six years of the exams,\nhighlighting LLMs' potential in a language that is typologically distant from\nEnglish. However, our evaluation also exposes critical limitations of the\ncurrent LLM APIs. First, LLMs sometimes select prohibited choices that should\nbe strictly avoided in medical practice in Japan, such as suggesting\neuthanasia. Further, our analysis shows that the API costs are generally higher\nand the maximum context size is smaller for Japanese because of the way\nnon-Latin scripts are currently tokenized in the pipeline. We release our\nbenchmark as Igaku QA as well as all model outputs and exam metadata. We hope\nthat our results and benchmark will spur progress on more diverse applications\nof LLMs. Our benchmark is available at https://github.com/jungokasai/IgakuQA.", "published": "2023-03-31 13:04:47", "link": "http://arxiv.org/abs/2303.18027v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "No Place to Hide: Dual Deep Interaction Channel Network for Fake News\n  Detection based on Data Augmentation", "abstract": "Online Social Network (OSN) has become a hotbed of fake news due to the low\ncost of information dissemination. Although the existing methods have made many\nattempts in news content and propagation structure, the detection of fake news\nis still facing two challenges: one is how to mine the unique key features and\nevolution patterns, and the other is how to tackle the problem of small samples\nto build the high-performance model. Different from popular methods which take\nfull advantage of the propagation topology structure, in this paper, we propose\na novel framework for fake news detection from perspectives of semantic,\nemotion and data enhancement, which excavates the emotional evolution patterns\nof news participants during the propagation process, and a dual deep\ninteraction channel network of semantic and emotion is designed to obtain a\nmore comprehensive and fine-grained news representation with the consideration\nof comments. Meanwhile, the framework introduces a data enhancement module to\nobtain more labeled data with high quality based on confidence which further\nimproves the performance of the classification model. Experiments show that the\nproposed approach outperforms the state-of-the-art methods.", "published": "2023-03-31 13:33:53", "link": "http://arxiv.org/abs/2303.18049v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UKP-SQuARE v3: A Platform for Multi-Agent QA Research", "abstract": "The continuous development of Question Answering (QA) datasets has drawn the\nresearch community's attention toward multi-domain models. A popular approach\nis to use multi-dataset models, which are models trained on multiple datasets\nto learn their regularities and prevent overfitting to a single dataset.\nHowever, with the proliferation of QA models in online repositories such as\nGitHub or Hugging Face, an alternative is becoming viable. Recent works have\ndemonstrated that combining expert agents can yield large performance gains\nover multi-dataset models. To ease research in multi-agent models, we extend\nUKP-SQuARE, an online platform for QA research, to support three families of\nmulti-agent systems: i) agent selection, ii) early-fusion of agents, and iii)\nlate-fusion of agents. We conduct experiments to evaluate their inference speed\nand discuss the performance vs. speed trade-off compared to multi-dataset\nmodels. UKP-SQuARE is open-source and publicly available at\nhttp://square.ukp-lab.de.", "published": "2023-03-31 15:07:36", "link": "http://arxiv.org/abs/2303.18120v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can AI Chatbots Pass the Fundamentals of Engineering (FE) and Principles\n  and Practice of Engineering (PE) Structural Exams?", "abstract": "The engineering community has recently witnessed the emergence of chatbot\ntechnology with the release of OpenAI ChatGPT-4 and Google Bard. While these\nchatbots have been reported to perform well and even pass various standardized\ntests, including medical and law exams, this forum paper explores whether these\nchatbots can also pass the Fundamentals of Engineering (FE) and Principles and\nPractice of Engineering (PE) exams. A diverse range of civil and environmental\nengineering questions and scenarios are used to evaluate the chatbots'\nperformance, as commonly present in the FE and PE exams. The chatbots'\nresponses were analyzed based on their relevance, accuracy, and clarity and\nthen compared against the recommendations of the National Council of Examiners\nfor Engineering and Surveying (NCEES). Our report shows that ChatGPT-4 and\nBard, respectively scored 70.9% and 39.2% in the FE exam and 46.2% and 41% in\nthe PE exam. It is evident that the current version of ChatGPT-4 could\npotentially pass the FE exam. While future editions are much more likely to\npass both exams, this study also highlights the potential of using chatbots as\nteaching assistants and guiding engineers.", "published": "2023-03-31 15:37:17", "link": "http://arxiv.org/abs/2303.18149v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Multiple Choices Reading Comprehension Corpus for Vietnamese Language\n  Education", "abstract": "Machine reading comprehension has been an interesting and challenging task in\nrecent years, with the purpose of extracting useful information from texts. To\nattain the computer ability to understand the reading text and answer relevant\ninformation, we introduce ViMMRC 2.0 - an extension of the previous ViMMRC for\nthe task of multiple-choice reading comprehension in Vietnamese Textbooks which\ncontain the reading articles for students from Grade 1 to Grade 12. This\ndataset has 699 reading passages which are prose and poems, and 5,273\nquestions. The questions in the new dataset are not fixed with four options as\nin the previous version. Moreover, the difficulty of questions is increased,\nwhich challenges the models to find the correct choice. The computer must\nunderstand the whole context of the reading passage, the question, and the\ncontent of each choice to extract the right answers. Hence, we propose the\nmulti-stage approach that combines the multi-step attention network (MAN) with\nthe natural language inference (NLI) task to enhance the performance of the\nreading comprehension model. Then, we compare the proposed methodology with the\nbaseline BERTology models on the new dataset and the ViMMRC 1.0. Our\nmulti-stage models achieved 58.81% by Accuracy on the test set, which is 5.34%\nbetter than the highest BERTology models. From the results of the error\nanalysis, we found the challenge of the reading comprehension models is\nunderstanding the implicit context in texts and linking them together in order\nto find the correct answers. Finally, we hope our new dataset will motivate\nfurther research in enhancing the language understanding ability of computers\nin the Vietnamese language.", "published": "2023-03-31 15:54:54", "link": "http://arxiv.org/abs/2303.18162v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing Language Model Deployment with Risk Cards", "abstract": "This paper introduces RiskCards, a framework for structured assessment and\ndocumentation of risks associated with an application of language models. As\nwith all language, text generated by language models can be harmful, or used to\nbring about harm. Automating language generation adds both an element of scale\nand also more subtle or emergent undesirable tendencies to the generated text.\nPrior work establishes a wide variety of language model harms to many different\nactors: existing taxonomies identify categories of harms posed by language\nmodels; benchmarks establish automated tests of these harms; and documentation\nstandards for models, tasks and datasets encourage transparent reporting.\nHowever, there is no risk-centric framework for documenting the complexity of a\nlandscape in which some risks are shared across models and contexts, while\nothers are specific, and where certain conditions may be required for risks to\nmanifest as harms. RiskCards address this methodological gap by providing a\ngeneric framework for assessing the use of a given language model in a given\nscenario. Each RiskCard makes clear the routes for the risk to manifest harm,\ntheir placement in harm taxonomies, and example prompt-output pairs. While\nRiskCards are designed to be open-source, dynamic and participatory, we present\na \"starter set\" of RiskCards taken from a broad literature survey, each of\nwhich details a concrete risk presentation. Language model RiskCards initiate a\ncommunity knowledge base which permits the mapping of risks and harms to a\nspecific model or its application scenario, ultimately contributing to a\nbetter, safer and shared understanding of the risk landscape.", "published": "2023-03-31 16:45:42", "link": "http://arxiv.org/abs/2303.18190v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Demo Alleviate: Demonstrating Artificial Intelligence Enabled Virtual\n  Assistance for Telehealth: The Mental Health Case", "abstract": "After the pandemic, artificial intelligence (AI) powered support for mental\nhealth care has become increasingly important. The breadth and complexity of\nsignificant challenges required to provide adequate care involve: (a)\nPersonalized patient understanding, (b) Safety-constrained and medically\nvalidated chatbot patient interactions, and (c) Support for continued\nfeedback-based refinements in design using chatbot-patient interactions. We\npropose Alleviate, a chatbot designed to assist patients suffering from mental\nhealth challenges with personalized care and assist clinicians with\nunderstanding their patients better. Alleviate draws from an array of publicly\navailable clinically valid mental-health texts and databases, allowing\nAlleviate to make medically sound and informed decisions. In addition,\nAlleviate's modular design and explainable decision-making lends itself to\nrobust and continued feedback-based refinements to its design. In this paper,\nwe explain the different modules of Alleviate and submit a short video\ndemonstrating Alleviate's capabilities to help patients and clinicians\nunderstand each other better to facilitate optimal care strategies.", "published": "2023-03-31 16:41:15", "link": "http://arxiv.org/abs/2304.00025v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying Symptoms of Delirium from Clinical Narratives Using Natural\n  Language Processing", "abstract": "Delirium is an acute decline or fluctuation in attention, awareness, or other\ncognitive function that can lead to serious adverse outcomes. Despite the\nsevere outcomes, delirium is frequently unrecognized and uncoded in patients'\nelectronic health records (EHRs) due to its transient and diverse nature.\nNatural language processing (NLP), a key technology that extracts medical\nconcepts from clinical narratives, has shown great potential in studies of\ndelirium outcomes and symptoms. To assist in the diagnosis and phenotyping of\ndelirium, we formed an expert panel to categorize diverse delirium symptoms,\ncomposed annotation guidelines, created a delirium corpus with diverse delirium\nsymptoms, and developed NLP methods to extract delirium symptoms from clinical\nnotes. We compared 5 state-of-the-art transformer models including 2 models\n(BERT and RoBERTa) from the general domain and 3 models (BERT_MIMIC,\nRoBERTa_MIMIC, and GatorTron) from the clinical domain. GatorTron achieved the\nbest strict and lenient F1 scores of 0.8055 and 0.8759, respectively. We\nconducted an error analysis to identify challenges in annotating delirium\nsymptoms and developing NLP systems. To the best of our knowledge, this is the\nfirst large language model-based delirium symptom extraction system. Our study\nlays the foundation for the future development of computable phenotypes and\ndiagnosis methods for delirium.", "published": "2023-03-31 20:16:44", "link": "http://arxiv.org/abs/2304.00111v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extracting Thyroid Nodules Characteristics from Ultrasound Reports Using\n  Transformer-based Natural Language Processing Methods", "abstract": "The ultrasound characteristics of thyroid nodules guide the evaluation of\nthyroid cancer in patients with thyroid nodules. However, the characteristics\nof thyroid nodules are often documented in clinical narratives such as\nultrasound reports. Previous studies have examined natural language processing\n(NLP) methods in extracting a limited number of characteristics (<9) using\nrule-based NLP systems. In this study, a multidisciplinary team of NLP experts\nand thyroid specialists, identified thyroid nodule characteristics that are\nimportant for clinical care, composed annotation guidelines, developed a\ncorpus, and compared 5 state-of-the-art transformer-based NLP methods,\nincluding BERT, RoBERTa, LongFormer, DeBERTa, and GatorTron, for extraction of\nthyroid nodule characteristics from ultrasound reports. Our GatorTron model, a\ntransformer-based large language model trained using over 90 billion words of\ntext, achieved the best strict and lenient F1-score of 0.8851 and 0.9495 for\nthe extraction of a total number of 16 thyroid nodule characteristics, and\n0.9321 for linking characteristics to nodules, outperforming other clinical\ntransformer models. To the best of our knowledge, this is the first study to\nsystematically categorize and apply transformer-based NLP models to extract a\nlarge number of clinical relevant thyroid nodule characteristics from\nultrasound reports. This study lays ground for assessing the documentation\nquality of thyroid ultrasound reports and examining outcomes of patients with\nthyroid nodules using electronic health records.", "published": "2023-03-31 20:23:58", "link": "http://arxiv.org/abs/2304.00115v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Genlangs\" and Zipf's Law: Do languages generated by ChatGPT\n  statistically look human?", "abstract": "OpenAI's GPT-4 is a Large Language Model (LLM) that can generate coherent\nconstructed languages, or \"conlangs,\" which we propose be called \"genlangs\"\nwhen generated by Artificial Intelligence (AI). The genlangs created by ChatGPT\nfor this research (Voxphera, Vivenzia, and Lumivoxa) each have unique features,\nappear facially coherent, and plausibly \"translate\" into English. This study\ninvestigates whether genlangs created by ChatGPT follow Zipf's law. Zipf's law\napproximately holds across all natural and artificially constructed human\nlanguages. According to Zipf's law, the word frequencies in a text corpus are\ninversely proportional to their rank in the frequency table. This means that\nthe most frequent word appears about twice as often as the second most frequent\nword, three times as often as the third most frequent word, and so on. We\nhypothesize that Zipf's law will hold for genlangs because (1) genlangs created\nby ChatGPT fundamentally operate in the same way as human language with respect\nto the semantic usefulness of certain tokens, and (2) ChatGPT has been trained\non a corpora of text that includes many different languages, all of which\nexhibit Zipf's law to varying degrees. Through statistical linguistics, we aim\nto understand if LLM-based languages statistically look human. Our findings\nindicate that genlangs adhere closely to Zipf's law, supporting the hypothesis\nthat genlangs created by ChatGPT exhibit similar statistical properties to\nnatural and artificial human languages. We also conclude that with human\nassistance, AI is already capable of creating the world's first\nfully-functional genlang, and we call for its development.", "published": "2023-03-31 20:10:59", "link": "http://arxiv.org/abs/2304.12191v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CQSumDP: A ChatGPT-Annotated Resource for Query-Focused Abstractive\n  Summarization Based on Debatepedia", "abstract": "Debatepedia is a publicly available dataset consisting of arguments and\ncounter-arguments on controversial topics that has been widely used for the\nsingle-document query-focused abstractive summarization task in recent years.\nHowever, it has been recently found that this dataset is limited by noise and\neven most queries in this dataset do not have any relevance to the respective\ndocument. In this paper, we present a methodology for cleaning the Debatepedia\ndataset by leveraging the generative power of large language models to make it\nsuitable for query-focused abstractive summarization. More specifically, we\nharness the language generation capabilities of ChatGPT to regenerate its\nqueries. We evaluate the effectiveness of the proposed ChatGPT annotated\nversion of the Debatepedia dataset using several benchmark summarization models\nand demonstrate that the newly annotated version of Debatepedia outperforms the\noriginal dataset in terms of both query relevance as well as summary generation\nquality. We will make this annotated and cleaned version of the dataset\npublicly available.", "published": "2023-03-31 15:39:54", "link": "http://arxiv.org/abs/2305.06147v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attention is Not Always What You Need: Towards Efficient Classification\n  of Domain-Specific Text", "abstract": "For large-scale IT corpora with hundreds of classes organized in a hierarchy,\nthe task of accurate classification of classes at the higher level in the\nhierarchies is crucial to avoid errors propagating to the lower levels. In the\nbusiness world, an efficient and explainable ML model is preferred over an\nexpensive black-box model, especially if the performance increase is marginal.\nA current trend in the Natural Language Processing (NLP) community is towards\nemploying huge pre-trained language models (PLMs) or what is known as\nself-attention models (e.g., BERT) for almost any kind of NLP task (e.g.,\nquestion-answering, sentiment analysis, text classification). Despite the\nwidespread use of PLMs and the impressive performance in a broad range of NLP\ntasks, there is a lack of a clear and well-justified need to as why these\nmodels are being employed for domain-specific text classification (TC) tasks,\ngiven the monosemic nature of specialized words (i.e., jargon) found in\ndomain-specific text which renders the purpose of contextualized embeddings\n(e.g., PLMs) futile. In this paper, we compare the accuracies of some\nstate-of-the-art (SOTA) models reported in the literature against a Linear SVM\nclassifier and TFIDF vectorization model on three TC datasets. Results show a\ncomparable performance for the LinearSVM. The findings of this study show that\nfor domain-specific TC tasks, a linear model can provide a comparable, cheap,\nreproducible, and interpretable alternative to attention-based models.", "published": "2023-03-31 03:17:23", "link": "http://arxiv.org/abs/2303.17786v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GPT-4 can pass the Korean National Licensing Examination for Korean\n  Medicine Doctors", "abstract": "Traditional Korean medicine (TKM) emphasizes individualized diagnosis and\ntreatment. This uniqueness makes AI modeling difficult due to limited data and\nimplicit processes. Large language models (LLMs) have demonstrated impressive\nmedical inference, even without advanced training in medical texts. This study\nassessed the capabilities of GPT-4 in TKM, using the Korean National Licensing\nExamination for Korean Medicine Doctors (K-NLEKMD) as a benchmark. The\nK-NLEKMD, administered by a national organization, encompasses 12 major\nsubjects in TKM. We optimized prompts with Chinese-term annotation, English\ntranslation for questions and instruction, exam-optimized instruction, and\nself-consistency. GPT-4 with optimized prompts achieved 66.18% accuracy,\nsurpassing both the examination's average pass mark of 60% and the 40% minimum\nfor each subject. The gradual introduction of language-related prompts and\nprompting techniques enhanced the accuracy from 51.82% to its maximum accuracy.\nGPT-4 showed low accuracy in subjects including public health &\nmedicine-related law, internal medicine (2) which are localized in Korea and\nTKM. The model's accuracy was lower for questions requiring TKM-specialized\nknowledge. It exhibited higher accuracy in diagnosis-based and recall-based\nquestions than in intervention-based questions. A positive correlation was\nobserved between the consistency and accuracy of GPT-4's responses. This study\nunveils both the potential and challenges of applying LLMs to TKM. These\nfindings underline the potential of LLMs like GPT-4 in culturally adapted\nmedicine, especially TKM, for tasks such as clinical assistance, medical\neducation, and research. But they also point towards the necessity for the\ndevelopment of methods to mitigate cultural bias inherent in large language\nmodels and validate their efficacy in real-world clinical settings.", "published": "2023-03-31 05:43:21", "link": "http://arxiv.org/abs/2303.17807v2", "categories": ["cs.CL", "cs.LG", "J.3"], "primary_category": "cs.CL"}
{"title": "Dataset and Baseline System for Multi-lingual Extraction and\n  Normalization of Temporal and Numerical Expressions", "abstract": "Temporal and numerical expression understanding is of great importance in\nmany downstream Natural Language Processing (NLP) and Information Retrieval\n(IR) tasks. However, much previous work covers only a few sub-types and focuses\nonly on entity extraction, which severely limits the usability of identified\nmentions. In order for such entities to be useful in downstream scenarios,\ncoverage and granularity of sub-types are important; and, even more so,\nproviding resolution into concrete values that can be manipulated. Furthermore,\nmost previous work addresses only a handful of languages. Here we describe a\nmulti-lingual evaluation dataset - NTX - covering diverse temporal and\nnumerical expressions across 14 languages and covering extraction,\nnormalization, and resolution. Along with the dataset we provide a robust\nrule-based system as a strong baseline for comparisons against other models to\nbe evaluated in this dataset. Data and code are available at\n\\url{https://aka.ms/NTX}.", "published": "2023-03-31 14:49:05", "link": "http://arxiv.org/abs/2303.18103v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pair Programming with Large Language Models for Sampling and Estimation\n  of Copulas", "abstract": "Without writing a single line of code by a human, an example Monte Carlo\nsimulation based application for stochastic dependence modeling with copulas is\ndeveloped using a state-of-the-art large language model (LLM) fine-tuned for\nconversations. This includes interaction with ChatGPT in natural language and\nusing mathematical formalism, which, under careful supervision by a\nhuman-expert, led to producing a working code in MATLAB, Python and R for\nsampling from a given copula model, evaluation of the model's density,\nperforming maximum likelihood estimation, optimizing the code for parallel\ncomputing for CPUs as well as for GPUs, and visualization of the computed\nresults. In contrast to other emerging studies that assess the accuracy of LLMs\nlike ChatGPT on tasks from a selected area, this work rather investigates ways\nhow to achieve a successful solution of a standard statistical task in a\ncollaboration of a human-expert and artificial intelligence (AI). Particularly,\nthrough careful prompt engineering, we separate successful solutions generated\nby ChatGPT from unsuccessful ones, resulting in a comprehensive list of related\npros and cons. It is demonstrated that if the typical pitfalls are avoided, we\ncan substantially benefit from collaborating with an AI partner. For example,\nwe show that if ChatGPT is not able to provide a correct solution due to a lack\nof or incorrect knowledge, the human-expert can feed it with the correct\nknowledge, e.g., in the form of mathematical theorems and formulas, and make it\nto apply the gained knowledge in order to provide a solution that is correct.\nSuch ability presents an attractive opportunity to achieve a programmed\nsolution even for users with rather limited knowledge of programming\ntechniques.", "published": "2023-03-31 15:02:48", "link": "http://arxiv.org/abs/2303.18116v1", "categories": ["cs.CL", "stat.CO", "65C60, 68N19, 68T50"], "primary_category": "cs.CL"}
{"title": "BERTino: an Italian DistilBERT model", "abstract": "The recent introduction of Transformers language representation models\nallowed great improvements in many natural language processing (NLP) tasks.\nHowever, if on one hand the performances achieved by this kind of architectures\nare surprising, on the other their usability is limited by the high number of\nparameters which constitute their network, resulting in high computational and\nmemory demands. In this work we present BERTino, a DistilBERT model which\nproposes to be the first lightweight alternative to the BERT architecture\nspecific for the Italian language. We evaluated BERTino on the Italian ISDT,\nItalian ParTUT, Italian WikiNER and multiclass classification tasks, obtaining\nF1 scores comparable to those obtained by a BERTBASE with a remarkable\nimprovement in training and inference speed.", "published": "2023-03-31 15:07:40", "link": "http://arxiv.org/abs/2303.18121v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A Survey of Large Language Models", "abstract": "Language is essentially a complex, intricate system of human expressions\ngoverned by grammatical rules. It poses a significant challenge to develop\ncapable AI algorithms for comprehending and grasping a language. As a major\napproach, language modeling has been widely studied for language understanding\nand generation in the past two decades, evolving from statistical language\nmodels to neural language models. Recently, pre-trained language models (PLMs)\nhave been proposed by pre-training Transformer models over large-scale corpora,\nshowing strong capabilities in solving various NLP tasks. Since researchers\nhave found that model scaling can lead to performance improvement, they further\nstudy the scaling effect by increasing the model size to an even larger size.\nInterestingly, when the parameter scale exceeds a certain level, these enlarged\nlanguage models not only achieve a significant performance improvement but also\nshow some special abilities that are not present in small-scale language\nmodels. To discriminate the difference in parameter scale, the research\ncommunity has coined the term large language models (LLM) for the PLMs of\nsignificant size. Recently, the research on LLMs has been largely advanced by\nboth academia and industry, and a remarkable progress is the launch of ChatGPT,\nwhich has attracted widespread attention from society. The technical evolution\nof LLMs has been making an important impact on the entire AI community, which\nwould revolutionize the way how we develop and use AI algorithms. In this\nsurvey, we review the recent advances of LLMs by introducing the background,\nkey findings, and mainstream techniques. In particular, we focus on four major\naspects of LLMs, namely pre-training, adaptation tuning, utilization, and\ncapacity evaluation. Besides, we also summarize the available resources for\ndeveloping LLMs and discuss the remaining issues for future directions.", "published": "2023-03-31 17:28:46", "link": "http://arxiv.org/abs/2303.18223v16", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Large Language Models with Climate Resources", "abstract": "Large language models (LLMs) have significantly transformed the landscape of\nartificial intelligence by demonstrating their ability in generating human-like\ntext across diverse topics. However, despite their impressive capabilities,\nLLMs lack recent information and often employ imprecise language, which can be\ndetrimental in domains where accuracy is crucial, such as climate change. In\nthis study, we make use of recent ideas to harness the potential of LLMs by\nviewing them as agents that access multiple sources, including databases\ncontaining recent and precise information about organizations, institutions,\nand companies. We demonstrate the effectiveness of our method through a\nprototype agent that retrieves emission data from ClimateWatch\n(https://www.climatewatchdata.org/) and leverages general Google search. By\nintegrating these resources with LLMs, our approach overcomes the limitations\nassociated with imprecise language and delivers more reliable and accurate\ninformation in the critical domain of climate change. This work paves the way\nfor future advancements in LLMs and their application in domains where\nprecision is of paramount importance.", "published": "2023-03-31 20:24:14", "link": "http://arxiv.org/abs/2304.00116v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Decoding the End-to-end Writing Trajectory in Scholarly Manuscripts", "abstract": "Scholarly writing presents a complex space that generally follows a\nmethodical procedure to plan and produce both rationally sound and creative\ncompositions. Recent works involving large language models (LLM) demonstrate\nconsiderable success in text generation and revision tasks; however, LLMs still\nstruggle to provide structural and creative feedback on the document level that\nis crucial to academic writing. In this paper, we introduce a novel taxonomy\nthat categorizes scholarly writing behaviors according to intention, writer\nactions, and the information types of the written data. We also provide\nManuScript, an original dataset annotated with a simplified version of our\ntaxonomy to show writer actions and the intentions behind them. Motivated by\ncognitive writing theory, our taxonomy for scientific papers includes three\nlevels of categorization in order to trace the general writing flow and\nidentify the distinct writer activities embedded within each higher-level\nprocess. ManuScript intends to provide a complete picture of the scholarly\nwriting process by capturing the linearity and non-linearity of writing\ntrajectory, such that writing assistants can provide stronger feedback and\nsuggestions on an end-to-end level. The collected writing trajectories are\nviewed at https://minnesotanlp.github.io/REWARD_demo/", "published": "2023-03-31 20:33:03", "link": "http://arxiv.org/abs/2304.00121v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "FCC: Fusing Conversation History and Candidate Provenance for Contextual\n  Response Ranking in Dialogue Systems", "abstract": "Response ranking in dialogues plays a crucial role in retrieval-based\nconversational systems. In a multi-turn dialogue, to capture the gist of a\nconversation, contextual information serves as essential knowledge to achieve\nthis goal. In this paper, we present a flexible neural framework that can\nintegrate contextual information from multiple channels. Specifically for the\ncurrent task, our approach is to provide two information channels in parallel,\nFusing Conversation history and domain knowledge extracted from Candidate\nprovenance (FCC), where candidate responses are curated, as contextual\ninformation to improve the performance of multi-turn dialogue response ranking.\nThe proposed approach can be generalized as a module to incorporate\nmiscellaneous contextual features for other context-oriented tasks. We evaluate\nour model on the MSDialog dataset widely used for evaluating conversational\nresponse ranking tasks. Our experimental results show that our framework\nsignificantly outperforms the previous state-of-the-art models, improving\nRecall@1 by 7% and MAP by 4%. Furthermore, we conduct ablation studies to\nevaluate the contributions of each information channel, and of the framework\ncomponents, to the overall ranking performance, providing additional insights\nand directions for further improvements.", "published": "2023-03-31 23:58:28", "link": "http://arxiv.org/abs/2304.00180v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Mitigating ChatGPT's Negative Impact on Education: Optimizing\n  Question Design through Bloom's Taxonomy", "abstract": "The popularity of generative text AI tools in answering questions has led to\nconcerns regarding their potential negative impact on students' academic\nperformance and the challenges that educators face in evaluating student\nlearning. To address these concerns, this paper introduces an evolutionary\napproach that aims to identify the best set of Bloom's taxonomy keywords to\ngenerate questions that these tools have low confidence in answering. The\neffectiveness of this approach is evaluated through a case study that uses\nquestions from a Data Structures and Representation course being taught at the\nUniversity of New South Wales in Canberra, Australia. The results demonstrate\nthat the optimization algorithm is able to find keywords from different\ncognitive levels to create questions that ChatGPT has low confidence in\nanswering. This study is a step forward to offer valuable insights for\neducators seeking to create more effective questions that promote critical\nthinking among students.", "published": "2023-03-31 00:01:59", "link": "http://arxiv.org/abs/2304.08176v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Improving Scene Text Recognition for Character-Level Long-Tailed\n  Distribution", "abstract": "Despite the recent remarkable improvements in scene text recognition (STR),\nthe majority of the studies focused mainly on the English language, which only\nincludes few number of characters. However, STR models show a large performance\ndegradation on languages with a numerous number of characters (e.g., Chinese\nand Korean), especially on characters that rarely appear due to the long-tailed\ndistribution of characters in such languages. To address such an issue, we\nconducted an empirical analysis using synthetic datasets with different\ncharacter-level distributions (e.g., balanced and long-tailed distributions).\nWhile increasing a substantial number of tail classes without considering the\ncontext helps the model to correctly recognize characters individually,\ntraining with such a synthetic dataset interferes the model with learning the\ncontextual information (i.e., relation among characters), which is also\nimportant for predicting the whole word. Based on this motivation, we propose a\nnovel Context-Aware and Free Experts Network (CAFE-Net) using two experts: 1)\ncontext-aware expert learns the contextual representation trained with a\nlong-tailed dataset composed of common words used in everyday life and 2)\ncontext-free expert focuses on correctly predicting individual characters by\nutilizing a dataset with a balanced number of characters. By training two\nexperts to focus on learning contextual and visual representations,\nrespectively, we propose a novel confidence ensemble method to compensate the\nlimitation of each expert. Through the experiments, we demonstrate that\nCAFE-Net improves the STR performance on languages containing numerous number\nof characters. Moreover, we show that CAFE-Net is easily applicable to various\nSTR models.", "published": "2023-03-31 06:11:33", "link": "http://arxiv.org/abs/2304.08592v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Dialog act guided contextual adapter for personalized speech recognition", "abstract": "Personalization in multi-turn dialogs has been a long standing challenge for\nend-to-end automatic speech recognition (E2E ASR) models. Recent work on\ncontextual adapters has tackled rare word recognition using user catalogs. This\nadaptation, however, does not incorporate an important cue, the dialog act,\nwhich is available in a multi-turn dialog scenario. In this work, we propose a\ndialog act guided contextual adapter network. Specifically, it leverages dialog\nacts to select the most relevant user catalogs and creates queries based on\nboth -- the audio as well as the semantic relationship between the carrier\nphrase and user catalogs to better guide the contextual biasing. On industrial\nvoice assistant datasets, our model outperforms both the baselines - dialog act\nencoder-only model, and the contextual adaptation, leading to the most\nimprovement over the no-context model: 58% average relative word error rate\nreduction (WERR) in the multi-turn dialog scenario, in comparison to the\nprior-art contextual adapter, which has achieved 39% WERR over the no-context\nmodel.", "published": "2023-03-31 05:13:44", "link": "http://arxiv.org/abs/2303.17799v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Zero-shot Referring Image Segmentation with Global-Local Context\n  Features", "abstract": "Referring image segmentation (RIS) aims to find a segmentation mask given a\nreferring expression grounded to a region of the input image. Collecting\nlabelled datasets for this task, however, is notoriously costly and\nlabor-intensive. To overcome this issue, we propose a simple yet effective\nzero-shot referring image segmentation method by leveraging the pre-trained\ncross-modal knowledge from CLIP. In order to obtain segmentation masks grounded\nto the input text, we propose a mask-guided visual encoder that captures global\nand local contextual information of an input image. By utilizing instance masks\nobtained from off-the-shelf mask proposal techniques, our method is able to\nsegment fine-detailed Istance-level groundings. We also introduce a\nglobal-local text encoder where the global feature captures complex\nsentence-level semantics of the entire input expression while the local feature\nfocuses on the target noun phrase extracted by a dependency parser. In our\nexperiments, the proposed method outperforms several zero-shot baselines of the\ntask and even the weakly supervised referring expression segmentation method\nwith substantial margins. Our code is available at\nhttps://github.com/Seonghoon-Yu/Zero-shot-RIS.", "published": "2023-03-31 06:00:50", "link": "http://arxiv.org/abs/2303.17811v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Can AI Put Gamma-Ray Astrophysicists Out of a Job?", "abstract": "In what will likely be a litany of generative-model-themed arXiv submissions\ncelebrating April the 1st, we evaluate the capacity of state-of-the-art\ntransformer models to create a paper detailing the detection of a Pulsar Wind\nNebula with a non-existent Imaging Atmospheric Cherenkov Telescope (IACT)\nArray. We do this to evaluate the ability of such models to interpret\nastronomical observations and sources based on language information alone, and\nto assess potential means by which fraudulently generated scientific papers\ncould be identified during peer review (given that reliable generative model\nwatermarking has yet to be deployed for these tools). We conclude that our jobs\nas astronomers are safe for the time being. From this point on, prompts given\nto ChatGPT and Stable Diffusion are shown in orange, text generated by ChatGPT\nis shown in black, whereas analysis by the (human) authors is in blue.", "published": "2023-03-31 07:29:47", "link": "http://arxiv.org/abs/2303.17853v2", "categories": ["physics.pop-ph", "astro-ph.HE", "cs.CL"], "primary_category": "physics.pop-ph"}
{"title": "The Edinburgh International Accents of English Corpus: Towards the\n  Democratization of English ASR", "abstract": "English is the most widely spoken language in the world, used daily by\nmillions of people as a first or second language in many different contexts. As\na result, there are many varieties of English. Although the great many advances\nin English automatic speech recognition (ASR) over the past decades, results\nare usually reported based on test datasets which fail to represent the\ndiversity of English as spoken today around the globe. We present the first\nrelease of The Edinburgh International Accents of English Corpus (EdAcc). This\ndataset attempts to better represent the wide diversity of English,\nencompassing almost 40 hours of dyadic video call conversations between\nfriends. Unlike other datasets, EdAcc includes a wide range of first and\nsecond-language varieties of English and a linguistic background profile of\neach speaker. Results on latest public, and commercial models show that EdAcc\nhighlights shortcomings of current English ASR models. The best performing\nmodel, trained on 680 thousand hours of transcribed data, obtains an average of\n19.7% word error rate (WER) -- in contrast to the 2.7% WER obtained when\nevaluated on US English clean read speech. Across all models, we observe a drop\nin performance on Indian, Jamaican, and Nigerian English speakers. Recordings,\nlinguistic backgrounds, data statement, and evaluation scripts are released on\nour website (https://groups.inf.ed.ac.uk/edacc/) under CC-BY-SA license.", "published": "2023-03-31 14:56:54", "link": "http://arxiv.org/abs/2303.18110v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "SemiMemes: A Semi-supervised Learning Approach for Multimodal Memes\n  Analysis", "abstract": "The prevalence of memes on social media has created the need to sentiment\nanalyze their underlying meanings for censoring harmful content. Meme censoring\nsystems by machine learning raise the need for a semi-supervised learning\nsolution to take advantage of the large number of unlabeled memes available on\nthe internet and make the annotation process less challenging. Moreover, the\napproach needs to utilize multimodal data as memes' meanings usually come from\nboth images and texts. This research proposes a multimodal semi-supervised\nlearning approach that outperforms other multimodal semi-supervised learning\nand supervised learning state-of-the-art models on two datasets, the Multimedia\nAutomatic Misogyny Identification and Hateful Memes dataset. Building on the\ninsights gained from Contrastive Language-Image Pre-training, which is an\neffective multimodal learning technique, this research introduces SemiMemes, a\nnovel training method that combines auto-encoder and classification task to\nmake use of the resourceful unlabeled data.", "published": "2023-03-31 11:22:03", "link": "http://arxiv.org/abs/2304.00020v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Dense Sparse Retrieval: Using Sparse Language Models for Inference\n  Efficient Dense Retrieval", "abstract": "Vector-based retrieval systems have become a common staple for academic and\nindustrial search applications because they provide a simple and scalable way\nof extending the search to leverage contextual representations for documents\nand queries. As these vector-based systems rely on contextual language models,\ntheir usage commonly requires GPUs, which can be expensive and difficult to\nmanage. Given recent advances in introducing sparsity into language models for\nimproved inference efficiency, in this paper, we study how sparse language\nmodels can be used for dense retrieval to improve inference efficiency. Using\nthe popular retrieval library Tevatron and the MSMARCO, NQ, and TriviaQA\ndatasets, we find that sparse language models can be used as direct\nreplacements with little to no drop in accuracy and up to 4.3x improved\ninference speeds", "published": "2023-03-31 20:21:32", "link": "http://arxiv.org/abs/2304.00114v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Practical Conformer: Optimizing size, speed and flops of Conformer for\n  on-Device and cloud ASR", "abstract": "Conformer models maintain a large number of internal states, the vast\nmajority of which are associated with self-attention layers. With limited\nmemory bandwidth, reading these from memory at each inference step can slow\ndown inference. In this paper, we design an optimized conformer that is small\nenough to meet on-device restrictions and has fast inference on TPUs. We\nexplore various ideas to improve the execution speed, including replacing lower\nconformer blocks with convolution-only blocks, strategically downsizing the\narchitecture, and utilizing an RNNAttention-Performer. Our optimized conformer\ncan be readily incorporated into a cascaded-encoder setting, allowing a\nsecond-pass decoder to operate on its output and improve the accuracy whenever\nmore resources are available. Altogether, we find that these optimizations can\nreduce latency by a factor of 6.8x, and come at a reasonable trade-off in\nquality. With the cascaded second-pass, we show that the recognition accuracy\nis completely recoverable. Thus, our proposed encoder can double as a strong\nstandalone encoder in on device, and as the first part of a high-performance\nASR pipeline.", "published": "2023-03-31 23:30:48", "link": "http://arxiv.org/abs/2304.00171v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Lego-Features: Exporting modular encoder features for streaming and\n  deliberation ASR", "abstract": "In end-to-end (E2E) speech recognition models, a representational\ntight-coupling inevitably emerges between the encoder and the decoder. We build\nupon recent work that has begun to explore building encoders with modular\nencoded representations, such that encoders and decoders from different models\ncan be stitched together in a zero-shot manner without further fine-tuning.\nWhile previous research only addresses full-context speech models, we explore\nthe problem in a streaming setting as well. Our framework builds on top of\nexisting encoded representations, converting them to modular features, dubbed\nas Lego-Features, without modifying the pre-trained model. The features remain\ninterchangeable when the model is retrained with distinct initializations.\nThough sparse, we show that the Lego-Features are powerful when tested with\nRNN-T or LAS decoders, maintaining high-quality downstream performance. They\nare also rich enough to represent the first-pass prediction during two-pass\ndeliberation. In this scenario, they outperform the N-best hypotheses, since\nthey do not need to be supplemented with acoustic features to deliver the best\nresults. Moreover, generating the Lego-Features does not require beam search or\nauto-regressive computation. Overall, they present a modular, powerful and\ncheap alternative to the standard encoder output, as well as the N-best\nhypotheses.", "published": "2023-03-31 23:33:21", "link": "http://arxiv.org/abs/2304.00173v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Self-Supervised Multimodal Learning: A Survey", "abstract": "Multimodal learning, which aims to understand and analyze information from\nmultiple modalities, has achieved substantial progress in the supervised regime\nin recent years. However, the heavy dependence on data paired with expensive\nhuman annotations impedes scaling up models. Meanwhile, given the availability\nof large-scale unannotated data in the wild, self-supervised learning has\nbecome an attractive strategy to alleviate the annotation bottleneck. Building\non these two directions, self-supervised multimodal learning (SSML) provides\nways to learn from raw multimodal data. In this survey, we provide a\ncomprehensive review of the state-of-the-art in SSML, in which we elucidate\nthree major challenges intrinsic to self-supervised learning with multimodal\ndata: (1) learning representations from multimodal data without labels, (2)\nfusion of different modalities, and (3) learning with unaligned data. We then\ndetail existing solutions to these challenges. Specifically, we consider (1)\nobjectives for learning from multimodal unlabeled data via self-supervision,\n(2) model architectures from the perspective of different multimodal fusion\nstrategies, and (3) pair-free learning strategies for coarse-grained and\nfine-grained alignment. We also review real-world applications of SSML\nalgorithms in diverse fields such as healthcare, remote sensing, and machine\ntranslation. Finally, we discuss challenges and future directions for SSML. A\ncollection of related resources can be found at:\nhttps://github.com/ys-zong/awesome-self-supervised-multimodal-learning.", "published": "2023-03-31 16:11:56", "link": "http://arxiv.org/abs/2304.01008v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Quick Dense Retrievers Consume KALE: Post Training Kullback Leibler\n  Alignment of Embeddings for Asymmetrical dual encoders", "abstract": "In this paper, we consider the problem of improving the inference latency of\nlanguage model-based dense retrieval systems by introducing structural\ncompression and model size asymmetry between the context and query encoders.\nFirst, we investigate the impact of pre and post-training compression on the\nMSMARCO, Natural Questions, TriviaQA, SQUAD, and SCIFACT, finding that\nasymmetry in the dual encoders in dense retrieval can lead to improved\ninference efficiency. Knowing this, we introduce Kullback Leibler Alignment of\nEmbeddings (KALE), an efficient and accurate method for increasing the\ninference efficiency of dense retrieval methods by pruning and aligning the\nquery encoder after training. Specifically, KALE extends traditional Knowledge\nDistillation after bi-encoder training, allowing for effective query encoder\ncompression without full retraining or index generation. Using KALE and\nasymmetric training, we can generate models which exceed the performance of\nDistilBERT despite having 3x faster inference.", "published": "2023-03-31 15:44:13", "link": "http://arxiv.org/abs/2304.01016v3", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "CAMEL: Communicative Agents for \"Mind\" Exploration of Large Language\n  Model Society", "abstract": "The rapid advancement of chat-based language models has led to remarkable\nprogress in complex task-solving. However, their success heavily relies on\nhuman input to guide the conversation, which can be challenging and\ntime-consuming. This paper explores the potential of building scalable\ntechniques to facilitate autonomous cooperation among communicative agents, and\nprovides insight into their \"cognitive\" processes. To address the challenges of\nachieving autonomous cooperation, we propose a novel communicative agent\nframework named role-playing. Our approach involves using inception prompting\nto guide chat agents toward task completion while maintaining consistency with\nhuman intentions. We showcase how role-playing can be used to generate\nconversational data for studying the behaviors and capabilities of a society of\nagents, providing a valuable resource for investigating conversational language\nmodels. In particular, we conduct comprehensive studies on\ninstruction-following cooperation in multi-agent settings. Our contributions\ninclude introducing a novel communicative agent framework, offering a scalable\napproach for studying the cooperative behaviors and capabilities of multi-agent\nsystems, and open-sourcing our library to support research on communicative\nagents and beyond: https://github.com/camel-ai/camel.", "published": "2023-03-31 01:09:00", "link": "http://arxiv.org/abs/2303.17760v2", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Learning Procedure-aware Video Representation from Instructional Videos\n  and Their Narrations", "abstract": "The abundance of instructional videos and their narrations over the Internet\noffers an exciting avenue for understanding procedural activities. In this\nwork, we propose to learn video representation that encodes both action steps\nand their temporal ordering, based on a large-scale dataset of web\ninstructional videos and their narrations, without using human annotations. Our\nmethod jointly learns a video representation to encode individual step\nconcepts, and a deep probabilistic model to capture both temporal dependencies\nand immense individual variations in the step ordering. We empirically\ndemonstrate that learning temporal ordering not only enables new capabilities\nfor procedure reasoning, but also reinforces the recognition of individual\nsteps. Our model significantly advances the state-of-the-art results on step\nclassification (+2.8% / +3.3% on COIN / EPIC-Kitchens) and step forecasting\n(+7.4% on COIN). Moreover, our model attains promising results in zero-shot\ninference for step classification and forecasting, as well as in predicting\ndiverse and plausible steps for incomplete procedures. Our code is available at\nhttps://github.com/facebookresearch/ProcedureVRL.", "published": "2023-03-31 07:02:26", "link": "http://arxiv.org/abs/2303.17839v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Multiple Hankel matrix rank minimization for audio inpainting", "abstract": "Sasaki et al. (2018) presented an efficient audio declipping algorithm, based\non the properties of Hankel-structure matrices constructed from time-domain\nsignal blocks. We adapt their approach to solving the audio inpainting problem,\nwhere samples are missing in the signal. We analyze the algorithm and provide\nmodifications, some of them leading to an improved performance. Overall, it\nturns out that the new algorithms perform reasonably well for speech signals\nbut they are not competitive in the case of music signals.", "published": "2023-03-31 13:02:02", "link": "http://arxiv.org/abs/2303.18023v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Evaluation of Noise Reduction Methods for Sentence Recognition by\n  Sinhala Speaking Listeners", "abstract": "Noise reduction is a crucial aspect of hearing aids, which researchers have\nbeen striving to address over the years. However, most existing noise reduction\nalgorithms have primarily been evaluated using English. Considering the\nlinguistic differences between English and Sinhala languages, including\nvariation in syllable structures and vowel duration, it is very important to\nassess the performance of noise reduction tailored to the Sinhala language.\nThis paper presents a comprehensive analysis between wavelet transformation and\nadaptive filters for noise reduction in Sinhala languages. We investigate the\nperformance of ten wavelet families with soft and hard thresholding methods\nagainst adaptive filters with Normalized Least Mean Square, Least Mean Square\nAverage Normalized Least Mean Square, Recursive Least Square, and Adaptive\nFiltering Averaging optimization algorithms along with cepstral and\nenergy-based voice activity detection algorithms. The performance evaluation is\ndone using objective metrics; Signal to Noise Ratio (SNR) and Perceptual\nEvaluation of Speech Quality (PESQ) and a subjective metric; Mean Opinion Score\n(MOS). A newly recorded Sinhala language audio dataset and the NOIZEUS database\nby the University of Texas, Dallas were used for the evaluation. Our code is\navailable at\nhttps://github.com/ChathukiKet/Evaluation-of-Noise-Reduction-Methods", "published": "2023-03-31 06:52:46", "link": "http://arxiv.org/abs/2303.17829v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised Anomaly Detection and Localization of Machine Audio: A\n  GAN-based Approach", "abstract": "Automatic detection of machine anomaly remains challenging for machine\nlearning. We believe the capability of generative adversarial network (GAN)\nsuits the need of machine audio anomaly detection, yet rarely has this been\ninvestigated by previous work. In this paper, we propose AEGAN-AD, a totally\nunsupervised approach in which the generator (also an autoencoder) is trained\nto reconstruct input spectrograms. It is pointed out that the denoising nature\nof reconstruction deprecates its capacity. Thus, the discriminator is\nredesigned to aid the generator during both training stage and detection stage.\nThe performance of AEGAN-AD on the dataset of DCASE 2022 Challenge TASK 2\ndemonstrates the state-of-the-art result on five machine types. A novel anomaly\nlocalization method is also investigated. Source code available at:\nwww.github.com/jianganbai/AEGAN-AD", "published": "2023-03-31 10:27:36", "link": "http://arxiv.org/abs/2303.17949v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
