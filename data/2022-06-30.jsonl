{"title": "Democratizing Ethical Assessment of Natural Language Generation Models", "abstract": "Natural language generation models are computer systems that generate\ncoherent language when prompted with a sequence of words as context. Despite\ntheir ubiquity and many beneficial applications, language generation models\nalso have the potential to inflict social harms by generating discriminatory\nlanguage, hateful speech, profane content, and other harmful material. Ethical\nassessment of these models is therefore critical. But it is also a challenging\ntask, requiring an expertise in several specialized domains, such as\ncomputational linguistics and social justice. While significant strides have\nbeen made by the research community in this domain, accessibility of such\nethical assessments to the wider population is limited due to the high entry\nbarriers. This article introduces a new tool to democratize and standardize\nethical assessment of natural language generation models: Tool for Ethical\nAssessment of Language generation models (TEAL), a component of Credo AI Lens,\nan open-source assessment framework.", "published": "2022-06-30 12:20:31", "link": "http://arxiv.org/abs/2207.10576v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Diversity and Uncertainty in Moderation\" are the Key to Data Selection\n  for Multilingual Few-shot Transfer", "abstract": "Few-shot transfer often shows substantial gain over zero-shot\ntransfer~\\cite{lauscher2020zero}, which is a practically useful trade-off\nbetween fully supervised and unsupervised learning approaches for multilingual\npretrained model-based systems. This paper explores various strategies for\nselecting data for annotation that can result in a better few-shot transfer.\nThe proposed approaches rely on multiple measures such as data entropy using\n$n$-gram language model, predictive entropy, and gradient embedding. We propose\na loss embedding method for sequence labeling tasks, which induces diversity\nand uncertainty sampling similar to gradient embedding. The proposed data\nselection strategies are evaluated and compared for POS tagging, NER, and NLI\ntasks for up to 20 languages. Our experiments show that the gradient and loss\nembedding-based strategies consistently outperform random data selection\nbaselines, with gains varying with the initial performance of the zero-shot\ntransfer. Furthermore, the proposed method shows similar trends in improvement\neven when the model is fine-tuned using a lower proportion of the original\ntask-specific labeled training data for zero-shot transfer.", "published": "2022-06-30 04:22:27", "link": "http://arxiv.org/abs/2206.15010v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modern Question Answering Datasets and Benchmarks: A Survey", "abstract": "Question Answering (QA) is one of the most important natural language\nprocessing (NLP) tasks. It aims using NLP technologies to generate a\ncorresponding answer to a given question based on the massive unstructured\ncorpus. With the development of deep learning, more and more challenging QA\ndatasets are being proposed, and lots of new methods for solving them are also\nemerging. In this paper, we investigate influential QA datasets that have been\nreleased in the era of deep learning. Specifically, we begin with introducing\ntwo of the most common QA tasks - textual question answer and visual question\nanswering - separately, covering the most representative datasets, and then\ngive some current challenges of QA research.", "published": "2022-06-30 05:53:56", "link": "http://arxiv.org/abs/2206.15030v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Story-thinking, computational-thinking, programming and software\n  engineering", "abstract": "Working with stories and working with computations require very different\nmodes of thought. We call the first mode \"story-thinking\" and the second\n\"computational-thinking\". The aim of this curiosity-driven paper is to explore\nthe nature of these two modes of thinking, and to do so in relation to\nprogramming, including software engineering as programming-in-the-large. We\nsuggest that story-thinking and computational-thinking may be understood as two\nways of attending to the world, and that each both contributes and neglects the\nworld, though in different ways and for different ends. We formulate two\nfundamental problems, i.e., the problem of \"neglectful representations\" and the\nproblem of oppositional ways of thinking. We briefly suggest two ways in which\nthese problems might be tackled and identify candidate hypotheses about the\ncurrent state of the world, one assertion about a possible future state, and\nseveral research questions for future research.", "published": "2022-06-30 07:00:11", "link": "http://arxiv.org/abs/2206.15066v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BigBIO: A Framework for Data-Centric Biomedical Natural Language\n  Processing", "abstract": "Training and evaluating language models increasingly requires the\nconstruction of meta-datasets --diverse collections of curated data with clear\nprovenance. Natural language prompting has recently lead to improved zero-shot\ngeneralization by transforming existing, supervised datasets into a diversity\nof novel pretraining tasks, highlighting the benefits of meta-dataset curation.\nWhile successful in general-domain text, translating these data-centric\napproaches to biomedical language modeling remains challenging, as labeled\nbiomedical datasets are significantly underrepresented in popular data hubs. To\naddress this challenge, we introduce BigBIO a community library of 126+\nbiomedical NLP datasets, currently covering 12 task categories and 10+\nlanguages. BigBIO facilitates reproducible meta-dataset curation via\nprogrammatic access to datasets and their metadata, and is compatible with\ncurrent platforms for prompt engineering and end-to-end few/zero shot language\nmodel evaluation. We discuss our process for task schema harmonization, data\nauditing, contribution guidelines, and outline two illustrative use cases:\nzero-shot evaluation of biomedical prompts and large-scale, multi-task\nlearning. BigBIO is an ongoing community effort and is available at\nhttps://github.com/bigscience-workshop/biomedical", "published": "2022-06-30 07:15:45", "link": "http://arxiv.org/abs/2206.15076v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain Adaptive Pretraining for Multilingual Acronym Extraction", "abstract": "This paper presents our findings from participating in the multilingual\nacronym extraction shared task SDU@AAAI-22. The task consists of acronym\nextraction from documents in 6 languages within scientific and legal domains.\nTo address multilingual acronym extraction we employed BiLSTM-CRF with\nmultilingual XLM-RoBERTa embeddings. We pretrained the XLM-RoBERTa model on the\nshared task corpus to further adapt XLM-RoBERTa embeddings to the shared task\ndomain(s). Our system (team: SMR-NLP) achieved competitive performance for\nacronym extraction across all the languages.", "published": "2022-06-30 12:11:39", "link": "http://arxiv.org/abs/2206.15221v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How direct is the link between words and images?", "abstract": "Current word embedding models despite their success, still suffer from their\nlack of grounding in the real world. In this line of research, Gunther et al.\n2022 proposed a behavioral experiment to investigate the relationship between\nwords and images. In their setup, participants were presented with a target\nnoun and a pair of images, one chosen by their model and another chosen\nrandomly. Participants were asked to select the image that best matched the\ntarget noun. In most cases, participants preferred the image selected by the\nmodel. Gunther et al., therefore, concluded the possibility of a direct link\nbetween words and embodied experience. We took their experiment as a point of\ndeparture and addressed the following questions. 1. Apart from utilizing\nvisually embodied simulation of given images, what other strategies might\nsubjects have used to solve this task? To what extent does this setup rely on\nvisual information from images? Can it be solved using purely textual\nrepresentations? 2. Do current visually grounded embeddings explain subjects'\nselection behavior better than textual embeddings? 3. Does visual grounding\nimprove the semantic representations of both concrete and abstract words? To\naddress these questions, we designed novel experiments by using pre-trained\ntextual and visually grounded word embeddings. Our experiments reveal that\nsubjects' selection behavior is explained to a large extent based on purely\ntext-based embeddings and word-based similarities, suggesting a minor\ninvolvement of active embodied experiences. Visually grounded embeddings\noffered modest advantages over textual embeddings only in certain cases. These\nfindings indicate that the experiment by Gunther et al. may not be well suited\nfor tapping into the perceptual experience of participants, and therefore the\nextent to which it measures visually grounded knowledge is unclear.", "published": "2022-06-30 15:58:17", "link": "http://arxiv.org/abs/2206.15381v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hate Speech Criteria: A Modular Approach to Task-Specific Hate Speech\n  Definitions", "abstract": "\\textbf{Offensive Content Warning}: This paper contains offensive language\nonly for providing examples that clarify this research and do not reflect the\nauthors' opinions. Please be aware that these examples are offensive and may\ncause you distress.\n  The subjectivity of recognizing \\textit{hate speech} makes it a complex task.\nThis is also reflected by different and incomplete definitions in NLP. We\npresent \\textit{hate speech} criteria, developed with perspectives from law and\nsocial science, with the aim of helping researchers create more precise\ndefinitions and annotation guidelines on five aspects: (1) target groups, (2)\ndominance, (3) perpetrator characteristics, (4) type of negative group\nreference, and the (5) type of potential consequences/effects. Definitions can\nbe structured so that they cover a more broad or more narrow phenomenon. As\nsuch, conscious choices can be made on specifying criteria or leaving them\nopen. We argue that the goal and exact task developers have in mind should\ndetermine how the scope of \\textit{hate speech} is defined. We provide an\noverview of the properties of English datasets from \\url{hatespeechdata.com}\nthat may help select the most suitable dataset for a specific scenario.", "published": "2022-06-30 17:50:16", "link": "http://arxiv.org/abs/2206.15455v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Human-Agent Communication via the Information Bottleneck\n  Principle", "abstract": "Emergent communication research often focuses on optimizing task-specific\nutility as a driver for communication. However, human languages appear to\nevolve under pressure to efficiently compress meanings into communication\nsignals by optimizing the Information Bottleneck tradeoff between\ninformativeness and complexity. In this work, we study how trading off these\nthree factors -- utility, informativeness, and complexity -- shapes emergent\ncommunication, including compared to human communication. To this end, we\npropose Vector-Quantized Variational Information Bottleneck (VQ-VIB), a method\nfor training neural agents to compress inputs into discrete signals embedded in\na continuous space. We train agents via VQ-VIB and compare their performance to\npreviously proposed neural architectures in grounded environments and in a\nLewis reference game. Across all neural architectures and settings, taking into\naccount communicative informativeness benefits communication convergence rates,\nand penalizing communicative complexity leads to human-like lexicon sizes while\nmaintaining high utility. Additionally, we find that VQ-VIB outperforms other\ndiscrete communication methods. This work demonstrates how fundamental\nprinciples that are believed to characterize human language evolution may\ninform emergent communication in artificial agents.", "published": "2022-06-30 20:10:20", "link": "http://arxiv.org/abs/2207.00088v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Building Multilingual Machine Translation Systems That Serve Arbitrary\n  X-Y Translations", "abstract": "Multilingual Neural Machine Translation (MNMT) enables one system to\ntranslate sentences from multiple source languages to multiple target\nlanguages, greatly reducing deployment costs compared with conventional\nbilingual systems. The MNMT training benefit, however, is often limited to\nmany-to-one directions. The model suffers from poor performance in one-to-many\nand many-to-many with zero-shot setup. To address this issue, this paper\ndiscusses how to practically build MNMT systems that serve arbitrary X-Y\ntranslation directions while leveraging multilinguality with a two-stage\ntraining strategy of pretraining and finetuning. Experimenting with the WMT'21\nmultilingual translation task, we demonstrate that our systems outperform the\nconventional baselines of direct bilingual models and pivot translation models\nfor most directions, averagely giving +6.0 and +4.1 BLEU, without the need for\narchitecture change or extra data collection. Moreover, we also examine our\nproposed approach in an extremely large-scale data setting to accommodate\npractical deployment scenarios.", "published": "2022-06-30 02:18:15", "link": "http://arxiv.org/abs/2206.14982v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Compressing Pre-trained Transformers via Low-Bit NxM Sparsity for\n  Natural Language Understanding", "abstract": "In recent years, large pre-trained Transformer networks have demonstrated\ndramatic improvements in many natural language understanding tasks. However,\nthe huge size of these models brings significant challenges to their\nfine-tuning and online deployment due to latency and cost constraints. New\nhardware supporting both N:M semi-structured sparsity and low-precision integer\ncomputation is a promising solution to boost DNN model serving efficiency.\nHowever, there have been very few studies that systematically investigate to\nwhat extent pre-trained Transformer networks benefit from the combination of\nthese techniques, as well as how to best compress each component of the\nTransformer. We propose a flexible compression framework NxMiFormer that\nperforms simultaneous sparsification and quantization using ADMM and STE-based\nQAT. Furthermore, we present and inexpensive, heuristic-driven search algorithm\nthat identifies promising heterogeneous compression configurations that meet a\ncompression ratio constraint. When evaluated across the GLUE suite of NLU\nbenchmarks, our approach can achieve up to 93% compression of the encoders of a\nBERT model while retaining 98.2% of the original model accuracy and taking full\nadvantage of the hardware's capabilities. Heterogeneous configurations found\nthe by the search heuristic maintain 99.5% of the baseline accuracy while still\ncompressing the model by 87.5%.", "published": "2022-06-30 04:33:50", "link": "http://arxiv.org/abs/2206.15014v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "esCorpius: A Massive Spanish Crawling Corpus", "abstract": "In the recent years, transformer-based models have lead to significant\nadvances in language modelling for natural language processing. However, they\nrequire a vast amount of data to be (pre-)trained and there is a lack of\ncorpora in languages other than English. Recently, several initiatives have\npresented multilingual datasets obtained from automatic web crawling. However,\nthe results in Spanish present important shortcomings, as they are either too\nsmall in comparison with other languages, or present a low quality derived from\nsub-optimal cleaning and deduplication. In this paper, we introduce esCorpius,\na Spanish crawling corpus obtained from near 1 Pb of Common Crawl data. It is\nthe most extensive corpus in Spanish with this level of quality in the\nextraction, purification and deduplication of web textual content. Our data\ncuration process involves a novel highly parallel cleaning pipeline and\nencompasses a series of deduplication mechanisms that together ensure the\nintegrity of both document and paragraph boundaries. Additionally, we maintain\nboth the source web page URL and the WARC shard origin URL in order to complain\nwith EU regulations. esCorpius has been released under CC BY-NC-ND 4.0 license\nand is available on HuggingFace.", "published": "2022-06-30 09:29:18", "link": "http://arxiv.org/abs/2206.15147v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficient Entity Candidate Generation for Low-Resource Languages", "abstract": "Candidate generation is a crucial module in entity linking. It also plays a\nkey role in multiple NLP tasks that have been proven to beneficially leverage\nknowledge bases. Nevertheless, it has often been overlooked in the monolingual\nEnglish entity linking literature, as naive approaches obtain very good\nperformance. Unfortunately, the existing approaches for English cannot be\nsuccessfully transferred to poorly resourced languages. This paper constitutes\nan in-depth analysis of the candidate generation problem in the context of\ncross-lingual entity linking with a focus on low-resource languages. Among\nother contributions, we point out limitations in the evaluation conducted in\nprevious works. We introduce a characterization of queries into types based on\ntheir difficulty, which improves the interpretability of the performance of\ndifferent methods. We also propose a light-weight and simple solution based on\nthe construction of indexes whose design is motivated by more complex transfer\nlearning based neural approaches. A thorough empirical analysis on 9 real-world\ndatasets under 2 evaluation settings shows that our simple solution outperforms\nthe state-of-the-art approach in terms of both quality and efficiency for\nalmost all datasets and query types.", "published": "2022-06-30 09:49:53", "link": "http://arxiv.org/abs/2206.15163v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FL-Tuning: Layer Tuning for Feed-Forward Network in Transformer", "abstract": "Prompt tuning is an emerging way of adapting pre-trained language models to\ndownstream tasks. However, the existing studies are mainly to add prompts to\nthe input sequence. This way would not work as expected due to the intermediate\nmulti-head self-attention and feed-forward network computation, making model\noptimization not very smooth. Hence, we propose a novel tuning way called layer\ntuning, aiming to add learnable parameters in Transformer layers. Specifically,\nwe focus on layer tuning for feed-forward network in the Transformer, namely\nFL-tuning. It introduces additional units into the hidden layer of each\nfeed-forward network. We conduct extensive experiments on the public CLUE\nbenchmark. The results show that: 1) Our FL-tuning outperforms prompt tuning\nmethods under both full-data and few-shot settings in almost all cases. In\nparticular, it improves accuracy by 17.93% (full-data setting) on WSC 1.0 and\nF1 by 16.142% (few-shot setting) on CLUENER over P-tuning v2. 2) Our FL-tuning\nis more stable and converges about 1.17 times faster than P-tuning v2. 3) With\nonly about 3% of Transformer's parameters to be trained, FL-tuning is\ncomparable with fine-tuning on most datasets, and significantly outperforms\nfine-tuning (e.g., accuracy improved by 12.9% on WSC 1.1) on several datasets.\nThe source codes are available at https://github.com/genggui001/FL-Tuning.", "published": "2022-06-30 14:30:50", "link": "http://arxiv.org/abs/2206.15312v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Two-Stage Classifier for COVID-19 Misinformation Detection Using BERT: a\n  Study on Indonesian Tweets", "abstract": "The COVID-19 pandemic has caused globally significant impacts since the\nbeginning of 2020. This brought a lot of confusion to society, especially due\nto the spread of misinformation through social media. Although there were\nalready several studies related to the detection of misinformation in social\nmedia data, most studies focused on the English dataset. Research on COVID-19\nmisinformation detection in Indonesia is still scarce. Therefore, through this\nresearch, we collect and annotate datasets for Indonesian and build prediction\nmodels for detecting COVID-19 misinformation by considering the tweet's\nrelevance. The dataset construction is carried out by a team of annotators who\nlabeled the relevance and misinformation of the tweet data. In this study, we\npropose the two-stage classifier model using IndoBERT pre-trained language\nmodel for the Tweet misinformation detection task. We also experiment with\nseveral other baseline models for text classification. The experimental results\nshow that the combination of the BERT sequence classifier for relevance\nprediction and Bi-LSTM for misinformation detection outperformed other machine\nlearning models with an accuracy of 87.02%. Overall, the BERT utilization\ncontributes to the higher performance of most prediction models. We release a\nhigh-quality COVID-19 misinformation Tweet corpus in the Indonesian language,\nindicated by the high inter-annotator agreement.", "published": "2022-06-30 15:33:20", "link": "http://arxiv.org/abs/2206.15359v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Forecasting Future World Events with Neural Networks", "abstract": "Forecasting future world events is a challenging but valuable task. Forecasts\nof climate, geopolitical conflict, pandemics and economic indicators help shape\npolicy and decision making. In these domains, the judgment of expert humans\ncontributes to the best forecasts. Given advances in language modeling, can\nthese forecasts be automated? To this end, we introduce Autocast, a dataset\ncontaining thousands of forecasting questions and an accompanying news corpus.\nQuestions are taken from forecasting tournaments, ensuring high quality,\nreal-world importance, and diversity. The news corpus is organized by date,\nallowing us to precisely simulate the conditions under which humans made past\nforecasts (avoiding leakage from the future). Motivated by the difficulty of\nforecasting numbers across orders of magnitude (e.g. global cases of COVID-19\nin 2022), we also curate IntervalQA, a dataset of numerical questions and\nmetrics for calibration. We test language models on our forecasting task and\nfind that performance is far below a human expert baseline. However,\nperformance improves with increased model size and incorporation of relevant\ninformation from the news corpus. In sum, Autocast poses a novel challenge for\nlarge language models and improved performance could bring large practical\nbenefits.", "published": "2022-06-30 17:59:14", "link": "http://arxiv.org/abs/2206.15474v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Language model compression with weighted low-rank factorization", "abstract": "Factorizing a large matrix into small matrices is a popular strategy for\nmodel compression. Singular value decomposition (SVD) plays a vital role in\nthis compression strategy, approximating a learned matrix with fewer\nparameters. However, SVD minimizes the squared error toward reconstructing the\noriginal matrix without gauging the importance of the parameters, potentially\ngiving a larger reconstruction error for those who affect the task accuracy\nmore. In other words, the optimization objective of SVD is not aligned with the\ntrained model's task accuracy. We analyze this previously unexplored problem,\nmake observations, and address it by introducing Fisher information to weigh\nthe importance of parameters affecting the model prediction. This idea leads to\nour method: Fisher-Weighted SVD (FWSVD). Although the factorized matrices from\nour approach do not result in smaller reconstruction errors, we find that our\nresulting task accuracy is much closer to the original model's performance. We\nperform analysis with the transformer-based language models, showing our\nweighted SVD largely alleviates the mismatched optimization objectives and can\nmaintain model performance with a higher compression rate. Our method can\ndirectly compress a task-specific model while achieving better performance than\nother compact model strategies requiring expensive model pre-training.\nMoreover, the evaluation of compressing an already compact model shows our\nmethod can further reduce 9% to 30% parameters with an insignificant impact on\ntask accuracy.", "published": "2022-06-30 21:57:07", "link": "http://arxiv.org/abs/2207.00112v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Code Translation with Compiler Representations", "abstract": "In this paper, we leverage low-level compiler intermediate representations\n(IR) to improve code translation. Traditional transpilers rely on syntactic\ninformation and handcrafted rules, which limits their applicability and\nproduces unnatural-looking code. Applying neural machine translation (NMT)\napproaches to code has successfully broadened the set of programs on which one\ncan get a natural-looking translation. However, they treat the code as\nsequences of text tokens, and still do not differentiate well enough between\nsimilar pieces of code which have different semantics in different languages.\nThe consequence is low quality translation, reducing the practicality of NMT,\nand stressing the need for an approach significantly increasing its accuracy.\nHere we propose to augment code translation with IRs, specifically LLVM IR,\nwith results on the C++, Java, Rust, and Go languages. Our method improves upon\nthe state of the art for unsupervised code translation, increasing the number\nof correct translations by 11% on average, and up to 79% for the Java -> Rust\npair with greedy decoding. We extend previous test sets for code translation,\nby adding hundreds of Go and Rust functions. Additionally, we train models with\nhigh performance on the problem of IR decompilation, generating programming\nsource code from IR, and study using IRs as intermediary pivot for translation.", "published": "2022-06-30 14:21:57", "link": "http://arxiv.org/abs/2207.03578v5", "categories": ["cs.PL", "cs.CL", "cs.LG"], "primary_category": "cs.PL"}
{"title": "Masked Part-Of-Speech Model: Does Modeling Long Context Help\n  Unsupervised POS-tagging?", "abstract": "Previous Part-Of-Speech (POS) induction models usually assume certain\nindependence assumptions (e.g., Markov, unidirectional, local dependency) that\ndo not hold in real languages. For example, the subject-verb agreement can be\nboth long-term and bidirectional. To facilitate flexible dependency modeling,\nwe propose a Masked Part-of-Speech Model (MPoSM), inspired by the recent\nsuccess of Masked Language Models (MLM). MPoSM can model arbitrary tag\ndependency and perform POS induction through the objective of masked POS\nreconstruction. We achieve competitive results on both the English Penn WSJ\ndataset as well as the universal treebank containing 10 diverse languages.\nThough modeling the long-term dependency should ideally help this task, our\nablation study shows mixed trends in different languages. To better understand\nthis phenomenon, we design a novel synthetic experiment that can specifically\ndiagnose the model's ability to learn tag agreement. Surprisingly, we find that\neven strong baselines fail to solve this problem consistently in a very\nsimplified setting: the agreement between adjacent words. Nonetheless, MPoSM\nachieves overall better performance. Lastly, we conduct a detailed error\nanalysis to shed light on other remaining challenges. Our code is available at\nhttps://github.com/owenzx/MPoSM", "published": "2022-06-30 01:43:05", "link": "http://arxiv.org/abs/2206.14969v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GSCLIP : A Framework for Explaining Distribution Shifts in Natural\n  Language", "abstract": "Helping end users comprehend the abstract distribution shifts can greatly\nfacilitate AI deployment. Motivated by this, we propose a novel task, dataset\nexplanation. Given two image data sets, dataset explanation aims to\nautomatically point out their dataset-level distribution shifts with natural\nlanguage. Current techniques for monitoring distribution shifts provide\ninadequate information to understand datasets with the goal of improving data\nquality. Therefore, we introduce GSCLIP, a training-free framework to solve the\ndataset explanation task. In GSCLIP, we propose the selector as the first\nquantitative evaluation method to identify explanations that are proper to\nsummarize dataset shifts. Furthermore, we leverage this selector to demonstrate\nthe superiority of a generator based on language model generation. Systematic\nevaluation on natural data shift verifies that GSCLIP, a combined system of a\nhybrid generator group and an efficient selector is not only easy-to-use but\nalso powerful for dataset explanation at scale.", "published": "2022-06-30 04:06:26", "link": "http://arxiv.org/abs/2206.15007v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Topological BERT: Transforming Attention into Topology for Natural\n  Language Processing", "abstract": "In recent years, the introduction of the Transformer models sparked a\nrevolution in natural language processing (NLP). BERT was one of the first text\nencoders using only the attention mechanism without any recurrent parts to\nachieve state-of-the-art results on many NLP tasks.\n  This paper introduces a text classifier using topological data analysis. We\nuse BERT's attention maps transformed into attention graphs as the only input\nto that classifier. The model can solve tasks such as distinguishing spam from\nham messages, recognizing whether a sentence is grammatically correct, or\nevaluating a movie review as negative or positive. It performs comparably to\nthe BERT baseline and outperforms it on some tasks.\n  Additionally, we propose a new method to reduce the number of BERT's\nattention heads considered by the topological classifier, which allows us to\nprune the number of heads from 144 down to as few as ten with no reduction in\nperformance. Our work also shows that the topological model displays higher\nrobustness against adversarial attacks than the original BERT model, which is\nmaintained during the pruning process. To the best of our knowledge, this work\nis the first to confront topological-based models with adversarial attacks in\nthe context of NLP.", "published": "2022-06-30 11:25:31", "link": "http://arxiv.org/abs/2206.15195v1", "categories": ["cs.CL", "cs.LG", "math.AT"], "primary_category": "cs.CL"}
{"title": "Improving Visual Grounding by Encouraging Consistent Gradient-based\n  Explanations", "abstract": "We propose a margin-based loss for tuning joint vision-language models so\nthat their gradient-based explanations are consistent with region-level\nannotations provided by humans for relatively smaller grounding datasets. We\nrefer to this objective as Attention Mask Consistency (AMC) and demonstrate\nthat it produces superior visual grounding results than previous methods that\nrely on using vision-language models to score the outputs of object detectors.\nParticularly, a model trained with AMC on top of standard vision-language\nmodeling objectives obtains a state-of-the-art accuracy of 86.49% in the\nFlickr30k visual grounding benchmark, an absolute improvement of 5.38% when\ncompared to the best previous model trained under the same level of\nsupervision. Our approach also performs exceedingly well on established\nbenchmarks for referring expression comprehension where it obtains 80.34%\naccuracy in the easy test of RefCOCO+, and 64.55% in the difficult split. AMC\nis effective, easy to implement, and is general as it can be adopted by any\nvision-language model, and can use any type of region annotations.", "published": "2022-06-30 17:55:12", "link": "http://arxiv.org/abs/2206.15462v4", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "MultiViz: Towards Visualizing and Understanding Multimodal Models", "abstract": "The promise of multimodal models for real-world applications has inspired\nresearch in visualizing and understanding their internal mechanics with the end\ngoal of empowering stakeholders to visualize model behavior, perform model\ndebugging, and promote trust in machine learning models. However, modern\nmultimodal models are typically black-box neural networks, which makes it\nchallenging to understand their internal mechanics. How can we visualize the\ninternal modeling of multimodal interactions in these models? Our paper aims to\nfill this gap by proposing MultiViz, a method for analyzing the behavior of\nmultimodal models by scaffolding the problem of interpretability into 4 stages:\n(1) unimodal importance: how each modality contributes towards downstream\nmodeling and prediction, (2) cross-modal interactions: how different modalities\nrelate with each other, (3) multimodal representations: how unimodal and\ncross-modal interactions are represented in decision-level features, and (4)\nmultimodal prediction: how decision-level features are composed to make a\nprediction. MultiViz is designed to operate on diverse modalities, models,\ntasks, and research areas. Through experiments on 8 trained models across 6\nreal-world tasks, we show that the complementary stages in MultiViz together\nenable users to (1) simulate model predictions, (2) assign interpretable\nconcepts to features, (3) perform error analysis on model misclassifications,\nand (4) use insights from error analysis to debug models. MultiViz is publicly\navailable, will be regularly updated with new interpretation tools and metrics,\nand welcomes inputs from the community.", "published": "2022-06-30 18:42:06", "link": "http://arxiv.org/abs/2207.00056v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.LG"}
{"title": "GLD-Net: Improving Monaural Speech Enhancement by Learning Global and\n  Local Dependency Features with GLD Block", "abstract": "For monaural speech enhancement, contextual information is important for\naccurate speech estimation. However, commonly used convolution neural networks\n(CNNs) are weak in capturing temporal contexts since they only build blocks\nthat process one local neighborhood at a time. To address this problem, we\nlearn from human auditory perception to introduce a two-stage trainable\nreasoning mechanism, referred as global-local dependency (GLD) block. GLD\nblocks capture long-term dependency of time-frequency bins both in global level\nand local level from the noisy spectrogram to help detecting correlations among\nspeech part, noise part, and whole noisy input. What is more, we conduct a\nmonaural speech enhancement network called GLD-Net, which adopts\nencoder-decoder architecture and consists of speech object branch, interference\nbranch, and global noisy branch. The extracted speech feature at global-level\nand local-level are efficiently reasoned and aggregated in each of the\nbranches. We compare the proposed GLD-Net with existing state-of-art methods on\nWSJ0 and DEMAND dataset. The results show that GLD-Net outperforms the\nstate-of-the-art methods in terms of PESQ and STOI.", "published": "2022-06-30 01:16:40", "link": "http://arxiv.org/abs/2206.14962v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "TTS-by-TTS 2: Data-selective augmentation for neural speech synthesis\n  using ranking support vector machine with variational autoencoder", "abstract": "Recent advances in synthetic speech quality have enabled us to train\ntext-to-speech (TTS) systems by using synthetic corpora. However, merely\nincreasing the amount of synthetic data is not always advantageous for\nimproving training efficiency. Our aim in this study is to selectively choose\nsynthetic data that are beneficial to the training process. In the proposed\nmethod, we first adopt a variational autoencoder whose posterior distribution\nis utilized to extract latent features representing acoustic similarity between\nthe recorded and synthetic corpora. By using those learned features, we then\ntrain a ranking support vector machine (RankSVM) that is well known for\neffectively ranking relative attributes among binary classes. By setting the\nrecorded and synthetic ones as two opposite classes, RankSVM is used to\ndetermine how the synthesized speech is acoustically similar to the recorded\ndata. Then, synthetic TTS data, whose distribution is close to the recorded\ndata, are selected from large-scale synthetic corpora. By using these data for\nretraining the TTS model, the synthetic quality can be significantly improved.\nObjective and subjective evaluation results show the superiority of the\nproposed method over the conventional methods.", "published": "2022-06-30 02:26:53", "link": "http://arxiv.org/abs/2206.14984v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Interpretable Melody Generation from Lyrics with Discrete-Valued\n  Adversarial Training", "abstract": "Generating melody from lyrics is an interesting yet challenging task in the\narea of artificial intelligence and music. However, the difficulty of keeping\nthe consistency between input lyrics and generated melody limits the generation\nquality of previous works. In our proposal, we demonstrate our proposed\ninterpretable lyrics-to-melody generation system which can interact with users\nto understand the generation process and recreate the desired songs. To improve\nthe reliability of melody generation that matches lyrics, mutual information is\nexploited to strengthen the consistency between lyrics and generated melodies.\nGumbel-Softmax is exploited to solve the non-differentiability problem of\ngenerating discrete music attributes by Generative Adversarial Networks (GANs).\nMoreover, the predicted probabilities output by the generator is utilized to\nrecommend music attributes. Interacting with our lyrics-to-melody generation\nsystem, users can listen to the generated AI song as well as recreate a new\nsong by selecting from recommended music attributes.", "published": "2022-06-30 05:45:47", "link": "http://arxiv.org/abs/2206.15027v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Language Model-Based Emotion Prediction Methods for Emotional Speech\n  Synthesis Systems", "abstract": "This paper proposes an effective emotional text-to-speech (TTS) system with a\npre-trained language model (LM)-based emotion prediction method. Unlike\nconventional systems that require auxiliary inputs such as manually defined\nemotion classes, our system directly estimates emotion-related attributes from\nthe input text. Specifically, we utilize generative pre-trained transformer\n(GPT)-3 to jointly predict both an emotion class and its strength in\nrepresenting emotions coarse and fine properties, respectively. Then, these\nattributes are combined in the emotional embedding space and used as\nconditional features of the TTS model for generating output speech signals.\nConsequently, the proposed system can produce emotional speech only from text\nwithout any auxiliary inputs. Furthermore, because the GPT-3 enables to capture\nemotional context among the consecutive sentences, the proposed method can\neffectively handle the paragraph-level generation of emotional speech.", "published": "2022-06-30 07:03:01", "link": "http://arxiv.org/abs/2206.15067v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Evaluation of Three-Stage Voice Conversion Framework for Noisy and\n  Reverberant Conditions", "abstract": "This paper presents a new voice conversion (VC) framework capable of dealing\nwith both additive noise and reverberation, and its performance evaluation.\nThere have been studied some VC researches focusing on real-world circumstances\nwhere speech data are interfered with background noise and reverberation. To\ndeal with more practical conditions where no clean target dataset is available,\none possible approach is zero-shot VC, but its performance tends to degrade\ncompared with VC using sufficient amount of target speech data. To leverage\nlarge amount of noisy-reverberant target speech data, we propose a three-stage\nVC framework based on denoising process using a pretrained denoising model,\ndereverberation process using a dereverberation model, and VC process using a\nnonparallel VC model based on a variational autoencoder. The experimental\nresults show that 1) noise and reverberation additively cause significant VC\nperformance degradation, 2) the proposed method alleviates the adverse effects\ncaused by both noise and reverberation, and significantly outperforms the\nbaseline directly trained on the noisy-reverberant speech data, and 3) the\npotential degradation introduced by the denoising and dereverberation still\ncauses noticeable adverse effects on VC performance.", "published": "2022-06-30 09:39:33", "link": "http://arxiv.org/abs/2206.15155v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "libACA, pyACA, and ACA-Code: Audio Content Analysis in 3 Languages", "abstract": "The three packages libACA, pyACA, and ACA-Code provide reference\nimplementations for basic approaches and algorithms for the analysis of musical\naudio signals in three different languages: C++, Python, and Matlab. All three\npackages cover the same algorithms, such as extraction of low level audio\nfeatures, fundamental frequency estimation, as well as simple approaches to\nchord recognition, musical key detection, and onset detection. In addition, it\nimplementations of more generic algorithms useful in audio content analysis\nsuch as dynamic time warping and the Viterbi algorithm are provided. The three\npackages thus provide a practical cross-language and cross-platform reference\nto students and engineers implementing audio analysis algorithms and enable\nimplementation-focused learning of algorithms for audio content analysis and\nmusic information retrieval.", "published": "2022-06-30 12:09:41", "link": "http://arxiv.org/abs/2206.15219v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sonification as a Reliable Alternative to Conventional Visual Surgical\n  Navigation", "abstract": "Despite the undeniable advantages of image-guided surgical assistance systems\nin terms of accuracy, such systems have not yet fully met surgeons' needs or\nexpectations regarding usability, time efficiency, and their integration into\nthe surgical workflow. On the other hand, perceptual studies have shown that\npresenting independent but causally correlated information via multimodal\nfeedback involving different sensory modalities can improve task performance.\nThis article investigates an alternative method for computer-assisted surgical\nnavigation, introduces a novel sonification methodology for navigated pedicle\nscrew placement, and discusses advanced solutions based on multisensory\nfeedback. The proposed method comprises a novel sonification solution for\nalignment tasks in four degrees of freedom based on frequency modulation (FM)\nsynthesis. We compared the resulting accuracy and execution time of the\nproposed sonification method with visual navigation, which is currently\nconsidered the state of the art. We conducted a phantom study in which 17\nsurgeons executed the pedicle screw placement task in the lumbar spine, guided\nby either the proposed sonification-based or the traditional visual navigation\nmethod. The results demonstrated that the proposed method is as accurate as the\nstate of the art while decreasing the surgeon's need to focus on visual\nnavigation displays instead of the natural focus on surgical tools and targeted\nanatomy during task execution.", "published": "2022-06-30 13:49:10", "link": "http://arxiv.org/abs/2206.15291v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Acoustic Room Compensation Using Local PCA-based Room Average Power\n  Response Estimation", "abstract": "Acoustic room compensation techniques, which allow a sound reproduction\nsystem to counteract undesired alteration to the sound scene due to excessive\nroom resonances, have been widely studied. Extensive efforts have been reported\nto enlarge the region over which room equalization is effective and to contrast\nvariations of room transfer functions in space. A speaker-tuning technology\n\"Trueplay\" allows users to compensate for undesired room effects over an\nextended listening area based on a spatially averaged power response of the\nroom, which is conventionally measured using microphones on portable devices\nwhen users move around the room. In this work, we propose a novel system that\nleverages measured speaker echo path self-responses to predict the room average\npower responses using a local PCA based approach. Experimental results confirm\nthe effectiveness of the proposed estimation method, which further leads to a\nroom compensation filter design that achieves a good sound similarity compared\nto the reference system with the ground-truth room average power response while\noutperforming other systems that do not leverage the proposed estimator.", "published": "2022-06-30 15:31:33", "link": "http://arxiv.org/abs/2206.15356v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Visual Speech Enhancement Network by Learning Audio-visual\n  Affinity with Multi-head Attention", "abstract": "Audio-visual speech enhancement system is regarded as one of promising\nsolutions for isolating and enhancing speech of desired speaker. Typical\nmethods focus on predicting clean speech spectrum via a naive convolution\nneural network based encoder-decoder architecture, and these methods a) are not\nadequate to use data fully, b) are unable to effectively balance audio-visual\nfeatures. The proposed model alleviates these drawbacks by a) applying a model\nthat fuses audio and visual features layer by layer in encoding phase, and that\nfeeds fused audio-visual features to each corresponding decoder layer, and more\nimportantly, b) introducing a 2-stage multi-head cross attention (MHCA)\nmechanism to infer audio-visual speech enhancement for balancing the fused\naudio-visual features and eliminating irrelevant features. This paper proposes\nattentional audio-visual multi-layer feature fusion model, in which MHCA units\nare applied to feature mapping at every layer of decoder. The proposed model\ndemonstrates the superior performance of the network against the\nstate-of-the-art models.", "published": "2022-06-30 01:20:43", "link": "http://arxiv.org/abs/2206.14964v1", "categories": ["eess.AS", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "FeaRLESS: Feature Refinement Loss for Ensembling Self-Supervised\n  Learning Features in Robust End-to-end Speech Recognition", "abstract": "Self-supervised learning representations (SSLR) have resulted in robust\nfeatures for downstream tasks in many fields. Recently, several SSLRs have\nshown promising results on automatic speech recognition (ASR) benchmark\ncorpora. However, previous studies have only shown performance for solitary\nSSLRs as an input feature for ASR models. In this study, we propose to\ninvestigate the effectiveness of diverse SSLR combinations using various fusion\nmethods within end-to-end (E2E) ASR models. In addition, we will show there are\ncorrelations between these extracted SSLRs. As such, we further propose a\nfeature refinement loss for decorrelation to efficiently combine the set of\ninput features. For evaluation, we show that the proposed 'FeaRLESS learning\nfeatures' perform better than systems without the proposed feature refinement\nloss for both the WSJ and Fearless Steps Challenge (FSC) corpora.", "published": "2022-06-30 06:39:40", "link": "http://arxiv.org/abs/2206.15056v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "R-MelNet: Reduced Mel-Spectral Modeling for Neural TTS", "abstract": "This paper introduces R-MelNet, a two-part autoregressive architecture with a\nfrontend based on the first tier of MelNet and a backend WaveRNN-style audio\ndecoder for neural text-to-speech synthesis. Taking as input a mixed sequence\nof characters and phonemes, with an optional audio priming sequence, this model\nproduces low-resolution mel-spectral features which are interpolated and used\nby a WaveRNN decoder to produce an audio waveform. Coupled with half precision\ntraining, R-MelNet uses under 11 gigabytes of GPU memory on a single commodity\nGPU (NVIDIA 2080Ti). We detail a number of critical implementation details for\nstable half precision training, including an approximate, numerically stable\nmixture of logistics attention. Using a stochastic, multi-sample per step\ninference scheme, the resulting model generates highly varied audio, while\nenabling text and audio based controls to modify output waveforms. Qualitative\nand quantitative evaluations of an R-MelNet system trained on a single speaker\nTTS dataset demonstrate the effectiveness of our approach.", "published": "2022-06-30 13:29:31", "link": "http://arxiv.org/abs/2206.15276v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning Audio-Text Agreement for Open-vocabulary Keyword Spotting", "abstract": "In this paper, we propose a novel end-to-end user-defined keyword spotting\nmethod that utilizes linguistically corresponding patterns between speech and\ntext sequences. Unlike previous approaches requiring speech keyword enrollment,\nour method compares input queries with an enrolled text keyword sequence. To\nplace the audio and text representations within a common latent space, we adopt\nan attention-based cross-modal matching approach that is trained in an\nend-to-end manner with monotonic matching loss and keyword classification loss.\nWe also utilize a de-noising loss for the acoustic embedding network to improve\nrobustness in noisy environments. Additionally, we introduce the LibriPhrase\ndataset, a new short-phrase dataset based on LibriSpeech for efficiently\ntraining keyword spotting models. Our proposed method achieves competitive\nresults on various evaluation sets compared to other single-modal and\ncross-modal baselines.", "published": "2022-06-30 16:40:31", "link": "http://arxiv.org/abs/2206.15400v2", "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Sub-8-Bit Quantization Aware Training for 8-Bit Neural Network\n  Accelerator with On-Device Speech Recognition", "abstract": "We present a novel sub-8-bit quantization-aware training (S8BQAT) scheme for\n8-bit neural network accelerators. Our method is inspired from Lloyd-Max\ncompression theory with practical adaptations for a feasible computational\noverhead during training. With the quantization centroids derived from a 32-bit\nbaseline, we augment training loss with a Multi-Regional Absolute Cosine\n(MRACos) regularizer that aggregates weights towards their nearest centroid,\neffectively acting as a pseudo compressor. Additionally, a periodically invoked\nhard compressor is introduced to improve the convergence rate by emulating\nruntime model weight quantization. We apply S8BQAT on speech recognition tasks\nusing Recurrent Neural NetworkTransducer (RNN-T) architecture. With S8BQAT, we\nare able to increase the model parameter size to reduce the word error rate by\n4-16% relatively, while still improving latency by 5%.", "published": "2022-06-30 16:52:07", "link": "http://arxiv.org/abs/2206.15408v1", "categories": ["eess.AS", "cs.AI", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Implicit Neural Spatial Filtering for Multichannel Source Separation in\n  the Waveform Domain", "abstract": "We present a single-stage casual waveform-to-waveform multichannel model that\ncan separate moving sound sources based on their broad spatial locations in a\ndynamic acoustic scene. We divide the scene into two spatial regions\ncontaining, respectively, the target and the interfering sound sources. The\nmodel is trained end-to-end and performs spatial processing implicitly, without\nany components based on traditional processing or use of hand-crafted spatial\nfeatures. We evaluate the proposed model on a real-world dataset and show that\nthe model matches the performance of an oracle beamformer followed by a\nstate-of-the-art single-channel enhancement network.", "published": "2022-06-30 17:13:01", "link": "http://arxiv.org/abs/2206.15423v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "j-Wave: An open-source differentiable wave simulator", "abstract": "We present an open-source differentiable acoustic simulator, j-Wave, which\ncan solve time-varying and time-harmonic acoustic problems. It supports\nautomatic differentiation, which is a program transformation technique that has\nmany applications, especially in machine learning and scientific computing.\nj-Wave is composed of modular components that can be easily customized and\nreused. At the same time, it is compatible with some of the most popular\nmachine learning libraries, such as JAX and TensorFlow. The accuracy of the\nsimulation results for known configurations is evaluated against the widely\nused k-Wave toolbox and a cohort of acoustic simulation software. j-Wave is\navailable from https://github.com/ucl-bug/jwave.", "published": "2022-06-30 16:19:21", "link": "http://arxiv.org/abs/2207.01499v1", "categories": ["physics.comp-ph", "cs.LG", "cs.MS", "cs.SD", "eess.AS", "physics.med-ph"], "primary_category": "physics.comp-ph"}
