{"title": "Learning Concept Abstractness Using Weak Supervision", "abstract": "We introduce a weakly supervised approach for inferring the property of\nabstractness of words and expressions in the complete absence of labeled data.\nExploiting only minimal linguistic clues and the contextual usage of a concept\nas manifested in textual data, we train sufficiently powerful classifiers,\nobtaining high correlation with human labels. The results imply the\napplicability of this approach to additional properties of concepts, additional\nlanguages, and resource-scarce scenarios.", "published": "2018-09-05 01:08:23", "link": "http://arxiv.org/abs/1809.01285v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Policy Shaping and Generalized Update Equations for Semantic Parsing\n  from Denotations", "abstract": "Semantic parsing from denotations faces two key challenges in model training:\n(1) given only the denotations (e.g., answers), search for good candidate\nsemantic parses, and (2) choose the best model update algorithm. We propose\neffective and general solutions to each of them. Using policy shaping, we bias\nthe search procedure towards semantic parses that are more compatible to the\ntext, which provide better supervision signals for training. In addition, we\npropose an update equation that generalizes three different families of\nlearning algorithms, which enables fast model exploration. When experimented on\na recently proposed sequential question answering dataset, our framework leads\nto a new state-of-the-art model that outperforms previous work by 5.0% absolute\non exact match accuracy.", "published": "2018-09-05 02:15:02", "link": "http://arxiv.org/abs/1809.01299v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BPE and CharCNNs for Translation of Morphology: A Cross-Lingual\n  Comparison and Analysis", "abstract": "Neural Machine Translation (NMT) in low-resource settings and of\nmorphologically rich languages is made difficult in part by data sparsity of\nvocabulary words. Several methods have been used to help reduce this sparsity,\nnotably Byte-Pair Encoding (BPE) and a character-based CNN layer (charCNN).\nHowever, the charCNN has largely been neglected, possibly because it has only\nbeen compared to BPE rather than combined with it. We argue for a\nreconsideration of the charCNN, based on cross-lingual improvements on\nlow-resource data. We translate from 8 languages into English, using a\nmulti-way parallel collection of TED transcripts. We find that in most cases,\nusing both BPE and a charCNN performs best, while in Hebrew, using a charCNN\nover words is best.", "published": "2018-09-05 02:26:09", "link": "http://arxiv.org/abs/1809.01301v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RNNs as psycholinguistic subjects: Syntactic state and grammatical\n  dependency", "abstract": "Recurrent neural networks (RNNs) are the state of the art in sequence\nmodeling for natural language. However, it remains poorly understood what\ngrammatical characteristics of natural language they implicitly learn and\nrepresent as a consequence of optimizing the language modeling objective. Here\nwe deploy the methods of controlled psycholinguistic experimentation to shed\nlight on to what extent RNN behavior reflects incremental syntactic state and\ngrammatical dependency representations known to characterize human linguistic\nbehavior. We broadly test two publicly available long short-term memory (LSTM)\nEnglish sequence models, and learn and test a new Japanese LSTM. We demonstrate\nthat these models represent and maintain incremental syntactic state, but that\nthey do not always generalize in the same way as humans. Furthermore, none of\nour models learn the appropriate grammatical dependency configurations\nlicensing reflexive pronouns or negative polarity items.", "published": "2018-09-05 05:12:34", "link": "http://arxiv.org/abs/1809.01329v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural MultiVoice Models for Expressing Novel Personalities in Dialog", "abstract": "Natural language generators for task-oriented dialog should be able to vary\nthe style of the output utterance while still effectively realizing the system\ndialog actions and their associated semantics. While the use of neural\ngeneration for training the response generation component of conversational\nagents promises to simplify the process of producing high quality responses in\nnew domains, to our knowledge, there has been very little investigation of\nneural generators for task-oriented dialog that can vary their response style,\nand we know of no experiments on models that can generate responses that are\ndifferent in style from those seen during training, while still maintain- ing\nsemantic fidelity to the input meaning representation. Here, we show that a\nmodel that is trained to achieve a single stylis- tic personality target can\nproduce outputs that combine stylistic targets. We carefully evaluate the\nmultivoice outputs for both semantic fidelity and for similarities to and\ndifferences from the linguistic features that characterize the original\ntraining style. We show that contrary to our predictions, the learned models do\nnot always simply interpolate model parameters, but rather produce styles that\nare distinct, and novel from the personalities they were trained on.", "published": "2018-09-05 05:24:00", "link": "http://arxiv.org/abs/1809.01331v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Firearms and Tigers are Dangerous, Kitchen Knives and Zebras are Not:\n  Testing whether Word Embeddings Can Tell", "abstract": "This paper presents an approach for investigating the nature of semantic\ninformation captured by word embeddings. We propose a method that extends an\nexisting human-elicited semantic property dataset with gold negative examples\nusing crowd judgments. Our experimental approach tests the ability of\nsupervised classifiers to identify semantic features in word embedding vectors\nand com- pares this to a feature-identification method based on full vector\ncosine similarity. The idea behind this method is that properties identified by\nclassifiers, but not through full vector comparison are captured by embeddings.\nProperties that cannot be identified by either method are not. Our results\nprovide an initial indication that semantic properties relevant for the way\nentities interact (e.g. dangerous) are captured, while perceptual information\n(e.g. colors) is not represented. We conclude that, though preliminary, these\nresults show that our method is suitable for identifying which properties are\ncaptured by embeddings.", "published": "2018-09-05 08:14:16", "link": "http://arxiv.org/abs/1809.01375v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pre-training on high-resource speech recognition improves low-resource\n  speech-to-text translation", "abstract": "We present a simple approach to improve direct speech-to-text translation\n(ST) when the source language is low-resource: we pre-train the model on a\nhigh-resource automatic speech recognition (ASR) task, and then fine-tune its\nparameters for ST. We demonstrate that our approach is effective by\npre-training on 300 hours of English ASR data to improve Spanish-English ST\nfrom 10.8 to 20.2 BLEU when only 20 hours of Spanish-English ST training data\nare available. Through an ablation study, we find that the pre-trained encoder\n(acoustic model) accounts for most of the improvement, despite the fact that\nthe shared language in these tasks is the target language text, not the source\nlanguage audio. Applying this insight, we show that pre-training on ASR helps\nST even when the ASR language differs from both source and target ST languages:\npre-training on French ASR also improves Spanish-English ST. Finally, we show\nthat the approach improves performance on a true low-resource task:\npre-training on a combination of English ASR and French ASR improves\nMboshi-French ST, where only 4 hours of data are available, from 3.5 to 7.1\nBLEU.", "published": "2018-09-05 10:56:30", "link": "http://arxiv.org/abs/1809.01431v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Free as in Free Word Order: An Energy Based Model for Word Segmentation\n  and Morphological Tagging in Sanskrit", "abstract": "The configurational information in sentences of a free word order language\nsuch as Sanskrit is of limited use. Thus, the context of the entire sentence\nwill be desirable even for basic processing tasks such as word segmentation. We\npropose a structured prediction framework that jointly solves the word\nsegmentation and morphological tagging tasks in Sanskrit. We build an energy\nbased model where we adopt approaches generally employed in graph based parsing\ntechniques (McDonald et al., 2005a; Carreras, 2007). Our model outperforms the\nstate of the art with an F-Score of 96.92 (percentage improvement of 7.06%)\nwhile using less than one-tenth of the task-specific training data. We find\nthat the use of a graph based ap- proach instead of a traditional lattice-based\nsequential labelling approach leads to a percentage gain of 12.6% in F-Score\nfor the segmentation task.", "published": "2018-09-05 11:44:13", "link": "http://arxiv.org/abs/1809.01446v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Appendix - Recommended Statistical Significance Tests for NLP Tasks", "abstract": "Statistical significance testing plays an important role when drawing\nconclusions from experimental results in NLP papers. Particularly, it is a\nvaluable tool when one would like to establish the superiority of one algorithm\nover another. This appendix complements the guide for testing statistical\nsignificance in NLP presented in \\cite{dror2018hitchhiker} by proposing valid\nstatistical tests for the common tasks and evaluation measures in the field.", "published": "2018-09-05 11:55:05", "link": "http://arxiv.org/abs/1809.01448v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Copenhagen at CoNLL--SIGMORPHON 2018: Multilingual Inflection in Context\n  with Explicit Morphosyntactic Decoding", "abstract": "This paper documents the Team Copenhagen system which placed first in the\nCoNLL--SIGMORPHON 2018 shared task on universal morphological reinflection,\nTask 2 with an overall accuracy of 49.87. Task 2 focuses on morphological\ninflection in context: generating an inflected word form, given the lemma of\nthe word and the context it occurs in. Previous SIGMORPHON shared tasks have\nfocused on context-agnostic inflection---the \"inflection in context\" task was\nintroduced this year. We approach this with an encoder-decoder architecture\nover character sequences with three core innovations, all contributing to an\nimprovement in performance: (1) a wide context window; (2) a multi-task\nlearning approach with the auxiliary task of MSD prediction; (3) training\nmodels in a multilingual fashion.", "published": "2018-09-05 14:31:04", "link": "http://arxiv.org/abs/1809.01541v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamically Context-Sensitive Time-Decay Attention for Dialogue Modeling", "abstract": "Spoken language understanding (SLU) is an essential component in\nconversational systems. Considering that contexts provide informative cues for\nbetter understanding, history can be leveraged for contextual SLU. However,\nmost prior work only paid attention to the related content in history\nutterances and ignored the temporal information. In dialogues, it is intuitive\nthat the most recent utterances are more important than the least recent ones,\nand time-aware attention should be in a decaying manner. Therefore, this paper\nallows the model to automatically learn a time-decay attention function where\nthe attentional weights can be dynamically decided based on the content of each\nrole's contexts, which effectively integrates both content-aware and time-aware\nperspectives and demonstrates remarkable flexibility to complex dialogue\ncontexts. The experiments on the benchmark Dialogue State Tracking Challenge\n(DSTC4) dataset show that the proposed dynamically context-sensitive time-decay\nattention mechanisms significantly improve the state-of-the-art model for\ncontextual understanding performance.", "published": "2018-09-05 14:53:14", "link": "http://arxiv.org/abs/1809.01557v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stance Prediction for Russian: Data and Analysis", "abstract": "Stance detection is a critical component of rumour and fake news\nidentification. It involves the extraction of the stance a particular author\ntakes related to a given claim, both expressed in text. This paper investigates\nstance classification for Russian. It introduces a new dataset, RuStance, of\nRussian tweets and news comments from multiple sources, covering multiple\nstories, as well as text classification approaches to stance detection as\nbenchmarks over this data in this language. As well as presenting this\nopenly-available dataset, the first of its kind for Russian, the paper presents\na baseline for stance prediction in the language.", "published": "2018-09-05 15:20:41", "link": "http://arxiv.org/abs/1809.01574v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Document-Level Neural Machine Translation with Hierarchical Attention\n  Networks", "abstract": "Neural Machine Translation (NMT) can be improved by including document-level\ncontextual information. For this purpose, we propose a hierarchical attention\nmodel to capture the context in a structured and dynamic manner. The model is\nintegrated in the original NMT architecture as another level of abstraction,\nconditioning on the NMT model's own previous hidden states. Experiments show\nthat hierarchical attention significantly improves the BLEU score over a strong\nNMT baseline with the state-of-the-art in context-aware methods, and that both\nthe encoder and decoder benefit from context in complementary ways.", "published": "2018-09-05 15:27:16", "link": "http://arxiv.org/abs/1809.01576v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Accelerated Reinforcement Learning for Sentence Generation by Vocabulary\n  Prediction", "abstract": "A major obstacle in reinforcement learning-based sentence generation is the\nlarge action space whose size is equal to the vocabulary size of the\ntarget-side language. To improve the efficiency of reinforcement learning, we\npresent a novel approach for reducing the action space based on dynamic\nvocabulary prediction. Our method first predicts a fixed-size small vocabulary\nfor each input to generate its target sentence. The input-specific vocabularies\nare then used at supervised and reinforcement learning steps, and also at test\ntime. In our experiments on six machine translation and two image captioning\ndatasets, our method achieves faster reinforcement learning ($\\sim$2.7x faster)\nwith less GPU memory ($\\sim$2.3x less) than the full-vocabulary counterpart.\nThe reinforcement learning with our method consistently leads to significant\nimprovement of BLEU scores, and the scores are equal to or better than those of\nbaselines using the full vocabularies, with faster decoding time ($\\sim$3x\nfaster) on CPUs.", "published": "2018-09-05 19:11:14", "link": "http://arxiv.org/abs/1809.01694v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Question Answering by Commonsense-Based Pre-Training", "abstract": "Although neural network approaches achieve remarkable success on a variety of\nNLP tasks, many of them struggle to answer questions that require commonsense\nknowledge. We believe the main reason is the lack of commonsense\n\\mbox{connections} between concepts. To remedy this, we provide a simple and\neffective method that leverages external commonsense knowledge base such as\nConceptNet. We pre-train direct and indirect relational functions between\nconcepts, and show that these pre-trained functions could be easily added to\nexisting neural network models. Results show that incorporating\ncommonsense-based function improves the baseline on three question answering\ntasks that require commonsense reasoning. Further analysis shows that our\nsystem \\mbox{discovers} and leverages useful evidence from an external\ncommonsense knowledge base, which is missing in existing neural network models\nand help derive the correct answer.", "published": "2018-09-05 13:04:56", "link": "http://arxiv.org/abs/1809.03568v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Localizing Moments in Video with Temporal Language", "abstract": "Localizing moments in a longer video via natural language queries is a new,\nchallenging task at the intersection of language and video understanding.\nThough moment localization with natural language is similar to other language\nand vision tasks like natural language object retrieval in images, moment\nlocalization offers an interesting opportunity to model temporal dependencies\nand reasoning in text. We propose a new model that explicitly reasons about\ndifferent temporal segments in a video, and shows that temporal context is\nimportant for localizing phrases which include temporal language. To benchmark\nwhether our model, and other recent video localization models, can effectively\nreason about temporal language, we collect the novel TEMPOral reasoning in\nvideo and language (TEMPO) dataset. Our dataset consists of two parts: a\ndataset with real videos and template sentences (TEMPO - Template Language)\nwhich allows for controlled studies on temporal language, and a human language\ndataset which consists of temporal sentences annotated by humans (TEMPO - Human\nLanguage).", "published": "2018-09-05 05:58:47", "link": "http://arxiv.org/abs/1809.01337v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Sentylic at IEST 2018: Gated Recurrent Neural Network and Capsule\n  Network Based Approach for Implicit Emotion Detection", "abstract": "In this paper, we present the system we have used for the Implicit WASSA 2018\nImplicit Emotion Shared Task. The task is to predict the emotion of a tweet of\nwhich the explicit mentions of emotion terms have been removed. The idea is to\ncome up with a model which has the ability to implicitly identify the emotion\nexpressed given the context words. We have used a Gated Recurrent Neural\nNetwork (GRU) and a Capsule Network based model for the task. Pre-trained word\nembeddings have been utilized to incorporate contextual knowledge about words\ninto the model. GRU layer learns latent representations using the input word\nembeddings. Subsequent Capsule Network layer learns high-level features from\nthat hidden representation. The proposed model managed to achieve a macro-F1\nscore of 0.692.", "published": "2018-09-05 12:04:35", "link": "http://arxiv.org/abs/1809.01452v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bimodal network architectures for automatic generation of image\n  annotation from text", "abstract": "Medical image analysis practitioners have embraced big data methodologies.\nThis has created a need for large annotated datasets. The source of big data is\ntypically large image collections and clinical reports recorded for these\nimages. In many cases, however, building algorithms aimed at segmentation and\ndetection of disease requires a training dataset with markings of the areas of\ninterest on the image that match with the described anomalies. This process of\nannotation is expensive and needs the involvement of clinicians. In this work\nwe propose two separate deep neural network architectures for automatic marking\nof a region of interest (ROI) on the image best representing a finding\nlocation, given a textual report or a set of keywords. One architecture\nconsists of LSTM and CNN components and is trained end to end with images,\nmatching text, and markings of ROIs for those images. The output layer\nestimates the coordinates of the vertices of a polygonal region. The second\narchitecture uses a network pre-trained on a large dataset of the same image\ntypes for learning feature representations of the findings of interest. We show\nthat for a variety of findings from chest X-ray images, both proposed\narchitectures learn to estimate the ROI, as validated by clinical annotations.\nThere is a clear advantage obtained from the architecture with pre-trained\nimaging network. The centroids of the ROIs marked by this network were on\naverage at a distance equivalent to 5.1% of the image width from the centroids\nof the ground truth ROIs.", "published": "2018-09-05 16:30:08", "link": "http://arxiv.org/abs/1809.01610v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Deep Relevance Ranking Using Enhanced Document-Query Interactions", "abstract": "We explore several new models for document relevance ranking, building upon\nthe Deep Relevance Matching Model (DRMM) of Guo et al. (2016). Unlike DRMM,\nwhich uses context-insensitive encodings of terms and query-document term\ninteractions, we inject rich context-sensitive encodings throughout our models,\ninspired by PACRR's (Hui et al., 2017) convolutional n-gram matching features,\nbut extended in several ways including multiple views of query and document\ninputs. We test our models on datasets from the BIOASQ question answering\nchallenge (Tsatsaronis et al., 2015) and TREC ROBUST 2004 (Voorhees, 2005),\nshowing they outperform BM25-based baselines, DRMM, and PACRR.", "published": "2018-09-05 18:18:34", "link": "http://arxiv.org/abs/1809.01682v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Learning User Preferences and Understanding Calendar Contexts for Event\n  Scheduling", "abstract": "With online calendar services gaining popularity worldwide, calendar data has\nbecome one of the richest context sources for understanding human behavior.\nHowever, event scheduling is still time-consuming even with the development of\nonline calendars. Although machine learning based event scheduling models have\nautomated scheduling processes to some extent, they often fail to understand\nsubtle user preferences and complex calendar contexts with event titles written\nin natural language. In this paper, we propose Neural Event Scheduling\nAssistant (NESA) which learns user preferences and understands calendar\ncontexts, directly from raw online calendars for fully automated and highly\neffective event scheduling. We leverage over 593K calendar events for NESA to\nlearn scheduling personal events, and we further utilize NESA for\nmulti-attendee event scheduling. NESA successfully incorporates deep neural\nnetworks such as Bidirectional Long Short-Term Memory, Convolutional Neural\nNetwork, and Highway Network for learning the preferences of each user and\nunderstanding calendar context based on natural languages. The experimental\nresults show that NESA significantly outperforms previous baseline models in\nterms of various evaluation metrics on both personal and multi-attendee event\nscheduling tasks. Our qualitative analysis demonstrates the effectiveness of\neach layer in NESA and learned user preferences.", "published": "2018-09-05 04:15:13", "link": "http://arxiv.org/abs/1809.01316v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Embedding Multimodal Relational Data for Knowledge Base Completion", "abstract": "Representing entities and relations in an embedding space is a well-studied\napproach for machine learning on relational data. Existing approaches, however,\nprimarily focus on simple link structure between a finite set of entities,\nignoring the variety of data types that are often used in knowledge bases, such\nas text, images, and numerical values. In this paper, we propose multimodal\nknowledge base embeddings (MKBE) that use different neural encoders for this\nvariety of observed data, and combine them with existing relational models to\nlearn embeddings of the entities and multimodal data. Further, using these\nlearned embedings and different neural decoders, we introduce a novel\nmultimodal imputation model to generate missing multimodal values, like text\nand images, from information in the knowledge base. We enrich existing\nrelational datasets to create two novel benchmarks that contain additional\ninformation such as textual descriptions and images of the original entities.\nWe demonstrate that our models utilize this additional information effectively\nto provide more accurate link prediction, achieving state-of-the-art results\nwith a considerable gap of 5-7% over existing methods. Further, we evaluate the\nquality of our generated multimodal values via a user study. We have release\nthe datasets and the open-source implementation of our models at\nhttps://github.com/pouyapez/mkbe", "published": "2018-09-05 06:07:31", "link": "http://arxiv.org/abs/1809.01341v2", "categories": ["cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.AI"}
{"title": "Utilizing Character and Word Embeddings for Text Normalization with\n  Sequence-to-Sequence Models", "abstract": "Text normalization is an important enabling technology for several NLP tasks.\nRecently, neural-network-based approaches have outperformed well-established\nmodels in this task. However, in languages other than English, there has been\nlittle exploration in this direction. Both the scarcity of annotated data and\nthe complexity of the language increase the difficulty of the problem. To\naddress these challenges, we use a sequence-to-sequence model with\ncharacter-based attention, which in addition to its self-learned character\nembeddings, uses word embeddings pre-trained with an approach that also models\nsubword information. This provides the neural model with access to more\nlinguistic information especially suitable for text normalization, without\nlarge parallel corpora. We show that providing the model with word-level\nfeatures bridges the gap for the neural network approach to achieve a\nstate-of-the-art F1 score on a standard Arabic language correction shared task\ndataset.", "published": "2018-09-05 16:44:04", "link": "http://arxiv.org/abs/1809.01534v1", "categories": ["cs.CL", "cs.LG", "stat.ML", "I.2.6"], "primary_category": "cs.CL"}
{"title": "TVQA: Localized, Compositional Video Question Answering", "abstract": "Recent years have witnessed an increasing interest in image-based\nquestion-answering (QA) tasks. However, due to data limitations, there has been\nmuch less work on video-based QA. In this paper, we present TVQA, a large-scale\nvideo QA dataset based on 6 popular TV shows. TVQA consists of 152,545 QA pairs\nfrom 21,793 clips, spanning over 460 hours of video. Questions are designed to\nbe compositional in nature, requiring systems to jointly localize relevant\nmoments within a clip, comprehend subtitle-based dialogue, and recognize\nrelevant visual concepts. We provide analyses of this new dataset as well as\nseveral baselines and a multi-stream end-to-end trainable neural network\nframework for the TVQA task. The dataset is publicly available at\nhttp://tvqa.cs.unc.edu.", "published": "2018-09-05 19:14:11", "link": "http://arxiv.org/abs/1809.01696v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Attention-based Audio-Visual Fusion for Robust Automatic Speech\n  Recognition", "abstract": "Automatic speech recognition can potentially benefit from the lip motion\npatterns, complementing acoustic speech to improve the overall recognition\nperformance, particularly in noise. In this paper we propose an audio-visual\nfusion strategy that goes beyond simple feature concatenation and learns to\nautomatically align the two modalities, leading to enhanced representations\nwhich increase the recognition accuracy in both clean and noisy conditions. We\ntest our strategy on the TCD-TIMIT and LRS2 datasets, designed for large\nvocabulary continuous speech recognition, applying three types of noise at\ndifferent power ratios. We also exploit state of the art Sequence-to-Sequence\narchitectures, showing that our method can be easily integrated. Results show\nrelative improvements from 7% up to 30% on TCD-TIMIT over the acoustic modality\nalone, depending on the acoustic noise level. We anticipate that the fusion\nstrategy can easily generalise to many other multimodal tasks which involve\ncorrelated modalities. Code available online on GitHub:\nhttps://github.com/georgesterpu/Sigmedia-AVSR", "published": "2018-09-05 20:38:48", "link": "http://arxiv.org/abs/1809.01728v3", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.IV", "stat.ML"], "primary_category": "eess.AS"}
