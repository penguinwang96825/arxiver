{"title": "Improving Semantic Parsing with Neural Generator-Reranker Architecture", "abstract": "Semantic parsing is the problem of deriving machine interpretable meaning\nrepresentations from natural language utterances. Neural models with\nencoder-decoder architectures have recently achieved substantial improvements\nover traditional methods. Although neural semantic parsers appear to have\nrelatively high recall using large beam sizes, there is room for improvement\nwith respect to one-best precision. In this work, we propose a\ngenerator-reranker architecture for semantic parsing. The generator produces a\nlist of potential candidates and the reranker, which consists of a\npre-processing step for the candidates followed by a novel critic network,\nreranks these candidates based on the similarity between each candidate and the\ninput sentence. We show the advantages of this approach along with how it\nimproves the parsing performance through extensive analysis. We experiment our\nmodel on three semantic parsing datasets (GEO, ATIS, and OVERNIGHT). The\noverall architecture achieves the state-of-the-art results in all three\ndatasets.", "published": "2019-09-27 16:10:06", "link": "http://arxiv.org/abs/1909.12764v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "In-training Matrix Factorization for Parameter-frugal Neural Machine\n  Translation", "abstract": "In this paper, we propose the use of in-training matrix factorization to\nreduce the model size for neural machine translation. Using in-training matrix\nfactorization, parameter matrices may be decomposed into the products of\nsmaller matrices, which can compress large machine translation architectures by\nvastly reducing the number of learnable parameters. We apply in-training matrix\nfactorization to different layers of standard neural architectures and show\nthat in-training factorization is capable of reducing nearly 50% of learnable\nparameters without any associated loss in BLEU score. Further, we find that\nin-training matrix factorization is especially powerful on embedding layers,\nproviding a simple and effective method to curtail the number of parameters\nwith minimal impact on model performance, and, at times, an increase in\nperformance.", "published": "2019-09-27 08:58:55", "link": "http://arxiv.org/abs/1910.06393v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HateMonitors: Language Agnostic Abuse Detection in Social Media", "abstract": "Reducing hateful and offensive content in online social media pose a dual\nproblem for the moderators. On the one hand, rigid censorship on social media\ncannot be imposed. On the other, the free flow of such content cannot be\nallowed. Hence, we require efficient abusive language detection system to\ndetect such harmful content in social media. In this paper, we present our\nmachine learning model, HateMonitor, developed for Hate Speech and Offensive\nContent Identification in Indo-European Languages (HASOC), a shared task at\nFIRE 2019. We have used a Gradient Boosting model, along with BERT and LASER\nembeddings, to make the system language agnostic. Our model came at First\nposition for the German sub-task A. We have also made our model public at\nhttps://github.com/punyajoy/HateMonitors-HASOC .", "published": "2019-09-27 12:15:58", "link": "http://arxiv.org/abs/1909.12642v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "End-to-End Code-Switching ASR for Low-Resourced Language Pairs", "abstract": "Despite the significant progress in end-to-end (E2E) automatic speech\nrecognition (ASR), E2E ASR for low resourced code-switching (CS) speech has not\nbeen well studied. In this work, we describe an E2E ASR pipeline for the\nrecognition of CS speech in which a low-resourced language is mixed with a high\nresourced language. Low-resourcedness in acoustic data hinders the performance\nof E2E ASR systems more severely than the conventional ASR systems.~To mitigate\nthis problem in the transcription of archives with code-switching Frisian-Dutch\nspeech, we integrate a designated decoding scheme and perform rescoring with\nneural network-based language models to enable better utilization of the\navailable textual resources. We first incorporate a multi-graph decoding\napproach which creates parallel search spaces for each monolingual and mixed\nrecognition tasks to maximize the utilization of the textual resources from\neach language. Further, language model rescoring is performed using a recurrent\nneural network pre-trained with cross-lingual embedding and further adapted\nwith the limited amount of in-domain CS text. The ASR experiments demonstrate\nthe effectiveness of the described techniques in improving the recognition\nperformance of an E2E CS ASR system in a low-resourced scenario.", "published": "2019-09-27 13:38:20", "link": "http://arxiv.org/abs/1909.12681v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "On the use of BERT for Neural Machine Translation", "abstract": "Exploiting large pretrained models for various NMT tasks have gained a lot of\nvisibility recently. In this work we study how BERT pretrained models could be\nexploited for supervised Neural Machine Translation. We compare various ways to\nintegrate pretrained BERT model with NMT model and study the impact of the\nmonolingual data used for BERT training on the final translation quality. We\nuse WMT-14 English-German, IWSLT15 English-German and IWSLT14 English-Russian\ndatasets for these experiments. In addition to standard task test set\nevaluation, we perform evaluation on out-of-domain test sets and noise injected\ntest sets, in order to assess how BERT pretrained representations affect model\nrobustness.", "published": "2019-09-27 15:23:17", "link": "http://arxiv.org/abs/1909.12744v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reweighted Proximal Pruning for Large-Scale Language Representation", "abstract": "Recently, pre-trained language representation flourishes as the mainstay of\nthe natural language understanding community, e.g., BERT. These pre-trained\nlanguage representations can create state-of-the-art results on a wide range of\ndownstream tasks. Along with continuous significant performance improvement,\nthe size and complexity of these pre-trained neural models continue to increase\nrapidly. Is it possible to compress these large-scale language representation\nmodels? How will the pruned language representation affect the downstream\nmulti-task transfer learning objectives? In this paper, we propose Reweighted\nProximal Pruning (RPP), a new pruning method specifically designed for a\nlarge-scale language representation model. Through experiments on SQuAD and the\nGLUE benchmark suite, we show that proximal pruned BERT keeps high accuracy for\nboth the pre-training task and the downstream multiple fine-tuning tasks at\nhigh prune ratio. RPP provides a new perspective to help us analyze what\nlarge-scale language representation might learn. Additionally, RPP makes it\npossible to deploy a large state-of-the-art language representation model such\nas BERT on a series of distinct devices (e.g., online servers, mobile phones,\nand edge devices).", "published": "2019-09-27 04:10:10", "link": "http://arxiv.org/abs/1909.12486v2", "categories": ["cs.LG", "cs.CL", "cs.NE", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Multi-Modal Citizen Science: From Disambiguation to Transcription of\n  Classical Literature", "abstract": "The engagement of citizens in the research projects, including Digital\nHumanities projects, has risen in prominence in recent years. This type of\nengagement not only leads to incidental learning of participants but also\nindicates the added value of corpus enrichment via different types of\nannotations undertaken by users generating so-called smart texts. Our work\nfocuses on the continuous task of adding new layers of annotation to Classical\nLiterature. We aim to provide more extensive tools for readers of smart texts,\nenhancing their reading comprehension and at the same time empowering the\nlanguage learning by introducing intellectual tasks, i.e., linking, tagging,\nand disambiguation. The current study adds a new mode of annotation-audio\nannotations-to the extensively annotated corpus of poetry by the Persian poet\nHafiz. By proposing tasks with three different difficulty levels, we estimate\nthe users' ability of providing correct annotations in order to rate their\nanswers in further stages of the project, where no ground truth data is\navailable. While proficiency in Persian is beneficial, annotators with no\nknowledge of Persian are also able to add annotations to the corpus.", "published": "2019-09-27 11:19:21", "link": "http://arxiv.org/abs/1909.12622v1", "categories": ["cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "A Constructive Prediction of the Generalization Error Across Scales", "abstract": "The dependency of the generalization error of neural networks on model and\ndataset size is of critical importance both in practice and for understanding\nthe theory of neural networks. Nevertheless, the functional form of this\ndependency remains elusive. In this work, we present a functional form which\napproximates well the generalization error in practice. Capitalizing on the\nsuccessful concept of model scaling (e.g., width, depth), we are able to\nsimultaneously construct such a form and specify the exact models which can\nattain it across model/data scales. Our construction follows insights obtained\nfrom observations conducted over a range of model/data scales, in various model\ntypes and datasets, in vision and language tasks. We show that the form both\nfits the observations well across scales, and provides accurate predictions\nfrom small- to large-scale models and data.", "published": "2019-09-27 13:27:53", "link": "http://arxiv.org/abs/1909.12673v2", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Automatically Learning Data Augmentation Policies for Dialogue Tasks", "abstract": "Automatic data augmentation (AutoAugment) (Cubuk et al., 2019) searches for\noptimal perturbation policies via a controller trained using performance\nrewards of a sampled policy on the target task, hence reducing data-level model\nbias. While being a powerful algorithm, their work has focused on computer\nvision tasks, where it is comparatively easy to apply imperceptible\nperturbations without changing an image's semantic meaning. In our work, we\nadapt AutoAugment to automatically discover effective perturbation policies for\nnatural language processing (NLP) tasks such as dialogue generation. We start\nwith a pool of atomic operations that apply subtle semantic-preserving\nperturbations to the source inputs of a dialogue task (e.g., different POS-tag\ntypes of stopword dropout, grammatical errors, and paraphrasing). Next, we\nallow the controller to learn more complex augmentation policies by searching\nover the space of the various combinations of these atomic operations.\nMoreover, we also explore conditioning the controller on the source inputs of\nthe target task, since certain strategies may not apply to inputs that do not\ncontain that strategy's required linguistic features. Empirically, we\ndemonstrate that both our input-agnostic and input-aware controllers discover\nuseful data augmentation policies, and achieve significant improvements over\nthe previous state-of-the-art, including trained on manually-designed policies.", "published": "2019-09-27 18:40:23", "link": "http://arxiv.org/abs/1909.12868v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Urban Sound Tagging using Convolutional Neural Networks", "abstract": "In this paper, we propose a framework for environmental sound classification\nin a low-data context (less than 100 labeled examples per class). We show that\nusing pre-trained image classification models along with the usage of data\naugmentation techniques results in higher performance over alternative\napproaches. We applied this system to the task of Urban Sound Tagging, part of\nthe DCASE 2019. The objective was to label different sources of noise from raw\naudio data. A modified form of MobileNetV2, a convolutional neural network\n(CNN) model was trained to classify both coarse and fine tags jointly. The\nproposed model uses log-scaled Mel-spectrogram as the representation format for\nthe audio data. Mixup, Random erasing, scaling, and shifting are used as data\naugmentation techniques. A second model that uses scaled labels was built to\naccount for human errors in the annotations. The proposed model achieved the\nfirst rank on the leaderboard with Micro-AUPRC values of 0.751 and 0.860 on\nfine and coarse tags, respectively.", "published": "2019-09-27 14:14:58", "link": "http://arxiv.org/abs/1909.12699v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning to Have an Ear for Face Super-Resolution", "abstract": "We propose a novel method to use both audio and a low-resolution image to\nperform extreme face super-resolution (a 16x increase of the input size). When\nthe resolution of the input image is very low (e.g., 8x8 pixels), the loss of\ninformation is so dire that important details of the original identity have\nbeen lost and audio can aid the recovery of a plausible high-resolution image.\nIn fact, audio carries information about facial attributes, such as gender and\nage. To combine the aural and visual modalities, we propose a method to first\nbuild the latent representations of a face from the lone audio track and then\nfrom the lone low-resolution image. We then train a network to fuse these two\nrepresentations. We show experimentally that audio can assist in recovering\nattributes such as the gender, the age and the identity, and thus improve the\ncorrectness of the high-resolution image reconstruction process. Our procedure\ndoes not make use of human annotation and thus can be easily trained with\nexisting video datasets. Moreover, we show that our model builds a factorized\nrepresentation of images and audio as it allows one to mix low-resolution\nimages and audio from different videos and to generate realistic faces with\nsemantically meaningful combinations.", "published": "2019-09-27 16:28:55", "link": "http://arxiv.org/abs/1909.12780v3", "categories": ["cs.CV", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
