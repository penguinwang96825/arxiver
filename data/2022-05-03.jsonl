{"title": "SemAttack: Natural Textual Attacks via Different Semantic Spaces", "abstract": "Recent studies show that pre-trained language models (LMs) are vulnerable to\ntextual adversarial attacks. However, existing attack methods either suffer\nfrom low attack success rates or fail to search efficiently in the\nexponentially large perturbation space. We propose an efficient and effective\nframework SemAttack to generate natural adversarial text by constructing\ndifferent semantic perturbation functions. In particular, SemAttack optimizes\nthe generated perturbations constrained on generic semantic spaces, including\ntypo space, knowledge space (e.g., WordNet), contextualized semantic space\n(e.g., the embedding space of BERT clusterings), or the combination of these\nspaces. Thus, the generated adversarial texts are more semantically close to\nthe original inputs. Extensive experiments reveal that state-of-the-art (SOTA)\nlarge-scale LMs (e.g., DeBERTa-v2) and defense strategies (e.g., FreeLB) are\nstill vulnerable to SemAttack. We further demonstrate that SemAttack is general\nand able to generate natural adversarial texts for different languages (e.g.,\nEnglish and Chinese) with high attack success rates. Human evaluations also\nconfirm that our generated adversarial texts are natural and barely affect\nhuman performance. Our code is publicly available at\nhttps://github.com/AI-secure/SemAttack.", "published": "2022-05-03 03:44:03", "link": "http://arxiv.org/abs/2205.01287v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Textual Entailment for Event Argument Extraction: Zero- and Few-Shot\n  with Multi-Source Learning", "abstract": "Recent work has shown that NLP tasks such as Relation Extraction (RE) can be\nrecasted as Textual Entailment tasks using verbalizations, with strong\nperformance in zero-shot and few-shot settings thanks to pre-trained entailment\nmodels. The fact that relations in current RE datasets are easily verbalized\ncasts doubts on whether entailment would be effective in more complex tasks. In\nthis work we show that entailment is also effective in Event Argument\nExtraction (EAE), reducing the need of manual annotation to 50% and 20% in ACE\nand WikiEvents respectively, while achieving the same performance as with full\ntraining. More importantly, we show that recasting EAE as entailment alleviates\nthe dependency on schemas, which has been a road-block for transferring\nannotations between domains. Thanks to the entailment, the multi-source\ntransfer between ACE and WikiEvents further reduces annotation down to 10% and\n5% (respectively) of the full training without transfer. Our analysis shows\nthat the key to good results is the use of several entailment datasets to\npre-train the entailment model. Similar to previous approaches, our method\nrequires a small amount of effort for manual verbalization: only less than 15\nminutes per event argument type is needed, and comparable results can be\nachieved with users with different level of expertise.", "published": "2022-05-03 08:53:55", "link": "http://arxiv.org/abs/2205.01376v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Kompetencer: Fine-grained Skill Classification in Danish Job Postings\n  via Distant Supervision and Transfer Learning", "abstract": "Skill Classification (SC) is the task of classifying job competences from job\npostings. This work is the first in SC applied to Danish job vacancy data. We\nrelease the first Danish job posting dataset: Kompetencer (en: competences),\nannotated for nested spans of competences. To improve upon coarse-grained\nannotations, we make use of The European Skills, Competences, Qualifications\nand Occupations (ESCO; le Vrang et al., 2014) taxonomy API to obtain\nfine-grained labels via distant supervision. We study two setups: The zero-shot\nand few-shot classification setting. We fine-tune English-based models and\nRemBERT (Chung et al., 2020) and compare them to in-language Danish models. Our\nresults show RemBERT significantly outperforms all other models in both the\nzero-shot and the few-shot setting.", "published": "2022-05-03 09:13:55", "link": "http://arxiv.org/abs/2205.01381v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exact Paired-Permutation Testing for Structured Test Statistics", "abstract": "Significance testing -- especially the paired-permutation test -- has played\na vital role in developing NLP systems to provide confidence that the\ndifference in performance between two systems (i.e., the test statistic) is not\ndue to luck. However, practitioners rely on Monte Carlo approximation to\nperform this test due to a lack of a suitable exact algorithm. In this paper,\nwe provide an efficient exact algorithm for the paired-permutation test for a\nfamily of structured test statistics. Our algorithm runs in $\\mathcal{O}(GN\n(\\log GN )(\\log N ))$ time where $N$ is the dataset size and $G$ is the range\nof the test statistic. We found that our exact algorithm was $10$x faster than\nthe Monte Carlo approximation with $20000$ samples on a common dataset.", "published": "2022-05-03 11:00:59", "link": "http://arxiv.org/abs/2205.01416v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inducing and Using Alignments for Transition-based AMR Parsing", "abstract": "Transition-based parsers for Abstract Meaning Representation (AMR) rely on\nnode-to-word alignments. These alignments are learned separately from parser\ntraining and require a complex pipeline of rule-based components,\npre-processing, and post-processing to satisfy domain-specific constraints.\nParsers also train on a point-estimate of the alignment pipeline, neglecting\nthe uncertainty due to the inherent ambiguity of alignment. In this work we\nexplore two avenues for overcoming these limitations. First, we propose a\nneural aligner for AMR that learns node-to-word alignments without relying on\ncomplex pipelines. We subsequently explore a tighter integration of aligner and\nparser training by considering a distribution over oracle action sequences\narising from aligner uncertainty. Empirical results show this approach leads to\nmore accurate alignments and generalization better from the AMR2.0 to AMR3.0\ncorpora. We attain a new state-of-the art for gold-only trained models,\nmatching silver-trained performance without the need for beam search on AMR3.0.", "published": "2022-05-03 12:58:36", "link": "http://arxiv.org/abs/2205.01464v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Diversity in Dialogue with Natural Language Inference", "abstract": "Generating diverse, interesting responses to chitchat conversations is a\nproblem for neural conversational agents. This paper makes two substantial\ncontributions to improving diversity in dialogue generation. First, we propose\na novel metric which uses Natural Language Inference (NLI) to measure the\nsemantic diversity of a set of model responses for a conversation. We evaluate\nthis metric using an established framework (Tevet and Berant, 2021) and find\nstrong evidence indicating NLI Diversity is correlated with semantic diversity.\nSpecifically, we show that the contradiction relation is more useful than the\nneutral relation for measuring this diversity and that incorporating the NLI\nmodel's confidence achieves state-of-the-art results. Second, we demonstrate\nhow to iteratively improve the semantic diversity of a sampled set of responses\nvia a new generation procedure called Diversity Threshold Generation, which\nresults in an average 137% increase in NLI Diversity compared to standard\ngeneration procedures.", "published": "2022-05-03 13:56:32", "link": "http://arxiv.org/abs/2205.01497v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ElitePLM: An Empirical Study on General Language Ability Evaluation of\n  Pretrained Language Models", "abstract": "Nowadays, pretrained language models (PLMs) have dominated the majority of\nNLP tasks. While, little research has been conducted on systematically\nevaluating the language abilities of PLMs. In this paper, we present a\nlarge-scale empirical study on general language ability evaluation of PLMs\n(ElitePLM). In our study, we design four evaluation dimensions, i.e. memory,\ncomprehension, reasoning, and composition, to measure ten widely-used PLMs\nwithin five categories. Our empirical results demonstrate that: (1) PLMs with\nvarying training objectives and strategies are good at different ability tests;\n(2) fine-tuning PLMs in downstream tasks is usually sensitive to the data size\nand distribution; (3) PLMs have excellent transferability between similar\ntasks. Moreover, the prediction results of PLMs in our experiments are released\nas an open resource for more deep and detailed analysis on the language\nabilities of PLMs. This paper can guide the future work to select, apply, and\ndesign PLMs for specific tasks. We have made all the details of experiments\npublicly available at https://github.com/RUCAIBox/ElitePLM.", "published": "2022-05-03 14:18:10", "link": "http://arxiv.org/abs/2205.01523v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SUBS: Subtree Substitution for Compositional Semantic Parsing", "abstract": "Although sequence-to-sequence models often achieve good performance in\nsemantic parsing for i.i.d. data, their performance is still inferior in\ncompositional generalization. Several data augmentation methods have been\nproposed to alleviate this problem. However, prior work only leveraged\nsuperficial grammar or rules for data augmentation, which resulted in limited\nimprovement. We propose to use subtree substitution for compositional data\naugmentation, where we consider subtrees with similar semantic functions as\nexchangeable. Our experiments showed that such augmented data led to\nsignificantly better performance on SCAN and GeoQuery, and reached new SOTA on\ncompositional split of GeoQuery.", "published": "2022-05-03 14:47:35", "link": "http://arxiv.org/abs/2205.01538v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Transfer Prompts for Text Generation", "abstract": "Pretrained language models (PLMs) have made remarkable progress in text\ngeneration tasks via fine-tuning. While, it is challenging to fine-tune PLMs in\na data-scarce situation. Therefore, it is non-trivial to develop a general and\nlightweight model that can adapt to various text generation tasks based on\nPLMs. To fulfill this purpose, the recent prompt-based learning offers a\npotential solution. In this paper, we improve this technique and propose a\nnovel prompt-based method (PTG) for text generation in a transferable setting.\nFirst, PTG learns a set of source prompts for various source generation tasks\nand then transfers these prompts as target prompts to perform target generation\ntasks. To consider both task- and instance-level information, we design an\nadaptive attention mechanism to derive the target prompts. For each data\ninstance, PTG learns a specific target prompt by attending to highly relevant\nsource prompts. In extensive experiments, PTG yields competitive or better\nresults than fine-tuning methods. We release our source prompts as an open\nresource, where users can add or reuse them to improve new text generation\ntasks for future research. Code and data can be available at\nhttps://github.com/RUCAIBox/Transfer-Prompts-for-Text-Generation.", "published": "2022-05-03 14:53:48", "link": "http://arxiv.org/abs/2205.01543v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training Mixed-Domain Translation Models via Federated Learning", "abstract": "Training mixed-domain translation models is a complex task that demands\ntailored architectures and costly data preparation techniques. In this work, we\nleverage federated learning (FL) in order to tackle the problem. Our\ninvestigation demonstrates that with slight modifications in the training\nprocess, neural machine translation (NMT) engines can be easily adapted when an\nFL-based aggregation is applied to fuse different domains. Experimental results\nalso show that engines built via FL are able to perform on par with\nstate-of-the-art baselines that rely on centralized training techniques. We\nevaluate our hypothesis in the presence of five datasets with different sizes,\nfrom different domains, to translate from German into English and discuss how\nFL and NMT can mutually benefit from each other. In addition to providing\nbenchmarking results on the union of FL and NMT, we also propose a novel\ntechnique to dynamically control the communication bandwidth by selecting\nimpactful parameters during FL updates. This is a significant achievement\nconsidering the large size of NMT engines that need to be exchanged between FL\nparties.", "published": "2022-05-03 15:16:51", "link": "http://arxiv.org/abs/2205.01557v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CTM -- A Model for Large-Scale Multi-View Tweet Topic Classification", "abstract": "Automatically associating social media posts with topics is an important\nprerequisite for effective search and recommendation on many social media\nplatforms. However, topic classification of such posts is quite challenging\nbecause of (a) a large topic space (b) short text with weak topical cues, and\n(c) multiple topic associations per post. In contrast to most prior work which\nonly focuses on post classification into a small number of topics ($10$-$20$),\nwe consider the task of large-scale topic classification in the context of\nTwitter where the topic space is $10$ times larger with potentially multiple\ntopic associations per Tweet. We address the challenges above by proposing a\nnovel neural model, CTM that (a) supports a large topic space of $300$ topics\nand (b) takes a holistic approach to tweet content modeling -- leveraging\nmulti-modal content, author context, and deeper semantic cues in the Tweet. Our\nmethod offers an effective way to classify Tweets into topics at scale by\nyielding superior performance to other approaches (a relative lift of\n$\\mathbf{20}\\%$ in median average precision score) and has been successfully\ndeployed in production at Twitter.", "published": "2022-05-03 16:32:09", "link": "http://arxiv.org/abs/2205.01603v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unifying the Convergences in Multilingual Neural Machine Translation", "abstract": "Although all-in-one-model multilingual neural machine translation\n(multilingual NMT) has achieved remarkable progress, the convergence\ninconsistency in the joint training is ignored, i.e., different language pairs\nreaching convergence in different epochs. This leads to the trained MNMT model\nover-fitting low-resource language translations while under-fitting\nhigh-resource ones. In this paper, we propose a novel training strategy named\nLSSD (Language-Specific Self-Distillation), which can alleviate the convergence\ninconsistency and help MNMT models achieve the best performance on each\nlanguage pair simultaneously. Specifically, LSSD picks up language-specific\nbest checkpoints for each language pair to teach the current model on the fly.\nFurthermore, we systematically explore three sample-level manipulations of\nknowledge transferring. Experimental results on three datasets show that LSSD\nobtains consistent improvements towards all language pairs and achieves the\nstate-of-the-art.", "published": "2022-05-03 16:57:40", "link": "http://arxiv.org/abs/2205.01620v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving In-Context Few-Shot Learning via Self-Supervised Training", "abstract": "Self-supervised pretraining has made few-shot learning possible for many NLP\ntasks. But the pretraining objectives are not typically adapted specifically\nfor in-context few-shot learning. In this paper, we propose to use\nself-supervision in an intermediate training stage between pretraining and\ndownstream few-shot usage with the goal to teach the model to perform\nin-context few shot learning. We propose and evaluate four self-supervised\nobjectives on two benchmarks. We find that the intermediate self-supervision\nstage produces models that outperform strong baselines. Ablation study shows\nthat several factors affect the downstream performance, such as the amount of\ntraining data and the diversity of the self-supervised objectives.\nHuman-annotated cross-task supervision and self-supervision are complementary.\nQualitative analysis suggests that the self-supervised-trained models are\nbetter at following task requirements.", "published": "2022-05-03 18:01:07", "link": "http://arxiv.org/abs/2205.01703v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Themes of Revenge: Automatic Identification of Vengeful Content in\n  Textual Data", "abstract": "Revenge is a powerful motivating force reported to underlie the behavior of\nvarious solo perpetrators, from school shooters to right wing terrorists. In\nthis paper, we develop an automated methodology for identifying vengeful themes\nin textual data. Testing the model on four datasets (vengeful texts from social\nmedia, school shooters, Right Wing terrorist and Islamic terrorists), we\npresent promising results, even when the methodology is tested on extremely\nimbalanced datasets. The paper not only presents a simple and powerful\nmethodology that may be used for the screening of solo perpetrators but also\nvalidate the simple theoretical model of revenge.", "published": "2022-05-03 19:01:28", "link": "http://arxiv.org/abs/2205.01731v1", "categories": ["cs.CL", "J.4"], "primary_category": "cs.CL"}
{"title": "Mixed-effects transformers for hierarchical adaptation", "abstract": "Language use differs dramatically from context to context. To some degree,\nmodern language models like GPT-3 are able to account for such variance by\nconditioning on a string of previous input text, or prompt. Yet prompting is\nineffective when contexts are sparse, out-of-sample, or extra-textual; for\ninstance, accounting for when and where the text was produced or who produced\nit. In this paper, we introduce the mixed-effects transformer (MET), a novel\napproach for learning hierarchically-structured prefixes -- lightweight modules\nprepended to the input -- to account for structured variation. Specifically, we\nshow how the popular class of mixed-effects models may be extended to\ntransformer-based architectures using a regularized prefix-tuning procedure\nwith dropout. We evaluate this approach on several domain-adaptation\nbenchmarks, finding that it efficiently adapts to novel contexts with minimal\ndata while still effectively generalizing to unseen contexts.", "published": "2022-05-03 19:34:15", "link": "http://arxiv.org/abs/2205.01749v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Deep Taylor Decomposition for Reliability Assessment in the\n  Wild", "abstract": "We argue that we need to evaluate model interpretability methods 'in the\nwild', i.e., in situations where professionals make critical decisions, and\nmodels can potentially assist them. We present an in-the-wild evaluation of\ntoken attribution based on Deep Taylor Decomposition, with professional\njournalists performing reliability assessments. We find that using this method\nin conjunction with RoBERTa-Large, fine-tuned on the Gossip Corpus, led to\nfaster and better human decision-making, as well as a more critical attitude\ntoward news sources among the journalists. We present a comparison of human and\nmodel rationales, as well as a qualitative analysis of the journalists'\nexperiences with machine-in-the-loop decision making.", "published": "2022-05-03 12:59:21", "link": "http://arxiv.org/abs/2206.02661v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Embedding Hallucination for Few-Shot Language Fine-tuning", "abstract": "Few-shot language learners adapt knowledge from a pre-trained model to\nrecognize novel classes from a few-labeled sentences. In such settings,\nfine-tuning a pre-trained language model can cause severe over-fitting. In this\npaper, we propose an Embedding Hallucination (EmbedHalluc) method, which\ngenerates auxiliary embedding-label pairs to expand the fine-tuning dataset.\nThe hallucinator is trained by playing an adversarial game with the\ndiscriminator, such that the hallucinated embedding is indiscriminative to the\nreal ones in the fine-tuning dataset. By training with the extended dataset,\nthe language learner effectively learns from the diverse hallucinated\nembeddings to overcome the over-fitting issue. Experiments demonstrate that our\nproposed method is effective in a wide range of language tasks, outperforming\ncurrent fine-tuning methods. Further, we show that EmbedHalluc outperforms\nother methods that address this over-fitting problem, such as common data\naugmentation, semi-supervised pseudo-labeling, and regularization. The code\nwill be made available at: https://github.com/yiren-jian/EmbedHalluc.", "published": "2022-05-03 04:55:50", "link": "http://arxiv.org/abs/2205.01307v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Contrastive Learning for Prompt-Based Few-Shot Language Learners", "abstract": "The impressive performance of GPT-3 using natural language prompts and\nin-context learning has inspired work on better fine-tuning of moderately-sized\nmodels under this paradigm. Following this line of work, we present a\ncontrastive learning framework that clusters inputs from the same class for\nbetter generality of models trained with only limited examples. Specifically,\nwe propose a supervised contrastive framework that clusters inputs from the\nsame class under different augmented \"views\" and repel the ones from different\nclasses. We create different \"views\" of an example by appending it with\ndifferent language prompts and contextual demonstrations. Combining a\ncontrastive loss with the standard masked language modeling (MLM) loss in\nprompt-based few-shot learners, the experimental results show that our method\ncan improve over the state-of-the-art methods in a diverse set of 15 language\ntasks. Our framework makes minimal assumptions on the task or the base model,\nand can be applied to many recent methods with little modification. The code\nwill be made available at: https://github.com/yiren-jian/LM-SupCon.", "published": "2022-05-03 04:56:45", "link": "http://arxiv.org/abs/2205.01308v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Finding patterns in Knowledge Attribution for Transformers", "abstract": "We analyze the Knowledge Neurons framework for the attribution of factual and\nrelational knowledge to particular neurons in the transformer network. We use a\n12-layer multi-lingual BERT model for our experiments. Our study reveals\nvarious interesting phenomena. We observe that mostly factual knowledge can be\nattributed to middle and higher layers of the network($\\ge 6$). Further\nanalysis reveals that the middle layers($6-9$) are mostly responsible for\nrelational information, which is further refined into actual factual knowledge\nor the \"correct answer\" in the last few layers($10-12$). Our experiments also\nshow that the model handles prompts in different languages, but representing\nthe same fact, similarly, providing further evidence for effectiveness of\nmulti-lingual pre-training. Applying the attribution scheme for grammatical\nknowledge, we find that grammatical knowledge is far more dispersed among the\nneurons than factual knowledge.", "published": "2022-05-03 08:30:51", "link": "http://arxiv.org/abs/2205.01366v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hidden behind the obvious: misleading keywords and implicitly abusive\n  language on social media", "abstract": "While social media offers freedom of self-expression, abusive language carry\nsignificant negative social impact. Driven by the importance of the issue,\nresearch in the automated detection of abusive language has witnessed growth\nand improvement. However, these detection models display a reliance on strongly\nindicative keywords, such as slurs and profanity. This means that they can\nfalsely (1a) miss abuse without such keywords or (1b) flag non-abuse with such\nkeywords, and that (2) they perform poorly on unseen data. Despite the\nrecognition of these problems, gaps and inconsistencies remain in the\nliterature. In this study, we analyse the impact of keywords from dataset\nconstruction to model behaviour in detail, with a focus on how models make\nmistakes on (1a) and (1b), and how (1a) and (1b) interact with (2). Through the\nanalysis, we provide suggestions for future research to address all three\nproblems.", "published": "2022-05-03 08:46:44", "link": "http://arxiv.org/abs/2205.01374v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "BasqueParl: A Bilingual Corpus of Basque Parliamentary Transcriptions", "abstract": "Parliamentary transcripts provide a valuable resource to understand the\nreality and know about the most important facts that occur over time in our\nsocieties. Furthermore, the political debates captured in these transcripts\nfacilitate research on political discourse from a computational social science\nperspective. In this paper we release the first version of a newly compiled\ncorpus from Basque parliamentary transcripts. The corpus is characterized by\nheavy Basque-Spanish code-switching, and represents an interesting resource to\nstudy political discourse in contrasting languages such as Basque and Spanish.\nWe enrich the corpus with metadata related to relevant attributes of the\nspeakers and speeches (language, gender, party...) and process the text to\nobtain named entities and lemmas. The obtained metadata is then used to perform\na detailed corpus analysis which provides interesting insights about the\nlanguage use of the Basque political representatives across time, parties and\ngender.", "published": "2022-05-03 14:02:24", "link": "http://arxiv.org/abs/2205.01506v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficient Fine-Tuning of BERT Models on the Edge", "abstract": "Resource-constrained devices are increasingly the deployment targets of\nmachine learning applications. Static models, however, do not always suffice\nfor dynamic environments. On-device training of models allows for quick\nadaptability to new scenarios. With the increasing size of deep neural\nnetworks, as noted with the likes of BERT and other natural language processing\nmodels, comes increased resource requirements, namely memory, computation,\nenergy, and time. Furthermore, training is far more resource intensive than\ninference. Resource-constrained on-device learning is thus doubly difficult,\nespecially with large BERT-like models. By reducing the memory usage of\nfine-tuning, pre-trained BERT models can become efficient enough to fine-tune\non resource-constrained devices. We propose Freeze And Reconfigure (FAR), a\nmemory-efficient training regime for BERT-like models that reduces the memory\nusage of activation maps during fine-tuning by avoiding unnecessary parameter\nupdates. FAR reduces fine-tuning time on the DistilBERT model and CoLA dataset\nby 30%, and time spent on memory operations by 47%. More broadly, reductions in\nmetric performance on the GLUE and SQuAD datasets are around 1% on average.", "published": "2022-05-03 14:51:53", "link": "http://arxiv.org/abs/2205.01541v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Adaptable Adapters", "abstract": "State-of-the-art pretrained NLP models contain a hundred million to trillion\nparameters. Adapters provide a parameter-efficient alternative for the full\nfinetuning in which we can only finetune lightweight neural network layers on\ntop of pretrained weights. Adapter layers are initialized randomly. However,\nexisting work uses the same adapter architecture -- i.e., the same adapter\nlayer on top of each layer of the pretrained model -- for every dataset,\nregardless of the properties of the dataset or the amount of available training\ndata. In this work, we introduce adaptable adapters that contain (1) learning\ndifferent activation functions for different layers and different input data,\nand (2) a learnable switch to select and only use the beneficial adapter\nlayers. We show that adaptable adapters achieve on-par performances with the\nstandard adapter architecture while using a considerably smaller number of\nadapter layers. In addition, we show that the selected adapter architecture by\nadaptable adapters transfers well across different data settings and similar\ntasks. We propose to use adaptable adapters for designing efficient and\neffective adapter architectures. The resulting adapters (a) contain about 50%\nof the learning parameters of the standard adapter and are therefore more\nefficient at training and inference, and require less storage space, and (b)\nachieve considerably higher performances in low-data settings.", "published": "2022-05-03 14:59:27", "link": "http://arxiv.org/abs/2205.01549v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SparCAssist: A Model Risk Assessment Assistant Based on Sparse Generated\n  Counterfactuals", "abstract": "We introduce SparcAssist, a general-purpose risk assessment tool for the\nmachine learning models trained for language tasks. It evaluates models' risk\nby inspecting their behavior on counterfactuals, namely out-of-distribution\ninstances generated based on the given data instance. The counterfactuals are\ngenerated by replacing tokens in rational subsequences identified by ExPred,\nwhile the replacements are retrieved using HotFlip or\nMasked-Language-Model-based algorithms. The main purpose of our system is to\nhelp the human annotators to assess the model's risk on deployment. The\ncounterfactual instances generated during the assessment are the by-product and\ncan be used to train more robust NLP models in the future.", "published": "2022-05-03 16:10:45", "link": "http://arxiv.org/abs/2205.01588v1", "categories": ["cs.CL", "cs.AI", "I.2.m"], "primary_category": "cs.CL"}
{"title": "Quiz Design Task: Helping Teachers Create Quizzes with Automated\n  Question Generation", "abstract": "Question generation (QGen) models are often evaluated with standardized NLG\nmetrics that are based on n-gram overlap. In this paper, we measure whether\nthese metric improvements translate to gains in a practical setting, focusing\non the use case of helping teachers automate the generation of reading\ncomprehension quizzes. In our study, teachers building a quiz receive question\nsuggestions, which they can either accept or refuse with a reason. Even though\nwe find that recent progress in QGen leads to a significant increase in\nquestion acceptance rates, there is still large room for improvement, with the\nbest model having only 68.4% of its questions accepted by the ten teachers who\nparticipated in our study. We then leverage the annotations we collected to\nanalyze standard NLG metrics and find that model performance has reached\nprojected upper-bounds, suggesting new automatic metrics are needed to guide\nQGen research forward.", "published": "2022-05-03 18:59:03", "link": "http://arxiv.org/abs/2205.01730v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "XLTime: A Cross-Lingual Knowledge Transfer Framework for Temporal\n  Expression Extraction", "abstract": "Temporal Expression Extraction (TEE) is essential for understanding time in\nnatural language. It has applications in Natural Language Processing (NLP)\ntasks such as question answering, information retrieval, and causal inference.\nTo date, work in this area has mostly focused on English as there is a scarcity\nof labeled data for other languages. We propose XLTime, a novel framework for\nmultilingual TEE. XLTime works on top of pre-trained language models and\nleverages multi-task learning to prompt cross-language knowledge transfer both\nfrom English and within the non-English languages. XLTime alleviates problems\ncaused by a shortage of data in the target language. We apply XLTime with\ndifferent language models and show that it outperforms the previous automatic\nSOTA methods on French, Spanish, Portuguese, and Basque, by large margins.\nXLTime also closes the gap considerably on the handcrafted HeidelTime method.", "published": "2022-05-03 20:00:42", "link": "http://arxiv.org/abs/2205.01757v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scientific Explanation and Natural Language: A Unified\n  Epistemological-Linguistic Perspective for Explainable AI", "abstract": "A fundamental research goal for Explainable AI (XAI) is to build models that\nare capable of reasoning through the generation of natural language\nexplanations. However, the methodologies to design and evaluate\nexplanation-based inference models are still poorly informed by theoretical\naccounts on the nature of explanation. As an attempt to provide an\nepistemologically grounded characterisation for XAI, this paper focuses on the\nscientific domain, aiming to bridge the gap between theory and practice on the\nnotion of a scientific explanation. Specifically, the paper combines a detailed\nsurvey of the modern accounts of scientific explanation in Philosophy of\nScience with a systematic analysis of corpora of natural language explanations,\nclarifying the nature and function of explanatory arguments from both a\ntop-down (categorical) and a bottom-up (corpus-based) perspective. Through a\nmixture of quantitative and qualitative methodologies, the presented study\nallows deriving the following main conclusions: (1) Explanations cannot be\nentirely characterised in terms of inductive or deductive arguments as their\nmain function is to perform unification; (2) An explanation must cite causes\nand mechanisms that are responsible for the occurrence of the event to be\nexplained; (3) While natural language explanations possess an intrinsic\ncausal-mechanistic nature, they are not limited to causes and mechanisms, also\naccounting for pragmatic elements such as definitions, properties and taxonomic\nrelations; (4) Patterns of unification naturally emerge in corpora of\nexplanations even if not intentionally modelled; (5) Unification is realised\nthrough a process of abstraction, whose function is to provide the inference\nsubstrate for subsuming the event to be explained under recurring patterns and\nhigh-level regularities.", "published": "2022-05-03 22:31:42", "link": "http://arxiv.org/abs/2205.01809v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "A Holistic Framework for Analyzing the COVID-19 Vaccine Debate", "abstract": "The Covid-19 pandemic has led to infodemic of low quality information leading\nto poor health decisions. Combating the outcomes of this infodemic is not only\na question of identifying false claims, but also reasoning about the decisions\nindividuals make. In this work we propose a holistic analysis framework\nconnecting stance and reason analysis, and fine-grained entity level moral\nsentiment analysis. We study how to model the dependencies between the\ndifferent level of analysis and incorporate human insights into the learning\nprocess. Experiments show that our framework provides reliable predictions even\nin the low-supervision settings.", "published": "2022-05-03 23:32:53", "link": "http://arxiv.org/abs/2205.01817v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Open vs Closed-ended questions in attitudinal surveys -- comparing,\n  combining, and interpreting using natural language processing", "abstract": "To improve the traveling experience, researchers have been analyzing the role\nof attitudes in travel behavior modeling. Although most researchers use\nclosed-ended surveys, the appropriate method to measure attitudes is debatable.\nTopic Modeling could significantly reduce the time to extract information from\nopen-ended responses and eliminate subjective bias, thereby alleviating analyst\nconcerns. Our research uses Topic Modeling to extract information from\nopen-ended questions and compare its performance with closed-ended responses.\nFurthermore, some respondents might prefer answering questions using their\npreferred questionnaire type. So, we propose a modeling framework that allows\nrespondents to use their preferred questionnaire type to answer the survey and\nenable analysts to use the modeling frameworks of their choice to predict\nbehavior. We demonstrate this using a dataset collected from the USA that\nmeasures the intention to use Autonomous Vehicles for commute trips.\nRespondents were presented with alternative questionnaire versions (open- and\nclosed- ended). Since our objective was also to compare the performance of\nalternative questionnaire versions, the survey was designed to eliminate\ninfluences resulting from statements, behavioral framework, and the choice\nexperiment. Results indicate the suitability of using Topic Modeling to extract\ninformation from open-ended responses; however, the models estimated using the\nclosed-ended questions perform better compared to them. Besides, the proposed\nmodel performs better compared to the models used currently. Furthermore, our\nproposed framework will allow respondents to choose the questionnaire type to\nanswer, which could be particularly beneficial to them when using voice-based\nsurveys.", "published": "2022-05-03 06:01:03", "link": "http://arxiv.org/abs/2205.01317v1", "categories": ["econ.GN", "cs.CL", "cs.LG", "q-fin.EC"], "primary_category": "econ.GN"}
{"title": "Predicting Issue Types with seBERT", "abstract": "Pre-trained transformer models are the current state-of-the-art for natural\nlanguage models processing. seBERT is such a model, that was developed based on\nthe BERT architecture, but trained from scratch with software engineering data.\nWe fine-tuned this model for the NLBSE challenge for the task of issue type\nprediction. Our model dominates the baseline fastText for all three issue types\nin both recall and precisio} to achieve an overall F1-score of 85.7%, which is\nan increase of 4.1% over the baseline.", "published": "2022-05-03 06:47:13", "link": "http://arxiv.org/abs/2205.01335v1", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Data Determines Distributional Robustness in Contrastive Language Image\n  Pre-training (CLIP)", "abstract": "Contrastively trained language-image models such as CLIP, ALIGN, and BASIC\nhave demonstrated unprecedented robustness to multiple challenging natural\ndistribution shifts. Since these language-image models differ from previous\ntraining approaches in several ways, an important question is what causes the\nlarge robustness gains. We answer this question via a systematic experimental\ninvestigation. Concretely, we study five different possible causes for the\nrobustness gains: (i) the training set size, (ii) the training distribution,\n(iii) language supervision at training time, (iv) language supervision at test\ntime, and (v) the contrastive loss function. Our experiments show that the more\ndiverse training distribution is the main cause for the robustness gains, with\nthe other factors contributing little to no robustness. Beyond our experimental\nresults, we also introduce ImageNet-Captions, a version of ImageNet with\noriginal text annotations from Flickr, to enable further controlled experiments\nof language-image training.", "published": "2022-05-03 10:06:51", "link": "http://arxiv.org/abs/2205.01397v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Neural Language Taskonomy: Which NLP Tasks are the most Predictive of\n  fMRI Brain Activity?", "abstract": "Several popular Transformer based language models have been found to be\nsuccessful for text-driven brain encoding. However, existing literature\nleverages only pretrained text Transformer models and has not explored the\nefficacy of task-specific learned Transformer representations. In this work, we\nexplore transfer learning from representations learned for ten popular natural\nlanguage processing tasks (two syntactic and eight semantic) for predicting\nbrain responses from two diverse datasets: Pereira (subjects reading sentences\nfrom paragraphs) and Narratives (subjects listening to the spoken stories).\nEncoding models based on task features are used to predict activity in\ndifferent regions across the whole brain. Features from coreference resolution,\nNER, and shallow syntax parsing explain greater variance for the reading\nactivity. On the other hand, for the listening activity, tasks such as\nparaphrase generation, summarization, and natural language inference show\nbetter encoding performance. Experiments across all 10 task representations\nprovide the following cognitive insights: (i) language left hemisphere has\nhigher predictive brain activity versus language right hemisphere, (ii)\nposterior medial cortex, temporo-parieto-occipital junction, dorsal frontal\nlobe have higher correlation versus early auditory and auditory association\ncortex, (iii) syntactic and semantic tasks display a good predictive\nperformance across brain regions for reading and listening stimuli resp.", "published": "2022-05-03 10:23:08", "link": "http://arxiv.org/abs/2205.01404v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Meta Learning for Natural Language Processing: A Survey", "abstract": "Deep learning has been the mainstream technique in natural language\nprocessing (NLP) area. However, the techniques require many labeled data and\nare less generalizable across domains. Meta-learning is an arising field in\nmachine learning studying approaches to learn better learning algorithms.\nApproaches aim at improving algorithms in various aspects, including data\nefficiency and generalizability. Efficacy of approaches has been shown in many\nNLP tasks, but there is no systematic survey of these approaches in NLP, which\nhinders more researchers from joining the field. Our goal with this survey\npaper is to offer researchers pointers to relevant meta-learning works in NLP\nand attract more attention from the NLP community to drive future innovation.\nThis paper first introduces the general concepts of meta-learning and the\ncommon approaches. Then we summarize task construction settings and application\nof meta-learning for various NLP problems and review the development of\nmeta-learning in NLP community.", "published": "2022-05-03 13:58:38", "link": "http://arxiv.org/abs/2205.01500v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Comparison of Approaches for Imbalanced Classification Problems in the\n  Context of Retrieving Relevant Documents for an Analysis", "abstract": "One of the first steps in many text-based social science studies is to\nretrieve documents that are relevant for the analysis from large corpora of\notherwise irrelevant documents. The conventional approach in social science to\naddress this retrieval task is to apply a set of keywords and to consider those\ndocuments to be relevant that contain at least one of the keywords. But the\napplication of incomplete keyword lists risks drawing biased inferences. More\ncomplex and costly methods such as query expansion techniques, topic\nmodel-based classification rules, and active as well as passive supervised\nlearning could have the potential to more accurately separate relevant from\nirrelevant documents and thereby reduce the potential size of bias. Yet,\nwhether applying these more expensive approaches increases retrieval\nperformance compared to keyword lists at all, and if so, by how much, is\nunclear as a comparison of these approaches is lacking. This study closes this\ngap by comparing these methods across three retrieval tasks associated with a\ndata set of German tweets (Linder, 2017), the Social Bias Inference Corpus\n(SBIC) (Sap et al., 2020), and the Reuters-21578 corpus (Lewis, 1997). Results\nshow that query expansion techniques and topic model-based classification rules\nin most studied settings tend to decrease rather than increase retrieval\nperformance. Active supervised learning, however, if applied on a not too small\nset of labeled training instances (e.g. 1,000 documents), reaches a\nsubstantially higher retrieval performance than keyword lists.", "published": "2022-05-03 16:22:42", "link": "http://arxiv.org/abs/2205.01600v1", "categories": ["cs.IR", "cs.CL", "stat.AP", "stat.ML", "I.2.7; H.3.3"], "primary_category": "cs.IR"}
{"title": "Adversarial Training for High-Stakes Reliability", "abstract": "In the future, powerful AI systems may be deployed in high-stakes settings,\nwhere a single failure could be catastrophic. One technique for improving AI\nsafety in high-stakes settings is adversarial training, which uses an adversary\nto generate examples to train on in order to achieve better worst-case\nperformance.\n  In this work, we used a safe language generation task (``avoid injuries'') as\na testbed for achieving high reliability through adversarial training. We\ncreated a series of adversarial training techniques -- including a tool that\nassists human adversaries -- to find and eliminate failures in a classifier\nthat filters text completions suggested by a generator. In our task, we\ndetermined that we can set very conservative classifier thresholds without\nsignificantly impacting the quality of the filtered outputs. We found that\nadversarial training increased robustness to the adversarial attacks that we\ntrained on -- doubling the time for our contractors to find adversarial\nexamples both with our tool (from 13 to 26 minutes) and without (from 20 to 44\nminutes) -- without affecting in-distribution performance.\n  We hope to see further work in the high-stakes reliability setting, including\nmore powerful tools for enhancing human adversaries and better ways to measure\nhigh levels of reliability, until we can confidently rule out the possibility\nof catastrophic deployment-time failures of powerful models.", "published": "2022-05-03 17:50:06", "link": "http://arxiv.org/abs/2205.01663v5", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Don't sweat the small stuff, classify the rest: Sample Shielding to\n  protect text classifiers against adversarial attacks", "abstract": "Deep learning (DL) is being used extensively for text classification.\nHowever, researchers have demonstrated the vulnerability of such classifiers to\nadversarial attacks. Attackers modify the text in a way which misleads the\nclassifier while keeping the original meaning close to intact. State-of-the-art\n(SOTA) attack algorithms follow the general principle of making minimal changes\nto the text so as to not jeopardize semantics. Taking advantage of this we\npropose a novel and intuitive defense strategy called Sample Shielding. It is\nattacker and classifier agnostic, does not require any reconfiguration of the\nclassifier or external resources and is simple to implement. Essentially, we\nsample subsets of the input text, classify them and summarize these into a\nfinal decision. We shield three popular DL text classifiers with Sample\nShielding, test their resilience against four SOTA attackers across three\ndatasets in a realistic threat setting. Even when given the advantage of\nknowing about our shielding strategy the adversary's attack success rate is\n<=10% with only one exception and often < 5%. Additionally, Sample Shielding\nmaintains near original accuracy when applied to original texts. Crucially, we\nshow that the `make minimal changes' approach of SOTA attackers leads to\ncritical vulnerabilities that can be defended against with an intuitive\nsampling strategy.", "published": "2022-05-03 18:24:20", "link": "http://arxiv.org/abs/2205.01714v1", "categories": ["cs.CL", "cs.CR", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "On monoaural speech enhancement for automatic recognition of real noisy\n  speech using mixture invariant training", "abstract": "In this paper, we explore an improved framework to train a monoaural neural\nenhancement model for robust speech recognition. The designed training\nframework extends the existing mixture invariant training criterion to exploit\nboth unpaired clean speech and real noisy data. It is found that the unpaired\nclean speech is crucial to improve quality of separated speech from real noisy\nspeech. The proposed method also performs remixing of processed and unprocessed\nsignals to alleviate the processing artifacts. Experiments on the\nsingle-channel CHiME-3 real test sets show that the proposed method improves\nsignificantly in terms of speech recognition performance over the enhancement\nsystem trained either on the mismatched simulated data in a supervised fashion\nor on the matched real data in an unsupervised fashion. Between 16% and 39%\nrelative WER reduction has been achieved by the proposed system compared to the\nunprocessed signal using end-to-end and hybrid acoustic models without\nretraining on distorted data.", "published": "2022-05-03 19:37:58", "link": "http://arxiv.org/abs/2205.01751v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Zero-shot Sonnet Generation with Discourse-level Planning and Aesthetics\n  Features", "abstract": "Poetry generation, and creative language generation in general, usually\nsuffers from the lack of large training data. In this paper, we present a novel\nframework to generate sonnets that does not require training on poems. We\ndesign a hierarchical framework which plans the poem sketch before decoding.\nSpecifically, a content planning module is trained on non-poetic texts to\nobtain discourse-level coherence; then a rhyme module generates rhyme words and\na polishing module introduces imagery and similes for aesthetics purposes.\nFinally, we design a constrained decoding algorithm to impose the\nmeter-and-rhyme constraint of the generated sonnets. Automatic and human\nevaluation show that our multi-stage approach without training on poem corpora\ngenerates more coherent, poetic, and creative sonnets than several strong\nbaselines.", "published": "2022-05-03 23:44:28", "link": "http://arxiv.org/abs/2205.01821v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Detection of Propaganda Techniques in Visuo-Lingual Metaphor in Memes", "abstract": "The exponential rise of social media networks has allowed the production,\ndistribution, and consumption of data at a phenomenal rate. Moreover, the\nsocial media revolution has brought a unique phenomenon to social media\nplatforms called Internet memes. Internet memes are one of the most popular\ncontents used on social media, and they can be in the form of images with a\nwitty, catchy, or satirical text description. In this paper, we are dealing\nwith propaganda that is often seen in Internet memes in recent times.\nPropaganda is communication, which frequently includes psychological and\nrhetorical techniques to manipulate or influence an audience to act or respond\nas the propagandist wants. To detect propaganda in Internet memes, we propose a\nmultimodal deep learning fusion system that fuses the text and image feature\nrepresentations and outperforms individual models based solely on either text\nor image modalities.", "published": "2022-05-03 18:33:27", "link": "http://arxiv.org/abs/2205.02937v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Explain and Conquer: Personalised Text-based Reviews to Achieve\n  Transparency", "abstract": "There are many contexts in which dyadic data are present. Social networks are\na well-known example. In these contexts, pairs of elements are linked building\na network that reflects interactions. Explaining why these relationships are\nestablished is essential to obtain transparency, an increasingly important\nnotion. These explanations are often presented using text, thanks to the spread\nof the natural language understanding tasks. Our aim is to represent and\nexplain pairs established by any agent (e.g., a recommender system or a paid\npromotion mechanism), so that text-based personalisation is taken into account.\nWe have focused on the TripAdvisor platform, considering the applicability to\nother dyadic data contexts. The items are a subset of users and restaurants and\nthe interactions the reviews posted by these users. We propose the PTER\n(Personalised TExt-based Reviews) model. We predict, from the available reviews\nfor a given restaurant, those that fit to the specific user interactions. PTER\nleverages the BERT (Bidirectional Encoders Representations from Transformers)\ntransformer-encoder model. We customised a deep neural network following the\nfeature-based approach, presenting a LTR (Learning To Rank) downstream task. We\ncarried out several comparisons of our proposal with a random baseline and\nother models of the state of the art, following the EXTRA (EXplanaTion RAnking)\nbenchmark. Our method outperforms other collaborative filtering proposals.", "published": "2022-05-03 20:04:32", "link": "http://arxiv.org/abs/2205.01759v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR", "cs.SI", "I.2.7; I.5.1; I.5.2; I.5.3; I.5.4"], "primary_category": "cs.LG"}
{"title": "i-Code: An Integrative and Composable Multimodal Learning Framework", "abstract": "Human intelligence is multimodal; we integrate visual, linguistic, and\nacoustic signals to maintain a holistic worldview. Most current pretraining\nmethods, however, are limited to one or two modalities. We present i-Code, a\nself-supervised pretraining framework where users may flexibly combine the\nmodalities of vision, speech, and language into unified and general-purpose\nvector representations. In this framework, data from each modality are first\ngiven to pretrained single-modality encoders. The encoder outputs are then\nintegrated with a multimodal fusion network, which uses novel attention\nmechanisms and other architectural innovations to effectively combine\ninformation from the different modalities. The entire system is pretrained\nend-to-end with new objectives including masked modality unit modeling and\ncross-modality contrastive learning. Unlike previous research using only video\nfor pretraining, the i-Code framework can dynamically process single, dual, and\ntriple-modality data during training and inference, flexibly projecting\ndifferent combinations of modalities into a single representation space.\nExperimental results demonstrate how i-Code can outperform state-of-the-art\ntechniques on five video understanding tasks and the GLUE NLP benchmark,\nimproving by as much as 11% and demonstrating the power of integrative\nmultimodal pretraining.", "published": "2022-05-03 23:38:50", "link": "http://arxiv.org/abs/2205.01818v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Few-Shot Musical Source Separation", "abstract": "Deep learning-based approaches to musical source separation are often limited\nto the instrument classes that the models are trained on and do not generalize\nto separate unseen instruments. To address this, we propose a few-shot musical\nsource separation paradigm. We condition a generic U-Net source separation\nmodel using few audio examples of the target instrument. We train a few-shot\nconditioning encoder jointly with the U-Net to encode the audio examples into a\nconditioning vector to configure the U-Net via feature-wise linear modulation\n(FiLM). We evaluate the trained models on real musical recordings in the\nMUSDB18 and MedleyDB datasets. We show that our proposed few-shot conditioning\nparadigm outperforms the baseline one-hot instrument-class conditioned model\nfor both seen and unseen instruments. To extend the scope of our approach to a\nwider variety of real-world scenarios, we also experiment with different\nconditioning example characteristics, including examples from different\nrecordings, with multiple sources, or negative conditioning examples.", "published": "2022-05-03 02:18:46", "link": "http://arxiv.org/abs/2205.01273v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Dual-Microphone Speech Enhancement by Learning Cross-Channel\n  Features with Multi-Head Attention", "abstract": "Hand-crafted spatial features, such as inter-channel intensity difference\n(IID) and inter-channel phase difference (IPD), play a fundamental role in\nrecent deep learning based dual-microphone speech enhancement (DMSE) systems.\nHowever, learning the mutual relationship between artificially designed spatial\nand spectral features is hard in the end-to-end DMSE. In this work, a novel\narchitecture for DMSE using a multi-head cross-attention based convolutional\nrecurrent network (MHCA-CRN) is presented. The proposed MHCA-CRN model includes\na channel-wise encoding structure for preserving intra-channel features and a\nmulti-head cross-attention mechanism for fully exploiting cross-channel\nfeatures. In addition, the proposed approach specifically formulates the\ndecoder with an extra SNR estimator to estimate frame-level SNR under a\nmulti-task learning framework, which is expected to avoid speech distortion led\nby end-to-end DMSE module. Finally, a spectral gain function is adopted to\nfurther suppress the unnatural residual noise. Experiment results demonstrated\nsuperior performance of the proposed model against several state-of-the-art\nmodels.", "published": "2022-05-03 02:58:03", "link": "http://arxiv.org/abs/2205.01280v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Efficient dynamic filter for robust and low computational feature\n  extraction", "abstract": "Unseen noise signal which is not considered in a model training process is\ndifficult to anticipate and would lead to performance degradation. Various\nmethods have been investigated to mitigate unseen noise. In our previous work,\nan Instance-level Dynamic Filter (IDF) and a Pixel Dynamic Filter (PDF) were\nproposed to extract noise-robust features. However, the performance of the\ndynamic filter might be degraded since simple feature pooling is used to reduce\nthe computational resource in the IDF part. In this paper, we propose an\nefficient dynamic filter to enhance the performance of the dynamic filter.\nInstead of utilizing the simple feature mean, we separate Time-Frequency (T-F)\nfeatures as non-overlapping chunks, and separable convolutions are carried out\nfor each feature direction (inter chunks and intra chunks). Additionally, we\npropose Dynamic Attention Pooling that maps high dimensional features as low\ndimensional feature embeddings. These methods are applied to the IDF for\nkeyword spotting and speaker verification tasks. We confirm that our proposed\nmethod performs better in unseen environments (unseen noise and unseen\nspeakers) than state-of-the-art models.", "published": "2022-05-03 04:51:31", "link": "http://arxiv.org/abs/2205.01304v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Attentive activation function for improving end-to-end spoofing\n  countermeasure systems", "abstract": "The main objective of the spoofing countermeasure system is to detect the\nartifacts within the input speech caused by the speech synthesis or voice\nconversion process. In order to achieve this, we propose to adopt an attentive\nactivation function, more specifically attention rectified linear unit (AReLU)\nto the end-to-end spoofing countermeasure system. Since the AReLU employs the\nattention mechanism to boost the contribution of relevant input features while\nsuppressing the irrelevant ones, introducing AReLU can help the countermeasure\nsystem to focus on the features related to the artifacts. The proposed\nframework was experimented on the logical access (LA) task of ASVSpoof2019\ndataset, and outperformed the systems using the standard non-learnable\nactivation functions.", "published": "2022-05-03 14:30:00", "link": "http://arxiv.org/abs/2205.01528v1", "categories": ["eess.AS", "cs.CR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The ICML 2022 Expressive Vocalizations Workshop and Competition:\n  Recognizing, Generating, and Personalizing Vocal Bursts", "abstract": "The ICML Expressive Vocalization (ExVo) Competition is focused on\nunderstanding and generating vocal bursts: laughs, gasps, cries, and other\nnon-verbal vocalizations that are central to emotional expression and\ncommunication. ExVo 2022, includes three competition tracks using a large-scale\ndataset of 59,201 vocalizations from 1,702 speakers. The first, ExVo-MultiTask,\nrequires participants to train a multi-task model to recognize expressed\nemotions and demographic traits from vocal bursts. The second, ExVo-Generate,\nrequires participants to train a generative model that produces vocal bursts\nconveying ten different emotions. The third, ExVo-FewShot, requires\nparticipants to leverage few-shot learning incorporating speaker identity to\ntrain a model for the recognition of 10 emotions conveyed by vocal bursts. This\npaper describes the three tracks and provides performance measures for baseline\nmodels using state-of-the-art machine learning strategies. The baseline for\neach track is as follows, for ExVo-MultiTask, a combined score, computing the\nharmonic mean of Concordance Correlation Coefficient (CCC), Unweighted Average\nRecall (UAR), and inverted Mean Absolute Error (MAE) ($S_{MTL}$) is at best,\n0.335 $S_{MTL}$; for ExVo-Generate, we report Fr\\'echet inception distance\n(FID) scores ranging from 4.81 to 8.27 (depending on the emotion) between the\ntraining set and generated samples. We then combine the inverted FID with\nperceptual ratings of the generated samples ($S_{Gen}$) and obtain 0.174\n$S_{Gen}$; and for ExVo-FewShot, a mean CCC of 0.444 is obtained.", "published": "2022-05-03 21:06:44", "link": "http://arxiv.org/abs/2205.01780v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Synthesized Speech Detection Using Convolutional Transformer-Based\n  Spectrogram Analysis", "abstract": "Synthesized speech is common today due to the prevalence of virtual\nassistants, easy-to-use tools for generating and modifying speech signals, and\nremote work practices. Synthesized speech can also be used for nefarious\npurposes, including creating a purported speech signal and attributing it to\nsomeone who did not speak the content of the signal. We need methods to detect\nif a speech signal is synthesized. In this paper, we analyze speech signals in\nthe form of spectrograms with a Compact Convolutional Transformer (CCT) for\nsynthesized speech detection. A CCT utilizes a convolutional layer that\nintroduces inductive biases and shared weights into a network, allowing a\ntransformer architecture to perform well with fewer data samples used for\ntraining. The CCT uses an attention mechanism to incorporate information from\nall parts of a signal under analysis. Trained on both genuine human voice\nsignals and synthesized human voice signals, we demonstrate that our CCT\napproach successfully differentiates between genuine and synthesized speech\nsignals.", "published": "2022-05-03 22:05:35", "link": "http://arxiv.org/abs/2205.01800v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Frequency Domain-Based Detection of Generated Audio", "abstract": "Attackers may manipulate audio with the intent of presenting falsified\nreports, changing an opinion of a public figure, and winning influence and\npower. The prevalence of inauthentic multimedia continues to rise, so it is\nimperative to develop a set of tools that determines the legitimacy of media.\nWe present a method that analyzes audio signals to determine whether they\ncontain real human voices or fake human voices (i.e., voices generated by\nneural acoustic and waveform models). Instead of analyzing the audio signals\ndirectly, the proposed approach converts the audio signals into spectrogram\nimages displaying frequency, intensity, and temporal content and evaluates them\nwith a Convolutional Neural Network (CNN). Trained on both genuine human voice\nsignals and synthesized voice signals, we show our approach achieves high\naccuracy on this classification task.", "published": "2022-05-03 22:27:51", "link": "http://arxiv.org/abs/2205.01806v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
