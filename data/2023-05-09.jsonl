{"title": "Read, Diagnose and Chat: Towards Explainable and Interactive\n  LLMs-Augmented Depression Detection in Social Media", "abstract": "This paper proposes a new depression detection system based on LLMs that is\nboth interpretable and interactive. It not only provides a diagnosis, but also\ndiagnostic evidence and personalized recommendations based on natural language\ndialogue with the user. We address challenges such as the processing of large\namounts of text and integrate professional diagnostic criteria. Our system\noutperforms traditional methods across various settings and is demonstrated\nthrough case studies.", "published": "2023-05-09 02:49:09", "link": "http://arxiv.org/abs/2305.05138v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "E2TIMT: Efficient and Effective Modal Adapter for Text Image Machine\n  Translation", "abstract": "Text image machine translation (TIMT) aims to translate texts embedded in\nimages from one source language to another target language. Existing methods,\nboth two-stage cascade and one-stage end-to-end architectures, suffer from\ndifferent issues. The cascade models can benefit from the large-scale optical\ncharacter recognition (OCR) and MT datasets but the two-stage architecture is\nredundant. The end-to-end models are efficient but suffer from training data\ndeficiency. To this end, in our paper, we propose an end-to-end TIMT model\nfully making use of the knowledge from existing OCR and MT datasets to pursue\nboth an effective and efficient framework. More specifically, we build a novel\nmodal adapter effectively bridging the OCR encoder and MT decoder. End-to-end\nTIMT loss and cross-modal contrastive loss are utilized jointly to align the\nfeature distribution of the OCR and MT tasks. Extensive experiments show that\nthe proposed method outperforms the existing two-stage cascade models and\none-stage end-to-end models with a lighter and faster architecture.\nFurthermore, the ablation studies verify the generalization of our method,\nwhere the proposed modal adapter is effective to bridge various OCR and MT\nmodels.", "published": "2023-05-09 04:25:52", "link": "http://arxiv.org/abs/2305.05166v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Summarization with Precise Length Control", "abstract": "Many applications of text generation such as summarization benefit from\naccurately controlling the text length. Existing approaches on\nlength-controlled summarization either result in degraded performance or can\nonly control the length approximately. In this work, we present a framework to\ngenerate summaries with precisely the specified number of tokens or sentences,\nwhile maintaining or even improving the text quality. In addition, we jointly\ntrain the models to predict the lengths, so our model can generate summaries\nwith optimal length. We evaluate the proposed framework on the CNNDM dataset\nand show improved performance compared to existing methods.", "published": "2023-05-09 04:45:24", "link": "http://arxiv.org/abs/2305.05171v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CharSpan: Utilizing Lexical Similarity to Enable Zero-Shot Machine\n  Translation for Extremely Low-resource Languages", "abstract": "We address the task of machine translation (MT) from extremely low-resource\nlanguage (ELRL) to English by leveraging cross-lingual transfer from\n'closely-related' high-resource language (HRL). The development of an MT system\nfor ELRL is challenging because these languages typically lack parallel corpora\nand monolingual corpora, and their representations are absent from large\nmultilingual language models. Many ELRLs share lexical similarities with some\nHRLs, which presents a novel modeling opportunity. However, existing\nsubword-based neural MT models do not explicitly harness this lexical\nsimilarity, as they only implicitly align HRL and ELRL latent embedding space.\nTo overcome this limitation, we propose a novel, CharSpan, approach based on\n'character-span noise augmentation' into the training data of HRL. This serves\nas a regularization technique, making the model more robust to 'lexical\ndivergences' between the HRL and ELRL, thus facilitating effective\ncross-lingual transfer. Our method significantly outperformed strong baselines\nin zero-shot settings on closely related HRL and ELRL pairs from three diverse\nlanguage families, emerging as the state-of-the-art model for ELRLs.", "published": "2023-05-09 07:23:01", "link": "http://arxiv.org/abs/2305.05214v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Teacher Knowledge Distillation For Text Image Machine Translation", "abstract": "Text image machine translation (TIMT) has been widely used in various\nreal-world applications, which translates source language texts in images into\nanother target language sentence. Existing methods on TIMT are mainly divided\ninto two categories: the recognition-then-translation pipeline model and the\nend-to-end model. However, how to transfer knowledge from the pipeline model\ninto the end-to-end model remains an unsolved problem. In this paper, we\npropose a novel Multi-Teacher Knowledge Distillation (MTKD) method to\neffectively distillate knowledge into the end-to-end TIMT model from the\npipeline model. Specifically, three teachers are utilized to improve the\nperformance of the end-to-end TIMT model. The image encoder in the end-to-end\nTIMT model is optimized with the knowledge distillation guidance from the\nrecognition teacher encoder, while the sequential encoder and decoder are\nimproved by transferring knowledge from the translation sequential and decoder\nteacher models. Furthermore, both token and sentence-level knowledge\ndistillations are incorporated to better boost the translation performance.\nExtensive experimental results show that our proposed MTKD effectively improves\nthe text image translation performance and outperforms existing end-to-end and\npipeline models with fewer parameters and less decoding time, illustrating that\nMTKD can take advantage of both pipeline and end-to-end models.", "published": "2023-05-09 07:41:17", "link": "http://arxiv.org/abs/2305.05226v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attack Named Entity Recognition by Entity Boundary Interference", "abstract": "Named Entity Recognition (NER) is a cornerstone NLP task while its robustness\nhas been given little attention. This paper rethinks the principles of NER\nattacks derived from sentence classification, as they can easily violate the\nlabel consistency between the original and adversarial NER examples. This is\ndue to the fine-grained nature of NER, as even minor word changes in the\nsentence can result in the emergence or mutation of any entities, resulting in\ninvalid adversarial examples. To this end, we propose a novel one-word\nmodification NER attack based on a key insight, NER models are always\nvulnerable to the boundary position of an entity to make their decision. We\nthus strategically insert a new boundary into the sentence and trigger the\nEntity Boundary Interference that the victim model makes the wrong prediction\neither on this boundary word or on other words in the sentence. We call this\nattack Virtual Boundary Attack (ViBA), which is shown to be remarkably\neffective when attacking both English and Chinese models with a 70%-90% attack\nsuccess rate on state-of-the-art language models (e.g. RoBERTa, DeBERTa) and\nalso significantly faster than previous methods.", "published": "2023-05-09 08:21:11", "link": "http://arxiv.org/abs/2305.05253v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Perfect Victim: Computational Analysis of Judicial Attitudes towards\n  Victims of Sexual Violence", "abstract": "We develop computational models to analyze court statements in order to\nassess judicial attitudes toward victims of sexual violence in the Israeli\ncourt system. The study examines the resonance of \"rape myths\" in the criminal\njustice system's response to sex crimes, in particular in judicial assessment\nof victim's credibility. We begin by formulating an ontology for evaluating\njudicial attitudes toward victim's credibility, with eight ordinal labels and\nbinary categorizations. Second, we curate a manually annotated dataset for\njudicial assessments of victim's credibility in the Hebrew language, as well as\na model that can extract credibility labels from court cases. The dataset\nconsists of 855 verdict decision documents in sexual assault cases from\n1990-2021, annotated with the help of legal experts and trained law students.\nThe model uses a combined approach of syntactic and latent structures to find\nsentences that convey the judge's attitude towards the victim and classify them\naccording to the credibility label set. Our ontology, data, and models will be\nmade available upon request, in the hope they spur future progress in this\njudicial important task.", "published": "2023-05-09 09:45:44", "link": "http://arxiv.org/abs/2305.05302v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ArgU: A Controllable Factual Argument Generator", "abstract": "Effective argumentation is essential towards a purposeful conversation with a\nsatisfactory outcome. For example, persuading someone to reconsider smoking\nmight involve empathetic, well founded arguments based on facts and expert\nopinions about its ill-effects and the consequences on one's family. However,\nthe automatic generation of high-quality factual arguments can be challenging.\nAddressing existing controllability issues can make the recent advances in\ncomputational models for argument generation a potential solution. In this\npaper, we introduce ArgU: a neural argument generator capable of producing\nfactual arguments from input facts and real-world concepts that can be\nexplicitly controlled for stance and argument structure using Walton's argument\nscheme-based control codes. Unfortunately, computational argument generation is\na relatively new field and lacks datasets conducive to training. Hence, we have\ncompiled and released an annotated corpora of 69,428 arguments spanning six\ntopics and six argument schemes, making it the largest publicly available\ncorpus for identifying argument schemes; the paper details our annotation and\ndataset creation framework. We further experiment with an argument generation\nstrategy that establishes an inference strategy by generating an ``argument\ntemplate'' before actual argument generation. Our results demonstrate that it\nis possible to automatically generate diverse arguments exhibiting different\ninference patterns for the same set of facts by using control codes based on\nargument schemes and stance.", "published": "2023-05-09 10:49:45", "link": "http://arxiv.org/abs/2305.05334v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rudolf Christoph Eucken at SemEval-2023 Task 4: An Ensemble Approach for\n  Identifying Human Values from Arguments", "abstract": "The subtle human values we acquire through life experiences govern our\nthoughts and gets reflected in our speech. It plays an integral part in\ncapturing the essence of our individuality and making it imperative to identify\nsuch values in computational systems that mimic human actions. Computational\nargumentation is a field that deals with the argumentation capabilities of\nhumans and can benefit from identifying such values. Motivated by that, we\npresent an ensemble approach for detecting human values from argument text. Our\nensemble comprises three models: (i) An entailment-based model for determining\nthe human values based on their descriptions, (ii) A Roberta-based classifier\nthat predicts the set of human values from an argument. (iii) A Roberta-based\nclassifier to predict a reduced set of human values from an argument. We\nexperiment with different ways of combining the models and report our results.\nFurthermore, our best combination achieves an overall F1 score of 0.48 on the\nmain test set.", "published": "2023-05-09 10:54:34", "link": "http://arxiv.org/abs/2305.05335v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PLM-GNN: A Webpage Classification Method based on Joint Pre-trained\n  Language Model and Graph Neural Network", "abstract": "The number of web pages is growing at an exponential rate, accumulating\nmassive amounts of data on the web. It is one of the key processes to classify\nwebpages in web information mining. Some classical methods are based on\nmanually building features of web pages and training classifiers based on\nmachine learning or deep learning. However, building features manually requires\nspecific domain knowledge and usually takes a long time to validate the\nvalidity of features. Considering webpages generated by the combination of text\nand HTML Document Object Model(DOM) trees, we propose a representation and\nclassification method based on a pre-trained language model and graph neural\nnetwork, named PLM-GNN. It is based on the joint encoding of text and HTML DOM\ntrees in the web pages. It performs well on the KI-04 and SWDE datasets and on\npractical dataset AHS for the project of scholar's homepage crawling.", "published": "2023-05-09 12:19:10", "link": "http://arxiv.org/abs/2305.05378v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COKE: A Cognitive Knowledge Graph for Machine Theory of Mind", "abstract": "Theory of mind (ToM) refers to humans' ability to understand and infer the\ndesires, beliefs, and intentions of others. The acquisition of ToM plays a key\nrole in humans' social cognition and interpersonal relations. Though\nindispensable for social intelligence, ToM is still lacking for modern AI and\nNLP systems since they cannot access the human mental state and cognitive\nprocess beneath the training corpus. To empower AI systems with the ToM ability\nand narrow the gap between them and humans, in this paper, we propose COKE: the\nfirst cognitive knowledge graph for machine theory of mind. Specifically, COKE\nformalizes ToM as a collection of 45k+ manually verified cognitive chains that\ncharacterize human mental activities and subsequent behavioral/affective\nresponses when facing specific social circumstances. In addition, we further\ngeneralize COKE using LLMs and build a powerful generation model COLM tailored\nfor cognitive reasoning. Experimental results in both automatic and human\nevaluation demonstrate the high quality of COKE, the superior ToM ability of\nCOLM, and its potential to significantly enhance social applications.", "published": "2023-05-09 12:36:58", "link": "http://arxiv.org/abs/2305.05390v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models Need Holistically Thought in Medical\n  Conversational QA", "abstract": "The medical conversational question answering (CQA) system aims at providing\na series of professional medical services to improve the efficiency of medical\ncare. Despite the success of large language models (LLMs) in complex reasoning\ntasks in various fields, such as mathematics, logic, and commonsense QA, they\nstill need to improve with the increased complexity and specialization of the\nmedical field. This is because medical CQA tasks require not only strong\nmedical reasoning, but also the ability to think broadly and deeply. In this\npaper, to address these challenges in medical CQA tasks that need to be\nconsidered and understood in many aspects, we propose the Holistically Thought\n(HoT) method, which is designed to guide the LLMs to perform the diffused and\nfocused thinking for generating high-quality medical responses. The proposed\nHoT method has been evaluated through automated and manual assessments in three\ndifferent medical CQA datasets containing the English and Chinese languages.\nThe extensive experimental results show that our method can produce more\ncorrectness, professional, and considerate answers than several\nstate-of-the-art (SOTA) methods, manifesting its effectiveness. Our code in\nhttps://github.com/WENGSYX/HoT.", "published": "2023-05-09 12:57:28", "link": "http://arxiv.org/abs/2305.05410v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Estimating related words computationally using language model from the\n  Mahabharata -- an Indian epic", "abstract": "'Mahabharata' is the most popular among many Indian pieces of literature\nreferred to in many domains for completely different purposes. This text itself\nis having various dimension and aspects which is useful for the human being in\ntheir personal life and professional life. This Indian Epic is originally\nwritten in the Sanskrit Language. Now in the era of Natural Language\nProcessing, Artificial Intelligence, Machine Learning, and Human-Computer\ninteraction this text can be processed according to the domain requirement. It\nis interesting to process this text and get useful insights from Mahabharata.\nThe limitation of the humans while analyzing Mahabharata is that they always\nhave a sentiment aspect towards the story narrated by the author. Apart from\nthat, the human cannot memorize statistical or computational details, like\nwhich two words are frequently coming in one sentence? What is the average\nlength of the sentences across the whole literature? Which word is the most\npopular word across the text, what are the lemmas of the words used across the\nsentences? Thus, in this paper, we propose an NLP pipeline to get some\nstatistical and computational insights along with the most relevant word\nsearching method from the largest epic 'Mahabharata'. We stacked the different\ntext-processing approaches to articulate the best results which can be further\nused in the various domain where Mahabharata needs to be referred.", "published": "2023-05-09 13:13:26", "link": "http://arxiv.org/abs/2305.05420v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What is the best recipe for character-level encoder-only modelling?", "abstract": "This paper aims to benchmark recent progress in language understanding models\nthat output contextualised representations at the character level. Many such\nmodelling architectures and methods to train those architectures have been\nproposed, but it is currently unclear what the relative contributions of the\narchitecture vs. the pretraining objective are to final model performance. We\nexplore the design space of such models, comparing architectural innovations\nand a variety of different pretraining objectives on a suite of evaluation\ntasks with a fixed training procedure in order to find the currently optimal\nway to build and train character-level BERT-like models. We find that our best\nperforming character-level model exceeds the performance of a token-based model\ntrained with the same settings on the same data, suggesting that\ncharacter-level models are ready for more widespread adoption. Unfortunately,\nthe best method to train character-level models still relies on a subword-level\ntokeniser during pretraining, and final model performance is highly dependent\non tokeniser quality. We believe our results demonstrate the readiness of\ncharacter-level models for multilingual language representation, and encourage\nNLP practitioners to try them as drop-in replacements for token-based models.", "published": "2023-05-09 14:00:15", "link": "http://arxiv.org/abs/2305.05461v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Good Intentions: Reporting the Research Landscape of NLP for\n  Social Good", "abstract": "With the recent advances in natural language processing (NLP), a vast number\nof applications have emerged across various use cases. Among the plethora of\nNLP applications, many academic researchers are motivated to do work that has a\npositive social impact, in line with the recent initiatives of NLP for Social\nGood (NLP4SG). However, it is not always obvious to researchers how their\nresearch efforts are tackling today's big social problems. Thus, in this paper,\nwe introduce NLP4SG Papers, a scientific dataset with three associated tasks\nthat can help identify NLP4SG papers and characterize the NLP4SG landscape by:\n(1) identifying the papers that address a social problem, (2) mapping them to\nthe corresponding UN Sustainable Development Goals (SDGs), and (3) identifying\nthe task they are solving and the methods they are using. Using\nstate-of-the-art NLP models, we address each of these tasks and use them on the\nentire ACL Anthology, resulting in a visualization workspace that gives\nresearchers a comprehensive overview of the field of NLP4SG. Our website is\navailable at https://nlp4sg.vercel.app. We released our data at\nhttps://huggingface.co/datasets/feradauto/NLP4SGPapers and code at\nhttps://github.com/feradauto/nlp4sg", "published": "2023-05-09 14:16:25", "link": "http://arxiv.org/abs/2305.05471v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MAUPQA: Massive Automatically-created Polish Question Answering Dataset", "abstract": "Recently, open-domain question answering systems have begun to rely heavily\non annotated datasets to train neural passage retrievers. However, manually\nannotating such datasets is both difficult and time-consuming, which limits\ntheir availability for less popular languages. In this work, we experiment with\nseveral methods for automatically collecting weakly labeled datasets and show\nhow they affect the performance of the neural passage retrieval models. As a\nresult of our work, we publish the MAUPQA dataset, consisting of nearly 400,000\nquestion-passage pairs for Polish, as well as the HerBERT-QA neural retriever.", "published": "2023-05-09 14:36:04", "link": "http://arxiv.org/abs/2305.05486v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Pseudo Image Captions for Multimodal Summarization", "abstract": "Cross-modal contrastive learning in vision language pretraining (VLP) faces\nthe challenge of (partial) false negatives. In this paper, we study this\nproblem from the perspective of Mutual Information (MI) optimization. It is\ncommon sense that InfoNCE loss used in contrastive learning will maximize the\nlower bound of MI between anchors and their positives, while we theoretically\nprove that MI involving negatives also matters when noises commonly exist.\nGuided by a more general lower bound form for optimization, we propose a\ncontrastive learning strategy regulated by progressively refined cross-modal\nsimilarity, to more accurately optimize MI between an image/text anchor and its\nnegative texts/images instead of improperly minimizing it. Our method performs\ncompetitively on four downstream cross-modal tasks and systematically balances\nthe beneficial and harmful effects of (partial) false negative samples under\ntheoretical guidance.", "published": "2023-05-09 14:47:25", "link": "http://arxiv.org/abs/2305.05496v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Exploration of Encoder-Decoder Approaches to Multi-Label\n  Classification for Legal and Biomedical Text", "abstract": "Standard methods for multi-label text classification largely rely on\nencoder-only pre-trained language models, whereas encoder-decoder models have\nproven more effective in other classification tasks. In this study, we compare\nfour methods for multi-label classification, two based on an encoder only, and\ntwo based on an encoder-decoder. We carry out experiments on four datasets --\ntwo in the legal domain and two in the biomedical domain, each with two levels\nof label granularity -- and always depart from the same pre-trained model, T5.\nOur results show that encoder-decoder methods outperform encoder-only methods,\nwith a growing advantage on more complex datasets and labeling schemes of finer\ngranularity. Using encoder-decoder models in a non-autoregressive fashion, in\nparticular, yields the best performance overall, so we further study this\napproach through ablations to better understand its strengths.", "published": "2023-05-09 17:13:53", "link": "http://arxiv.org/abs/2305.05627v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "$2 * n$ is better than $n^2$: Decomposing Event Coreference Resolution\n  into Two Tractable Problems", "abstract": "Event Coreference Resolution (ECR) is the task of linking mentions of the\nsame event either within or across documents. Most mention pairs are not\ncoreferent, yet many that are coreferent can be identified through simple\ntechniques such as lemma matching of the event triggers or the sentences in\nwhich they appear. Existing methods for training coreference systems sample\nfrom a largely skewed distribution, making it difficult for the algorithm to\nlearn coreference beyond surface matching. Additionally, these methods are\nintractable because of the quadratic operations needed. To address these\nchallenges, we break the problem of ECR into two parts: a) a heuristic to\nefficiently filter out a large number of non-coreferent pairs, and b) a\ntraining approach on a balanced set of coreferent and non-coreferent mention\npairs. By following this approach, we show that we get comparable results to\nthe state of the art on two popular ECR datasets while significantly reducing\ncompute requirements. We also analyze the mention pairs that are \"hard\" to\naccurately classify as coreferent or non-coreferent. Code at\nhttps://github.com/ahmeshaf/lemma_ce_coref", "published": "2023-05-09 05:33:32", "link": "http://arxiv.org/abs/2305.05672v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilevel Sentence Embeddings for Personality Prediction", "abstract": "Representing text into a multidimensional space can be done with sentence\nembedding models such as Sentence-BERT (SBERT). However, training these models\nwhen the data has a complex multilevel structure requires individually trained\nclass-specific models, which increases time and computing costs. We propose a\ntwo step approach which enables us to map sentences according to their\nhierarchical memberships and polarity. At first we teach the upper level\nsentence space through an AdaCos loss function and then finetune with a novel\nloss function mainly based on the cosine similarity of intra-level pairs. We\napply this method to three different datasets: two weakly supervised Big Five\npersonality dataset obtained from English and Japanese Twitter data and the\nbenchmark MNLI dataset. We show that our single model approach performs better\nthan multiple class-specific classification models.", "published": "2023-05-09 20:02:18", "link": "http://arxiv.org/abs/2305.05748v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Alleviating Over-smoothing for Unsupervised Sentence Representation", "abstract": "Currently, learning better unsupervised sentence representations is the\npursuit of many natural language processing communities. Lots of approaches\nbased on pre-trained language models (PLMs) and contrastive learning have\nachieved promising results on this task. Experimentally, we observe that the\nover-smoothing problem reduces the capacity of these powerful PLMs, leading to\nsub-optimal sentence representations. In this paper, we present a Simple method\nnamed Self-Contrastive Learning (SSCL) to alleviate this issue, which samples\nnegatives from PLMs intermediate layers, improving the quality of the sentence\nrepresentation. Our proposed method is quite simple and can be easily extended\nto various state-of-the-art models for performance boosting, which can be seen\nas a plug-and-play contrastive framework for learning unsupervised sentence\nrepresentation. Extensive results prove that SSCL brings the superior\nperformance improvements of different strong baselines (e.g., BERT and SimCSE)\non Semantic Textual Similarity and Transfer datasets. Our codes are available\nat https://github.com/nuochenpku/SSCL.", "published": "2023-05-09 11:00:02", "link": "http://arxiv.org/abs/2305.06154v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChatGPT as a Text Simplification Tool to Remove Bias", "abstract": "The presence of specific linguistic signals particular to a certain sub-group\nof people can be picked up by language models during training. If the model\nbegins to associate specific language with a distinct group, any decisions made\nbased upon this language would hold a strong correlation to a decision based\nupon their protected characteristic, leading to possible discrimination. We\nexplore a potential technique for bias mitigation in the form of simplification\nof text. The driving force of this idea is that simplifying text should\nstandardise language between different sub-groups to one way of speaking while\nkeeping the same meaning. The experiment shows promising results as the\nclassifier accuracy for predicting the sensitive attribute drops by up to 17%\nfor the simplified data.", "published": "2023-05-09 13:10:23", "link": "http://arxiv.org/abs/2305.06166v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Phishing Attacks using ChatGPT", "abstract": "The ability of ChatGPT to generate human-like responses and understand\ncontext has made it a popular tool for conversational agents, content creation,\ndata analysis, and research and innovation. However, its effectiveness and ease\nof accessibility makes it a prime target for generating malicious content, such\nas phishing attacks, that can put users at risk. In this work, we identify\nseveral malicious prompts that can be provided to ChatGPT to generate\nfunctional phishing websites. Through an iterative approach, we find that these\nphishing websites can be made to imitate popular brands and emulate several\nevasive tactics that have been known to avoid detection by anti-phishing\nentities. These attacks can be generated using vanilla ChatGPT without the need\nof any prior adversarial exploits (jailbreaking).", "published": "2023-05-09 02:38:05", "link": "http://arxiv.org/abs/2305.05133v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Effective Medical Code Prediction via Label Internal Alignment", "abstract": "The clinical notes are usually typed into the system by physicians. They are\ntypically required to be marked by standard medical codes, and each code\nrepresents a diagnosis or medical treatment procedure. Annotating these notes\nis time consuming and prone to error. In this paper, we proposed a multi-view\nattention based Neural network to predict medical codes from clinical texts.\nOur method incorporates three aspects of information, the semantic context of\nthe clinical text, the relationship among the label (medical codes) space, and\nthe alignment between each pair of a clinical text and medical code. Our method\nis verified to be effective on the open source dataset. The experimental result\nshows that our method achieves better performance against the prior\nstate-of-art on multiple metrics.", "published": "2023-05-09 04:14:20", "link": "http://arxiv.org/abs/2305.05162v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MoT: Memory-of-Thought Enables ChatGPT to Self-Improve", "abstract": "Large Language Models (LLMs) have shown impressive abilities in various\ntasks. However, fundamentally improving them depends on high-quality datasets\nor computationally expensive fine-tuning. On the contrary, humans can easily\nimprove themselves by self-thinking and memory, without external resources. In\nthis paper, we propose a framework, MoT, to let the LLM self-improve through\nMemory-of-Thought, without annotated datasets and parameter updates.\nSpecifically, MoT is divided into two stages: 1. before the test stage, the LLM\npre-thinks on the unlabeled dataset and saves the high-confidence thoughts as\nexternal memory; 2. During the test stage, given a test question, the LLM\nrecalls relevant memory to help itself reason and answer it. Experimental\nresults show that MoT can help ChatGPT significantly improve its abilities in\narithmetic reasoning, commonsense reasoning, factual reasoning, and natural\nlanguage inference. Further analyses show that each component contributes\ncritically to the improvements and MoT can lead to consistent improvements\nacross various CoT methods and LLMs.", "published": "2023-05-09 05:25:05", "link": "http://arxiv.org/abs/2305.05181v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CSED: A Chinese Semantic Error Diagnosis Corpus", "abstract": "Recently, much Chinese text error correction work has focused on Chinese\nSpelling Check (CSC) and Chinese Grammatical Error Diagnosis (CGED). In\ncontrast, little attention has been paid to the complicated problem of Chinese\nSemantic Error Diagnosis (CSED), which lacks relevant datasets. The study of\nsemantic errors is important because they are very common and may lead to\nsyntactic irregularities or even problems of comprehension. To investigate\nthis, we build the CSED corpus, which includes two datasets. The one is for the\nCSED-Recognition (CSED-R) task. The other is for the CSED-Correction (CSED-C)\ntask. Our annotation guarantees high-quality data through quality assurance\nmechanisms. Our experiments show that powerful pre-trained models perform\npoorly on this corpus. We also find that the CSED task is challenging, as\nevidenced by the fact that even humans receive a low score. This paper proposes\nsyntax-aware models to specifically adapt to the CSED task. The experimental\nresults show that the introduction of the syntax-aware approach is meaningful.", "published": "2023-05-09 05:33:31", "link": "http://arxiv.org/abs/2305.05183v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with\n  Large Language Models", "abstract": "Diffusion models, which have emerged to become popular text-to-image\ngeneration models, can produce high-quality and content-rich images guided by\ntextual prompts. However, there are limitations to semantic understanding and\ncommonsense reasoning in existing models when the input prompts are concise\nnarrative, resulting in low-quality image generation. To improve the capacities\nfor narrative prompts, we propose a simple-yet-effective parameter-efficient\nfine-tuning approach called the Semantic Understanding and Reasoning adapter\n(SUR-adapter) for pre-trained diffusion models. To reach this goal, we first\ncollect and annotate a new dataset SURD which consists of more than 57,000\nsemantically corrected multi-modal samples. Each sample contains a simple\nnarrative prompt, a complex keyword-based prompt, and a high-quality image.\nThen, we align the semantic representation of narrative prompts to the complex\nprompts and transfer knowledge of large language models (LLMs) to our\nSUR-adapter via knowledge distillation so that it can acquire the powerful\nsemantic understanding and reasoning capabilities to build a high-quality\ntextual semantic representation for text-to-image generation. We conduct\nexperiments by integrating multiple LLMs and popular pre-trained diffusion\nmodels to show the effectiveness of our approach in enabling diffusion models\nto understand and reason concise natural language without image quality\ndegradation. Our approach can make text-to-image diffusion models easier to use\nwith better user experience, which demonstrates our approach has the potential\nfor further advancing the development of user-friendly text-to-image generation\nmodels by bridging the semantic gap between simple narrative prompts and\ncomplex keyword-based prompts. The code is released at\nhttps://github.com/Qrange-group/SUR-adapter.", "published": "2023-05-09 05:48:38", "link": "http://arxiv.org/abs/2305.05189v4", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "COLA: Contextualized Commonsense Causal Reasoning from the Causal\n  Inference Perspective", "abstract": "Detecting commonsense causal relations (causation) between events has long\nbeen an essential yet challenging task. Given that events are complicated, an\nevent may have different causes under various contexts. Thus, exploiting\ncontext plays an essential role in detecting causal relations. Meanwhile,\nprevious works about commonsense causation only consider two events and ignore\ntheir context, simplifying the task formulation. This paper proposes a new task\nto detect commonsense causation between two events in an event sequence (i.e.,\ncontext), called contextualized commonsense causal reasoning. We also design a\nzero-shot framework: COLA (Contextualized Commonsense Causality Reasoner) to\nsolve the task from the causal inference perspective. This framework obtains\nrich incidental supervision from temporality and balances covariates from\nmultiple timestamps to remove confounding effects. Our extensive experiments\nshow that COLA can detect commonsense causality more accurately than baselines.", "published": "2023-05-09 05:56:58", "link": "http://arxiv.org/abs/2305.05191v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Getting More Juice Out of Your Data: Hard Pair Refinement Enhances\n  Visual-Language Models Without Extra Data", "abstract": "Contrastive Language-Image Pre-training (CLIP) has become the standard for\ncross-modal image-text representation learning. Improving CLIP typically\nrequires additional data and retraining with new loss functions, but these\ndemands raise resource and time costs, limiting practical use. In this work, we\nintroduce HELIP, a cost-effective strategy that improves CLIP models by\nexploiting challenging text-image pairs within existing datasets in continuous\ntraining. This eliminates the need for additional data or extensive retraining.\nMoreover, HELIP integrates effortlessly into current training pipelines with\nminimal code modifications, allowing for quick and seamless implementation. On\ncomprehensive benchmarks, HELIP consistently boosts existing models. In\nparticular, within just two epochs of training, it improves zero-shot\nclassification accuracy on ImageNet for SLIP models pre-trained on CC3M, CC12M,\nand YFCC15M datasets by 3.05%, 4.47%, and 10.1% , respectively. In addition, on\nfine-grained classification datasets, HELIP improves the zero-shot performance\nof CLIP and SLIP by an average of 8.4% and 18.6%, and their linear probe\nperformance by an average of 9.5% and 3.0%. The code is publicly available at:\nhttps://github.com/haonan3/HELIP-NACCL-2025.git.", "published": "2023-05-09 07:00:17", "link": "http://arxiv.org/abs/2305.05208v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Distilling Script Knowledge from Large Language Models for Constrained\n  Language Planning", "abstract": "In everyday life, humans often plan their actions by following step-by-step\ninstructions in the form of goal-oriented scripts. Previous work has exploited\nlanguage models (LMs) to plan for abstract goals of stereotypical activities\n(e.g., \"make a cake\"), but leaves more specific goals with multi-facet\nconstraints understudied (e.g., \"make a cake for diabetics\"). In this paper, we\ndefine the task of constrained language planning for the first time. We propose\nan overgenerate-then-filter approach to improve large language models (LLMs) on\nthis task, and use it to distill a novel constrained language planning dataset,\nCoScript, which consists of 55,000 scripts. Empirical results demonstrate that\nour method significantly improves the constrained language planning ability of\nLLMs, especially on constraint faithfulness. Furthermore, CoScript is\ndemonstrated to be quite effective in endowing smaller LMs with constrained\nlanguage planning ability.", "published": "2023-05-09 08:19:32", "link": "http://arxiv.org/abs/2305.05252v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VCSUM: A Versatile Chinese Meeting Summarization Dataset", "abstract": "Compared to news and chat summarization, the development of meeting\nsummarization is hugely decelerated by the limited data. To this end, we\nintroduce a versatile Chinese meeting summarization dataset, dubbed VCSum,\nconsisting of 239 real-life meetings, with a total duration of over 230 hours.\nWe claim our dataset is versatile because we provide the annotations of topic\nsegmentation, headlines, segmentation summaries, overall meeting summaries, and\nsalient sentences for each meeting transcript. As such, the dataset can adapt\nto various summarization tasks or methods, including segmentation-based\nsummarization, multi-granularity summarization and retrieval-then-generate\nsummarization. Our analysis confirms the effectiveness and robustness of VCSum.\nWe also provide a set of benchmark models regarding different downstream\nsummarization tasks on VCSum to facilitate further research. The dataset and\ncode will be released at https://github.com/hahahawu/VCSum.", "published": "2023-05-09 09:07:15", "link": "http://arxiv.org/abs/2305.05280v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dialogue Planning via Brownian Bridge Stochastic Process for\n  Goal-directed Proactive Dialogue", "abstract": "Goal-directed dialogue systems aim to proactively reach a pre-determined\ntarget through multi-turn conversations. The key to achieving this task lies in\nplanning dialogue paths that smoothly and coherently direct conversations\ntowards the target. However, this is a challenging and under-explored task. In\nthis work, we propose a coherent dialogue planning approach that uses a\nstochastic process to model the temporal dynamics of dialogue paths. We define\na latent space that captures the coherence of goal-directed behavior using a\nBrownian bridge process, which allows us to incorporate user feedback flexibly\nin dialogue planning. Based on the derived latent trajectories, we generate\ndialogue paths explicitly using pre-trained language models. We finally employ\nthese paths as natural language prompts to guide dialogue generation. Our\nexperiments show that our approach generates more coherent utterances and\nachieves the goal with a higher success rate.", "published": "2023-05-09 09:28:23", "link": "http://arxiv.org/abs/2305.05290v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Boosting Zero-shot Cross-lingual Retrieval by Training on Artificially\n  Code-Switched Data", "abstract": "Transferring information retrieval (IR) models from a high-resource language\n(typically English) to other languages in a zero-shot fashion has become a\nwidely adopted approach. In this work, we show that the effectiveness of\nzero-shot rankers diminishes when queries and documents are present in\ndifferent languages. Motivated by this, we propose to train ranking models on\nartificially code-switched data instead, which we generate by utilizing\nbilingual lexicons. To this end, we experiment with lexicons induced from (1)\ncross-lingual word embeddings and (2) parallel Wikipedia page titles. We use\nthe mMARCO dataset to extensively evaluate reranking models on 36 language\npairs spanning Monolingual IR (MoIR), Cross-lingual IR (CLIR), and Multilingual\nIR (MLIR). Our results show that code-switching can yield consistent and\nsubstantial gains of 5.1 MRR@10 in CLIR and 3.9 MRR@10 in MLIR, while\nmaintaining stable performance in MoIR. Encouragingly, the gains are especially\npronounced for distant languages (up to 2x absolute gain). We further show that\nour approach is robust towards the ratio of code-switched tokens and also\nextends to unseen languages. Our results demonstrate that training on\ncode-switched data is a cheap and effective way of generalizing zero-shot\nrankers for cross-lingual and multilingual retrieval.", "published": "2023-05-09 09:32:19", "link": "http://arxiv.org/abs/2305.05295v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Structured Sentiment Analysis as Transition-based Dependency Parsing", "abstract": "Structured sentiment analysis (SSA) aims to automatically extract people's\nopinions from a text in natural language and adequately represent that\ninformation in a graph structure. One of the most accurate methods for\nperforming SSA was recently proposed and consists of approaching it as a\ndependency parsing task. Although we can find in the literature how\ntransition-based algorithms excel in dependency parsing in terms of accuracy\nand efficiency, all proposed attempts to tackle SSA following that approach\nwere based on graph-based models. In this article, we present the first\ntransition-based method to address SSA as dependency parsing. Specifically, we\ndesign a transition system that processes the input text in a left-to-right\npass, incrementally generating the graph structure containing all identified\nopinions. To effectively implement our final transition-based model, we resort\nto a Pointer Network architecture as a backbone. From an extensive evaluation,\nwe demonstrate that our model offers the best performance to date in\npractically all cases among prior dependency-based methods, and surpass recent\ntask-specific techniques on the most challenging datasets. We additionally\ninclude an in-depth analysis and empirically prove that the overall\ntime-complexity cost of our approach is quadratic in the sentence length, being\nmore efficient than top-performing graph-based parsers.", "published": "2023-05-09 10:03:34", "link": "http://arxiv.org/abs/2305.05311v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Detection of depression on social networks using transformers and\n  ensembles", "abstract": "As the impact of technology on our lives is increasing, we witness increased\nuse of social media that became an essential tool not only for communication\nbut also for sharing information with community about our thoughts and\nfeelings. This can be observed also for people with mental health disorders\nsuch as depression where they use social media for expressing their thoughts\nand asking for help. This opens a possibility to automatically process social\nmedia posts and detect signs of depression. We build several large pre-trained\nlanguage model based classifiers for depression detection from social media\nposts. Besides fine-tuning BERT, RoBERTA, BERTweet, and mentalBERT were also\nconstruct two types of ensembles. We analyze the performance of our models on\ntwo data sets of posts from social platforms Reddit and Twitter, and\ninvestigate also the performance of transfer learning across the two data sets.\nThe results show that transformer ensembles improve over the single\ntransformer-based classifiers.", "published": "2023-05-09 10:21:14", "link": "http://arxiv.org/abs/2305.05325v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Explainable Recommender with Geometric Information Bottleneck", "abstract": "Explainable recommender systems can explain their recommendation decisions,\nenhancing user trust in the systems. Most explainable recommender systems\neither rely on human-annotated rationales to train models for explanation\ngeneration or leverage the attention mechanism to extract important text spans\nfrom reviews as explanations. The extracted rationales are often confined to an\nindividual review and may fail to identify the implicit features beyond the\nreview text. To avoid the expensive human annotation process and to generate\nexplanations beyond individual reviews, we propose to incorporate a geometric\nprior learnt from user-item interactions into a variational network which\ninfers latent factors from user-item reviews. The latent factors from an\nindividual user-item pair can be used for both recommendation and explanation\ngeneration, which naturally inherit the global characteristics encoded in the\nprior knowledge. Experimental results on three e-commerce datasets show that\nour model significantly improves the interpretability of a variational\nrecommender using the Wasserstein distance while achieving performance\ncomparable to existing content-based recommender systems in terms of\nrecommendation behaviours.", "published": "2023-05-09 10:38:36", "link": "http://arxiv.org/abs/2305.05331v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "CaseEncoder: A Knowledge-enhanced Pre-trained Model for Legal Case\n  Encoding", "abstract": "Legal case retrieval is a critical process for modern legal information\nsystems. While recent studies have utilized pre-trained language models (PLMs)\nbased on the general domain self-supervised pre-training paradigm to build\nmodels for legal case retrieval, there are limitations in using general domain\nPLMs as backbones. Specifically, these models may not fully capture the\nunderlying legal features in legal case documents. To address this issue, we\npropose CaseEncoder, a legal document encoder that leverages fine-grained legal\nknowledge in both the data sampling and pre-training phases. In the data\nsampling phase, we enhance the quality of the training data by utilizing\nfine-grained law article information to guide the selection of positive and\nnegative examples. In the pre-training phase, we design legal-specific\npre-training tasks that align with the judging criteria of relevant legal\ncases. Based on these tasks, we introduce an innovative loss function called\nBiased Circle Loss to enhance the model's ability to recognize case relevance\nin fine grains. Experimental results on multiple benchmarks demonstrate that\nCaseEncoder significantly outperforms both existing general pre-training models\nand legal-specific pre-training models in zero-shot legal case retrieval.", "published": "2023-05-09 12:40:19", "link": "http://arxiv.org/abs/2305.05393v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "WikiWeb2M: A Page-Level Multimodal Wikipedia Dataset", "abstract": "Webpages have been a rich resource for language and vision-language tasks.\nYet only pieces of webpages are kept: image-caption pairs, long text articles,\nor raw HTML, never all in one place. Webpage tasks have resultingly received\nlittle attention and structured image-text data underused. To study multimodal\nwebpage understanding, we introduce the Wikipedia Webpage 2M (WikiWeb2M) suite;\nthe first to retain the full set of images, text, and structure data available\nin a page. WikiWeb2M can be used for tasks like page description generation,\nsection summarization, and contextual image captioning.", "published": "2023-05-09 13:20:59", "link": "http://arxiv.org/abs/2305.05432v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Going beyond research datasets: Novel intent discovery in the industry\n  setting", "abstract": "Novel intent discovery automates the process of grouping similar messages\n(questions) to identify previously unknown intents. However, current research\nfocuses on publicly available datasets which have only the question field and\nsignificantly differ from real-life datasets. This paper proposes methods to\nimprove the intent discovery pipeline deployed in a large e-commerce platform.\nWe show the benefit of pre-training language models on in-domain data: both\nself-supervised and with weak supervision. We also devise the best method to\nutilize the conversational structure (i.e., question and answer) of real-life\ndatasets during fine-tuning for clustering tasks, which we call Conv. All our\nmethods combined to fully utilize real-life datasets give up to 33pp\nperformance boost over state-of-the-art Constrained Deep Adaptive Clustering\n(CDAC) model for question only. By comparison CDAC model for the question data\nonly gives only up to 13pp performance boost over the naive baseline.", "published": "2023-05-09 14:21:29", "link": "http://arxiv.org/abs/2305.05474v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language Models Humanize Technology", "abstract": "Large Language Models (LLMs) have made rapid progress in recent months and\nweeks, garnering significant public attention. This has sparked concerns about\naligning these models with human values, their impact on labor markets, and the\npotential need for regulation in further research and development. However, the\ndiscourse often lacks a focus on the imperative to widely diffuse the societal\nbenefits of LLMs. To qualify this societal benefit, we assert that LLMs exhibit\nemergent abilities to humanize technology more effectively than previous\ntechnologies, and for people across language, occupation, and accessibility\ndivides. We argue that they do so by addressing three mechanizing bottlenecks\nin today's computing technologies: creating diverse and accessible content,\nlearning complex digital tools, and personalizing machine learning algorithms.\nWe adopt a case-based approach and illustrate each bottleneck with two examples\nwhere current technology imposes bottlenecks that LLMs demonstrate the ability\nto address. Given this opportunity to humanize technology widely, we advocate\nfor more widespread understanding of LLMs, tools and methods to simplify use of\nLLMs, and cross-cutting institutional capacity.", "published": "2023-05-09 16:05:36", "link": "http://arxiv.org/abs/2305.05576v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "StrAE: Autoencoding for Pre-Trained Embeddings using Explicit Structure", "abstract": "This work presents StrAE: a Structured Autoencoder framework that through\nstrict adherence to explicit structure, and use of a novel contrastive\nobjective over tree-structured representations, enables effective learning of\nmulti-level representations. Through comparison over different forms of\nstructure, we verify that our results are directly attributable to the\ninformativeness of the structure provided as input, and show that this is not\nthe case for existing tree models. We then further extend StrAE to allow the\nmodel to define its own compositions using a simple localised-merge algorithm.\nThis variant, called Self-StrAE, outperforms baselines that don't involve\nexplicit hierarchical compositions, and is comparable to models given\ninformative structure (e.g. constituency parses). Our experiments are conducted\nin a data-constrained (circa 10M tokens) setting to help tease apart the\ncontribution of the inductive bias to effective learning. However, we find that\nthis framework can be robust to scale, and when extended to a much larger\ndataset (circa 100M tokens), our 430 parameter model performs comparably to a\n6-layer RoBERTa many orders of magnitude larger in size. Our findings support\nthe utility of incorporating explicit composition as an inductive bias for\neffective representation learning.", "published": "2023-05-09 16:20:48", "link": "http://arxiv.org/abs/2305.05588v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Case Records of ChatGPT: Language Models and Complex Clinical\n  Questions", "abstract": "Background: Artificial intelligence language models have shown promise in\nvarious applications, including assisting with clinical decision-making as\ndemonstrated by strong performance of large language models on medical\nlicensure exams. However, their ability to solve complex, open-ended cases,\nwhich may be representative of clinical practice, remains unexplored. Methods:\nIn this study, the accuracy of large language AI models GPT4 and GPT3.5 in\ndiagnosing complex clinical cases was investigated using published Case Records\nof the Massachusetts General Hospital. A total of 50 cases requiring a\ndiagnosis and diagnostic test published from January 1, 2022 to April 16, 2022\nwere identified. For each case, models were given a prompt requesting the top\nthree specific diagnoses and associated diagnostic tests, followed by case\ntext, labs, and figure legends. Model outputs were assessed in comparison to\nthe final clinical diagnosis and whether the model-predicted test would result\nin a correct diagnosis. Results: GPT4 and GPT3.5 accurately provided the\ncorrect diagnosis in 26% and 22% of cases in one attempt, and 46% and 42%\nwithin three attempts, respectively. GPT4 and GPT3.5 provided a correct\nessential diagnostic test in 28% and 24% of cases in one attempt, and 44% and\n50% within three attempts, respectively. No significant differences were found\nbetween the two models, and multiple trials with identical prompts using the\nGPT3.5 model provided similar results. Conclusions: In summary, these models\ndemonstrate potential usefulness in generating differential diagnoses but\nremain limited in their ability to provide a single unifying diagnosis in\ncomplex, open-ended cases. Future research should focus on evaluating model\nperformance in larger datasets of open-ended clinical challenges and exploring\npotential human-AI collaboration strategies to enhance clinical\ndecision-making.", "published": "2023-05-09 16:58:32", "link": "http://arxiv.org/abs/2305.05609v1", "categories": ["cs.CL", "stat.AP"], "primary_category": "cs.CL"}
{"title": "CodeIE: Large Code Generation Models are Better Few-Shot Information\n  Extractors", "abstract": "Large language models (LLMs) pre-trained on massive corpora have demonstrated\nimpressive few-shot learning ability on many NLP tasks. A common practice is to\nrecast the task into a text-to-text format such that generative LLMs of natural\nlanguage (NL-LLMs) like GPT-3 can be prompted to solve it. However, it is\nnontrivial to perform information extraction (IE) tasks with NL-LLMs since the\noutput of the IE task is usually structured and therefore is hard to be\nconverted into plain text. In this paper, we propose to recast the structured\noutput in the form of code instead of natural language and utilize generative\nLLMs of code (Code-LLMs) such as Codex to perform IE tasks, in particular,\nnamed entity recognition and relation extraction. In contrast to NL-LLMs, we\nshow that Code-LLMs can be well-aligned with these IE tasks by designing\ncode-style prompts and formulating these IE tasks as code generation tasks.\nExperiment results on seven benchmarks show that our method consistently\noutperforms fine-tuning moderate-size pre-trained models specially designed for\nIE tasks (e.g., UIE) and prompting NL-LLMs under few-shot settings. We further\nconduct a series of in-depth analyses to demonstrate the merits of leveraging\nCode-LLMs for IE tasks.", "published": "2023-05-09 18:40:31", "link": "http://arxiv.org/abs/2305.05711v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Review of Vision-Language Models and their Performance on the Hateful\n  Memes Challenge", "abstract": "Moderation of social media content is currently a highly manual task, yet\nthere is too much content posted daily to do so effectively. With the advent of\na number of multimodal models, there is the potential to reduce the amount of\nmanual labor for this task. In this work, we aim to explore different models\nand determine what is most effective for the Hateful Memes Challenge, a\nchallenge by Meta designed to further machine learning research in content\nmoderation. Specifically, we explore the differences between early fusion and\nlate fusion models in classifying multimodal memes containing text and images.\nWe first implement a baseline using unimodal models for text and images\nseparately using BERT and ResNet-152, respectively. The outputs from these\nunimodal models were then concatenated together to create a late fusion model.\nIn terms of early fusion models, we implement ConcatBERT, VisualBERT, ViLT,\nCLIP, and BridgeTower. It was found that late fusion performed significantly\nworse than early fusion models, with the best performing model being CLIP which\nachieved an AUROC of 70.06. The code for this work is available at\nhttps://github.com/bzhao18/CS-7643-Project.", "published": "2023-05-09 17:55:29", "link": "http://arxiv.org/abs/2305.06159v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Who Needs Decoders? Efficient Estimation of Sequence-level Attributes", "abstract": "State-of-the-art sequence-to-sequence models often require autoregressive\ndecoding, which can be highly expensive. However, for some downstream tasks\nsuch as out-of-distribution (OOD) detection and resource allocation, the actual\ndecoding output is not needed just a scalar attribute of this sequence. In\nthese scenarios, where for example knowing the quality of a system's output to\npredict poor performance prevails over knowing the output itself, is it\npossible to bypass the autoregressive decoding? We propose Non-Autoregressive\nProxy (NAP) models that can efficiently predict general scalar-valued\nsequence-level attributes. Importantly, NAPs predict these metrics directly\nfrom the encodings, avoiding the expensive autoregressive decoding stage. We\nconsider two sequence-to-sequence task: Machine Translation (MT); and Automatic\nSpeech Recognition (ASR). In OOD for MT, NAPs outperform a deep ensemble while\nbeing significantly faster. NAPs are also shown to be able to predict\nperformance metrics such as BERTScore (MT) or word error rate (ASR). For\ndownstream tasks, such as data filtering and resource optimization, NAPs\ngenerate performance predictions that outperform predictive uncertainty while\nbeing highly inference efficient.", "published": "2023-05-09 00:01:32", "link": "http://arxiv.org/abs/2305.05098v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "FrugalGPT: How to Use Large Language Models While Reducing Cost and\n  Improving Performance", "abstract": "There is a rapidly growing number of large language models (LLMs) that users\ncan query for a fee. We review the cost associated with querying popular LLM\nAPIs, e.g. GPT-4, ChatGPT, J1-Jumbo, and find that these models have\nheterogeneous pricing structures, with fees that can differ by two orders of\nmagnitude. In particular, using LLMs on large collections of queries and text\ncan be expensive. Motivated by this, we outline and discuss three types of\nstrategies that users can exploit to reduce the inference cost associated with\nusing LLMs: 1) prompt adaptation, 2) LLM approximation, and 3) LLM cascade. As\nan example, we propose FrugalGPT, a simple yet flexible instantiation of LLM\ncascade which learns which combinations of LLMs to use for different queries in\norder to reduce cost and improve accuracy. Our experiments show that FrugalGPT\ncan match the performance of the best individual LLM (e.g. GPT-4) with up to\n98% cost reduction or improve the accuracy over GPT-4 by 4% with the same cost.\nThe ideas and findings presented here lay a foundation for using LLMs\nsustainably and efficiently.", "published": "2023-05-09 05:11:02", "link": "http://arxiv.org/abs/2305.05176v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SE"], "primary_category": "cs.LG"}
{"title": "Exploration of Language Dependency for Japanese Self-Supervised Speech\n  Representation Models", "abstract": "Self-supervised learning (SSL) has been dramatically successful not only in\nmonolingual but also in cross-lingual settings. However, since the two settings\nhave been studied individually in general, there has been little research\nfocusing on how effective a cross-lingual model is in comparison with a\nmonolingual model. In this paper, we investigate this fundamental question\nempirically with Japanese automatic speech recognition (ASR) tasks. First, we\nbegin by comparing the ASR performance of cross-lingual and monolingual models\nfor two different language tasks while keeping the acoustic domain as identical\nas possible. Then, we examine how much unlabeled data collected in Japanese is\nneeded to achieve performance comparable to a cross-lingual model pre-trained\nwith tens of thousands of hours of English and/or multilingual data. Finally,\nwe extensively investigate the effectiveness of SSL in Japanese and demonstrate\nstate-of-the-art performance on multiple ASR tasks. Since there is no\ncomprehensive SSL study for Japanese, we hope this study will guide Japanese\nSSL research.", "published": "2023-05-09 06:28:10", "link": "http://arxiv.org/abs/2305.05201v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Robust Acoustic and Semantic Contextual Biasing in Neural Transducers\n  for Speech Recognition", "abstract": "Attention-based contextual biasing approaches have shown significant\nimprovements in the recognition of generic and/or personal rare-words in\nEnd-to-End Automatic Speech Recognition (E2E ASR) systems like neural\ntransducers. These approaches employ cross-attention to bias the model towards\nspecific contextual entities injected as bias-phrases to the model. Prior\napproaches typically relied on subword encoders for encoding the bias phrases.\nHowever, subword tokenizations are coarse and fail to capture granular\npronunciation information which is crucial for biasing based on acoustic\nsimilarity. In this work, we propose to use lightweight character\nrepresentations to encode fine-grained pronunciation features to improve\ncontextual biasing guided by acoustic similarity between the audio and the\ncontextual entities (termed acoustic biasing). We further integrate pretrained\nneural language model (NLM) based encoders to encode the utterance's semantic\ncontext along with contextual entities to perform biasing informed by the\nutterance's semantic context (termed semantic biasing). Experiments using a\nConformer Transducer model on the Librispeech dataset show a 4.62% - 9.26%\nrelative WER improvement on different biasing list sizes over the baseline\ncontextual model when incorporating our proposed acoustic and semantic biasing\napproach. On a large-scale in-house dataset, we observe 7.91% relative WER\nimprovement compared to our baseline model. On tail utterances, the\nimprovements are even more pronounced with 36.80% and 23.40% relative WER\nimprovements on Librispeech rare words and an in-house testset respectively.", "published": "2023-05-09 08:51:44", "link": "http://arxiv.org/abs/2305.05271v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Taxonomy of Foundation Model based Systems through the Lens of\n  Software Architecture", "abstract": "The recent release of large language model (LLM) based chatbots, such as\nChatGPT, has attracted huge interest in foundation models. It is widely\nbelieved that foundation models will serve as the fundamental building blocks\nfor future AI systems. As foundation models are in their early stages, the\ndesign of foundation model based systems has not yet been systematically\nexplored. There is limited understanding about the impact of introducing\nfoundation models in software architecture. Therefore, in this paper, we\npropose a taxonomy of foundation model based systems, which classifies and\ncompares the characteristics of foundation models and design options of\nfoundation model based systems. Our taxonomy comprises three categories: the\npretraining and adaptation of foundation models, the architecture design of\nfoundation model based systems, and responsible-AI-by-design. This taxonomy can\nserve as concrete guidance for making major architectural design decisions when\ndesigning foundation model based systems and highlights trade-offs arising from\ndesign decisions.", "published": "2023-05-09 11:37:16", "link": "http://arxiv.org/abs/2305.05352v6", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Large Language Model Programs", "abstract": "In recent years, large pre-trained language models (LLMs) have demonstrated\nthe ability to follow instructions and perform novel tasks from a few examples.\nThe possibility to parameterise an LLM through such in-context examples widens\ntheir capability at a much lower cost than finetuning. We extend this line of\nreasoning and present a method which further expands the capabilities of an LLM\nby embedding it within an algorithm or program. To demonstrate the benefits of\nthis approach, we present an illustrative example of evidence-supported\nquestion-answering. We obtain a 6.4\\% improvement over the chain of thought\nbaseline through a more algorithmic approach without any finetuning.\nFurthermore, we highlight recent work from this perspective and discuss the\nadvantages and disadvantages in comparison to the standard approaches.", "published": "2023-05-09 11:55:36", "link": "http://arxiv.org/abs/2305.05364v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Consistent Text Categorization using Data Augmentation in e-Commerce", "abstract": "The categorization of massive e-Commerce data is a crucial, well-studied\ntask, which is prevalent in industrial settings. In this work, we aim to\nimprove an existing product categorization model that is already in use by a\nmajor web company, serving multiple applications. At its core, the product\ncategorization model is a text classification model that takes a product title\nas an input and outputs the most suitable category out of thousands of\navailable candidates. Upon a closer inspection, we found inconsistencies in the\nlabeling of similar items. For example, minor modifications of the product\ntitle pertaining to colors or measurements majorly impacted the model's output.\nThis phenomenon can negatively affect downstream recommendation or search\napplications, leading to a sub-optimal user experience.\n  To address this issue, we propose a new framework for consistent text\ncategorization. Our goal is to improve the model's consistency while\nmaintaining its production-level performance. We use a semi-supervised approach\nfor data augmentation and presents two different methods for utilizing\nunlabeled samples. One method relies directly on existing catalogs, while the\nother uses a generative model. We compare the pros and cons of each approach\nand present our experimental results.", "published": "2023-05-09 12:47:28", "link": "http://arxiv.org/abs/2305.05402v2", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Completeness, Recall, and Negation in Open-World Knowledge Bases: A\n  Survey", "abstract": "General-purpose knowledge bases (KBs) are a cornerstone of knowledge-centric\nAI. Many of them are constructed pragmatically from Web sources, and are thus\nfar from complete. This poses challenges for the consumption as well as the\ncuration of their content. While several surveys target the problem of\ncompleting incomplete KBs, the first problem is arguably to know whether and\nwhere the KB is incomplete in the first place, and to which degree.\n  In this survey we discuss how knowledge about completeness, recall, and\nnegation in KBs can be expressed, extracted, and inferred. We cover (i) the\nlogical foundations of knowledge representation and querying under partial\nclosed-world semantics; (ii) the estimation of this information via statistical\npatterns; (iii) the extraction of information about recall from KBs and text;\n(iv) the identification of interesting negative statements; and (v) relaxed\nnotions of relative recall.\n  This survey is targeted at two types of audiences: (1) practitioners who are\ninterested in tracking KB quality, focusing extraction efforts, and building\nquality-aware downstream applications; and (2) data management, knowledge base\nand semantic web researchers who wish to understand the state of the art of\nknowledge bases beyond the open-world assumption. Consequently, our survey\npresents both fundamental methodologies and their working, and gives\npractice-oriented recommendations on how to choose between different approaches\nfor a problem at hand.", "published": "2023-05-09 12:50:16", "link": "http://arxiv.org/abs/2305.05403v2", "categories": ["cs.AI", "cs.CL", "cs.DB", "cs.DL"], "primary_category": "cs.AI"}
{"title": "Effects of sub-word segmentation on performance of transformer language\n  models", "abstract": "Language modeling is a fundamental task in natural language processing, which\nhas been thoroughly explored with various architectures and hyperparameters.\nHowever, few studies focus on the effect of sub-word segmentation on the\nperformance of language models (LMs). In this paper, we compare GPT and BERT\nmodels trained with the statistical segmentation algorithm BPE vs. two\nunsupervised algorithms for morphological segmentation -- Morfessor and\nStateMorph. We train the models for several languages -- including ones with\nvery rich morphology -- and compare their performance with different\nsegmentation algorithms, vocabulary sizes, and model sizes. The results show\nthat training with morphological segmentation allows the LMs to: 1. achieve\nlower perplexity, 2. converge more efficiently in terms of training time, and\n3. achieve equivalent or better evaluation scores on downstream tasks. Lastly,\nwe show 4. that LMs of smaller size using morphological segmentation can\nperform comparably to models of larger size trained with BPE -- both in terms\nof (1) perplexity and (3) scores on downstream tasks. Points (2) and (4) impact\non sustainability of LMs, since they reduce the model cost: size and\ncomputation time. While (2) reduces cost only in the training phase, (4) does\nso also in the inference phase.", "published": "2023-05-09 14:30:29", "link": "http://arxiv.org/abs/2305.05480v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Representation Learning for Person or Entity-centric Knowledge Graphs:\n  An Application in Healthcare", "abstract": "Knowledge graphs (KGs) are a popular way to organise information based on\nontologies or schemas and have been used across a variety of scenarios from\nsearch to recommendation. Despite advances in KGs, representing knowledge\nremains a non-trivial task across industries and it is especially challenging\nin the biomedical and healthcare domains due to complex interdependent\nrelations between entities, heterogeneity, lack of standardization, and\nsparseness of data. KGs are used to discover diagnoses or prioritize genes\nrelevant to disease, but they often rely on schemas that are not centred around\na node or entity of interest, such as a person. Entity-centric KGs are\nrelatively unexplored but hold promise in representing important facets\nconnected to a central node and unlocking downstream tasks beyond graph\ntraversal and reasoning, such as generating graph embeddings and training graph\nneural networks for a wide range of predictive tasks. This paper presents an\nend-to-end representation learning framework to extract entity-centric KGs from\nstructured and unstructured data. We introduce a star-shaped ontology to\nrepresent the multiple facets of a person and use it to guide KG creation.\nCompact representations of the graphs are created leveraging graph neural\nnetworks and experiments are conducted using different levels of heterogeneity\nor explicitness. A readmission prediction task is used to evaluate the results\nof the proposed framework, showing a stable system, robust to missing data,\nthat outperforms a range of baseline machine learning classifiers. We highlight\nthat this approach has several potential applications across domains and is\nopen-sourced. Lastly, we discuss lessons learned, challenges, and next steps\nfor the adoption of the framework in practice.", "published": "2023-05-09 17:39:45", "link": "http://arxiv.org/abs/2305.05640v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Towards Building the Federated GPT: Federated Instruction Tuning", "abstract": "While \"instruction-tuned\" generative large language models (LLMs) have\ndemonstrated an impressive ability to generalize to new tasks, the training\nphases heavily rely on large amounts of diverse and high-quality instruction\ndata (such as ChatGPT and GPT-4). Unfortunately, acquiring high-quality data,\nespecially when it comes to human-written data, can pose significant challenges\nboth in terms of cost and accessibility. Moreover, concerns related to privacy\ncan further limit access to such data, making the process of obtaining it a\ncomplex and nuanced undertaking. Consequently, this hinders the generality of\nthe tuned models and may restrict their effectiveness in certain contexts. To\ntackle this issue, our study introduces a new approach called Federated\nInstruction Tuning (FedIT), which leverages federated learning (FL) as the\nlearning framework for the instruction tuning of LLMs. This marks the first\nexploration of FL-based instruction tuning for LLMs. This is especially\nimportant since text data is predominantly generated by end users. Therefore,\nit is imperative to design and adapt FL approaches to effectively leverage\nthese users' diverse instructions stored on local devices, while preserving\nprivacy and ensuring data security. In the current paper, by conducting widely\nused GPT-4 auto-evaluation, we demonstrate that by exploiting the heterogeneous\nand diverse sets of instructions on the client's end with the proposed\nframework FedIT, we improved the performance of LLMs compared to centralized\ntraining with only limited local instructions. Further, in this paper, we\ndeveloped a Github repository named Shepherd. This repository offers a\nfoundational framework for exploring federated fine-tuning of LLMs using\nheterogeneous instructions across diverse categories.", "published": "2023-05-09 17:42:34", "link": "http://arxiv.org/abs/2305.05644v2", "categories": ["cs.CL", "cs.DC", "cs.SY", "eess.SY"], "primary_category": "cs.CL"}
{"title": "When and What to Ask Through World States and Text Instructions: IGLU\n  NLP Challenge Solution", "abstract": "In collaborative tasks, effective communication is crucial for achieving\njoint goals. One such task is collaborative building where builders must\ncommunicate with each other to construct desired structures in a simulated\nenvironment such as Minecraft. We aim to develop an intelligent builder agent\nto build structures based on user input through dialogue. However, in\ncollaborative building, builders may encounter situations that are difficult to\ninterpret based on the available information and instructions, leading to\nambiguity. In the NeurIPS 2022 Competition NLP Task, we address two key\nresearch questions, with the goal of filling this gap: when should the agent\nask for clarification, and what clarification questions should it ask? We move\ntowards this target with two sub-tasks, a classification task and a ranking\ntask. For the classification task, the goal is to determine whether the agent\nshould ask for clarification based on the current world state and dialogue\nhistory. For the ranking task, the goal is to rank the relevant clarification\nquestions from a pool of candidates. In this report, we briefly introduce our\nmethods for the classification and ranking task. For the classification task,\nour model achieves an F1 score of 0.757, which placed the 3rd on the\nleaderboard. For the ranking task, our model achieves about 0.38 for Mean\nReciprocal Rank by extending the traditional ranking model. Lastly, we discuss\nvarious neural approaches for the ranking task and future direction.", "published": "2023-05-09 20:23:17", "link": "http://arxiv.org/abs/2305.05754v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Ranking & Reweighting Improves Group Distributional Robustness", "abstract": "Recent work has shown that standard training via empirical risk minimization\n(ERM) can produce models that achieve high accuracy on average but low accuracy\non underrepresented groups due to the prevalence of spurious features. A\npredominant approach to tackle this group robustness problem minimizes the\nworst group error (akin to a minimax strategy) on the training data, hoping it\nwill generalize well on the testing data. However, this is often suboptimal,\nespecially when the out-of-distribution (OOD) test data contains previously\nunseen groups. Inspired by ideas from the information retrieval and\nlearning-to-rank literature, this paper first proposes to use Discounted\nCumulative Gain (DCG) as a metric of model quality for facilitating better\nhyperparameter tuning and model selection. Being a ranking-based metric, DCG\nweights multiple poorly-performing groups (instead of considering just the\ngroup with the worst performance). As a natural next step, we build on our\nresults to propose a ranking-based training method called Discounted Rank\nUpweighting (DRU), which differentially reweights a ranked list of\npoorly-performing groups in the training data to learn models that exhibit\nstrong OOD performance on the test data. Results on several synthetic and\nreal-world datasets highlight the superior generalization ability of our\ngroup-ranking-based (akin to soft-minimax) approach in selecting and learning\nmodels that are robust to group distributional shifts.", "published": "2023-05-09 20:37:16", "link": "http://arxiv.org/abs/2305.05759v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "The Vault: A Comprehensive Multilingual Dataset for Advancing Code\n  Understanding and Generation", "abstract": "We present The Vault, a dataset of high-quality code-text pairs in multiple\nprogramming languages for training large language models to understand and\ngenerate code. We present methods for thoroughly extracting samples that use\nboth rule-based and deep learning-based methods to ensure that they contain\nhigh-quality pairs of code and text, resulting in a dataset of 43 million\nhigh-quality code-text pairs. Our extensive evaluations on common coding tasks\nincluding code generation, code search and code summarization show that when\nfine-tuning Code Large Language Models on The Vault, such models outperform the\nsame models trained on other datasets such as CodeSearchNet. We also provide\ndetailed analyses of our datasets to assess the effects of various programming\nlanguages and docstrings on the performance of such models.", "published": "2023-05-09 09:35:03", "link": "http://arxiv.org/abs/2305.06156v2", "categories": ["cs.CL", "cs.AI", "cs.PL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "StarCoder: may the source be with you!", "abstract": "The BigCode community, an open-scientific collaboration working on the\nresponsible development of Large Language Models for Code (Code LLMs),\nintroduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context\nlength, infilling capabilities and fast large-batch inference enabled by\nmulti-query attention. StarCoderBase is trained on 1 trillion tokens sourced\nfrom The Stack, a large collection of permissively licensed GitHub repositories\nwith inspection tools and an opt-out process. We fine-tuned StarCoderBase on\n35B Python tokens, resulting in the creation of StarCoder. We perform the most\ncomprehensive evaluation of Code LLMs to date and show that StarCoderBase\noutperforms every open Code LLM that supports multiple programming languages\nand matches or outperforms the OpenAI code-cushman-001 model. Furthermore,\nStarCoder outperforms every model that is fine-tuned on Python, can be prompted\nto achieve 40\\% pass@1 on HumanEval, and still retains its performance on other\nprogramming languages. We take several important steps towards a safe\nopen-access model release, including an improved PII redaction pipeline and a\nnovel attribution tracing tool, and make the StarCoder models publicly\navailable under a more commercially viable version of the Open Responsible AI\nModel license.", "published": "2023-05-09 08:16:42", "link": "http://arxiv.org/abs/2305.06161v2", "categories": ["cs.CL", "cs.AI", "cs.PL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Fine-tuning Language Models with Generative Adversarial Reward Modelling", "abstract": "Reinforcement Learning with Human Feedback (RLHF) has been demonstrated to\nsignificantly enhance the performance of large language models (LLMs) by\naligning their outputs with desired human values through instruction tuning.\nHowever, RLHF is constrained by the expertise and productivity limitations of\nhuman evaluators. A response to this downside is to fall back to supervised\nfine-tuning (SFT) with additional carefully selected expert demonstrations.\nHowever, while this method has been proven to be effective, it invariably also\nleads to increased human-in-the-loop overhead. In this study, we propose\nanother alternative approach: Reinforcement Learning with Generative\nAdversarial Feedback (RLGAF) to RLHF and SFT, which uses a generative\nadversarial training style to enable the LLMs to learn useful human expert\ndemonstrations without being directly exposed to the training examples, thus\nenabling good generalization capabilities while preserving sample efficiency.\nOur preliminary findings indicate that RLGAF can help align LLMs outputs with\ncompetitive performance against RLHF and SFT, while not suffering from their\nrespective inherent restrictions, suggesting promising avenues for further\nresearch on automating AI alignment.", "published": "2023-05-09 17:06:06", "link": "http://arxiv.org/abs/2305.06176v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "QVoice: Arabic Speech Pronunciation Learning Application", "abstract": "This paper introduces a novel Arabic pronunciation learning application\nQVoice, powered with end-to-end mispronunciation detection and feedback\ngenerator module. The application is designed to support non-native Arabic\nspeakers in enhancing their pronunciation skills, while also helping native\nspeakers mitigate any potential influence from regional dialects on their\nModern Standard Arabic (MSA) pronunciation. QVoice employs various learning\ncues to aid learners in comprehending meaning, drawing connections with their\nexisting knowledge of English language, and offers detailed feedback for\npronunciation correction, along with contextual examples showcasing word usage.\nThe learning cues featured in QVoice encompass a wide range of meaningful\ninformation, such as visualizations of phrases/words and their translations, as\nwell as phonetic transcriptions and transliterations. QVoice provides\npronunciation feedback at the character level and assesses performance at the\nword level.", "published": "2023-05-09 07:21:46", "link": "http://arxiv.org/abs/2305.07445v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "TidyBot: Personalized Robot Assistance with Large Language Models", "abstract": "For a robot to personalize physical assistance effectively, it must learn\nuser preferences that can be generally reapplied to future scenarios. In this\nwork, we investigate personalization of household cleanup with robots that can\ntidy up rooms by picking up objects and putting them away. A key challenge is\ndetermining the proper place to put each object, as people's preferences can\nvary greatly depending on personal taste or cultural background. For instance,\none person may prefer storing shirts in the drawer, while another may prefer\nthem on the shelf. We aim to build systems that can learn such preferences from\njust a handful of examples via prior interactions with a particular person. We\nshow that robots can combine language-based planning and perception with the\nfew-shot summarization capabilities of large language models (LLMs) to infer\ngeneralized user preferences that are broadly applicable to future\ninteractions. This approach enables fast adaptation and achieves 91.2% accuracy\non unseen objects in our benchmark dataset. We also demonstrate our approach on\na real-world mobile manipulator called TidyBot, which successfully puts away\n85.0% of objects in real-world test scenarios.", "published": "2023-05-09 17:52:59", "link": "http://arxiv.org/abs/2305.05658v2", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Semi-Supervised Federated Learning for Keyword Spotting", "abstract": "Keyword Spotting (KWS) is a critical aspect of audio-based applications on\nmobile devices and virtual assistants. Recent developments in Federated\nLearning (FL) have significantly expanded the ability to train machine learning\nmodels by utilizing the computational and private data resources of numerous\ndistributed devices. However, existing FL methods typically require that\ndevices possess accurate ground-truth labels, which can be both expensive and\nimpractical when dealing with local audio data. In this study, we first\ndemonstrate the effectiveness of Semi-Supervised Federated Learning (SSL) and\nFL for KWS. We then extend our investigation to Semi-Supervised Federated\nLearning (SSFL) for KWS, where devices possess completely unlabeled data, while\nthe server has access to a small amount of labeled data. We perform numerical\nanalyses using state-of-the-art SSL, FL, and SSFL techniques to demonstrate\nthat the performance of KWS models can be significantly improved by leveraging\nthe abundant unlabeled heterogeneous data available on devices.", "published": "2023-05-09 00:46:12", "link": "http://arxiv.org/abs/2305.05110v1", "categories": ["cs.LG", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Joint Multi-scale Cross-lingual Speaking Style Transfer with\n  Bidirectional Attention Mechanism for Automatic Dubbing", "abstract": "Automatic dubbing, which generates a corresponding version of the input\nspeech in another language, could be widely utilized in many real-world\nscenarios such as video and game localization. In addition to synthesizing the\ntranslated scripts, automatic dubbing needs to further transfer the speaking\nstyle in the original language to the dubbed speeches to give audiences the\nimpression that the characters are speaking in their native tongue. However,\nstate-of-the-art automatic dubbing systems only model the transfer on duration\nand speaking rate, neglecting the other aspects in speaking style such as\nemotion, intonation and emphasis which are also crucial to fully perform the\ncharacters and speech understanding. In this paper, we propose a joint\nmulti-scale cross-lingual speaking style transfer framework to simultaneously\nmodel the bidirectional speaking style transfer between languages at both\nglobal (i.e. utterance level) and local (i.e. word level) scales. The global\nand local speaking styles in each language are extracted and utilized to\npredicted the global and local speaking styles in the other language with an\nencoder-decoder framework for each direction and a shared bidirectional\nattention mechanism for both directions. A multi-scale speaking style enhanced\nFastSpeech 2 is then utilized to synthesize the predicted the global and local\nspeaking styles to speech for each language. Experiment results demonstrate the\neffectiveness of our proposed framework, which outperforms a baseline with only\nduration transfer in both objective and subjective evaluations.", "published": "2023-05-09 06:44:34", "link": "http://arxiv.org/abs/2305.05203v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learn to Sing by Listening: Building Controllable Virtual Singer by\n  Unsupervised Learning from Voice Recordings", "abstract": "The virtual world is being established in which digital humans are created\nindistinguishable from real humans. Producing their audio-related capabilities\nis crucial since voice conveys extensive personal characteristics. We aim to\ncreate a controllable audio-form virtual singer; however, supervised modeling\nand controlling all different factors of the singing voice, such as timbre,\ntempo, pitch, and lyrics, is extremely difficult since accurately labeling all\nsuch information needs enormous labor work. In this paper, we propose a\nframework that could digitize a person's voice by simply \"listening\" to the\nclean voice recordings of any content in a fully unsupervised manner and\npredict singing voices even only using speaking recordings. A variational\nauto-encoder (VAE) based framework is developed, which leverages a set of\npre-trained models to encode the audio as various hidden embeddings\nrepresenting different factors of the singing voice, and further decodes the\nembeddings into raw audio. By manipulating the hidden embeddings for different\nfactors, the resulting singing voices can be controlled, and new virtual\nsingers can also be further generated by interpolating between timbres.\nEvaluations of different types of experiments demonstrate the proposed method's\neffectiveness. The proposed method is the critical technique for producing the\nAI choir, which empowered the human-AI symbiotic orchestra in Hong Kong in July\n2022.", "published": "2023-05-09 12:45:45", "link": "http://arxiv.org/abs/2305.05401v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Exploration into the Performance of Unsupervised Cross-Task Speech\n  Representations for \"In the Wild'' Edge Applications", "abstract": "Unsupervised speech models are becoming ubiquitous in the speech and machine\nlearning communities. Upstream models are responsible for learning meaningful\nrepresentations from raw audio. Later, these representations serve as input to\ndownstream models to solve a number of tasks, such as keyword spotting or\nemotion recognition. As edge speech applications start to emerge, it is\nimportant to gauge how robust these cross-task representations are on edge\ndevices with limited resources and different noise levels. To this end, in this\nstudy we evaluate the robustness of four different versions of HuBERT, namely:\nbase, large, and extra-large versions, as well as a recent version termed\nRobust-HuBERT. Tests are conducted under different additive and convolutive\nnoise conditions for three downstream tasks: keyword spotting, intent\nclassification, and emotion recognition. Our results show that while larger\nmodels can provide some important robustness to environmental factors, they may\nnot be applicable to edge applications. Smaller models, on the other hand,\nshowed substantial accuracy drops in noisy conditions, especially in the\npresence of room reverberation. These findings suggest that cross-task speech\nrepresentations are not yet ready for edge applications and innovations are\nstill needed.", "published": "2023-05-09 13:37:54", "link": "http://arxiv.org/abs/2305.05443v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Accurate Real-Time Estimation of 2-Dimensional Direction of Arrival\n  using a 3-Microphone Array", "abstract": "This paper presents a method for real-time estimation of 2-dimensional\ndirection of arrival (2D-DOA) of one or more sound sources using a nonlinear\narray of three microphones. 2D-DOA is estimated employing frame-level time\ndifference of arrival (TDOA) measurements. Unlike conventional methods, which\ninfer location parameters from TDOAs using a theoretical model, we propose a\nmore practical approach based on supervised learning. The proposed model\nemploys nearest neighbor search (NNS) applied to a spherical Fibonacci lattice\nconsisting of TDOA to 2D-DOA mappings learned directly in the field. Filtering\nand clustering post-processors are also introduced for improved source\ndetection and localization robustness.", "published": "2023-05-09 17:20:22", "link": "http://arxiv.org/abs/2305.05630v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Temporal Convolution Network Based Onset Detection and Query by Humming\n  System Design", "abstract": "Onsets are a key factor to split audio into several notes. In this paper, we\nensemble multiple temporal convolution network (TCN) based model and utilize a\nrestricted frequency range spectrogram to achieve more robust onset detection.\nDifferent from the present onset detection of QBH system which is only\navailable in a clean scenario, our proposal of onset detection and speech\nenhancement can prevent noise from affecting onset detection function (ODF).\nCompared to the CNN model which exploits spatial features of the spectrogram,\nthe TCN model exploits both spatial and temporal features of the spectrogram.\nAs the usage of QBH in noisy scenarios, we apply the TCN-based speech\nenhancement as a preprocessor of QBH. With the combinations of TCN-based speech\nenhancement and onset detection, simulations show that the proposal can enable\nthe QBH system in both noisy and clean circumstances with short response time.", "published": "2023-05-09 02:52:27", "link": "http://arxiv.org/abs/2305.05139v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Who is Speaking Actually? Robust and Versatile Speaker Traceability for\n  Voice Conversion", "abstract": "Voice conversion (VC), as a voice style transfer technology, is becoming\nincreasingly prevalent while raising serious concerns about its illegal use.\nProactively tracing the origins of VC-generated speeches, i.e., speaker\ntraceability, can prevent the misuse of VC, but unfortunately has not been\nextensively studied. In this paper, we are the first to investigate the speaker\ntraceability for VC and propose a traceable VC framework named VoxTracer. Our\nVoxTracer is similar to but beyond the paradigm of audio watermarking. We first\nuse unique speaker embedding to represent speaker identity. Then we design a\nVAE-Glow structure, in which the hiding process imperceptibly integrates the\nsource speaker identity into the VC, and the tracing process accurately\nrecovers the source speaker identity and even the source speech in spite of\nsevere speech quality degradation. To address the speech mismatch between the\nhiding and tracing processes affected by different distortions, we also adopt\nan asynchronous training strategy to optimize the VAE-Glow models. The\nVoxTracer is versatile enough to be applied to arbitrary VC methods and popular\naudio coding standards. Extensive experiments demonstrate that the VoxTracer\nachieves not only high imperceptibility in hiding, but also nearly 100% tracing\naccuracy against various types of audio lossy compressions (AAC, MP3, Opus and\nSILK) with a broad range of bitrates (16 kbps - 128 kbps) even in a very short\ntime duration (0.74s). Our speech demo is available at\nhttps://anonymous.4open.science/w/DEMOofVoxTracer.", "published": "2023-05-09 03:33:12", "link": "http://arxiv.org/abs/2305.05152v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Privacy in Speech Technology", "abstract": "Speech technology for communication, accessing information and services has\nrapidly improved in quality. It is convenient and appealing because speech is\nthe primary mode of communication for humans. Such technology however also\npresents proven threats to privacy. Speech is a tool for communication and it\nwill thus inherently contain private information. Importantly, it however also\ncontains a wealth of side information, such as information related to health,\nemotions, affiliations, and relationships, all of which are private. Exposing\nsuch private information can lead to serious threats such as price gouging,\nharassment, extortion, and stalking. This paper is a tutorial on privacy issues\nrelated to speech technology, modeling their threats, approaches for protecting\nusers' privacy, measuring the performance of privacy-protecting methods,\nperception of privacy as well as societal and legal consequences. In addition\nto a tutorial overview, it also presents lines for further development where\nimprovements are most urgently needed.", "published": "2023-05-09 07:41:36", "link": "http://arxiv.org/abs/2305.05227v2", "categories": ["eess.AS", "cs.CR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AudioSlots: A slot-centric generative model for audio separation", "abstract": "In a range of recent works, object-centric architectures have been shown to\nbe suitable for unsupervised scene decomposition in the vision domain. Inspired\nby these methods we present AudioSlots, a slot-centric generative model for\nblind source separation in the audio domain. AudioSlots is built using\npermutation-equivariant encoder and decoder networks. The encoder network based\non the Transformer architecture learns to map a mixed audio spectrogram to an\nunordered set of independent source embeddings. The spatial broadcast decoder\nnetwork learns to generate the source spectrograms from the source embeddings.\nWe train the model in an end-to-end manner using a permutation invariant loss\nfunction. Our results on Libri2Mix speech separation constitute a proof of\nconcept that this approach shows promise. We discuss the results and\nlimitations of our approach in detail, and further outline potential ways to\novercome the limitations and directions for future work.", "published": "2023-05-09 16:28:07", "link": "http://arxiv.org/abs/2305.05591v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Inter-SubNet: Speech Enhancement with Subband Interaction", "abstract": "Subband-based approaches process subbands in parallel through the model with\nshared parameters to learn the commonality of local spectrums for noise\nreduction. In this way, they have achieved remarkable results with fewer\nparameters. However, in some complex environments, the lack of global spectral\ninformation has a negative impact on the performance of these subband-based\napproaches. To this end, this paper introduces the subband interaction as a new\nway to complement the subband model with the global spectral information such\nas cross-band dependencies and global spectral patterns, and proposes a new\nlightweight single-channel speech enhancement framework called Interactive\nSubband Network (Inter-SubNet). Experimental results on DNS Challenge -\nInterspeech 2021 dataset show that the proposed Inter-SubNet yields a\nsignificant improvement over the subband model and outperforms other\nstate-of-the-art speech enhancement approaches, which demonstrate the\neffectiveness of subband interaction.", "published": "2023-05-09 16:47:20", "link": "http://arxiv.org/abs/2305.05599v1", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "VSMask: Defending Against Voice Synthesis Attack via Real-Time\n  Predictive Perturbation", "abstract": "Deep learning based voice synthesis technology generates artificial\nhuman-like speeches, which has been used in deepfakes or identity theft\nattacks. Existing defense mechanisms inject subtle adversarial perturbations\ninto the raw speech audios to mislead the voice synthesis models. However,\noptimizing the adversarial perturbation not only consumes substantial\ncomputation time, but it also requires the availability of entire speech.\nTherefore, they are not suitable for protecting live speech streams, such as\nvoice messages or online meetings. In this paper, we propose VSMask, a\nreal-time protection mechanism against voice synthesis attacks. Different from\noffline protection schemes, VSMask leverages a predictive neural network to\nforecast the most effective perturbation for the upcoming streaming speech.\nVSMask introduces a universal perturbation tailored for arbitrary speech input\nto shield a real-time speech in its entirety. To minimize the audio distortion\nwithin the protected speech, we implement a weight-based perturbation\nconstraint to reduce the perceptibility of the added perturbation. We\ncomprehensively evaluate VSMask protection performance under different\nscenarios. The experimental results indicate that VSMask can effectively defend\nagainst 3 popular voice synthesis models. None of the synthetic voice could\ndeceive the speaker verification models or human ears with VSMask protection.\nIn a physical world experiment, we demonstrate that VSMask successfully\nsafeguards the real-time speech by injecting the perturbation over the air.", "published": "2023-05-09 19:31:58", "link": "http://arxiv.org/abs/2305.05736v1", "categories": ["cs.SD", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Enhancing Gappy Speech Audio Signals with Generative Adversarial\n  Networks", "abstract": "Gaps, dropouts and short clips of corrupted audio are a common problem and\nparticularly annoying when they occur in speech. This paper uses machine\nlearning to regenerate gaps of up to 320ms in an audio speech signal. Audio\nregeneration is translated into image regeneration by transforming audio into a\nMel-spectrogram and using image in-painting to regenerate the gaps. The full\nMel-spectrogram is then transferred back to audio using the Parallel-WaveGAN\nvocoder and integrated into the audio stream. Using a sample of 1300 spoken\naudio clips of between 1 and 10 seconds taken from the publicly-available\nLJSpeech dataset our results show regeneration of audio gaps in close to real\ntime using GANs with a GPU equipped system. As expected, the smaller the gap in\nthe audio, the better the quality of the filled gaps. On a gap of 240ms the\naverage mean opinion score (MOS) for the best performing models was 3.737, on a\nscale of 1 (worst) to 5 (best) which is sufficient for a human to perceive as\nclose to uninterrupted human speech.", "published": "2023-05-09 21:58:54", "link": "http://arxiv.org/abs/2305.05780v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Zero-shot personalized lip-to-speech synthesis with face image based\n  voice control", "abstract": "Lip-to-Speech (Lip2Speech) synthesis, which predicts corresponding speech\nfrom talking face images, has witnessed significant progress with various\nmodels and training strategies in a series of independent studies. However,\nexisting studies can not achieve voice control under zero-shot condition,\nbecause extra speaker embeddings need to be extracted from natural reference\nspeech and are unavailable when only the silent video of an unseen speaker is\ngiven. In this paper, we propose a zero-shot personalized Lip2Speech synthesis\nmethod, in which face images control speaker identities. A variational\nautoencoder is adopted to disentangle the speaker identity and linguistic\ncontent representations, which enables speaker embeddings to control the voice\ncharacteristics of synthetic speech for unseen speakers. Furthermore, we\npropose associated cross-modal representation learning to promote the ability\nof face-based speaker embeddings (FSE) on voice control. Extensive experiments\nverify the effectiveness of the proposed method whose synthetic utterances are\nmore natural and matching with the personality of input video than the compared\nmethods. To our best knowledge, this paper makes the first attempt on zero-shot\npersonalized Lip2Speech synthesis with a face image rather than reference audio\nto control voice characteristics.", "published": "2023-05-09 02:37:29", "link": "http://arxiv.org/abs/2305.14359v1", "categories": ["cs.MM", "cs.AI", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
