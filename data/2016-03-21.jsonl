{"title": "Static and Dynamic Feature Selection in Morphosyntactic Analyzers", "abstract": "We study the use of greedy feature selection methods for morphosyntactic\ntagging under a number of different conditions. We compare a static ordering of\nfeatures to a dynamic ordering based on mutual information statistics, and we\napply the techniques to standalone taggers as well as joint systems for tagging\nand parsing. Experiments on five languages show that feature selection can\nresult in more compact models as well as higher accuracy under all conditions,\nbut also that a dynamic ordering works better than a static ordering and that\njoint systems benefit more than standalone taggers. We also show that the same\ntechniques can be used to select which morphosyntactic categories to predict in\norder to maximize syntactic accuracy in a joint system. Our final results\nrepresent a substantial improvement of the state of the art for several\nlanguages, while at the same time reducing both the number of features and the\nrunning time by up to 80% in some cases.", "published": "2016-03-21 17:20:34", "link": "http://arxiv.org/abs/1603.06503v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stack-propagation: Improved Representation Learning for Syntax", "abstract": "Traditional syntax models typically leverage part-of-speech (POS) information\nby constructing features from hand-tuned templates. We demonstrate that a\nbetter approach is to utilize POS tags as a regularizer of learned\nrepresentations. We propose a simple method for learning a stacked pipeline of\nmodels which we call \"stack-propagation\". We apply this to dependency parsing\nand tagging, where we use the hidden layer of the tagger network as a\nrepresentation of the input tokens for the parser. At test time, our parser\ndoes not require predicted POS tags. On 19 languages from the Universal\nDependencies, our method is 1.3% (absolute) more accurate than a\nstate-of-the-art graph-based approach and 2.7% more accurate than the most\ncomparable greedy model.", "published": "2016-03-21 20:12:44", "link": "http://arxiv.org/abs/1603.06598v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bayesian Neural Word Embedding", "abstract": "Recently, several works in the domain of natural language processing\npresented successful methods for word embedding. Among them, the Skip-Gram with\nnegative sampling, known also as word2vec, advanced the state-of-the-art of\nvarious linguistics tasks. In this paper, we propose a scalable Bayesian neural\nword embedding algorithm. The algorithm relies on a Variational Bayes solution\nfor the Skip-Gram objective and a detailed step by step description is\nprovided. We present experimental results that demonstrate the performance of\nthe proposed algorithm for word analogy and similarity tasks on six different\ndatasets and show it is competitive with the original Skip-Gram method.", "published": "2016-03-21 16:32:06", "link": "http://arxiv.org/abs/1603.06571v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Harnessing Deep Neural Networks with Logic Rules", "abstract": "Combining deep neural networks with structured logic rules is desirable to\nharness flexibility and reduce uninterpretability of the neural models. We\npropose a general framework capable of enhancing various types of neural\nnetworks (e.g., CNNs and RNNs) with declarative first-order logic rules.\nSpecifically, we develop an iterative distillation method that transfers the\nstructured information of logic rules into the weights of neural networks. We\ndeploy the framework on a CNN for sentiment analysis, and an RNN for named\nentity recognition. With a few highly intuitive rules, we obtain substantial\nimprovements and achieve state-of-the-art or comparable results to previous\nbest-performing systems.", "published": "2016-03-21 03:33:20", "link": "http://arxiv.org/abs/1603.06318v6", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Incorporating Copying Mechanism in Sequence-to-Sequence Learning", "abstract": "We address an important problem in sequence-to-sequence (Seq2Seq) learning\nreferred to as copying, in which certain segments in the input sequence are\nselectively replicated in the output sequence. A similar phenomenon is\nobservable in human language communication. For example, humans tend to repeat\nentity names or even long phrases in conversation. The challenge with regard to\ncopying in Seq2Seq is that new machinery is needed to decide when to perform\nthe operation. In this paper, we incorporate copying into neural network-based\nSeq2Seq learning and propose a new model called CopyNet with encoder-decoder\nstructure. CopyNet can nicely integrate the regular way of word generation in\nthe decoder with the new copying mechanism which can choose sub-sequences in\nthe input sequence and put them at proper places in the output sequence. Our\nempirical study on both synthetic data sets and real world data sets\ndemonstrates the efficacy of CopyNet. For example, CopyNet can outperform\nregular RNN-based model with remarkable margins on text summarization tasks.", "published": "2016-03-21 11:35:08", "link": "http://arxiv.org/abs/1603.06393v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "A System for Probabilistic Linking of Thesauri and Classification\n  Systems", "abstract": "This paper presents a system which creates and visualizes probabilistic\nsemantic links between concepts in a thesaurus and classes in a classification\nsystem. For creating the links, we build on the Polylingual Labeled Topic Model\n(PLL-TM). PLL-TM identifies probable thesaurus descriptors for each class in\nthe classification system by using information from the natural language text\nof documents, their assigned thesaurus descriptors and their designated\nclasses. The links are then presented to users of the system in an interactive\nvisualization, providing them with an automatically generated overview of the\nrelations between the thesaurus and the classification system.", "published": "2016-03-21 16:34:13", "link": "http://arxiv.org/abs/1603.06485v1", "categories": ["cs.AI", "cs.CL", "cs.DL"], "primary_category": "cs.AI"}
