{"title": "Aligning Coordinated Text Streams through Burst Information Network\n  Construction and Decipherment", "abstract": "Aligning coordinated text streams from multiple sources and multiple\nlanguages has opened many new research venues on cross-lingual knowledge\ndiscovery. In this paper we aim to advance state-of-the-art by: (1). extending\ncoarse-grained topic-level knowledge mining to fine-grained information units\nsuch as entities and events; (2). following a novel\nData-to-Network-to-Knowledge (D2N2K) paradigm to construct and utilize network\nstructures to capture and propagate reliable evidence. We introduce a novel\nBurst Information Network (BINet) representation that can display the most\nimportant information and illustrate the connections among bursty entities,\nevents and keywords in the corpus. We propose an effective approach to\nconstruct and decipher BINets, incorporating novel criteria based on\nmulti-dimensional clues from pronunciation, translation, burst, neighbor and\ngraph topological structure. The experimental results on Chinese and English\ncoordinated text streams show that our approach can accurately decipher the\nnodes with high confidence in the BINets and that the algorithm can be\nefficiently run in parallel, which makes it possible to apply it to huge\namounts of streaming data for never-ending language and information\ndecipherment.", "published": "2016-09-27 01:19:41", "link": "http://arxiv.org/abs/1609.08237v1", "categories": ["cs.CL", "H.2.8; I.2.7"], "primary_category": "cs.CL"}
{"title": "The Effects of Data Size and Frequency Range on Distributional Semantic\n  Models", "abstract": "This paper investigates the effects of data size and frequency range on\ndistributional semantic models. We compare the performance of a number of\nrepresentative models for several test settings over data of varying sizes, and\nover test items of various frequency. Our results show that neural\nnetwork-based models underperform when the data is small, and that the most\nreliable model over data of varying sizes and frequency ranges is the inverted\nfactorized model.", "published": "2016-09-27 07:38:29", "link": "http://arxiv.org/abs/1609.08293v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "emoji2vec: Learning Emoji Representations from their Description", "abstract": "Many current natural language processing applications for social media rely\non representation learning and utilize pre-trained word embeddings. There\ncurrently exist several publicly-available, pre-trained sets of word\nembeddings, but they contain few or no emoji representations even as emoji\nusage in social media has increased. In this paper we release emoji2vec,\npre-trained embeddings for all Unicode emoji which are learned from their\ndescription in the Unicode emoji standard. The resulting emoji embeddings can\nbe readily used in downstream social natural language processing applications\nalongside word2vec. We demonstrate, for the downstream task of sentiment\nanalysis, that emoji embeddings learned from short descriptions outperforms a\nskip-gram model trained on a large collection of tweets, while avoiding the\nneed for contexts in which emoji need to appear frequently in order to estimate\na representation.", "published": "2016-09-27 11:32:25", "link": "http://arxiv.org/abs/1609.08359v2", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "OC16-CE80: A Chinese-English Mixlingual Database and A Speech\n  Recognition Baseline", "abstract": "We present the OC16-CE80 Chinese-English mixlingual speech database which was\nreleased as a main resource for training, development and test for the\nChinese-English mixlingual speech recognition (MixASR-CHEN) challenge on\nO-COCOSDA 2016. This database consists of 80 hours of speech signals recorded\nfrom more than 1,400 speakers, where the utterances are in Chinese but each\ninvolves one or several English words. Based on the database and another two\nfree data resources (THCHS30 and the CMU dictionary), a speech recognition\n(ASR) baseline was constructed with the deep neural network-hidden Markov model\n(DNN-HMM) hybrid system. We then report the baseline results following the\nMixASR-CHEN evaluation rules and demonstrate that OC16-CE80 is a reasonable\ndata resource for mixlingual research.", "published": "2016-09-27 13:25:51", "link": "http://arxiv.org/abs/1609.08412v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Reinforcement Learning for Mention-Ranking Coreference Models", "abstract": "Coreference resolution systems are typically trained with heuristic loss\nfunctions that require careful tuning. In this paper we instead apply\nreinforcement learning to directly optimize a neural mention-ranking model for\ncoreference evaluation metrics. We experiment with two approaches: the\nREINFORCE policy gradient algorithm and a reward-rescaled max-margin objective.\nWe find the latter to be more effective, resulting in significant improvements\nover the current state-of-the-art on the English and Chinese portions of the\nCoNLL 2012 Shared Task.", "published": "2016-09-27 21:00:26", "link": "http://arxiv.org/abs/1609.08667v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Hackathon for Classical Tibetan", "abstract": "We describe the course of a hackathon dedicated to the development of\nlinguistic tools for Tibetan Buddhist studies. Over a period of five days, a\ngroup of seventeen scholars, scientists, and students developed and compared\nalgorithms for intertextual alignment and text classification, along with some\nbasic language tools, including a stemmer and word segmenter.", "published": "2016-09-27 12:55:10", "link": "http://arxiv.org/abs/1609.08389v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Modelling Radiological Language with Bidirectional Long Short-Term\n  Memory Networks", "abstract": "Motivated by the need to automate medical information extraction from\nfree-text radiological reports, we present a bi-directional long short-term\nmemory (BiLSTM) neural network architecture for modelling radiological\nlanguage. The model has been used to address two NLP tasks: medical\nnamed-entity recognition (NER) and negation detection. We investigate whether\nlearning several types of word embeddings improves BiLSTM's performance on\nthose tasks. Using a large dataset of chest x-ray reports, we compare the\nproposed model to a baseline dictionary-based NER system and a negation\ndetection system that leverages the hand-crafted rules of the NegEx algorithm\nand the grammatical relations obtained from the Stanford Dependency Parser.\nCompared to these more traditional rule-based systems, we argue that BiLSTM\noffers a strong alternative for both our tasks.", "published": "2016-09-27 13:25:10", "link": "http://arxiv.org/abs/1609.08409v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Local Training for PLDA in Speaker Verification", "abstract": "PLDA is a popular normalization approach for the i-vector model, and it has\ndelivered state-of-the-art performance in speaker verification. However, PLDA\ntraining requires a large amount of labeled development data, which is highly\nexpensive in most cases. A possible approach to mitigate the problem is various\nunsupervised adaptation methods, which use unlabeled data to adapt the PLDA\nscattering matrices to the target domain.\n  In this paper, we present a new `local training' approach that utilizes\ninaccurate but much cheaper local labels to train the PLDA model. These local\nlabels discriminate speakers within a single conversion only, and so are much\neasier to obtain compared to the normal `global labels'. Our experiments show\nthat the proposed approach can deliver significant performance improvement,\nparticularly with limited globally-labeled data.", "published": "2016-09-27 13:37:13", "link": "http://arxiv.org/abs/1609.08433v1", "categories": ["cs.SD", "cs.CL"], "primary_category": "cs.SD"}
{"title": "Collaborative Learning for Language and Speaker Recognition", "abstract": "This paper presents a unified model to perform language and speaker\nrecognition simultaneously and altogether. The model is based on a multi-task\nrecurrent neural network where the output of one task is fed as the input of\nthe other, leading to a collaborative learning framework that can improve both\nlanguage and speaker recognition by borrowing information from each other. Our\nexperiments demonstrated that the multi-task model outperforms the\ntask-specific models on both tasks.", "published": "2016-09-27 13:48:01", "link": "http://arxiv.org/abs/1609.08442v2", "categories": ["cs.SD", "cs.CL"], "primary_category": "cs.SD"}
{"title": "AP16-OL7: A Multilingual Database for Oriental Languages and A Language\n  Recognition Baseline", "abstract": "We present the AP16-OL7 database which was released as the training and test\ndata for the oriental language recognition (OLR) challenge on APSIPA 2016.\nBased on the database, a baseline system was constructed on the basis of the\ni-vector model. We report the baseline results evaluated in various metrics\ndefined by the AP16-OLR evaluation plan and demonstrate that AP16-OL7 is a\nreasonable data resource for multilingual research.", "published": "2016-09-27 13:50:13", "link": "http://arxiv.org/abs/1609.08445v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WS4A: a Biomedical Question and Answering System based on public Web\n  Services and Ontologies", "abstract": "This paper describes our system, dubbed WS4A (Web Services for All), that\nparticipated in the fourth edition of the BioASQ challenge (2016). We used WS4A\nto perform the Question and Answering (QA) task 4b, which consisted on the\nretrieval of relevant concepts, documents, snippets, RDF triples, exact answers\nand ideal answers for each given question. The novelty in our approach consists\non the maximum exploitation of existing web services in each step of WS4A, such\nas the annotation of text, and the retrieval of metadata for each annotation.\nThe information retrieved included concept identifiers, ontologies, ancestors,\nand most importantly, PubMed identifiers. The paper describes the WS4A pipeline\nand also presents the precision, recall and f-measure values obtained in task\n4b. Our system achieved two second places in two subtasks on one of the five\nbatches.", "published": "2016-09-27 15:14:04", "link": "http://arxiv.org/abs/1609.08492v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Multi-task Recurrent Model for True Multilingual Speech Recognition", "abstract": "Research on multilingual speech recognition remains attractive yet\nchallenging. Recent studies focus on learning shared structures under the\nmulti-task paradigm, in particular a feature sharing structure. This approach\nhas been found effective to improve performance on each individual language.\nHowever, this approach is only useful when the deployed system supports just\none language. In a true multilingual scenario where multiple languages are\nallowed, performance will be significantly reduced due to the competition among\nlanguages in the decoding space. This paper presents a multi-task recurrent\nmodel that involves a multilingual speech recognition (ASR) component and a\nlanguage recognition (LR) component, and the ASR component is informed of the\nlanguage information by the LR component, leading to a language-aware\nrecognition. We tested the approach on an English-Chinese bilingual recognition\ntask. The results show that the proposed multi-task recurrent model can improve\nperformance of multilingual recognition systems.", "published": "2016-09-27 09:56:09", "link": "http://arxiv.org/abs/1609.08337v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Weakly Supervised PLDA Training", "abstract": "PLDA is a popular normalization approach for the i-vector model, and it has\ndelivered state-of-the-art performance in speaker verification. However, PLDA\ntraining requires a large amount of labelled development data, which is highly\nexpensive in most cases. We present a cheap PLDA training approach, which\nassumes that speakers in the same session can be easily separated, and speakers\nin different sessions are simply different. This results in `weak labels' which\nare not fully accurate but cheap, leading to a weak PLDA training.\n  Our experimental results on real-life large-scale telephony customer service\nachieves demonstrated that the weak training can offer good performance when\nhuman-labelled data are limited. More interestingly, the weak training can be\nemployed as a discriminative adaptation approach, which is more efficient than\nthe prevailing unsupervised method when human-labelled data are insufficient.", "published": "2016-09-27 13:46:55", "link": "http://arxiv.org/abs/1609.08441v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "cs.LG"}
{"title": "Topic Modeling over Short Texts by Incorporating Word Embeddings", "abstract": "Inferring topics from the overwhelming amount of short texts becomes a\ncritical but challenging task for many content analysis tasks, such as content\ncharactering, user interest profiling, and emerging topic detecting. Existing\nmethods such as probabilistic latent semantic analysis (PLSA) and latent\nDirichlet allocation (LDA) cannot solve this prob- lem very well since only\nvery limited word co-occurrence information is available in short texts. This\npaper studies how to incorporate the external word correlation knowledge into\nshort texts to improve the coherence of topic modeling. Based on recent results\nin word embeddings that learn se- mantically representations for words from a\nlarge corpus, we introduce a novel method, Embedding-based Topic Model (ETM),\nto learn latent topics from short texts. ETM not only solves the problem of\nvery limited word co-occurrence information by aggregating short texts into\nlong pseudo- texts, but also utilizes a Markov Random Field regularized model\nthat gives correlated words a better chance to be put into the same topic. The\nexperiments on real-world datasets validate the effectiveness of our model\ncomparing with the state-of-the-art models.", "published": "2016-09-27 15:26:07", "link": "http://arxiv.org/abs/1609.08496v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Optimizing Neural Network Hyperparameters with Gaussian Processes for\n  Dialog Act Classification", "abstract": "Systems based on artificial neural networks (ANNs) have achieved\nstate-of-the-art results in many natural language processing tasks. Although\nANNs do not require manually engineered features, ANNs have many\nhyperparameters to be optimized. The choice of hyperparameters significantly\nimpacts models' performances. However, the ANN hyperparameters are typically\nchosen by manual, grid, or random search, which either requires expert\nexperiences or is computationally expensive. Recent approaches based on\nBayesian optimization using Gaussian processes (GPs) is a more systematic way\nto automatically pinpoint optimal or near-optimal machine learning\nhyperparameters. Using a previously published ANN model yielding\nstate-of-the-art results for dialog act classification, we demonstrate that\noptimizing hyperparameters using GP further improves the results, and reduces\nthe computational time by a factor of 4 compared to a random search. Therefore\nit is a useful technique for tuning ANN models to yield the best performances\nfor natural language processing tasks.", "published": "2016-09-27 23:10:42", "link": "http://arxiv.org/abs/1609.08703v1", "categories": ["cs.CL", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
