{"title": "Your Autoregressive Generative Model Can be Better If You Treat It as an\n  Energy-Based One", "abstract": "Autoregressive generative models are commonly used, especially for those\ntasks involving sequential data. They have, however, been plagued by a slew of\ninherent flaws due to the intrinsic characteristics of chain-style conditional\nmodeling (e.g., exposure bias or lack of long-range coherence), severely\nlimiting their ability to model distributions properly. In this paper, we\npropose a unique method termed E-ARM for training autoregressive generative\nmodels that takes advantage of a well-designed energy-based learning objective.\nBy leveraging the extra degree of freedom of the softmax operation, we are\nallowed to make the autoregressive model itself be an energy-based model for\nmeasuring the likelihood of input without introducing any extra parameters.\nFurthermore, we show that E-ARM can be trained efficiently and is capable of\nalleviating the exposure bias problem and increase temporal coherence for\nautoregressive generative models. Extensive empirical results, covering\nbenchmarks like language modeling, neural machine translation, and image\ngeneration, demonstrate the effectiveness of the proposed approach.", "published": "2022-06-26 10:58:41", "link": "http://arxiv.org/abs/2206.12840v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Contextual embedding and model weighting by fusing domain knowledge on\n  Biomedical Question Answering", "abstract": "Biomedical Question Answering aims to obtain an answer to the given question\nfrom the biomedical domain. Due to its high requirement of biomedical domain\nknowledge, it is difficult for the model to learn domain knowledge from limited\ntraining data. We propose a contextual embedding method that combines\nopen-domain QA model \\aoa and \\biobert model pre-trained on biomedical domain\ndata. We adopt unsupervised pre-training on large biomedical corpus and\nsupervised fine-tuning on biomedical question answering dataset. Additionally,\nwe adopt an MLP-based model weighting layer to automatically exploit the\nadvantages of two models to provide the correct answer. The public dataset\n\\biomrc constructed from PubMed corpus is used to evaluate our method.\nExperimental results show that our model outperforms state-of-the-art system by\na large margin.", "published": "2022-06-26 12:47:38", "link": "http://arxiv.org/abs/2206.12866v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Explainable and High-Performance Hate and Offensive Speech Detection", "abstract": "The spread of information through social media platforms can create\nenvironments possibly hostile to vulnerable communities and silence certain\ngroups in society. To mitigate such instances, several models have been\ndeveloped to detect hate and offensive speech. Since detecting hate and\noffensive speech in social media platforms could incorrectly exclude\nindividuals from social media platforms, which can reduce trust, there is a\nneed to create explainable and interpretable models. Thus, we build an\nexplainable and interpretable high performance model based on the XGBoost\nalgorithm, trained on Twitter data. For unbalanced Twitter data, XGboost\noutperformed the LSTM, AutoGluon, and ULMFiT models on hate speech detection\nwith an F1 score of 0.75 compared to 0.38 and 0.37, and 0.38 respectively. When\nwe down-sampled the data to three separate classes of approximately 5000\ntweets, XGBoost performed better than LSTM, AutoGluon, and ULMFiT; with F1\nscores for hate speech detection of 0.79 vs 0.69, 0.77, and 0.66 respectively.\nXGBoost also performed better than LSTM, AutoGluon, and ULMFiT in the\ndown-sampled version for offensive speech detection with F1 score of 0.83 vs\n0.88, 0.82, and 0.79 respectively. We use Shapley Additive Explanations (SHAP)\non our XGBoost models' outputs to makes it explainable and interpretable\ncompared to LSTM, AutoGluon and ULMFiT that are black-box models.", "published": "2022-06-26 22:02:53", "link": "http://arxiv.org/abs/2206.12983v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Are We There Yet? A Decision Framework for Replacing Term Based\n  Retrieval with Dense Retrieval Systems", "abstract": "Recently, several dense retrieval (DR) models have demonstrated competitive\nperformance to term-based retrieval that are ubiquitous in search systems. In\ncontrast to term-based matching, DR projects queries and documents into a dense\nvector space and retrieves results via (approximate) nearest neighbor search.\nDeploying a new system, such as DR, inevitably involves tradeoffs in aspects of\nits performance. Established retrieval systems running at scale are usually\nwell understood in terms of effectiveness and costs, such as query latency,\nindexing throughput, or storage requirements. In this work, we propose a\nframework with a set of criteria that go beyond simple effectiveness measures\nto thoroughly compare two retrieval systems with the explicit goal of assessing\nthe readiness of one system to replace the other. This includes careful\ntradeoff considerations between effectiveness and various cost factors.\nFurthermore, we describe guardrail criteria, since even a system that is better\non average may have systematic failures on a minority of queries. The\nguardrails check for failures on certain query characteristics and novel\nfailure types that are only possible in dense retrieval systems. We demonstrate\nour decision framework on a Web ranking scenario. In that scenario,\nstate-of-the-art DR models have surprisingly strong results, not only on\naverage performance but passing an extensive set of guardrail tests, showing\nrobustness on different query characteristics, lexical matching,\ngeneralization, and number of regressions. It is impossible to predict whether\nDR will become ubiquitous in the future, but one way this is possible is\nthrough repeated applications of decision processes such as the one presented\nhere.", "published": "2022-06-26 23:16:05", "link": "http://arxiv.org/abs/2206.12993v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Low-resource Accent Classification in Geographically-proximate Settings:\n  A Forensic and Sociophonetics Perspective", "abstract": "Accented speech recognition and accent classification are relatively\nunder-explored research areas in speech technology. Recently, deep\nlearning-based methods and Transformer-based pretrained models have achieved\nsuperb performances in both areas. However, most accent classification tasks\nfocused on classifying different kinds of English accents and little attention\nwas paid to geographically-proximate accent classification, especially under a\nlow-resource setting where forensic speech science tasks usually encounter. In\nthis paper, we explored three main accent modelling methods combined with two\ndifferent classifiers based on 105 speaker recordings retrieved from five urban\nvarieties in Northern England. Although speech representations generated from\npretrained models generally have better performances in downstream\nclassification, traditional methods like Mel Frequency Cepstral Coefficients\n(MFCCs) and formant measurements are equipped with specific strengths. These\nresults suggest that in forensic phonetics scenario where data are relatively\nscarce, a simple modelling method and classifier could be competitive with\nstate-of-the-art pretrained speech models as feature extractors, which could\nenhance a sooner estimation for the accent information in practices. Besides,\nour findings also cross-validated a new methodology in quantifying\nsociophonetic changes.", "published": "2022-06-26 01:25:17", "link": "http://arxiv.org/abs/2206.12759v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Meta Auxiliary Learning for Low-resource Spoken Language Understanding", "abstract": "Spoken language understanding (SLU) treats automatic speech recognition (ASR)\nand natural language understanding (NLU) as a unified task and usually suffers\nfrom data scarcity. We exploit an ASR and NLU joint training method based on\nmeta auxiliary learning to improve the performance of low-resource SLU task by\nonly taking advantage of abundant manual transcriptions of speech data. One\nobvious advantage of such method is that it provides a flexible framework to\nimplement a low-resource SLU training task without requiring access to any\nfurther semantic annotations. In particular, a NLU model is taken as label\ngeneration network to predict intent and slot tags from texts; a multi-task\nnetwork trains ASR task and SLU task synchronously from speech; and the\npredictions of label generation network are delivered to the multi-task network\nas semantic targets. The efficiency of the proposed algorithm is demonstrated\nwith experiments on the public CATSLU dataset, which produces more suitable ASR\nhypotheses for the downstream NLU task.", "published": "2022-06-26 03:12:33", "link": "http://arxiv.org/abs/2206.12774v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Memory-Guided Multi-View Multi-Domain Fake News Detection", "abstract": "The wide spread of fake news is increasingly threatening both individuals and\nsociety. Great efforts have been made for automatic fake news detection on a\nsingle domain (e.g., politics). However, correlations exist commonly across\nmultiple news domains, and thus it is promising to simultaneously detect fake\nnews of multiple domains. Based on our analysis, we pose two challenges in\nmulti-domain fake news detection: 1) domain shift, caused by the discrepancy\namong domains in terms of words, emotions, styles, etc. 2) domain labeling\nincompleteness, stemming from the real-world categorization that only outputs\none single domain label, regardless of topic diversity of a news piece. In this\npaper, we propose a Memory-guided Multi-view Multi-domain Fake News Detection\nFramework (M$^3$FEND) to address these two challenges. We model news pieces\nfrom a multi-view perspective, including semantics, emotion, and style.\nSpecifically, we propose a Domain Memory Bank to enrich domain information\nwhich could discover potential domain labels based on seen news pieces and\nmodel domain characteristics. Then, with enriched domain information as input,\na Domain Adapter could adaptively aggregate discriminative information from\nmultiple views for news in various domains. Extensive offline experiments on\nEnglish and Chinese datasets demonstrate the effectiveness of M$^3$FEND, and\nonline tests verify its superiority in practice. Our code is available at\nhttps://github.com/ICTMCG/M3FEND.", "published": "2022-06-26 07:09:23", "link": "http://arxiv.org/abs/2206.12808v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Comparison of Encoders for Attention based End to End Speech\n  Recognition in Standalone and Rescoring Mode", "abstract": "The streaming automatic speech recognition (ASR) models are more popular and\nsuitable for voice-based applications. However, non-streaming models provide\nbetter performance as they look at the entire audio context. To leverage the\nbenefits of the non-streaming model in streaming applications like voice\nsearch, it is commonly used in second pass re-scoring mode. The candidate\nhypothesis generated using steaming models is re-scored using a non-streaming\nmodel. In this work, we evaluate the non-streaming attention-based end-to-end\nASR models on the Flipkart voice search task in both standalone and re-scoring\nmodes. These models are based on Listen-Attend-Spell (LAS) encoder-decoder\narchitecture. We experiment with different encoder variations based on LSTM,\nTransformer, and Conformer. We compare the latency requirements of these models\nalong with their performance. Overall we show that the Transformer model offers\nacceptable WER with the lowest latency requirements. We report a relative WER\nimprovement of around 16% with the second pass LAS re-scoring with latency\noverhead under 5ms. We also highlight the importance of CNN front-end with\nTransformer architecture to achieve comparable word error rates (WER).\nMoreover, we observe that in the second pass re-scoring mode all the encoders\nprovide similar benefits whereas the difference in performance is prominent in\nstandalone text generation mode.", "published": "2022-06-26 09:12:27", "link": "http://arxiv.org/abs/2206.12829v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Data Augmentation for Dementia Detection in Spoken Language", "abstract": "Dementia is a growing problem as our society ages, and detection methods are\noften invasive and expensive. Recent deep-learning techniques can offer a\nfaster diagnosis and have shown promising results. However, they require large\namounts of labelled data which is not easily available for the task of dementia\ndetection. One effective solution to sparse data problems is data augmentation,\nthough the exact methods need to be selected carefully. To date, there has been\nno empirical study of data augmentation on Alzheimer's disease (AD) datasets\nfor NLP and speech processing. In this work, we investigate data augmentation\ntechniques for the task of AD detection and perform an empirical evaluation of\nthe different approaches on two kinds of models for both the text and audio\ndomains. We use a transformer-based model for both domains, and SVM and Random\nForest models for the text and audio domains, respectively. We generate\nadditional samples using traditional as well as deep learning based methods and\nshow that data augmentation improves performance for both the text- and\naudio-based models and that such results are comparable to state-of-the-art\nresults on the popular ADReSS set, with carefully crafted architectures and\nfeatures.", "published": "2022-06-26 13:40:25", "link": "http://arxiv.org/abs/2206.12879v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Annotated Speech Corpus for Low Resource Indian Languages: Awadhi,\n  Bhojpuri, Braj and Magahi", "abstract": "In this paper we discuss an in-progress work on the development of a speech\ncorpus for four low-resource Indo-Aryan languages -- Awadhi, Bhojpuri, Braj and\nMagahi using the field methods of linguistic data collection. The total size of\nthe corpus currently stands at approximately 18 hours (approx. 4-5 hours each\nlanguage) and it is transcribed and annotated with grammatical information such\nas part-of-speech tags, morphological features and Universal dependency\nrelationships. We discuss our methodology for data collection in these\nlanguages, most of which was done in the middle of the COVID-19 pandemic, with\none of the aims being to generate some additional income for low-income groups\nspeaking these languages. In the paper, we also discuss the results of the\nbaseline experiments for automatic speech recognition system in these\nlanguages.", "published": "2022-06-26 17:28:38", "link": "http://arxiv.org/abs/2206.12931v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Improving the Training Recipe for a Robust Conformer-based Hybrid Model", "abstract": "Speaker adaptation is important to build robust automatic speech recognition\n(ASR) systems. In this work, we investigate various methods for speaker\nadaptive training (SAT) based on feature-space approaches for a conformer-based\nacoustic model (AM) on the Switchboard 300h dataset. We propose a method,\ncalled Weighted-Simple-Add, which adds weighted speaker information vectors to\nthe input of the multi-head self-attention module of the conformer AM. Using\nthis method for SAT, we achieve 3.5% and 4.5% relative improvement in terms of\nWER on the CallHome part of Hub5'00 and Hub5'01 respectively. Moreover, we\nbuild on top of our previous work where we proposed a novel and competitive\ntraining recipe for a conformer-based hybrid AM. We extend and improve this\nrecipe where we achieve 11% relative improvement in terms of word-error-rate\n(WER) on Switchboard 300h Hub5'00 dataset. We also make this recipe efficient\nby reducing the total number of parameters by 34% relative.", "published": "2022-06-26 20:01:08", "link": "http://arxiv.org/abs/2206.12955v1", "categories": ["cs.CL", "eess.AS", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Transport-Oriented Feature Aggregation for Speaker Embedding Learning", "abstract": "Pooling is needed to aggregate frame-level features into utterance-level\nrepresentations for speaker modeling. Given the success of statistics-based\npooling methods, we hypothesize that speaker characteristics are well\nrepresented in the statistical distribution over the pre-aggregation layer's\noutput, and propose to use transport-oriented feature aggregation for deriving\nspeaker embeddings. The aggregated representation encodes the geometric\nstructure of the underlying feature distribution, which is expected to contain\nvaluable speaker-specific information that may not be represented by the\ncommonly used statistical measures like mean and variance. The original\ntransport-oriented feature aggregation is also extended to a weighted-frame\nversion to incorporate the attention mechanism. Experiments on speaker\nverification with the Voxceleb dataset show improvement over statistics pooling\nand its attentive variant.", "published": "2022-06-26 12:22:53", "link": "http://arxiv.org/abs/2206.12857v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploiting Transformation Invariance and Equivariance for\n  Self-supervised Sound Localisation", "abstract": "We present a simple yet effective self-supervised framework for audio-visual\nrepresentation learning, to localize the sound source in videos. To understand\nwhat enables to learn useful representations, we systematically investigate the\neffects of data augmentations, and reveal that (1) composition of data\naugmentations plays a critical role, i.e. explicitly encouraging the\naudio-visual representations to be invariant to various transformations~({\\em\ntransformation invariance}); (2) enforcing geometric consistency substantially\nimproves the quality of learned representations, i.e. the detected sound source\nshould follow the same transformation applied on input video frames~({\\em\ntransformation equivariance}). Extensive experiments demonstrate that our model\nsignificantly outperforms previous methods on two sound localization\nbenchmarks, namely, Flickr-SoundNet and VGG-Sound. Additionally, we also\nevaluate audio retrieval and cross-modal retrieval tasks. In both cases, our\nself-supervised models demonstrate superior retrieval performances, even\ncompetitive with the supervised approach in audio retrieval. This reveals the\nproposed framework learns strong multi-modal representations that are\nbeneficial to sound localisation and generalization to further applications.\n\\textit{All codes will be available}.", "published": "2022-06-26 03:00:02", "link": "http://arxiv.org/abs/2206.12772v2", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "State of the Art of Audio- and Video-Based Solutions for AAL", "abstract": "The report illustrates the state of the art of the most successful AAL\napplications and functions based on audio and video data, namely (i)\nlifelogging and self-monitoring, (ii) remote monitoring of vital signs, (iii)\nemotional state recognition, (iv) food intake monitoring, activity and\nbehaviour recognition, (v) activity and personal assistance, (vi) gesture\nrecognition, (vii) fall detection and prevention, (viii) mobility assessment\nand frailty recognition, and (ix) cognitive and motor rehabilitation. For these\napplication scenarios, the report illustrates the state of play in terms of\nscientific advances, available products and research project. The open\nchallenges are also highlighted.", "published": "2022-06-26 14:27:33", "link": "http://arxiv.org/abs/2207.01487v2", "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.SD", "eess.AS", "I.2"], "primary_category": "cs.CY"}
