{"title": "WIDAR -- Weighted Input Document Augmented ROUGE", "abstract": "The task of automatic text summarization has gained a lot of traction due to\nthe recent advancements in machine learning techniques. However, evaluating the\nquality of a generated summary remains to be an open problem. The literature\nhas widely adopted Recall-Oriented Understudy for Gisting Evaluation (ROUGE) as\nthe standard evaluation metric for summarization. However, ROUGE has some\nlong-established limitations; a major one being its dependence on the\navailability of good quality reference summary. In this work, we propose the\nmetric WIDAR which in addition to utilizing the reference summary uses also the\ninput document in order to evaluate the quality of the generated summary. The\nproposed metric is versatile, since it is designed to adapt the evaluation\nscore according to the quality of the reference summary. The proposed metric\ncorrelates better than ROUGE by 26%, 76%, 82%, and 15%, respectively, in\ncoherence, consistency, fluency, and relevance on human judgement scores\nprovided in the SummEval dataset. The proposed metric is able to obtain\ncomparable results with other state-of-the-art metrics while requiring a\nrelatively short computational time.", "published": "2022-01-23 14:40:42", "link": "http://arxiv.org/abs/2201.09282v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Supervised Visual Attention for Simultaneous Multimodal Machine\n  Translation", "abstract": "Recently, there has been a surge in research in multimodal machine\ntranslation (MMT), where additional modalities such as images are used to\nimprove translation quality of textual systems. A particular use for such\nmultimodal systems is the task of simultaneous machine translation, where\nvisual context has been shown to complement the partial information provided by\nthe source sentence, especially in the early phases of translation. In this\npaper, we propose the first Transformer-based simultaneous MMT architecture,\nwhich has not been previously explored in the field. Additionally, we extend\nthis model with an auxiliary supervision signal that guides its visual\nattention mechanism using labelled phrase-region alignments. We perform\ncomprehensive experiments on three language directions and conduct thorough\nquantitative and qualitative analyses using both automatic metrics and manual\ninspection. Our results show that (i) supervised visual attention consistently\nimproves the translation quality of the MMT models, and (ii) fine-tuning the\nMMT with supervision loss enabled leads to better performance than training the\nMMT from scratch. Compared to the state-of-the-art, our proposed model achieves\nimprovements of up to 2.3 BLEU and 3.5 METEOR points.", "published": "2022-01-23 17:25:57", "link": "http://arxiv.org/abs/2201.09324v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Large and Diverse Arabic Corpus for Language Modeling", "abstract": "Language models (LMs) have introduced a major paradigm shift in Natural\nLanguage Processing (NLP) modeling where large pre-trained LMs became integral\nto most of the NLP tasks. The LMs are intelligent enough to find useful and\nrelevant representations of the language without any supervision. Perhaps,\nthese models are used to fine-tune typical NLP tasks with significantly high\naccuracy as compared to the traditional approaches. Conversely, the training of\nthese models requires a massively large corpus that is a good representation of\nthe language. English LMs generally perform better than their other language\ncounterparts, due to the availability of massive English corpora. This work\nelaborates on the design and development of a large Arabic corpus. It consists\nof over 500 GB of Arabic cleaned text targeted at improving cross-domain\nknowledge and downstream generalization capability of large-scale language\nmodels. Moreover, the corpus is utilized in the training of a large Arabic LM.\nIn order to evaluate the effectiveness of the LM, a number of typical NLP tasks\nare fine-tuned. The tasks demonstrate a significant boost from 4.5 to 8.5% when\ncompared to tasks fine-tuned on multi-lingual BERT (mBERT). To the best of my\nknowledge, this is currently the largest clean and diverse Arabic corpus ever\ncollected.", "published": "2022-01-23 11:17:53", "link": "http://arxiv.org/abs/2201.09227v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Gradient-guided Unsupervised Text Style Transfer via Contrastive\n  Learning", "abstract": "Text style transfer is a challenging text generation problem, which aims at\naltering the style of a given sentence to a target one while keeping its\ncontent unchanged. Since there is a natural scarcity of parallel datasets,\nrecent works mainly focus on solving the problem in an unsupervised manner.\nHowever, previous gradient-based works generally suffer from the deficiencies\nas follows, namely: (1) Content migration. Previous approaches lack explicit\nmodeling of content invariance and are thus susceptible to content shift\nbetween the original sentence and the transferred one. (2) Style\nmisclassification. A natural drawback of the gradient-guided approaches is that\nthe inference process is homogeneous with a line of adversarial attack, making\nlatent optimization easily becomes an attack to the classifier due to\nmisclassification. This leads to difficulties in achieving high transfer\naccuracy. To address the problems, we propose a novel gradient-guided model\nthrough a contrastive paradigm for text style transfer, to explicitly gather\nsimilar semantic sentences, and to design a siamese-structure based style\nclassifier for alleviating such two issues, respectively. Experiments on two\ndatasets show the effectiveness of our proposed approach, as compared to the\nstate-of-the-arts.", "published": "2022-01-23 12:45:00", "link": "http://arxiv.org/abs/2202.00469v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Application of Pseudo-Log-Likelihoods to Natural Language Scoring", "abstract": "Language models built using semi-supervised machine learning on large corpora\nof natural language have very quickly enveloped the fields of natural language\ngeneration and understanding. In this paper we apply a zero-shot approach\nindependently developed by a number of researchers now gaining recognition as a\nsignificant alternative to fine-tuning for evaluation on common sense tasks. A\nlanguage model with relatively few parameters and training steps compared to a\nmore recent language model (T5) can outperform it on a recent large data set\n(TimeDial), while displaying robustness in its performance across a similar\nclass of language tasks. Surprisingly, this result is achieved by using a\nhyperparameter-free zero-shot method with the smaller model, compared to\nfine-tuning to the larger model. We argue that robustness of the smaller model\nought to be understood in terms of compositionality, in a sense that we draw\nfrom recent literature on a class of similar models. We identify a practical\ncost for our method and model: high GPU-time for natural language evaluation.\nThe zero-shot measurement technique that produces remarkable stability, both\nfor ALBERT and other BERT variants, is an application of pseudo-log-likelihoods\nto masked language models for the relative measurement of probability for\nsubstitution alternatives in forced choice language tasks such as the Winograd\nSchema Challenge, Winogrande, and others. One contribution of this paper is to\nbring together a number of similar, but independent strands of research. We\nproduce some absolute state-of-the-art results for common sense reasoning in\nbinary choice tasks, performing better than any published result in the\nliterature, including fine-tuned efforts. We show a remarkable consistency of\nthe model's performance under adversarial settings, which we argue is best\nexplained by the model's compositionality of representations.", "published": "2022-01-23 22:00:54", "link": "http://arxiv.org/abs/2201.09377v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "OntoProtein: Protein Pretraining With Gene Ontology Embedding", "abstract": "Self-supervised protein language models have proved their effectiveness in\nlearning the proteins representations. With the increasing computational power,\ncurrent protein language models pre-trained with millions of diverse sequences\ncan advance the parameter scale from million-level to billion-level and achieve\nremarkable improvement. However, those prevailing approaches rarely consider\nincorporating knowledge graphs (KGs), which can provide rich structured\nknowledge facts for better protein representations. We argue that informative\nbiology knowledge in KGs can enhance protein representation with external\nknowledge. In this work, we propose OntoProtein, the first general framework\nthat makes use of structure in GO (Gene Ontology) into protein pre-training\nmodels. We construct a novel large-scale knowledge graph that consists of GO\nand its related proteins, and gene annotation texts or protein sequences\ndescribe all nodes in the graph. We propose novel contrastive learning with\nknowledge-aware negative sampling to jointly optimize the knowledge graph and\nprotein embedding during pre-training. Experimental results show that\nOntoProtein can surpass state-of-the-art methods with pre-trained protein\nlanguage models in TAPE benchmark and yield better performance compared with\nbaselines in protein-protein interaction and protein function prediction. Code\nand datasets are available in https://github.com/zjunlp/OntoProtein.", "published": "2022-01-23 14:49:49", "link": "http://arxiv.org/abs/2201.11147v6", "categories": ["q-bio.BM", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "q-bio.BM"}
{"title": "A Pre-trained Audio-Visual Transformer for Emotion Recognition", "abstract": "In this paper, we introduce a pretrained audio-visual Transformer trained on\nmore than 500k utterances from nearly 4000 celebrities from the VoxCeleb2\ndataset for human behavior understanding. The model aims to capture and extract\nuseful information from the interactions between human facial and auditory\nbehaviors, with application in emotion recognition. We evaluate the model\nperformance on two datasets, namely CREMAD-D (emotion classification) and\nMSP-IMPROV (continuous emotion regression). Experimental results show that\nfine-tuning the pre-trained model helps improving emotion classification\naccuracy by 5-7% and Concordance Correlation Coefficients (CCC) in continuous\nemotion recognition by 0.03-0.09 compared to the same model trained from\nscratch. We also demonstrate the robustness of finetuning the pre-trained model\nin a low-resource setting. With only 10% of the original training set provided,\nfine-tuning the pre-trained model can lead to at least 10% better emotion\nrecognition accuracy and a CCC score improvement by at least 0.1 for continuous\nemotion recognition.", "published": "2022-01-23 03:09:16", "link": "http://arxiv.org/abs/2201.09165v1", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
