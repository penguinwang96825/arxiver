{"title": "On the impressive performance of randomly weighted encoders in\n  summarization tasks", "abstract": "In this work, we investigate the performance of untrained randomly\ninitialized encoders in a general class of sequence to sequence models and\ncompare their performance with that of fully-trained encoders on the task of\nabstractive summarization. We hypothesize that random projections of an input\ntext have enough representational power to encode the hierarchical structure of\nsentences and semantics of documents. Using a trained decoder to produce\nabstractive text summaries, we empirically demonstrate that architectures with\nuntrained randomly initialized encoders perform competitively with respect to\nthe equivalent architectures with fully-trained encoders. We further find that\nthe capacity of the encoder not only improves overall model generalization but\nalso closes the performance gap between untrained randomly initialized and\nfull-trained encoders. To our knowledge, it is the first time that general\nsequence to sequence models with attention are assessed for trained and\nrandomly projected representations on abstractive summarization.", "published": "2020-02-21 01:47:09", "link": "http://arxiv.org/abs/2002.09084v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Guider l'attention dans les modeles de sequence a sequence pour la\n  prediction des actes de dialogue", "abstract": "The task of predicting dialog acts (DA) based on conversational dialog is a\nkey component in the development of conversational agents. Accurately\npredicting DAs requires a precise modeling of both the conversation and the\nglobal tag dependencies. We leverage seq2seq approaches widely adopted in\nNeural Machine Translation (NMT) to improve the modelling of tag sequentiality.\nSeq2seq models are known to learn complex global dependencies while currently\nproposed approaches using linear conditional random fields (CRF) only model\nlocal tag dependencies. In this work, we introduce a seq2seq model tailored for\nDA classification using: a hierarchical encoder, a novel guided attention\nmechanism and beam search applied to both training and inference. Compared to\nthe state of the art our model does not require handcrafted features and is\ntrained end-to-end. Furthermore, the proposed approach achieves an unmatched\naccuracy score of 85% on SwDA, and state-of-the-art accuracy score of 91.6% on\nMRDA.", "published": "2020-02-21 17:09:19", "link": "http://arxiv.org/abs/2002.09419v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modelling Latent Skills for Multitask Language Generation", "abstract": "We present a generative model for multitask conditional language generation.\nOur guiding hypothesis is that a shared set of latent skills underlies many\ndisparate language generation tasks, and that explicitly modelling these skills\nin a task embedding space can help with both positive transfer across tasks and\nwith efficient adaptation to new tasks. We instantiate this task embedding\nspace as a latent variable in a latent variable sequence-to-sequence model. We\nevaluate this hypothesis by curating a series of monolingual text-to-text\nlanguage generation datasets - covering a broad range of tasks and domains -\nand comparing the performance of models both in the multitask and few-shot\nregimes. We show that our latent task variable model outperforms other\nsequence-to-sequence baselines on average across tasks in the multitask\nsetting. In the few-shot learning setting on an unseen test dataset (i.e., a\nnew task), we demonstrate that model adaptation based on inference in the\nlatent task space is more robust than standard fine-tuning based parameter\nadaptation and performs comparably in terms of overall performance. Finally, we\nexamine the latent task representations learnt by our model and show that they\ncluster tasks in a natural way.", "published": "2020-02-21 20:39:09", "link": "http://arxiv.org/abs/2002.09543v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Dynamic Belief Graphs to Generalize on Text-Based Games", "abstract": "Playing text-based games requires skills in processing natural language and\nsequential decision making. Achieving human-level performance on text-based\ngames remains an open challenge, and prior research has largely relied on\nhand-crafted structured representations and heuristics. In this work, we\ninvestigate how an agent can plan and generalize in text-based games using\ngraph-structured representations learned end-to-end from raw text. We propose a\nnovel graph-aided transformer agent (GATA) that infers and updates latent\nbelief graphs during planning to enable effective action selection by capturing\nthe underlying game dynamics. GATA is trained using a combination of\nreinforcement and self-supervised learning. Our work demonstrates that the\nlearned graph-based representations help agents converge to better policies\nthan their text-only counterparts and facilitate effective generalization\nacross game configurations. Experiments on 500+ unique games from the TextWorld\nsuite show that our best agent outperforms text-based baselines by an average\nof 24.2%.", "published": "2020-02-21 04:38:37", "link": "http://arxiv.org/abs/2002.09127v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Refinement of Unsupervised Cross-Lingual Word Embeddings", "abstract": "Cross-lingual word embeddings aim to bridge the gap between high-resource and\nlow-resource languages by allowing to learn multilingual word representations\neven without using any direct bilingual signal. The lion's share of the methods\nare projection-based approaches that map pre-trained embeddings into a shared\nlatent space. These methods are mostly based on the orthogonal transformation,\nwhich assumes language vector spaces to be isomorphic. However, this criterion\ndoes not necessarily hold, especially for morphologically-rich languages. In\nthis paper, we propose a self-supervised method to refine the alignment of\nunsupervised bilingual word embeddings. The proposed model moves vectors of\nwords and their corresponding translations closer to each other as well as\nenforces length- and center-invariance, thus allowing to better align\ncross-lingual embeddings. The experimental results demonstrate the\neffectiveness of our approach, as in most cases it outperforms state-of-the-art\nmethods in a bilingual lexicon induction task.", "published": "2020-02-21 10:39:53", "link": "http://arxiv.org/abs/2002.09213v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Crowdsourced Collective Entity Resolution with Relational Match\n  Propagation", "abstract": "Knowledge bases (KBs) store rich yet heterogeneous entities and facts. Entity\nresolution (ER) aims to identify entities in KBs which refer to the same\nreal-world object. Recent studies have shown significant benefits of involving\nhumans in the loop of ER. They often resolve entities with pairwise similarity\nmeasures over attribute values and resort to the crowds to label uncertain\nones. However, existing methods still suffer from high labor costs and\ninsufficient labeling to some extent. In this paper, we propose a novel\napproach called crowdsourced collective ER, which leverages the relationships\nbetween entities to infer matches jointly rather than independently.\nSpecifically, it iteratively asks human workers to label picked entity pairs\nand propagates the labeling information to their neighbors in distance. During\nthis process, we address the problems of candidate entity pruning,\nprobabilistic propagation, optimal question selection and error-tolerant truth\ninference. Our experiments on real-world datasets demonstrate that, compared\nwith state-of-the-art methods, our approach achieves superior accuracy with\nmuch less labeling.", "published": "2020-02-21 15:33:53", "link": "http://arxiv.org/abs/2002.09361v1", "categories": ["cs.DB", "cs.CL"], "primary_category": "cs.DB"}
{"title": "Is Aligning Embedding Spaces a Challenging Task? A Study on\n  Heterogeneous Embedding Alignment Methods", "abstract": "Representation Learning of words and Knowledge Graphs (KG) into low\ndimensional vector spaces along with its applications to many real-world\nscenarios have recently gained momentum. In order to make use of multiple KG\nembeddings for knowledge-driven applications such as question answering, named\nentity disambiguation, knowledge graph completion, etc., alignment of different\nKG embedding spaces is necessary. In addition to multilinguality and\ndomain-specific information, different KGs pose the problem of structural\ndifferences making the alignment of the KG embeddings more challenging. This\npaper provides a theoretical analysis and comparison of the state-of-the-art\nalignment methods between two embedding spaces representing entity-entity and\nentity-word. This paper also aims at assessing the capability and short-comings\nof the existing alignment methods on the pretext of different applications.", "published": "2020-02-21 12:37:12", "link": "http://arxiv.org/abs/2002.09247v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language as a Cognitive Tool to Imagine Goals in Curiosity-Driven\n  Exploration", "abstract": "Developmental machine learning studies how artificial agents can model the\nway children learn open-ended repertoires of skills. Such agents need to create\nand represent goals, select which ones to pursue and learn to achieve them.\nRecent approaches have considered goal spaces that were either fixed and\nhand-defined or learned using generative models of states. This limited agents\nto sample goals within the distribution of known effects. We argue that the\nability to imagine out-of-distribution goals is key to enable creative\ndiscoveries and open-ended learning. Children do so by leveraging the\ncompositionality of language as a tool to imagine descriptions of outcomes they\nnever experienced before, targeting them as goals during play. We introduce\nIMAGINE, an intrinsically motivated deep reinforcement learning architecture\nthat models this ability. Such imaginative agents, like children, benefit from\nthe guidance of a social peer who provides language descriptions. To take\nadvantage of goal imagination, agents must be able to leverage these\ndescriptions to interpret their imagined out-of-distribution goals. This\ngeneralization is made possible by modularity: a decomposition between learned\ngoal-achievement reward function and policy relying on deep sets, gated\nattention and object-centered representations. We introduce the Playground\nenvironment and study how this form of goal imagination improves generalization\nand exploration over agents lacking this capacity. In addition, we identify the\nproperties of goal imagination that enable these results and study the impacts\nof modularity and social interactions.", "published": "2020-02-21 12:59:57", "link": "http://arxiv.org/abs/2002.09253v4", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Addressing Some Limitations of Transformers with Feedback Memory", "abstract": "Transformers have been successfully applied to sequential, auto-regressive\ntasks despite being feedforward networks. Unlike recurrent neural networks,\nTransformers use attention to capture temporal relations while processing input\ntokens in parallel. While this parallelization makes them computationally\nefficient, it restricts the model from fully exploiting the sequential nature\nof the input. The representation at a given layer can only access\nrepresentations from lower layers, rather than the higher level representations\nalready available. In this work, we propose the Feedback Transformer\narchitecture that exposes all previous representations to all future\nrepresentations, meaning the lowest representation of the current timestep is\nformed from the highest-level abstract representation of the past. We\ndemonstrate on a variety of benchmarks in language modeling, machine\ntranslation, and reinforcement learning that the increased representation\ncapacity can create small, shallow models with much stronger performance than\ncomparable Transformers.", "published": "2020-02-21 16:37:57", "link": "http://arxiv.org/abs/2002.09402v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Rhythm, Chord and Melody Generation for Lead Sheets using Recurrent\n  Neural Networks", "abstract": "Music that is generated by recurrent neural networks often lacks a sense of\ndirection and coherence. We therefore propose a two-stage LSTM-based model for\nlead sheet generation, in which the harmonic and rhythmic templates of the song\nare produced first, after which, in a second stage, a sequence of melody notes\nis generated conditioned on these templates. A subjective listening test shows\nthat our approach outperforms the baselines and increases perceived musical\ncoherence.", "published": "2020-02-21 09:36:24", "link": "http://arxiv.org/abs/2002.10266v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Gated Mechanism for Attention Based Multimodal Sentiment Analysis", "abstract": "Multimodal sentiment analysis has recently gained popularity because of its\nrelevance to social media posts, customer service calls and video blogs. In\nthis paper, we address three aspects of multimodal sentiment analysis; 1. Cross\nmodal interaction learning, i.e. how multiple modalities contribute to the\nsentiment, 2. Learning long-term dependencies in multimodal interactions and 3.\nFusion of unimodal and cross modal cues. Out of these three, we find that\nlearning cross modal interactions is beneficial for this problem. We perform\nexperiments on two benchmark datasets, CMU Multimodal Opinion level Sentiment\nIntensity (CMU-MOSI) and CMU Multimodal Opinion Sentiment and Emotion Intensity\n(CMU-MOSEI) corpus. Our approach on both these tasks yields accuracies of 83.9%\nand 81.1% respectively, which is 1.6% and 1.34% absolute improvement over\ncurrent state-of-the-art.", "published": "2020-02-21 06:58:03", "link": "http://arxiv.org/abs/2003.01043v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "KryptoOracle: A Real-Time Cryptocurrency Price Prediction Platform Using\n  Twitter Sentiments", "abstract": "Cryptocurrencies, such as Bitcoin, are becoming increasingly popular, having\nbeen widely used as an exchange medium in areas such as financial transaction\nand asset transfer verification. However, there has been a lack of solutions\nthat can support real-time price prediction to cope with high currency\nvolatility, handle massive heterogeneous data volumes, including social media\nsentiments, while supporting fault tolerance and persistence in real time, and\nprovide real-time adaptation of learning algorithms to cope with new price and\nsentiment data. In this paper we introduce KryptoOracle, a novel real-time and\nadaptive cryptocurrency price prediction platform based on Twitter sentiments.\nThe integrative and modular platform is based on (i) a Spark-based architecture\nwhich handles the large volume of incoming data in a persistent and fault\ntolerant way; (ii) an approach that supports sentiment analysis which can\nrespond to large amounts of natural language processing queries in real time;\nand (iii) a predictive method grounded on online learning in which a model\nadapts its weights to cope with new prices and sentiments. Besides providing an\narchitectural design, the paper also describes the KryptoOracle platform\nimplementation and experimental evaluation. Overall, the proposed platform can\nhelp accelerate decision-making, uncover new opportunities and provide more\ntimely insights based on the available and ever-larger financial data volume\nand variety.", "published": "2020-02-21 20:38:46", "link": "http://arxiv.org/abs/2003.04967v1", "categories": ["cs.CL", "cs.LG", "q-fin.ST"], "primary_category": "cs.CL"}
{"title": "Few-shot acoustic event detection via meta-learning", "abstract": "We study few-shot acoustic event detection (AED) in this paper. Few-shot\nlearning enables detection of new events with very limited labeled data.\nCompared to other research areas like computer vision, few-shot learning for\naudio recognition has been under-studied. We formulate few-shot AED problem and\nexplore different ways of utilizing traditional supervised methods for this\nsetting as well as a variety of meta-learning approaches, which are\nconventionally used to solve few-shot classification problem. Compared to\nsupervised baselines, meta-learning models achieve superior performance, thus\nshowing its effectiveness on generalization to new audio events. Our analysis\nincluding impact of initialization and domain discrepancy further validate the\nadvantage of meta-learning approaches in few-shot AED.", "published": "2020-02-21 06:02:11", "link": "http://arxiv.org/abs/2002.09143v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "AutoFoley: Artificial Synthesis of Synchronized Sound Tracks for Silent\n  Videos with Deep Learning", "abstract": "In movie productions, the Foley Artist is responsible for creating an overlay\nsoundtrack that helps the movie come alive for the audience. This requires the\nartist to first identify the sounds that will enhance the experience for the\nlistener thereby reinforcing the Directors's intention for a given scene. In\nthis paper, we present AutoFoley, a fully-automated deep learning tool that can\nbe used to synthesize a representative audio track for videos. AutoFoley can be\nused in the applications where there is either no corresponding audio file\nassociated with the video or in cases where there is a need to identify\ncritical scenarios and provide a synthesized, reinforced soundtrack. An\nimportant performance criterion of the synthesized soundtrack is to be\ntime-synchronized with the input video, which provides for a realistic and\nbelievable portrayal of the synthesized sound. Unlike existing sound prediction\nand generation architectures, our algorithm is capable of precise recognition\nof actions as well as inter-frame relations in fast moving video clips by\nincorporating an interpolation technique and Temporal Relationship Networks\n(TRN). We employ a robust multi-scale Recurrent Neural Network (RNN) associated\nwith a Convolutional Neural Network (CNN) for a better understanding of the\nintricate input-to-output associations over time. To evaluate AutoFoley, we\ncreate and introduce a large scale audio-video dataset containing a variety of\nsounds frequently used as Foley effects in movies. Our experiments show that\nthe synthesized sounds are realistically portrayed with accurate temporal\nsynchronization of the associated visual inputs. Human qualitative testing of\nAutoFoley show over 73% of the test subjects considered the generated\nsoundtrack as original, which is a noteworthy improvement in cross-modal\nresearch in sound synthesis.", "published": "2020-02-21 09:08:28", "link": "http://arxiv.org/abs/2002.10981v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
