{"title": "Gradient-guided Attention Map Editing: Towards Efficient Contextual Hallucination Mitigation", "abstract": "In tasks like summarization and open-book question answering (QA), Large\nLanguage Models (LLMs) often encounter \"contextual hallucination\", where they\nproduce irrelevant or incorrect responses despite having access to accurate\nsource information. This typically occurs because these models tend to\nprioritize self-generated content over the input context, causing them to\ndisregard pertinent details. To address this challenge, we introduce a novel\nmethod called \"Guided Attention Map Editing\" (GAME), which dynamically adjusts\nattention maps to improve contextual relevance. During inference, GAME employs\na trained classifier to identify attention maps prone to inducing\nhallucinations and executes targeted interventions. These interventions, guided\nby gradient-informed \"edit directions'', strategically redistribute attention\nweights across various heads to effectively reduce hallucination. Comprehensive\nevaluations on challenging summarization and open-book QA tasks show that GAME\nconsistently reduces hallucinations across a variety of open-source models.\nSpecifically, GAME reduces hallucinations by 10% in the XSum summarization task\nwhile achieving a 7X speed-up in computational efficiency compared to the\nstate-of-the-art baselines.", "published": "2025-03-11 23:55:45", "link": "http://arxiv.org/abs/2503.08963v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Exhaustive Evaluation of TTS- and VC-based Data Augmentation for ASR", "abstract": "Augmenting the training data of automatic speech recognition (ASR) systems\nwith synthetic data generated by text-to-speech (TTS) or voice conversion (VC)\nhas gained popularity in recent years. Several works have demonstrated\nimprovements in ASR performance using this augmentation approach. However,\nbecause of the lower diversity of synthetic speech, naively combining synthetic\nand real data often does not yield the best results. In this work, we leverage\nrecently proposed flow-based TTS/VC models allowing greater speech diversity,\nand assess the respective impact of augmenting various speech attributes on the\nword error rate (WER) achieved by several ASR models. Pitch augmentation and\nVC-based speaker augmentation are found to be ineffective in our setup. Jointly\naugmenting all other attributes reduces the WER of a Conformer-Transducer model\nby 11\\% relative on Common Voice and by up to 35\\% relative on LibriSpeech\ncompared to training on real data only.", "published": "2025-03-11 23:09:06", "link": "http://arxiv.org/abs/2503.08954v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Backtracking for Safety", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, but ensuring their safety and alignment with human values\nremains crucial. Current safety alignment methods, such as supervised\nfine-tuning and reinforcement learning-based approaches, can exhibit\nvulnerabilities to adversarial attacks and often result in shallow safety\nalignment, primarily focusing on preventing harmful content in the initial\ntokens of the generated output. While methods like resetting can help recover\nfrom unsafe generations by discarding previous tokens and restarting the\ngeneration process, they are not well-suited for addressing nuanced safety\nviolations like toxicity that may arise within otherwise benign and lengthy\ngenerations. In this paper, we propose a novel backtracking method designed to\naddress these limitations. Our method allows the model to revert to a safer\ngeneration state, not necessarily at the beginning, when safety violations\noccur during generation. This approach enables targeted correction of\nproblematic segments without discarding the entire generated text, thereby\npreserving efficiency. We demonstrate that our method dramatically reduces\ntoxicity appearing through the generation process with minimal impact to\nefficiency.", "published": "2025-03-11 22:04:22", "link": "http://arxiv.org/abs/2503.08919v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Interpreting the Repeated Token Phenomenon in Large Language Models", "abstract": "Large Language Models (LLMs), despite their impressive capabilities, often\nfail to accurately repeat a single word when prompted to, and instead output\nunrelated text. This unexplained failure mode represents a vulnerability,\nallowing even end-users to diverge models away from their intended behavior. We\naim to explain the causes for this phenomenon and link it to the concept of\n``attention sinks'', an emergent LLM behavior crucial for fluency, in which the\ninitial token receives disproportionately high attention scores. Our\ninvestigation identifies the neural circuit responsible for attention sinks and\nshows how long repetitions disrupt this circuit. We extend this finding to\nother non-repeating sequences that exhibit similar circuit disruptions. To\naddress this, we propose a targeted patch that effectively resolves the issue\nwithout negatively impacting the model's overall performance. This study\nprovides a mechanistic explanation for an LLM vulnerability, demonstrating how\ninterpretability can diagnose and address issues, and offering insights that\npave the way for more secure and reliable models.", "published": "2025-03-11 21:40:58", "link": "http://arxiv.org/abs/2503.08908v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Prompt-OT: An Optimal Transport Regularization Paradigm for Knowledge Preservation in Vision-Language Model Adaptation", "abstract": "Vision-language models (VLMs) such as CLIP demonstrate strong performance but\nstruggle when adapted to downstream tasks. Prompt learning has emerged as an\nefficient and effective strategy to adapt VLMs while preserving their\npre-trained knowledge. However, existing methods still lead to overfitting and\ndegrade zero-shot generalization. To address this challenge, we propose an\noptimal transport (OT)-guided prompt learning framework that mitigates\nforgetting by preserving the structural consistency of feature distributions\nbetween pre-trained and fine-tuned models. Unlike conventional point-wise\nconstraints, OT naturally captures cross-instance relationships and expands the\nfeasible parameter space for prompt tuning, allowing a better trade-off between\nadaptation and generalization. Our approach enforces joint constraints on both\nvision and text representations, ensuring a holistic feature alignment.\nExtensive experiments on benchmark datasets demonstrate that our simple yet\neffective method can outperform existing prompt learning strategies in\nbase-to-novel generalization, cross-dataset evaluation, and domain\ngeneralization without additional augmentation or ensemble techniques. The code\nis available at https://github.com/ChongQingNoSubway/Prompt-OT", "published": "2025-03-11 21:38:34", "link": "http://arxiv.org/abs/2503.08906v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "EvalTree: Profiling Language Model Weaknesses via Hierarchical Capability Trees", "abstract": "An ideal model evaluation should achieve two goals: identifying where the\nmodel fails and providing actionable improvement guidance. Toward these goals\nfor Language Model (LM) evaluations, we formulate the problem of generating a\nweakness profile, a set of weaknesses expressed in natural language, given an\nLM's performance on every individual instance in a benchmark. We introduce a\nsuite of quantitative assessments to compare different weakness profiling\nmethods. We also propose a weakness profiling method EvalTree. It constructs a\ncapability tree where each node represents a capability described in natural\nlanguage and is linked to a subset of benchmark instances that specifically\nevaluate this capability; it then extracts nodes where the LM performs poorly\nto generate a weakness profile. On the MATH and WildChat benchmarks, we show\nthat EvalTree outperforms baseline weakness profiling methods by identifying\nweaknesses more precisely and comprehensively. Weakness profiling further\nenables weakness-guided data collection, and training data collection guided by\nEvalTree-identified weaknesses improves LM performance more than other data\ncollection strategies. We also show how EvalTree exposes flaws in Chatbot\nArena's human-voter-based evaluation practice. To facilitate future work, we\nrelease our code and an interface that allows practitioners to interactively\nexplore the capability trees built by EvalTree.", "published": "2025-03-11 21:12:48", "link": "http://arxiv.org/abs/2503.08893v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PlainQAFact: Automatic Factuality Evaluation Metric for Biomedical Plain Language Summaries Generation", "abstract": "Hallucinated outputs from language models pose risks in the medical domain,\nespecially for lay audiences making health-related decisions. Existing\nfactuality evaluation methods, such as entailment- and question-answering-based\n(QA), struggle with plain language summary (PLS) generation due to elaborative\nexplanation phenomenon, which introduces external content (e.g., definitions,\nbackground, examples) absent from the source document to enhance comprehension.\nTo address this, we introduce PlainQAFact, a framework trained on a\nfine-grained, human-annotated dataset PlainFact, to evaluate the factuality of\nboth source-simplified and elaboratively explained sentences. PlainQAFact first\nclassifies factuality type and then assesses factuality using a\nretrieval-augmented QA-based scoring method. Our approach is lightweight and\ncomputationally efficient. Empirical results show that existing factuality\nmetrics fail to effectively evaluate factuality in PLS, especially for\nelaborative explanations, whereas PlainQAFact achieves state-of-the-art\nperformance. We further analyze its effectiveness across external knowledge\nsources, answer extraction strategies, overlap measures, and document\ngranularity levels, refining its overall factuality assessment.", "published": "2025-03-11 20:59:53", "link": "http://arxiv.org/abs/2503.08890v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Seeing What's Not There: Spurious Correlation in Multimodal LLMs", "abstract": "Unimodal vision models are known to rely on spurious correlations, but it\nremains unclear to what extent Multimodal Large Language Models (MLLMs) exhibit\nsimilar biases despite language supervision. In this paper, we investigate\nspurious bias in MLLMs and introduce SpurLens, a pipeline that leverages GPT-4\nand open-set object detectors to automatically identify spurious visual cues\nwithout human supervision. Our findings reveal that spurious correlations cause\ntwo major failure modes in MLLMs: (1) over-reliance on spurious cues for object\nrecognition, where removing these cues reduces accuracy, and (2) object\nhallucination, where spurious cues amplify the hallucination by over 10x. We\nvalidate our findings in various MLLMs and datasets. Beyond diagnosing these\nfailures, we explore potential mitigation strategies, such as prompt ensembling\nand reasoning-based prompting, and conduct ablation studies to examine the root\ncauses of spurious bias in MLLMs. By exposing the persistence of spurious\ncorrelations, our study calls for more rigorous evaluation methods and\nmitigation strategies to enhance the reliability of MLLMs.", "published": "2025-03-11 20:53:00", "link": "http://arxiv.org/abs/2503.08884v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for Efficient Long-Context Inference", "abstract": "Efficient long-context inference is critical as large language models (LLMs)\nadopt context windows of ranging from 128K to 1M tokens. However, the growing\nkey-value (KV) cache and the high computational complexity of attention create\nsignificant bottlenecks in memory usage and latency. In this paper, we find\nthat attention in diverse long-context tasks exhibits sparsity, and LLMs\nimplicitly \"know\" which tokens can be dropped or evicted at the head level\nafter the pre-filling stage. Based on this insight, we propose Self-Attention\nGuided Eviction~(SAGE-KV), a simple and effective KV eviction cache method for\nlong-context inference. After prefilling, our method performs a one-time top-k\nselection at both the token and head levels to compress the KV cache, enabling\nefficient inference with the reduced cache. Evaluations on LongBench and three\nlong-context LLMs (Llama3.1-8B-Instruct-128k, Llama3-8B-Prolong-512k-Instruct,\nand Qwen2.5-7B-Instruct-128k) show that SAGE-KV maintains accuracy comparable\nto full attention while significantly improving efficiency. Specifically,\nSAGE-KV achieves 4x higher memory efficiency with improved accuracy over the\nstatic KV cache selection method StreamLLM, and 2x higher memory efficiency\nwith better accuracy than the dynamic KV cache selection method Quest.", "published": "2025-03-11 20:45:02", "link": "http://arxiv.org/abs/2503.08879v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Interpretable and Robust Dialogue State Tracking via Natural Language Summarization with LLMs", "abstract": "This paper introduces a novel approach to Dialogue State Tracking (DST) that\nleverages Large Language Models (LLMs) to generate natural language\ndescriptions of dialogue states, moving beyond traditional slot-value\nrepresentations. Conventional DST methods struggle with open-domain dialogues\nand noisy inputs. Motivated by the generative capabilities of LLMs, our Natural\nLanguage DST (NL-DST) framework trains an LLM to directly synthesize\nhuman-readable state descriptions. We demonstrate through extensive experiments\non MultiWOZ 2.1 and Taskmaster-1 datasets that NL-DST significantly outperforms\nrule-based and discriminative BERT-based DST baselines, as well as generative\nslot-filling GPT-2 DST models, in both Joint Goal Accuracy and Slot Accuracy.\nAblation studies and human evaluations further validate the effectiveness of\nnatural language state generation, highlighting its robustness to noise and\nenhanced interpretability. Our findings suggest that NL-DST offers a more\nflexible, accurate, and human-understandable approach to dialogue state\ntracking, paving the way for more robust and adaptable task-oriented dialogue\nsystems.", "published": "2025-03-11 19:52:02", "link": "http://arxiv.org/abs/2503.08857v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contrastive Speaker-Aware Learning for Multi-party Dialogue Generation with LLMs", "abstract": "Multi-party dialogue generation presents significant challenges due to the\ncomplex interplay of multiple speakers and interwoven conversational threads.\nTraditional approaches often fall short in capturing these complexities,\nparticularly when relying on manually annotated dialogue relations. This paper\nintroduces Speaker-Attentive LLM (SA-LLM), a novel generative model that\nleverages pre-trained Large Language Models (LLMs) and a speaker-aware\ncontrastive learning strategy to address these challenges. SA-LLM incorporates\na speaker-attributed input encoding and a contrastive learning objective to\nimplicitly learn contextual coherence and speaker roles without explicit\nrelation annotations. Extensive experiments on the Ubuntu IRC and Movie\nDialogues datasets demonstrate that SA-LLM significantly outperforms\nstate-of-the-art baselines in automatic and human evaluations, achieving\nsuperior performance in fluency, coherence, informativeness, and response\ndiversity. Ablation studies and detailed error analyses further validate the\neffectiveness of the proposed speaker-attentive training approach, highlighting\nits robustness across different speaker roles and context lengths. The results\nunderscore the potential of SA-LLM as a powerful and annotation-free solution\nfor high-quality multi-party dialogue generation.", "published": "2025-03-11 19:28:12", "link": "http://arxiv.org/abs/2503.08842v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "evoBPE: Evolutionary Protein Sequence Tokenization", "abstract": "Recent advancements in computational biology have drawn compelling parallels\nbetween protein sequences and linguistic structures, highlighting the need for\nsophisticated tokenization methods that capture the intricate evolutionary\ndynamics of protein sequences. Current subword tokenization techniques,\nprimarily developed for natural language processing, often fail to represent\nprotein sequences' complex structural and functional properties adequately.\nThis study introduces evoBPE, a novel tokenization approach that integrates\nevolutionary mutation patterns into sequence segmentation, addressing critical\nlimitations in existing methods. By leveraging established substitution\nmatrices, evoBPE transcends traditional frequency-based tokenization\nstrategies. The method generates candidate token pairs through biologically\ninformed mutations, evaluating them based on pairwise alignment scores and\nfrequency thresholds. Extensive experiments on human protein sequences show\nthat evoBPE performs better across multiple dimensions. Domain conservation\nanalysis reveals that evoBPE consistently outperforms standard Byte-Pair\nEncoding, particularly as vocabulary size increases. Furthermore, embedding\nsimilarity analysis using ESM-2 suggests that mutation-based token replacements\npreserve biological sequence properties more effectively than arbitrary\nsubstitutions. The research contributes to protein sequence representation by\nintroducing a mutation-aware tokenization method that better captures\nevolutionary nuances. By bridging computational linguistics and molecular\nbiology, evoBPE opens new possibilities for machine learning applications in\nprotein function prediction, structural modeling, and evolutionary analysis.", "published": "2025-03-11 19:19:48", "link": "http://arxiv.org/abs/2503.08838v1", "categories": ["cs.CL", "q-bio.QM"], "primary_category": "cs.CL"}
{"title": "ResBench: Benchmarking LLM-Generated FPGA Designs with Resource Awareness", "abstract": "Field-Programmable Gate Arrays (FPGAs) are widely used in modern hardware\ndesign, yet writing Hardware Description Language (HDL) code for FPGA\nimplementation remains a complex and time-consuming task. Large Language Models\n(LLMs) have emerged as a promising tool for HDL generation, but existing\nbenchmarks for LLM-based code generation primarily focus on functional\ncorrectness while overlooking hardware resource usage. Furthermore, current\nbenchmarks offer limited diversity and do not fully represent the wide range of\nreal-world FPGA applications. To address these shortcomings, we introduce\nResBench, the first resource-focused benchmark explicitly designed to\ndistinguish between resource-optimized and inefficient LLM-generated HDL code.\nResBench consists of 56 problems across 12 categories, covering applications\nfrom finite state machines to financial computing. Our open-source evaluation\nframework automatically tests LLMs by generating Verilog code, verifying\ncorrectness, and measuring resource usage. The experiments, which primarily\nanalyze Lookup Table (LUT) usage, reveal significant differences among LLMs,\ndemonstrating ResBench's capability to identify models that generate more\nresource-optimized FPGA designs.", "published": "2025-03-11 18:54:17", "link": "http://arxiv.org/abs/2503.08823v2", "categories": ["cs.AR", "cs.AI", "cs.CL", "cs.ET", "cs.LG", "I.2.2"], "primary_category": "cs.AR"}
{"title": "Cross-Examiner: Evaluating Consistency of Large Language Model-Generated Explanations", "abstract": "Large Language Models (LLMs) are often asked to explain their outputs to\nenhance accuracy and transparency. However, evidence suggests that these\nexplanations can misrepresent the models' true reasoning processes. One\neffective way to identify inaccuracies or omissions in these explanations is\nthrough consistency checking, which typically involves asking follow-up\nquestions. This paper introduces, cross-examiner, a new method for generating\nfollow-up questions based on a model's explanation of an initial question. Our\nmethod combines symbolic information extraction with language model-driven\nquestion generation, resulting in better follow-up questions than those\nproduced by LLMs alone. Additionally, this approach is more flexible than other\nmethods and can generate a wider variety of follow-up questions.", "published": "2025-03-11 18:50:43", "link": "http://arxiv.org/abs/2503.08815v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ESNLIR: A Spanish Multi-Genre Dataset with Causal Relationships", "abstract": "Natural Language Inference (NLI), also known as Recognizing Textual\nEntailment (RTE), serves as a crucial area within the domain of Natural\nLanguage Processing (NLP). This area fundamentally empowers machines to discern\nsemantic relationships between assorted sections of text. Even though\nconsiderable work has been executed for the English language, it has been\nobserved that efforts for the Spanish language are relatively sparse. Keeping\nthis in view, this paper focuses on generating a multi-genre Spanish dataset\nfor NLI, ESNLIR, particularly accounting for causal Relationships. A\npreliminary baseline has been conceptualized and subjected to an evaluation,\nleveraging models drawn from the BERT family. The findings signify that the\nenrichment of genres essentially contributes to the enrichment of the model's\ncapability to generalize.\n  The code, notebooks and whole datasets for this experiments is available at:\nhttps://zenodo.org/records/15002575. If you are interested only in the dataset\nyou can find it here: https://zenodo.org/records/15002371.", "published": "2025-03-11 18:32:16", "link": "http://arxiv.org/abs/2503.08803v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Perplexity Trap: PLM-Based Retrievers Overrate Low Perplexity Documents", "abstract": "Previous studies have found that PLM-based retrieval models exhibit a\npreference for LLM-generated content, assigning higher relevance scores to\nthese documents even when their semantic quality is comparable to human-written\nones. This phenomenon, known as source bias, threatens the sustainable\ndevelopment of the information access ecosystem. However, the underlying causes\nof source bias remain unexplored. In this paper, we explain the process of\ninformation retrieval with a causal graph and discover that PLM-based\nretrievers learn perplexity features for relevance estimation, causing source\nbias by ranking the documents with low perplexity higher. Theoretical analysis\nfurther reveals that the phenomenon stems from the positive correlation between\nthe gradients of the loss functions in language modeling task and retrieval\ntask. Based on the analysis, a causal-inspired inference-time debiasing method\nis proposed, called Causal Diagnosis and Correction (CDC). CDC first diagnoses\nthe bias effect of the perplexity and then separates the bias effect from the\noverall estimated relevance score. Experimental results across three domains\ndemonstrate the superior debiasing effectiveness of CDC, emphasizing the\nvalidity of our proposed explanatory framework. Source codes are available at\nhttps://github.com/WhyDwelledOnAi/Perplexity-Trap.", "published": "2025-03-11 17:59:00", "link": "http://arxiv.org/abs/2503.08684v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Self-Taught Self-Correction for Small Language Models", "abstract": "Although large language models (LLMs) have achieved remarkable performance\nacross various tasks, they remain prone to errors. A key challenge is enabling\nthem to self-correct. While prior research has relied on external tools or\nlarge proprietary models, this work explores self-correction in small language\nmodels (SLMs) through iterative fine-tuning using solely self-generated data.\nWe introduce the Self-Taught Self-Correction (STaSC) algorithm, which\nincorporates multiple algorithmic design choices. Experimental results on a\nquestion-answering task demonstrate that STaSC effectively learns\nself-correction, leading to significant performance improvements. Our analysis\nfurther provides insights into the mechanisms of self-correction and the impact\nof different design choices on learning dynamics and overall performance. To\nsupport future research, we release our user-friendly codebase and lightweight\nmodels.", "published": "2025-03-11 17:57:44", "link": "http://arxiv.org/abs/2503.08681v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Chain-of-Thought Reasoning In The Wild Is Not Always Faithful", "abstract": "Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art\nAI capabilities. However, recent studies have shown that CoT reasoning is not\nalways faithful, i.e. CoT reasoning does not always reflect how models arrive\nat conclusions. So far, most of these studies have focused on unfaithfulness in\nunnatural contexts where an explicit bias has been introduced. In contrast, we\nshow that unfaithful CoT can occur on realistic prompts with no artificial\nbias. Our results reveal non-negligible rates of several forms of unfaithful\nreasoning in frontier models: Sonnet 3.7 (16.3%), DeepSeek R1 (5.3%) and\nChatGPT-4o (7.0%) all answer a notable proportion of question pairs\nunfaithfully. Specifically, we find that models rationalize their implicit\nbiases in answers to binary questions (\"implicit post-hoc rationalization\").\nFor example, when separately presented with the questions \"Is X bigger than Y?\"\nand \"Is Y bigger than X?\", models sometimes produce superficially coherent\narguments to justify answering Yes to both questions or No to both questions,\ndespite such responses being logically contradictory. We also investigate\nrestoration errors (Dziri et al., 2023), where models make and then silently\ncorrect errors in their reasoning, and unfaithful shortcuts, where models use\nclearly illogical reasoning to simplify solving problems in Putnam questions (a\nhard benchmark). Our findings raise challenges for AI safety work that relies\non monitoring CoT to detect undesired behavior.", "published": "2025-03-11 17:56:30", "link": "http://arxiv.org/abs/2503.08679v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "AgentOrca: A Dual-System Framework to Evaluate Language Agents on Operational Routine and Constraint Adherence", "abstract": "As language agents progressively automate critical tasks across domains,\ntheir ability to operate within operational constraints and safety protocols\nbecomes essential. While extensive research has demonstrated these agents'\neffectiveness in downstream task completion, their reliability in following\noperational procedures and constraints remains largely unexplored. To this end,\nwe present AgentOrca, a dual-system framework for evaluating language agents'\ncompliance with operational constraints and routines. Our framework encodes\naction constraints and routines through both natural language prompts for\nagents and corresponding executable code serving as ground truth for automated\nverification. Through an automated pipeline of test case generation and\nevaluation across five real-world domains, we quantitatively assess current\nlanguage agents' adherence to operational constraints. Our findings reveal\nnotable performance gaps among state-of-the-art models, with large reasoning\nmodels like o1 demonstrating superior compliance while others show\nsignificantly lower performance, particularly when encountering complex\nconstraints or user persuasion attempts.", "published": "2025-03-11 17:53:02", "link": "http://arxiv.org/abs/2503.08669v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring the Word Sense Disambiguation Capabilities of Large Language Models", "abstract": "Word Sense Disambiguation (WSD) is a historical task in computational\nlinguistics that has received much attention over the years. However, with the\nadvent of Large Language Models (LLMs), interest in this task (in its classical\ndefinition) has decreased. In this study, we evaluate the performance of\nvarious LLMs on the WSD task. We extend a previous benchmark (XL-WSD) to\nre-design two subtasks suitable for LLM: 1) given a word in a sentence, the LLM\nmust generate the correct definition; 2) given a word in a sentence and a set\nof predefined meanings, the LLM must select the correct one. The extended\nbenchmark is built using the XL-WSD and BabelNet. The results indicate that\nLLMs perform well in zero-shot learning but cannot surpass current\nstate-of-the-art methods. However, a fine-tuned model with a medium number of\nparameters outperforms all other models, including the state-of-the-art.", "published": "2025-03-11 17:50:44", "link": "http://arxiv.org/abs/2503.08662v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploiting Instruction-Following Retrievers for Malicious Information Retrieval", "abstract": "Instruction-following retrievers have been widely adopted alongside LLMs in\nreal-world applications, but little work has investigated the safety risks\nsurrounding their increasing search capabilities. We empirically study the\nability of retrievers to satisfy malicious queries, both when used directly and\nwhen used in a retrieval augmented generation-based setup. Concretely, we\ninvestigate six leading retrievers, including NV-Embed and LLM2Vec, and find\nthat given malicious requests, most retrievers can (for >50% of queries) select\nrelevant harmful passages. For example, LLM2Vec correctly selects passages for\n61.35% of our malicious queries. We further uncover an emerging risk with\ninstruction-following retrievers, where highly relevant harmful information can\nbe surfaced by exploiting their instruction-following capabilities. Finally, we\nshow that even safety-aligned LLMs, such as Llama3, can satisfy malicious\nrequests when provided with harmful retrieved passages in-context. In summary,\nour findings underscore the malicious misuse risks associated with increasing\nretriever capability.", "published": "2025-03-11 17:36:53", "link": "http://arxiv.org/abs/2503.08644v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse Attention", "abstract": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale.", "published": "2025-03-11 17:30:58", "link": "http://arxiv.org/abs/2503.08640v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding the Quality-Diversity Trade-off in Diffusion Language Models", "abstract": "Diffusion models have seen immense success in modelling continuous data\nacross a range of domains such as vision and audio. Despite the challenges of\nadapting diffusion models to discrete data, recent work explores their\napplication to text generation by working in the continuous embedding space.\nHowever, these models lack a natural means to control the inherent trade-off\nbetween quality and diversity as afforded by the temperature hyperparameter in\nautoregressive models, hindering understanding of model performance and\nrestricting generation quality. This work proposes the use of classifier-free\nguidance and stochastic clamping for manipulating the quality-diversity\ntrade-off on sequence-to-sequence tasks, demonstrating that these techniques\nmay be used to improve the performance of a diffusion language model.", "published": "2025-03-11 17:18:01", "link": "http://arxiv.org/abs/2503.10683v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NSF-SciFy: Mining the NSF Awards Database for Scientific Claims", "abstract": "We present NSF-SciFy, a large-scale dataset for scientific claim extraction\nderived from the National Science Foundation (NSF) awards database, comprising\nover 400K grant abstracts spanning five decades. While previous datasets relied\non published literature, we leverage grant abstracts which offer a unique\nadvantage: they capture claims at an earlier stage in the research lifecycle\nbefore publication takes effect. We also introduce a new task to distinguish\nbetween existing scientific claims and aspirational research intentions in\nproposals. Using zero-shot prompting with frontier large language models, we\njointly extract 114K scientific claims and 145K investigation proposals from\n16K grant abstracts in the materials science domain to create a focused subset\ncalled NSF-SciFy-MatSci. We use this dataset to evaluate 3 three key tasks: (1)\ntechnical to non-technical abstract generation, where models achieve high\nBERTScore (0.85+ F1); (2) scientific claim extraction, where fine-tuned models\noutperform base models by 100% relative improvement; and (3) investigation\nproposal extraction, showing 90%+ improvement with fine-tuning. We introduce\nnovel LLM-based evaluation metrics for robust assessment of claim/proposal\nextraction quality. As the largest scientific claim dataset to date -- with an\nestimated 2.8 million claims across all STEM disciplines funded by the NSF --\nNSF-SciFy enables new opportunities for claim verification and meta-scientific\nresearch. We publicly release all datasets, trained models, and evaluation code\nto facilitate further research.", "published": "2025-03-11 16:35:08", "link": "http://arxiv.org/abs/2503.08600v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Llms, Virtual Users, and Bias: Predicting Any Survey Question Without Human Data", "abstract": "Large Language Models (LLMs) offer a promising alternative to traditional\nsurvey methods, potentially enhancing efficiency and reducing costs. In this\nstudy, we use LLMs to create virtual populations that answer survey questions,\nenabling us to predict outcomes comparable to human responses. We evaluate\nseveral LLMs-including GPT-4o, GPT-3.5, Claude 3.5-Sonnet, and versions of the\nLlama and Mistral models-comparing their performance to that of a traditional\nRandom Forests algorithm using demographic data from the World Values Survey\n(WVS). LLMs demonstrate competitive performance overall, with the significant\nadvantage of requiring no additional training data. However, they exhibit\nbiases when predicting responses for certain religious and population groups,\nunderperforming in these areas. On the other hand, Random Forests demonstrate\nstronger performance than LLMs when trained with sufficient data. We observe\nthat removing censorship mechanisms from LLMs significantly improves predictive\naccuracy, particularly for underrepresented demographic segments where censored\nmodels struggle. These findings highlight the importance of addressing biases\nand reconsidering censorship approaches in LLMs to enhance their reliability\nand fairness in public opinion research.", "published": "2025-03-11 16:27:20", "link": "http://arxiv.org/abs/2503.16498v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.HC"}
{"title": "BiasEdit: Debiasing Stereotyped Language Models via Model Editing", "abstract": "Previous studies have established that language models manifest stereotyped\nbiases. Existing debiasing strategies, such as retraining a model with\ncounterfactual data, representation projection, and prompting often fail to\nefficiently eliminate bias or directly alter the models' biased internal\nrepresentations. To address these issues, we propose BiasEdit, an efficient\nmodel editing method to remove stereotypical bias from language models through\nlightweight networks that act as editors to generate parameter updates.\nBiasEdit employs a debiasing loss guiding editor networks to conduct local\nedits on partial parameters of a language model for debiasing while preserving\nthe language modeling abilities during editing through a retention loss.\nExperiments on StereoSet and Crows-Pairs demonstrate the effectiveness,\nefficiency, and robustness of BiasEdit in eliminating bias compared to\ntangental debiasing baselines and little to no impact on the language models'\ngeneral capabilities. In addition, we conduct bias tracing to probe bias in\nvarious modules and explore bias editing impacts on different components of\nlanguage models.", "published": "2025-03-11 16:25:36", "link": "http://arxiv.org/abs/2503.08588v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DeepReview: Improving LLM-based Paper Review with Human-like Deep Thinking Process", "abstract": "Large Language Models (LLMs) are increasingly utilized in scientific research\nassessment, particularly in automated paper review. However, existing LLM-based\nreview systems face significant challenges, including limited domain expertise,\nhallucinated reasoning, and a lack of structured evaluation. To address these\nlimitations, we introduce DeepReview, a multi-stage framework designed to\nemulate expert reviewers by incorporating structured analysis, literature\nretrieval, and evidence-based argumentation. Using DeepReview-13K, a curated\ndataset with structured annotations, we train DeepReviewer-14B, which\noutperforms CycleReviewer-70B with fewer tokens. In its best mode,\nDeepReviewer-14B achieves win rates of 88.21\\% and 80.20\\% against GPT-o1 and\nDeepSeek-R1 in evaluations. Our work sets a new benchmark for LLM-based paper\nreview, with all resources publicly available. The code, model, dataset and\ndemo have be released in http://ai-researcher.net.", "published": "2025-03-11 15:59:43", "link": "http://arxiv.org/abs/2503.08569v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transferring Extreme Subword Style Using Ngram Model-Based Logit Scaling", "abstract": "We present an ngram model-based logit scaling technique that effectively\ntransfers extreme subword stylistic variation to large language models at\ninference time. We demonstrate its efficacy by tracking the perplexity of\ngenerated text with respect to the ngram interpolated and original versions of\nan evaluation model. Minimizing the former measure while the latter approaches\nthe perplexity of a text produced by a target author or character lets us\nselect a sufficient degree of adaptation while retaining fluency.", "published": "2025-03-11 15:36:41", "link": "http://arxiv.org/abs/2503.08550v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph of AI Ideas: Leveraging Knowledge Graphs and LLMs for AI Research Idea Generation", "abstract": "Reading relevant scientific papers and analyzing research development trends\nis a critical step in generating new scientific ideas. However, the rapid\nincrease in the volume of research literature and the complex citation\nrelationships make it difficult for researchers to quickly analyze and derive\nmeaningful research trends. The development of large language models (LLMs) has\nprovided a novel approach for automatically summarizing papers and generating\ninnovative research ideas. However, existing paper-based idea generation\nmethods either simply input papers into LLMs via prompts or form logical chains\nof creative development based on citation relationships, without fully\nexploiting the semantic information embedded in these citations. Inspired by\nknowledge graphs and human cognitive processes, we propose a framework called\nthe Graph of AI Ideas (GoAI) for the AI research field, which is dominated by\nopen-access papers. This framework organizes relevant literature into entities\nwithin a knowledge graph and summarizes the semantic information contained in\ncitations into relations within the graph. This organization effectively\nreflects the relationships between two academic papers and the advancement of\nthe AI research field. Such organization aids LLMs in capturing the current\nprogress of research, thereby enhancing their creativity. Experimental results\ndemonstrate the effectiveness of our approach in generating novel, clear, and\neffective research ideas.", "published": "2025-03-11 15:36:38", "link": "http://arxiv.org/abs/2503.08549v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "DAFE: LLM-Based Evaluation Through Dynamic Arbitration for Free-Form Question-Answering", "abstract": "Evaluating Large Language Models (LLMs) free-form generated responses remains\na challenge due to their diverse and open-ended nature. Traditional supervised\nsignal-based automatic metrics fail to capture semantic equivalence or handle\nthe variability of open-ended responses, while human evaluation, though\nreliable, is resource-intensive. Leveraging LLMs as evaluators offers a\npromising alternative due to their strong language understanding and\ninstruction-following capabilities. Taking advantage of these capabilities, we\npropose the Dynamic Arbitration Framework for Evaluation (DAFE), which employs\ntwo primary LLM-as-judges and engages a third arbitrator only in cases of\ndisagreements. This selective arbitration prioritizes evaluation reliability\nwhile reducing unnecessary computational demands compared to conventional\nmajority voting. DAFE utilizes task-specific reference answers with dynamic\narbitration to enhance judgment accuracy, resulting in significant improvements\nin evaluation metrics such as Macro F1 and Cohen's Kappa. Through experiments,\nincluding a comprehensive human evaluation, we demonstrate DAFE's ability to\nprovide consistent, scalable, and resource-efficient assessments, establishing\nit as a robust framework for evaluating free-form model outputs.", "published": "2025-03-11 15:29:55", "link": "http://arxiv.org/abs/2503.08542v1", "categories": ["cs.CL", "cs.AI", "I.2.0; I.2.7"], "primary_category": "cs.CL"}
{"title": "ESPnet-SDS: Unified Toolkit and Demo for Spoken Dialogue Systems", "abstract": "Advancements in audio foundation models (FMs) have fueled interest in\nend-to-end (E2E) spoken dialogue systems, but different web interfaces for each\nsystem makes it challenging to compare and contrast them effectively. Motivated\nby this, we introduce an open-source, user-friendly toolkit designed to build\nunified web interfaces for various cascaded and E2E spoken dialogue systems.\nOur demo further provides users with the option to get on-the-fly automated\nevaluation metrics such as (1) latency, (2) ability to understand user input,\n(3) coherence, diversity, and relevance of system response, and (4)\nintelligibility and audio quality of system output. Using the evaluation\nmetrics, we compare various cascaded and E2E spoken dialogue systems with a\nhuman-human conversation dataset as a proxy. Our analysis demonstrates that the\ntoolkit allows researchers to effortlessly compare and contrast different\ntechnologies, providing valuable insights such as current E2E systems having\npoorer audio quality and less diverse responses. An example demo produced using\nour toolkit is publicly available here:\nhttps://huggingface.co/spaces/Siddhant/Voice_Assistant_Demo.", "published": "2025-03-11 15:24:02", "link": "http://arxiv.org/abs/2503.08533v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Position-Aware Depth Decay Decoding ($D^3$): Boosting Large Language Model Inference Efficiency", "abstract": "Due to the large number of parameters, the inference phase of Large Language\nModels (LLMs) is resource-intensive. Unlike traditional model compression,\nwhich needs retraining, recent dynamic computation methods show that not all\ncomponents are required for inference, enabling a training-free pipeline. In\nthis paper, we focus on the dynamic depth of LLM generation. A token-position\naware layer skipping framework is proposed to save 1.5x times operations\nefficiently while maintaining performance. We first observed that tokens\npredicted later have lower perplexity and thus require less computation. Then,\nwe propose a training-free algorithm called Position-Aware Depth Decay Decoding\n($D^3$), which leverages a power-law decay function, $\\left\\lfloor L \\times\n(\\alpha^i) \\right\\rfloor$, to determine the number of layers to retain when\ngenerating token $T_i$. Remarkably, without any retraining, the $D^3$ achieves\nsuccess across a wide range of generation tasks for the first time. Experiments\non large language models (\\ie the Llama) with $7 \\sim 70$ billion parameters\nshow that $D^3$ can achieve an average 1.5x speedup compared with the\nfull-inference pipeline while maintaining comparable performance with nearly no\nperformance drop ($<1\\%$) on the GSM8K and BBH benchmarks.", "published": "2025-03-11 15:15:54", "link": "http://arxiv.org/abs/2503.08524v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ReviewAgents: Bridging the Gap Between Human and AI-Generated Paper Reviews", "abstract": "Academic paper review is a critical yet time-consuming task within the\nresearch community. With the increasing volume of academic publications,\nautomating the review process has become a significant challenge. The primary\nissue lies in generating comprehensive, accurate, and reasoning-consistent\nreview comments that align with human reviewers' judgments. In this paper, we\naddress this challenge by proposing ReviewAgents, a framework that leverages\nlarge language models (LLMs) to generate academic paper reviews. We first\nintroduce a novel dataset, Review-CoT, consisting of 142k review comments,\ndesigned for training LLM agents. This dataset emulates the structured\nreasoning process of human reviewers-summarizing the paper, referencing\nrelevant works, identifying strengths and weaknesses, and generating a review\nconclusion. Building upon this, we train LLM reviewer agents capable of\nstructured reasoning using a relevant-paper-aware training method. Furthermore,\nwe construct ReviewAgents, a multi-role, multi-LLM agent review framework, to\nenhance the review comment generation process. Additionally, we propose\nReviewBench, a benchmark for evaluating the review comments generated by LLMs.\nOur experimental results on ReviewBench demonstrate that while existing LLMs\nexhibit a certain degree of potential for automating the review process, there\nremains a gap when compared to human-generated reviews. Moreover, our\nReviewAgents framework further narrows this gap, outperforming advanced LLMs in\ngenerating review comments.", "published": "2025-03-11 14:56:58", "link": "http://arxiv.org/abs/2503.08506v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Multi-Hop Fact Verification with Structured Knowledge-Augmented Large Language Models", "abstract": "The rapid development of social platforms exacerbates the dissemination of\nmisinformation, which stimulates the research in fact verification. Recent\nstudies tend to leverage semantic features to solve this problem as a\nsingle-hop task. However, the process of verifying a claim requires several\npieces of evidence with complicated inner logic and relations to verify the\ngiven claim in real-world situations. Recent studies attempt to improve both\nunderstanding and reasoning abilities to enhance the performance, but they\noverlook the crucial relations between entities that benefit models to\nunderstand better and facilitate the prediction. To emphasize the significance\nof relations, we resort to Large Language Models (LLMs) considering their\nexcellent understanding ability. Instead of other methods using LLMs as the\npredictor, we take them as relation extractors, for they do better in\nunderstanding rather than reasoning according to the experimental results.\nThus, to solve the challenges above, we propose a novel Structured\nKnowledge-Augmented LLM-based Network (LLM-SKAN) for multi-hop fact\nverification. Specifically, we utilize an LLM-driven Knowledge Extractor to\ncapture fine-grained information, including entities and their complicated\nrelations. Besides, we leverage a Knowledge-Augmented Relation Graph Fusion\nmodule to interact with each node and learn better claim-evidence\nrepresentations comprehensively. The experimental results on four common-used\ndatasets demonstrate the effectiveness and superiority of our model.", "published": "2025-03-11 14:47:24", "link": "http://arxiv.org/abs/2503.08495v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-end Learning of Sparse Interventions on Activations to Steer Generation", "abstract": "The growing use of generative models in daily life calls for efficient\nmechanisms to control their generation, to e.g., produce safe content or\nprovide users with tools to explore style changes. Ideally, such mechanisms\nshould be cheap, both at train and inference time, while preserving output\nquality. Recent research has shown that such mechanisms can be obtained by\nintervening exclusively on model activations, with the goal of correcting\ndistributional differences between activations seen when using prompts from a\nsource vs. a target set (e.g., toxic and non-toxic sentences). While cheap,\nthese fast methods are inherently crude: their maps are tuned locally, not\naccounting for their impact on downstream layers, resulting in interventions\nthat cause unintended shifts when used out-of-sample. We propose in this work\nlinear end-to-end activation steering (LinEAS), an approach trained with a\nglobal loss that accounts simultaneously for all layerwise distributional\nshifts. In addition to being more robust, the loss used to train LinEAS can be\nregularized with sparsifying norms, which can automatically carry out neuron\nand layer selection. Empirically, LinEAS only requires a handful of samples to\nbe effective, and beats similar baselines on toxicity mitigation, while\nperforming on par with far more involved finetuning approaches. We show that\nLinEAS interventions can be composed, study the impact of sparsity on their\nperformance, and showcase applications in text-to-image diffusions.", "published": "2025-03-11 14:09:04", "link": "http://arxiv.org/abs/2503.10679v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Stick to Facts: Towards Fidelity-oriented Product Description Generation", "abstract": "Different from other text generation tasks, in product description\ngeneration, it is of vital importance to generate faithful descriptions that\nstick to the product attribute information. However, little attention has been\npaid to this problem. To bridge this gap, we propose a model named\nFidelity-oriented Product Description Generator (FPDG). FPDG takes the entity\nlabel of each word into account, since the product attribute information is\nalways conveyed by entity words. Specifically, we first propose a Recurrent\nNeural Network (RNN) decoder based on the Entity-label-guided Long Short-Term\nMemory (ELSTM) cell, taking both the embedding and the entity label of each\nword as input. Second, we establish a keyword memory that stores the entity\nlabels as keys and keywords as values, allowing FPDG to attend to keywords by\nattending to their entity labels. Experiments conducted on a large-scale\nreal-world product description dataset show that our model achieves\nstate-of-the-art performance in terms of both traditional generation metrics\nand human evaluations. Specifically, FPDG increases the fidelity of the\ngenerated descriptions by 25%.", "published": "2025-03-11 14:04:24", "link": "http://arxiv.org/abs/2503.08454v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exposing Product Bias in LLM Investment Recommendation", "abstract": "Large language models (LLMs), as a new generation of recommendation engines,\npossess powerful summarization and data analysis capabilities, surpassing\ntraditional recommendation systems in both scope and performance. One promising\napplication is investment recommendation. In this paper, we reveal a novel\nproduct bias in LLM investment recommendation, where LLMs exhibit systematic\npreferences for specific products. Such preferences can subtly influence user\ninvestment decisions, potentially leading to inflated valuations of products\nand financial bubbles, posing risks to both individual investors and market\nstability. To comprehensively study the product bias, we develop an automated\npipeline to create a dataset of 567,000 samples across five asset classes\n(stocks, mutual funds, cryptocurrencies, savings, and portfolios). With this\ndataset, we present the bf first study on product bias in LLM investment\nrecommendations. Our findings reveal that LLMs exhibit clear product\npreferences, such as certain stocks (e.g., `AAPL' from Apple and `MSFT' from\nMicrosoft). Notably, this bias persists even after applying debiasing\ntechniques. We urge AI researchers to take heed of the product bias in LLM\ninvestment recommendations and its implications, ensuring fairness and security\nin the digital space and market.", "published": "2025-03-11 13:10:00", "link": "http://arxiv.org/abs/2503.08750v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Fact-checking with Generative AI: A Systematic Cross-Topic Examination of LLMs Capacity to Detect Veracity of Political Information", "abstract": "The purpose of this study is to assess how large language models (LLMs) can\nbe used for fact-checking and contribute to the broader debate on the use of\nautomated means for veracity identification. To achieve this purpose, we use AI\nauditing methodology that systematically evaluates performance of five LLMs\n(ChatGPT 4, Llama 3 (70B), Llama 3.1 (405B), Claude 3.5 Sonnet, and Google\nGemini) using prompts regarding a large set of statements fact-checked by\nprofessional journalists (16,513). Specifically, we use topic modeling and\nregression analysis to investigate which factors (e.g. topic of the prompt or\nthe LLM type) affect evaluations of true, false, and mixed statements. Our\nfindings reveal that while ChatGPT 4 and Google Gemini achieved higher accuracy\nthan other models, overall performance across models remains modest. Notably,\nthe results indicate that models are better at identifying false statements,\nespecially on sensitive topics such as COVID-19, American political\ncontroversies, and social issues, suggesting possible guardrails that may\nenhance accuracy on these topics. The major implication of our findings is that\nthere are significant challenges for using LLMs for factchecking, including\nsignificant variation in performance across different LLMs and unequal quality\nof outputs for specific topics which can be attributed to deficits of training\ndata. Our research highlights the potential and limitations of LLMs in\npolitical fact-checking, suggesting potential avenues for further improvements\nin guardrails as well as fine-tuning.", "published": "2025-03-11 13:06:40", "link": "http://arxiv.org/abs/2503.08404v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "OpenRAG: Optimizing RAG End-to-End via In-Context Retrieval Learning", "abstract": "In this paper, we analyze and empirically show that the learned relevance for\nconventional information retrieval (IR) scenarios may be inconsistent in\nretrieval-augmented generation (RAG) scenarios. To bridge this gap, we\nintroduce OpenRAG, a RAG framework that is optimized end-to-end by tuning the\nretriever to capture in-context relevance, enabling adaptation to the diverse\nand evolving needs. Extensive experiments across a wide range of tasks\ndemonstrate that OpenRAG, by tuning a retriever end-to-end, leads to a\nconsistent improvement of 4.0% over the original retriever, consistently\noutperforming existing state-of-the-art retrievers by 2.1%. Additionally, our\nresults indicate that for some tasks, an end-to-end tuned 0.2B retriever can\nachieve improvements that surpass those of RAG-oriented or instruction-tuned 8B\nlarge language models (LLMs), highlighting the cost-effectiveness of our\napproach in enhancing RAG systems.", "published": "2025-03-11 13:04:05", "link": "http://arxiv.org/abs/2503.08398v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "JurisTCU: A Brazilian Portuguese Information Retrieval Dataset with Query Relevance Judgments", "abstract": "This paper introduces JurisTCU, a Brazilian Portuguese dataset for legal\ninformation retrieval (LIR). The dataset is freely available and consists of\n16,045 jurisprudential documents from the Brazilian Federal Court of Accounts,\nalong with 150 queries annotated with relevance judgments. It addresses the\nscarcity of Portuguese-language LIR datasets with query relevance annotations.\nThe queries are organized into three groups: real user keyword-based queries,\nsynthetic keyword-based queries, and synthetic question-based queries.\nRelevance judgments were produced through a hybrid approach combining LLM-based\nscoring with expert domain validation. We used JurisTCU in 14 experiments using\nlexical search (document expansion methods) and semantic search (BERT-based and\nOpenAI embeddings). We show that the document expansion methods significantly\nimprove the performance of standard BM25 search on this dataset, with\nimprovements exceeding 45% in P@10, R@10, and nDCG@10 metrics when evaluating\nshort keyword-based queries. Among the embedding models, the OpenAI models\nproduced the best results, with improvements of approximately 70% in P@10,\nR@10, and nDCG@10 metrics for short keyword-based queries, suggesting that\nthese dense embeddings capture semantic relationships in this domain,\nsurpassing the reliance on lexical terms. Besides offering a dataset for the\nPortuguese-language IR research community, suitable for evaluating search\nsystems, the results also contribute to enhancing a search system highly\nrelevant to Brazilian citizens.", "published": "2025-03-11 12:39:04", "link": "http://arxiv.org/abs/2503.08379v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Adding Chocolate to Mint: Mitigating Metric Interference in Machine Translation", "abstract": "As automatic metrics become increasingly stronger and widely adopted, the\nrisk of unintentionally \"gaming the metric\" during model development rises.\nThis issue is caused by metric interference (Mint), i.e., the use of the same\nor related metrics for both model tuning and evaluation. Mint can misguide\npractitioners into being overoptimistic about the performance of their systems:\nas system outputs become a function of the interfering metric, their estimated\nquality loses correlation with human judgments. In this work, we analyze two\ncommon cases of Mint in machine translation-related tasks: filtering of\ntraining data, and decoding with quality signals. Importantly, we find that\nMint strongly distorts instance-level metric scores, even when metrics are not\ndirectly optimized for -- questioning the common strategy of leveraging a\ndifferent, yet related metric for evaluation that is not used for tuning. To\naddress this problem, we propose MintAdjust, a method for more reliable\nevaluation under Mint. On the WMT24 MT shared task test set, MintAdjust ranks\ntranslations and systems more accurately than state-of-the-art-metrics across a\nmajority of language pairs, especially for high-quality systems. Furthermore,\nMintAdjust outperforms AutoRank, the ensembling method used by the organizers.", "published": "2025-03-11 11:40:10", "link": "http://arxiv.org/abs/2503.08327v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Scalable and Cross-Lingual Specialist Language Models for Oncology", "abstract": "Clinical oncology generates vast, unstructured data that often contain\ninconsistencies, missing information, and ambiguities, making it difficult to\nextract reliable insights for data-driven decision-making. General-purpose\nlarge language models (LLMs) struggle with these challenges due to their lack\nof domain-specific reasoning, including specialized clinical terminology,\ncontext-dependent interpretations, and multi-modal data integration. We address\nthese issues with an oncology-specialized, efficient, and adaptable NLP\nframework that combines instruction tuning, retrieval-augmented generation\n(RAG), and graph-based knowledge integration. Our lightweight models prove\neffective at oncology-specific tasks, such as named entity recognition (e.g.,\nidentifying cancer diagnoses), entity linking (e.g., linking entities to\nstandardized ontologies), TNM staging, document classification (e.g., cancer\nsubtype classification from pathology reports), and treatment response\nprediction. Our framework emphasizes adaptability and resource efficiency. We\ninclude minimal German instructions, collected at the University Hospital\nZurich (USZ), to test whether small amounts of non-English language data can\neffectively transfer knowledge across languages. This approach mirrors our\nmotivation for lightweight models, which balance strong performance with\nreduced computational costs, making them suitable for resource-limited\nhealthcare settings. We validated our models on oncology datasets,\ndemonstrating strong results in named entity recognition, relation extraction,\nand document classification.", "published": "2025-03-11 11:34:57", "link": "http://arxiv.org/abs/2503.08323v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models for Outpatient Referral: Problem Definition, Benchmarking and Challenges", "abstract": "Large language models (LLMs) are increasingly applied to outpatient referral\ntasks across healthcare systems. However, there is a lack of standardized\nevaluation criteria to assess their effectiveness, particularly in dynamic,\ninteractive scenarios. In this study, we systematically examine the\ncapabilities and limitations of LLMs in managing tasks within Intelligent\nOutpatient Referral (IOR) systems and propose a comprehensive evaluation\nframework specifically designed for such systems. This framework comprises two\ncore tasks: static evaluation, which focuses on evaluating the ability of\npredefined outpatient referrals, and dynamic evaluation, which evaluates\ncapabilities of refining outpatient referral recommendations through iterative\ndialogues. Our findings suggest that LLMs offer limited advantages over\nBERT-like models, but show promise in asking effective questions during\ninteractive dialogues.", "published": "2025-03-11 11:05:42", "link": "http://arxiv.org/abs/2503.08292v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Beyond Outlining: Heterogeneous Recursive Planning for Adaptive Long-form Writing with Language Models", "abstract": "Long-form writing agents require flexible integration and interaction across\ninformation retrieval, reasoning, and composition. Current approaches rely on\npredetermined workflows and rigid thinking patterns to generate outlines before\nwriting, resulting in constrained adaptability during writing. In this paper we\npropose a general agent framework that achieves human-like adaptive writing\nthrough recursive task decomposition and dynamic integration of three\nfundamental task types, i.e. retrieval, reasoning, and composition. Our\nmethodology features: 1) a planning mechanism that interleaves recursive task\ndecomposition and execution, eliminating artificial restrictions on writing\nworkflow; and 2) integration of task types that facilitates heterogeneous task\ndecomposition. Evaluations on both fiction writing and technical report\ngeneration show that our method consistently outperforms state-of-the-art\napproaches across all automatic evaluation metrics, which demonstrate the\neffectiveness and broad applicability of our proposed framework.", "published": "2025-03-11 10:43:01", "link": "http://arxiv.org/abs/2503.08275v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Investigating Execution-Aware Language Models for Code Optimization", "abstract": "Code optimization is the process of enhancing code efficiency, while\npreserving its intended functionality. This process often requires a deep\nunderstanding of the code execution behavior at run-time to identify and\naddress inefficiencies effectively. Recent studies have shown that language\nmodels can play a significant role in automating code optimization. However,\nthese models may have insufficient knowledge of how code execute at run-time.\nTo address this limitation, researchers have developed strategies that\nintegrate code execution information into language models. These strategies\nhave shown promise, enhancing the effectiveness of language models in various\nsoftware engineering tasks. However, despite the close relationship between\ncode execution behavior and efficiency, the specific impact of these strategies\non code optimization remains largely unexplored. This study investigates how\nincorporating code execution information into language models affects their\nability to optimize code. Specifically, we apply three different training\nstrategies to incorporate four code execution aspects -- line executions, line\ncoverage, branch coverage, and variable states -- into CodeT5+, a well-known\nlanguage model for code. Our results indicate that execution-aware models\nprovide limited benefits compared to the standard CodeT5+ model in optimizing\ncode.", "published": "2025-03-11 09:46:07", "link": "http://arxiv.org/abs/2503.08228v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PF"], "primary_category": "cs.SE"}
{"title": "A Grey-box Text Attack Framework using Explainable AI", "abstract": "Explainable AI is a strong strategy implemented to understand complex\nblack-box model predictions in a human interpretable language. It provides the\nevidence required to execute the use of trustworthy and reliable AI systems. On\nthe other hand, however, it also opens the door to locating possible\nvulnerabilities in an AI model. Traditional adversarial text attack uses word\nsubstitution, data augmentation techniques and gradient-based attacks on\npowerful pre-trained Bidirectional Encoder Representations from Transformers\n(BERT) variants to generate adversarial sentences. These attacks are generally\nwhitebox in nature and not practical as they can be easily detected by humans\nE.g. Changing the word from \"Poor\" to \"Rich\". We proposed a simple yet\neffective Grey-box cum Black-box approach that does not require the knowledge\nof the model while using a set of surrogate Transformer/BERT models to perform\nthe attack using Explainable AI techniques. As Transformers are the current\nstate-of-the-art models for almost all Natural Language Processing (NLP) tasks,\nan attack generated from BERT1 is transferable to BERT2. This transferability\nis made possible due to the attention mechanism in the transformer that allows\nthe model to capture long-range dependencies in a sequence. Using the power of\nBERT generalisation via attention, we attempt to exploit how transformers learn\nby attacking a few surrogate transformer variants which are all based on a\ndifferent architecture. We demonstrate that this approach is highly effective\nto generate semantically good sentences by changing as little as one word that\nis not detectable by humans while still fooling other BERT models.", "published": "2025-03-11 09:44:17", "link": "http://arxiv.org/abs/2503.08226v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DeepRAG: Building a Custom Hindi Embedding Model for Retrieval Augmented Generation from Scratch", "abstract": "In this paper, I present our work on DeepRAG, a specialized embedding model\nwe built specifically for Hindi language in RAG systems. While LLMs have gotten\nreally good at generating text, their performance in retrieval tasks still\ndepends heavily on having quality embeddings - something that's been lacking\nfor Hindi despite being one of the world's most spoken languages. We tackled\nthis by creating embeddings from the ground up rather than just fine-tuning\nexisting models. Our process involved collecting diverse Hindi texts (over 2.7M\nsamples), training a custom SentencePiece tokenizer that actually understands\nHindi morphology, designing transformer architecture with Hindi-specific\nattention mechanisms, and optimizing with contrastive learning. Results were\nhonestly better than I expected - we saw a 23% improvement in retrieval\nprecision compared to the multilingual models everyone's been using. The paper\ndetails our methodology, which I think could help others working with\nlow-resource languages where the one-size-fits-all multilingual models fall\nshort. We've also integrated our embeddings with LangChain to build complete\nHindi RAG systems, which might be useful for practitioners. While there's still\ntons more to explore, I believe this work addresses a critical gap for Hindi\nNLP and demonstrates why language-specific approaches matter.", "published": "2025-03-11 09:27:56", "link": "http://arxiv.org/abs/2503.08213v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dialogue Injection Attack: Jailbreaking LLMs through Context Manipulation", "abstract": "Large language models (LLMs) have demonstrated significant utility in a wide\nrange of applications; however, their deployment is plagued by security\nvulnerabilities, notably jailbreak attacks. These attacks manipulate LLMs to\ngenerate harmful or unethical content by crafting adversarial prompts. While\nmuch of the current research on jailbreak attacks has focused on single-turn\ninteractions, it has largely overlooked the impact of historical dialogues on\nmodel behavior. In this paper, we introduce a novel jailbreak paradigm,\nDialogue Injection Attack (DIA), which leverages the dialogue history to\nenhance the success rates of such attacks. DIA operates in a black-box setting,\nrequiring only access to the chat API or knowledge of the LLM's chat template.\nWe propose two methods for constructing adversarial historical dialogues: one\nadapts gray-box prefilling attacks, and the other exploits deferred responses.\nOur experiments show that DIA achieves state-of-the-art attack success rates on\nrecent LLMs, including Llama-3.1 and GPT-4o. Additionally, we demonstrate that\nDIA can bypass 5 different defense mechanisms, highlighting its robustness and\neffectiveness.", "published": "2025-03-11 09:00:45", "link": "http://arxiv.org/abs/2503.08195v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automating Violence Detection and Categorization from Ancient Texts", "abstract": "Violence descriptions in literature offer valuable insights for a wide range\nof research in the humanities. For historians, depictions of violence are of\nspecial interest for analyzing the societal dynamics surrounding large wars and\nindividual conflicts of influential people. Harvesting data for violence\nresearch manually is laborious and time-consuming. This study is the first one\nto evaluate the effectiveness of large language models (LLMs) in identifying\nviolence in ancient texts and categorizing it across multiple dimensions. Our\nexperiments identify LLMs as a valuable tool to scale up the accurate analysis\nof historical texts and show the effect of fine-tuning and data augmentation,\nyielding an F1-score of up to 0.93 for violence detection and 0.86 for\nfine-grained violence categorization.", "published": "2025-03-11 08:55:52", "link": "http://arxiv.org/abs/2503.08192v1", "categories": ["cs.CL", "cs.DL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RigoChat 2: an adapted language model to Spanish using a bounded dataset and reduced hardware", "abstract": "Large Language Models (LLMs) have become a key element of modern artificial\nintelligence, demonstrating the ability to address a wide range of language\nprocessing tasks at unprecedented levels of accuracy without the need of\ncollecting problem-specific data. However, these versatile models face a\nsignificant challenge: both their training and inference processes require\nsubstantial computational resources, time, and memory. Consequently, optimizing\nthis kind of models to minimize these requirements is crucial. In this article,\nwe demonstrate that, with minimal resources and in a remarkably short time, it\nis possible to enhance a state-of-the-art model, specifically for a given\nlanguage task, without compromising its overall capabilities using a relatively\nsmall pretrained LLM as a basis. Specifically, we present our use case,\nRigoChat 2, illustrating how LLMs can be adapted to achieve superior results in\nSpanish-language tasks.", "published": "2025-03-11 08:53:53", "link": "http://arxiv.org/abs/2503.08188v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FASIONAD++ : Integrating High-Level Instruction and Information Bottleneck in FAt-Slow fusION Systems for Enhanced Safety in Autonomous Driving with Adaptive Feedback", "abstract": "Ensuring safe, comfortable, and efficient planning is crucial for autonomous\ndriving systems. While end-to-end models trained on large datasets perform well\nin standard driving scenarios, they struggle with complex low-frequency events.\nRecent Large Language Models (LLMs) and Vision Language Models (VLMs)\nadvancements offer enhanced reasoning but suffer from computational\ninefficiency. Inspired by the dual-process cognitive model \"Thinking, Fast and\nSlow\", we propose $\\textbf{FASIONAD}$ -- a novel dual-system framework that\nsynergizes a fast end-to-end planner with a VLM-based reasoning module. The\nfast system leverages end-to-end learning to achieve real-time trajectory\ngeneration in common scenarios, while the slow system activates through\nuncertainty estimation to perform contextual analysis and complex scenario\nresolution. Our architecture introduces three key innovations: (1) A dynamic\nswitching mechanism enabling slow system intervention based on real-time\nuncertainty assessment; (2) An information bottleneck with high-level plan\nfeedback that optimizes the slow system's guidance capability; (3) A\nbidirectional knowledge exchange where visual prompts enhance the slow system's\nreasoning while its feedback refines the fast planner's decision-making. To\nstrengthen VLM reasoning, we develop a question-answering mechanism coupled\nwith reward-instruct training strategy. In open-loop experiments, FASIONAD\nachieves a $6.7\\%$ reduction in average $L2$ trajectory error and $28.1\\%$\nlower collision rate.", "published": "2025-03-11 08:27:01", "link": "http://arxiv.org/abs/2503.08162v1", "categories": ["cs.RO", "cs.CL"], "primary_category": "cs.RO"}
{"title": "OASIS: Order-Augmented Strategy for Improved Code Search", "abstract": "Code embeddings capture the semantic representations of code and are crucial\nfor various code-related large language model (LLM) applications, such as code\nsearch. Previous training primarily relies on optimizing the InfoNCE loss by\ncomparing positive natural language (NL)-code pairs with in-batch negatives.\nHowever, due to the sparse nature of code contexts, training solely by\ncomparing the major differences between positive and negative pairs may fail to\ncapture deeper semantic nuances. To address this issue, we propose a novel\norder-augmented strategy for improved code search (OASIS). It leverages\norder-based similarity labels to train models to capture subtle differences in\nsimilarity among negative pairs. Extensive benchmark evaluations demonstrate\nthat our OASIS model significantly outperforms previous state-of-the-art models\nfocusing solely on major positive-negative differences. It underscores the\nvalue of exploiting subtle differences among negative pairs with order labels\nfor effective code embedding training.", "published": "2025-03-11 08:26:37", "link": "http://arxiv.org/abs/2503.08161v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Mimicking How Humans Interpret Out-of-Context Sentences Through Controlled Toxicity Decoding", "abstract": "Interpretations of a single sentence can vary, particularly when its context\nis lost. This paper aims to simulate how readers perceive content with varying\ntoxicity levels by generating diverse interpretations of out-of-context\nsentences. By modeling toxicity, we can anticipate misunderstandings and reveal\nhidden toxic meanings. Our proposed decoding strategy explicitly controls\ntoxicity in the set of generated interpretations by (i) aligning interpretation\ntoxicity with the input, (ii) relaxing toxicity constraints for more toxic\ninput sentences, and (iii) promoting diversity in toxicity levels within the\nset of generated interpretations. Experimental results show that our method\nimproves alignment with human-written interpretations in both syntax and\nsemantics while reducing model prediction uncertainty.", "published": "2025-03-11 08:16:31", "link": "http://arxiv.org/abs/2503.08159v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AI-native Memory 2.0: Second Me", "abstract": "Human interaction with the external world fundamentally involves the exchange\nof personal memory, whether with other individuals, websites, applications, or,\nin the future, AI agents. A significant portion of this interaction is\nredundant, requiring users to repeatedly provide the same information across\ndifferent contexts. Existing solutions, such as browser-stored credentials,\nautofill mechanisms, and unified authentication systems, have aimed to mitigate\nthis redundancy by serving as intermediaries that store and retrieve commonly\nused user data. The advent of large language models (LLMs) presents an\nopportunity to redefine memory management through an AI-native paradigm: SECOND\nME. SECOND ME acts as an intelligent, persistent memory offload system that\nretains, organizes, and dynamically utilizes user-specific knowledge. By\nserving as an intermediary in user interactions, it can autonomously generate\ncontext-aware responses, prefill required information, and facilitate seamless\ncommunication with external systems, significantly reducing cognitive load and\ninteraction friction. Unlike traditional memory storage solutions, SECOND ME\nextends beyond static data retention by leveraging LLM-based memory\nparameterization. This enables structured organization, contextual reasoning,\nand adaptive knowledge retrieval, facilitating a more systematic and\nintelligent approach to memory management. As AI-driven personal agents like\nSECOND ME become increasingly integrated into digital ecosystems, SECOND ME\nfurther represents a critical step toward augmenting human-world interaction\nwith persistent, contextually aware, and self-optimizing memory systems. We\nhave open-sourced the fully localizable deployment system at GitHub:\nhttps://github.com/Mindverse/Second-Me.", "published": "2025-03-11 07:05:52", "link": "http://arxiv.org/abs/2503.08102v2", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Advancing Sentiment Analysis: A Novel LSTM Framework with Multi-head Attention", "abstract": "This work proposes an LSTM-based sentiment classification model with\nmulti-head attention mechanism and TF-IDF optimization. Through the integration\nof TF-IDF feature extraction and multi-head attention, the model significantly\nimproves text sentiment analysis performance. Experimental results on public\ndata sets demonstrate that the new method achieves substantial improvements in\nthe most critical metrics like accuracy, recall, and F1-score compared to\nbaseline models. Specifically, the model achieves an accuracy of 80.28% on the\ntest set, which is improved by about 12% in comparison with standard LSTM\nmodels. Ablation experiments also support the necessity and necessity of all\nmodules, in which the impact of multi-head attention is greatest to performance\nimprovement. This research provides a proper approach to sentiment analysis,\nwhich can be utilized in public opinion monitoring, product recommendation,\netc.", "published": "2025-03-11 06:21:49", "link": "http://arxiv.org/abs/2503.08079v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MuCoS: Efficient Drug Target Discovery via Multi Context Aware Sampling in Knowledge Graphs", "abstract": "Accurate prediction of drug target interactions is critical for accelerating\ndrug discovery and elucidating complex biological mechanisms. In this work, we\nframe drug target prediction as a link prediction task on heterogeneous\nbiomedical knowledge graphs (KG) that integrate drugs, proteins, diseases,\npathways, and other relevant entities. Conventional KG embedding methods such\nas TransE and ComplEx SE are hindered by their reliance on computationally\nintensive negative sampling and their limited generalization to unseen drug\ntarget pairs. To address these challenges, we propose Multi Context Aware\nSampling (MuCoS), a novel framework that prioritizes high-density neighbours to\ncapture salient structural patterns and integrates these with contextual\nembeddings derived from BERT. By unifying structural and textual modalities and\nselectively sampling highly informative patterns, MuCoS circumvents the need\nfor negative sampling, significantly reducing computational overhead while\nenhancing predictive accuracy for novel drug target associations and drug\ntargets. Extensive experiments on the KEGG50k dataset demonstrate that MuCoS\noutperforms state-of-the-art baselines, achieving up to a 13\\% improvement in\nmean reciprocal rank (MRR) in predicting any relation in the dataset and a 6\\%\nimprovement in dedicated drug target relation prediction.", "published": "2025-03-11 06:08:42", "link": "http://arxiv.org/abs/2503.08075v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-aware Biases for Length Extrapolation", "abstract": "Transformers' ability to generalize to longer sequences than they have been\ntrained on, known as length extrapolation, degrades as sequence length\nincreases. Most of Relative Positional Encoding (RPE) methods address this\nproblem by either adding constant linear biases or learning general biases,\nlacking the ability to specialize for different sequences. In this work,\ninspired by ALiBi, we propose Context-aware Biases for Length Extrapolation\n(Cable), that learns token-specific biases for each head in decoder-based\ntransformers. Cable learns adaptive, context-aware biases, overcoming the\nlimitations of fixed patterns by adding dynamic biases specific to each token\nin the sequence. Results show that when tested on a sequence length of 1024, a\nGPT-3 Medium (334M parameters) with our positional encoding, trained on a\nsequence length of 512, achieves better perplexity (-0.65) than a similar\nnetwork with sinusoidal positional encoding trained on a sequence length of\n1024. This is achieved with 48% lower memory usage, and only 3.5% higher\ntraining time. Furthermore, our method notably improves the extrapolation\nability of existing RPE methods on the Edu-FineWeb10B and WikiText-103\ndatasets. Code is available at: https://github.com/axiomlab/Cable", "published": "2025-03-11 05:54:58", "link": "http://arxiv.org/abs/2503.08067v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Odysseus Navigates the Sirens' Song: Dynamic Focus Decoding for Factual and Diverse Open-Ended Text Generation", "abstract": "Large Language Models (LLMs) are increasingly required to generate text that\nis both factually accurate and diverse across various open-ended applications.\nHowever, current stochastic decoding methods struggle to balance such\nobjectives. We introduce Dynamic Focus Decoding (DFD), a novel plug-and-play\nstochastic approach that resolves this trade-off without requiring additional\ndata, knowledge, or models. DFD adaptively adjusts the decoding focus based on\ndistributional differences across layers, leveraging the modular and\nhierarchical nature of factual knowledge within LLMs. This dynamic adjustment\nimproves factuality in knowledge-intensive decoding steps and promotes\ndiversity in less knowledge-reliant steps. DFD can be easily integrated with\nexisting decoding methods, enhancing both factuality and diversity with minimal\ncomputational overhead. Extensive experiments across seven datasets demonstrate\nthat DFD significantly improves performance, providing a scalable and efficient\nsolution for open-ended text generation.", "published": "2025-03-11 05:27:28", "link": "http://arxiv.org/abs/2503.08057v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting Large Language Models for Parameter-Efficient Log Anomaly Detection", "abstract": "Log Anomaly Detection (LAD) seeks to identify atypical patterns in log data\nthat are crucial to assessing the security and condition of systems. Although\nLarge Language Models (LLMs) have shown tremendous success in various fields,\nthe use of LLMs in enabling the detection of log anomalies is largely\nunexplored. This work aims to fill this gap. Due to the prohibitive costs\ninvolved in fully fine-tuning LLMs, we explore the use of parameter-efficient\nfine-tuning techniques (PEFTs) for adapting LLMs to LAD. To have an in-depth\nexploration of the potential of LLM-driven LAD, we present a comprehensive\ninvestigation of leveraging two of the most popular PEFTs -- Low-Rank\nAdaptation (LoRA) and Representation Fine-tuning (ReFT) -- to tap into three\nprominent LLMs of varying size, including RoBERTa, GPT-2, and Llama-3, for\nparameter-efficient LAD. Comprehensive experiments on four public log datasets\nare performed to reveal important insights into effective LLM-driven LAD in\nseveral key perspectives, including the efficacy of these PEFT-based LLM-driven\nLAD methods, their stability, sample efficiency, robustness w.r.t. unstable\nlogs, and cross-dataset generalization. Code is available at\nhttps://github.com/mala-lab/LogADReft.", "published": "2025-03-11 05:00:19", "link": "http://arxiv.org/abs/2503.08045v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A General Framework to Evaluate Methods for Assessing Dimensions of Lexical Semantic Change Using LLM-Generated Synthetic Data", "abstract": "Lexical Semantic Change (LSC) offers insights into cultural and social\ndynamics. Yet, the validity of methods for measuring kinds of LSC has yet to be\nestablished due to the absence of historical benchmark datasets. To address\nthis gap, we develop a novel three-stage evaluation framework that involves: 1)\ncreating a scalable, domain-general methodology for generating synthetic\ndatasets that simulate theory-driven LSC across time, leveraging In-Context\nLearning and a lexical database; 2) using these datasets to evaluate the\neffectiveness of various methods; and 3) assessing their suitability for\nspecific dimensions and domains. We apply this framework to simulate changes\nacross key dimensions of LSC (SIB: Sentiment, Intensity, and Breadth) using\nexamples from psychology, and evaluate the sensitivity of selected methods to\ndetect these artificially induced changes. Our findings support the utility of\nthe synthetic data approach, validate the efficacy of tailored methods for\ndetecting synthetic changes in SIB, and reveal that a state-of-the-art LSC\nmodel faces challenges in detecting affective dimensions of LSC. This framework\nprovides a valuable tool for dimension- and domain-specific bench-marking and\nevaluation of LSC methods, with particular benefits for the social sciences.", "published": "2025-03-11 04:48:22", "link": "http://arxiv.org/abs/2503.08042v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Group Preference Alignment: Customized LLM Response Generation from In-Situ Conversations", "abstract": "LLMs often fail to meet the specialized needs of distinct user groups due to\ntheir one-size-fits-all training paradigm \\cite{lucy-etal-2024-one} and there\nis limited research on what personalization aspects each group expect. To\naddress these limitations, we propose a group-aware personalization framework,\nGroup Preference Alignment (GPA), that identifies context-specific variations\nin conversational preferences across user groups and then steers LLMs to\naddress those preferences. Our approach consists of two steps: (1) Group-Aware\nPreference Extraction, where maximally divergent user-group preferences are\nextracted from real-world conversation logs and distilled into interpretable\nrubrics, and (2) Tailored Response Generation, which leverages these rubrics\nthrough two methods: a) Context-Tuned Inference (GAP-CT), that dynamically\nadjusts responses via context-dependent prompt instructions, and b)\nRubric-Finetuning Inference (GPA-FT), which uses the rubrics to generate\ncontrastive synthetic data for personalization of group-specific models via\nalignment. Experiments demonstrate that our framework significantly improves\nalignment of the output with respect to user preferences and outperforms\nbaseline methods, while maintaining robust performance on standard benchmarks.", "published": "2025-03-11 04:32:54", "link": "http://arxiv.org/abs/2503.08035v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Search Effective Example Sequences for In-Context Learning", "abstract": "Large language models (LLMs) demonstrate impressive few-shot learning\ncapabilities, but their performance varies widely based on the sequence of\nin-context examples. Key factors influencing this include the sequence's\nlength, composition, and arrangement, as well as its relation to the specific\nquery. Existing methods often tackle these factors in isolation, overlooking\ntheir interdependencies. Moreover, the extensive search space for selecting\noptimal sequences complicates the development of a holistic approach. In this\nwork, we introduce Beam Search-based Example Sequence Constructor (BESC), a\nnovel method for learning to construct optimal example sequences. BESC\naddresses all key factors involved in sequence selection by considering them\njointly during inference, while incrementally building the sequence. This\ndesign enables the use of beam search to significantly reduce the complexity of\nthe search space. Experiments across various datasets and language models show\nnotable improvements in performance.", "published": "2025-03-11 04:24:59", "link": "http://arxiv.org/abs/2503.08030v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "In Prospect and Retrospect: Reflective Memory Management for Long-term Personalized Dialogue Agents", "abstract": "Large Language Models (LLMs) have made significant progress in open-ended\ndialogue, yet their inability to retain and retrieve relevant information from\nlong-term interactions limits their effectiveness in applications requiring\nsustained personalization. External memory mechanisms have been proposed to\naddress this limitation, enabling LLMs to maintain conversational continuity.\nHowever, existing approaches struggle with two key challenges. First, rigid\nmemory granularity fails to capture the natural semantic structure of\nconversations, leading to fragmented and incomplete representations. Second,\nfixed retrieval mechanisms cannot adapt to diverse dialogue contexts and user\ninteraction patterns. In this work, we propose Reflective Memory Management\n(RMM), a novel mechanism for long-term dialogue agents, integrating forward-\nand backward-looking reflections: (1) Prospective Reflection, which dynamically\nsummarizes interactions across granularities-utterances, turns, and\nsessions-into a personalized memory bank for effective future retrieval, and\n(2) Retrospective Reflection, which iteratively refines the retrieval in an\nonline reinforcement learning (RL) manner based on LLMs' cited evidence.\nExperiments show that RMM demonstrates consistent improvement across various\nmetrics and benchmarks. For example, RMM shows more than 10% accuracy\nimprovement over the baseline without memory management on the LongMemEval\ndataset.", "published": "2025-03-11 04:15:52", "link": "http://arxiv.org/abs/2503.08026v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SQLCritic: Correcting Text-to-SQL Generation via Clause-wise Critic", "abstract": "Recent advancements in Text-to-SQL systems have improved the conversion of\nnatural language queries into SQL, but challenges remain in ensuring accuracy\nand reliability. While self-correction techniques refine outputs, they often\nintroduce new errors. Existing methods focused on execution feedback mainly\naddress syntax issues, leaving semantic errors -- where the query's logic fails\nto align with the user's intent -- largely unaddressed. We propose a novel\napproach combining structured execution feedback with a trained critic agent\nthat provides detailed, interpretable critiques. This method effectively\nidentifies and corrects both syntactic and semantic errors, enhancing accuracy\nand interpretability. Experimental results show significant improvements on two\nmajor Text-to-SQL benchmarks, Spider and BIRD, demonstrating the effectiveness\nof our approach.", "published": "2025-03-11 02:52:39", "link": "http://arxiv.org/abs/2503.07996v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Enhancing Multilingual Language Models for Code-Switched Input Data", "abstract": "Code-switching, or alternating between languages within a single\nconversation, presents challenges for multilingual language models on NLP\ntasks. This research investigates if pre-training Multilingual BERT (mBERT) on\ncode-switched datasets improves the model's performance on critical NLP tasks\nsuch as part of speech tagging, sentiment analysis, named entity recognition,\nand language identification. We use a dataset of Spanglish tweets for\npre-training and evaluate the pre-trained model against a baseline model.\n  Our findings show that our pre-trained mBERT model outperforms or matches the\nbaseline model in the given tasks, with the most significant improvements seen\nfor parts of speech tagging. Additionally, our latent analysis uncovers more\nhomogenous English and Spanish embeddings for language identification tasks,\nproviding insights for future modeling work.\n  This research highlights potential for adapting multilingual LMs for\ncode-switched input data in order for advanced utility in globalized and\nmultilingual contexts. Future work includes extending experiments to other\nlanguage pairs, incorporating multiform data, and exploring methods for better\nunderstanding context-dependent code-switches.", "published": "2025-03-11 02:49:41", "link": "http://arxiv.org/abs/2503.07990v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation", "abstract": "Retrieval-Augmented Generation (RAG) has gained significant attention in\nrecent years for its potential to enhance natural language understanding and\ngeneration by combining large-scale retrieval systems with generative models.\nRAG leverages external knowledge sources, such as documents, databases, or\nstructured data, to improve model performance and generate more accurate and\ncontextually relevant outputs. This survey aims to provide a comprehensive\noverview of RAG by examining its fundamental components, including retrieval\nmechanisms, generation processes, and the integration between the two. We\ndiscuss the key characteristics of RAG, such as its ability to augment\ngenerative models with dynamic external knowledge, and the challenges\nassociated with aligning retrieved information with generative objectives. We\nalso present a taxonomy that categorizes RAG methods, ranging from basic\nretrieval-augmented approaches to more advanced models incorporating\nmulti-modal data and reasoning capabilities. Additionally, we review the\nevaluation benchmarks and datasets commonly used to assess RAG systems, along\nwith a detailed exploration of its applications in fields such as question\nanswering, summarization, and information retrieval. Finally, we highlight\nemerging research directions and opportunities for improving RAG systems, such\nas enhanced retrieval efficiency, model interpretability, and domain-specific\nadaptations. This paper concludes by outlining the prospects for RAG in\naddressing real-world challenges and its potential to drive further\nadvancements in natural language processing.", "published": "2025-03-11 01:59:35", "link": "http://arxiv.org/abs/2503.10677v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LabelCoRank: Revolutionizing Long Tail Multi-Label Classification with Co-Occurrence Reranking", "abstract": "Motivation: Despite recent advancements in semantic representation driven by\npre-trained and large-scale language models, addressing long tail challenges in\nmulti-label text classification remains a significant issue. Long tail\nchallenges have persistently posed difficulties in accurately classifying less\nfrequent labels. Current approaches often focus on improving text semantics\nwhile neglecting the crucial role of label relationships. Results: This paper\nintroduces LabelCoRank, a novel approach inspired by ranking principles.\nLabelCoRank leverages label co-occurrence relationships to refine initial label\nclassifications through a dual-stage reranking process. The first stage uses\ninitial classification results to form a preliminary ranking. In the second\nstage, a label co-occurrence matrix is utilized to rerank the preliminary\nresults, enhancing the accuracy and relevance of the final classifications. By\nintegrating the reranked label representations as additional text features,\nLabelCoRank effectively mitigates long tail issues in multi-labeltext\nclassification. Experimental evaluations on popular datasets including MAG-CS,\nPubMed, and AAPD demonstrate the effectiveness and robustness of LabelCoRank.", "published": "2025-03-11 01:52:39", "link": "http://arxiv.org/abs/2503.07968v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EFPC: Towards Efficient and Flexible Prompt Compression", "abstract": "The emergence of large language models (LLMs) like GPT-4 has revolutionized\nnatural language processing (NLP), enabling diverse, complex tasks. However,\nextensive token counts lead to high computational and financial burdens. To\naddress this, we propose Efficient and Flexible Prompt Compression (EFPC), a\nnovel method unifying task-aware and task-agnostic compression for a favorable\naccuracy-efficiency trade-off. EFPC uses GPT-4 to generate compressed prompts\nand integrates them with original prompts for training. During training and\ninference, we selectively prepend user instructions and compress prompts based\non predicted probabilities. EFPC is highly data-efficient, achieving\nsignificant performance with minimal data. Compared to the state-of-the-art\nmethod LLMLingua-2, EFPC achieves a 4.8% relative improvement in F1-score with\n1% additional data at a 4x compression rate, and an 11.4% gain with 10%\nadditional data on the LongBench single-doc QA benchmark. EFPC's unified\nframework supports broad applicability and enhances performance across various\nmodels, tasks, and domains, offering a practical advancement in NLP.", "published": "2025-03-11 01:34:03", "link": "http://arxiv.org/abs/2503.07956v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Sentiment Analysis through Multimodal Fusion: A BERT-DINOv2 Approach", "abstract": "Multimodal sentiment analysis enhances conventional sentiment analysis, which\ntraditionally relies solely on text, by incorporating information from\ndifferent modalities such as images, text, and audio. This paper proposes a\nnovel multimodal sentiment analysis architecture that integrates text and image\ndata to provide a more comprehensive understanding of sentiments. For text\nfeature extraction, we utilize BERT, a natural language processing model. For\nimage feature extraction, we employ DINOv2, a vision-transformer-based model.\nThe textual and visual latent features are integrated using proposed fusion\ntechniques, namely the Basic Fusion Model, Self Attention Fusion Model, and\nDual Attention Fusion Model. Experiments on three datasets, Memotion 7k\ndataset, MVSA single dataset, and MVSA multi dataset, demonstrate the viability\nand practicality of the proposed multimodal architecture.", "published": "2025-03-11 00:53:45", "link": "http://arxiv.org/abs/2503.07943v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Sorting permutations using a pop stack with a bypass", "abstract": "We introduce a new sorting device for permutations which makes use of a pop\nstack augmented with a bypass operation. This results in a sorting machine,\nwhich is more powerful than the usual Popstacksort algorithm and seems to have\nnever been investigated previously. In the present paper, we give a\ncharacterization of sortable permutations in terms of forbidden patterns and\nreinterpret the resulting enumerating sequence using a class of restricted\nMotzkin paths. Moreover, we describe an algorithm to compute the set of all\npreimages of a given permutation, thanks to which we characterize permutations\nhaving a small number of preimages. Finally, we provide a full description of\nthe preimages of principal classes of permutations, and we discuss the device\nconsisting of two pop stacks in parallel, again with a bypass operation.", "published": "2025-03-11 10:59:07", "link": "http://arxiv.org/abs/2503.08285v1", "categories": ["cs.DM", "math.CO", "05A05, 05A10, 05A15", "G.2.1"], "primary_category": "cs.DM"}
{"title": "Permanent of bipartite graphs in terms of determinants", "abstract": "Computing the permanent of a $(0,1)$-matrix is a well-known $\\#P$-complete\nproblem. In this paper, we present an expression for the permanent of a\nbipartite graph in terms of the determinant of the graph and its subgraphs,\nobtained by successively removing rows and columns corresponding to vertices\ninvolved in vertex-disjoint $4k$-cycles. Our formula establishes a general\nrelationship between the permanent and the determinant for any bipartite graph.\nSince computing the permanent of a biadjacency matrix is equivalent to counting\nthe number of its perfect matchings, this approach also provides a more\nefficient method for counting perfect matchings in certain types of bipartite\ngraphs.", "published": "2025-03-11 07:45:51", "link": "http://arxiv.org/abs/2503.08128v2", "categories": ["cs.DM", "15A15, 05C31", "G.2.2; G.2.1"], "primary_category": "cs.DM"}
{"title": "Secure domination in $P_5$-free graphs", "abstract": "A dominating set of a graph $G$ is a set $S \\subseteq V(G)$ such that every\nvertex in $V(G) \\setminus S$ has a neighbor in $S$, where two vertices are\nneighbors if they are adjacent. A secure dominating set of $G$ is a dominating\nset $S$ of $G$ with the additional property that for every vertex $v \\in V(G)\n\\setminus S$, there exists a neighbor $u$ of $v$ in $S$ such that $(S \\setminus\n\\{u\\}) \\cup \\{v\\}$ is a dominating set of $G$. The secure domination number of\n$G$, denoted by $\\gamma_s(G)$, is the minimum cardinality of a secure\ndominating set of $G$. We prove that if $G$ is a $P_5$-free graph, then\n$\\gamma_s(G) \\le \\frac{3}{2}\\alpha(G)$, where $\\alpha(G)$ denotes the\nindependence number of $G$. We further show that if $G$ is a connected $(P_5,\nH)$-free graph for some $H \\in \\{ P_3 \\cup P_1, K_2 \\cup 2K_1, ~\\text{paw},~\nC_4\\}$, then $\\gamma_s(G)\\le \\max\\{3,\\alpha(G)\\}$. We also show that if $G$ is\na $(P_3 \\cup P_2)$-free graph, then $\\gamma_s(G)\\le \\alpha(G)+1$.", "published": "2025-03-11 06:40:49", "link": "http://arxiv.org/abs/2503.08088v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "An upper bound on the size of a code with $s$ distances", "abstract": "Let $C$ be a binary code of length $n$ with distances $0<d_1<\\cdots<d_s\\le\nn$. In this note we prove a general upper bound on the size of $C$ without any\nrestriction on the distances $d_i$. The bound is asymptotically optimal.", "published": "2025-03-11 22:54:57", "link": "http://arxiv.org/abs/2503.08948v1", "categories": ["math.CO", "cs.IT", "math.IT", "05A05, 05A20, 94B25, 94B65"], "primary_category": "math.CO"}
{"title": "Beyond Diagonal RIS-Aided Wireless Communications Systems: State-of-the-Art and Future Research Directions", "abstract": "Integrating BD-RIS into wireless communications systems has attracted\nsignificant interest due to its transformative potential in enhancing system\nperformance. This survey provides a comprehensive analysis of BD-RIS\ntechnology, examining its modeling, structural characteristics, and network\nintegration while highlighting its advantages over traditional diagonal RIS.\nSpecifically, we review various BD-RIS modeling approaches, including multiport\nnetwork theory, graph theory, and matrix theory, and emphasize their\napplication in diverse wireless scenarios. The survey also covers BD-RIS's\nstructural diversity, including different scattering matrix types, transmission\nmodes, intercell architectures, and circuit topologies, showing their\nflexibility in improving network performance. We delve into the potential\napplications of BD-RIS, such as enhancing wireless coverage, improving PLS,\nenabling multi-cell interference cancellation, improving precise sensing and\nlocalization, and optimizing channel manipulation. Further, we explore BD-RIS\narchitectural development, providing insights into new configurations focusing\non channel estimation, optimization, performance analysis, and circuit\ncomplexity perspectives. Additionally, we investigate the integration of BD-RIS\nwith emerging wireless technologies, such as millimeter-wave and terahertz\ncommunications, integrated sensing and communications, mobile edge computing,\nand other cutting-edge technologies. These integrations are pivotal in\nadvancing the capabilities and efficiency of future wireless networks. Finally,\nthe survey identifies key challenges, including channel state information\nestimation, interference modeling, and phase-shift designs, and outlines future\nresearch directions. The survey aims to provide valuable insights into BD-RIS's\npotential in shaping the future of wireless communications systems.", "published": "2025-03-11 19:00:04", "link": "http://arxiv.org/abs/2503.08826v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Task-Oriented Co-Design of Communication, Computing, and Control for Edge-Enabled Industrial Cyber-Physical Systems", "abstract": "This paper proposes a task-oriented co-design framework that integrates\ncommunication, computing, and control to address the key challenges of\nbandwidth limitations, noise interference, and latency in mission-critical\nindustrial Cyber-Physical Systems (CPS). To improve communication efficiency\nand robustness, we design a task-oriented Joint Source-Channel Coding (JSCC)\nusing Information Bottleneck (IB) to enhance data transmission efficiency by\nprioritizing task-specific information. To mitigate the perceived End-to-End\n(E2E) delays, we develop a Delay-Aware Trajectory-Guided Control Prediction\n(DTCP) strategy that integrates trajectory planning with control prediction,\npredicting commands based on E2E delay. Moreover, the DTCP is co-designed with\ntask-oriented JSCC, focusing on transmitting task-specific information for\ntimely and reliable autonomous driving. Experimental results in the CARLA\nsimulator demonstrate that, under an E2E delay of 1 second (20 time slots), the\nproposed framework achieves a driving score of 48.12, which is 31.59 points\nhigher than using Better Portable Graphics (BPG) while reducing bandwidth usage\nby 99.19%.", "published": "2025-03-11 17:50:23", "link": "http://arxiv.org/abs/2503.08661v1", "categories": ["cs.IT", "cs.CV", "eess.IV", "math.IT"], "primary_category": "cs.IT"}
{"title": "Secret-Key Generation from Private Identifiers under Channel Uncertainty", "abstract": "This study investigates secret-key generation for device authentication using\nphysical identifiers, such as responses from physical unclonable functions\n(PUFs). The system includes two legitimate terminals (encoder and decoder) and\nan eavesdropper (Eve), each with access to different measurements of the\nidentifier. From the device identifier, the encoder generates a secret key,\nwhich is securely stored in a private database, along with helper data that is\nsaved in a public database accessible by the decoder for key reconstruction.\nEve, who also has access to the public database, may use both her own\nmeasurements and the helper data to attempt to estimate the secret key and\nidentifier. Our setup focuses on authentication scenarios where channel\nstatistics are uncertain, with the involved parties employing multiple antennas\nto enhance signal reception. Our contributions include deriving inner and outer\nbounds on the optimal trade-off among secret-key, storage, and privacy-leakage\nrates for general discrete sources, and showing that these bounds are tight for\nGaussian sources.", "published": "2025-03-11 17:20:48", "link": "http://arxiv.org/abs/2503.08632v1", "categories": ["cs.CR", "cs.IT", "math.IT"], "primary_category": "cs.CR"}
{"title": "Can Non-Signaling Assistance Increase the Degrees of Freedom of a Wireless Network?", "abstract": "An open question posed by Fawzi and Ferme [Transactions on Information Theory\n2024], asks whether non-signaling (NS) assistance can increase the capacity of\na broadcast channel (BC). We answer this question in the affirmative, by\nshowing that for a certain K-receiver BC setting, called Coordinated Multipoint\n(CoMP) that arises naturally in wireless networks, NS-assistance provides\nmultiplicative gains in capacity and degrees of freedom (DoF), even achieving\nK-fold improvements in some cases. Somewhat surprisingly, this is shown to be\ntrue even for 2-receiver broadcast channels that are semi-deterministic and/or\ndegraded. In a CoMP BC, B single-antenna transmitters, supported by a backhaul\nthat allows them to share data, act as one B-antenna transmitter, to send\nindependent messages to K receivers, each equipped with a single receive\nantenna. A fixed and globally known connectivity matrix M, specifies for each\ntransmit antenna, the subset of receivers that are connected to (have a\nnon-zero channel coefficient to) that antenna. Besides the connectivity, there\nis no channel state information at the transmitter. The DoF region is fully\ncharacterized for a class of connectivity patterns associated with tree graphs.\nSum-capacity with NS-assistance for arbitrary connectivity patterns is bounded\nbelow and above by the triangle number and the min-rank of the connectivity\nmatrix, respectively. While translations to Gaussian settings are demonstrated,\nmost of our results are presented under noise-free, finite-field (Fq) models.\nConverse proofs for classical DoF adapt the Aligned Images bounds to the finite\nfield model. Converse bounds for NS-assisted capacity extend the same-marginals\nproperty to the BC with NS-assistance available to all parties. Even stronger\n(unbounded) gains are established for certain 'communication with\nside-information' settings, such as the fading dirty paper channel.", "published": "2025-03-11 16:33:26", "link": "http://arxiv.org/abs/2503.08597v2", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "LoS Blockage in Pinching-Antenna Systems: Curse or Blessing?", "abstract": "This letter is to investigate the impact of line-of-sight (LoS) blockage on\npinching-antenna systems. Analytical results are developed for both single-user\nand multi-user cases to reveal that the presence of LoS blockage is beneficial\nfor increasing the performance gain of pinching antennas over conventional\nantennas. This letter also reveals that LoS blockage is particularly useful in\nmulti-user cases, where co-channel interference can be effectively suppressed\nby LoS blockage.", "published": "2025-03-11 15:45:31", "link": "http://arxiv.org/abs/2503.08554v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous Driving", "abstract": "Vehicle-to-vehicle (V2V) cooperative autonomous driving holds great promise\nfor improving safety by addressing the perception and prediction uncertainties\ninherent in single-agent systems. However, traditional cooperative methods are\nconstrained by rigid collaboration protocols and limited generalization to\nunseen interactive scenarios. While LLM-based approaches offer generalized\nreasoning capabilities, their challenges in spatial planning and unstable\ninference latency hinder their direct application in cooperative driving. To\naddress these limitations, we propose CoLMDriver, the first full-pipeline\nLLM-based cooperative driving system, enabling effective language-based\nnegotiation and real-time driving control. CoLMDriver features a parallel\ndriving pipeline with two key components: (i) an LLM-based negotiation module\nunder an actor-critic paradigm, which continuously refines cooperation policies\nthrough feedback from previous decisions of all vehicles; and (ii) an\nintention-guided waypoint generator, which translates negotiation outcomes into\nexecutable waypoints. Additionally, we introduce InterDrive, a CARLA-based\nsimulation benchmark comprising 10 challenging interactive driving scenarios\nfor evaluating V2V cooperation. Experimental results demonstrate that\nCoLMDriver significantly outperforms existing approaches, achieving an 11%\nhigher success rate across diverse highly interactive V2V driving scenarios.\nCode will be released on https://github.com/cxliu0314/CoLMDriver.", "published": "2025-03-11 17:58:42", "link": "http://arxiv.org/abs/2503.08683v1", "categories": ["cs.CV", "cs.AI", "cs.MA"], "primary_category": "cs.CV"}
{"title": "InfluenceNet: AI Models for Banzhaf and Shapley Value Prediction", "abstract": "Power indices are essential in assessing the contribution and influence of\nindividual agents in multi-agent systems, providing crucial insights into\ncollaborative dynamics and decision-making processes. While invaluable,\ntraditional computational methods for exact or estimated power indices values\nrequire significant time and computational constraints, especially for large\n$(n\\ge10)$ coalitions. These constraints have historically limited researchers'\nability to analyse complex multi-agent interactions comprehensively. To address\nthis limitation, we introduce a novel Neural Networks-based approach that\nefficiently estimates power indices for voting games, demonstrating comparable\nand often superiour performance to existing tools in terms of both speed and\naccuracy. This method not only addresses existing computational bottlenecks,\nbut also enables rapid analysis of large coalitions, opening new avenues for\nmulti-agent system research by overcoming previous computational limitations\nand providing researchers with a more accessible, scalable analytical tool.This\nincreased efficiency will allow for the analysis of more complex and realistic\nmulti-agent scenarios.", "published": "2025-03-11 12:40:42", "link": "http://arxiv.org/abs/2503.08381v1", "categories": ["cs.MA", "cs.AI", "I.2; F.2.1"], "primary_category": "cs.MA"}
{"title": "Cooperative Bearing-Only Target Pursuit via Multiagent Reinforcement Learning: Design and Experiment", "abstract": "This paper addresses the multi-robot pursuit problem for an unknown target,\nencompassing both target state estimation and pursuit control. First, in state\nestimation, we focus on using only bearing information, as it is readily\navailable from vision sensors and effective for small, distant targets.\nChallenges such as instability due to the nonlinearity of bearing measurements\nand singularities in the two-angle representation are addressed through a\nproposed uniform bearing-only information filter. This filter integrates\nmultiple 3D bearing measurements, provides a concise formulation, and enhances\nstability and resilience to target loss caused by limited field of view (FoV).\nSecond, in target pursuit control within complex environments, where challenges\nsuch as heterogeneity and limited FoV arise, conventional methods like\ndifferential games or Voronoi partitioning often prove inadequate. To address\nthese limitations, we propose a novel multiagent reinforcement learning (MARL)\nframework, enabling multiple heterogeneous vehicles to search, localize, and\nfollow a target while effectively handling those challenges. Third, to bridge\nthe sim-to-real gap, we propose two key techniques: incorporating adjustable\nlow-level control gains in training to replicate the dynamics of real-world\nautonomous ground vehicles (AGVs), and proposing spectral-normalized RL\nalgorithms to enhance policy smoothness and robustness. Finally, we demonstrate\nthe successful zero-shot transfer of the MARL controllers to AGVs, validating\nthe effectiveness and practical feasibility of our approach. The accompanying\nvideo is available at https://youtu.be/HO7FJyZiJ3E.", "published": "2025-03-11 08:21:35", "link": "http://arxiv.org/abs/2503.08740v1", "categories": ["cs.MA", "cs.RO", "cs.SY", "eess.SY"], "primary_category": "cs.MA"}
{"title": "Enhancing Traffic Signal Control through Model-based Reinforcement Learning and Policy Reuse", "abstract": "Multi-agent reinforcement learning (MARL) has shown significant potential in\ntraffic signal control (TSC). However, current MARL-based methods often suffer\nfrom insufficient generalization due to the fixed traffic patterns and road\nnetwork conditions used during training. This limitation results in poor\nadaptability to new traffic scenarios, leading to high retraining costs and\ncomplex deployment. To address this challenge, we propose two algorithms:\nPLight and PRLight. PLight employs a model-based reinforcement learning\napproach, pretraining control policies and environment models using predefined\nsource-domain traffic scenarios. The environment model predicts the state\ntransitions, which facilitates the comparison of environmental features.\nPRLight further enhances adaptability by adaptively selecting pre-trained\nPLight agents based on the similarity between the source and target domains to\naccelerate the learning process in the target domain. We evaluated the\nalgorithms through two transfer settings: (1) adaptability to different traffic\nscenarios within the same road network, and (2) generalization across different\nroad networks. The results show that PRLight significantly reduces the\nadaptation time compared to learning from scratch in new TSC scenarios,\nachieving optimal performance using similarities between available and target\nscenarios.", "published": "2025-03-11 01:21:13", "link": "http://arxiv.org/abs/2503.08728v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "The Algorithmic State Architecture (ASA): An Integrated Framework for AI-Enabled Government", "abstract": "As artificial intelligence transforms public sector operations, governments\nstruggle to integrate technological innovations into coherent systems for\neffective service delivery. This paper introduces the Algorithmic State\nArchitecture (ASA), a novel four-layer framework conceptualising how Digital\nPublic Infrastructure, Data-for-Policy, Algorithmic Government/Governance, and\nGovTech interact as an integrated system in AI-enabled states. Unlike\napproaches that treat these as parallel developments, ASA positions them as\ninterdependent layers with specific enabling relationships and feedback\nmechanisms. Through comparative analysis of implementations in Estonia,\nSingapore, India, and the UK, we demonstrate how foundational digital\ninfrastructure enables systematic data collection, which powers algorithmic\ndecision-making processes, ultimately manifesting in user-facing services. Our\nanalysis reveals that successful implementations require balanced development\nacross all layers, with particular attention to integration mechanisms between\nthem. The framework contributes to both theory and practice by bridging\npreviously disconnected domains of digital government research, identifying\ncritical dependencies that influence implementation success, and providing a\nstructured approach for analysing the maturity and development pathways of\nAI-enabled government systems.", "published": "2025-03-11 00:20:56", "link": "http://arxiv.org/abs/2503.08725v2", "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.MA", "cs.SY", "eess.SY"], "primary_category": "cs.CY"}
{"title": "Randomization in Optimal Execution Games", "abstract": "We study optimal execution in markets with transient price impact in a\ncompetitive setting with $N$ traders. Motivated by prior negative results on\nthe existence of pure Nash equilibria, we consider randomized strategies for\nthe traders and whether allowing such strategies can restore the existence of\nequilibria. We show that given a randomized strategy, there is a non-randomized\nstrategy with strictly lower expected execution cost, and moreover this\nde-randomization can be achieved by a simple averaging procedure. As a\nconsequence, Nash equilibria cannot contain randomized strategies, and\nnon-existence of pure equilibria implies non-existence of randomized\nequilibria. Separately, we also establish uniqueness of equilibria. Both\nresults hold in a general transaction cost model given by a strictly positive\ndefinite impact decay kernel and a convex trading cost.", "published": "2025-03-11 19:15:19", "link": "http://arxiv.org/abs/2503.08833v1", "categories": ["q-fin.TR", "q-fin.MF", "91A06, 91A15, 91G10"], "primary_category": "q-fin.TR"}
{"title": "Modeling Stock Return Distributions and Pricing Options", "abstract": "This paper provides evidence that stock returns, after truncation, might be\nmodeled by a special type of continuous mixtures or normals, so-called\n$q$-Gaussians. Negative binomial distributions might model the counts for\nextreme returns. A generalized jump-diffusion model is proposed, and an\nexplicit option pricing formula is obtained.", "published": "2025-03-11 17:52:35", "link": "http://arxiv.org/abs/2503.08666v1", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "Existence of Optimal Contracts for Principal-Agent Problem with Drift Control and Quadratic Effort Cost", "abstract": "The existence of optimal contracts of the principal-agent problem is a\nlong-standing problem. According to the general framework in Cvitani\\'c et al.\n[2], this existence can be derived from the existence of a classical solution\nto a degenerated fully nonlinear parabolic partial differential equation\nproblem. In this work we consider the simple case with drift control and\nquadratic cost function, then prove the existence of classical solution to that\nPDE.", "published": "2025-03-11 14:55:44", "link": "http://arxiv.org/abs/2503.08503v2", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "Liquidity Competition Between Brokers and an Informed Trader", "abstract": "We study a multi-agent setting in which brokers transact with an informed\ntrader. Through a sequential Stackelberg-type game, brokers manage trading\ncosts and adverse selection with an informed trader. In particular, supplying\nliquidity to the informed traders allows the brokers to speculate based on the\nflow information. They simultaneously attempt to minimize inventory risk and\ntrading costs with the lit market based on the informed order flow, also known\nas the internalization-externalization strategy. We solve in closed form for\nthe trading strategy that the informed trader uses with each broker and propose\na system of equations which classify the equilibrium strategies of the brokers.\nBy solving these equations numerically we may study the resulting strategies in\nequilibrium. Finally, we formulate a competitive game between brokers in order\nto determine the liquidity prices subject to precommitment supplied to the\ninformed trader and provide a numerical example in which the resulting\nequilibrium is not Pareto efficient.", "published": "2025-03-11 10:59:49", "link": "http://arxiv.org/abs/2503.08287v1", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "Learning Control of Neural Sound Effects Synthesis from Physically Inspired Models", "abstract": "Sound effects model design commonly uses digital signal processing techniques\nwith full control ability, but it is difficult to achieve realism within a\nlimited number of parameters. Recently, neural sound effects synthesis methods\nhave emerged as a promising approach for generating high-quality and realistic\nsounds, but the process of synthesizing the desired sound poses difficulties in\nterms of control. This paper presents a real-time neural synthesis model guided\nby a physically inspired model, enabling the generation of high-quality sounds\nwhile inheriting the control interface of the physically inspired model. We\nshowcase the superior performance of our model in terms of sound quality and\ncontrol.", "published": "2025-03-11 18:35:38", "link": "http://arxiv.org/abs/2503.08806v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Contextual Speech Extraction: Leveraging Textual History as an Implicit Cue for Target Speech Extraction", "abstract": "In this paper, we investigate a novel approach for Target Speech Extraction\n(TSE), which relies solely on textual context to extract the target speech. We\nrefer to this task as Contextual Speech Extraction (CSE). Unlike traditional\nTSE methods that rely on pre-recorded enrollment utterances, video of the\ntarget speaker's face, spatial information, or other explicit cues to identify\nthe target stream, our proposed method requires only a few turns of previous\ndialogue (or monologue) history. This approach is naturally feasible in mobile\nmessaging environments where voice recordings are typically preceded by textual\ndialogue that can be leveraged implicitly. We present three CSE models and\nanalyze their performances on three datasets. Through our experiments, we\ndemonstrate that even when the model relies purely on dialogue history, it can\nachieve over 90 % accuracy in identifying the correct target stream with only\ntwo previous dialogue turns. Furthermore, we show that by leveraging both\ntextual context and enrollment utterances as cues during training, we further\nenhance our model's flexibility and effectiveness, allowing us to use either\ncue during inference, or combine both for improved performance. Samples and\ncode available on https://miraodasilva.github.io/cse-project-page .", "published": "2025-03-11 18:26:10", "link": "http://arxiv.org/abs/2503.08798v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Lend a Hand: Semi Training-Free Cued Speech Recognition via MLLM-Driven Hand Modeling for Barrier-free Communication", "abstract": "Cued Speech (CS) is an innovative visual communication system that integrates\nlip-reading with hand coding, designed to enhance effective communication for\nindividuals with hearing impairments. Automatic CS Recognition (ACSR) refers to\nthe AI-driven process of automatically recognizing hand gestures and lip\nmovements in CS, converting them into text. However, previous work often relies\non complex fusion modules and training techniques. Additionally, due to the\nlimited amount of data in CS, the extraction of hand features, as well as\nrecognition modeling, has consistently been subpar, significantly limiting the\neffectiveness of ACSR. To address this issue, we have innovatively explored the\ncapabilities of Multimodal large language models (MLLMs) in recognizing hand\nshapes and positions in CS. More precisely, we propose a new Semi Training-Free\nparadigm for ACSR, named STF-ACSR. This approach leverages zero-shot\nrecognition of hand movements through the Chinese CS Prompt Module (CCSPM),\nwhich equipped a training-free keyframe filtering and customized prompt\nengineering based on MLLM. It then integrates the recognition results into the\nlip-reading model using a Minimalist Fusion Module (MFM), effectively achieving\nsuperior recognition results. Furthermore, specifically for this study, we have\nsupplemented the existing dataset of 6 normal hearing CS cuers by recording\nadditional data from 8 cuers with hearing impairments, resulting in a new mixed\ndataset. Extensive experiments have demonstrated that STF-ACSR significantly\noutperforms previous methods on both normal and hearing-impaired data.\nImplementation and checkpoints are available at\nhttps://github.com/DennisHgj/STF_ACSR.", "published": "2025-03-11 18:18:03", "link": "http://arxiv.org/abs/2503.21785v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "YuE: Scaling Open Foundation Models for Long-Form Music Generation", "abstract": "We tackle the task of long-form music generation--particularly the\nchallenging \\textbf{lyrics-to-song} problem--by introducing YuE, a family of\nopen foundation models based on the LLaMA2 architecture. Specifically, YuE\nscales to trillions of tokens and generates up to five minutes of music while\nmaintaining lyrical alignment, coherent musical structure, and engaging vocal\nmelodies with appropriate accompaniment. It achieves this through (1)\ntrack-decoupled next-token prediction to overcome dense mixture signals, (2)\nstructural progressive conditioning for long-context lyrical alignment, and (3)\na multitask, multiphase pre-training recipe to converge and generalize. In\naddition, we redesign the in-context learning technique for music generation,\nenabling versatile style transfer (e.g., converting Japanese city pop into an\nEnglish rap while preserving the original accompaniment) and bidirectional\ngeneration. Through extensive evaluation, we demonstrate that YuE matches or\neven surpasses some of the proprietary systems in musicality and vocal agility.\nIn addition, fine-tuning YuE enables additional controls and enhanced support\nfor tail languages. Furthermore, beyond generation, we show that YuE's learned\nrepresentations can perform well on music understanding tasks, where the\nresults of YuE match or exceed state-of-the-art methods on the MARBLE\nbenchmark. Keywords: lyrics2song, song generation, long-form, foundation model,\nmusic generation", "published": "2025-03-11 17:26:50", "link": "http://arxiv.org/abs/2503.08638v1", "categories": ["eess.AS", "cs.AI", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Mellow: a small audio language model for reasoning", "abstract": "Multimodal Audio-Language Models (ALMs) can understand and reason over both\naudio and text. Typically, reasoning performance correlates with model size,\nwith the best results achieved by models exceeding 8 billion parameters.\nHowever, no prior work has explored enabling small audio-language models to\nperform reasoning tasks, despite the potential applications for edge devices.\nTo address this gap, we introduce Mellow, a small Audio-Language Model\nspecifically designed for reasoning. Mellow achieves state-of-the-art\nperformance among existing small audio-language models and surpasses several\nlarger models in reasoning capabilities. For instance, Mellow scores 52.11 on\nMMAU, comparable to SoTA Qwen2 Audio (which scores 52.5) while using 50 times\nfewer parameters and being trained on 60 times less data (audio hrs). To train\nMellow, we introduce ReasonAQA, a dataset designed to enhance audio-grounded\nreasoning in models. It consists of a mixture of existing datasets (30% of the\ndata) and synthetically generated data (70%). The synthetic dataset is derived\nfrom audio captioning datasets, where Large Language Models (LLMs) generate\ndetailed and multiple-choice questions focusing on audio events, objects,\nacoustic scenes, signal properties, semantics, and listener emotions. To\nevaluate Mellow's reasoning ability, we benchmark it on a diverse set of tasks,\nassessing on both in-distribution and out-of-distribution data, including audio\nunderstanding, deductive reasoning, and comparative reasoning. Finally, we\nconduct extensive ablation studies to explore the impact of projection layer\nchoices, synthetic data generation methods, and language model pretraining on\nreasoning performance. Our training dataset, findings, and baseline pave the\nway for developing small ALMs capable of reasoning.", "published": "2025-03-11 15:29:00", "link": "http://arxiv.org/abs/2503.08540v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FilmComposer: LLM-Driven Music Production for Silent Film Clips", "abstract": "In this work, we implement music production for silent film clips using\nLLM-driven method. Given the strong professional demands of film music\nproduction, we propose the FilmComposer, simulating the actual workflows of\nprofessional musicians. FilmComposer is the first to combine large generative\nmodels with a multi-agent approach, leveraging the advantages of both waveform\nmusic and symbolic music generation. Additionally, FilmComposer is the first to\nfocus on the three core elements of music production for film-audio quality,\nmusicality, and musical development-and introduces various controls, such as\nrhythm, semantics, and visuals, to enhance these key aspects. Specifically,\nFilmComposer consists of the visual processing module, rhythm-controllable\nMusicGen, and multi-agent assessment, arrangement and mix. In addition, our\nframework can seamlessly integrate into the actual music production pipeline\nand allows user intervention in every step, providing strong interactivity and\na high degree of creative freedom. Furthermore, we propose MusicPro-7k which\nincludes 7,418 film clips, music, description, rhythm spots and main melody,\nconsidering the lack of a professional and high-quality film music dataset.\nFinally, both the standard metrics and the new specialized metrics we propose\ndemonstrate that the music generated by our model achieves state-of-the-art\nperformance in terms of quality, consistency with video, diversity, musicality,\nand musical development. Project page:\nhttps://apple-jun.github.io/FilmComposer.github.io/", "published": "2025-03-11 08:05:11", "link": "http://arxiv.org/abs/2503.08147v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Boundary Regression for Leitmotif Detection in Music Audio", "abstract": "Leitmotifs are musical phrases that are reprised in various forms throughout\na piece. Due to diverse variations and instrumentation, detecting the\noccurrence of leitmotifs from audio recordings is a highly challenging task.\nLeitmotif detection may be handled as a subcategory of audio event detection,\nwhere leitmotif activity is predicted at the frame level. However, as\nleitmotifs embody distinct, coherent musical structures, a more holistic\napproach akin to bounding box regression in visual object detection can be\nhelpful. This method captures the entirety of a motif rather than fragmenting\nit into individual frames, thereby preserving its musical integrity and\nproducing more useful predictions. We present our experimental results on\ntackling leitmotif detection as a boundary regression task.", "published": "2025-03-11 02:21:58", "link": "http://arxiv.org/abs/2503.07977v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "I.2.0, I.2.1"], "primary_category": "cs.SD"}
