{"title": "No Tick-Size Too Small: A General Method for Modelling Small Tick Limit Order Books", "abstract": "Tick sizes not only influence the granularity of the price formation process\nbut also affect market agents' behavior. We investigate the disparity in the\nmicrostructural properties of the Limit Order Book (LOB) across different\nrelative tick sizes. A key contribution of this study is the identification of\nseveral stylized facts, which are used to differentiate between large, medium,\nand small tick stocks, along with clear metrics for their measurement. We\nprovide cross-asset visualizations to illustrate how these attributes vary with\nrelative tick size. Further, we propose a Hawkes Process model that accounts\nfor sparsity, multi-tick level price moves, and the shape of the book in\nsmall-tick stocks. Through simulation studies, we demonstrate the universality\nof the model and identify key variables that determine whether a simulated LOB\nresembles a large-tick or small-tick stock. Our tests show that stylized facts\nlike sparsity, shape, and relative returns distribution can be smoothly\ntransitioned from a large-tick to a small-tick asset using our model. We test\nthis model's assumptions, showcase its challenges and propose questions for\nfurther directions in this area of research.", "published": "2024-10-11 12:02:21", "link": "http://arxiv.org/abs/2410.08744v2", "categories": ["q-fin.TR", "cs.CE", "q-fin.CP"], "primary_category": "q-fin.TR"}
{"title": "Scalable Signature-Based Distribution Regression via Reference Sets", "abstract": "Distribution Regression (DR) on stochastic processes describes the learning\ntask of regression on collections of time series. Path signatures, a technique\nprevalent in stochastic analysis, have been used to solve the DR problem.\nRecent works have demonstrated the ability of such solutions to leverage the\ninformation encoded in paths via signature-based features. However, current\nstate of the art DR solutions are memory intensive and incur a high computation\ncost. This leads to a trade-off between path length and the number of paths\nconsidered. This computational bottleneck limits the application to small\nsample sizes which consequently introduces estimation uncertainty. In this\npaper, we present a methodology for addressing the above issues; resolving\nestimation uncertainties whilst also proposing a pipeline that enables us to\nuse DR for a wide variety of learning tasks. Integral to our approach is our\nnovel distance approximator. This allows us to seamlessly apply our methodology\nacross different application domains, sampling rates, and stochastic process\ndimensions. We show that our model performs well in applications related to\nestimation theory, quantitative finance, and physical sciences. We demonstrate\nthat our model generalises well, not only to unseen data within a given\ndistribution, but also under unseen regimes (unseen classes of stochastic\nmodels).", "published": "2024-10-11 18:58:28", "link": "http://arxiv.org/abs/2410.09196v1", "categories": ["cs.LG", "q-fin.MF", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Term structure shapes and their consistent dynamics in the Svensson family", "abstract": "We examine the shapes attainable by the forward- and yield-curve in the\nwidely-used Svensson family, including the Nelson-Siegel and Bliss subfamilies.\nWe provide a complete classification of all attainable shapes and partition the\nparameter space of each family according to these shapes. Building upon these\nresults, we then examine the consistent dynamic evolution of the Svensson\nfamily under absence of arbitrage. Our analysis shows that consistent dynamics\nfurther restrict the set of attainable shapes, and we demonstrate that certain\ncomplex shapes can no longer appear after a deterministic time horizon.\nMoreover a single shape (either inverse of normal curves) must dominate in the\nlong-run.", "published": "2024-10-11 13:46:52", "link": "http://arxiv.org/abs/2410.08808v1", "categories": ["q-fin.MF", "econ.TH", "math.DS", "91G30"], "primary_category": "q-fin.MF"}
{"title": "Cross-Currency Basis Swaps Referencing Backward-Looking Rates", "abstract": "The financial industry has undergone a significant transition from the London\nInterbank Offered Rate (LIBOR) to Risk Free Rates (RFR) such as, e.g., the\nSecured Overnight Financing Rate (SOFR) in the U.S. and the AUD Overnight Index\nAverage (AONIA) in Australia, as the primary benchmark rate for borrowing\ncosts. The paper examines the pricing and hedging method for SOFR-related\nfinancial products in a cross-currency context with the special emphasis on the\nCompound SOFR vs Average AONIA cross-currency basis swaps. While the SOFR and\nAONIA serve as a particular case of a cross-currency basis swap (CCBS), the\napproach developed is able to handle backward-looking term rates for any two\ncurrencies. We give explicit pricing and hedging results for collateralized\ncross-currency basis swaps using interest rate and currency futures contracts\nas hedging tools within an arbitrage-free multi-curve setting.", "published": "2024-10-11 03:09:39", "link": "http://arxiv.org/abs/2410.08477v1", "categories": ["q-fin.MF", "0H10, 60H30, 91G30, 91G40"], "primary_category": "q-fin.MF"}
{"title": "Generation with Dynamic Vocabulary", "abstract": "We introduce a new dynamic vocabulary for language models. It can involve\narbitrary text spans during generation. These text spans act as basic\ngeneration bricks, akin to tokens in the traditional static vocabularies. We\nshow that, the ability to generate multi-tokens atomically improve both\ngeneration quality and efficiency (compared to the standard language model, the\nMAUVE metric is increased by 25%, the latency is decreased by 20%). The dynamic\nvocabulary can be deployed in a plug-and-play way, thus is attractive for\nvarious downstream applications. For example, we demonstrate that dynamic\nvocabulary can be applied to different domains in a training-free manner. It\nalso helps to generate reliable citations in question answering tasks\n(substantially enhancing citation results without compromising answer\naccuracy).", "published": "2024-10-11 03:19:15", "link": "http://arxiv.org/abs/2410.08481v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parameter-Efficient Fine-Tuning of Large Language Models using Semantic\n  Knowledge Tuning", "abstract": "Large Language Models (LLMs) are gaining significant popularity in recent\nyears for specialized tasks using prompts due to their low computational cost.\nStandard methods like prefix tuning utilize special, modifiable tokens that\nlack semantic meaning and require extensive training for best performance,\noften falling short. In this context, we propose a novel method called Semantic\nKnowledge Tuning (SK-Tuning) for prompt and prefix tuning that employs\nmeaningful words instead of random tokens. This method involves using a fixed\nLLM to understand and process the semantic content of the prompt through\nzero-shot capabilities. Following this, it integrates the processed prompt with\nthe input text to improve the model's performance on particular tasks. Our\nexperimental results show that SK-Tuning exhibits faster training times, fewer\nparameters, and superior performance on tasks such as text classification and\nunderstanding compared to other tuning methods. This approach offers a\npromising method for optimizing the efficiency and effectiveness of LLMs in\nprocessing language tasks.", "published": "2024-10-11 07:55:09", "link": "http://arxiv.org/abs/2410.08598v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "StraGo: Harnessing Strategic Guidance for Prompt Optimization", "abstract": "Prompt engineering is pivotal for harnessing the capabilities of large\nlanguage models (LLMs) across diverse applications. While existing prompt\noptimization methods improve prompt effectiveness, they often lead to prompt\ndrifting, where newly generated prompts can adversely impact previously\nsuccessful cases while addressing failures. Furthermore, these methods tend to\nrely heavily on LLMs' intrinsic capabilities for prompt optimization tasks. In\nthis paper, we introduce StraGo (Strategic-Guided Optimization), a novel\napproach designed to mitigate prompt drifting by leveraging insights from both\nsuccessful and failed cases to identify critical factors for achieving\noptimization objectives. StraGo employs a how-to-do methodology, integrating\nin-context learning to formulate specific, actionable strategies that provide\ndetailed, step-by-step guidance for prompt optimization. Extensive experiments\nconducted across a range of tasks, including reasoning, natural language\nunderstanding, domain-specific knowledge, and industrial applications,\ndemonstrate StraGo's superior performance. It establishes a new\nstate-of-the-art in prompt optimization, showcasing its ability to deliver\nstable and effective prompt improvements.", "published": "2024-10-11 07:55:42", "link": "http://arxiv.org/abs/2410.08601v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Guidelines for Fine-grained Sentence-level Arabic Readability Annotation", "abstract": "This paper presents the foundational framework and initial findings of the\nBalanced Arabic Readability Evaluation Corpus (BAREC) project, designed to\naddress the need for comprehensive Arabic language resources aligned with\ndiverse readability levels. Inspired by the Taha/Arabi21 readability reference,\nBAREC aims to provide a standardized reference for assessing sentence-level\nArabic text readability across 19 distinct levels, ranging in targets from\nkindergarten to postgraduate comprehension. Our ultimate goal with BAREC is to\ncreate a comprehensive and balanced corpus that represents a wide range of\ngenres, topics, and regional variations through a multifaceted approach\ncombining manual annotation with AI-driven tools. This paper focuses on our\nmeticulous annotation guidelines, demonstrated through the analysis of 10,631\nsentences/phrases (113,651 words). The average pairwise inter-annotator\nagreement, measured by Quadratic Weighted Kappa, is 79.9%, reflecting a high\nlevel of substantial agreement. We also report competitive results for\nbenchmarking automatic readability assessment. We will make the BAREC corpus\nand guidelines openly accessible to support Arabic language research and\neducation.", "published": "2024-10-11 09:59:46", "link": "http://arxiv.org/abs/2410.08674v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AMPO: Automatic Multi-Branched Prompt Optimization", "abstract": "Prompt engineering is very important to enhance the performance of large\nlanguage models (LLMs). When dealing with complex issues, prompt engineers tend\nto distill multiple patterns from examples and inject relevant solutions to\noptimize the prompts, achieving satisfying results. However, existing automatic\nprompt optimization techniques are only limited to producing single flow\ninstructions, struggling with handling diverse patterns. In this paper, we\npresent AMPO, an automatic prompt optimization method that can iteratively\ndevelop a multi-branched prompt using failure cases as feedback. Our goal is to\nexplore a novel way of structuring prompts with multi-branches to better handle\nmultiple patterns in complex tasks, for which we introduce three modules:\nPattern Recognition, Branch Adjustment, and Branch Pruning. In experiments\nacross five tasks, AMPO consistently achieves the best results. Additionally,\nour approach demonstrates significant optimization efficiency due to our\nadoption of a minimal search strategy.", "published": "2024-10-11 10:34:28", "link": "http://arxiv.org/abs/2410.08696v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring the Groundedness of Legal Question-Answering Systems", "abstract": "In high-stakes domains like legal question-answering, the accuracy and\ntrustworthiness of generative AI systems are of paramount importance. This work\npresents a comprehensive benchmark of various methods to assess the\ngroundedness of AI-generated responses, aiming to significantly enhance their\nreliability. Our experiments include similarity-based metrics and natural\nlanguage inference models to evaluate whether responses are well-founded in the\ngiven contexts. We also explore different prompting strategies for large\nlanguage models to improve the detection of ungrounded responses. We validated\nthe effectiveness of these methods using a newly created grounding\nclassification corpus, designed specifically for legal queries and\ncorresponding responses from retrieval-augmented prompting, focusing on their\nalignment with source material. Our results indicate potential in groundedness\nclassification of generated responses, with the best method achieving a\nmacro-F1 score of 0.8. Additionally, we evaluated the methods in terms of their\nlatency to determine their suitability for real-world applications, as this\nstep typically follows the generation process. This capability is essential for\nprocesses that may trigger additional manual verification or automated response\nregeneration. In summary, this study demonstrates the potential of various\ndetection methods to improve the trustworthiness of generative AI in legal\nsettings.", "published": "2024-10-11 12:23:45", "link": "http://arxiv.org/abs/2410.08764v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the State of NLP Approaches to Modeling Depression in Social Media: A\n  Post-COVID-19 Outlook", "abstract": "Computational approaches to predicting mental health conditions in social\nmedia have been substantially explored in the past years. Multiple reviews have\nbeen published on this topic, providing the community with comprehensive\naccounts of the research in this area. Among all mental health conditions,\ndepression is the most widely studied due to its worldwide prevalence. The\nCOVID-19 global pandemic, starting in early 2020, has had a great impact on\nmental health worldwide. Harsh measures employed by governments to slow the\nspread of the virus (e.g., lockdowns) and the subsequent economic downturn\nexperienced in many countries have significantly impacted people's lives and\nmental health. Studies have shown a substantial increase of above 50% in the\nrate of depression in the population. In this context, we present a review on\nnatural language processing (NLP) approaches to modeling depression in social\nmedia, providing the reader with a post-COVID-19 outlook. This review\ncontributes to the understanding of the impacts of the pandemic on modeling\ndepression in social media. We outline how state-of-the-art approaches and new\ndatasets have been used in the context of the COVID-19 pandemic. Finally, we\nalso discuss ethical issues in collecting and processing mental health data,\nconsidering fairness, accountability, and ethics.", "published": "2024-10-11 13:20:54", "link": "http://arxiv.org/abs/2410.08793v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Processing for the OpenGPT-X Model Family", "abstract": "This paper presents a comprehensive overview of the data preparation pipeline\ndeveloped for the OpenGPT-X project, a large-scale initiative aimed at creating\nopen and high-performance multilingual large language models (LLMs). The\nproject goal is to deliver models that cover all major European languages, with\na particular focus on real-world applications within the European Union. We\nexplain all data processing steps, starting with the data selection and\nrequirement definition to the preparation of the final datasets for model\ntraining. We distinguish between curated data and web data, as each of these\ncategories is handled by distinct pipelines, with curated data undergoing\nminimal filtering and web data requiring extensive filtering and deduplication.\nThis distinction guided the development of specialized algorithmic solutions\nfor both pipelines. In addition to describing the processing methodologies, we\nprovide an in-depth analysis of the datasets, increasing transparency and\nalignment with European data regulations. Finally, we share key insights and\nchallenges faced during the project, offering recommendations for future\nendeavors in large-scale multilingual data preparation for LLMs.", "published": "2024-10-11 13:34:24", "link": "http://arxiv.org/abs/2410.08800v1", "categories": ["cs.CL", "H.3.1; I.2.7"], "primary_category": "cs.CL"}
{"title": "Which Demographics do LLMs Default to During Annotation?", "abstract": "Demographics and cultural background of annotators influence the labels they\nassign in text annotation -- for instance, an elderly woman might find it\noffensive to read a message addressed to a \"bro\", but a male teenager might\nfind it appropriate. It is therefore important to acknowledge label variations\nto not under-represent members of a society. Two research directions developed\nout of this observation in the context of using large language models (LLM) for\ndata annotations, namely (1) studying biases and inherent knowledge of LLMs and\n(2) injecting diversity in the output by manipulating the prompt with\ndemographic information. We combine these two strands of research and ask the\nquestion to which demographics an LLM resorts to when no demographics is given.\nTo answer this question, we evaluate which attributes of human annotators LLMs\ninherently mimic. Furthermore, we compare non-demographic conditioned prompts\nand placebo-conditioned prompts (e.g., \"you are an annotator who lives in house\nnumber 5\") to demographics-conditioned prompts (\"You are a 45 year old man and\nan expert on politeness annotation. How do you rate {instance}\"). We study\nthese questions for politeness and offensiveness annotations on the POPQUORN\ndata set, a corpus created in a controlled manner to investigate human label\nvariations based on demographics which has not been used for LLM-based analyses\nso far. We observe notable influences related to gender, race, and age in\ndemographic prompting, which contrasts with previous studies that found no such\neffects.", "published": "2024-10-11 14:02:42", "link": "http://arxiv.org/abs/2410.08820v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DeepNote: Note-Centric Deep Retrieval-Augmented Generation", "abstract": "Retrieval-Augmented Generation (RAG) mitigates factual errors and\nhallucinations in Large Language Models (LLMs) for question-answering (QA) by\nincorporating external knowledge. However, existing adaptive RAG methods rely\non LLMs to predict retrieval timing and directly use retrieved information for\ngeneration, often failing to reflect real information needs and fully leverage\nretrieved knowledge. We develop DeepNote, an adaptive RAG framework that\nachieves in-depth and robust exploration of knowledge sources through\nnote-centric adaptive retrieval. DeepNote employs notes as carriers for\nrefining and accumulating knowledge. During in-depth exploration, it uses these\nnotes to determine retrieval timing, formulate retrieval queries, and\niteratively assess knowledge growth, ultimately leveraging the best note for\nanswer generation. Extensive experiments and analyses demonstrate that DeepNote\nsignificantly outperforms all baselines (+10.2% to +20.1%) and exhibits the\nability to gather knowledge with both high density and quality. Additionally,\nDPO further improves the performance of DeepNote. The code and data are\navailable at https://github.com/thunlp/DeepNote.", "published": "2024-10-11 14:03:29", "link": "http://arxiv.org/abs/2410.08821v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring the Inconsistency of Large Language Models in Preferential\n  Ranking", "abstract": "Despite large language models' (LLMs) recent advancements, their bias and\nhallucination issues persist, and their ability to offer consistent\npreferential rankings remains underexplored. This study investigates the\ncapacity of LLMs to provide consistent ordinal preferences, a crucial aspect in\nscenarios with dense decision space or lacking absolute answers. We introduce a\nformalization of consistency based on order theory, outlining criteria such as\ntransitivity, asymmetry, reversibility, and independence from irrelevant\nalternatives. Our diagnostic experiments on selected state-of-the-art LLMs\nreveal their inability to meet these criteria, indicating a strong positional\nbias and poor transitivity, with preferences easily swayed by irrelevant\nalternatives. These findings highlight a significant inconsistency in\nLLM-generated preferential rankings, underscoring the need for further research\nto address these limitations.", "published": "2024-10-11 14:27:18", "link": "http://arxiv.org/abs/2410.08851v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RoRA-VLM: Robust Retrieval-Augmented Vision Language Models", "abstract": "Current vision-language models (VLMs) still exhibit inferior performance on\nknowledge-intensive tasks, primarily due to the challenge of accurately\nencoding all the associations between visual objects and scenes to their\ncorresponding entities and background knowledge. While retrieval augmentation\nmethods offer an efficient way to integrate external knowledge, extending them\nto vision-language domain presents unique challenges in (1) precisely\nretrieving relevant information from external sources due to the inherent\ndiscrepancy within the multimodal queries, and (2) being resilient to the\nirrelevant, extraneous and noisy information contained in the retrieved\nmultimodal knowledge snippets. In this work, we introduce RORA-VLM, a novel and\nrobust retrieval augmentation framework specifically tailored for VLMs, with\ntwo key innovations: (1) a 2-stage retrieval process with image-anchored\ntextual-query expansion to synergistically combine the visual and textual\ninformation in the query and retrieve the most relevant multimodal knowledge\nsnippets; and (2) a robust retrieval augmentation method that strengthens the\nresilience of VLMs against irrelevant information in the retrieved multimodal\nknowledge by injecting adversarial noises into the retrieval-augmented training\nprocess, and filters out extraneous visual information, such as unrelated\nentities presented in images, via a query-oriented visual token refinement\nstrategy. We conduct extensive experiments to validate the effectiveness and\nrobustness of our proposed methods on three widely adopted benchmark datasets.\nOur results demonstrate that with a minimal amount of training instance,\nRORA-VLM enables the base model to achieve significant performance improvement\nand constantly outperform state-of-the-art retrieval-augmented VLMs on all\nbenchmarks while also exhibiting a novel zero-shot domain transfer capability.", "published": "2024-10-11 14:51:00", "link": "http://arxiv.org/abs/2410.08876v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lifelong Event Detection via Optimal Transport", "abstract": "Continual Event Detection (CED) poses a formidable challenge due to the\ncatastrophic forgetting phenomenon, where learning new tasks (with new coming\nevent types) hampers performance on previous ones. In this paper, we introduce\na novel approach, Lifelong Event Detection via Optimal Transport (LEDOT), that\nleverages optimal transport principles to align the optimization of our\nclassification module with the intrinsic nature of each class, as defined by\ntheir pre-trained language modeling. Our method integrates replay sets,\nprototype latent representations, and an innovative Optimal Transport\ncomponent. Extensive experiments on MAVEN and ACE datasets demonstrate LEDOT's\nsuperior performance, consistently outperforming state-of-the-art baselines.\nThe results underscore LEDOT as a pioneering solution in continual event\ndetection, offering a more effective and nuanced approach to addressing\ncatastrophic forgetting in evolving environments.", "published": "2024-10-11 15:26:03", "link": "http://arxiv.org/abs/2410.08905v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AutoPersuade: A Framework for Evaluating and Explaining Persuasive\n  Arguments", "abstract": "We introduce AutoPersuade, a three-part framework for constructing persuasive\nmessages. First, we curate a large dataset of arguments with human evaluations.\nNext, we develop a novel topic model to identify argument features that\ninfluence persuasiveness. Finally, we use this model to predict the\neffectiveness of new arguments and assess the causal impact of different\ncomponents to provide explanations. We validate AutoPersuade through an\nexperimental study on arguments for veganism, demonstrating its effectiveness\nwith human studies and out-of-sample predictions.", "published": "2024-10-11 15:46:05", "link": "http://arxiv.org/abs/2410.08917v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extra Global Attention Designation Using Keyword Detection in Sparse\n  Transformer Architectures", "abstract": "In this paper, we propose an extension to Longformer Encoder-Decoder, a\npopular sparse transformer architecture. One common challenge with sparse\ntransformers is that they can struggle with encoding of long range context,\nsuch as connections between topics discussed at a beginning and end of a\ndocument. A method to selectively increase global attention is proposed and\ndemonstrated for abstractive summarization tasks on several benchmark data\nsets. By prefixing the transcript with additional keywords and encoding global\nattention on these keywords, improvement in zero-shot, few-shot, and fine-tuned\ncases is demonstrated for some benchmark data sets.", "published": "2024-10-11 16:41:11", "link": "http://arxiv.org/abs/2410.08971v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hypothesis-only Biases in Large Language Model-Elicited Natural Language\n  Inference", "abstract": "We test whether replacing crowdsource workers with LLMs to write Natural\nLanguage Inference (NLI) hypotheses similarly results in annotation artifacts.\nWe recreate a portion of the Stanford NLI corpus using GPT-4, Llama-2 and\nMistral 7b, and train hypothesis-only classifiers to determine whether\nLLM-elicited hypotheses contain annotation artifacts. On our LLM-elicited NLI\ndatasets, BERT-based hypothesis-only classifiers achieve between 86-96%\naccuracy, indicating these datasets contain hypothesis-only artifacts. We also\nfind frequent \"give-aways\" in LLM-generated hypotheses, e.g. the phrase\n\"swimming in a pool\" appears in more than 10,000 contradictions generated by\nGPT-4. Our analysis provides empirical evidence that well-attested biases in\nNLI can persist in LLM-generated data.", "published": "2024-10-11 17:09:22", "link": "http://arxiv.org/abs/2410.08996v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SuperCorrect: Advancing Small LLM Reasoning with Thought Template\n  Distillation and Self-Correction", "abstract": "Large language models (LLMs) like GPT-4, DeepSeek-R1, and ReasonFlux have\nshown significant improvements in various reasoning tasks. However, smaller\nLLMs still struggle with complex mathematical reasoning because they fail to\neffectively identify and correct reasoning errors. Recent reflection-based\nmethods aim to address these issues by enabling self-reflection and\nself-correction, but they still face challenges in independently detecting\nerrors in their reasoning steps. To overcome these limitations, we propose\nSuperCorrect, a novel two-stage framework that uses a large teacher model to\nsupervise and correct both the reasoning and reflection processes of a smaller\nstudent model. In the first stage, we extract hierarchical high-level and\ndetailed thought templates from the teacher model to guide the student model in\neliciting more fine-grained reasoning thoughts. In the second stage, we\nintroduce cross-model collaborative direct preference optimization (DPO) to\nenhance the self-correction abilities of the student model by following the\nteacher's correction traces during training. This cross-model DPO approach\nteaches the student model to effectively locate and resolve erroneous thoughts\nwith error-driven insights from the teacher model, breaking the bottleneck of\nits thoughts and acquiring new skills and knowledge to tackle challenging\nproblems. Extensive experiments consistently demonstrate our superiority over\nprevious methods. Notably, our SuperCorrect-7B model significantly surpasses\npowerful DeepSeekMath-7B by 7.8%/5.3% and Qwen2.5-Math-7B by 15.1%/6.3% on\nMATH/GSM8K benchmarks, achieving new SOTA performance among all 7B models.\nCode: https://github.com/YangLing0818/SuperCorrect-llm", "published": "2024-10-11 17:25:52", "link": "http://arxiv.org/abs/2410.09008v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Impact of Visual Information in Chinese Characters: Evaluating Large\n  Models' Ability to Recognize and Utilize Radicals", "abstract": "The glyphic writing system of Chinese incorporates information-rich visual\nfeatures in each character, such as radicals that provide hints about meaning\nor pronunciation. However, there has been no investigation into whether\ncontemporary Large Language Models (LLMs) and Vision-Language Models (VLMs) can\nharness these sub-character features in Chinese through prompting. In this\nstudy, we establish a benchmark to evaluate LLMs' and VLMs' understanding of\nvisual elements in Chinese characters, including radicals, composition\nstructures, strokes, and stroke counts. Our results reveal that models\nsurprisingly exhibit some, but still limited, knowledge of the visual\ninformation, regardless of whether images of characters are provided. To incite\nmodels' ability to use radicals, we further experiment with incorporating\nradicals into the prompts for Chinese language processing (CLP) tasks. We\nobserve consistent improvement in Part-Of-Speech tagging when providing\nadditional information about radicals, suggesting the potential to enhance CLP\nby integrating sub-character information.", "published": "2024-10-11 17:30:02", "link": "http://arxiv.org/abs/2410.09013v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MedMobile: A mobile-sized language model with expert-level clinical\n  capabilities", "abstract": "Language models (LMs) have demonstrated expert-level reasoning and recall\nabilities in medicine. However, computational costs and privacy concerns are\nmounting barriers to wide-scale implementation. We introduce a parsimonious\nadaptation of phi-3-mini, MedMobile, a 3.8 billion parameter LM capable of\nrunning on a mobile device, for medical applications. We demonstrate that\nMedMobile scores 75.7% on the MedQA (USMLE), surpassing the passing mark for\nphysicians (~60%), and approaching the scores of models 100 times its size. We\nsubsequently perform a careful set of ablations, and demonstrate that chain of\nthought, ensembling, and fine-tuning lead to the greatest performance gains,\nwhile unexpectedly retrieval augmented generation fails to demonstrate\nsignificant improvements", "published": "2024-10-11 17:32:59", "link": "http://arxiv.org/abs/2410.09019v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention\n  Manipulation", "abstract": "This paper studies the vulnerabilities of transformer-based Large Language\nModels (LLMs) to jailbreaking attacks, focusing specifically on the\noptimization-based Greedy Coordinate Gradient (GCG) strategy. We first observe\na positive correlation between the effectiveness of attacks and the internal\nbehaviors of the models. For instance, attacks tend to be less effective when\nmodels pay more attention to system prompts designed to ensure LLM safety\nalignment. Building on this discovery, we introduce an enhanced method that\nmanipulates models' attention scores to facilitate LLM jailbreaking, which we\nterm AttnGCG. Empirically, AttnGCG shows consistent improvements in attack\nefficacy across diverse LLMs, achieving an average increase of ~7% in the\nLlama-2 series and ~10% in the Gemma series. Our strategy also demonstrates\nrobust attack transferability against both unseen harmful goals and black-box\nLLMs like GPT-3.5 and GPT-4. Moreover, we note our attention-score\nvisualization is more interpretable, allowing us to gain better insights into\nhow our targeted attention manipulation facilitates more effective\njailbreaking. We release the code at\nhttps://github.com/UCSC-VLAA/AttnGCG-attack.", "published": "2024-10-11 17:55:09", "link": "http://arxiv.org/abs/2410.09040v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hybrid Training Approaches for LLMs: Leveraging Real and Synthetic Data\n  to Enhance Model Performance in Domain-Specific Applications", "abstract": "This research explores a hybrid approach to fine-tuning large language models\n(LLMs) by integrating real-world and synthetic data to boost model performance,\nparticularly in generating accurate and contextually relevant responses. By\nleveraging a dataset combining transcribed real interactions with high-quality\nsynthetic sessions, we aimed to overcome the limitations of scarce, noisy, and\ndomain-specific real data. Synthetic personas and scenarios were employed to\nenhance training diversity. The study evaluated three models: a base\nfoundational model, a model fine-tuned with real data, and a hybrid fine-tuned\nmodel. Experimental results showed that the hybrid model consistently\noutperformed the others in specific vertical applications, achieving the\nhighest scores across all metrics. Further testing confirmed the hybrid model's\nsuperior adaptability and contextual understanding across diverse scenarios.\nThese findings suggest that combining real and synthetic data can significantly\nimprove the robustness and contextual sensitivity of LLMs, particularly in\ndomain-specific and vertical use cases.", "published": "2024-10-11 18:16:03", "link": "http://arxiv.org/abs/2410.09168v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-Aware SQL Error Correction Using Few-Shot Learning -- A Novel\n  Approach Based on NLQ, Error, and SQL Similarity", "abstract": "In recent years, the demand for automated SQL generation has increased\nsignificantly, driven by the need for efficient data querying in various\napplications. However, generating accurate SQL queries remains a challenge due\nto the complexity and variability of natural language inputs. This paper\nintroduces a novel few-shot learning-based approach for error correction in SQL\ngeneration, enhancing the accuracy of generated queries by selecting the most\nsuitable few-shot error correction examples for a given natural language\nquestion (NLQ). In our experiments with the open-source Gretel dataset, the\nproposed model offers a 39.2% increase in fixing errors from the baseline\napproach with no error correction and a 10% increase from a simple error\ncorrection method. The proposed technique leverages embedding-based similarity\nmeasures to identify the closest matches from a repository of few-shot\nexamples. Each example comprises an incorrect SQL query, the resulting error,\nthe correct SQL query, and detailed steps to transform the incorrect query into\nthe correct one. By employing this method, the system can effectively guide the\ncorrection of errors in newly generated SQL queries. Our approach demonstrates\nsignificant improvements in SQL generation accuracy by providing contextually\nrelevant examples that facilitate error identification and correction. The\nexperimental results highlight the effectiveness of embedding-based selection\nin enhancing the few-shot learning process, leading to more precise and\nreliable SQL query generation. This research contributes to the field of\nautomated SQL generation by offering a robust framework for error correction,\npaving the way for more advanced and user-friendly database interaction tools.", "published": "2024-10-11 18:22:08", "link": "http://arxiv.org/abs/2410.09174v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-Tuning In-House Large Language Models to Infer Differential\n  Diagnosis from Radiology Reports", "abstract": "Radiology reports summarize key findings and differential diagnoses derived\nfrom medical imaging examinations. The extraction of differential diagnoses is\ncrucial for downstream tasks, including patient management and treatment\nplanning. However, the unstructured nature of these reports, characterized by\ndiverse linguistic styles and inconsistent formatting, presents significant\nchallenges. Although proprietary large language models (LLMs) such as GPT-4 can\neffectively retrieve clinical information, their use is limited in practice by\nhigh costs and concerns over the privacy of protected health information (PHI).\nThis study introduces a pipeline for developing in-house LLMs tailored to\nidentify differential diagnoses from radiology reports. We first utilize GPT-4\nto create 31,056 labeled reports, then fine-tune open source LLM using this\ndataset. Evaluated on a set of 1,067 reports annotated by clinicians, the\nproposed model achieves an average F1 score of 92.1\\%, which is on par with\nGPT-4 (90.8\\%). Through this study, we provide a methodology for constructing\nin-house LLMs that: match the performance of GPT, reduce dependence on\nexpensive proprietary models, and enhance the privacy and security of PHI.", "published": "2024-10-11 20:16:25", "link": "http://arxiv.org/abs/2410.09234v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sui Generis: Large Language Models for Authorship Attribution and\n  Verification in Latin", "abstract": "This paper evaluates the performance of Large Language Models (LLMs) in\nauthorship attribution and authorship verification tasks for Latin texts of the\nPatristic Era. The study showcases that LLMs can be robust in zero-shot\nauthorship verification even on short texts without sophisticated feature\nengineering. Yet, the models can also be easily \"mislead\" by semantics. The\nexperiments also demonstrate that steering the model's authorship analysis and\ndecision-making is challenging, unlike what is reported in the studies dealing\nwith high-resource modern languages. Although LLMs prove to be able to beat,\nunder certain circumstances, the traditional baselines, obtaining a nuanced and\ntruly explainable decision requires at best a lot of experimentation.", "published": "2024-10-11 20:41:49", "link": "http://arxiv.org/abs/2410.09245v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparative Analysis of Static and Contextual Embeddings for Analyzing\n  Semantic Changes in Medieval Latin Charters", "abstract": "The Norman Conquest of 1066 C.E. brought profound transformations to\nEngland's administrative, societal, and linguistic practices. The DEEDS\n(Documents of Early England Data Set) database offers a unique opportunity to\nexplore these changes by examining shifts in word meanings within a vast\ncollection of Medieval Latin charters. While computational linguistics\ntypically relies on vector representations of words like static and contextual\nembeddings to analyze semantic changes, existing embeddings for scarce and\nhistorical Medieval Latin are limited and may not be well-suited for this task.\nThis paper presents the first computational analysis of semantic change pre-\nand post-Norman Conquest and the first systematic comparison of static and\ncontextual embeddings in a scarce historical data set. Our findings confirm\nthat, consistent with existing studies, contextual embeddings outperform static\nword embeddings in capturing semantic change within a scarce historical corpus.", "published": "2024-10-11 22:19:17", "link": "http://arxiv.org/abs/2410.09283v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "oRetrieval Augmented Generation for 10 Large Language Models and its\n  Generalizability in Assessing Medical Fitness", "abstract": "Large Language Models (LLMs) show potential for medical applications but\noften lack specialized clinical knowledge. Retrieval Augmented Generation (RAG)\nallows customization with domain-specific information, making it suitable for\nhealthcare. This study evaluates the accuracy, consistency, and safety of RAG\nmodels in determining fitness for surgery and providing preoperative\ninstructions. We developed LLM-RAG models using 35 local and 23 international\npreoperative guidelines and tested them against human-generated responses. A\ntotal of 3,682 responses were evaluated. Clinical documents were processed\nusing Llamaindex, and 10 LLMs, including GPT3.5, GPT4, and Claude-3, were\nassessed. Fourteen clinical scenarios were analyzed, focusing on seven aspects\nof preoperative instructions. Established guidelines and expert judgment were\nused to determine correct responses, with human-generated answers serving as\ncomparisons. The LLM-RAG models generated responses within 20 seconds,\nsignificantly faster than clinicians (10 minutes). The GPT4 LLM-RAG model\nachieved the highest accuracy (96.4% vs. 86.6%, p=0.016), with no\nhallucinations and producing correct instructions comparable to clinicians.\nResults were consistent across both local and international guidelines. This\nstudy demonstrates the potential of LLM-RAG models for preoperative healthcare\ntasks, highlighting their efficiency, scalability, and reliability.", "published": "2024-10-11 00:34:20", "link": "http://arxiv.org/abs/2410.08431v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring the Role of Reasoning Structures for Constructing Proofs in\n  Multi-Step Natural Language Reasoning with Large Language Models", "abstract": "When performing complex multi-step reasoning tasks, the ability of Large\nLanguage Models (LLMs) to derive structured intermediate proof steps is\nimportant for ensuring that the models truly perform the desired reasoning and\nfor improving models' explainability. This paper is centred around a focused\nstudy: whether the current state-of-the-art generalist LLMs can leverage the\nstructures in a few examples to better construct the proof structures with\n\\textit{in-context learning}. Our study specifically focuses on structure-aware\ndemonstration and structure-aware pruning. We demonstrate that they both help\nimprove performance. A detailed analysis is provided to help understand the\nresults.", "published": "2024-10-11 00:45:50", "link": "http://arxiv.org/abs/2410.08436v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AutoEval: Autonomous Evaluation of LLMs for Truth Maintenance and\n  Reasoning Tasks", "abstract": "This paper presents AutoEval, a novel benchmark for scaling Large Language\nModel (LLM) assessment in formal tasks with clear notions of correctness, such\nas truth maintenance in translation and logical reasoning. AutoEval is the\nfirst benchmarking paradigm that offers several key advantages necessary for\nscaling objective evaluation of LLMs without human labeling: (a) ability to\nevaluate LLMs of increasing sophistication by auto-generating tasks at\ndifferent levels of difficulty; (b) auto-generation of ground truth that\neliminates dependence on expensive and time-consuming human annotation; (c) the\nuse of automatically generated, randomized datasets that mitigate the ability\nof successive LLMs to overfit to static datasets used in many contemporary\nbenchmarks. Empirical analysis shows that an LLM's performance on AutoEval is\nhighly indicative of its performance on a diverse array of other benchmarks\nfocusing on translation and reasoning tasks, making it a valuable autonomous\nevaluation paradigm in settings where hand-curated datasets can be hard to\nobtain and/or update.", "published": "2024-10-11 00:56:37", "link": "http://arxiv.org/abs/2410.08437v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Simultaneous Reward Distillation and Preference Learning: Get You a\n  Language Model Who Can Do Both", "abstract": "Traditional RLHF-based LLM alignment methods explicitly maximize the expected\nrewards from a separate reward model. More recent supervised alignment methods\nlike Direct Preference Optimization (DPO) circumvent this phase to avoid\nproblems including model drift and reward overfitting. Although popular due to\nits simplicity, DPO and similar direct alignment methods which rely heavily on\nthe Bradley-Terry-based pairwise preference formulation can still lead to\ndegenerate policies when challenged by non-deterministic or noisy preference\nlabels, for example human scoring of two candidate outputs with low confidence.\nThis paper introduces DRDO (Direct Reward Distillation and\npolicy-Optimization), which simultaneously models rewards and preferences to\navoid such degeneracy. DRDO directly mimics rewards assigned by an oracle while\nlearning human preferences with a novel preference likelihood formulation.\nResults on the Ultrafeedback and TL;DR datasets demonstrate that DRDO-trained\npolicies surpass methods such as DPO and e-DPO in terms of expected rewards and\nare more robust, on average, to noisy preference signals as well as\nout-of-distribution (OOD) settings.", "published": "2024-10-11 02:19:11", "link": "http://arxiv.org/abs/2410.08458v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SPORTU: A Comprehensive Sports Understanding Benchmark for Multimodal\n  Large Language Models", "abstract": "Multimodal Large Language Models (MLLMs) are advancing the ability to reason\nabout complex sports scenarios by integrating textual and visual information.\nTo comprehensively evaluate their capabilities, we introduce SPORTU, a\nbenchmark designed to assess MLLMs across multi-level sports reasoning tasks.\nSPORTU comprises two key components: SPORTU-text, featuring 900 multiple-choice\nquestions with human-annotated explanations for rule comprehension and strategy\nunderstanding. This component focuses on testing models' ability to reason\nabout sports solely through question-answering (QA), without requiring visual\ninputs; SPORTU-video, consisting of 1,701 slow-motion video clips across 7\ndifferent sports and 12,048 QA pairs, designed to assess multi-level reasoning,\nfrom simple sports recognition to complex tasks like foul detection and rule\napplication. We evaluate four prevalent LLMs mainly utilizing few-shot learning\nparadigms supplemented by chain-of-thought (CoT) prompting on the SPORTU-text\npart. We evaluate four LLMs using few-shot learning and chain-of-thought (CoT)\nprompting on SPORTU-text. GPT-4o achieves the highest accuracy of 71%, but\nstill falls short of human-level performance, highlighting room for improvement\nin rule comprehension and reasoning. The evaluation for the SPORTU-video part\nincludes 7 proprietary and 6 open-source MLLMs. Experiments show that models\nfall short on hard tasks that require deep reasoning and rule-based\nunderstanding. Claude-3.5-Sonnet performs the best with only 52.6% accuracy on\nthe hard task, showing large room for improvement. We hope that SPORTU will\nserve as a critical step toward evaluating models' capabilities in sports\nunderstanding and reasoning.", "published": "2024-10-11 02:58:38", "link": "http://arxiv.org/abs/2410.08474v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "GIVE: Structured Reasoning of Large Language Models with Knowledge Graph\n  Inspired Veracity Extrapolation", "abstract": "Existing approaches based on context prompting or reinforcement learning (RL)\nto improve the reasoning capacities of large language models (LLMs) depend on\nthe LLMs' internal knowledge to produce reliable Chain-Of-Thought (CoT).\nHowever, no matter the size of LLMs, certain problems cannot be resolved in a\nsingle forward pass. Meanwhile, agent-based reasoning systems require access to\na comprehensive nonparametric knowledge base, which is often costly or not\nfeasible for use in scientific and niche domains. We present Graph Inspired\nVeracity Extrapolation (GIVE), a novel reasoning method that merges parametric\nand non-parametric memories to improve accurate reasoning with minimal external\ninput. GIVE guides the LLM agent to select the most pertinent expert data\n(observe), engage in query-specific divergent thinking (reflect), and then\nsynthesize this information to produce the final output (speak). Extensive\nexperiments demonstrated the following benefits of our framework: (1) GIVE\nboosts the performance of LLMs across various sizes. (2) In some scenarios,\nGIVE allows smaller LLMs to surpass larger, more sophisticated ones in\nscientific tasks (GPT3.5T + GIVE > GPT4). (3) GIVE is effective on scientific\nand open-domain assessments. (4) GIVE is a training-free method that enables\nLLMs to tackle new problems that extend beyond their training data (up to 43.5%\n-> 88.2%} accuracy improvement). (5) GIVE allows LLM agents to reason using\nboth restricted (very small) and noisy (very large) knowledge sources,\naccommodating knowledge graphs (KG) ranging from 135 to more than 840k nodes.\n(6) The reasoning process involved in GIVE is fully interpretable.", "published": "2024-10-11 03:05:06", "link": "http://arxiv.org/abs/2410.08475v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Humanity in AI: Detecting the Personality of Large Language Models", "abstract": "Questionnaires are a common method for detecting the personality of Large\nLanguage Models (LLMs). However, their reliability is often compromised by two\nmain issues: hallucinations (where LLMs produce inaccurate or irrelevant\nresponses) and the sensitivity of responses to the order of the presented\noptions. To address these issues, we propose combining text mining with\nquestionnaires method. Text mining can extract psychological features from the\nLLMs' responses without being affected by the order of options. Furthermore,\nbecause this method does not rely on specific answers, it reduces the influence\nof hallucinations. By normalizing the scores from both methods and calculating\nthe root mean square error, our experiment results confirm the effectiveness of\nthis approach. To further investigate the origins of personality traits in\nLLMs, we conduct experiments on both pre-trained language models (PLMs), such\nas BERT and GPT, as well as conversational models (ChatLLMs), such as ChatGPT.\nThe results show that LLMs do contain certain personalities, for example,\nChatGPT and ChatGLM exhibit the personality traits of 'Conscientiousness'.\nAdditionally, we find that the personalities of LLMs are derived from their\npre-trained data. The instruction data used to train ChatLLMs can enhance the\ngeneration of data containing personalities and expose their hidden\npersonality. We compare the results with the human average personality score,\nand we find that the personality of FLAN-T5 in PLMs and ChatGPT in ChatLLMs is\nmore similar to that of a human, with score differences of 0.34 and 0.22,\nrespectively.", "published": "2024-10-11 05:53:11", "link": "http://arxiv.org/abs/2410.08545v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Similar Phrases for Cause of Actions of Civil Cases", "abstract": "In the Taiwanese judicial system, Cause of Actions (COAs) are essential for\nidentifying relevant legal judgments. However, the lack of standardized COA\nlabeling creates challenges in filtering cases using basic methods. This\nresearch addresses this issue by leveraging embedding and clustering techniques\nto analyze the similarity between COAs based on cited legal articles. The study\nimplements various similarity measures, including Dice coefficient and\nPearson's correlation coefficient. An ensemble model combines rankings, and\nsocial network analysis identifies clusters of related COAs. This approach\nenhances legal analysis by revealing inconspicuous connections between COAs,\noffering potential applications in legal research beyond civil law.", "published": "2024-10-11 06:43:45", "link": "http://arxiv.org/abs/2410.08564v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Retrieving Contextual Information for Long-Form Question Answering using\n  Weak Supervision", "abstract": "Long-form question answering (LFQA) aims at generating in-depth answers to\nend-user questions, providing relevant information beyond the direct answer.\nHowever, existing retrievers are typically optimized towards information that\ndirectly targets the question, missing out on such contextual information.\nFurthermore, there is a lack of training data for relevant context. To this\nend, we propose and compare different weak supervision techniques to optimize\nretrieval for contextual information. Experiments demonstrate improvements on\nthe end-to-end QA performance on ASQA, a dataset for long-form question\nanswering. Importantly, as more contextual information is retrieved, we improve\nthe relevant page recall for LFQA by 14.7% and the groundedness of generated\nlong-form answers by 12.5%. Finally, we show that long-form answers often\nanticipate likely follow-up questions, via experiments on a conversational QA\ndataset.", "published": "2024-10-11 08:42:02", "link": "http://arxiv.org/abs/2410.08623v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "QEFT: Quantization for Efficient Fine-Tuning of LLMs", "abstract": "With the rapid growth in the use of fine-tuning for large language models\n(LLMs), optimizing fine-tuning while keeping inference efficient has become\nhighly important. However, this is a challenging task as it requires\nimprovements in all aspects, including inference speed, fine-tuning speed,\nmemory consumption, and, most importantly, model quality. Previous studies have\nattempted to achieve this by combining quantization with fine-tuning, but they\nhave failed to enhance all four aspects simultaneously. In this study, we\npropose a new lightweight technique called Quantization for Efficient\nFine-Tuning (QEFT). QEFT accelerates both inference and fine-tuning, is\nsupported by robust theoretical foundations, offers high flexibility, and\nmaintains good hardware compatibility. Our extensive experiments demonstrate\nthat QEFT matches the quality and versatility of full-precision\nparameter-efficient fine-tuning, while using fewer resources. Our code is\navailable at https://github.com/xvyaward/qeft.", "published": "2024-10-11 09:39:33", "link": "http://arxiv.org/abs/2410.08661v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SocialGaze: Improving the Integration of Human Social Norms in Large\n  Language Models", "abstract": "While much research has explored enhancing the reasoning capabilities of\nlarge language models (LLMs) in the last few years, there is a gap in\nunderstanding the alignment of these models with social values and norms. We\nintroduce the task of judging social acceptance. Social acceptance requires\nmodels to judge and rationalize the acceptability of people's actions in social\nsituations. For example, is it socially acceptable for a neighbor to ask others\nin the community to keep their pets indoors at night? We find that LLMs'\nunderstanding of social acceptance is often misaligned with human consensus. To\nalleviate this, we introduce SocialGaze, a multi-step prompting framework, in\nwhich a language model verbalizes a social situation from multiple perspectives\nbefore forming a judgment. Our experiments demonstrate that the SocialGaze\napproach improves the alignment with human judgments by up to 11 F1 points with\nthe GPT-3.5 model. We also identify biases and correlations in LLMs in\nassigning blame that is related to features such as the gender (males are\nsignificantly more likely to be judged unfairly) and age (LLMs are more aligned\nwith humans for older narrators).", "published": "2024-10-11 10:35:58", "link": "http://arxiv.org/abs/2410.08698v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "On the token distance modeling ability of higher RoPE attention\n  dimension", "abstract": "Length extrapolation algorithms based on Rotary position embedding (RoPE)\nhave shown promising results in extending the context length of language\nmodels. However, understanding how position embedding can capture longer-range\ncontextual information remains elusive. Based on the intuition that different\ndimensions correspond to different frequency of changes in RoPE encoding, we\nconducted a dimension-level analysis to investigate the correlation between a\nhidden dimension of an attention head and its contribution to capturing\nlong-distance dependencies. Using our correlation metric, we identified a\nparticular type of attention heads, which we named Positional Heads, from\nvarious length-extrapolated models. These heads exhibit a strong focus on\nlong-range information interaction and play a pivotal role in long input\nprocessing, as evidence by our ablation. We further demonstrate the correlation\nbetween the efficiency of length extrapolation and the extension of the\nhigh-dimensional attention allocation of these heads. The identification of\nPositional Heads provides insights for future research in long-text\ncomprehension.", "published": "2024-10-11 10:47:02", "link": "http://arxiv.org/abs/2410.08703v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From N-grams to Pre-trained Multilingual Models For Language\n  Identification", "abstract": "In this paper, we investigate the use of N-gram models and Large Pre-trained\nMultilingual models for Language Identification (LID) across 11 South African\nlanguages. For N-gram models, this study shows that effective data size\nselection remains crucial for establishing effective frequency distributions of\nthe target languages, that efficiently model each language, thus, improving\nlanguage ranking. For pre-trained multilingual models, we conduct extensive\nexperiments covering a diverse set of massively pre-trained multilingual (PLM)\nmodels -- mBERT, RemBERT, XLM-r, and Afri-centric multilingual models --\nAfriBERTa, Afro-XLMr, AfroLM, and Serengeti. We further compare these models\nwith available large-scale Language Identification tools: Compact Language\nDetector v3 (CLD V3), AfroLID, GlotLID, and OpenLID to highlight the importance\nof focused-based LID. From these, we show that Serengeti is a superior model\nacross models: N-grams to Transformers on average. Moreover, we propose a\nlightweight BERT-based LID model (za_BERT_lid) trained with NHCLT + Vukzenzele\ncorpus, which performs on par with our best-performing Afri-centric models.", "published": "2024-10-11 11:35:57", "link": "http://arxiv.org/abs/2410.08728v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Developing a Pragmatic Benchmark for Assessing Korean Legal Language\n  Understanding in Large Language Models", "abstract": "Large language models (LLMs) have demonstrated remarkable performance in the\nlegal domain, with GPT-4 even passing the Uniform Bar Exam in the U.S. However\ntheir efficacy remains limited for non-standardized tasks and tasks in\nlanguages other than English. This underscores the need for careful evaluation\nof LLMs within each legal system before application. Here, we introduce KBL, a\nbenchmark for assessing the Korean legal language understanding of LLMs,\nconsisting of (1) 7 legal knowledge tasks (510 examples), (2) 4 legal reasoning\ntasks (288 examples), and (3) the Korean bar exam (4 domains, 53 tasks, 2,510\nexamples). First two datasets were developed in close collaboration with\nlawyers to evaluate LLMs in practical scenarios in a certified manner.\nFurthermore, considering legal practitioners' frequent use of extensive legal\ndocuments for research, we assess LLMs in both a closed book setting, where\nthey rely solely on internal knowledge, and a retrieval-augmented generation\n(RAG) setting, using a corpus of Korean statutes and precedents. The results\nindicate substantial room and opportunities for improvement.", "published": "2024-10-11 11:41:02", "link": "http://arxiv.org/abs/2410.08731v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Social Context-aware Graph-based Multimodal Attentive Learning\n  Framework for Disaster Content Classification during Emergencies", "abstract": "In times of crisis, the prompt and precise classification of disaster-related\ninformation shared on social media platforms is crucial for effective disaster\nresponse and public safety. During such critical events, individuals use social\nmedia to communicate, sharing multimodal textual and visual content. However,\ndue to the significant influx of unfiltered and diverse data, humanitarian\norganizations face challenges in leveraging this information efficiently.\nExisting methods for classifying disaster-related content often fail to model\nusers' credibility, emotional context, and social interaction information,\nwhich are essential for accurate classification. To address this gap, we\npropose CrisisSpot, a method that utilizes a Graph-based Neural Network to\ncapture complex relationships between textual and visual modalities, as well as\nSocial Context Features to incorporate user-centric and content-centric\ninformation. We also introduce Inverted Dual Embedded Attention (IDEA), which\ncaptures both harmonious and contrasting patterns within the data to enhance\nmultimodal interactions and provide richer insights. Additionally, we present\nTSEqD (Turkey-Syria Earthquake Dataset), a large annotated dataset for a single\ndisaster event, containing 10,352 samples. Through extensive experiments,\nCrisisSpot demonstrated significant improvements, achieving an average F1-score\ngain of 9.45% and 5.01% compared to state-of-the-art methods on the publicly\navailable CrisisMMD dataset and the TSEqD dataset, respectively.", "published": "2024-10-11 13:51:46", "link": "http://arxiv.org/abs/2410.08814v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via\n  Inference-time Hybrid Information Structurization", "abstract": "Retrieval-augmented generation (RAG) is a key means to effectively enhance\nlarge language models (LLMs) in many knowledge-based tasks. However, existing\nRAG methods struggle with knowledge-intensive reasoning tasks, because useful\ninformation required to these tasks are badly scattered. This characteristic\nmakes it difficult for existing RAG methods to accurately identify key\ninformation and perform global reasoning with such noisy augmentation. In this\npaper, motivated by the cognitive theories that humans convert raw information\ninto various structured knowledge when tackling knowledge-intensive reasoning,\nwe proposes a new framework, StructRAG, which can identify the optimal\nstructure type for the task at hand, reconstruct original documents into this\nstructured format, and infer answers based on the resulting structure.\nExtensive experiments across various knowledge-intensive tasks show that\nStructRAG achieves state-of-the-art performance, particularly excelling in\nchallenging scenarios, demonstrating its potential as an effective solution for\nenhancing LLMs in complex real-world applications.", "published": "2024-10-11 13:52:44", "link": "http://arxiv.org/abs/2410.08815v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Audio Description Generation in the Era of LLMs and VLMs: A Review of\n  Transferable Generative AI Technologies", "abstract": "Audio descriptions (ADs) function as acoustic commentaries designed to assist\nblind persons and persons with visual impairments in accessing digital media\ncontent on television and in movies, among other settings. As an accessibility\nservice typically provided by trained AD professionals, the generation of ADs\ndemands significant human effort, making the process both time-consuming and\ncostly. Recent advancements in natural language processing (NLP) and computer\nvision (CV), particularly in large language models (LLMs) and vision-language\nmodels (VLMs), have allowed for getting a step closer to automatic AD\ngeneration. This paper reviews the technologies pertinent to AD generation in\nthe era of LLMs and VLMs: we discuss how state-of-the-art NLP and CV\ntechnologies can be applied to generate ADs and identify essential research\ndirections for the future.", "published": "2024-10-11 14:40:51", "link": "http://arxiv.org/abs/2410.08860v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "A Benchmark for Cross-Domain Argumentative Stance Classification on\n  Social Media", "abstract": "Argumentative stance classification plays a key role in identifying authors'\nviewpoints on specific topics. However, generating diverse pairs of\nargumentative sentences across various domains is challenging. Existing\nbenchmarks often come from a single domain or focus on a limited set of topics.\nAdditionally, manual annotation for accurate labeling is time-consuming and\nlabor-intensive. To address these challenges, we propose leveraging platform\nrules, readily available expert-curated content, and large language models to\nbypass the need for human annotation. Our approach produces a multidomain\nbenchmark comprising 4,498 topical claims and 30,961 arguments from three\nsources, spanning 21 domains. We benchmark the dataset in fully supervised,\nzero-shot, and few-shot settings, shedding light on the strengths and\nlimitations of different methodologies. We release the dataset and code in this\nstudy at hidden for anonymity.", "published": "2024-10-11 15:20:11", "link": "http://arxiv.org/abs/2410.08900v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language Imbalance Driven Rewarding for Multilingual Self-improving", "abstract": "Large Language Models (LLMs) have achieved state-of-the-art performance\nacross numerous tasks. However, these advancements have predominantly benefited\n\"first-class\" languages such as English and Chinese, leaving many other\nlanguages underrepresented. This imbalance, while limiting broader\napplications, generates a natural preference ranking between languages,\noffering an opportunity to bootstrap the multilingual capabilities of LLM in a\nself-improving manner. Thus, we propose $\\textit{Language Imbalance Driven\nRewarding}$, where the inherent imbalance between dominant and non-dominant\nlanguages within LLMs is leveraged as a reward signal. Iterative DPO training\ndemonstrates that this approach not only enhances LLM performance in\nnon-dominant languages but also improves the dominant language's capacity,\nthereby yielding an iterative reward signal. Fine-tuning\nMeta-Llama-3-8B-Instruct over two iterations of this approach results in\ncontinuous improvements in multilingual performance across\ninstruction-following and arithmetic reasoning tasks, evidenced by an average\nimprovement of 7.46% win rate on the X-AlpacaEval leaderboard and 13.9%\naccuracy on the MGSM benchmark. This work serves as an initial exploration,\npaving the way for multilingual self-improvement of LLMs. The code is available\nat https://github.com/ZNLP/Language-Imbalance-Driven-Rewarding", "published": "2024-10-11 16:32:05", "link": "http://arxiv.org/abs/2410.08964v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Controllable Safety Alignment: Inference-Time Adaptation to Diverse\n  Safety Requirements", "abstract": "The current paradigm for safety alignment of large language models (LLMs)\nfollows a one-size-fits-all approach: the model refuses to interact with any\ncontent deemed unsafe by the model provider. This approach lacks flexibility in\nthe face of varying social norms across cultures and regions. In addition,\nusers may have diverse safety needs, making a model with static safety\nstandards too restrictive to be useful, as well as too costly to be re-aligned.\n  We propose Controllable Safety Alignment (CoSA), a framework designed to\nadapt models to diverse safety requirements without re-training. Instead of\naligning a fixed model, we align models to follow safety configs -- free-form\nnatural language descriptions of the desired safety behaviors -- that are\nprovided as part of the system prompt. To adjust model safety behavior,\nauthorized users only need to modify such safety configs at inference time. To\nenable that, we propose CoSAlign, a data-centric method for aligning LLMs to\neasily adapt to diverse safety configs. Furthermore, we devise a novel\ncontrollability evaluation protocol that considers both helpfulness and\nconfigured safety, summarizing them into CoSA-Score, and construct CoSApien, a\nhuman-authored benchmark that consists of real-world LLM use cases with diverse\nsafety requirements and corresponding evaluation prompts. We show that CoSAlign\nleads to substantial gains of controllability over strong baselines including\nin-context alignment. Our framework encourages better representation and\nadaptation to pluralistic human values in LLMs, and thereby increasing their\npracticality.", "published": "2024-10-11 16:38:01", "link": "http://arxiv.org/abs/2410.08968v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NoVo: Norm Voting off Hallucinations with Attention Heads in Large\n  Language Models", "abstract": "Hallucinations in Large Language Models (LLMs) remain a major obstacle,\nparticularly in high-stakes applications where factual accuracy is critical.\nWhile representation editing and reading methods have made strides in reducing\nhallucinations, their heavy reliance on specialised tools and training on\nin-domain samples, makes them difficult to scale and prone to overfitting. This\nlimits their accuracy gains and generalizability to diverse datasets. This\npaper presents a lightweight method, Norm Voting (NoVo), which harnesses the\nuntapped potential of attention head norms to dramatically enhance factual\naccuracy in zero-shot multiple-choice questions (MCQs). NoVo begins by\nautomatically selecting truth-correlated head norms with an efficient,\ninference-only algorithm using only 30 random samples, allowing NoVo to\neffortlessly scale to diverse datasets. Afterwards, selected head norms are\nemployed in a simple voting algorithm, which yields significant gains in\nprediction accuracy. On TruthfulQA MC1, NoVo surpasses the current\nstate-of-the-art and all previous methods by an astounding margin -- at least\n19 accuracy points. NoVo demonstrates exceptional generalization to 20 diverse\ndatasets, with significant gains in over 90\\% of them, far exceeding all\ncurrent representation editing and reading methods. NoVo also reveals promising\ngains to finetuning strategies and building textual adversarial defence. NoVo's\neffectiveness with head norms opens new frontiers in LLM interpretability,\nrobustness and reliability.", "published": "2024-10-11 16:40:03", "link": "http://arxiv.org/abs/2410.08970v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Trustworthy Knowledge Graph Reasoning: An Uncertainty Aware\n  Perspective", "abstract": "Recently, Knowledge Graphs (KGs) have been successfully coupled with Large\nLanguage Models (LLMs) to mitigate their hallucinations and enhance their\nreasoning capability, such as in KG-based retrieval-augmented frameworks.\nHowever, current KG-LLM frameworks lack rigorous uncertainty estimation,\nlimiting their reliable deployment in high-stakes applications. Directly\nincorporating uncertainty quantification into KG-LLM frameworks presents\nchallenges due to their complex architectures and the intricate interactions\nbetween the knowledge graph and language model components. To address this gap,\nwe propose a new trustworthy KG-LLM framework, Uncertainty Aware\nKnowledge-Graph Reasoning (UAG), which incorporates uncertainty quantification\ninto the KG-LLM framework. We design an uncertainty-aware multi-step reasoning\nframework that leverages conformal prediction to provide a theoretical\nguarantee on the prediction set. To manage the error rate of the multi-step\nprocess, we additionally introduce an error rate control module to adjust the\nerror rate within the individual components. Extensive experiments show that\nour proposed UAG can achieve any pre-defined coverage rate while reducing the\nprediction set/interval size by 40% on average over the baselines.", "published": "2024-10-11 16:57:30", "link": "http://arxiv.org/abs/2410.08985v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Science is Exploration: Computational Frontiers for Conceptual Metaphor\n  Theory", "abstract": "Metaphors are everywhere. They appear extensively across all domains of\nnatural language, from the most sophisticated poetry to seemingly dry academic\nprose. A significant body of research in the cognitive science of language\nargues for the existence of conceptual metaphors, the systematic structuring of\none domain of experience in the language of another. Conceptual metaphors are\nnot simply rhetorical flourishes but are crucial evidence of the role of\nanalogical reasoning in human cognition. In this paper, we ask whether Large\nLanguage Models (LLMs) can accurately identify and explain the presence of such\nconceptual metaphors in natural language data. Using a novel prompting\ntechnique based on metaphor annotation guidelines, we demonstrate that LLMs are\na promising tool for large-scale computational research on conceptual\nmetaphors. Further, we show that LLMs are able to apply procedural guidelines\ndesigned for human annotators, displaying a surprising depth of linguistic\nknowledge.", "published": "2024-10-11 17:03:13", "link": "http://arxiv.org/abs/2410.08991v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Parameter-Efficient Fine-Tuning of State Space Models", "abstract": "Deep State Space Models (SSMs), such as Mamba (Gu & Dao, 2024), have become\npowerful tools for language modeling, offering high performance and linear\nscalability with sequence length. However, the application of\nparameter-efficient fine-tuning (PEFT) methods to SSM-based models remains\nunderexplored. We start by investigating two fundamental questions on existing\nPEFT methods: (i) How do they perform on SSM-based models? (ii) Which\nparameters should they target for optimal results? Our analysis shows that LoRA\nand its variants consistently outperform all other PEFT methods. While LoRA is\neffective for linear projection matrices, it fails on SSM modules-yet still\noutperforms other methods applicable to SSMs, indicating their limitations.\nThis underscores the need for a specialized SSM tuning approach. To address\nthis, we propose Sparse Dimension Tuning (SDT), a PEFT method tailored for SSM\nmodules. Combining SDT for SSMs with LoRA for linear projection matrices, we\nachieve state-of-the-art performance across extensive experiments.", "published": "2024-10-11 17:30:28", "link": "http://arxiv.org/abs/2410.09016v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Mentor-KD: Making Small Language Models Better Multi-step Reasoners", "abstract": "Large Language Models (LLMs) have displayed remarkable performances across\nvarious complex tasks by leveraging Chain-of-Thought (CoT) prompting. Recently,\nstudies have proposed a Knowledge Distillation (KD) approach, reasoning\ndistillation, which transfers such reasoning ability of LLMs through\nfine-tuning language models of multi-step rationales generated by LLM teachers.\nHowever, they have inadequately considered two challenges regarding\ninsufficient distillation sets from the LLM teacher model, in terms of 1) data\nquality and 2) soft label provision. In this paper, we propose Mentor-KD, which\neffectively distills the multi-step reasoning capability of LLMs to smaller LMs\nwhile addressing the aforementioned challenges. Specifically, we exploit a\nmentor, intermediate-sized task-specific fine-tuned model, to augment\nadditional CoT annotations and provide soft labels for the student model during\nreasoning distillation. We conduct extensive experiments and confirm\nMentor-KD's effectiveness across various models and complex reasoning tasks.", "published": "2024-10-11 17:53:27", "link": "http://arxiv.org/abs/2410.09037v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SimpleStrat: Diversifying Language Model Generation with Stratification", "abstract": "Generating diverse responses from large language models (LLMs) is crucial for\napplications such as planning/search and synthetic data generation, where\ndiversity provides distinct answers across generations. Prior approaches rely\non increasing temperature to increase diversity. However, contrary to popular\nbelief, we show not only does this approach produce lower quality individual\ngenerations as temperature increases, but it depends on model's next-token\nprobabilities being similar to the true distribution of answers. We propose\nSimpleStrat, an alternative approach that uses the language model itself to\npartition the space into strata. At inference, a random stratum is selected and\na sample drawn from within the strata. To measure diversity, we introduce\nCoverageQA, a dataset of underspecified questions with multiple equally\nplausible answers, and assess diversity by measuring KL Divergence between the\noutput distribution and uniform distribution over valid ground truth answers.\nAs computing probability per response/solution for proprietary models is\ninfeasible, we measure recall on ground truth solutions. Our evaluation show\nusing SimpleStrat achieves higher recall by 0.05 compared to GPT-4o and 0.36\naverage reduction in KL Divergence compared to Llama 3.", "published": "2024-10-11 17:54:14", "link": "http://arxiv.org/abs/2410.09038v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MiRAGeNews: Multimodal Realistic AI-Generated News Detection", "abstract": "The proliferation of inflammatory or misleading \"fake\" news content has\nbecome increasingly common in recent years. Simultaneously, it has become\neasier than ever to use AI tools to generate photorealistic images depicting\nany scene imaginable. Combining these two -- AI-generated fake news content --\nis particularly potent and dangerous. To combat the spread of AI-generated fake\nnews, we propose the MiRAGeNews Dataset, a dataset of 12,500 high-quality real\nand AI-generated image-caption pairs from state-of-the-art generators. We find\nthat our dataset poses a significant challenge to humans (60% F-1) and\nstate-of-the-art multi-modal LLMs (< 24% F-1). Using our dataset we train a\nmulti-modal detector (MiRAGe) that improves by +5.1% F-1 over state-of-the-art\nbaselines on image-caption pairs from out-of-domain image generators and news\npublishers. We release our code and data to aid future work on detecting\nAI-generated content.", "published": "2024-10-11 17:58:02", "link": "http://arxiv.org/abs/2410.09045v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "L3Cube-MahaSum: A Comprehensive Dataset and BART Models for Abstractive\n  Text Summarization in Marathi", "abstract": "We present the MahaSUM dataset, a large-scale collection of diverse news\narticles in Marathi, designed to facilitate the training and evaluation of\nmodels for abstractive summarization tasks in Indic languages. The dataset,\ncontaining 25k samples, was created by scraping articles from a wide range of\nonline news sources and manually verifying the abstract summaries.\nAdditionally, we train an IndicBART model, a variant of the BART model tailored\nfor Indic languages, using the MahaSUM dataset. We evaluate the performance of\nour trained models on the task of abstractive summarization and demonstrate\ntheir effectiveness in producing high-quality summaries in Marathi. Our work\ncontributes to the advancement of natural language processing research in Indic\nlanguages and provides a valuable resource for future research in this area\nusing state-of-the-art models. The dataset and models are shared publicly at\nhttps://github.com/l3cube-pune/MarathiNLP", "published": "2024-10-11 18:37:37", "link": "http://arxiv.org/abs/2410.09184v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Long Range Named Entity Recognition for Marathi Documents", "abstract": "The demand for sophisticated natural language processing (NLP) methods,\nparticularly Named Entity Recognition (NER), has increased due to the\nexponential growth of Marathi-language digital content. In particular, NER is\nessential for recognizing distant entities and for arranging and understanding\nunstructured Marathi text data. With an emphasis on managing long-range\nentities, this paper offers a comprehensive analysis of current NER techniques\ndesigned for Marathi documents. It dives into current practices and\ninvestigates the BERT transformer model's potential for long-range Marathi NER.\nAlong with analyzing the effectiveness of earlier methods, the report draws\ncomparisons between NER in English literature and suggests adaptation\nstrategies for Marathi literature. The paper discusses the difficulties caused\nby Marathi's particular linguistic traits and contextual subtleties while\nacknowledging NER's critical role in NLP. To conclude, this project is a major\nstep forward in improving Marathi NER techniques, with potential wider\napplications across a range of NLP tasks and domains.", "published": "2024-10-11 18:48:20", "link": "http://arxiv.org/abs/2410.09192v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "P-FOLIO: Evaluating and Improving Logical Reasoning with Abundant\n  Human-Written Reasoning Chains", "abstract": "Existing methods on understanding the capabilities of LLMs in logical\nreasoning rely on binary entailment classification or synthetically derived\nrationales, which are not sufficient for proper investigation of model's\ncapabilities. We present P-FOLIO, a human-annotated dataset consisting of\ndiverse and complex reasoning chains for a set of realistic logical reasoning\nstories also written by humans. P-FOLIO is collected with an annotation\nprotocol that facilitates humans to annotate well-structured natural language\nproofs for first-order logic reasoning problems in a step-by-step manner. The\nnumber of reasoning steps in P-FOLIO span from 0 to 20. We further use P-FOLIO\nto evaluate and improve large-language-model (LLM) reasoning capabilities. We\nevaluate LLM reasoning capabilities at a fine granularity via single-step\ninference rule classification, with more diverse inference rules of more\ndiverse and higher levels of complexities than previous works. Given that a\nsingle model-generated reasoning chain could take a completely different path\nthan the human-annotated one, we sample multiple reasoning chains from a model\nand use pass@k metrics for evaluating the quality of model-generated reasoning\nchains. We show that human-written reasoning chains significantly boost the\nlogical reasoning capabilities of LLMs via many-shot prompting and fine-tuning.\nFurthermore, fine-tuning Llama3-7B on P-FOLIO improves the model performance by\n10% or more on three other out-of-domain logical reasoning datasets. We also\nconduct detailed analysis to show where most powerful LLMs fall short in\nreasoning. We will release the dataset and code publicly.", "published": "2024-10-11 19:22:57", "link": "http://arxiv.org/abs/2410.09207v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "The Same But Different: Structural Similarities and Differences in\n  Multilingual Language Modeling", "abstract": "We employ new tools from mechanistic interpretability in order to ask whether\nthe internal structure of large language models (LLMs) shows correspondence to\nthe linguistic structures which underlie the languages on which they are\ntrained. In particular, we ask (1) when two languages employ the same\nmorphosyntactic processes, do LLMs handle them using shared internal circuitry?\nand (2) when two languages require different morphosyntactic processes, do LLMs\nhandle them using different internal circuitry? Using English and Chinese\nmultilingual and monolingual models, we analyze the internal circuitry involved\nin two tasks. We find evidence that models employ the same circuit to handle\nthe same syntactic process independently of the language in which it occurs,\nand that this is the case even for monolingual models trained completely\nindependently. Moreover, we show that multilingual models employ\nlanguage-specific components (attention heads and feed-forward networks) when\nneeded to handle linguistic processes (e.g., morphological marking) that only\nexist in some languages. Together, our results provide new insights into how\nLLMs trade off between exploiting common structures and preserving linguistic\ndifferences when tasked with modeling multiple languages simultaneously.", "published": "2024-10-11 19:57:55", "link": "http://arxiv.org/abs/2410.09223v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Semantic Understanding in Speech Language Models via\n  Brain-tuning", "abstract": "Speech language models align with human brain responses to natural language\nto an impressive degree. However, current models rely heavily on low-level\nspeech features, indicating they lack brain-relevant semantics which limits\ntheir utility as model organisms of semantic processing in the brain. In this\nwork, we address this limitation by inducing brain-relevant bias directly into\nthe models via fine-tuning with fMRI recordings of people listening to natural\nstories, a process we name brain-tuning. After testing it on 3 different\npretrained model families, we show that brain-tuning not only improves overall\nalignment with new brain recordings in semantic language regions, but also\nreduces the reliance on low-level speech features for this alignment.\nExcitingly, we further show that brain-tuning leads to 1) consistent\nimprovements in performance on a range of downstream tasks and 2) a\nrepresentational space with increased semantic preference. Our results provide\nconverging evidence, for the first time, that incorporating brain signals into\nthe training of language models improves the models' semantic understanding.", "published": "2024-10-11 20:06:21", "link": "http://arxiv.org/abs/2410.09230v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "nach0-pc: Multi-task Language Model with Molecular Point Cloud Encoder", "abstract": "Recent advancements have integrated Language Models (LMs) into a drug\ndiscovery pipeline. However, existing models mostly work with SMILES and\nSELFIES chemical string representations, which lack spatial features vital for\ndrug discovery. Additionally, attempts to translate chemical 3D structures into\ntext format encounter issues such as excessive length and insufficient atom\nconnectivity information. To address these issues, we introduce nach0-pc, a\nmodel combining domain-specific encoder and textual representation to handle\nspatial arrangement of atoms effectively. Our approach utilizes a molecular\npoint cloud encoder for concise and order-invariant structure representation.\nWe introduce a novel pre-training scheme for molecular point clouds to\ndistillate the knowledge from spatial molecular structures datasets. After\nfine-tuning within both single-task and multi-task frameworks, nach0-pc\ndemonstrates performance comparable with other diffusion models in terms of\ngenerated samples quality across several established spatial molecular\ngeneration tasks. Notably, our model is a multi-task approach, in contrast to\ndiffusion models being limited to single tasks. Additionally, it is capable of\nprocessing point cloud-related data, which language models are not capable of\nhandling due to memory limitations. These lead to our model having reduced\ntraining and inference time while maintaining on par performance.", "published": "2024-10-11 20:30:44", "link": "http://arxiv.org/abs/2410.09240v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Natural Language Counterfactual Explanations for Graphs Using Large\n  Language Models", "abstract": "Explainable Artificial Intelligence (XAI) has emerged as a critical area of\nresearch to unravel the opaque inner logic of (deep) machine learning models.\nAmong the various XAI techniques proposed in the literature, counterfactual\nexplanations stand out as one of the most promising approaches. However, these\n\"what-if\" explanations are frequently complex and technical, making them\ndifficult for non-experts to understand and, more broadly, challenging for\nhumans to interpret. To bridge this gap, in this work, we exploit the power of\nopen-source Large Language Models to generate natural language explanations\nwhen prompted with valid counterfactual instances produced by state-of-the-art\nexplainers for graph-based models. Experiments across several graph datasets\nand counterfactual explainers show that our approach effectively produces\naccurate natural language representations of counterfactual instances, as\ndemonstrated by key performance metrics.", "published": "2024-10-11 23:06:07", "link": "http://arxiv.org/abs/2410.09295v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Exact Byte-Level Probabilities from Tokenized Language Models for\n  FIM-Tasks and Model Ensembles", "abstract": "Tokenization is associated with many poorly understood shortcomings in\nlanguage models (LMs), yet remains an important component for long sequence\nscaling purposes. This work studies how tokenization impacts model performance\nby analyzing and comparing the stochastic behavior of tokenized models with\ntheir byte-level, or token-free, counterparts. We discover that, even when the\ntwo models are statistically equivalent, their predictive distributions over\nthe next byte can be substantially different, a phenomenon we term as\n\"tokenization bias''. To fully characterize this phenomenon, we introduce the\nByte-Token Representation Lemma, a framework that establishes a mapping between\nthe learned token distribution and its equivalent byte-level distribution. From\nthis result, we develop a next-byte sampling algorithm that eliminates\ntokenization bias without requiring further training or optimization. In other\nwords, this enables zero-shot conversion of tokenized LMs into statistically\nequivalent token-free ones. We demonstrate its broad applicability with two use\ncases: fill-in-the-middle (FIM) tasks and model ensembles. In FIM tasks where\ninput prompts may terminate mid-token, leading to out-of-distribution\ntokenization, our method mitigates performance degradation and achieves an\napproximately 18% improvement in FIM coding benchmarks, consistently\noutperforming the standard token healing fix. For model ensembles where each\nmodel employs a distinct vocabulary, our approach enables seamless integration,\nresulting in improved performance (up to 3.7%) over individual models across\nvarious standard baselines in reasoning, knowledge, and coding.", "published": "2024-10-11 23:30:42", "link": "http://arxiv.org/abs/2410.09303v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "JAILJUDGE: A Comprehensive Jailbreak Judge Benchmark with Multi-Agent\n  Enhanced Explanation Evaluation Framework", "abstract": "Despite advancements in enhancing LLM safety against jailbreak attacks,\nevaluating LLM defenses remains a challenge, with current methods often lacking\nexplainability and generalization to complex scenarios, leading to incomplete\nassessments (e.g., direct judgment without reasoning, low F1 score of GPT-4 in\ncomplex cases, bias in multilingual scenarios). To address this, we present\nJAILJUDGE, a comprehensive benchmark featuring diverse risk scenarios,\nincluding synthetic, adversarial, in-the-wild, and multilingual prompts, along\nwith high-quality human-annotated datasets. The JAILJUDGE dataset includes over\n35k+ instruction-tune data with reasoning explainability and JAILJUDGETEST, a\n4.5k+ labeled set for risk scenarios, and a 6k+ multilingual set across ten\nlanguages. To enhance evaluation with explicit reasoning, we propose the\nJailJudge MultiAgent framework, which enables explainable, fine-grained scoring\n(1 to 10). This framework supports the construction of instruction-tuning\nground truth and facilitates the development of JAILJUDGE Guard, an end-to-end\njudge model that provides reasoning and eliminates API costs. Additionally, we\nintroduce JailBoost, an attacker-agnostic attack enhancer, and GuardShield, a\nmoderation defense, both leveraging JAILJUDGE Guard. Our experiments\ndemonstrate the state-of-the-art performance of JailJudge methods (JailJudge\nMultiAgent, JAILJUDGE Guard) across diverse models (e.g., GPT-4, Llama-Guard)\nand zero-shot scenarios. JailBoost and GuardShield significantly improve\njailbreak attack and defense tasks under zero-shot settings, with JailBoost\nenhancing performance by 29.24% and GuardShield reducing defense ASR from\n40.46% to 0.15%.", "published": "2024-10-11 14:56:28", "link": "http://arxiv.org/abs/2410.12855v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Optimized Biomedical Question-Answering Services with LLM and Multi-BERT\n  Integration", "abstract": "We present a refined approach to biomedical question-answering (QA) services\nby integrating large language models (LLMs) with Multi-BERT configurations. By\nenhancing the ability to process and prioritize vast amounts of complex\nbiomedical data, this system aims to support healthcare professionals in\ndelivering better patient outcomes and informed decision-making. Through\ninnovative use of BERT and BioBERT models, combined with a multi-layer\nperceptron (MLP) layer, we enable more specialized and efficient responses to\nthe growing demands of the healthcare sector. Our approach not only addresses\nthe challenge of overfitting by freezing one BERT model while training another\nbut also improves the overall adaptability of QA services. The use of extensive\ndatasets, such as BioASQ and BioMRC, demonstrates the system's ability to\nsynthesize critical information. This work highlights how advanced language\nmodels can make a tangible difference in healthcare, providing reliable and\nresponsive tools for professionals to manage complex information, ultimately\nserving the broader goal of improved care and data-driven insights.", "published": "2024-10-11 17:13:31", "link": "http://arxiv.org/abs/2410.12856v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models for Medical OSCE Assessment: A Novel Approach to\n  Transcript Analysis", "abstract": "Grading Objective Structured Clinical Examinations (OSCEs) is a\ntime-consuming and expensive process, traditionally requiring extensive manual\neffort from human experts. In this study, we explore the potential of Large\nLanguage Models (LLMs) to assess skills related to medical student\ncommunication. We analyzed 2,027 video-recorded OSCE examinations from the\nUniversity of Texas Southwestern Medical Center (UTSW), spanning four years\n(2019-2022), and several different medical cases or \"stations.\" Specifically,\nour focus was on evaluating students' ability to summarize patients' medical\nhistory: we targeted the rubric item 'did the student summarize the patients'\nmedical history?' from the communication skills rubric. After transcribing\nspeech audio captured by OSCE videos using Whisper-v3, we studied the\nperformance of various LLM-based approaches for grading students on this\nsummarization task based on their examination transcripts. Using various\nfrontier-level open-source and proprietary LLMs, we evaluated different\ntechniques such as zero-shot chain-of-thought prompting, retrieval augmented\ngeneration, and multi-model ensemble methods. Our results show that frontier\nLLM models like GPT-4 achieved remarkable alignment with human graders,\ndemonstrating a Cohen's kappa agreement of 0.88 and indicating strong potential\nfor LLM-based OSCE grading to augment the current grading process. Open-source\nmodels also showed promising results, suggesting potential for widespread,\ncost-effective deployment. Further, we present a failure analysis identifying\nconditions where LLM grading may be less reliable in this context and recommend\nbest practices for deploying LLMs in medical education settings.", "published": "2024-10-11 19:16:03", "link": "http://arxiv.org/abs/2410.12858v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLMD: A Large Language Model for Interpreting Longitudinal Medical\n  Records", "abstract": "We introduce LLMD, a large language model designed to analyze a patient's\nmedical history based on their medical records. Along with domain knowledge,\nLLMD is trained on a large corpus of records collected over time and across\nfacilities, as well as tasks and labels that make nuanced connections among\nthem. This approach is critical to an accurate picture of patient health, and\nhas distinctive advantages over models trained on knowledge alone, unlabeled\nrecords, structured EHR data, or records from a single health system.\n  The recipe for LLMD continues pretraining a foundational model on both domain\nknowledge and the contents of millions of records. These span an average of 10\nyears of care and as many as 140 care sites per patient. LLMD is then\ninstruction fine-tuned on structuring and abstraction tasks. The former jointly\nidentify and normalize document metadata, provenance information, clinical\nnamed-entities, and ontology mappings, while the latter roll these into\nhigher-level representations, such a continuous era of time a patient was on a\nmedication. LLMD is deployed within a layered validation system that includes\ncontinual random audits and review by experts, e.g. based on uncertainty,\ndisease-specific rules, or use-case.\n  LLMD exhibits large gains over both more-powerful generalized models and\ndomain-specific models. On medical knowledge benchmarks, LLMD-8B achieves state\nof the art accuracy on PubMedQA text responses, besting orders-of-magnitude\nlarger models. On production tasks, we show that LLMD significantly outperforms\nall other models evaluated, and among alternatives, large general purpose LLMs\nlike GPT-4o are more accurate than models emphasizing medical knowledge. We\nfind strong evidence that accuracy on today's medical benchmarks is not the\nmost significant factor when analyzing real-world patient data, an insight with\nimplications for future medical LLMs.'", "published": "2024-10-11 20:55:51", "link": "http://arxiv.org/abs/2410.12860v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Observing the Southern US Culture of Honor Using Large-Scale Social\n  Media Analysis", "abstract": "A \\textit{culture of honor} refers to a social system where individuals'\nstatus, reputation, and esteem play a central role in governing interpersonal\nrelations. Past works have associated this concept with the United States (US)\nSouth and related with it various traits such as higher sensitivity to insult,\na higher value on reputation, and a tendency to react violently to insults. In\nthis paper, we hypothesize and confirm that internet users from the US South,\nwhere a \\textit{culture of honor} is more prevalent, are more likely to display\na trait predicted by their belonging to a \\textit{culture of honor}.\nSpecifically, we test the hypothesis that US Southerners are more likely to\nretaliate to personal attacks by personally attacking back. We leverage\nOpenAI's GPT-3.5 API to both geolocate internet users and to automatically\ndetect whether users are insulting each other. We validate the use of GPT-3.5\nby measuring its performance on manually-labeled subsets of the data. Our work\ndemonstrates the potential of formulating a hypothesis based on a conceptual\nframework, operationalizing it in a way that is amenable to large-scale\nLLM-aided analysis, manually validating the use of the LLM, and drawing a\nconclusion.", "published": "2024-10-11 14:51:27", "link": "http://arxiv.org/abs/2410.13887v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Semantic Token Reweighting for Interpretable and Controllable Text\n  Embeddings in CLIP", "abstract": "A text encoder within Vision-Language Models (VLMs) like CLIP plays a crucial\nrole in translating textual input into an embedding space shared with images,\nthereby facilitating the interpretative analysis of vision tasks through\nnatural language. Despite the varying significance of different textual\nelements within a sentence depending on the context, efforts to account for\nvariation of importance in constructing text embeddings have been lacking. We\npropose a framework of Semantic Token Reweighting to build Interpretable text\nembeddings (SToRI), which incorporates controllability as well. SToRI refines\nthe text encoding process in CLIP by differentially weighting semantic elements\nbased on contextual importance, enabling finer control over emphasis responsive\nto data-driven insights and user preferences. The efficacy of SToRI is\ndemonstrated through comprehensive experiments on few-shot image classification\nand image retrieval tailored to user preferences.", "published": "2024-10-11 02:42:13", "link": "http://arxiv.org/abs/2410.08469v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Improving Legal Entity Recognition Using a Hybrid Transformer Model and\n  Semantic Filtering Approach", "abstract": "Legal Entity Recognition (LER) is critical in automating legal workflows such\nas contract analysis, compliance monitoring, and litigation support. Existing\napproaches, including rule-based systems and classical machine learning models,\nstruggle with the complexity of legal documents and domain specificity,\nparticularly in handling ambiguities and nested entity structures. This paper\nproposes a novel hybrid model that enhances the accuracy and precision of\nLegal-BERT, a transformer model fine-tuned for legal text processing, by\nintroducing a semantic similarity-based filtering mechanism. We evaluate the\nmodel on a dataset of 15,000 annotated legal documents, achieving an F1 score\nof 93.4%, demonstrating significant improvements in precision and recall over\nprevious methods.", "published": "2024-10-11 04:51:28", "link": "http://arxiv.org/abs/2410.08521v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "\"I Am the One and Only, Your Cyber BFF\": Understanding the Impact of\n  GenAI Requires Understanding the Impact of Anthropomorphic AI", "abstract": "Many state-of-the-art generative AI (GenAI) systems are increasingly prone to\nanthropomorphic behaviors, i.e., to generating outputs that are perceived to be\nhuman-like. While this has led to scholars increasingly raising concerns about\npossible negative impacts such anthropomorphic AI systems can give rise to,\nanthropomorphism in AI development, deployment, and use remains vastly\noverlooked, understudied, and underspecified. In this perspective, we argue\nthat we cannot thoroughly map the social impacts of generative AI without\nmapping the social impacts of anthropomorphic AI, and outline a call to action.", "published": "2024-10-11 04:57:41", "link": "http://arxiv.org/abs/2410.08526v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Scaling Laws for Predicting Downstream Performance in LLMs", "abstract": "Precise estimation of downstream performance in large language models (LLMs)\nprior to training is essential for guiding their development process. Scaling\nlaws analysis utilizes the statistics of a series of significantly smaller\nsampling language models (LMs) to predict the performance of the target LLM.\nFor downstream performance prediction, the critical challenge lies in the\nemergent abilities in LLMs that occur beyond task-specific computational\nthresholds. In this work, we focus on the pre-training loss as a more\ncomputation-efficient metric for performance estimation. Our two-stage approach\nFLP consists of first estimating a function that maps computational resources\n(e.g., FLOPs) to the pre-training Loss using a series of fully-converged\nsampling models, followed by mapping the pre-training loss to downstream task\nPerformance using the intermediate models with emerged performance. In our\nexperiments, this FLP solution accurately predicts the performance of LLMs with\n7B and 13B parameters using a series of sampling LMs up to 3B, achieving error\nmargins of 5% and 10%, respectively, and significantly outperforming the\nFLOPs-to-Performance approach. Further, we present FLP-M, a fundamental\napproach for performance prediction that addresses the practical need to\nintegrate datasets from multiple sources during pre-training. FLP-M extends the\npower law analytical function to predict domain-specific pre-training loss\nbased on FLOPs across data sources, and employs a two-layer neural network to\nmodel the non-linear relationship between multiple domain-specific loss and\ndownstream performance. By utilizing a 3B LLM trained on a specific ratio and a\nseries of smaller sampling LMs, FLP-M can effectively forecast the performance\nof 3B and 7B LLMs across various data mixtures for most benchmarks within 10%\nerror margins.", "published": "2024-10-11 04:57:48", "link": "http://arxiv.org/abs/2410.08527v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Balancing Innovation and Privacy: Data Security Strategies in Natural\n  Language Processing Applications", "abstract": "This research addresses privacy protection in Natural Language Processing\n(NLP) by introducing a novel algorithm based on differential privacy, aimed at\nsafeguarding user data in common applications such as chatbots, sentiment\nanalysis, and machine translation. With the widespread application of NLP\ntechnology, the security and privacy protection of user data have become\nimportant issues that need to be solved urgently. This paper proposes a new\nprivacy protection algorithm designed to effectively prevent the leakage of\nuser sensitive information. By introducing a differential privacy mechanism,\nour model ensures the accuracy and reliability of data analysis results while\nadding random noise. This method not only reduces the risk caused by data\nleakage but also achieves effective processing of data while protecting user\nprivacy. Compared to traditional privacy methods like data anonymization and\nhomomorphic encryption, our approach offers significant advantages in terms of\ncomputational efficiency and scalability while maintaining high accuracy in\ndata analysis. The proposed algorithm's efficacy is demonstrated through\nperformance metrics such as accuracy (0.89), precision (0.85), and recall\n(0.88), outperforming other methods in balancing privacy and utility. As\nprivacy protection regulations become increasingly stringent, enterprises and\ndevelopers must take effective measures to deal with privacy risks. Our\nresearch provides an important reference for the application of privacy\nprotection technology in the field of NLP, emphasizing the need to achieve a\nbalance between technological innovation and user privacy. In the future, with\nthe continuous advancement of technology, privacy protection will become a core\nelement of data-driven applications and promote the healthy development of the\nentire industry.", "published": "2024-10-11 06:05:10", "link": "http://arxiv.org/abs/2410.08553v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Baichuan-Omni Technical Report", "abstract": "The salient multimodal capabilities and interactive experience of GPT-4o\nhighlight its critical role in practical applications, yet it lacks a\nhigh-performing open-source counterpart. In this paper, we introduce\nBaichuan-omni, the first open-source 7B Multimodal Large Language Model (MLLM)\nadept at concurrently processing and analyzing modalities of image, video,\naudio, and text, while delivering an advanced multimodal interactive experience\nand strong performance. We propose an effective multimodal training schema\nstarting with 7B model and proceeding through two stages of multimodal\nalignment and multitask fine-tuning across audio, image, video, and text modal.\nThis approach equips the language model with the ability to handle visual and\naudio data effectively. Demonstrating strong performance across various\nomni-modal and multimodal benchmarks, we aim for this contribution to serve as\na competitive baseline for the open-source community in advancing multimodal\nunderstanding and real-time interaction.", "published": "2024-10-11 06:44:31", "link": "http://arxiv.org/abs/2410.08565v4", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Words as Beacons: Guiding RL Agents with High-Level Language Prompts", "abstract": "Sparse reward environments in reinforcement learning (RL) pose significant\nchallenges for exploration, often leading to inefficient or incomplete learning\nprocesses. To tackle this issue, this work proposes a teacher-student RL\nframework that leverages Large Language Models (LLMs) as \"teachers\" to guide\nthe agent's learning process by decomposing complex tasks into subgoals. Due to\ntheir inherent capability to understand RL environments based on a textual\ndescription of structure and purpose, LLMs can provide subgoals to accomplish\nthe task defined for the environment in a similar fashion to how a human would\ndo. In doing so, three types of subgoals are proposed: positional targets\nrelative to the agent, object representations, and language-based instructions\ngenerated directly by the LLM. More importantly, we show that it is possible to\nquery the LLM only during the training phase, enabling agents to operate within\nthe environment without any LLM intervention. We assess the performance of this\nproposed framework by evaluating three state-of-the-art open-source LLMs\n(Llama, DeepSeek, Qwen) eliciting subgoals across various procedurally\ngenerated environment of the MiniGrid benchmark. Experimental results\ndemonstrate that this curriculum-based approach accelerates learning and\nenhances exploration in complex tasks, achieving up to 30 to 200 times faster\nconvergence in training steps compared to recent baselines designed for sparse\nreward environments.", "published": "2024-10-11 08:54:45", "link": "http://arxiv.org/abs/2410.08632v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "More than Memes: A Multimodal Topic Modeling Approach to Conspiracy\n  Theories on Telegram", "abstract": "To address the increasing prevalence of (audio-)visual data on social media,\nand to capture the evolving and dynamic nature of this communication,\nresearchers have begun to explore the potential of unsupervised approaches for\nanalyzing multimodal online content. However, existing research often neglects\nvisual content beyond memes, and in addition lacks methods to compare topic\nmodels across modalities. Our study addresses these gaps by applying multimodal\ntopic modeling for analyzing conspiracy theories in German-language Telegram\nchannels. We use BERTopic with CLIP for the analysis of textual and visual data\nin a corpus of ~40, 000 Telegram messages posted in October 2023 in 571\nGerman-language Telegram channels known for disseminating conspiracy theories.\nThrough this dataset, we provide insights into unimodal and multimodal topic\nmodels by analyzing symmetry and intersections of topics across modalities. We\ndemonstrate the variety of textual and visual content shared in the channels\ndiscovered through the topic modeling, and propose a conceptual framework for\nthe analysis of textual and visual discursive strategies in the communication\nof conspiracy theories. We apply the framework in a case study of the topic\ngroup Israel Gaza.", "published": "2024-10-11 09:10:26", "link": "http://arxiv.org/abs/2410.08642v2", "categories": ["cs.SI", "cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.SI"}
{"title": "Integrating Supertag Features into Neural Discontinuous Constituent\n  Parsing", "abstract": "Syntactic parsing is essential in natural-language processing, with\nconstituent structure being one widely used description of syntax. Traditional\nviews of constituency demand that constituents consist of adjacent words, but\nthis poses challenges in analysing syntax with non-local dependencies, common\nin languages like German. Therefore, in a number of treebanks like NeGra and\nTIGER for German and DPTB for English, long-range dependencies are represented\nby crossing edges. Various grammar formalisms have been used to describe\ndiscontinuous trees - often with high time complexities for parsing.\nTransition-based parsing aims at reducing this factor by eliminating the need\nfor an explicit grammar. Instead, neural networks are trained to produce trees\ngiven raw text input using supervised learning on large annotated corpora. An\nelegant proposal for a stack-free transition-based parser developed by Coavoux\nand Cohen (2019) successfully allows for the derivation of any discontinuous\nconstituent tree over a sentence in worst-case quadratic time.\n  The purpose of this work is to explore the introduction of supertag\ninformation into transition-based discontinuous constituent parsing. In\nlexicalised grammar formalisms like CCG (Steedman, 1989) informative categories\nare assigned to the words in a sentence and act as the building blocks for\ncomposing the sentence's syntax. These supertags indicate a word's structural\nrole and syntactic relationship with surrounding items. The study examines\nincorporating supertag information by using a dedicated supertagger as\nadditional input for a neural parser (pipeline) and by jointly training a\nneural model for both parsing and supertagging (multi-task). In addition to\nCCG, several other frameworks (LTAG-spinal, LCFRS) and sequence labelling tasks\n(chunking, dependency parsing) will be compared in terms of their suitability\nas auxiliary tasks for parsing.", "published": "2024-10-11 12:28:26", "link": "http://arxiv.org/abs/2410.08766v1", "categories": ["cs.CL", "cs.AI", "cs.FL"], "primary_category": "cs.CL"}
{"title": "PoisonBench: Assessing Large Language Model Vulnerability to Data\n  Poisoning", "abstract": "Preference learning is a central component for aligning current LLMs, but\nthis process can be vulnerable to data poisoning attacks. To address this\nconcern, we introduce PoisonBench, a benchmark for evaluating large language\nmodels' susceptibility to data poisoning during preference learning. Data\npoisoning attacks can manipulate large language model responses to include\nhidden malicious content or biases, potentially causing the model to generate\nharmful or unintended outputs while appearing to function normally. We deploy\ntwo distinct attack types across eight realistic scenarios, assessing 21\nwidely-used models. Our findings reveal concerning trends: (1) Scaling up\nparameter size does not inherently enhance resilience against poisoning\nattacks; (2) There exists a log-linear relationship between the effects of the\nattack and the data poison ratio; (3) The effect of data poisoning can\ngeneralize to extrapolated triggers that are not included in the poisoned data.\nThese results expose weaknesses in current preference learning techniques,\nhighlighting the urgent need for more robust defenses against malicious models\nand data manipulation.", "published": "2024-10-11 13:50:50", "link": "http://arxiv.org/abs/2410.08811v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Enhancing Indonesian Automatic Speech Recognition: Evaluating\n  Multilingual Models with Diverse Speech Variabilities", "abstract": "An ideal speech recognition model has the capability to transcribe speech\naccurately under various characteristics of speech signals, such as speaking\nstyle (read and spontaneous), speech context (formal and informal), and\nbackground noise conditions (clean and moderate). Building such a model\nrequires a significant amount of training data with diverse speech\ncharacteristics. Currently, Indonesian data is dominated by read, formal, and\nclean speech, leading to a scarcity of Indonesian data with other speech\nvariabilities. To develop Indonesian automatic speech recognition (ASR), we\npresent our research on state-of-the-art speech recognition models, namely\nMassively Multilingual Speech (MMS) and Whisper, as well as compiling a dataset\ncomprising Indonesian speech with variabilities to facilitate our study. We\nfurther investigate the models' predictive ability to transcribe Indonesian\nspeech data across different variability groups. The best results were achieved\nby the Whisper fine-tuned model across datasets with various characteristics,\nas indicated by the decrease in word error rate (WER) and character error rate\n(CER). Moreover, we found that speaking style variability affected model\nperformance the most.", "published": "2024-10-11 14:07:07", "link": "http://arxiv.org/abs/2410.08828v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Unintentional Unalignment: Likelihood Displacement in Direct Preference\n  Optimization", "abstract": "Direct Preference Optimization (DPO) and its variants are increasingly used\nfor aligning language models with human preferences. Although these methods are\ndesigned to teach a model to generate preferred responses more frequently\nrelative to dispreferred responses, prior work has observed that the likelihood\nof preferred responses often decreases during training. The current work sheds\nlight on the causes and implications of this counter-intuitive phenomenon,\nwhich we term likelihood displacement. We demonstrate that likelihood\ndisplacement can be catastrophic, shifting probability mass from preferred\nresponses to responses with an opposite meaning. As a simple example, training\na model to prefer $\\texttt{No}$ over $\\texttt{Never}$ can sharply increase the\nprobability of $\\texttt{Yes}$. Moreover, when aligning the model to refuse\nunsafe prompts, we show that such displacement can unintentionally lead to\nunalignment, by shifting probability mass from preferred refusal responses to\nharmful responses (e.g., reducing the refusal rate of Llama-3-8B-Instruct from\n74.4% to 33.4%). We theoretically characterize that likelihood displacement is\ndriven by preferences that induce similar embeddings, as measured by a centered\nhidden embedding similarity (CHES) score. Empirically, the CHES score enables\nidentifying which training samples contribute most to likelihood displacement\nin a given dataset. Filtering out these samples effectively mitigated\nunintentional unalignment in our experiments. More broadly, our results\nhighlight the importance of curating data with sufficiently distinct\npreferences, for which we believe the CHES score may prove valuable.", "published": "2024-10-11 14:22:44", "link": "http://arxiv.org/abs/2410.08847v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Towards Multilingual LLM Evaluation for European Languages", "abstract": "The rise of Large Language Models (LLMs) has revolutionized natural language\nprocessing across numerous languages and tasks. However, evaluating LLM\nperformance in a consistent and meaningful way across multiple European\nlanguages remains challenging, especially due to the scarcity of\nlanguage-parallel multilingual benchmarks. We introduce a multilingual\nevaluation approach tailored for European languages. We employ translated\nversions of five widely-used benchmarks to assess the capabilities of 40 LLMs\nacross 21 European languages. Our contributions include examining the\neffectiveness of translated benchmarks, assessing the impact of different\ntranslation services, and offering a multilingual evaluation framework for LLMs\nthat includes newly created datasets: EU20-MMLU, EU20-HellaSwag, EU20-ARC,\nEU20-TruthfulQA, and EU20-GSM8K. The benchmarks and results are made publicly\navailable to encourage further research in multilingual LLM evaluation.", "published": "2024-10-11 15:53:24", "link": "http://arxiv.org/abs/2410.08928v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents", "abstract": "The robustness of LLMs to jailbreak attacks, where users design prompts to\ncircumvent safety measures and misuse model capabilities, has been studied\nprimarily for LLMs acting as simple chatbots. Meanwhile, LLM agents -- which\nuse external tools and can execute multi-stage tasks -- may pose a greater risk\nif misused, but their robustness remains underexplored. To facilitate research\non LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark\nincludes a diverse set of 110 explicitly malicious agent tasks (440 with\naugmentations), covering 11 harm categories including fraud, cybercrime, and\nharassment. In addition to measuring whether models refuse harmful agentic\nrequests, scoring well on AgentHarm requires jailbroken agents to maintain\ntheir capabilities following an attack to complete a multi-step task. We\nevaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly\ncompliant with malicious agent requests without jailbreaking, (2) simple\nuniversal jailbreak templates can be adapted to effectively jailbreak agents,\nand (3) these jailbreaks enable coherent and malicious multi-step agent\nbehavior and retain model capabilities. To enable simple and reliable\nevaluation of attacks and defenses for LLM-based agents, we publicly release\nAgentHarm at https://huggingface.co/datasets/ai-safety-institute/AgentHarm.", "published": "2024-10-11 17:39:22", "link": "http://arxiv.org/abs/2410.09024v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "PEAR: A Robust and Flexible Automation Framework for Ptychography\n  Enabled by Multiple Large Language Model Agents", "abstract": "Ptychography is an advanced computational imaging technique in X-ray and\nelectron microscopy. It has been widely adopted across scientific research\nfields, including physics, chemistry, biology, and materials science, as well\nas in industrial applications such as semiconductor characterization. In\npractice, obtaining high-quality ptychographic images requires simultaneous\noptimization of numerous experimental and algorithmic parameters.\nTraditionally, parameter selection often relies on trial and error, leading to\nlow-throughput workflows and potential human bias. In this work, we develop the\n\"Ptychographic Experiment and Analysis Robot\" (PEAR), a framework that\nleverages large language models (LLMs) to automate data analysis in\nptychography. To ensure high robustness and accuracy, PEAR employs multiple LLM\nagents for tasks including knowledge retrieval, code generation, parameter\nrecommendation, and image reasoning. Our study demonstrates that PEAR's\nmulti-agent design significantly improves the workflow success rate, even with\nsmaller open-weight models such as LLaMA 3.1 8B. PEAR also supports various\nautomation levels and is designed to work with customized local knowledge\nbases, ensuring flexibility and adaptability across different research\nenvironments.", "published": "2024-10-11 17:50:59", "link": "http://arxiv.org/abs/2410.09034v1", "categories": ["cs.CE", "cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.CE"}
{"title": "Unraveling and Mitigating Safety Alignment Degradation of\n  Vision-Language Models", "abstract": "The safety alignment ability of Vision-Language Models (VLMs) is prone to be\ndegraded by the integration of the vision module compared to its LLM backbone.\nWe investigate this phenomenon, dubbed as ''safety alignment degradation'' in\nthis paper, and show that the challenge arises from the representation gap that\nemerges when introducing vision modality to VLMs. In particular, we show that\nthe representations of multi-modal inputs shift away from that of text-only\ninputs which represent the distribution that the LLM backbone is optimized for.\nAt the same time, the safety alignment capabilities, initially developed within\nthe textual embedding space, do not successfully transfer to this new\nmulti-modal representation space. To reduce safety alignment degradation, we\nintroduce Cross-Modality Representation Manipulation (CMRM), an inference time\nrepresentation intervention method for recovering the safety alignment ability\nthat is inherent in the LLM backbone of VLMs, while simultaneously preserving\nthe functional capabilities of VLMs. The empirical results show that our\nframework significantly recovers the alignment ability that is inherited from\nthe LLM backbone with minimal impact on the fluency and linguistic capabilities\nof pre-trained VLMs even without additional training. Specifically, the unsafe\nrate of LLaVA-7B on multi-modal input can be reduced from 61.53% to as low as\n3.15% with only inference-time intervention.\n  WARNING: This paper contains examples of toxic or harmful language.", "published": "2024-10-11 17:59:31", "link": "http://arxiv.org/abs/2410.09047v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "nextlocllm: next location prediction using LLMs", "abstract": "Next location prediction is a critical task in human mobility analysis and\nserves as a foundation for various downstream applications. Existing methods\ntypically rely on discrete IDs to represent locations, which inherently\noverlook spatial relationships and cannot generalize across cities. In this\npaper, we propose NextLocLLM, which leverages the advantages of large language\nmodels (LLMs) in processing natural language descriptions and their strong\ngeneralization capabilities for next location prediction. Specifically, instead\nof using IDs, NextLocLLM encodes locations based on continuous spatial\ncoordinates to better model spatial relationships. These coordinates are\nfurther normalized to enable robust cross-city generalization. Another\nhighlight of NextlocLLM is its LLM-enhanced POI embeddings. It utilizes LLMs'\nability to encode each POI category's natural language description into\nembeddings. These embeddings are then integrated via nonlinear projections to\nform this LLM-enhanced POI embeddings, effectively capturing locations'\nfunctional attributes. Furthermore, task and data prompt prefix, together with\ntrajectory embeddings, are incorporated as input for partly-frozen LLM\nbackbone. NextLocLLM further introduces prediction retrieval module to ensure\nstructural consistency in prediction. Experiments show that NextLocLLM\noutperforms existing models in next location prediction, excelling in both\nsupervised and zero-shot settings.", "published": "2024-10-11 10:59:14", "link": "http://arxiv.org/abs/2410.09129v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ACER: Automatic Language Model Context Extension via Retrieval", "abstract": "Long-context modeling is one of the critical capabilities of language AI for\ndigesting and reasoning over complex information pieces. In practice,\nlong-context capabilities are typically built into a pre-trained language\nmodel~(LM) through a carefully designed context extension stage, with the goal\nof producing generalist long-context capabilities. In our preliminary\nexperiments, however, we discovered that the current open-weight generalist\nlong-context models are still lacking in practical long-context processing\ntasks. While this means perfectly effective long-context modeling demands\ntask-specific data, the cost can be prohibitive. In this paper, we draw\ninspiration from how humans process a large body of information: a lossy\n\\textbf{retrieval} stage ranks a large set of documents while the reader ends\nup reading deeply only the top candidates. We build an \\textbf{automatic} data\nsynthesis pipeline that mimics this process using short-context LMs. The\nshort-context LMs are further tuned using these self-generated data to obtain\ntask-specific long-context capabilities. Similar to how pre-training learns\nfrom imperfect data, we hypothesize and further demonstrate that the\nshort-context model can bootstrap over the synthetic data, outperforming not\nonly long-context generalist models but also the retrieval and read pipeline\nused to synthesize the training data in real-world tasks such as long-context\nretrieval augmented generation.", "published": "2024-10-11 17:57:06", "link": "http://arxiv.org/abs/2410.09141v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automated Rewards via LLM-Generated Progress Functions", "abstract": "Large Language Models (LLMs) have the potential to automate reward\nengineering by leveraging their broad domain knowledge across various tasks.\nHowever, they often need many iterations of trial-and-error to generate\neffective reward functions. This process is costly because evaluating every\nsampled reward function requires completing the full policy optimization\nprocess for each function. In this paper, we introduce an LLM-driven reward\ngeneration framework that is able to produce state-of-the-art policies on the\nchallenging Bi-DexHands benchmark with 20x fewer reward function samples than\nthe prior state-of-the-art work. Our key insight is that we reduce the problem\nof generating task-specific rewards to the problem of coarsely estimating task\nprogress. Our two-step solution leverages the task domain knowledge and the\ncode synthesis abilities of LLMs to author progress functions that estimate\ntask progress from a given state. Then, we use this notion of progress to\ndiscretize states, and generate count-based intrinsic rewards using the\nlow-dimensional state space. We show that the combination of LLM-generated\nprogress functions and count-based intrinsic rewards is essential for our\nperformance gains, while alternatives such as generic hash-based counts or\nusing progress directly as a reward function fall short.", "published": "2024-10-11 18:41:15", "link": "http://arxiv.org/abs/2410.09187v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Encoding Agent Trajectories as Representations with Sequence\n  Transformers", "abstract": "Spatiotemporal data faces many analogous challenges to natural language text\nincluding the ordering of locations (words) in a sequence, long range\ndependencies between locations, and locations having multiple meanings. In this\nwork, we propose a novel model for representing high dimensional spatiotemporal\ntrajectories as sequences of discrete locations and encoding them with a\nTransformer-based neural network architecture. Similar to language models, our\nSequence Transformer for Agent Representation Encodings (STARE) model can learn\nrepresentations and structure in trajectory data through both supervisory tasks\n(e.g., classification), and self-supervisory tasks (e.g., masked modelling). We\npresent experimental results on various synthetic and real trajectory datasets\nand show that our proposed model can learn meaningful encodings that are useful\nfor many downstream tasks including discriminating between labels and\nindicating similarity between locations. Using these encodings, we also learn\nrelationships between agents and locations present in spatiotemporal data.", "published": "2024-10-11 19:18:47", "link": "http://arxiv.org/abs/2410.09204v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "M3Hop-CoT: Misogynous Meme Identification with Multimodal Multi-hop\n  Chain-of-Thought", "abstract": "In recent years, there has been a significant rise in the phenomenon of hate\nagainst women on social media platforms, particularly through the use of\nmisogynous memes. These memes often target women with subtle and obscure cues,\nmaking their detection a challenging task for automated systems. Recently,\nLarge Language Models (LLMs) have shown promising results in reasoning using\nChain-of-Thought (CoT) prompting to generate the intermediate reasoning chains\nas the rationale to facilitate multimodal tasks, but often neglect cultural\ndiversity and key aspects like emotion and contextual knowledge hidden in the\nvisual modalities. To address this gap, we introduce a Multimodal Multi-hop CoT\n(M3Hop-CoT) framework for Misogynous meme identification, combining a\nCLIP-based classifier and a multimodal CoT module with\nentity-object-relationship integration. M3Hop-CoT employs a three-step\nmultimodal prompting principle to induce emotions, target awareness, and\ncontextual knowledge for meme analysis. Our empirical evaluation, including\nboth qualitative and quantitative analysis, validates the efficacy of the\nM3Hop-CoT framework on the SemEval-2022 Task 5 (MAMI task) dataset,\nhighlighting its strong performance in the macro-F1 score. Furthermore, we\nevaluate the model's generalizability by evaluating it on various benchmark\nmeme datasets, offering a thorough insight into the effectiveness of our\napproach across different datasets.", "published": "2024-10-11 19:50:53", "link": "http://arxiv.org/abs/2410.09220v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Benchmark Inflation: Revealing LLM Performance Gaps Using Retro-Holdouts", "abstract": "The training data for many Large Language Models (LLMs) is contaminated with\ntest data. This means that public benchmarks used to assess LLMs are\ncompromised, suggesting a performance gap between benchmark scores and actual\ncapabilities. Ideally, a private holdout set could be used to accurately verify\nscores. Unfortunately, such datasets do not exist for most benchmarks, and\npost-hoc construction of sufficiently similar datasets is non-trivial. To\naddress these issues, we introduce a systematic methodology for (i)\nretrospectively constructing a holdout dataset for a target dataset, (ii)\ndemonstrating the statistical indistinguishability of this retro-holdout\ndataset, and (iii) comparing LLMs on the two datasets to quantify the\nperformance gap due to the dataset's public availability. Applying these\nmethods to TruthfulQA, we construct and release Retro-Misconceptions, on which\nwe evaluate twenty LLMs and find that some have inflated scores by as much as\n16 percentage points. Our results demonstrate that public benchmark scores do\nnot always accurately assess model properties, and underscore the importance of\nimproved data practices in the field.", "published": "2024-10-11 20:46:56", "link": "http://arxiv.org/abs/2410.09247v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ReasonPlanner: Enhancing Autonomous Planning in Dynamic Environments\n  with Temporal Knowledge Graphs and LLMs", "abstract": "Planning and performing interactive tasks, such as conducting experiments to\ndetermine the melting point of an unknown substance, is straightforward for\nhumans but poses significant challenges for autonomous agents. We introduce\nReasonPlanner, a novel generalist agent designed for reflective thinking,\nplanning, and interactive reasoning. This agent leverages LLMs to plan\nhypothetical trajectories by building a World Model based on a Temporal\nKnowledge Graph. The agent interacts with the environment using a natural\nlanguage actor-critic module, where the actor translates the imagined\ntrajectory into a sequence of actionable steps, and the critic determines if\nreplanning is necessary. ReasonPlanner significantly outperforms previous\nstate-of-the-art prompting-based methods on the ScienceWorld benchmark by more\nthan 1.8 times, while being more sample-efficient and interpretable. It relies\nsolely on frozen weights thus requiring no gradient updates. ReasonPlanner can\nbe deployed and utilized without specialized knowledge of Machine Learning,\nmaking it accessible to a wide range of users.", "published": "2024-10-11 20:58:51", "link": "http://arxiv.org/abs/2410.09252v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Nudging: Inference-time Alignment via Model Collaboration", "abstract": "Large language models (LLMs) require alignment, such as instruction-tuning or\nreinforcement learning from human feedback, to effectively and safely follow\nuser instructions. This process necessitates training aligned versions for\nevery model size in each model family, resulting in significant computational\noverhead. In this work, we propose nudging, a simple, plug-and-play, and\ntraining-free algorithm that aligns any base model at inference time using a\nsmall aligned model. Nudging is motivated by recent findings that alignment\nprimarily alters the model's behavior on a small subset of stylistic tokens,\nsuch as \"Sure\" or \"Thank\". We find that base models are significantly more\nuncertain when generating these tokens. Leveraging this observation, nudging\nemploys a small aligned model to generate nudging tokens to steer the large\nbase model's output toward desired directions when the base model's uncertainty\nis high. We evaluate the effectiveness of nudging across 3 model families and\n13 tasks, covering reasoning, general knowledge, instruction following, and\nsafety benchmarks. Without any additional training, nudging a large base model\nwith a 7x - 14x smaller aligned model achieves zero-shot performance comparable\nto, and sometimes surpassing, that of large aligned models. For example,\nnudging OLMo-7b with OLMo-1b-instruct, affecting less than 9% of tokens,\nachieves a 10% absolute improvement on GSM8K over OLMo-7b-instruct. Unlike\nprior inference-time tuning methods, nudging enables off-the-shelf\ncollaboration between model families. For instance, nudging Gemma-2-27b with\nLlama-2-7b-chat outperforms Llama-2-70b-chat on various tasks. Overall, this\nwork introduces a simple yet powerful approach to token-level model\ncollaboration, offering a modular solution to LLM alignment. Our project\nwebsite: https://fywalter.github.io/nudging/ .", "published": "2024-10-11 23:24:38", "link": "http://arxiv.org/abs/2410.09300v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Multi-Step Reasoning Abilities of Language Models through\n  Direct Q-Function Optimization", "abstract": "Reinforcement Learning (RL) plays a crucial role in aligning large language\nmodels (LLMs) with human preferences and improving their ability to perform\ncomplex tasks. However, current approaches either require significant\ncomputational resources due to the use of multiple models and extensive online\nsampling for training (e.g., PPO) or are framed as bandit problems (e.g., DPO,\nDRO), which often struggle with multi-step reasoning tasks, such as math\nproblem solving and complex reasoning that involve long chains of thought. To\novercome these limitations, we introduce Direct Q-function Optimization (DQO),\nwhich formulates the response generation process as a Markov Decision Process\n(MDP) and utilizes the soft actor-critic (SAC) framework to optimize a\nQ-function directly parameterized by the language model. The MDP formulation of\nDQO offers structural advantages over bandit-based methods, enabling more\neffective process supervision. Experimental results on two math problem-solving\ndatasets, GSM8K and MATH, demonstrate that DQO outperforms previous methods,\nestablishing it as a promising offline reinforcement learning approach for\naligning language models.", "published": "2024-10-11 23:29:20", "link": "http://arxiv.org/abs/2410.09302v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Enterprise Benchmarks for Large Language Model Evaluation", "abstract": "The advancement of large language models (LLMs) has led to a greater\nchallenge of having a rigorous and systematic evaluation of complex tasks\nperformed, especially in enterprise applications. Therefore, LLMs need to be\nable to benchmark enterprise datasets for various tasks. This work presents a\nsystematic exploration of benchmarking strategies tailored to LLM evaluation,\nfocusing on the utilization of domain-specific datasets and consisting of a\nvariety of NLP tasks. The proposed evaluation framework encompasses 25 publicly\navailable datasets from diverse enterprise domains like financial services,\nlegal, cyber security, and climate and sustainability. The diverse performance\nof 13 models across different enterprise tasks highlights the importance of\nselecting the right model based on the specific requirements of each task. Code\nand prompts are available on GitHub.", "published": "2024-10-11 18:19:05", "link": "http://arxiv.org/abs/2410.12857v1", "categories": ["cs.CL", "cs.AI", "cs.CE"], "primary_category": "cs.CL"}
{"title": "Enhancing Long Context Performance in LLMs Through Inner Loop Query\n  Mechanism", "abstract": "Transformers have a quadratic scaling of computational complexity with input\nsize, which limits the input context window size of large language models\n(LLMs) in both training and inference. Meanwhile, retrieval-augmented\ngeneration (RAG) besed models can better handle longer contexts by using a\nretrieval system to filter out unnecessary information. However, most RAG\nmethods only perform retrieval based on the initial query, which may not work\nwell with complex questions that require deeper reasoning. We introduce a novel\napproach, Inner Loop Memory Augmented Tree Retrieval (ILM-TR), involving\ninner-loop queries, based not only on the query question itself but also on\nintermediate findings. At inference time, our model retrieves information from\nthe RAG system, integrating data from lengthy documents at various levels of\nabstraction. Based on the information retrieved, the LLM generates texts stored\nin an area named Short-Term Memory (STM) which is then used to formulate the\nnext query. This retrieval process is repeated until the text in STM converged.\nOur experiments demonstrate that retrieval with STM offers improvements over\ntraditional retrieval-augmented LLMs, particularly in long context tests such\nas Multi-Needle In A Haystack (M-NIAH) and BABILong.", "published": "2024-10-11 19:49:05", "link": "http://arxiv.org/abs/2410.12859v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Systematic Survey on Large Language Models for Algorithm Design", "abstract": "Algorithm Design (AD) is crucial for effective problem-solving across various\ndomains. The advent of Large Language Models (LLMs) has notably enhanced the\nautomation and innovation within this field, offering new perspectives and\npromising solutions. Over the past three years, the integration of LLMs into AD\n(LLM4AD) has seen substantial progress, with applications spanning\noptimization, machine learning, mathematical reasoning, and scientific\ndiscovery. Given the rapid advancements and expanding scope of this field, a\nsystematic review is both timely and necessary. This paper provides a\nsystematic review of LLM4AD. First, we offer an overview and summary of\nexisting studies. Then, we introduce a taxonomy and review the literature\nacross four dimensions: the roles of LLMs, search methods, prompt methods, and\napplication domains with a discussion of potential and achievements of LLMs in\nAD. Finally, we identify current challenges and highlight several promising\ndirections for future research.", "published": "2024-10-11 13:17:19", "link": "http://arxiv.org/abs/2410.14716v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "UniGlyph: A Seven-Segment Script for Universal Language Representation", "abstract": "UniGlyph is a constructed language (conlang) designed to create a universal\ntransliteration system using a script derived from seven-segment characters.\nThe goal of UniGlyph is to facilitate cross-language communication by offering\na flexible and consistent script that can represent a wide range of phonetic\nsounds. This paper explores the design of UniGlyph, detailing its script\nstructure, phonetic mapping, and transliteration rules. The system addresses\nimperfections in the International Phonetic Alphabet (IPA) and traditional\ncharacter sets by providing a compact, versatile method to represent phonetic\ndiversity across languages. With pitch and length markers, UniGlyph ensures\naccurate phonetic representation while maintaining a small character set.\nApplications of UniGlyph include artificial intelligence integrations, such as\nnatural language processing and multilingual speech recognition, enhancing\ncommunication across different languages. Future expansions are discussed,\nincluding the addition of animal phonetic sounds, where unique scripts are\nassigned to different species, broadening the scope of UniGlyph beyond human\ncommunication. This study presents the challenges and solutions in developing\nsuch a universal script, demonstrating the potential of UniGlyph to bridge\nlinguistic gaps in cross-language communication, educational phonetics, and\nAI-driven applications.", "published": "2024-10-11 16:46:09", "link": "http://arxiv.org/abs/2410.08974v1", "categories": ["cs.CL", "cs.HC", "cs.SC", "cs.SD", "eess.AS", "68T50, 68T01", "H.5.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Can a large language model be a gaslighter?", "abstract": "Large language models (LLMs) have gained human trust due to their\ncapabilities and helpfulness. However, this in turn may allow LLMs to affect\nusers' mindsets by manipulating language. It is termed as gaslighting, a\npsychological effect. In this work, we aim to investigate the vulnerability of\nLLMs under prompt-based and fine-tuning-based gaslighting attacks. Therefore,\nwe propose a two-stage framework DeepCoG designed to: 1) elicit gaslighting\nplans from LLMs with the proposed DeepGaslighting prompting template, and 2)\nacquire gaslighting conversations from LLMs through our Chain-of-Gaslighting\nmethod. The gaslighting conversation dataset along with a corresponding safe\ndataset is applied to fine-tuning-based attacks on open-source LLMs and\nanti-gaslighting safety alignment on these LLMs. Experiments demonstrate that\nboth prompt-based and fine-tuning-based attacks transform three open-source\nLLMs into gaslighters. In contrast, we advanced three safety alignment\nstrategies to strengthen (by 12.05%) the safety guardrail of LLMs. Our safety\nalignment strategies have minimal impacts on the utility of LLMs. Empirical\nstudies indicate that an LLM may be a potential gaslighter, even if it passed\nthe harmfulness test on general dangerous queries.", "published": "2024-10-11 18:35:27", "link": "http://arxiv.org/abs/2410.09181v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Low-complexity Attention-based Unsupervised Anomalous Sound Detection\n  exploiting Separable Convolutions and Angular Loss", "abstract": "In this work, a novel deep neural network, designed to enhance the efficiency\nand effectiveness of unsupervised sound anomaly detection, is presented. The\nproposed model exploits an attention module and separable convolutions to\nidentify salient time-frequency patterns in audio data to discriminate between\nnormal and anomalous sounds with reduced computational complexity. The approach\nis validated through extensive experiments using the Task 2 dataset of the\nDCASE 2020 challenge. Results demonstrate superior performance in terms of\nanomaly detection accuracy while having fewer parameters than state-of-the-art\nmethods. Implementation details, code, and pre-trained models are available in\nhttps://github.com/michaelneri/unsupervised-audio-anomaly-detection.", "published": "2024-10-11 15:46:18", "link": "http://arxiv.org/abs/2410.08919v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Small Tunes Transformer: Exploring Macro & Micro-Level Hierarchies for\n  Skeleton-Conditioned Melody Generation", "abstract": "Recently, symbolic music generation has become a focus of numerous deep\nlearning research. Structure as an important part of music, contributes to\nimproving the quality of music, and an increasing number of works start to\nstudy the hierarchical structure. In this study, we delve into the multi-level\nstructures within music from macro-level and micro-level hierarchies. At the\nmacro-level hierarchy, we conduct phrase segmentation algorithm to explore how\nphrases influence the overall development of music, and at the micro-level\nhierarchy, we design skeleton notes extraction strategy to explore how skeleton\nnotes within each phrase guide the melody generation. Furthermore, we propose a\nnovel Phrase-level Cross-Attention mechanism to capture the intrinsic\nrelationship between macro-level hierarchy and micro-level hierarchy. Moreover,\nin response to the current lack of research on Chinese-style music, we\nconstruct our Small Tunes Dataset: a substantial collection of MIDI files\ncomprising 10088 Small Tunes, a category of traditional Chinese Folk Songs.\nThis dataset serves as the focus of our study. We generate Small Tunes songs\nutilizing the extracted skeleton notes as conditions, and experiment results\nindicate that our proposed model, Small Tunes Transformer, outperforms other\nstate-of-the-art models. Besides, we design three novel objective evaluation\nmetrics to evaluate music from both rhythm and melody dimensions.", "published": "2024-10-11 08:46:56", "link": "http://arxiv.org/abs/2410.08626v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Enhancing Infant Crying Detection with Gradient Boosting for Improved\n  Emotional and Mental Health Diagnostics", "abstract": "Infant crying can serve as a crucial indicator of various physiological and\nemotional states. This paper introduces a comprehensive approach detecting\ninfant cries within audio data. We integrate Wav2Vec with traditional audio\nfeatures and employ Gradient Boosting Machines for cry classification. We\nvalidate our approach on a real world dataset, demonstrating significant\nperformance improvements over existing methods.", "published": "2024-10-11 20:19:36", "link": "http://arxiv.org/abs/2410.09236v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Quantum-Trained Convolutional Neural Network for Deepfake Audio\n  Detection", "abstract": "The rise of deepfake technologies has posed significant challenges to\nprivacy, security, and information integrity, particularly in audio and\nmultimedia content. This paper introduces a Quantum-Trained Convolutional\nNeural Network (QT-CNN) framework designed to enhance the detection of deepfake\naudio, leveraging the computational power of quantum machine learning (QML).\nThe QT-CNN employs a hybrid quantum-classical approach, integrating Quantum\nNeural Networks (QNNs) with classical neural architectures to optimize training\nefficiency while reducing the number of trainable parameters. Our method\nincorporates a novel quantum-to-classical parameter mapping that effectively\nutilizes quantum states to enhance the expressive power of the model, achieving\nup to 70% parameter reduction compared to classical models without compromising\naccuracy. Data pre-processing involved extracting essential audio features,\nlabel encoding, feature scaling, and constructing sequential datasets for\nrobust model evaluation. Experimental results demonstrate that the QT-CNN\nachieves comparable performance to traditional CNNs, maintaining high accuracy\nduring training and testing phases across varying configurations of QNN blocks.\nThe QT framework's ability to reduce computational overhead while maintaining\nperformance underscores its potential for real-world applications in deepfake\ndetection and other resource-constrained scenarios. This work highlights the\npractical benefits of integrating quantum computing into artificial\nintelligence, offering a scalable and efficient approach to advancing deepfake\ndetection technologies.", "published": "2024-10-11 20:52:10", "link": "http://arxiv.org/abs/2410.09250v1", "categories": ["cs.SD", "cs.AI", "eess.AS", "quant-ph"], "primary_category": "cs.SD"}
{"title": "Multimodal Audio-based Disease Prediction with Transformer-based\n  Hierarchical Fusion Network", "abstract": "Audio-based disease prediction is emerging as a promising supplement to\ntraditional medical diagnosis methods, facilitating early, convenient, and\nnon-invasive disease detection and prevention. Multimodal fusion, which\nintegrates features from various domains within or across bio-acoustic\nmodalities, has proven effective in enhancing diagnostic performance. However,\nmost existing methods in the field employ unilateral fusion strategies that\nfocus solely on either intra-modal or inter-modal fusion. This approach limits\nthe full exploitation of the complementary nature of diverse acoustic feature\ndomains and bio-acoustic modalities. Additionally, the inadequate and isolated\nexploration of latent dependencies within modality-specific and modality-shared\nspaces curtails their capacity to manage the inherent heterogeneity in\nmultimodal data. To fill these gaps, we propose a transformer-based\nhierarchical fusion network designed for general multimodal audio-based disease\nprediction. Specifically, we seamlessly integrate intra-modal and inter-modal\nfusion in a hierarchical manner and proficiently encode the necessary\nintra-modal and inter-modal complementary correlations, respectively.\nComprehensive experiments demonstrate that our model achieves state-of-the-art\nperformance in predicting three diseases: COVID-19, Parkinson's disease, and\npathological dysarthria, showcasing its promising potential in a broad context\nof audio-based disease prediction tasks. Additionally, extensive ablation\nstudies and qualitative analyses highlight the significant benefits of each\nmain component within our model.", "published": "2024-10-11 22:37:52", "link": "http://arxiv.org/abs/2410.09289v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Efficient Fine-Grained Guidance for Diffusion-Based Symbolic Music\n  Generation", "abstract": "Developing generative models to create or conditionally create symbolic music\npresents unique challenges due to the combination of limited data availability\nand the need for high precision in note pitch. To address these challenges, we\nintroduce an efficient Fine-Grained Guidance (FGG) approach within diffusion\nmodels. FGG guides the diffusion models to generate music that aligns more\nclosely with the control and intent of expert composers, which is critical to\nimprove the accuracy, listenability, and quality of generated music. This\napproach empowers diffusion models to excel in advanced applications such as\nimprovisation, and interactive music creation. We derive theoretical\ncharacterizations for both the challenges in symbolic music generation and the\neffects of the FGG approach. We provide numerical experiments and subjective\nevaluation to demonstrate the effectiveness of our approach. We have published\na demo page to showcase performances, as one of the first in the symbolic music\nliterature's demo pages that enables real-time interactive generation.", "published": "2024-10-11 00:41:46", "link": "http://arxiv.org/abs/2410.08435v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
