{"title": "AutoQGS: Auto-Prompt for Low-Resource Knowledge-based Question\n  Generation from SPARQL", "abstract": "This study investigates the task of knowledge-based question generation\n(KBQG). Conventional KBQG works generated questions from fact triples in the\nknowledge graph, which could not express complex operations like aggregation\nand comparison in SPARQL. Moreover, due to the costly annotation of large-scale\nSPARQL-question pairs, KBQG from SPARQL under low-resource scenarios urgently\nneeds to be explored. Recently, since the generative pre-trained language\nmodels (PLMs) typically trained in natural language (NL)-to-NL paradigm have\nbeen proven effective for low-resource generation, e.g., T5 and BART, how to\neffectively utilize them to generate NL-question from non-NL SPARQL is\nchallenging. To address these challenges, AutoQGS, an auto-prompt approach for\nlow-resource KBQG from SPARQL, is proposed. Firstly, we put forward to generate\nquestions directly from SPARQL for the KBQG task to handle complex operations.\nSecondly, we propose an auto-prompter trained on large-scale unsupervised data\nto rephrase SPARQL into NL description, smoothing the low-resource\ntransformation from non-NL SPARQL to NL question with PLMs. Experimental\nresults on the WebQuestionsSP, ComlexWebQuestions 1.1, and PathQuestions show\nthat our model achieves state-of-the-art performance, especially in\nlow-resource settings. Furthermore, a corpus of 330k factoid complex\nquestion-SPARQL pairs is generated for further KBQG research.", "published": "2022-08-26 06:53:46", "link": "http://arxiv.org/abs/2208.12461v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Nearest Neighbor Non-autoregressive Text Generation", "abstract": "Non-autoregressive (NAR) models can generate sentences with less computation\nthan autoregressive models but sacrifice generation quality. Previous studies\naddressed this issue through iterative decoding. This study proposes using\nnearest neighbors as the initial state of an NAR decoder and editing them\niteratively. We present a novel training strategy to learn the edit operations\non neighbors to improve NAR text generation. Experimental results show that the\nproposed method (NeighborEdit) achieves higher translation quality (1.69 points\nhigher than the vanilla Transformer) with fewer decoding iterations\n(one-eighteenth fewer iterations) on the JRC-Acquis En-De dataset, the common\nbenchmark dataset for machine translation using nearest neighbors. We also\nconfirm the effectiveness of the proposed method on a data-to-text task\n(WikiBio). In addition, the proposed method outperforms an NAR baseline on the\nWMT'14 En-De dataset. We also report analysis on neighbor examples used in the\nproposed method.", "published": "2022-08-26 08:21:21", "link": "http://arxiv.org/abs/2208.12496v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Task-specific Pre-training and Prompt Decomposition for Knowledge Graph\n  Population with Language Models", "abstract": "We present a system for knowledge graph population with Language Models,\nevaluated on the Knowledge Base Construction from Pre-trained Language Models\n(LM-KBC) challenge at ISWC 2022. Our system involves task-specific pre-training\nto improve LM representation of the masked object tokens, prompt decomposition\nfor progressive generation of candidate objects, among other methods for\nhigher-quality retrieval. Our system is the winner of track 1 of the LM-KBC\nchallenge, based on BERT LM; it achieves 55.0% F-1 score on the hidden test set\nof the challenge.", "published": "2022-08-26 09:56:27", "link": "http://arxiv.org/abs/2208.12539v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SeSQL: Yet Another Large-scale Session-level Chinese Text-to-SQL Dataset", "abstract": "As the first session-level Chinese dataset, CHASE contains two separate\nparts, i.e., 2,003 sessions manually constructed from scratch (CHASE-C), and\n3,456 sessions translated from English SParC (CHASE-T). We find the two parts\nare highly discrepant and incompatible as training and evaluation data. In this\nwork, we present SeSQL, yet another large-scale session-level text-to-SQL\ndataset in Chinese, consisting of 5,028 sessions all manually constructed from\nscratch. In order to guarantee data quality, we adopt an iterative annotation\nworkflow to facilitate intense and in-time review of previous-round natural\nlanguage (NL) questions and SQL queries. Moreover, by completing all\ncontext-dependent NL questions, we obtain 27,012 context-independent\nquestion/SQL pairs, allowing SeSQL to be used as the largest dataset for\nsingle-round multi-DB text-to-SQL parsing. We conduct benchmark session-level\ntext-to-SQL parsing experiments on SeSQL by employing three competitive\nsession-level parsers, and present detailed analysis.", "published": "2022-08-26 15:11:10", "link": "http://arxiv.org/abs/2208.12711v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Coalescing Global and Local Information for Procedural Text\n  Understanding", "abstract": "Procedural text understanding is a challenging language reasoning task that\nrequires models to track entity states across the development of a narrative. A\ncomplete procedural understanding solution should combine three core aspects:\nlocal and global views of the inputs, and global view of outputs. Prior methods\nconsidered a subset of these aspects, resulting in either low precision or low\nrecall. In this paper, we propose Coalescing Global and Local Information\n(CGLI), a new model that builds entity- and timestep-aware input\nrepresentations (local input) considering the whole context (global input), and\nwe jointly model the entity states with a structured prediction objective\n(global output). Thus, CGLI simultaneously optimizes for both precision and\nrecall. We extend CGLI with additional output layers and integrate it into a\nstory reasoning framework. Extensive experiments on a popular procedural text\nunderstanding dataset show that our model achieves state-of-the-art results;\nexperiments on a story reasoning benchmark show the positive impact of our\nmodel on downstream reasoning.", "published": "2022-08-26 19:16:32", "link": "http://arxiv.org/abs/2208.12848v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Transfer Learning for Fake News Detector in a Low-Resource\n  Language", "abstract": "Development of methods to detect fake news (FN) in low-resource languages has\nbeen impeded by a lack of training data. In this study, we solve the problem by\nusing only training data from a high-resource language. Our FN-detection system\npermitted this strategy by applying adversarial learning that transfers the\ndetection knowledge through languages. To assist the knowledge transfer, our\nsystem judges the reliability of articles by exploiting source information,\nwhich is a cross-lingual feature that represents the credibility of the\nspeaker. In experiments, our system got 3.71% higher accuracy than a system\nthat uses a machine-translated training dataset. In addition, our suggested\ncross-lingual feature exploitation for fake news detection improved accuracy by\n3.03%.", "published": "2022-08-26 07:41:27", "link": "http://arxiv.org/abs/2208.12482v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GRASP: Guiding model with RelAtional Semantics using Prompt for Dialogue\n  Relation Extraction", "abstract": "The dialogue-based relation extraction (DialogRE) task aims to predict the\nrelations between argument pairs that appear in dialogue. Most previous studies\nutilize fine-tuning pre-trained language models (PLMs) only with extensive\nfeatures to supplement the low information density of the dialogue by multiple\nspeakers. To effectively exploit inherent knowledge of PLMs without extra\nlayers and consider scattered semantic cues on the relation between the\narguments, we propose a Guiding model with RelAtional Semantics using Prompt\n(GRASP). We adopt a prompt-based fine-tuning approach and capture relational\nsemantic clues of a given dialogue with 1) an argument-aware prompt marker\nstrategy and 2) the relational clue detection task. In the experiments, GRASP\nachieves state-of-the-art performance in terms of both F1 and F1c scores on a\nDialogRE dataset even though our method only leverages PLMs without adding any\nextra layers.", "published": "2022-08-26 08:19:28", "link": "http://arxiv.org/abs/2208.12494v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AiM: Taking Answers in Mind to Correct Chinese Cloze Tests in\n  Educational Applications", "abstract": "To automatically correct handwritten assignments, the traditional approach is\nto use an OCR model to recognize characters and compare them to answers. The\nOCR model easily gets confused on recognizing handwritten Chinese characters,\nand the textual information of the answers is missing during the model\ninference. However, teachers always have these answers in mind to review and\ncorrect assignments. In this paper, we focus on the Chinese cloze tests\ncorrection and propose a multimodal approach (named AiM). The encoded\nrepresentations of answers interact with the visual information of students'\nhandwriting. Instead of predicting 'right' or 'wrong', we perform the sequence\nlabeling on the answer text to infer which answer character differs from the\nhandwritten content in a fine-grained way. We take samples of OCR datasets as\nthe positive samples for this task, and develop a negative sample augmentation\nmethod to scale up the training data. Experimental results show that AiM\noutperforms OCR-based methods by a large margin. Extensive studies demonstrate\nthe effectiveness of our multimodal approach.", "published": "2022-08-26 08:56:32", "link": "http://arxiv.org/abs/2208.12505v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "What Do NLP Researchers Believe? Results of the NLP Community Metasurvey", "abstract": "We present the results of the NLP Community Metasurvey. Run from May to June\n2022, the survey elicited opinions on controversial issues, including industry\ninfluence in the field, concerns about AGI, and ethics. Our results put\nconcrete numbers to several controversies: For example, respondents are split\nalmost exactly in half on questions about the importance of artificial general\nintelligence, whether language models understand language, and the necessity of\nlinguistic structure and inductive bias for solving NLP problems. In addition,\nthe survey posed meta-questions, asking respondents to predict the distribution\nof survey responses. This allows us not only to gain insight on the spectrum of\nbeliefs held by NLP researchers, but also to uncover false sociological beliefs\nwhere the community's predictions don't match reality. We find such mismatches\non a wide range of issues. Among other results, the community greatly\noverestimates its own belief in the usefulness of benchmarks and the potential\nfor scaling to solve real-world problems, while underestimating its own belief\nin the importance of linguistic structure, inductive bias, and\ninterdisciplinary science.", "published": "2022-08-26 19:45:51", "link": "http://arxiv.org/abs/2208.12852v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Deep Learning-Based Sentiment Analysis of COVID-19 Vaccination Responses\n  from Twitter Data", "abstract": "This COVID-19 pandemic is so dreadful that it leads to severe anxiety,\nphobias, and complicated feelings or emotions. Even after vaccination against\nCoronavirus has been initiated, people feelings have become more diverse and\ncomplex, and our goal is to understand and unravel their sentiments in this\nresearch using some Deep Learning techniques. Social media is currently the\nbest way to express feelings and emotions, and with the help of it,\nspecifically Twitter, one can have a better idea of what is trending and what\nis going on in people minds. Our motivation for this research is to understand\nthe sentiment of people regarding the vaccination process, and their diverse\nthoughts regarding this. In this research, the timeline of the collected tweets\nwas from December 21 to July 21, and contained tweets about the most common\nvaccines available recently from all across the world. The sentiments of people\nregarding vaccines of all sorts were assessed by using a Natural Language\nProcessing (NLP) tool named Valence Aware Dictionary for sEntiment Reasoner\n(VADER). By initializing the sentiment polarities into 3 groups (positive,\nnegative and neutral), the overall scenario was visualized here and our\nfindings came out as 33.96% positive, 17.55% negative and 48.49% neutral\nresponses. Recurrent Neural Network (RNN) oriented architecture such as Long\nShort-Term Memory (LSTM and Bi-LSTM) is used to assess the performance of the\npredictive models, with LSTM achieving an accuracy of 90.59% and Bi-LSTM\nachieving an accuracy of 90.83%. Other performance metrics such as Precision,\nRecall, F-1 score, and Confusion matrix were also shown to validate our models\nand findings more effectively. This study will help everyone understand public\nopinion on the COVID-19 vaccines and impact the aim of eradicating the\nCoronavirus from our beautiful world.", "published": "2022-08-26 18:07:37", "link": "http://arxiv.org/abs/2209.12604v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "MuLan: A Joint Embedding of Music Audio and Natural Language", "abstract": "Music tagging and content-based retrieval systems have traditionally been\nconstructed using pre-defined ontologies covering a rigid set of music\nattributes or text queries. This paper presents MuLan: a first attempt at a new\ngeneration of acoustic models that link music audio directly to unconstrained\nnatural language music descriptions. MuLan takes the form of a two-tower, joint\naudio-text embedding model trained using 44 million music recordings (370K\nhours) and weakly-associated, free-form text annotations. Through its\ncompatibility with a wide range of music genres and text styles (including\nconventional music tags), the resulting audio-text representation subsumes\nexisting ontologies while graduating to true zero-shot functionalities. We\ndemonstrate the versatility of the MuLan embeddings with a range of experiments\nincluding transfer learning, zero-shot music tagging, language understanding in\nthe music domain, and cross-modal retrieval applications.", "published": "2022-08-26 03:13:21", "link": "http://arxiv.org/abs/2208.12415v1", "categories": ["eess.AS", "cs.CL", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Effectiveness of Mining Audio and Text Pairs from Public Data for\n  Improving ASR Systems for Low-Resource Languages", "abstract": "End-to-end (E2E) models have become the default choice for state-of-the-art\nspeech recognition systems. Such models are trained on large amounts of\nlabelled data, which are often not available for low-resource languages.\nTechniques such as self-supervised learning and transfer learning hold promise,\nbut have not yet been effective in training accurate models. On the other hand,\ncollecting labelled datasets on a diverse set of domains and speakers is very\nexpensive. In this work, we demonstrate an inexpensive and effective\nalternative to these approaches by ``mining'' text and audio pairs for Indian\nlanguages from public sources, specifically from the public archives of All\nIndia Radio. As a key component, we adapt the Needleman-Wunsch algorithm to\nalign sentences with corresponding audio segments given a long audio and a PDF\nof its transcript, while being robust to errors due to OCR, extraneous text,\nand non-transcribed speech. We thus create Shrutilipi, a dataset which contains\nover 6,400 hours of labelled audio across 12 Indian languages totalling to\n4.95M sentences. On average, Shrutilipi results in a 2.3x increase over\npublicly available labelled data. We establish the quality of Shrutilipi with\n21 human evaluators across the 12 languages. We also establish the diversity of\nShrutilipi in terms of represented regions, speakers, and mentioned named\nentities. Significantly, we show that adding Shrutilipi to the training set of\nWav2Vec models leads to an average decrease in WER of 5.8\\% for 7 languages on\nthe IndicSUPERB benchmark. For Hindi, which has the most benchmarks (7), the\naverage WER falls from 18.8% to 13.5%. This improvement extends to efficient\nmodels: We show a 2.3% drop in WER for a Conformer model (10x smaller than\nWav2Vec). Finally, we demonstrate the diversity of Shrutilipi by showing that\nthe model trained with it is more robust to noisy input.", "published": "2022-08-26 13:37:45", "link": "http://arxiv.org/abs/2208.12666v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Building the Intent Landscape of Real-World Conversational Corpora with\n  Extractive Question-Answering Transformers", "abstract": "For companies with customer service, mapping intents inside their\nconversational data is crucial in building applications based on natural\nlanguage understanding (NLU). Nevertheless, there is no established automated\ntechnique to gather the intents from noisy online chats or voice transcripts.\nSimple clustering approaches are not suited to intent-sparse dialogues. To\nsolve this intent-landscape task, we propose an unsupervised pipeline that\nextracts the intents and the taxonomy of intents from real-world dialogues. Our\npipeline mines intent-span candidates with an extractive Question-Answering\nElectra model and leverages sentence embeddings to apply a low-level density\nclustering followed by a top-level hierarchical clustering. Our results\ndemonstrate the generalization ability of an ELECTRA large model fine-tuned on\nthe SQuAD2 dataset to understand dialogues. With the right prompting question,\nthis model achieves a rate of linguistic validation on intent spans beyond 85%.\nWe furthermore reconstructed the intent schemes of five domains from the\nMultiDoGo dataset with an average recall of 94.3%.", "published": "2022-08-26 22:53:19", "link": "http://arxiv.org/abs/2208.12886v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Investigating data partitioning strategies for crosslinguistic\n  low-resource ASR evaluation", "abstract": "Many automatic speech recognition (ASR) data sets include a single\npre-defined test set consisting of one or more speakers whose speech never\nappears in the training set. This \"hold-speaker(s)-out\" data partitioning\nstrategy, however, may not be ideal for data sets in which the number of\nspeakers is very small. This study investigates ten different data split\nmethods for five languages with minimal ASR training resources. We find that\n(1) model performance varies greatly depending on which speaker is selected for\ntesting; (2) the average word error rate (WER) across all held-out speakers is\ncomparable not only to the average WER over multiple random splits but also to\nany given individual random split; (3) WER is also generally comparable when\nthe data is split heuristically or adversarially; (4) utterance duration and\nintensity are comparatively more predictive factors of variability regardless\nof the data split. These results suggest that the widely used hold-speakers-out\napproach to ASR data partitioning can yield results that do not reflect model\nperformance on unseen data or speakers. Random splits can yield more reliable\nand generalizable estimates when facing data sparsity.", "published": "2022-08-26 23:00:49", "link": "http://arxiv.org/abs/2208.12888v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Music Separation Enhancement with Generative Modeling", "abstract": "Despite phenomenal progress in recent years, state-of-the-art music\nseparation systems produce source estimates with significant perceptual\nshortcomings, such as adding extraneous noise or removing harmonics. We propose\na post-processing model (the Make it Sound Good (MSG) post-processor) to\nenhance the output of music source separation systems. We apply our\npost-processing model to state-of-the-art waveform-based and spectrogram-based\nmusic source separators, including a separator unseen by MSG during training.\nOur analysis of the errors produced by source separators shows that waveform\nmodels tend to introduce more high-frequency noise, while spectrogram models\ntend to lose transients and high frequency content. We introduce objective\nmeasures to quantify both kinds of errors and show MSG improves the source\nreconstruction of both kinds of errors. Crowdsourced subjective evaluations\ndemonstrate that human listeners prefer source estimates of bass and drums that\nhave been post-processed by MSG.", "published": "2022-08-26 00:44:37", "link": "http://arxiv.org/abs/2208.12387v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Leveraging Symmetrical Convolutional Transformer Networks for Speech to\n  Singing Voice Style Transfer", "abstract": "In this paper, we propose a model to perform style transfer of speech to\nsinging voice. Contrary to the previous signal processing-based methods, which\nrequire high-quality singing templates or phoneme synchronization, we explore a\ndata-driven approach for the problem of converting natural speech to singing\nvoice. We develop a novel neural network architecture, called SymNet, which\nmodels the alignment of the input speech with the target melody while\npreserving the speaker identity and naturalness. The proposed SymNet model is\ncomprised of symmetrical stack of three types of layers - convolutional,\ntransformer, and self-attention layers. The paper also explores novel data\naugmentation and generative loss annealing methods to facilitate the model\ntraining. Experiments are performed on the\n  NUS and NHSS datasets which consist of parallel data of speech and singing\nvoice. In these experiments, we show that the proposed SymNet model improves\nthe objective reconstruction quality significantly over the previously\npublished methods and baseline architectures. Further, a subjective listening\ntest confirms the improved quality of the audio obtained using the proposed\napproach (absolute improvement of 0.37 in mean opinion score measure over the\nbaseline system).", "published": "2022-08-26 02:54:57", "link": "http://arxiv.org/abs/2208.12410v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Concept-Based Techniques for \"Musicologist-friendly\" Explanations in a\n  Deep Music Classifier", "abstract": "Current approaches for explaining deep learning systems applied to musical\ndata provide results in a low-level feature space, e.g., by highlighting\npotentially relevant time-frequency bins in a spectrogram or time-pitch bins in\na piano roll. This can be difficult to understand, particularly for\nmusicologists without technical knowledge. To address this issue, we focus on\nmore human-friendly explanations based on high-level musical concepts. Our\nresearch targets trained systems (post-hoc explanations) and explores two\napproaches: a supervised one, where the user can define a musical concept and\ntest if it is relevant to the system; and an unsupervised one, where musical\nexcerpts containing relevant concepts are automatically selected and given to\nthe user for interpretation. We demonstrate both techniques on an existing\nsymbolic composer classification system, showcase their potential, and\nhighlight their intrinsic limitations.", "published": "2022-08-26 07:45:29", "link": "http://arxiv.org/abs/2208.12485v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Mel Spectrogram Inversion with Stable Pitch", "abstract": "Vocoders are models capable of transforming a low-dimensional spectral\nrepresentation of an audio signal, typically the mel spectrogram, to a\nwaveform. Modern speech generation pipelines use a vocoder as their final\ncomponent. Recent vocoder models developed for speech achieve a high degree of\nrealism, such that it is natural to wonder how they would perform on music\nsignals. Compared to speech, the heterogeneity and structure of the musical\nsound texture offers new challenges. In this work we focus on one specific\nartifact that some vocoder models designed for speech tend to exhibit when\napplied to music: the perceived instability of pitch when synthesizing\nsustained notes. We argue that the characteristic sound of this artifact is due\nto the lack of horizontal phase coherence, which is often the result of using a\ntime-domain target space with a model that is invariant to time-shifts, such as\na convolutional neural network. We propose a new vocoder model that is\nspecifically designed for music. Key to improving the pitch stability is the\nchoice of a shift-invariant target space that consists of the magnitude\nspectrum and the phase gradient. We discuss the reasons that inspired us to\nre-formulate the vocoder task, outline a working example, and evaluate it on\nmusical signals. Our method results in 60% and 10% improved reconstruction of\nsustained notes and chords with respect to existing models, using a novel\nharmonic error metric.", "published": "2022-08-26 17:01:57", "link": "http://arxiv.org/abs/2208.12782v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech Emotion Recognition using Supervised Deep Recurrent System for\n  Mental Health Monitoring", "abstract": "Understanding human behavior and monitoring mental health are essential to\nmaintaining the community and society's safety. As there has been an increase\nin mental health problems during the COVID-19 pandemic due to uncontrolled\nmental health, early detection of mental issues is crucial. Nowadays, the usage\nof Intelligent Virtual Personal Assistants (IVA) has increased worldwide.\nIndividuals use their voices to control these devices to fulfill requests and\nacquire different services. This paper proposes a novel deep learning model\nbased on the gated recurrent neural network and convolution neural network to\nunderstand human emotion from speech to improve their IVA services and monitor\ntheir mental health.", "published": "2022-08-26 01:14:31", "link": "http://arxiv.org/abs/2208.12812v3", "categories": ["eess.AS", "cs.HC", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
