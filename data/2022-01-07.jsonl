{"title": "An Unsupervised Masking Objective for Abstractive Multi-Document News\n  Summarization", "abstract": "We show that a simple unsupervised masking objective can approach near\nsupervised performance on abstractive multi-document news summarization. Our\nmethod trains a state-of-the-art neural summarization model to predict the\nmasked out source document with highest lexical centrality relative to the\nmulti-document group. In experiments on the Multi-News dataset, our masked\ntraining objective yields a system that outperforms past unsupervised methods\nand, in human evaluation, surpasses the best supervised method without\nrequiring access to any ground-truth summaries. Further, we evaluate how\ndifferent measures of lexical centrality, inspired by past work on extractive\nsummarization, affect final performance.", "published": "2022-01-07 04:19:53", "link": "http://arxiv.org/abs/2201.02321v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Defeat of the Winograd Schema Challenge", "abstract": "The Winograd Schema Challenge - a set of twin sentences involving pronoun\nreference disambiguation that seem to require the use of commonsense knowledge\n- was proposed by Hector Levesque in 2011. By 2019, a number of AI systems,\nbased on large pre-trained transformer-based language models and fine-tuned on\nthese kinds of problems, achieved better than 90% accuracy. In this paper, we\nreview the history of the Winograd Schema Challenge and discuss the lasting\ncontributions of the flurry of research that has taken place on the WSC in the\nlast decade. We discuss the significance of various datasets developed for WSC,\nand the research community's deeper understanding of the role of surrogate\ntasks in assessing the intelligence of an AI system.", "published": "2022-01-07 10:22:08", "link": "http://arxiv.org/abs/2201.02387v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic-based Data Augmentation for Math Word Problems", "abstract": "It's hard for neural MWP solvers to deal with tiny local variances. In MWP\ntask, some local changes conserve the original semantic while the others may\ntotally change the underlying logic. Currently, existing datasets for MWP task\ncontain limited samples which are key for neural models to learn to\ndisambiguate different kinds of local variances in questions and solve the\nquestions correctly. In this paper, we propose a set of novel data augmentation\napproaches to supplement existing datasets with such data that are augmented\nwith different kinds of local variances, and help to improve the generalization\nability of current neural models. New samples are generated by knowledge guided\nentity replacement, and logic guided problem reorganization. The augmentation\napproaches are ensured to keep the consistency between the new data and their\nlabels. Experimental results have shown the necessity and the effectiveness of\nour methods.", "published": "2022-01-07 15:07:56", "link": "http://arxiv.org/abs/2201.02489v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Development of an Extractive Clinical Question Answering Dataset with\n  Multi-Answer and Multi-Focus Questions", "abstract": "Background: Extractive question-answering (EQA) is a useful natural language\nprocessing (NLP) application for answering patient-specific questions by\nlocating answers in their clinical notes. Realistic clinical EQA can have\nmultiple answers to a single question and multiple focus points in one\nquestion, which are lacking in the existing datasets for development of\nartificial intelligence solutions. Objective: Create a dataset for developing\nand evaluating clinical EQA systems that can handle natural multi-answer and\nmulti-focus questions. Methods: We leveraged the annotated relations from the\n2018 National NLP Clinical Challenges (n2c2) corpus to generate an EQA dataset.\nSpecifically, the 1-to-N, M-to-1, and M-to-N drug-reason relations were\nincluded to form the multi-answer and multi-focus QA entries, which represent\nmore complex and natural challenges in addition to the basic\none-drug-one-reason cases. A baseline solution was developed and tested on the\ndataset. Results: The derived RxWhyQA dataset contains 96,939 QA entries. Among\nthe answerable questions, 25% require multiple answers, and 2% ask about\nmultiple drugs within one question. There are frequent cues observed around the\nanswers in the text, and 90% of the drug and reason terms occur within the same\nor an adjacent sentence. The baseline EQA solution achieved a best f1-measure\nof 0.72 on the entire dataset, and on specific subsets, it was: 0.93 on the\nunanswerable questions, 0.48 on single-drug questions versus 0.60 on multi-drug\nquestions, 0.54 on the single-answer questions versus 0.43 on multi-answer\nquestions. Discussion: The RxWhyQA dataset can be used to train and evaluate\nsystems that need to handle multi-answer and multi-focus questions.\nSpecifically, multi-answer EQA appears to be challenging and therefore warrants\nmore investment in research.", "published": "2022-01-07 15:58:58", "link": "http://arxiv.org/abs/2201.02517v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Intent Matching based Customer Services Chatbot with Natural Language\n  Understanding", "abstract": "Customer service is the lifeblood of any business. Excellent customer service\nnot only generates return business but also creates new customers. Looking at\nthe demanding market to provide a 24/7 service to customers, many organisations\nare increasingly engaged in popular social media and text messaging platforms\nsuch as WhatsApp and Facebook Messenger in providing a 24/7 service to\ncustomers in the current demanding market. In this paper, we present an intent\nmatching based customer services chatbot (IMCSC), which is capable of replacing\nthe customer service work of sales personnel, whilst interacting in a more\nnatural and human-like manner through the employment of Natural Language\nUnderstanding (NLU). The bot is able to answer the most common frequently asked\nquestions and we have also integrated features for the processing and exporting\nof customer orders to a Google Sheet.", "published": "2022-01-07 08:30:32", "link": "http://arxiv.org/abs/2202.00480v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Repurposing Existing Deep Networks for Caption and Aesthetic-Guided\n  Image Cropping", "abstract": "We propose a novel optimization framework that crops a given image based on\nuser description and aesthetics. Unlike existing image cropping methods, where\none typically trains a deep network to regress to crop parameters or cropping\nactions, we propose to directly optimize for the cropping parameters by\nrepurposing pre-trained networks on image captioning and aesthetic tasks,\nwithout any fine-tuning, thereby avoiding training a separate network.\nSpecifically, we search for the best crop parameters that minimize a combined\nloss of the initial objectives of these networks. To make the optimization\ntable, we propose three strategies: (i) multi-scale bilinear sampling, (ii)\nannealing the scale of the crop region, therefore effectively reducing the\nparameter space, (iii) aggregation of multiple optimization results. Through\nvarious quantitative and qualitative evaluations, we show that our framework\ncan produce crops that are well-aligned to intended user descriptions and\naesthetically pleasing.", "published": "2022-01-07 00:23:40", "link": "http://arxiv.org/abs/2201.02280v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A Transfer Learning Pipeline for Educational Resource Discovery with\n  Application in Leading Paragraph Generation", "abstract": "Effective human learning depends on a wide selection of educational materials\nthat align with the learner's current understanding of the topic. While the\nInternet has revolutionized human learning or education, a substantial resource\naccessibility barrier still exists. Namely, the excess of online information\ncan make it challenging to navigate and discover high-quality learning\nmaterials. In this paper, we propose the educational resource discovery (ERD)\npipeline that automates web resource discovery for novel domains. The pipeline\nconsists of three main steps: data collection, feature extraction, and resource\nclassification. We start with a known source domain and conduct resource\ndiscovery on two unseen target domains via transfer learning. We first collect\nfrequent queries from a set of seed documents and search on the web to obtain\ncandidate resources, such as lecture slides and introductory blog posts. Then\nwe introduce a novel pretrained information retrieval deep neural network\nmodel, query-document masked language modeling (QD-MLM), to extract deep\nfeatures of these candidate resources. We apply a tree-based classifier to\ndecide whether the candidate is a positive learning resource. The pipeline\nachieves F1 scores of 0.94 and 0.82 when evaluated on two similar but novel\ntarget domains. Finally, we demonstrate how this pipeline can benefit an\napplication: leading paragraph generation for surveys. This is the first study\nthat considers various web resources for survey generation, to the best of our\nknowledge. We also release a corpus of 39,728 manually labeled web resources\nand 659 queries from NLP, Computer Vision (CV), and Statistics (STATS).", "published": "2022-01-07 03:35:40", "link": "http://arxiv.org/abs/2201.02312v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Progressive Video Summarization via Multimodal Self-supervised Learning", "abstract": "Modern video summarization methods are based on deep neural networks that\nrequire a large amount of annotated data for training. However, existing\ndatasets for video summarization are small-scale, easily leading to\nover-fitting of the deep models. Considering that the annotation of large-scale\ndatasets is time-consuming, we propose a multimodal self-supervised learning\nframework to obtain semantic representations of videos, which benefits the\nvideo summarization task. Specifically, the self-supervised learning is\nconducted by exploring the semantic consistency between the videos and text in\nboth coarse-grained and fine-grained fashions, as well as recovering masked\nframes in the videos. The multimodal framework is trained on a newly-collected\ndataset that consists of video-text pairs. Additionally, we introduce a\nprogressive video summarization method, where the important content in a video\nis pinpointed progressively to generate better summaries. Extensive experiments\nhave proved the effectiveness and superiority of our method in rank correlation\ncoefficients and F-score.", "published": "2022-01-07 15:21:46", "link": "http://arxiv.org/abs/2201.02494v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Imagined versus Remembered Stories: Quantifying Differences in Narrative\n  Flow", "abstract": "Lifelong experiences and learned knowledge lead to shared expectations about\nhow common situations tend to unfold. Such knowledge of narrative event flow\nenables people to weave together a story. However, comparable computational\ntools to evaluate the flow of events in narratives are limited. We quantify the\ndifferences between autobiographical and imagined stories by introducing\nsequentiality, a measure of narrative flow of events, drawing probabilistic\ninferences from a cutting-edge large language model (GPT-3). Sequentiality\ncaptures the flow of a narrative by comparing the probability of a sentence\nwith and without its preceding story context. We applied our measure to study\nthousands of diary-like stories, collected from crowdworkers about either a\nrecent remembered experience or an imagined story on the same topic. The\nresults show that imagined stories have higher sequentiality than\nautobiographical stories and that the sequentiality of autobiographical stories\nincreases when the memories are retold several months later. In pursuit of\ndeeper understandings of how sequentiality measures the flow of narratives, we\nexplore proportions of major and minor events in story sentences, as annotated\nby crowdworkers. We find that lower sequentiality is associated with higher\nproportions of major events. The methods and results highlight opportunities to\nuse cutting-edge computational analyses, such as sequentiality, on large\ncorpora of matched imagined and autobiographical stories to investigate the\ninfluences of memory and reasoning on language generation processes.", "published": "2022-01-07 20:10:47", "link": "http://arxiv.org/abs/2201.02662v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatic Speech Recognition Datasets in Cantonese: A Survey and New\n  Dataset", "abstract": "Automatic speech recognition (ASR) on low resource languages improves the\naccess of linguistic minorities to technological advantages provided by\nartificial intelligence (AI). In this paper, we address the problem of data\nscarcity for the Hong Kong Cantonese language by creating a new Cantonese\ndataset. Our dataset, Multi-Domain Cantonese Corpus (MDCC), consists of 73.6\nhours of clean read speech paired with transcripts, collected from Cantonese\naudiobooks from Hong Kong. It comprises philosophy, politics, education,\nculture, lifestyle and family domains, covering a wide range of topics. We also\nreview all existing Cantonese datasets and analyze them according to their\nspeech type, data source, total size and availability. We further conduct\nexperiments with Fairseq S2T Transformer, a state-of-the-art ASR model, on the\nbiggest existing dataset, Common Voice zh-HK, and our proposed MDCC, and the\nresults show the effectiveness of our dataset. In addition, we create a\npowerful and robust Cantonese ASR model by applying multi-dataset learning on\nMDCC and Common Voice zh-HK.", "published": "2022-01-07 12:09:15", "link": "http://arxiv.org/abs/2201.02419v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Sign Language Video Retrieval with Free-Form Textual Queries", "abstract": "Systems that can efficiently search collections of sign language videos have\nbeen highlighted as a useful application of sign language technology. However,\nthe problem of searching videos beyond individual keywords has received limited\nattention in the literature. To address this gap, in this work we introduce the\ntask of sign language retrieval with free-form textual queries: given a written\nquery (e.g., a sentence) and a large collection of sign language videos, the\nobjective is to find the signing video in the collection that best matches the\nwritten query. We propose to tackle this task by learning cross-modal\nembeddings on the recently introduced large-scale How2Sign dataset of American\nSign Language (ASL). We identify that a key bottleneck in the performance of\nthe system is the quality of the sign video embedding which suffers from a\nscarcity of labeled training data. We, therefore, propose SPOT-ALIGN, a\nframework for interleaving iterative rounds of sign spotting and feature\nalignment to expand the scope and scale of available training data. We validate\nthe effectiveness of SPOT-ALIGN for learning a robust sign video embedding\nthrough improvements in both sign recognition and the proposed video retrieval\ntask.", "published": "2022-01-07 15:22:18", "link": "http://arxiv.org/abs/2201.02495v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Textual Data Augmentation for Arabic-English Code-Switching Speech\n  Recognition", "abstract": "The pervasiveness of intra-utterance code-switching (CS) in spoken content\nrequires that speech recognition (ASR) systems handle mixed language. Designing\na CS-ASR system has many challenges, mainly due to data scarcity, grammatical\nstructure complexity, and domain mismatch. The most common method for\naddressing CS is to train an ASR system with the available transcribed CS\nspeech, along with monolingual data. In this work, we propose a zero-shot\nlearning methodology for CS-ASR by augmenting the monolingual data with\nartificially generating CS text. We based our approach on random lexical\nreplacements and Equivalence Constraint (EC) while exploiting aligned\ntranslation pairs to generate random and grammatically valid CS content. Our\nempirical results show a 65.5% relative reduction in language model perplexity,\nand 7.7% in ASR WER on two ecologically valid CS test sets. The human\nevaluation of the generated text using EC suggests that more than 80% is of\nadequate quality.", "published": "2022-01-07 17:14:19", "link": "http://arxiv.org/abs/2201.02550v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A New Amharic Speech Emotion Dataset and Classification Benchmark", "abstract": "In this paper we present the Amharic Speech Emotion Dataset (ASED), which\ncovers four dialects (Gojjam, Wollo, Shewa and Gonder) and five different\nemotions (neutral, fearful, happy, sad and angry). We believe it is the first\nSpeech Emotion Recognition (SER) dataset for the Amharic language. 65 volunteer\nparticipants, all native speakers, recorded 2,474 sound samples, two to four\nseconds in length. Eight judges assigned emotions to the samples with high\nagreement level (Fleiss kappa = 0.8). The resulting dataset is freely available\nfor download. Next, we developed a four-layer variant of the well-known VGG\nmodel which we call VGGb. Three experiments were then carried out using VGGb\nfor SER, using ASED. First, we investigated whether Mel-spectrogram features or\nMel-frequency Cepstral coefficient (MFCC) features work best for Amharic. This\nwas done by training two VGGb SER models on ASED, one using Mel-spectrograms\nand the other using MFCC. Four forms of training were tried, standard\ncross-validation, and three variants based on sentences, dialects and speaker\ngroups. Thus, a sentence used for training would not be used for testing, and\nthe same for a dialect and speaker group. The conclusion was that MFCC features\nare superior under all four training schemes. MFCC was therefore adopted for\nExperiment 2, where VGGb and three other existing models were compared on ASED:\nRESNet50, Alex-Net and LSTM. VGGb was found to have very good accuracy (90.73%)\nas well as the fastest training time. In Experiment 3, the performance of VGGb\nwas compared when trained on two existing SER datasets, RAVDESS (English) and\nEMO-DB (German) as well as on ASED (Amharic). Results are comparable across\nthese languages, with ASED being the highest. This suggests that VGGb can be\nsuccessfully applied to other languages. We hope that ASED will encourage\nresearchers to experiment with other models for Amharic SER.", "published": "2022-01-07 23:50:34", "link": "http://arxiv.org/abs/2201.02710v1", "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Semi-Supervised Clustering with Contrastive Learning for Discovering New\n  Intents", "abstract": "Most dialogue systems in real world rely on predefined intents and answers\nfor QA service, so discovering potential intents from large corpus previously\nis really important for building such dialogue services. Considering that most\nscenarios have few intents known already and most intents waiting to be\ndiscovered, we focus on semi-supervised text clustering and try to make the\nproposed method benefit from labeled samples for better overall clustering\nperformance. In this paper, we propose Deep Contrastive Semi-supervised\nClustering (DCSC), which aims to cluster text samples in a semi-supervised way\nand provide grouped intents to operation staff. To make DCSC fully utilize the\nlimited known intents, we propose a two-stage training procedure for DCSC, in\nwhich DCSC will be trained on both labeled samples and unlabeled samples, and\nachieve better text representation and clustering performance. We conduct\nexperiments on two public datasets to compare our model with several popular\nmethods, and the results show DCSC achieve best performance across all datasets\nand circumstances, indicating the effect of the improvements in our work.", "published": "2022-01-07 09:58:43", "link": "http://arxiv.org/abs/2201.07604v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MERLOT Reserve: Neural Script Knowledge through Vision and Language and\n  Sound", "abstract": "As humans, we navigate a multimodal world, building a holistic understanding\nfrom all our senses. We introduce MERLOT Reserve, a model that represents\nvideos jointly over time -- through a new training objective that learns from\naudio, subtitles, and video frames. Given a video, we replace snippets of text\nand audio with a MASK token; the model learns by choosing the correct\nmasked-out snippet. Our objective learns faster than alternatives, and performs\nwell at scale: we pretrain on 20 million YouTube videos.\n  Empirical results show that MERLOT Reserve learns strong multimodal\nrepresentations. When finetuned, it sets state-of-the-art on Visual Commonsense\nReasoning (VCR), TVQA, and Kinetics-600; outperforming prior work by 5%, 7%,\nand 1.5% respectively. Ablations show that these tasks benefit from audio\npretraining -- even VCR, a QA task centered around images (without sound).\nMoreover, our objective enables out-of-the-box prediction, revealing strong\nmultimodal commonsense understanding. In a fully zero-shot setting, our model\nobtains competitive results on four video tasks, even outperforming supervised\napproaches on the recently proposed Situated Reasoning (STAR) benchmark.\n  We analyze why audio enables better vision-language representations,\nsuggesting significant opportunities for future research. We conclude by\ndiscussing ethical and societal implications of multimodal pretraining.", "published": "2022-01-07 19:00:21", "link": "http://arxiv.org/abs/2201.02639v4", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "A sinusoidal signal reconstruction method for the inversion of the\n  mel-spectrogram", "abstract": "The synthesis of sound via deep learning methods has recently received much\nattention. Some problems for deep learning approaches to sound synthesis relate\nto the amount of data needed to specify an audio signal and the necessity of\npreserving both the long and short time coherence of the synthesised signal.\nVisual time-frequency representations such as the log-mel-spectrogram have\ngained in popularity. The log-mel-spectrogram is a perceptually informed\nrepresentation of audio that greatly compresses the amount of information\nrequired for the description of the sound. However, because of this\ncompression, this representation is not directly invertible. Both signal\nprocessing and machine learning techniques have previously been applied to the\ninversion of the log-mel-spectrogram but they both caused audible distortions\nin the synthesized sounds due to issues of temporal and spectral coherence. In\nthis paper, we outline the application of a sinusoidal model to the inversion\nof the log-mel-spectrogram for pitched musical instrument sounds outperforming\nstate-of-the-art deep learning methods. The approach could be later used as a\ngeneral decoding step from spectral to time intervals in neural applications.", "published": "2022-01-07 15:03:06", "link": "http://arxiv.org/abs/2201.02483v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio representations for deep learning in sound synthesis: A review", "abstract": "The rise of deep learning algorithms has led many researchers to withdraw\nfrom using classic signal processing methods for sound generation. Deep\nlearning models have achieved expressive voice synthesis, realistic sound\ntextures, and musical notes from virtual instruments. However, the most\nsuitable deep learning architecture is still under investigation. The choice of\narchitecture is tightly coupled to the audio representations. A sound's\noriginal waveform can be too dense and rich for deep learning models to deal\nwith efficiently - and complexity increases training time and computational\ncost. Also, it does not represent sound in the manner in which it is perceived.\nTherefore, in many cases, the raw audio has been transformed into a compressed\nand more meaningful form using upsampling, feature-extraction, or even by\nadopting a higher level illustration of the waveform. Furthermore, conditional\non the form chosen, additional conditioning representations, different model\narchitectures, and numerous metrics for evaluating the reconstructed sound have\nbeen investigated. This paper provides an overview of audio representations\napplied to sound synthesis using deep learning. Additionally, it presents the\nmost significant methods for developing and evaluating a sound synthesis\narchitecture using deep learning models, always depending on the audio\nrepresentation.", "published": "2022-01-07 15:08:47", "link": "http://arxiv.org/abs/2201.02490v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
