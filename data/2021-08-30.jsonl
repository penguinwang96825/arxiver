{"title": "LOT: A Story-Centric Benchmark for Evaluating Chinese Long Text\n  Understanding and Generation", "abstract": "Standard multi-task benchmarks are essential for developing pretraining\nmodels that can generalize to various downstream tasks. Existing benchmarks for\nnatural language processing (NLP) usually focus only on understanding or\ngenerating short texts. However, long text modeling requires many distinct\nabilities in contrast to short texts, such as the modeling of long-range\ndiscourse and commonsense relations, and the coherence and controllability of\ngeneration. The lack of standardized benchmarks makes it difficult to assess\nthese abilities of a model and fairly compare different models, especially\nChinese models. Therefore, we propose a story-centric benchmark named LOT for\nevaluating Chinese long text modeling, which aggregates two understanding tasks\nand two generation tasks. We construct new datasets for these tasks based on\nhuman-written Chinese stories with hundreds of words. Furthermore, we release\nan encoder-decoder-based Chinese long text pretraining model named LongLM with\nup to 1 billion parameters. We pretrain LongLM on 120G Chinese novels with two\ngenerative tasks including text infilling and conditional continuation.\nExtensive experiments show that LongLM outperforms similar-sized pretraining\nmodels substantially on both the understanding and generation tasks in LOT.", "published": "2021-08-30 02:38:32", "link": "http://arxiv.org/abs/2108.12960v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scheduled Sampling Based on Decoding Steps for Neural Machine\n  Translation", "abstract": "Scheduled sampling is widely used to mitigate the exposure bias problem for\nneural machine translation. Its core motivation is to simulate the inference\nscene during training by replacing ground-truth tokens with predicted tokens,\nthus bridging the gap between training and inference. However, vanilla\nscheduled sampling is merely based on training steps and equally treats all\ndecoding steps. Namely, it simulates an inference scene with uniform error\nrates, which disobeys the real inference scene, where larger decoding steps\nusually have higher error rates due to error accumulations. To alleviate the\nabove discrepancy, we propose scheduled sampling methods based on decoding\nsteps, increasing the selection chance of predicted tokens with the growth of\ndecoding steps. Consequently, we can more realistically simulate the inference\nscene during training, thus better bridging the gap between training and\ninference. Moreover, we investigate scheduled sampling based on both training\nsteps and decoding steps for further improvements. Experimentally, our\napproaches significantly outperform the Transformer baseline and vanilla\nscheduled sampling on three large-scale WMT tasks. Additionally, our approaches\nalso generalize well to the text summarization task on two popular benchmarks.", "published": "2021-08-30 02:41:42", "link": "http://arxiv.org/abs/2108.12963v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Base Completion Meets Transfer Learning", "abstract": "The aim of knowledge base completion is to predict unseen facts from existing\nfacts in knowledge bases. In this work, we introduce the first approach for\ntransfer of knowledge from one collection of facts to another without the need\nfor entity or relation matching. The method works for both canonicalized\nknowledge bases and uncanonicalized or open knowledge bases, i.e., knowledge\nbases where more than one copy of a real-world entity or relation may exist.\nSuch knowledge bases are a natural output of automated information extraction\ntools that extract structured data from unstructured text. Our main\ncontribution is a method that can make use of a large-scale pre-training on\nfacts, collected from unstructured text, to improve predictions on structured\ndata from a specific domain. The introduced method is the most impactful on\nsmall datasets such as ReVerb20K, where we obtained 6% absolute increase of\nmean reciprocal rank and 65% relative decrease of mean rank over the previously\nbest method, despite not relying on large pre-trained models like BERT.", "published": "2021-08-30 09:13:29", "link": "http://arxiv.org/abs/2108.13073v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NEREL: A Russian Dataset with Nested Named Entities, Relations and\n  Events", "abstract": "In this paper, we present NEREL, a Russian dataset for named entity\nrecognition and relation extraction. NEREL is significantly larger than\nexisting Russian datasets: to date it contains 56K annotated named entities and\n39K annotated relations. Its important difference from previous datasets is\nannotation of nested named entities, as well as relations within nested\nentities and at the discourse level. NEREL can facilitate development of novel\nmodels that can extract relations between nested named entities, as well as\nrelations on both sentence and document levels. NEREL also contains the\nannotation of events involving named entities and their roles in the events.\nThe NEREL collection is available via https://github.com/nerel-ds/NEREL.", "published": "2021-08-30 10:40:20", "link": "http://arxiv.org/abs/2108.13112v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Factual Consistency Evaluation for Text Summarization via Counterfactual\n  Estimation", "abstract": "Despite significant progress has been achieved in text summarization, factual\ninconsistency in generated summaries still severely limits its practical\napplications. Among the key factors to ensure factual consistency, a reliable\nautomatic evaluation metric is the first and the most crucial one. However,\nexisting metrics either neglect the intrinsic cause of the factual\ninconsistency or rely on auxiliary tasks, leading to an unsatisfied correlation\nwith human judgments or increasing the inconvenience of usage in practice. In\nlight of these challenges, we propose a novel metric to evaluate the factual\nconsistency in text summarization via counterfactual estimation, which\nformulates the causal relationship among the source document, the generated\nsummary, and the language prior. We remove the effect of language prior, which\ncan cause factual inconsistency, from the total causal effect on the generated\nsummary, and provides a simple yet effective way to evaluate consistency\nwithout relying on other auxiliary tasks. We conduct a series of experiments on\nthree public abstractive text summarization datasets, and demonstrate the\nadvantages of the proposed metric in both improving the correlation with human\njudgments and the convenience of usage. The source code is available at\nhttps://github.com/xieyxclack/factual_coco.", "published": "2021-08-30 11:48:41", "link": "http://arxiv.org/abs/2108.13134v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neuron-level Interpretation of Deep NLP Models: A Survey", "abstract": "The proliferation of deep neural networks in various domains has seen an\nincreased need for interpretability of these models. Preliminary work done\nalong this line and papers that surveyed such, are focused on high-level\nrepresentation analysis. However, a recent branch of work has concentrated on\ninterpretability at a more granular level of analyzing neurons within these\nmodels. In this paper, we survey the work done on neuron analysis including: i)\nmethods to discover and understand neurons in a network, ii) evaluation\nmethods, iii) major findings including cross architectural comparisons that\nneuron analysis has unraveled, iv) applications of neuron probing such as:\ncontrolling the model, domain adaptation etc., and v) a discussion on open\nissues and future research directions.", "published": "2021-08-30 11:54:21", "link": "http://arxiv.org/abs/2108.13138v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CSDS: A Fine-Grained Chinese Dataset for Customer Service Dialogue\n  Summarization", "abstract": "Dialogue summarization has drawn much attention recently. Especially in the\ncustomer service domain, agents could use dialogue summaries to help boost\ntheir works by quickly knowing customer's issues and service progress. These\napplications require summaries to contain the perspective of a single speaker\nand have a clear topic flow structure, while neither are available in existing\ndatasets. Therefore, in this paper, we introduce a novel Chinese dataset for\nCustomer Service Dialogue Summarization (CSDS). CSDS improves the abstractive\nsummaries in two aspects: (1) In addition to the overall summary for the whole\ndialogue, role-oriented summaries are also provided to acquire different\nspeakers' viewpoints. (2) All the summaries sum up each topic separately, thus\ncontaining the topic-level structure of the dialogue. We define tasks in CSDS\nas generating the overall summary and different role-oriented summaries for a\ngiven dialogue. Next, we compare various summarization methods on CSDS, and\nexperiment results show that existing methods are prone to generate redundant\nand incoherent summaries. Besides, the performance becomes much worse when\nanalyzing the performance on role-oriented summaries and topic structures. We\nhope that this study could benchmark Chinese dialogue summarization and benefit\nfurther studies.", "published": "2021-08-30 11:56:58", "link": "http://arxiv.org/abs/2108.13139v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AEDA: An Easier Data Augmentation Technique for Text Classification", "abstract": "This paper proposes AEDA (An Easier Data Augmentation) technique to help\nimprove the performance on text classification tasks. AEDA includes only random\ninsertion of punctuation marks into the original text. This is an easier\ntechnique to implement for data augmentation than EDA method (Wei and Zou,\n2019) with which we compare our results. In addition, it keeps the order of the\nwords while changing their positions in the sentence leading to a better\ngeneralized performance. Furthermore, the deletion operation in EDA can cause\nloss of information which, in turn, misleads the network, whereas AEDA\npreserves all the input information. Following the baseline, we perform\nexperiments on five different datasets for text classification. We show that\nusing the AEDA-augmented data for training, the models show superior\nperformance compared to using the EDA-augmented data in all five datasets. The\nsource code is available for further study and reproduction of the results.", "published": "2021-08-30 13:32:33", "link": "http://arxiv.org/abs/2108.13230v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "N24News: A New Dataset for Multimodal News Classification", "abstract": "Current news datasets merely focus on text features on the news and rarely\nleverage the feature of images, excluding numerous essential features for news\nclassification. In this paper, we propose a new dataset, N24News, which is\ngenerated from New York Times with 24 categories and contains both text and\nimage information in each news. We use a multitask multimodal method and the\nexperimental results show multimodal news classification performs better than\ntext-only news classification. Depending on the length of the text, the\nclassification accuracy can be increased by up to 8.11%. Our research reveals\nthe relationship between the performance of a multimodal classifier and its\nsub-classifiers, and also the possible improvements when applying multimodal in\nnews classification. N24News is shown to have great potential to prompt the\nmultimodal news studies.", "published": "2021-08-30 15:46:09", "link": "http://arxiv.org/abs/2108.13327v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Consistent Document-level Entity Linking: Joint Models for\n  Entity Linking and Coreference Resolution", "abstract": "We consider the task of document-level entity linking (EL), where it is\nimportant to make consistent decisions for entity mentions over the full\ndocument jointly. We aim to leverage explicit \"connections\" among mentions\nwithin the document itself: we propose to join the EL task with that of\ncoreference resolution (coref). This is complementary to related works that\nexploit either (i) implicit document information (e.g., latent relations among\nentity mentions, or general language models) or (ii) connections between the\ncandidate links (e.g, as inferred from the external knowledge base).\nSpecifically, we cluster mentions that are linked via coreference, and enforce\na single EL for all of the clustered mentions together. The latter constraint\nhas the added benefit of increased coverage by joining EL candidate lists for\nthe thus clustered mentions. We formulate the coref+EL problem as a structured\nprediction task over directed trees and use a globally normalized model to\nsolve it. Experimental results on two datasets show a boost of up to +5%\nF1-score on both coref and EL tasks, compared to their standalone counterparts.\nFor a subset of hard cases, with individual mentions lacking the correct EL in\ntheir candidate entity list, we obtain a +50% increase in accuracy.", "published": "2021-08-30 21:46:12", "link": "http://arxiv.org/abs/2108.13530v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hallucinated but Factual! Inspecting the Factuality of Hallucinations in\n  Abstractive Summarization", "abstract": "State-of-the-art abstractive summarization systems often generate\n\\emph{hallucinations}; i.e., content that is not directly inferable from the\nsource text. Despite being assumed incorrect, we find that much hallucinated\ncontent is factual, namely consistent with world knowledge. These factual\nhallucinations can be beneficial in a summary by providing useful background\ninformation. In this work, we propose a novel detection approach that separates\nfactual from non-factual hallucinations of entities. Our method utilizes an\nentity's prior and posterior probabilities according to pre-trained and\nfinetuned masked language models, respectively. Empirical results suggest that\nour approach vastly outperforms two baselines %in both accuracy and F1 scores\nand strongly correlates with human judgments. % on factuality classification\ntasks. Furthermore, we show that our detector, when used as a reward signal in\nan off-line reinforcement learning (RL) algorithm, significantly improves the\nfactuality of summaries while maintaining the level of abstractiveness.", "published": "2021-08-30 15:40:52", "link": "http://arxiv.org/abs/2109.09784v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RetroGAN: A Cyclic Post-Specialization System for Improving\n  Out-of-Knowledge and Rare Word Representations", "abstract": "Retrofitting is a technique used to move word vectors closer together or\nfurther apart in their space to reflect their relationships in a Knowledge Base\n(KB). However, retrofitting only works on concepts that are present in that KB.\nRetroGAN uses a pair of Generative Adversarial Networks (GANs) to learn a\none-to-one mapping between concepts and their retrofitted counterparts. It\napplies that mapping (post-specializes) to handle concepts that do not appear\nin the original KB in a manner similar to how some natural language systems\nhandle out-of-vocabulary entries. We test our system on three word-similarity\nbenchmarks and a downstream sentence simplification task and achieve the state\nof the art (CARD-660). Altogether, our results demonstrate our system's\neffectiveness for out-of-knowledge and rare word generalization.", "published": "2021-08-30 00:34:23", "link": "http://arxiv.org/abs/2108.12941v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Selective Differential Privacy for Language Modeling", "abstract": "With the increasing applications of language models, it has become crucial to\nprotect these models from leaking private information. Previous work has\nattempted to tackle this challenge by training RNN-based language models with\ndifferential privacy guarantees. However, applying classical differential\nprivacy to language models leads to poor model performance as the underlying\nprivacy notion is over-pessimistic and provides undifferentiated protection for\nall tokens in the data. Given that the private information in natural language\nis sparse (for example, the bulk of an email might not carry personally\nidentifiable information), we propose a new privacy notion, selective\ndifferential privacy, to provide rigorous privacy guarantees on the sensitive\nportion of the data to improve model utility. To realize such a new notion, we\ndevelop a corresponding privacy mechanism, Selective-DPSGD, for RNN-based\nlanguage models. Besides language modeling, we also apply the method to a more\nconcrete application--dialog systems. Experiments on both language modeling and\ndialog system building show that the proposed privacy-preserving mechanism\nachieves better utilities while remaining safe under various privacy attacks\ncompared to the baselines. The data and code are released at\nhttps://github.com/wyshi/lm_privacy to facilitate future research .", "published": "2021-08-30 01:11:10", "link": "http://arxiv.org/abs/2108.12944v3", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Shatter: An Efficient Transformer Encoder with Single-Headed\n  Self-Attention and Relative Sequence Partitioning", "abstract": "The highly popular Transformer architecture, based on self-attention, is the\nfoundation of large pretrained models such as BERT, that have become an\nenduring paradigm in NLP. While powerful, the computational resources and time\nrequired to pretrain such models can be prohibitive. In this work, we present\nan alternative self-attention architecture, Shatter, that more efficiently\nencodes sequence information by softly partitioning the space of relative\npositions and applying different value matrices to different parts of the\nsequence. This mechanism further allows us to simplify the multi-headed\nattention in Transformer to single-headed. We conduct extensive experiments\nshowing that Shatter achieves better performance than BERT, with pretraining\nbeing faster per step (15% on TPU), converging in fewer steps, and offering\nconsiderable memory savings (>50%). Put together, Shatter can be pretrained on\n8 V100 GPUs in 7 days, and match the performance of BERT_Base -- making the\ncost of pretraining much more affordable.", "published": "2021-08-30 07:42:12", "link": "http://arxiv.org/abs/2108.13032v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DuTrust: A Sentiment Analysis Dataset for Trustworthiness Evaluation", "abstract": "While deep learning models have greatly improved the performance of most\nartificial intelligence tasks, they are often criticized to be untrustworthy\ndue to the black-box problem. Consequently, many works have been proposed to\nstudy the trustworthiness of deep learning. However, as most open datasets are\ndesigned for evaluating the accuracy of model outputs, there is still a lack of\nappropriate datasets for evaluating the inner workings of neural networks. The\nlack of datasets obviously hinders the development of trustworthiness research.\nTherefore, in order to systematically evaluate the factors for building\ntrustworthy systems, we propose a novel and well-annotated sentiment analysis\ndataset to evaluate robustness and interpretability. To evaluate these factors,\nour dataset contains diverse annotations about the challenging distribution of\ninstances, manual adversarial instances and sentiment explanations. Several\nevaluation metrics are further proposed for interpretability and robustness.\nBased on the dataset and metrics, we conduct comprehensive comparisons for the\ntrustworthiness of three typical models, and also study the relations between\naccuracy, robustness and interpretability. We release this trustworthiness\nevaluation dataset at \\url{https://github/xyz} and hope our work can facilitate\nthe progress on building more trustworthy systems for real-world applications.", "published": "2021-08-30 11:58:16", "link": "http://arxiv.org/abs/2108.13140v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The effects of data size on Automated Essay Scoring engines", "abstract": "We study the effects of data size and quality on the performance on Automated\nEssay Scoring (AES) engines that are designed in accordance with three\ndifferent paradigms; A frequency and hand-crafted feature-based model, a\nrecurrent neural network model, and a pretrained transformer-based language\nmodel that is fine-tuned for classification. We expect that each type of model\nbenefits from the size and the quality of the training data in very different\nways. Standard practices for developing training data for AES engines were\nestablished with feature-based methods in mind, however, since neural networks\nare increasingly being considered in a production setting, this work seeks to\ninform us as to how to establish better training data for neural networks that\nwill be used in production.", "published": "2021-08-30 14:39:59", "link": "http://arxiv.org/abs/2108.13275v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FedKD: Communication Efficient Federated Learning via Knowledge\n  Distillation", "abstract": "Federated learning is widely used to learn intelligent models from\ndecentralized data. In federated learning, clients need to communicate their\nlocal model updates in each iteration of model learning. However, model updates\nare large in size if the model contains numerous parameters, and there usually\nneeds many rounds of communication until model converges. Thus, the\ncommunication cost in federated learning can be quite heavy. In this paper, we\npropose a communication efficient federated learning method based on knowledge\ndistillation. Instead of directly communicating the large models between\nclients and server, we propose an adaptive mutual distillation framework to\nreciprocally learn a student and a teacher model on each client, where only the\nstudent model is shared by different clients and updated collaboratively to\nreduce the communication cost. Both the teacher and student on each client are\nlearned on its local data and the knowledge distilled from each other, where\ntheir distillation intensities are controlled by their prediction quality. To\nfurther reduce the communication cost, we propose a dynamic gradient\napproximation method based on singular value decomposition to approximate the\nexchanged gradients with dynamic precision. Extensive experiments on benchmark\ndatasets in different tasks show that our approach can effectively reduce the\ncommunication cost and achieve competitive results.", "published": "2021-08-30 15:39:54", "link": "http://arxiv.org/abs/2108.13323v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "On the Multilingual Capabilities of Very Large-Scale English Language\n  Models", "abstract": "Generative Pre-trained Transformers (GPTs) have recently been scaled to\nunprecedented sizes in the history of machine learning. These models, solely\ntrained on the language modeling objective, have been shown to exhibit\noutstanding few-shot learning capabilities in a number of different tasks.\nNevertheless, aside from anecdotal experiences, little is known regarding their\nmultilingual capabilities, given the fact that the pre-training corpus is\nalmost entirely composed of English text. In this work, we investigate the\nmultilingual skills of GPT-3, focusing on one language that barely appears in\nthe pre-training corpus, Catalan, which makes the results especially\nmeaningful; we assume that our results may be relevant for other languages as\nwell. We find that the model shows an outstanding performance, particularly in\ngenerative tasks, with predictable limitations mostly in language understanding\ntasks but still with remarkable results given the zero-shot scenario. We\ninvestigate its potential and limits in extractive question-answering and\nnatural language generation, as well as the effect of scale in terms of model\nsize.", "published": "2021-08-30 16:18:50", "link": "http://arxiv.org/abs/2108.13349v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Want To Reduce Labeling Cost? GPT-3 Can Help", "abstract": "Data annotation is a time-consuming and labor-intensive process for many NLP\ntasks. Although there exist various methods to produce pseudo data labels, they\nare often task-specific and require a decent amount of labeled data to start\nwith. Recently, the immense language model GPT-3 with 175 billion parameters\nhas achieved tremendous improvement across many few-shot learning tasks. In\nthis paper, we explore ways to leverage GPT-3 as a low-cost data labeler to\ntrain other models. We find that, to make the downstream model achieve the same\nperformance on a variety of NLU and NLG tasks, it costs 50% to 96% less to use\nlabels from GPT-3 than using labels from humans. Furthermore, we propose a\nnovel framework of combining pseudo labels from GPT-3 with human labels, which\nleads to even better performance with limited labeling budget. These results\npresent a cost-effective data labeling methodology that is generalizable to\nmany practical applications.", "published": "2021-08-30 19:18:24", "link": "http://arxiv.org/abs/2108.13487v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Semi-Supervised Exaggeration Detection of Health Science Press Releases", "abstract": "Public trust in science depends on honest and factual communication of\nscientific papers. However, recent studies have demonstrated a tendency of news\nmedia to misrepresent scientific papers by exaggerating their findings. Given\nthis, we present a formalization of and study into the problem of exaggeration\ndetection in science communication. While there are an abundance of scientific\npapers and popular media articles written about them, very rarely do the\narticles include a direct link to the original paper, making data collection\nchallenging. We address this by curating a set of labeled press\nrelease/abstract pairs from existing expert annotated studies on exaggeration\nin press releases of scientific papers suitable for benchmarking the\nperformance of machine learning models on the task. Using limited data from\nthis and previous studies on exaggeration detection in science, we introduce\nMT-PET, a multi-task version of Pattern Exploiting Training (PET), which\nleverages knowledge from complementary cloze-style QA tasks to improve few-shot\nlearning. We demonstrate that MT-PET outperforms PET and supervised learning\nboth when data is limited, as well as when there is an abundance of data for\nthe main task.", "published": "2021-08-30 19:32:20", "link": "http://arxiv.org/abs/2108.13493v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ConVIScope: Visual Analytics for Exploring Patient Conversations", "abstract": "The proliferation of text messaging for mobile health is generating a large\namount of patient-doctor conversations that can be extremely valuable to health\ncare professionals. We present ConVIScope, a visual text analytic system that\ntightly integrates interactive visualization with natural language processing\nin analyzing patient-doctor conversations. ConVIScope was developed in\ncollaboration with healthcare professionals following a user-centered iterative\ndesign. Case studies with six domain experts suggest the potential utility of\nConVIScope and reveal lessons for further developments.", "published": "2021-08-30 20:52:59", "link": "http://arxiv.org/abs/2108.13514v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Linguistic Characterization of Divisive Topics Online: Case Studies on\n  Contentiousness in Abortion, Climate Change, and Gun Control", "abstract": "As public discourse continues to move and grow online, conversations about\ndivisive topics on social media platforms have also increased. These divisive\ntopics prompt both contentious and non-contentious conversations. Although what\ndistinguishes these conversations, often framed as what makes these\nconversations contentious, is known in broad strokes, much less is known about\nthe linguistic signature of these conversations. Prior work has shown that\ncontentious content and structure can be a predictor for this task, however,\nmost of them have been focused on conversation in general, very specific\nevents, or complex structural analysis. Additionally, many models used in prior\nwork have lacked interpret-ability, a key factor in online moderation. Our work\nfills these gaps by focusing on conversations from highly divisive topics\n(abortion, climate change, and gun control), operationalizing a set of novel\nlinguistic and conversational characteristics and user factors, and\nincorporating them to build interpretable models. We demonstrate that such\ncharacteristics can largely improve the performance of prediction on this task,\nand also enable nuanced interpretability. Our case studies on these three\ncontentious topics suggest that certain generic linguistic characteristics are\nhighly correlated with contentiousness in conversations while others\ndemonstrate significant contextual influences on specific divisive topics.", "published": "2021-08-30 23:55:38", "link": "http://arxiv.org/abs/2108.13556v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "HELMHOLTZ: A Verifier for Tezos Smart Contracts Based on Refinement\n  Types", "abstract": "A smart contract is a program executed on a blockchain, based on which many\ncryptocurrencies are implemented, and is being used for automating\ntransactions. Due to the large amount of money that smart contracts deal with,\nthere is a surging demand for a method that can statically and formally verify\nthem.\n  This article describes our type-based static verification tool HELMHOLTZ for\nMichelson, which is a statically typed stack-based language for writing smart\ncontracts that are executed on the blockchain platform Tezos. HELMHOLTZ is\ndesigned on top of our extension of Michelson's type system with refinement\ntypes. HELMHOLTZ takes a Michelson program annotated with a user-defined\nspecification written in the form of a refinement type as input; it then\ntypechecks the program against the specification based on the refinement type\nsystem, discharging the generated verification conditions with the SMT solver\nZ3. We briefly introduce our refinement type system for the core calculus\nMini-Michelson of Michelson, which incorporates the characteristic features\nsuch as compound datatypes (e.g., lists and pairs), higher-order functions, and\ninvocation of another contract. \\HELMHOLTZ{} successfully verifies several\npractical Michelson programs, including one that transfers money to an account\nand that checks a digital signature.", "published": "2021-08-30 03:02:37", "link": "http://arxiv.org/abs/2108.12971v2", "categories": ["cs.PL", "cs.CL", "cs.LO", "cs.SE"], "primary_category": "cs.PL"}
{"title": "ASR-GLUE: A New Multi-task Benchmark for ASR-Robust Natural Language\n  Understanding", "abstract": "Language understanding in speech-based systems have attracted much attention\nin recent years with the growing demand for voice interface applications.\nHowever, the robustness of natural language understanding (NLU) systems to\nerrors introduced by automatic speech recognition (ASR) is under-examined. %To\nfacilitate the research on ASR-robust general language understanding, In this\npaper, we propose ASR-GLUE benchmark, a new collection of 6 different NLU tasks\nfor evaluating the performance of models under ASR error across 3 different\nlevels of background noise and 6 speakers with various voice characteristics.\nBased on the proposed benchmark, we systematically investigate the effect of\nASR error on NLU tasks in terms of noise intensity, error type and speaker\nvariants. We further purpose two ways, correction-based method and data\naugmentation-based method to improve robustness of the NLU systems. Extensive\nexperimental results and analysises show that the proposed methods are\neffective to some extent, but still far from human performance, demonstrating\nthat NLU under ASR error is still very challenging and requires further\nresearch.", "published": "2021-08-30 08:11:39", "link": "http://arxiv.org/abs/2108.13048v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Speaker-Conditioned Hierarchical Modeling for Automated Speech Scoring", "abstract": "Automatic Speech Scoring (ASS) is the computer-assisted evaluation of a\ncandidate's speaking proficiency in a language. ASS systems face many\nchallenges like open grammar, variable pronunciations, and unstructured or\nsemi-structured content. Recent deep learning approaches have shown some\npromise in this domain. However, most of these approaches focus on extracting\nfeatures from a single audio, making them suffer from the lack of\nspeaker-specific context required to model such a complex task. We propose a\nnovel deep learning technique for non-native ASS, called speaker-conditioned\nhierarchical modeling. In our technique, we take advantage of the fact that\noral proficiency tests rate multiple responses for a candidate. We extract\ncontext vectors from these responses and feed them as additional\nspeaker-specific context to our network to score a particular response. We\ncompare our technique with strong baselines and find that such modeling\nimproves the model's average performance by 6.92% (maximum = 12.86%, minimum =\n4.51%). We further show both quantitative and qualitative insights into the\nimportance of this additional context in solving the problem of ASS.", "published": "2021-08-30 07:00:28", "link": "http://arxiv.org/abs/2109.00928v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Robust Cybersecurity Topic Classification Tool", "abstract": "In this research, we use user defined labels from three internet text sources\n(Reddit, Stackexchange, Arxiv) to train 21 different machine learning models\nfor the topic classification task of detecting cybersecurity discussions in\nnatural text. We analyze the false positive and false negative rates of each of\nthe 21 model's in a cross validation experiment. Then we present a\nCybersecurity Topic Classification (CTC) tool, which takes the majority vote of\nthe 21 trained machine learning models as the decision mechanism for detecting\ncybersecurity related text. We also show that the majority vote mechanism of\nthe CTC tool provides lower false negative and false positive rates on average\nthan any of the 21 individual models. We show that the CTC tool is scalable to\nthe hundreds of thousands of documents with a wall clock time on the order of\nhours.", "published": "2021-08-30 00:48:48", "link": "http://arxiv.org/abs/2109.02473v4", "categories": ["cs.IR", "cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Differentiable Prompt Makes Pre-trained Language Models Better Few-shot\n  Learners", "abstract": "Large-scale pre-trained language models have contributed significantly to\nnatural language processing by demonstrating remarkable abilities as few-shot\nlearners. However, their effectiveness depends mainly on scaling the model\nparameters and prompt design, hindering their implementation in most real-world\napplications. This study proposes a novel pluggable, extensible, and efficient\napproach named DifferentiAble pRompT (DART), which can convert small language\nmodels into better few-shot learners without any prompt engineering. The main\nprinciple behind this approach involves reformulating potential natural\nlanguage processing tasks into the task of a pre-trained language model and\ndifferentially optimizing the prompt template as well as the target label with\nbackpropagation. Furthermore, the proposed approach can be: (i) Plugged to any\npre-trained language models; (ii) Extended to widespread classification tasks.\nA comprehensive evaluation of standard NLP tasks demonstrates that the proposed\napproach achieves a better few-shot performance. Code is available in\nhttps://github.com/zjunlp/DART.", "published": "2021-08-30 12:29:25", "link": "http://arxiv.org/abs/2108.13161v7", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "InSE-NET: A Perceptually Coded Audio Quality Model based on CNN", "abstract": "Automatic coded audio quality assessment is an important task whose progress\nis hampered by the scarcity of human annotations, poor generalization to unseen\ncodecs, bitrates, content-types, and a lack of flexibility of existing\napproaches. One of the typical human-perception-related metrics, ViSQOL v3\n(ViV3), has been proven to provide a high correlation to the quality scores\nrated by humans. In this study, we take steps to tackle problems of predicting\ncoded audio quality by completely utilizing programmatically generated data\nthat is informed with expert domain knowledge. We propose a learnable neural\nnetwork, entitled InSE-NET, with a backbone of Inception and\nSqueeze-and-Excitation modules to assess the perceived quality of coded audio\nat a 48kHz sample rate. We demonstrate that synthetic data augmentation is\ncapable of enhancing the prediction. Our proposed method is intrusive, i.e. it\nrequires Gammatone spectrograms of unencoded reference signals. Besides a\ncomparable performance to ViV3, our approach provides a more robust prediction\ntowards higher bitrates.", "published": "2021-08-30 09:51:39", "link": "http://arxiv.org/abs/2108.13087v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Artificial bandwidth extension using deep neural network and $H^\\infty$\n  sampled-data control theory", "abstract": "Artificial bandwidth extension is applied to speech signals to improve their\nquality in narrowband telephonic communication. For accomplishing this, the\nmissing high-frequency (high-band) components of speech signals are recovered\nby utilizing a new extrapolation process based on sampled-data control theory\nand deep neural network (DNN). The $H^\\infty$ sampled-data control theory helps\nin designing of a high-band filter to recover the high-frequency signals by\noptimally utilizing the inter-sample signals. Non-stationary (time-varying)\ncharacteristics of speech signals forces to use numerous high-band filters.\nHence, we use a deep neural network for estimating the high-band filter\ninformation and a gain factor for a specified narrowband information of the\nunseen signal. The objective analysis is done on the TIMIT dataset and RSR15\ndataset. Additionally, the objective analysis is performed separately for the\nvoiced speech as well as for the unvoiced speech as generally needed in speech\nprocessing. Subjective analysis is done on the RSR15 dataset.", "published": "2021-08-30 15:45:39", "link": "http://arxiv.org/abs/2108.13326v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "RSKNet-MTSP: Effective and Portable Deep Architecture for Speaker\n  Verification", "abstract": "The convolutional neural network (CNN) based approaches have shown great\nsuccess for speaker verification (SV) tasks, where modeling long temporal\ncontext and reducing information loss of speaker characteristics are two\nimportant challenges significantly affecting the verification performance.\nPrevious works have introduced dilated convolution and multi-scale aggregation\nmethods to address above challenges. However, such methods are also hard to\nmake full use of some valuable information, which make it difficult to\nsubstantially improve the verification performance. To address above issues, we\nconstruct a novel CNN-based architecture for SV, called RSKNet-MTSP, where a\nresidual selective kernel block (RSKBlock) and a multiple time-scale statistics\npooling (MTSP) module are first proposed. The RSKNet-MTSP can capture both long\ntemporal context and neighbouring information, and gather more\nspeaker-discriminative information from multi-scale features. In order to\ndesign a portable model for real applications with limited resources, we then\npresent a lightweight version of RSKNet-MTSP, namely RSKNet-MTSP-L, which\nemploys a combination technique associating the depthwise separable\nconvolutions with low-rank factorization of weight matrices. Extensive\nexperiments are conducted on two public SV datasets, VoxCeleb and Speaker in\nthe Wild (SITW). The results demonstrate that 1) RSKNet-MTSP outperforms the\nstate-of-the-art deep embedding architectures by at least 9%-26% in all test\nsets. 2) RSKNet-MTSP-L achieves competitive performance compared with baseline\nmodels with 17%-39% less network parameters. The ablation experiments further\nillustrate that our proposed approaches can achieve substantial improvement\nover prior methods.", "published": "2021-08-30 14:08:23", "link": "http://arxiv.org/abs/2108.13249v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-Channel Transformer Transducer for Speech Recognition", "abstract": "Multi-channel inputs offer several advantages over single-channel, to improve\nthe robustness of on-device speech recognition systems. Recent work on\nmulti-channel transformer, has proposed a way to incorporate such inputs into\nend-to-end ASR for improved accuracy. However, this approach is characterized\nby a high computational complexity, which prevents it from being deployed in\non-device systems. In this paper, we present a novel speech recognition model,\nMulti-Channel Transformer Transducer (MCTT), which features end-to-end\nmulti-channel training, low computation cost, and low latency so that it is\nsuitable for streaming decoding in on-device speech recognition. In a far-field\nin-house dataset, our MCTT outperforms stagewise multi-channel models with\ntransformer-transducer up to 6.01% relative WER improvement (WERR). In\naddition, MCTT outperforms the multi-channel transformer up to 11.62% WERR, and\nis 15.8 times faster in terms of inference speed. We further show that we can\nimprove the computational cost of MCTT by constraining the future and previous\ncontext in attention computations.", "published": "2021-08-30 01:50:51", "link": "http://arxiv.org/abs/2108.12953v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised Learning of Deep Features for Music Segmentation", "abstract": "Music segmentation refers to the dual problem of identifying boundaries\nbetween, and labeling, distinct music segments, e.g., the chorus, verse, bridge\netc. in popular music. The performance of a range of music segmentation\nalgorithms has been shown to be dependent on the audio features chosen to\nrepresent the audio. Some approaches have proposed learning feature\ntransformations from music segment annotation data, although, such data is time\nconsuming or expensive to create and as such these approaches are likely\nlimited by the size of their datasets. While annotated music segmentation data\nis a scarce resource, the amount of available music audio is much greater. In\nthe neighboring field of semantic audio unsupervised deep learning has shown\npromise in improving the performance of solutions to the query-by-example and\nsound classification tasks. In this work, unsupervised training of deep feature\nembeddings using convolutional neural networks (CNNs) is explored for music\nsegmentation. The proposed techniques exploit only the time proximity of audio\nfeatures that is implicit in any audio timeline. Employing these embeddings in\na classic music segmentation algorithm is shown not only to significantly\nimprove the performance of this algorithm, but obtain state of the art\nperformance in unsupervised music segmentation.", "published": "2021-08-30 01:55:44", "link": "http://arxiv.org/abs/2108.12955v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Armor: A Benchmark for Meta-evaluation of Artificial Music", "abstract": "Objective evaluation (OE) is essential to artificial music, but it's often\nvery hard to determine the quality of OEs. Hitherto, subjective evaluation (SE)\nremains reliable and prevailing but suffers inevitable disadvantages that OEs\nmay overcome. Therefore, a meta-evaluation system is necessary for designers to\ntest the effectiveness of OEs. In this paper, we present Armor, a complex and\ncross-domain benchmark dataset that serves for this purpose. Since OEs should\ncorrelate with human judgment, we provide music as test cases for OEs and human\njudgment scores as touchstones. We also provide two meta-evaluation scenarios\nand their corresponding testing methods to assess the effectiveness of OEs. To\nthe best of our knowledge, Armor is the first comprehensive and rigorous\nframework that future works could follow, take example by, and improve upon for\nthe task of evaluating computer-generated music and the field of computational\nmusic as a whole. By analyzing different OE methods on our dataset, we observe\nthat there is still a huge gap between SE and OE, meaning that hard-coded\nalgorithms are far from catching human's judgment to the music.", "published": "2021-08-30 03:09:15", "link": "http://arxiv.org/abs/2108.12973v1", "categories": ["cs.SD", "cs.MM", "eess.AS", "H.5.5; J.5"], "primary_category": "cs.SD"}
{"title": "Neural HMMs are all you need (for high-quality attention-free TTS)", "abstract": "Neural sequence-to-sequence TTS has achieved significantly better output\nquality than statistical speech synthesis using HMMs. However, neural TTS is\ngenerally not probabilistic and uses non-monotonic attention. Attention\nfailures increase training time and can make synthesis babble incoherently.\nThis paper describes how the old and new paradigms can be combined to obtain\nthe advantages of both worlds, by replacing attention in neural TTS with an\nautoregressive left-right no-skip hidden Markov model defined by a neural\nnetwork. Based on this proposal, we modify Tacotron 2 to obtain an HMM-based\nneural TTS model with monotonic alignment, trained to maximise the full\nsequence likelihood without approximation. We also describe how to combine\nideas from classical and contemporary TTS for best results. The resulting\nexample system is smaller and simpler than Tacotron 2, and learns to speak with\nfewer iterations and less data, whilst achieving comparable naturalness prior\nto the post-net. Our approach also allows easy control over speaking rate.", "published": "2021-08-30 15:38:00", "link": "http://arxiv.org/abs/2108.13320v6", "categories": ["eess.AS", "cs.HC", "cs.LG", "cs.SD", "68T07", "I.2.7; I.2.6; G.3; H.5.5"], "primary_category": "eess.AS"}
