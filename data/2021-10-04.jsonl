{"title": "Structured abbreviation expansion in context", "abstract": "Ad hoc abbreviations are commonly found in informal communication channels\nthat favor shorter messages. We consider the task of reversing these\nabbreviations in context to recover normalized, expanded versions of\nabbreviated messages. The problem is related to, but distinct from, spelling\ncorrection, in that ad hoc abbreviations are intentional and may involve\nsubstantial differences from the original words. Ad hoc abbreviations are\nproductively generated on-the-fly, so they cannot be resolved solely by\ndictionary lookup. We generate a large, open-source data set of ad hoc\nabbreviations. This data is used to study abbreviation strategies and to\ndevelop two strong baselines for abbreviation expansion", "published": "2021-10-04 01:22:43", "link": "http://arxiv.org/abs/2110.01140v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TLDR9+: A Large Scale Resource for Extreme Summarization of Social Media\n  Posts", "abstract": "Recent models in developing summarization systems consist of millions of\nparameters and the model performance is highly dependent on the abundance of\ntraining data. While most existing summarization corpora contain data in the\norder of thousands to one million, generation of large-scale summarization\ndatasets in order of couple of millions is yet to be explored. Practically,\nmore data is better at generalizing the training patterns to unseen data. In\nthis paper, we introduce TLDR9+ -- a large-scale summarization dataset --\ncontaining over 9 million training instances extracted from Reddit discussion\nforum (https://github.com/sajastu/reddit_collector). This dataset is\nspecifically gathered to perform extreme summarization (i.e., generating\none-sentence summary in high compression and abstraction) and is more than\ntwice larger than the previously proposed dataset. We go one step further and\nwith the help of human annotations, we distill a more fine-grained dataset by\nsampling High-Quality instances from TLDR9+ and call it TLDRHQ dataset. We\nfurther pinpoint different state-of-the-art summarization models on our\nproposed datasets.", "published": "2021-10-04 02:40:55", "link": "http://arxiv.org/abs/2110.01159v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextualized Semantic Distance between Highly Overlapped Texts", "abstract": "Overlapping frequently occurs in paired texts in natural language processing\ntasks like text editing and semantic similarity evaluation. Better evaluation\nof the semantic distance between the overlapped sentences benefits the language\nsystem's understanding and guides the generation. Since conventional semantic\nmetrics are based on word representations, they are vulnerable to the\ndisturbance of overlapped components with similar representations. This paper\naims to address the issue with a mask-and-predict strategy. We take the words\nin the longest common sequence (LCS) as neighboring words and use masked\nlanguage modeling (MLM) from pre-trained language models (PLMs) to predict the\ndistributions on their positions. Our metric, Neighboring Distribution\nDivergence (NDD), represent the semantic distance by calculating the divergence\nbetween distributions in the overlapped parts. Experiments on Semantic Textual\nSimilarity show NDD to be more sensitive to various semantic differences,\nespecially on highly overlapped paired texts. Based on the discovery, we\nfurther implement an unsupervised and training-free method for text\ncompression, leading to a significant improvement on the previous\nperplexity-based method. The high scalability of our method even enables NDD to\noutperform the supervised state-of-the-art in domain adaption by a huge margin.\nFurther experiments on syntax and semantics analyses verify the awareness of\ninternal sentence structures, indicating the high potential of NDD for further\nstudies.", "published": "2021-10-04 03:59:15", "link": "http://arxiv.org/abs/2110.01176v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Self-Training for Few-Shot Learning of Language Model", "abstract": "As unlabeled data carry rich task-relevant information, they are proven\nuseful for few-shot learning of language model. The question is how to\neffectively make use of such data. In this work, we revisit the self-training\ntechnique for language model fine-tuning and present a state-of-the-art\nprompt-based few-shot learner, SFLM. Given two views of a text sample via weak\nand strong augmentation techniques, SFLM generates a pseudo label on the weakly\naugmented version. Then, the model predicts the same pseudo label when\nfine-tuned with the strongly augmented version. This simple approach is shown\nto outperform other state-of-the-art supervised and semi-supervised\ncounterparts on six sentence classification and six sentence-pair\nclassification benchmarking tasks. In addition, SFLM only relies on a few\nin-domain unlabeled data. We conduct a comprehensive analysis to demonstrate\nthe robustness of our proposed approach under various settings, including\naugmentation techniques, model scale, and few-shot knowledge transfer across\ntasks.", "published": "2021-10-04 08:51:36", "link": "http://arxiv.org/abs/2110.01256v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Aelf-supervised Tibetan-chinese Vocabulary Alignment Method Based On\n  Adversarial Learning", "abstract": "Tibetan is a low-resource language. In order to alleviate the shortage of\nparallel corpus between Tibetan and Chinese, this paper uses two monolingual\ncorpora and a small number of seed dictionaries to learn the semi-supervised\nmethod with seed dictionaries and self-supervised adversarial training method\nthrough the similarity calculation of word clusters in different embedded\nspaces and puts forward an improved self-supervised adversarial learning method\nof Tibetan and Chinese monolingual data alignment only. The experimental\nresults are as follows. First, the experimental results of Tibetan syllables\nChinese characters are not good, which reflects the weak semantic correlation\nbetween Tibetan syllables and Chinese characters; second, the seed dictionary\nof semi-supervised method made before 10 predicted word accuracy of 66.5\n(Tibetan - Chinese) and 74.8 (Chinese - Tibetan) results, to improve the\nself-supervision methods in both language directions have reached 53.5\naccuracy.", "published": "2021-10-04 08:56:33", "link": "http://arxiv.org/abs/2110.01258v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Information Bottleneck for Scientific Document Summarization", "abstract": "This paper presents an unsupervised extractive approach to summarize\nscientific long documents based on the Information Bottleneck principle.\nInspired by previous work which uses the Information Bottleneck principle for\nsentence compression, we extend it to document level summarization with two\nseparate steps. In the first step, we use signal(s) as queries to retrieve the\nkey content from the source document. Then, a pre-trained language model\nconducts further sentence search and edit to return the final extracted\nsummaries. Importantly, our work can be flexibly extended to a multi-view\nframework by different signals. Automatic evaluation on three scientific\ndocument datasets verifies the effectiveness of the proposed framework. The\nfurther human evaluation suggests that the extracted summaries cover more\ncontent aspects than previous systems.", "published": "2021-10-04 09:43:47", "link": "http://arxiv.org/abs/2110.01280v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SPaR.txt, a cheap Shallow Parsing approach for Regulatory texts", "abstract": "Automated Compliance Checking (ACC) systems aim to semantically parse\nbuilding regulations to a set of rules. However, semantic parsing is known to\nbe hard and requires large amounts of training data. The complexity of creating\nsuch training data has led to research that focuses on small sub-tasks, such as\nshallow parsing or the extraction of a limited subset of rules. This study\nintroduces a shallow parsing task for which training data is relatively cheap\nto create, with the aim of learning a lexicon for ACC. We annotate a small\ndomain-specific dataset of 200 sentences, SPaR.txt, and train a sequence tagger\nthat achieves 79,93 F1-score on the test set. We then show through manual\nevaluation that the model identifies most (89,84%) defined terms in a set of\nbuilding regulation documents, and that both contiguous and discontiguous\nMulti-Word Expressions (MWE) are discovered with reasonable accuracy (70,3%).", "published": "2021-10-04 10:00:22", "link": "http://arxiv.org/abs/2110.01295v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Protagonists' Tagger in Literary Domain -- New Datasets and a Method for\n  Person Entity Linkage", "abstract": "Semantic annotation of long texts, such as novels, remains an open challenge\nin Natural Language Processing (NLP). This research investigates the problem of\ndetecting person entities and assigning them unique identities, i.e.,\nrecognizing people (especially main characters) in novels. We prepared a method\nfor person entity linkage (named entity recognition and disambiguation) and new\ntesting datasets. The datasets comprise 1,300 sentences from 13 classic novels\nof different genres that a novel reader had manually annotated. Our process of\nidentifying literary characters in a text, implemented in protagonistTagger,\ncomprises two stages: (1) named entity recognition (NER) of persons, (2) named\nentity disambiguation (NED) - matching each recognized person with the literary\ncharacter's full name, based on approximate text matching. The\nprotagonistTagger achieves both precision and recall of above 83% on the\nprepared testing sets. Finally, we gathered a corpus of 13 full-text novels\ntagged with protagonistTagger that comprises more than 35,000 mentions of\nliterary characters.", "published": "2021-10-04 11:54:43", "link": "http://arxiv.org/abs/2110.01349v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "JuriBERT: A Masked-Language Model Adaptation for French Legal Text", "abstract": "Language models have proven to be very useful when adapted to specific\ndomains. Nonetheless, little research has been done on the adaptation of\ndomain-specific BERT models in the French language. In this paper, we focus on\ncreating a language model adapted to French legal text with the goal of helping\nlaw professionals. We conclude that some specific tasks do not benefit from\ngeneric language models pre-trained on large amounts of data. We explore the\nuse of smaller architectures in domain-specific sub-languages and their\nbenefits for French legal text. We prove that domain-specific pre-trained\nmodels can perform better than their equivalent generalised ones in the legal\ndomain. Finally, we release JuriBERT, a new set of BERT models adapted to the\nFrench legal domain.", "published": "2021-10-04 14:51:24", "link": "http://arxiv.org/abs/2110.01485v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics", "abstract": "Much of recent progress in NLU was shown to be due to models' learning\ndataset-specific heuristics. We conduct a case study of generalization in NLI\n(from MNLI to the adversarially constructed HANS dataset) in a range of\nBERT-based architectures (adapters, Siamese Transformers, HEX debiasing), as\nwell as with subsampling the data and increasing the model size. We report 2\nsuccessful and 3 unsuccessful strategies, all providing insights into how\nTransformer-based models learn to generalize.", "published": "2021-10-04 15:37:07", "link": "http://arxiv.org/abs/2110.01518v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Theme Detection in Personal Finance Questions", "abstract": "Banking call centers receive millions of calls annually, with much of the\ninformation in these calls unavailable to analysts interested in tracking new\nand emerging call center trends. In this study we present an approach to call\ncenter theme detection that captures the occurrence of multiple themes in a\nquestion, using a publicly available corpus of StackExchange personal finance\nquestions, labeled by users with topic tags, as a testbed. To capture the\noccurrence of multiple themes in a single question, the approach encodes and\nclusters at the sentence- rather than question-level. We also present a\ncomparison of state-of-the-art sentence encoding models, including the SBERT\nfamily of sentence encoders. We frame our evaluation as a multiclass\nclassification task and show that a simple combination of the original sentence\ntext, Universal Sentence Encoder, and KMeans outperforms more sophisticated\ntechniques that involve semantic parsing, SBERT-family models, and HDBSCAN. Our\nhighest performing approach achieves a Micro-F1 of 0.46 for this task and we\nshow that the resulting clusters, even when slightly noisy, contain sentences\nthat are topically consistent with the label associated with the cluster.", "published": "2021-10-04 16:44:16", "link": "http://arxiv.org/abs/2110.01550v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text-based automatic personality prediction: A bibliographic review", "abstract": "Personality detection is an old topic in psychology and Automatic Personality\nPrediction (or Perception) (APP) is the automated (computationally) forecasting\nof the personality on different types of human generated/exchanged contents\n(such as text, speech, image, video). The principal objective of this study is\nto offer a shallow (overall) review of natural language processing approaches\non APP since 2010. With the advent of deep learning and following it\ntransfer-learning and pre-trained model in NLP, APP research area has been a\nhot topic, so in this review, methods are categorized into three; pre-trained\nindependent, pre-trained model based, multimodal approaches. Also, to achieve a\ncomprehensive comparison, reported results are informed by datasets.", "published": "2021-10-04 04:51:11", "link": "http://arxiv.org/abs/2110.01186v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Identifying non-natural language artifacts in bug reports", "abstract": "Bug reports are a popular target for natural language processing (NLP).\nHowever, bug reports often contain artifacts such as code snippets, log outputs\nand stack traces. These artifacts not only inflate the bug reports with noise,\nbut often constitute a real problem for the NLP approach at hand and have to be\nremoved. In this paper, we present a machine learning based approach to\nclassify content into natural language and artifacts at line level implemented\nin Python. We show how data from GitHub issue trackers can be used for\nautomated training set generation, and present a custom preprocessing approach\nfor bug reports. Our model scores at 0.95 ROC-AUC and 0.93 F1 against our\nmanually annotated validation set, and classifies 10k lines in 0.72 seconds. We\ncross evaluated our model against a foreign dataset and a foreign R model for\nthe same task. The Python implementation of our model and our datasets are made\npublicly available under an open source license.", "published": "2021-10-04 11:33:51", "link": "http://arxiv.org/abs/2110.01336v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "DeepA2: A Modular Framework for Deep Argument Analysis with Pretrained\n  Neural Text2Text Language Models", "abstract": "In this paper, we present and implement a multi-dimensional, modular\nframework for performing deep argument analysis (DeepA2) using current\npre-trained language models (PTLMs). ArgumentAnalyst -- a T5 model (Raffel et\nal. 2020) set up and trained within DeepA2 -- reconstructs argumentative texts,\nwhich advance an informal argumentation, as valid arguments: It inserts, e.g.,\nmissing premises and conclusions, formalizes inferences, and coherently links\nthe logical reconstruction to the source text. We create a synthetic corpus for\ndeep argument analysis, and evaluate ArgumentAnalyst on this new dataset as\nwell as on existing data, specifically EntailmentBank (Dalvi et al. 2021). Our\nempirical findings vindicate the overall framework and highlight the advantages\nof a modular design, in particular its ability to emulate established\nheuristics (such as hermeneutic cycles), to explore the model's uncertainty, to\ncope with the plurality of correct solutions (underdetermination), and to\nexploit higher-order evidence.", "published": "2021-10-04 15:24:07", "link": "http://arxiv.org/abs/2110.01509v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Proposed Conceptual Framework for a Representational Approach to\n  Information Retrieval", "abstract": "This paper outlines a conceptual framework for understanding recent\ndevelopments in information retrieval and natural language processing that\nattempts to integrate dense and sparse retrieval methods. I propose a\nrepresentational approach that breaks the core text retrieval problem into a\nlogical scoring model and a physical retrieval model. The scoring model is\ndefined in terms of encoders, which map queries and documents into a\nrepresentational space, and a comparison function that computes query-document\nscores. The physical retrieval model defines how a system produces the top-$k$\nscoring documents from an arbitrarily large corpus with respect to a query. The\nscoring model can be further analyzed along two dimensions: dense vs. sparse\nrepresentations and supervised (learned) vs. unsupervised approaches. I show\nthat many recently proposed retrieval methods, including multi-stage ranking\ndesigns, can be seen as different parameterizations in this framework, and that\na unified view suggests a number of open research questions, providing a\nroadmap for future work. As a bonus, this conceptual framework establishes\nconnections to sentence similarity tasks in natural language processing and\ninformation access \"technologies\" prior to the dawn of computing.", "published": "2021-10-04 15:57:02", "link": "http://arxiv.org/abs/2110.01529v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Encoder Adaptation of Dense Passage Retrieval for Open-Domain Question\n  Answering", "abstract": "One key feature of dense passage retrievers (DPR) is the use of separate\nquestion and passage encoder in a bi-encoder design. Previous work on\ngeneralization of DPR mainly focus on testing both encoders in tandem on\nout-of-distribution (OOD) question-answering (QA) tasks, which is also known as\ndomain adaptation. However, it is still unknown how DPR's individual\nquestion/passage encoder affects generalization. Specifically, in this paper,\nwe want to know how an in-distribution (IND) question/passage encoder would\ngeneralize if paired with an OOD passage/question encoder from another domain.\nWe refer to this challenge as \\textit{encoder adaptation}. To answer this\nquestion, we inspect different combinations of DPR's question and passage\nencoder learned from five benchmark QA datasets on both in-domain and\nout-of-domain questions. We find that the passage encoder has more influence on\nthe lower bound of generalization while the question encoder seems to affect\nthe upper bound in general. For example, applying an OOD passage encoder\nusually hurts the retrieval accuracy while an OOD question encoder sometimes\neven improves the accuracy.", "published": "2021-10-04 17:51:07", "link": "http://arxiv.org/abs/2110.01599v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Privacy enabled Financial Text Classification using Differential Privacy\n  and Federated Learning", "abstract": "Privacy is important considering the financial Domain as such data is highly\nconfidential and sensitive. Natural Language Processing (NLP) techniques can be\napplied for text classification and entity detection purposes in financial\ndomains such as customer feedback sentiment analysis, invoice entity detection,\ncategorisation of financial documents by type etc. Due to the sensitive nature\nof such data, privacy measures need to be taken for handling and training large\nmodels with such data. In this work, we propose a contextualized transformer\n(BERT and RoBERTa) based text classification model integrated with privacy\nfeatures such as Differential Privacy (DP) and Federated Learning (FL). We\npresent how to privately train NLP models and desirable privacy-utility\ntradeoffs and evaluate them on the Financial Phrase Bank dataset.", "published": "2021-10-04 18:15:32", "link": "http://arxiv.org/abs/2110.01643v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "AI Chains: Transparent and Controllable Human-AI Interaction by Chaining\n  Large Language Model Prompts", "abstract": "Although large language models (LLMs) have demonstrated impressive potential\non simple tasks, their breadth of scope, lack of transparency, and insufficient\ncontrollability can make them less effective when assisting humans on more\ncomplex tasks. In response, we introduce the concept of Chaining LLM steps\ntogether, where the output of one step becomes the input for the next, thus\naggregating the gains per step. We first define a set of LLM primitive\noperations useful for Chain construction, then present an interactive system\nwhere users can modify these Chains, along with their intermediate results, in\na modular way. In a 20-person user study, we found that Chaining not only\nimproved the quality of task outcomes, but also significantly enhanced system\ntransparency, controllability, and sense of collaboration. Additionally, we saw\nthat users developed new ways of interacting with LLMs through Chains: they\nleveraged sub-tasks to calibrate model expectations, compared and contrasted\nalternative strategies by observing parallel downstream effects, and debugged\nunexpected model outputs by \"unit-testing\" sub-components of a Chain. In two\ncase studies, we further explore how LLM Chains may be used in future\napplications", "published": "2021-10-04 19:59:38", "link": "http://arxiv.org/abs/2110.01691v3", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Using Single-Trial Representational Similarity Analysis with EEG to\n  track semantic similarity in emotional word processing", "abstract": "Electroencephalography (EEG) is a powerful non-invasive brain imaging\ntechnique with a high temporal resolution that has seen extensive use across\nmultiple areas of cognitive science research. This thesis adapts\nrepresentational similarity analysis (RSA) to single-trial EEG datasets and\nintroduces its principles to EEG researchers unfamiliar with multivariate\nanalyses. We have two separate aims: 1. we want to explore the effectiveness of\nsingle-trial RSA on EEG datasets; 2. we want to utilize single-trial RSA and\ncomputational semantic models to investigate the role of semantic meaning in\nemotional word processing. We report two primary findings: 1. single-trial RSA\non EEG datasets can produce meaningful and interpretable results given a high\nnumber of trials and subjects; 2. single-trial RSA reveals that emotional\nprocessing in the 500-800ms time window is associated with additional semantic\nanalysis.", "published": "2021-10-04 17:17:38", "link": "http://arxiv.org/abs/2110.03529v1", "categories": ["q-bio.NC", "cs.CL"], "primary_category": "q-bio.NC"}
{"title": "TBCOV: Two Billion Multilingual COVID-19 Tweets with Sentiment, Entity,\n  Geo, and Gender Labels", "abstract": "The widespread usage of social networks during mass convergence events, such\nas health emergencies and disease outbreaks, provides instant access to\ncitizen-generated data that carry rich information about public opinions,\nsentiments, urgent needs, and situational reports. Such information can help\nauthorities understand the emergent situation and react accordingly. Moreover,\nsocial media plays a vital role in tackling misinformation and disinformation.\nThis work presents TBCOV, a large-scale Twitter dataset comprising more than\ntwo billion multilingual tweets related to the COVID-19 pandemic collected\nworldwide over a continuous period of more than one year. More importantly,\nseveral state-of-the-art deep learning models are used to enrich the data with\nimportant attributes, including sentiment labels, named-entities (e.g.,\nmentions of persons, organizations, locations), user types, and gender\ninformation. Last but not least, a geotagging method is proposed to assign\ncountry, state, county, and city information to tweets, enabling a myriad of\ndata analysis tasks to understand real-world issues. Our sentiment and trend\nanalyses reveal interesting insights and confirm TBCOV's broad coverage of\nimportant topics.", "published": "2021-10-04 06:17:12", "link": "http://arxiv.org/abs/2110.03664v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Classification of hierarchical text using geometric deep learning: the\n  case of clinical trials corpus", "abstract": "We consider the hierarchical representation of documents as graphs and use\ngeometric deep learning to classify them into different categories. While graph\nneural networks can efficiently handle the variable structure of hierarchical\ndocuments using the permutation invariant message passing operations, we show\nthat we can gain extra performance improvements using our proposed selective\ngraph pooling operation that arises from the fact that some parts of the\nhierarchy are invariable across different documents. We applied our model to\nclassify clinical trial (CT) protocols into completed and terminated\ncategories. We use bag-of-words based, as well as pre-trained transformer-based\nembeddings to featurize the graph nodes, achieving f1-scores around 0.85 on a\npublicly available large scale CT registry of around 360K protocols. We further\ndemonstrate how the selective pooling can add insights into the CT termination\nstatus prediction. We make the source code and dataset splits accessible.", "published": "2021-10-04 20:50:17", "link": "http://arxiv.org/abs/2110.15710v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Interplay Between Sparsity, Naturalness, Intelligibility, and\n  Prosody in Speech Synthesis", "abstract": "Are end-to-end text-to-speech (TTS) models over-parametrized? To what extent\ncan these models be pruned, and what happens to their synthesis capabilities?\nThis work serves as a starting point to explore pruning both spectrogram\nprediction networks and vocoders. We thoroughly investigate the tradeoffs\nbetween sparsity and its subsequent effects on synthetic speech. Additionally,\nwe explored several aspects of TTS pruning: amount of finetuning data versus\nsparsity, TTS-Augmentation to utilize unspoken text, and combining knowledge\ndistillation and pruning. Our findings suggest that not only are end-to-end TTS\nmodels highly prunable, but also, perhaps surprisingly, pruned TTS models can\nproduce synthetic speech with equal or higher naturalness and intelligibility,\nwith similar prosody. All of our experiments are conducted on publicly\navailable models, and findings in this work are backed by large-scale\nsubjective tests and objective measures. Code and 200 pruned models are made\navailable to facilitate future research on efficiency in TTS.", "published": "2021-10-04 02:03:28", "link": "http://arxiv.org/abs/2110.01147v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Beyond Topics: Discovering Latent Healthcare Objectives from Event\n  Sequences", "abstract": "A meaningful understanding of clinical protocols and patient pathways helps\nimprove healthcare outcomes. Electronic health records (EHR) reflect real-world\ntreatment behaviours that are used to enhance healthcare management but present\nchallenges; protocols and pathways are often loosely defined and with elements\nfrequently not recorded in EHRs, complicating the enhancement. To solve this\nchallenge, healthcare objectives associated with healthcare management\nactivities can be indirectly observed in EHRs as latent topics. Topic models,\nsuch as Latent Dirichlet Allocation (LDA), are used to identify latent patterns\nin EHR data. However, they do not examine the ordered nature of EHR sequences,\nnor do they appraise individual events in isolation. Our novel approach, the\nCategorical Sequence Encoder (CaSE) addresses these shortcomings. The\nsequential nature of EHRs is captured by CaSE's event-level representations,\nrevealing latent healthcare objectives. In synthetic EHR sequences, CaSE\noutperforms LDA by up to 37% at identifying healthcare objectives. In the\nreal-world MIMIC-III dataset, CaSE identifies meaningful representations that\ncould critically enhance protocol and pathway development.", "published": "2021-10-04 02:52:14", "link": "http://arxiv.org/abs/2110.01160v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LawSum: A weakly supervised approach for Indian Legal Document\n  Summarization", "abstract": "Unlike the courts in western countries, public records of Indian judiciary\nare completely unstructured and noisy. No large scale publicly available\nannotated datasets of Indian legal documents exist till date. This limits the\nscope for legal analytics research. In this work, we propose a new dataset\nconsisting of over 10,000 judgements delivered by the supreme court of India\nand their corresponding hand written summaries. The proposed dataset is\npre-processed by normalising common legal abbreviations, handling spelling\nvariations in named entities, handling bad punctuations and accurate sentence\ntokenization. Each sentence is tagged with their rhetorical roles. We also\nannotate each judgement with several attributes like date, names of the\nplaintiffs, defendants and the people representing them, judges who delivered\nthe judgement, acts/statutes that are cited and the most common citations used\nto refer the judgement. Further, we propose an automatic labelling technique\nfor identifying sentences which have summary worthy information. We demonstrate\nthat this auto labeled data can be used effectively to train a weakly\nsupervised sentence extractor with high accuracy. Some possible applications of\nthis dataset besides legal document summarization can be in retrieval, citation\nanalysis and prediction of decisions by a particular judge.", "published": "2021-10-04 04:54:50", "link": "http://arxiv.org/abs/2110.01188v3", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Building a Noisy Audio Dataset to Evaluate Machine Learning Approaches\n  for Automatic Speech Recognition Systems", "abstract": "Automatic speech recognition systems are part of people's daily lives,\nembedded in personal assistants and mobile phones, helping as a facilitator for\nhuman-machine interaction while allowing access to information in a practically\nintuitive way. Such systems are usually implemented using machine learning\ntechniques, especially with deep neural networks. Even with its high\nperformance in the task of transcribing text from speech, few works address the\nissue of its recognition in noisy environments and, usually, the datasets used\ndo not contain noisy audio examples, while only mitigating this issue using\ndata augmentation techniques. This work aims to present the process of building\na dataset of noisy audios, in a specific case of degenerated audios due to\ninterference, commonly present in radio transmissions. Additionally, we present\ninitial results of a classifier that uses such data for evaluation, indicating\nthe benefits of using this dataset in the recognizer's training process. Such\nrecognizer achieves an average result of 0.4116 in terms of character error\nrate in the noisy set (SNR = 30).", "published": "2021-10-04 13:08:53", "link": "http://arxiv.org/abs/2110.01425v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Perhaps PTLMs Should Go to School -- A Task to Assess Open Book and\n  Closed Book QA", "abstract": "Our goal is to deliver a new task and leaderboard to stimulate research on\nquestion answering and pre-trained language models (PTLMs) to understand a\nsignificant instructional document, e.g., an introductory college textbook or a\nmanual. PTLMs have shown great success in many question-answering tasks, given\nsignificant supervised training, but much less so in zero-shot settings. We\npropose a new task that includes two college-level introductory texts in the\nsocial sciences (American Government 2e) and humanities (U.S. History),\nhundreds of true/false statements based on review questions written by the\ntextbook authors, validation/development tests based on the first eight\nchapters of the textbooks, blind tests based on the remaining textbook\nchapters, and baseline results given state-of-the-art PTLMs. Since the\nquestions are balanced, random performance should be ~50%. T5, fine-tuned with\nBoolQ achieves the same performance, suggesting that the textbook's content is\nnot pre-represented in the PTLM. Taking the exam closed book, but having read\nthe textbook (i.e., adding the textbook to T5's pre-training), yields at best\nminor improvement (56%), suggesting that the PTLM may not have \"understood\" the\ntextbook (or perhaps misunderstood the questions). Performance is better (~60%)\nwhen the exam is taken open-book (i.e., allowing the machine to automatically\nretrieve a paragraph and use it to answer the question).", "published": "2021-10-04 16:45:28", "link": "http://arxiv.org/abs/2110.01552v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Rerunning OCR: A Machine Learning Approach to Quality Assessment and\n  Enhancement Prediction", "abstract": "Iterating with new and improved OCR solutions enforces decision making when\nit comes to targeting the right candidates for reprocessing. This especially\napplies when the underlying data collection is of considerable size and rather\ndiverse in terms of fonts, languages, periods of publication and consequently\nOCR quality. This article captures the efforts of the National Library of\nLuxembourg to support those targeting decisions. They are crucial in order to\nguarantee low computational overhead and reduced quality degradation risks,\ncombined with a more quantifiable OCR improvement. In particular, this work\nexplains the methodology of the library with respect to text block level\nquality assessment. Through extension of this technique, a regression model,\nthat is able to take into account the enhancement potential of a new OCR\nengine, is also presented. They both mark promising approaches, especially for\ncultural institutions dealing with historical data of lower quality.", "published": "2021-10-04 18:52:59", "link": "http://arxiv.org/abs/2110.01661v5", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Quantifying the Suicidal Tendency on Social Media: A Survey", "abstract": "Amid lockdown period more people express their feelings over social media\nplatforms due to closed third-place and academic researchers have witnessed\nstrong associations between the mental healthcare and social media posts. The\nstress for a brief period may lead to clinical depressions and the long-lasting\ntraits of prevailing depressions can be life threatening with suicidal ideation\nas the possible outcome. The increasing concern towards the rise in number of\nsuicide cases is because it is one of the leading cause of premature but\npreventable death. Recent studies have shown that mining social media data has\nhelped in quantifying the suicidal tendency of users at risk. This potential\nmanuscript elucidates the taxonomy of mental healthcare and highlights some\nrecent attempts in examining the potential of quantifying suicidal tendency on\nsocial media data. This manuscript presents the classification of heterogeneous\nfeatures from social media data and handling feature vector representation.\nAiming to identify the new research directions and advances in the development\nof Machine Learning (ML) and Deep Learning (DL) based models, a quantitative\nsynthesis and a qualitative review was carried out with corpus of over 77\npotential research articles related to stress, depression and suicide risk from\n2013 to 2021.", "published": "2021-10-04 12:26:14", "link": "http://arxiv.org/abs/2110.03663v3", "categories": ["cs.SI", "cs.CL", "cs.IR"], "primary_category": "cs.SI"}
{"title": "Skill Induction and Planning with Latent Language", "abstract": "We present a framework for learning hierarchical policies from\ndemonstrations, using sparse natural language annotations to guide the\ndiscovery of reusable skills for autonomous decision-making. We formulate a\ngenerative model of action sequences in which goals generate sequences of\nhigh-level subtask descriptions, and these descriptions generate sequences of\nlow-level actions. We describe how to train this model using primarily\nunannotated demonstrations by parsing demonstrations into sequences of named\nhigh-level subtasks, using only a small number of seed annotations to ground\nlanguage in action. In trained models, natural language commands index a\ncombinatorial library of skills; agents can use these skills to plan by\ngenerating high-level instruction sequences tailored to novel goals. We\nevaluate this approach in the ALFRED household simulation environment,\nproviding natural language annotations for only 10% of demonstrations. It\nachieves task completion rates comparable to state-of-the-art models\n(outperforming several recent methods with access to ground-truth plans during\ntraining and evaluation) while providing structured and human-readable\nhigh-level plans.", "published": "2021-10-04 15:36:32", "link": "http://arxiv.org/abs/2110.01517v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.RO"], "primary_category": "cs.LG"}
{"title": "Audio Captioning Using Sound Event Detection", "abstract": "This technical report proposes an audio captioning system for DCASE 2021 Task\n6 audio captioning challenge. Our proposed model is based on an encoder-decoder\narchitecture with bi-directional Gated Recurrent Units (BiGRU) using pretrained\naudio features and sound event detection. A pretrained neural network (PANN) is\nused to extract audio features and Word2Vec is selected with the aim of\nextracting word embeddings from the audio captions. To create semantically\nmeaningful captions, we extract sound events from the audio clips and feed the\nencoder-decoder architecture with sound events in addition to PANNs features.\nOur experiments on the Clotho dataset show that our proposed method\nsignificantly achieves better results than the challenge baseline model across\nall evaluation metrics.", "published": "2021-10-04 06:49:18", "link": "http://arxiv.org/abs/2110.01210v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Individualized sound pressure equalization in hearing devices exploiting\n  an electro-acoustic model", "abstract": "To improve sound quality in hearing devices, the hearing device output should\nbe appropriately equalized. To achieve optimal individualized equalization\ntypically requires knowledge of all transfer functions between the source, the\nhearing device, and the individual eardrum. However, in practice the\nmeasurement of all of these transfer functions is not feasible. This study\ninvestigates sound pressure equalization using different transfer function\nestimates. Specifically, an electro-acoustic model is used to predict the sound\npressure at the individual eardrum, and average estimates are used to predict\nthe remaining transfer functions. Experimental results show that using these\nassumptions a practically feasible and close-to-optimal individualized sound\npressure equalization can be achieved.", "published": "2021-10-04 12:56:42", "link": "http://arxiv.org/abs/2110.01422v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "WaveBeat: End-to-end beat and downbeat tracking in the time domain", "abstract": "Deep learning approaches for beat and downbeat tracking have brought\nadvancements. However, these approaches continue to rely on hand-crafted,\nsubsampled spectral features as input, restricting the information available to\nthe model. In this work, we propose WaveBeat, an end-to-end approach for joint\nbeat and downbeat tracking operating directly on waveforms. This method forgoes\nengineered spectral features, and instead, produces beat and downbeat\npredictions directly from the waveform, the first of its kind for this task.\nOur model utilizes temporal convolutional networks (TCNs) operating on\nwaveforms that achieve a very large receptive field ($\\geq$ 30 s) at audio\nsample rates in a memory efficient manner by employing rapidly growing dilation\nfactors with fewer layers. With a straightforward data augmentation strategy,\nour method outperforms previous state-of-the-art methods on some datasets,\nwhile producing comparable results on others, demonstrating the potential for\ntime domain approaches.", "published": "2021-10-04 13:31:42", "link": "http://arxiv.org/abs/2110.01436v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploiting Pre-Trained ASR Models for Alzheimer's Disease Recognition\n  Through Spontaneous Speech", "abstract": "Alzheimer's disease (AD) is a progressive neurodegenerative disease and\nrecently attracts extensive attention worldwide. Speech technology is\nconsidered a promising solution for the early diagnosis of AD and has been\nenthusiastically studied. Most recent works concentrate on the use of advanced\nBERT-like classifiers for AD detection. Input to these classifiers are speech\ntranscripts produced by automatic speech recognition (ASR) models. The major\nchallenge is that the quality of transcription could degrade significantly\nunder complex acoustic conditions in the real world. The detection performance,\nin consequence, is largely limited. This paper tackles the problem via\ntailoring and adapting pre-trained neural-network based ASR model for the\ndownstream AD recognition task. Only bottom layers of the ASR model are\nretained. A simple fully-connected neural network is added on top of the\ntailored ASR model for classification. The heavy BERT classifier is discarded.\nThe resulting model is light-weight and can be fine-tuned in an end-to-end\nmanner for AD recognition. Our proposed approach takes only raw speech as\ninput, and no extra transcription process is required. The linguistic\ninformation of speech is implicitly encoded in the tailored ASR model and\ncontributes to boosting the performance. Experiments show that our proposed\napproach outperforms the best manual transcript-based RoBERTa by an absolute\nmargin of 4.6% in terms of accuracy. Our best-performing models achieve the\naccuracy of 83.2% and 78.0% in the long-audio and short-audio competition\ntracks of the 2021 NCMMSC Alzheimer's Disease Recognition Challenge,\nrespectively.", "published": "2021-10-04 15:02:23", "link": "http://arxiv.org/abs/2110.01493v1", "categories": ["eess.AS", "q-bio.QM"], "primary_category": "eess.AS"}
{"title": "Decoupling Speaker-Independent Emotions for Voice Conversion Via\n  Source-Filter Networks", "abstract": "Emotional voice conversion (VC) aims to convert a neutral voice to an\nemotional (e.g. happy) one while retaining the linguistic information and\nspeaker identity. We note that the decoupling of emotional features from other\nspeech information (such as speaker, content, etc.) is the key to achieving\nremarkable performance. Some recent attempts about speech representation\ndecoupling on the neutral speech can not work well on the emotional speech, due\nto the more complex acoustic properties involved in the latter. To address this\nproblem, here we propose a novel Source-Filter-based Emotional VC model (SFEVC)\nto achieve proper filtering of speaker-independent emotion features from both\nthe timbre and pitch features. Our SFEVC model consists of multi-channel\nencoders, emotion separate encoders, and one decoder. Note that all encoder\nmodules adopt a designed information bottlenecks auto-encoder. Additionally, to\nfurther improve the conversion quality for various emotions, a novel two-stage\ntraining strategy based on the 2D Valence-Arousal (VA) space was proposed.\nExperimental results show that the proposed SFEVC along with a two-stage\ntraining strategy outperforms all baselines and achieves the state-of-the-art\nperformance in speaker-independent emotional VC with nonparallel data.", "published": "2021-10-04 03:14:48", "link": "http://arxiv.org/abs/2110.01164v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "The Second DiCOVA Challenge: Dataset and performance analysis for\n  COVID-19 diagnosis using acoustics", "abstract": "The Second Diagnosis of COVID-19 using Acoustics (DiCOVA) Challenge aimed at\naccelerating the research in acoustics based detection of COVID-19, a topic at\nthe intersection of acoustics, signal processing, machine learning, and\nhealthcare. This paper presents the details of the challenge, which was an open\ncall for researchers to analyze a dataset of audio recordings consisting of\nbreathing, cough and speech signals. This data was collected from individuals\nwith and without COVID-19 infection, and the task in the challenge was a\ntwo-class classification. The development set audio recordings were collected\nfrom 965 (172 COVID-19 positive) individuals, while the evaluation set\ncontained data from 471 individuals (71 COVID-19 positive). The challenge\nfeatured four tracks, one associated with each sound category of cough, speech\nand breathing, and a fourth fusion track. A baseline system was also released\nto benchmark the participants. In this paper, we present an overview of the\nchallenge, the rationale for the data collection and the baseline system.\nFurther, a performance analysis for the systems submitted by the $16$\nparticipating teams in the leaderboard is also presented.", "published": "2021-10-04 04:09:53", "link": "http://arxiv.org/abs/2110.01177v3", "categories": ["eess.AS", "cs.SD", "q-bio.QM"], "primary_category": "eess.AS"}
{"title": "AASIST: Audio Anti-Spoofing using Integrated Spectro-Temporal Graph\n  Attention Networks", "abstract": "Artefacts that differentiate spoofed from bona-fide utterances can reside in\nspectral or temporal domains. Their reliable detection usually depends upon\ncomputationally demanding ensemble systems where each subsystem is tuned to\nsome specific artefacts. We seek to develop an efficient, single system that\ncan detect a broad range of different spoofing attacks without score-level\nensembles. We propose a novel heterogeneous stacking graph attention layer\nwhich models artefacts spanning heterogeneous temporal and spectral domains\nwith a heterogeneous attention mechanism and a stack node. With a new max graph\noperation that involves a competitive mechanism and an extended readout scheme,\nour approach, named AASIST, outperforms the current state-of-the-art by 20%\nrelative. Even a lightweight variant, AASIST-L, with only 85K parameters,\noutperforms all competing systems.", "published": "2021-10-04 05:48:25", "link": "http://arxiv.org/abs/2110.01200v1", "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Towards efficient end-to-end speech recognition with\n  biologically-inspired neural networks", "abstract": "Automatic speech recognition (ASR) is a capability which enables a program to\nprocess human speech into a written form. Recent developments in artificial\nintelligence (AI) have led to high-accuracy ASR systems based on deep neural\nnetworks, such as the recurrent neural network transducer (RNN-T). However, the\ncore components and the performed operations of these approaches depart from\nthe powerful biological counterpart, i.e., the human brain. On the other hand,\nthe current developments in biologically-inspired ASR models, based on spiking\nneural networks (SNNs), lag behind in terms of accuracy and focus primarily on\nsmall scale applications. In this work, we revisit the incorporation of\nbiologically-plausible models into deep learning and we substantially enhance\ntheir capabilities, by taking inspiration from the diverse neural and synaptic\ndynamics found in the brain. In particular, we introduce neural connectivity\nconcepts emulating the axo-somatic and the axo-axonic synapses. Based on this,\nwe propose novel deep learning units with enriched neuro-synaptic dynamics and\nintegrate them into the RNN-T architecture. We demonstrate for the first time,\nthat a biologically realistic implementation of a large-scale ASR model can\nyield competitive performance levels compared to the existing deep learning\nmodels. Specifically, we show that such an implementation bears several\nadvantages, such as a reduced computational cost and a lower latency, which are\ncritical for speech recognition applications.", "published": "2021-10-04 21:24:10", "link": "http://arxiv.org/abs/2110.02743v2", "categories": ["eess.AS", "cs.LG", "cs.NE", "q-bio.QM"], "primary_category": "eess.AS"}
