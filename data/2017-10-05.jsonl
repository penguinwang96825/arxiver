{"title": "Machine Learning Based Detection of Clickbait Posts in Social Media", "abstract": "Clickbait (headlines) make use of misleading titles that hide critical\ninformation from or exaggerate the content on the landing target pages to\nentice clicks. As clickbaits often use eye-catching wording to attract viewers,\ntarget contents are often of low quality. Clickbaits are especially widespread\non social media such as Twitter, adversely impacting user experience by causing\nimmense dissatisfaction. Hence, it has become increasingly important to put\nforward a widely applicable approach to identify and detect clickbaits. In this\npaper, we make use of a dataset from the clickbait challenge 2017\n(clickbait-challenge.com) comprising of over 21,000 headlines/titles, each of\nwhich is annotated by at least five judgments from crowdsourcing on how\nclickbait it is. We attempt to build an effective computational clickbait\ndetection model on this dataset. We first considered a total of 331 features,\nfiltered out many features to avoid overfitting and improve the running time of\nlearning, and eventually selected the 60 most important features for our final\nmodel. Using these features, Random Forest Regression achieved the following\nresults: MSE=0.035 MSE, Accuracy=0.82, and F1-sore=0.61 on the clickbait class.", "published": "2017-10-05 11:54:34", "link": "http://arxiv.org/abs/1710.01977v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Effective Use of Pretraining for Natural Language Inference", "abstract": "Neural networks have excelled at many NLP tasks, but there remain open\nquestions about the performance of pretrained distributed word representations\nand their interaction with weight initialization and other hyperparameters. We\naddress these questions empirically using attention-based sequence-to-sequence\nmodels for natural language inference (NLI). Specifically, we compare three\ntypes of embeddings: random, pretrained (GloVe, word2vec), and retrofitted\n(pretrained plus WordNet information). We show that pretrained embeddings\noutperform both random and retrofitted ones in a large NLI corpus. Further\nexperiments on more controlled data sets shed light on the contexts for which\nretrofitted embeddings can be useful. We also explore two principled approaches\nto initializing the rest of the model parameters, Gaussian and orthogonal,\nshowing that the latter yields gains of up to 2.9% in the NLI task.", "published": "2017-10-05 15:29:33", "link": "http://arxiv.org/abs/1710.02076v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Indowordnets help in Indian Language Machine Translation", "abstract": "Being less resource languages, Indian-Indian and English-Indian language MT\nsystem developments faces the difficulty to translate various lexical\nphenomena. In this paper, we present our work on a comparative study of 440\nphrase-based statistical trained models for 110 language pairs across 11 Indian\nlanguages. We have developed 110 baseline Statistical Machine Translation\nsystems. Then we have augmented the training corpus with Indowordnet synset\nword entries of lexical database and further trained 110 models on top of the\nbaseline system. We have done a detailed performance comparison using various\nevaluation metrics such as BLEU score, METEOR and TER. We observed significant\nimprovement in evaluations of translation quality across all the 440 models\nafter using the Indowordnet. These experiments give a detailed insight in two\nways : (1) usage of lexical database with synset mapping for resource poor\nlanguages (2) efficient usage of Indowordnet sysnset mapping. More over, synset\nmapped lexical entries helped the SMT system to handle the ambiguity to a great\nextent during the translation.", "published": "2017-10-05 16:03:42", "link": "http://arxiv.org/abs/1710.02086v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Morphology Generation for Statistical Machine Translation", "abstract": "When translating into morphologically rich languages, Statistical MT\napproaches face the problem of data sparsity. The severity of the sparseness\nproblem will be high when the corpus size of morphologically richer language is\nless. Even though we can use factored models to correctly generate\nmorphological forms of words, the problem of data sparseness limits their\nperformance. In this paper, we describe a simple and effective solution which\nis based on enriching the input corpora with various morphological forms of\nwords. We use this method with the phrase-based and factor-based experiments on\ntwo morphologically rich languages: Hindi and Marathi when translating from\nEnglish. We evaluate the performance of our experiments both in terms automatic\nevaluation and subjective evaluation such as adequacy and fluency. We observe\nthat the morphology injection method helps in improving the quality of\ntranslation. We further analyze that the morph injection method helps in\nhandling the data sparseness problem to a great level.", "published": "2017-10-05 16:15:23", "link": "http://arxiv.org/abs/1710.02093v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Machine Translation Evaluation with Neural Networks", "abstract": "We present a framework for machine translation evaluation using neural\nnetworks in a pairwise setting, where the goal is to select the better\ntranslation from a pair of hypotheses, given the reference translation. In this\nframework, lexical, syntactic and semantic information from the reference and\nthe two hypotheses is embedded into compact distributed vector representations,\nand fed into a multi-layer neural network that models nonlinear interactions\nbetween each of the hypotheses and the reference, as well as between the two\nhypotheses. We experiment with the benchmark datasets from the WMT Metrics\nshared task, on which we obtain the best results published so far, with the\nbasic network configuration. We also perform a series of experiments to analyze\nand understand the contribution of the different components of the network. We\nevaluate variants and extensions, including fine-tuning of the semantic\nembeddings, and sentence-based representations modeled with convolutional and\nrecurrent neural networks. In summary, the proposed framework is flexible and\ngeneralizable, allows for efficient learning and scoring, and provides an MT\nevaluation metric that correlates with human judgments, and is on par with the\nstate of the art.", "published": "2017-10-05 16:18:08", "link": "http://arxiv.org/abs/1710.02095v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Phrase Pair Mappings for Hindi-English Statistical Machine Translation", "abstract": "In this paper, we present our work on the creation of lexical resources for\nthe Machine Translation between English and Hindi. We describes the development\nof phrase pair mappings for our experiments and the comparative performance\nevaluation between different trained models on top of the baseline Statistical\nMachine Translation system. We focused on augmenting the parallel corpus with\nmore vocabulary as well as with various inflected forms by exploring different\nways. We have augmented the training corpus with various lexical resources such\nas lexical words, synset words, function words and verb phrases. We have\ndescribed the case studies, automatic and subjective evaluations, detailed\nerror analysis for both the English to Hindi and Hindi to English machine\ntranslation systems. We further analyzed that, there is an incremental growth\nin the quality of machine translation with the usage of various lexical\nresources. Thus lexical resources do help uplift the translation quality of\nresource poor langugaes.", "published": "2017-10-05 16:21:55", "link": "http://arxiv.org/abs/1710.02100v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BPEmb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages", "abstract": "We present BPEmb, a collection of pre-trained subword unit embeddings in 275\nlanguages, based on Byte-Pair Encoding (BPE). In an evaluation using\nfine-grained entity typing as testbed, BPEmb performs competitively, and for\nsome languages bet- ter than alternative subword approaches, while requiring\nvastly fewer resources and no tokenization. BPEmb is available at\nhttps://github.com/bheinzerling/bpemb", "published": "2017-10-05 19:24:07", "link": "http://arxiv.org/abs/1710.02187v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bilingual Words and Phrase Mappings for Marathi and Hindi SMT", "abstract": "Lack of proper linguistic resources is the major challenges faced by the\nMachine Translation system developments when dealing with the resource poor\nlanguages. In this paper, we describe effective ways to utilize the lexical\nresources to improve the quality of statistical machine translation. Our\nresearch on the usage of lexical resources mainly focused on two ways, such as;\naugmenting the parallel corpus with more vocabulary and to provide various word\nforms. We have augmented the training corpus with various lexical resources\nsuch as lexical words, function words, kridanta pairs and verb phrases. We have\ndescribed the case studies, evaluations and detailed error analysis for both\nMarathi to Hindi and Hindi to Marathi machine translation systems. From the\nevaluations we observed that, there is an incremental growth in the quality of\nmachine translation as the usage of various lexical resources increases.\nMoreover, usage of various lexical resources helps to improve the coverage and\nquality of machine translation where limited parallel corpus is available.", "published": "2017-10-05 16:26:43", "link": "http://arxiv.org/abs/1710.02398v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic speech retrieval with a visually grounded model of\n  untranscribed speech", "abstract": "There is growing interest in models that can learn from unlabelled speech\npaired with visual context. This setting is relevant for low-resource speech\nprocessing, robotics, and human language acquisition research. Here we study\nhow a visually grounded speech model, trained on images of scenes paired with\nspoken captions, captures aspects of semantics. We use an external image tagger\nto generate soft text labels from images, which serve as targets for a neural\nmodel that maps untranscribed speech to (semantic) keyword labels. We introduce\na newly collected data set of human semantic relevance judgements and an\nassociated task, semantic speech retrieval, where the goal is to search for\nspoken utterances that are semantically relevant to a given text query. Without\nseeing any text, the model trained on parallel speech and images achieves a\nprecision of almost 60% on its top ten semantic retrievals. Compared to a\nsupervised model trained on transcriptions, our model matches human judgements\nbetter by some measures, especially in retrieving non-verbatim semantic\nmatches. We perform an extensive analysis of the model and its resulting\nrepresentations.", "published": "2017-10-05 10:24:46", "link": "http://arxiv.org/abs/1710.01949v2", "categories": ["cs.CL", "cs.CV", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Head shadow enhancement with low-frequency beamforming improves sound\n  localization and speech perception for simulated bimodal listeners", "abstract": "Many hearing-impaired listeners struggle to localize sounds due to poor\navailability of binaural cues. Listeners with a cochlear implant and a\ncontralateral hearing aid -- so-called bimodal listeners -- are amongst the\nworst performers, as both interaural time and level differences are poorly\ntransmitted. We present a new method to enhance head shadow in the low\nfrequencies. Head shadow enhancement is achieved with a fixed beamformer with\ncontralateral attenuation in each ear. The method results in interaural level\ndifferences which vary monotonically with angle. It also improves low-frequency\nsignal-to-noise ratios in conditions with spatially separated speech and noise.\nWe validated the method in two experiments with acoustic simulations of bimodal\nlistening. In the localization experiment, performance improved from 50.5{\\deg}\nto 26.8{\\deg} root-mean-square error compared with standard omni-directional\nmicrophones. In the speech-in-noise experiment, speech was presented from the\nfrontal direction. Speech reception thresholds improved by 15.7 dB SNR when the\nnoise was presented from the cochlear implant side, improved by 7.6 dB SNR when\nthe noise was presented from the hearing aid side, and was not affected when\nnoise was presented from all directions. Apart from bimodal listeners, the\nmethod might also be promising for bilateral cochlear implant or hearing aid\nusers. Its low computational complexity makes the method suitable for\napplication in current clinical devices.\n  Keywords: head shadow enhancement, enhancement of interaural level\ndifferences, sound localization, directional hearing, speech in noise, speech\nintelligibility\n  PACS: 43.60.Fg, 43.66.Pn, 43.66.Qp, 43.66.Rq, 43.66.Ts, 43.71.-k, 43.71.Es,\n43.71.Ky", "published": "2017-10-05 07:58:23", "link": "http://arxiv.org/abs/1710.01904v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
