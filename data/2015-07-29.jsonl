{"title": "EESEN: End-to-End Speech Recognition using Deep RNN Models and\n  WFST-based Decoding", "abstract": "The performance of automatic speech recognition (ASR) has improved\ntremendously due to the application of deep neural networks (DNNs). Despite\nthis progress, building a new ASR system remains a challenging task, requiring\nvarious resources, multiple training stages and significant expertise. This\npaper presents our Eesen framework which drastically simplifies the existing\npipeline to build state-of-the-art ASR systems. Acoustic modeling in Eesen\ninvolves learning a single recurrent neural network (RNN) predicting\ncontext-independent targets (phonemes or characters). To remove the need for\npre-generated frame labels, we adopt the connectionist temporal classification\n(CTC) objective function to infer the alignments between speech and label\nsequences. A distinctive feature of Eesen is a generalized decoding approach\nbased on weighted finite-state transducers (WFSTs), which enables the efficient\nincorporation of lexicons and language models into CTC decoding. Experiments\nshow that compared with the standard hybrid DNN systems, Eesen achieves\ncomparable word error rates (WERs), while at the same time speeding up decoding\nsignificantly.", "published": "2015-07-29 17:53:50", "link": "http://arxiv.org/abs/1507.08240v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Document Embedding with Paragraph Vectors", "abstract": "Paragraph Vectors has been recently proposed as an unsupervised method for\nlearning distributed representations for pieces of texts. In their work, the\nauthors showed that the method can learn an embedding of movie review texts\nwhich can be leveraged for sentiment analysis. That proof of concept, while\nencouraging, was rather narrow. Here we consider tasks other than sentiment\nanalysis, provide a more thorough comparison of Paragraph Vectors to other\ndocument modelling algorithms such as Latent Dirichlet Allocation, and evaluate\nperformance of the method as we vary the dimensionality of the learned\nrepresentation. We benchmarked the models on two document similarity data sets,\none from Wikipedia, one from arXiv. We observe that the Paragraph Vector method\nperforms significantly better than other methods, and propose a simple\nimprovement to enhance embedding quality. Somewhat surprisingly, we also show\nthat much like word embeddings, vector operations on Paragraph Vectors can\nperform useful semantic results.", "published": "2015-07-29 01:04:28", "link": "http://arxiv.org/abs/1507.07998v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
