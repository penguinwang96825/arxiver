{"title": "Sotto Voce: Federated Speech Recognition with Differential Privacy\n  Guarantees", "abstract": "Speech data is expensive to collect, and incredibly sensitive to its sources.\nIt is often the case that organizations independently collect small datasets\nfor their own use, but often these are not performant for the demands of\nmachine learning. Organizations could pool these datasets together and jointly\nbuild a strong ASR system; sharing data in the clear, however, comes with\ntremendous risk, in terms of intellectual property loss as well as loss of\nprivacy of the individuals who exist in the dataset. In this paper, we offer a\npotential solution for learning an ML model across multiple organizations where\nwe can provide mathematical guarantees limiting privacy loss. We use a\nFederated Learning approach built on a strong foundation of Differential\nPrivacy techniques. We apply these to a senone classification prototype and\ndemonstrate that the model improves with the addition of private data while\nstill respecting privacy.", "published": "2022-07-16 02:48:54", "link": "http://arxiv.org/abs/2207.07816v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Meta-Referential Games to Learn Compositional Learning Behaviours", "abstract": "Human beings use compositionality to generalise from past experiences to\nnovel experiences. We assume a separation of our experiences into fundamental\natomic components that can be recombined in novel ways to support our ability\nto engage with novel experiences. We frame this as the ability to learn to\ngeneralise compositionally, and we will refer to behaviours making use of this\nability as compositional learning behaviours (CLBs). A central problem to\nlearning CLBs is the resolution of a binding problem (BP). While it is another\nfeat of intelligence that human beings perform with ease, it is not the case\nfor state-of-the-art artificial agents. Thus, in order to build artificial\nagents able to collaborate with human beings, we propose to develop a novel\nbenchmark to investigate agents' abilities to exhibit CLBs by solving a\ndomain-agnostic version of the BP. We take inspiration from the language\nemergence and grounding framework of referential games and propose a\nmeta-learning extension of referential games, entitled Meta-Referential Games,\nand use this framework to build our benchmark, the Symbolic Behaviour Benchmark\n(S2B). We provide baseline results and error analysis showing that our\nbenchmark is a compelling challenge that we hope will spur the research\ncommunity towards developing more capable artificial agents.", "published": "2022-07-16 20:37:46", "link": "http://arxiv.org/abs/2207.08012v5", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multimodal Dialog Systems with Dual Knowledge-enhanced Generative\n  Pretrained Language Model", "abstract": "Text response generation for multimodal task-oriented dialog systems, which\naims to generate the proper text response given the multimodal context, is an\nessential yet challenging task. Although existing efforts have achieved\ncompelling success, they still suffer from two pivotal limitations: 1) overlook\nthe benefit of generative pre-training, and 2) ignore the textual context\nrelated knowledge. To address these limitations, we propose a novel dual\nknowledge-enhanced generative pretrained language model for multimodal\ntask-oriented dialog systems (DKMD), consisting of three key components: dual\nknowledge selection, dual knowledge-enhanced context learning, and\nknowledge-enhanced response generation. To be specific, the dual knowledge\nselection component aims to select the related knowledge according to both\ntextual and visual modalities of the given context. Thereafter, the dual\nknowledge-enhanced context learning component targets seamlessly integrating\nthe selected knowledge into the multimodal context learning from both global\nand local perspectives, where the cross-modal semantic relation is also\nexplored. Moreover, the knowledge-enhanced response generation component\ncomprises a revised BART decoder, where an additional dot-product\nknowledge-decoder attention sub-layer is introduced for explicitly utilizing\nthe knowledge to advance the text response generation. Extensive experiments on\na public dataset verify the superiority of the proposed DKMD over\nstate-of-the-art competitors.", "published": "2022-07-16 13:02:54", "link": "http://arxiv.org/abs/2207.07934v2", "categories": ["cs.CL", "cs.HC", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Reducing Geographic Disparities in Automatic Speech Recognition via\n  Elastic Weight Consolidation", "abstract": "We present an approach to reduce the performance disparity between geographic\nregions without degrading performance on the overall user population for ASR. A\npopular approach is to fine-tune the model with data from regions where the ASR\nmodel has a higher word error rate (WER). However, when the ASR model is\nadapted to get better performance on these high-WER regions, its parameters\nwander from the previous optimal values, which can lead to worse performance in\nother regions. In our proposed method, we utilize the elastic weight\nconsolidation (EWC) regularization loss to identify directions in parameters\nspace along which the ASR weights can vary to improve for high-error regions,\nwhile still maintaining performance on the speaker population overall. Our\nresults demonstrate that EWC can reduce the word error rate (WER) in the region\nwith highest WER by 3.2% relative while reducing the overall WER by 1.3%\nrelative. We also evaluate the role of language and acoustic models in ASR\nfairness and propose a clustering algorithm to identify WER disparities based\non geographic region.", "published": "2022-07-16 06:04:52", "link": "http://arxiv.org/abs/2207.07850v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Do uHear? Validation of uHear App for Preliminary Screening of Hearing\n  Ability in Soundscape Studies", "abstract": "Studies involving soundscape perception often exclude participants with\nhearing loss to prevent impaired perception from affecting experimental\nresults. Participants are typically screened with pure tone audiometry, the\n\"gold standard\" for identifying and quantifying hearing loss at specific\nfrequencies, and excluded if a study-dependent threshold is not met. However,\nprocuring professional audiometric equipment for soundscape studies may be\ncost-ineffective, and manually performing audiometric tests is\nlabour-intensive. Moreover, testing requirements for soundscape studies may not\nrequire sensitivities and specificities as high as that in a medical diagnosis\nsetting. Hence, in this study, we investigate the effectiveness of the uHear\napp, an iOS application, as an affordable and automatic alternative to a\nconventional audiometer in screening participants for hearing loss for the\npurpose of soundscape studies or listening tests in general. Based on\naudiometric comparisons with the audiometer of 163 participants, the uHear app\nwas found to have high precision (98.04%) when using the World Health\nOrganization (WHO) grading scheme for assessing normal hearing. Precision is\nfurther improved (98.69%) when all frequencies assessed with the uHear app is\nconsidered in the grading, which lends further support to this cost-effective,\nautomated alternative to screen for normal hearing.", "published": "2022-07-16 08:19:30", "link": "http://arxiv.org/abs/2207.09221v1", "categories": ["eess.AS", "stat.AP"], "primary_category": "eess.AS"}
{"title": "Visually-aware Acoustic Event Detection using Heterogeneous Graphs", "abstract": "Perception of auditory events is inherently multimodal relying on both audio\nand visual cues. A large number of existing multimodal approaches process each\nmodality using modality-specific models and then fuse the embeddings to encode\nthe joint information. In contrast, we employ heterogeneous graphs to\nexplicitly capture the spatial and temporal relationships between the\nmodalities and represent detailed information about the underlying signal.\nUsing heterogeneous graph approaches to address the task of visually-aware\nacoustic event classification, which serves as a compact, efficient and\nscalable way to represent data in the form of graphs. Through heterogeneous\ngraphs, we show efficiently modelling of intra- and inter-modality\nrelationships both at spatial and temporal scales. Our model can easily be\nadapted to different scales of events through relevant hyperparameters.\nExperiments on AudioSet, a large benchmark, shows that our model achieves\nstate-of-the-art performance.", "published": "2022-07-16 13:09:25", "link": "http://arxiv.org/abs/2207.07935v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
