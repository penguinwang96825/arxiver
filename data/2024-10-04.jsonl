{"title": "Harnessing Generative AI for Economic Insights", "abstract": "We use generative AI to extract managerial expectations about their economic\noutlook from over 120,000 corporate conference call transcripts. The overall\nmeasure, AI Economy Score, robustly predicts future economic indicators such as\nGDP growth, production, and employment, both in the short term and to 10\nquarters. This predictive power is incremental to that of existing measures,\nincluding survey forecasts. Moreover, industry and firm-level measures provide\nvaluable information about sector-specific and individual firm activities. Our\nfindings suggest that managerial expectations carry unique insights about\neconomic activities, with implications for both macroeconomic and microeconomic\ndecision-making.", "published": "2024-10-04 19:57:37", "link": "http://arxiv.org/abs/2410.03897v3", "categories": ["q-fin.CP", "cs.LG", "econ.GN", "q-fin.EC"], "primary_category": "q-fin.CP"}
{"title": "A Dynamic Approach to Stock Price Prediction: Comparing RNN and Mixture of Experts Models Across Different Volatility Profiles", "abstract": "This study evaluates the effectiveness of a Mixture of Experts (MoE) model\nfor stock price prediction by comparing it to a Recurrent Neural Network (RNN)\nand a linear regression model. The MoE framework combines an RNN for volatile\nstocks and a linear model for stable stocks, dynamically adjusting the weight\nof each model through a gating network. Results indicate that the MoE approach\nsignificantly improves predictive accuracy across different volatility\nprofiles. The RNN effectively captures non-linear patterns for volatile\ncompanies but tends to overfit stable data, whereas the linear model performs\nwell for predictable trends. The MoE model's adaptability allows it to\noutperform each individual model, reducing errors such as Mean Squared Error\n(MSE) and Mean Absolute Error (MAE). Future work should focus on enhancing the\ngating mechanism and validating the model with real-world datasets to optimize\nits practical applicability.", "published": "2024-10-04 14:36:21", "link": "http://arxiv.org/abs/2410.07234v1", "categories": ["q-fin.CP", "cs.LG", "econ.EM"], "primary_category": "q-fin.CP"}
{"title": "Leveraging Fundamental Analysis for Stock Trend Prediction for Profit", "abstract": "This paper investigates the application of machine learning models, Long\nShort-Term Memory (LSTM), one-dimensional Convolutional Neural Networks (1D\nCNN), and Logistic Regression (LR), for predicting stock trends based on\nfundamental analysis. Unlike most existing studies that predominantly utilize\ntechnical or sentiment analysis, we emphasize the use of a company's financial\nstatements and intrinsic value for trend forecasting. Using a dataset of 269\ndata points from publicly traded companies across various sectors from 2019 to\n2023, we employ key financial ratios and the Discounted Cash Flow (DCF) model\nto formulate two prediction tasks: Annual Stock Price Difference (ASPD) and\nDifference between Current Stock Price and Intrinsic Value (DCSPIV). These\ntasks assess the likelihood of annual profit and current profitability,\nrespectively. Our results demonstrate that LR models outperform CNN and LSTM\nmodels, achieving an average test accuracy of 74.66% for ASPD and 72.85% for\nDCSPIV. This study contributes to the limited literature on integrating\nfundamental analysis into machine learning for stock prediction, offering\nvaluable insights for both academic research and practical investment\nstrategies. By leveraging fundamental data, our approach highlights the\npotential for long-term stock trend prediction, supporting portfolio managers\nin their decision-making processes.", "published": "2024-10-04 20:36:19", "link": "http://arxiv.org/abs/2410.03913v1", "categories": ["q-fin.ST", "cs.AI", "cs.LG"], "primary_category": "q-fin.ST"}
{"title": "Cyber Risk Taxonomies: Statistical Analysis of Cybersecurity Risk Classifications", "abstract": "Cyber risk classifications are widely used in the modeling of cyber event\ndistributions, yet their effectiveness in out of sample forecasting performance\nremains underexplored. In this paper, we analyse the most commonly used\nclassifications and argue in favour of switching the attention from\ngoodness-of-fit and in-sample predictive performance, to focusing on the out-of\nsample forecasting performance. We use a rolling window analysis, to compare\ncyber risk distribution forecasts via threshold weighted scoring functions. Our\nresults indicate that business motivated cyber risk classifications appear to\nbe too restrictive and not flexible enough to capture the heterogeneity of\ncyber risk events. We investigate how dynamic and impact-based cyber risk\nclassifiers seem to be better suited in forecasting future cyber risk losses\nthan the other considered classifications. These findings suggest that cyber\nrisk types provide limited forecasting ability concerning cyber event severity\ndistribution, and cyber insurance ratemakers should utilize cyber risk types\nonly when modeling the cyber event frequency distribution. Our study offers\nvaluable insights for decision-makers and policymakers alike, contributing to\nthe advancement of scientific knowledge in the field of cyber risk management.", "published": "2024-10-04 04:12:34", "link": "http://arxiv.org/abs/2410.05297v1", "categories": ["cs.CR", "q-fin.RM", "q-fin.ST"], "primary_category": "cs.CR"}
{"title": "Multilingual Topic Classification in X: Dataset and Analysis", "abstract": "In the dynamic realm of social media, diverse topics are discussed daily,\ntranscending linguistic boundaries. However, the complexities of understanding\nand categorising this content across various languages remain an important\nchallenge with traditional techniques like topic modelling often struggling to\naccommodate this multilingual diversity. In this paper, we introduce X-Topic, a\nmultilingual dataset featuring content in four distinct languages (English,\nSpanish, Japanese, and Greek), crafted for the purpose of tweet topic\nclassification. Our dataset includes a wide range of topics, tailored for\nsocial media content, making it a valuable resource for scientists and\nprofessionals working on cross-linguistic analysis, the development of robust\nmultilingual models, and computational scientists studying online dialogue.\nFinally, we leverage X-Topic to perform a comprehensive cross-linguistic and\nmultilingual analysis, and compare the capabilities of current general- and\ndomain-specific language models.", "published": "2024-10-04 01:37:26", "link": "http://arxiv.org/abs/2410.03075v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoCoHD: Congress Committee Hearing Dataset", "abstract": "U.S. congressional hearings significantly influence the national economy and\nsocial fabric, impacting individual lives. Despite their importance, there is a\nlack of comprehensive datasets for analyzing these discourses. To address this,\nwe propose the Congress Committee Hearing Dataset (CoCoHD), covering hearings\nfrom 1997 to 2024 across 86 committees, with 32,697 records. This dataset\nenables researchers to study policy language on critical issues like\nhealthcare, LGBTQ+ rights, and climate justice. We demonstrate its potential\nwith a case study on 1,000 energy-related sentences, analyzing the Energy and\nCommerce Committee's stance on fossil fuel consumption. By fine-tuning\npre-trained language models, we create energy-relevant measures for each\nhearing. Our market analysis shows that natural language analysis using CoCoHD\ncan predict and highlight trends in the energy sector.", "published": "2024-10-04 02:49:18", "link": "http://arxiv.org/abs/2410.03099v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "X-ALMA: Plug & Play Modules and Adaptive Rejection for Quality\n  Translation at Scale", "abstract": "Large language models (LLMs) have achieved remarkable success across various\nNLP tasks with a focus on English due to English-centric pre-training and\nlimited multilingual data. In this work, we focus on the problem of\ntranslation, and while some multilingual LLMs claim to support for hundreds of\nlanguages, models often fail to provide high-quality responses for mid- and\nlow-resource languages, leading to imbalanced performance heavily skewed in\nfavor of high-resource languages. We introduce **X-ALMA**, a model designed to\nensure top-tier performance across 50 diverse languages, regardless of their\nresource levels. X-ALMA surpasses state-of-the-art open-source multilingual\nLLMs, such as Aya-101 and Aya-23, in every single translation direction on the\nFLORES-200 and WMT'23 test datasets according to COMET-22. This is achieved by\nplug-and-play language-specific module architecture to prevent language\nconflicts during training and a carefully designed training regimen with novel\noptimization methods to maximize the translation performance. After the final\nstage of training regimen, our proposed **A**daptive **R**ejection\n**P**reference **O**ptimization (**ARPO**) surpasses existing preference\noptimization methods in translation tasks.", "published": "2024-10-04 03:17:27", "link": "http://arxiv.org/abs/2410.03115v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Precision, Stability, and Generalization: A Comprehensive Assessment of\n  RNNs learnability capability for Classifying Counter and Dyck Languages", "abstract": "This study investigates the learnability of Recurrent Neural Networks (RNNs)\nin classifying structured formal languages, focusing on counter and Dyck\nlanguages. Traditionally, both first-order (LSTM) and second-order (O2RNN) RNNs\nhave been considered effective for such tasks, primarily based on their\ntheoretical expressiveness within the Chomsky hierarchy. However, our research\nchallenges this notion by demonstrating that RNNs primarily operate as state\nmachines, where their linguistic capabilities are heavily influenced by the\nprecision of their embeddings and the strategies used for sampling negative\nexamples. Our experiments revealed that performance declines significantly as\nthe structural similarity between positive and negative examples increases.\nRemarkably, even a basic single-layer classifier using RNN embeddings performed\nbetter than chance. To evaluate generalization, we trained models on strings up\nto a length of 40 and tested them on strings from lengths 41 to 500, using 10\nunique seeds to ensure statistical robustness. Stability comparisons between\nLSTM and O2RNN models showed that O2RNNs generally offer greater stability\nacross various scenarios. We further explore the impact of different\ninitialization strategies revealing that our hypothesis is consistent with\nvarious RNNs. Overall, this research questions established beliefs about RNNs'\ncomputational capabilities, highlighting the importance of data structure and\nsampling techniques in assessing neural networks' potential for language\nclassification tasks. It emphasizes that stronger constraints on expressivity\nare crucial for understanding true learnability, as mere expressivity does not\ncapture the essence of learning.", "published": "2024-10-04 03:22:49", "link": "http://arxiv.org/abs/2410.03118v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deliberate Reasoning in Language Models as Structure-Aware Planning with\n  an Accurate World Model", "abstract": "Enhancing the reasoning capabilities of language models (LMs) remains a key\nchallenge, especially for tasks that require complex, multi-step\ndecision-making where existing Chain-of-Thought (CoT) approaches struggle with\nconsistency and verification. In this paper, we propose a novel reasoning\nframework, referred to as Structure-aware Planning with an Accurate World Model\n(SWAP), that integrates structured knowledge representation with learned\nplanning. Unlike prior methods that rely purely on natural language reasoning,\nSWAP leverages entailment graphs to encode structured dependencies and enable\nsymbolic verification of intermediate steps. To systematically construct and\nupdate the graph, SWAP employs a policy model to propose candidate expansions\nand a world model to predict structural updates. To improve accuracy, the world\nmodel generates multiple alternative updates, and a discriminator re-ranks them\nbased on plausibility. To encourage diverse exploration, we introduce\nDiversity-based Modelling (DM), which samples candidates from the remaining\nprobability mass after removing previously sampled candidates from the original\npolicy distribution. Additionally, SWAP improves the discrimination accuracy\nthrough Contrastive Ranking (CR), which directly compares candidates within\nprompts and incorporates meta-knowledge to improve ranking quality. We evaluate\nSWAP across diverse reasoning-intensive benchmarks including math reasoning,\nlogical reasoning, and coding tasks. Extensive experiments demonstrate that\nSWAP significantly improves upon the base models and consistently outperforms\nexisting reasoning methods.", "published": "2024-10-04 04:23:36", "link": "http://arxiv.org/abs/2410.03136v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SAG: Style-Aligned Article Generation via Model Collaboration", "abstract": "Large language models (LLMs) have increased the demand for personalized and\nstylish content generation. However, closed-source models like GPT-4 present\nlimitations in optimization opportunities, while the substantial training costs\nand inflexibility of open-source alternatives, such as Qwen-72B, pose\nconsiderable challenges. Conversely, small language models (SLMs) struggle with\nunderstanding complex instructions and transferring learned capabilities to new\ncontexts, often exhibiting more pronounced limitations. In this paper, we\npresent a novel collaborative training framework that leverages the strengths\nof both LLMs and SLMs for style article generation, surpassing the performance\nof either model alone. We freeze the LLMs to harness their robust\ninstruction-following capabilities and subsequently apply supervised\nfine-tuning on the SLM using style-specific data. Additionally, we introduce a\nself-improvement method to enhance style consistency. Our new benchmark,\nNoteBench, thoroughly evaluates style-aligned generation. Extensive experiments\nshow that our approach achieves state-of-the-art performance, with improvements\nof 0.78 in ROUGE-L and 0.55 in BLEU-4 scores compared to GPT-4, while\nmaintaining a low hallucination rate regarding factual and faithfulness.", "published": "2024-10-04 04:24:42", "link": "http://arxiv.org/abs/2410.03137v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Margin Matching Preference Optimization: Enhanced Model Alignment with\n  Granular Feedback", "abstract": "Large language models (LLMs) fine-tuned with alignment techniques, such as\nreinforcement learning from human feedback, have been instrumental in\ndeveloping some of the most capable AI systems to date. Despite their success,\nexisting methods typically rely on simple binary labels, such as those\nindicating preferred outputs in pairwise preferences, which fail to capture the\nsubtle differences in relative quality between pairs. To address this\nlimitation, we introduce an approach called Margin Matching Preference\nOptimization (MMPO), which incorporates relative quality margins into\noptimization, leading to improved LLM policies and reward models. Specifically,\ngiven quality margins in pairwise preferences, we design soft target\nprobabilities based on the Bradley-Terry model, which are then used to train\nmodels with the standard cross-entropy objective. Experiments with both human\nand AI feedback data demonstrate that MMPO consistently outperforms baseline\nmethods, often by a substantial margin, on popular benchmarks including\nMT-bench and RewardBench. Notably, the 7B model trained with MMPO achieves\nstate-of-the-art performance on RewardBench as of June 2024, outperforming\nother models of the same scale. Our analysis also shows that MMPO is more\nrobust to overfitting, leading to better-calibrated models.", "published": "2024-10-04 04:56:11", "link": "http://arxiv.org/abs/2410.03145v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Learnability in Memory-Augmented Recurrent Neural Networks:\n  Precision, Stability, and Empirical Insights", "abstract": "This study explores the learnability of memory-less and memory-augmented\nRNNs, which are theoretically equivalent to Pushdown Automata. Empirical\nresults show that these models often fail to generalize on longer sequences,\nrelying more on precision than mastering symbolic grammar. Experiments on fully\ntrained and component-frozen models reveal that freezing the memory component\nsignificantly improves performance, achieving state-of-the-art results on the\nPenn Treebank dataset (test perplexity reduced from 123.5 to 120.5). Models\nwith frozen memory retained up to 90% of initial performance on longer\nsequences, compared to a 60% drop in standard models. Theoretical analysis\nsuggests that freezing memory stabilizes temporal dependencies, leading to\nrobust convergence. These findings stress the need for stable memory designs\nand long-sequence evaluations to understand RNNs true learnability limits.", "published": "2024-10-04 05:29:51", "link": "http://arxiv.org/abs/2410.03154v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Autoregressive Large Language Models are Computationally Universal", "abstract": "We show that autoregressive decoding of a transformer-based language model\ncan realize universal computation, without external intervention or\nmodification of the model's weights. Establishing this result requires\nunderstanding how a language model can process arbitrarily long inputs using a\nbounded context. For this purpose, we consider a generalization of\nautoregressive decoding where, given a long input, emitted tokens are appended\nto the end of the sequence as the context window advances. We first show that\nthe resulting system corresponds to a classical model of computation, a Lag\nsystem, that has long been known to be computationally universal. By leveraging\na new proof, we show that a universal Turing machine can be simulated by a Lag\nsystem with 2027 production rules. We then investigate whether an existing\nlarge language model can simulate the behaviour of such a universal Lag system.\nWe give an affirmative answer by showing that a single system-prompt can be\ndeveloped for gemini-1.5-pro-001 that drives the model, under deterministic\n(greedy) decoding, to correctly apply each of the 2027 production rules. We\nconclude that, by the Church-Turing thesis, prompted gemini-1.5-pro-001 with\nextended autoregressive (greedy) decoding is a general purpose computer.", "published": "2024-10-04 06:05:17", "link": "http://arxiv.org/abs/2410.03170v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Kiss up, Kick down: Exploring Behavioral Changes in Multi-modal Large\n  Language Models with Assigned Visual Personas", "abstract": "This study is the first to explore whether multi-modal large language models\n(LLMs) can align their behaviors with visual personas, addressing a significant\ngap in the literature that predominantly focuses on text-based personas. We\ndeveloped a novel dataset of 5K fictional avatar images for assignment as\nvisual personas to LLMs, and analyzed their negotiation behaviors based on the\nvisual traits depicted in these images, with a particular focus on\naggressiveness. The results indicate that LLMs assess the aggressiveness of\nimages in a manner similar to humans and output more aggressive negotiation\nbehaviors when prompted with an aggressive visual persona. Interestingly, the\nLLM exhibited more aggressive negotiation behaviors when the opponent's image\nappeared less aggressive than their own, and less aggressive behaviors when the\nopponents image appeared more aggressive.", "published": "2024-10-04 06:38:38", "link": "http://arxiv.org/abs/2410.03181v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parallel Corpus Augmentation using Masked Language Models", "abstract": "In this paper we propose a novel method of augmenting parallel text corpora\nwhich promises good quality and is also capable of producing many fold larger\ncorpora than the seed corpus we start with. We do not need any additional\nmonolingual corpora. We use Multi-Lingual Masked Language Model to mask and\npredict alternative words in context and we use Sentence Embeddings to check\nand select sentence pairs which are likely to be translations of each other. We\ncross check our method using metrics for MT Quality Estimation. We believe this\nmethod can greatly alleviate the data scarcity problem for all language pairs\nfor which a reasonable seed corpus is available.", "published": "2024-10-04 07:15:07", "link": "http://arxiv.org/abs/2410.03194v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Transfer for Automatic Question Generation by Learning\n  Interrogative Structures in Target Languages", "abstract": "Automatic question generation (QG) serves a wide range of purposes, such as\naugmenting question-answering (QA) corpora, enhancing chatbot systems, and\ndeveloping educational materials. Despite its importance, most existing\ndatasets predominantly focus on English, resulting in a considerable gap in\ndata availability for other languages. Cross-lingual transfer for QG (XLT-QG)\naddresses this limitation by allowing models trained on high-resource language\ndatasets to generate questions in low-resource languages. In this paper, we\npropose a simple and efficient XLT-QG method that operates without the need for\nmonolingual, parallel, or labeled data in the target language, utilizing a\nsmall language model. Our model, trained solely on English QA datasets, learns\ninterrogative structures from a limited set of question exemplars, which are\nthen applied to generate questions in the target language. Experimental results\nshow that our method outperforms several XLT-QG baselines and achieves\nperformance comparable to GPT-3.5-turbo across different languages.\nAdditionally, the synthetic data generated by our model proves beneficial for\ntraining multilingual QA models. With significantly fewer parameters than large\nlanguage models and without requiring additional training for target languages,\nour approach offers an effective solution for QG and QA tasks across various\nlanguages.", "published": "2024-10-04 07:29:35", "link": "http://arxiv.org/abs/2410.03197v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PersoBench: Benchmarking Personalized Response Generation in Large\n  Language Models", "abstract": "While large language models (LLMs) have exhibited impressive conversational\ncapabilities, their proficiency in delivering personalized responses remains\nunclear. Although recent benchmarks automatically evaluate persona consistency\nin role-playing contexts using LLM-based judgment, the evaluation of\npersonalization in response generation remains underexplored. To address this\ngap, we present a new benchmark, PersoBench, to evaluate the personalization\nability of LLMs in persona-aware dialogue generation within a zero-shot\nsetting. We assess the performance of three open-source and three closed-source\nLLMs using well-known datasets and a range of metrics. Our analysis, conducted\non three well-known persona-aware datasets, evaluates multiple dimensions of\nresponse quality, including fluency, diversity, coherence, and personalization,\nacross both standard and chain-of-thought prompting methods. Our findings\nreveal that while LLMs excel at generating fluent and diverse responses, they\nare far from satisfactory in delivering personalized and coherent responses\nconsidering both the conversation context and the provided personas. Our\nbenchmark implementation is available at\nhttps://github.com/salehafzoon/PersoBench.", "published": "2024-10-04 07:29:41", "link": "http://arxiv.org/abs/2410.03198v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Consultation on Industrial Machine Faults with Large language Models", "abstract": "Industrial machine fault diagnosis is a critical component of operational\nefficiency and safety in manufacturing environments. Traditional methods rely\nheavily on expert knowledge and specific machine learning models, which can be\nlimited in their adaptability and require extensive labeled data. This paper\nintroduces a novel approach leveraging Large Language Models (LLMs),\nspecifically through a structured multi-round prompting technique, to improve\nfault diagnosis accuracy. By dynamically crafting prompts, our method enhances\nthe model's ability to synthesize information from diverse data sources,\nleading to improved contextual understanding and actionable recommendations.\nExperimental results demonstrate that our approach outperforms baseline models,\nachieving an accuracy of 91% in diagnosing various fault types. The findings\nunderscore the potential of LLMs in revolutionizing industrial fault\nconsultation practices, paving the way for more effective maintenance\nstrategies in complex environments.", "published": "2024-10-04 08:22:16", "link": "http://arxiv.org/abs/2410.03223v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ALR$^2$: A Retrieve-then-Reason Framework for Long-context Question\n  Answering", "abstract": "The context window of large language models (LLMs) has been extended\nsignificantly in recent years. However, while the context length that the LLM\ncan process has grown, the capability of the model to accurately reason over\nthat context degrades noticeably. This occurs because modern LLMs often become\noverwhelmed by the vast amount of information in the context; when answering\nquestions, the model must identify and reason over relevant evidence sparsely\ndistributed throughout the text. To alleviate the challenge of long-context\nreasoning, we develop a retrieve-then-reason framework, enabling LLMs to reason\nover relevant evidence collected during an intermediate retrieval step. We find\nthat modern LLMs struggle to accurately retrieve relevant facts and instead,\noften hallucinate \"retrieved facts\", resulting in flawed reasoning and the\nproduction of incorrect answers. To address these issues, we introduce ALR$^2$,\na method that augments the long-context reasoning capability of LLMs via an\nexplicit two-stage procedure, i.e., aligning LLMs with the objectives of both\nretrieval and reasoning. We demonstrate the efficacy of ALR$^2$ for mitigating\nperformance degradation in long-context reasoning tasks. Through extensive\nexperiments on long-context QA benchmarks, we find our method to outperform\ncompetitive baselines by large margins, achieving at least 8.4 and 7.9 EM gains\non the long-context versions of HotpotQA and SQuAD datasets, respectively.", "published": "2024-10-04 08:29:12", "link": "http://arxiv.org/abs/2410.03227v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Film Subtitles: Is YouTube the Best Approximation of Spoken\n  Vocabulary?", "abstract": "Word frequency is a key variable in psycholinguistics, useful for modeling\nhuman familiarity with words even in the era of large language models (LLMs).\nFrequency in film subtitles has proved to be a particularly good approximation\nof everyday language exposure. For many languages, however, film subtitles are\nnot easily available, or are overwhelmingly translated from English. We\ndemonstrate that frequencies extracted from carefully processed YouTube\nsubtitles provide an approximation comparable to, and often better than, the\nbest currently available resources. Moreover, they are available for languages\nfor which a high-quality subtitle or speech corpus does not exist. We use\nYouTube subtitles to construct frequency norms for five diverse languages,\nChinese, English, Indonesian, Japanese, and Spanish, and evaluate their\ncorrelation with lexical decision time, word familiarity, and lexical\ncomplexity. In addition to being strongly correlated with two psycholinguistic\nvariables, a simple linear regression on the new frequencies achieves a new\nhigh score on a lexical complexity prediction task in English and Japanese,\nsurpassing both models trained on film subtitle frequencies and the LLM GPT-4.\nOur code, the frequency lists, fastText word embeddings, and statistical\nlanguage models are freely available at https://github.com/naist-nlp/tubelex.", "published": "2024-10-04 09:04:20", "link": "http://arxiv.org/abs/2410.03240v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Expert-Level Language Models Expert-Level Annotators?", "abstract": "Data annotation refers to the labeling or tagging of textual data with\nrelevant information. A large body of works have reported positive results on\nleveraging LLMs as an alternative to human annotators. However, existing\nstudies focus on classic NLP tasks, and the extent to which LLMs as data\nannotators perform in domains requiring expert knowledge remains underexplored.\nIn this work, we investigate comprehensive approaches across three highly\nspecialized domains and discuss practical suggestions from a cost-effectiveness\nperspective. To the best of our knowledge, we present the first systematic\nevaluation of LLMs as expert-level data annotators.", "published": "2024-10-04 09:17:09", "link": "http://arxiv.org/abs/2410.03254v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive BPE Tokenization for Enhanced Vocabulary Adaptation in\n  Finetuning Pretrained Language Models", "abstract": "In this work, we show a fundamental limitation in vocabulary adaptation\napproaches that use Byte-Pair Encoding (BPE) tokenization scheme for\nfine-tuning pretrained language models (PLMs) to expert domains. Current\napproaches trivially append the target domain-specific vocabulary at the end of\nthe PLM vocabulary. This approach leads to a lower priority score and causes\nsub-optimal tokenization in BPE that iteratively uses merge rules to tokenize a\ngiven text. To mitigate this issue, we propose AdaptBPE where the BPE\ntokenization initialization phase is modified to first perform the longest\nstring matching on the added (target) vocabulary before tokenizing at the\ncharacter level. We perform an extensive evaluation of AdaptBPE versus the\nstandard BPE over various classification and summarization tasks; AdaptBPE\nimproves by 3.57% (in terms of accuracy) and 1.87% (in terms of Rouge-L),\nrespectively. AdaptBPE for MEDVOC works particularly well when reference\nsummaries have high OOV concentration or are longer in length. We also conduct\na human evaluation, revealing that AdaptBPE generates more relevant and more\nfaithful summaries as compared to MEDVOC. We make our codebase publicly\navailable at https://github.com/gb-kgp/adaptbpe.", "published": "2024-10-04 09:24:55", "link": "http://arxiv.org/abs/2410.03258v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Multi-task Learning Framework for Evaluating Machine Translation of\n  Emotion-loaded User-generated Content", "abstract": "Machine translation (MT) of user-generated content (UGC) poses unique\nchallenges, including handling slang, emotion, and literary devices like irony\nand sarcasm. Evaluating the quality of these translations is challenging as\ncurrent metrics do not focus on these ubiquitous features of UGC. To address\nthis issue, we utilize an existing emotion-related dataset that includes\nemotion labels and human-annotated translation errors based on\nMulti-dimensional Quality Metrics. We extend it with sentence-level evaluation\nscores and word-level labels, leading to a dataset suitable for sentence- and\nword-level translation evaluation and emotion classification, in a multi-task\nsetting. We propose a new architecture to perform these tasks concurrently,\nwith a novel combined loss function, which integrates different loss\nheuristics, like the Nash and Aligned losses. Our evaluation compares existing\nfine-tuning and multi-task learning approaches, assessing generalization with\nablative experiments over multiple datasets. Our approach achieves\nstate-of-the-art performance and we present a comprehensive analysis for MT\nevaluation of UGC.", "published": "2024-10-04 09:49:57", "link": "http://arxiv.org/abs/2410.03277v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What do Large Language Models Need for Machine Translation Evaluation?", "abstract": "Leveraging large language models (LLMs) for various natural language\nprocessing tasks has led to superlative claims about their performance. For the\nevaluation of machine translation (MT), existing research shows that LLMs are\nable to achieve results comparable to fine-tuned multilingual pre-trained\nlanguage models. In this paper, we explore what translation information, such\nas the source, reference, translation errors and annotation guidelines, is\nneeded for LLMs to evaluate MT quality. In addition, we investigate prompting\ntechniques such as zero-shot, Chain of Thought (CoT) and few-shot prompting for\neight language pairs covering high-, medium- and low-resource languages,\nleveraging varying LLM variants. Our findings indicate the importance of\nreference translations for an LLM-based evaluation. While larger models do not\nnecessarily fare better, they tend to benefit more from CoT prompting, than\nsmaller models. We also observe that LLMs do not always provide a numerical\nscore when generating evaluations, which poses a question on their reliability\nfor the task. Our work presents a comprehensive analysis for\nresource-constrained and training-less LLM-based evaluation of machine\ntranslation. We release the accrued prompt templates, code and data publicly\nfor reproducibility.", "published": "2024-10-04 09:50:45", "link": "http://arxiv.org/abs/2410.03278v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Fact Verification via Natural Logic and Large Language Models", "abstract": "The recent development of fact verification systems with natural logic has\nenhanced their explainability by aligning claims with evidence through\nset-theoretic operators, providing faithful justifications. Despite these\nadvancements, such systems often rely on a large amount of training data\nannotated with natural logic. To address this issue, we propose a zero-shot\nmethod that utilizes the generalization capabilities of instruction-tuned large\nlanguage models. To comprehensively assess the zero-shot capabilities of our\nmethod and other fact verification systems, we evaluate all models on both\nartificial and real-world claims, including multilingual datasets. We also\ncompare our method against other fact verification systems in two setups.\nFirst, in the zero-shot generalization setup, we demonstrate that our approach\noutperforms other systems that were not specifically trained on natural logic\ndata, achieving an average accuracy improvement of 8.96 points over the\nbest-performing baseline. Second, in the zero-shot transfer setup, we show that\ncurrent systems trained on natural logic data do not generalize well to other\ndomains, and our method outperforms these systems across all datasets with\nreal-world claims.", "published": "2024-10-04 11:57:32", "link": "http://arxiv.org/abs/2410.03341v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Should Cross-Lingual AMR Parsing go Meta? An Empirical Assessment of\n  Meta-Learning and Joint Learning AMR Parsing", "abstract": "Cross-lingual AMR parsing is the task of predicting AMR graphs in a target\nlanguage when training data is available only in a source language. Due to the\nsmall size of AMR training data and evaluation data, cross-lingual AMR parsing\nhas only been explored in a small set of languages such as English, Spanish,\nGerman, Chinese, and Italian. Taking inspiration from Langedijk et al. (2022),\nwho apply meta-learning to tackle cross-lingual syntactic parsing, we\ninvestigate the use of meta-learning for cross-lingual AMR parsing. We evaluate\nour models in $k$-shot scenarios (including 0-shot) and assess their\neffectiveness in Croatian, Farsi, Korean, Chinese, and French. Notably, Korean\nand Croatian test sets are developed as part of our work, based on the existing\nThe Little Prince English AMR corpus, and made publicly available. We\nempirically study our method by comparing it to classical joint learning. Our\nfindings suggest that while the meta-learning model performs slightly better in\n0-shot evaluation for certain languages, the performance gain is minimal or\nabsent when $k$ is higher than 0.", "published": "2024-10-04 12:24:02", "link": "http://arxiv.org/abs/2410.03357v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cogs in a Machine, Doing What They're Meant to Do -- The AMI Submission\n  to the WMT24 General Translation Task", "abstract": "This paper presents the submission of the \\'Arni Magnusson Institute's team\nto the WMT24 General translation task. We work on the English->Icelandic\ntranslation direction. Our system comprises four translation models and a\ngrammar correction model. For training our models we carefully curate our\ndatasets, aggressively filtering out sentence pairs that may detrimentally\naffect the quality of our system's output. Some of our data are collected from\nhuman translations and some are synthetically generated. A part of the\nsynthetic data is generated using an LLM, and we find that it increases the\ntranslation capability of our system significantly.", "published": "2024-10-04 12:48:32", "link": "http://arxiv.org/abs/2410.03381v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Killing Two Flies with One Stone: An Attempt to Break LLMs Using\n  English->Icelandic Idioms and Proper Names", "abstract": "This paper presents the submission of the \\'Arni Magn\\'usson Institute's team\nto the WMT24 test suite subtask, focusing on idiomatic expressions and proper\nnames for the English->Icelandic translation direction.\n  Intuitively and empirically, idioms and proper names are known to be a\nsignificant challenge for modern translation models. We create two different\ntest suites. The first evaluates the competency of MT systems in translating\ncommon English idiomatic expressions, as well as testing whether systems can\ndistinguish between those expressions and the same phrases when used in a\nliteral context. The second test suite consists of place names that should be\ntranslated into their Icelandic exonyms (and correctly inflected) and pairs of\nIcelandic names that share a surface form between the male and female variants,\nso that incorrect translations impact meaning as well as readability.\n  The scores reported are relatively low, especially for idiomatic expressions\nand place names, and indicate considerable room for improvement.", "published": "2024-10-04 12:57:00", "link": "http://arxiv.org/abs/2410.03394v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Team MTS @ AutoMin 2021: An Overview of Existing Summarization\n  Approaches and Comparison to Unsupervised Summarization Techniques", "abstract": "Remote communication through video or audio conferences has become more\npopular than ever because of the worldwide pandemic. These events, therefore,\nhave provoked the development of systems for automatic minuting of spoken\nlanguage leading to AutoMin 2021 challenge. The following paper illustrates the\nresults of the research that team MTS has carried out while participating in\nthe Automatic Minutes challenge. In particular, in this paper we analyze\nexisting approaches to text and speech summarization, propose an unsupervised\nsummarization technique based on clustering and provide a pipeline that\nincludes an adapted automatic speech recognition block able to run on real-life\nrecordings. The proposed unsupervised technique outperforms pre-trained\nsummarization models on the automatic minuting task with Rouge 1, Rouge 2 and\nRouge L values of 0.21, 0.02 and 0.2 on the dev set, with Rouge 1, Rouge 2,\nRouge L, Adequacy, Grammatical correctness and Fluency values of 0.180, 0.035,\n0.098, 1.857, 2.304, 1.911 on the test set accordingly", "published": "2024-10-04 13:23:50", "link": "http://arxiv.org/abs/2410.03412v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Surgical, Cheap, and Flexible: Mitigating False Refusal in Language\n  Models via Single Vector Ablation", "abstract": "Training a language model to be both helpful and harmless requires careful\ncalibration of refusal behaviours: Models should refuse to follow malicious\ninstructions or give harmful advice (e.g.\"how do I kill someone?\"), but they\nshould not refuse safe requests, even if they superficially resemble unsafe\nones (e.g. \"how do I kill a Python process?\"). Avoiding such false refusal, as\nprior work has shown, is challenging even for highly-capable language models.\nIn this paper, we propose a simple and surgical method for mitigating false\nrefusal in language models via single vector ablation. For a given model, we\nextract a false refusal vector and show that ablating this vector reduces false\nrefusal rate while preserving the model's safety and general capabilities. We\nalso show that our approach can be used for fine-grained calibration of model\nsafety. Our approach is training-free and model-agnostic, making it useful for\nmitigating the problem of false refusal in current and future language models.", "published": "2024-10-04 13:25:32", "link": "http://arxiv.org/abs/2410.03415v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Hard is this Test Set? NLI Characterization by Exploiting Training\n  Dynamics", "abstract": "Natural Language Inference (NLI) evaluation is crucial for assessing language\nunderstanding models; however, popular datasets suffer from systematic spurious\ncorrelations that artificially inflate actual model performance. To address\nthis, we propose a method for the automated creation of a challenging test set\nwithout relying on the manual construction of artificial and unrealistic\nexamples. We categorize the test set of popular NLI datasets into three\ndifficulty levels by leveraging methods that exploit training dynamics. This\ncategorization significantly reduces spurious correlation measures, with\nexamples labeled as having the highest difficulty showing markedly decreased\nperformance and encompassing more realistic and diverse linguistic phenomena.\nWhen our characterization method is applied to the training set, models trained\nwith only a fraction of the data achieve comparable performance to those\ntrained on the full dataset, surpassing other dataset characterization\ntechniques. Our research addresses limitations in NLI dataset construction,\nproviding a more authentic evaluation of model performance with implications\nfor diverse NLU applications.", "published": "2024-10-04 13:39:21", "link": "http://arxiv.org/abs/2410.03429v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ToolGen: Unified Tool Retrieval and Calling via Generation", "abstract": "As large language models (LLMs) advance, their inability to autonomously\nexecute tasks by directly interacting with external tools remains a critical\nlimitation. Traditional methods rely on inputting tool descriptions as context,\nwhich is constrained by context length and requires separate, often\ninefficient, retrieval mechanisms. We introduce ToolGen, a paradigm shift that\nintegrates tool knowledge directly into the LLM's parameters by representing\neach tool as a unique token. This enables the LLM to generate tool calls and\narguments as part of its next token prediction capabilities, seamlessly\nblending tool invocation with language generation. Our framework allows the LLM\nto access and utilize a vast amount of tools with no additional retrieval step,\nsignificantly enhancing both performance and scalability. Experimental results\nwith over 47,000 tools show that ToolGen not only achieves superior results in\nboth tool retrieval and autonomous task completion but also sets the stage for\na new era of AI agents that can adapt to tools across diverse domains. By\nfundamentally transforming tool retrieval into a generative process, ToolGen\npaves the way for more versatile, efficient, and autonomous AI systems. ToolGen\nenables end-to-end tool learning and opens opportunities for integration with\nother advanced techniques such as chain-of-thought and reinforcement learning,\nthereby expanding the practical capabilities of LLMs.", "published": "2024-10-04 13:52:32", "link": "http://arxiv.org/abs/2410.03439v3", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "How Language Models Prioritize Contextual Grammatical Cues?", "abstract": "Transformer-based language models have shown an excellent ability to\neffectively capture and utilize contextual information. Although various\nanalysis techniques have been used to quantify and trace the contribution of\nsingle contextual cues to a target task such as subject-verb agreement or\ncoreference resolution, scenarios in which multiple relevant cues are available\nin the context remain underexplored. In this paper, we investigate how language\nmodels handle gender agreement when multiple gender cue words are present, each\ncapable of independently disambiguating a target gender pronoun. We analyze two\nwidely used Transformer-based models: BERT, an encoder-based, and GPT-2, a\ndecoder-based model. Our analysis employs two complementary approaches: context\nmixing analysis, which tracks information flow within the model, and a variant\nof activation patching, which measures the impact of cues on the model's\nprediction. We find that BERT tends to prioritize the first cue in the context\nto form both the target word representations and the model's prediction, while\nGPT-2 relies more on the final cue. Our findings reveal striking differences in\nhow encoder-based and decoder-based models prioritize and use contextual\ninformation for their predictions.", "published": "2024-10-04 14:09:05", "link": "http://arxiv.org/abs/2410.03447v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoCoLoFa: A Dataset of News Comments with Common Logical Fallacies\n  Written by LLM-Assisted Crowds", "abstract": "Detecting logical fallacies in texts can help users spot argument flaws, but\nautomating this detection is not easy. Manually annotating fallacies in\nlarge-scale, real-world text data to create datasets for developing and\nvalidating detection models is costly. This paper introduces CoCoLoFa, the\nlargest known logical fallacy dataset, containing 7,706 comments for 648 news\narticles, with each comment labeled for fallacy presence and type. We recruited\n143 crowd workers to write comments embodying specific fallacy types (e.g.,\nslippery slope) in response to news articles. Recognizing the complexity of\nthis writing task, we built an LLM-powered assistant into the workers'\ninterface to aid in drafting and refining their comments. Experts rated the\nwriting quality and labeling validity of CoCoLoFa as high and reliable.\nBERT-based models fine-tuned using CoCoLoFa achieved the highest fallacy\ndetection (F1=0.86) and classification (F1=0.87) performance on its test set,\noutperforming the state-of-the-art LLMs. Our work shows that combining\ncrowdsourcing and LLMs enables us to more effectively construct datasets for\ncomplex linguistic phenomena that crowd workers find challenging to produce on\ntheir own.", "published": "2024-10-04 14:15:56", "link": "http://arxiv.org/abs/2410.03457v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Dialect Vietnamese: Task, Dataset, Baseline Models and Challenges", "abstract": "Vietnamese, a low-resource language, is typically categorized into three\nprimary dialect groups that belong to Northern, Central, and Southern Vietnam.\nHowever, each province within these regions exhibits its own distinct\npronunciation variations. Despite the existence of various speech recognition\ndatasets, none of them has provided a fine-grained classification of the 63\ndialects specific to individual provinces of Vietnam. To address this gap, we\nintroduce Vietnamese Multi-Dialect (ViMD) dataset, a novel comprehensive\ndataset capturing the rich diversity of 63 provincial dialects spoken across\nVietnam. Our dataset comprises 102.56 hours of audio, consisting of\napproximately 19,000 utterances, and the associated transcripts contain over\n1.2 million words. To provide benchmarks and simultaneously demonstrate the\nchallenges of our dataset, we fine-tune state-of-the-art pre-trained models for\ntwo downstream tasks: (1) Dialect identification and (2) Speech recognition.\nThe empirical results suggest two implications including the influence of\ngeographical factors on dialects, and the constraints of current approaches in\nspeech recognition tasks involving multi-dialect speech data. Our dataset is\navailable for research purposes.", "published": "2024-10-04 14:17:56", "link": "http://arxiv.org/abs/2410.03458v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is Safer Better? The Impact of Guardrails on the Argumentative Strength\n  of LLMs in Hate Speech Countering", "abstract": "The potential effectiveness of counterspeech as a hate speech mitigation\nstrategy is attracting increasing interest in the NLG research community,\nparticularly towards the task of automatically producing it. However,\nautomatically generated responses often lack the argumentative richness which\ncharacterises expert-produced counterspeech. In this work, we focus on two\naspects of counterspeech generation to produce more cogent responses. First, by\ninvestigating the tension between helpfulness and harmlessness of LLMs, we test\nwhether the presence of safety guardrails hinders the quality of the\ngenerations. Secondly, we assess whether attacking a specific component of the\nhate speech results in a more effective argumentative strategy to fight online\nhate. By conducting an extensive human and automatic evaluation, we show how\nthe presence of safety guardrails can be detrimental also to a task that\ninherently aims at fostering positive social interactions. Moreover, our\nresults show that attacking a specific component of the hate speech, and in\nparticular its implicit negative stereotype and its hateful parts, leads to\nhigher-quality generations.", "published": "2024-10-04 14:31:37", "link": "http://arxiv.org/abs/2410.03466v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Reproducible LLM Evaluation: Quantifying Uncertainty in LLM\n  Benchmark Scores", "abstract": "Large language models (LLMs) are stochastic, and not all models give\ndeterministic answers, even when setting temperature to zero with a fixed\nrandom seed. However, few benchmark studies attempt to quantify uncertainty,\npartly due to the time and cost of repeated experiments. We use benchmarks\ndesigned for testing LLMs' capacity to reason about cardinal directions to\nexplore the impact of experimental repeats on mean score and prediction\ninterval. We suggest a simple method for cost-effectively quantifying the\nuncertainty of a benchmark score and make recommendations concerning\nreproducible LLM evaluation.", "published": "2024-10-04 15:04:28", "link": "http://arxiv.org/abs/2410.03492v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CliMedBench: A Large-Scale Chinese Benchmark for Evaluating Medical\n  Large Language Models in Clinical Scenarios", "abstract": "With the proliferation of Large Language Models (LLMs) in diverse domains,\nthere is a particular need for unified evaluation standards in clinical medical\nscenarios, where models need to be examined very thoroughly. We present\nCliMedBench, a comprehensive benchmark with 14 expert-guided core clinical\nscenarios specifically designed to assess the medical ability of LLMs across 7\npivot dimensions. It comprises 33,735 questions derived from real-world medical\nreports of top-tier tertiary hospitals and authentic examination exercises. The\nreliability of this benchmark has been confirmed in several ways. Subsequent\nexperiments with existing LLMs have led to the following findings: (i) Chinese\nmedical LLMs underperform on this benchmark, especially where medical reasoning\nand factual consistency are vital, underscoring the need for advances in\nclinical knowledge and diagnostic accuracy. (ii) Several general-domain LLMs\ndemonstrate substantial potential in medical clinics, while the limited input\ncapacity of many medical LLMs hinders their practical use. These findings\nreveal both the strengths and limitations of LLMs in clinical scenarios and\noffer critical insights for medical research.", "published": "2024-10-04 15:15:36", "link": "http://arxiv.org/abs/2410.03502v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Steering Large Language Models between Code Execution and Textual\n  Reasoning", "abstract": "While a lot of recent research focuses on enhancing the textual reasoning\ncapabilities of Large Language Models (LLMs) by optimizing the multi-agent\nframework or reasoning chains, several benchmark tasks can be solved with 100\\%\nsuccess through direct coding, which is more scalable and avoids the\ncomputational overhead associated with textual iterating and searching. Textual\nreasoning has inherent limitations in solving tasks with challenges in math,\nlogics, optimization, and searching, which is unlikely to be solved by simply\nscaling up the model and data size. The recently released OpenAI GPT Code\nInterpreter and multi-agent frameworks such as AutoGen have demonstrated\nremarkable proficiency of integrating code generation and execution to solve\ncomplex tasks using LLMs. However, based on our experiments on 7 existing\npopular methods for steering code/text generation in both single- and\nmulti-turn settings with 14 tasks and 6 types of LLMs (including the new\nO1-preview), currently there is no optimal method to correctly steer LLMs to\nwrite code when needed. We discover some interesting patterns on when models\nuse code vs. textual reasoning with the evolution to task complexity and model\nsizes, which even result in an astonishingly inverse scaling behavior. We also\ndiscover that results from LLM written code are not always better than using\ntextual reasoning, even if the task could be solved through code. To mitigate\nthe above issues, we propose three methods to better steer LLM code/text\ngeneration and achieve a notable improvement. The costs of token lengths and\nruntime are thoroughly discussed for all the methods. We believe the problem of\nsteering LLM code/text generation is critical for future research and has much\nspace for further improvement. Project Page, Datasets, and Codes are available\nat https://yongchao98.github.io/CodeSteer/.", "published": "2024-10-04 15:44:47", "link": "http://arxiv.org/abs/2410.03524v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Re-examining Sexism and Misogyny Classification with Annotator Attitudes", "abstract": "Gender-Based Violence (GBV) is an increasing problem online, but existing\ndatasets fail to capture the plurality of possible annotator perspectives or\nensure the representation of affected groups. We revisit two important stages\nin the moderation pipeline for GBV: (1) manual data labelling; and (2)\nautomated classification. For (1), we examine two datasets to investigate the\nrelationship between annotator identities and attitudes and the responses they\ngive to two GBV labelling tasks. To this end, we collect demographic and\nattitudinal information from crowd-sourced annotators using three validated\nsurveys from Social Psychology. We find that higher Right Wing Authoritarianism\nscores are associated with a higher propensity to label text as sexist, while\nfor Social Dominance Orientation and Neosexist Attitudes, higher scores are\nassociated with a negative tendency to do so. For (2), we conduct\nclassification experiments using Large Language Models and five prompting\nstrategies, including infusing prompts with annotator information. We find: (i)\nannotator attitudes affect the ability of classifiers to predict their labels;\n(ii) including attitudinal information can boost performance when we use\nwell-structured brief annotator descriptions; and (iii) models struggle to\nreflect the increased complexity and imbalanced classes of the new label sets.", "published": "2024-10-04 15:57:58", "link": "http://arxiv.org/abs/2410.03543v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Data Quality through Simple De-duplication: Navigating\n  Responsible Computational Social Science Research", "abstract": "Research in natural language processing (NLP) for Computational Social\nScience (CSS) heavily relies on data from social media platforms. This data\nplays a crucial role in the development of models for analysing\nsocio-linguistic phenomena within online communities. In this work, we conduct\nan in-depth examination of 20 datasets extensively used in NLP for CSS to\ncomprehensively examine data quality. Our analysis reveals that social media\ndatasets exhibit varying levels of data duplication. Consequently, this gives\nrise to challenges like label inconsistencies and data leakage, compromising\nthe reliability of models. Our findings also suggest that data duplication has\nan impact on the current claims of state-of-the-art performance, potentially\nleading to an overestimation of model effectiveness in real-world scenarios.\nFinally, we propose new protocols and best practices for improving dataset\ndevelopment from social media data and its usage.", "published": "2024-10-04 15:58:15", "link": "http://arxiv.org/abs/2410.03545v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Table Question Answering for Low-resourced Indic Languages", "abstract": "TableQA is the task of answering questions over tables of structured\ninformation, returning individual cells or tables as output. TableQA research\nhas focused primarily on high-resource languages, leaving medium- and\nlow-resource languages with little progress due to scarcity of annotated data\nand neural models. We address this gap by introducing a fully automatic\nlarge-scale tableQA data generation process for low-resource languages with\nlimited budget. We incorporate our data generation method on two Indic\nlanguages, Bengali and Hindi, which have no tableQA datasets or models. TableQA\nmodels trained on our large-scale datasets outperform state-of-the-art LLMs. We\nfurther study the trained models on different aspects, including mathematical\nreasoning capabilities and zero-shot cross-lingual transfer. Our work is the\nfirst on low-resource tableQA focusing on scalable data generation and\nevaluation procedures. Our proposed data generation method can be applied to\nany low-resource language with a web presence. We release datasets, models, and\ncode (https://github.com/kolk/Low-Resource-TableQA-Indic-languages).", "published": "2024-10-04 16:26:12", "link": "http://arxiv.org/abs/2410.03576v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explicit, Implicit, and Scattered: Revisiting Event Extraction to\n  Capture Complex Arguments", "abstract": "Prior works formulate the extraction of event-specific arguments as a span\nextraction problem, where event arguments are explicit -- i.e. assumed to be\ncontiguous spans of text in a document. In this study, we revisit this\ndefinition of Event Extraction (EE) by introducing two key argument types that\ncannot be modeled by existing EE frameworks. First, implicit arguments are\nevent arguments which are not explicitly mentioned in the text, but can be\ninferred through context. Second, scattered arguments are event arguments that\nare composed of information scattered throughout the text. These two argument\ntypes are crucial to elicit the full breadth of information required for proper\nevent modeling.\n  To support the extraction of explicit, implicit, and scattered arguments, we\ndevelop a novel dataset, DiscourseEE, which includes 7,464 argument annotations\nfrom online health discourse. Notably, 51.2% of the arguments are implicit, and\n17.4% are scattered, making DiscourseEE a unique corpus for complex event\nextraction. Additionally, we formulate argument extraction as a text generation\nproblem to facilitate the extraction of complex argument types. We provide a\ncomprehensive evaluation of state-of-the-art models and highlight critical open\nchallenges in generative event extraction. Our data and codebase are available\nat https://omar-sharif03.github.io/DiscourseEE.", "published": "2024-10-04 16:54:30", "link": "http://arxiv.org/abs/2410.03594v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficiently Identifying Watermarked Segments in Mixed-Source Texts", "abstract": "Text watermarks in large language models (LLMs) are increasingly used to\ndetect synthetic text, mitigating misuse cases like fake news and academic\ndishonesty. While existing watermarking detection techniques primarily focus on\nclassifying entire documents as watermarked or not, they often neglect the\ncommon scenario of identifying individual watermark segments within longer,\nmixed-source documents. Drawing inspiration from plagiarism detection systems,\nwe propose two novel methods for partial watermark detection. First, we develop\na geometry cover detection framework aimed at determining whether there is a\nwatermark segment in long text. Second, we introduce an adaptive online\nlearning algorithm to pinpoint the precise location of watermark segments\nwithin the text. Evaluated on three popular watermarking techniques\n(KGW-Watermark, Unigram-Watermark, and Gumbel-Watermark), our approach achieves\nhigh accuracy, significantly outperforming baseline methods. Moreover, our\nframework is adaptable to other watermarking techniques, offering new insights\nfor precise watermark detection.", "published": "2024-10-04 16:58:41", "link": "http://arxiv.org/abs/2410.03600v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Searching for Best Practices in Medical Transcription with Large\n  Language Model", "abstract": "The transcription of medical monologues, especially those containing a high\ndensity of specialized terminology and delivered with a distinct accent,\npresents a significant challenge for existing automated systems. This paper\nintroduces a novel approach leveraging a Large Language Model (LLM) to generate\nhighly accurate medical transcripts from audio recordings of doctors'\nmonologues, specifically focusing on Indian accents. Our methodology integrates\nadvanced language modeling techniques to lower the Word Error Rate (WER) and\nensure the precise recognition of critical medical terms. Through rigorous\ntesting on a comprehensive dataset of medical recordings, our approach\ndemonstrates substantial improvements in both overall transcription accuracy\nand the fidelity of key medical terminologies. These results suggest that our\nproposed system could significantly aid in clinical documentation processes,\noffering a reliable tool for healthcare providers to streamline their\ntranscription needs while maintaining high standards of accuracy.", "published": "2024-10-04 03:41:16", "link": "http://arxiv.org/abs/2410.03797v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Misinformation with Legal Consequences (MisLC): A New Task Towards\n  Harnessing Societal Harm of Misinformation", "abstract": "Misinformation, defined as false or inaccurate information, can result in\nsignificant societal harm when it is spread with malicious or even innocuous\nintent. The rapid online information exchange necessitates advanced detection\nmechanisms to mitigate misinformation-induced harm. Existing research, however,\nhas predominantly focused on assessing veracity, overlooking the legal\nimplications and social consequences of misinformation. In this work, we take a\nnovel angle to consolidate the definition of misinformation detection using\nlegal issues as a measurement of societal ramifications, aiming to bring\ninterdisciplinary efforts to tackle misinformation and its consequence. We\nintroduce a new task: Misinformation with Legal Consequence (MisLC), which\nleverages definitions from a wide range of legal domains covering 4 broader\nlegal topics and 11 fine-grained legal issues, including hate speech, election\nlaws, and privacy regulations. For this task, we advocate a two-step dataset\ncuration approach that utilizes crowd-sourced checkworthiness and expert\nevaluations of misinformation. We provide insights about the MisLC task through\nempirical evidence, from the problem definition to experiments and expert\ninvolvement. While the latest large language models and retrieval-augmented\ngeneration are effective baselines for the task, we find they are still far\nfrom replicating expert performance.", "published": "2024-10-04 18:00:28", "link": "http://arxiv.org/abs/2410.03829v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FaithCAMERA: Construction of a Faithful Dataset for Ad Text Generation", "abstract": "In ad text generation (ATG), desirable ad text is both faithful and\ninformative. That is, it should be faithful to the input document, while at the\nsame time containing important information that appeals to potential customers.\nThe existing evaluation data, CAMERA (arXiv:2309.12030), is suitable for\nevaluating informativeness, as it consists of reference ad texts created by ad\ncreators. However, these references often include information unfaithful to the\ninput, which is a notable obstacle in promoting ATG research. In this study, we\ncollaborate with in-house ad creators to refine the CAMERA references and\ndevelop an alternative ATG evaluation dataset called FaithCAMERA, in which the\nfaithfulness of references is guaranteed. Using FaithCAMERA, we can evaluate\nhow well existing methods for improving faithfulness can generate informative\nad text while maintaining faithfulness. Our experiments show that removing\ntraining data that contains unfaithful entities improves the faithfulness and\ninformativeness at the entity level, but decreases both at the sentence level.\nThis result suggests that for future ATG research, it is essential not only to\nscale the training data but also to ensure their faithfulness. Our dataset will\nbe publicly available.", "published": "2024-10-04 18:13:24", "link": "http://arxiv.org/abs/2410.03839v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "You Know What I'm Saying: Jailbreak Attack via Implicit Reference", "abstract": "While recent advancements in large language model (LLM) alignment have\nenabled the effective identification of malicious objectives involving scene\nnesting and keyword rewriting, our study reveals that these methods remain\ninadequate at detecting malicious objectives expressed through context within\nnested harmless objectives. This study identifies a previously overlooked\nvulnerability, which we term Attack via Implicit Reference (AIR). AIR\ndecomposes a malicious objective into permissible objectives and links them\nthrough implicit references within the context. This method employs multiple\nrelated harmless objectives to generate malicious content without triggering\nrefusal responses, thereby effectively bypassing existing detection\ntechniques.Our experiments demonstrate AIR's effectiveness across\nstate-of-the-art LLMs, achieving an attack success rate (ASR) exceeding 90% on\nmost models, including GPT-4o, Claude-3.5-Sonnet, and Qwen-2-72B. Notably, we\nobserve an inverse scaling phenomenon, where larger models are more vulnerable\nto this attack method. These findings underscore the urgent need for defense\nmechanisms capable of understanding and preventing contextual attacks.\nFurthermore, we introduce a cross-model attack strategy that leverages less\nsecure models to generate malicious contexts, thereby further increasing the\nASR when targeting other models.Our code and jailbreak artifacts can be found\nat https://github.com/Lucas-TY/llm_Implicit_reference.", "published": "2024-10-04 18:42:57", "link": "http://arxiv.org/abs/2410.03857v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Language Models Reason about Individualistic Human Values and\n  Preferences?", "abstract": "Recent calls for pluralistic alignment emphasize that AI systems should\naddress the diverse needs of all people. Yet, efforts in this space often\nrequire sorting people into fixed buckets of pre-specified diversity-defining\ndimensions (e.g., demographics, personalities, communication styles), risking\nsmoothing out or even stereotyping the rich spectrum of individualistic\nvariations. To achieve an authentic representation of diversity that respects\nindividuality, we propose individualistic alignment. While individualistic\nalignment can take various forms, in this paper, we introduce\nIndieValueCatalog, a dataset transformed from the influential World Values\nSurvey (WVS), to study language models (LMs) on the specific challenge of\nindividualistic value reasoning. Specifically, given a sample of an\nindividual's value-expressing statements, models are tasked with predicting\ntheir value judgments in novel cases. With IndieValueCatalog, we reveal\ncritical limitations in frontier LMs' abilities to reason about individualistic\nhuman values with accuracies, only ranging between 55% to 65%. Moreover, our\nresults highlight that a precise description of individualistic values cannot\nbe approximated only via demographic information. We also identify a partiality\nof LMs in reasoning about global individualistic values, as measured by our\nproposed Value Inequity Index ({\\sigma}INEQUITY). Finally, we train a series of\nIndividualistic Value Reasoners (IndieValueReasoner) using IndieValueCatalog to\nenhance models' individualistic value reasoning capability, revealing new\npatterns and dynamics into global human values. We outline future research\nchallenges and opportunities for advancing individualistic alignment.", "published": "2024-10-04 19:03:41", "link": "http://arxiv.org/abs/2410.03868v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Pixels to Personas: Investigating and Modeling\n  Self-Anthropomorphism in Human-Robot Dialogues", "abstract": "Self-anthropomorphism in robots manifests itself through their display of\nhuman-like characteristics in dialogue, such as expressing preferences and\nemotions. Our study systematically analyzes self-anthropomorphic expression\nwithin various dialogue datasets, outlining the contrasts between\nself-anthropomorphic and non-self-anthropomorphic responses in dialogue\nsystems. We show significant differences in these two types of responses and\npropose transitioning from one type to the other. We also introduce\nPix2Persona, a novel dataset aimed at developing ethical and engaging AI\nsystems in various embodiments. This dataset preserves the original dialogues\nfrom existing corpora and enhances them with paired responses:\nself-anthropomorphic and non-self-anthropomorphic for each original bot\nresponse. Our work not only uncovers a new category of bot responses that were\npreviously under-explored but also lays the groundwork for future studies about\ndynamically adjusting self-anthropomorphism levels in AI systems to align with\nethical standards and user expectations.", "published": "2024-10-04 19:06:24", "link": "http://arxiv.org/abs/2410.03870v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PersonalSum: A User-Subjective Guided Personalized Summarization Dataset\n  for Large Language Models", "abstract": "With the rapid advancement of Natural Language Processing in recent years,\nnumerous studies have shown that generic summaries generated by Large Language\nModels (LLMs) can sometimes surpass those annotated by experts, such as\njournalists, according to human evaluations. However, there is limited research\non whether these generic summaries meet the individual needs of ordinary\npeople. The biggest obstacle is the lack of human-annotated datasets from the\ngeneral public. Existing work on personalized summarization often relies on\npseudo datasets created from generic summarization datasets or controllable\ntasks that focus on specific named entities or other aspects, such as the\nlength and specificity of generated summaries, collected from hypothetical\ntasks without the annotators' initiative. To bridge this gap, we propose a\nhigh-quality, personalized, manually annotated abstractive summarization\ndataset called PersonalSum. This dataset is the first to investigate whether\nthe focus of public readers differs from the generic summaries generated by\nLLMs. It includes user profiles, personalized summaries accompanied by source\nsentences from given articles, and machine-generated generic summaries along\nwith their sources. We investigate several personal signals - entities/topics,\nplot, and structure of articles - that may affect the generation of\npersonalized summaries using LLMs in a few-shot in-context learning scenario.\nOur preliminary results and analysis indicate that entities/topics are merely\none of the key factors that impact the diverse preferences of users, and\npersonalized summarization remains a significant challenge for existing LLMs.", "published": "2024-10-04 20:12:39", "link": "http://arxiv.org/abs/2410.03905v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ActPlan-1K: Benchmarking the Procedural Planning Ability of Visual\n  Language Models in Household Activities", "abstract": "Large language models~(LLMs) have been adopted to process textual task\ndescription and accomplish procedural planning in embodied AI tasks because of\ntheir powerful reasoning ability. However, there is still lack of study on how\nvision language models~(VLMs) behave when multi-modal task inputs are\nconsidered. Counterfactual planning that evaluates the model's reasoning\nability over alternative task situations are also under exploited. In order to\nevaluate the planning ability of both multi-modal and counterfactual aspects,\nwe propose ActPlan-1K. ActPlan-1K is a multi-modal planning benchmark\nconstructed based on ChatGPT and household activity simulator iGibson2. The\nbenchmark consists of 153 activities and 1,187 instances. Each instance\ndescribing one activity has a natural language task description and multiple\nenvironment images from the simulator. The gold plan of each instance is action\nsequences over the objects in provided scenes. Both the correctness and\ncommonsense satisfaction are evaluated on typical VLMs. It turns out that\ncurrent VLMs are still struggling at generating human-level procedural plans\nfor both normal activities and counterfactual activities. We further provide\nautomatic evaluation metrics by finetuning over BLEURT model to facilitate\nfuture research on our benchmark.", "published": "2024-10-04 20:21:40", "link": "http://arxiv.org/abs/2410.03907v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question-Answering System for Bangla: Fine-tuning BERT-Bangla for a\n  Closed Domain", "abstract": "Question-answering systems for Bengali have seen limited development,\nparticularly in domain-specific applications. Leveraging advancements in\nnatural language processing, this paper explores a fine-tuned BERT-Bangla model\nto address this gap. It presents the development of a question-answering system\nfor Bengali using a fine-tuned BERT-Bangla model in a closed domain. The\ndataset was sourced from Khulna University of Engineering \\& Technology's\n(KUET) website and other relevant texts. The system was trained and evaluated\nwith 2500 question-answer pairs generated from curated data. Key metrics,\nincluding the Exact Match (EM) score and F1 score, were used for evaluation,\nachieving scores of 55.26\\% and 74.21\\%, respectively. The results demonstrate\npromising potential for domain-specific Bengali question-answering systems.\nFurther refinements are needed to improve performance for more complex queries.", "published": "2024-10-04 20:57:08", "link": "http://arxiv.org/abs/2410.03923v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structured List-Grounded Question Answering", "abstract": "Document-grounded dialogue systems aim to answer user queries by leveraging\nexternal information. Previous studies have mainly focused on handling\nfree-form documents, often overlooking structured data such as lists, which can\nrepresent a range of nuanced semantic relations. Motivated by the observation\nthat even advanced language models like GPT-3.5 often miss semantic cues from\nlists, this paper aims to enhance question answering (QA) systems for better\ninterpretation and use of structured lists. To this end, we introduce the\nLIST2QA dataset, a novel benchmark to evaluate the ability of QA systems to\nrespond effectively using list information. This dataset is created from\nunlabeled customer service documents using language models and model-based\nfiltering processes to enhance data quality, and can be used to fine-tune and\nevaluate QA models. Apart from directly generating responses through fine-tuned\nmodels, we further explore the explicit use of Intermediate Steps for Lists\n(ISL), aligning list items with user backgrounds to better reflect how humans\ninterpret list items before generating responses. Our experimental results\ndemonstrate that models trained on LIST2QA with our ISL approach outperform\nbaselines across various metrics. Specifically, our fine-tuned Flan-T5-XL model\nshows increases of 3.1% in ROUGE-L, 4.6% in correctness, 4.5% in faithfulness,\nand 20.6% in completeness compared to models without applying filtering and the\nproposed ISL method.", "published": "2024-10-04 22:21:43", "link": "http://arxiv.org/abs/2410.03950v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MetricX-24: The Google Submission to the WMT 2024 Metrics Shared Task", "abstract": "In this paper, we present the MetricX-24 submissions to the WMT24 Metrics\nShared Task and provide details on the improvements we made over the previous\nversion of MetricX. Our primary submission is a hybrid reference-based/-free\nmetric, which can score a translation irrespective of whether it is given the\nsource segment, the reference, or both. The metric is trained on previous WMT\ndata in a two-stage fashion, first on the DA ratings only, then on a mixture of\nMQM and DA ratings. The training set in both stages is augmented with synthetic\nexamples that we created to make the metric more robust to several common\nfailure modes, such as fluent but unrelated translation, or undertranslation.\nWe demonstrate the benefits of the individual modifications via an ablation\nstudy, and show a significant performance increase over MetricX-23 on the WMT23\nMQM ratings, as well as our new synthetic challenge set.", "published": "2024-10-04 23:52:28", "link": "http://arxiv.org/abs/2410.03983v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Llettuce: An Open Source Natural Language Processing Tool for the\n  Translation of Medical Terms into Uniform Clinical Encoding", "abstract": "This paper introduces Llettuce, an open-source tool designed to address the\ncomplexities of converting medical terms into OMOP standard concepts. Unlike\nexisting solutions such as the Athena database search and Usagi, which struggle\nwith semantic nuances and require substantial manual input, Llettuce leverages\nadvanced natural language processing, including large language models and fuzzy\nmatching, to automate and enhance the mapping process. Developed with a focus\non GDPR compliance, Llettuce can be deployed locally, ensuring data protection\nwhile maintaining high performance in converting informal medical terms to\nstandardised concepts.", "published": "2024-10-04 16:11:15", "link": "http://arxiv.org/abs/2410.09076v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DocKD: Knowledge Distillation from LLMs for Open-World Document\n  Understanding Models", "abstract": "Visual document understanding (VDU) is a challenging task that involves\nunderstanding documents across various modalities (text and image) and layouts\n(forms, tables, etc.). This study aims to enhance generalizability of small VDU\nmodels by distilling knowledge from LLMs. We identify that directly prompting\nLLMs often fails to generate informative and useful data. In response, we\npresent a new framework (called DocKD) that enriches the data generation\nprocess by integrating external document knowledge. Specifically, we provide an\nLLM with various document elements like key-value pairs, layouts, and\ndescriptions, to elicit open-ended answers. Our experiments show that DocKD\nproduces high-quality document annotations and surpasses the direct knowledge\ndistillation approach that does not leverage external document knowledge.\nMoreover, student VDU models trained with solely DocKD-generated data are not\nonly comparable to those trained with human-annotated data on in-domain tasks\nbut also significantly excel them on out-of-domain tasks.", "published": "2024-10-04 00:53:32", "link": "http://arxiv.org/abs/2410.03061v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Enhancing Short-Text Topic Modeling with LLM-Driven Context Expansion\n  and Prefix-Tuned VAEs", "abstract": "Topic modeling is a powerful technique for uncovering hidden themes within a\ncollection of documents. However, the effectiveness of traditional topic models\noften relies on sufficient word co-occurrence, which is lacking in short texts.\nTherefore, existing approaches, whether probabilistic or neural, frequently\nstruggle to extract meaningful patterns from such data, resulting in incoherent\ntopics. To address this challenge, we propose a novel approach that leverages\nlarge language models (LLMs) to extend short texts into more detailed sequences\nbefore applying topic modeling. To further improve the efficiency and solve the\nproblem of semantic inconsistency from LLM-generated texts, we propose to use\nprefix tuning to train a smaller language model coupled with a variational\nautoencoder for short-text topic modeling. Our method significantly improves\nshort-text topic modeling performance, as demonstrated by extensive experiments\non real-world datasets with extreme data sparsity, outperforming current\nstate-of-the-art topic models.", "published": "2024-10-04 01:28:56", "link": "http://arxiv.org/abs/2410.03071v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "CommonIT: Commonality-Aware Instruction Tuning for Large Language Models\n  via Data Partitions", "abstract": "With instruction tuning, Large Language Models (LLMs) can enhance their\nability to adhere to commands. Diverging from most works focusing on data\nmixing, our study concentrates on enhancing the model's capabilities from the\nperspective of data sampling during training. Drawing inspiration from the\nhuman learning process, where it is generally easier to master solutions to\nsimilar topics through focused practice on a single type of topic, we introduce\na novel instruction tuning strategy termed CommonIT: Commonality-aware\nInstruction Tuning. Specifically, we cluster instruction datasets into distinct\ngroups with three proposed metrics (Task, Embedding and Length). We ensure each\ntraining mini-batch, or \"partition\", consists solely of data from a single\ngroup, which brings about both data randomness across mini-batches and\nintra-batch data similarity. Rigorous testing on LLaMa models demonstrates\nCommonIT's effectiveness in enhancing the instruction-following capabilities of\nLLMs through IT datasets (FLAN, CoT, and Alpaca) and models (LLaMa2-7B,\nQwen2-7B, LLaMa 13B, and BLOOM 7B). CommonIT consistently boosts an average\nimprovement of 2.1\\% on the general domain (i.e., the average score of\nKnowledge, Reasoning, Multilinguality and Coding) with the Length metric, and\n5.2\\% on the special domain (i.e., GSM, Openfunctions and Code) with the Task\nmetric, and 3.8\\% on the specific tasks (i.e., MMLU) with the Embedding metric.\nCode is available at \\url{https://github.com/raojay7/CommonIT}.", "published": "2024-10-04 01:42:35", "link": "http://arxiv.org/abs/2410.03077v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Scaling Parameter-Constrained Language Models with Quality Data", "abstract": "Scaling laws in language modeling traditionally quantify training loss as a\nfunction of dataset size and model parameters, providing compute-optimal\nestimates but often neglecting the impact of data quality on model\ngeneralization. In this paper, we extend the conventional understanding of\nscaling law by offering a microscopic view of data quality within the original\nformulation -- effective training tokens -- which we posit to be a critical\ndeterminant of performance for parameter-constrained language models.\nSpecifically, we formulate the proposed term of effective training tokens to be\na combination of two readily-computed indicators of text: (i) text diversity\nand (ii) syntheticity as measured by a teacher model. We pretrained over $200$\nmodels of 25M to 1.5B parameters on a diverse set of sampled, synthetic data,\nand estimated the constants that relate text quality, model size, training\ntokens, and eight reasoning task accuracy scores. We demonstrated the estimated\nconstants yield +0.83 Pearson correlation with true accuracies, and analyzed it\nin scenarios involving widely-used data techniques such as data sampling and\nsynthesis which aim to improve data quality.", "published": "2024-10-04 02:07:17", "link": "http://arxiv.org/abs/2410.03083v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large\n  Language Model Inference", "abstract": "Deploying large language models (LLMs) is challenging due to their high\nmemory and computational demands, especially during long-context inference.\nWhile key-value (KV) caching accelerates inference by reusing previously\ncomputed keys and values, it also introduces significant memory overhead.\nExisting KV cache compression methods such as eviction and merging typically\ncompress the KV cache after it is generated and overlook the eviction of hidden\nstates, failing to improve the speed of the prefilling stage. Additionally,\napplying a uniform compression rate across different attention heads can harm\ncrucial retrieval heads in needle-in-a-haystack tasks due to excessive\ncompression. In this paper, we propose UNComp, an uncertainty-aware compression\nscheme that leverages matrix entropy to estimate model uncertainty across\nlayers and heads at the token sequence level. By grouping layers and heads\nbased on their uncertainty, UNComp adaptively compresses both the hidden states\nand the KV cache. Our method achieves a 1.6x speedup in the prefilling stage\nand reduces the KV cache to 4.74% of its original size, resulting in a 6.4x\nincrease in throughput and a 1.4x speedup in inference with only a 1.41%\nperformance loss. Remarkably, in needle-in-a-haystack tasks, UNComp outperforms\nthe full-size KV cache even when compressed to 9.38% of its original size. Our\napproach offers an efficient, training-free Grouped-Query Attention paradigm\nthat can be seamlessly integrated into existing KV cache schemes.", "published": "2024-10-04 02:32:36", "link": "http://arxiv.org/abs/2410.03090v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Unsupervised Prompt Learning for Classification with Black-box\n  Language Models", "abstract": "Large language models (LLMs) have achieved impressive success in\ntext-formatted learning problems, and most popular LLMs have been deployed in a\nblack-box fashion. Meanwhile, fine-tuning is usually necessary for a specific\ndownstream task to obtain better performance, and this functionality is\nprovided by the owners of the black-box LLMs. To fine-tune a black-box LLM,\nlabeled data are always required to adjust the model parameters. However, in\nmany real-world applications, LLMs can label textual datasets with even better\nquality than skilled human annotators, motivating us to explore the possibility\nof fine-tuning black-box LLMs with unlabeled data. In this paper, we propose\nunsupervised prompt learning for classification with black-box LLMs, where the\nlearning parameters are the prompt itself and the pseudo labels of unlabeled\ndata. Specifically, the prompt is modeled as a sequence of discrete tokens, and\nevery token has its own to-be-learned categorical distribution. On the other\nhand, for learning the pseudo labels, we are the first to consider the\nin-context learning (ICL) capabilities of LLMs: we first identify reliable\npseudo-labeled data using the LLM, and then assign pseudo labels to other\nunlabeled data based on the prompt, allowing the pseudo-labeled data to serve\nas in-context demonstrations alongside the prompt. Those in-context\ndemonstrations matter: previously, they are involved when the prompt is used\nfor prediction while they are not involved when the prompt is trained; thus,\ntaking them into account during training makes the prompt-learning and\nprompt-using stages more consistent. Experiments on benchmark datasets show the\neffectiveness of our proposed algorithm. After unsupervised prompt learning, we\ncan use the pseudo-labeled dataset for further fine-tuning by the owners of the\nblack-box LLMs.", "published": "2024-10-04 03:39:28", "link": "http://arxiv.org/abs/2410.03124v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "In-context Learning in Presence of Spurious Correlations", "abstract": "Large language models exhibit a remarkable capacity for in-context learning,\nwhere they learn to solve tasks given a few examples. Recent work has shown\nthat transformers can be trained to perform simple regression tasks in-context.\nThis work explores the possibility of training an in-context learner for\nclassification tasks involving spurious features. We find that the conventional\napproach of training in-context learners is susceptible to spurious features.\nMoreover, when the meta-training dataset includes instances of only one task,\nthe conventional approach leads to task memorization and fails to produce a\nmodel that leverages context for predictions. Based on these observations, we\npropose a novel technique to train such a learner for a given classification\ntask. Remarkably, this in-context learner matches and sometimes outperforms\nstrong methods like ERM and GroupDRO. However, unlike these algorithms, it does\nnot generalize well to other tasks. We show that it is possible to obtain an\nin-context learner that generalizes to unseen tasks by training on a diverse\ndataset of synthetic in-context learning instances.", "published": "2024-10-04 04:26:36", "link": "http://arxiv.org/abs/2410.03140v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Media Framing through the Lens of Event-Centric Narratives", "abstract": "From a communications perspective, a frame defines the packaging of the\nlanguage used in such a way as to encourage certain interpretations and to\ndiscourage others. For example, a news article can frame immigration as either\na boost or a drain on the economy, and thus communicate very different\ninterpretations of the same phenomenon. In this work, we argue that to explain\nframing devices we have to look at the way narratives are constructed. As a\nfirst step in this direction, we propose a framework that extracts events and\ntheir relations to other events, and groups them into high-level narratives\nthat help explain frames in news articles. We show that our framework can be\nused to analyze framing in U.S. news for two different domains: immigration and\ngun control.", "published": "2024-10-04 05:21:42", "link": "http://arxiv.org/abs/2410.03151v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Can Watermarked LLMs be Identified by Users via Crafted Prompts?", "abstract": "Text watermarking for Large Language Models (LLMs) has made significant\nprogress in detecting LLM outputs and preventing misuse. Current watermarking\ntechniques offer high detectability, minimal impact on text quality, and\nrobustness to text editing. However, current researches lack investigation into\nthe imperceptibility of watermarking techniques in LLM services. This is\ncrucial as LLM providers may not want to disclose the presence of watermarks in\nreal-world scenarios, as it could reduce user willingness to use the service\nand make watermarks more vulnerable to attacks. This work is the first to\ninvestigate the imperceptibility of watermarked LLMs. We design an\nidentification algorithm called Water-Probe that detects watermarks through\nwell-designed prompts to the LLM. Our key motivation is that current\nwatermarked LLMs expose consistent biases under the same watermark key,\nresulting in similar differences across prompts under different watermark keys.\nExperiments show that almost all mainstream watermarking algorithms are easily\nidentified with our well-designed prompts, while Water-Probe demonstrates a\nminimal false positive rate for non-watermarked LLMs. Finally, we propose that\nthe key to enhancing the imperceptibility of watermarked LLMs is to increase\nthe randomness of watermark key selection. Based on this, we introduce the\nWater-Bag strategy, which significantly improves watermark imperceptibility by\nmerging multiple watermark keys.", "published": "2024-10-04 06:01:27", "link": "http://arxiv.org/abs/2410.03168v3", "categories": ["cs.CR", "cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CR"}
{"title": "Generating bilingual example sentences with large language models as\n  lexicography assistants", "abstract": "We present a study of LLMs' performance in generating and rating example\nsentences for bilingual dictionaries across languages with varying resource\nlevels: French (high-resource), Indonesian (mid-resource), and Tetun\n(low-resource), with English as the target language. We evaluate the quality of\nLLM-generated examples against the GDEX (Good Dictionary EXample) criteria:\ntypicality, informativeness, and intelligibility. Our findings reveal that\nwhile LLMs can generate reasonably good dictionary examples, their performance\ndegrades significantly for lower-resourced languages. We also observe high\nvariability in human preferences for example quality, reflected in low\ninter-annotator agreement rates. To address this, we demonstrate that\nin-context learning can successfully align LLMs with individual annotator\npreferences. Additionally, we explore the use of pre-trained language models\nfor automated rating of examples, finding that sentence perplexity serves as a\ngood proxy for typicality and intelligibility in higher-resourced languages.\nOur study also contributes a novel dataset of 600 ratings for LLM-generated\nsentence pairs, and provides insights into the potential of LLMs in reducing\nthe cost of lexicographic work, particularly for low-resource languages.", "published": "2024-10-04 06:45:48", "link": "http://arxiv.org/abs/2410.03182v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Semantic Structure through First-Order-Logic Translation", "abstract": "In this paper, we study whether transformer-based language models can extract\npredicate argument structure from simple sentences. We firstly show that\nlanguage models sometimes confuse which predicates apply to which objects. To\nmitigate this, we explore two tasks: question answering (Q/A), and first order\nlogic (FOL) translation, and two regimes, prompting and finetuning. In FOL\ntranslation, we finetune several large language models on synthetic datasets\ndesigned to gauge their generalization abilities. For Q/A, we finetune encoder\nmodels like BERT and RoBERTa and use prompting for LLMs. The results show that\nFOL translation for LLMs is better suited to learn predicate argument\nstructure.", "published": "2024-10-04 07:39:34", "link": "http://arxiv.org/abs/2410.03203v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NLIP_Lab-IITH Low-Resource MT System for WMT24 Indic MT Shared Task", "abstract": "In this paper, we describe our system for the WMT 24 shared task of\nLow-Resource Indic Language Translation. We consider eng $\\leftrightarrow$ {as,\nkha, lus, mni} as participating language pairs. In this shared task, we explore\nthe finetuning of a pre-trained model motivated by the pre-trained objective of\naligning embeddings closer by alignment augmentation \\cite{lin-etal-2020-pre}\nfor 22 scheduled Indian languages. Our primary system is based on\nlanguage-specific finetuning on a pre-trained model. We achieve chrF2 scores of\n50.6, 42.3, 54.9, and 66.3 on the official public test set for\neng$\\rightarrow$as, eng$\\rightarrow$kha, eng$\\rightarrow$lus,\neng$\\rightarrow$mni respectively. We also explore multilingual training\nwith/without language grouping and layer-freezing. Our code, models, and\ngenerated translations are available here:\nhttps://github.com/pramitsahoo/WMT2024-LRILT.", "published": "2024-10-04 08:02:43", "link": "http://arxiv.org/abs/2410.03215v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Frame-Voyager: Learning to Query Frames for Video Large Language Models", "abstract": "Video Large Language Models (Video-LLMs) have made remarkable progress in\nvideo understanding tasks. However, they are constrained by the maximum length\nof input tokens, making it impractical to input entire videos. Existing frame\nselection approaches, such as uniform frame sampling and text-frame retrieval,\nfail to account for the information density variations in the videos or the\ncomplex instructions in the tasks, leading to sub-optimal performance. In this\npaper, we propose Frame-Voyager that learns to query informative frame\ncombinations, based on the given textual queries in the task. To train\nFrame-Voyager, we introduce a new data collection and labeling pipeline, by\nranking frame combinations using a pre-trained Video-LLM. Given a video of M\nframes, we traverse its T-frame combinations, feed them into a Video-LLM, and\nrank them based on Video-LLM's prediction losses. Using this ranking as\nsupervision, we train Frame-Voyager to query the frame combinations with lower\nlosses. In experiments, we evaluate Frame-Voyager on four Video Question\nAnswering benchmarks by plugging it into two different Video-LLMs. The\nexperimental results demonstrate that Frame-Voyager achieves impressive results\nin all settings, highlighting its potential as a plug-and-play solution for\nVideo-LLMs.", "published": "2024-10-04 08:26:06", "link": "http://arxiv.org/abs/2410.03226v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Showing LLM-Generated Code Selectively Based on Confidence of LLMs", "abstract": "Large Language Models (LLMs) have shown impressive abilities in code\ngeneration, but they may generate erroneous programs. Reading a program takes\nten times longer than writing it. Showing these erroneous programs to\ndevelopers will waste developers' energies and introduce security risks to\nsoftware.\n  To address the above limitations, we propose HonestCoder, a novel LLM-based\ncode generation approach. HonestCoder selectively shows the generated programs\nto developers based on LLMs' confidence. The confidence provides valuable\ninsights into the correctness of generated programs. To achieve this goal, we\npropose a novel approach to estimate LLMs' confidence in code generation. It\nestimates confidence by measuring the multi-modal similarity between\nLLMs-generated programs.\n  We collect and release a multilingual benchmark named TruthCodeBench, which\nconsists of 2,265 samples and covers two popular programming languages (i.e.,\nPython and Java). We apply HonestCoder to four popular LLMs (e.g.,\nDeepSeek-Coder and Code Llama) and evaluate it on TruthCodeBench. Based on the\nexperiments, we obtain the following insights. (1) HonestCoder can effectively\nestimate LLMs' confidence and accurately determine the correctness of generated\nprograms. For example, HonestCoder outperforms the state-of-the-art baseline by\n27.79% in AUROC and 63.74% in AUCPR. (2) HonestCoder can decrease the number of\nerroneous programs shown to developers. Compared to eight baselines, it can\nshow more correct programs and fewer erroneous programs to developers. (3)\nCompared to showing code indiscriminately, HonestCoder only adds slight time\noverhead (approximately 0.4 seconds per requirement). (4) We discuss future\ndirections to facilitate the application of LLMs in software development. We\nhope this work can motivate broad discussions about measuring the reliability\nof LLMs' outputs in performing code-related tasks.", "published": "2024-10-04 08:51:31", "link": "http://arxiv.org/abs/2410.03234v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Towards a Benchmark for Large Language Models for Business Process\n  Management Tasks", "abstract": "An increasing number of organizations are deploying Large Language Models\n(LLMs) for a wide range of tasks. Despite their general utility, LLMs are prone\nto errors, ranging from inaccuracies to hallucinations. To objectively assess\nthe capabilities of existing LLMs, performance benchmarks are conducted.\nHowever, these benchmarks often do not translate to more specific real-world\ntasks. This paper addresses the gap in benchmarking LLM performance in the\nBusiness Process Management (BPM) domain. Currently, no BPM-specific benchmarks\nexist, creating uncertainty about the suitability of different LLMs for BPM\ntasks. This paper systematically compares LLM performance on four BPM tasks\nfocusing on small open-source models. The analysis aims to identify\ntask-specific performance variations, compare the effectiveness of open-source\nversus commercial models, and assess the impact of model size on BPM task\nperformance. This paper provides insights into the practical applications of\nLLMs in BPM, guiding organizations in selecting appropriate models for their\nspecific needs.", "published": "2024-10-04 09:18:54", "link": "http://arxiv.org/abs/2410.03255v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Comparing zero-shot self-explanations with human rationales in text\n  classification", "abstract": "Instruction-tuned LLMs are able to provide an explanation about their output\nto users by generating self-explanations. These do not require gradient\ncomputations or the application of possibly complex XAI methods. In this paper,\nwe analyse whether this ability results in a good explanation. We evaluate\nself-explanations in the form of input rationales with respect to their\nplausibility to humans as well as their faithfulness to models. We study two\ntext classification tasks: sentiment classification and forced labour\ndetection, i.e., identifying pre-defined risk indicators of forced labour. In\naddition to English, we include Danish and Italian translations of the\nsentiment classification task and compare self-explanations to human\nannotations for all samples. To allow for direct comparisons, we also compute\npost-hoc feature attribution, i.e., layer-wise relevance propagation (LRP) and\nanalyse 4 LLMs. We show that self-explanations align more closely with human\nannotations compared to LRP, while maintaining a comparable level of\nfaithfulness. This finding suggests that self-explanations indeed provide good\nexplanations for text classification.", "published": "2024-10-04 10:14:12", "link": "http://arxiv.org/abs/2410.03296v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Context and System Fusion in Post-ASR Emotion Recognition with Large\n  Language Models", "abstract": "Large language models (LLMs) have started to play a vital role in modelling\nspeech and text. To explore the best use of context and multiple systems'\noutputs for post-ASR speech emotion prediction, we study LLM prompting on a\nrecent task named GenSEC. Our techniques include ASR transcript ranking,\nvariable conversation context, and system output fusion. We show that the\nconversation context has diminishing returns and the metric used to select the\ntranscript for prediction is crucial. Finally, our best submission surpasses\nthe provided baseline by 20% in absolute accuracy.", "published": "2024-10-04 10:50:18", "link": "http://arxiv.org/abs/2410.03312v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "One2set + Large Language Model: Best Partners for Keyphrase Generation", "abstract": "Keyphrase generation (KPG) aims to automatically generate a collection of\nphrases representing the core concepts of a given document. The dominant\nparadigms in KPG include one2seq and one2set. Recently, there has been\nincreasing interest in applying large language models (LLMs) to KPG. Our\npreliminary experiments reveal that it is challenging for a single model to\nexcel in both recall and precision. Further analysis shows that: 1) the one2set\nparadigm owns the advantage of high recall, but suffers from improper\nassignments of supervision signals during training; 2) LLMs are powerful in\nkeyphrase selection, but existing selection methods often make redundant\nselections. Given these observations, we introduce a generate-then-select\nframework decomposing KPG into two steps, where we adopt a one2set-based model\nas generator to produce candidates and then use an LLM as selector to select\nkeyphrases from these candidates. Particularly, we make two important\nimprovements on our generator and selector: 1) we design an Optimal\nTransport-based assignment strategy to address the above improper assignments;\n2) we model the keyphrase selection as a sequence labeling task to alleviate\nredundant selections. Experimental results on multiple benchmark datasets show\nthat our framework significantly surpasses state-of-the-art models, especially\nin absent keyphrase prediction.", "published": "2024-10-04 13:31:09", "link": "http://arxiv.org/abs/2410.03421v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Images Speak Volumes: User-Centric Assessment of Image Generation for\n  Accessible Communication", "abstract": "Explanatory images play a pivotal role in accessible and easy-to-read (E2R)\ntexts. However, the images available in online databases are not tailored\ntoward the respective texts, and the creation of customized images is\nexpensive. In this large-scale study, we investigated whether text-to-image\ngeneration models can close this gap by providing customizable images quickly\nand easily. We benchmarked seven, four open- and three closed-source, image\ngeneration models and provide an extensive evaluation of the resulting images.\nIn addition, we performed a user study with people from the E2R target group to\nexamine whether the images met their requirements. We find that some of the\nmodels show remarkable performance, but none of the models are ready to be used\nat a larger scale without human supervision. Our research is an important step\ntoward facilitating the creation of accessible information for E2R creators and\ntailoring accessible images to the target group's needs.", "published": "2024-10-04 13:40:15", "link": "http://arxiv.org/abs/2410.03430v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Exploring the Benefit of Activation Sparsity in Pre-training", "abstract": "Pre-trained Transformers inherently possess the characteristic of sparse\nactivation, where only a small fraction of the neurons are activated for each\ntoken. While sparse activation has been explored through post-training methods,\nits potential in pre-training remains untapped. In this work, we first study\nhow activation properties change during pre-training. Our examination reveals\nthat Transformers exhibit sparse activation throughout the majority of the\npre-training process while the activation correlation keeps evolving as\ntraining progresses. Leveraging this observation, we propose Switchable\nSparse-Dense Learning (SSD). SSD adaptively switches between the\nMixtures-of-Experts (MoE) based sparse training and the conventional dense\ntraining during the pre-training process, leveraging the efficiency of sparse\ntraining and avoiding the static activation correlation of sparse training.\nCompared to dense training, SSD achieves comparable performance with identical\nmodel size and reduces pre-training costs. Moreover, the models trained with\nSSD can be directly used as MoE models for sparse inference and achieve the\nsame performance as dense models with up to $2\\times$ faster inference speed.\nCodes are available at https://github.com/thunlp/moefication.", "published": "2024-10-04 13:53:33", "link": "http://arxiv.org/abs/2410.03440v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Auto-GDA: Automatic Domain Adaptation for Efficient Grounding\n  Verification in Retrieval-Augmented Generation", "abstract": "While retrieval-augmented generation (RAG) has been shown to enhance\nfactuality of large language model (LLM) outputs, LLMs still suffer from\nhallucination, generating incorrect or irrelevant information. A common\ndetection strategy involves prompting the LLM again to assess whether its\nresponse is grounded in the retrieved evidence, but this approach is costly.\nAlternatively, lightweight natural language inference (NLI) models for\nefficient grounding verification can be used at inference time. While existing\npre-trained NLI models offer potential solutions, their performance remains\nsubpar compared to larger models on realistic RAG inputs. RAG inputs are more\ncomplex than most datasets used for training NLI models and have\ncharacteristics specific to the underlying knowledge base, requiring adaptation\nof the NLI models to a specific target domain. Additionally, the lack of\nlabeled instances in the target domain makes supervised domain adaptation,\ne.g., through fine-tuning, infeasible. To address these challenges, we\nintroduce Automatic Generative Domain Adaptation (Auto-GDA). Our framework\nenables unsupervised domain adaptation through synthetic data generation.\nUnlike previous methods that rely on handcrafted filtering and augmentation\nstrategies, Auto-GDA employs an iterative process to continuously improve the\nquality of generated samples using weak labels from less efficient teacher\nmodels and discrete optimization to select the most promising augmented\nsamples. Experimental results demonstrate the effectiveness of our approach,\nwith models fine-tuned on synthetic data using Auto-GDA often surpassing the\nperformance of the teacher model and reaching the performance level of LLMs at\n10% of their computational cost.", "published": "2024-10-04 14:21:27", "link": "http://arxiv.org/abs/2410.03461v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "No Need to Talk: Asynchronous Mixture of Language Models", "abstract": "We introduce SmallTalk LM, an innovative method for training a mixture of\nlanguage models in an almost asynchronous manner. Each model of the mixture\nspecializes in distinct parts of the data distribution, without the need of\nhigh-bandwidth communication between the nodes training each model. At\ninference, a lightweight router directs a given sequence to a single expert,\naccording to a short prefix. This inference scheme naturally uses a fraction of\nthe parameters from the overall mixture model. Our experiments on language\nmodeling demonstrate tha SmallTalk LM achieves significantly lower perplexity\nthan dense model baselines for the same total training FLOPs and an almost\nidentical inference cost. Finally, in our downstream evaluations we outperform\nthe dense baseline on $75\\%$ of the tasks.", "published": "2024-10-04 15:50:10", "link": "http://arxiv.org/abs/2410.03529v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MARE: Multi-Aspect Rationale Extractor on Unsupervised Rationale\n  Extraction", "abstract": "Unsupervised rationale extraction aims to extract text snippets to support\nmodel predictions without explicit rationale annotation. Researchers have made\nmany efforts to solve this task. Previous works often encode each aspect\nindependently, which may limit their ability to capture meaningful internal\ncorrelations between aspects. While there has been significant work on\nmitigating spurious correlations, our approach focuses on leveraging the\nbeneficial internal correlations to improve multi-aspect rationale extraction.\nIn this paper, we propose a Multi-Aspect Rationale Extractor (MARE) to explain\nand predict multiple aspects simultaneously. Concretely, we propose a\nMulti-Aspect Multi-Head Attention (MAMHA) mechanism based on hard deletion to\nencode multiple text chunks simultaneously. Furthermore, multiple special\ntokens are prepended in front of the text with each corresponding to one\ncertain aspect. Finally, multi-task training is deployed to reduce the training\noverhead. Experimental results on two unsupervised rationale extraction\nbenchmarks show that MARE achieves state-of-the-art performance. Ablation\nstudies further demonstrate the effectiveness of our method. Our codes have\nbeen available at https://github.com/CSU-NLP-Group/MARE.", "published": "2024-10-04 15:52:29", "link": "http://arxiv.org/abs/2410.03531v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Structure-Enhanced Protein Instruction Tuning: Towards General-Purpose\n  Protein Understanding", "abstract": "Proteins, as essential biomolecules, play a central role in biological\nprocesses, including metabolic reactions and DNA replication. Accurate\nprediction of their properties and functions is crucial in biological\napplications. Recent development of protein language models (pLMs) with\nsupervised fine tuning provides a promising solution to this problem. However,\nthe fine-tuned model is tailored for particular downstream prediction task, and\nachieving general-purpose protein understanding remains a challenge. In this\npaper, we introduce Structure-Enhanced Protein Instruction Tuning (SEPIT)\nframework to bridge this gap. Our approach integrates a noval structure-aware\nmodule into pLMs to inform them with structural knowledge, and then connects\nthese enhanced pLMs to large language models (LLMs) to generate understanding\nof proteins. In this framework, we propose a novel two-stage instruction tuning\npipeline that first establishes a basic understanding of proteins through\ncaption-based instructions and then refines this understanding using a mixture\nof experts (MoEs) to learn more complex properties and functional information\nwith the same amount of activated parameters. Moreover, we construct the\nlargest and most comprehensive protein instruction dataset to date, which\nallows us to train and evaluate the general-purpose protein understanding\nmodel. Extensive experimental results on open-ended generation and closed-set\nanswer tasks demonstrate the superior performance of SEPIT over both\nclosed-source general LLMs and open-source LLMs trained with protein knowledge.", "published": "2024-10-04 16:02:50", "link": "http://arxiv.org/abs/2410.03553v2", "categories": ["cs.CL", "q-bio.BM"], "primary_category": "cs.CL"}
{"title": "Towards Linguistically-Aware and Language-Independent Tokenization for\n  Large Language Models (LLMs)", "abstract": "This paper presents a comprehensive study on the tokenization techniques\nemployed by state-of-the-art large language models (LLMs) and their\nimplications on the cost and availability of services across different\nlanguages, especially low resource languages. The analysis considers multiple\nLLMs, including GPT-4 (using cl100k_base embeddings), GPT-3 (with p50k_base\nembeddings), and DaVinci (employing r50k_base embeddings), as well as the\nwidely used BERT base tokenizer. The study evaluates the tokenization\nvariability observed across these models and investigates the challenges of\nlinguistic representation in subword tokenization. The research underscores the\nimportance of fostering linguistically-aware development practices, especially\nfor languages that are traditionally under-resourced. Moreover, this paper\nintroduces case studies that highlight the real-world implications of\ntokenization choices, particularly in the context of electronic health record\n(EHR) systems. This research aims to promote generalizable Internationalization\n(I18N) practices in the development of AI services in this domain and beyond,\nwith a strong emphasis on inclusivity, particularly for languages traditionally\nunderrepresented in AI applications.", "published": "2024-10-04 16:18:29", "link": "http://arxiv.org/abs/2410.03568v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RAFT: Realistic Attacks to Fool Text Detectors", "abstract": "Large language models (LLMs) have exhibited remarkable fluency across various\ntasks. However, their unethical applications, such as disseminating\ndisinformation, have become a growing concern. Although recent works have\nproposed a number of LLM detection methods, their robustness and reliability\nremain unclear. In this paper, we present RAFT: a grammar error-free black-box\nattack against existing LLM detectors. In contrast to previous attacks for\nlanguage models, our method exploits the transferability of LLM embeddings at\nthe word-level while preserving the original text quality. We leverage an\nauxiliary embedding to greedily select candidate words to perturb against the\ntarget detector. Experiments reveal that our attack effectively compromises all\ndetectors in the study across various domains by up to 99%, and are\ntransferable across source models. Manual human evaluation studies show our\nattacks are realistic and indistinguishable from original human-written text.\nWe also show that examples generated by RAFT can be used to train adversarially\nrobust detectors. Our work shows that current LLM detectors are not\nadversarially robust, underscoring the urgent need for more resilient detection\nmechanisms.", "published": "2024-10-04 17:59:00", "link": "http://arxiv.org/abs/2410.03658v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unraveling Cross-Modality Knowledge Conflicts in Large Vision-Language\n  Models", "abstract": "Large Vision-Language Models (LVLMs) have demonstrated impressive\ncapabilities for capturing and reasoning over multimodal inputs. However, these\nmodels are prone to parametric knowledge conflicts, which arise from\ninconsistencies of represented knowledge between their vision and language\ncomponents. In this paper, we formally define the problem of\n$\\textbf{cross-modality parametric knowledge conflict}$ and present a\nsystematic approach to detect, interpret, and mitigate them. We introduce a\npipeline that identifies conflicts between visual and textual answers, showing\na persistently high conflict rate across modalities in recent LVLMs regardless\nof the model size. We further investigate how these conflicts interfere with\nthe inference process and propose a contrastive metric to discern the\nconflicting samples from the others. Building on these insights, we develop a\nnovel dynamic contrastive decoding method that removes undesirable logits\ninferred from the less confident modality components based on answer\nconfidence. For models that do not provide logits, we also introduce two\nprompt-based strategies to mitigate the conflicts. Our methods achieve\npromising improvements in accuracy on both the ViQuAE and InfoSeek datasets.\nSpecifically, using LLaVA-34B, our proposed dynamic contrastive decoding\nimproves an average accuracy of 2.24%.", "published": "2024-10-04 17:59:28", "link": "http://arxiv.org/abs/2410.03659v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Learning from Committee: Reasoning Distillation from a Mixture of\n  Teachers with Peer-Review", "abstract": "While reasoning capabilities typically emerge in large language models (LLMs)\nwith tens of billions of parameters, recent research focuses on improving\nsmaller open-source models through knowledge distillation (KD) from commercial\nLLMs. However, many of these studies rely solely on responses from a single LLM\nas the gold rationale, unlike the natural human learning process, which\ninvolves understanding both the correct answers and the reasons behind\nmistakes. In this paper, we introduce a novel Fault-Aware DistIllation via\nPeer-Review (FAIR) approach: 1) Instead of merely obtaining rationales from\nteachers, our method asks teachers to identify and explain the student's\nmistakes, providing customized instruction learning data. 2) We design a\nsimulated peer-review process between teacher LLMs, which selects only the\ngenerated rationales above the acceptance threshold. This reduces the chance of\nteachers guessing correctly with flawed rationale, improving instructional data\nquality. Comprehensive experiments and analysis on mathematical, commonsense,\nand logical reasoning tasks demonstrate the effectiveness of our method.", "published": "2024-10-04 17:59:41", "link": "http://arxiv.org/abs/2410.03663v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Metadata Matters for Time Series: Informative Forecasting with\n  Transformers", "abstract": "Time series forecasting is prevalent in extensive real-world applications,\nsuch as financial analysis and energy planning. Previous studies primarily\nfocus on time series modality, endeavoring to capture the intricate variations\nand dependencies inherent in time series. Beyond numerical time series data, we\nnotice that metadata (e.g.~dataset and variate descriptions) also carries\nvaluable information essential for forecasting, which can be used to identify\nthe application scenario and provide more interpretable knowledge than digit\nsequences. Inspired by this observation, we propose a Metadata-informed Time\nSeries Transformer (MetaTST), which incorporates multiple levels of\ncontext-specific metadata into Transformer forecasting models to enable\ninformative time series forecasting. To tackle the unstructured nature of\nmetadata, MetaTST formalizes them into natural languages by pre-designed\ntemplates and leverages large language models (LLMs) to encode these texts into\nmetadata tokens as a supplement to classic series tokens, resulting in an\ninformative embedding. Further, a Transformer encoder is employed to\ncommunicate series and metadata tokens, which can extend series representations\nby metadata information for more accurate forecasting. This design also allows\nthe model to adaptively learn context-specific patterns across various\nscenarios, which is particularly effective in handling large-scale,\ndiverse-scenario forecasting tasks. Experimentally, MetaTST achieves\nstate-of-the-art compared to advanced time series models and LLM-based methods\non widely acknowledged short- and long-term forecasting benchmarks, covering\nboth single-dataset individual and multi-dataset joint training settings.", "published": "2024-10-04 11:37:55", "link": "http://arxiv.org/abs/2410.03806v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD", "abstract": "Open-source Electronic Design Automation (EDA) tools are rapidly transforming\nchip design by addressing key barriers of commercial EDA tools such as\ncomplexity, costs, and access. Recent advancements in Large Language Models\n(LLMs) have further enhanced efficiency in chip design by providing user\nassistance across a range of tasks like setup, decision-making, and flow\nautomation. This paper introduces ORAssistant, a conversational assistant for\nOpenROAD, based on Retrieval-Augmented Generation (RAG). ORAssistant aims to\nimprove the user experience for the OpenROAD flow, from RTL-GDSII by providing\ncontext-specific responses to common user queries, including installation,\ncommand usage, flow setup, and execution, in prose format. Currently,\nORAssistant integrates OpenROAD, OpenROAD-flow-scripts, Yosys, OpenSTA, and\nKLayout. The data model is built from publicly available documentation and\nGitHub resources. The proposed architecture is scalable, supporting extensions\nto other open-source tools, operating modes, and LLM models. We use Google\nGemini as the base LLM model to build and test ORAssistant. Early evaluation\nresults of the RAG-based model show notable improvements in performance and\naccuracy compared to non-fine-tuned LLMs.", "published": "2024-10-04 18:22:58", "link": "http://arxiv.org/abs/2410.03845v2", "categories": ["cs.CL", "cs.AR"], "primary_category": "cs.CL"}
{"title": "Using Prompts to Guide Large Language Models in Imitating a Real\n  Person's Language Style", "abstract": "Large language models (LLMs), such as GPT series and Llama series have\ndemonstrated strong capabilities in natural language processing, contextual\nunderstanding, and text generation. In recent years, researchers are trying to\nenhance the abilities of LLMs in performing various tasks, and numerous studies\nhave proved that well-designed prompts can significantly improve the\nperformance of LLMs on these tasks. This study compares the language style\nimitation ability of three different large language models under the guidance\nof the same zero-shot prompt. It also involves comparing the imitation ability\nof the same large language model when guided by three different prompts\nindividually. Additionally, by applying a Tree-of-Thoughts (ToT) Prompting\nmethod to Llama 3, a conversational AI with the language style of a real person\nwas created. In this study, three evaluation methods were used to evaluate LLMs\nand prompts. The results show that Llama 3 performs best at imitating language\nstyles, and that the ToT prompting method is the most effective to guide it in\nimitating language styles. Using a ToT framework, Llama 3 was guided to\ninteract with users in the language style of a specific individual without\naltering its core parameters, thereby creating a text-based conversational AI\nthat reflects the language style of the individual.", "published": "2024-10-04 18:30:34", "link": "http://arxiv.org/abs/2410.03848v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Detecting Machine-Generated Long-Form Content with Latent-Space\n  Variables", "abstract": "The increasing capability of large language models (LLMs) to generate fluent\nlong-form texts is presenting new challenges in distinguishing\nmachine-generated outputs from human-written ones, which is crucial for\nensuring authenticity and trustworthiness of expressions. Existing zero-shot\ndetectors primarily focus on token-level distributions, which are vulnerable to\nreal-world domain shifts, including different prompting and decoding\nstrategies, and adversarial attacks. We propose a more robust method that\nincorporates abstract elements, such as event transitions, as key deciding\nfactors to detect machine versus human texts by training a latent-space model\non sequences of events or topics derived from human-written texts. In three\ndifferent domains, machine-generated texts, which are originally inseparable\nfrom human texts on the token level, can be better distinguished with our\nlatent-space model, leading to a 31% improvement over strong baselines such as\nDetectGPT. Our analysis further reveals that, unlike humans, modern LLMs like\nGPT-4 generate event triggers and their transitions differently, an inherent\ndisparity that helps our method to robustly detect machine-generated texts.", "published": "2024-10-04 18:42:09", "link": "http://arxiv.org/abs/2410.03856v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Still Not Quite There! Evaluating Large Language Models for Comorbid\n  Mental Health Diagnosis", "abstract": "In this study, we introduce ANGST, a novel, first-of-its kind benchmark for\ndepression-anxiety comorbidity classification from social media posts. Unlike\ncontemporary datasets that often oversimplify the intricate interplay between\ndifferent mental health disorders by treating them as isolated conditions,\nANGST enables multi-label classification, allowing each post to be\nsimultaneously identified as indicating depression and/or anxiety. Comprising\n2876 meticulously annotated posts by expert psychologists and an additional\n7667 silver-labeled posts, ANGST posits a more representative sample of online\nmental health discourse. Moreover, we benchmark ANGST using various\nstate-of-the-art language models, ranging from Mental-BERT to GPT-4. Our\nresults provide significant insights into the capabilities and limitations of\nthese models in complex diagnostic scenarios. While GPT-4 generally outperforms\nother models, none achieve an F1 score exceeding 72% in multi-class comorbid\nclassification, underscoring the ongoing challenges in applying language models\nto mental health diagnostics.", "published": "2024-10-04 20:24:11", "link": "http://arxiv.org/abs/2410.03908v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "C3PA: An Open Dataset of Expert-Annotated and Regulation-Aware Privacy\n  Policies to Enable Scalable Regulatory Compliance Audits", "abstract": "The development of tools and techniques to analyze and extract organizations\ndata habits from privacy policies are critical for scalable regulatory\ncompliance audits. Unfortunately, these tools are becoming increasingly limited\nin their ability to identify compliance issues and fixes. After all, most were\ndeveloped using regulation-agnostic datasets of annotated privacy policies\nobtained from a time before the introduction of landmark privacy regulations\nsuch as EUs GDPR and Californias CCPA. In this paper, we describe the first\nopen regulation-aware dataset of expert-annotated privacy policies, C3PA (CCPA\nPrivacy Policy Provision Annotations), aimed to address this challenge. C3PA\ncontains over 48K expert-labeled privacy policy text segments associated with\nresponses to CCPA-specific disclosure mandates from 411 unique organizations.\nWe demonstrate that the C3PA dataset is uniquely suited for aiding automated\naudits of compliance with CCPA-related disclosure mandates.", "published": "2024-10-04 21:04:39", "link": "http://arxiv.org/abs/2410.03925v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "LLM-TOPLA: Efficient LLM Ensemble by Maximising Diversity", "abstract": "Combining large language models during training or at inference time has\nshown substantial performance gain over component LLMs. This paper presents\nLLM-TOPLA, a diversity-optimized LLM ensemble method with three unique\nproperties: (i) We introduce the focal diversity metric to capture the\ndiversity-performance correlation among component LLMs of an ensemble. (ii) We\ndevelop a diversity-optimized ensemble pruning algorithm to select the top-k\nsub-ensembles from a pool of $N$ base LLMs. Our pruning method recommends\ntop-performing LLM subensembles of size $S$, often much smaller than $N$. (iii)\nWe generate new output for each prompt query by utilizing a learn-to-ensemble\napproach, which learns to detect and resolve the output inconsistency among all\ncomponent LLMs of an ensemble. Extensive evaluation on four different\nbenchmarks shows good performance gain over the best LLM ensemble methods: (i)\nIn constrained solution set problems, LLM-TOPLA outperforms the best-performing\nensemble (Mixtral) by 2.2\\% in accuracy on MMLU and the best-performing LLM\nensemble (MoreAgent) on GSM8k by 2.1\\%. (ii) In generative tasks, LLM-TOPLA\noutperforms the top-2 performers (Llama70b/Mixtral) on SearchQA by\n$3.9\\mathrm{x}$ in F1, and on XSum by more than $38$ in ROUGE-1. Our code and\ndataset, which contains outputs of 8 modern LLMs on 4 benchmarks is available\nat https://github.com/git-disl/llm-topla", "published": "2024-10-04 22:31:15", "link": "http://arxiv.org/abs/2410.03953v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Arabic Multi-Label Emotion Classification using Stacked\n  Embeddings and Hybrid Loss Function", "abstract": "In multi-label emotion classification, particularly for low-resource\nlanguages like Arabic, the challenges of class imbalance and label correlation\nhinder model performance, especially in accurately predicting minority\nemotions. To address these issues, this study proposes a novel approach that\ncombines stacked embeddings, meta-learning, and a hybrid loss function to\nenhance multi-label emotion classification for the Arabic language. The study\nextracts contextual embeddings from three fine-tuned language\nmodels-ArabicBERT, MarBERT, and AraBERT-which are then stacked to form enriched\nembeddings. A meta-learner is trained on these stacked embeddings, and the\nresulting concatenated representations are provided as input to a Bi-LSTM\nmodel, followed by a fully connected neural network for multi-label\nclassification. To further improve performance, a hybrid loss function is\nintroduced, incorporating class weighting, label correlation matrix, and\ncontrastive learning, effectively addressing class imbalances and improving the\nhandling of label correlations. Extensive experiments validate the proposed\nmodel's performance across key metrics such as Precision, Recall, F1-Score,\nJaccard Accuracy, and Hamming Loss. The class-wise performance analysis\ndemonstrates the hybrid loss function's ability to significantly reduce\ndisparities between majority and minority classes, resulting in a more balanced\nemotion classification. An ablation study highlights the contribution of each\ncomponent, showing the superiority of the model compared to baseline approaches\nand other loss functions. This study not only advances multi-label emotion\nclassification for Arabic but also presents a generalizable framework that can\nbe adapted to other languages and domains, providing a significant step forward\nin addressing the challenges of low-resource emotion classification tasks.", "published": "2024-10-04 23:37:21", "link": "http://arxiv.org/abs/2410.03979v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Output Scouting: Auditing Large Language Models for Catastrophic\n  Responses", "abstract": "Recent high profile incidents in which the use of Large Language Models\n(LLMs) resulted in significant harm to individuals have brought about a growing\ninterest in AI safety. One reason LLM safety issues occur is that models often\nhave at least some non-zero probability of producing harmful outputs. In this\nwork, we explore the following scenario: imagine an AI safety auditor is\nsearching for catastrophic responses from an LLM (e.g. a \"yes\" responses to\n\"can I fire an employee for being pregnant?\"), and is able to query the model a\nlimited number times (e.g. 1000 times). What is a strategy for querying the\nmodel that would efficiently find those failure responses? To this end, we\npropose output scouting: an approach that aims to generate semantically fluent\noutputs to a given prompt matching any target probability distribution. We then\nrun experiments using two LLMs and find numerous examples of catastrophic\nresponses. We conclude with a discussion that includes advice for practitioners\nwho are looking to implement LLM auditing for catastrophic responses. We also\nrelease an open-source toolkit (https://github.com/joaopfonseca/outputscouting)\nthat implements our auditing framework using the Hugging Face transformers\nlibrary.", "published": "2024-10-04 18:18:53", "link": "http://arxiv.org/abs/2410.05305v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Large Language Model-based Framework for Semi-Structured Tender\n  Document Retrieval-Augmented Generation", "abstract": "The drafting of documents in the procurement field has progressively become\nmore complex and diverse, driven by the need to meet legal requirements, adapt\nto technological advancements, and address stakeholder demands. While large\nlanguage models (LLMs) show potential in document generation, most LLMs lack\nspecialized knowledge in procurement. To address this gap, we use\nretrieval-augmented techniques to achieve professional document generation,\nensuring accuracy and relevance in procurement documentation.", "published": "2024-10-04 16:46:07", "link": "http://arxiv.org/abs/2410.09077v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Crafting Narrative Closures: Zero-Shot Learning with SSM Mamba for Short\n  Story Ending Generation", "abstract": "Writing stories is an engaging yet challenging endeavor. Often, authors\nencounter moments of creative block, where the path forward in their narrative\nbecomes obscured. This paper is designed to address such moments by providing\nan innovative solution: A tool that completes stories based on given prompts.\nBy inputting a short story prompt, users can receive a conclusion to their\nstory, articulated in one sentence or more, thereby enhancing the storytelling\nprocess with AI-driven creativity. This tool aims not only to assist authors in\nnavigating writer's block but also to offer a fun and interactive way for\nanyone to expand on story ideas spontaneously. Through this paper, we explore\nthe intersection of artificial intelligence and creative writing, pushing the\nboundaries of how stories can be crafted and concluded. To create our final\ntext-generation models, we used a pre-trained GPT-3.5 model and a newly created\nfinetuned SSM-Mamba model, both of which perform well on a comprehensive list\nof metrics including BERT score, METEOR, BLEU, ROUGE, and Perplexity. The SSM\nmodel has also been made public for the NLP community on HuggingFace models as\nan open source contribution, which for the timebeing is a first of its kind\nstate-space model for story-generation task on HuggingFace.", "published": "2024-10-04 18:56:32", "link": "http://arxiv.org/abs/2410.10848v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Scalable Frame-based Construction of Sociocultural NormBases for\n  Socially-Aware Dialogues", "abstract": "Sociocultural norms serve as guiding principles for personal conduct in\nsocial interactions, emphasizing respect, cooperation, and appropriate\nbehavior, which is able to benefit tasks including conversational information\nretrieval, contextual information retrieval and retrieval-enhanced machine\nlearning. We propose a scalable approach for constructing a Sociocultural Norm\n(SCN) Base using Large Language Models (LLMs) for socially aware dialogues. We\nconstruct a comprehensive and publicly accessible Chinese Sociocultural\nNormBase. Our approach utilizes socially aware dialogues, enriched with\ncontextual frames, as the primary data source to constrain the generating\nprocess and reduce the hallucinations. This enables extracting of high-quality\nand nuanced natural-language norm statements, leveraging the pragmatic\nimplications of utterances with respect to the situation. As real dialogue\nannotated with gold frames are not readily available, we propose using\nsynthetic data. Our empirical results show: (i) the quality of the SCNs derived\nfrom synthetic data is comparable to that from real dialogues annotated with\ngold frames, and (ii) the quality of the SCNs extracted from real data,\nannotated with either silver (predicted) or gold frames, surpasses that without\nthe frame annotations. We further show the effectiveness of the extracted SCNs\nin a RAG-based (Retrieval-Augmented Generation) model to reason about multiple\ndownstream dialogue tasks.", "published": "2024-10-04 00:08:46", "link": "http://arxiv.org/abs/2410.03049v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Horizon-Length Prediction: Advancing Fill-in-the-Middle Capabilities for\n  Code Generation with Lookahead Planning", "abstract": "Fill-in-the-Middle (FIM) has become integral to code language models,\nenabling generation of missing code given both left and right contexts.\nHowever, the current FIM training paradigm, which reorders original training\nsequences and then performs regular next-token prediction (NTP), often leads to\nmodels struggling to generate content that aligns smoothly with the surrounding\ncontext. Crucially, while existing works rely on rule-based post-processing to\ncircumvent this weakness, such methods are not practically usable in\nopen-domain code completion tasks as they depend on restrictive,\ndataset-specific assumptions (e.g., generating the same number of lines as in\nthe ground truth). Moreover, model performance on FIM tasks deteriorates\nsignificantly without these unrealistic assumptions.\n  We hypothesize that NTP alone is insufficient for models to learn effective\nplanning conditioned on the distant right context, a critical factor for\nsuccessful code infilling. To overcome this, we propose Horizon-Length\nPrediction (HLP), a novel training objective that teaches models to predict the\nnumber of remaining middle tokens (i.e., horizon length) at each step. HLP\nadvances FIM with lookahead planning, enabling models to inherently learn\ninfilling boundaries for arbitrary left and right contexts without relying on\ndataset-specific post-processing. Our evaluation across different models and\nsizes shows that HLP significantly improves FIM performance by up to 24%\nrelatively on diverse benchmarks, across file-level and repository-level, and\nwithout resorting to unrealistic post-processing methods. Furthermore, the\nenhanced planning capability gained through HLP boosts model performance on\ncode reasoning. Importantly, HLP only incurs negligible training overhead and\nno additional inference cost, ensuring its practicality for real-world\nscenarios.", "published": "2024-10-04 02:53:52", "link": "http://arxiv.org/abs/2410.03103v2", "categories": ["cs.LG", "cs.CL", "cs.SE"], "primary_category": "cs.LG"}
{"title": "Mamba in Vision: A Comprehensive Survey of Techniques and Applications", "abstract": "Mamba is emerging as a novel approach to overcome the challenges faced by\nConvolutional Neural Networks (CNNs) and Vision Transformers (ViTs) in computer\nvision. While CNNs excel at extracting local features, they often struggle to\ncapture long-range dependencies without complex architectural modifications. In\ncontrast, ViTs effectively model global relationships but suffer from high\ncomputational costs due to the quadratic complexity of their self-attention\nmechanisms. Mamba addresses these limitations by leveraging Selective\nStructured State Space Models to effectively capture long-range dependencies\nwith linear computational complexity. This survey analyzes the unique\ncontributions, computational benefits, and applications of Mamba models while\nalso identifying challenges and potential future research directions. We\nprovide a foundational resource for advancing the understanding and growth of\nMamba models in computer vision. An overview of this work is available at\nhttps://github.com/maklachur/Mamba-in-Computer-Vision.", "published": "2024-10-04 02:58:49", "link": "http://arxiv.org/abs/2410.03105v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "LoRC: Low-Rank Compression for LLMs KV Cache with a Progressive\n  Compression Strategy", "abstract": "The Key-Value (KV) cache is a crucial component in serving transformer-based\nautoregressive large language models (LLMs), enabling faster inference by\nstoring previously computed KV vectors. However, its memory consumption scales\nlinearly with sequence length and batch size, posing a significant bottleneck\nin LLM deployment. Existing approaches to mitigate this issue include: (1)\nefficient attention variants integrated in upcycling stages, which requires\nextensive parameter tuning thus unsuitable for pre-trained LLMs; (2) KV cache\ncompression at test time, primarily through token eviction policies, which\noften overlook inter-layer dependencies and can be task-specific.\n  This paper introduces an orthogonal approach to KV cache compression. We\npropose a low-rank approximation of KV weight matrices, allowing for plug-in\nintegration with existing transformer-based LLMs without model retraining. To\neffectively compress KV cache at the weight level, we adjust for layerwise\nsensitivity and introduce a progressive compression strategy, which is\nsupported by our theoretical analysis on how compression errors accumulate in\ndeep networks. Our method is designed to function without model tuning in\nupcycling stages or task-specific profiling in test stages. Extensive\nexperiments with LLaMA models ranging from 8B to 70B parameters across various\ntasks show that our approach significantly reduces the GPU memory footprint\nwhile maintaining performance.", "published": "2024-10-04 03:10:53", "link": "http://arxiv.org/abs/2410.03111v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2"], "primary_category": "cs.LG"}
{"title": "ProcBench: Benchmark for Multi-Step Reasoning and Following Procedure", "abstract": "Reasoning is central to a wide range of intellectual activities, and while\nthe capabilities of large language models (LLMs) continue to advance, their\nperformance in reasoning tasks remains limited. The processes and mechanisms\nunderlying reasoning are not yet fully understood, but key elements include\npath exploration, selection of relevant knowledge, and multi-step inference.\nProblems are solved through the synthesis of these components. In this paper,\nwe propose a benchmark that focuses on a specific aspect of reasoning ability:\nthe direct evaluation of multi-step inference. To this end, we design a special\nreasoning task where multi-step inference is specifically focused by largely\neliminating path exploration and implicit knowledge utilization. Our dataset\ncomprises pairs of explicit instructions and corresponding questions, where the\nprocedures necessary for solving the questions are entirely detailed within the\ninstructions. This setup allows models to solve problems solely by following\nthe provided directives. By constructing problems that require varying numbers\nof steps to solve and evaluating responses at each step, we enable a thorough\nassessment of state-of-the-art LLMs' ability to follow instructions. To ensure\nthe robustness of our evaluation, we include multiple distinct tasks.\nFurthermore, by comparing accuracy across tasks, utilizing step-aware metrics,\nand applying separately defined measures of complexity, we conduct experiments\nthat offer insights into the capabilities and limitations of LLMs in reasoning\ntasks. Our findings have significant implications for the development of LLMs\nand highlight areas for future research in advancing their reasoning abilities.\nOur dataset is available at\n\\url{https://huggingface.co/datasets/ifujisawa/procbench} and code at\n\\url{https://github.com/ifujisawa/proc-bench}.", "published": "2024-10-04 03:21:24", "link": "http://arxiv.org/abs/2410.03117v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "RIPPLECOT: Amplifying Ripple Effect of Knowledge Editing in Language\n  Models via Chain-of-Thought In-Context Learning", "abstract": "The ripple effect poses a significant challenge in knowledge editing for\nlarge language models. Namely, when a single fact is edited, the model\nstruggles to accurately update the related facts in a sequence, which is\nevaluated by multi-hop questions linked to a chain of related facts. Recent\nstrategies have moved away from traditional parameter updates to more flexible,\nless computation-intensive methods, proven to be more effective in addressing\nthe ripple effect. In-context learning (ICL) editing uses a simple\ndemonstration `Imagine that + new fact` to guide LLMs, but struggles with\ncomplex multi-hop questions as the new fact alone fails to specify the chain of\nfacts involved in such scenarios. Besides, memory-based editing maintains\nadditional storage for all edits and related facts, requiring continuous\nupdates to stay effective. As a result of these design limitations, the\nchallenge remains, with the highest accuracy being only 33.8% on the MQuAKE-cf\nbenchmarks for Vicuna-7B. To address this, we propose RippleCOT, a novel ICL\nediting approach integrating Chain-of-Thought (COT) reasoning. RippleCOT\nstructures demonstrations as `newfact, question, thought, answer`,\nincorporating a thought component to identify and decompose the multi-hop logic\nwithin questions. This approach effectively guides the model through complex\nmulti-hop questions with chains of related facts. Comprehensive experiments\ndemonstrate that RippleCOT significantly outperforms the state-of-the-art on\nthe ripple effect, achieving accuracy gains ranging from 7.8% to 87.1%.", "published": "2024-10-04 03:37:36", "link": "http://arxiv.org/abs/2410.03122v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ARB-LLM: Alternating Refined Binarizations for Large Language Models", "abstract": "Large Language Models (LLMs) have greatly pushed forward advancements in\nnatural language processing, yet their high memory and computational demands\nhinder practical deployment. Binarization, as an effective compression\ntechnique, can shrink model weights to just 1 bit, significantly reducing the\nhigh demands on computation and memory. However, current binarization methods\nstruggle to narrow the distribution gap between binarized and full-precision\nweights, while also overlooking the column deviation in LLM weight\ndistribution. To tackle these issues, we propose ARB-LLM, a novel 1-bit\npost-training quantization (PTQ) technique tailored for LLMs. To narrow the\ndistribution shift between binarized and full-precision weights, we first\ndesign an alternating refined binarization (ARB) algorithm to progressively\nupdate the binarization parameters, which significantly reduces the\nquantization error. Moreover, considering the pivot role of calibration data\nand the column deviation in LLM weights, we further extend ARB to ARB-X and\nARB-RC. In addition, we refine the weight partition strategy with column-group\nbitmap (CGB), which further enhance performance. Equipping ARB-X and ARB-RC\nwith CGB, we obtain ARB-LLM$_\\text{X}$ and ARB-LLM$_\\text{RC}$ respectively,\nwhich significantly outperform state-of-the-art (SOTA) binarization methods for\nLLMs. As a binary PTQ method, our ARB-LLM$_\\text{RC}$ is the first to surpass\nFP16 models of the same size. The code and models will be available at\nhttps://github.com/ZHITENGLI/ARB-LLM.", "published": "2024-10-04 03:50:10", "link": "http://arxiv.org/abs/2410.03129v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "AIME: AI System Optimization via Multiple LLM Evaluators", "abstract": "Text-based AI system optimization typically involves a feedback loop scheme\nwhere a single LLM generates an evaluation in natural language of the current\noutput to improve the next iteration's output. However, in this work, we\nempirically demonstrate that for a practical and complex task (code generation)\nwith multiple criteria to evaluate, utilizing only one LLM evaluator tends to\nlet errors in generated code go undetected, thus leading to incorrect\nevaluations and ultimately suboptimal test case performance. Motivated by this\nfailure case, we assume there exists an optimal evaluation policy that samples\nan evaluation between response and ground truth. We then theoretically prove\nthat a linear combination of multiple evaluators can approximate this optimal\npolicy. From this insight, we propose AI system optimization via Multiple LLM\nEvaluators (AIME). AIME is an evaluation protocol that utilizes multiple LLMs\nthat each independently generate an evaluation on separate criteria and then\ncombine them via concatenation. We provide an extensive empirical study showing\nAIME outperforming baseline methods in code generation tasks, with up to $62\\%$\nhigher error detection rate and up to $16\\%$ higher success rate than a single\nLLM evaluation protocol on LeetCodeHard and HumanEval datasets. We also show\nthat the selection of the number of evaluators and which criteria to utilize is\nnon-trivial as it can impact pact success rate by up to $12\\%$.", "published": "2024-10-04 04:03:24", "link": "http://arxiv.org/abs/2410.03131v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Analysis and Detection of Differences in Spoken User Behaviors between\n  Autonomous and Wizard-of-Oz Systems", "abstract": "This study examined users' behavioral differences in a large corpus of\nJapanese human-robot interactions, comparing interactions between a\ntele-operated robot and an autonomous dialogue system. We analyzed user spoken\nbehaviors in both attentive listening and job interview dialogue scenarios.\nResults revealed significant differences in metrics such as speech length,\nspeaking rate, fillers, backchannels, disfluencies, and laughter between\noperator-controlled and autonomous conditions. Furthermore, we developed\npredictive models to distinguish between operator and autonomous system\nconditions. Our models demonstrated higher accuracy and precision compared to\nthe baseline model, with several models also achieving a higher F1 score than\nthe baseline.", "published": "2024-10-04 05:07:55", "link": "http://arxiv.org/abs/2410.03147v1", "categories": ["cs.CL", "cs.HC", "cs.RO"], "primary_category": "cs.CL"}
{"title": "How Much Can We Forget about Data Contamination?", "abstract": "The leakage of benchmark data into the training data has emerged as a\nsignificant challenge for evaluating the capabilities of large language models\n(LLMs). In this work, we challenge the common assumption that small-scale\ncontamination renders benchmark evaluations invalid. First, we experimentally\nquantify the magnitude of benchmark overfitting based on scaling along three\ndimensions: The number of model parameters (up to 1.6B), the number of times an\nexample is seen (up to 144), and the number of training tokens (up to 40B). If\nmodel and data follow the Chinchilla scaling laws, minor contamination indeed\nleads to overfitting. At the same time, even 144 times of contamination can be\nforgotten if the training data is scaled beyond five times Chinchilla, a regime\ncharacteristic of many modern LLMs. Continual pre-training of OLMo-7B\ncorroborates these results. Next, we study the impact of the weight decay\nparameter on example forgetting, showing that empirical forgetting occurs\nfaster than the cumulative weight decay. This allows us to gauge the degree of\nexample forgetting in large-scale training runs, indicating that many LLMs,\nincluding Lllama 3 405B, have forgotten the data seen at the beginning of\ntraining.", "published": "2024-10-04 09:14:11", "link": "http://arxiv.org/abs/2410.03249v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Generating Equivalent Representations of Code By A Self-Reflection\n  Approach", "abstract": "Equivalent Representations (ERs) of code are textual representations that\npreserve the same semantics as the code itself, e.g., natural language comments\nand pseudocode. ERs play a critical role in software development and\nmaintenance. However, how to automatically generate ERs of code remains an open\nchallenge. In this paper, we propose a self-reflection approach to generating\nERs of code. It enables two Large Language Models (LLMs) to work mutually and\nproduce an ER through a reflection process. Depending on whether constraints on\nERs are applied, our approach generates ERs in both open and constrained\nsettings. We conduct a empirical study to generate ERs in two settings and\nobtain eight findings. (1) Generating ERs in the open setting. In the open\nsetting, we allow LLMs to represent code without any constraints, analyzing the\nresulting ERs and uncovering five key findings. These findings shed light on\nhow LLMs comprehend syntactic structures, APIs, and numerical computations in\ncode. (2) Generating ERs in the constrained setting. In the constrained\nsetting, we impose constraints on ERs, such as natural language comments,\npseudocode, and flowcharts. This allows our approach to address a range of\nsoftware engineering tasks. Based on our experiments, we have three findings\ndemonstrating that our approach can effectively generate ERs that adhere to\nspecific constraints, thus supporting various software engineering tasks. (3)\nFuture directions. We also discuss potential future research directions, such\nas deriving intermediate languages for code generation, exploring LLM-friendly\nrequirement descriptions, and further supporting software engineering tasks. We\nbelieve that this paper will spark discussions in research communities and\ninspire many follow-up studies.", "published": "2024-10-04 12:17:08", "link": "http://arxiv.org/abs/2410.03351v1", "categories": ["cs.CL", "cs.PL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "A General Framework for Producing Interpretable Semantic Text Embeddings", "abstract": "Semantic text embedding is essential to many tasks in Natural Language\nProcessing (NLP). While black-box models are capable of generating high-quality\nembeddings, their lack of interpretability limits their use in tasks that\ndemand transparency. Recent approaches have improved interpretability by\nleveraging domain-expert-crafted or LLM-generated questions, but these methods\nrely heavily on expert input or well-prompt design, which restricts their\ngeneralizability and ability to generate discriminative questions across a wide\nrange of tasks. To address these challenges, we introduce \\algo{CQG-MBQA}\n(Contrastive Question Generation - Multi-task Binary Question Answering), a\ngeneral framework for producing interpretable semantic text embeddings across\ndiverse tasks. Our framework systematically generates highly discriminative,\nlow cognitive load yes/no questions through the \\algo{CQG} method and answers\nthem efficiently with the \\algo{MBQA} model, resulting in interpretable\nembeddings in a cost-effective manner. We validate the effectiveness and\ninterpretability of \\algo{CQG-MBQA} through extensive experiments and ablation\nstudies, demonstrating that it delivers embedding quality comparable to many\nadvanced black-box models while maintaining inherently interpretability.\nAdditionally, \\algo{CQG-MBQA} outperforms other interpretable text embedding\nmethods across various downstream tasks.", "published": "2024-10-04 13:51:19", "link": "http://arxiv.org/abs/2410.03435v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Uncertainty In Natural Language Processing", "abstract": "The last decade in deep learning has brought on increasingly capable systems\nthat are deployed on a wide variety of applications. In natural language\nprocessing, the field has been transformed by a number of breakthroughs\nincluding large language models, which are used in increasingly many\nuser-facing applications. In order to reap the benefits of this technology and\nreduce potential harms, it is important to quantify the reliability of model\npredictions and the uncertainties that shroud their development.\n  This thesis studies how uncertainty in natural language processing can be\ncharacterized from a linguistic, statistical and neural perspective, and how it\ncan be reduced and quantified through the design of the experimental pipeline.\nWe further explore uncertainty quantification in modeling by theoretically and\nempirically investigating the effect of inductive model biases in text\nclassification tasks. The corresponding experiments include data for three\ndifferent languages (Danish, English and Finnish) and tasks as well as a large\nset of different uncertainty quantification approaches. Additionally, we\npropose a method for calibrated sampling in natural language generation based\non non-exchangeable conformal prediction, which provides tighter token sets\nwith better coverage of the actual continuation. Lastly, we develop an approach\nto quantify confidence in large black-box language models using auxiliary\npredictors, where the confidence is predicted from the input to and generated\noutput text of the target model alone.", "published": "2024-10-04 14:08:02", "link": "http://arxiv.org/abs/2410.03446v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Understanding Reasoning in Chain-of-Thought from the Hopfieldian View", "abstract": "Large Language Models have demonstrated remarkable abilities across various\ntasks, with Chain-of-Thought (CoT) prompting emerging as a key technique to\nenhance reasoning capabilities. However, existing research primarily focuses on\nimproving performance, lacking a comprehensive framework to explain and\nunderstand the fundamental factors behind CoT's success. To bridge this gap, we\nintroduce a novel perspective grounded in the Hopfieldian view of cognition in\ncognitive neuroscience. We establish a connection between CoT reasoning and key\ncognitive elements such as stimuli, actions, neural populations, and\nrepresentation spaces. From our view, we can understand the reasoning process\nas the movement between these representation spaces. Building on this insight,\nwe develop a method for localizing reasoning errors in the response of CoTs.\nMoreover, we propose the Representation-of-Thought (RoT) framework, which\nleverages the robustness of low-dimensional representation spaces to enhance\nthe robustness of the reasoning process in CoTs. Experimental results\ndemonstrate that RoT improves the robustness and interpretability of CoT\nreasoning while offering fine-grained control over the reasoning process.", "published": "2024-10-04 16:55:30", "link": "http://arxiv.org/abs/2410.03595v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "TICKing All the Boxes: Generated Checklists Improve LLM Evaluation and\n  Generation", "abstract": "Given the widespread adoption and usage of Large Language Models (LLMs), it\nis crucial to have flexible and interpretable evaluations of their\ninstruction-following ability. Preference judgments between model outputs have\nbecome the de facto evaluation standard, despite distilling complex,\nmulti-faceted preferences into a single ranking. Furthermore, as human\nannotation is slow and costly, LLMs are increasingly used to make these\njudgments, at the expense of reliability and interpretability. In this work, we\npropose TICK (Targeted Instruct-evaluation with ChecKlists), a fully automated,\ninterpretable evaluation protocol that structures evaluations with\nLLM-generated, instruction-specific checklists. We first show that, given an\ninstruction, LLMs can reliably produce high-quality, tailored evaluation\nchecklists that decompose the instruction into a series of YES/NO questions.\nEach question asks whether a candidate response meets a specific requirement of\nthe instruction. We demonstrate that using TICK leads to a significant increase\n(46.4% $\\to$ 52.2%) in the frequency of exact agreements between LLM judgements\nand human preferences, as compared to having an LLM directly score an output.\nWe then show that STICK (Self-TICK) can be used to improve generation quality\nacross multiple benchmarks via self-refinement and Best-of-N selection. STICK\nself-refinement on LiveBench reasoning tasks leads to an absolute gain of\n$+$7.8%, whilst Best-of-N selection with STICK attains $+$6.3% absolute\nimprovement on the real-world instruction dataset, WildBench. In light of this,\nstructured, multi-faceted self-improvement is shown to be a promising way to\nfurther advance LLM capabilities. Finally, by providing LLM-generated\nchecklists to human evaluators tasked with directly scoring LLM responses to\nWildBench instructions, we notably increase inter-annotator agreement (0.194\n$\\to$ 0.256).", "published": "2024-10-04 17:09:08", "link": "http://arxiv.org/abs/2410.03608v1", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.AI"}
{"title": "What Matters for Model Merging at Scale?", "abstract": "Model merging aims to combine multiple expert models into a more capable\nsingle model, offering benefits such as reduced storage and serving costs,\nimproved generalization, and support for decentralized model development.\nDespite its promise, previous studies have primarily focused on merging a few\nsmall models. This leaves many unanswered questions about the effect of scaling\nmodel size and how it interplays with other key factors -- like the base model\nquality and number of expert models -- , to affect the merged model's\nperformance. This work systematically evaluates the utility of model merging at\nscale, examining the impact of these different factors. We experiment with\nmerging fully fine-tuned models using 4 popular merging methods -- Averaging,\nTask~Arithmetic, Dare, and TIES -- across model sizes ranging from 1B-64B\nparameters and merging up to 8 different expert models. We evaluate the merged\nmodels on both held-in tasks, i.e., the expert's training tasks, and zero-shot\ngeneralization to unseen held-out tasks. Our experiments provide several new\ninsights about model merging at scale and the interplay between different\nfactors. First, we find that merging is more effective when experts are created\nfrom strong base models, i.e., models with good zero-shot performance. Second,\nlarger models facilitate easier merging. Third merging consistently improves\ngeneralization capabilities. Notably, when merging 8 large expert models, the\nmerged models often generalize better compared to the multitask trained models.\nFourth, we can better merge more expert models when working with larger models.\nFifth, different merging methods behave very similarly at larger scales.\nOverall, our findings shed light on some interesting properties of model\nmerging while also highlighting some limitations. We hope that this study will\nserve as a reference point on large-scale merging for upcoming research.", "published": "2024-10-04 17:17:19", "link": "http://arxiv.org/abs/2410.03617v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Aligning LLMs with Individual Preferences via Interaction", "abstract": "As large language models (LLMs) demonstrate increasingly advanced\ncapabilities, aligning their behaviors with human values and preferences\nbecomes crucial for their wide adoption. While previous research focuses on\ngeneral alignment to principles such as helpfulness, harmlessness, and honesty,\nthe need to account for individual and diverse preferences has been largely\noverlooked, potentially undermining customized human experiences. To address\nthis gap, we train LLMs that can ''interact to align'', essentially cultivating\nthe meta-skill of LLMs to implicitly infer the unspoken personalized\npreferences of the current user through multi-turn conversations, and then\ndynamically align their following behaviors and responses to these inferred\npreferences. Our approach involves establishing a diverse pool of 3,310\ndistinct user personas by initially creating seed examples, which are then\nexpanded through iterative self-generation and filtering. Guided by distinct\nuser personas, we leverage multi-LLM collaboration to develop a multi-turn\npreference dataset containing 3K+ multi-turn conversations in tree structures.\nFinally, we apply supervised fine-tuning and reinforcement learning to enhance\nLLMs using this dataset. For evaluation, we establish the ALOE (ALign With\nCustOmized PrEferences) benchmark, consisting of 100 carefully selected\nexamples and well-designed metrics to measure the customized alignment\nperformance during conversations. Experimental results demonstrate the\neffectiveness of our method in enabling dynamic, personalized alignment via\ninteraction.", "published": "2024-10-04 17:48:29", "link": "http://arxiv.org/abs/2410.03642v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Self-Powered LLM Modality Expansion for Large Speech-Text Models", "abstract": "Large language models (LLMs) exhibit remarkable performance across diverse\ntasks, indicating their potential for expansion into large speech-text models\n(LSMs) by integrating speech capabilities. Although unified speech-text\npre-training and multimodal data instruction-tuning offer considerable\nbenefits, these methods generally entail significant resource demands and tend\nto overfit specific tasks. This study aims to refine the use of speech datasets\nfor LSM training by addressing the limitations of vanilla instruction tuning.\nWe explore the instruction-following dynamics within LSMs, identifying a\ncritical issue termed speech anchor bias-a tendency for LSMs to over-rely on\nspeech inputs, mistakenly interpreting the entire speech modality as\ndirectives, thereby neglecting textual instructions. To counteract this bias,\nwe introduce a self-powered LSM that leverages augmented automatic speech\nrecognition data generated by the model itself for more effective instruction\ntuning. Our experiments across a range of speech-based tasks demonstrate that\nself-powered LSM mitigates speech anchor bias and improves the fusion of speech\nand text modalities in LSMs. Data, code and scripts are freely available at\nhttps://github.com/ytf-philp/Self-powered-LSM.", "published": "2024-10-04 04:34:24", "link": "http://arxiv.org/abs/2410.03798v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Mixture of Attentions For Speculative Decoding", "abstract": "The growth in the number of parameters of Large Language Models (LLMs) has\nled to a significant surge in computational requirements, making them\nchallenging and costly to deploy. Speculative decoding (SD) leverages smaller\nmodels to efficiently propose future tokens, which are then verified by the LLM\nin parallel. Small models that utilise activations from the LLM currently\nachieve the fastest decoding speeds. However, we identify several limitations\nof SD models including the lack of on-policyness during training and partial\nobservability. To address these shortcomings, we propose a more grounded\narchitecture for small models by introducing a Mixture of Attentions for SD.\nOur novel architecture can be applied in two scenarios: a conventional single\ndevice deployment and a novel client-server deployment where the small model is\nhosted on a consumer device and the LLM on a server. In a single-device\nscenario, we demonstrate state-of-the-art speedups improving EAGLE-2 by 9.5%\nand its acceptance length by 25%. In a client-server setting, our experiments\ndemonstrate: 1) state-of-the-art latencies with minimal calls to the server for\ndifferent network conditions, and 2) in the event of a complete disconnection,\nour approach can maintain higher accuracy compared to other SD methods and\ndemonstrates advantages over API calls to LLMs, which would otherwise be unable\nto continue the generation process.", "published": "2024-10-04 10:25:52", "link": "http://arxiv.org/abs/2410.03804v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can Mamba Always Enjoy the \"Free Lunch\"?", "abstract": "Transformers have been the cornerstone of current Large Language Models\n(LLMs); however, its linear growth in overhead during inference with respect to\nsequence length poses challenges for modeling long sequences. In this context,\nMamba has gradually attracted attention due to its constant-level size during\ninference and existing empirical results have shown that it can perform\ncomparably to Transformers in sequence modeling while offering significant\nsavings. However, one may ask that, can Mamba always enjoy the ``free lunch\"?\nIn this paper, we focus on analyzing the expressive ability of Mamba from a\ntheoretical standpoint. First, inspired by the connection between Mamba and\nlinear attention, we investigate potential shortcomings of the Mamba when\nperforming the COPY operation. Our results indicate that Mamba with constant\nsize may encounter bottlenecks when handling COPY, while it can achieve perfect\nperformance when the size scales linearly with sequence length. Based on this\nobservation, we analyze Mamba's ability to tackle DP problems when equipped\nwith Chain of Thought (CoT). Our findings suggest that to solve arbitrary DP\nproblems, the total cost of Mamba is comparable to standard and efficient\nTransformers. However, similar to efficient Transformers, when facing DP\nproblems with favorable properties such as locality, Mamba can provide savings\nin overhead. Our results contribute to a deeper understanding of Mamba.", "published": "2024-10-04 13:31:24", "link": "http://arxiv.org/abs/2410.03810v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Large Language Models can be Strong Self-Detoxifiers", "abstract": "Reducing the likelihood of generating harmful and toxic output is an\nessential task when aligning large language models (LLMs). Existing methods\nmainly rely on training an external reward model (i.e., another language model)\nor fine-tuning the LLM using self-generated data to influence the outcome. In\nthis paper, we show that LLMs have the capability of self-detoxification\nwithout the use of an additional reward model or re-training. We propose\n\\textit{Self-disciplined Autoregressive Sampling (SASA)}, a lightweight\ncontrolled decoding algorithm for toxicity reduction of LLMs. SASA leverages\nthe contextual representations from an LLM to learn linear subspaces\ncharacterizing toxic v.s. non-toxic output in analytical forms. When\nauto-completing a response token-by-token, SASA dynamically tracks the margin\nof the current output to steer the generation away from the toxic subspace, by\nadjusting the autoregressive sampling strategy. Evaluated on LLMs of different\nscale and nature, namely Llama-3.1-Instruct (8B), Llama-2 (7B), and GPT2-L\nmodels with the RealToxicityPrompts, BOLD, and AttaQ benchmarks, SASA markedly\nenhances the quality of the generated sentences relative to the original models\nand attains comparable performance to state-of-the-art detoxification\ntechniques, significantly reducing the toxicity level by only using the LLM's\ninternal representations.", "published": "2024-10-04 17:45:15", "link": "http://arxiv.org/abs/2410.03818v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Learning Code Preference via Synthetic Evolution", "abstract": "Large Language Models (LLMs) have recently demonstrated remarkable coding\ncapabilities. However, assessing code generation based on well-formed\nproperties and aligning it with developer preferences remains challenging. In\nthis paper, we explore two key questions under the new challenge of code\npreference learning: (i) How do we train models to predict meaningful\npreferences for code? and (ii) How do human and LLM preferences align with\nverifiable code properties and developer code tastes? To this end, we propose\nCodeFavor, a framework for training pairwise code preference models from\nsynthetic evolution data, including code commits and code critiques. To\nevaluate code preferences, we introduce CodePrefBench, a benchmark comprising\n1364 rigorously curated code preference tasks to cover three verifiable\nproperties-correctness, efficiency, and security-along with human preference.\nOur evaluation shows that CodeFavor holistically improves the accuracy of\nmodel-based code preferences by up to 28.8%. Meanwhile, CodeFavor models can\nmatch the performance of models with 6-9x more parameters while being 34x more\ncost-effective. We also rigorously validate the design choices in CodeFavor via\na comprehensive set of controlled experiments. Furthermore, we discover the\nprohibitive costs and limitations of human-based code preference: despite\nspending 23.4 person-minutes on each task, 15.1-40.3% of tasks remain unsolved.\nCompared to model-based preference, human preference tends to be more accurate\nunder the objective of code correctness, while being sub-optimal for\nnon-functional objectives.", "published": "2024-10-04 18:05:22", "link": "http://arxiv.org/abs/2410.03837v2", "categories": ["cs.LG", "cs.CL", "cs.SE"], "primary_category": "cs.LG"}
{"title": "SWE-bench Multimodal: Do AI Systems Generalize to Visual Software\n  Domains?", "abstract": "Autonomous systems for software engineering are now capable of fixing bugs\nand developing features. These systems are commonly evaluated on SWE-bench\n(Jimenez et al., 2024a), which assesses their ability to solve software issues\nfrom GitHub repositories. However, SWE-bench uses only Python repositories,\nwith problem statements presented predominantly as text and lacking visual\nelements such as images. This limited coverage motivates our inquiry into how\nexisting systems might perform on unrepresented software engineering domains\n(e.g., front-end, game development, DevOps), which use different programming\nlanguages and paradigms. Therefore, we propose SWE-bench Multimodal (SWE-bench\nM), to evaluate systems on their ability to fix bugs in visual, user-facing\nJavaScript software. SWE-bench M features 617 task instances collected from 17\nJavaScript libraries used for web interface design, diagramming, data\nvisualization, syntax highlighting, and interactive mapping. Each SWE-bench M\ntask instance contains at least one image in its problem statement or unit\ntests. Our analysis finds that top-performing SWE-bench systems struggle with\nSWE-bench M, revealing limitations in visual problem-solving and cross-language\ngeneralization. Lastly, we show that SWE-agent's flexible language-agnostic\nfeatures enable it to substantially outperform alternatives on SWE-bench M,\nresolving 12% of task instances compared to 6% for the next best system.", "published": "2024-10-04 18:48:58", "link": "http://arxiv.org/abs/2410.03859v1", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "DOTS: Learning to Reason Dynamically in LLMs via Optimal Reasoning\n  Trajectories Search", "abstract": "Enhancing the capability of large language models (LLMs) in reasoning has\ngained significant attention in recent years. Previous studies have\ndemonstrated the effectiveness of various prompting strategies in aiding LLMs\nin reasoning (called \"reasoning actions\"), such as step-by-step thinking,\nreflecting before answering, solving with programs, and their combinations.\nHowever, these approaches often applied static, predefined reasoning actions\nuniformly to all questions, without considering the specific characteristics of\neach question or the capability of the task-solving LLM. In this paper, we\npropose DOTS, an approach enabling LLMs to reason dynamically via optimal\nreasoning trajectory search, tailored to the specific characteristics of each\nquestion and the inherent capability of the task-solving LLM. Our approach\ninvolves three key steps: i) defining atomic reasoning action modules that can\nbe composed into various reasoning action trajectories; ii) searching for the\noptimal action trajectory for each training question through iterative\nexploration and evaluation for the specific task-solving LLM; and iii) using\nthe collected optimal trajectories to train an LLM to plan for the reasoning\ntrajectories of unseen questions. In particular, we propose two learning\nparadigms, i.e., fine-tuning an external LLM as a planner to guide the\ntask-solving LLM, or directly fine-tuning the task-solving LLM with an\ninternalized capability for reasoning actions planning. Our experiments across\neight reasoning tasks show that our method consistently outperforms static\nreasoning techniques and the vanilla instruction tuning approach. Further\nanalysis reveals that our method enables LLMs to adjust their computation based\non problem complexity, allocating deeper thinking and reasoning to harder\nproblems.", "published": "2024-10-04 18:58:09", "link": "http://arxiv.org/abs/2410.03864v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "KidLM: Advancing Language Models for Children -- Early Insights and\n  Future Directions", "abstract": "Recent studies highlight the potential of large language models in creating\neducational tools for children, yet significant challenges remain in\nmaintaining key child-specific properties such as linguistic nuances, cognitive\nneeds, and safety standards. In this paper, we explore foundational steps\ntoward the development of child-specific language models, emphasizing the\nnecessity of high-quality pre-training data. We introduce a novel user-centric\ndata collection pipeline that involves gathering and validating a corpus\nspecifically written for and sometimes by children. Additionally, we propose a\nnew training objective, Stratified Masking, which dynamically adjusts masking\nprobabilities based on our domain-specific child language data, enabling models\nto prioritize vocabulary and concepts more suitable for children. Experimental\nevaluations demonstrate that our model excels in understanding lower\ngrade-level text, maintains safety by avoiding stereotypes, and captures\nchildren's unique preferences. Furthermore, we provide actionable insights for\nfuture research and development in child-specific language modeling.", "published": "2024-10-04 19:35:44", "link": "http://arxiv.org/abs/2410.03884v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Reverb: Open-Source ASR and Diarization from Rev", "abstract": "Today, we are open-sourcing our core speech recognition and diarization\nmodels for non-commercial use. We are releasing both a full production pipeline\nfor developers as well as pared-down research models for experimentation. Rev\nhopes that these releases will spur research and innovation in the fast-moving\ndomain of voice technology. The speech recognition models released today\noutperform all existing open source speech recognition models across a variety\nof long-form speech recognition domains.", "published": "2024-10-04 21:13:58", "link": "http://arxiv.org/abs/2410.03930v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Grounding Language in Multi-Perspective Referential Communication", "abstract": "We introduce a task and dataset for referring expression generation and\ncomprehension in multi-agent embodied environments. In this task, two agents in\na shared scene must take into account one another's visual perspective, which\nmay be different from their own, to both produce and understand references to\nobjects in a scene and the spatial relations between them. We collect a dataset\nof 2,970 human-written referring expressions, each paired with human\ncomprehension judgments, and evaluate the performance of automated models as\nspeakers and listeners paired with human partners, finding that model\nperformance in both reference generation and comprehension lags behind that of\npairs of human agents. Finally, we experiment training an open-weight speaker\nmodel with evidence of communicative success when paired with a listener,\nresulting in an improvement from 58.9 to 69.3% in communicative success and\neven outperforming the strongest proprietary model.", "published": "2024-10-04 22:42:30", "link": "http://arxiv.org/abs/2410.03959v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.GR"], "primary_category": "cs.CL"}
{"title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation", "abstract": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb.", "published": "2024-10-04 22:45:26", "link": "http://arxiv.org/abs/2410.03960v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Variational Language Concepts for Interpreting Foundation Language\n  Models", "abstract": "Foundation Language Models (FLMs) such as BERT and its variants have achieved\nremarkable success in natural language processing. To date, the\ninterpretability of FLMs has primarily relied on the attention weights in their\nself-attention layers. However, these attention weights only provide word-level\ninterpretations, failing to capture higher-level structures, and are therefore\nlacking in readability and intuitiveness. To address this challenge, we first\nprovide a formal definition of conceptual interpretation and then propose a\nvariational Bayesian framework, dubbed VAriational Language Concept (VALC), to\ngo beyond word-level interpretations and provide concept-level interpretations.\nOur theoretical analysis shows that our VALC finds the optimal language\nconcepts to interpret FLM predictions. Empirical results on several real-world\ndatasets show that our method can successfully provide conceptual\ninterpretation for FLMs.", "published": "2024-10-04 23:05:19", "link": "http://arxiv.org/abs/2410.03964v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Knowledge-Augmented Reasoning for EUAIA Compliance and Adversarial\n  Robustness of LLMs", "abstract": "The EU AI Act (EUAIA) introduces requirements for AI systems which intersect\nwith the processes required to establish adversarial robustness. However, given\nthe ambiguous language of regulation and the dynamicity of adversarial attacks,\ndevelopers of systems with highly complex models such as LLMs may find their\neffort to be duplicated without the assurance of having achieved either\ncompliance or robustness. This paper presents a functional architecture that\nfocuses on bridging the two properties, by introducing components with clear\nreference to their source. Taking the detection layer recommended by the\nliterature, and the reporting layer required by the law, we aim to support\ndevelopers and auditors with a reasoning layer based on knowledge augmentation\n(rules, assurance cases, contextual mappings). Our findings demonstrate a novel\ndirection for ensuring LLMs deployed in the EU are both compliant and\nadversarially robust, which underpin trustworthiness.", "published": "2024-10-04 18:23:14", "link": "http://arxiv.org/abs/2410.09078v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SE"], "primary_category": "cs.CL"}
{"title": "BIPEFT: Budget-Guided Iterative Search for Parameter Efficient\n  Fine-Tuning of Large Pretrained Language Models", "abstract": "Parameter Efficient Fine-Tuning (PEFT) offers an efficient solution for\nfine-tuning large pretrained language models for downstream tasks. However,\nmost PEFT strategies are manually designed, often resulting in suboptimal\nperformance. Recent automatic PEFT approaches aim to address this but face\nchallenges such as search space entanglement, inefficiency, and lack of\nintegration between parameter budgets and search processes. To overcome these\nissues, we introduce a novel Budget-guided Iterative search strategy for\nautomatic PEFT (BIPEFT), significantly enhancing search efficiency. BIPEFT\nemploys a new iterative search strategy to disentangle the binary module and\nrank dimension search spaces. Additionally, we design early selection\nstrategies based on parameter budgets, accelerating the learning process by\ngradually removing unimportant modules and fixing rank dimensions. Extensive\nexperiments on public benchmarks demonstrate the superior performance of BIPEFT\nin achieving efficient and effective PEFT for downstream tasks with a low\nparameter budget.", "published": "2024-10-04 18:50:46", "link": "http://arxiv.org/abs/2410.09079v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Social Determinants of Health in Alzheimer's Research Using\n  LLM-Augmented Literature Mining and Knowledge Graphs", "abstract": "Growing evidence suggests that social determinants of health (SDoH), a set of\nnonmedical factors, affect individuals' risks of developing Alzheimer's disease\n(AD) and related dementias. Nevertheless, the etiological mechanisms underlying\nsuch relationships remain largely unclear, mainly due to difficulties in\ncollecting relevant information. This study presents a novel, automated\nframework that leverages recent advancements of large language model (LLM) and\nnatural language processing techniques to mine SDoH knowledge from extensive\nliterature and integrate it with AD-related biological entities extracted from\nthe general-purpose knowledge graph PrimeKG. Utilizing graph neural networks,\nwe performed link prediction tasks to evaluate the resultant SDoH-augmented\nknowledge graph. Our framework shows promise for enhancing knowledge discovery\nin AD and can be generalized to other SDoH-related research areas, offering a\nnew tool for exploring the impact of social determinants on health outcomes.\nOur code is available at: https://github.com/hwq0726/SDoHenPKG", "published": "2024-10-04 21:39:30", "link": "http://arxiv.org/abs/2410.09080v1", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Five Years of COVID-19 Discourse on Instagram: A Labeled Instagram\n  Dataset of Over Half a Million Posts for Multilingual Sentiment Analysis", "abstract": "The work presented in this paper makes three scientific contributions with a\nspecific focus on mining and analysis of COVID-19-related posts on Instagram.\nFirst, it presents a multilingual dataset of 500,153 Instagram posts about\nCOVID-19 published between January 2020 and September 2024. This dataset,\navailable at https://dx.doi.org/10.21227/d46p-v480, contains Instagram posts in\n161 different languages as well as 535,021 distinct hashtags. After the\ndevelopment of this dataset, multilingual sentiment analysis was performed,\nwhich involved classifying each post as positive, negative, or neutral. The\nresults of sentiment analysis are presented as a separate attribute in this\ndataset. Second, it presents the results of performing sentiment analysis per\nyear from 2020 to 2024. The findings revealed the trends in sentiment related\nto COVID-19 on Instagram since the beginning of the pandemic. For instance,\nbetween 2020 and 2024, the sentiment trends show a notable shift, with positive\nsentiment decreasing from 38.35% to 28.69%, while neutral sentiment rising from\n44.19% to 58.34%. Finally, the paper also presents findings of\nlanguage-specific sentiment analysis. This analysis highlighted similar and\ncontrasting trends of sentiment across posts published in different languages\non Instagram. For instance, out of all English posts, 49.68% were positive,\n14.84% were negative, and 35.48% were neutral. In contrast, among Hindi posts,\n4.40% were positive, 57.04% were negative, and 38.56% were neutral, reflecting\ndistinct differences in the sentiment distribution between these two languages.", "published": "2024-10-04 10:06:55", "link": "http://arxiv.org/abs/2410.03293v3", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.SI", "I.2.7; I.2.8; I.5.4; K.4.2; H.2.8; I.2.6"], "primary_category": "cs.CL"}
{"title": "Chain-of-Jailbreak Attack for Image Generation Models via Editing Step\n  by Step", "abstract": "Text-based image generation models, such as Stable Diffusion and DALL-E 3,\nhold significant potential in content creation and publishing workflows, making\nthem the focus in recent years. Despite their remarkable capability to generate\ndiverse and vivid images, considerable efforts are being made to prevent the\ngeneration of harmful content, such as abusive, violent, or pornographic\nmaterial. To assess the safety of existing models, we introduce a novel\njailbreaking method called Chain-of-Jailbreak (CoJ) attack, which compromises\nimage generation models through a step-by-step editing process. Specifically,\nfor malicious queries that cannot bypass the safeguards with a single prompt,\nwe intentionally decompose the query into multiple sub-queries. The image\ngeneration models are then prompted to generate and iteratively edit images\nbased on these sub-queries. To evaluate the effectiveness of our CoJ attack\nmethod, we constructed a comprehensive dataset, CoJ-Bench, encompassing nine\nsafety scenarios, three types of editing operations, and three editing\nelements. Experiments on four widely-used image generation services provided by\nGPT-4V, GPT-4o, Gemini 1.5 and Gemini 1.5 Pro, demonstrate that our CoJ attack\nmethod can successfully bypass the safeguards of models for over 60% cases,\nwhich significantly outperforms other jailbreaking methods (i.e., 14%).\nFurther, to enhance these models' safety against our CoJ attack method, we also\npropose an effective prompting-based method, Think Twice Prompting, that can\nsuccessfully defend over 95% of CoJ attack. We release our dataset and code to\nfacilitate the AI safety research.", "published": "2024-10-04 19:04:43", "link": "http://arxiv.org/abs/2410.03869v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.CV", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Textless Streaming Speech-to-Speech Translation using Semantic Speech\n  Tokens", "abstract": "Cascaded speech-to-speech translation systems often suffer from the error\naccumulation problem and high latency, which is a result of cascaded modules\nwhose inference delays accumulate. In this paper, we propose a transducer-based\nspeech translation model that outputs discrete speech tokens in a low-latency\nstreaming fashion. This approach eliminates the need for generating text output\nfirst, followed by machine translation (MT) and text-to-speech (TTS) systems.\nThe produced speech tokens can be directly used to generate a speech signal\nwith low latency by utilizing an acoustic language model (LM) to obtain\nacoustic tokens and an audio codec model to retrieve the waveform. Experimental\nresults show that the proposed method outperforms other existing approaches and\nachieves state-of-the-art results for streaming translation in terms of BLEU,\naverage latency, and BLASER 2.0 scores for multiple language pairs using the\nCVSS-C dataset as a benchmark.", "published": "2024-10-04 10:21:15", "link": "http://arxiv.org/abs/2410.03298v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "How does the teacher rate? Observations from the NeuroPiano dataset", "abstract": "This paper provides a detailed analysis of the NeuroPiano dataset, which\ncomprise 104 audio recordings of student piano performances accompanied with\n2255 textual feedback and ratings given by professional pianists. We offer a\nstatistical overview of the dataset, focusing on the standardization of\nannotations and inter-annotator agreement across 12 evaluative questions\nconcerning performance quality. We also explore the predictive relationship\nbetween audio features and teacher ratings via machine learning, as well as\nannotations provided for text analysis of the responses.", "published": "2024-10-04 04:26:11", "link": "http://arxiv.org/abs/2410.03139v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Biodenoising: Animal Vocalization Denoising without Access to Clean Data", "abstract": "Animal vocalization denoising is a task similar to human speech enhancement,\nwhich is relatively well-studied. In contrast to the latter, it comprises a\nhigher diversity of sound production mechanisms and recording environments, and\nthis higher diversity is a challenge for existing models. Adding to the\nchallenge and in contrast to speech, we lack large and diverse datasets\ncomprising clean vocalizations. As a solution we use as training data\npseudo-clean targets, i.e. pre-denoised vocalizations, and segments of\nbackground noise without a vocalization. We propose a train set derived from\nbioacoustics datasets and repositories representing diverse species, acoustic\nenvironments, geographic regions. Additionally, we introduce a non-overlapping\nbenchmark set comprising clean vocalizations from different taxa and noise\nsamples. We show that that denoising models (demucs, CleanUNet) trained on\npseudo-clean targets obtained with speech enhancement models achieve\ncompetitive results on the benchmarking set. We publish data, code, libraries,\nand demos at https://earthspecies.github.io/biodenoising/.", "published": "2024-10-04 13:37:07", "link": "http://arxiv.org/abs/2410.03427v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MultiVerse: Efficient and Expressive Zero-Shot Multi-Task Text-to-Speech", "abstract": "Text-to-speech (TTS) systems that scale up the amount of training data have\nachieved significant improvements in zero-shot speech synthesis. However, these\nsystems have certain limitations: they require a large amount of training data,\nwhich increases costs, and often overlook prosody similarity. To address these\nissues, we propose MultiVerse, a zero-shot multi-task TTS system that is able\nto perform TTS or speech style transfer in zero-shot and cross-lingual\nconditions. MultiVerse requires much less training data than traditional\ndata-driven approaches. To ensure zero-shot performance even with limited data,\nwe leverage source-filter theory-based disentanglement, utilizing the prompt\nfor modeling filter-related and source-related representations. Additionally,\nto further enhance prosody similarity, we adopt a prosody modeling approach\ncombining prompt-based autoregressive and non-autoregressive methods.\nEvaluations demonstrate the remarkable zero-shot multi-task TTS performance of\nMultiVerse and show that MultiVerse not only achieves zero-shot TTS performance\ncomparable to data-driven TTS systems with much less data, but also\nsignificantly outperforms other zero-shot TTS systems trained with the same\nsmall amount of data. In particular, our novel prosody modeling technique\nsignificantly contributes to MultiVerse's ability to generate speech with high\nprosody similarity to the given prompts. Our samples are available at\nhttps://nc-ai.github.io/speech/publications/multiverse/index.html", "published": "2024-10-04 07:10:25", "link": "http://arxiv.org/abs/2410.03192v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Enriching Music Descriptions with a Finetuned-LLM and Metadata for\n  Text-to-Music Retrieval", "abstract": "Text-to-Music Retrieval, finding music based on a given natural language\nquery, plays a pivotal role in content discovery within extensive music\ndatabases. To address this challenge, prior research has predominantly focused\non a joint embedding of music audio and text, utilizing it to retrieve music\ntracks that exactly match descriptive queries related to musical attributes\n(i.e. genre, instrument) and contextual elements (i.e. mood, theme). However,\nusers also articulate a need to explore music that shares similarities with\ntheir favorite tracks or artists, such as \\textit{I need a similar track to\nSuperstition by Stevie Wonder}. To address these concerns, this paper proposes\nan improved Text-to-Music Retrieval model, denoted as TTMR++, which utilizes\nrich text descriptions generated with a finetuned large language model and\nmetadata. To accomplish this, we obtained various types of seed text from\nseveral existing music tag and caption datasets and a knowledge graph dataset\nof artists and tracks. The experimental results show the effectiveness of\nTTMR++ in comparison to state-of-the-art music-text joint embedding models\nthrough a comprehensive evaluation involving various musical text queries.", "published": "2024-10-04 09:33:34", "link": "http://arxiv.org/abs/2410.03264v1", "categories": ["cs.SD", "cs.IR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Manikin-Recorded Cardiopulmonary Sounds Dataset Using Digital\n  Stethoscope", "abstract": "Heart and lung sounds are crucial for healthcare monitoring. Recent\nimprovements in stethoscope technology have made it possible to capture patient\nsounds with enhanced precision. In this dataset, we used a digital stethoscope\nto capture both heart and lung sounds, including individual and mixed\nrecordings. To our knowledge, this is the first dataset to offer both separate\nand mixed cardiorespiratory sounds. The recordings were collected from a\nclinical manikin, a patient simulator designed to replicate human physiological\nconditions, generating clean heart and lung sounds at different body locations.\nThis dataset includes both normal sounds and various abnormalities (i.e.,\nmurmur, atrial fibrillation, tachycardia, atrioventricular block, third and\nfourth heart sound, wheezing, crackles, rhonchi, pleural rub, and gurgling\nsounds). The dataset includes audio recordings of chest examinations performed\nat different anatomical locations, as determined by specialist nurses. Each\nrecording has been enhanced using frequency filters to highlight specific sound\ntypes. This dataset is useful for applications in artificial intelligence, such\nas automated cardiopulmonary disease detection, sound classification,\nunsupervised separation techniques, and deep learning algorithms related to\naudio signal processing.", "published": "2024-10-04 09:53:16", "link": "http://arxiv.org/abs/2410.03280v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Audio-Agent: Leveraging LLMs For Audio Generation, Editing and\n  Composition", "abstract": "We introduce Audio-Agent, a multimodal framework for audio generation,\nediting and composition based on text or video inputs. Conventional approaches\nfor text-to-audio (TTA) tasks often make single-pass inferences from text\ndescriptions. While straightforward, this design struggles to produce\nhigh-quality audio when given complex text conditions. In our method, we\nutilize a pre-trained TTA diffusion network as the audio generation agent to\nwork in tandem with GPT-4, which decomposes the text condition into atomic,\nspecific instructions and calls the agent for audio generation. In doing so,\nAudio-Agent can generate high-quality audio that is closely aligned with the\nprovided text or video exhibiting complex and multiple events, while supporting\nvariable-length and variable-volume generation. For video-to-audio (VTA) tasks,\nmost existing methods require training a timestamp detector to synchronize\nvideo events with the generated audio, a process that can be tedious and\ntime-consuming. Instead, we propose a simpler approach by fine-tuning a\npre-trained Large Language Model (LLM), e.g., Gemma2-2B-it, to obtain both\nsemantic and temporal conditions that bridge the video and audio modality.\nConsequently, our framework contributes a comprehensive solution for both TTA\nand VTA tasks without substantial computational overhead in training.", "published": "2024-10-04 11:40:53", "link": "http://arxiv.org/abs/2410.03335v2", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SoundSignature: What Type of Music Do You Like?", "abstract": "SoundSignature is a music application that integrates a custom OpenAI\nAssistant to analyze users' favorite songs. The system incorporates\nstate-of-the-art Music Information Retrieval (MIR) Python packages to combine\nextracted acoustic/musical features with the assistant's extensive knowledge of\nthe artists and bands. Capitalizing on this combined knowledge, SoundSignature\nleverages semantic audio and principles from the emerging Internet of Sounds\n(IoS) ecosystem, integrating MIR with AI to provide users with personalized\ninsights into the acoustic properties of their music, akin to a musical\npreference personality report. Users can then interact with the chatbot to\nexplore deeper inquiries about the acoustic analyses performed and how they\nrelate to their musical taste. This interactivity transforms the application,\nacting not only as an informative resource about familiar and/or favorite\nsongs, but also as an educational platform that enables users to deepen their\nunderstanding of musical features, music theory, acoustic properties commonly\nused in signal processing, and the artists behind the music. Beyond general\nusability, the application also incorporates several well-established\nopen-source musician-specific tools, such as a chord recognition algorithm\n(CREMA), a source separation algorithm (DEMUCS), and an audio-to-MIDI converter\n(basic-pitch). These features allow users without coding skills to access\nadvanced, open-source music processing algorithms simply by interacting with\nthe chatbot (e.g., can you give me the stems of this song?). In this paper, we\nhighlight the application's innovative features and educational potential, and\npresent findings from a pilot user study that evaluates its efficacy and\nusability.", "published": "2024-10-04 12:40:45", "link": "http://arxiv.org/abs/2410.03375v1", "categories": ["cs.SD", "cs.AI", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SOI: Scaling Down Computational Complexity by Estimating Partial States\n  of the Model", "abstract": "Consumer electronics used to follow the miniaturization trend described by\nMoore's Law. Despite increased processing power in Microcontroller Units\n(MCUs), MCUs used in the smallest appliances are still not capable of running\neven moderately big, state-of-the-art artificial neural networks (ANNs)\nespecially in time-sensitive scenarios. In this work, we present a novel method\ncalled Scattered Online Inference (SOI) that aims to reduce the computational\ncomplexity of ANNs. SOI leverages the continuity and seasonality of time-series\ndata and model predictions, enabling extrapolation for processing speed\nimprovements, particularly in deeper layers. By applying compression, SOI\ngenerates more general inner partial states of ANN, allowing skipping full\nmodel recalculation at each inference.", "published": "2024-10-04 13:53:15", "link": "http://arxiv.org/abs/2410.03813v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "SONIQUE: Video Background Music Generation Using Unpaired Audio-Visual\n  Data", "abstract": "We present SONIQUE, a model for generating background music tailored to video\ncontent. Unlike traditional video-to-music generation approaches, which rely\nheavily on paired audio-visual datasets, SONIQUE leverages unpaired data,\ncombining royalty-free music and independent video sources. By utilizing large\nlanguage models (LLMs) for video understanding and converting visual\ndescriptions into musical tags, alongside a U-Net-based conditional diffusion\nmodel, SONIQUE enables customizable music generation. Users can control\nspecific aspects of the music, such as instruments, genres, tempo, and\nmelodies, ensuring the generated output fits their creative vision. SONIQUE is\nopen-source, with a demo available online.", "published": "2024-10-04 19:22:35", "link": "http://arxiv.org/abs/2410.03879v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Did You Hear That? Introducing AADG: A Framework for Generating\n  Benchmark Data in Audio Anomaly Detection", "abstract": "We introduce a novel, general-purpose audio generation framework specifically\ndesigned for anomaly detection and localization. Unlike existing datasets that\npredominantly focus on industrial and machine-related sounds, our framework\nfocuses a broader range of environments, particularly useful in real-world\nscenarios where only audio data are available, such as in video-derived or\ntelephonic audio. To generate such data, we propose a new method inspired by\nthe LLM-Modulo framework, which leverages large language models(LLMs) as world\nmodels to simulate such real-world scenarios. This tool is modular allowing a\nplug-and-play approach. It operates by first using LLMs to predict plausible\nreal-world scenarios. An LLM further extracts the constituent sounds, the order\nand the way in which these should be merged to create coherent wholes. Much\nlike the LLM-Modulo framework, we include rigorous verification of each output\nstage, ensuring the reliability of the generated data. The data produced using\nthe framework serves as a benchmark for anomaly detection applications,\npotentially enhancing the performance of models trained on audio data,\nparticularly in handling out-of-distribution cases. Our contributions thus fill\na critical void in audio anomaly detection resources and provide a scalable\ntool for generating diverse, realistic audio data.", "published": "2024-10-04 20:12:35", "link": "http://arxiv.org/abs/2410.03904v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Edge Computing in Distributed Acoustic Sensing: An Application in\n  Traffic Monitoring", "abstract": "Distributed acoustic sensing (DAS) technology leverages fiber optic cables to\ndetect vibrations and acoustic events, which is a promising solution for\nreal-time traffic monitoring. In this paper, we introduce a novel methodology\nfor detecting and tracking vehicles using DAS data, focusing on real-time\nprocessing through edge computing. Our approach applies the Hough transform to\ndetect straight-line segments in the spatiotemporal DAS data, corresponding to\nvehicles crossing the Astfjord bridge in Norway. These segments are further\nclustered using the Density-based spatial clustering of applications with noise\n(DBSCAN) algorithm to consolidate multiple detections of the same vehicle,\nreducing noise and improving accuracy. The proposed workflow effectively counts\nvehicles and estimates their speed with only tens of seconds latency, enabling\nreal-time traffic monitoring on the edge. To validate the system, we compare\nDAS data with simultaneous video footage, achieving high accuracy in vehicle\ndetection, including the distinction between cars and trucks based on signal\nstrength and frequency content. Results show that the system is capable of\nprocessing large volumes of data efficiently. We also analyze vehicle speeds\nand traffic patterns, identifying temporal trends and variations in traffic\nflow. Real-time deployment on edge devices allows immediate analysis and\nvisualization via cloud-based platforms. In addition to traffic monitoring, the\nmethod successfully detected structural responses in the bridge, highlighting\nits potential use in structural health monitoring.", "published": "2024-10-04 07:31:03", "link": "http://arxiv.org/abs/2410.16278v1", "categories": ["cs.NI", "cs.SD", "eess.AS"], "primary_category": "cs.NI"}
{"title": "Generative Semantic Communication for Text-to-Speech Synthesis", "abstract": "Semantic communication is a promising technology to improve communication\nefficiency by transmitting only the semantic information of the source data.\nHowever, traditional semantic communication methods primarily focus on data\nreconstruction tasks, which may not be efficient for emerging generative tasks\nsuch as text-to-speech (TTS) synthesis. To address this limitation, this paper\ndevelops a novel generative semantic communication framework for TTS synthesis,\nleveraging generative artificial intelligence technologies. Firstly, we utilize\na pre-trained large speech model called WavLM and the residual vector\nquantization method to construct two semantic knowledge bases (KBs) at the\ntransmitter and receiver, respectively. The KB at the transmitter enables\neffective semantic extraction, while the KB at the receiver facilitates\nlifelike speech synthesis. Then, we employ a transformer encoder and a\ndiffusion model to achieve efficient semantic coding without introducing\nsignificant communication overhead. Finally, numerical results demonstrate that\nour framework achieves much higher fidelity for the generated speech than four\nbaselines, in both cases with additive white Gaussian noise channel and\nRayleigh fading channel.", "published": "2024-10-04 14:18:31", "link": "http://arxiv.org/abs/2410.03459v1", "categories": ["cs.SD", "cs.IT", "cs.LG", "eess.AS", "math.IT"], "primary_category": "cs.SD"}
{"title": "Episodic fine-tuning prototypical networks for optimization-based\n  few-shot learning: Application to audio classification", "abstract": "The Prototypical Network (ProtoNet) has emerged as a popular choice in\nFew-shot Learning (FSL) scenarios due to its remarkable performance and\nstraightforward implementation. Building upon such success, we first propose a\nsimple (yet novel) method to fine-tune a ProtoNet on the (labeled) support set\nof the test episode of a C-way-K-shot test episode (without using the query set\nwhich is only used for evaluation). We then propose an algorithmic framework\nthat combines ProtoNet with optimization-based FSL algorithms (MAML and\nMeta-Curvature) to work with such a fine-tuning method. Since\noptimization-based algorithms endow the target learner model with the ability\nto fast adaption to only a few samples, we utilize ProtoNet as the target model\nto enhance its fine-tuning performance with the help of a specifically designed\nepisodic fine-tuning strategy. The experimental results confirm that our\nproposed models, MAML-Proto and MC-Proto, combined with our unique fine-tuning\nmethod, outperform regular ProtoNet by a large margin in few-shot audio\nclassification tasks on the ESC-50 and Speech Commands v2 datasets. We note\nthat although we have only applied our model to the audio domain, it is a\ngeneral method and can be easily extended to other domains.", "published": "2024-10-04 12:39:29", "link": "http://arxiv.org/abs/2410.05302v1", "categories": ["eess.AS", "cs.LG", "cs.MM", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Diffusion-based Unsupervised Audio-visual Speech Enhancement", "abstract": "This paper proposes a new unsupervised audio-visual speech enhancement (AVSE)\napproach that combines a diffusion-based audio-visual speech generative model\nwith a non-negative matrix factorization (NMF) noise model. First, the\ndiffusion model is pre-trained on clean speech conditioned on corresponding\nvideo data to simulate the speech generative distribution. This pre-trained\nmodel is then paired with the NMF-based noise model to estimate clean speech\niteratively. Specifically, a diffusion-based posterior sampling approach is\nimplemented within the reverse diffusion process, where after each iteration, a\nspeech estimate is obtained and used to update the noise parameters.\nExperimental results confirm that the proposed AVSE approach not only\noutperforms its audio-only counterpart but also generalizes better than a\nrecent supervised-generative AVSE method. Additionally, the new inference\nalgorithm offers a better balance between inference speed and performance\ncompared to the previous diffusion-based method. Code and demo available at:\nhttps://jeaneudesayilo.github.io/fast_UdiffSE", "published": "2024-10-04 12:22:54", "link": "http://arxiv.org/abs/2410.05301v2", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
