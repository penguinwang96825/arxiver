{"title": "Context-Aware Learning for Neural Machine Translation", "abstract": "Interest in larger-context neural machine translation, including\ndocument-level and multi-modal translation, has been growing. Multiple works\nhave proposed new network architectures or evaluation schemes, but potentially\nhelpful context is still sometimes ignored by larger-context translation\nmodels. In this paper, we propose a novel learning algorithm that explicitly\nencourages a neural translation model to take into account additional context\nusing a multilevel pair-wise ranking loss. We evaluate the proposed learning\nalgorithm with a transformer-based larger-context translation system on\ndocument-level translation. By comparing performance using actual and random\ncontexts, we show that a model trained with the proposed algorithm is more\nsensitive to the additional context.", "published": "2019-03-12 03:55:13", "link": "http://arxiv.org/abs/1903.04715v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syllable-based Neural Named Entity Recognition for Myanmar Language", "abstract": "Named Entity Recognition (NER) for Myanmar Language is essential to Myanmar\nnatural language processing research work. In this work, NER for Myanmar\nlanguage is treated as a sequence tagging problem and the effectiveness of deep\nneural networks on NER for Myanmar language has been investigated. Experiments\nare performed by applying deep neural network architectures on syllable level\nMyanmar contexts. Very first manually annotated NER corpus for Myanmar language\nis also constructed and proposed. In developing our in-house NER corpus,\nsentences from online news website and also sentences supported from\nALT-Parallel-Corpus are also used. This ALT corpus is one part of the Asian\nLanguage Treebank (ALT) project under ASEAN IVO. This paper contributes the\nfirst evaluation of neural network models on NER task for Myanmar language. The\nexperimental results show that those neural sequence models can produce\npromising results compared to the baseline CRF model. Among those neural\narchitectures, bidirectional LSTM network added CRF layer above gives the\nhighest F-score value. This work also aims to discover the effectiveness of\nneural network approaches to Myanmar textual processing as well as to promote\nfurther researches on this understudied language.", "published": "2019-03-12 05:52:41", "link": "http://arxiv.org/abs/1903.04739v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Character Eyes: Seeing Language through Character-Level Taggers", "abstract": "Character-level models have been used extensively in recent years in NLP\ntasks as both supplements and replacements for closed-vocabulary token-level\nword representations. In one popular architecture, character-level LSTMs are\nused to feed token representations into a sequence tagger predicting\ntoken-level annotations such as part-of-speech (POS) tags. In this work, we\nexamine the behavior of POS taggers across languages from the perspective of\nindividual hidden units within the character LSTM. We aggregate the behavior of\nthese units into language-level metrics which quantify the challenges that\ntaggers face on languages with different morphological properties, and identify\nlinks between synthesis and affixation preference and emergent behavior of the\nhidden tagger layer. In a comparative experiment, we show how modifying the\nbalance between forward and backward hidden units affects model arrangement and\nperformance in these types of languages.", "published": "2019-03-12 16:42:39", "link": "http://arxiv.org/abs/1903.05041v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scaling Multi-Domain Dialogue State Tracking via Query Reformulation", "abstract": "We present a novel approach to dialogue state tracking and referring\nexpression resolution tasks. Successful contextual understanding of multi-turn\nspoken dialogues requires resolving referring expressions across turns and\ntracking the entities relevant to the conversation across turns. Tracking\nconversational state is particularly challenging in a multi-domain scenario\nwhen there exist multiple spoken language understanding (SLU) sub-systems, and\neach SLU sub-system operates on its domain-specific meaning representation.\nWhile previous approaches have addressed the disparate schema issue by learning\ncandidate transformations of the meaning representation, in this paper, we\ninstead model the reference resolution as a dialogue context-aware user query\nreformulation task -- the dialog state is serialized to a sequence of natural\nlanguage tokens representing the conversation. We develop our model for query\nreformulation using a pointer-generator network and a novel multi-task learning\nsetup. In our experiments, we show a significant improvement in absolute F1 on\nan internal as well as a, soon to be released, public benchmark respectively.", "published": "2019-03-12 19:26:39", "link": "http://arxiv.org/abs/1903.05164v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bootstrapping Method for Developing Part-of-Speech Tagged Corpus in Low\n  Resource Languages Tagset - A Focus on an African Igbo", "abstract": "Most languages, especially in Africa, have fewer or no established\npart-of-speech (POS) tagged corpus. However, POS tagged corpus is essential for\nnatural language processing (NLP) to support advanced researches such as\nmachine translation, speech recognition, etc. Even in cases where there is no\nPOS tagged corpus, there are some languages for which parallel texts are\navailable online. The task of POS tagging a new language corpus with a new\ntagset usually face a bootstrapping problem at the initial stages of the\nannotation process. The unavailability of automatic taggers to help the human\nannotator makes the annotation process to appear infeasible to quickly produce\nadequate amounts of POS tagged corpus for advanced NLP research and training\nthe taggers. In this paper, we demonstrate the efficacy of a POS annotation\nmethod that employed the services of two automatic approaches to assist POS\ntagged corpus creation for a novel language in NLP. The two approaches are\ncross-lingual and monolingual POS tags projection. We used cross-lingual to\nautomatically create an initial 'errorful' tagged corpus for a target language\nvia word-alignment. The resources for creating this are derived from a source\nlanguage rich in NLP resources. A monolingual method is applied to clean the\ninduce noise via an alignment process and to transform the source language tags\nto the target language tags. We used English and Igbo as our case study. This\nis possible because there are parallel texts that exist between English and\nIgbo, and the source language English has available NLP resources. The results\nof the experiment show a steady improvement in accuracy and rate of tags\ntransformation with score ranges of 6.13% to 83.79% and 8.67% to 98.37%\nrespectively. The rate of tags transformation evaluates the rate at which\nsource language tags are translated to target language tags.", "published": "2019-03-12 21:24:25", "link": "http://arxiv.org/abs/1903.05225v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntax-aware Neural Semantic Role Labeling with Supertags", "abstract": "We introduce a new syntax-aware model for dependency-based semantic role\nlabeling that outperforms syntax-agnostic models for English and Spanish. We\nuse a BiLSTM to tag the text with supertags extracted from dependency parses,\nand we feed these supertags, along with words and parts of speech, into a deep\nhighway BiLSTM for semantic role labeling. Our model combines the strengths of\nearlier models that performed SRL on the basis of a full dependency parse with\nmore recent models that use no syntactic information at all. Our local and\nnon-ensemble model achieves state-of-the-art performance on the CoNLL 09\nEnglish and Spanish datasets. SRL models benefit from syntactic information,\nand we show that supertagging is a simple, powerful, and robust way to\nincorporate syntax into a neural SRL system.", "published": "2019-03-12 23:35:32", "link": "http://arxiv.org/abs/1903.05260v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-To-End Speech Recognition Using A High Rank LSTM-CTC Based Model", "abstract": "Long Short Term Memory Connectionist Temporal Classification (LSTM-CTC) based\nend-to-end models are widely used in speech recognition due to its simplicity\nin training and efficiency in decoding. In conventional LSTM-CTC based models,\na bottleneck projection matrix maps the hidden feature vectors obtained from\nLSTM to softmax output layer. In this paper, we propose to use a high rank\nprojection layer to replace the projection matrix. The output from the high\nrank projection layer is a weighted combination of vectors that are projected\nfrom the hidden feature vectors via different projection matrices and\nnon-linear activation function. The high rank projection layer is able to\nimprove the expressiveness of LSTM-CTC models. The experimental results show\nthat on Wall Street Journal (WSJ) corpus and LibriSpeech data set, the proposed\nmethod achieves 4%-6% relative word error rate (WER) reduction over the\nbaseline CTC system. They outperform other published CTC based end-to-end (E2E)\nmodels under the condition that no external data or data augmentation is\napplied. Code has been made available at https://github.com/mobvoi/lstm_ctc.", "published": "2019-03-12 23:40:27", "link": "http://arxiv.org/abs/1903.05261v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interaction Embeddings for Prediction and Explanation in Knowledge\n  Graphs", "abstract": "Knowledge graph embedding aims to learn distributed representations for\nentities and relations, and is proven to be effective in many applications.\nCrossover interactions --- bi-directional effects between entities and\nrelations --- help select related information when predicting a new triple, but\nhaven't been formally discussed before. In this paper, we propose CrossE, a\nnovel knowledge graph embedding which explicitly simulates crossover\ninteractions. It not only learns one general embedding for each entity and\nrelation as most previous methods do, but also generates multiple triple\nspecific embeddings for both of them, named interaction embeddings. We evaluate\nembeddings on typical link prediction tasks and find that CrossE achieves\nstate-of-the-art results on complex and more challenging datasets. Furthermore,\nwe evaluate embeddings from a new perspective --- giving explanations for\npredicted triples, which is important for real applications. In this work, an\nexplanation for a triple is regarded as a reliable closed-path between the head\nand the tail entity. Compared to other baselines, we show experimentally that\nCrossE, benefiting from interaction embeddings, is more capable of generating\nreliable explanations to support its predictions.", "published": "2019-03-12 07:12:46", "link": "http://arxiv.org/abs/1903.04750v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Few-Shot and Zero-Shot Learning for Historical Text Normalization", "abstract": "Historical text normalization often relies on small training datasets. Recent\nwork has shown that multi-task learning can lead to significant improvements by\nexploiting synergies with related datasets, but there has been no systematic\nstudy of different multi-task learning architectures. This paper evaluates\n63~multi-task learning configurations for sequence-to-sequence-based historical\ntext normalization across ten datasets from eight languages, using\nautoencoding, grapheme-to-phoneme mapping, and lemmatization as auxiliary\ntasks. We observe consistent, significant improvements across languages when\ntraining data for the target task is limited, but minimal or no improvements\nwhen training data is abundant. We also show that zero-shot learning\noutperforms the simple, but relatively strong, identity baseline.", "published": "2019-03-12 12:30:54", "link": "http://arxiv.org/abs/1903.04870v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Topological Analysis of Syntactic Structures", "abstract": "We use the persistent homology method of topological data analysis and\ndimensional analysis techniques to study data of syntactic structures of world\nlanguages. We analyze relations between syntactic parameters in terms of\ndimensionality, of hierarchical clustering structures, and of non-trivial\nloops. We show there are relations that hold across language families and\nadditional relations that are family-specific. We then analyze the trees\ndescribing the merging structure of persistent connected components for\nlanguages in different language families and we show that they partly correlate\nto historical phylogenetic trees but with significant differences. We also show\nthe existence of interesting non-trivial persistent first homology groups in\nvarious language families. We give examples where explicit generators for the\npersistent first homology can be identified, some of which appear to correspond\nto homoplasy phenomena, while others may have an explanation in terms of\nhistorical linguistics, corresponding to known cases of syntactic borrowing\nacross different language subfamilies.", "published": "2019-03-12 19:57:34", "link": "http://arxiv.org/abs/1903.05181v1", "categories": ["cs.CL", "math.AT", "91F20, 55U10, 55N35, 62-07"], "primary_category": "cs.CL"}
{"title": "\"Hang in There\": Lexical and Visual Analysis to Identify Posts\n  Warranting Empathetic Responses", "abstract": "In the past few years, social media has risen as a platform where people\nexpress and share personal incidences about abuse, violence and mental health\nissues. There is a need to pinpoint such posts and learn the kind of response\nexpected. For this purpose, we understand the sentiment that a personal story\nelicits on different posts present on different social media sites, on the\ntopics of abuse or mental health. In this paper, we propose a method supported\nby hand-crafted features to judge if the post requires an empathetic response.\nThe model is trained upon posts from various web-pages and corresponding\ncomments, on both the captions and the images. We were able to obtain 80%\naccuracy in tagging posts requiring empathetic responses.", "published": "2019-03-12 20:55:09", "link": "http://arxiv.org/abs/1903.05210v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Offensive Language Analysis using Deep Learning Architecture", "abstract": "SemEval-2019 Task 6 (Zampieri et al., 2019b) requires us to identify and\ncategorise offensive language in social media. In this paper we will describe\nthe process we took to tackle this challenge. Our process is heavily inspired\nby Sosa (2017) where he proposed CNN-LSTM and LSTM-CNN models to conduct\ntwitter sentiment analysis. We decided to follow his approach as well as\nfurther his work by testing out different variations of RNN models with CNN.\nSpecifically, we have divided the challenge into two parts: data processing and\nsampling and choosing the optimal deep learning architecture. In preprocessing,\nwe experimented with two techniques, SMOTE and Class Weights to counter the\nimbalance between classes. Once we are happy with the quality of our input\ndata, we proceed to choosing the optimal deep learning architecture for this\ntask. Given the quality and quantity of data we have been given, we found that\nthe addition of CNN layer provides very little to no additional improvement to\nour model's performance and sometimes even lead to a decrease in our F1-score.\nIn the end, the deep learning architecture that gives us the highest macro\nF1-score is a simple BiLSTM-CNN.", "published": "2019-03-12 09:36:25", "link": "http://arxiv.org/abs/1903.05280v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Pitfalls of Measuring Emergent Communication", "abstract": "How do we know if communication is emerging in a multi-agent system? The vast\nmajority of recent papers on emergent communication show that adding a\ncommunication channel leads to an increase in reward or task success. This is a\nuseful indicator, but provides only a coarse measure of the agent's learned\ncommunication abilities. As we move towards more complex environments, it\nbecomes imperative to have a set of finer tools that allow qualitative and\nquantitative insights into the emergence of communication. This may be\nespecially useful to allow humans to monitor agents' behaviour, whether for\nfault detection, assessing performance, or even building trust. In this paper,\nwe examine a few intuitive existing metrics for measuring communication, and\nshow that they can be misleading. Specifically, by training deep reinforcement\nlearning agents to play simple matrix games augmented with a communication\nchannel, we find a scenario where agents appear to communicate (their messages\nprovide information about their subsequent action), and yet the messages do not\nimpact the environment or other agent in any way. We explain this phenomenon\nusing ablation studies and by visualizing the representations of the learned\npolicies. We also survey some commonly used metrics for measuring emergent\ncommunication, and provide recommendations as to when these metrics should be\nused.", "published": "2019-03-12 19:33:49", "link": "http://arxiv.org/abs/1903.05168v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Progressive Generative Adversarial Binary Networks for Music Generation", "abstract": "Recent improvements in generative adversarial network (GAN) training\ntechniques prove that progressively training a GAN drastically stabilizes the\ntraining and improves the quality of outputs produced. Adding layers after the\nprevious ones have converged has proven to help in better overall convergence\nand stability of the model as well as reducing the training time by a\nsufficient amount. Thus we use this training technique to train the model\nprogressively in the time and pitch domain i.e. starting from a very small time\nvalue and pitch range we gradually expand the matrix sizes until the end result\nis a completely trained model giving outputs having tensor sizes [4 (bar) x 96\n(time steps) x 84 (pitch values) x 8 (tracks)]. As proven in previously\nproposed models deterministic binary neurons also help in improving the\nresults. Thus we make use of a layer of deterministic binary neurons at the end\nof the generator to get binary valued outputs instead of fractional values\nexisting between 0 and 1.", "published": "2019-03-12 04:16:20", "link": "http://arxiv.org/abs/1903.04722v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Skipping Behavior of Users of Music Streaming Services and its\n  Relation to Musical Structure", "abstract": "The behavior of users of music streaming services is investigated from the\npoint of view of the temporal dimension of individual songs; specifically, the\nmain object of the analysis is the point in time within a song at which users\nstop listening and start streaming another song (\"skip\"). The main contribution\nof this study is the ascertainment of a correlation between the distribution in\ntime of skipping events and the musical structure of songs. It is also shown\nthat such distribution is not only specific to the individual songs, but also\nindependent of the cohort of users and, under stationary conditions, date of\nobservation. Finally, user behavioral data is used to train a predictor of the\nmusical structure of a song solely from its acoustic content; it is shown that\nthe use of such data, available in large quantities to music streaming\nservices, yields significant improvements in accuracy over the customary\nfashion of training this class of algorithms, in which only smaller amounts of\nhand-labeled data are available.", "published": "2019-03-12 08:35:49", "link": "http://arxiv.org/abs/1903.06008v1", "categories": ["cs.IR", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
