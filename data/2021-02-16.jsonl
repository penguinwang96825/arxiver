{"title": "Have Attention Heads in BERT Learned Constituency Grammar?", "abstract": "With the success of pre-trained language models in recent years, more and\nmore researchers focus on opening the \"black box\" of these models. Following\nthis interest, we carry out a qualitative and quantitative analysis of\nconstituency grammar in attention heads of BERT and RoBERTa. We employ the\nsyntactic distance method to extract implicit constituency grammar from the\nattention weights of each head. Our results show that there exist heads that\ncan induce some grammar types much better than baselines, suggesting that some\nheads act as a proxy for constituency grammar. We also analyze how attention\nheads' constituency grammar inducing (CGI) ability changes after fine-tuning\nwith two kinds of tasks, including sentence meaning similarity (SMS) tasks and\nnatural language inference (NLI) tasks. Our results suggest that SMS tasks\ndecrease the average CGI ability of upper layers, while NLI tasks increase it.\nLastly, we investigate the connections between CGI ability and natural language\nunderstanding ability on QQP and MNLI tasks.", "published": "2021-02-16 02:31:05", "link": "http://arxiv.org/abs/2102.07926v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FEWS: Large-Scale, Low-Shot Word Sense Disambiguation with the\n  Dictionary", "abstract": "Current models for Word Sense Disambiguation (WSD) struggle to disambiguate\nrare senses, despite reaching human performance on global WSD metrics. This\nstems from a lack of data for both modeling and evaluating rare senses in\nexisting WSD datasets. In this paper, we introduce FEWS (Few-shot Examples of\nWord Senses), a new low-shot WSD dataset automatically extracted from example\nsentences in Wiktionary. FEWS has high sense coverage across different natural\nlanguage domains and provides: (1) a large training set that covers many more\nsenses than previous datasets and (2) a comprehensive evaluation set containing\nfew- and zero-shot examples of a wide variety of senses. We establish baselines\non FEWS with knowledge-based and neural WSD approaches and present transfer\nlearning experiments demonstrating that models additionally trained with FEWS\nbetter capture rare senses in existing WSD datasets. Finally, we find humans\noutperform the best baseline models on FEWS, indicating that FEWS will support\nsignificant future work on low-shot WSD.", "published": "2021-02-16 07:13:34", "link": "http://arxiv.org/abs/2102.07983v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Non-Autoregressive Text Generation with Pre-trained Language Models", "abstract": "Non-autoregressive generation (NAG) has recently attracted great attention\ndue to its fast inference speed. However, the generation quality of existing\nNAG models still lags behind their autoregressive counterparts. In this work,\nwe show that BERT can be employed as the backbone of a NAG model to greatly\nimprove performance. Additionally, we devise mechanisms to alleviate the two\ncommon problems of vanilla NAG models: the inflexibility of prefixed output\nlength and the conditional independence of individual token predictions.\nLastly, to further increase the speed advantage of the proposed model, we\npropose a new decoding strategy, ratio-first, for applications where the output\nlengths can be approximately estimated beforehand. For a comprehensive\nevaluation, we test the proposed model on three text generation tasks,\nincluding text summarization, sentence compression and machine translation.\nExperimental results show that our model significantly outperforms existing\nnon-autoregressive baselines and achieves competitive performance with many\nstrong autoregressive models. In addition, we also conduct extensive analysis\nexperiments to reveal the effect of each proposed component.", "published": "2021-02-16 15:30:33", "link": "http://arxiv.org/abs/2102.08220v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NoiseQA: Challenge Set Evaluation for User-Centric Question Answering", "abstract": "When Question-Answering (QA) systems are deployed in the real world, users\nquery them through a variety of interfaces, such as speaking to voice\nassistants, typing questions into a search engine, or even translating\nquestions to languages supported by the QA system. While there has been\nsignificant community attention devoted to identifying correct answers in\npassages assuming a perfectly formed question, we show that components in the\npipeline that precede an answering engine can introduce varied and considerable\nsources of error, and performance can degrade substantially based on these\nupstream noise sources even for powerful pre-trained QA models. We conclude\nthat there is substantial room for progress before QA systems can be\neffectively deployed, highlight the need for QA evaluation to expand to\nconsider real-world use, and hope that our findings will spur greater community\ninterest in the issues that arise when our systems actually need to be of\nutility to humans.", "published": "2021-02-16 18:35:29", "link": "http://arxiv.org/abs/2102.08345v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Searching for Search Errors in Neural Morphological Inflection", "abstract": "Neural sequence-to-sequence models are currently the predominant choice for\nlanguage generation tasks. Yet, on word-level tasks, exact inference of these\nmodels reveals the empty string is often the global optimum. Prior works have\nspeculated this phenomenon is a result of the inadequacy of neural models for\nlanguage generation. However, in the case of morphological inflection, we find\nthat the empty string is almost never the most probable solution under the\nmodel. Further, greedy search often finds the global optimum. These\nobservations suggest that the poor calibration of many neural models may stem\nfrom characteristics of a specific subset of tasks rather than general\nill-suitedness of such models for language generation.", "published": "2021-02-16 19:42:42", "link": "http://arxiv.org/abs/2102.08424v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "User-Inspired Posterior Network for Recommendation Reason Generation", "abstract": "Recommendation reason generation, aiming at showing the selling points of\nproducts for customers, plays a vital role in attracting customers' attention\nas well as improving user experience. A simple and effective way is to extract\nkeywords directly from the knowledge-base of products, i.e., attributes or\ntitle, as the recommendation reason. However, generating recommendation reason\nfrom product knowledge doesn't naturally respond to users' interests.\nFortunately, on some E-commerce websites, there exists more and more\nuser-generated content (user-content for short), i.e., product\nquestion-answering (QA) discussions, which reflect user-cared aspects.\nTherefore, in this paper, we consider generating the recommendation reason by\ntaking into account not only the product attributes but also the\ncustomer-generated product QA discussions. In reality, adequate user-content is\nonly possible for the most popular commodities, whereas large sums of long-tail\nproducts or new products cannot gather a sufficient number of user-content. To\ntackle this problem, we propose a user-inspired multi-source posterior\ntransformer (MSPT), which induces the model reflecting the users' interests\nwith a posterior multiple QA discussions module, and generating recommendation\nreasons containing the product attributes as well as the user-cared aspects.\nExperimental results show that our model is superior to traditional generative\nmodels. Additionally, the analysis also shows that our model can focus more on\nthe user-cared aspects than baselines.", "published": "2021-02-16 02:08:52", "link": "http://arxiv.org/abs/2102.07919v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Hierarchical Transformer-based Large-Context End-to-end ASR with\n  Large-Context Knowledge Distillation", "abstract": "We present a novel large-context end-to-end automatic speech recognition\n(E2E-ASR) model and its effective training method based on knowledge\ndistillation. Common E2E-ASR models have mainly focused on utterance-level\nprocessing in which each utterance is independently transcribed. On the other\nhand, large-context E2E-ASR models, which take into account long-range\nsequential contexts beyond utterance boundaries, well handle a sequence of\nutterances such as discourses and conversations. However, the transformer\narchitecture, which has recently achieved state-of-the-art ASR performance\namong utterance-level ASR systems, has not yet been introduced into the\nlarge-context ASR systems. We can expect that the transformer architecture can\nbe leveraged for effectively capturing not only input speech contexts but also\nlong-range sequential contexts beyond utterance boundaries. Therefore, this\npaper proposes a hierarchical transformer-based large-context E2E-ASR model\nthat combines the transformer architecture with hierarchical encoder-decoder\nbased large-context modeling. In addition, in order to enable the proposed\nmodel to use long-range sequential contexts, we also propose a large-context\nknowledge distillation that distills the knowledge from a pre-trained\nlarge-context language model in the training phase. We evaluate the\neffectiveness of the proposed model and proposed training method on Japanese\ndiscourse ASR tasks.", "published": "2021-02-16 03:15:15", "link": "http://arxiv.org/abs/2102.07935v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring Transformers in Natural Language Generation: GPT, BERT, and\n  XLNet", "abstract": "Recent years have seen a proliferation of attention mechanisms and the rise\nof Transformers in Natural Language Generation (NLG). Previously,\nstate-of-the-art NLG architectures such as RNN and LSTM ran into vanishing\ngradient problems; as sentences grew larger, distance between positions\nremained linear, and sequential computation hindered parallelization since\nsentences were processed word by word. Transformers usher in a new era. In this\npaper, we explore three major Transformer-based models, namely GPT, BERT, and\nXLNet, that carry significant implications for the field. NLG is a burgeoning\narea that is now bolstered with rapid developments in attention mechanisms.\nFrom poetry generation to summarization, text generation derives benefit as\nTransformer-based language models achieve groundbreaking results.", "published": "2021-02-16 09:18:16", "link": "http://arxiv.org/abs/2102.08036v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large-Context Conversational Representation Learning: Self-Supervised\n  Learning for Conversational Documents", "abstract": "This paper presents a novel self-supervised learning method for handling\nconversational documents consisting of transcribed text of human-to-human\nconversations. One of the key technologies for understanding conversational\ndocuments is utterance-level sequential labeling, where labels are estimated\nfrom the documents in an utterance-by-utterance manner. The main issue with\nutterance-level sequential labeling is the difficulty of collecting labeled\nconversational documents, as manual annotations are very costly. To deal with\nthis issue, we propose large-context conversational representation learning\n(LC-CRL), a self-supervised learning method specialized for conversational\ndocuments. A self-supervised learning task in LC-CRL involves the estimation of\nan utterance using all the surrounding utterances based on large-context\nlanguage modeling. In this way, LC-CRL enables us to effectively utilize\nunlabeled conversational documents and thereby enhances the utterance-level\nsequential labeling. The results of experiments on scene segmentation tasks\nusing contact center conversational datasets demonstrate the effectiveness of\nthe proposed method.", "published": "2021-02-16 13:37:08", "link": "http://arxiv.org/abs/2102.08147v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "End-to-End Automatic Speech Recognition with Deep Mutual Learning", "abstract": "This paper is the first study to apply deep mutual learning (DML) to\nend-to-end ASR models. In DML, multiple models are trained simultaneously and\ncollaboratively by mimicking each other throughout the training process, which\nhelps to attain the global optimum and prevent models from making\nover-confident predictions. While previous studies applied DML to simple\nmulti-class classification problems, there are no studies that have used it on\nmore complex sequence-to-sequence mapping problems. For this reason, this paper\npresents a method to apply DML to state-of-the-art Transformer-based end-to-end\nASR models. In particular, we propose to combine DML with recent representative\ntraining techniques. i.e., label smoothing, scheduled sampling, and\nSpecAugment, each of which are essential for powerful end-to-end ASR models. We\nexpect that these training techniques work well with DML because DML has\ncomplementary characteristics. We experimented with two setups for Japanese ASR\ntasks: large-scale modeling and compact modeling. We demonstrate that DML\nimproves the ASR performance of both modeling setups compared with conventional\nlearning methods including knowledge distillation. We also show that combining\nDML with the existing training techniques effectively improves ASR performance.", "published": "2021-02-16 13:52:06", "link": "http://arxiv.org/abs/2102.08154v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Revisiting Language Encoding in Learning Multilingual Representations", "abstract": "Transformer has demonstrated its great power to learn contextual word\nrepresentations for multiple languages in a single model. To process\nmultilingual sentences in the model, a learnable vector is usually assigned to\neach language, which is called \"language embedding\". The language embedding can\nbe either added to the word embedding or attached at the beginning of the\nsentence. It serves as a language-specific signal for the Transformer to\ncapture contextual representations across languages. In this paper, we revisit\nthe use of language embedding and identify several problems in the existing\nformulations. By investigating the interaction between language embedding and\nword embedding in the self-attention module, we find that the current methods\ncannot reflect the language-specific word correlation well. Given these\nfindings, we propose a new approach called Cross-lingual Language Projection\n(XLP) to replace language embedding. For a sentence, XLP projects the word\nembeddings into language-specific semantic space, and then the projected\nembeddings will be fed into the Transformer model to process with their\nlanguage-specific meanings. In such a way, XLP achieves the purpose of\nappropriately encoding \"language\" in a multilingual Transformer model.\nExperimental results show that XLP can freely and significantly boost the model\nperformance on extensive multilingual benchmark datasets. Codes and models will\nbe released at https://github.com/lsj2408/XLP.", "published": "2021-02-16 18:47:10", "link": "http://arxiv.org/abs/2102.08357v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "COCO-LM: Correcting and Contrasting Text Sequences for Language Model\n  Pretraining", "abstract": "We present a self-supervised learning framework, COCO-LM, that pretrains\nLanguage Models by COrrecting and COntrasting corrupted text sequences.\nFollowing ELECTRA-style pretraining, COCO-LM employs an auxiliary language\nmodel to corrupt text sequences, upon which it constructs two new tasks for\npretraining the main model. The first token-level task, Corrective Language\nModeling, is to detect and correct tokens replaced by the auxiliary model, in\norder to better capture token-level semantics. The second sequence-level task,\nSequence Contrastive Learning, is to align text sequences originated from the\nsame source input while ensuring uniformity in the representation space.\nExperiments on GLUE and SQuAD demonstrate that COCO-LM not only outperforms\nrecent state-of-the-art pretrained models in accuracy, but also improves\npretraining efficiency. It achieves the MNLI accuracy of ELECTRA with 50% of\nits pretraining GPU hours. With the same pretraining steps of standard\nbase/large-sized models, COCO-LM outperforms the previous best models by 1+\nGLUE average points.", "published": "2021-02-16 22:24:29", "link": "http://arxiv.org/abs/2102.08473v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale\n  Language Models", "abstract": "Model parallelism has become a necessity for training modern large-scale deep\nlanguage models. In this work, we identify a new and orthogonal dimension from\nexisting model parallel approaches: it is possible to perform pipeline\nparallelism within a single training sequence for Transformer-based language\nmodels thanks to its autoregressive property. This enables a more fine-grained\npipeline compared with previous work. With this key idea, we design TeraPipe, a\nhigh-performance token-level pipeline parallel algorithm for synchronous\nmodel-parallel training of Transformer-based language models. We develop a\nnovel dynamic programming-based algorithm to calculate the optimal pipelining\nexecution scheme given a specific model and cluster configuration. We show that\nTeraPipe can speed up the training by 5.0x for the largest GPT-3 model with 175\nbillion parameters on an AWS cluster with 48 p3.16xlarge instances compared\nwith state-of-the-art model-parallel methods. The code for reproduction can be\nfound at https://github.com/zhuohan123/terapipe", "published": "2021-02-16 07:34:32", "link": "http://arxiv.org/abs/2102.07988v2", "categories": ["cs.LG", "cs.CL", "cs.DC"], "primary_category": "cs.LG"}
{"title": "Improving speech recognition models with small samples for air traffic\n  control systems", "abstract": "In the domain of air traffic control (ATC) systems, efforts to train a\npractical automatic speech recognition (ASR) model always faces the problem of\nsmall training samples since the collection and annotation of speech samples\nare expert- and domain-dependent task. In this work, a novel training approach\nbased on pretraining and transfer learning is proposed to address this issue,\nand an improved end-to-end deep learning model is developed to address the\nspecific challenges of ASR in the ATC domain. An unsupervised pretraining\nstrategy is first proposed to learn speech representations from unlabeled\nsamples for a certain dataset. Specifically, a masking strategy is applied to\nimprove the diversity of the sample without losing their general patterns.\nSubsequently, transfer learning is applied to fine-tune a pretrained or other\noptimized baseline models to finally achieves the supervised ASR task. By\nvirtue of the common terminology used in the ATC domain, the transfer learning\ntask can be regarded as a sub-domain adaption task, in which the transferred\nmodel is optimized using a joint corpus consisting of baseline samples and new\ntranscribed samples from the target dataset. This joint corpus construction\nstrategy enriches the size and diversity of the training samples, which is\nimportant for addressing the issue of the small transcribed corpus. In\naddition, speed perturbation is applied to augment the new transcribed samples\nto further improve the quality of the speech corpus. Three real ATC datasets\nare used to validate the proposed ASR model and training strategies. The\nexperimental results demonstrate that the ASR performance is significantly\nimproved on all three datasets, with an absolute character error rate only\none-third of that achieved through the supervised training. The applicability\nof the proposed strategies to other ASR approaches is also validated.", "published": "2021-02-16 08:28:52", "link": "http://arxiv.org/abs/2102.08015v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GradInit: Learning to Initialize Neural Networks for Stable and\n  Efficient Training", "abstract": "Innovations in neural architectures have fostered significant breakthroughs\nin language modeling and computer vision. Unfortunately, novel architectures\noften result in challenging hyper-parameter choices and training instability if\nthe network parameters are not properly initialized. A number of\narchitecture-specific initialization schemes have been proposed, but these\nschemes are not always portable to new architectures. This paper presents\nGradInit, an automated and architecture agnostic method for initializing neural\nnetworks. GradInit is based on a simple heuristic; the norm of each network\nlayer is adjusted so that a single step of SGD or Adam with prescribed\nhyperparameters results in the smallest possible loss value. This adjustment is\ndone by introducing a scalar multiplier variable in front of each parameter\nblock, and then optimizing these variables using a simple numerical scheme.\nGradInit accelerates the convergence and test performance of many convolutional\narchitectures, both with or without skip connections, and even without\nnormalization layers. It also improves the stability of the original\nTransformer architecture for machine translation, enabling training it without\nlearning rate warmup using either Adam or SGD under a wide range of learning\nrates and momentum coefficients. Code is available at\nhttps://github.com/zhuchen03/gradinit.", "published": "2021-02-16 11:45:35", "link": "http://arxiv.org/abs/2102.08098v3", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "IronMan: GNN-assisted Design Space Exploration in High-Level Synthesis\n  via Reinforcement Learning", "abstract": "Despite the great success of High-Level Synthesis (HLS) tools, we observe\nseveral unresolved challenges: 1) the high-level abstraction of programming\nstyles in HLS sometimes conceals optimization opportunities; 2) existing HLS\ntools do not provide flexible trade-off (Pareto) solutions among different\nobjectives and constraints; 3) the actual quality of the resulting RTL designs\nis hard to predict. To address these challenges, we propose an end-to-end\nframework, namelyIronMan. The primary goal is to enable a flexible and\nautomated design space exploration (DSE), to provide either optimal solutions\nunder user-specified constraints, or various trade-offs among different\nobjectives (such as different types of resources, area, and latency). Such DSE\neither requires tedious manual efforts or is not achievable to attain these\ngoals through existing HLS tools. There are three components in IronMan: 1)\nGPP, a highly accurate graph-neural-network-based performance and resource\npredictor; 2) RLMD, a reinforcement-learning-based multi-objective DSE engine\nthat explores the optimal resource allocation strategy, to provide Pareto\nsolutions between different objectives; 3) CT, a code transformer to assist\nRLMD and GPP, which extracts the data flow graph from original HLS C/C++ and\nautomatically generates synthesizable code with HLS directives. The\nexperimental results show that: 1) GPP achieves high prediction accuracy,\nreducing prediction errors of HLS tools by 10.9x in resource utilization and\n5.7x in timing; 2) RLMD obtains optimal or Pareto solutions that outperform the\ngenetic algorithm and simulated annealing by 12.7% and 12.9%, respectively; 3)\nIronMan is able to find optimized solutions perfectly matching various DSP\nconstraints, with 2.54x fewer DSPs and up to 6x shorter latency than those of\nHLS tools while being up to 400x faster than the heuristic algorithms and HLS\ntools.", "published": "2021-02-16 13:22:00", "link": "http://arxiv.org/abs/2102.08138v2", "categories": ["cs.AR", "cs.CL", "cs.LG"], "primary_category": "cs.AR"}
{"title": "A Cooperative Memory Network for Personalized Task-oriented Dialogue\n  Systems with Incomplete User Profiles", "abstract": "There is increasing interest in developing personalized Task-oriented\nDialogue Systems (TDSs). Previous work on personalized TDSs often assumes that\ncomplete user profiles are available for most or even all users. This is\nunrealistic because (1) not everyone is willing to expose their profiles due to\nprivacy concerns; and (2) rich user profiles may involve a large number of\nattributes (e.g., gender, age, tastes, . . .). In this paper, we study\npersonalized TDSs without assuming that user profiles are complete. We propose\na Cooperative Memory Network (CoMemNN) that has a novel mechanism to gradually\nenrich user profiles as dialogues progress and to simultaneously improve\nresponse selection based on the enriched profiles. CoMemNN consists of two core\nmodules: User Profile Enrichment (UPE) and Dialogue Response Selection (DRS).\nThe former enriches incomplete user profiles by utilizing collaborative\ninformation from neighbor users as well as current dialogues. The latter uses\nthe enriched profiles to update the current user query so as to encode more\nuseful information, based on which a personalized response to a user request is\nselected.\n  We conduct extensive experiments on the personalized bAbI dialogue benchmark\ndatasets. We find that CoMemNN is able to enrich user profiles effectively,\nwhich results in an improvement of 3.06% in terms of response selection\naccuracy compared to state-of-the-art methods. We also test the robustness of\nCoMemNN against incompleteness of user profiles by randomly discarding\nattribute values from user profiles. Even when discarding 50% of the attribute\nvalues, CoMemNN is able to match the performance of the best performing\nbaseline without discarding user profiles, showing the robustness of CoMemNN.", "published": "2021-02-16 18:05:54", "link": "http://arxiv.org/abs/2102.08322v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Boosting Low-Resource Biomedical QA via Entity-Aware Masking Strategies", "abstract": "Biomedical question-answering (QA) has gained increased attention for its\ncapability to provide users with high-quality information from a vast\nscientific literature. Although an increasing number of biomedical QA datasets\nhas been recently made available, those resources are still rather limited and\nexpensive to produce. Transfer learning via pre-trained language models (LMs)\nhas been shown as a promising approach to leverage existing general-purpose\nknowledge. However, finetuning these large models can be costly and time\nconsuming, often yielding limited benefits when adapting to specific themes of\nspecialised domains, such as the COVID-19 literature. To bootstrap further\ntheir domain adaptation, we propose a simple yet unexplored approach, which we\ncall biomedical entity-aware masking (BEM). We encourage masked language models\nto learn entity-centric knowledge based on the pivotal entities characterizing\nthe domain at hand, and employ those entities to drive the LM fine-tuning. The\nresulting strategy is a downstream process applicable to a wide variety of\nmasked LMs, not requiring additional memory or components in the neural\narchitectures. Experimental results show performance on par with\nstate-of-the-art models on several biomedical QA datasets.", "published": "2021-02-16 18:51:13", "link": "http://arxiv.org/abs/2102.08366v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Conversations Gone Alright: Quantifying and Predicting Prosocial\n  Outcomes in Online Conversations", "abstract": "Online conversations can go in many directions: some turn out poorly due to\nantisocial behavior, while others turn out positively to the benefit of all.\nResearch on improving online spaces has focused primarily on detecting and\nreducing antisocial behavior. Yet we know little about positive outcomes in\nonline conversations and how to increase them-is a prosocial outcome simply the\nlack of antisocial behavior or something more? Here, we examine how\nconversational features lead to prosocial outcomes within online discussions.\nWe introduce a series of new theory-inspired metrics to define prosocial\noutcomes such as mentoring and esteem enhancement. Using a corpus of 26M Reddit\nconversations, we show that these outcomes can be forecasted from the initial\ncomment of an online conversation, with the best model providing a relative 24%\nimprovement over human forecasting performance at ranking conversations for\npredicted outcome. Our results indicate that platforms can use these early cues\nin their algorithmic ranking of early conversations to prioritize better\noutcomes.", "published": "2021-02-16 18:53:41", "link": "http://arxiv.org/abs/2102.08368v1", "categories": ["cs.CY", "cs.CL", "cs.SI"], "primary_category": "cs.CY"}
{"title": "Semi-Supervised Singing Voice Separation with Noisy Self-Training", "abstract": "Recent progress in singing voice separation has primarily focused on\nsupervised deep learning methods. However, the scarcity of ground-truth data\nwith clean musical sources has been a problem for long. Given a limited set of\nlabeled data, we present a method to leverage a large volume of unlabeled data\nto improve the model's performance. Following the noisy self-training\nframework, we first train a teacher network on the small labeled dataset and\ninfer pseudo-labels from the large corpus of unlabeled mixtures. Then, a larger\nstudent network is trained on combined ground-truth and self-labeled datasets.\nEmpirical results show that the proposed self-training scheme, along with data\naugmentation methods, effectively leverage the large unlabeled corpus and\nobtain superior performance compared to supervised methods.", "published": "2021-02-16 05:06:09", "link": "http://arxiv.org/abs/2102.07961v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Deep Learning based Multi-Source Localization with Source Splitting and\n  its Effectiveness in Multi-Talker Speech Recognition", "abstract": "Multi-source localization is an important and challenging technique for\nmulti-talker conversation analysis. This paper proposes a novel supervised\nlearning method using deep neural networks to estimate the direction of arrival\n(DOA) of all the speakers simultaneously from the audio mixture. At the heart\nof the proposal is a source splitting mechanism that creates source-specific\nintermediate representations inside the network. This allows our model to give\nsource-specific posteriors as the output unlike the traditional multi-label\nclassification approach. Existing deep learning methods perform a frame level\nprediction, whereas our approach performs an utterance level prediction by\nincorporating temporal selection and averaging inside the network to avoid\npost-processing. We also experiment with various loss functions and show that a\nvariant of earth mover distance (EMD) is very effective in classifying DOA at a\nvery high resolution by modeling inter-class relationships. In addition to\nusing the prediction error as a metric for evaluating our localization model,\nwe also establish its potency as a frontend with automatic speech recognition\n(ASR) as the downstream task. We convert the estimated DOAs into a feature\nsuitable for ASR and pass it as an additional input feature to a strong\nmulti-channel and multi-talker speech recognition baseline. This added input\nfeature drastically improves the ASR performance and gives a word error rate\n(WER) of 6.3% on the evaluation data of our simulated noisy two speaker\nmixtures, while the baseline which doesn't use explicit localization input has\na WER of 11.5%. We also perform ASR evaluation on real recordings with the\noverlapped set of the MC-WSJ-AV corpus in addition to simulated mixtures.", "published": "2021-02-16 04:27:19", "link": "http://arxiv.org/abs/2102.07955v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Voice Gender Scoring and Independent Acoustic Characterization of\n  Perceived Masculinity and Femininity", "abstract": "Previous research has found that voices can provide reliable information to\nbe used for gender classification with a high level of accuracy. In social\npsychology, perceived masculinity and femininity (masculinity and femininity\nrated by humans) has often been considered an important feature when\ninvestigating the influence of vocal features on social behaviours. While\nprevious studies have characterised the acoustic features that contributed to\nperceivers' judgements of speakers' masculinity or femininity, there is limited\nresearch on developing a machine masculinity/femininity scoring model and\ncharacterizing the independent acoustic factors that contribute to perceivers'\nmasculinity and femininity judgements. In this work, we first propose a machine\nscoring model of perceived masculinity/femininity based on the Extreme Random\nForest and then characterize the independent and meaningful acoustic factors\nthat contribute to perceivers' judgements by using a correlation matrix based\nhierarchical clustering method. Our results show that the machine ratings of\nmasculinity and femininity strongly correlated with the human ratings of\nmasculinity and femininity when we used an optimal speech duration of 7\nseconds, with a correlation coefficient of up to .63 for females and .77 for\nmales. Nine independent clusters of acoustic measures were generated from our\nmodelling of femininity judgements for female voices and eight clusters were\nfound for masculinity judgements for male voices. The results revealed that,\nfor both genders, the F0 mean is the most important acoustic measure affecting\nthe judgement of acoustic-related masculinity and femininity. The F3 mean, F4\nmean and VTL estimators were found to be highly inter-correlated and appeared\nin the same cluster, forming the second most significant factor in influencing\nthe assessment of acoustic-related masculinity and femininity.", "published": "2021-02-16 07:10:42", "link": "http://arxiv.org/abs/2102.07982v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A multispeaker dataset of raw and reconstructed speech production\n  real-time MRI video and 3D volumetric images", "abstract": "Real-time magnetic resonance imaging (RT-MRI) of human speech production is\nenabling significant advances in speech science, linguistics, bio-inspired\nspeech technology development, and clinical applications. Easy access to RT-MRI\nis however limited, and comprehensive datasets with broad access are needed to\ncatalyze research across numerous domains. The imaging of the rapidly moving\narticulators and dynamic airway shaping during speech demands high\nspatio-temporal resolution and robust reconstruction methods. Further, while\nreconstructed images have been published, to-date there is no open dataset\nproviding raw multi-coil RT-MRI data from an optimized speech production\nexperimental setup. Such datasets could enable new and improved methods for\ndynamic image reconstruction, artifact correction, feature extraction, and\ndirect extraction of linguistically-relevant biomarkers. The present dataset\noffers a unique corpus of 2D sagittal-view RT-MRI videos along with\nsynchronized audio for 75 subjects performing linguistically motivated speech\ntasks, alongside the corresponding first-ever public domain raw RT-MRI data.\nThe dataset also includes 3D volumetric vocal tract MRI during sustained speech\nsounds and high-resolution static anatomical T2-weighted upper airway MRI for\neach subject.", "published": "2021-02-16 00:16:33", "link": "http://arxiv.org/abs/2102.07896v1", "categories": ["eess.SP", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "eess.SP"}
{"title": "Through-the-Wall Radar under Electromagnetic Complex Wall: A Deep\n  Learning Approach", "abstract": "This paper employed deep learning to do two-dimensional, multi-target\nlocating in Through-the-Wall Radar under conditions where the wall is treated\nas a complex electromagnetic medium. We made five assumptions about the wall\nand two about the number of targets. There are two target modes available:\nsingle target and double targets. The wall scenarios include a homogeneous\nwall, a wall with an air gap, an inhomogeneous wall, an anisotropic wall, and\nan inhomogeneous-anisotropic wall. Target locating is accomplished through the\nuse of a deep neural network technique. We constructed a dataset using the\nPython FDTD module and then modeled it using deep learning. Assuming the wall\nis a complex electromagnetic medium, we achieved 97.7% accuracy for\nsingle-target 2D locating and 94.1% accuracy for two-target locating.\nAdditionally, we noticed a loss of 10% to 20% inaccuracy when noise was added\nat low SNRs, although this decrease dropped to less than 10% at high SNRs.", "published": "2021-02-16 07:36:08", "link": "http://arxiv.org/abs/2102.07990v2", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Semi Supervised Learning For Few-shot Audio Classification By Episodic\n  Triplet Mining", "abstract": "Few-shot learning aims to generalize unseen classes that appear during\ntesting but are unavailable during training. Prototypical networks incorporate\nfew-shot metric learning, by constructing a class prototype in the form of a\nmean vector of the embedded support points within a class. The performance of\nprototypical networks in extreme few-shot scenarios (like one-shot) degrades\ndrastically, mainly due to the desuetude of variations within the clusters\nwhile constructing prototypes. In this paper, we propose to replace the typical\nprototypical loss function with an Episodic Triplet Mining (ETM) technique. The\nconventional triplet selection leads to overfitting, because of all possible\ncombinations being used during training. We incorporate episodic training for\nmining the semi hard positive and the semi hard negative triplets to overcome\nthe overfitting. We also propose an adaptation to make use of unlabeled\ntraining samples for better modeling. Experimenting on two different audio\nprocessing tasks, namely speaker recognition and audio event detection; show\nimproved performances and hence the efficacy of ETM over the prototypical loss\nfunction and other meta-learning frameworks. Further, we show improved\nperformances when unlabeled training samples are used.", "published": "2021-02-16 10:55:31", "link": "http://arxiv.org/abs/2102.08074v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Axial Residual Networks for CycleGAN-based Voice Conversion", "abstract": "We propose a novel architecture and improved training objectives for\nnon-parallel voice conversion. Our proposed CycleGAN-based model performs a\nshape-preserving transformation directly on a high frequency-resolution\nmagnitude spectrogram, converting its style (i.e. speaker identity) while\npreserving the speech content. Throughout the entire conversion process, the\nmodel does not resort to compressed intermediate representations of any sort\n(e.g. mel spectrogram, low resolution spectrogram, decomposed network feature).\nWe propose an efficient axial residual block architecture to support this\nexpensive procedure and various modifications to the CycleGAN losses to\nstabilize the training process. We demonstrate via experiments that our\nproposed model outperforms Scyclone and shows a comparable or better\nperformance to that of CycleGAN-VC2 even without employing a neural vocoder.", "published": "2021-02-16 10:55:35", "link": "http://arxiv.org/abs/2102.08075v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Comparison of semi-supervised deep learning algorithms for audio\n  classification", "abstract": "In this article, we adapted five recent SSL methods to the task of audio\nclassification. The first two methods, namely Deep Co-Training (DCT) and Mean\nTeacher (MT), involve two collaborative neural networks. The three other\nalgorithms, called MixMatch (MM), ReMixMatch (RMM), and FixMatch (FM), are\nsingle-model methods that rely primarily on data augmentation strategies. Using\nthe Wide-ResNet-28-2 architecture in all our experiments, 10% of labeled data\nand the remaining 90% as unlabeled data for training, we first compare the\nerror rates of the five methods on three standard benchmark audio datasets:\nEnvironmental Sound Classification (ESC-10), UrbanSound8K (UBS8K), and Google\nSpeech Commands (GSC). In all but one cases, MM, RMM, and FM outperformed MT\nand DCT significantly, MM and RMM being the best methods in most experiments.\nOn UBS8K and GSC, MM achieved 18.02% and 3.25% error rate (ER), respectively,\noutperforming models trained with 100% of the available labeled data, which\nreached 23.29% and 4.94%, respectively. RMM achieved the best results on ESC-10\n(12.00% ER), followed by FM which reached 13.33%. Second, we explored adding\nthe mixup augmentation, used in MM and RMM, to DCT, MT, and FM. In almost all\ncases, mixup brought consistent gains. For instance, on GSC, FM reached 4.44%\nand 3.31% ER without and with mixup. Our PyTorch code will be made available\nupon paper acceptance at https:// github. com/ Labbe ti/ SSLH.", "published": "2021-02-16 14:33:05", "link": "http://arxiv.org/abs/2102.08183v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Context-Aware Prosody Correction for Text-Based Speech Editing", "abstract": "Text-based speech editors expedite the process of editing speech recordings\nby permitting editing via intuitive cut, copy, and paste operations on a speech\ntranscript. A major drawback of current systems, however, is that edited\nrecordings often sound unnatural because of prosody mismatches around edited\nregions. In our work, we propose a new context-aware method for more natural\nsounding text-based editing of speech. To do so, we 1) use a series of neural\nnetworks to generate salient prosody features that are dependent on the prosody\nof speech surrounding the edit and amenable to fine-grained user control 2) use\nthe generated features to control a standard pitch-shift and time-stretch\nmethod and 3) apply a denoising neural network to remove artifacts induced by\nthe signal manipulation to yield a high-fidelity result. We evaluate our\napproach using a subjective listening test, provide a detailed comparative\nanalysis, and conclude several interesting insights.", "published": "2021-02-16 18:16:30", "link": "http://arxiv.org/abs/2102.08328v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
