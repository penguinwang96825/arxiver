{"title": "Learning Transductions and Alignments with RNN Seq2seq Models", "abstract": "The paper studies the capabilities of Recurrent-Neural-Network sequence to\nsequence (RNN seq2seq) models in learning four transduction tasks: identity,\nreversal, total reduplication, and quadratic copying. These transductions are\ntraditionally well studied under finite state transducers and attributed with\nincreasing complexity. We find that RNN seq2seq models are only able to\napproximate a mapping that fits the training or in-distribution data, instead\nof learning the underlying functions. Although attention makes learning more\nefficient and robust, it does not overcome the out-of-distribution\ngeneralization limitation. We establish a novel complexity hierarchy for\nlearning the four tasks for attention-less RNN seq2seq models, which may be\nunderstood in terms of the complexity hierarchy of formal languages, instead of\nstring transductions. RNN variants also play a role in the results. In\nparticular, we show that Simple RNN seq2seq models cannot count the input\nlength.", "published": "2023-03-13 04:15:33", "link": "http://arxiv.org/abs/2303.06841v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Human Subject Study of Named Entity Recognition (NER) in\n  Conversational Music Recommendation Queries", "abstract": "We conducted a human subject study of named entity recognition on a noisy\ncorpus of conversational music recommendation queries, with many irregular and\nnovel named entities. We evaluated the human NER linguistic behaviour in these\nchallenging conditions and compared it with the most common NER systems\nnowadays, fine-tuned transformers. Our goal was to learn about the task to\nguide the design of better evaluation methods and NER algorithms. The results\nshowed that NER in our context was quite hard for both human and algorithms\nunder a strict evaluation schema; humans had higher precision, while the model\nhigher recall because of entity exposure especially during pre-training; and\nentity types had different error patterns (e.g. frequent typing errors for\nartists). The released corpus goes beyond predefined frames of interaction and\ncan support future work in conversational music recommendation.", "published": "2023-03-13 09:22:48", "link": "http://arxiv.org/abs/2303.06944v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating multiple-choice questions for medical question answering with\n  distractors and cue-masking", "abstract": "Medical multiple-choice question answering (MCQA) is particularly difficult.\nQuestions may describe patient symptoms and ask for the correct diagnosis,\nwhich requires domain knowledge and complex reasoning. Standard language\nmodeling pretraining alone is not sufficient to achieve the best results.\n\\citet{jin2020disease} showed that focusing masked language modeling on disease\nname prediction when using medical encyclopedic paragraphs as input leads to\nconsiderable MCQA accuracy improvement. In this work, we show that (1)\nfine-tuning on generated MCQA dataset outperforms the masked language modeling\nbased objective and (2) correctly masking the cues to the answers is critical\nfor good performance. We release new pretraining datasets and achieve\nstate-of-the-art results on 4 MCQA datasets, notably +5.7\\% with base-size\nmodel on MedQA-USMLE.", "published": "2023-03-13 12:45:01", "link": "http://arxiv.org/abs/2303.07069v1", "categories": ["cs.CL", "H.4; H.5; I.2"], "primary_category": "cs.CL"}
{"title": "Large Language Models in the Workplace: A Case Study on Prompt\n  Engineering for Job Type Classification", "abstract": "This case study investigates the task of job classification in a real-world\nsetting, where the goal is to determine whether an English-language job posting\nis appropriate for a graduate or entry-level position. We explore multiple\napproaches to text classification, including supervised approaches such as\ntraditional models like Support Vector Machines (SVMs) and state-of-the-art\ndeep learning methods such as DeBERTa. We compare them with Large Language\nModels (LLMs) used in both few-shot and zero-shot classification settings. To\naccomplish this task, we employ prompt engineering, a technique that involves\ndesigning prompts to guide the LLMs towards the desired output. Specifically,\nwe evaluate the performance of two commercially available state-of-the-art\nGPT-3.5-based language models, text-davinci-003 and gpt-3.5-turbo. We also\nconduct a detailed analysis of the impact of different aspects of prompt\nengineering on the model's performance. Our results show that, with a\nwell-designed prompt, a zero-shot gpt-3.5-turbo classifier outperforms all\nother models, achieving a 6% increase in Precision@95% Recall compared to the\nbest supervised approach. Furthermore, we observe that the wording of the\nprompt is a critical factor in eliciting the appropriate \"reasoning\" in the\nmodel, and that seemingly minor aspects of the prompt significantly affect the\nmodel's performance.", "published": "2023-03-13 14:09:53", "link": "http://arxiv.org/abs/2303.07142v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformer-based approaches to Sentiment Detection", "abstract": "The use of transfer learning methods is largely responsible for the present\nbreakthrough in Natural Learning Processing (NLP) tasks across multiple\ndomains. In order to solve the problem of sentiment detection, we examined the\nperformance of four different types of well-known state-of-the-art transformer\nmodels for text classification. Models such as Bidirectional Encoder\nRepresentations from Transformers (BERT), Robustly Optimized BERT Pre-training\nApproach (RoBERTa), a distilled version of BERT (DistilBERT), and a large\nbidirectional neural network architecture (XLNet) were proposed. The\nperformance of the four models that were used to detect disaster in the text\nwas compared. All the models performed well enough, indicating that\ntransformer-based models are suitable for the detection of disaster in text.\nThe RoBERTa transformer model performs best on the test dataset with a score of\n82.6% and is highly recommended for quality predictions. Furthermore, we\ndiscovered that the learning algorithms' performance was influenced by the\npre-processing techniques, the nature of words in the vocabulary, unbalanced\nlabeling, and the model parameters.", "published": "2023-03-13 17:12:03", "link": "http://arxiv.org/abs/2303.07292v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MetaTroll: Few-shot Detection of State-Sponsored Trolls with Transformer\n  Adapters", "abstract": "State-sponsored trolls are the main actors of influence campaigns on social\nmedia and automatic troll detection is important to combat misinformation at\nscale. Existing troll detection models are developed based on training data for\nknown campaigns (e.g.\\ the influence campaign by Russia's Internet Research\nAgency on the 2016 US Election), and they fall short when dealing with {\\em\nnovel} campaigns with new targets. We propose MetaTroll, a text-based troll\ndetection model based on the meta-learning framework that enables high\nportability and parameter-efficient adaptation to new campaigns using only a\nhandful of labelled samples for few-shot transfer. We introduce\n\\textit{campaign-specific} transformer adapters to MetaTroll to ``memorise''\ncampaign-specific knowledge so as to tackle catastrophic forgetting, where a\nmodel ``forgets'' how to detect trolls from older campaigns due to continual\nadaptation. Our experiments demonstrate that MetaTroll substantially\noutperforms baselines and state-of-the-art few-shot text classification models.\nLastly, we explore simple approaches to extend MetaTroll to multilingual and\nmultimodal detection. Source code for MetaTroll is available at:\nhttps://github.com/ltian678/metatroll-code.git.", "published": "2023-03-13 06:39:38", "link": "http://arxiv.org/abs/2303.07354v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rediscovering Hashed Random Projections for Efficient Quantization of\n  Contextualized Sentence Embeddings", "abstract": "Training and inference on edge devices often requires an efficient setup due\nto computational limitations. While pre-computing data representations and\ncaching them on a server can mitigate extensive edge device computation, this\nleads to two challenges. First, the amount of storage required on the server\nthat scales linearly with the number of instances. Second, the bandwidth\nrequired to send extensively large amounts of data to an edge device. To reduce\nthe memory footprint of pre-computed data representations, we propose a simple,\nyet effective approach that uses randomly initialized hyperplane projections.\nTo further reduce their size by up to 98.96%, we quantize the resulting\nfloating-point representations into binary vectors. Despite the greatly reduced\nsize, we show that the embeddings remain effective for training models across\nvarious English and German sentence classification tasks that retain 94%--99%\nof their floating-point.", "published": "2023-03-13 10:53:00", "link": "http://arxiv.org/abs/2304.02481v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Single Items: Exploring User Preferences in Item Sets with the\n  Conversational Playlist Curation Dataset", "abstract": "Users in consumption domains, like music, are often able to more efficiently\nprovide preferences over a set of items (e.g. a playlist or radio) than over\nsingle items (e.g. songs). Unfortunately, this is an underexplored area of\nresearch, with most existing recommendation systems limited to understanding\npreferences over single items. Curating an item set exponentiates the search\nspace that recommender systems must consider (all subsets of items!): this\nmotivates conversational approaches-where users explicitly state or refine\ntheir preferences and systems elicit preferences in natural language-as an\nefficient way to understand user needs. We call this task conversational item\nset curation and present a novel data collection methodology that efficiently\ncollects realistic preferences about item sets in a conversational setting by\nobserving both item-level and set-level feedback. We apply this methodology to\nmusic recommendation to build the Conversational Playlist Curation Dataset\n(CPCD), where we show that it leads raters to express preferences that would\nnot be otherwise expressed. Finally, we propose a wide range of conversational\nretrieval models as baselines for this task and evaluate them on the dataset.", "published": "2023-03-13 00:39:04", "link": "http://arxiv.org/abs/2303.06791v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "The System Description of dun_oscar team for The ICPR MSR Challenge", "abstract": "This paper introduces the system submitted by dun_oscar team for the ICPR MSR\nChallenge. Three subsystems for task1-task3 are descripted respectively. In\ntask1, we develop a visual system which includes a OCR model, a text tracker,\nand a NLP classifier for distinguishing subtitles and non-subtitles. In task2,\nwe employ an ASR system which includes an AM with 18 layers and a 4-gram LM.\nSemi-supervised learning on unlabeled data is also vital. In task3, we employ\nthe ASR system to improve the visual system, some false subtitles can be\ncorrected by a fusion module.", "published": "2023-03-13 05:53:42", "link": "http://arxiv.org/abs/2303.06878v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Addressing Biases in the Texts using an End-to-End Pipeline Approach", "abstract": "The concept of fairness is gaining popularity in academia and industry.\nSocial media is especially vulnerable to media biases and toxic language and\ncomments. We propose a fair ML pipeline that takes a text as input and\ndetermines whether it contains biases and toxic content. Then, based on\npre-trained word embeddings, it suggests a set of new words by substituting the\nbi-ased words, the idea is to lessen the effects of those biases by replacing\nthem with alternative words. We compare our approach to existing fairness\nmodels to determine its effectiveness. The results show that our proposed\npipeline can de-tect, identify, and mitigate biases in social media data", "published": "2023-03-13 11:41:28", "link": "http://arxiv.org/abs/2303.07024v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Empirical Evaluation of Existing Word Embedding\n  Approaches", "abstract": "Vector-based word representations help countless Natural Language Processing\n(NLP) tasks capture the language's semantic and syntactic regularities. In this\npaper, we present the characteristics of existing word embedding approaches and\nanalyze them with regard to many classification tasks. We categorize the\nmethods into two main groups - Traditional approaches mostly use matrix\nfactorization to produce word representations, and they are not able to capture\nthe semantic and syntactic regularities of the language very well. On the other\nhand, Neural-network-based approaches can capture sophisticated regularities of\nthe language and preserve the word relationships in the generated word\nrepresentations. We report experimental results on multiple classification\ntasks and highlight the scenarios where one approach performs better than the\nrest.", "published": "2023-03-13 15:34:19", "link": "http://arxiv.org/abs/2303.07196v2", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Scaling Vision-Language Models with Sparse Mixture of Experts", "abstract": "The field of natural language processing (NLP) has made significant strides\nin recent years, particularly in the development of large-scale vision-language\nmodels (VLMs). These models aim to bridge the gap between text and visual\ninformation, enabling a more comprehensive understanding of multimedia data.\nHowever, as these models become larger and more complex, they also become more\nchallenging to train and deploy. One approach to addressing this challenge is\nthe use of sparsely-gated mixture-of-experts (MoE) techniques, which divide the\nmodel into smaller, specialized sub-models that can jointly solve a task. In\nthis paper, we explore the effectiveness of MoE in scaling vision-language\nmodels, demonstrating its potential to achieve state-of-the-art performance on\na range of benchmarks over dense models of equivalent computational cost. Our\nresearch offers valuable insights into stabilizing the training of MoE models,\nunderstanding the impact of MoE on model interpretability, and balancing the\ntrade-offs between compute performance when scaling VLMs. We hope our work will\ninspire further research into the use of MoE for scaling large-scale\nvision-language models and other multimodal machine learning applications.", "published": "2023-03-13 16:00:31", "link": "http://arxiv.org/abs/2303.07226v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Are Models Trained on Indian Legal Data Fair?", "abstract": "Recent advances and applications of language technology and artificial\nintelligence have enabled much success across multiple domains like law,\nmedical and mental health. AI-based Language Models, like Judgement Prediction,\nhave recently been proposed for the legal sector. However, these models are\nstrife with encoded social biases picked up from the training data. While bias\nand fairness have been studied across NLP, most studies primarily locate\nthemselves within a Western context. In this work, we present an initial\ninvestigation of fairness from the Indian perspective in the legal domain. We\nhighlight the propagation of learnt algorithmic biases in the bail prediction\ntask for models trained on Hindi legal documents. We evaluate the fairness gap\nusing demographic parity and show that a decision tree model trained for the\nbail prediction task has an overall fairness disparity of 0.237 between input\nfeatures associated with Hindus and Muslims. Additionally, we highlight the\nneed for further research and studies in the avenues of fairness/bias in\napplying AI in the legal sector with a specific focus on the Indian context.", "published": "2023-03-13 16:20:33", "link": "http://arxiv.org/abs/2303.07247v3", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Meet in the Middle: A New Pre-training Paradigm", "abstract": "Most language models (LMs) are trained and applied in an autoregressive\nleft-to-right fashion, assuming that the next token only depends on the\npreceding ones. However, this assumption ignores the potential benefits of\nusing the full sequence information during training, and the possibility of\nhaving context from both sides during inference. In this paper, we propose a\nnew pre-training paradigm with techniques that jointly improve the training\ndata efficiency and the capabilities of the LMs in the infilling task. The\nfirst is a training objective that aligns the predictions of a left-to-right LM\nwith those of a right-to-left LM, trained on the same data but in reverse\norder. The second is a bidirectional inference procedure that enables both LMs\nto meet in the middle. We show the effectiveness of our pre-training paradigm\nwith extensive experiments on both programming and natural language models,\noutperforming strong baselines.", "published": "2023-03-13 17:17:11", "link": "http://arxiv.org/abs/2303.07295v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Model-tuning Via Prompts Makes NLP Models Adversarially Robust", "abstract": "In recent years, NLP practitioners have converged on the following practice:\n(i) import an off-the-shelf pretrained (masked) language model; (ii) append a\nmultilayer perceptron atop the CLS token's hidden representation (with randomly\ninitialized weights); and (iii) fine-tune the entire model on a downstream task\n(MLP-FT). This procedure has produced massive gains on standard NLP benchmarks,\nbut these models remain brittle, even to mild adversarial perturbations. In\nthis work, we demonstrate surprising gains in adversarial robustness enjoyed by\nModel-tuning Via Prompts (MVP), an alternative method of adapting to downstream\ntasks. Rather than appending an MLP head to make output prediction, MVP appends\na prompt template to the input, and makes prediction via text\ninfilling/completion. Across 5 NLP datasets, 4 adversarial attacks, and 3\ndifferent models, MVP improves performance against adversarial substitutions by\nan average of 8% over standard methods and even outperforms adversarial\ntraining-based state-of-art defenses by 3.5%. By combining MVP with adversarial\ntraining, we achieve further improvements in adversarial robustness while\nmaintaining performance on unperturbed examples. Finally, we conduct ablations\nto investigate the mechanism underlying these gains. Notably, we find that the\nmain causes of vulnerability of MLP-FT can be attributed to the misalignment\nbetween pre-training and fine-tuning tasks, and the randomly initialized MLP\nparameters.", "published": "2023-03-13 17:41:57", "link": "http://arxiv.org/abs/2303.07320v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AMOM: Adaptive Masking over Masking for Conditional Masked Language\n  Model", "abstract": "Transformer-based autoregressive (AR) methods have achieved appealing\nperformance for varied sequence-to-sequence generation tasks, e.g., neural\nmachine translation, summarization, and code generation, but suffer from low\ninference efficiency. To speed up the inference stage, many non-autoregressive\n(NAR) strategies have been proposed in the past few years. Among them, the\nconditional masked language model (CMLM) is one of the most versatile\nframeworks, as it can support many different sequence generation scenarios and\nachieve very competitive performance on these tasks. In this paper, we further\nintroduce a simple yet effective adaptive masking over masking strategy to\nenhance the refinement capability of the decoder and make the encoder\noptimization easier. Experiments on \\textbf{3} different tasks (neural machine\ntranslation, summarization, and code generation) with \\textbf{15} datasets in\ntotal confirm that our proposed simple method achieves significant performance\nimprovement over the strong CMLM model. Surprisingly, our proposed model yields\nstate-of-the-art performance on neural machine translation (\\textbf{34.62} BLEU\non WMT16 EN$\\to$RO, \\textbf{34.82} BLEU on WMT16 RO$\\to$EN, and \\textbf{34.84}\nBLEU on IWSLT De$\\to$En) and even better performance than the \\textbf{AR}\nTransformer on \\textbf{7} benchmark datasets with at least \\textbf{2.2$\\times$}\nspeedup. Our code is available at GitHub.", "published": "2023-03-13 20:34:56", "link": "http://arxiv.org/abs/2303.07457v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Architext: Language-Driven Generative Architecture Design", "abstract": "Architectural design is a highly complex practice that involves a wide\ndiversity of disciplines, technologies, proprietary design software, expertise,\nand an almost infinite number of constraints, across a vast array of design\ntasks. Enabling intuitive, accessible, and scalable design processes is an\nimportant step towards performance-driven and sustainable design for all. To\nthat end, we introduce Architext, a novel semantic generation assistive tool.\nArchitext enables design generation with only natural language prompts, given\nto large-scale Language Models, as input. We conduct a thorough quantitative\nevaluation of Architext's downstream task performance, focusing on semantic\naccuracy and diversity for a number of pre-trained language models ranging from\n120 million to 6 billion parameters. Architext models are able to learn the\nspecific design task, generating valid residential layouts at a near 100% rate.\nAccuracy shows great improvement when scaling the models, with the largest\nmodel (GPT-J) yielding impressive accuracy ranging between 25% to over 80% for\ndifferent prompt categories. We open source the finetuned Architext models and\nour synthetic dataset, hoping to inspire experimentation in this exciting area\nof design research.", "published": "2023-03-13 23:11:05", "link": "http://arxiv.org/abs/2303.07519v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Minimizing Fuzzy Interpretations in Fuzzy Description Logics by Using\n  Crisp Bisimulations", "abstract": "The problem of minimizing finite fuzzy interpretations in fuzzy description\nlogics (FDLs) is worth studying. For example, the structure of a fuzzy/weighted\nsocial network can be treated as a fuzzy interpretation in FDLs, where actors\nare individuals and actions are roles. Minimizing the structure of a\nfuzzy/weighted social network makes it more compact, thus making network\nanalysis tasks more efficient. In this work, we study the problem of minimizing\na finite fuzzy interpretation in a FDL by using the largest crisp\nauto-bisimulation. The considered FDLs use the Baaz projection operator and\ntheir semantics is specified using an abstract algebra of fuzzy truth values,\nwhich can be any linear and complete residuated lattice. We provide an\nefficient algorithm with a complexity of $O((m \\log{l} + n) \\log{n})$ for\nminimizing a given finite fuzzy interpretation $\\mathcal{I}$, where $n$ is the\nsize of the domain of $\\mathcal{I}$, $m$ is number of nonzero instances of\natomic roles of $\\mathcal{I}$ and $l$ is the number of different fuzzy values\nused for instances of atomic roles of $\\mathcal{I}$. We prove that the fuzzy\ninterpretation returned by the algorithm is minimal among the ones that\npreserve fuzzy TBoxes and ABoxes under certain conditions.", "published": "2023-03-13 09:04:17", "link": "http://arxiv.org/abs/2303.11438v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Neural Diarization with Non-autoregressive Intermediate Attractors", "abstract": "End-to-end neural diarization (EEND) with encoder-decoder-based attractors\n(EDA) is a promising method to handle the whole speaker diarization problem\nsimultaneously with a single neural network. While the EEND model can produce\nall frame-level speaker labels simultaneously, it disregards output label\ndependency. In this work, we propose a novel EEND model that introduces the\nlabel dependency between frames. The proposed method generates\nnon-autoregressive intermediate attractors to produce speaker labels at the\nlower layers and conditions the subsequent layers with these labels. While the\nproposed model works in a non-autoregressive manner, the speaker labels are\nrefined by referring to the whole sequence of intermediate labels. The\nexperiments with the two-speaker CALLHOME dataset show that the intermediate\nlabels with the proposed non-autoregressive intermediate attractors boost the\ndiarization performance. The proposed method with the deeper network benefits\nmore from the intermediate labels, resulting in better performance and training\nthroughput than EEND-EDA.", "published": "2023-03-13 01:28:55", "link": "http://arxiv.org/abs/2303.06806v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Robust Contrastive Language-Image Pre-training against Data Poisoning\n  and Backdoor Attacks", "abstract": "Contrastive vision-language representation learning has achieved\nstate-of-the-art performance for zero-shot classification, by learning from\nmillions of image-caption pairs crawled from the internet. However, the massive\ndata that powers large multimodal models such as CLIP, makes them extremely\nvulnerable to various types of targeted data poisoning and backdoor attacks.\nDespite this vulnerability, robust contrastive vision-language pre-training\nagainst such attacks has remained unaddressed. In this work, we propose ROCLIP,\nthe first effective method for robust pre-training multimodal vision-language\nmodels against targeted data poisoning and backdoor attacks. ROCLIP effectively\nbreaks the association between poisoned image-caption pairs by considering a\nrelatively large and varying pool of random captions, and matching every image\nwith the text that is most similar to it in the pool instead of its own\ncaption, every few epochs.It also leverages image and text augmentations to\nfurther strengthen the defense and improve the performance of the model. Our\nextensive experiments show that ROCLIP renders state-of-the-art targeted data\npoisoning and backdoor attacks ineffective during pre-training CLIP models. In\nparticular, ROCLIP decreases the success rate for targeted data poisoning\nattacks from 93.75% to 12.5% and that of backdoor attacks down to 0%, while\nimproving the model's linear probe performance by 10% and maintains a similar\nzero shot performance compared to CLIP. By increasing the frequency of\nmatching, ROCLIP is able to defend strong attacks, which add up to 1% poisoned\nexamples to the data, and successfully maintain a low attack success rate of\n12.5%, while trading off the performance on some tasks.", "published": "2023-03-13 04:49:46", "link": "http://arxiv.org/abs/2303.06854v2", "categories": ["cs.CV", "cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Contextually-rich human affect perception using multimodal scene\n  information", "abstract": "The process of human affect understanding involves the ability to infer\nperson specific emotional states from various sources including images, speech,\nand language. Affect perception from images has predominantly focused on\nexpressions extracted from salient face crops. However, emotions perceived by\nhumans rely on multiple contextual cues including social settings, foreground\ninteractions, and ambient visual scenes. In this work, we leverage pretrained\nvision-language (VLN) models to extract descriptions of foreground context from\nimages. Further, we propose a multimodal context fusion (MCF) module to combine\nforeground cues with the visual scene and person-based contextual information\nfor emotion prediction. We show the effectiveness of our proposed modular\ndesign on two datasets associated with natural scenes and TV shows.", "published": "2023-03-13 07:46:41", "link": "http://arxiv.org/abs/2303.06904v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "NeuroQL: A Neuro-Symbolic Language and Dataset for Inter-Subjective\n  Reasoning", "abstract": "We present a new AI task and baseline solution for Inter-Subjective\nReasoning. We define inter-subjective information, to be a mixture of objective\nand subjective information possibly shared by different parties. Examples may\ninclude commodities and their objective properties as reported by IR\n(Information Retrieval) systems, that need to be cross-referenced with\nsubjective user reviews from an online forum. For an AI system to successfully\nreason about both, it needs to be able to combine symbolic reasoning of\nobjective facts with the shared consensus found on subjective user reviews. To\nthis end we introduce the NeuroQL dataset and DSL (Domain-specific Language) as\na baseline solution for this problem. NeuroQL is a neuro-symbolic language that\nextends logical unification with neural primitives for extraction and\nretrieval. It can function as a target for automatic translation of\ninter-subjective questions (posed in natural language) into the neuro-symbolic\ncode that can answer them.", "published": "2023-03-13 14:16:59", "link": "http://arxiv.org/abs/2303.07146v1", "categories": ["cs.PL", "cs.AI", "cs.CL", "cs.SE"], "primary_category": "cs.PL"}
{"title": "PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical\n  Documents", "abstract": "Foundation models trained on large-scale dataset gain a recent surge in CV\nand NLP. In contrast, development in biomedical domain lags far behind due to\ndata scarcity. To address this issue, we build and release PMC-OA, a biomedical\ndataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess\nsubset, which is 8 times larger than before. PMC-OA covers diverse modalities\nor diseases, with majority of the image-caption samples aligned at\nfiner-grained level, i.e., subfigure and subcaption. While pretraining a\nCLIP-style model on PMC-OA, our model named PMC-CLIP achieves state-of-the-art\nresults on various downstream tasks, including image-text retrieval on ROCO,\nMedMNIST image classification, Medical VQA, i.e. +8.1% R@10 on image-text\nretrieval, +3.9% accuracy on image classification.", "published": "2023-03-13 16:13:16", "link": "http://arxiv.org/abs/2303.07240v1", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of\n  Synthetic and Compositional Images", "abstract": "Weird, unusual, and uncanny images pique the curiosity of observers because\nthey challenge commonsense. For example, an image released during the 2022\nworld cup depicts the famous soccer stars Lionel Messi and Cristiano Ronaldo\nplaying chess, which playfully violates our expectation that their competition\nshould occur on the football field. Humans can easily recognize and interpret\nthese unconventional images, but can AI models do the same? We introduce\nWHOOPS!, a new dataset and benchmark for visual commonsense. The dataset is\ncomprised of purposefully commonsense-defying images created by designers using\npublicly-available image generation tools like Midjourney. We consider several\ntasks posed over the dataset. In addition to image captioning, cross-modal\nmatching, and visual question answering, we introduce a difficult explanation\ngeneration task, where models must identify and explain why a given image is\nunusual. Our results show that state-of-the-art models such as GPT3 and BLIP2\nstill lag behind human performance on WHOOPS!. We hope our dataset will inspire\nthe development of AI models with stronger visual commonsense reasoning\nabilities. Data, models and code are available at the project website:\nwhoops-benchmark.github.io", "published": "2023-03-13 16:49:43", "link": "http://arxiv.org/abs/2303.07274v4", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Audio Visual Language Maps for Robot Navigation", "abstract": "While interacting in the world is a multi-sensory experience, many robots\ncontinue to predominantly rely on visual perception to map and navigate in\ntheir environments. In this work, we propose Audio-Visual-Language Maps\n(AVLMaps), a unified 3D spatial map representation for storing cross-modal\ninformation from audio, visual, and language cues. AVLMaps integrate the\nopen-vocabulary capabilities of multimodal foundation models pre-trained on\nInternet-scale data by fusing their features into a centralized 3D voxel grid.\nIn the context of navigation, we show that AVLMaps enable robot systems to\nindex goals in the map based on multimodal queries, e.g., textual descriptions,\nimages, or audio snippets of landmarks. In particular, the addition of audio\ninformation enables robots to more reliably disambiguate goal locations.\nExtensive experiments in simulation show that AVLMaps enable zero-shot\nmultimodal goal navigation from multimodal prompts and provide 50% better\nrecall in ambiguous scenarios. These capabilities extend to mobile robots in\nthe real world - navigating to landmarks referring to visual, audio, and\nspatial concepts. Videos and code are available at: https://avlmaps.github.io.", "published": "2023-03-13 23:17:51", "link": "http://arxiv.org/abs/2303.07522v2", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "The NPU-Elevoc Personalized Speech Enhancement System for ICASSP2023 DNS\n  Challenge", "abstract": "This paper describes our NPU-Elevoc personalized speech enhancement system\n(NAPSE) for the 5th Deep Noise Suppression Challenge at ICASSP 2023. Based on\nthe superior two-stage model TEA-PSE 2.0, our system particularly explores\nbetter strategy for speaker embedding fusion, optimizes the model training\npipeline, and leverages adversarial training and multi-scale loss. According to\nthe results, our system is tied for the 1st place in the headset track (track\n1) and ranked 2nd in the speakerphone track (track 2).", "published": "2023-03-13 01:59:14", "link": "http://arxiv.org/abs/2303.06811v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Two-step Band-split Neural Network Approach for Full-band Residual Echo\n  Suppression", "abstract": "This paper describes a Two-step Band-split Neural Network (TBNN) approach for\nfull-band acoustic echo cancellation. Specifically, after linear filtering, we\nsplit the full-band signal into wide-band (16KHz) and high-band (16-48KHz) for\nresidual echo removal with lower modeling difficulty. The wide-band signal is\nprocessed by an updated gated convolutional recurrent network (GCRN) with U$^2$\nencoder while the high-band signal is processed by a high-band post-filter net\nwith lower complexity. Our approach submitted to ICASSP 2023 AEC Challenge has\nachieved an overall mean opinion score (MOS) of 4.344 and a word accuracy\n(WAcc) ratio of 0.795, leading to the 2$^{nd}$ (tied) in the ranking of the\nnon-personalized track.", "published": "2023-03-13 03:07:01", "link": "http://arxiv.org/abs/2303.06828v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Learning-based Robust Speaker Counting and Separation with the Aid of\n  Spatial Coherence", "abstract": "A three-stage approach is proposed for speaker counting and speech separation\nin noisy and reverberant environments. In the spatial feature extraction, a\nspatial coherence matrix (SCM) is computed using whitened relative transfer\nfunctions (wRTFs) across time frames. The global activity functions of each\nspeaker are estimated from a simplex constructed using the eigenvectors of the\nSCM, while the local coherence functions are computed from the coherence\nbetween the wRTFs of a time-frequency bin and the global activity\nfunction-weighted RTF of the target speaker. In speaker counting, we use the\neigenvalues of the SCM and the maximum similarity of the interframe global\nactivity distributions between two speakers as the input features to the\nspeaker counting network (SCnet). In speaker separation, a global and local\nactivity-driven network (GLADnet) is used to extract each independent speaker\nsignal, which is particularly useful for highly overlapping speech signals.\nExperimental results obtained from the real meeting recordings show that the\nproposed system achieves superior speaker counting and speaker separation\nperformance compared to previous publications without the prior knowledge of\nthe array configurations.", "published": "2023-03-13 05:28:06", "link": "http://arxiv.org/abs/2303.06867v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Real-Time Audio-Visual End-to-End Speech Enhancement", "abstract": "Audio-visual speech enhancement (AV-SE) methods utilize auxiliary visual cues\nto enhance speakers' voices. Therefore, technically they should be able to\noutperform the audio-only speech enhancement (SE) methods. However, there are\nfew works in the literature on an AV-SE system that can work in real time on a\nCPU. In this paper, we propose a low-latency real-time audio-visual end-to-end\nenhancement (AV-E3Net) model based on the recently proposed end-to-end\nenhancement network (E3Net). Our main contribution includes two aspects: 1) We\nemploy a dense connection module to solve the performance degradation caused by\nthe deep model structure. This module significantly improves the model's\nperformance on the AV-SE task. 2) We propose a multi-stage gating-and-summation\n(GS) fusion module to merge audio and visual cues. Our results show that the\nproposed model provides better perceptual quality and intelligibility than the\nbaseline E3net model with a negligible computational cost increase.", "published": "2023-03-13 11:01:34", "link": "http://arxiv.org/abs/2303.07005v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Adaptive Dereverberation, Noise and Interferer Reduction Using Sparse\n  Weighted Linearly Constrained Minimum Power Beamforming", "abstract": "Interfering sources, background noise and reverberation degrade speech\nquality and intelligibility in hearing aid applications. In this paper, we\npresent an adaptive algorithm aiming at dereverberation, noise and interferer\nreduction and preservation of binaural cues based on the wBLCMP beamformer. The\nwBLCMP beamformer unifies the multi-channel weighted prediction error method\nperforming dereverberation and the linearly constrained minimum power\nbeamformer performing noise and interferer reduction into a single\nconvolutional beamformer. We propose to adaptively compute the optimal filter\nby incorporating an exponential window into a sparsity-promoting lp-norm cost\nfunction, which enables to track a moving target speaker. Simulation results\nwith successive target speakers at different positions show that the proposed\nadaptive version of the wBLCMP beamformer outperforms a non-adaptive version in\nterms of objective speech enhancement performance measures.", "published": "2023-03-13 11:44:08", "link": "http://arxiv.org/abs/2303.07027v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Can spoofing countermeasure and speaker verification systems be jointly\n  optimised?", "abstract": "Spoofing countermeasure (CM) and automatic speaker verification (ASV)\nsub-systems can be used in tandem with a backend classifier as a solution to\nthe spoofing aware speaker verification (SASV) task. The two sub-systems are\ntypically trained independently to solve different tasks. While our previous\nwork demonstrated the potential of joint optimisation, it also showed a\ntendency to over-fit to speakers and a lack of sub-system complementarity.\nUsing only a modest quantity of auxiliary data collected from new speakers, we\nshow that joint optimisation degrades the performance of separate CM and ASV\nsub-systems, but that it nonetheless improves complementarity, thereby\ndelivering superior SASV performance. Using standard SASV evaluation data and\nprotocols, joint optimisation reduces the equal error rate by 27\\% relative to\nperformance obtained using fixed, independently-optimised sub-systems under\nlike-for-like training conditions.", "published": "2023-03-13 12:52:02", "link": "http://arxiv.org/abs/2303.07073v4", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Analysing the Masked predictive coding training criterion for\n  pre-training a Speech Representation Model", "abstract": "Recent developments in pre-trained speech representation utilizing\nself-supervised learning (SSL) have yielded exceptional results on a variety of\ndownstream tasks. One such technique, known as masked predictive coding (MPC),\nhas been employed by some of the most high-performing models. In this study, we\ninvestigate the impact of MPC loss on the type of information learnt at various\nlayers in the HuBERT model, using nine probing tasks. Our findings indicate\nthat the amount of content information learned at various layers of the HuBERT\nmodel has a positive correlation to the MPC loss. Additionally, it is also\nobserved that any speaker-related information learned at intermediate layers of\nthe model, is an indirect consequence of the learning process, and therefore\ncannot be controlled using the MPC loss. These findings may serve as\ninspiration for further research in the speech community, specifically in the\ndevelopment of new pre-training tasks or the exploration of new pre-training\ncriterion's that directly preserves both speaker and content information at\nvarious layers of a learnt model.", "published": "2023-03-13 10:32:44", "link": "http://arxiv.org/abs/2303.06982v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A two-stage speaker extraction algorithm under adverse acoustic\n  conditions using a single-microphone", "abstract": "In this work, we present a two-stage method for speaker extraction under\nreverberant and noisy conditions. Given a reference signal of the desired\nspeaker, the clean, but the still reverberant, desired speaker is first\nextracted from the noisy-mixed signal. In the second stage, the extracted\nsignal is further enhanced by joint dereverberation and residual noise and\ninterference reduction. The proposed architecture comprises two sub-networks,\none for the extraction task and the second for the dereverberation task. We\npresent a training strategy for this architecture and show that the performance\nof the proposed method is on par with other state-of-the-art (SOTA) methods\nwhen applied to the WHAMR! dataset. Furthermore, we present a new dataset with\nmore realistic adverse acoustic conditions and show that our method outperforms\nthe competing methods when applied to this dataset as well.", "published": "2023-03-13 12:48:15", "link": "http://arxiv.org/abs/2303.07072v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A processing framework to access large quantities of whispered speech\n  found in ASMR", "abstract": "Whispering is a ubiquitous mode of communication that humans use daily.\nDespite this, whispered speech has been poorly served by existing speech\ntechnology due to a shortage of resources and processing methodology. To remedy\nthis, this paper provides a processing framework that enables access to large\nand unique data of high-quality whispered speech. We obtain the data from\nrecordings submitted to online platforms as part of the ASMR media-cultural\nphenomenon. We describe our processing pipeline and a method for improved\nwhispered activity detection (WAD) in the ASMR data. To efficiently obtain\nlabelled, clean whispered speech, we complement the automatic WAD by using\nEdyson, a bulk audio-annotation tool with human-in-the-loop. We also tackle a\nproblem particular to ASMR: separation of whisper from other acoustic triggers\npresent in the genre. We show that the proposed WAD and the efficient labelling\nallows to build extensively augmented data and train a classifier that extracts\nclean whisper segments from ASMR audio. Our large and growing dataset enables\nwhisper-capable, data-driven speech technology and linguistic analysis. It also\nopens opportunities in e.g. HCI as a resource that may elicit emotional,\npsychological and neuro-physiological responses in the listener.", "published": "2023-03-13 19:50:17", "link": "http://arxiv.org/abs/2303.07442v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Online Binaural Speech Separation of Moving Speakers With a Wavesplit\n  Network", "abstract": "Binaural speech separation in real-world scenarios often involves moving\nspeakers. Most current speech separation methods use utterance-level\npermutation invariant training (u-PIT) for training. In inference time,\nhowever, the order of outputs can be inconsistent over time particularly in\nlong-form speech separation. This situation which is referred to as the speaker\nswap problem is even more problematic when speakers constantly move in space\nand therefore poses a challenge for consistent placement of speakers in output\nchannels. Here, we describe a real-time binaural speech separation model based\non a Wavesplit network to mitigate the speaker swap problem for moving speaker\nseparation. Our model computes a speaker embedding for each speaker at each\ntime frame from the mixed audio, aggregates embeddings using online clustering,\nand uses cluster centroids as speaker profiles to track each speaker throughout\nthe long duration. Experimental results on reverberant, long-form moving\nmultitalker speech separation show that the proposed method is less prone to\nspeaker swap and achieves comparable performance with u-PIT based models with\nground truth tracking in both separation accuracy and preserving the interaural\ncues.", "published": "2023-03-13 20:38:14", "link": "http://arxiv.org/abs/2303.07458v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Intelligibility Classifiers from 550k Disordered Speech Samples", "abstract": "We developed dysarthric speech intelligibility classifiers on 551,176\ndisordered speech samples contributed by a diverse set of 468 speakers, with a\nrange of self-reported speaking disorders and rated for their overall\nintelligibility on a five-point scale. We trained three models following\ndifferent deep learning approaches and evaluated them on ~94K utterances from\n100 speakers. We further found the models to generalize well (without further\ntraining) on the TORGO database (100% accuracy), UASpeech (0.93 correlation),\nALS-TDI PMP (0.81 AUC) datasets as well as on a dataset of realistic unprompted\nspeech we gathered (106 dysarthric and 76 control speakers,~2300 samples).", "published": "2023-03-13 23:38:56", "link": "http://arxiv.org/abs/2303.07533v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cross-device Federated Learning for Mobile Health Diagnostics: A First\n  Study on COVID-19 Detection", "abstract": "Federated learning (FL) aided health diagnostic models can incorporate data\nfrom a large number of personal edge devices (e.g., mobile phones) while\nkeeping the data local to the originating devices, largely ensuring privacy.\nHowever, such a cross-device FL approach for health diagnostics still imposes\nmany challenges due to both local data imbalance (as extreme as local data\nconsists of a single disease class) and global data imbalance (the disease\nprevalence is generally low in a population). Since the federated server has no\naccess to data distribution information, it is not trivial to solve the\nimbalance issue towards an unbiased model. In this paper, we propose FedLoss, a\nnovel cross-device FL framework for health diagnostics. Here the federated\nserver averages the models trained on edge devices according to the predictive\nloss on the local data, rather than using only the number of samples as\nweights. As the predictive loss better quantifies the data distribution at a\ndevice, FedLoss alleviates the impact of data imbalance. Through a real-world\ndataset on respiratory sound and symptom-based COVID-$19$ detection task, we\nvalidate the superiority of FedLoss. It achieves competitive COVID-$19$\ndetection performance compared to a centralised model with an AUC-ROC of\n$79\\%$. It also outperforms the state-of-the-art FL baselines in sensitivity\nand convergence speed. Our work not only demonstrates the promise of federated\nCOVID-$19$ detection but also paves the way to a plethora of mobile health\nmodel development in a privacy-preserving fashion.", "published": "2023-03-13 12:42:02", "link": "http://arxiv.org/abs/2303.07067v1", "categories": ["cs.LG", "cs.DC", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Multi-Microphone Speaker Separation by Spatial Regions", "abstract": "We consider the task of region-based source separation of reverberant\nmulti-microphone recordings. We assume pre-defined spatial regions with a\nsingle active source per region. The objective is to estimate the signals from\nthe individual spatial regions as captured by a reference microphone while\nretaining a correspondence between signals and spatial regions. We propose a\ndata-driven approach using a modified version of a state-of-the-art network,\nwhere different layers model spatial and spectro-temporal information. The\nnetwork is trained to enforce a fixed mapping of regions to network outputs.\nUsing speech from LibriMix, we construct a data set specifically designed to\ncontain the region information. Additionally, we train the network with\npermutation invariant training. We show that both training methods result in a\nfixed mapping of regions to network outputs, achieve comparable performance,\nand that the networks exploit spatial information. The proposed network\noutperforms a baseline network by 1.5 dB in scale-invariant\nsignal-to-distortion ratio.", "published": "2023-03-13 14:11:34", "link": "http://arxiv.org/abs/2303.07143v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Blind Acoustic Room Parameter Estimation Using Phase Features", "abstract": "Modeling room acoustics in a field setting involves some degree of blind\nparameter estimation from noisy and reverberant audio. Modern approaches\nleverage convolutional neural networks (CNNs) in tandem with time-frequency\nrepresentation. Using short-time Fourier transforms to develop these\nspectrogram-like features has shown promising results, but this method\nimplicitly discards a significant amount of audio information in the phase\ndomain. Inspired by recent works in speech enhancement, we propose utilizing\nnovel phase-related features to extend recent approaches to blindly estimate\nthe so-called \"reverberation fingerprint\" parameters, namely, volume and RT60.\nThe addition of these features is shown to outperform existing methods that\nrely solely on magnitude-based spectral features across a wide range of\nacoustics spaces. We evaluate the effectiveness of the deployment of these\nnovel features in both single-parameter and multi-parameter estimation\nstrategies, using a novel dataset that consists of publicly available room\nimpulse responses (RIRs), synthesized RIRs, and in-house measurements of real\nacoustic spaces.", "published": "2023-03-13 20:05:41", "link": "http://arxiv.org/abs/2303.07449v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Guided Speech Enhancement Network", "abstract": "High quality speech capture has been widely studied for both voice\ncommunication and human computer interface reasons. To improve the capture\nperformance, we can often find multi-microphone speech enhancement techniques\ndeployed on various devices. Multi-microphone speech enhancement problem is\noften decomposed into two decoupled steps: a beamformer that provides spatial\nfiltering and a single-channel speech enhancement model that cleans up the\nbeamformer output. In this work, we propose a speech enhancement solution that\ntakes both the raw microphone and beamformer outputs as the input for an ML\nmodel. We devise a simple yet effective training scheme that allows the model\nto learn from the cues of the beamformer by contrasting the two inputs and\ngreatly boost its capability in spatial rejection, while conducting the general\ntasks of denoising and dereverberation. The proposed solution takes advantage\nof classical spatial filtering algorithms instead of competing with them. By\ndesign, the beamformer module then could be selected separately and does not\nrequire a large amount of data to be optimized for a given form factor, and the\nnetwork model can be considered as a standalone module which is highly\ntransferable independently from the microphone array. We name the ML module in\nour solution as GSENet, short for Guided Speech Enhancement Network. We\ndemonstrate its effectiveness on real world data collected on multi-microphone\ndevices in terms of the suppression of noise and interfering speech.", "published": "2023-03-13 21:48:20", "link": "http://arxiv.org/abs/2303.07486v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "HiSSNet: Sound Event Detection and Speaker Identification via\n  Hierarchical Prototypical Networks for Low-Resource Headphones", "abstract": "Modern noise-cancelling headphones have significantly improved users'\nauditory experiences by removing unwanted background noise, but they can also\nblock out sounds that matter to users. Machine learning (ML) models for sound\nevent detection (SED) and speaker identification (SID) can enable headphones to\nselectively pass through important sounds; however, implementing these models\nfor a user-centric experience presents several unique challenges. First, most\npeople spend limited time customizing their headphones, so the sound detection\nshould work reasonably well out of the box. Second, the models should be able\nto learn over time the specific sounds that are important to users based on\ntheir implicit and explicit interactions. Finally, such models should have a\nsmall memory footprint to run on low-power headphones with limited on-chip\nmemory. In this paper, we propose addressing these challenges using HiSSNet\n(Hierarchical SED and SID Network). HiSSNet is an SEID (SED and SID) model that\nuses a hierarchical prototypical network to detect both general and specific\nsounds of interest and characterize both alarm-like and speech sounds. We show\nthat HiSSNet outperforms an SEID model trained using non-hierarchical\nprototypical networks by 6.9 - 8.6 percent. When compared to state-of-the-art\n(SOTA) models trained specifically for SED or SID alone, HiSSNet achieves\nsimilar or better performance while reducing the memory footprint required to\nsupport multiple capabilities on-device.", "published": "2023-03-13 23:49:09", "link": "http://arxiv.org/abs/2303.07538v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
