{"title": "Probabilistic Bag-Of-Hyperlinks Model for Entity Linking", "abstract": "Many fundamental problems in natural language processing rely on determining\nwhat entities appear in a given text. Commonly referenced as entity linking,\nthis step is a fundamental component of many NLP tasks such as text\nunderstanding, automatic summarization, semantic search or machine translation.\nName ambiguity, word polysemy, context dependencies and a heavy-tailed\ndistribution of entities contribute to the complexity of this problem.\n  We here propose a probabilistic approach that makes use of an effective\ngraphical model to perform collective entity disambiguation. Input mentions\n(i.e.,~linkable token spans) are disambiguated jointly across an entire\ndocument by combining a document-level prior of entity co-occurrences with\nlocal information captured from mentions and their surrounding context. The\nmodel is based on simple sufficient statistics extracted from data, thus\nrelying on few parameters to be learned.\n  Our method does not require extensive feature engineering, nor an expensive\ntraining procedure. We use loopy belief propagation to perform approximate\ninference. The low complexity of our model makes this step sufficiently fast\nfor real-time usage. We demonstrate the accuracy of our approach on a wide\nrange of benchmark datasets, showing that it matches, and in many cases\noutperforms, existing state-of-the-art methods.", "published": "2015-09-08 09:43:13", "link": "http://arxiv.org/abs/1509.02301v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Domain Discovery using Latent Dirichlet Allocation for\n  Acoustic Modelling in Speech Recognition", "abstract": "Speech recognition systems are often highly domain dependent, a fact widely\nreported in the literature. However the concept of domain is complex and not\nbound to clear criteria. Hence it is often not evident if data should be\nconsidered to be out-of-domain. While both acoustic and language models can be\ndomain specific, work in this paper concentrates on acoustic modelling. We\npresent a novel method to perform unsupervised discovery of domains using\nLatent Dirichlet Allocation (LDA) modelling. Here a set of hidden domains is\nassumed to exist in the data, whereby each audio segment can be considered to\nbe a weighted mixture of domain properties. The classification of audio\nsegments into domains allows the creation of domain specific acoustic models\nfor automatic speech recognition. Experiments are conducted on a dataset of\ndiverse speech data covering speech from radio and TV broadcasts, telephone\nconversations, meetings, lectures and read speech, with a joint training set of\n60 hours and a test set of 6 hours. Maximum A Posteriori (MAP) adaptation to\nLDA based domains was shown to yield relative Word Error Rate (WER)\nimprovements of up to 16% relative, compared to pooled training, and up to 10%,\ncompared with models adapted with human-labelled prior domain knowledge.", "published": "2015-09-08 15:29:23", "link": "http://arxiv.org/abs/1509.02412v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data-selective Transfer Learning for Multi-Domain Speech Recognition", "abstract": "Negative transfer in training of acoustic models for automatic speech\nrecognition has been reported in several contexts such as domain change or\nspeaker characteristics. This paper proposes a novel technique to overcome\nnegative transfer by efficient selection of speech data for acoustic model\ntraining. Here data is chosen on relevance for a specific target. A submodular\nfunction based on likelihood ratios is used to determine how acoustically\nsimilar each training utterance is to a target test set. The approach is\nevaluated on a wide-domain data set, covering speech from radio and TV\nbroadcasts, telephone conversations, meetings, lectures and read speech.\nExperiments demonstrate that the proposed technique both finds relevant data\nand limits negative transfer. Results on a 6--hour test set show a relative\nimprovement of 4% with data selection over using all data in PLP based models,\nand 2% with DNN features.", "published": "2015-09-08 15:20:12", "link": "http://arxiv.org/abs/1509.02409v1", "categories": ["cs.LG", "cs.CL", "cs.SD"], "primary_category": "cs.LG"}
{"title": "Improved Twitter Sentiment Prediction through Cluster-then-Predict Model", "abstract": "Over the past decade humans have experienced exponential growth in the use of\nonline resources, in particular social media and microblogging websites such as\nFacebook, Twitter, YouTube and also mobile applications such as WhatsApp, Line,\netc. Many companies have identified these resources as a rich mine of marketing\nknowledge. This knowledge provides valuable feedback which allows them to\nfurther develop the next generation of their product. In this paper, sentiment\nanalysis of a product is performed by extracting tweets about that product and\nclassifying the tweets showing it as positive and negative sentiment. The\nauthors propose a hybrid approach which combines unsupervised learning in the\nform of K-means clustering to cluster the tweets and then performing supervised\nlearning methods such as Decision Trees and Support Vector Machines for\nclassification.", "published": "2015-09-08 16:36:04", "link": "http://arxiv.org/abs/1509.02437v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.IR"}
