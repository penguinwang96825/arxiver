{"title": "Chain-of-Layer: Iteratively Prompting Large Language Models for Taxonomy\n  Induction from Limited Examples", "abstract": "Automatic taxonomy induction is crucial for web search, recommendation\nsystems, and question answering. Manual curation of taxonomies is expensive in\nterms of human effort, making automatic taxonomy construction highly desirable.\nIn this work, we introduce Chain-of-Layer which is an in-context learning\nframework designed to induct taxonomies from a given set of entities.\nChain-of-Layer breaks down the task into selecting relevant candidate entities\nin each layer and gradually building the taxonomy from top to bottom. To\nminimize errors, we introduce the Ensemble-based Ranking Filter to reduce the\nhallucinated content generated at each iteration. Through extensive\nexperiments, we demonstrate that Chain-of-Layer achieves state-of-the-art\nperformance on four real-world benchmarks.", "published": "2024-02-12 03:05:54", "link": "http://arxiv.org/abs/2402.07386v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can LLMs Produce Faithful Explanations For Fact-checking? Towards\n  Faithful Explainable Fact-Checking via Multi-Agent Debate", "abstract": "Fact-checking research has extensively explored verification but less so the\ngeneration of natural-language explanations, crucial for user trust. While\nLarge Language Models (LLMs) excel in text generation, their capability for\nproducing faithful explanations in fact-checking remains underexamined. Our\nstudy investigates LLMs' ability to generate such explanations, finding that\nzero-shot prompts often result in unfaithfulness. To address these challenges,\nwe propose the Multi-Agent Debate Refinement (MADR) framework, leveraging\nmultiple LLMs as agents with diverse roles in an iterative refining process\naimed at enhancing faithfulness in generated explanations. MADR ensures that\nthe final explanation undergoes rigorous validation, significantly reducing the\nlikelihood of unfaithful elements and aligning closely with the provided\nevidence. Experimental results demonstrate that MADR significantly improves the\nfaithfulness of LLM-generated explanations to the evidence, advancing the\ncredibility and trustworthiness of these explanations.", "published": "2024-02-12 04:32:33", "link": "http://arxiv.org/abs/2402.07401v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "D\u00f3lares or Dollars? Unraveling the Bilingual Prowess of Financial LLMs\n  Between Spanish and English", "abstract": "Despite Spanish's pivotal role in the global finance industry, a pronounced\ngap exists in Spanish financial natural language processing (NLP) and\napplication studies compared to English, especially in the era of large\nlanguage models (LLMs). To bridge this gap, we unveil Tois\\'on de Oro, the\nfirst bilingual framework that establishes instruction datasets, finetuned\nLLMs, and evaluation benchmark for financial LLMs in Spanish joint with\nEnglish. We construct a rigorously curated bilingual instruction dataset\nincluding over 144K Spanish and English samples from 15 datasets covering 7\ntasks. Harnessing this, we introduce FinMA-ES, an LLM designed for bilingual\nfinancial applications. We evaluate our model and existing LLMs using FLARE-ES,\nthe first comprehensive bilingual evaluation benchmark with 21 datasets\ncovering 9 tasks. The FLARE-ES benchmark results reveal a significant\nmultilingual performance gap and bias in existing LLMs. FinMA-ES models surpass\nSOTA LLMs such as GPT-4 in Spanish financial tasks, due to strategic\ninstruction tuning and leveraging data from diverse linguistic resources,\nhighlighting the positive impact of cross-linguistic transfer. All our\ndatasets, models, and benchmarks have been released.", "published": "2024-02-12 04:50:31", "link": "http://arxiv.org/abs/2402.07405v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Intrinsic Task-based Evaluation for Referring Expression Generation", "abstract": "Recently, a human evaluation study of Referring Expression Generation (REG)\nmodels had an unexpected conclusion: on \\textsc{webnlg}, Referring Expressions\n(REs) generated by the state-of-the-art neural models were not only\nindistinguishable from the REs in \\textsc{webnlg} but also from the REs\ngenerated by a simple rule-based system. Here, we argue that this limitation\ncould stem from the use of a purely ratings-based human evaluation (which is a\ncommon practice in Natural Language Generation). To investigate these issues,\nwe propose an intrinsic task-based evaluation for REG models, in which, in\naddition to rating the quality of REs, participants were asked to accomplish\ntwo meta-level tasks. One of these tasks concerns the referential success of\neach RE; the other task asks participants to suggest a better alternative for\neach RE. The outcomes suggest that, in comparison to previous evaluations, the\nnew evaluation protocol assesses the performance of each REG model more\ncomprehensively and makes the participants' ratings more reliable and\ndiscriminable.", "published": "2024-02-12 06:21:35", "link": "http://arxiv.org/abs/2402.07432v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quality Does Matter: A Detailed Look at the Quality and Utility of\n  Web-Mined Parallel Corpora", "abstract": "We conducted a detailed analysis on the quality of web-mined corpora for two\nlow-resource languages (making three language pairs, English-Sinhala,\nEnglish-Tamil and Sinhala-Tamil). We ranked each corpus according to a\nsimilarity measure and carried out an intrinsic and extrinsic evaluation on\ndifferent portions of this ranked corpus. We show that there are significant\nquality differences between different portions of web-mined corpora and that\nthe quality varies across languages and datasets. We also show that, for some\nweb-mined datasets, Neural Machine Translation (NMT) models trained with their\nhighest-ranked 25k portion can be on par with human-curated datasets.", "published": "2024-02-12 07:03:14", "link": "http://arxiv.org/abs/2402.07446v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pushing The Limit of LLM Capacity for Text Classification", "abstract": "The value of text classification's future research has encountered challenges\nand uncertainties, due to the extraordinary efficacy demonstrated by large\nlanguage models (LLMs) across numerous downstream NLP tasks. In this era of\nopen-ended language modeling, where task boundaries are gradually fading, an\nurgent question emerges: have we made significant advances in text\nclassification under the full benefit of LLMs? To answer this question, we\npropose RGPT, an adaptive boosting framework tailored to produce a specialized\ntext classification LLM by recurrently ensembling a pool of strong base\nlearners. The base learners are constructed by adaptively adjusting the\ndistribution of training samples and iteratively fine-tuning LLMs with them.\nSuch base learners are then ensembled to be a specialized text classification\nLLM, by recurrently incorporating the historical predictions from the previous\nlearners. Through a comprehensive empirical comparison, we show that RGPT\nsignificantly outperforms 8 SOTA PLMs and 7 SOTA LLMs on four benchmarks by\n1.36% on average. Further evaluation experiments show a clear surpassing of\nRGPT over human classification.", "published": "2024-02-12 08:14:03", "link": "http://arxiv.org/abs/2402.07470v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topic Modeling as Multi-Objective Contrastive Optimization", "abstract": "Recent representation learning approaches enhance neural topic models by\noptimizing the weighted linear combination of the evidence lower bound (ELBO)\nof the log-likelihood and the contrastive learning objective that contrasts\npairs of input documents. However, document-level contrastive learning might\ncapture low-level mutual information, such as word ratio, which disturbs topic\nmodeling. Moreover, there is a potential conflict between the ELBO loss that\nmemorizes input details for better reconstruction quality, and the contrastive\nloss which attempts to learn topic representations that generalize among input\ndocuments. To address these issues, we first introduce a novel contrastive\nlearning method oriented towards sets of topic vectors to capture useful\nsemantics that are shared among a set of input documents. Secondly, we\nexplicitly cast contrastive topic modeling as a gradient-based multi-objective\noptimization problem, with the goal of achieving a Pareto stationary solution\nthat balances the trade-off between the ELBO and the contrastive objective.\nExtensive experiments demonstrate that our framework consistently produces\nhigher-performing neural topic models in terms of topic coherence, topic\ndiversity, and downstream performance.", "published": "2024-02-12 11:18:32", "link": "http://arxiv.org/abs/2402.07577v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting the Clinical Features of Difficult-to-Treat Depression using\n  Synthetic Data from Large Language Models", "abstract": "Difficult-to-treat depression (DTD) has been proposed as a broader and more\nclinically comprehensive perspective on a person's depressive disorder where\ndespite treatment, they continue to experience significant burden. We sought to\ndevelop a Large Language Model (LLM)-based tool capable of interrogating\nroutinely-collected, narrative (free-text) electronic health record (EHR) data\nto locate published prognostic factors that capture the clinical syndrome of\nDTD. In this work, we use LLM-generated synthetic data (GPT3.5) and a\nNon-Maximum Suppression (NMS) algorithm to train a BERT-based span extraction\nmodel. The resulting model is then able to extract and label spans related to a\nvariety of relevant positive and negative factors in real clinical data (i.e.\nspans of text that increase or decrease the likelihood of a patient matching\nthe DTD syndrome). We show it is possible to obtain good overall performance\n(0.70 F1 across polarity) on real clinical data on a set of as many as 20\ndifferent factors, and high performance (0.85 F1 with 0.95 precision) on a\nsubset of important DTD factors such as history of abuse, family history of\naffective disorder, illness severity and suicidality by training the model\nexclusively on synthetic data. Our results show promise for future healthcare\napplications especially in applications where traditionally, highly\nconfidential medical data and human-expert annotation would normally be\nrequired.", "published": "2024-02-12 13:34:33", "link": "http://arxiv.org/abs/2402.07645v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Auxiliary Tasks to Boost Biaffine Semantic Dependency Parsing", "abstract": "The biaffine parser of Dozat and Manning (2017) was successfully extended to\nsemantic dependency parsing (SDP) (Dozat and Manning, 2018). Its performance on\ngraphs is surprisingly high given that, without the constraint of producing a\ntree, all arcs for a given sentence are predicted independently from each other\n(modulo a shared representation of tokens). To circumvent such an independence\nof decision, while retaining the O(n^2) complexity and highly parallelizable\narchitecture, we propose to use simple auxiliary tasks that introduce some form\nof interdependence between arcs. Experiments on the three English acyclic\ndatasets of SemEval 2015 task 18 (Oepen et al., 2015), and on French deep\nsyntactic cyclic graphs (Ribeyre et al., 2014) show modest but systematic\nperformance gains on a near state-of-the-art baseline using transformer-based\ncontextualized representations. This provides a simple and robust method to\nboost SDP performance.", "published": "2024-02-12 14:42:33", "link": "http://arxiv.org/abs/2402.07682v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Sign Language Translation and Generation", "abstract": "Motivated by the success of unsupervised neural machine translation (UNMT),\nwe introduce an unsupervised sign language translation and generation network\n(USLNet), which learns from abundant single-modality (text and video) data\nwithout parallel sign language data. USLNet comprises two main components:\nsingle-modality reconstruction modules (text and video) that rebuild the input\nfrom its noisy version in the same modality and cross-modality back-translation\nmodules (text-video-text and video-text-video) that reconstruct the input from\nits noisy version in the different modality using back-translation\nprocedure.Unlike the single-modality back-translation procedure in text-based\nUNMT, USLNet faces the cross-modality discrepancy in feature representation, in\nwhich the length and the feature dimension mismatch between text and video\nsequences. We propose a sliding window method to address the issues of aligning\nvariable-length text with video sequences. To our knowledge, USLNet is the\nfirst unsupervised sign language translation and generation model capable of\ngenerating both natural language text and sign language video in a unified\nmanner. Experimental results on the BBC-Oxford Sign Language dataset (BOBSL)\nand Open-Domain American Sign Language dataset (OpenASL) reveal that USLNet\nachieves competitive results compared to supervised baseline models, indicating\nits effectiveness in sign language translation and generation.", "published": "2024-02-12 15:39:05", "link": "http://arxiv.org/abs/2402.07726v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Detoxification as Style Transfer in English and Hindi", "abstract": "This paper focuses on text detoxification, i.e., automatically converting\ntoxic text into non-toxic text. This task contributes to safer and more\nrespectful online communication and can be considered a Text Style Transfer\n(TST) task, where the text style changes while its content is preserved. We\npresent three approaches: knowledge transfer from a similar task, multi-task\nlearning approach, combining sequence-to-sequence modeling with various\ntoxicity classification tasks, and delete and reconstruct approach. To support\nour research, we utilize a dataset provided by Dementieva et al.(2021), which\ncontains multiple versions of detoxified texts corresponding to toxic texts. In\nour experiments, we selected the best variants through expert human annotators,\ncreating a dataset where each toxic sentence is paired with a single,\nappropriate detoxified version. Additionally, we introduced a small Hindi\nparallel dataset, aligning with a part of the English dataset, suitable for\nevaluation purposes. Our results demonstrate that our approach effectively\nbalances text detoxication while preserving the actual content and maintaining\nfluency.", "published": "2024-02-12 16:30:41", "link": "http://arxiv.org/abs/2402.07767v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TELLER: A Trustworthy Framework for Explainable, Generalizable and\n  Controllable Fake News Detection", "abstract": "The proliferation of fake news has emerged as a severe societal problem,\nraising significant interest from industry and academia. While existing\ndeep-learning based methods have made progress in detecting fake news\naccurately, their reliability may be compromised caused by the non-transparent\nreasoning processes, poor generalization abilities and inherent risks of\nintegration with large language models (LLMs). To address this challenge, we\npropose {\\methodname}, a novel framework for trustworthy fake news detection\nthat prioritizes explainability, generalizability and controllability of\nmodels. This is achieved via a dual-system framework that integrates cognition\nand decision systems, adhering to the principles above. The cognition system\nharnesses human expertise to generate logical predicates, which guide LLMs in\ngenerating human-readable logic atoms. Meanwhile, the decision system deduces\ngeneralizable logic rules to aggregate these atoms, enabling the identification\nof the truthfulness of the input news across diverse domains and enhancing\ntransparency in the decision-making process. Finally, we present comprehensive\nevaluation results on four datasets, demonstrating the feasibility and\ntrustworthiness of our proposed framework. Our implementation is available at\n\\url{https://github.com/less-and-less-bugs/Trust_TELLER}.", "published": "2024-02-12 16:41:54", "link": "http://arxiv.org/abs/2402.07776v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Intent Attribute-Aware Text Matching in Searching", "abstract": "Text matching systems have become a fundamental service in most searching\nplatforms. For instance, they are responsible for matching user queries to\nrelevant candidate items, or rewriting the user-input query to a pre-selected\nhigh-performing one for a better search experience. In practice, both the\nqueries and items often contain multiple attributes, such as the category of\nthe item and the location mentioned in the query, which represent condensed key\ninformation that is helpful for matching. However, most of the existing works\ndownplay the effectiveness of attributes by integrating them into text\nrepresentations as supplementary information. Hence, in this work, we focus on\nexploring the relationship between the attributes from two sides. Since\nattributes from two ends are often not aligned in terms of number and type, we\npropose to exploit the benefit of attributes by multiple-intent modeling. The\nintents extracted from attributes summarize the diverse needs of queries and\nprovide rich content of items, which are more refined and abstract, and can be\naligned for paired inputs. Concretely, we propose a multi-intent\nattribute-aware matching model (MIM), which consists of three main components:\nattribute-aware encoder, multi-intent modeling, and intent-aware matching. In\nthe attribute-aware encoder, the text and attributes are weighted and processed\nthrough a scaled attention mechanism with regard to the attributes' importance.\nAfterward, the multi-intent modeling extracts intents from two ends and aligns\nthem. Herein, we come up with a distribution loss to ensure the learned intents\nare diverse but concentrated, and a kullback-leibler divergence loss that\naligns the learned intents. Finally, in the intent-aware matching, the intents\nare evaluated by a self-supervised masking task, and then incorporated to\noutput the final matching result.", "published": "2024-02-12 16:54:22", "link": "http://arxiv.org/abs/2402.07788v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Injecting Wiktionary to improve token-level contextual representations\n  using contrastive learning", "abstract": "While static word embeddings are blind to context, for lexical semantics\ntasks context is rather too present in contextual word embeddings, vectors of\nsame-meaning occurrences being too different (Ethayarajh, 2019). Fine-tuning\npre-trained language models (PLMs) using contrastive learning was proposed,\nleveraging automatically self-augmented examples (Liu et al., 2021b). In this\npaper, we investigate how to inject a lexicon as an alternative source of\nsupervision, using the English Wiktionary. We also test how dimensionality\nreduction impacts the resulting contextual word embeddings. We evaluate our\napproach on the Word-In-Context (WiC) task, in the unsupervised setting (not\nusing the training set). We achieve new SoTA result on the original WiC test\nset. We also propose two new WiC test sets for which we show that our\nfine-tuning method achieves substantial improvements. We also observe\nimprovements, although modest, for the semantic frame induction task. Although\nwe experimented on English to allow comparison with related work, our method is\nadaptable to the many languages for which large Wiktionaries exist.", "published": "2024-02-12 17:22:42", "link": "http://arxiv.org/abs/2402.07817v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aya Model: An Instruction Finetuned Open-Access Multilingual Language\n  Model", "abstract": "Recent breakthroughs in large language models (LLMs) have centered around a\nhandful of data-rich languages. What does it take to broaden access to\nbreakthroughs beyond first-class citizen languages? Our work introduces Aya, a\nmassively multilingual generative language model that follows instructions in\n101 languages of which over 50% are considered as lower-resourced. Aya\noutperforms mT0 and BLOOMZ on the majority of tasks while covering double the\nnumber of languages. We introduce extensive new evaluation suites that broaden\nthe state-of-art for multilingual eval across 99 languages -- including\ndiscriminative and generative tasks, human evaluation, and simulated win rates\nthat cover both held-out tasks and in-distribution performance. Furthermore, we\nconduct detailed investigations on the optimal finetuning mixture composition,\ndata pruning, as well as the toxicity, bias, and safety of our models. We\nopen-source our instruction datasets and our model at\nhttps://hf.co/CohereForAI/aya-101", "published": "2024-02-12 17:34:13", "link": "http://arxiv.org/abs/2402.07827v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Membership Inference Attacks Work on Large Language Models?", "abstract": "Membership inference attacks (MIAs) attempt to predict whether a particular\ndatapoint is a member of a target model's training data. Despite extensive\nresearch on traditional machine learning models, there has been limited work\nstudying MIA on the pre-training data of large language models (LLMs). We\nperform a large-scale evaluation of MIAs over a suite of language models (LMs)\ntrained on the Pile, ranging from 160M to 12B parameters. We find that MIAs\nbarely outperform random guessing for most settings across varying LLM sizes\nand domains. Our further analyses reveal that this poor performance can be\nattributed to (1) the combination of a large dataset and few training\niterations, and (2) an inherently fuzzy boundary between members and\nnon-members. We identify specific settings where LLMs have been shown to be\nvulnerable to membership inference and show that the apparent success in such\nsettings can be attributed to a distribution shift, such as when members and\nnon-members are drawn from the seemingly identical domain but with different\ntemporal ranges. We release our code and data as a unified benchmark package\nthat includes all existing MIAs, supporting future work.", "published": "2024-02-12 17:52:05", "link": "http://arxiv.org/abs/2402.07841v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Suppressing Pink Elephants with Direct Principle Feedback", "abstract": "Existing methods for controlling language models, such as RLHF and\nConstitutional AI, involve determining which LLM behaviors are desirable and\ntraining them into a language model. However, in many cases, it is desirable\nfor LLMs to be controllable at inference time, so that they can be used in\nmultiple contexts with diverse needs. We illustrate this with the Pink Elephant\nProblem: instructing an LLM to avoid discussing a certain entity (a ``Pink\nElephant''), and instead discuss a preferred entity (``Grey Elephant''). We\napply a novel simplification of Constitutional AI, Direct Principle Feedback,\nwhich skips the ranking of responses and uses DPO directly on critiques and\nrevisions. Our results show that after DPF fine-tuning on our synthetic Pink\nElephants dataset, our 13B fine-tuned LLaMA 2 model significantly outperforms\nLlama-2-13B-Chat and a prompted baseline, and performs as well as GPT-4 in on\nour curated test set assessing the Pink Elephant Problem.", "published": "2024-02-12 18:57:46", "link": "http://arxiv.org/abs/2402.07896v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Walia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and\n  Generative Datasets", "abstract": "Large language models (LLMs) have received a lot of attention in natural\nlanguage processing (NLP) research because of their exceptional performance in\nunderstanding and generating human languages. However, low-resource languages\nare left behind due to the unavailability of resources. In this work, we focus\non enhancing the LLaMA-2-Amharic model by integrating task-specific and\ngenerative datasets to improve language model performance for Amharic. We\ncompile an Amharic instruction fine-tuning dataset and fine-tuned\nLLaMA-2-Amharic model. The fine-tuned model shows promising results in\ndifferent NLP tasks. We open-source our dataset creation pipeline, instruction\ndatasets, trained models, and evaluation outputs to promote language-specific\nstudies on these models.", "published": "2024-02-12 19:25:11", "link": "http://arxiv.org/abs/2402.08015v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SALAD: Smart AI Language Assistant Daily", "abstract": "SALAD is an AI-driven language-learning application designed to help\nforeigners learn Japanese. It offers translations in Kanji-Kana-Romaji, speech\nrecognition, translated audio, vocabulary tracking, grammar explanations, and\nsongs generated from newly learned words. The app targets beginners and\nintermediate learners, aiming to make language acquisition more accessible and\nenjoyable. SALAD uses daily translations to enhance fluency and comfort in\ncommunication with native speakers. The primary objectives include effective\nJapanese language learning, user engagement, and progress tracking. A survey by\nus found that 39% of foreigners in Japan face discomfort in conversations with\nJapanese speakers. Over 60% of foreigners expressed confidence in SALAD's\nability to enhance their Japanese language skills. The app uses large language\nmodels, speech recognition, and diffusion models to bridge the language gap and\nfoster a more inclusive community in Japan.", "published": "2024-02-12 06:15:24", "link": "http://arxiv.org/abs/2402.07431v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "T-RAG: Lessons from the LLM Trenches", "abstract": "Large Language Models (LLM) have shown remarkable language capabilities\nfueling attempts to integrate them into applications across a wide range of\ndomains. An important application area is question answering over private\nenterprise documents where the main considerations are data security, which\nnecessitates applications that can be deployed on-prem, limited computational\nresources and the need for a robust application that correctly responds to\nqueries. Retrieval-Augmented Generation (RAG) has emerged as the most prominent\nframework for building LLM-based applications. While building a RAG is\nrelatively straightforward, making it robust and a reliable application\nrequires extensive customization and relatively deep knowledge of the\napplication domain. We share our experiences building and deploying an LLM\napplication for question answering over private organizational documents. Our\napplication combines the use of RAG with a finetuned open-source LLM.\nAdditionally, our system, which we call Tree-RAG (T-RAG), uses a tree structure\nto represent entity hierarchies within the organization. This is used to\ngenerate a textual description to augment the context when responding to user\nqueries pertaining to entities within the organization's hierarchy. Our\nevaluations, including a Needle in a Haystack test, show that this combination\nperforms better than a simple RAG or finetuning implementation. Finally, we\nshare some lessons learned based on our experiences building an LLM application\nfor real-world use.", "published": "2024-02-12 08:45:08", "link": "http://arxiv.org/abs/2402.07483v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "MAFIA: Multi-Adapter Fused Inclusive LanguAge Models", "abstract": "Pretrained Language Models (PLMs) are widely used in NLP for various tasks.\nRecent studies have identified various biases that such models exhibit and have\nproposed methods to correct these biases. However, most of the works address a\nlimited set of bias dimensions independently such as gender, race, or religion.\nMoreover, the methods typically involve finetuning the full model to maintain\nthe performance on the downstream task. In this work, we aim to modularly\ndebias a pretrained language model across multiple dimensions. Previous works\nextensively explored debiasing PLMs using limited US-centric counterfactual\ndata augmentation (CDA). We use structured knowledge and a large generative\nmodel to build a diverse CDA across multiple bias dimensions in a\nsemi-automated way. We highlight how existing debiasing methods do not consider\ninteractions between multiple societal biases and propose a debiasing model\nthat exploits the synergy amongst various societal biases and enables\nmulti-bias debiasing simultaneously. An extensive evaluation on multiple tasks\nand languages demonstrates the efficacy of our approach.", "published": "2024-02-12 09:41:00", "link": "http://arxiv.org/abs/2402.07519v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "FinLLM-B: When Large Language Models Meet Financial Breakout Trading", "abstract": "Trading range breakout is a key method in the technical analysis of financial\ntrading, widely employed by traders in financial markets such as stocks,\nfutures, and foreign exchange. However, distinguishing between true and false\nbreakout and providing the correct rationale cause significant challenges to\ninvestors. Traditional quantitative methods require large amounts of data and\ncannot directly present the reasoning process, making them less than perfect in\nthis field. Recently, large language models have achieved success in various\ndownstream applications, but their effectiveness in the domain of financial\nbreakout detection has been subpar. The reason is that the unique data and\nspecific knowledge are required in breakout detection. To address these issues,\nwe create the first financial breakout dataset and introduce FinLLM-B, the\npremier large language model for financial breakout detection, which enhances\nthe effectiveness of breakout trading strategies. Furthermore, we have\ndeveloped a novel framework for large language models, namely multi-stage\nstructure, effectively reducing mistakes in downstream applications.\nExperimental results indicate that compared to GPT-3.5, FinLLM-B improves the\naverage accuracy of answers and rational by 49.97%, with the multi-stage\nstructure contributing 9.72% to the improvement. Additionally, it outperforms\nChatGPT-4 by 42.38%.", "published": "2024-02-12 10:04:07", "link": "http://arxiv.org/abs/2402.07536v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping", "abstract": "Self-alignment is an effective way to reduce the cost of human annotation\nwhile ensuring promising model capability. However, most current methods\ncomplete the data collection and training steps in a single round, which may\noverlook the continuously improving ability of self-aligned models. This gives\nrise to a key query: What if we do multi-time bootstrapping self-alignment?\nDoes this strategy enhance model performance or lead to rapid degradation? In\nthis paper, our pioneering exploration delves into the impact of bootstrapping\nself-alignment on large language models. Our findings reveal that bootstrapping\nself-alignment markedly surpasses the single-round approach, by guaranteeing\ndata diversity from in-context learning. To further exploit the capabilities of\nbootstrapping, we investigate and adjust the training order of data, which\nyields improved performance of the model. Drawing on these findings, we propose\nStep-On-Feet Tuning (SOFT) which leverages model's continuously enhanced\nfew-shot ability to boost zero or one-shot performance. Based on easy-to-hard\ntraining recipe, we propose SOFT+ which further boost self-alignment's\nperformance. Our experiments demonstrate the efficiency of SOFT (SOFT+) across\nvarious classification and generation tasks, highlighting the potential of\nbootstrapping self-alignment on continually enhancing model alignment\nperformance.", "published": "2024-02-12 12:30:42", "link": "http://arxiv.org/abs/2402.07610v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Anchor-based Large Language Models", "abstract": "Large language models (LLMs) predominantly employ decoder-only transformer\narchitectures, necessitating the retention of keys/values information for\nhistorical tokens to provide contextual information and avoid redundant\ncomputation. However, the substantial size and parameter volume of these LLMs\nrequire massive GPU memory. This memory demand increases with the length of the\ninput text, leading to an urgent need for more efficient methods of information\nstorage and processing. This study introduces Anchor-based LLMs (AnLLMs), which\nutilize an innovative anchor-based self-attention network (AnSAN) and also an\nanchor-based inference strategy. This approach enables LLMs to compress\nsequence information into an anchor token, reducing the keys/values cache and\nenhancing inference efficiency. Experiments on question-answering benchmarks\nreveal that AnLLMs maintain similar accuracy levels while achieving up to 99%\nkeys/values cache reduction and up to 3.5 times faster inference. Despite a\nminor compromise in accuracy, the substantial enhancements of AnLLMs employing\nthe AnSAN technique in resource utilization and computational efficiency\nunderscore their potential for practical LLM applications.", "published": "2024-02-12 12:48:02", "link": "http://arxiv.org/abs/2402.07616v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models \"Ad Referendum\": How Good Are They at Machine\n  Translation in the Legal Domain?", "abstract": "This study evaluates the machine translation (MT) quality of two\nstate-of-the-art large language models (LLMs) against a tradition-al neural\nmachine translation (NMT) system across four language pairs in the legal\ndomain. It combines automatic evaluation met-rics (AEMs) and human evaluation\n(HE) by professional transla-tors to assess translation ranking, fluency and\nadequacy. The re-sults indicate that while Google Translate generally\noutperforms LLMs in AEMs, human evaluators rate LLMs, especially GPT-4,\ncomparably or slightly better in terms of producing contextually adequate and\nfluent translations. This discrepancy suggests LLMs' potential in handling\nspecialized legal terminology and context, highlighting the importance of human\nevaluation methods in assessing MT quality. The study underscores the evolving\ncapabil-ities of LLMs in specialized domains and calls for reevaluation of\ntraditional AEMs to better capture the nuances of LLM-generated translations.", "published": "2024-02-12 14:40:54", "link": "http://arxiv.org/abs/2402.07681v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "OrderBkd: Textual backdoor attack through repositioning", "abstract": "The use of third-party datasets and pre-trained machine learning models poses\na threat to NLP systems due to possibility of hidden backdoor attacks. Existing\nattacks involve poisoning the data samples such as insertion of tokens or\nsentence paraphrasing, which either alter the semantics of the original texts\nor can be detected. Our main difference from the previous work is that we use\nthe reposition of a two words in a sentence as a trigger. By designing and\napplying specific part-of-speech (POS) based rules for selecting these tokens,\nwe maintain high attack success rate on SST-2 and AG classification datasets\nwhile outperforming existing attacks in terms of perplexity and semantic\nsimilarity to the clean samples. In addition, we show the robustness of our\nattack to the ONION defense method. All the code and data for the paper can be\nobtained at https://github.com/alekseevskaia/OrderBkd.", "published": "2024-02-12 14:53:37", "link": "http://arxiv.org/abs/2402.07689v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation", "abstract": "Low-Rank Adaptation (LoRA) is currently the most commonly used\nParameter-efficient fine-tuning (PEFT) method, it introduces auxiliary\nparameters for each layer to fine-tune the pre-trained model under limited\ncomputing resources. However, it still faces resource consumption challenges\nduring training when scaling up to larger models. Most previous studies have\ntackled this issue by using pruning techniques, which involve removing LoRA\nparameters deemed unimportant. Nonetheless, these efforts only analyze LoRA\nparameter features to evaluate their importance, such as parameter count, size,\nand gradient. In fact, the output of LoRA (product of LoRA parameter and hidden\nstate), directly impacts the final results. Preliminary experiments indicate\nthat a fraction of LoRA elements possesses significantly high output values,\nsubstantially influencing the layer output. Motivated by the observation, we\npropose LoRA-drop. Concretely, LoRA-drop evaluates the importance of LoRA based\non the LoRA output. Then we retain LoRA for important layers and the other\nlayers share the same LoRA. We conduct abundant experiments with models of\ndifferent scales on NLU and NLG tasks. Results demonstrate that LoRA-drop can\nachieve performance comparable to full fine-tuning and LoRA, while retaining\n50\\% of the LoRA parameters on average.", "published": "2024-02-12 15:34:56", "link": "http://arxiv.org/abs/2402.07721v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Asking Multimodal Clarifying Questions in Mixed-Initiative\n  Conversational Search", "abstract": "In mixed-initiative conversational search systems, clarifying questions are\nused to help users who struggle to express their intentions in a single query.\nThese questions aim to uncover user's information needs and resolve query\nambiguities. We hypothesize that in scenarios where multimodal information is\npertinent, the clarification process can be improved by using non-textual\ninformation. Therefore, we propose to add images to clarifying questions and\nformulate the novel task of asking multimodal clarifying questions in\nopen-domain, mixed-initiative conversational search systems. To facilitate\nresearch into this task, we collect a dataset named Melon that contains over 4k\nmultimodal clarifying questions, enriched with over 14k images. We also propose\na multimodal query clarification model named Marto and adopt a prompt-based,\ngenerative fine-tuning strategy to perform the training of different stages\nwith different prompts. Several analyses are conducted to understand the\nimportance of multimodal contents during the query clarification phase.\nExperimental results indicate that the addition of images leads to significant\nimprovements of up to 90% in retrieval performance when selecting the relevant\nimages. Extensive analyses are also performed to show the superiority of Marto\ncompared with discriminative baselines in terms of effectiveness and\nefficiency.", "published": "2024-02-12 16:04:01", "link": "http://arxiv.org/abs/2402.07742v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Extensible Multi-Granularity Fusion Network for Aspect-based Sentiment\n  Analysis", "abstract": "Aspect-based Sentiment Analysis (ABSA) evaluates sentiment expressions within\na text to comprehend sentiment information. Previous studies integrated\nexternal knowledge, such as knowledge graphs, to enhance the semantic features\nin ABSA models. Recent research has examined the use of Graph Neural Networks\n(GNNs) on dependency and constituent trees for syntactic analysis. With the\nongoing development of ABSA, more innovative linguistic and structural features\nare being incorporated (e.g. latent graph), but this also introduces complexity\nand confusion. As of now, a scalable framework for integrating diverse\nlinguistic and structural features into ABSA does not exist. This paper\npresents the Extensible Multi-Granularity Fusion (EMGF) network, which\nintegrates information from dependency and constituent syntactic, attention\nsemantic , and external knowledge graphs. EMGF, equipped with multi-anchor\ntriplet learning and orthogonal projection, efficiently harnesses the combined\npotential of each granularity feature and their synergistic interactions,\nresulting in a cumulative effect without additional computational expenses.\nExperimental findings on SemEval 2014 and Twitter datasets confirm EMGF's\nsuperiority over existing ABSA methods.", "published": "2024-02-12 16:52:26", "link": "http://arxiv.org/abs/2402.07787v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Mercury: A Code Efficiency Benchmark for Code Large Language Models", "abstract": "Amidst the recent strides in evaluating Large Language Models for Code (Code\nLLMs), existing benchmarks have mainly focused on the functional correctness of\ngenerated code, neglecting the importance of their computational efficiency. To\nfill the gap, we present Mercury, the first code efficiency benchmark for Code\nLLMs. It comprises 1,889 Python tasks, each accompanied by adequate solutions\nthat serve as real-world efficiency baselines, enabling a comprehensive\nanalysis of the runtime distribution. Based on the distribution, we introduce a\nnew metric Beyond, which computes a runtime-percentile-weighted Pass score to\nreflect functional correctness and code efficiency simultaneously. On Mercury,\nleading Code LLMs can achieve 65% on Pass, while less than 50% on Beyond. Given\nthat an ideal Beyond score would be aligned with the Pass score, it indicates\nthat while Code LLMs exhibit impressive capabilities in generating functionally\ncorrect code, there remains a notable gap in their efficiency. Finally, our\nempirical experiments reveal that Direct Preference Optimization (DPO) serves\nas a robust baseline for enhancing code efficiency compared with Supervised\nFine Tuning (SFT), which paves a promising avenue for future exploration of\nefficient code generation. Our code and data are available on GitHub:\nhttps://github.com/Elfsong/Mercury.", "published": "2024-02-12 17:53:22", "link": "http://arxiv.org/abs/2402.07844v4", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Lissard: Long and Simple Sequential Reasoning Datasets", "abstract": "Language models are now capable of solving tasks that require dealing with\nlong sequences consisting of hundreds of thousands of tokens. However, they\noften fail on tasks that require repetitive use of simple rules, even on\nsequences that are much shorter than those seen during training. For example,\nstate-of-the-art LLMs can find common items in two lists with up to 20 items\nbut fail when lists have 80 items. In this paper, we introduce Lissard, a\nbenchmark comprising seven tasks whose goal is to assess the ability of models\nto process and generate wide-range sequence lengths, requiring repetitive\nprocedural execution. Our evaluation of open-source (Mistral-7B and\nMixtral-8x7B) and proprietary models (GPT-3.5 and GPT-4) show a consistent\ndecline in performance across all models as the complexity of the sequence\nincreases. The datasets and code are available at\nhttps://github.com/unicamp-dl/Lissard", "published": "2024-02-12 18:10:17", "link": "http://arxiv.org/abs/2402.07859v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Label-Efficient Model Selection for Text Generation", "abstract": "Model selection for a given target task can be costly, as it may entail\nextensive annotation of the quality of outputs of different models. We\nintroduce DiffUse, an efficient method to make an informed decision between\ncandidate text generation models based on preference annotations. DiffUse\nreduces the required amount of annotations, thus saving valuable time and\nresources in performing evaluation. DiffUse intelligently selects instances by\nclustering embeddings that represent the semantic differences between model\noutputs. Thus, it is able to identify a subset of examples that are more\ninformative for preference decisions. Our method is model-agnostic, and can be\napplied to any text generation model for selecting between models, prompts and\nconfigurations. Moreover, we propose a practical iterative approach for\ndynamically determining how many instances to annotate. In a series of\nexperiments over hundreds of model pairs, we demonstrate that DiffUse can\ndramatically reduce the required number of annotations -- by up to 75% -- while\nmaintaining high evaluation reliability.", "published": "2024-02-12 18:54:02", "link": "http://arxiv.org/abs/2402.07891v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A systematic investigation of learnability from single child linguistic\n  input", "abstract": "Language models (LMs) have demonstrated remarkable proficiency in generating\nlinguistically coherent text, sparking discussions about their relevance to\nunderstanding human language learnability. However, a significant gap exists\nbetween the training data for these models and the linguistic input a child\nreceives. LMs are typically trained on data that is orders of magnitude larger\nand fundamentally different from child-directed speech (Warstadt and Bowman,\n2022; Warstadt et al., 2023; Frank, 2023a). Addressing this discrepancy, our\nresearch focuses on training LMs on subsets of a single child's linguistic\ninput. Previously, Wang, Vong, Kim, and Lake (2023) found that LMs trained in\nthis setting can form syntactic and semantic word clusters and develop\nsensitivity to certain linguistic phenomena, but they only considered LSTMs and\nsimpler neural networks trained from just one single-child dataset. Here, to\nexamine the robustness of learnability from single-child input, we\nsystematically train six different model architectures on five datasets (3\nsingle-child and 2 baselines). We find that the models trained on single-child\ndatasets showed consistent results that matched with previous work,\nunderscoring the robustness of forming meaningful syntactic and semantic\nrepresentations from a subset of a child's linguistic input.", "published": "2024-02-12 18:58:58", "link": "http://arxiv.org/abs/2402.07899v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Refined Direct Preference Optimization with Synthetic Data for\n  Behavioral Alignment of LLMs", "abstract": "In this paper, we introduce \\emph{refined Direct Preference Optimization}\n(rDPO), a method for improving the behavioral alignment of Large Language\nModels (LLMs) without the need for human-annotated data. The method involves\ncreating synthetic data using self-critique prompting by a teacher LLM and then\nutilising a generalized DPO loss function to distil to a student LLM. The loss\nfunction incorporates an additional external reward model to improve the\nquality of synthetic data, making rDPO robust to potential noise in the\nsynthetic dataset. rDPO is shown to be effective in a diverse set of\nbehavioural alignment tasks, such as improved safety, robustness against\nrole-playing, and reduced sycophancy. Code to be released at\nhttps://github.com/vicgalle/refined-dpo.", "published": "2024-02-12 19:10:13", "link": "http://arxiv.org/abs/2402.08005v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Careless Whisper: Speech-to-Text Hallucination Harms", "abstract": "Speech-to-text services aim to transcribe input audio as accurately as\npossible. They increasingly play a role in everyday life, for example in\npersonal voice assistants or in customer-company interactions. We evaluate Open\nAI's Whisper, a state-of-the-art automated speech recognition service\noutperforming industry competitors, as of 2023. While many of Whisper's\ntranscriptions were highly accurate, we find that roughly 1\\% of audio\ntranscriptions contained entire hallucinated phrases or sentences which did not\nexist in any form in the underlying audio. We thematically analyze the\nWhisper-hallucinated content, finding that 38\\% of hallucinations include\nexplicit harms such as perpetuating violence, making up inaccurate\nassociations, or implying false authority. We then study why hallucinations\noccur by observing the disparities in hallucination rates between speakers with\naphasia (who have a lowered ability to express themselves using speech and\nvoice) and a control group. We find that hallucinations disproportionately\noccur for individuals who speak with longer shares of non-vocal durations -- a\ncommon symptom of aphasia. We call on industry practitioners to ameliorate\nthese language-model-based hallucinations in Whisper, and to raise awareness of\npotential biases amplified by hallucinations in downstream applications of\nspeech-to-text models.", "published": "2024-02-12 19:35:37", "link": "http://arxiv.org/abs/2402.08021v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Beyond LLMs: Advancing the Landscape of Complex Reasoning", "abstract": "Since the advent of Large Language Models a few years ago, they have often\nbeen considered the de facto solution for many AI problems. However, in\naddition to the many deficiencies of LLMs that prevent them from broad industry\nadoption, such as reliability, cost, and speed, there is a whole class of\ncommon real world problems that Large Language Models perform poorly on,\nnamely, constraint satisfaction and optimization problems. These problems are\nubiquitous and current solutions are highly specialized and expensive to\nimplement. At Elemental Cognition, we developed our EC AI platform which takes\na neuro-symbolic approach to solving constraint satisfaction and optimization\nproblems. The platform employs, at its core, a precise and high performance\nlogical reasoning engine, and leverages LLMs for knowledge acquisition and user\ninteraction. This platform supports developers in specifying application logic\nin natural and concise language while generating application user interfaces to\ninteract with users effectively. We evaluated LLMs against systems built on the\nEC AI platform in three domains and found the EC AI systems to significantly\noutperform LLMs on constructing valid and optimal solutions, on validating\nproposed solutions, and on repairing invalid solutions.", "published": "2024-02-12 21:14:45", "link": "http://arxiv.org/abs/2402.08064v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Large Language Models as Agents in Two-Player Games", "abstract": "By formally defining the training processes of large language models (LLMs),\nwhich usually encompasses pre-training, supervised fine-tuning, and\nreinforcement learning with human feedback, within a single and unified machine\nlearning paradigm, we can glean pivotal insights for advancing LLM\ntechnologies. This position paper delineates the parallels between the training\nmethods of LLMs and the strategies employed for the development of agents in\ntwo-player games, as studied in game theory, reinforcement learning, and\nmulti-agent systems. We propose a re-conceptualization of LLM learning\nprocesses in terms of agent learning in language-based games. This framework\nunveils innovative perspectives on the successes and challenges in LLM\ndevelopment, offering a fresh understanding of addressing alignment issues\namong other strategic considerations. Furthermore, our two-player game approach\nsheds light on novel data preparation and machine learning techniques for\ntraining LLMs.", "published": "2024-02-12 21:44:32", "link": "http://arxiv.org/abs/2402.08078v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Investigating the Impact of Data Contamination of Large Language Models\n  in Text-to-SQL Translation", "abstract": "Understanding textual description to generate code seems to be an achieved\ncapability of instruction-following Large Language Models (LLMs) in zero-shot\nscenario. However, there is a severe possibility that this translation ability\nmay be influenced by having seen target textual descriptions and the related\ncode. This effect is known as Data Contamination.\n  In this study, we investigate the impact of Data Contamination on the\nperformance of GPT-3.5 in the Text-to-SQL code-generating tasks. Hence, we\nintroduce a novel method to detect Data Contamination in GPTs and examine\nGPT-3.5's Text-to-SQL performances using the known Spider Dataset and our new\nunfamiliar dataset Termite. Furthermore, we analyze GPT-3.5's efficacy on\ndatabases with modified information via an adversarial table disconnection\n(ATD) approach, complicating Text-to-SQL tasks by removing structural pieces of\ninformation from the database. Our results indicate a significant performance\ndrop in GPT-3.5 on the unfamiliar Termite dataset, even with ATD modifications,\nhighlighting the effect of Data Contamination on LLMs in Text-to-SQL\ntranslation tasks.", "published": "2024-02-12 22:35:40", "link": "http://arxiv.org/abs/2402.08100v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Addressing cognitive bias in medical language models", "abstract": "There is increasing interest in the application large language models (LLMs)\nto the medical field, in part because of their impressive performance on\nmedical exam questions. While promising, exam questions do not reflect the\ncomplexity of real patient-doctor interactions. In reality, physicians'\ndecisions are shaped by many complex factors, such as patient compliance,\npersonal experience, ethical beliefs, and cognitive bias. Taking a step toward\nunderstanding this, our hypothesis posits that when LLMs are confronted with\nclinical questions containing cognitive biases, they will yield significantly\nless accurate responses compared to the same questions presented without such\nbiases. In this study, we developed BiasMedQA, a benchmark for evaluating\ncognitive biases in LLMs applied to medical tasks. Using BiasMedQA we evaluated\nsix LLMs, namely GPT-4, Mixtral-8x70B, GPT-3.5, PaLM-2, Llama 2 70B-chat, and\nthe medically specialized PMC Llama 13B. We tested these models on 1,273\nquestions from the US Medical Licensing Exam (USMLE) Steps 1, 2, and 3,\nmodified to replicate common clinically-relevant cognitive biases. Our analysis\nrevealed varying effects for biases on these LLMs, with GPT-4 standing out for\nits resilience to bias, in contrast to Llama 2 70B-chat and PMC Llama 13B,\nwhich were disproportionately affected by cognitive bias. Our findings\nhighlight the critical need for bias mitigation in the development of medical\nLLMs, pointing towards safer and more reliable applications in healthcare.", "published": "2024-02-12 23:08:37", "link": "http://arxiv.org/abs/2402.08113v3", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Assessing Generalization for Subpopulation Representative Modeling via\n  In-Context Learning", "abstract": "This study evaluates the ability of Large Language Model (LLM)-based\nSubpopulation Representative Models (SRMs) to generalize from empirical data,\nutilizing in-context learning with data from the 2016 and 2020 American\nNational Election Studies. We explore generalization across response variables\nand demographic subgroups. While conditioning with empirical data improves\nperformance on the whole, the benefit of in-context learning varies\nconsiderably across demographics, sometimes hurting performance for one\ndemographic while helping performance for others. The inequitable benefits of\nin-context learning for SRM present a challenge for practitioners implementing\nSRMs, and for decision-makers who might come to rely on them. Our work\nhighlights a need for fine-grained benchmarks captured from diverse\nsubpopulations that test not only fidelity but generalization.", "published": "2024-02-12 01:55:51", "link": "http://arxiv.org/abs/2402.07368v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Making Flow-Matching-Based Zero-Shot Text-to-Speech Laugh as You Like", "abstract": "Laughter is one of the most expressive and natural aspects of human speech,\nconveying emotions, social cues, and humor. However, most text-to-speech (TTS)\nsystems lack the ability to produce realistic and appropriate laughter sounds,\nlimiting their applications and user experience. While there have been prior\nworks to generate natural laughter, they fell short in terms of controlling the\ntiming and variety of the laughter to be generated. In this work, we propose\nELaTE, a zero-shot TTS that can generate natural laughing speech of any speaker\nbased on a short audio prompt with precise control of laughter timing and\nexpression. Specifically, ELaTE works on the audio prompt to mimic the voice\ncharacteristic, the text prompt to indicate the contents of the generated\nspeech, and the input to control the laughter expression, which can be either\nthe start and end times of laughter, or the additional audio prompt that\ncontains laughter to be mimicked. We develop our model based on the foundation\nof conditional flow-matching-based zero-shot TTS, and fine-tune it with\nframe-level representation from a laughter detector as additional conditioning.\nWith a simple scheme to mix small-scale laughter-conditioned data with\nlarge-scale pre-training data, we demonstrate that a pre-trained zero-shot TTS\nmodel can be readily fine-tuned to generate natural laughter with precise\ncontrollability, without losing any quality of the pre-trained zero-shot TTS\nmodel. Through objective and subjective evaluations, we show that ELaTE can\ngenerate laughing speech with significantly higher quality and controllability\ncompared to conventional models. See https://aka.ms/elate/ for demo samples.", "published": "2024-02-12 02:58:10", "link": "http://arxiv.org/abs/2402.07383v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Leveraging AI to Advance Science and Computing Education across Africa:\n  Challenges, Progress and Opportunities", "abstract": "Across the African continent, students grapple with various educational\nchallenges, including limited access to essential resources such as computers,\ninternet connectivity, reliable electricity, and a shortage of qualified\nteachers. Despite these challenges, recent advances in AI such as BERT, and\nGPT-4 have demonstrated their potential for advancing education. Yet, these AI\ntools tend to be deployed and evaluated predominantly within the context of\nWestern educational settings, with limited attention directed towards the\nunique needs and challenges faced by students in Africa. In this chapter, we\ndiscuss challenges with using AI to advance education across Africa. Then, we\ndescribe our work developing and deploying AI in Education tools in Africa for\nscience and computing education: (1) SuaCode, an AI-powered app that enables\nAfricans to learn to code using their smartphones, (2) AutoGrad, an automated\ngrading, and feedback tool for graphical and interactive coding assignments,\n(3) a tool for code plagiarism detection that shows visual evidence of\nplagiarism, (4) Kwame, a bilingual AI teaching assistant for coding courses,\n(5) Kwame for Science, a web-based AI teaching assistant that provides instant\nanswers to students' science questions and (6) Brilla AI, an AI contestant for\nthe National Science and Maths Quiz competition. Finally, we discuss potential\nopportunities to leverage AI to advance education across Africa.", "published": "2024-02-12 04:10:09", "link": "http://arxiv.org/abs/2402.07397v2", "categories": ["cs.CY", "cs.CL", "cs.HC"], "primary_category": "cs.CY"}
{"title": "The Balancing Act: Unmasking and Alleviating ASR Biases in Portuguese", "abstract": "In the field of spoken language understanding, systems like Whisper and\nMultilingual Massive Speech (MMS) have shown state-of-the-art performances.\nThis study is dedicated to a comprehensive exploration of the Whisper and MMS\nsystems, with a focus on assessing biases in automatic speech recognition (ASR)\ninherent to casual conversation speech specific to the Portuguese language. Our\ninvestigation encompasses various categories, including gender, age, skin tone\ncolor, and geo-location. Alongside traditional ASR evaluation metrics such as\nWord Error Rate (WER), we have incorporated p-value statistical significance\nfor gender bias analysis. Furthermore, we extensively examine the impact of\ndata distribution and empirically show that oversampling techniques alleviate\nsuch stereotypical biases. This research represents a pioneering effort in\nquantifying biases in the Portuguese language context through the application\nof MMS and Whisper, contributing to a better understanding of ASR systems'\nperformance in multilingual settings.", "published": "2024-02-12 09:35:13", "link": "http://arxiv.org/abs/2402.07513v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "PKG API: A Tool for Personal Knowledge Graph Management", "abstract": "Personal knowledge graphs (PKGs) offer individuals a way to store and\nconsolidate their fragmented personal data in a central place, improving\nservice personalization while maintaining full user control. Despite their\npotential, practical PKG implementations with user-friendly interfaces remain\nscarce. This work addresses this gap by proposing a complete solution to\nrepresent, manage, and interface with PKGs. Our approach includes (1) a\nuser-facing PKG Client, enabling end-users to administer their personal data\neasily via natural language statements, and (2) a service-oriented PKG API. To\ntackle the complexity of representing these statements within a PKG, we present\nan RDF-based PKG vocabulary that supports this, along with properties for\naccess rights and provenance.", "published": "2024-02-12 10:09:16", "link": "http://arxiv.org/abs/2402.07540v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Show Me How It's Done: The Role of Explanations in Fine-Tuning Language\n  Models", "abstract": "Our research demonstrates the significant benefits of using fine-tuning with\nexplanations to enhance the performance of language models. Unlike prompting,\nwhich maintains the model's parameters, fine-tuning allows the model to learn\nand update its parameters during a training phase. In this study, we applied\nfine-tuning to various sized language models using data that contained\nexplanations of the output rather than merely presenting the answers. We found\nthat even smaller language models with as few as 60 million parameters\nbenefited substantially from this approach. Interestingly, our results\nindicated that the detailed explanations were more beneficial to smaller models\nthan larger ones, with the latter gaining nearly the same advantage from any\nform of explanation, irrespective of its length. Additionally, we demonstrate\nthat the inclusion of explanations enables the models to solve tasks that they\nwere not able to solve without explanations. Lastly, we argue that despite the\nchallenging nature of adding explanations, samples that contain explanations\nnot only reduce the volume of data required for training but also promote a\nmore effective generalization by the model. In essence, our findings suggest\nthat fine-tuning with explanations significantly bolsters the performance of\nlarge language models.", "published": "2024-02-12 10:11:50", "link": "http://arxiv.org/abs/2402.07543v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Autonomous Data Selection with Zero-shot Generative Classifiers for\n  Mathematical Texts", "abstract": "We present Autonomous Data Selection (AutoDS), a method that leverages base\nlanguage models themselves as zero-shot \"generative classifiers\" to\nautomatically curate high-quality mathematical texts. Unlike prior approaches\nthat require human annotations or training a dedicated data filter, AutoDS\nrelies solely on a model's logits to determine whether a given passage is\nmathematically informative and educational. By integrating AutoDS into a\ncontinual pretraining pipeline, we substantially boost downstream performance\non challenging math benchmarks (MATH, GSM8K, and BBH) while using far fewer\ntokens than previous methods. Empirically, our approach achieves roughly a\ntwofold improvement in pretraining token efficiency over strong baselines,\nunderscoring the potential of self-directed data selection in enhancing\nmathematical reasoning. We release our curated AutoMathText dataset to\nfacilitate future research in automated domain-specific data curation. The\nAutoMathText dataset is available at\nhttps://huggingface.co/datasets/math-ai/AutoMathText. The code is available at\nhttps://github.com/yifanzhang-pro/AutoMathText.", "published": "2024-02-12 13:09:21", "link": "http://arxiv.org/abs/2402.07625v5", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Sound of Healthcare: Improving Medical Transcription ASR Accuracy\n  with Large Language Models", "abstract": "In the rapidly evolving landscape of medical documentation, transcribing\nclinical dialogues accurately is increasingly paramount. This study explores\nthe potential of Large Language Models (LLMs) to enhance the accuracy of\nAutomatic Speech Recognition (ASR) systems in medical transcription. Utilizing\nthe PriMock57 dataset, which encompasses a diverse range of primary care\nconsultations, we apply advanced LLMs to refine ASR-generated transcripts. Our\nresearch is multifaceted, focusing on improvements in general Word Error Rate\n(WER), Medical Concept WER (MC-WER) for the accurate transcription of essential\nmedical terms, and speaker diarization accuracy. Additionally, we assess the\nrole of LLM post-processing in improving semantic textual similarity, thereby\npreserving the contextual integrity of clinical dialogues. Through a series of\nexperiments, we compare the efficacy of zero-shot and Chain-of-Thought (CoT)\nprompting techniques in enhancing diarization and correction accuracy. Our\nfindings demonstrate that LLMs, particularly through CoT prompting, not only\nimprove the diarization accuracy of existing ASR systems but also achieve\nstate-of-the-art performance in this domain. This improvement extends to more\naccurately capturing medical concepts and enhancing the overall semantic\ncoherence of the transcribed dialogues. These findings illustrate the dual role\nof LLMs in augmenting ASR outputs and independently excelling in transcription\ntasks, holding significant promise for transforming medical ASR systems and\nleading to more accurate and reliable patient records in healthcare settings.", "published": "2024-02-12 14:01:12", "link": "http://arxiv.org/abs/2402.07658v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "AIR-Bench: Benchmarking Large Audio-Language Models via Generative\n  Comprehension", "abstract": "Recently, instruction-following audio-language models have received broad\nattention for human-audio interaction. However, the absence of benchmarks\ncapable of evaluating audio-centric interaction capabilities has impeded\nadvancements in this field. Previous models primarily focus on assessing\ndifferent fundamental tasks, such as Automatic Speech Recognition (ASR), and\nlack an assessment of the open-ended generative capabilities centered around\naudio. Thus, it is challenging to track the progression in the Large\nAudio-Language Models (LALMs) domain and to provide guidance for future\nimprovement. In this paper, we introduce AIR-Bench (\\textbf{A}udio\n\\textbf{I}nst\\textbf{R}uction \\textbf{Bench}mark), the first benchmark designed\nto evaluate the ability of LALMs to understand various types of audio signals\n(including human speech, natural sounds, and music), and furthermore, to\ninteract with humans in the textual format. AIR-Bench encompasses two\ndimensions: \\textit{foundation} and \\textit{chat} benchmarks. The former\nconsists of 19 tasks with approximately 19k single-choice questions, intending\nto inspect the basic single-task ability of LALMs. The latter one contains 2k\ninstances of open-ended question-and-answer data, directly assessing the\ncomprehension of the model on complex audio and its capacity to follow\ninstructions. Both benchmarks require the model to generate hypotheses\ndirectly. We design a unified framework that leverages advanced language\nmodels, such as GPT-4, to evaluate the scores of generated hypotheses given the\nmeta-information of the audio. Experimental results demonstrate a high level of\nconsistency between GPT-4-based evaluation and human evaluation. By revealing\nthe limitations of existing LALMs through evaluation results, AIR-Bench can\nprovide insights into the direction of future research.", "published": "2024-02-12 15:41:22", "link": "http://arxiv.org/abs/2402.07729v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards Unified Alignment Between Agents, Humans, and Environment", "abstract": "The rapid progress of foundation models has led to the prosperity of\nautonomous agents, which leverage the universal capabilities of foundation\nmodels to conduct reasoning, decision-making, and environmental interaction.\nHowever, the efficacy of agents remains limited when operating in intricate,\nrealistic environments. In this work, we introduce the principles of\n$\\mathbf{U}$nified $\\mathbf{A}$lignment for $\\mathbf{A}$gents\n($\\mathbf{UA}^2$), which advocate for the simultaneous alignment of agents with\nhuman intentions, environmental dynamics, and self-constraints such as the\nlimitation of monetary budgets. From the perspective of $\\mathbf{UA}^2$, we\nreview the current agent research and highlight the neglected factors in\nexisting agent benchmarks and method candidates. We also conduct\nproof-of-concept studies by introducing realistic features to WebShop,\nincluding user profiles to demonstrate intentions, personalized reranking for\ncomplex environmental dynamics, and runtime cost statistics to reflect\nself-constraints. We then follow the principles of $\\mathbf{UA}^2$ to propose\nan initial design of our agent, and benchmark its performance with several\ncandidate baselines in the retrofitted WebShop. The extensive experimental\nresults further prove the importance of the principles of $\\mathbf{UA}^2$. Our\nresearch sheds light on the next steps of autonomous agent research with\nimproved general problem-solving abilities.", "published": "2024-02-12 16:14:22", "link": "http://arxiv.org/abs/2402.07744v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language\n  Models", "abstract": "Recently, diffusion models have garnered significant interest in the field of\ntext processing due to their many potential advantages compared to conventional\nautoregressive models. In this work, we propose Diffusion-of-Thought (DoT), a\nnovel approach that integrates diffusion models with Chain-of-Thought, a\nwell-established technique for improving the reasoning ability of\nautoregressive language models. In contrast to autoregressive language models\nthat make decisions in a left-to-right, token-by-token manner, DoT allows\nreasoning steps to diffuse over time through a diffusion language model and\noffers greater flexibility in trading-off computation for reasoning\nperformance. Our experimental results demonstrate the effectiveness of DoT in\nmulti-digit multiplication, boolean logic, and grade school math problems, with\na small diffusion model outperforming a much larger autoregressive model in\nboth efficiency and accuracy. In addition to that, DoT showcases promising\nself-correction abilities and benefits from existing reasoning-enhancing\ntechniques like self-consistency decoding. Our findings contribute to the\nunderstanding and development of reasoning with diffusion language models.", "published": "2024-02-12 16:23:28", "link": "http://arxiv.org/abs/2402.07754v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Had enough of experts? Quantitative knowledge retrieval from large\n  language models", "abstract": "Large language models (LLMs) have been extensively studied for their\nabilities to generate convincing natural language sequences, however their\nutility for quantitative information retrieval is less well understood. Here we\nexplore the feasibility of LLMs as a mechanism for quantitative knowledge\nretrieval to aid two data analysis tasks: elicitation of prior distributions\nfor Bayesian models and imputation of missing data. We introduce a framework\nthat leverages LLMs to enhance Bayesian workflows by eliciting expert-like\nprior knowledge and imputing missing data. Tested on diverse datasets, this\napproach can improve predictive accuracy and reduce data requirements, offering\nsignificant potential in healthcare, environmental science and engineering\napplications. We discuss the implications and challenges of treating LLMs as\n'experts'.", "published": "2024-02-12 16:32:37", "link": "http://arxiv.org/abs/2402.07770v2", "categories": ["cs.IR", "cs.CL", "stat.AP"], "primary_category": "cs.IR"}
{"title": "Retrieval Augmented Thought Process for Private Data Handling in\n  Healthcare", "abstract": "Large Language Models (LLMs) have demonstrated the strong potential to assist\nboth clinicians and the general public with their extensive medical knowledge.\nHowever, their application in healthcare is constrained due to concerns about\nthe privacy of data used in training, which prevents the integration of private\nand personal information because of security and ethical issues. Moreover, if\ntheir capabilities can be enhanced with information retrieval to access\nup-to-date knowledge, the current integration of LLMs with Information\nretrieval lacks robustness to imperfect retrieval, which can hinder their\neffectiveness and even reduce overall performance. In this work, we address\nthis challenge by introducing the Retrieval-Augmented Thought Process (RATP).\nGiven access to external knowledge, RATP formulates the thought generation of\nLLMs as a multiple-step decision process. To optimise such a thought process,\nRATP leverages Monte-Carlo Tree Search and learns a proxy reward function that\npermits cost-efficient inference. On a private dataset of electronic medical\nrecords, deliberately excluded from any LLM training set, RATP achieves 35%\nadditional accuracy compared to in-context retrieval-augmented generation for\nthe question-answering task.", "published": "2024-02-12 17:17:50", "link": "http://arxiv.org/abs/2402.07812v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "H.3.3; I.2.6; I.2.7; I.2.8"], "primary_category": "cs.CL"}
{"title": "Differentially Private Zeroth-Order Methods for Scalable Large Language\n  Model Finetuning", "abstract": "Fine-tuning on task-specific datasets is a widely-embraced paradigm of\nharnessing the powerful capability of pretrained LLMs for various downstream\ntasks. Due to the popularity of LLMs fine-tuning and its accompanying privacy\nconcerns, differentially private (DP) fine-tuning of pretrained LLMs has been\nwidely used to safeguarding the privacy of task-specific datasets. Lying at the\ndesign core of DP LLM fine-tuning methods is the satisfactory tradeoff among\nprivacy, utility, and scalability. Most existing methods build upon the seminal\nwork of DP-SGD. Despite pushing the scalability of DP-SGD to its limit,\nDP-SGD-based fine-tuning methods are unfortunately limited by the inherent\ninefficiency of SGD.\n  In this paper, we investigate the potential of DP zeroth-order methods for\nLLM pretraining, which avoids the scalability bottleneck of SGD by\napproximating the gradient with the more efficient zeroth-order gradient.\nRather than treating the zeroth-order method as a drop-in replacement for SGD,\nthis paper presents a comprehensive study both theoretically and empirically.\nFirst, we propose the stagewise DP zeroth-order method (DP-ZOSO) that\ndynamically schedules key hyperparameters. This design is grounded on the\nsynergy between DP random perturbation and the gradient approximation error of\nthe zeroth-order method, and its effect on fine-tuning trajectory.\n  We provide theoretical analysis for both proposed methods. We conduct\nextensive empirical analysis on both encoder-only masked language model and\ndecoder-only autoregressive language model, achieving impressive results in\nterms of scalability and utility regardless of the class of tasks (compared\nwith DPZero, DP-ZOPO improves $4.5\\%$ on SST-5, $5.5\\%$ on MNLI with\nRoBERTa-Large and 9.2\\% on CB, 3.9\\% on BoolQ with OPT-2.7b when $\\epsilon=4$,\ndemonstrates more significant enhancement in performance on more complicated\ntasks).", "published": "2024-02-12 17:24:15", "link": "http://arxiv.org/abs/2402.07818v6", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "AI-Augmented Predictions: LLM Assistants Improve Human Forecasting\n  Accuracy", "abstract": "Large language models (LLMs) match and sometimes exceeding human performance\nin many domains. This study explores the potential of LLMs to augment human\njudgement in a forecasting task. We evaluate the effect on human forecasters of\ntwo LLM assistants: one designed to provide high-quality (\"superforecasting\")\nadvice, and the other designed to be overconfident and base-rate neglecting,\nthus providing noisy forecasting advice. We compare participants using these\nassistants to a control group that received a less advanced model that did not\nprovide numerical predictions or engaged in explicit discussion of predictions.\nParticipants (N = 991) answered a set of six forecasting questions and had the\noption to consult their assigned LLM assistant throughout. Our preregistered\nanalyses show that interacting with each of our frontier LLM assistants\nsignificantly enhances prediction accuracy by between 24 percent and 28 percent\ncompared to the control group. Exploratory analyses showed a pronounced outlier\neffect in one forecasting item, without which we find that the superforecasting\nassistant increased accuracy by 41 percent, compared with 29 percent for the\nnoisy assistant. We further examine whether LLM forecasting augmentation\ndisproportionately benefits less skilled forecasters, degrades the\nwisdom-of-the-crowd by reducing prediction diversity, or varies in\neffectiveness with question difficulty. Our data do not consistently support\nthese hypotheses. Our results suggest that access to a frontier LLM assistant,\neven a noisy one, can be a helpful decision aid in cognitively demanding tasks\ncompared to a less powerful model that does not provide specific forecasting\nadvice. However, the effects of outliers suggest that further research into the\nrobustness of this pattern is needed.", "published": "2024-02-12 18:14:43", "link": "http://arxiv.org/abs/2402.07862v2", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Prismatic VLMs: Investigating the Design Space of Visually-Conditioned\n  Language Models", "abstract": "Visually-conditioned language models (VLMs) have seen growing adoption in\napplications such as visual dialogue, scene understanding, and robotic task\nplanning; adoption that has fueled a wealth of new models such as LLaVa,\nInstructBLIP, and PaLI-3. Despite the volume of new releases, key design\ndecisions around image preprocessing, architecture, and optimization are\nunder-explored, making it challenging to understand what factors account for\nmodel performance $-$ a challenge further complicated by the lack of objective,\nconsistent evaluations. To address these gaps, we first compile a suite of\nstandardized evaluations spanning visual question answering, object\nlocalization, and challenge sets that probe properties such as hallucination;\nevaluations that provide fine-grained insight VLM capabilities. Second, we\nrigorously investigate VLMs along key design axes, including pretrained visual\nrepresentations and training from base vs. instruct-tuned language models,\namongst others. We couple our analysis with three resource contributions: (1) a\nunified framework for evaluating VLMs, (2) optimized, flexible training code,\nand (3) checkpoints for all models, including a family of VLMs at the 7-13B\nscale that strictly outperform InstructBLIP and LLaVa v1.5, the\nstate-of-the-art in open VLMs.", "published": "2024-02-12 18:21:14", "link": "http://arxiv.org/abs/2402.07865v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Scaling Laws for Fine-Grained Mixture of Experts", "abstract": "Mixture of Experts (MoE) models have emerged as a primary solution for\nreducing the computational cost of Large Language Models. In this work, we\nanalyze their scaling properties, incorporating an expanded range of variables.\nSpecifically, we introduce a new hyperparameter, granularity, whose adjustment\nenables precise control over the size of the experts. Building on this, we\nestablish scaling laws for fine-grained MoE, taking into account the number of\ntraining tokens, model size, and granularity. Leveraging these laws, we derive\nthe optimal training configuration for a given computational budget. Our\nfindings not only show that MoE models consistently outperform dense\nTransformers but also highlight that the efficiency gap between dense and MoE\nmodels widens as we scale up the model size and training budget. Furthermore,\nwe demonstrate that the common practice of setting the size of experts in MoE\nto mirror the feed-forward layer is not optimal at almost any computational\nbudget.", "published": "2024-02-12 18:33:47", "link": "http://arxiv.org/abs/2402.07871v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs", "abstract": "Vision language models (VLMs) have shown impressive capabilities across a\nvariety of tasks, from logical reasoning to visual understanding. This opens\nthe door to richer interaction with the world, for example robotic control.\nHowever, VLMs produce only textual outputs, while robotic control and other\nspatial tasks require outputting continuous coordinates, actions, or\ntrajectories. How can we enable VLMs to handle such settings without\nfine-tuning on task-specific data?\n  In this paper, we propose a novel visual prompting approach for VLMs that we\ncall Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as\niterative visual question answering. In each iteration, the image is annotated\nwith a visual representation of proposals that the VLM can refer to (e.g.,\ncandidate robot actions, localizations, or trajectories). The VLM then selects\nthe best ones for the task. These proposals are iteratively refined, allowing\nthe VLM to eventually zero in on the best available answer. We investigate\nPIVOT on real-world robotic navigation, real-world manipulation from images,\ninstruction following in simulation, and additional spatial inference tasks\nsuch as localization. We find, perhaps surprisingly, that our approach enables\nzero-shot control of robotic systems without any robot training data,\nnavigation in a variety of environments, and other capabilities. Although\ncurrent performance is far from perfect, our work highlights potentials and\nlimitations of this new regime and shows a promising approach for\nInternet-Scale VLMs in robotic and spatial reasoning domains. Website:\npivot-prompt.github.io and HuggingFace:\nhttps://huggingface.co/spaces/pivot-prompt/pivot-prompt-demo.", "published": "2024-02-12 18:33:47", "link": "http://arxiv.org/abs/2402.07872v1", "categories": ["cs.RO", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Policy Improvement using Language Feedback Models", "abstract": "We introduce Language Feedback Models (LFMs) that identify desirable\nbehaviour - actions that help achieve tasks specified in the instruction - for\nimitation learning in instruction following. To train LFMs, we obtain feedback\nfrom Large Language Models (LLMs) on visual trajectories verbalized to language\ndescriptions. First, by using LFMs to identify desirable behaviour to imitate,\nwe improve in task-completion rate over strong behavioural cloning baselines on\nthree distinct language grounding environments (Touchdown, ScienceWorld, and\nALFWorld). Second, LFMs outperform using LLMs as experts to directly predict\nactions, when controlling for the number of LLM output tokens. Third, LFMs\ngeneralize to unseen environments, improving task-completion rate by 3.5-12.0%\nthrough one round of adaptation. Finally, LFM can be modified to provide\nhuman-interpretable feedback without performance loss, allowing human\nverification of desirable behaviour for imitation learning.", "published": "2024-02-12 18:41:34", "link": "http://arxiv.org/abs/2402.07876v6", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Lumos : Empowering Multimodal LLMs with Scene Text Recognition", "abstract": "We introduce Lumos, the first end-to-end multimodal question-answering system\nwith text understanding capabilities. At the core of Lumos is a Scene Text\nRecognition (STR) component that extracts text from first person point-of-view\nimages, the output of which is used to augment input to a Multimodal Large\nLanguage Model (MM-LLM). While building Lumos, we encountered numerous\nchallenges related to STR quality, overall latency, and model inference. In\nthis paper, we delve into those challenges, and discuss the system\narchitecture, design choices, and modeling techniques employed to overcome\nthese obstacles. We also provide a comprehensive evaluation for each component,\nshowcasing high quality and efficiency.", "published": "2024-02-12 19:27:26", "link": "http://arxiv.org/abs/2402.08017v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Text-centric Alignment for Multi-Modality Learning", "abstract": "This research paper addresses the challenge of modality mismatch in\nmultimodal learning, where the modalities available during inference differ\nfrom those available at training. We propose the Text-centric Alignment for\nMulti-Modality Learning (TAMML) approach, an innovative method that utilizes\nLarge Language Models (LLMs) with in-context learning and foundation models to\nenhance the generalizability of multimodal systems under these conditions. By\nleveraging the unique properties of text as a unified semantic space, TAMML\ndemonstrates significant improvements in handling unseen, diverse, and\nunpredictable modality combinations. TAMML not only adapts to varying\nmodalities but also maintains robust performance, showcasing the potential of\nfoundation models in overcoming the limitations of traditional fixed-modality\nframeworks in embedding representations. This study contributes to the field by\noffering a flexible, effective solution for real-world applications where\nmodality availability is dynamic and uncertain.", "published": "2024-02-12 22:07:43", "link": "http://arxiv.org/abs/2402.08086v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "BASE TTS: Lessons from building a billion-parameter Text-to-Speech model\n  on 100K hours of data", "abstract": "We introduce a text-to-speech (TTS) model called BASE TTS, which stands for\n$\\textbf{B}$ig $\\textbf{A}$daptive $\\textbf{S}$treamable TTS with\n$\\textbf{E}$mergent abilities. BASE TTS is the largest TTS model to-date,\ntrained on 100K hours of public domain speech data, achieving a new\nstate-of-the-art in speech naturalness. It deploys a 1-billion-parameter\nautoregressive Transformer that converts raw texts into discrete codes\n(\"speechcodes\") followed by a convolution-based decoder which converts these\nspeechcodes into waveforms in an incremental, streamable manner. Further, our\nspeechcodes are built using a novel speech tokenization technique that features\nspeaker ID disentanglement and compression with byte-pair encoding. Echoing the\nwidely-reported \"emergent abilities\" of large language models when trained on\nincreasing volume of data, we show that BASE TTS variants built with 10K+ hours\nand 500M+ parameters begin to demonstrate natural prosody on textually complex\nsentences. We design and share a specialized dataset to measure these emergent\nabilities for text-to-speech. We showcase state-of-the-art naturalness of BASE\nTTS by evaluating against baselines that include publicly available large-scale\ntext-to-speech systems: YourTTS, Bark and TortoiseTTS. Audio samples generated\nby the model can be heard at https://amazon-ltts-paper.com/.", "published": "2024-02-12 22:21:30", "link": "http://arxiv.org/abs/2402.08093v2", "categories": ["cs.LG", "cs.CL", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Active Preference Learning for Large Language Models", "abstract": "As large language models (LLMs) become more capable, fine-tuning techniques\nfor aligning with human intent are increasingly important. A key consideration\nfor aligning these models is how to most effectively use human resources, or\nmodel resources in the case where LLMs themselves are used as oracles.\nReinforcement learning from Human or AI preferences (RLHF/RLAIF) is the most\nprominent example of such a technique, but is complex and often unstable.\nDirect Preference Optimization (DPO) has recently been proposed as a simpler\nand more stable alternative. In this work, we develop an active learning\nstrategy for DPO to make better use of preference labels. We propose a\npractical acquisition function for prompt/completion pairs based on the\npredictive entropy of the language model and a measure of certainty of the\nimplicit preference model optimized by DPO. We demonstrate how our approach\nimproves both the rate of learning and final performance of fine-tuning on\npairwise preference data.", "published": "2024-02-12 23:09:00", "link": "http://arxiv.org/abs/2402.08114v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Relative Preference Optimization: Enhancing LLM Alignment through\n  Contrasting Responses across Identical and Diverse Prompts", "abstract": "In the field of large language models (LLMs), aligning models with the\ndiverse preferences of users is a critical challenge. Direct Preference\nOptimization (DPO) has played a key role in this area. It works by using pairs\nof preferences derived from the same prompts, and it functions without needing\nan additional reward model. However, DPO does not fully reflect the complex\nnature of human learning, which often involves understanding contrasting\nresponses to not only identical but also similar questions. To overcome this\nshortfall, we propose Relative Preference Optimization (RPO). RPO is designed\nto discern between more and less preferred responses derived from both\nidentical and related prompts. It introduces a contrastive weighting mechanism,\nenabling the tuning of LLMs using a broader range of preference data, including\nboth paired and unpaired sets. This approach expands the learning capabilities\nof the model, allowing it to leverage insights from a more varied set of\nprompts. Through empirical tests, including dialogue and summarization tasks,\nand evaluations using the AlpacaEval2.0 leaderboard, RPO has demonstrated a\nsuperior ability to align LLMs with user preferences and to improve their\nadaptability during the training process. Our code can be viewed at\nhttps://github.com/yinyueqin/relative-preference-optimization", "published": "2024-02-12 22:47:57", "link": "http://arxiv.org/abs/2402.10958v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EvoGPT-f: An Evolutionary GPT Framework for Benchmarking Formal Math\n  Languages", "abstract": "Formal mathematics is the discipline of translating mathematics into a\nprogramming language in which any statement can be unequivocally checked by a\ncomputer. Mathematicians and computer scientists have spent decades of\npainstaking formalization efforts developing languages such as Coq, HOL, and\nLean. Machine learning research has converged on these formal math corpora and\ngiven rise to an assortment of methodologies to aid in interactive and\nautomated theorem proving. However, these papers have primarily focused on one\nmethod, for one proof task, in one language. This paper introduces EvoGPT-f: a\nnovel evolutionary framework for the first systematic quantitative analysis of\nthe differential machine learnability of five formal math corpora (Lean 3, Lean\n4, Coq, HOL 4, HOL Light) using four tokenization methods (character,\nword-level, Byte Pair Encoding and StarCoder tokenizer). This paper does not\nput to rest the question of the \"best\" or \"easiest\" language to learn. Rather,\nthis framework and preliminary findings begin to illuminate the differential\nmachine learnability of these languages, offering a foundation to forge more\nsystematic quantitative and qualitative comparative research across\ncommunities.", "published": "2024-02-12 19:10:11", "link": "http://arxiv.org/abs/2402.16878v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.AI"}
{"title": "AraSpider: Democratizing Arabic-to-SQL", "abstract": "This study presents AraSpider, the first Arabic version of the Spider\ndataset, aimed at improving natural language processing (NLP) in the\nArabic-speaking community. Four multilingual translation models were tested for\ntheir effectiveness in translating English to Arabic. Additionally, two models\nwere assessed for their ability to generate SQL queries from Arabic text. The\nresults showed that using back translation significantly improved the\nperformance of both ChatGPT 3.5 and SQLCoder models, which are considered top\nperformers on the Spider dataset. Notably, ChatGPT 3.5 demonstrated\nhigh-quality translation, while SQLCoder excelled in text-to-SQL tasks. The\nstudy underscores the importance of incorporating contextual schema and\nemploying back translation strategies to enhance model performance in Arabic\nNLP tasks. Moreover, the provision of detailed methodologies for\nreproducibility and translation of the dataset into other languages highlights\nthe research's commitment to promoting transparency and collaborative knowledge\nsharing in the field. Overall, these contributions advance NLP research,\nempower Arabic-speaking researchers, and enrich the global discourse on\nlanguage comprehension and database interrogation.", "published": "2024-02-12 07:11:13", "link": "http://arxiv.org/abs/2402.07448v1", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MINT: Boosting Audio-Language Model via Multi-Target Pre-Training and\n  Instruction Tuning", "abstract": "In the realm of audio-language pre-training (ALP), the challenge of achieving\ncross-modal alignment is significant. Moreover, the integration of audio inputs\nwith diverse distributions and task variations poses challenges in developing\ngeneric audio-language models. In this study, we present MINT, a novel ALP\nframework boosting audio-language models through multi-target pre-training and\ninstruction tuning. MINT leverages the strength of frozen pre-trained audio\nencoders and large language models (LLM) to improve audio-language\npre-training, enabling effective transferablility to both audio-text\nunderstanding and generation tasks. To address the modality gap, we introduce\nBridge-Net, a trainable module that enhances cross-modality alignment and the\nmodel's ability to follow instructions for a variety of audio-text tasks.\nBridge-Net is pivotal within MINT, initially enhancing audio-language\nrepresentation learning through a multi-target pre-training approach.\nSubsequently, Bridge-Net further boosts audio-to-language generative learning\nby integrating a frozen language model with instruction tuning. This\nintegration empowers MINT to extract features in a flexible and effective\nmanner, specifically tailored to the provided instructions for diverse tasks.\nExperimental results demonstrate that MINT attains superior performance across\nvarious audio-language understanding and generation tasks, highlighting its\nrobust generalization capabilities even in zero-shot scenarios.", "published": "2024-02-12 08:51:06", "link": "http://arxiv.org/abs/2402.07485v5", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Interactive singing melody extraction based on active adaptation", "abstract": "Extraction of predominant pitch from polyphonic audio is one of the\nfundamental tasks in the field of music information retrieval and computational\nmusicology. To accomplish this task using machine learning, a large amount of\nlabeled audio data is required to train the model. However, a classical model\npre-trained on data from one domain (source), e.g., songs of a particular\nsinger or genre, may not perform comparatively well in extracting melody from\nother domains (target). The performance of such models can be boosted by\nadapting the model using very little annotated data from the target domain. In\nthis work, we propose an efficient interactive melody adaptation method. Our\nmethod selects the regions in the target audio that require human annotation\nusing a confidence criterion based on normalized true class probability. The\nannotations are used by the model to adapt itself to the target domain using\nmeta-learning. Our method also provides a novel meta-learning approach that\nhandles class imbalance, i.e., a few representative samples from a few classes\nare available for adaptation in the target domain. Experimental results show\nthat the proposed method outperforms other adaptive melody extraction\nbaselines. The proposed method is model-agnostic and hence can be applied to\nother non-adaptive melody extraction models to boost their performance. Also,\nwe released a Hindustani Alankaar and Raga (HAR) dataset containing 523 audio\nfiles of about 6.86 hours of duration intended for singing melody extraction\ntasks.", "published": "2024-02-12 11:58:41", "link": "http://arxiv.org/abs/2402.07599v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Computational Model of the Electrically or Acoustically Evoked\n  Compound Action Potential in Cochlear Implant Users with Residual Hearing", "abstract": "Objective: In cochlear implant users with residual acoustic hearing, compound\naction potentials (CAPs) can be evoked by acoustic (aCAP) or electric (eCAP)\nstimulation and recorded through the electrodes of the implant. We propose a\nnovel computational model to simulate aCAPs and eCAPs in humans, considering\nthe interaction between combined electric-acoustic stimulation that occurs in\nthe auditory nerve. Methods: The model consists of three components: a 3D\nfinite element method model of an implanted cochlea, a phenomenological\nsingle-neuron spiking model for electric-acoustic stimulation, and a\nphysiological multi-compartment neuron model to simulate the individual nerve\nfiber contributions to the CAP. Results: The CAP morphologies closely resembled\nthose known from humans. The spread of excitation derived from eCAPs by varying\nthe recording electrode along the cochlear implant electrode array was\nconsistent with published human data. The predicted CAP amplitude growth\nfunctions largely resembled human data, with deviations in absolute CAP\namplitudes for acoustic stimulation. The model reproduced the suppression of\neCAPs by simultaneously presented acoustic tone bursts for different masker\nfrequencies and probe stimulation electrodes. Conclusion: The proposed model\ncan simulate CAP responses to electric, acoustic, or combined electric-acoustic\nstimulation. It considers the dependence on stimulation and recording sites in\nthe cochlea, as well as the interaction between electric and acoustic\nstimulation in the auditory nerve. Significance: The model enhances\ncomprehension of CAPs and peripheral electric-acoustic interaction. It can be\nused in the future to investigate objective methods, such as hearing threshold\nassessment or estimation of neural health through aCAPs or eCAPs.", "published": "2024-02-12 14:29:27", "link": "http://arxiv.org/abs/2402.07673v2", "categories": ["physics.med-ph", "eess.AS"], "primary_category": "physics.med-ph"}
{"title": "Sheet Music Transformer: End-To-End Optical Music Recognition Beyond\n  Monophonic Transcription", "abstract": "State-of-the-art end-to-end Optical Music Recognition (OMR) has, to date,\nprimarily been carried out using monophonic transcription techniques to handle\ncomplex score layouts, such as polyphony, often by resorting to simplifications\nor specific adaptations. Despite their efficacy, these approaches imply\nchallenges related to scalability and limitations. This paper presents the\nSheet Music Transformer, the first end-to-end OMR model designed to transcribe\ncomplex musical scores without relying solely on monophonic strategies. Our\nmodel employs a Transformer-based image-to-sequence framework that predicts\nscore transcriptions in a standard digital music encoding format from input\nimages. Our model has been tested on two polyphonic music datasets and has\nproven capable of handling these intricate music structures effectively. The\nexperimental outcomes not only indicate the competence of the model, but also\nshow that it is better than the state-of-the-art methods, thus contributing to\nadvancements in end-to-end OMR transcription.", "published": "2024-02-12 11:52:21", "link": "http://arxiv.org/abs/2402.07596v2", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Developing a Multi-variate Prediction Model For COVID-19 From\n  Crowd-sourced Respiratory Voice Data", "abstract": "COVID-19 has affected more than 223 countries worldwide and in the Post-COVID\nEra, there is a pressing need for non-invasive, low-cost, and highly scalable\nsolutions to detect COVID-19. We develop a deep learning model to identify\nCOVID-19 from voice recording data. The novelty of this work is in the\ndevelopment of deep learning models for COVID-19 identification from only voice\nrecordings. We use the Cambridge COVID-19 Sound database which contains 893\nspeech samples, crowd-sourced from 4352 participants via a COVID-19 Sounds app.\nVoice features including Mel-spectrograms and Mel-frequency cepstral\ncoefficients (MFCC) and CNN Encoder features are extracted. Based on the voice\ndata, we develop deep learning classification models to detect COVID-19 cases.\nThese models include Long Short-Term Memory (LSTM) and Convolutional Neural\nNetwork (CNN) and Hidden-Unit BERT (HuBERT). We compare their predictive power\nto baseline machine learning models. HuBERT achieves the highest accuracy of\n86\\% and the highest AUC of 0.93. The results achieved with the proposed models\nsuggest promising results in COVID-19 diagnosis from voice recordings when\ncompared to the results obtained from the state-of-the-art.", "published": "2024-02-12 12:52:47", "link": "http://arxiv.org/abs/2402.07619v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
