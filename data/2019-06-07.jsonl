{"title": "Modeling financial analysts' decision making via the pragmatics and\n  semantics of earnings calls", "abstract": "Every fiscal quarter, companies hold earnings calls in which company\nexecutives respond to questions from analysts. After these calls, analysts\noften change their price target recommendations, which are used in equity\nresearch reports to help investors make decisions. In this paper, we examine\nanalysts' decision making behavior as it pertains to the language content of\nearnings calls. We identify a set of 20 pragmatic features of analysts'\nquestions which we correlate with analysts' pre-call investor recommendations.\nWe also analyze the degree to which semantic and pragmatic features from an\nearnings call complement market data in predicting analysts' post-call changes\nin price targets. Our results show that earnings calls are moderately\npredictive of analysts' decisions even though these decisions are influenced by\na number of other factors including private communication with company\nexecutives and market conditions. A breakdown of model errors indicates\ndisparate performance on calls from different market sectors.", "published": "2019-06-07 02:30:31", "link": "http://arxiv.org/abs/1906.02868v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Preference-based Interactive Multi-Document Summarisation", "abstract": "Interactive NLP is a promising paradigm to close the gap between automatic\nNLP systems and the human upper bound. Preference-based interactive learning\nhas been successfully applied, but the existing methods require several\nthousand interaction rounds even in simulations with perfect user feedback. In\nthis paper, we study preference-based interactive summarisation. To reduce the\nnumber of interaction rounds, we propose the Active Preference-based\nReInforcement Learning (APRIL) framework. APRIL uses Active Learning to query\nthe user, Preference Learning to learn a summary ranking function from the\npreferences, and neural Reinforcement Learning to efficiently search for the\n(near-)optimal summary. Our results show that users can easily provide reliable\npreferences over summaries and that APRIL outperforms the state-of-the-art\npreference-based interactive method in both simulation and real-user\nexperiments.", "published": "2019-06-07 06:42:24", "link": "http://arxiv.org/abs/1906.02923v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Wind of Change: Detecting and Evaluating Lexical Semantic Change\n  across Times and Domains", "abstract": "We perform an interdisciplinary large-scale evaluation for detecting lexical\nsemantic divergences in a diachronic and in a synchronic task: semantic sense\nchanges across time, and semantic sense changes across domains. Our work\naddresses the superficialness and lack of comparison in assessing models of\ndiachronic lexical change, by bringing together and extending benchmark models\non a common state-of-the-art evaluation task. In addition, we demonstrate that\nthe same evaluation task and modelling approaches can successfully be utilised\nfor the synchronic detection of domain-specific sense divergences in the field\nof term extraction.", "published": "2019-06-07 09:19:47", "link": "http://arxiv.org/abs/1906.02979v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Compositionality Prediction of Noun Phrases using Poincar\u00e9\n  Embeddings", "abstract": "The compositionality degree of multiword expressions indicates to what extent\nthe meaning of a phrase can be derived from the meaning of its constituents and\ntheir grammatical relations. Prediction of (non)-compositionality is a task\nthat has been frequently addressed with distributional semantic models. We\nintroduce a novel technique to blend hierarchical information with\ndistributional information for predicting compositionality. In particular, we\nuse hypernymy information of the multiword and its constituents encoded in the\nform of the recently introduced Poincar\\'e embeddings in addition to the\ndistributional information to detect compositionality for noun phrases. Using a\nweighted average of the distributional similarity and a Poincar\\'e similarity\nfunction, we obtain consistent and substantial, statistically significant\nimprovement across three gold standard datasets over state-of-the-art models\nbased on distributional information only. Unlike traditional approaches that\nsolely use an unsupervised setting, we have also framed the problem as a\nsupervised task, obtaining comparable improvements. Further, we publicly\nrelease our Poincar\\'e embeddings, which are trained on the output of\nhandcrafted lexical-syntactic patterns on a large corpus.", "published": "2019-06-07 11:05:30", "link": "http://arxiv.org/abs/1906.03007v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RankQA: Neural Question Answering with Answer Re-Ranking", "abstract": "The conventional paradigm in neural question answering (QA) for narrative\ncontent is limited to a two-stage process: first, relevant text passages are\nretrieved and, subsequently, a neural network for machine comprehension\nextracts the likeliest answer. However, both stages are largely isolated in the\nstatus quo and, hence, information from the two phases is never properly fused.\nIn contrast, this work proposes RankQA: RankQA extends the conventional\ntwo-stage process in neural QA with a third stage that performs an additional\nanswer re-ranking. The re-ranking leverages different features that are\ndirectly extracted from the QA pipeline, i.e., a combination of retrieval and\ncomprehension features. While our intentionally simple design allows for an\nefficient, data-sparse estimation, it nevertheless outperforms more complex QA\nsystems by a significant margin: in fact, RankQA achieves state-of-the-art\nperformance on 3 out of 4 benchmark datasets. Furthermore, its performance is\nespecially superior in settings where the size of the corpus is dynamic. Here\nthe answer re-ranking provides an effective remedy against the underlying\nnoise-information trade-off due to a variable corpus size. As a consequence,\nRankQA represents a novel, powerful, and thus challenging baseline for future\nresearch in content-based QA.", "published": "2019-06-07 11:06:11", "link": "http://arxiv.org/abs/1906.03008v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Relation Extraction by Pre-trained Language Representations", "abstract": "Current state-of-the-art relation extraction methods typically rely on a set\nof lexical, syntactic, and semantic features, explicitly computed in a\npre-processing step. Training feature extraction models requires additional\nannotated language resources, which severely restricts the applicability and\nportability of relation extraction to novel languages. Similarly,\npre-processing introduces an additional source of error. To address these\nlimitations, we introduce TRE, a Transformer for Relation Extraction, extending\nthe OpenAI Generative Pre-trained Transformer [Radford et al., 2018]. Unlike\nprevious relation extraction models, TRE uses pre-trained deep language\nrepresentations instead of explicit linguistic features to inform the relation\nclassification and combines it with the self-attentive Transformer architecture\nto effectively model long-range dependencies between entity mentions. TRE\nallows us to learn implicit linguistic features solely from plain text corpora\nby unsupervised pre-training, before fine-tuning the learned language\nrepresentations on the relation extraction task. TRE obtains a new\nstate-of-the-art result on the TACRED and SemEval 2010 Task 8 datasets,\nachieving a test F1 of 67.4 and 87.1, respectively. Furthermore, we observe a\nsignificant increase in sample efficiency. With only 20% of the training\nexamples, TRE matches the performance of our baselines and our model trained\nfrom scratch on 100% of the TACRED dataset. We open-source our trained models,\nexperiments, and source code.", "published": "2019-06-07 13:31:09", "link": "http://arxiv.org/abs/1906.03088v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Embeddings for the Armenian Language: Intrinsic and Extrinsic\n  Evaluation", "abstract": "In this work, we intrinsically and extrinsically evaluate and compare\nexisting word embedding models for the Armenian language. Alongside, new\nembeddings are presented, trained using GloVe, fastText, CBOW, SkipGram\nalgorithms. We adapt and use the word analogy task in intrinsic evaluation of\nembeddings. For extrinsic evaluation, two tasks are employed: morphological\ntagging and text classification. Tagging is performed on a deep neural network,\nusing ArmTDP v2.3 dataset. For text classification, we propose a corpus of news\narticles categorized into 7 classes. The datasets are made public to serve as\nbenchmarks for future models.", "published": "2019-06-07 14:45:49", "link": "http://arxiv.org/abs/1906.03134v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building a Production Model for Retrieval-Based Chatbots", "abstract": "Response suggestion is an important task for building human-computer\nconversation systems. Recent approaches to conversation modeling have\nintroduced new model architectures with impressive results, but relatively\nlittle attention has been paid to whether these models would be practical in a\nproduction setting. In this paper, we describe the unique challenges of\nbuilding a production retrieval-based conversation system, which selects\noutputs from a whitelist of candidate responses. To address these challenges,\nwe propose a dual encoder architecture which performs rapid inference and\nscales well with the size of the whitelist. We also introduce and compare two\nmethods for generating whitelists, and we carry out a comprehensive analysis of\nthe model and whitelists. Experimental results on a large, proprietary help\ndesk chat dataset, including both offline metrics and a human evaluation,\nindicate production-quality performance and illustrate key lessons about\nconversation modeling in practice.", "published": "2019-06-07 16:17:31", "link": "http://arxiv.org/abs/1906.03209v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data-to-text Generation with Entity Modeling", "abstract": "Recent approaches to data-to-text generation have shown great promise thanks\nto the use of large-scale datasets and the application of neural network\narchitectures which are trained end-to-end. These models rely on representation\nlearning to select content appropriately, structure it coherently, and\nverbalize it grammatically, treating entities as nothing more than vocabulary\ntokens. In this work we propose an entity-centric neural architecture for\ndata-to-text generation. Our model creates entity-specific representations\nwhich are dynamically updated. Text is generated conditioned on the data input\nand entity memory representations using hierarchical attention at each time\nstep. We present experiments on the RotoWire benchmark and a (five times\nlarger) new dataset on the baseball domain which we create. Our results show\nthat the proposed model outperforms competitive baselines in automatic and\nhuman evaluation.", "published": "2019-06-07 16:41:52", "link": "http://arxiv.org/abs/1906.03221v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Word Embeddings with Domain Awareness", "abstract": "Word embeddings are traditionally trained on a large corpus in an\nunsupervised setting, with no specific design for incorporating domain\nknowledge. This can lead to unsatisfactory performances when training data\noriginate from heterogeneous domains. In this paper, we propose two novel\nmechanisms for domain-aware word embedding training, namely domain indicator\nand domain attention, which integrate domain-specific knowledge into the widely\nused SG and CBOW models, respectively. The two methods are based on a joint\nlearning paradigm and ensure that words in a target domain are intensively\nfocused when trained on a source domain corpus. Qualitative and quantitative\nevaluation confirm the validity and effectiveness of our models. Compared to\nbaseline methods, our method is particularly effective in near-cold-start\nscenarios.", "published": "2019-06-07 17:29:18", "link": "http://arxiv.org/abs/1906.03249v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dissecting Content and Context in Argumentative Relation Analysis", "abstract": "When assessing relations between argumentative units (e.g., support or\nattack), computational systems often exploit disclosing indicators or markers\nthat are not part of elementary argumentative units (EAUs) themselves, but are\ngained from their context (position in paragraph, preceding tokens, etc.). We\nshow that this dependency is much stronger than previously assumed. In fact, we\nshow that by completely masking the EAU text spans and only feeding information\nfrom their context, a competitive system may function even better. We argue\nthat an argument analysis system that relies more on discourse context than the\nargument's content is unsafe, since it can easily be tricked. To alleviate this\nissue, we separate argumentative units from their context such that the system\nis forced to model and rely on an EAU's content. We show that the resulting\nclassification system is more robust, and argue that such models are better\nsuited for predicting argumentative relations across documents.", "published": "2019-06-07 21:36:49", "link": "http://arxiv.org/abs/1906.03338v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Figure Captioning with Reasoning and Sequence-Level Training", "abstract": "Figures, such as bar charts, pie charts, and line plots, are widely used to\nconvey important information in a concise format. They are usually\nhuman-friendly but difficult for computers to process automatically. In this\nwork, we investigate the problem of figure captioning where the goal is to\nautomatically generate a natural language description of the figure. While\nnatural image captioning has been studied extensively, figure captioning has\nreceived relatively little attention and remains a challenging problem. First,\nwe introduce a new dataset for figure captioning, FigCAP, based on FigureQA.\nSecond, we propose two novel attention mechanisms. To achieve accurate\ngeneration of labels in figures, we propose Label Maps Attention. To model the\nrelations between figure labels, we propose Relation Maps Attention. Third, we\nuse sequence-level training with reinforcement learning in order to directly\noptimizes evaluation metrics, which alleviates the exposure bias issue and\nfurther improves the models in generating long captions. Extensive experiments\nshow that the proposed method outperforms the baselines, thus demonstrating a\nsignificant potential for the automatic captioning of vast repositories of\nfigures.", "published": "2019-06-07 00:54:53", "link": "http://arxiv.org/abs/1906.02850v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Learning to Recommend Third-Party Library Migration Opportunities at the\n  API Level", "abstract": "The manual migration between different third-party libraries represents a\nchallenge for software developers. Developers typically need to explore both\nlibraries Application Programming Interfaces, along with reading their\ndocumentation, in order to locate the suitable mappings between replacing and\nreplaced methods. In this paper, we introduce RAPIM, a novel machine learning\napproach that recommends mappings between methods from two different libraries.\nOur model learns from previous migrations, manually performed in mined software\nsystems, and extracts a set of features related to the similarity between\nmethod signatures and method textual documentation. We evaluate our model using\n8 popular migrations, collected from 57,447 open-source Java projects. Results\nshow that RAPIM is able to recommend relevant library API mappings with an\naverage accuracy score of 87%. Finally, we provide the community with an API\nrecommendation web service that could be used to support the migration process.", "published": "2019-06-07 03:20:46", "link": "http://arxiv.org/abs/1906.02882v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Visually Grounded Neural Syntax Acquisition", "abstract": "We present the Visually Grounded Neural Syntax Learner (VG-NSL), an approach\nfor learning syntactic representations and structures without any explicit\nsupervision. The model learns by looking at natural images and reading paired\ncaptions. VG-NSL generates constituency parse trees of texts, recursively\ncomposes representations for constituents, and matches them with images. We\ndefine concreteness of constituents by their matching scores with images, and\nuse it to guide the parsing of text. Experiments on the MSCOCO data set show\nthat VG-NSL outperforms various unsupervised parsing approaches that do not use\nvisual grounding, in terms of F1 scores against gold parse trees. We find that\nVGNSL is much more stable with respect to the choice of random initialization\nand the amount of training data. We also find that the concreteness acquired by\nVG-NSL correlates well with a similar measure defined by linguists. Finally, we\nalso apply VG-NSL to multiple languages in the Multi30K data set, showing that\nour model consistently outperforms prior unsupervised approaches.", "published": "2019-06-07 04:03:53", "link": "http://arxiv.org/abs/1906.02890v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Semi-supervised Stochastic Multi-Domain Learning using Variational\n  Inference", "abstract": "Supervised models of NLP rely on large collections of text which closely\nresemble the intended testing setting. Unfortunately matching text is often not\navailable in sufficient quantity, and moreover, within any domain of text, data\nis often highly heterogenous. In this paper we propose a method to distill the\nimportant domain signal as part of a multi-domain learning system, using a\nlatent variable model in which parts of a neural model are stochastically gated\nbased on the inferred domain. We compare the use of discrete versus continuous\nlatent variables, operating in a domain-supervised or a domain semi-supervised\nsetting, where the domain is known only for a subset of training inputs. We\nshow that our model leads to substantial performance improvements over\ncompetitive benchmark domain adaptation methods, including methods using\nadversarial learning.", "published": "2019-06-07 05:01:47", "link": "http://arxiv.org/abs/1906.02897v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Compositional Questions Do Not Necessitate Multi-hop Reasoning", "abstract": "Multi-hop reading comprehension (RC) questions are challenging because they\nrequire reading and reasoning over multiple paragraphs. We argue that it can be\ndifficult to construct large multi-hop RC datasets. For example, even highly\ncompositional questions can be answered with a single hop if they target\nspecific entity types, or the facts needed to answer them are redundant. Our\nanalysis is centered on HotpotQA, where we show that single-hop reasoning can\nsolve much more of the dataset than previously thought. We introduce a\nsingle-hop BERT-based RC model that achieves 67 F1---comparable to\nstate-of-the-art multi-hop models. We also design an evaluation setting where\nhumans are not shown all of the necessary paragraphs for the intended multi-hop\nreasoning but can still answer over 80% of questions. Together with detailed\nerror analysis, these results suggest there should be an increasing focus on\nthe role of evidence in multi-hop reasoning and possibly even a shift towards\ninformation retrieval style evaluations with large and diverse evidence\ncollections.", "published": "2019-06-07 05:10:15", "link": "http://arxiv.org/abs/1906.02900v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-hop Reading Comprehension through Question Decomposition and\n  Rescoring", "abstract": "Multi-hop Reading Comprehension (RC) requires reasoning and aggregation\nacross several paragraphs. We propose a system for multi-hop RC that decomposes\na compositional question into simpler sub-questions that can be answered by\noff-the-shelf single-hop RC models. Since annotations for such decomposition\nare expensive, we recast sub-question generation as a span prediction problem\nand show that our method, trained using only 400 labeled examples, generates\nsub-questions that are as effective as human-authored sub-questions. We also\nintroduce a new global rescoring approach that considers each decomposition\n(i.e. the sub-questions and their answers) to select the best final answer,\ngreatly improving overall performance. Our experiments on HotpotQA show that\nthis approach achieves the state-of-the-art results, while providing\nexplainable evidence for its decision making in the form of sub-questions.", "published": "2019-06-07 06:22:17", "link": "http://arxiv.org/abs/1906.02916v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Shared-Private Bilingual Word Embeddings for Neural Machine Translation", "abstract": "Word embedding is central to neural machine translation (NMT), which has\nattracted intensive research interest in recent years. In NMT, the source\nembedding plays the role of the entrance while the target embedding acts as the\nterminal. These layers occupy most of the model parameters for representation\nlearning. Furthermore, they indirectly interface via a soft-attention\nmechanism, which makes them comparatively isolated. In this paper, we propose\nshared-private bilingual word embeddings, which give a closer relationship\nbetween the source and target embeddings, and which also reduce the number of\nmodel parameters. For similar source and target words, their embeddings tend to\nshare a part of the features and they cooperatively learn these common\nrepresentation units. Experiments on 5 language pairs belonging to 6 different\nlanguage families and written in 5 different alphabets demonstrate that the\nproposed model provides a significant performance boost over the strong\nbaselines with dramatically fewer model parameters.", "published": "2019-06-07 13:48:46", "link": "http://arxiv.org/abs/1906.03100v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Word-based Domain Adaptation for Neural Machine Translation", "abstract": "In this paper, we empirically investigate applying word-level weights to\nadapt neural machine translation to e-commerce domains, where small e-commerce\ndatasets and large out-of-domain datasets are available. In order to mine\nin-domain like words in the out-of-domain datasets, we compute word weights by\nusing a domain-specific and a non-domain-specific language model followed by\nsmoothing and binary quantization. The baseline model is trained on mixed\nin-domain and out-of-domain datasets. Experimental results on English to\nChinese e-commerce domain translation show that compared to continuing training\nwithout word weights, it improves MT quality by up to 2.11% BLEU absolute and\n1.59% TER. We have also trained models using fine-tuning on the in-domain data.\nPre-training a model with word weights improves fine-tuning up to 1.24% BLEU\nabsolute and 1.64% TER, respectively.", "published": "2019-06-07 14:32:17", "link": "http://arxiv.org/abs/1906.03129v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Matching the Blanks: Distributional Similarity for Relation Learning", "abstract": "General purpose relation extractors, which can model arbitrary relations, are\na core aspiration in information extraction. Efforts have been made to build\ngeneral purpose extractors that represent relations with their surface forms,\nor which jointly embed surface forms with relations from an existing knowledge\ngraph. However, both of these approaches are limited in their ability to\ngeneralize. In this paper, we build on extensions of Harris' distributional\nhypothesis to relations, as well as recent advances in learning text\nrepresentations (specifically, BERT), to build task agnostic relation\nrepresentations solely from entity-linked text. We show that these\nrepresentations significantly outperform previous work on exemplar based\nrelation extraction (FewRel) even without using any of that task's training\ndata. We also show that models initialized with our task agnostic\nrepresentations, and then tuned on supervised relation extraction datasets,\nsignificantly outperform the previous methods on SemEval 2010 Task 8, KBP37,\nand TACRED.", "published": "2019-06-07 15:26:50", "link": "http://arxiv.org/abs/1906.03158v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Assessing incrementality in sequence-to-sequence models", "abstract": "Since their inception, encoder-decoder models have successfully been applied\nto a wide array of problems in computational linguistics. The most recent\nsuccesses are predominantly due to the use of different variations of attention\nmechanisms, but their cognitive plausibility is questionable. In particular,\nbecause past representations can be revisited at any point in time,\nattention-centric methods seem to lack an incentive to build up incrementally\nmore informative representations of incoming sentences. This way of processing\nstands in stark contrast with the way in which humans are believed to process\nlanguage: continuously and rapidly integrating new information as it is\nencountered. In this work, we propose three novel metrics to assess the\nbehavior of RNNs with and without an attention mechanism and identify key\ndifferences in the way the different model types process sentences.", "published": "2019-06-07 18:45:51", "link": "http://arxiv.org/abs/1906.03293v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Classifying the reported ability in clinical mobility descriptions", "abstract": "Assessing how individuals perform different activities is key information for\nmodeling health states of individuals and populations. Descriptions of activity\nperformance in clinical free text are complex, including syntactic negation and\nsimilarities to textual entailment tasks. We explore a variety of methods for\nthe novel task of classifying four types of assertions about activity\nperformance: Able, Unable, Unclear, and None (no information). We find that\nensembling an SVM trained with lexical features and a CNN achieves 77.9% macro\nF1 score on our task, and yields nearly 80% recall on the rare Unclear and\nUnable samples. Finally, we highlight several challenges in classifying\nperformance assertions, including capturing information about sources of\nassistance, incorporating syntactic structure and negation scope, and handling\nnew modalities at test time. Our findings establish a strong baseline for this\nnovel task, and identify intriguing areas for further research.", "published": "2019-06-07 22:26:29", "link": "http://arxiv.org/abs/1906.03348v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Real or Fake? Learning to Discriminate Machine from Human Generated Text", "abstract": "Energy-based models (EBMs), a.k.a. un-normalized models, have had recent\nsuccesses in continuous spaces. However, they have not been successfully\napplied to model text sequences. While decreasing the energy at training\nsamples is straightforward, mining (negative) samples where the energy should\nbe increased is difficult. In part, this is because standard gradient-based\nmethods are not readily applicable when the input is high-dimensional and\ndiscrete. Here, we side-step this issue by generating negatives using\npre-trained auto-regressive language models. The EBM then works in the residual\nof the language model; and is trained to discriminate real text from text\ngenerated by the auto-regressive models. We investigate the generalization\nability of residual EBMs, a pre-requisite for using them in other applications.\nWe extensively analyze generalization for the task of classifying whether an\ninput is machine or human generated, a natural task given the training loss and\nhow we mine negatives. Overall, we observe that EBMs can generalize remarkably\nwell to changes in the architecture of the generators producing negatives.\nHowever, EBMs exhibit more sensitivity to the training set used by such\ngenerators.", "published": "2019-06-07 22:45:33", "link": "http://arxiv.org/abs/1906.03351v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "FAKTA: An Automatic End-to-End Fact Checking System", "abstract": "We present FAKTA which is a unified framework that integrates various\ncomponents of a fact checking process: document retrieval from media sources\nwith various types of reliability, stance detection of documents with respect\nto given claims, evidence extraction, and linguistic analysis. FAKTA predicts\nthe factuality of given claims and provides evidence at the document and\nsentence level to explain its predictions", "published": "2019-06-07 18:49:38", "link": "http://arxiv.org/abs/1906.04164v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Analyzing the Structure of Attention in a Transformer Language Model", "abstract": "The Transformer is a fully attention-based alternative to recurrent networks\nthat has achieved state-of-the-art results across a range of NLP tasks. In this\npaper, we analyze the structure of attention in a Transformer language model,\nthe GPT-2 small pretrained model. We visualize attention for individual\ninstances and analyze the interaction between attention and syntax over a large\ncorpus. We find that attention targets different parts of speech at different\nlayer depths within the model, and that attention aligns with dependency\nrelations most strongly in the middle layers. We also find that the deepest\nlayers of the model capture the most distant relationships. Finally, we extract\nexemplar sentences that reveal highly specific patterns targeted by particular\nattention heads.", "published": "2019-06-07 13:58:49", "link": "http://arxiv.org/abs/1906.04284v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Leveraging BERT for Extractive Text Summarization on Lectures", "abstract": "In the last two decades, automatic extractive text summarization on lectures\nhas demonstrated to be a useful tool for collecting key phrases and sentences\nthat best represent the content. However, many current approaches utilize dated\napproaches, producing sub-par outputs or requiring several hours of manual\ntuning to produce meaningful results. Recently, new machine learning\narchitectures have provided mechanisms for extractive summarization through the\nclustering of output embeddings from deep learning models. This paper reports\non the project called Lecture Summarization Service, a python based RESTful\nservice that utilizes the BERT model for text embeddings and KMeans clustering\nto identify sentences closes to the centroid for summary selection. The purpose\nof the service was to provide students a utility that could summarize lecture\ncontent, based on their desired number of sentences. On top of the summary\nwork, the service also includes lecture and summary management, storing content\non the cloud which can be used for collaboration. While the results of\nutilizing BERT for extractive summarization were promising, there were still\nareas where the model struggled, providing feature research opportunities for\nfurther improvement.", "published": "2019-06-07 19:50:30", "link": "http://arxiv.org/abs/1906.04165v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Beamforming and other methods for denoising microphone array data", "abstract": "Measured acoustic data can be contaminated by noise. This typically happens\nwhen microphones are mounted in a wind tunnel wall or on the fuselage of an\naircraft, where hydrodynamic pressure fluctuations of the Turbulent Boundary\nLayer (TBL) can mask the acoustic pressures of interest. For measurements done\nwith an array of microphones, methods exist for denoising the acoustic data.\nUse is made of the fact that the noise is usually concentrated in the diagonal\nof the Cross-Spectral Matrix, because of the short spatial coherence of TBL\nnoise. This paper reviews several existing denoising methods and considers the\nuse of Conventional Beamforming, Source Power Integration and CLEAN-SC for this\npurpose. A comparison between the methods is made using synthesized array data.", "published": "2019-06-07 08:50:07", "link": "http://arxiv.org/abs/1906.02965v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Audio tagging with noisy labels and minimal supervision", "abstract": "This paper introduces Task 2 of the DCASE2019 Challenge, titled \"Audio\ntagging with noisy labels and minimal supervision\". This task was hosted on the\nKaggle platform as \"Freesound Audio Tagging 2019\". The task evaluates systems\nfor multi-label audio tagging using a large set of noisy-labeled data, and a\nmuch smaller set of manually-labeled data, under a large vocabulary setting of\n80 everyday sound classes. In addition, the proposed dataset poses an acoustic\nmismatch problem between the noisy train set and the test set due to the fact\nthat they come from different web audio sources. This can correspond to a\nrealistic scenario given by the difficulty in gathering large amounts of\nmanually labeled data. We present the task setup, the FSDKaggle2019 dataset\nprepared for this scientific evaluation, and a baseline system consisting of a\nconvolutional neural network. All these resources are freely available.", "published": "2019-06-07 09:09:56", "link": "http://arxiv.org/abs/1906.02975v4", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
