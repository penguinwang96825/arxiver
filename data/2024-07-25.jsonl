{"title": "S2-Attention: Hardware-Aware Context Sharding Among Attention Heads", "abstract": "Sparse attention, which selectively attends to a subset of tokens in the\ncontext was supposed to be efficient. However, its theoretical reduction in\nFLOPs has rarely translated into wall-clock speed-up over its dense attention\ncounterparts due to the lack of hardware-aware optimizations like\nFlashAttention. Meanwhile, it remains unclear whether sparse attention can\nmaintain the model's quality at a scale of today's large language models (LLMs)\nand how. This paper presents Sparsely-Sharded(S2) Attention, a Triton library\nthat provides kernel optimization for sparse attention customizable at both\nper-head and per-context-range levels. S2-Attention enables the exploration of\nnovel and high-performance sparse attention techniques, which we demonstrate\nthrough extensive ablations across a wide range of sparse attention designs at\nvarious model scales. From these insights, we present several basic guidelines\nto design sparse attention that can achieve not only practical efficiency\nimprovements, but also strong downstream performance. To achieve high\nparallelization and optimized memory IO, sparse attention should shard the\ncontext heterogeneously across attention heads, where each head attends to a\ndifferent subset of tokens while collectively covering the full context.\nMeanwhile, we find hybrid architectures combining sparse and dense attention\nparticularly beneficial in practice. S2-Attention achieves wall-clock speedup\nof 8.79X, 15.87X, 25.3X compared to the strong FlashAttention-2 baseline with\nstrong downstream performance on-par with full attention and perfect retrieval\nperformance at a 128k context length. At inference, for 7B models, our model,\nwith the help of our S2-Attention kernel, achieves 4.5x speed-up compared to\ndense counterparts. S2-Attention is released with easy-to-customize APIs for\ndirect usage in Megatron and vLLM.", "published": "2024-07-25 00:27:07", "link": "http://arxiv.org/abs/2407.17678v7", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Large Language Models Possible to Conduct Cognitive Behavioral\n  Therapy?", "abstract": "In contemporary society, the issue of psychological health has become\nincreasingly prominent, characterized by the diversification, complexity, and\nuniversality of mental disorders. Cognitive Behavioral Therapy (CBT), currently\nthe most influential and clinically effective psychological treatment method\nwith no side effects, has limited coverage and poor quality in most countries.\nIn recent years, researches on the recognition and intervention of emotional\ndisorders using large language models (LLMs) have been validated, providing new\npossibilities for psychological assistance therapy. However, are LLMs truly\npossible to conduct cognitive behavioral therapy? Many concerns have been\nraised by mental health experts regarding the use of LLMs for therapy. Seeking\nto answer this question, we collected real CBT corpus from online video\nwebsites, designed and conducted a targeted automatic evaluation framework\ninvolving the evaluation of emotion tendency of generated text, structured\ndialogue pattern and proactive inquiry ability. For emotion tendency, we\ncalculate the emotion tendency score of the CBT dialogue text generated by each\nmodel. For structured dialogue pattern, we use a diverse range of automatic\nevaluation metrics to compare speaking style, the ability to maintain\nconsistency of topic and the use of technology in CBT between different models\n. As for inquiring to guide the patient, we utilize PQA (Proactive Questioning\nAbility) metric. We also evaluated the CBT ability of the LLM after integrating\na CBT knowledge base to explore the help of introducing additional knowledge to\nenhance the model's CBT counseling ability. Four LLM variants with excellent\nperformance on natural language processing are evaluated, and the experimental\nresult shows the great potential of LLMs in psychological counseling realm,\nespecially after combining with other technological means.", "published": "2024-07-25 03:01:47", "link": "http://arxiv.org/abs/2407.17730v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Entity Alignment: Towards Complete Knowledge Graph Alignment via\n  Entity-Relation Synergy", "abstract": "Knowledge Graph Alignment (KGA) aims to integrate knowledge from multiple\nsources to address the limitations of individual Knowledge Graphs (KGs) in\nterms of coverage and depth. However, current KGA models fall short in\nachieving a ``complete'' knowledge graph alignment. Existing models primarily\nemphasize the linkage of cross-graph entities but overlook aligning relations\nacross KGs, thereby providing only a partial solution to KGA. The semantic\ncorrelations embedded in relations are largely overlooked, potentially\nrestricting a comprehensive understanding of cross-KG signals. In this paper,\nwe propose to conceptualize relation alignment as an independent task and\nconduct KGA by decomposing it into two distinct but highly correlated\nsub-tasks: entity alignment and relation alignment. To capture the mutually\nreinforcing correlations between these objectives, we propose a novel\nExpectation-Maximization-based model, EREM, which iteratively optimizes both\nsub-tasks. Experimental results on real-world datasets demonstrate that EREM\nconsistently outperforms state-of-the-art models in both entity alignment and\nrelation alignment tasks.", "published": "2024-07-25 03:40:09", "link": "http://arxiv.org/abs/2407.17745v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BotEval: Facilitating Interactive Human Evaluation", "abstract": "Following the rapid progress in natural language processing (NLP) models,\nlanguage models are applied to increasingly more complex interactive tasks such\nas negotiations and conversation moderations. Having human evaluators directly\ninteract with these NLP models is essential for adequately evaluating the\nperformance on such interactive tasks. We develop BotEval, an easily\ncustomizable, open-source, evaluation toolkit that focuses on enabling\nhuman-bot interactions as part of the evaluation process, as opposed to human\nevaluators making judgements for a static input. BotEval balances flexibility\nfor customization and user-friendliness by providing templates for common use\ncases that span various degrees of complexity and built-in compatibility with\npopular crowdsourcing platforms. We showcase the numerous useful features of\nBotEval through a study that evaluates the performance of various chatbots on\ntheir effectiveness for conversational moderation and discuss how BotEval\ndiffers from other annotation tools.", "published": "2024-07-25 04:57:31", "link": "http://arxiv.org/abs/2407.17770v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Banyan: Improved Representation Learning with Explicit Structure", "abstract": "We present Banyan, a model that efficiently learns semantic representations\nby leveraging explicit hierarchical structure. While transformers excel at\nscale, they struggle in low-resource settings. Conversely recent structured\nmodels have shown promise as efficient learners, but lack performance. Banyan\nbridges this gap with two key innovations: an entangled hierarchical tree\nstructure and diagonalized message passing, enabling it to outperform larger\ntransformer models with just 14 non-embedding parameters. It excels in\nlow-resource settings, offering a viable alternative for under-represented\nlanguages and highlighting its potential for efficient, interpretable NLP in\nresource-constrained environments.", "published": "2024-07-25 04:58:08", "link": "http://arxiv.org/abs/2407.17771v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scaling A Simple Approach to Zero-Shot Speech Recognition", "abstract": "Despite rapid progress in increasing the language coverage of automatic\nspeech recognition, the field is still far from covering all languages with a\nknown writing script. Recent work showed promising results with a zero-shot\napproach requiring only a small amount of text data, however, accuracy heavily\ndepends on the quality of the used phonemizer which is often weak for unseen\nlanguages. In this paper, we present MMS Zero-shot a conceptually simpler\napproach based on romanization and an acoustic model trained on data in 1,078\ndifferent languages or three orders of magnitude more than prior art. MMS\nZero-shot reduces the average character error rate by a relative 46% over 100\nunseen languages compared to the best previous work. Moreover, the error rate\nof our approach is only 2.5x higher compared to in-domain supervised baselines,\nwhile our approach uses no labeled data for the evaluation languages at all.", "published": "2024-07-25 08:08:55", "link": "http://arxiv.org/abs/2407.17852v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Description-Augmented Dataless Intent Classification", "abstract": "In this work, we introduce several schemes to leverage description-augmented\nembedding similarity for dataless intent classification using current\nstate-of-the-art (SOTA) text embedding models. We report results of our methods\non four commonly used intent classification datasets and compare against\nprevious works of a similar nature. Our work shows promising results for\ndataless classification scaling to a large number of unseen intents. We show\ncompetitive results and significant improvements (+6.12\\% Avg.) over strong\nzero-shot baselines, all without training on labelled or task-specific data.\nFurthermore, we provide qualitative error analysis of the shortfalls of this\nmethodology to help guide future research in this area.", "published": "2024-07-25 08:31:57", "link": "http://arxiv.org/abs/2407.17862v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "factgenie: A Framework for Span-based Evaluation of Generated Texts", "abstract": "We present factgenie: a framework for annotating and visualizing word spans\nin textual model outputs. Annotations can capture various span-based phenomena\nsuch as semantic inaccuracies or irrelevant text. With factgenie, the\nannotations can be collected both from human crowdworkers and large language\nmodels. Our framework consists of a web interface for data visualization and\ngathering text annotations, powered by an easily extensible codebase.", "published": "2024-07-25 08:33:23", "link": "http://arxiv.org/abs/2407.17863v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modelling Multimodal Integration in Human Concept Processing with\n  Vision-and-Language Models", "abstract": "Representations from deep neural networks (DNNs) have proven remarkably\npredictive of neural activity involved in both visual and linguistic\nprocessing. Despite these successes, most studies to date concern unimodal\nDNNs, encoding either visual or textual input but not both. Yet, there is\ngrowing evidence that human meaning representations integrate linguistic and\nsensory-motor information. Here we investigate whether the integration of\nmultimodal information operated by current vision-and-language DNN models\n(VLMs) leads to representations that are more aligned with human brain activity\nthan those obtained by language-only and vision-only DNNs. We focus on fMRI\nresponses recorded while participants read concept words in the context of\neither a full sentence or an accompanying picture. Our results reveal that VLM\nrepresentations correlate more strongly than language- and vision-only DNNs\nwith activations in brain areas functionally related to language processing. A\ncomparison between different types of visuo-linguistic architectures shows that\nrecent generative VLMs tend to be less brain-aligned than previous\narchitectures with lower performance on downstream applications. Moreover,\nthrough an additional analysis comparing brain vs. behavioural alignment across\nmultiple VLMs, we show that -- with one remarkable exception -- representations\nthat strongly align with behavioural judgments do not correlate highly with\nbrain responses. This indicates that brain similarity does not go hand in hand\nwith behavioural similarity, and vice versa.", "published": "2024-07-25 10:08:37", "link": "http://arxiv.org/abs/2407.17914v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Curious Case of Representational Alignment: Unravelling\n  Visio-Linguistic Tasks in Emergent Communication", "abstract": "Natural language has the universal properties of being compositional and\ngrounded in reality. The emergence of linguistic properties is often\ninvestigated through simulations of emergent communication in referential\ngames. However, these experiments have yielded mixed results compared to\nsimilar experiments addressing linguistic properties of human language. Here we\naddress representational alignment as a potential contributing factor to these\nresults. Specifically, we assess the representational alignment between agent\nimage representations and between agent representations and input images. Doing\nso, we confirm that the emergent language does not appear to encode human-like\nconceptual visual features, since agent image representations drift away from\ninputs whilst inter-agent alignment increases. We moreover identify a strong\nrelationship between inter-agent alignment and topographic similarity, a common\nmetric for compositionality, and address its consequences. To address these\nissues, we introduce an alignment penalty that prevents representational drift\nbut interestingly does not improve performance on a compositional\ndiscrimination task. Together, our findings emphasise the key role\nrepresentational alignment plays in simulations of language emergence.", "published": "2024-07-25 11:29:27", "link": "http://arxiv.org/abs/2407.17960v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What does Kiki look like? Cross-modal associations between speech sounds\n  and visual shapes in vision-and-language models", "abstract": "Humans have clear cross-modal preferences when matching certain novel words\nto visual shapes. Evidence suggests that these preferences play a prominent\nrole in our linguistic processing, language learning, and the origins of\nsignal-meaning mappings. With the rise of multimodal models in AI, such as\nvision- and-language (VLM) models, it becomes increasingly important to uncover\nthe kinds of visio-linguistic associations these models encode and whether they\nalign with human representations. Informed by experiments with humans, we probe\nand compare four VLMs for a well-known human cross-modal preference, the\nbouba-kiki effect. We do not find conclusive evidence for this effect but\nsuggest that results may depend on features of the models, such as architecture\ndesign, model size, and training details. Our findings inform discussions on\nthe origins of the bouba-kiki effect in human cognition and future developments\nof VLMs that align well with human cross-modal associations.", "published": "2024-07-25 12:09:41", "link": "http://arxiv.org/abs/2407.17974v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption", "abstract": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache.", "published": "2024-07-25 12:56:22", "link": "http://arxiv.org/abs/2407.18003v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tracking linguistic information in transformer-based sentence embeddings\n  through targeted sparsification", "abstract": "Analyses of transformer-based models have shown that they encode a variety of\nlinguistic information from their textual input. While these analyses have shed\na light on the relation between linguistic information on one side, and\ninternal architecture and parameters on the other, a question remains\nunanswered: how is this linguistic information reflected in sentence\nembeddings? Using datasets consisting of sentences with known structure, we\ntest to what degree information about chunks (in particular noun, verb or\nprepositional phrases), such as grammatical number, or semantic role, can be\nlocalized in sentence embeddings. Our results show that such information is not\ndistributed over the entire sentence embedding, but rather it is encoded in\nspecific regions. Understanding how the information from an input text is\ncompressed into sentence embeddings helps understand current transformer models\nand help build future explainable neural models.", "published": "2024-07-25 15:27:08", "link": "http://arxiv.org/abs/2407.18119v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "The FIGNEWS Shared Task on News Media Narratives", "abstract": "We present an overview of the FIGNEWS shared task, organized as part of the\nArabicNLP 2024 conference co-located with ACL 2024. The shared task addresses\nbias and propaganda annotation in multilingual news posts. We focus on the\nearly days of the Israel War on Gaza as a case study. The task aims to foster\ncollaboration in developing annotation guidelines for subjective tasks by\ncreating frameworks for analyzing diverse narratives highlighting potential\nbias and propaganda. In a spirit of fostering and encouraging diversity, we\naddress the problem from a multilingual perspective, namely within five\nlanguages: English, French, Arabic, Hebrew, and Hindi. A total of 17 teams\nparticipated in two annotation subtasks: bias (16 teams) and propaganda (6\nteams). The teams competed in four evaluation tracks: guidelines development,\nannotation quality, annotation quantity, and consistency. Collectively, the\nteams produced 129,800 data points. Key findings and implications for the field\nare discussed.", "published": "2024-07-25 15:58:19", "link": "http://arxiv.org/abs/2407.18147v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Training with Direct Preference Optimization Improves\n  Chain-of-Thought Reasoning", "abstract": "Effective training of language models (LMs) for mathematical reasoning tasks\ndemands high-quality supervised fine-tuning data. Besides obtaining annotations\nfrom human experts, a common alternative is sampling from larger and more\npowerful LMs. However, this knowledge distillation approach can be costly and\nunstable, particularly when relying on closed-source, proprietary LMs like\nGPT-4, whose behaviors are often unpredictable. In this work, we demonstrate\nthat the reasoning abilities of small-scale LMs can be enhanced through\nself-training, a process where models learn from their own outputs. We also\nshow that the conventional self-training can be further augmented by a\npreference learning algorithm called Direct Preference Optimization (DPO). By\nintegrating DPO into self-training, we leverage preference data to guide LMs\ntowards more accurate and diverse chain-of-thought reasoning. We evaluate our\nmethod across various mathematical reasoning tasks using different base models.\nOur experiments show that this approach not only improves LMs' reasoning\nperformance but also offers a more cost-effective and scalable solution\ncompared to relying on large proprietary LMs.", "published": "2024-07-25 17:59:16", "link": "http://arxiv.org/abs/2407.18248v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Know Your Limits: A Survey of Abstention in Large Language Models", "abstract": "Abstention, the refusal of large language models (LLMs) to provide an answer,\nis increasingly recognized for its potential to mitigate hallucinations and\nenhance safety in LLM systems. In this survey, we introduce a framework to\nexamine abstention from three perspectives: the query, the model, and human\nvalues. We organize the literature on abstention methods, benchmarks, and\nevaluation metrics using this framework, and discuss merits and limitations of\nprior work. We further identify and motivate areas for future research, such as\nwhether abstention can be achieved as a meta-capability that transcends\nspecific tasks or domains, and opportunities to optimize abstention abilities\nin specific contexts. In doing so, we aim to broaden the scope and impact of\nabstention methodologies in AI systems.", "published": "2024-07-25 22:31:50", "link": "http://arxiv.org/abs/2407.18418v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Examining the Influence of Political Bias on Large Language Model\n  Performance in Stance Classification", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nexecuting tasks based on natural language queries. However, these models,\ntrained on curated datasets, inherently embody biases ranging from racial to\nnational and gender biases. It remains uncertain whether these biases impact\nthe performance of LLMs for certain tasks. In this study, we investigate the\npolitical biases of LLMs within the stance classification task, specifically\nexamining whether these models exhibit a tendency to more accurately classify\npolitically-charged stances. Utilizing three datasets, seven LLMs, and four\ndistinct prompting schemes, we analyze the performance of LLMs on politically\noriented statements and targets. Our findings reveal a statistically\nsignificant difference in the performance of LLMs across various politically\noriented stance classification tasks. Furthermore, we observe that this\ndifference primarily manifests at the dataset level, with models and prompting\nschemes showing statistically similar performances across different stance\nclassification datasets. Lastly, we observe that when there is greater\nambiguity in the target the statement is directed towards, LLMs have poorer\nstance classification accuracy.\n  Code & Dataset: http://doi.org/10.5281/zenodo.12938478", "published": "2024-07-25 01:11:38", "link": "http://arxiv.org/abs/2407.17688v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Agent Learning through World Dynamics Modeling", "abstract": "Large language models (LLMs) have been increasingly applied to tasks in\nlanguage understanding and interactive decision-making, with their impressive\nperformance largely attributed to the extensive domain knowledge embedded\nwithin them. However, the depth and breadth of this knowledge can vary across\ndomains. Many existing approaches assume that LLMs possess a comprehensive\nunderstanding of their environment, often overlooking potential gaps in their\ngrasp of actual world dynamics. To address this, we introduce Discover, Verify,\nand Evolve (DiVE), a framework that discovers world dynamics from a small\nnumber of demonstrations, verifies the accuracy of these dynamics, and evolves\nnew, advanced dynamics tailored to the current situation. Through extensive\nevaluations, we assess the impact of each component on performance and compare\nthe dynamics generated by DiVE to human-annotated dynamics. Our results show\nthat LLMs guided by DiVE make more informed decisions, achieving rewards\ncomparable to human players in the Crafter environment and surpassing methods\nthat require prior task-specific training in the MiniHack environment.", "published": "2024-07-25 01:32:41", "link": "http://arxiv.org/abs/2407.17695v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Demystifying Verbatim Memorization in Large Language Models", "abstract": "Large Language Models (LLMs) frequently memorize long sequences verbatim,\noften with serious legal and privacy implications. Much prior work has studied\nsuch verbatim memorization using observational data. To complement such work,\nwe develop a framework to study verbatim memorization in a controlled setting\nby continuing pre-training from Pythia checkpoints with injected sequences. We\nfind that (1) non-trivial amounts of repetition are necessary for verbatim\nmemorization to happen; (2) later (and presumably better) checkpoints are more\nlikely to verbatim memorize sequences, even for out-of-distribution sequences;\n(3) the generation of memorized sequences is triggered by distributed model\nstates that encode high-level features and makes important use of general\nlanguage modeling capabilities. Guided by these insights, we develop stress\ntests to evaluate unlearning methods and find they often fail to remove the\nverbatim memorized information, while also degrading the LM. Overall, these\nfindings challenge the hypothesis that verbatim memorization stems from\nspecific model weights or mechanisms. Rather, verbatim memorization is\nintertwined with the LM's general capabilities and thus will be very difficult\nto isolate and suppress without degrading model quality.", "published": "2024-07-25 07:10:31", "link": "http://arxiv.org/abs/2407.17817v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Is the Digital Forensics and Incident Response Pipeline Ready for\n  Text-Based Threats in LLM Era?", "abstract": "In the era of generative AI, the widespread adoption of Neural Text\nGenerators (NTGs) presents new cybersecurity challenges, particularly within\nthe realms of Digital Forensics and Incident Response (DFIR). These challenges\nprimarily involve the detection and attribution of sources behind advanced\nattacks like spearphishing and disinformation campaigns. As NTGs evolve, the\ntask of distinguishing between human and NTG-authored texts becomes critically\ncomplex. This paper rigorously evaluates the DFIR pipeline tailored for\ntext-based security systems, specifically focusing on the challenges of\ndetecting and attributing authorship of NTG-authored texts. By introducing a\nnovel human-NTG co-authorship text attack, termed CS-ACT, our study uncovers\nsignificant vulnerabilities in traditional DFIR methodologies, highlighting\ndiscrepancies between ideal scenarios and real-world conditions. Utilizing 14\ndiverse datasets and 43 unique NTGs, up to the latest GPT-4, our research\nidentifies substantial vulnerabilities in the forensic profiling phase,\nparticularly in attributing authorship to NTGs. Our comprehensive evaluation\npoints to factors such as model sophistication and the lack of distinctive\nstyle within NTGs as significant contributors for these vulnerabilities. Our\nfindings underscore the necessity for more sophisticated and adaptable\nstrategies, such as incorporating adversarial learning, stylizing NTGs, and\nimplementing hierarchical attribution through the mapping of NTG lineages to\nenhance source attribution. This sets the stage for future research and the\ndevelopment of more resilient text-based security systems.", "published": "2024-07-25 08:42:53", "link": "http://arxiv.org/abs/2407.17870v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Improving Domain-Specific ASR with LLM-Generated Contextual Descriptions", "abstract": "End-to-end automatic speech recognition (E2E ASR) systems have significantly\nimproved speech recognition through training on extensive datasets. Despite\nthese advancements, they still struggle to accurately recognize domain specific\nwords, such as proper nouns and technical terminologies. To address this\nproblem, we propose a method to utilize the state-of-the-art Whisper without\nmodifying its architecture, preserving its generalization performance while\nenabling it to leverage descriptions effectively. Moreover, we propose two\nadditional training techniques to improve the domain specific ASR: decoder\nfine-tuning, and context perturbation. We also propose a method to use a Large\nLanguage Model (LLM) to generate descriptions with simple metadata, when\ndescriptions are unavailable. Our experiments demonstrate that proposed methods\nnotably enhance domain-specific ASR accuracy on real-life datasets, with\nLLM-generated descriptions outperforming human-crafted ones in effectiveness.", "published": "2024-07-25 08:44:04", "link": "http://arxiv.org/abs/2407.17874v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Large-Scale Sensitivity Analysis on Latent Embeddings and\n  Dimensionality Reductions for Text Spatializations", "abstract": "The semantic similarity between documents of a text corpus can be visualized\nusing map-like metaphors based on two-dimensional scatterplot layouts. These\nlayouts result from a dimensionality reduction on the document-term matrix or a\nrepresentation within a latent embedding, including topic models. Thereby, the\nresulting layout depends on the input data and hyperparameters of the\ndimensionality reduction and is therefore affected by changes in them.\nFurthermore, the resulting layout is affected by changes in the input data and\nhyperparameters of the dimensionality reduction. However, such changes to the\nlayout require additional cognitive efforts from the user. In this work, we\npresent a sensitivity study that analyzes the stability of these layouts\nconcerning (1) changes in the text corpora, (2) changes in the hyperparameter,\nand (3) randomness in the initialization. Our approach has two stages: data\nmeasurement and data analysis. First, we derived layouts for the combination of\nthree text corpora and six text embeddings and a grid-search-inspired\nhyperparameter selection of the dimensionality reductions. Afterward, we\nquantified the similarity of the layouts through ten metrics, concerning local\nand global structures and class separation. Second, we analyzed the resulting\n42817 tabular data points in a descriptive statistical analysis. From this, we\nderived guidelines for informed decisions on the layout algorithm and highlight\nspecific hyperparameter settings. We provide our implementation as a Git\nrepository at\nhttps://github.com/hpicgs/Topic-Models-and-Dimensionality-Reduction-Sensitivity-Study\nand results as Zenodo archive at https://doi.org/10.5281/zenodo.12772898.", "published": "2024-07-25 08:46:49", "link": "http://arxiv.org/abs/2407.17876v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Power of Combining Data and Knowledge: GPT-4o is an Effective\n  Interpreter of Machine Learning Models in Predicting Lymph Node Metastasis of\n  Lung Cancer", "abstract": "Lymph node metastasis (LNM) is a crucial factor in determining the initial\ntreatment for patients with lung cancer, yet accurate preoperative diagnosis of\nLNM remains challenging. Recently, large language models (LLMs) have garnered\nsignificant attention due to their remarkable text generation capabilities.\nLeveraging the extensive medical knowledge learned from vast corpora, LLMs can\nestimate probabilities for clinical problems, though their performance has\nhistorically been inferior to data-driven machine learning models. In this\npaper, we propose a novel ensemble method that combines the medical knowledge\nacquired by LLMs with the latent patterns identified by machine learning models\nto enhance LNM prediction performance. Initially, we developed machine learning\nmodels using patient data. We then designed a prompt template to integrate the\npatient data with the predicted probability from the machine learning model.\nSubsequently, we instructed GPT-4o, the most advanced LLM developed by OpenAI,\nto estimate the likelihood of LNM based on patient data and then adjust the\nestimate using the machine learning output. Finally, we collected three outputs\nfrom the GPT-4o using the same prompt and ensembled these results as the final\nprediction. Using the proposed method, our models achieved an AUC value of\n0.778 and an AP value of 0.426 for LNM prediction, significantly improving\npredictive performance compared to baseline machine learning models. The\nexperimental results indicate that GPT-4o can effectively leverage its medical\nknowledge and the probabilities predicted by machine learning models to achieve\nmore accurate LNM predictions. These findings demonstrate that LLMs can perform\nwell in clinical risk prediction tasks, offering a new paradigm for integrating\nmedical knowledge and patient data in clinical predictions.", "published": "2024-07-25 09:42:24", "link": "http://arxiv.org/abs/2407.17900v5", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Positive Text Reframing under Multi-strategy Optimization", "abstract": "Differing from sentiment transfer, positive reframing seeks to substitute\nnegative perspectives with positive expressions while preserving the original\nmeaning. With the emergence of pre-trained language models (PLMs), it is\npossible to achieve acceptable results by fine-tuning PLMs. Nevertheless,\ngenerating fluent, diverse and task-constrained reframing text remains a\nsignificant challenge. To tackle this issue, a \\textbf{m}ulti-\\textbf{s}trategy\n\\textbf{o}ptimization \\textbf{f}ramework (MSOF) is proposed in this paper.\nStarting from the objective of positive reframing, we first design positive\nsentiment reward and content preservation reward to encourage the model to\ntransform the negative expressions of the original text while ensuring the\nintegrity and consistency of the semantics. Then, different decoding\noptimization approaches are introduced to improve the quality of text\ngeneration. Finally, based on the modeling formula of positive reframing, we\npropose a multi-dimensional re-ranking method that further selects candidate\nsentences from three dimensions: strategy consistency, text similarity and\nfluency. Extensive experiments on two Seq2Seq PLMs, BART and T5, demonstrate\nour framework achieves significant improvements on unconstrained and controlled\npositive reframing tasks.", "published": "2024-07-25 10:58:42", "link": "http://arxiv.org/abs/2407.17940v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GermanPartiesQA: Benchmarking Commercial Large Language Models for\n  Political Bias and Sycophancy", "abstract": "LLMs are changing the way humans create and interact with content,\npotentially affecting citizens' political opinions and voting decisions. As\nLLMs increasingly shape our digital information ecosystems, auditing to\nevaluate biases, sycophancy, or steerability has emerged as an active field of\nresearch. In this paper, we evaluate and compare the alignment of six LLMs by\nOpenAI, Anthropic, and Cohere with German party positions and evaluate\nsycophancy based on a prompt experiment. We contribute to evaluating political\nbias and sycophancy in multi-party systems across major commercial LLMs. First,\nwe develop the benchmark dataset GermanPartiesQA based on the Voting Advice\nApplication Wahl-o-Mat covering 10 state and 1 national elections between 2021\nand 2023. In our study, we find a left-green tendency across all examined LLMs.\nWe then conduct our prompt experiment for which we use the benchmark and\nsociodemographic data of leading German parliamentarians to evaluate changes in\nLLMs responses. To differentiate between sycophancy and steerabilty, we use 'I\nam [politician X], ...' and 'You are [politician X], ...' prompts. Against our\nexpectations, we do not observe notable differences between prompting 'I am'\nand 'You are'. While our findings underscore that LLM responses can be\nideologically steered with political personas, they suggest that observed\nchanges in LLM outputs could be better described as personalization to the\ngiven context rather than sycophancy.", "published": "2024-07-25 13:04:25", "link": "http://arxiv.org/abs/2407.18008v1", "categories": ["cs.CY", "cs.CL", "K.4"], "primary_category": "cs.CY"}
{"title": "Difficulty Estimation and Simplification of French Text Using LLMs", "abstract": "We leverage generative large language models for language learning\napplications, focusing on estimating the difficulty of foreign language texts\nand simplifying them to lower difficulty levels. We frame both tasks as\nprediction problems and develop a difficulty classification model using labeled\nexamples, transfer learning, and large language models, demonstrating superior\naccuracy compared to previous approaches. For simplification, we evaluate the\ntrade-off between simplification quality and meaning preservation, comparing\nzero-shot and fine-tuned performances of large language models. We show that\nmeaningful text simplifications can be obtained with limited fine-tuning. Our\nexperiments are conducted on French texts, but our methods are\nlanguage-agnostic and directly applicable to other foreign languages.", "published": "2024-07-25 14:16:08", "link": "http://arxiv.org/abs/2407.18061v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PEFT-U: Parameter-Efficient Fine-Tuning for User Personalization", "abstract": "The recent emergence of Large Language Models (LLMs) has heralded a new era\nof human-AI interaction. These sophisticated models, exemplified by Chat-GPT\nand its successors, have exhibited remarkable capabilities in language\nunderstanding. However, as these LLMs have undergone exponential growth, a\ncrucial dimension that remains understudied is the personalization of these\nmodels. Large foundation models such as GPT-3 etc. focus on creating a\nuniversal model that serves a broad range of tasks and users. This approach\nemphasizes the model's generalization capabilities, treating users as a\ncollective rather than as distinct individuals. While practical for many common\napplications, this one-size-fits-all approach often fails to address the rich\ntapestry of human diversity and individual needs. To explore this issue we\nintroduce the PEFT-U Benchmark: a new dataset for building and evaluating NLP\nmodels for user personalization. \\datasetname{} consists of a series of\nuser-centered tasks containing diverse and individualized expressions where the\npreferences of users can potentially differ for the same input. Using PEFT-U,\nwe explore the challenge of efficiently personalizing LLMs to accommodate\nuser-specific preferences in the context of diverse user-centered tasks.", "published": "2024-07-25 14:36:18", "link": "http://arxiv.org/abs/2407.18078v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dallah: A Dialect-Aware Multimodal Large Language Model for Arabic", "abstract": "Recent advancements have significantly enhanced the capabilities of\nMultimodal Large Language Models (MLLMs) in generating and understanding\nimage-to-text content. Despite these successes, progress is predominantly\nlimited to English due to the scarcity of high quality multimodal resources in\nother languages. This limitation impedes the development of competitive models\nin languages such as Arabic. To alleviate this situation, we introduce an\nefficient Arabic multimodal assistant, dubbed Dallah, that utilizes an advanced\nlanguage model based on LLaMA-2 to facilitate multimodal interactions. Dallah\ndemonstrates state-of-the-art performance in Arabic MLLMs. Through fine-tuning\nsix Arabic dialects, Dallah showcases its capability to handle complex\ndialectal interactions incorporating both textual and visual elements. The\nmodel excels in two benchmark tests: one evaluating its performance on Modern\nStandard Arabic (MSA) and another specifically designed to assess dialectal\nresponses. Beyond its robust performance in multimodal interaction tasks,\nDallah has the potential to pave the way for further development of\ndialect-aware Arabic MLLMs.", "published": "2024-07-25 15:36:48", "link": "http://arxiv.org/abs/2407.18129v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Robust Claim Verification Through Fact Detection", "abstract": "Claim verification can be a challenging task. In this paper, we present a\nmethod to enhance the robustness and reasoning capabilities of automated claim\nverification through the extraction of short facts from evidence. Our novel\napproach, FactDetect, leverages Large Language Models (LLMs) to generate\nconcise factual statements from evidence and label these facts based on their\nsemantic relevance to the claim and evidence. The generated facts are then\ncombined with the claim and evidence. To train a lightweight supervised model,\nwe incorporate a fact-detection task into the claim verification process as a\nmultitasking approach to improve both performance and explainability. We also\nshow that augmenting FactDetect in the claim verification prompt enhances\nperformance in zero-shot claim verification using LLMs. Our method demonstrates\ncompetitive results in the supervised claim verification model by 15% on the F1\nscore when evaluated for challenging scientific claim verification datasets. We\nalso demonstrate that FactDetect can be augmented with claim and evidence for\nzero-shot prompting (AugFactDetect) in LLMs for verdict prediction. We show\nthat AugFactDetect outperforms the baseline with statistical significance on\nthree challenging scientific claim verification datasets with an average of\n17.3% performance gain compared to the best performing baselines.", "published": "2024-07-25 20:03:43", "link": "http://arxiv.org/abs/2407.18367v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Trust or Escalate: LLM Judges with Provable Guarantees for Human\n  Agreement", "abstract": "We present a principled approach to provide LLM-based evaluation with a\nrigorous guarantee of human agreement. We first propose that a reliable\nevaluation method should not uncritically rely on model preferences for\npairwise evaluation, but rather assess the confidence of judge models and\nselectively decide when to trust its judgement. We then show that under this\nselective evaluation framework, human agreement can be provably guaranteed --\nsuch that the model evaluation aligns with that of humans to a user-specified\nagreement level. As part of our framework, we also introduce Simulated\nAnnotators, a novel confidence estimation method that significantly improves\njudge calibration and thus enables high coverage of evaluated instances.\nFinally, we propose Cascaded Selective Evaluation, where we use cheaper models\nas initial judges and escalate to stronger models only when necessary -- again,\nwhile still providing a provable guarantee of human agreement. Experimental\nresults show that Cascaded Selective Evaluation guarantees strong alignment\nwith humans, far beyond what LLM judges could achieve without selective\nevaluation. For example, on a subset of Chatbot Arena where GPT-4 almost never\nachieves 80% human agreement, our method, even while employing substantially\ncost-effective models such as Mistral-7B, guarantees over 80% human agreement\nwith almost 80% test coverage.", "published": "2024-07-25 20:04:59", "link": "http://arxiv.org/abs/2407.18370v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Self-Directed Synthetic Dialogues and Revisions Technical Report", "abstract": "Synthetic data has become an important tool in the fine-tuning of language\nmodels to follow instructions and solve complex problems. Nevertheless, the\nmajority of open data to date is often lacking multi-turn data and collected on\nclosed models, limiting progress on advancing open fine-tuning methods. We\nintroduce Self Directed Synthetic Dialogues (SDSD), an experimental dataset\nconsisting of guided conversations of language models talking to themselves.\nThe dataset consists of multi-turn conversations generated with DBRX, Llama 2\n70B, and Mistral Large, all instructed to follow a conversation plan generated\nprior to the conversation. We also explore including principles from\nConstitutional AI and other related works to create synthetic preference data\nvia revisions to the final conversation turn. We hope this work encourages\nfurther exploration in multi-turn data and the use of open models for expanding\nthe impact of synthetic data.", "published": "2024-07-25 22:42:36", "link": "http://arxiv.org/abs/2407.18421v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Understanding the Interplay of Scale, Data, and Bias in Language Models:\n  A Case Study with BERT", "abstract": "In the current landscape of language model research, larger models, larger\ndatasets and more compute seems to be the only way to advance towards\nintelligence. While there have been extensive studies of scaling laws and\nmodels' scaling behaviors, the effect of scale on a model's social biases and\nstereotyping tendencies has received less attention. In this study, we explore\nthe influence of model scale and pre-training data on its learnt social biases.\nWe focus on BERT -- an extremely popular language model -- and investigate\nbiases as they show up during language modeling (upstream), as well as during\nclassification applications after fine-tuning (downstream). Our experiments on\nfour architecture sizes of BERT demonstrate that pre-training data\nsubstantially influences how upstream biases evolve with model scale. With\nincreasing scale, models pre-trained on large internet scrapes like Common\nCrawl exhibit higher toxicity, whereas models pre-trained on moderated data\nsources like Wikipedia show greater gender stereotypes. However, downstream\nbiases generally decrease with increasing model scale, irrespective of the\npre-training data. Our results highlight the qualitative role of pre-training\ndata in the biased behavior of language models, an often overlooked aspect in\nthe study of scale. Through a detailed case study of BERT, we shed light on the\ncomplex interplay of data and model scale, and investigate how it translates to\nconcrete biases.", "published": "2024-07-25 23:09:33", "link": "http://arxiv.org/abs/2407.21058v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Closing the gap between open-source and commercial large language models\n  for medical evidence summarization", "abstract": "Large language models (LLMs) hold great promise in summarizing medical\nevidence. Most recent studies focus on the application of proprietary LLMs.\nUsing proprietary LLMs introduces multiple risk factors, including a lack of\ntransparency and vendor dependency. While open-source LLMs allow better\ntransparency and customization, their performance falls short compared to\nproprietary ones. In this study, we investigated to what extent fine-tuning\nopen-source LLMs can further improve their performance in summarizing medical\nevidence. Utilizing a benchmark dataset, MedReview, consisting of 8,161 pairs\nof systematic reviews and summaries, we fine-tuned three broadly-used,\nopen-sourced LLMs, namely PRIMERA, LongT5, and Llama-2. Overall, the fine-tuned\nLLMs obtained an increase of 9.89 in ROUGE-L (95% confidence interval:\n8.94-10.81), 13.21 in METEOR score (95% confidence interval: 12.05-14.37), and\n15.82 in CHRF score (95% confidence interval: 13.89-16.44). The performance of\nfine-tuned LongT5 is close to GPT-3.5 with zero-shot settings. Furthermore,\nsmaller fine-tuned models sometimes even demonstrated superior performance\ncompared to larger zero-shot models. The above trends of improvement were also\nmanifested in both human and GPT4-simulated evaluations. Our results can be\napplied to guide model selection for tasks demanding particular domain\nknowledge, such as medical evidence summarization.", "published": "2024-07-25 05:03:01", "link": "http://arxiv.org/abs/2408.00588v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Describe Where You Are: Improving Noise-Robustness for Speech Emotion\n  Recognition with Text Description of the Environment", "abstract": "Speech emotion recognition (SER) systems often struggle in real-world\nenvironments, where ambient noise severely degrades their performance. This\npaper explores a novel approach that exploits prior knowledge of testing\nenvironments to maximize SER performance under noisy conditions. To address\nthis task, we propose a text-guided, environment-aware training where an SER\nmodel is trained with contaminated speech samples and their paired noise\ndescription. We use a pre-trained text encoder to extract the text-based\nenvironment embedding and then fuse it to a transformer-based SER model during\ntraining and inference. We demonstrate the effectiveness of our approach\nthrough our experiment with the MSP-Podcast corpus and real-world additive\nnoise samples collected from the Freesound repository. Our experiment indicates\nthat the text-based environment descriptions processed by a large language\nmodel (LLM) produce representations that improve the noise-robustness of the\nSER system. In addition, our proposed approach with an LLM yields better\nperformance than our environment-agnostic baselines, especially in low\nsignal-to-noise ratio (SNR) conditions. When testing at -5dB SNR level, our\nproposed method shows better performance than our best baseline model by 31.8 %\n(arousal), 23.5% (dominance), and 9.5% (valence).", "published": "2024-07-25 02:30:40", "link": "http://arxiv.org/abs/2407.17716v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Cost-effective Instruction Learning for Pathology Vision and Language\n  Analysis", "abstract": "The advent of vision-language models fosters the interactive conversations\nbetween AI-enabled models and humans. Yet applying these models into clinics\nmust deal with daunting challenges around large-scale training data, financial,\nand computational resources. Here we propose a cost-effective instruction\nlearning framework for conversational pathology named as CLOVER. CLOVER only\ntrains a lightweight module and uses instruction tuning while freezing the\nparameters of the large language model. Instead of using costly GPT-4, we\npropose well-designed prompts on GPT-3.5 for building generation-based\ninstructions, emphasizing the utility of pathological knowledge derived from\nthe Internet source. To augment the use of instructions, we construct a\nhigh-quality set of template-based instructions in the context of digital\npathology. From two benchmark datasets, our findings reveal the strength of\nhybrid-form instructions in the visual question-answer in pathology. Extensive\nresults show the cost-effectiveness of CLOVER in answering both open-ended and\nclosed-ended questions, where CLOVER outperforms strong baselines that possess\n37 times more training parameters and use instruction data generated from\nGPT-4. Through the instruction tuning, CLOVER exhibits robustness of few-shot\nlearning in the external clinical dataset. These findings demonstrate that\ncost-effective modeling of CLOVER could accelerate the adoption of rapid\nconversational applications in the landscape of digital pathology.", "published": "2024-07-25 03:12:57", "link": "http://arxiv.org/abs/2407.17734v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "ERIT Lightweight Multimodal Dataset for Elderly Emotion Recognition and\n  Multimodal Fusion Evaluation", "abstract": "ERIT is a novel multimodal dataset designed to facilitate research in a\nlightweight multimodal fusion. It contains text and image data collected from\nvideos of elderly individuals reacting to various situations, as well as seven\nemotion labels for each data sample. Because of the use of labeled images of\nelderly users reacting emotionally, it is also facilitating research on emotion\nrecognition in an underrepresented age group in machine learning visual emotion\nrecognition. The dataset is validated through comprehensive experiments\nindicating its importance in neural multimodal fusion research.", "published": "2024-07-25 05:02:27", "link": "http://arxiv.org/abs/2407.17772v1", "categories": ["cs.CV", "cs.CL", "cs.CY"], "primary_category": "cs.CV"}
{"title": "KiVA: Kid-inspired Visual Analogies for Testing Large Multimodal Models", "abstract": "This paper investigates visual analogical reasoning in large multimodal\nmodels (LMMs) compared to human adults and children. A \"visual analogy\" is an\nabstract rule inferred from one image and applied to another. While benchmarks\nexist for testing visual reasoning in LMMs, they require advanced skills and\nomit basic visual analogies that even young children can make. Inspired by\ndevelopmental psychology, we propose a new benchmark of 4,300 visual\ntransformations of everyday objects to test LMMs on visual analogical reasoning\nand compare them to children (ages three to five) and to adults. We structure\nthe evaluation into three stages: identifying what changed (e.g., color,\nnumber, etc.), how it changed (e.g., added one object), and applying the rule\nto new scenarios. Our findings show that while GPT-o1, GPT-4V, LLaVA-1.5, and\nMANTIS identify the \"what\" effectively, they struggle with quantifying the\n\"how\" and extrapolating this rule to new objects. In contrast, children and\nadults exhibit much stronger analogical reasoning at all three stages.\nAdditionally, the strongest tested model, GPT-o1, performs better in tasks\ninvolving simple surface-level visual attributes like color and size,\ncorrelating with quicker human adult response times. Conversely, more complex\ntasks such as number, rotation, and reflection, which necessitate extensive\ncognitive processing and understanding of extrinsic spatial properties in the\nphysical world, present more significant challenges. Altogether, these findings\nhighlight the limitations of training models on data that primarily consists of\n2D images and text.", "published": "2024-07-25 05:02:39", "link": "http://arxiv.org/abs/2407.17773v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Unified Lexical Representation for Interpretable Visual-Language\n  Alignment", "abstract": "Visual-Language Alignment (VLA) has gained a lot of attention since CLIP's\ngroundbreaking work. Although CLIP performs well, the typical direct latent\nfeature alignment lacks clarity in its representation and similarity scores. On\nthe other hand, lexical representation, a vector whose element represents the\nsimilarity between the sample and a word from the vocabulary, is a natural\nsparse representation and interpretable, providing exact matches for individual\nwords. However, lexical representations are difficult to learn due to no\nground-truth supervision and false-discovery issues, and thus requires complex\ndesign to train effectively. In this paper, we introduce LexVLA, a more\ninterpretable VLA framework by learning a unified lexical representation for\nboth modalities without complex design. We use DINOv2 as our visual model for\nits local-inclined features and Llama 2, a generative language model, to\nleverage its in-context lexical prediction ability. To avoid the false\ndiscovery, we propose an overuse penalty to refrain the lexical representation\nfrom falsely frequently activating meaningless words. We demonstrate that these\ntwo pre-trained uni-modal models can be well-aligned by fine-tuning on the\nmodest multi-modal dataset and avoid intricate training configurations. On\ncross-modal retrieval benchmarks, LexVLA, trained on the CC-12M multi-modal\ndataset, outperforms baselines fine-tuned on larger datasets (e.g., YFCC15M)\nand those trained from scratch on even bigger datasets (e.g., 1.1B data,\nincluding CC-12M). We conduct extensive experiments to analyze LexVLA. Codes\nare available at https://github.com/Clementine24/LexVLA.", "published": "2024-07-25 07:35:27", "link": "http://arxiv.org/abs/2407.17827v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Shapley Value-based Contrastive Alignment for Multimodal Information\n  Extraction", "abstract": "The rise of social media and the exponential growth of multimodal\ncommunication necessitates advanced techniques for Multimodal Information\nExtraction (MIE). However, existing methodologies primarily rely on direct\nImage-Text interactions, a paradigm that often faces significant challenges due\nto semantic and modality gaps between images and text. In this paper, we\nintroduce a new paradigm of Image-Context-Text interaction, where large\nmultimodal models (LMMs) are utilized to generate descriptive textual context\nto bridge these gaps. In line with this paradigm, we propose a novel Shapley\nValue-based Contrastive Alignment (Shap-CA) method, which aligns both\ncontext-text and context-image pairs. Shap-CA initially applies the Shapley\nvalue concept from cooperative game theory to assess the individual\ncontribution of each element in the set of contexts, texts and images towards\ntotal semantic and modality overlaps. Following this quantitative evaluation, a\ncontrastive learning strategy is employed to enhance the interactive\ncontribution within context-text/image pairs, while minimizing the influence\nacross these pairs. Furthermore, we design an adaptive fusion module for\nselective cross-modal fusion. Extensive experiments across four MIE datasets\ndemonstrate that our method significantly outperforms existing state-of-the-art\nmethods.", "published": "2024-07-25 08:15:43", "link": "http://arxiv.org/abs/2407.17854v1", "categories": ["cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.AI"}
{"title": "On the Effect of Purely Synthetic Training Data for Different Automatic\n  Speech Recognition Architectures", "abstract": "In this work we evaluate the utility of synthetic data for training automatic\nspeech recognition (ASR). We use the ASR training data to train a\ntext-to-speech (TTS) system similar to FastSpeech-2. With this TTS we reproduce\nthe original training data, training ASR systems solely on synthetic data. For\nASR, we use three different architectures, attention-based encoder-decoder,\nhybrid deep neural network hidden Markov model and a Gaussian mixture hidden\nMarkov model, showing the different sensitivity of the models to synthetic data\ngeneration. In order to extend previous work, we present a number of ablation\nstudies on the effectiveness of synthetic vs. real training data for ASR. In\nparticular we focus on how the gap between training on synthetic and real data\nchanges by varying the speaker embedding or by scaling the model size. For the\nlatter we show that the TTS models generalize well, even when training scores\nindicate overfitting.", "published": "2024-07-25 12:44:45", "link": "http://arxiv.org/abs/2407.17997v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "RestoreAgent: Autonomous Image Restoration Agent via Multimodal Large\n  Language Models", "abstract": "Natural images captured by mobile devices often suffer from multiple types of\ndegradation, such as noise, blur, and low light. Traditional image restoration\nmethods require manual selection of specific tasks, algorithms, and execution\nsequences, which is time-consuming and may yield suboptimal results. All-in-one\nmodels, though capable of handling multiple tasks, typically support only a\nlimited range and often produce overly smooth, low-fidelity outcomes due to\ntheir broad data distribution fitting. To address these challenges, we first\ndefine a new pipeline for restoring images with multiple degradations, and then\nintroduce RestoreAgent, an intelligent image restoration system leveraging\nmultimodal large language models. RestoreAgent autonomously assesses the type\nand extent of degradation in input images and performs restoration through (1)\ndetermining the appropriate restoration tasks, (2) optimizing the task\nsequence, (3) selecting the most suitable models, and (4) executing the\nrestoration. Experimental results demonstrate the superior performance of\nRestoreAgent in handling complex degradation, surpassing human experts.\nFurthermore, the system modular design facilitates the fast integration of new\ntasks and models, enhancing its flexibility and scalability for various\napplications.", "published": "2024-07-25 13:29:37", "link": "http://arxiv.org/abs/2407.18035v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Scaling Trends in Language Model Robustness", "abstract": "Language models exhibit scaling laws, whereby increasing model and dataset\nsize predictably decrease negative log likelihood, unlocking a dazzling array\nof capabilities. At the same time, even the most capable systems are currently\nvulnerable to adversarial inputs such as jailbreaks and prompt injections,\ndespite concerted efforts to make them robust. As compute becomes more\naccessible to both attackers and defenders, which side will benefit more from\nscale? We attempt to answer this question with a detailed study of robustness\non language models spanning three orders of magnitude in parameter count. From\nthe defender's perspective, we find that in the absence of other interventions,\nincreasing model size alone does not consistently improve robustness. In\nadversarial training, we find that larger models are more sample-efficient and\nless compute-efficient than smaller models, and often better generalize their\ndefense to new threat models. From the attacker's perspective, we find that\nincreasing attack compute smoothly and reliably increases attack success rate\nagainst both finetuned and adversarially trained models. Finally, we show that\nacross model sizes studied, doubling compute on adversarial training only\nforces an attacker to less than double attack compute to maintain the same\nattack success rate. However, adversarial training becomes more and more\neffective on larger models, suggesting that defenders could eventually have the\nadvantage with increasing model size. These results underscore the value of\nadopting a scaling lens when discussing robustness of frontier models.", "published": "2024-07-25 17:26:41", "link": "http://arxiv.org/abs/2407.18213v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "I.2.7"], "primary_category": "cs.LG"}
{"title": "Recursive Introspection: Teaching Language Model Agents How to\n  Self-Improve", "abstract": "A central piece in enabling intelligent agentic behavior in foundation models\nis to make them capable of introspecting upon their behavior, reasoning, and\ncorrecting their mistakes as more computation or interaction is available. Even\nthe strongest proprietary large language models (LLMs) do not quite exhibit the\nability of continually improving their responses sequentially, even in\nscenarios where they are explicitly told that they are making a mistake. In\nthis paper, we develop RISE: Recursive IntroSpEction, an approach for\nfine-tuning LLMs to introduce this capability, despite prior work hypothesizing\nthat this capability may not be possible to attain. Our approach prescribes an\niterative fine-tuning procedure, which attempts to teach the model how to alter\nits response after having executed previously unsuccessful attempts to solve a\nhard test-time problem, with optionally additional environment feedback. RISE\nposes fine-tuning for a single-turn prompt as solving a multi-turn Markov\ndecision process (MDP), where the initial state is the prompt. Inspired by\nprinciples in online imitation learning and reinforcement learning, we propose\nstrategies for multi-turn data collection and training so as to imbue an LLM\nwith the capability to recursively detect and correct its previous mistakes in\nsubsequent iterations. Our experiments show that RISE enables Llama2, Llama3,\nand Mistral models to improve themselves with more turns on math reasoning\ntasks, outperforming several single-turn strategies given an equal amount of\ninference-time computation. We also find that RISE scales well, often attaining\nlarger benefits with more capable models. Our analysis shows that RISE makes\nmeaningful improvements to responses to arrive at the correct solution for\nchallenging prompts, without disrupting one-turn abilities as a result of\nexpressing more complex distributions.", "published": "2024-07-25 17:35:59", "link": "http://arxiv.org/abs/2407.18219v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LoRA-Pro: Are Low-Rank Adapters Properly Optimized?", "abstract": "Low-rank adaptation, also known as LoRA, has emerged as a prominent method\nfor parameter-efficient fine-tuning of foundation models. Despite its\ncomputational efficiency, LoRA still yields inferior performance compared to\nfull fine-tuning. In this paper, we first uncover a fundamental connection\nbetween the optimization processes of LoRA and full fine-tuning: using LoRA for\noptimization is mathematically equivalent to full fine-tuning using a low-rank\ngradient for parameter updates. And this low-rank gradient can be expressed in\nterms of the gradients of the two low-rank matrices in LoRA. Leveraging this\ninsight, we introduce LoRA-Pro, a method that enhances LoRA's performance by\nstrategically adjusting the gradients of these low-rank matrices. This\nadjustment allows the low-rank gradient to more accurately approximate the full\nfine-tuning gradient, thereby narrowing the performance gap between LoRA and\nfull fine-tuning. Furthermore, we theoretically derive the optimal solutions\nfor adjusting the gradients of the low-rank matrices, applying them during\nfine-tuning in LoRA-Pro. We conduct extensive experiments across natural\nlanguage understanding, dialogue generation, mathematical reasoning, code\ngeneration, and image classification tasks, demonstrating that LoRA-Pro\nsubstantially improves LoRA's performance, effectively narrowing the gap with\nfull fine-tuning. Code is publicly available at\nhttps://github.com/mrflogs/LoRA-Pro.", "published": "2024-07-25 17:57:12", "link": "http://arxiv.org/abs/2407.18242v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "PersonaGym: Evaluating Persona Agents and LLMs", "abstract": "Persona agents, which are LLM agents that act according to an assigned\npersona, have demonstrated impressive contextual response capabilities across\nvarious applications. These persona agents offer significant enhancements\nacross diverse sectors, such as education, healthcare, and entertainment, where\nmodel developers can align agent responses to different user requirements\nthereby broadening the scope of agent applications. However, evaluating persona\nagent performance is incredibly challenging due to the complexity of assessing\npersona adherence in free-form interactions across various environments that\nare relevant to each persona agent. We introduce PersonaGym, the first dynamic\nevaluation framework for assessing persona agents, and PersonaScore, the first\nautomated human-aligned metric grounded in decision theory for comprehensive\nlarge-scale evaluation of persona agents. Our evaluation of 6 open and\nclosed-source LLMs, using a benchmark encompassing 200 personas and 10,000\nquestions, reveals significant opportunities for advancement in persona agent\ncapabilities across state-of-the-art models. For example, Claude 3.5 Sonnet\nonly has a 2.97% relative improvement in PersonaScore than GPT 3.5 despite\nbeing a much more advanced model. Importantly, we find that increased model\nsize and complexity do not necessarily imply enhanced persona agent\ncapabilities thereby highlighting the pressing need for algorithmic and\narchitectural invention towards faithful and performant persona agents.", "published": "2024-07-25 22:24:45", "link": "http://arxiv.org/abs/2407.18416v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Stay Tuned: An Empirical Study of the Impact of Hyperparameters on LLM\n  Tuning in Real-World Applications", "abstract": "Fine-tuning Large Language Models (LLMs) is an effective method to enhance\ntheir performance on downstream tasks. However, choosing the appropriate\nsetting of tuning hyperparameters (HPs) is a labor-intensive and\ncomputationally expensive process. Here, we provide recommended HP\nconfigurations for practical use-cases that represent a better starting point\nfor practitioners, when considering two SOTA LLMs and two commonly used tuning\nmethods. We describe Coverage-based Search (CBS), a process for ranking HP\nconfigurations based on an offline extensive grid search, such that the top\nranked configurations collectively provide a practical robust recommendation\nfor a wide range of datasets and domains. We focus our experiments on\nLlama-3-8B and Mistral-7B, as well as full fine-tuning and LoRa, conducting a\ntotal of > 10,000 tuning experiments. Our results suggest that, in general,\nLlama-3-8B and LoRA should be preferred, when possible. Moreover, we show that\nfor both models and tuning methods, exploring only a few HP configurations, as\nrecommended by our analysis, can provide excellent results in practice, making\nthis work a valuable resource for practitioners.", "published": "2024-07-25 12:07:55", "link": "http://arxiv.org/abs/2407.18990v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Learn while Unlearn: An Iterative Unlearning Framework for Generative\n  Language Models", "abstract": "Recent advances in machine learning, particularly in Natural Language\nProcessing (NLP), have produced powerful models trained on vast datasets.\nHowever, these models risk leaking sensitive information, raising privacy\nconcerns. In response, regulatory measures such as the European Union's General\nData Protection Regulation (GDPR) have driven increasing interest in Machine\nUnlearning techniques, which enable models to selectively forget specific data\nentries. Early unlearning approaches primarily relied on pre-processing\nmethods, while more recent research has shifted towards training-based\nsolutions. Despite their effectiveness, a key limitation persists: most methods\nrequire access to original training data, which is often unavailable.\nAdditionally, directly applying unlearning techniques bears the cost of\nundermining the model's expressive capabilities. To address these challenges,\nwe introduce the Iterative Contrastive Unlearning (ICU) framework, which\nconsists of three core components: A Knowledge Unlearning Induction module\ndesigned to target specific knowledge for removal using an unlearning loss; A\nContrastive Learning Enhancement module to preserve the model's expressive\ncapabilities against the pure unlearning goal; And an Iterative Unlearning\nRefinement module that dynamically adjusts the unlearning process through\nongoing evaluation and updates. Experimental results demonstrate the efficacy\nof our ICU method in unlearning sensitive information while maintaining the\nmodel's overall performance, offering a promising solution for\nprivacy-conscious machine learning applications.", "published": "2024-07-25 07:09:35", "link": "http://arxiv.org/abs/2407.20271v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "An Efficient Inference Framework for Early-exit Large Language Models", "abstract": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up.", "published": "2024-07-25 07:50:17", "link": "http://arxiv.org/abs/2407.20272v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring the Plausibility of Hate and Counter Speech Detectors with\n  Explainable AI", "abstract": "In this paper we investigate the explainability of transformer models and\ntheir plausibility for hate speech and counter speech detection. We compare\nrepresentatives of four different explainability approaches, i.e.,\ngradient-based, perturbation-based, attention-based, and prototype-based\napproaches, and analyze them quantitatively with an ablation study and\nqualitatively in a user study. Results show that perturbation-based\nexplainability performs best, followed by gradient-based and attention-based\nexplainability. Prototypebased experiments did not yield useful results.\nOverall, we observe that explainability strongly supports the users in better\nunderstanding the model predictions.", "published": "2024-07-25 10:17:04", "link": "http://arxiv.org/abs/2407.20274v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Multi-group Uncertainty Quantification for Long-form Text Generation", "abstract": "While large language models are rapidly moving towards consumer-facing\napplications, they are often still prone to factual errors and hallucinations.\nIn order to reduce the potential harms that may come from these errors, it is\nimportant for users to know to what extent they can trust an LLM when it makes\na factual claim. To this end, we study the problem of uncertainty\nquantification of factual correctness in long-form natural language generation.\nGiven some output from a large language model, we study both uncertainty at the\nlevel of individual claims contained within the output (via calibration) and\nuncertainty across the entire output itself (via conformal prediction).\nMoreover, we invoke multicalibration and multivalid conformal prediction to\nensure that such uncertainty guarantees are valid both marginally and across\ndistinct groups of prompts. Using the task of biography generation, we\ndemonstrate empirically that having access to and making use of additional\ngroup attributes for each prompt improves both overall and group-wise\nperformance. As the problems of calibration, conformal prediction, and their\nmulti-group counterparts have not been extensively explored previously in the\ncontext of long-form text generation, we consider these empirical results to\nform a benchmark for this setting.", "published": "2024-07-25 02:59:52", "link": "http://arxiv.org/abs/2407.21057v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transformers on Markov Data: Constant Depth Suffices", "abstract": "Attention-based transformers have been remarkably successful at modeling\ngenerative processes across various domains and modalities. In this paper, we\nstudy the behavior of transformers on data drawn from \\kth Markov processes,\nwhere the conditional distribution of the next symbol in a sequence depends on\nthe previous $k$ symbols observed. We observe a surprising phenomenon\nempirically which contradicts previous findings: when trained for sufficiently\nlong, a transformer with a fixed depth and $1$ head per layer is able to\nachieve low test loss on sequences drawn from \\kth Markov sources, even as $k$\ngrows. Furthermore, this low test loss is achieved by the transformer's ability\nto represent and learn the in-context conditional empirical distribution. On\nthe theoretical side, our main result is that a transformer with a single head\nand three layers can represent the in-context conditional empirical\ndistribution for \\kth Markov sources, concurring with our empirical\nobservations. Along the way, we prove that \\textit{attention-only} transformers\nwith $O(\\log_2(k))$ layers can represent the in-context conditional empirical\ndistribution by composing induction heads to track the previous $k$ symbols in\nthe sequence. These results provide more insight into our current understanding\nof the mechanisms by which transformers learn to capture context, by\nunderstanding their behavior on Markov sources.", "published": "2024-07-25 01:07:09", "link": "http://arxiv.org/abs/2407.17686v1", "categories": ["cs.LG", "cs.CL", "cs.IT", "math.IT", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Innovative Speech-Based Deep Learning Approaches for Parkinson's Disease\n  Classification: A Systematic Review", "abstract": "Parkinson's disease (PD), the second most prevalent neurodegenerative\ndisorder worldwide, frequently presents with early-stage speech impairments.\nRecent advancements in Artificial Intelligence (AI), particularly deep learning\n(DL), have significantly enhanced PD diagnosis through the analysis of speech\ndata. Nevertheless, the progress of research is restricted by the limited\navailability of publicly accessible speech-based PD datasets, primarily due to\nprivacy concerns. The goal of this systematic review is to explore the current\nlandscape of speech-based DL approaches for PD classification, based on 33\nscientific works published between January 2020 and March 2024. We discuss\ntheir available resources, capabilities, and potential limitations, and issues\nrelated to bias, explainability, and privacy. Furthermore, this review provides\nan overview of publicly accessible speech-based datasets and open-source\nmaterial for PD. The DL approaches identified are categorized into end-to-end\n(E2E) learning, transfer learning (TL), and deep acoustic feature extraction\n(DAFE). Among E2E approaches, Convolutional Neural Networks (CNNs) are\nprevalent, though Transformers are increasingly popular. E2E approaches face\nchallenges such as limited data and computational resources, especially with\nTransformers. TL addresses these issues by providing more robust PD diagnosis\nand better generalizability across languages. DAFE aims to improve the\nexplainability and interpretability of results by examining the specific\neffects of deep features on both other DL approaches and more traditional\nmachine learning (ML) methods. However, it often underperforms compared to E2E\nand TL approaches.", "published": "2024-07-25 07:58:19", "link": "http://arxiv.org/abs/2407.17844v4", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Financial Statement Analysis with Large Language Models", "abstract": "We investigate whether large language models (LLMs) can successfully perform\nfinancial statement analysis in a way similar to a professional human analyst.\nWe provide standardized and anonymous financial statements to GPT4 and instruct\nthe model to analyze them to determine the direction of firms' future earnings.\nEven without narrative or industry-specific information, the LLM outperforms\nfinancial analysts in its ability to predict earnings changes directionally.\nThe LLM exhibits a relative advantage over human analysts in situations when\nthe analysts tend to struggle. Furthermore, we find that the prediction\naccuracy of the LLM is on par with a narrowly trained state-of-the-art ML\nmodel. LLM prediction does not stem from its training memory. Instead, we find\nthat the LLM generates useful narrative insights about a company's future\nperformance. Lastly, our trading strategies based on GPT's predictions yield a\nhigher Sharpe ratio and alphas than strategies based on other models. Our\nresults suggest that LLMs may take a central role in analysis and\ndecision-making.", "published": "2024-07-25 08:36:58", "link": "http://arxiv.org/abs/2407.17866v3", "categories": ["q-fin.ST", "cs.AI", "cs.CL", "q-fin.GN", "q-fin.PM"], "primary_category": "q-fin.ST"}
{"title": "I can listen but cannot read: An evaluation of two-tower multimodal\n  systems for instrument recognition", "abstract": "Music two-tower multimodal systems integrate audio and text modalities into a\njoint audio-text space, enabling direct comparison between songs and their\ncorresponding labels. These systems enable new approaches for classification\nand retrieval, leveraging both modalities. Despite the promising results they\nhave shown for zero-shot classification and retrieval tasks, closer inspection\nof the embeddings is needed. This paper evaluates the inherent zero-shot\nproperties of joint audio-text spaces for the case-study of instrument\nrecognition. We present an evaluation and analysis of two-tower systems for\nzero-shot instrument recognition and a detailed analysis of the properties of\nthe pre-joint and joint embeddings spaces. Our findings suggest that audio\nencoders alone demonstrate good quality, while challenges remain within the\ntext encoder or joint space projection. Specifically, two-tower systems exhibit\nsensitivity towards specific words, favoring generic prompts over musically\ninformed ones. Despite the large size of textual encoders, they do not yet\nleverage additional textual context or infer instruments accurately from their\ndescriptions. Lastly, a novel approach for quantifying the semantic\nmeaningfulness of the textual space leveraging an instrument ontology is\nproposed. This method reveals deficiencies in the systems' understanding of\ninstruments and provides evidence of the need for fine-tuning text encoders on\nmusical data.", "published": "2024-07-25 14:15:05", "link": "http://arxiv.org/abs/2407.18058v1", "categories": ["cs.SD", "cs.CL", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploring Bengali Religious Dialect Biases in Large Language Models with\n  Evaluation Perspectives", "abstract": "While Large Language Models (LLM) have created a massive technological impact\nin the past decade, allowing for human-enabled applications, they can produce\noutput that contains stereotypes and biases, especially when using low-resource\nlanguages. This can be of great ethical concern when dealing with sensitive\ntopics such as religion. As a means toward making LLMS more fair, we explore\nbias from a religious perspective in Bengali, focusing specifically on two main\nreligious dialects: Hindu and Muslim-majority dialects. Here, we perform\ndifferent experiments and audit showing the comparative analysis of different\nsentences using three commonly used LLMs: ChatGPT, Gemini, and Microsoft\nCopilot, pertaining to the Hindu and Muslim dialects of specific words and\nshowcasing which ones catch the social biases and which do not. Furthermore, we\nanalyze our findings and relate them to potential reasons and evaluation\nperspectives, considering their global impact with over 300 million speakers\nworldwide. With this work, we hope to establish the rigor for creating more\nfairness in LLMs, as these are widely used as creative writing agents.", "published": "2024-07-25 20:19:29", "link": "http://arxiv.org/abs/2407.18376v1", "categories": ["cs.HC", "cs.CL", "cs.CY", "cs.MM", "cs.SI"], "primary_category": "cs.HC"}
{"title": "Multi-Stage Face-Voice Association Learning with Keynote Speaker\n  Diarization", "abstract": "The human brain has the capability to associate the unknown person's voice\nand face by leveraging their general relationship, referred to as ``cross-modal\nspeaker verification''. This task poses significant challenges due to the\ncomplex relationship between the modalities. In this paper, we propose a\n``Multi-stage Face-voice Association Learning with Keynote Speaker\nDiarization''~(MFV-KSD) framework. MFV-KSD contains a keynote speaker\ndiarization front-end to effectively address the noisy speech inputs issue. To\nbalance and enhance the intra-modal feature learning and inter-modal\ncorrelation understanding, MFV-KSD utilizes a novel three-stage training\nstrategy. Our experimental results demonstrated robust performance, achieving\nthe first rank in the 2024 Face-voice Association in Multilingual Environments\n(FAME) challenge with an overall Equal Error Rate (EER) of 19.9%. Details can\nbe found in https://github.com/TaoRuijie/MFV-KSD.", "published": "2024-07-25 09:46:04", "link": "http://arxiv.org/abs/2407.17902v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Reshape Dimensions Network for Speaker Recognition", "abstract": "In this paper, we present Reshape Dimensions Network (ReDimNet), a novel\nneural network architecture for extracting utterance-level speaker\nrepresentations. Our approach leverages dimensionality reshaping of 2D feature\nmaps to 1D signal representation and vice versa, enabling the joint usage of 1D\nand 2D blocks. We propose an original network topology that preserves the\nvolume of channel-timestep-frequency outputs of 1D and 2D blocks, facilitating\nefficient residual feature maps aggregation. Moreover, ReDimNet is efficiently\nscalable, and we introduce a range of model sizes, varying from 1 to 15 M\nparameters and from 0.5 to 20 GMACs. Our experimental results demonstrate that\nReDimNet achieves state-of-the-art performance in speaker recognition while\nreducing computational complexity and the number of model parameters.", "published": "2024-07-25 17:39:34", "link": "http://arxiv.org/abs/2407.18223v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Audio Entailment: Assessing Deductive Reasoning for Audio Understanding", "abstract": "Recent literature uses language to build foundation models for audio. These\nAudio-Language Models (ALMs) are trained on a vast number of audio-text pairs\nand show remarkable performance in tasks including Text-to-Audio Retrieval,\nCaptioning, and Question Answering. However, their ability to engage in more\ncomplex open-ended tasks, like Interactive Question-Answering, requires\nproficiency in logical reasoning -- a skill not yet benchmarked. We introduce\nthe novel task of Audio Entailment to evaluate an ALM's deductive reasoning\nability. This task assesses whether a text description (hypothesis) of audio\ncontent can be deduced from an audio recording (premise), with potential\nconclusions being entailment, neutral, or contradiction, depending on the\nsufficiency of the evidence. We create two datasets for this task with audio\nrecordings sourced from two audio captioning datasets -- AudioCaps and Clotho\n-- and hypotheses generated using Large Language Models (LLMs). We benchmark\nstate-of-the-art ALMs and find deficiencies in logical reasoning with both\nzero-shot and linear probe evaluations. Finally, we propose\n\"caption-before-reason\", an intermediate step of captioning that improves the\nzero-shot and linear-probe performance of ALMs by an absolute 6% and 3%,\nrespectively.", "published": "2024-07-25 14:17:56", "link": "http://arxiv.org/abs/2407.18062v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Detection of manatee vocalisations using the Audio Spectrogram\n  Transformer", "abstract": "The Antillean manatee (\\emph{Trichechus manatus}) is an endangered\nherbivorous aquatic mammal whose role as an ecological balancer and umbrella\nspecies underscores the importance of its conservation. An innovative approach\nto monitor manatee populations is passive acoustic monitoring (PAM), where\nvocalisations are extracted from submarine audio. We propose a novel end-to-end\napproach to detect manatee vocalisations building on the Audio Spectrogram\nTransformer (AST). In a transfer learning spirit, we fine-tune AST to detect\nmanatee calls by redesigning its filterbanks and adapting a real-world dataset\ncontaining partial positive labels. Our experimental evaluation reveals the two\nkey features of the proposed model: i) it performs on par with the state of the\nart without requiring hand-tuned denoising or detection stages, and ii) it can\nsuccessfully identify missed vocalisations in the training dataset, thus\nreducing the workload of expert bioacoustic labellers. This work is a\npreliminary relevant step to develop novel, user-friendly tools for the\nconservation of the different species of manatees.", "published": "2024-07-25 14:46:50", "link": "http://arxiv.org/abs/2407.18083v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Implementation and Applications of WakeWords Integrated with Speaker\n  Recognition: A Case Study", "abstract": "This paper explores the application of artificial intelligence techniques in\naudio and voice processing, focusing on the integration of wake words and\nspeaker recognition for secure access in embedded systems. With the growing\nprevalence of voice-activated devices such as Amazon Alexa, ensuring secure and\nuser-specific interactions has become paramount. Our study aims to enhance the\nsecurity framework of these systems by leveraging wake words for initial\nactivation and speaker recognition to validate user permissions. By\nincorporating these AI-driven methodologies, we propose a robust solution that\nrestricts system usage to authorized individuals, thereby mitigating\nunauthorized access risks. This research delves into the algorithms and\ntechnologies underpinning wake word detection and speaker recognition,\nevaluates their effectiveness in real-world applications, and discusses the\npotential for their implementation in various embedded systems, emphasizing\nsecurity and user convenience. The findings underscore the feasibility and\nadvantages of employing these AI techniques to create secure, user-friendly\nvoice-activated systems.", "published": "2024-07-25 02:02:26", "link": "http://arxiv.org/abs/2407.18985v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Model-driven Heart Rate Estimation and Heart Murmur Detection based on\n  Phonocardiogram", "abstract": "Acoustic signals are crucial for health monitoring, particularly heart sounds\nwhich provide essential data like heart rate and detect cardiac anomalies such\nas murmurs. This study utilizes a publicly available phonocardiogram (PCG)\ndataset to estimate heart rate using model-driven methods and extends the\nbest-performing model to a multi-task learning (MTL) framework for simultaneous\nheart rate estimation and murmur detection. Heart rate estimates are derived\nusing a sliding window technique on heart sound snippets, analyzed with a\ncombination of acoustic features (Mel spectrogram, cepstral coefficients, power\nspectral density, root mean square energy). Our findings indicate that a 2D\nconvolutional neural network (\\textbf{\\texttt{2dCNN}}) is most effective for\nheart rate estimation, achieving a mean absolute error (MAE) of 1.312 bpm. We\nsystematically investigate the impact of different feature combinations and\nfind that utilizing all four features yields the best results. The MTL model\n(\\textbf{\\texttt{2dCNN-MTL}}) achieves accuracy over 95% in murmur detection,\nsurpassing existing models, while maintaining an MAE of 1.636 bpm in heart rate\nestimation, satisfying the requirements stated by Association for the\nAdvancement of Medical Instrumentation (AAMI).", "published": "2024-07-25 22:56:21", "link": "http://arxiv.org/abs/2407.18424v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Simulation of Neural Responses to Classical Music Using Organoid\n  Intelligence Methods", "abstract": "Music is a complex auditory stimulus capable of eliciting significant changes\nin brain activity, influencing cognitive processes such as memory, attention,\nand emotional regulation. However, the underlying mechanisms of music-induced\ncognitive processes remain largely unknown. Organoid intelligence and deep\nlearning models show promise for simulating and analyzing these neural\nresponses to classical music, an area significantly unexplored in computational\nneuroscience. Hence, we present the PyOrganoid library, an innovative tool that\nfacilitates the simulation of organoid learning models, integrating\nsophisticated machine learning techniques with biologically inspired organoid\nsimulations. Our study features the development of the Pianoid model, a \"deep\norganoid learning\" model that utilizes a Bidirectional LSTM network to predict\nEEG responses based on audio features from classical music recordings. This\nmodel demonstrates the feasibility of using computational methods to replicate\ncomplex neural processes, providing valuable insights into music perception and\ncognition. Likewise, our findings emphasize the utility of synthetic models in\nneuroscience research and highlight the PyOrganoid library's potential as a\nversatile tool for advancing studies in neuroscience and artificial\nintelligence.", "published": "2024-07-25 22:11:30", "link": "http://arxiv.org/abs/2407.18413v1", "categories": ["cs.NE", "cs.AI", "cs.LG", "cs.SD", "eess.AS", "I.2; I.6; J.3; J.4; J.5"], "primary_category": "cs.NE"}
