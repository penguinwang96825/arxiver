{"title": "Argumentation in Waltz's \"Emerging Structure of International Politics''", "abstract": "We present an annotation scheme for argumentative and domain-specific aspects\nof scholarly articles on the theory of International Relations. At\nargumentation level we identify Claims and Support/Attack relations. At domain\nlevel we model discourse content in terms of Theory and Data-related\nstatements. We annotate Waltz's 1993 text on structural realism and show that\nour scheme can be reliably applied by domain experts enables insights on two\nresearch questions on justifications of claims.", "published": "2023-12-31 01:51:43", "link": "http://arxiv.org/abs/2401.00366v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FusionMind -- Improving question and answering with external context\n  fusion", "abstract": "Answering questions using pre-trained language models (LMs) and knowledge\ngraphs (KGs) presents challenges in identifying relevant knowledge and\nperforming joint reasoning.We compared LMs (fine-tuned for the task) with the\npreviously published QAGNN method for the Question-answering (QA) objective and\nfurther measured the impact of additional factual context on the QAGNN\nperformance. The QAGNN method employs LMs to encode QA context and estimate KG\nnode importance, and effectively update the question choice entity\nrepresentations using Graph Neural Networks (GNNs). We further experimented\nwith enhancing the QA context encoding by incorporating relevant knowledge\nfacts for the question stem. The models are trained on the OpenbookQA dataset,\nwhich contains ~6000 4-way multiple choice questions and is widely used as a\nbenchmark for QA tasks. Through our experimentation, we found that\nincorporating knowledge facts context led to a significant improvement in\nperformance. In contrast, the addition of knowledge graphs to language models\nresulted in only a modest increase. This suggests that the integration of\ncontextual knowledge facts may be more impactful for enhancing question\nanswering performance compared to solely adding knowledge graphs.", "published": "2023-12-31 03:51:31", "link": "http://arxiv.org/abs/2401.00388v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RAGTruth: A Hallucination Corpus for Developing Trustworthy\n  Retrieval-Augmented Language Models", "abstract": "Retrieval-augmented generation (RAG) has become a main technique for\nalleviating hallucinations in large language models (LLMs). Despite the\nintegration of RAG, LLMs may still present unsupported or contradictory claims\nto the retrieved contents. In order to develop effective hallucination\nprevention strategies under RAG, it is important to create benchmark datasets\nthat can measure the extent of hallucination. This paper presents RAGTruth, a\ncorpus tailored for analyzing word-level hallucinations in various domains and\ntasks within the standard RAG frameworks for LLM applications. RAGTruth\ncomprises nearly 18,000 naturally generated responses from diverse LLMs using\nRAG. These responses have undergone meticulous manual annotations at both the\nindividual cases and word levels, incorporating evaluations of hallucination\nintensity. We not only benchmark hallucination frequencies across different\nLLMs, but also critically assess the effectiveness of several existing\nhallucination detection methodologies. Furthermore, we show that using a\nhigh-quality dataset such as RAGTruth, it is possible to finetune a relatively\nsmall LLM and achieve a competitive level of performance in hallucination\ndetection when compared to the existing prompt-based approaches using\nstate-of-the-art large language models such as GPT-4.", "published": "2023-12-31 04:43:45", "link": "http://arxiv.org/abs/2401.00396v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SDIF-DA: A Shallow-to-Deep Interaction Framework with Data Augmentation\n  for Multi-modal Intent Detection", "abstract": "Multi-modal intent detection aims to utilize various modalities to understand\nthe user's intentions, which is essential for the deployment of dialogue\nsystems in real-world scenarios. The two core challenges for multi-modal intent\ndetection are (1) how to effectively align and fuse different features of\nmodalities and (2) the limited labeled multi-modal intent training data. In\nthis work, we introduce a shallow-to-deep interaction framework with data\naugmentation (SDIF-DA) to address the above challenges. Firstly, SDIF-DA\nleverages a shallow-to-deep interaction module to progressively and effectively\nalign and fuse features across text, video, and audio modalities. Secondly, we\npropose a ChatGPT-based data augmentation approach to automatically augment\nsufficient training data. Experimental results demonstrate that SDIF-DA can\neffectively align and fuse multi-modal features by achieving state-of-the-art\nperformance. In addition, extensive analyses show that the introduced data\naugmentation approach can successfully distill knowledge from the large\nlanguage model.", "published": "2023-12-31 08:33:37", "link": "http://arxiv.org/abs/2401.00424v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GeoGalactica: A Scientific Large Language Model in Geoscience", "abstract": "Large language models (LLMs) have achieved huge success for their general\nknowledge and ability to solve a wide spectrum of tasks in natural language\nprocessing (NLP). Due to their impressive abilities, LLMs have shed light on\npotential inter-discipline applications to foster scientific discoveries of a\nspecific domain by using artificial intelligence (AI for science, AI4S). In the\nmeantime, utilizing NLP techniques in geoscience research and practice is wide\nand convoluted, contributing from knowledge extraction and document\nclassification to question answering and knowledge discovery. In this work, we\ntake the initial step to leverage LLM for science, through a rather\nstraightforward approach. We try to specialize an LLM into geoscience, by\nfurther pre-training the model with a vast amount of texts in geoscience, as\nwell as supervised fine-tuning (SFT) the resulting model with our custom\ncollected instruction tuning dataset. These efforts result in a model\nGeoGalactica consisting of 30 billion parameters. To our best knowledge, it is\nthe largest language model for the geoscience domain. More specifically,\nGeoGalactica is from further pre-training of Galactica. We train GeoGalactica\nover a geoscience-related text corpus containing 65 billion tokens, preserving\nas the largest geoscience-specific text corpus. Then we fine-tune the model\nwith 1 million pairs of instruction-tuning data consisting of questions that\ndemand professional geoscience knowledge to answer. In this technical report,\nwe will illustrate in detail all aspects of GeoGalactica, including data\ncollection, data cleaning, base model selection, pre-training, SFT, and\nevaluation. We open-source our data curation tools and the checkpoints of\nGeoGalactica during the first 3/4 of pre-training.", "published": "2023-12-31 09:22:54", "link": "http://arxiv.org/abs/2401.00434v2", "categories": ["cs.CL", "I.2.7; F.4.1"], "primary_category": "cs.CL"}
{"title": "BatchEval: Towards Human-like Text Evaluation", "abstract": "Significant progress has been made in automatic text evaluation with the\nintroduction of large language models (LLMs) as evaluators. However, current\nsample-wise evaluation paradigm suffers from the following issues: (1)\nSensitive to prompt design; (2) Poor resistance to noise; (3) Inferior ensemble\nperformance with static reference. Inspired by the fact that humans treat both\ncriterion definition and inter sample comparison as references for evaluation,\nwe propose BatchEval, a paradigm that conducts batch-wise evaluation\niteratively to alleviate the above problems. We explore variants under this\nparadigm and confirm the optimal settings are two stage procedure with\nheterogeneous batch composition strategy and decimal scoring format.\nComprehensive experiments across 3 LLMs on 4 text evaluation tasks demonstrate\nthat BatchEval outperforms state-of-the-art methods by 10.5% on Pearson\ncorrelations with only 64% API cost on average. Further analyses have been\nconducted to verify the robustness, generalization, and working mechanism of\nBatchEval.", "published": "2023-12-31 09:34:51", "link": "http://arxiv.org/abs/2401.00437v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HSC-GPT: A Large Language Model for Human Settlements Construction", "abstract": "The field of human settlement construction encompasses a range of spatial\ndesigns and management tasks, including urban planning and landscape\narchitecture design. These tasks involve a plethora of instructions and\ndescriptions presented in natural language, which are essential for\nunderstanding design requirements and producing effective design solutions.\nRecent research has sought to integrate natural language processing (NLP) and\ngenerative artificial intelligence (AI) into human settlement construction\ntasks. Due to the efficient processing and analysis capabilities of AI with\ndata, significant successes have been achieved in design within this domain.\nHowever, this task still faces several fundamental challenges. The semantic\ninformation involved includes complex spatial details, diverse data source\nformats, high sensitivity to regional culture, and demanding requirements for\ninnovation and rigor in work scenarios. These factors lead to limitations when\napplying general generative AI in this field, further exacerbated by a lack of\nhigh-quality data for model training. To address these challenges, this paper\nfirst proposes HSC-GPT, a large-scale language model framework specifically\ndesigned for tasks in human settlement construction, considering the unique\ncharacteristics of this domain.", "published": "2023-12-31 13:56:15", "link": "http://arxiv.org/abs/2401.00504v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Networks Against (and For) Self-Training: Classification with\n  Small Labeled and Large Unlabeled Sets", "abstract": "We propose a semi-supervised text classifier based on self-training using one\npositive and one negative property of neural networks. One of the weaknesses of\nself-training is the semantic drift problem, where noisy pseudo-labels\naccumulate over iterations and consequently the error rate soars. In order to\ntackle this challenge, we reshape the role of pseudo-labels and create a\nhierarchical order of information. In addition, a crucial step in self-training\nis to use the classifier confidence prediction to select the best candidate\npseudo-labels. This step cannot be efficiently done by neural networks, because\nit is known that their output is poorly calibrated. To overcome this challenge,\nwe propose a hybrid metric to replace the plain confidence measurement. Our\nmetric takes into account the prediction uncertainty via a subsampling\ntechnique. We evaluate our model in a set of five standard benchmarks, and show\nthat it significantly outperforms a set of ten diverse baseline models.\nFurthermore, we show that the improvement achieved by our model is additive to\nlanguage model pretraining, which is a widely used technique for using\nunlabeled documents. Our code is available at\nhttps://github.com/p-karisani/RST.", "published": "2023-12-31 19:25:34", "link": "http://arxiv.org/abs/2401.00575v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "State of What Art? A Call for Multi-Prompt LLM Evaluation", "abstract": "Recent advances in large language models (LLMs) have led to the development\nof various evaluation benchmarks. These benchmarks typically rely on a single\ninstruction template for evaluating all LLMs on a specific task. In this paper,\nwe comprehensively analyze the brittleness of results obtained via\nsingle-prompt evaluations across 6.5M instances, involving 20 different LLMs\nand 39 tasks from 3 benchmarks. To improve robustness of the analysis, we\npropose to evaluate LLMs with a set of diverse prompts instead. We discuss\ntailored evaluation metrics for specific use cases (e.g., LLM developers vs.\ndevelopers interested in a specific downstream task), ensuring a more reliable\nand meaningful assessment of LLM capabilities. We then implement these criteria\nand conduct evaluations of multiple models, providing insights into the true\nstrengths and limitations of current LLMs.", "published": "2023-12-31 22:21:36", "link": "http://arxiv.org/abs/2401.00595v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DocLLM: A layout-aware generative language model for multimodal document\n  understanding", "abstract": "Enterprise documents such as forms, invoices, receipts, reports, contracts,\nand other similar records, often carry rich semantics at the intersection of\ntextual and spatial modalities. The visual cues offered by their complex\nlayouts play a crucial role in comprehending these documents effectively. In\nthis paper, we present DocLLM, a lightweight extension to traditional large\nlanguage models (LLMs) for reasoning over visual documents, taking into account\nboth textual semantics and spatial layout. Our model differs from existing\nmultimodal LLMs by avoiding expensive image encoders and focuses exclusively on\nbounding box information to incorporate the spatial layout structure.\nSpecifically, the cross-alignment between text and spatial modalities is\ncaptured by decomposing the attention mechanism in classical transformers to a\nset of disentangled matrices. Furthermore, we devise a pre-training objective\nthat learns to infill text segments. This approach allows us to address\nirregular layouts and heterogeneous content frequently encountered in visual\ndocuments. The pre-trained model is fine-tuned using a large-scale instruction\ndataset, covering four core document intelligence tasks. We demonstrate that\nour solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks,\nand generalizes well to 4 out of 5 previously unseen datasets.", "published": "2023-12-31 22:37:52", "link": "http://arxiv.org/abs/2401.00908v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Text Embeddings with Large Language Models", "abstract": "In this paper, we introduce a novel and simple method for obtaining\nhigh-quality text embeddings using only synthetic data and less than 1k\ntraining steps. Unlike existing methods that often depend on multi-stage\nintermediate pre-training with billions of weakly-supervised text pairs,\nfollowed by fine-tuning with a few labeled datasets, our method does not\nrequire building complex training pipelines or relying on manually collected\ndatasets that are often constrained by task diversity and language coverage. We\nleverage proprietary LLMs to generate diverse synthetic data for hundreds of\nthousands of text embedding tasks across 93 languages. We then fine-tune\nopen-source decoder-only LLMs on the synthetic data using standard contrastive\nloss. Experiments demonstrate that our method achieves strong performance on\nhighly competitive text embedding benchmarks without using any labeled data.\nFurthermore, when fine-tuned with a mixture of synthetic and labeled data, our\nmodel sets new state-of-the-art results on the BEIR and MTEB benchmarks.", "published": "2023-12-31 02:13:18", "link": "http://arxiv.org/abs/2401.00368v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Predicting Evoked Emotions in Conversations", "abstract": "Understanding and predicting the emotional trajectory in multi-party\nmulti-turn conversations is of great significance. Such information can be\nused, for example, to generate empathetic response in human-machine interaction\nor to inform models of pre-emptive toxicity detection. In this work, we\nintroduce the novel problem of Predicting Emotions in Conversations (PEC) for\nthe next turn (n+1), given combinations of textual and/or emotion input up to\nturn n. We systematically approach the problem by modeling three dimensions\ninherently connected to evoked emotions in dialogues, including (i) sequence\nmodeling, (ii) self-dependency modeling, and (iii) recency modeling. These\nmodeling dimensions are then incorporated into two deep neural network\narchitectures, a sequence model and a graph convolutional network model. The\nformer is designed to capture the sequence of utterances in a dialogue, while\nthe latter captures the sequence of utterances and the network formation of\nmulti-party dialogues. We perform a comprehensive empirical evaluation of the\nvarious proposed models for addressing the PEC problem. The results indicate\n(i) the importance of the self-dependency and recency model dimensions for the\nprediction task, (ii) the quality of simpler sequence models in short\ndialogues, (iii) the importance of the graph neural models in improving the\npredictions in long dialogues.", "published": "2023-12-31 03:30:42", "link": "http://arxiv.org/abs/2401.00383v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "keqing: knowledge-based question answering is a nature chain-of-thought\n  mentor of LLM", "abstract": "Large language models (LLMs) have exhibited remarkable performance on various\nnatural language processing (NLP) tasks, especially for question answering.\nHowever, in the face of problems beyond the scope of knowledge, these LLMs tend\nto talk nonsense with a straight face, where the potential solution could be\nincorporating an Information Retrieval (IR) module and generating response\nbased on these retrieved knowledge. In this paper, we present a novel framework\nto assist LLMs, such as ChatGPT, to retrieve question-related structured\ninformation on the knowledge graph, and demonstrate that Knowledge-based\nquestion answering (Keqing) could be a nature Chain-of-Thought (CoT) mentor to\nguide the LLM to sequentially find the answer entities of a complex question\nthrough interpretable logical chains. Specifically, the workflow of Keqing will\nexecute decomposing a complex question according to predefined templates,\nretrieving candidate entities on knowledge graph, reasoning answers of\nsub-questions, and finally generating response with reasoning paths, which\ngreatly improves the reliability of LLM's response. The experimental results on\nKBQA datasets show that Keqing can achieve competitive performance and\nillustrate the logic of answering each question.", "published": "2023-12-31 08:39:04", "link": "http://arxiv.org/abs/2401.00426v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model\n  Scaling Laws", "abstract": "Large language model (LLM) scaling laws are empirical formulas that estimate\nchanges in model quality as a result of increasing parameter count and training\ndata. However, these formulas, including the popular Deepmind Chinchilla\nscaling laws, neglect to include the cost of inference. We modify the\nChinchilla scaling laws to calculate the optimal LLM parameter count and\npre-training data size to train and deploy a model of a given quality and\ninference demand. We conduct our analysis both in terms of a compute budget and\nreal-world costs and find that LLM researchers expecting reasonably large\ninference demand (~1B requests) should train models smaller and longer than\nChinchilla-optimal. Furthermore, we train 47 models of varying sizes and\nparameter counts to validate our formula and find that model quality continues\nto improve as we scale tokens per parameter to extreme ranges (up to 10,000).\nFinally, we ablate the procedure used to fit the Chinchilla scaling law\ncoefficients and find that developing scaling laws only from data collected at\ntypical token/parameter ratios overestimates the impact of additional tokens at\nthese extreme ranges.", "published": "2023-12-31 10:53:58", "link": "http://arxiv.org/abs/2401.00448v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Multi-Task, Multi-Modal Approach for Predicting Categorical and\n  Dimensional Emotions", "abstract": "Speech emotion recognition (SER) has received a great deal of attention in\nrecent years in the context of spontaneous conversations. While there have been\nnotable results on datasets like the well known corpus of naturalistic dyadic\nconversations, IEMOCAP, for both the case of categorical and dimensional\nemotions, there are few papers which try to predict both paradigms at the same\ntime. Therefore, in this work, we aim to highlight the performance contribution\nof multi-task learning by proposing a multi-task, multi-modal system that\npredicts categorical and dimensional emotions. The results emphasise the\nimportance of cross-regularisation between the two types of emotions. Our\napproach consists of a multi-task, multi-modal architecture that uses parallel\nfeature refinement through self-attention for the feature of each modality. In\norder to fuse the features, our model introduces a set of learnable bridge\ntokens that merge the acoustic and linguistic features with the help of\ncross-attention. Our experiments for categorical emotions on 10-fold validation\nyield results comparable to the current state-of-the-art. In our configuration,\nour multi-task approach provides better results compared to learning each\nparadigm separately. On top of that, our best performing model achieves a high\nresult for valence compared to the previous multi-task experiments.", "published": "2023-12-31 16:48:03", "link": "http://arxiv.org/abs/2401.00536v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "An Analysis of Embedding Layers and Similarity Scores using Siamese\n  Neural Networks", "abstract": "Large Lanugage Models (LLMs) are gaining increasing popularity in a variety\nof use cases, from language understanding and writing to assistance in\napplication development. One of the most important aspects for optimal\nfuncionality of LLMs is embedding layers. Word embeddings are distributed\nrepresentations of words in a continuous vector space. In the context of LLMs,\nwords or tokens from the input text are transformed into high-dimensional\nvectors using unique algorithms specific to the model. Our research examines\nthe embedding algorithms from leading companies in the industry, such as\nOpenAI, Google's PaLM, and BERT. Using medical data, we have analyzed\nsimilarity scores of each embedding layer, observing differences in performance\namong each algorithm. To enhance each model and provide an additional encoding\nlayer, we also implemented Siamese Neural Networks. After observing changes in\nperformance with the addition of the model, we measured the carbon footage per\nepoch of training. The carbon footprint associated with large language models\n(LLMs) is a significant concern, and should be taken into consideration when\nselecting algorithms for a variety of use cases. Overall, our research compared\nthe accuracy different, leading embedding algorithms and their carbon footage,\nallowing for a holistic review of each embedding algorithm.", "published": "2023-12-31 20:21:58", "link": "http://arxiv.org/abs/2401.00582v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Survey of Personality, Persona, and Profile in Conversational Agents\n  and Chatbots", "abstract": "We present a review of personality in neural conversational agents (CAs),\nalso called chatbots. First, we define Personality, Persona, and Profile. We\nexplain all personality schemes which have been used in CAs, and list models\nunder the scheme(s) which they use. Second we describe 21 datasets which have\nbeen developed in recent CA personality research. Third, we define the methods\nused to embody personality in a CA, and review recent models using them.\nFourth, we survey some relevant reviews on CAs, personality, and related\ntopics. Finally, we draw conclusions and identify some research challenges for\nthis important emerging field.", "published": "2023-12-31 23:41:41", "link": "http://arxiv.org/abs/2401.00609v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring the Effectiveness of Instruction Tuning in Biomedical Language\n  Processing", "abstract": "Large Language Models (LLMs), particularly those similar to ChatGPT, have\nsignificantly influenced the field of Natural Language Processing (NLP). While\nthese models excel in general language tasks, their performance in\ndomain-specific downstream tasks such as biomedical and clinical Named Entity\nRecognition (NER), Relation Extraction (RE), and Medical Natural Language\nInference (NLI) is still evolving. In this context, our study investigates the\npotential of instruction tuning for biomedical language processing, applying\nthis technique to two general LLMs of substantial scale. We present a\ncomprehensive, instruction-based model trained on a dataset that consists of\napproximately $200,000$ instruction-focused samples. This dataset represents a\ncarefully curated compilation of existing data, meticulously adapted and\nreformatted to align with the specific requirements of our instruction-based\ntasks. This initiative represents an important step in utilising such models to\nachieve results on par with specialised encoder-only models like BioBERT and\nBioClinicalBERT for various classical biomedical NLP tasks. Our work includes\nan analysis of the dataset's composition and its impact on model performance,\nproviding insights into the intricacies of instruction tuning. By sharing our\ncodes, models, and the distinctively assembled instruction-based dataset, we\nseek to encourage ongoing research and development in this area.", "published": "2023-12-31 20:02:10", "link": "http://arxiv.org/abs/2401.00579v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "LaFFi: Leveraging Hybrid Natural Language Feedback for Fine-tuning\n  Language Models", "abstract": "Fine-tuning Large Language Models (LLMs) adapts a trained model to specific\ndownstream tasks, significantly improving task-specific performance. Supervised\nFine-Tuning (SFT) is a common approach, where an LLM is trained to produce\ndesired answers. However, LLMs trained with SFT sometimes make simple mistakes\nand result in hallucinations on reasoning tasks such as question-answering.\nWithout external feedback, it is difficult for SFT to learn a good mapping\nbetween the question and the desired answer, especially with a small dataset.\nThis paper introduces an alternative to SFT called Natural Language Feedback\nfor Finetuning LLMs (LaFFi). LaFFi has LLMs directly predict the feedback they\nwill receive from an annotator. We find that requiring such reflection can\nsignificantly improve the accuracy in in-domain question-answering tasks,\nproviding a promising direction for the application of natural language\nfeedback in the realm of SFT LLMs. Additional ablation studies show that the\nportion of human-annotated data in the annotated datasets affects the\nfine-tuning performance.", "published": "2023-12-31 21:18:16", "link": "http://arxiv.org/abs/2401.00907v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Learning from a Generative AI Predecessor -- The Many Motivations for\n  Interacting with Conversational Agents", "abstract": "For generative AI to succeed, how engaging a conversationalist must it be?\nFor almost sixty years, some conversational agents have responded to any\nquestion or comment to keep a conversation going. In recent years, several\nutilized machine learning or sophisticated language processing, such as Tay,\nXiaoice, Zo, Hugging Face, Kuki, and Replika. Unlike generative AI, they\nfocused on engagement, not expertise. Millions of people were motivated to\nengage with them. What were the attractions? Will generative AI do better if it\nis equally engaging, or should it be less engaging? Prior to the emergence of\ngenerative AI, we conducted a large-scale quantitative and qualitative analysis\nto learn what motivated millions of people to engage with one such 'virtual\ncompanion,' Microsoft's Zo. We examined the complete chat logs of 2000\nanonymized people. We identified over a dozen motivations that people had for\ninteracting with this software. Designers learned different ways to increase\nengagement. Generative conversational AI does not yet have a clear revenue\nmodel to address its high cost. It might benefit from being more engaging, even\nas it supports productivity and creativity. Our study and analysis point to\nopportunities and challenges.", "published": "2023-12-31 03:29:16", "link": "http://arxiv.org/abs/2401.02978v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "I.2.0"], "primary_category": "cs.CL"}
{"title": "Are we describing the same sound? An analysis of word embedding spaces\n  of expressive piano performance", "abstract": "Semantic embeddings play a crucial role in natural language-based information\nretrieval. Embedding models represent words and contexts as vectors whose\nspatial configuration is derived from the distribution of words in large text\ncorpora. While such representations are generally very powerful, they might\nfail to account for fine-grained domain-specific nuances. In this article, we\ninvestigate this uncertainty for the domain of characterizations of expressive\npiano performance. Using a music research dataset of free text performance\ncharacterizations and a follow-up study sorting the annotations into clusters,\nwe derive a ground truth for a domain-specific semantic similarity structure.\nWe test five embedding models and their similarity structure for correspondence\nwith the ground truth. We further assess the effects of contextualizing\nprompts, hubness reduction, cross-modal similarity, and k-means clustering. The\nquality of embedding models shows great variability with respect to this task;\nmore general models perform better than domain-adapted ones and the best model\nconfigurations reach human-level agreement.", "published": "2023-12-31 12:20:03", "link": "http://arxiv.org/abs/2401.02979v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Generation Z's Ability to Discriminate Between AI-generated and\n  Human-Authored Text on Discord", "abstract": "The growing popularity of generative artificial intelligence (AI) chatbots\nsuch as ChatGPT is having transformative effects on social media. As the\nprevalence of AI-generated content grows, concerns have been raised regarding\nprivacy and misinformation online. Among social media platforms, Discord\nenables AI integrations -- making their primarily \"Generation Z\" userbase\nparticularly exposed to AI-generated content. We surveyed Generation Z aged\nindividuals (n = 335) to evaluate their proficiency in discriminating between\nAI-generated and human-authored text on Discord. The investigation employed\none-shot prompting of ChatGPT, disguised as a text message received on the\nDiscord.com platform. We explore the influence of demographic factors on\nability, as well as participants' familiarity with Discord and artificial\nintelligence technologies. We find that Generation Z individuals are unable to\ndiscern between AI and human-authored text (p = 0.011), and that those with\nlower self-reported familiarity with Discord demonstrated an improved ability\nin identifying human-authored compared to those with self-reported experience\nwith AI (p << 0.0001). Our results suggest that there is a nuanced relationship\nbetween AI technology and popular modes of communication for Generation Z,\ncontributing valuable insights into human-computer interactions, digital\ncommunication, and artificial intelligence literacy.", "published": "2023-12-31 11:52:15", "link": "http://arxiv.org/abs/2401.04120v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Sounding Out Reconstruction Error-Based Evaluation of Generative Models\n  of Expressive Performance", "abstract": "Generative models of expressive piano performance are usually assessed by\ncomparing their predictions to a reference human performance. A generative\nalgorithm is taken to be better than competing ones if it produces performances\nthat are closer to a human reference performance. However, expert human\nperformers can (and do) interpret music in different ways, making for different\npossible references, and quantitative closeness is not necessarily aligned with\nperceptual similarity, raising concerns about the validity of this evaluation\napproach. In this work, we present a number of experiments that shed light on\nthis problem. Using precisely measured high-quality performances of classical\npiano music, we carry out a listening test indicating that listeners can\nsometimes perceive subtle performance difference that go unnoticed under\nquantitative evaluation. We further present tests that indicate that such\nevaluation frameworks show a lot of variability in reliability and validity\nacross different reference performances and pieces. We discuss these results\nand their implications for quantitative evaluation, and hope to foster a\ncritical appreciation of the uncertainties involved in quantitative assessments\nof such performances within the wider music information retrieval (MIR)\ncommunity.", "published": "2023-12-31 11:59:20", "link": "http://arxiv.org/abs/2401.00471v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "E-chat: Emotion-sensitive Spoken Dialogue System with Large Language\n  Models", "abstract": "This study focuses on emotion-sensitive spoken dialogue in human-machine\nspeech interaction. With the advancement of Large Language Models (LLMs),\ndialogue systems can handle multimodal data, including audio. Recent models\nhave enhanced the understanding of complex audio signals through the\nintegration of various audio events. However, they are unable to generate\nappropriate responses based on emotional speech. To address this, we introduce\nthe Emotional chat Model (E-chat), a novel spoken dialogue system capable of\ncomprehending and responding to emotions conveyed from speech. This model\nleverages an emotion embedding extracted by a speech encoder, combined with\nLLMs, enabling it to respond according to different emotional contexts.\nAdditionally, we introduce the E-chat200 dataset, designed explicitly for\nemotion-sensitive spoken dialogue. In various evaluation metrics, E-chat\nconsistently outperforms baseline model, demonstrating its potential in\nemotional comprehension and human-machine interaction.", "published": "2023-12-31 12:29:12", "link": "http://arxiv.org/abs/2401.00475v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Online Symbolic Music Alignment with Offline Reinforcement Learning", "abstract": "Symbolic Music Alignment is the process of matching performed MIDI notes to\ncorresponding score notes. In this paper, we introduce a reinforcement learning\n(RL)-based online symbolic music alignment technique. The RL agent - an\nattention-based neural network - iteratively estimates the current score\nposition from local score and performance contexts. For this symbolic alignment\ntask, environment states can be sampled exhaustively and the reward is dense,\nrendering a formulation as a simplified offline RL problem straightforward. We\nevaluate the trained agent in three ways. First, in its capacity to identify\ncorrect score positions for sampled test contexts; second, as the core\ntechnique of a complete algorithm for symbolic online note-wise alignment; and\nfinally, as a real-time symbolic score follower. We further investigate the\npitch-based score and performance representations used as the agent's inputs.\nTo this end, we develop a second model, a two-step Dynamic Time Warping\n(DTW)-based offline alignment algorithm leveraging the same input\nrepresentation. The proposed model outperforms a state-of-the-art reference\nmodel of offline symbolic music alignment.", "published": "2023-12-31 11:42:42", "link": "http://arxiv.org/abs/2401.00466v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Detecting the presence of sperm whales echolocation clicks in noisy\n  environments", "abstract": "Sperm whales (Physeter macrocephalus) navigate underwater with a series of\nimpulsive, click-like sounds known as echolocation clicks. These clicks are\ncharacterized by a multipulse structure (MPS) that serves as a distinctive\npattern. In this work, we use the stability of the MPS as a detection metric\nfor recognizing and classifying the presence of clicks in noisy environments.\nTo distinguish between noise transients and to handle simultaneous emissions\nfrom multiple sperm whales, our approach clusters a time series of MPS measures\nwhile removing potential clicks that do not fulfil the limits of inter-click\ninterval, duration and spectrum. As a result, our approach can handle high\nnoise transients and low signal-to-noise ratio. The performance of our\ndetection approach is examined using three datasets: seven months of recordings\nfrom the Mediterranean Sea containing manually verified ambient noise; several\ndays of manually labelled data collected from the Dominica Island containing\napproximately 40,000 clicks from multiple sperm whales; and a dataset from the\nBahamas containing 1,203 labelled clicks from a single sperm whale. Comparing\nwith the results of two benchmark detectors, a better trade-off between\nprecision and recall is observed as well as a significant reduction in false\ndetection rates, especially in noisy environments. To ensure reproducibility,\nwe provide our database of labelled clicks along with our implementation code.", "published": "2023-12-31 13:24:44", "link": "http://arxiv.org/abs/2401.00900v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
