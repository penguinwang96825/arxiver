{"title": "Named Entity Disambiguation for Noisy Text", "abstract": "We address the task of Named Entity Disambiguation (NED) for noisy text. We\npresent WikilinksNED, a large-scale NED dataset of text fragments from the web,\nwhich is significantly noisier and more challenging than existing news-based\ndatasets. To capture the limited and noisy local context surrounding each\nmention, we design a neural model and train it with a novel method for sampling\ninformative negative examples. We also describe a new way of initializing word\nand entity embeddings that significantly improves performance. Our model\nsignificantly outperforms existing state-of-the-art methods on WikilinksNED\nwhile achieving comparable performance on a smaller newswire dataset.", "published": "2017-06-28 07:26:55", "link": "http://arxiv.org/abs/1706.09147v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The E2E Dataset: New Challenges For End-to-End Generation", "abstract": "This paper describes the E2E data, a new dataset for training end-to-end,\ndata-driven natural language generation systems in the restaurant domain, which\nis ten times bigger than existing, frequently used datasets in this area. The\nE2E dataset poses new challenges: (1) its human reference texts show more\nlexical richness and syntactic variation, including discourse phenomena; (2)\ngenerating from this set requires content selection. As such, learning from\nthis dataset promises more natural, varied and less template-like system\nutterances. We also establish a baseline on this dataset, which illustrates\nsome of the difficulties associated with this data.", "published": "2017-06-28 12:38:53", "link": "http://arxiv.org/abs/1706.09254v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Generating Appealing Brand Names", "abstract": "Providing appealing brand names to newly launched products, newly formed\ncompanies or for renaming existing companies is highly important as it can play\na crucial role in deciding its success or failure. In this work, we propose a\ncomputational method to generate appealing brand names based on the description\nof such entities. We use quantitative scores for readability, pronounceability,\nmemorability and uniqueness of the generated names to rank order them. A set of\ndiverse appealing names is recommended to the user for the brand naming task.\nExperimental results show that the names generated by our approach are more\nappealing than names which prior approaches and recruited humans could come up.", "published": "2017-06-28 15:50:26", "link": "http://arxiv.org/abs/1706.09335v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data-driven Natural Language Generation: Paving the Road to Success", "abstract": "We argue that there are currently two major bottlenecks to the commercial use\nof statistical machine learning approaches for natural language generation\n(NLG): (a) The lack of reliable automatic evaluation metrics for NLG, and (b)\nThe scarcity of high quality in-domain corpora. We address the first problem by\nthoroughly analysing current evaluation metrics and motivating the need for a\nnew, more reliable metric. The second problem is addressed by presenting a\nnovel framework for developing and evaluating a high quality corpus for NLG\ntraining.", "published": "2017-06-28 18:17:30", "link": "http://arxiv.org/abs/1706.09433v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toward Computation and Memory Efficient Neural Network Acoustic Models\n  with Binary Weights and Activations", "abstract": "Neural network acoustic models have significantly advanced state of the art\nspeech recognition over the past few years. However, they are usually\ncomputationally expensive due to the large number of matrix-vector\nmultiplications and nonlinearity operations. Neural network models also require\nsignificant amounts of memory for inference because of the large model size.\nFor these two reasons, it is challenging to deploy neural network based speech\nrecognizers on resource-constrained platforms such as embedded devices. This\npaper investigates the use of binary weights and activations for computation\nand memory efficient neural network acoustic models. Compared to real-valued\nweight matrices, binary weights require much fewer bits for storage, thereby\ncutting down the memory footprint. Furthermore, with binary weights or\nactivations, the matrix-vector multiplications are turned into addition and\nsubtraction operations, which are computationally much faster and more energy\nefficient for hardware platforms. In this paper, we study the applications of\nbinary weights and activations for neural network acoustic modeling, reporting\nencouraging results on the WSJ and AMI corpora.", "published": "2017-06-28 19:41:25", "link": "http://arxiv.org/abs/1706.09453v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AP17-OLR Challenge: Data, Plan, and Baseline", "abstract": "We present the data profile and the evaluation plan of the second oriental\nlanguage recognition (OLR) challenge AP17-OLR. Compared to the event last year\n(AP16-OLR), the new challenge involves more languages and focuses more on short\nutterances. The data is offered by SpeechOcean and the NSFC M2ASR project. Two\ntypes of baselines are constructed to assist the participants, one is based on\nthe i-vector model and the other is based on various neural networks. We report\nthe baseline results evaluated with various metrics defined by the AP17-OLR\nevaluation plan and demonstrate that the combined database is a reasonable data\nresource for multilingual research. All the data is free for participants, and\nthe Kaldi recipes for the baselines have been published online.", "published": "2017-06-28 13:37:29", "link": "http://arxiv.org/abs/1706.09742v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
