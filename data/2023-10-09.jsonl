{"title": "Resolving the Imbalance Issue in Hierarchical Disciplinary Topic\n  Inference via LLM-based Data Augmentation", "abstract": "In addressing the imbalanced issue of data within the realm of Natural\nLanguage Processing, text data augmentation methods have emerged as pivotal\nsolutions. This data imbalance is prevalent in the research proposals submitted\nduring the funding application process. Such imbalances, resulting from the\nvarying popularity of disciplines or the emergence of interdisciplinary\nstudies, significantly impede the precision of downstream topic models that\ndeduce the affiliated disciplines of these proposals. At the data level,\nproposals penned by experts and scientists are inherently complex technological\ntexts, replete with intricate terminologies, which augmenting such specialized\ntext data poses unique challenges. At the system level, this, in turn,\ncompromises the fairness of AI-assisted reviewer assignment systems, which\nraises a spotlight on solving this issue. This study leverages large language\nmodels (Llama V1) as data generators to augment research proposals categorized\nwithin intricate disciplinary hierarchies, aiming to rectify data imbalances\nand enhance the equity of expert assignments. We first sample within the\nhierarchical structure to find the under-represented class. Then we designed a\nprompt for keyword-based research proposal generation. Our experiments attests\nto the efficacy of the generated data, demonstrating that research proposals\nproduced using the prompts can effectively address the aforementioned issues\nand generate high quality scientific text data, thus help the model overcome\nthe imbalanced issue.", "published": "2023-10-09 00:45:20", "link": "http://arxiv.org/abs/2310.05318v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GROVE: A Retrieval-augmented Complex Story Generation Framework with A\n  Forest of Evidence", "abstract": "Conditional story generation is significant in human-machine interaction,\nparticularly in producing stories with complex plots. While Large language\nmodels (LLMs) perform well on multiple NLP tasks, including story generation,\nit is challenging to generate stories with both complex and creative plots.\nExisting methods often rely on detailed prompts to guide LLMs to meet target\nconditions, which inadvertently restrict the creative potential of the\ngenerated stories. We argue that leveraging information from exemplary\nhuman-written stories facilitates generating more diverse plotlines. Delving\ndeeper into story details helps build complex and credible plots. In this\npaper, we propose a retrieval-au\\textbf{G}mented sto\\textbf{R}y generation\nframework with a f\\textbf{O}rest of e\\textbf{V}id\\textbf{E}nce (GROVE) to\nenhance stories' complexity. We build a retrieval repository for target\nconditions to produce few-shot examples to prompt LLMs. Additionally, we design\nan ``asking-why'' prompting scheme that extracts a forest of evidence,\nproviding compensation for the ambiguities that may occur in the generated\nstory. This iterative process uncovers underlying story backgrounds. Finally,\nwe select the most fitting chains of evidence from the evidence forest and\nintegrate them into the generated story, thereby enhancing the narrative's\ncomplexity and credibility. Experimental results and numerous examples verify\nthe effectiveness of our method.", "published": "2023-10-09 03:55:55", "link": "http://arxiv.org/abs/2310.05388v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Maze of Multilingual Modeling", "abstract": "Multilingual language models have gained significant attention in recent\nyears, enabling the development of applications that meet diverse linguistic\ncontexts. In this paper, we present a comprehensive evaluation of three popular\nmultilingual language models: mBERT, XLM-R, and GPT-3. We assess their\nperformance across a diverse set of languages, with a focus on understanding\nthe impact of resource availability (general and model-specific), language\nfamily, script type, and word order on model performance, under two distinct\ntasks - text classification and text generation. Our findings reveal that while\nthe amount of language-specific pretraining data plays a crucial role in model\nperformance, we also identify other factors such as general resource\navailability, language family, and script type, as important features. We hope\nthat our study contributes to a deeper understanding of multilingual language\nmodels to enhance their performance across languages and linguistic contexts.", "published": "2023-10-09 04:48:14", "link": "http://arxiv.org/abs/2310.05404v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Fast and Robust Early-Exiting Framework for Autoregressive Language\n  Models with Synchronized Parallel Decoding", "abstract": "To tackle the high inference latency exhibited by autoregressive language\nmodels, previous studies have proposed an early-exiting framework that\nallocates adaptive computation paths for each token based on the complexity of\ngenerating the subsequent token. However, we observed several shortcomings,\nincluding performance degradation caused by a state copying mechanism or\nnumerous exit paths, and sensitivity to exit confidence thresholds.\nConsequently, we propose a Fast and Robust Early-Exiting (FREE) framework,\nwhich incorporates a shallow-deep module and a synchronized parallel decoding.\nOur framework enables faster inference by synchronizing the decoding process of\nthe current token with previously stacked early-exited tokens. Furthermore, as\nparallel decoding allows us to observe predictions from both shallow and deep\nmodels, we present a novel adaptive threshold estimator that exploits a Beta\nmixture model to determine suitable confidence thresholds. We empirically\ndemonstrated the superiority of our proposed framework on extensive generation\ntasks.", "published": "2023-10-09 05:53:05", "link": "http://arxiv.org/abs/2310.05424v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Establishing Trustworthiness: Rethinking Tasks and Model Evaluation", "abstract": "Language understanding is a multi-faceted cognitive capability, which the\nNatural Language Processing (NLP) community has striven to model\ncomputationally for decades. Traditionally, facets of linguistic intelligence\nhave been compartmentalized into tasks with specialized model architectures and\ncorresponding evaluation protocols. With the advent of large language models\n(LLMs) the community has witnessed a dramatic shift towards general purpose,\ntask-agnostic approaches powered by generative models. As a consequence, the\ntraditional compartmentalized notion of language tasks is breaking down,\nfollowed by an increasing challenge for evaluation and analysis. At the same\ntime, LLMs are being deployed in more real-world scenarios, including\npreviously unforeseen zero-shot setups, increasing the need for trustworthy and\nreliable systems. Therefore, we argue that it is time to rethink what\nconstitutes tasks and model evaluation in NLP, and pursue a more holistic view\non language, placing trustworthiness at the center. Towards this goal, we\nreview existing compartmentalized approaches for understanding the origins of a\nmodel's functional capacity, and provide recommendations for more multi-faceted\nevaluation protocols.", "published": "2023-10-09 06:32:10", "link": "http://arxiv.org/abs/2310.05442v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Empower Nested Boolean Logic via Self-Supervised Curriculum Learning", "abstract": "Beyond the great cognitive powers showcased by language models, it is crucial\nto scrutinize whether their reasoning capabilities stem from strong\ngeneralization or merely exposure to relevant data. As opposed to constructing\nincreasingly complex logic, this paper probes into the boolean logic, the root\ncapability of a logical reasoner. We find that any pre-trained language models\neven including large language models only behave like a random selector in the\nface of multi-nested boolean logic, a task that humans can handle with ease. To\nempower language models with this fundamental capability, this paper proposes a\nnew self-supervised learning method \\textit{Curriculum Logical Reasoning}\n(\\textsc{Clr}), where we augment the training data with nested boolean logic\nchain step-by-step, and program the training from simpler logical patterns\ngradually to harder ones. This new training paradigm allows language models to\neffectively generalize to much harder and longer-hop logic, which can hardly be\nlearned through naive training. Furthermore, we show that boolean logic is a\ngreat foundation for improving the subsequent general logical tasks.", "published": "2023-10-09 06:54:02", "link": "http://arxiv.org/abs/2310.05450v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XAL: EXplainable Active Learning Makes Classifiers Better Low-resource\n  Learners", "abstract": "Active learning (AL), which aims to construct an effective training set by\niteratively curating the most formative unlabeled data for annotation, has been\nwidely used in low-resource tasks. Most active learning techniques in\nclassification rely on the model's uncertainty or disagreement to choose\nunlabeled data, suffering from the problem of over-confidence in superficial\npatterns and a lack of exploration. Inspired by the cognitive processes in\nwhich humans deduce and predict through causal information, we take an initial\nattempt towards integrating rationales into AL and propose a novel Explainable\nActive Learning framework (XAL) for low-resource text classification, which\naims to encourage classifiers to justify their inferences and delve into\nunlabeled data for which they cannot provide reasonable explanations.\nSpecifically, besides using a pre-trained bi-directional encoder for\nclassification, we employ a pre-trained uni-directional decoder to generate and\nscore the explanation. We further facilitate the alignment of the model with\nhuman reasoning preference through a proposed ranking loss. During the\nselection of unlabeled data, the predicted uncertainty of the encoder and the\nexplanation score of the decoder complement each other as the final metric to\nacquire informative data. Extensive experiments on six datasets show that XAL\nachieves consistent improvement over 9 strong baselines. Analysis indicates\nthat the proposed method can generate corresponding explanations for its\npredictions.", "published": "2023-10-09 08:07:04", "link": "http://arxiv.org/abs/2310.05502v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Regulation and NLP (RegNLP): Taming Large Language Models", "abstract": "The scientific innovation in Natural Language Processing (NLP) and more\nbroadly in artificial intelligence (AI) is at its fastest pace to date. As\nlarge language models (LLMs) unleash a new era of automation, important debates\nemerge regarding the benefits and risks of their development, deployment and\nuse. Currently, these debates have been dominated by often polarized narratives\nmainly led by the AI Safety and AI Ethics movements. This polarization, often\namplified by social media, is swaying political agendas on AI regulation and\ngovernance and posing issues of regulatory capture. Capture occurs when the\nregulator advances the interests of the industry it is supposed to regulate, or\nof special interest groups rather than pursuing the general public interest.\nMeanwhile in NLP research, attention has been increasingly paid to the\ndiscussion of regulating risks and harms. This often happens without systematic\nmethodologies or sufficient rooting in the disciplines that inspire an extended\nscope of NLP research, jeopardizing the scientific integrity of these\nendeavors. Regulation studies are a rich source of knowledge on how to\nsystematically deal with risk and uncertainty, as well as with scientific\nevidence, to evaluate and compare regulatory options. This resource has largely\nremained untapped so far. In this paper, we argue how NLP research on these\ntopics can benefit from proximity to regulatory studies and adjacent fields. We\ndo so by discussing basic tenets of regulation, and risk and uncertainty, and\nby highlighting the shortcomings of current NLP discussions dealing with risk\nassessment. Finally, we advocate for the development of a new multidisciplinary\nresearch space on regulation and NLP (RegNLP), focused on connecting scientific\nknowledge to regulatory processes based on systematic methodologies.", "published": "2023-10-09 09:22:40", "link": "http://arxiv.org/abs/2310.05553v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can language models learn analogical reasoning? Investigating training\n  objectives and comparisons to human performance", "abstract": "While analogies are a common way to evaluate word embeddings in NLP, it is\nalso of interest to investigate whether or not analogical reasoning is a task\nin itself that can be learned. In this paper, we test several ways to learn\nbasic analogical reasoning, specifically focusing on analogies that are more\ntypical of what is used to evaluate analogical reasoning in humans than those\nin commonly used NLP benchmarks. Our experiments find that models are able to\nlearn analogical reasoning, even with a small amount of data. We additionally\ncompare our models to a dataset with a human baseline, and find that after\ntraining, models approach human performance.", "published": "2023-10-09 10:34:38", "link": "http://arxiv.org/abs/2310.05597v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LAiW: A Chinese Legal Large Language Models Benchmark", "abstract": "General and legal domain LLMs have demonstrated strong performance in various\ntasks of LegalAI. However, the current evaluations of these LLMs in LegalAI are\ndefined by the experts of computer science, lacking consistency with the logic\nof legal practice, making it difficult to judge their practical capabilities.\nTo address this challenge, we are the first to build the Chinese legal LLMs\nbenchmark LAiW, based on the logic of legal practice. To align with the\nthinking process of legal experts and legal practice (syllogism), we divide the\nlegal capabilities of LLMs from easy to difficult into three levels: basic\ninformation retrieval, legal foundation inference, and complex legal\napplication. Each level contains multiple tasks to ensure a comprehensive\nevaluation. Through automated evaluation of current general and legal domain\nLLMs on our benchmark, we indicate that these LLMs may not align with the logic\nof legal practice. LLMs seem to be able to directly acquire complex legal\napplication capabilities but perform poorly in some basic tasks, which may pose\nobstacles to their practical application and acceptance by legal experts. To\nfurther confirm the complex legal application capabilities of current LLMs in\nlegal application scenarios, we also incorporate human evaluation with legal\nexperts. The results indicate that while LLMs may demonstrate strong\nperformance, they still require reinforcement of legal logic.", "published": "2023-10-09 11:19:55", "link": "http://arxiv.org/abs/2310.05620v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Verifiable Generation: A Benchmark for Knowledge-aware Language\n  Model Attribution", "abstract": "Although achieving great success, Large Language Models (LLMs) usually suffer\nfrom unreliable hallucinations. Although language attribution can be a\npotential solution, there are no suitable benchmarks and evaluation metrics to\nattribute LLMs to structured knowledge. In this paper, we define a new task of\nKnowledge-aware Language Model Attribution (KaLMA) that improves upon three\ncore concerns with conventional attributed LMs. First, we extend attribution\nsource from unstructured texts to Knowledge Graph (KG), whose rich structures\nbenefit both the attribution performance and working scenarios. Second, we\npropose a new ``Conscious Incompetence\" setting considering the incomplete\nknowledge repository, where the model identifies the need for supporting\nknowledge beyond the provided KG. Third, we propose a comprehensive automatic\nevaluation metric encompassing text quality, citation quality, and text\ncitation alignment. To implement the above innovations, we build a dataset in\nbiography domain BioKaLMA via evolutionary question generation strategy, to\ncontrol the question complexity and necessary knowledge to the answer. For\nevaluation, we develop a baseline solution and demonstrate the room for\nimprovement in LLMs' citation generation, emphasizing the importance of\nincorporating the \"Conscious Incompetence\" setting, and the critical role of\nretrieval accuracy.", "published": "2023-10-09 11:45:59", "link": "http://arxiv.org/abs/2310.05634v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ReZG: Retrieval-Augmented Zero-Shot Counter Narrative Generation for\n  Hate Speech", "abstract": "The proliferation of hate speech (HS) on social media poses a serious threat\nto societal security. Automatic counter narrative (CN) generation, as an active\nstrategy for HS intervention, has garnered increasing attention in recent\nyears. Existing methods for automatically generating CNs mainly rely on\nre-training or fine-tuning pre-trained language models (PLMs) on human-curated\nCN corpora. Unfortunately, the annotation speed of CN corpora cannot keep up\nwith the growth of HS targets, while generating specific and effective CNs for\nunseen targets remains a significant challenge for the model. To tackle this\nissue, we propose Retrieval-Augmented Zero-shot Generation (ReZG) to generate\nCNs with high-specificity for unseen targets. Specifically, we propose a\nmulti-dimensional hierarchical retrieval method that integrates stance,\nsemantics, and fitness, extending the retrieval metric from single dimension to\nmultiple dimensions suitable for the knowledge that refutes HS. Then, we\nimplement an energy-based constrained decoding mechanism that enables PLMs to\nuse differentiable knowledge preservation, countering, and fluency constraint\nfunctions instead of in-target CNs as control signals for generation, thereby\nachieving zero-shot CN generation. With the above techniques, ReZG can\nintegrate external knowledge flexibly and improve the specificity of CNs.\nExperimental results show that ReZG exhibits stronger generalization\ncapabilities and outperforms strong baselines with significant improvements of\n2.0%+ in the relevance and 4.5%+ in the countering success rate metrics.", "published": "2023-10-09 12:01:26", "link": "http://arxiv.org/abs/2310.05650v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Closer Look into Automatic Evaluation Using Large Language Models", "abstract": "Using large language models (LLMs) to evaluate text quality has recently\ngained popularity. Some prior works explore the idea of using LLMs for\nevaluation, while they differ in some details of the evaluation process. In\nthis paper, we analyze LLM evaluation (Chiang and Lee, 2023) and G-Eval (Liu et\nal., 2023), and we discuss how those details in the evaluation process change\nhow well the ratings given by LLMs correlate with human ratings. We find that\nthe auto Chain-of-Thought (CoT) used in G-Eval does not always make G-Eval more\naligned with human ratings. We also show that forcing the LLM to output only a\nnumeric rating, as in G-Eval, is suboptimal. Last, we reveal that asking the\nLLM to explain its own ratings consistently improves the correlation between\nthe ChatGPT and human ratings and pushes state-of-the-art (SoTA) correlations\non two meta-evaluation datasets.", "published": "2023-10-09 12:12:55", "link": "http://arxiv.org/abs/2310.05657v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Larth: Dataset and Machine Translation for Etruscan", "abstract": "Etruscan is an ancient language spoken in Italy from the 7th century BC to\nthe 1st century AD. There are no native speakers of the language at the present\nday, and its resources are scarce, as there exist only around 12,000 known\ninscriptions. To the best of our knowledge, there are no publicly available\nEtruscan corpora for natural language processing. Therefore, we propose a\ndataset for machine translation from Etruscan to English, which contains 2891\ntranslated examples from existing academic sources. Some examples are extracted\nmanually, while others are acquired in an automatic way. Along with the\ndataset, we benchmark different machine translation models observing that it is\npossible to achieve a BLEU score of 10.1 with a small transformer model.\nReleasing the dataset can help enable future research on this language, similar\nlanguages or other languages with scarce resources.", "published": "2023-10-09 12:56:08", "link": "http://arxiv.org/abs/2310.05688v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey of Large Language Models for Healthcare: from Data, Technology,\n  and Applications to Accountability and Ethics", "abstract": "The utilization of large language models (LLMs) in the Healthcare domain has\ngenerated both excitement and concern due to their ability to effectively\nrespond to freetext queries with certain professional knowledge. This survey\noutlines the capabilities of the currently developed LLMs for Healthcare and\nexplicates their development process, with the aim of providing an overview of\nthe development roadmap from traditional Pretrained Language Models (PLMs) to\nLLMs. Specifically, we first explore the potential of LLMs to enhance the\nefficiency and effectiveness of various Healthcare applications highlighting\nboth the strengths and limitations. Secondly, we conduct a comparison between\nthe previous PLMs and the latest LLMs, as well as comparing various LLMs with\neach other. Then we summarize related Healthcare training data, training\nmethods, optimization strategies, and usage. Finally, the unique concerns\nassociated with deploying LLMs in Healthcare settings are investigated,\nparticularly regarding fairness, accountability, transparency and ethics. Our\nsurvey provide a comprehensive investigation from perspectives of both computer\nscience and Healthcare specialty. Besides the discussion about Healthcare\nconcerns, we supports the computer science community by compiling a collection\nof open source resources, such as accessible datasets, the latest\nmethodologies, code implementations, and evaluation benchmarks in the Github.\nSummarily, we contend that a significant paradigm shift is underway,\ntransitioning from PLMs to LLMs. This shift encompasses a move from\ndiscriminative AI approaches to generative AI approaches, as well as a shift\nfrom model-centered methodologies to data-centered methodologies. Also, we\ndetermine that the biggest obstacle of using LLMs in Healthcare are fairness,\naccountability, transparency and ethics.", "published": "2023-10-09 13:15:23", "link": "http://arxiv.org/abs/2310.05694v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aligning Language Models with Human Preferences via a Bayesian Approach", "abstract": "In the quest to advance human-centric natural language generation (NLG)\nsystems, ensuring alignment between NLG models and human preferences is\ncrucial. For this alignment, current popular methods leverage a reinforcement\nlearning (RL) approach with a reward model trained on feedback from humans.\nHowever, inherent disagreements due to the subjective nature of human\npreferences pose a significant challenge for training the reward model,\nresulting in a deterioration of the NLG performance. To tackle this issue,\nprevious approaches typically rely on majority voting or averaging to\nconsolidate multiple inconsistent preferences into a merged one. Although\nstraightforward to understand and execute, such methods suffer from an\ninability to capture the nuanced degrees of disaggregation among humans and may\nonly represent a specialized subset of individuals, thereby lacking the ability\nto quantitatively disclose the universality of human preferences. To address\nthis challenge, this paper proposes a novel approach, which employs a Bayesian\nframework to account for the distribution of disagreements among human\npreferences as training a preference model, and names it as d-PM. Besides,\nconsidering the RL strategy's inefficient and complex training process over the\ntraining efficiency, we further propose utilizing the contrastive learning\nstrategy to train the NLG model with the preference scores derived from the\nd-PM model. Extensive experiments on two human-centric NLG tasks, i.e.,\nemotional support conversation and integrity \"Rule-of-Thumb\" generation, show\nthat our method consistently exceeds previous SOTA models in both automatic and\nhuman evaluations.", "published": "2023-10-09 15:15:05", "link": "http://arxiv.org/abs/2310.05782v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Problem-Solving Guide: Predicting the Algorithm Tags and Difficulty for\n  Competitive Programming Problems", "abstract": "The recent program development industries have required problem-solving\nabilities for engineers, especially application developers. However, AI-based\neducation systems to help solve computer algorithm problems have not yet\nattracted attention, while most big tech companies require the ability to solve\nalgorithm problems including Google, Meta, and Amazon. The most useful guide to\nsolving algorithm problems might be guessing the category (tag) of the facing\nproblems. Therefore, our study addresses the task of predicting the algorithm\ntag as a useful tool for engineers and developers. Moreover, we also consider\npredicting the difficulty levels of algorithm problems, which can be used as\nuseful guidance to calculate the required time to solve that problem. In this\npaper, we present a real-world algorithm problem multi-task dataset, AMT, by\nmainly collecting problem samples from the most famous and large competitive\nprogramming website Codeforces. To the best of our knowledge, our proposed\ndataset is the most large-scale dataset for predicting algorithm tags compared\nto previous studies. Moreover, our work is the first to address predicting the\ndifficulty levels of algorithm problems. We present a deep learning-based novel\nmethod for simultaneously predicting algorithm tags and the difficulty levels\nof an algorithm problem given. All datasets and source codes are available at\nhttps://github.com/sronger/PSG_Predicting_Algorithm_Tags_and_Difficulty.", "published": "2023-10-09 15:26:07", "link": "http://arxiv.org/abs/2310.05791v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SC-Safety: A Multi-round Open-ended Question Adversarial Safety\n  Benchmark for Large Language Models in Chinese", "abstract": "Large language models (LLMs), like ChatGPT and GPT-4, have demonstrated\nremarkable abilities in natural language understanding and generation. However,\nalongside their positive impact on our daily tasks, they can also produce\nharmful content that negatively affects societal perceptions. To systematically\nassess the safety of Chinese LLMs, we introduce SuperCLUE-Safety (SC-Safety) -\na multi-round adversarial benchmark with 4912 open-ended questions covering\nmore than 20 safety sub-dimensions. Adversarial human-model interactions and\nconversations significantly increase the challenges compared to existing\nmethods. Experiments on 13 major LLMs supporting Chinese yield the following\ninsights: 1) Closed-source models outperform open-sourced ones in terms of\nsafety; 2) Models released from China demonstrate comparable safety levels to\nLLMs like GPT-3.5-turbo; 3) Some smaller models with 6B-13B parameters can\ncompete effectively in terms of safety. By introducing SC-Safety, we aim to\npromote collaborative efforts to create safer and more trustworthy LLMs. The\nbenchmark and findings provide guidance on model selection. Our benchmark can\nbe found at https://www.CLUEbenchmarks.com", "published": "2023-10-09 16:03:22", "link": "http://arxiv.org/abs/2310.05818v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Terminology-Aware Translation with Constrained Decoding and Large\n  Language Model Prompting", "abstract": "Terminology correctness is important in the downstream application of machine\ntranslation, and a prevalent way to ensure this is to inject terminology\nconstraints into a translation system. In our submission to the WMT 2023\nterminology translation task, we adopt a translate-then-refine approach which\ncan be domain-independent and requires minimal manual efforts. We annotate\nrandom source words with pseudo-terminology translations obtained from word\nalignment to first train a terminology-aware model. Further, we explore two\npost-processing methods. First, we use an alignment process to discover whether\na terminology constraint has been violated, and if so, we re-decode with the\nviolating word negatively constrained. Alternatively, we leverage a large\nlanguage model to refine a hypothesis by providing it with terminology\nconstraints. Results show that our terminology-aware model learns to\nincorporate terminologies effectively, and the large language model refinement\nprocess can further improve terminology recall.", "published": "2023-10-09 16:08:23", "link": "http://arxiv.org/abs/2310.05824v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Compressing Context to Enhance Inference Efficiency of Large Language\n  Models", "abstract": "Large language models (LLMs) achieved remarkable performance across various\ntasks. However, they face challenges in managing long documents and extended\nconversations, due to significantly increased computational requirements, both\nin memory and inference time, and potential context truncation when the input\nexceeds the LLM's fixed context length. This paper proposes a method called\nSelective Context that enhances the inference efficiency of LLMs by identifying\nand pruning redundancy in the input context to make the input more compact. We\ntest our approach using common data sources requiring long context processing:\narXiv papers, news articles, and long conversations, on tasks of summarisation,\nquestion answering, and response generation. Experimental results show that\nSelective Context significantly reduces memory cost and decreases generation\nlatency while maintaining comparable performance compared to that achieved when\nfull context is used. Specifically, we achieve a 50\\% reduction in context\ncost, resulting in a 36\\% reduction in inference memory usage and a 32\\%\nreduction in inference time, while observing only a minor drop of .023 in\nBERTscore and .038 in faithfulness on four downstream applications, indicating\nthat our method strikes a good balance between efficiency and performance.", "published": "2023-10-09 23:03:24", "link": "http://arxiv.org/abs/2310.06201v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GPT-who: An Information Density-based Machine-Generated Text Detector", "abstract": "The Uniform Information Density (UID) principle posits that humans prefer to\nspread information evenly during language production. We examine if this UID\nprinciple can help capture differences between Large Language Models\n(LLMs)-generated and human-generated texts. We propose GPT-who, the first\npsycholinguistically-inspired domain-agnostic statistical detector. This\ndetector employs UID-based features to model the unique statistical signature\nof each LLM and human author for accurate detection. We evaluate our method\nusing 4 large-scale benchmark datasets and find that GPT-who outperforms\nstate-of-the-art detectors (both statistical- & non-statistical) such as GLTR,\nGPTZero, DetectGPT, OpenAI detector, and ZeroGPT by over $20$% across domains.\nIn addition to better performance, it is computationally inexpensive and\nutilizes an interpretable representation of text articles. We find that GPT-who\ncan distinguish texts generated by very sophisticated LLMs, even when the\noverlying text is indiscernible. UID-based measures for all datasets and code\nare available at https://github.com/saranya-venkatraman/gpt-who.", "published": "2023-10-09 23:06:05", "link": "http://arxiv.org/abs/2310.06202v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Emotion-Based Synthetic Consciousness: Using LLMs to Estimate\n  Emotion Probability Vectors", "abstract": "This paper shows how LLMs (Large Language Models) may be used to estimate a\nsummary of the emotional state associated with piece of text. The summary of\nemotional state is a dictionary of words used to describe emotion together with\nthe probability of the word appearing after a prompt comprising the original\ntext and an emotion eliciting tail. Through emotion analysis of Amazon product\nreviews we demonstrate emotion descriptors can be mapped into a PCA type space.\nIt was hoped that text descriptions of actions to improve a current text\ndescribed state could also be elicited through a tail prompt. Experiment seemed\nto indicate that this is not straightforward to make work. This failure put our\nhoped for selection of action via choosing the best predict ed outcome via\ncomparing emotional responses out of reach for the moment.", "published": "2023-10-09 13:29:36", "link": "http://arxiv.org/abs/2310.10673v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Large Language Models Geospatially Knowledgeable?", "abstract": "Despite the impressive performance of Large Language Models (LLM) for various\nnatural language processing tasks, little is known about their comprehension of\ngeographic data and related ability to facilitate informed geospatial\ndecision-making. This paper investigates the extent of geospatial knowledge,\nawareness, and reasoning abilities encoded within such pretrained LLMs. With a\nfocus on autoregressive language models, we devise experimental approaches\nrelated to (i) probing LLMs for geo-coordinates to assess geospatial knowledge,\n(ii) using geospatial and non-geospatial prepositions to gauge their geospatial\nawareness, and (iii) utilizing a multidimensional scaling (MDS) experiment to\nassess the models' geospatial reasoning capabilities and to determine locations\nof cities based on prompting. Our results confirm that it does not only take\nlarger, but also more sophisticated LLMs to synthesize geospatial knowledge\nfrom textual information. As such, this research contributes to understanding\nthe potential and limitations of LLMs in dealing with geospatial information.", "published": "2023-10-09 17:20:11", "link": "http://arxiv.org/abs/2310.13002v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Task-Adaptive Tokenization: Enhancing Long-Form Text Generation Efficacy\n  in Mental Health and Beyond", "abstract": "We propose task-adaptive tokenization as a way to adapt the generation\npipeline to the specifics of a downstream task and enhance long-form generation\nin mental health. Inspired by insights from cognitive science, our\ntask-adaptive tokenizer samples variable segmentations from multiple outcomes,\nwith sampling probabilities optimized based on task-specific data. We introduce\na strategy for building a specialized vocabulary and introduce a vocabulary\nmerging protocol that allows for the integration of task-specific tokens into\nthe pre-trained model's tokenization step. Through extensive experiments on\npsychological question-answering tasks in both Chinese and English, we find\nthat our task-adaptive tokenization approach brings a significant improvement\nin generation performance while using up to 60% fewer tokens. Preliminary\nexperiments point to promising results when using our tokenization approach\nwith very large language models.", "published": "2023-10-09 00:20:59", "link": "http://arxiv.org/abs/2310.05317v5", "categories": ["cs.CL", "cs.AI", "68", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Negative Object Presence Evaluation (NOPE) to Measure Object\n  Hallucination in Vision-Language Models", "abstract": "Object hallucination poses a significant challenge in vision-language (VL)\nmodels, often leading to the generation of nonsensical or unfaithful responses\nwith non-existent objects. However, the absence of a general measurement for\nevaluating object hallucination in VL models has hindered our understanding and\nability to mitigate this issue. In this work, we present NOPE (Negative Object\nPresence Evaluation), a novel benchmark designed to assess object hallucination\nin VL models through visual question answering (VQA). We propose a\ncost-effective and scalable approach utilizing large language models to\ngenerate 29.5k synthetic negative pronoun (NegP) data of high quality for NOPE.\nWe extensively investigate the performance of 10 state-of-the-art VL models in\ndiscerning the non-existence of objects in visual questions, where the ground\ntruth answers are denoted as NegP (e.g., \"none\"). Additionally, we evaluate\ntheir standard performance on visual questions on 9 other VQA datasets. Through\nour experiments, we demonstrate that no VL model is immune to the vulnerability\nof object hallucination, as all models achieve accuracy below 10\\% on NegP.\nFurthermore, we uncover that lexically diverse visual questions, question types\nwith large scopes, and scene-relevant objects capitalize the risk of object\nhallucination in VL models.", "published": "2023-10-09 01:52:27", "link": "http://arxiv.org/abs/2310.05338v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Universal Multi-modal Entity Alignment via Iteratively Fusing Modality\n  Similarity Paths", "abstract": "The objective of Entity Alignment (EA) is to identify equivalent entity pairs\nfrom multiple Knowledge Graphs (KGs) and create a more comprehensive and\nunified KG. The majority of EA methods have primarily focused on the structural\nmodality of KGs, lacking exploration of multi-modal information. A few\nmulti-modal EA methods have made good attempts in this field. Still, they have\ntwo shortcomings: (1) inconsistent and inefficient modality modeling that\ndesigns complex and distinct models for each modality; (2) ineffective modality\nfusion due to the heterogeneous nature of modalities in EA. To tackle these\nchallenges, we propose PathFusion, consisting of two main components: (1) MSP,\na unified modeling approach that simplifies the alignment process by\nconstructing paths connecting entities and modality nodes to represent multiple\nmodalities; (2) IRF, an iterative fusion method that effectively combines\ninformation from different modalities using the path as an information carrier.\nExperimental results on real-world datasets demonstrate the superiority of\nPathFusion over state-of-the-art methods, with 22.4%-28.9% absolute improvement\non Hits@1, and 0.194-0.245 absolute improvement on MRR.", "published": "2023-10-09 02:50:54", "link": "http://arxiv.org/abs/2310.05364v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transcending the Attention Paradigm: Representation Learning from\n  Geospatial Social Media Data", "abstract": "While transformers have pioneered attention-driven architectures as a\ncornerstone of language modeling, their dependence on explicitly contextual\ninformation underscores limitations in their abilities to tacitly learn\noverarching textual themes. This study challenges the heuristic paradigm of\nperformance benchmarking by investigating social media data as a source of\ndistributed patterns. In stark contrast to networks that rely on capturing\ncomplex long-term dependencies, models of online data inherently lack structure\nand are forced to detect latent structures in the aggregate. To properly\nrepresent these abstract relationships, this research dissects empirical social\nmedia corpora into their elemental components, analyzing over two billion\ntweets across population-dense locations. We create Bag-of-Word embedding\nspecific to each city and compare their respective representations. This finds\nthat even amidst noisy data, geographic location has a considerable influence\non online communication, and that hidden insights can be uncovered without the\ncrutch of advanced algorithms. This evidence presents valuable geospatial\nimplications in social science and challenges the notion that intricate models\nare prerequisites for pattern recognition in natural language. This aligns with\nthe evolving landscape that questions the embrace of absolute interpretability\nover abstract understanding and bridges the divide between sophisticated\nframeworks and intangible relationships.", "published": "2023-10-09 03:27:05", "link": "http://arxiv.org/abs/2310.05378v3", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Parrot Mind: Towards Explaining the Complex Task Reasoning of Pretrained\n  Large Language Models with Template-Content Structure", "abstract": "The pre-trained large language models (LLMs) have shown their extraordinary\ncapacity to solve reasoning tasks, even on tasks that require a complex process\ninvolving multiple sub-steps. However, given the vast possible generation space\nof all the tasks, how the pretrained model learns the reasoning ability remains\nan open question. We firstly propose that an intrinsic structural constraint on\nthe generated sequence of language-based reasoning -- we called it\ntemplate-content structure (T-C structure) -- is the key to explain why LLMs\ncan solve a large number of complex reasoning problems with limited training\ndata by showing this structure can reduce the possible space from exponential\nlevel to linear level. Furthermore, by generalizing this structure to the\nhierarchical case, we demonstrate that models can achieve task composition,\nfurther reducing the space needed to learn from linear to logarithmic, thereby\neffectively learning on complex reasoning involving multiple steps. We provide\nboth examples and formal theory of our T-C structure. We also experimentally\nvalidate the existence of the T-C structure in some current LLMs and its\neffectiveness for reasoning.", "published": "2023-10-09 06:57:45", "link": "http://arxiv.org/abs/2310.05452v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Generative Judge for Evaluating Alignment", "abstract": "The rapid development of Large Language Models (LLMs) has substantially\nexpanded the range of tasks they can address. In the field of Natural Language\nProcessing (NLP), researchers have shifted their focus from conventional NLP\ntasks (e.g., sequence tagging and parsing) towards tasks that revolve around\naligning with human needs (e.g., brainstorming and email writing). This shift\nin task distribution imposes new requirements on evaluating these aligned\nmodels regarding generality (i.e., assessing performance across diverse\nscenarios), flexibility (i.e., examining under different protocols), and\ninterpretability (i.e., scrutinizing models with explanations). In this paper,\nwe propose a generative judge with 13B parameters, Auto-J, designed to address\nthese challenges. Our model is trained on user queries and LLM-generated\nresponses under massive real-world scenarios and accommodates diverse\nevaluation protocols (e.g., pairwise response comparison and single-response\nevaluation) with well-structured natural language critiques. To demonstrate the\nefficacy of our approach, we construct a new testbed covering 58 different\nscenarios. Experimentally, Auto-J outperforms a series of strong competitors,\nincluding both open-source and closed-source models, by a large margin. We also\nprovide detailed analysis and case studies to further reveal the potential of\nour method and make a variety of resources public at\nhttps://github.com/GAIR-NLP/auto-j.", "published": "2023-10-09 07:27:15", "link": "http://arxiv.org/abs/2310.05470v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cabbage Sweeter than Cake? Analysing the Potential of Large Language\n  Models for Learning Conceptual Spaces", "abstract": "The theory of Conceptual Spaces is an influential cognitive-linguistic\nframework for representing the meaning of concepts. Conceptual spaces are\nconstructed from a set of quality dimensions, which essentially correspond to\nprimitive perceptual features (e.g. hue or size). These quality dimensions are\nusually learned from human judgements, which means that applications of\nconceptual spaces tend to be limited to narrow domains (e.g. modelling colour\nor taste). Encouraged by recent findings about the ability of Large Language\nModels (LLMs) to learn perceptually grounded representations, we explore the\npotential of such models for learning conceptual spaces. Our experiments show\nthat LLMs can indeed be used for learning meaningful representations to some\nextent. However, we also find that fine-tuned models of the BERT family are\nable to match or even outperform the largest GPT-3 model, despite being 2 to 3\norders of magnitude smaller.", "published": "2023-10-09 07:41:19", "link": "http://arxiv.org/abs/2310.05481v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DRIN: Dynamic Relation Interactive Network for Multimodal Entity Linking", "abstract": "Multimodal Entity Linking (MEL) is a task that aims to link ambiguous\nmentions within multimodal contexts to referential entities in a multimodal\nknowledge base. Recent methods for MEL adopt a common framework: they first\ninteract and fuse the text and image to obtain representations of the mention\nand entity respectively, and then compute the similarity between them to\npredict the correct entity. However, these methods still suffer from two\nlimitations: first, as they fuse the features of text and image before\nmatching, they cannot fully exploit the fine-grained alignment relations\nbetween the mention and entity. Second, their alignment is static, leading to\nlow performance when dealing with complex and diverse data. To address these\nissues, we propose a novel framework called Dynamic Relation Interactive\nNetwork (DRIN) for MEL tasks. DRIN explicitly models four different types of\nalignment between a mention and entity and builds a dynamic Graph Convolutional\nNetwork (GCN) to dynamically select the corresponding alignment relations for\ndifferent input samples. Experiments on two datasets show that DRIN outperforms\nstate-of-the-art methods by a large margin, demonstrating the effectiveness of\nour approach.", "published": "2023-10-09 10:21:42", "link": "http://arxiv.org/abs/2310.05589v1", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Dynamic Top-k Estimation Consolidates Disagreement between Feature\n  Attribution Methods", "abstract": "Feature attribution scores are used for explaining the prediction of a text\nclassifier to users by highlighting a k number of tokens. In this work, we\npropose a way to determine the number of optimal k tokens that should be\ndisplayed from sequential properties of the attribution scores. Our approach is\ndynamic across sentences, method-agnostic, and deals with sentence length bias.\nWe compare agreement between multiple methods and humans on an NLI task, using\nfixed k and dynamic k. We find that perturbation-based methods and Vanilla\nGradient exhibit highest agreement on most method--method and method--human\nagreement metrics with a static k. Their advantage over other methods\ndisappears with dynamic ks which mainly improve Integrated Gradient and\nGradientXInput. To our knowledge, this is the first evidence that sequential\nproperties of attribution scores are informative for consolidating attribution\nsignals for human interpretation.", "published": "2023-10-09 11:19:33", "link": "http://arxiv.org/abs/2310.05619v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The potential of large language models for improving probability\n  learning: A study on ChatGPT3.5 and first-year computer engineering students", "abstract": "In this paper, we assess the efficacy of ChatGPT (version Feb 2023), a\nlarge-scale language model, in solving probability problems typically presented\nin introductory computer engineering exams. Our study comprised a set of 23\nprobability exercises administered to students at Rey Juan Carlos University\n(URJC) in Madrid. The responses produced by ChatGPT were evaluated by a group\nof five statistics professors, who assessed them qualitatively and assigned\ngrades based on the same criteria used for students. Our results indicate that\nChatGPT surpasses the average student in terms of phrasing, organization, and\nlogical reasoning. The model's performance remained consistent for both the\nSpanish and English versions of the exercises. However, ChatGPT encountered\ndifficulties in executing basic numerical operations. Our experiments\ndemonstrate that requesting ChatGPT to provide the solution in the form of an R\nscript proved to be an effective approach for overcoming these limitations. In\nsummary, our results indicate that ChatGPT surpasses the average student in\nsolving probability problems commonly presented in introductory computer\nengineering exams. Nonetheless, the model exhibits limitations in reasoning\naround certain probability concepts. The model's ability to deliver\nhigh-quality explanations and illustrate solutions in any programming language,\ncoupled with its performance in solving probability exercises, suggests that\nlarge language models have the potential to serve as learning assistants.", "published": "2023-10-09 12:54:58", "link": "http://arxiv.org/abs/2310.05686v1", "categories": ["cs.CL", "cs.AI", "I2", "I.2"], "primary_category": "cs.CL"}
{"title": "LLMLingua: Compressing Prompts for Accelerated Inference of Large\n  Language Models", "abstract": "Large language models (LLMs) have been applied in various applications due to\ntheir astonishing capabilities. With advancements in technologies such as\nchain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fed\nto LLMs are becoming increasingly lengthy, even exceeding tens of thousands of\ntokens. To accelerate model inference and reduce cost, this paper presents\nLLMLingua, a coarse-to-fine prompt compression method that involves a budget\ncontroller to maintain semantic integrity under high compression ratios, a\ntoken-level iterative compression algorithm to better model the interdependence\nbetween compressed contents, and an instruction tuning based method for\ndistribution alignment between language models. We conduct experiments and\nanalysis over four datasets from different scenarios, i.e., GSM8K, BBH,\nShareGPT, and Arxiv-March23; showing that the proposed approach yields\nstate-of-the-art performance and allows for up to 20x compression with little\nperformance loss. Our code is available at https://aka.ms/LLMLingua.", "published": "2023-10-09 14:10:21", "link": "http://arxiv.org/abs/2310.05736v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and\n  Execution of LLM Agents in an Auction Arena", "abstract": "Recent advancements in Large Language Models (LLMs) showcase advanced\nreasoning, yet NLP evaluations often depend on static benchmarks. Evaluating\nthis necessitates environments that test strategic reasoning in dynamic,\ncompetitive scenarios requiring long-term planning. We introduce AucArena, a\nnovel evaluation suite that simulates auctions, a setting chosen for being\nhighly unpredictable and involving many skills related to resource and risk\nmanagement, while also being easy to evaluate. We conduct controlled\nexperiments using state-of-the-art LLMs to power bidding agents to benchmark\ntheir planning and execution skills. Our research demonstrates that LLMs, such\nas GPT-4, possess key skills for auction participation, such as budget\nmanagement and goal adherence, which improve with adaptive strategies. This\nhighlights LLMs' potential in modeling complex social interactions in\ncompetitive contexts. However, variability in LLM performance and occasional\noutperformance by simpler methods indicate opportunities for further\nadvancements in LLM design and the value of our simulation environment for\nongoing testing and refinement.", "published": "2023-10-09 14:22:09", "link": "http://arxiv.org/abs/2310.05746v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DiffuSeq-v2: Bridging Discrete and Continuous Text Spaces for\n  Accelerated Seq2Seq Diffusion Models", "abstract": "Diffusion models have gained prominence in generating high-quality sequences\nof text. Nevertheless, current approaches predominantly represent discrete text\nwithin a continuous diffusion space, which incurs substantial computational\noverhead during training and results in slower sampling speeds. In this paper,\nwe introduce a soft absorbing state that facilitates the diffusion model in\nlearning to reconstruct discrete mutations based on the underlying Gaussian\nspace, thereby enhancing its capacity to recover conditional signals. During\nthe sampling phase, we employ state-of-the-art ODE solvers within the\ncontinuous space to expedite the sampling process. Comprehensive experimental\nevaluations reveal that our proposed method effectively accelerates the\ntraining convergence by 4x and generates samples of similar quality 800x\nfaster, rendering it significantly closer to practical application.\n\\footnote{The code is released at \\url{https://github.com/Shark-NLP/DiffuSeq}", "published": "2023-10-09 15:29:10", "link": "http://arxiv.org/abs/2310.05793v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "GraphLLM: Boosting Graph Reasoning Ability of Large Language Model", "abstract": "The advancement of Large Language Models (LLMs) has remarkably pushed the\nboundaries towards artificial general intelligence (AGI), with their\nexceptional ability on understanding diverse types of information, including\nbut not limited to images and audio. Despite this progress, a critical gap\nremains in empowering LLMs to proficiently understand and reason on graph data.\nRecent studies underscore LLMs' underwhelming performance on fundamental graph\nreasoning tasks. In this paper, we endeavor to unearth the obstacles that\nimpede LLMs in graph reasoning, pinpointing the common practice of converting\ngraphs into natural language descriptions (Graph2Text) as a fundamental\nbottleneck. To overcome this impediment, we introduce GraphLLM, a pioneering\nend-to-end approach that synergistically integrates graph learning models with\nLLMs. This synergy equips LLMs with the ability to proficiently interpret and\nreason on graph data, harnessing the superior expressive power of graph\nlearning models. Our empirical evaluations across four fundamental graph\nreasoning tasks validate the effectiveness of GraphLLM. The results exhibit a\nsubstantial average accuracy enhancement of 54.44%, alongside a noteworthy\ncontext reduction of 96.45% across various graph reasoning tasks.", "published": "2023-10-09 16:42:00", "link": "http://arxiv.org/abs/2310.05845v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Controllable Chest X-Ray Report Generation from Longitudinal\n  Representations", "abstract": "Radiology reports are detailed text descriptions of the content of medical\nscans. Each report describes the presence/absence and location of relevant\nclinical findings, commonly including comparison with prior exams of the same\npatient to describe how they evolved. Radiology reporting is a time-consuming\nprocess, and scan results are often subject to delays. One strategy to speed up\nreporting is to integrate automated reporting systems, however clinical\ndeployment requires high accuracy and interpretability. Previous approaches to\nautomated radiology reporting generally do not provide the prior study as\ninput, precluding comparison which is required for clinical accuracy in some\ntypes of scans, and offer only unreliable methods of interpretability.\nTherefore, leveraging an existing visual input format of anatomical tokens, we\nintroduce two novel aspects: (1) longitudinal representation learning -- we\ninput the prior scan as an additional input, proposing a method to align,\nconcatenate and fuse the current and prior visual information into a joint\nlongitudinal representation which can be provided to the multimodal report\ngeneration model; (2) sentence-anatomy dropout -- a training strategy for\ncontrollability in which the report generator model is trained to predict only\nsentences from the original report which correspond to the subset of anatomical\nregions given as input. We show through in-depth experiments on the MIMIC-CXR\ndataset how the proposed approach achieves state-of-the-art results while\nenabling anatomy-wise controllable report generation.", "published": "2023-10-09 17:22:58", "link": "http://arxiv.org/abs/2310.05881v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "NEFTune: Noisy Embeddings Improve Instruction Finetuning", "abstract": "We show that language model finetuning can be improved, sometimes\ndramatically, with a simple augmentation. NEFTune adds noise to the embedding\nvectors during training. Standard finetuning of LLaMA-2-7B using Alpaca\nachieves 29.79% on AlpacaEval, which rises to 64.69% using noisy embeddings.\nNEFTune also improves over strong baselines on modern instruction datasets.\nModels trained with Evol-Instruct see a 10% improvement, with ShareGPT an 8%\nimprovement, and with OpenPlatypus an 8% improvement. Even powerful models\nfurther refined with RLHF such as LLaMA-2-Chat benefit from additional training\nwith NEFTune.", "published": "2023-10-09 17:58:34", "link": "http://arxiv.org/abs/2310.05914v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Few-Shot Spoken Language Understanding via Joint Speech-Text Models", "abstract": "Recent work on speech representation models jointly pre-trained with text has\ndemonstrated the potential of improving speech representations by encoding\nspeech and text in a shared space. In this paper, we leverage such shared\nrepresentations to address the persistent challenge of limited data\navailability in spoken language understanding tasks. By employing a pre-trained\nspeech-text model, we find that models fine-tuned on text can be effectively\ntransferred to speech testing data. With as little as 1 hour of labeled speech\ndata, our proposed approach achieves comparable performance on spoken language\nunderstanding tasks (specifically, sentiment analysis and named entity\nrecognition) when compared to previous methods using speech-only pre-trained\nmodels fine-tuned on 10 times more data. Beyond the proof-of-concept study, we\nalso analyze the latent representations. We find that the bottom layers of\nspeech-text models are largely task-agnostic and align speech and text\nrepresentations into a shared space, while the top layers are more\ntask-specific.", "published": "2023-10-09 17:59:21", "link": "http://arxiv.org/abs/2310.05919v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Auditing Gender Analyzers on Text Data", "abstract": "AI models have become extremely popular and accessible to the general public.\nHowever, they are continuously under the scanner due to their demonstrable\nbiases toward various sections of the society like people of color and\nnon-binary people. In this study, we audit three existing gender analyzers --\nuClassify, Readable and HackerFactor, for biases against non-binary\nindividuals. These tools are designed to predict only the cisgender binary\nlabels, which leads to discrimination against non-binary members of the\nsociety. We curate two datasets -- Reddit comments (660k) and, Tumblr posts\n(2.05M) and our experimental evaluation shows that the tools are highly\ninaccurate with the overall accuracy being ~50% on all platforms. Predictions\nfor non-binary comments on all platforms are mostly female, thus propagating\nthe societal bias that non-binary individuals are effeminate. To address this,\nwe fine-tune a BERT multi-label classifier on the two datasets in multiple\ncombinations, observe an overall performance of ~77% on the most realistically\ndeployable setting and a surprisingly higher performance of 90% for the\nnon-binary class. We also audit ChatGPT using zero-shot prompts on a small\ndataset (due to high pricing) and observe an average accuracy of 58% for Reddit\nand Tumblr combined (with overall better results for Reddit).\n  Thus, we show that existing systems, including highly advanced ones like\nChatGPT are biased, and need better audits and moderation and, that such\nsocietal biases can be addressed and alleviated through simple off-the-shelf\nmodels like BERT trained on more gender inclusive datasets.", "published": "2023-10-09 18:13:07", "link": "http://arxiv.org/abs/2310.06061v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "BYOC: Personalized Few-Shot Classification with Co-Authored Class\n  Descriptions", "abstract": "Text classification is a well-studied and versatile building block for many\nNLP applications. Yet, existing approaches require either large annotated\ncorpora to train a model with or, when using large language models as a base,\nrequire carefully crafting the prompt as well as using a long context that can\nfit many examples. As a result, it is not possible for end-users to build\nclassifiers for themselves. To address this issue, we propose a novel approach\nto few-shot text classification using an LLM. Rather than few-shot examples,\nthe LLM is prompted with descriptions of the salient features of each class.\nThese descriptions are coauthored by the user and the LLM interactively: while\nthe user annotates each few-shot example, the LLM asks relevant questions that\nthe user answers. Examples, questions, and answers are summarized to form the\nclassification prompt. Our experiments show that our approach yields high\naccuracy classifiers, within 82% of the performance of models trained with\nsignificantly larger datasets while using only 1% of their training sets.\nAdditionally, in a study with 30 participants, we show that end-users are able\nto build classifiers to suit their specific needs. The personalized classifiers\nshow an average accuracy of 90%, which is 15% higher than the state-of-the-art\napproach.", "published": "2023-10-09 19:37:38", "link": "http://arxiv.org/abs/2310.06111v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CAW-coref: Conjunction-Aware Word-level Coreference Resolution", "abstract": "State-of-the-art coreference resolutions systems depend on multiple LLM calls\nper document and are thus prohibitively expensive for many use cases (e.g.,\ninformation extraction with large corpora). The leading word-level coreference\nsystem (WL-coref) attains 96.6% of these SOTA systems' performance while being\nmuch more efficient. In this work, we identify a routine yet important failure\ncase of WL-coref: dealing with conjoined mentions such as 'Tom and Mary'. We\noffer a simple yet effective solution that improves the performance on the\nOntoNotes test set by 0.9% F1, shrinking the gap between efficient word-level\ncoreference resolution and expensive SOTA approaches by 34.6%. Our\nConjunction-Aware Word-level coreference model (CAW-coref) and code is\navailable at https://github.com/KarelDO/wl-coref.", "published": "2023-10-09 21:32:49", "link": "http://arxiv.org/abs/2310.06165v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Importance of Prompt Tuning for Automated Neuron Explanations", "abstract": "Recent advances have greatly increased the capabilities of large language\nmodels (LLMs), but our understanding of the models and their safety has not\nprogressed as fast. In this paper we aim to understand LLMs deeper by studying\ntheir individual neurons. We build upon previous work showing large language\nmodels such as GPT-4 can be useful in explaining what each neuron in a language\nmodel does. Specifically, we analyze the effect of the prompt used to generate\nexplanations and show that reformatting the explanation prompt in a more\nnatural way can significantly improve neuron explanation quality and greatly\nreduce computational cost. We demonstrate the effects of our new prompts in\nthree different ways, incorporating both automated and human evaluations.", "published": "2023-10-09 23:02:07", "link": "http://arxiv.org/abs/2310.06200v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Estimating Numbers without Regression", "abstract": "Despite recent successes in language models, their ability to represent\nnumbers is insufficient. Humans conceptualize numbers based on their\nmagnitudes, effectively projecting them on a number line; whereas subword\ntokenization fails to explicitly capture magnitude by splitting numbers into\narbitrary chunks. To alleviate this shortcoming, alternative approaches have\nbeen proposed that modify numbers at various stages of the language modeling\npipeline. These methods change either the (1) notation in which numbers are\nwritten (\\eg scientific vs decimal), the (2) vocabulary used to represent\nnumbers or the entire (3) architecture of the underlying language model, to\ndirectly regress to a desired number.\n  Previous work suggests that architectural change helps achieve\nstate-of-the-art on number estimation but we find an insightful ablation:\nchanging the model's vocabulary instead (\\eg introduce a new token for numbers\nin range 10-100) is a far better trade-off. In the context of masked number\nprediction, a carefully designed tokenization scheme is both the simplest to\nimplement and sufficient, \\ie with similar performance to the state-of-the-art\napproach that requires making significant architectural changes. Finally, we\nreport similar trends on the downstream task of numerical fact estimation (for\nFermi Problems) and discuss reasons behind our findings.", "published": "2023-10-09 23:07:05", "link": "http://arxiv.org/abs/2310.06204v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to\n  RLHF", "abstract": "Model alignment with human preferences is an essential step in making Large\nLanguage Models (LLMs) helpful and consistent with human values. It typically\nconsists of supervised fine-tuning (SFT) and reinforcement learning from human\nfeedback (RLHF) stages. However, RLHF faces inherent limitations stemming from\na complex training setup and its tendency to align the model with implicit\nvalues that end users cannot control at run-time. Moreover, reward models in\nRLHF stage commonly rely on single-dimensional feedback as opposed to explicit,\nmultifaceted signals that indicate attributes such as helpfulness, humor, and\ntoxicity. To address these limitations, we propose SteerLM, a supervised\nfine-tuning method that empowers end-users to control responses during\ninference. SteerLM conditions responses to conform to an explicitly defined\nmulti-dimensional set of attributes, thereby empowering a steerable AI capable\nof generating helpful and high-quality responses while maintaining\ncustomizability. Experiments show that SteerLM trained on open source datasets\ngenerates responses that are preferred by human and automatic evaluators to\nmany state-of-the-art baselines trained with RLHF while being much easier to\ntrain. Try SteerLM at https://huggingface.co/nvidia/SteerLM-llama2-13B", "published": "2023-10-09 02:11:21", "link": "http://arxiv.org/abs/2310.05344v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Glance is Enough: Extract Target Sentence By Looking at A keyword", "abstract": "This paper investigates the possibility of extracting a target sentence from\nmulti-talker speech using only a keyword as input. For example, in social\nsecurity applications, the keyword might be \"help\", and the goal is to identify\nwhat the person who called for help is articulating while ignoring other\nspeakers. To address this problem, we propose using the Transformer\narchitecture to embed both the keyword and the speech utterance and then rely\non the cross-attention mechanism to select the correct content from the\nconcatenated or overlapping speech. Experimental results on Librispeech\ndemonstrate that our proposed method can effectively extract target sentences\nfrom very noisy and mixed speech (SNR=-3dB), achieving a phone error rate (PER)\nof 26\\%, compared to the baseline system's PER of 96%.", "published": "2023-10-09 02:28:19", "link": "http://arxiv.org/abs/2310.05352v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Improving End-to-End Speech Processing by Efficient Text Data\n  Utilization with Latent Synthesis", "abstract": "Training a high performance end-to-end speech (E2E) processing model requires\nan enormous amount of labeled speech data, especially in the era of\ndata-centric artificial intelligence. However, labeled speech data are usually\nscarcer and more expensive for collection, compared to textual data. We propose\nLatent Synthesis (LaSyn), an efficient textual data utilization framework for\nE2E speech processing models. We train a latent synthesizer to convert textual\ndata into an intermediate latent representation of a pre-trained speech model.\nThese pseudo acoustic representations of textual data augment acoustic data for\nmodel training. We evaluate LaSyn on low-resource automatic speech recognition\n(ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an\nE2E baseline trained on LibriSpeech train-clean-100, with relative word error\nrate reductions over 22.3% on different test sets. For SLU, LaSyn improves our\nE2E baseline by absolute 4.1% for intent classification accuracy and 3.8% for\nslot filling SLU-F1 on SLURP, and absolute 4.49% and 2.25% for exact match (EM)\nand EM-Tree accuracies on STOP respectively. With fewer parameters, the results\nof LaSyn are competitive to published state-of-the-art works. The results\ndemonstrate the quality of the augmented training data.", "published": "2023-10-09 03:10:49", "link": "http://arxiv.org/abs/2310.05374v3", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "CCAE: A Corpus of Chinese-based Asian Englishes", "abstract": "Language models have been foundations in various scenarios of NLP\napplications, but it has not been well applied in language variety studies,\neven for the most popular language like English. This paper represents one of\nthe few initial efforts to utilize the NLP technology in the paradigm of World\nEnglishes, specifically in creating a multi-variety corpus for studying Asian\nEnglishes. We present an overview of the CCAE -- Corpus of Chinese-based Asian\nEnglish, a suite of corpora comprising six Chinese-based Asian English\nvarieties. It is based on 340 million tokens in 448 thousand web documents from\nsix regions. The ontology of data would make the corpus a helpful resource with\nenormous research potential for Asian Englishes (especially for Chinese\nEnglishes for which there has not been a publicly accessible corpus yet so far)\nand an ideal source for variety-specific language modeling and downstream\ntasks, thus setting the stage for NLP-based World Englishes studies. And\npreliminary experiments on this corpus reveal the practical value of CCAE.\nFinally, we make CCAE available at\n\\href{https://huggingface.co/datasets/CCAE/CCAE-Corpus}{this https URL}.", "published": "2023-10-09 03:34:15", "link": "http://arxiv.org/abs/2310.05381v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Humanoid Agents: Platform for Simulating Human-like Generative Agents", "abstract": "Just as computational simulations of atoms, molecules and cells have shaped\nthe way we study the sciences, true-to-life simulations of human-like agents\ncan be valuable tools for studying human behavior. We propose Humanoid Agents,\na system that guides Generative Agents to behave more like humans by\nintroducing three elements of System 1 processing: Basic needs (e.g. hunger,\nhealth and energy), Emotion and Closeness in Relationships. Humanoid Agents are\nable to use these dynamic elements to adapt their daily activities and\nconversations with other agents, as supported with empirical experiments. Our\nsystem is designed to be extensible to various settings, three of which we\ndemonstrate, as well as to other elements influencing human behavior (e.g.\nempathy, moral values and cultural background). Our platform also includes a\nUnity WebGL game interface for visualization and an interactive analytics\ndashboard to show agent statuses over time. Our platform is available on\nhttps://www.humanoidagents.com/ and code is on\nhttps://github.com/HumanoidAgents/HumanoidAgents", "published": "2023-10-09 05:30:42", "link": "http://arxiv.org/abs/2310.05418v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Automating Customer Service using LangChain: Building custom open-source\n  GPT Chatbot for organizations", "abstract": "In the digital age, the dynamics of customer service are evolving, driven by\ntechnological advancements and the integration of Large Language Models (LLMs).\nThis research paper introduces a groundbreaking approach to automating customer\nservice using LangChain, a custom LLM tailored for organizations. The paper\nexplores the obsolescence of traditional customer support techniques,\nparticularly Frequently Asked Questions (FAQs), and proposes a paradigm shift\ntowards responsive, context-aware, and personalized customer interactions. The\nheart of this innovation lies in the fusion of open-source methodologies, web\nscraping, fine-tuning, and the seamless integration of LangChain into customer\nservice platforms. This open-source state-of-the-art framework, presented as\n\"Sahaay,\" demonstrates the ability to scale across industries and\norganizations, offering real-time support and query resolution. Key elements of\nthis research encompass data collection via web scraping, the role of\nembeddings, the utilization of Google's Flan T5 XXL, Base and Small language\nmodels for knowledge retrieval, and the integration of the chatbot into\ncustomer service platforms. The results section provides insights into their\nperformance and use cases, here particularly within an educational institution.\nThis research heralds a new era in customer service, where technology is\nharnessed to create efficient, personalized, and responsive interactions.\nSahaay, powered by LangChain, redefines the customer-company relationship,\nelevating customer retention, value extraction, and brand image. As\norganizations embrace LLMs, customer service becomes a dynamic and\ncustomer-centric ecosystem.", "published": "2023-10-09 05:35:10", "link": "http://arxiv.org/abs/2310.05421v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "IDTraffickers: An Authorship Attribution Dataset to link and connect\n  Potential Human-Trafficking Operations on Text Escort Advertisements", "abstract": "Human trafficking (HT) is a pervasive global issue affecting vulnerable\nindividuals, violating their fundamental human rights. Investigations reveal\nthat a significant number of HT cases are associated with online advertisements\n(ads), particularly in escort markets. Consequently, identifying and connecting\nHT vendors has become increasingly challenging for Law Enforcement Agencies\n(LEAs). To address this issue, we introduce IDTraffickers, an extensive dataset\nconsisting of 87,595 text ads and 5,244 vendor labels to enable the\nverification and identification of potential HT vendors on online escort\nmarkets. To establish a benchmark for authorship identification, we train a\nDeCLUTR-small model, achieving a macro-F1 score of 0.8656 in a closed-set\nclassification environment. Next, we leverage the style representations\nextracted from the trained classifier to conduct authorship verification,\nresulting in a mean r-precision score of 0.8852 in an open-set ranking\nenvironment. Finally, to encourage further research and ensure responsible data\nsharing, we plan to release IDTraffickers for the authorship attribution task\nto researchers under specific conditions, considering the sensitive nature of\nthe data. We believe that the availability of our dataset and benchmarks will\nempower future researchers to utilize our findings, thereby facilitating the\neffective linkage of escort ads and the development of more robust approaches\nfor identifying HT indicators.", "published": "2023-10-09 07:43:57", "link": "http://arxiv.org/abs/2310.05484v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How Abilities in Large Language Models are Affected by Supervised\n  Fine-tuning Data Composition", "abstract": "Large language models (LLMs) with enormous pre-training tokens and parameters\nemerge diverse abilities, including math reasoning, code generation, and\ninstruction following. These abilities are further enhanced by supervised\nfine-tuning (SFT). While the open-source community has explored ad-hoc SFT for\nenhancing individual capabilities, proprietary LLMs exhibit versatility across\nvarious skills. Therefore, understanding the facilitation of multiple abilities\nvia SFT is paramount. In this study, we specifically focuses on the interplay\nof data composition between mathematical reasoning, code generation, and\ngeneral human-aligning abilities during SFT. We propose four intriguing\nresearch questions to explore the association between model performance and\nvarious factors including data amount, composition ratio, model size and SFT\nstrategies. Our experiments reveal that distinct capabilities scale differently\nand larger models generally show superior performance with same amount of data.\nMathematical reasoning and code generation consistently improve with increasing\ndata amount, whereas general abilities plateau after roughly a thousand\nsamples. Moreover, we observe data composition appears to enhance various\nabilities under limited data conditions, yet can lead to performance conflicts\nwhen data is plentiful. Our findings also suggest the amount of composition\ndata influences performance more than the composition ratio. In analysis of SFT\nstrategies, we find that sequentially learning multiple skills risks\ncatastrophic forgetting. Our proposed Dual-stage Mixed Fine-tuning (DMT)\nstrategy offers a promising solution to learn multiple abilities with different\nscaling patterns.", "published": "2023-10-09 07:56:16", "link": "http://arxiv.org/abs/2310.05492v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MuggleMath: Assessing the Impact of Query and Response Augmentation on\n  Math Reasoning", "abstract": "In math reasoning with large language models (LLMs), fine-tuning data\naugmentation by query evolution and diverse reasoning paths is empirically\nverified effective, profoundly narrowing the gap between open-sourced LLMs and\ncutting-edge proprietary LLMs. In this paper, we conduct an investigation for\nsuch data augmentation in math reasoning and are intended to answer: (1) What\nstrategies of data augmentation are more effective; (2) What is the scaling\nrelationship between the amount of augmented data and model performance; and\n(3) Can data augmentation incentivize generalization to out-of-domain\nmathematical reasoning tasks? To this end, we create two new dataset AugGSM8K\nand AugMATH, by complicating and diversifying the queries and sampling multiple\nreasoning paths from GSM8K and MATH. We obtained a series of LLMs called\nMuggleMath by fine-tuning LLaMA models on AugGSM8K and AugMATH. MuggleMath\nsubstantially achieves new state-of-the-art on GSM8K and MATH. A log-linear\nrelationship and a segmented log-linear are presented between MuggleMath's\nperformance and the amount of augmented data on GSM8K and MATH, respectively.\nWe also find that it is weak in out-of-domain math reasoning generalization\nfrom AugGSM8K to MATH and from AugMATH to GSM8K, which suggests that augmenting\nqueries that cover a broader range of subjects is more beneficial for\ngeneralization. We release our codes and augmented data in\nhttps://github.com/OFA-Sys/gsm8k-ScRel.", "published": "2023-10-09 08:18:58", "link": "http://arxiv.org/abs/2310.05506v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Findings of the 2023 ML-SUPERB Challenge: Pre-Training and Evaluation\n  over More Languages and Beyond", "abstract": "The 2023 Multilingual Speech Universal Performance Benchmark (ML-SUPERB)\nChallenge expands upon the acclaimed SUPERB framework, emphasizing\nself-supervised models in multilingual speech recognition and language\nidentification. The challenge comprises a research track focused on applying\nML-SUPERB to specific multilingual subjects, a Challenge Track for model\nsubmissions, and a New Language Track where language resource researchers can\ncontribute and evaluate their low-resource language data in the context of the\nlatest progress in multilingual speech recognition. The challenge garnered 12\nmodel submissions and 54 language corpora, resulting in a comprehensive\nbenchmark encompassing 154 languages. The findings indicate that merely scaling\nmodels is not the definitive solution for multilingual speech tasks, and a\nvariety of speech/voice types present significant challenges in multilingual\nspeech processing.", "published": "2023-10-09 08:30:01", "link": "http://arxiv.org/abs/2310.05513v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "InterroLang: Exploring NLP Models and Datasets through Dialogue-based\n  Explanations", "abstract": "While recently developed NLP explainability methods let us open the black box\nin various ways (Madsen et al., 2022), a missing ingredient in this endeavor is\nan interactive tool offering a conversational interface. Such a dialogue system\ncan help users explore datasets and models with explanations in a\ncontextualized manner, e.g. via clarification or follow-up questions, and\nthrough a natural language interface. We adapt the conversational explanation\nframework TalkToModel (Slack et al., 2022) to the NLP domain, add new\nNLP-specific operations such as free-text rationalization, and illustrate its\ngeneralizability on three NLP tasks (dialogue act classification, question\nanswering, hate speech detection). To recognize user queries for explanations,\nwe evaluate fine-tuned and few-shot prompting models and implement a novel\nAdapter-based approach. We then conduct two user studies on (1) the perceived\ncorrectness and helpfulness of the dialogues, and (2) the simulatability, i.e.\nhow objectively helpful dialogical explanations are for humans in figuring out\nthe model's predicted label when it's not shown. We found rationalization and\nfeature attribution were helpful in explaining the model behavior. Moreover,\nusers could more reliably predict the model outcome based on an explanation\ndialogue rather than one-off explanations.", "published": "2023-10-09 10:27:26", "link": "http://arxiv.org/abs/2310.05592v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Integrating Stock Features and Global Information via Large Language\n  Models for Enhanced Stock Return Prediction", "abstract": "The remarkable achievements and rapid advancements of Large Language Models\n(LLMs) such as ChatGPT and GPT-4 have showcased their immense potential in\nquantitative investment. Traders can effectively leverage these LLMs to analyze\nfinancial news and predict stock returns accurately. However, integrating LLMs\ninto existing quantitative models presents two primary challenges: the\ninsufficient utilization of semantic information embedded within LLMs and the\ndifficulties in aligning the latent information within LLMs with pre-existing\nquantitative stock features. We propose a novel framework consisting of two\ncomponents to surmount these challenges. The first component, the Local-Global\n(LG) model, introduces three distinct strategies for modeling global\ninformation. These approaches are grounded respectively on stock features, the\ncapabilities of LLMs, and a hybrid method combining the two paradigms. The\nsecond component, Self-Correlated Reinforcement Learning (SCRL), focuses on\naligning the embeddings of financial news generated by LLMs with stock features\nwithin the same semantic space. By implementing our framework, we have\ndemonstrated superior performance in Rank Information Coefficient and returns,\nparticularly compared to models relying only on stock features in the China\nA-share market.", "published": "2023-10-09 11:34:18", "link": "http://arxiv.org/abs/2310.05627v1", "categories": ["cs.CL", "cs.LG", "q-fin.ST"], "primary_category": "cs.CL"}
{"title": "Glitter or Gold? Deriving Structured Insights from Sustainability\n  Reports via Large Language Models", "abstract": "Over the last decade, several regulatory bodies have started requiring the\ndisclosure of non-financial information from publicly listed companies, in\nlight of the investors' increasing attention to Environmental, Social, and\nGovernance (ESG) issues. Publicly released information on sustainability\npractices is often disclosed in diverse, unstructured, and multi-modal\ndocumentation. This poses a challenge in efficiently gathering and aligning the\ndata into a unified framework to derive insights related to Corporate Social\nResponsibility (CSR). Thus, using Information Extraction (IE) methods becomes\nan intuitive choice for delivering insightful and actionable data to\nstakeholders. In this study, we employ Large Language Models (LLMs), In-Context\nLearning, and the Retrieval-Augmented Generation (RAG) paradigm to extract\nstructured insights related to ESG aspects from companies' sustainability\nreports. We then leverage graph-based representations to conduct statistical\nanalyses concerning the extracted insights. These analyses revealed that ESG\ncriteria cover a wide range of topics, exceeding 500, often beyond those\nconsidered in existing categorizations, and are addressed by companies through\na variety of initiatives. Moreover, disclosure similarities emerged among\ncompanies from the same region or sector, validating ongoing hypotheses in the\nESG literature. Lastly, by incorporating additional company attributes into our\nanalyses, we investigated which factors impact the most on companies' ESG\nratings, showing that ESG disclosure affects the obtained ratings more than\nother financial or company data.", "published": "2023-10-09 11:34:41", "link": "http://arxiv.org/abs/2310.05628v3", "categories": ["cs.CL", "cs.CE", "cs.CY"], "primary_category": "cs.CL"}
{"title": "An Attribution Method for Siamese Encoders", "abstract": "Despite the success of Siamese encoder models such as sentence transformers\n(ST), little is known about the aspects of inputs they pay attention to. A\nbarrier is that their predictions cannot be attributed to individual features,\nas they compare two inputs rather than processing a single one. This paper\nderives a local attribution method for Siamese encoders by generalizing the\nprinciple of integrated gradients to models with multiple inputs. The solution\ntakes the form of feature-pair attributions, and can be reduced to a\ntoken-token matrix for STs. Our method involves the introduction of integrated\nJacobians and inherits the advantageous formal properties of integrated\ngradients: it accounts for the model's full computation graph and is guaranteed\nto converge to the actual prediction. A pilot study shows that in an ST few\ntoken-pairs can often explain large fractions of predictions, and it focuses on\nnouns and verbs. For accurate predictions, it however needs to attend to the\nmajority of tokens and parts of speech.", "published": "2023-10-09 13:24:44", "link": "http://arxiv.org/abs/2310.05703v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Guiding Language Model Reasoning with Planning Tokens", "abstract": "Large language models (LLMs) have recently attracted considerable interest\nfor their ability to perform complex reasoning tasks, such as chain-of-thought\n(CoT) reasoning. However, most of the existing approaches to enhance this\nability rely heavily on data-driven methods, while neglecting the structural\naspects of the model's reasoning capacity. To encourage a more structural\ngeneration of CoT steps, we propose a hierarchical generation scheme: we let\nthe LM generate a planning token at the start of each reasoning step,\nintuitively serving as a high-level plan of the current step, and add their\nembeddings to the model parameters. Our approach requires a negligible increase\nin trainable parameters (0.001%) and can be applied through either full\nfine-tuning or a more parameter-efficient scheme. We demonstrate our method's\neffectiveness by applying it to three different LLMs, showing notable accuracy\nimprovements across three math word problem datasets and one multihop QA\ndataset with respect to standard fine-tuning baselines.", "published": "2023-10-09 13:29:37", "link": "http://arxiv.org/abs/2310.05707v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Program Testing Ability of Large Language Models for Code", "abstract": "Recent development of large language models (LLMs) for code like CodeX and\nCodeT5+ demonstrates tremendous promise in achieving code intelligence. Their\nability of synthesizing code that completes a program for performing a\npre-defined task has been intensively tested and verified on benchmark datasets\nincluding HumanEval and MBPP. Yet, evaluation of these LLMs from more\nperspectives (than just program synthesis) is also anticipated, considering\ntheir broad scope of applications in software engineering. In this paper, we\nexplore the ability of LLMs for testing programs/code. By performing thorough\nanalyses of recent LLMs for code in program testing, we show a series of\nintriguing properties of these models and demonstrate how program testing\nability of LLMs can be improved. Following recent work which utilizes generated\ntest cases to enhance program synthesis, we further leverage our findings in\nimproving the quality of the synthesized programs and show +11.77% and +4.22%\nhigher code pass rates on HumanEval+ comparing with the GPT-3.5-turbo baseline\nand the recent state-of-the-art, respectively.", "published": "2023-10-09 13:55:45", "link": "http://arxiv.org/abs/2310.05727v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Why Should This Article Be Deleted? Transparent Stance Detection in\n  Multilingual Wikipedia Editor Discussions", "abstract": "The moderation of content on online platforms is usually non-transparent. On\nWikipedia, however, this discussion is carried out publicly and the editors are\nencouraged to use the content moderation policies as explanations for making\nmoderation decisions. Currently, only a few comments explicitly mention those\npolicies -- 20% of the English ones, but as few as 2% of the German and Turkish\ncomments. To aid in this process of understanding how content is moderated, we\nconstruct a novel multilingual dataset of Wikipedia editor discussions along\nwith their reasoning in three languages. The dataset contains the stances of\nthe editors (keep, delete, merge, comment), along with the stated reason, and a\ncontent moderation policy, for each edit decision. We demonstrate that stance\nand corresponding reason (policy) can be predicted jointly with a high degree\nof accuracy, adding transparency to the decision-making process. We release\nboth our joint prediction models and the multilingual content moderation\ndataset for further research on automated transparent content moderation.", "published": "2023-10-09 15:11:02", "link": "http://arxiv.org/abs/2310.05779v2", "categories": ["cs.LG", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "In-Context Explainers: Harnessing LLMs for Explaining Black Box Models", "abstract": "Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional capabilities in complex tasks like machine translation, commonsense\nreasoning, and language understanding. One of the primary reasons for the\nadaptability of LLMs in such diverse tasks is their in-context learning (ICL)\ncapability, which allows them to perform well on new tasks by simply using a\nfew task samples in the prompt. Despite their effectiveness in enhancing the\nperformance of LLMs on diverse language and tabular tasks, these methods have\nnot been thoroughly explored for their potential to generate post hoc\nexplanations. In this work, we carry out one of the first explorations to\nanalyze the effectiveness of LLMs in explaining other complex predictive models\nusing ICL. To this end, we propose a novel framework, In-Context Explainers,\ncomprising of three novel approaches that exploit the ICL capabilities of LLMs\nto explain the predictions made by other predictive models. We conduct\nextensive analysis with these approaches on real-world tabular and text\ndatasets and demonstrate that LLMs are capable of explaining other predictive\nmodels similar to state-of-the-art post hoc explainers, opening up promising\navenues for future research into LLM-based post hoc explanations of complex\npredictive models.", "published": "2023-10-09 15:31:03", "link": "http://arxiv.org/abs/2310.05797v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Language-guided Adaptive Hyper-modality Representation for\n  Multimodal Sentiment Analysis", "abstract": "Though Multimodal Sentiment Analysis (MSA) proves effective by utilizing rich\ninformation from multiple sources (e.g., language, video, and audio), the\npotential sentiment-irrelevant and conflicting information across modalities\nmay hinder the performance from being further improved. To alleviate this, we\npresent Adaptive Language-guided Multimodal Transformer (ALMT), which\nincorporates an Adaptive Hyper-modality Learning (AHL) module to learn an\nirrelevance/conflict-suppressing representation from visual and audio features\nunder the guidance of language features at different scales. With the obtained\nhyper-modality representation, the model can obtain a complementary and joint\nrepresentation through multimodal fusion for effective MSA. In practice, ALMT\nachieves state-of-the-art performance on several popular datasets (e.g., MOSI,\nMOSEI and CH-SIMS) and an abundance of ablation demonstrates the validity and\nnecessity of our irrelevance/conflict suppression mechanism.", "published": "2023-10-09 15:43:07", "link": "http://arxiv.org/abs/2310.05804v2", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.AI"}
{"title": "Improving Summarization with Human Edits", "abstract": "Recent work has shown the promise of learning with human feedback paradigms\nto produce human-determined high-quality text. Existing works use human\nfeedback to train large language models (LLMs) in general domain abstractive\nsummarization and have obtained summary quality exceeding traditional\nlikelihood training. In this paper, we focus on a less explored form of human\nfeedback -- Human Edits. We propose Sequence Alignment (un)Likelihood Training\n(SALT), a novel technique to use both the human-edited and model-generated data\ntogether in the training loop. In addition, we demonstrate simulating Human\nEdits with ground truth summaries coming from existing training data --\nImitation edits, along with the model-generated summaries obtained after the\ntraining, to reduce the need for expensive human-edit data. In our experiments,\nwe extend human feedback exploration from general domain summarization to\nmedical domain summarization. Our results demonstrate the effectiveness of SALT\nin improving the summary quality with Human and Imitation Edits. Through\nadditional experiments, we show that SALT outperforms the conventional RLHF\nmethod (designed for human preferences) -- DPO, when applied to human-edit\ndata. We hope the evidence in our paper prompts researchers to explore,\ncollect, and better use different human feedback approaches scalably.", "published": "2023-10-09 16:52:07", "link": "http://arxiv.org/abs/2310.05857v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Rephrase, Augment, Reason: Visual Grounding of Questions for\n  Vision-Language Models", "abstract": "An increasing number of vision-language tasks can be handled with little to\nno training, i.e., in a zero and few-shot manner, by marrying large language\nmodels (LLMs) to vision encoders, resulting in large vision-language models\n(LVLMs). While this has huge upsides, such as not requiring training data or\ncustom architectures, how an input is presented to an LVLM can have a major\nimpact on zero-shot model performance. In particular, inputs phrased in an\nunderspecified way can result in incorrect answers due to factors like missing\nvisual information, complex implicit reasoning, or linguistic ambiguity.\nTherefore, adding visually-grounded information to the input as a preemptive\nclarification should improve model performance by reducing underspecification,\ne.g., by localizing objects and disambiguating references. Similarly, in the\nVQA setting, changing the way questions are framed can make them easier for\nmodels to answer. To this end, we present Rephrase, Augment and Reason\n(RepARe), a gradient-free framework that extracts salient details about the\nimage using the underlying LVLM as a captioner and reasoner, in order to\npropose modifications to the original question. We then use the LVLM's\nconfidence over a generated answer as an unsupervised scoring function to\nselect the rephrased question most likely to improve zero-shot performance.\nFocusing on three visual question answering tasks, we show that RepARe can\nresult in a 3.85% (absolute) increase in zero-shot accuracy on VQAv2, 6.41%,\nand 7.94% points increase on A-OKVQA, and VizWiz respectively. Additionally, we\nfind that using gold answers for oracle question candidate selection achieves a\nsubstantial gain in VQA accuracy by up to 14.41%. Through extensive analysis,\nwe demonstrate that outputs from RepARe increase syntactic complexity, and\neffectively utilize vision-language interaction and the frozen LLM.", "published": "2023-10-09 16:57:57", "link": "http://arxiv.org/abs/2310.05861v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ViCor: Bridging Visual Understanding and Commonsense Reasoning with\n  Large Language Models", "abstract": "In our work, we explore the synergistic capabilities of pre-trained\nvision-and-language models (VLMs) and large language models (LLMs) on visual\ncommonsense reasoning (VCR) problems. We find that VLMs and LLMs-based decision\npipelines are good at different kinds of VCR problems. Pre-trained VLMs exhibit\nstrong performance for problems involving understanding the literal visual\ncontent, which we noted as visual commonsense understanding (VCU). For problems\nwhere the goal is to infer conclusions beyond image content, which we noted as\nvisual commonsense inference (VCI), VLMs face difficulties, while LLMs, given\nsufficient visual evidence, can use commonsense to infer the answer well. We\nempirically validate this by letting LLMs classify VCR problems into these two\ncategories and show the significant difference between VLM and LLM with image\ncaption decision pipelines on two subproblems. Moreover, we identify a\nchallenge with VLMs' passive perception, which may miss crucial context\ninformation, leading to incorrect reasoning by LLMs. Based on these, we suggest\na collaborative approach, named ViCor, where pre-trained LLMs serve as problem\nclassifiers to analyze the problem category, then either use VLMs to answer the\nquestion directly or actively instruct VLMs to concentrate on and gather\nrelevant visual elements to support potential commonsense inferences. We\nevaluate our framework on two VCR benchmark datasets and outperform all other\nmethods that do not require in-domain fine-tuning.", "published": "2023-10-09 17:10:35", "link": "http://arxiv.org/abs/2310.05872v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A Meta-Learning Perspective on Transformers for Causal Language Modeling", "abstract": "The Transformer architecture has become prominent in developing large causal\nlanguage models. However, mechanisms to explain its capabilities are not well\nunderstood. Focused on the training process, here we establish a meta-learning\nview of the Transformer architecture when trained for the causal language\nmodeling task, by explicating an inner optimization process within the\nTransformer. Further, within the inner optimization, we discover and\ntheoretically analyze a special characteristic of the norms of learned token\nrepresentations within Transformer-based causal language models. Our analysis\nis supported by experiments in various settings.", "published": "2023-10-09 17:27:36", "link": "http://arxiv.org/abs/2310.05884v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SALMON: Self-Alignment with Instructable Reward Models", "abstract": "Supervised Fine-Tuning (SFT) on response demonstrations combined with\nReinforcement Learning from Human Feedback (RLHF) constitutes a powerful\nparadigm for aligning LLM-based AI agents. However, a significant limitation of\nsuch an approach is its dependency on high-quality human annotations, making\nits application to intricate tasks challenging due to difficulties in obtaining\nconsistent response demonstrations and in-distribution response preferences.\nThis paper presents a novel approach, namely SALMON, to align base language\nmodels with minimal human supervision, using only a small set of human-defined\nprinciples, yet achieving superior performance. Central to our approach is an\ninstructable reward model. Trained on synthetic preference data, this model can\ngenerate reward scores based on arbitrary human-defined principles. By merely\nadjusting these principles during the RL training phase, we gain full control\nover the preferences with the instructable reward model, subsequently\ninfluencing the behavior of the RL-trained policy models, and reducing the\nreliance on the collection of online human preferences. Applying our method to\nthe LLaMA-2-70b base language model, we developed an AI assistant named\nDromedary-2. With only 6 exemplars for in-context learning and 31 human-defined\nprinciples, Dromedary-2 significantly surpasses the performance of several\nstate-of-the-art AI systems, including LLaMA-2-Chat-70b, on various benchmark\ndatasets. We have open-sourced the code and model weights to encourage further\nresearch into aligning LLM-based AI agents with enhanced supervision\nefficiency, improved controllability, and scalable oversight.", "published": "2023-10-09 17:56:53", "link": "http://arxiv.org/abs/2310.05910v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FireAct: Toward Language Agent Fine-tuning", "abstract": "Recent efforts have augmented language models (LMs) with external tools or\nenvironments, leading to the development of language agents that can reason and\nact. However, most of these agents rely on few-shot prompting techniques with\noff-the-shelf LMs. In this paper, we investigate and argue for the overlooked\ndirection of fine-tuning LMs to obtain language agents. Using a setup of\nquestion answering (QA) with a Google search API, we explore a variety of base\nLMs, prompting methods, fine-tuning data, and QA tasks, and find language\nagents are consistently improved after fine-tuning their backbone LMs. For\nexample, fine-tuning Llama2-7B with 500 agent trajectories generated by GPT-4\nleads to a 77% HotpotQA performance increase. Furthermore, we propose FireAct,\na novel approach to fine-tuning LMs with trajectories from multiple tasks and\nprompting methods, and show having more diverse fine-tuning data can further\nimprove agents. Along with other findings regarding scaling effects,\nrobustness, generalization, efficiency and cost, our work establishes\ncomprehensive benefits of fine-tuning LMs for agents, and provides an initial\nset of experimental designs, insights, as well as open questions toward\nlanguage agent fine-tuning.", "published": "2023-10-09 17:58:38", "link": "http://arxiv.org/abs/2310.05915v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLM for SoC Security: A Paradigm Shift", "abstract": "As the ubiquity and complexity of system-on-chip (SoC) designs increase\nacross electronic devices, the task of incorporating security into an SoC\ndesign flow poses significant challenges. Existing security solutions are\ninadequate to provide effective verification of modern SoC designs due to their\nlimitations in scalability, comprehensiveness, and adaptability. On the other\nhand, Large Language Models (LLMs) are celebrated for their remarkable success\nin natural language understanding, advanced reasoning, and program synthesis\ntasks. Recognizing an opportunity, our research delves into leveraging the\nemergent capabilities of Generative Pre-trained Transformers (GPTs) to address\nthe existing gaps in SoC security, aiming for a more efficient, scalable, and\nadaptable methodology. By integrating LLMs into the SoC security verification\nparadigm, we open a new frontier of possibilities and challenges to ensure the\nsecurity of increasingly complex SoCs. This paper offers an in-depth analysis\nof existing works, showcases practical case studies, demonstrates comprehensive\nexperiments, and provides useful promoting guidelines. We also present the\nachievements, prospects, and challenges of employing LLM in different SoC\nsecurity verification tasks.", "published": "2023-10-09 18:02:38", "link": "http://arxiv.org/abs/2310.06046v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Leveraging Multilingual Self-Supervised Pretrained Models for\n  Sequence-to-Sequence End-to-End Spoken Language Understanding", "abstract": "A number of methods have been proposed for End-to-End Spoken Language\nUnderstanding (E2E-SLU) using pretrained models, however their evaluation often\nlacks multilingual setup and tasks that require prediction of lexical fillers,\nsuch as slot filling. In this work, we propose a unified method that integrates\nmultilingual pretrained speech and text models and performs E2E-SLU on six\ndatasets in four languages in a generative manner, including the prediction of\nlexical fillers. We investigate how the proposed method can be improved by\npretraining on widely available speech recognition data using several training\nobjectives. Pretraining on 7000 hours of multilingual data allows us to\noutperform the state-of-the-art ultimately on two SLU datasets and partly on\ntwo more SLU datasets. Finally, we examine the cross-lingual capabilities of\nthe proposed model and improve on the best known result on the\nPortMEDIA-Language dataset by almost half, achieving a Concept/Value Error Rate\nof 23.65%.", "published": "2023-10-09 19:22:51", "link": "http://arxiv.org/abs/2310.06103v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language\n  Models", "abstract": "We present Step-Back Prompting, a simple prompting technique that enables\nLLMs to do abstractions to derive high-level concepts and first principles from\ninstances containing specific details. Using the concepts and principles to\nguide reasoning, LLMs significantly improve their abilities in following a\ncorrect reasoning path towards the solution. We conduct experiments of\nStep-Back Prompting with PaLM-2L, GPT-4 and Llama2-70B models, and observe\nsubstantial performance gains on various challenging reasoning-intensive tasks\nincluding STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back\nPrompting improves PaLM-2L performance on MMLU (Physics and Chemistry) by 7%\nand 11% respectively, TimeQA by 27%, and MuSiQue by 7%.", "published": "2023-10-09 19:48:55", "link": "http://arxiv.org/abs/2310.06117v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "An Initial Investigation of Neural Replay Simulator for Over-the-Air\n  Adversarial Perturbations to Automatic Speaker Verification", "abstract": "Deep Learning has advanced Automatic Speaker Verification (ASV) in the past\nfew years. Although it is known that deep learning-based ASV systems are\nvulnerable to adversarial examples in digital access, there are few studies on\nadversarial attacks in the context of physical access, where a replay process\n(i.e., over the air) is involved. An over-the-air attack involves a\nloudspeaker, a microphone, and a replaying environment that impacts the\nmovement of the sound wave. Our initial experiment confirms that the replay\nprocess impacts the effectiveness of the over-the-air attack performance. This\nstudy performs an initial investigation towards utilizing a neural replay\nsimulator to improve over-the-air adversarial attack robustness. This is\nachieved by using a neural waveform synthesizer to simulate the replay process\nwhen estimating the adversarial perturbations. Experiments conducted on the\nASVspoof2019 dataset confirm that the neural replay simulator can considerably\nincrease the success rates of over-the-air adversarial attacks. This raises the\nconcern for adversarial attacks on speaker verification in physical access\napplications.", "published": "2023-10-09 02:31:05", "link": "http://arxiv.org/abs/2310.05354v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AdvSV: An Over-the-Air Adversarial Attack Dataset for Speaker\n  Verification", "abstract": "It is known that deep neural networks are vulnerable to adversarial attacks.\nAlthough Automatic Speaker Verification (ASV) built on top of deep neural\nnetworks exhibits robust performance in controlled scenarios, many studies\nconfirm that ASV is vulnerable to adversarial attacks. The lack of a standard\ndataset is a bottleneck for further research, especially reproducible research.\nIn this study, we developed an open-source adversarial attack dataset for\nspeaker verification research. As an initial step, we focused on the\nover-the-air attack. An over-the-air adversarial attack involves a perturbation\ngeneration algorithm, a loudspeaker, a microphone, and an acoustic environment.\nThe variations in the recording configurations make it very challenging to\nreproduce previous research. The AdvSV dataset is constructed using the\nVoxceleb1 Verification test set as its foundation. This dataset employs\nrepresentative ASV models subjected to adversarial attacks and records\nadversarial samples to simulate over-the-air attack settings. The scope of the\ndataset can be easily extended to include more types of adversarial attacks.\nThe dataset will be released to the public under the CC BY-SA 4.0. In addition,\nwe also provide a detection baseline for reproducible research.", "published": "2023-10-09 02:58:52", "link": "http://arxiv.org/abs/2310.05369v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Thech. Report: Genuinization of Speech waveform PMF for speaker\n  detection spoofing and countermeasures", "abstract": "In the context of spoofing attacks in speaker recognition systems, we\nobserved that the waveform probability mass function (PMF) of genuine speech\ndiffers significantly from the PMF of speech resulting from the attacks. This\nis true for synthesized or converted speech as well as replayed speech. We also\nnoticed that this observation seems to have a significant impact on spoofing\ndetection performance. In this article, we propose an algorithm, denoted\ngenuinization, capable of reducing the waveform distribution gap between\nauthentic speech and spoofing speech. Our genuinization algorithm is evaluated\non ASVspoof 2019 challenge datasets, using the baseline system provided by the\nchallenge organization. We first assess the influence of genuinization on\nspoofing performance. Using genuinization for the spoofing attacks degrades\nspoofing detection performance by up to a factor of 10. Next, we integrate the\ngenuinization algorithm in the spoofing countermeasures and we observe a huge\nspoofing detection improvement in different cases. The results of our\nexperiments show clearly that waveform distribution plays an important role and\nmust be taken into account by anti-spoofing systems.", "published": "2023-10-09 08:56:31", "link": "http://arxiv.org/abs/2310.05534v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Super Denoise Net: Speech Super Resolution with Noise Cancellation in\n  Low Sampling Rate Noisy Environments", "abstract": "Speech super-resolution (SSR) aims to predict a high resolution (HR) speech\nsignal from its low resolution (LR) corresponding part. Most neural SSR models\nfocus on producing the final result in a noise-free environment by recovering\nthe spectrogram of high-frequency part of the signal and concatenating it with\nthe original low-frequency part. Although these methods achieve high accuracy,\nthey become less effective when facing the real-world scenario, where\nunavoidable noise is present. To address this problem, we propose a Super\nDenoise Net (SDNet), a neural network for a joint task of super-resolution and\nnoise reduction from a low sampling rate signal. To that end, we design gated\nconvolution and lattice convolution blocks to enhance the repair capability and\ncapture information in the time-frequency axis, respectively. The experiments\nshow our method outperforms baseline speech denoising and SSR models on DNS\n2020 no-reverb test set with higher objective and subjective scores.", "published": "2023-10-09 11:40:09", "link": "http://arxiv.org/abs/2310.05629v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio compression-assisted feature extraction for voice replay attack\n  detection", "abstract": "Replay attack is one of the most effective and simplest voice spoofing\nattacks. Detecting replay attacks is challenging, according to the Automatic\nSpeaker Verification Spoofing and Countermeasures Challenge 2021 (ASVspoof\n2021), because they involve a loudspeaker, a microphone, and acoustic\nconditions (e.g., background noise). One obstacle to detecting replay attacks\nis finding robust feature representations that reflect the channel noise\ninformation added to the replayed speech. This study proposes a feature\nextraction approach that uses audio compression for assistance. Audio\ncompression compresses audio to preserve content and speaker information for\ntransmission. The missed information after decompression is expected to contain\ncontent- and speaker-independent information (e.g., channel noise added during\nthe replay process). We conducted a comprehensive experiment with a few data\naugmentation techniques and 3 classifiers on the ASVspoof 2021 physical access\n(PA) set and confirmed the effectiveness of the proposed feature extraction\napproach. To the best of our knowledge, the proposed approach achieves the\nlowest EER at 22.71% on the ASVspoof 2021 PA evaluation set.", "published": "2023-10-09 15:53:42", "link": "http://arxiv.org/abs/2310.05813v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "JVNV: A Corpus of Japanese Emotional Speech with Verbal Content and\n  Nonverbal Expressions", "abstract": "We present the JVNV, a Japanese emotional speech corpus with verbal content\nand nonverbal vocalizations whose scripts are generated by a large-scale\nlanguage model. Existing emotional speech corpora lack not only proper\nemotional scripts but also nonverbal vocalizations (NVs) that are essential\nexpressions in spoken language to express emotions. We propose an automatic\nscript generation method to produce emotional scripts by providing seed words\nwith sentiment polarity and phrases of nonverbal vocalizations to ChatGPT using\nprompt engineering. We select 514 scripts with balanced phoneme coverage from\nthe generated candidate scripts with the assistance of emotion confidence\nscores and language fluency scores. We demonstrate the effectiveness of JVNV by\nshowing that JVNV has better phoneme coverage and emotion recognizability than\nprevious Japanese emotional speech corpora. We then benchmark JVNV on emotional\ntext-to-speech synthesis using discrete codes to represent NVs. We show that\nthere still exists a gap between the performance of synthesizing read-aloud\nspeech and emotional speech, and adding NVs in the speech makes the task even\nharder, which brings new challenges for this task and makes JVNV a valuable\nresource for relevant works in the future. To our best knowledge, JVNV is the\nfirst speech corpus that generates scripts automatically using large language\nmodels.", "published": "2023-10-09 18:27:13", "link": "http://arxiv.org/abs/2310.06072v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Measuring Acoustics with Collaborative Multiple Agents", "abstract": "As humans, we hear sound every second of our life. The sound we hear is often\naffected by the acoustics of the environment surrounding us. For example, a\nspacious hall leads to more reverberation. Room Impulse Responses (RIR) are\ncommonly used to characterize environment acoustics as a function of the scene\ngeometry, materials, and source/receiver locations. Traditionally, RIRs are\nmeasured by setting up a loudspeaker and microphone in the environment for all\nsource/receiver locations, which is time-consuming and inefficient. We propose\nto let two robots measure the environment's acoustics by actively moving and\nemitting/receiving sweep signals. We also devise a collaborative multi-agent\npolicy where these two robots are trained to explore the environment's\nacoustics while being rewarded for wide exploration and accurate prediction. We\nshow that the robots learn to collaborate and move to explore environment\nacoustics while minimizing the prediction error. To the best of our knowledge,\nwe present the very first problem formulation and solution to the task of\ncollaborative environment acoustics measurements with multiple agents.", "published": "2023-10-09 02:58:27", "link": "http://arxiv.org/abs/2310.05368v1", "categories": ["cs.AI", "cs.MA", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
{"title": "Technocratic model of the human auditory system", "abstract": "In this work, we investigate the phenomenon of transverse resonance and\ntransverse standing waves that occur within the cochlea of living organisms. It\nis demonstrated that the predisposing factor for their occurrence is the\ncochlear shape, which resembles a conical acoustic tube coiled into a spiral\nand exhibits non-uniformities on its internal surface. This cochlear structure\nfacilitates the analysis of constituent sound signals akin to a spectrum\nanalyzer, with a corresponding interpretation of the physical processes\noccurring in the auditory system. Additionally, we conclude that the cochlear\nduct's scala media, composed of a system of membranes and the organ of Corti,\nfunctions primarily as an information collection and amplification system along\nthe cochlear spiral. Collectively, these findings enable the development of a\nnovel, highly realistic wave model of the auditory system in living organisms\nbased on a technocratic approach within the scientific context.", "published": "2023-10-09 11:51:22", "link": "http://arxiv.org/abs/2310.05639v1", "categories": ["q-bio.NC", "cs.SD", "eess.AS", "68T10", "I.2.7"], "primary_category": "q-bio.NC"}
{"title": "The First Cadenza Signal Processing Challenge: Improving Music for Those\n  With a Hearing Loss", "abstract": "The Cadenza project aims to improve the audio quality of music for those who\nhave a hearing loss. This is being done through a series of signal processing\nchallenges, to foster better and more inclusive technologies. In the first\nround, two common listening scenarios are considered: listening to music over\nheadphones, and with a hearing aid in a car. The first scenario is cast as a\ndemixing-remixing problem, where the music is decomposed into vocals, bass,\ndrums and other components. These can then be intelligently remixed in a\npersonalized way, to increase the audio quality for a person who has a hearing\nloss. In the second scenario, music is coming from car loudspeakers, and the\nmusic has to be enhanced to overcome the masking effect of the car noise. This\nis done by taking into account the music, the hearing ability of the listener,\nthe hearing aid and the speed of the car. The audio quality of the submissions\nwill be evaluated using the Hearing Aid Audio Quality Index (HAAQI) for\nobjective assessment and by a panel of people with hearing loss for subjective\nevaluation.", "published": "2023-10-09 15:36:15", "link": "http://arxiv.org/abs/2310.05799v1", "categories": ["eess.AS", "cs.LG", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Pre-trained Spatial Priors on Multichannel NMF for Music Source\n  Separation", "abstract": "This paper presents a novel approach to sound source separation that\nleverages spatial information obtained during the recording setup. Our method\ntrains a spatial mixing filter using solo passages to capture information about\nthe room impulse response and transducer response at each sensor location. This\npre-trained filter is then integrated into a multichannel non-negative matrix\nfactorization (MNMF) scheme to better capture the variances of different sound\nsources. The recording setup used in our experiments is the typical setup for\norchestra recordings, with a main microphone and a close \"cardioid\" or\n\"supercardioid\" microphone for each section of the orchestra. This makes the\nproposed method applicable to many existing recordings. Experiments on\npolyphonic ensembles demonstrate the effectiveness of the proposed framework in\nseparating individual sound sources, improving performance compared to\nconventional MNMF methods.", "published": "2023-10-09 16:05:43", "link": "http://arxiv.org/abs/2310.05821v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Fine-grained Audio-Visual Joint Representations for Multimodal Large\n  Language Models", "abstract": "Audio-visual large language models (LLM) have drawn significant attention,\nyet the fine-grained combination of both input streams is rather\nunder-explored, which is challenging but necessary for LLMs to understand\ngeneral video inputs. To this end, a fine-grained audio-visual joint\nrepresentation (FAVOR) learning framework for multimodal LLMs is proposed in\nthis paper, which extends a text-based LLM to simultaneously perceive speech\nand audio events in the audio input stream and images or videos in the visual\ninput stream, at the frame level. To fuse the audio and visual feature streams\ninto joint representations and to align the joint space with the LLM input\nembedding space, we propose a causal Q-Former structure with a causal attention\nmodule to enhance the capture of causal relations of the audio-visual frames\nacross time. An audio-visual evaluation benchmark (AVEB) is also proposed which\ncomprises six representative single-modal tasks with five cross-modal tasks\nreflecting audio-visual co-reasoning abilities. While achieving competitive\nsingle-modal performance on audio, speech and image tasks in AVEB, FAVOR\nachieved over 20% accuracy improvements on the video question-answering task\nwhen fine-grained information or temporal causal reasoning is required. FAVOR,\nin addition, demonstrated remarkable video comprehension and reasoning\nabilities on tasks that are unprecedented by other multimodal LLMs. An\ninteractive demo of FAVOR is available at\nhttps://github.com/BriansIDP/AudioVisualLLM.git, and the training code and\nmodel checkpoints will be released soon.", "published": "2023-10-09 17:00:20", "link": "http://arxiv.org/abs/2310.05863v2", "categories": ["eess.AS", "cs.AI", "cs.CV", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On Time Domain Conformer Models for Monaural Speech Separation in Noisy\n  Reverberant Acoustic Environments", "abstract": "Speech separation remains an important topic for multi-speaker technology\nresearchers. Convolution augmented transformers (conformers) have performed\nwell for many speech processing tasks but have been under-researched for speech\nseparation. Most recent state-of-the-art (SOTA) separation models have been\ntime-domain audio separation networks (TasNets). A number of successful models\nhave made use of dual-path (DP) networks which sequentially process local and\nglobal information. Time domain conformers (TD-Conformers) are an analogue of\nthe DP approach in that they also process local and global context sequentially\nbut have a different time complexity function. It is shown that for realistic\nshorter signal lengths, conformers are more efficient when controlling for\nfeature dimension. Subsampling layers are proposed to further improve\ncomputational efficiency. The best TD-Conformer achieves 14.6 dB and 21.2 dB\nSISDR improvement on the WHAMR and WSJ0-2Mix benchmarks, respectively.", "published": "2023-10-09 20:02:11", "link": "http://arxiv.org/abs/2310.06125v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
