{"title": "Enhancing Chinese Intent Classification by Dynamically Integrating\n  Character Features into Word Embeddings with Ensemble Techniques", "abstract": "Intent classification has been widely researched on English data with deep\nlearning approaches that are based on neural networks and word embeddings. The\nchallenge for Chinese intent classification stems from the fact that, unlike\nEnglish where most words are made up of 26 phonologic alphabet letters, Chinese\nis logographic, where a Chinese character is a more basic semantic unit that\ncan be informative and its meaning does not vary too much in contexts. Chinese\nword embeddings alone can be inadequate for representing words, and pre-trained\nembeddings can suffer from not aligning well with the task at hand. To account\nfor the inadequacy and leverage Chinese character information, we propose a\nlow-effort and generic way to dynamically integrate character embedding based\nfeature maps with word embedding based inputs, whose resulting word-character\nembeddings are stacked with a contextual information extraction module to\nfurther incorporate context information for predictions. On top of the proposed\nmodel, we employ an ensemble method to combine single models and obtain the\nfinal result. The approach is data-independent without relying on external\nsources like pre-trained word embeddings. The proposed model outperforms\nbaseline models and existing methods.", "published": "2018-05-23 00:18:42", "link": "http://arxiv.org/abs/1805.08914v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Attention-Based Message-Relevant Response Generation for Neural\n  Conversation Model", "abstract": "Using a sequence-to-sequence framework, many neural conversation models for\nchit-chat succeed in naturalness of the response. Nevertheless, the neural\nconversation models tend to give generic responses which are not specific to\ngiven messages, and it still remains as a challenge. To alleviate the tendency,\nwe propose a method to promote message-relevant and diverse responses for\nneural conversation model by using self-attention, which is time-efficient as\nwell as effective. Furthermore, we present an investigation of why and how\neffective self-attention is in deep comparison with the standard dialogue\ngeneration. The experiment results show that the proposed method improves the\nstandard dialogue generation in various evaluation metrics.", "published": "2018-05-23 07:14:21", "link": "http://arxiv.org/abs/1805.08983v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Transition-based Algorithm for Unrestricted AMR Parsing", "abstract": "Non-projective parsing can be useful to handle cycles and reentrancy in AMR\ngraphs. We explore this idea and introduce a greedy left-to-right\nnon-projective transition-based parser. At each parsing configuration, an\noracle decides whether to create a concept or whether to connect a pair of\nexisting concepts. The algorithm handles reentrancy and arbitrary cycles\nnatively, i.e. within the transition system itself. The model is evaluated on\nthe LDC2015E86 corpus, obtaining results close to the state of the art,\nincluding a Smatch of 64%, and showing good behavior on reentrant edges.", "published": "2018-05-23 08:20:06", "link": "http://arxiv.org/abs/1805.09007v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bilingual Sentiment Embeddings: Joint Projection of Sentiment Across\n  Languages", "abstract": "Sentiment analysis in low-resource languages suffers from a lack of annotated\ncorpora to estimate high-performing models. Machine translation and bilingual\nword embeddings provide some relief through cross-lingual sentiment approaches.\nHowever, they either require large amounts of parallel data or do not\nsufficiently capture sentiment information. We introduce Bilingual Sentiment\nEmbeddings (BLSE), which jointly represent sentiment information in a source\nand target language. This model only requires a small bilingual lexicon, a\nsource-language corpus annotated for sentiment, and monolingual word embeddings\nfor each language. We perform experiments on three language combinations\n(Spanish, Catalan, Basque) for sentence-level cross-lingual sentiment\nclassification and find that our model significantly outperforms\nstate-of-the-art methods on four out of six experimental setups, as well as\ncapturing complementary information to machine translation. Our analysis of the\nresulting embedding space provides evidence that it represents sentiment\ninformation in the resource-poor target language without any annotated data in\nthat language.", "published": "2018-05-23 08:56:15", "link": "http://arxiv.org/abs/1805.09016v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grounding the Semantics of Part-of-Day Nouns Worldwide using Twitter", "abstract": "The usage of part-of-day nouns, such as 'night', and their time-specific\ngreetings ('good night'), varies across languages and cultures. We show the\npossibilities that Twitter offers for studying the semantics of these terms and\nits variability between countries. We mine a worldwide sample of multilingual\ntweets with temporal greetings, and study how their frequencies vary in\nrelation with local time. The results provide insights into the semantics of\nthese temporal expressions and the cultural and sociological factors\ninfluencing their usage.", "published": "2018-05-23 11:04:46", "link": "http://arxiv.org/abs/1805.09055v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Selecting Machine-Translated Data for Quick Bootstrapping of a Natural\n  Language Understanding System", "abstract": "This paper investigates the use of Machine Translation (MT) to bootstrap a\nNatural Language Understanding (NLU) system for a new language for the use case\nof a large-scale voice-controlled device. The goal is to decrease the cost and\ntime needed to get an annotated corpus for the new language, while still having\na large enough coverage of user requests. Different methods of filtering MT\ndata in order to keep utterances that improve NLU performance and\nlanguage-specific post-processing methods are investigated. These methods are\ntested in a large-scale NLU task with translating around 10 millions training\nutterances from English to German. The results show a large improvement for\nusing MT data over a grammar-based and over an in-house data collection\nbaseline, while reducing the manual effort greatly. Both filtering and\npost-processing approaches improve results further.", "published": "2018-05-23 13:20:16", "link": "http://arxiv.org/abs/1805.09119v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How much does a word weigh? Weighting word embeddings for word sense\n  induction", "abstract": "The paper describes our participation in the first shared task on word sense\ninduction and disambiguation for the Russian language RUSSE'2018 (Panchenko et\nal., 2018). For each of several dozens of ambiguous words, the participants\nwere asked to group text fragments containing it according to the senses of\nthis word, which were not provided beforehand, therefore the \"induction\" part\nof the task. For instance, a word \"bank\" and a set of text fragments (also\nknown as \"contexts\") in which this word occurs, e.g. \"bank is a financial\ninstitution that accepts deposits\" and \"river bank is a slope beside a body of\nwater\" were given. A participant was asked to cluster such contexts in the\nunknown in advance number of clusters corresponding to, in this case, the\n\"company\" and the \"area\" senses of the word \"bank\". The organizers proposed\nthree evaluation datasets of varying complexity and text genres based\nrespectively on texts of Wikipedia, Web pages, and a dictionary of the Russian\nlanguage. We present two experiments: a positive and a negative one, based\nrespectively on clustering of contexts represented as a weighted average of\nword embeddings and on machine translation using two state-of-the-art\nproduction neural machine translation systems. Our team showed the second best\nresult on two datasets and the third best result on the remaining one dataset\namong 18 participating teams. We managed to substantially outperform\ncompetitive state-of-the-art baselines from the previous years based on sense\nembeddings.", "published": "2018-05-23 14:58:13", "link": "http://arxiv.org/abs/1805.09209v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Working Memory Networks: Augmenting Memory Networks with a Relational\n  Reasoning Module", "abstract": "During the last years, there has been a lot of interest in achieving some\nkind of complex reasoning using deep neural networks. To do that, models like\nMemory Networks (MemNNs) have combined external memory storages and attention\nmechanisms. These architectures, however, lack of more complex reasoning\nmechanisms that could allow, for instance, relational reasoning. Relation\nNetworks (RNs), on the other hand, have shown outstanding results in relational\nreasoning tasks. Unfortunately, their computational cost grows quadratically\nwith the number of memories, something prohibitive for larger problems. To\nsolve these issues, we introduce the Working Memory Network, a MemNN\narchitecture with a novel working memory storage and reasoning module. Our\nmodel retains the relational reasoning abilities of the RN while reducing its\ncomputational complexity from quadratic to linear. We tested our model on the\ntext QA dataset bAbI and the visual QA dataset NLVR. In the jointly trained\nbAbI-10k, we set a new state-of-the-art, achieving a mean error of less than\n0.5%. Moreover, a simple ensemble of two of our models solves all 20 tasks in\nthe joint version of the benchmark.", "published": "2018-05-23 18:03:08", "link": "http://arxiv.org/abs/1805.09354v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Embedding Syntax and Semantics of Prepositions via Tensor Decomposition", "abstract": "Prepositions are among the most frequent words in English and play complex\nroles in the syntax and semantics of sentences. Not surprisingly, they pose\nwell-known difficulties in automatic processing of sentences (prepositional\nattachment ambiguities and idiosyncratic uses in phrases). Existing methods on\npreposition representation treat prepositions no different from content words\n(e.g., word2vec and GloVe). In addition, recent studies aiming at solving\nprepositional attachment and preposition selection problems depend heavily on\nexternal linguistic resources and use dataset-specific word representations. In\nthis paper we use word-triple counts (one of the triples being a preposition)\nto capture a preposition's interaction with its attachment and complement. We\nthen derive preposition embeddings via tensor decomposition on a large\nunlabeled corpus. We reveal a new geometry involving Hadamard products and\nempirically demonstrate its utility in paraphrasing phrasal verbs. Furthermore,\nour preposition embeddings are used as simple features in two challenging\ndownstream tasks: preposition selection and prepositional attachment\ndisambiguation. We achieve results comparable to or better than the\nstate-of-the-art on multiple standardized datasets.", "published": "2018-05-23 19:20:43", "link": "http://arxiv.org/abs/1805.09389v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Interpersonal Influence of Verbal Behavior in Couples Therapy\n  Dyadic Interactions", "abstract": "Dyadic interactions among humans are marked by speakers continuously\ninfluencing and reacting to each other in terms of responses and behaviors,\namong others. Understanding how interpersonal dynamics affect behavior is\nimportant for successful treatment in psychotherapy domains. Traditional\nschemes that automatically identify behavior for this purpose have often looked\nat only the target speaker. In this work, we propose a Markov model of how a\ntarget speaker's behavior is influenced by their own past behavior as well as\ntheir perception of their partner's behavior, based on lexical features. Apart\nfrom incorporating additional potentially useful information, our model can\nalso control the degree to which the partner affects the target speaker. We\nevaluate our proposed model on the task of classifying Negative behavior in\nCouples Therapy and show that it is more accurate than the single-speaker\nmodel. Furthermore, we investigate the degree to which the optimal influence\nrelates to how well a couple does on the long-term, via relating to\nrelationship outcomes", "published": "2018-05-23 21:38:45", "link": "http://arxiv.org/abs/1805.09436v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Mine Aligned Code and Natural Language Pairs from Stack\n  Overflow", "abstract": "For tasks like code synthesis from natural language, code retrieval, and code\nsummarization, data-driven models have shown great promise. However, creating\nthese models require parallel data between natural language (NL) and code with\nfine-grained alignments. Stack Overflow (SO) is a promising source to create\nsuch a data set: the questions are diverse and most of them have corresponding\nanswers with high-quality code snippets. However, existing heuristic methods\n(e.g., pairing the title of a post with the code in the accepted answer) are\nlimited both in their coverage and the correctness of the NL-code pairs\nobtained. In this paper, we propose a novel method to mine high-quality aligned\ndata from SO using two sets of features: hand-crafted features considering the\nstructure of the extracted snippets, and correspondence features obtained by\ntraining a probabilistic model to capture the correlation between NL and code\nusing neural networks. These features are fed into a classifier that determines\nthe quality of mined NL-code pairs. Experiments using Python and Java as test\nbeds show that the proposed method greatly expands coverage and accuracy over\nexisting mining methods, even when using only a small number of labeled\nexamples. Further, we find that reasonable results are achieved even when\ntraining the classifier on one language and testing on another, showing promise\nfor scaling NL-code mining to a wide variety of programming languages beyond\nthose for which we are able to annotate data.", "published": "2018-05-23 03:39:04", "link": "http://arxiv.org/abs/1805.08949v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "RDF2Vec-based Classification of Ontology Alignment Changes", "abstract": "When ontologies cover overlapping topics, the overlap can be represented\nusing ontology alignments. These alignments need to be continuously adapted to\nchanging ontologies. Especially for large ontologies this is a costly task\noften consisting of manual work. Finding changes that do not lead to an\nadaption of the alignment can potentially make this process significantly\neasier. This work presents an approach to finding these changes based on RDF\nembeddings and common classification techniques. To examine the feasibility of\nthis approach, an evaluation on a real-world dataset is presented. In this\nevaluation, the best classifiers reached a precision of 0.8.", "published": "2018-05-23 13:34:51", "link": "http://arxiv.org/abs/1805.09145v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ASR-based Features for Emotion Recognition: A Transfer Learning Approach", "abstract": "During the last decade, the applications of signal processing have\ndrastically improved with deep learning. However areas of affecting computing\nsuch as emotional speech synthesis or emotion recognition from spoken language\nremains challenging. In this paper, we investigate the use of a neural\nAutomatic Speech Recognition (ASR) as a feature extractor for emotion\nrecognition. We show that these features outperform the eGeMAPS feature set to\npredict the valence and arousal emotional dimensions, which means that the\naudio-to-text mapping learning by the ASR system contain information related to\nthe emotional dimensions in spontaneous speech. We also examine the\nrelationship between first layers (closer to speech) and last layers (closer to\ntext) of the ASR and valence/arousal.", "published": "2018-05-23 14:38:39", "link": "http://arxiv.org/abs/1805.09197v3", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Pushing the bounds of dropout", "abstract": "We show that dropout training is best understood as performing MAP estimation\nconcurrently for a family of conditional models whose objectives are themselves\nlower bounded by the original dropout objective. This discovery allows us to\npick any model from this family after training, which leads to a substantial\nimprovement on regularisation-heavy language modelling. The family includes\nmodels that compute a power mean over the sampled dropout masks, and their less\nstochastic subvariants with tighter and higher lower bounds than the fully\nstochastic dropout objective. We argue that since the deterministic\nsubvariant's bound is equal to its objective, and the highest amongst these\nmodels, the predominant view of it as a good approximation to MC averaging is\nmisleading. Rather, deterministic dropout is the best available approximation\nto the true objective.", "published": "2018-05-23 14:55:39", "link": "http://arxiv.org/abs/1805.09208v2", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Scoring Lexical Entailment with a Supervised Directional Similarity\n  Network", "abstract": "We present the Supervised Directional Similarity Network (SDSN), a novel\nneural architecture for learning task-specific transformation functions on top\nof general-purpose word embeddings. Relying on only a limited amount of\nsupervision from task-specific scores on a subset of the vocabulary, our\narchitecture is able to generalise and transform a general-purpose\ndistributional vector space to model the relation of lexical entailment.\nExperiments show excellent performance on scoring graded lexical entailment,\nraising the state-of-the-art on the HyperLex dataset by approximately 25%.", "published": "2018-05-23 18:03:40", "link": "http://arxiv.org/abs/1805.09355v1", "categories": ["cs.CL", "cs.LG", "cs.NE", "I.2.7; I.2.6; I.5.1"], "primary_category": "cs.CL"}
{"title": "End-to-End Speech-Driven Facial Animation with Temporal GANs", "abstract": "Speech-driven facial animation is the process which uses speech signals to\nautomatically synthesize a talking character. The majority of work in this\ndomain creates a mapping from audio features to visual features. This often\nrequires post-processing using computer graphics techniques to produce\nrealistic albeit subject dependent results. We present a system for generating\nvideos of a talking head, using a still image of a person and an audio clip\ncontaining speech, that doesn't rely on any handcrafted intermediate features.\nTo the best of our knowledge, this is the first method capable of generating\nsubject independent realistic videos directly from raw audio. Our method can\ngenerate videos which have (a) lip movements that are in sync with the audio\nand (b) natural facial expressions such as blinks and eyebrow movements. We\nachieve this by using a temporal GAN with 2 discriminators, which are capable\nof capturing different aspects of the video. The effect of each component in\nour system is quantified through an ablation study. The generated videos are\nevaluated based on their sharpness, reconstruction quality, and lip-reading\naccuracy. Finally, a user study is conducted, confirming that temporal GANs\nlead to more natural sequences than a static GAN-based approach.", "published": "2018-05-23 17:54:32", "link": "http://arxiv.org/abs/1805.09313v4", "categories": ["eess.AS", "cs.CV", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
{"title": "Semi-supervised classification by reaching consensus among modalities", "abstract": "Deep learning has demonstrated abilities to learn complex structures, but\nthey can be restricted by available data. Recently, Consensus Networks (CNs)\nwere proposed to alleviate data sparsity by utilizing features from multiple\nmodalities, but they too have been limited by the size of labeled data. In this\npaper, we extend CN to Transductive Consensus Networks (TCNs), suitable for\nsemi-supervised learning. In TCNs, different modalities of input are compressed\ninto latent representations, which we encourage to become indistinguishable\nduring iterative adversarial training. To understand TCNs two mechanisms,\nconsensus and classification, we put forward its three variants in ablation\nstudies on these mechanisms. To further investigate TCN models, we treat the\nlatent representations as probability distributions and measure their\nsimilarities as the negative relative Jensen-Shannon divergences. We show that\na consensus state beneficial for classification desires a stable but imperfect\nsimilarity between the representations. Overall, TCNs outperform or align with\nthe best benchmark algorithms given 20 to 200 labeled samples on the Bank\nMarketing and the DementiaBank datasets.", "published": "2018-05-23 18:19:11", "link": "http://arxiv.org/abs/1805.09366v2", "categories": ["cs.LG", "cs.MM", "cs.SD", "eess.AS", "eess.SP", "stat.ML"], "primary_category": "cs.LG"}
