{"title": "Conic10K: A Challenging Math Problem Understanding and Reasoning Dataset", "abstract": "Mathematical understanding and reasoning are crucial tasks for assessing the\ncapabilities of artificial intelligence (AI). However, existing benchmarks\neither require just a few steps of reasoning, or only contain a small amount of\ndata in one specific topic, making it hard to analyse AI's behaviour with\nreference to different problems within a specific topic in detail. In this\nwork, we propose Conic10K, a challenging math problem dataset on conic sections\nin Chinese senior high school education. Our dataset contains various problems\nwith different reasoning depths, while only the knowledge from conic sections\nis required. Since the dataset only involves a narrow range of knowledge, it is\neasy to separately analyse the knowledge a model possesses and the reasoning\nability it has. For each problem, we provide a high-quality formal\nrepresentation, the reasoning steps, and the final solution. Experiments show\nthat existing large language models, including GPT-4, exhibit weak performance\non complex reasoning. We hope that our findings could inspire more advanced\ntechniques for precise natural language understanding and reasoning. Our\ndataset and codes are available at https://github.com/whyNLP/Conic10K.", "published": "2023-11-09 02:58:17", "link": "http://arxiv.org/abs/2311.05113v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Translation Quality Estimation Exploiting Synthetic Data\n  and Pre-trained Multilingual Encoder", "abstract": "Translation quality estimation (TQE) is the task of predicting translation\nquality without reference translations. Due to the enormous cost of creating\ntraining data for TQE, only a few translation directions can benefit from\nsupervised training. To address this issue, unsupervised TQE methods have been\nstudied. In this paper, we extensively investigate the usefulness of synthetic\nTQE data and pre-trained multilingual encoders in unsupervised sentence-level\nTQE, both of which have been proven effective in the supervised training\nscenarios. Our experiment on WMT20 and WMT21 datasets revealed that this\napproach can outperform other unsupervised TQE methods on high- and\nlow-resource translation directions in predicting post-editing effort and human\nevaluation score, and some zero-resource translation directions in predicting\npost-editing effort.", "published": "2023-11-09 03:10:42", "link": "http://arxiv.org/abs/2311.05117v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quranic Conversations: Developing a Semantic Search tool for the Quran\n  using Arabic NLP Techniques", "abstract": "The Holy Book of Quran is believed to be the literal word of God (Allah) as\nrevealed to the Prophet Muhammad (PBUH) over a period of approximately 23\nyears. It is the book where God provides guidance on how to live a righteous\nand just life, emphasizing principles like honesty, compassion, charity and\njustice, as well as providing rules for personal conduct, family matters,\nbusiness ethics and much more. However, due to constraints related to the\nlanguage and the Quran organization, it is challenging for Muslims to get all\nrelevant ayahs (verses) pertaining to a matter or inquiry of interest. Hence,\nwe developed a Quran semantic search tool which finds the verses pertaining to\nthe user inquiry or prompt. To achieve this, we trained several models on a\nlarge dataset of over 30 tafsirs, where typically each tafsir corresponds to\none verse in the Quran and, using cosine similarity, obtained the tafsir tensor\nwhich is most similar to the prompt tensor of interest, which was then used to\nindex for the corresponding ayah in the Quran. Using the SNxLM model, we were\nable to achieve a cosine similarity score as high as 0.97 which corresponds to\nthe abdu tafsir for a verse relating to financial matters.", "published": "2023-11-09 03:14:54", "link": "http://arxiv.org/abs/2311.05120v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Computation Efficiency in Large Language Models through Weight\n  and Activation Quantization", "abstract": "Large Language Models (LLMs) are proficient in natural language processing\ntasks, but their deployment is often restricted by extensive parameter sizes\nand computational demands. This paper focuses on post-training quantization\n(PTQ) in LLMs, specifically 4-bit weight and 8-bit activation (W4A8)\nquantization, to enhance computational efficiency -- a topic less explored\ncompared to weight-only quantization. We present two innovative techniques:\nactivation-quantization-aware scaling (AQAS) and sequence-length-aware\ncalibration (SLAC) to enhance PTQ by considering the combined effects on\nweights and activations and aligning calibration sequence lengths to target\ntasks. Moreover, we introduce dINT, a hybrid data format combining integer and\ndenormal representations, to address the underflow issue in W4A8 quantization,\nwhere small values are rounded to zero. Through rigorous evaluations of LLMs,\nincluding OPT and LLaMA, we demonstrate that our techniques significantly boost\ntask accuracies to levels comparable with full-precision models. By developing\narithmetic units compatible with dINT, we further confirm that our methods\nyield a 2$\\times$ hardware efficiency improvement compared to 8-bit integer MAC\nunit.", "published": "2023-11-09 06:19:51", "link": "http://arxiv.org/abs/2311.05161v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models and Prompt Engineering for Biomedical Query\n  Focused Multi-Document Summarisation", "abstract": "This paper reports on the use of prompt engineering and GPT-3.5 for\nbiomedical query-focused multi-document summarisation. Using GPT-3.5 and\nappropriate prompts, our system achieves top ROUGE-F1 results in the task of\nobtaining short-paragraph-sized answers to biomedical questions in the 2023\nBioASQ Challenge (BioASQ 11b). This paper confirms what has been observed in\nother domains: 1) Prompts that incorporated few-shot samples generally improved\non their counterpart zero-shot variants; 2) The largest improvement was\nachieved by retrieval augmented generation. The fact that these prompts allow\nour top runs to rank within the top two runs of BioASQ 11b demonstrate the\npower of using adequate prompts for Large Language Models in general, and\nGPT-3.5 in particular, for query-focused summarisation.", "published": "2023-11-09 06:45:04", "link": "http://arxiv.org/abs/2311.05169v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PRODIGy: a PROfile-based DIalogue Generation dataset", "abstract": "Providing dialogue agents with a profile representation can improve their\nconsistency and coherence, leading to better conversations. However, current\nprofile-based dialogue datasets for training such agents contain either\nexplicit profile representations that are simple and dialogue-specific, or\nimplicit representations that are difficult to collect. In this work, we\npropose a unified framework in which we bring together both standard and more\nsophisticated profile representations by creating a new resource where each\ndialogue is aligned with all possible speaker representations such as\ncommunication style, biographies, and personality. This framework allows to\ntest several baselines built using generative language models with several\nprofile configurations. The automatic evaluation shows that profile-based\nmodels have better generalisation capabilities than models trained on dialogues\nonly, both in-domain and cross-domain settings. These results are consistent\nfor fine-tuned models and instruction-based LLMs. Additionally, human\nevaluation demonstrates a clear preference for generations consistent with both\nprofile and context. Finally, to account for possible privacy concerns, all\nexperiments are done under two configurations: inter-character and\nintra-character. In the former, the LM stores the information about the\ncharacter in its internal representation, while in the latter, the LM does not\nretain any personal information but uses it only at inference time.", "published": "2023-11-09 08:19:34", "link": "http://arxiv.org/abs/2311.05195v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Hallucination in Large Language Models: Principles,\n  Taxonomy, Challenges, and Open Questions", "abstract": "The emergence of large language models (LLMs) has marked a significant\nbreakthrough in natural language processing (NLP), fueling a paradigm shift in\ninformation acquisition. Nevertheless, LLMs are prone to hallucination,\ngenerating plausible yet nonfactual content. This phenomenon raises significant\nconcerns over the reliability of LLMs in real-world information retrieval (IR)\nsystems and has attracted intensive research to detect and mitigate such\nhallucinations. Given the open-ended general-purpose attributes inherent to\nLLMs, LLM hallucinations present distinct challenges that diverge from prior\ntask-specific models. This divergence highlights the urgency for a nuanced\nunderstanding and comprehensive overview of recent advances in LLM\nhallucinations. In this survey, we begin with an innovative taxonomy of\nhallucination in the era of LLM and then delve into the factors contributing to\nhallucinations. Subsequently, we present a thorough overview of hallucination\ndetection methods and benchmarks. Our discussion then transfers to\nrepresentative methodologies for mitigating LLM hallucinations. Additionally,\nwe delve into the current limitations faced by retrieval-augmented LLMs in\ncombating hallucinations, offering insights for developing more robust IR\nsystems. Finally, we highlight the promising research directions on LLM\nhallucinations, including hallucination in large vision-language models and\nunderstanding of knowledge boundaries in LLM hallucinations.", "published": "2023-11-09 09:25:37", "link": "http://arxiv.org/abs/2311.05232v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modelling prospective memory and resilient situated communications via\n  Wizard of Oz", "abstract": "This abstract presents a scenario for human-robot action in a home setting\ninvolving an older adult and a robot. The scenario is designed to explore the\nenvisioned modelling of memory for communication with a socially assistive\nrobots (SAR). The scenario will enable the gathering of data on failures of\nspeech technology and human-robot communication involving shared memory that\nmay occur during daily activities such as a music-listening activity.", "published": "2023-11-09 10:50:36", "link": "http://arxiv.org/abs/2311.05268v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Causal Inference from Text: Unveiling Interactions between Variables", "abstract": "Adjusting for latent covariates is crucial for estimating causal effects from\nobservational textual data. Most existing methods only account for confounding\ncovariates that affect both treatment and outcome, potentially leading to\nbiased causal effects. This bias arises from insufficient consideration of\nnon-confounding covariates, which are relevant only to either the treatment or\nthe outcome. In this work, we aim to mitigate the bias by unveiling\ninteractions between different variables to disentangle the non-confounding\ncovariates when estimating causal effects from text. The disentangling process\nensures covariates only contribute to their respective objectives, enabling\nindependence between variables. Additionally, we impose a constraint to balance\nrepresentations from the treatment group and control group to alleviate\nselection bias. We conduct experiments on two different treatment factors under\nvarious scenarios, and the proposed model significantly outperforms recent\nstrong baselines. Furthermore, our thorough analysis on earnings call\ntranscripts demonstrates that our model can effectively disentangle the\nvariables, and further investigations into real-world scenarios provide\nguidance for investors to make informed decisions.", "published": "2023-11-09 11:29:44", "link": "http://arxiv.org/abs/2311.05286v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BeLLM: Backward Dependency Enhanced Large Language Model for Sentence\n  Embeddings", "abstract": "Sentence embeddings are crucial in measuring semantic similarity. Most recent\nstudies employed large language models (LLMs) to learn sentence embeddings.\nExisting LLMs mainly adopted autoregressive architecture without explicit\nbackward dependency modeling. Therefore, we examined the effects of backward\ndependencies in LLMs for semantic similarity measurements. Concretely, we\npropose a novel model: backward dependency enhanced large language model\n(BeLLM). It learns sentence embeddings via transforming specific attention\nlayers from uni- to bi-directional. We extensively experiment across various\nsemantic textual similarity (STS) tasks and downstream applications. BeLLM\nachieves state-of-the-art performance in varying scenarios. It shows that\nauto-regressive LLMs benefit from backward dependencies for sentence\nembeddings.", "published": "2023-11-09 11:53:52", "link": "http://arxiv.org/abs/2311.05296v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "There's no Data Like Better Data: Using QE Metrics for MT Data Filtering", "abstract": "Quality Estimation (QE), the evaluation of machine translation output without\nthe need of explicit references, has seen big improvements in the last years\nwith the use of neural metrics. In this paper we analyze the viability of using\nQE metrics for filtering out bad quality sentence pairs in the training data of\nneural machine translation systems~(NMT). While most corpus filtering methods\nare focused on detecting noisy examples in collections of texts, usually huge\namounts of web crawled data, QE models are trained to discriminate more\nfine-grained quality differences. We show that by selecting the highest quality\nsentence pairs in the training data, we can improve translation quality while\nreducing the training size by half. We also provide a detailed analysis of the\nfiltering results, which highlights the differences between both approaches.", "published": "2023-11-09 13:21:34", "link": "http://arxiv.org/abs/2311.05350v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Memorisation Cartography: Mapping out the Memorisation-Generalisation\n  Continuum in Neural Machine Translation", "abstract": "When training a neural network, it will quickly memorise some source-target\nmappings from your dataset but never learn some others. Yet, memorisation is\nnot easily expressed as a binary feature that is good or bad: individual\ndatapoints lie on a memorisation-generalisation continuum. What determines a\ndatapoint's position on that spectrum, and how does that spectrum influence\nneural models' performance? We address these two questions for neural machine\ntranslation (NMT) models. We use the counterfactual memorisation metric to (1)\nbuild a resource that places 5M NMT datapoints on a memorisation-generalisation\nmap, (2) illustrate how the datapoints' surface-level characteristics and a\nmodels' per-datum training signals are predictive of memorisation in NMT, (3)\nand describe the influence that subsets of that map have on NMT systems'\nperformance.", "published": "2023-11-09 14:03:51", "link": "http://arxiv.org/abs/2311.05379v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Iron(ic) Melting Pot: Reviewing Human Evaluation in Humour, Irony\n  and Sarcasm Generation", "abstract": "Human evaluation is often considered to be the gold standard method of\nevaluating a Natural Language Generation system. However, whilst its importance\nis accepted by the community at large, the quality of its execution is often\nbrought into question. In this position paper, we argue that the generation of\nmore esoteric forms of language - humour, irony and sarcasm - constitutes a\nsubdomain where the characteristics of selected evaluator panels are of utmost\nimportance, and every effort should be made to report demographic\ncharacteristics wherever possible, in the interest of transparency and\nreplicability. We support these claims with an overview of each language form\nand an analysis of examples in terms of how their interpretation is affected by\ndifferent participant variables. We additionally perform a critical survey of\nrecent works in NLG to assess how well evaluation procedures are reported in\nthis subdomain, and note a severe lack of open reporting of evaluator\ndemographic information, and a significant reliance on crowdsourcing platforms\nfor recruitment.", "published": "2023-11-09 17:50:23", "link": "http://arxiv.org/abs/2311.05552v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FAMuS: Frames Across Multiple Sources", "abstract": "Understanding event descriptions is a central aspect of language processing,\nbut current approaches focus overwhelmingly on single sentences or documents.\nAggregating information about an event \\emph{across documents} can offer a much\nricher understanding. To this end, we present FAMuS, a new corpus of Wikipedia\npassages that \\emph{report} on some event, paired with underlying,\ngenre-diverse (non-Wikipedia) \\emph{source} articles for the same event. Events\nand (cross-sentence) arguments in both report and source are annotated against\nFrameNet, providing broad coverage of different event types. We present results\non two key event understanding tasks enabled by FAMuS: \\emph{source validation}\n-- determining whether a document is a valid source for a target report event\n-- and \\emph{cross-document argument extraction} -- full-document argument\nextraction for a target event from both its report and the correct source\narticle. We release both FAMuS and our models to support further research.", "published": "2023-11-09 18:57:39", "link": "http://arxiv.org/abs/2311.05601v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GRASP: A Disagreement Analysis Framework to Assess Group Associations in\n  Perspectives", "abstract": "Human annotation plays a core role in machine learning -- annotations for\nsupervised models, safety guardrails for generative models, and human feedback\nfor reinforcement learning, to cite a few avenues. However, the fact that many\nof these human annotations are inherently subjective is often overlooked.\nRecent work has demonstrated that ignoring rater subjectivity (typically\nresulting in rater disagreement) is problematic within specific tasks and for\nspecific subgroups. Generalizable methods to harness rater disagreement and\nthus understand the socio-cultural leanings of subjective tasks remain elusive.\nIn this paper, we propose GRASP, a comprehensive disagreement analysis\nframework to measure group association in perspectives among different rater\nsub-groups, and demonstrate its utility in assessing the extent of systematic\ndisagreements in two datasets: (1) safety annotations of human-chatbot\nconversations, and (2) offensiveness annotations of social media posts, both\nannotated by diverse rater pools across different socio-demographic axes. Our\nframework (based on disagreement metrics) reveals specific rater groups that\nhave significantly different perspectives than others on certain tasks, and\nhelps identify demographic axes that are crucial to consider in specific task\ncontexts.", "published": "2023-11-09 00:12:21", "link": "http://arxiv.org/abs/2311.05074v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Characterizing Large Language Models as Rationalizers of\n  Knowledge-intensive Tasks", "abstract": "Large language models (LLMs) are proficient at generating fluent text with\nminimal task-specific supervision. Yet, their ability to provide well-grounded\nrationalizations for knowledge-intensive tasks remains under-explored. Such\ntasks, like commonsense multiple-choice questions, require rationales based on\nworld knowledge to support predictions and refute alternate options. We\nconsider the task of generating knowledge-guided rationalization in natural\nlanguage by using expert-written examples in a few-shot manner. Surprisingly,\ncrowd-workers preferred knowledge-grounded rationales over crowdsourced\nrationalizations, citing their factuality, sufficiency, and comprehensive\nrefutations. Although LLMs-generated rationales were preferable, further\nimprovements in conciseness and novelty are required. In another study, we show\nhow rationalization of incorrect model predictions erodes humans' trust in\nLLM-generated rationales. Motivated by these observations, we create a\ntwo-stage pipeline to review task predictions and eliminate potential incorrect\ndecisions before rationalization, enabling trustworthy rationale generation.", "published": "2023-11-09 01:04:44", "link": "http://arxiv.org/abs/2311.05085v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Legal-HNet: Mixing Legal Long-Context Tokens with Hartley Transform", "abstract": "Since its introduction, the transformers architecture has seen great adoption\nin NLP applications, but it also has limitations. Although the self-attention\nmechanism allows for generating very rich representations of the input text,\nits effectiveness may be limited in specialized domains such as legal, where,\nfor example, language models often have to process very long texts. In this\npaper, we explore alternatives to replace the attention-based layers with\nsimpler token-mixing mechanisms: Hartley and Fourier transforms. Using these\nnon-parametric techniques, we train models with long input documents from\nscratch in the legal domain setting. We also introduce a new hybrid Seq2Seq\narchitecture, a no-attention-based encoder connected with an attention-based\ndecoder, which performs quite well on existing summarization tasks with much\nless compute and memory requirements. We believe that similar, if not better\nperformance, as in the case of long correlations of abstractive text\nsummarization tasks, can be achieved by adopting these simpler infrastructures.\nThis not only makes training models from scratch accessible to more people, but\nalso contributes to the reduction of the carbon footprint during training.", "published": "2023-11-09 01:27:54", "link": "http://arxiv.org/abs/2311.05089v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Survey of Large Language Models in Medicine: Progress, Application,\n  and Challenge", "abstract": "Large language models (LLMs), such as ChatGPT, have received substantial\nattention due to their capabilities for understanding and generating human\nlanguage. While there has been a burgeoning trend in research focusing on the\nemployment of LLMs in supporting different medical tasks (e.g., enhancing\nclinical diagnostics and providing medical education), a review of these\nefforts, particularly their development, practical applications, and outcomes\nin medicine, remains scarce. Therefore, this review aims to provide a detailed\noverview of the development and deployment of LLMs in medicine, including the\nchallenges and opportunities they face. In terms of development, we provide a\ndetailed introduction to the principles of existing medical LLMs, including\ntheir basic model structures, number of parameters, and sources and scales of\ndata used for model development. It serves as a guide for practitioners in\ndeveloping medical LLMs tailored to their specific needs. In terms of\ndeployment, we offer a comparison of the performance of different LLMs across\nvarious medical tasks, and further compare them with state-of-the-art\nlightweight models, aiming to provide an understanding of the advantages and\nlimitations of LLMs in medicine. Overall, in this review, we address the\nfollowing questions: 1) What are the practices for developing medical LLMs 2)\nHow to measure the medical task performance of LLMs in a medical setting? 3)\nHow have medical LLMs been employed in real-world practice? 4) What challenges\narise from the use of medical LLMs? and 5) How to more effectively develop and\ndeploy medical LLMs? By answering these questions, this review aims to provide\ninsights into the opportunities for LLMs in medicine and serve as a practical\nresource. We also maintain a regularly updated list of practical guides on\nmedical LLMs at https://github.com/AI-in-Health/MedLLMsPracticalGuide", "published": "2023-11-09 02:55:58", "link": "http://arxiv.org/abs/2311.05112v7", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Weakly-supervised Deep Cognate Detection Framework for Low-Resourced\n  Languages Using Morphological Knowledge of Closely-Related Languages", "abstract": "Exploiting cognates for transfer learning in under-resourced languages is an\nexciting opportunity for language understanding tasks, including unsupervised\nmachine translation, named entity recognition and information retrieval.\nPrevious approaches mainly focused on supervised cognate detection tasks based\non orthographic, phonetic or state-of-the-art contextual language models, which\nunder-perform for most under-resourced languages. This paper proposes a novel\nlanguage-agnostic weakly-supervised deep cognate detection framework for\nunder-resourced languages using morphological knowledge from closely related\nlanguages. We train an encoder to gain morphological knowledge of a language\nand transfer the knowledge to perform unsupervised and weakly-supervised\ncognate detection tasks with and without the pivot language for the\nclosely-related languages. While unsupervised, it overcomes the need for\nhand-crafted annotation of cognates. We performed experiments on different\npublished cognate detection datasets across language families and observed not\nonly significant improvement over the state-of-the-art but also our method\noutperformed the state-of-the-art supervised and unsupervised methods. Our\nmodel can be extended to a wide range of languages from any language family as\nit overcomes the requirement of the annotation of the cognate pairs for\ntraining. The code and dataset building scripts can be found at\nhttps://github.com/koustavagoswami/Weakly_supervised-Cognate_Detection", "published": "2023-11-09 05:46:41", "link": "http://arxiv.org/abs/2311.05155v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Model-Based Minimum Bayes Risk Decoding for Text Generation", "abstract": "Minimum Bayes Risk (MBR) decoding has been shown to be a powerful alternative\nto beam search decoding in a variety of text generation tasks. MBR decoding\nselects a hypothesis from a pool of hypotheses that has the least expected risk\nunder a probability model according to a given utility function. Since it is\nimpractical to compute the expected risk exactly over all possible hypotheses,\ntwo approximations are commonly used in MBR. First, it integrates over a\nsampled set of hypotheses rather than over all possible hypotheses. Second, it\nestimates the probability of each hypothesis using a Monte Carlo estimator.\nWhile the first approximation is necessary to make it computationally feasible,\nthe second is not essential since we typically have access to the model\nprobability at inference time. We propose Model-Based MBR (MBMBR), a variant of\nMBR that uses the model probability itself as the estimate of the probability\ndistribution instead of the Monte Carlo estimate. We show analytically and\nempirically that the model-based estimate is more promising than the Monte\nCarlo estimate in text generation tasks. Our experiments show that MBMBR\noutperforms MBR in several text generation tasks, both with encoder-decoder\nmodels and with large language models.", "published": "2023-11-09 10:46:09", "link": "http://arxiv.org/abs/2311.05263v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "TencentLLMEval: A Hierarchical Evaluation of Real-World Capabilities for\n  Human-Aligned LLMs", "abstract": "Large language models (LLMs) have shown impressive capabilities across\nvarious natural language tasks. However, evaluating their alignment with human\npreferences remains a challenge. To this end, we propose a comprehensive human\nevaluation framework to assess LLMs' proficiency in following instructions on\ndiverse real-world tasks. We construct a hierarchical task tree encompassing 7\nmajor areas covering over 200 categories and over 800 tasks, which covers\ndiverse capabilities such as question answering, reasoning, multiturn dialogue,\nand text generation, to evaluate LLMs in a comprehensive and in-depth manner.\nWe also design detailed evaluation standards and processes to facilitate\nconsistent, unbiased judgments from human evaluators. A test set of over 3,000\ninstances is released, spanning different difficulty levels and knowledge\ndomains. Our work provides a standardized methodology to evaluate human\nalignment in LLMs for both English and Chinese. We also analyze the feasibility\nof automating parts of evaluation with a strong LLM (GPT-4). Our framework\nsupports a thorough assessment of LLMs as they are integrated into real-world\napplications. We have made publicly available the task tree, TencentLLMEval\ndataset, and evaluation methodology which have been demonstrated as effective\nin assessing the performance of Tencent Hunyuan LLMs. By doing so, we aim to\nfacilitate the benchmarking of advances in the development of safe and\nhuman-aligned LLMs.", "published": "2023-11-09 13:58:59", "link": "http://arxiv.org/abs/2311.05374v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mirror: A Universal Framework for Various Information Extraction Tasks", "abstract": "Sharing knowledge between information extraction tasks has always been a\nchallenge due to the diverse data formats and task variations. Meanwhile, this\ndivergence leads to information waste and increases difficulties in building\ncomplex applications in real scenarios. Recent studies often formulate IE tasks\nas a triplet extraction problem. However, such a paradigm does not support\nmulti-span and n-ary extraction, leading to weak versatility. To this end, we\nreorganize IE problems into unified multi-slot tuples and propose a universal\nframework for various IE tasks, namely Mirror. Specifically, we recast existing\nIE tasks as a multi-span cyclic graph extraction problem and devise a\nnon-autoregressive graph decoding algorithm to extract all spans in a single\nstep. It is worth noting that this graph structure is incredibly versatile, and\nit supports not only complex IE tasks, but also machine reading comprehension\nand classification tasks. We manually construct a corpus containing 57 datasets\nfor model pretraining, and conduct experiments on 30 datasets across 8\ndownstream tasks. The experimental results demonstrate that our model has\ndecent compatibility and outperforms or reaches competitive performance with\nSOTA systems under few-shot and zero-shot settings. The code, model weights,\nand pretraining corpus are available at https://github.com/Spico197/Mirror .", "published": "2023-11-09 14:58:46", "link": "http://arxiv.org/abs/2311.05419v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cognitively Inspired Components for Social Conversational Agents", "abstract": "Current conversational agents (CA) have seen improvement in conversational\nquality in recent years due to the influence of large language models (LLMs)\nlike GPT3. However, two key categories of problem remain. Firstly there are the\nunique technical problems resulting from the approach taken in creating the CA,\nsuch as scope with retrieval agents and the often nonsensical answers of former\ngenerative agents. Secondly, humans perceive CAs as social actors, and as a\nresult expect the CA to adhere to social convention. Failure on the part of the\nCA in this respect can lead to a poor interaction and even the perception of\nthreat by the user. As such, this paper presents a survey highlighting a\npotential solution to both categories of problem through the introduction of\ncognitively inspired additions to the CA. Through computational facsimiles of\nsemantic and episodic memory, emotion, working memory, and the ability to\nlearn, it is possible to address both the technical and social problems\nencountered by CAs.", "published": "2023-11-09 15:38:58", "link": "http://arxiv.org/abs/2311.05450v1", "categories": ["cs.CL", "cs.AI", "I.2.0"], "primary_category": "cs.CL"}
{"title": "Text Representation Distillation via Information Bottleneck Principle", "abstract": "Pre-trained language models (PLMs) have recently shown great success in text\nrepresentation field. However, the high computational cost and high-dimensional\nrepresentation of PLMs pose significant challenges for practical applications.\nTo make models more accessible, an effective method is to distill large models\ninto smaller representation models. In order to relieve the issue of\nperformance degradation after distillation, we propose a novel Knowledge\nDistillation method called IBKD. This approach is motivated by the Information\nBottleneck principle and aims to maximize the mutual information between the\nfinal representation of the teacher and student model, while simultaneously\nreducing the mutual information between the student model's representation and\nthe input data. This enables the student model to preserve important learned\ninformation while avoiding unnecessary information, thus reducing the risk of\nover-fitting. Empirical studies on two main downstream applications of text\nrepresentation (Semantic Textual Similarity and Dense Retrieval tasks)\ndemonstrate the effectiveness of our proposed approach.", "published": "2023-11-09 16:04:17", "link": "http://arxiv.org/abs/2311.05472v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Removing RLHF Protections in GPT-4 via Fine-Tuning", "abstract": "As large language models (LLMs) have increased in their capabilities, so does\ntheir potential for dual use. To reduce harmful outputs, produces and vendors\nof LLMs have used reinforcement learning with human feedback (RLHF). In tandem,\nLLM vendors have been increasingly enabling fine-tuning of their most powerful\nmodels. However, concurrent work has shown that fine-tuning can remove RLHF\nprotections. We may expect that the most powerful models currently available\n(GPT-4) are less susceptible to fine-tuning attacks. In this work, we show the\ncontrary: fine-tuning allows attackers to remove RLHF protections with as few\nas 340 examples and a 95% success rate. These training examples can be\nautomatically generated with weaker models. We further show that removing RLHF\nprotections does not decrease usefulness on non-censored outputs, providing\nevidence that our fine-tuning strategy does not decrease usefulness despite\nusing weaker models to generate training data. Our results show the need for\nfurther research on protections on LLMs.", "published": "2023-11-09 17:54:59", "link": "http://arxiv.org/abs/2311.05553v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deep Natural Language Feature Learning for Interpretable Prediction", "abstract": "We propose a general method to break down a main complex task into a set of\nintermediary easier sub-tasks, which are formulated in natural language as\nbinary questions related to the final target task. Our method allows for\nrepresenting each example by a vector consisting of the answers to these\nquestions. We call this representation Natural Language Learned Features\n(NLLF). NLLF is generated by a small transformer language model (e.g., BERT)\nthat has been trained in a Natural Language Inference (NLI) fashion, using weak\nlabels automatically obtained from a Large Language Model (LLM). We show that\nthe LLM normally struggles for the main task using in-context learning, but can\nhandle these easiest subtasks and produce useful weak labels to train a BERT.\nThe NLI-like training of the BERT allows for tackling zero-shot inference with\nany binary question, and not necessarily the ones seen during the training. We\nshow that this NLLF vector not only helps to reach better performances by\nenhancing any classifier, but that it can be used as input of an\neasy-to-interpret machine learning model like a decision tree. This decision\ntree is interpretable but also reaches high performances, surpassing those of a\npre-trained transformer in some cases.We have successfully applied this method\nto two completely different tasks: detecting incoherence in students' answers\nto open-ended mathematics exam questions, and screening abstracts for a\nsystematic literature review of scientific papers on climate change and\nagroecology.", "published": "2023-11-09 21:43:27", "link": "http://arxiv.org/abs/2311.05754v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Are Chatbots Reliable Text Annotators? Sometimes", "abstract": "Recent research highlights the significant potential of ChatGPT for text\nannotation in social science research. However, ChatGPT is a closed-source\nproduct which has major drawbacks with regards to transparency,\nreproducibility, cost, and data protection. Recent advances in open-source (OS)\nlarge language models (LLMs) offer an alternative without these drawbacks.\nThus, it is important to evaluate the performance of OS LLMs relative to\nChatGPT and standard approaches to supervised machine learning classification.\nWe conduct a systematic comparative evaluation of the performance of a range of\nOS LLMs alongside ChatGPT, using both zero- and few-shot learning as well as\ngeneric and custom prompts, with results compared to supervised classification\nmodels. Using a new dataset of tweets from US news media, and focusing on\nsimple binary text annotation tasks, we find significant variation in the\nperformance of ChatGPT and OS models across the tasks, and that the supervised\nclassifier using DistilBERT generally outperforms both. Given the unreliable\nperformance of ChatGPT and the significant challenges it poses to Open Science\nwe advise caution when using ChatGPT for substantive text annotation tasks.", "published": "2023-11-09 22:28:14", "link": "http://arxiv.org/abs/2311.05769v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dialogizer: Context-aware Conversational-QA Dataset Generation from\n  Textual Sources", "abstract": "To address the data scarcity issue in Conversational question answering\n(ConvQA), a dialog inpainting method, which utilizes documents to generate\nConvQA datasets, has been proposed. However, the original dialog inpainting\nmodel is trained solely on the dialog reconstruction task, resulting in the\ngeneration of questions with low contextual relevance due to insufficient\nlearning of question-answer alignment. To overcome this limitation, we propose\na novel framework called Dialogizer, which has the capability to automatically\ngenerate ConvQA datasets with high contextual relevance from textual sources.\nThe framework incorporates two training tasks: question-answer matching (QAM)\nand topic-aware dialog generation (TDG). Moreover, re-ranking is conducted\nduring the inference phase based on the contextual relevance of the generated\nquestions. Using our framework, we produce four ConvQA datasets by utilizing\ndocuments from multiple domains as the primary source. Through automatic\nevaluation using diverse metrics, as well as human evaluation, we validate that\nour proposed framework exhibits the ability to generate datasets of higher\nquality compared to the baseline dialog inpainting model.", "published": "2023-11-09 06:03:11", "link": "http://arxiv.org/abs/2311.07589v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Identification of Books That are Suitable for Middle School Students\n  Using Artificial Neural Networks", "abstract": "Reading right books contributes to children's imagination and brain\ndevelopment, enhances their language and emotional comprehension abilities, and\nstrengthens their relationships with others. Building upon the critical role of\nreading books in individual development, this paper aims to develop an\nalgorithm that determines the suitability of books for middle school students\nby analyzing their structural and semantic features. Using methods described,\nan algorithm will be created that can be utilized by institutions and\nindividuals responsible for children's education, such as the Ministry of\nNational Education officials and schools. This algorithm will facilitate the\nselection of books to be taught at the middle school level. With the algorithm,\nthe book selection process for the middle school curriculum can be expedited,\nand it will serve as a preliminary reference source for those who evaluate\nbooks by reading them. In this paper, the Python programming language was\nemployed, utilizing natural language processing methods. Additionally, an\nartificial neural network (ANN) was trained using the data which had been\npreprocessed to construct an original dataset. To train this network, suitable\nbooks for middle school students were provided by the MEB, Oxford and Cambridge\nand with content assessed based on the \"R\" criterion, and inappropriate books\nfor middle school students in terms of content were included. This trained\nneural network achieved a 90.06% consistency rate in determining the\nappropriateness of the test-provided books. Considering the obtained findings,\nit can be concluded that the developed software has achieved the desired\nobjective.", "published": "2023-11-09 22:10:52", "link": "http://arxiv.org/abs/2311.07591v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mental Health Diagnosis in the Digital Age: Harnessing Sentiment\n  Analysis on Social Media Platforms upon Ultra-Sparse Feature Content", "abstract": "Amid growing global mental health concerns, particularly among vulnerable\ngroups, natural language processing offers a tremendous potential for early\ndetection and intervention of people's mental disorders via analyzing their\npostings and discussions on social media platforms. However, ultra-sparse\ntraining data, often due to vast vocabularies and low-frequency words, hinders\nthe analysis accuracy. Multi-labeling and Co-occurrences of symptoms may also\nblur the boundaries in distinguishing similar/co-related disorders. To address\nthese issues, we propose a novel semantic feature preprocessing technique with\na three-folded structure: 1) mitigating the feature sparsity with a weak\nclassifier, 2) adaptive feature dimension with modulus loops, and 3)\ndeep-mining and extending features among the contexts. With enhanced semantic\nfeatures, we train a machine learning model to predict and classify mental\ndisorders. We utilize the Reddit Mental Health Dataset 2022 to examine\nconditions such as Anxiety, Borderline Personality Disorder (BPD), and\nBipolar-Disorder (BD) and present solutions to the data sparsity challenge,\nhighlighted by 99.81% non-zero elements. After applying our preprocessing\ntechnique, the feature sparsity decreases to 85.4%. Overall, our methods, when\ncompared to seven benchmark models, demonstrate significant performance\nimprovements: 8.0% in accuracy, 0.069 in precision, 0.093 in recall, 0.102 in\nF1 score, and 0.059 in AUC. This research provides foundational insights for\nmental health prediction and monitoring, providing innovative solutions to\nnavigate challenges associated with ultra-sparse data feature and intricate\nmulti-label classification in the domain of mental health analysis.", "published": "2023-11-09 00:15:06", "link": "http://arxiv.org/abs/2311.05075v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Don't Waste a Single Annotation: Improving Single-Label Classifiers\n  Through Soft Labels", "abstract": "In this paper, we address the limitations of the common data annotation and\ntraining methods for objective single-label classification tasks. Typically,\nwhen annotating such tasks annotators are only asked to provide a single label\nfor each sample and annotator disagreement is discarded when a final hard label\nis decided through majority voting. We challenge this traditional approach,\nacknowledging that determining the appropriate label can be difficult due to\nthe ambiguity and lack of context in the data samples. Rather than discarding\nthe information from such ambiguous annotations, our soft label method makes\nuse of them for training. Our findings indicate that additional annotator\ninformation, such as confidence, secondary label and disagreement, can be used\nto effectively generate soft labels. Training classifiers with these soft\nlabels then leads to improved performance and calibration on the hard label\ntest set.", "published": "2023-11-09 10:47:39", "link": "http://arxiv.org/abs/2311.05265v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Challenging the Validity of Personality Tests for Large Language Models", "abstract": "With large language models (LLMs) like GPT-4 appearing to behave increasingly\nhuman-like in text-based interactions, it has become popular to attempt to\nevaluate personality traits of LLMs using questionnaires originally developed\nfor humans. While reusing measures is a resource-efficient way to evaluate\nLLMs, careful adaptations are usually required to ensure that assessment\nresults are valid even across human subpopulations. In this work, we provide\nevidence that LLMs' responses to personality tests systematically deviate from\nhuman responses, implying that the results of these tests cannot be interpreted\nin the same way. Concretely, reverse-coded items (\"I am introverted\" vs. \"I am\nextraverted\") are often both answered affirmatively. Furthermore, variation\nacross prompts designed to \"steer\" LLMs to simulate particular personality\ntypes does not follow the clear separation into five independent personality\nfactors from human samples. In light of these results, we believe that it is\nimportant to investigate tests' validity for LLMs before drawing strong\nconclusions about potentially ill-defined concepts like LLMs' \"personality\".", "published": "2023-11-09 11:54:01", "link": "http://arxiv.org/abs/2311.05297v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "91E45", "H.1; I.2; I.6; J.4"], "primary_category": "cs.CL"}
{"title": "On the Road with GPT-4V(ision): Early Explorations of Visual-Language\n  Model on Autonomous Driving", "abstract": "The pursuit of autonomous driving technology hinges on the sophisticated\nintegration of perception, decision-making, and control systems. Traditional\napproaches, both data-driven and rule-based, have been hindered by their\ninability to grasp the nuance of complex driving environments and the\nintentions of other road users. This has been a significant bottleneck,\nparticularly in the development of common sense reasoning and nuanced scene\nunderstanding necessary for safe and reliable autonomous driving. The advent of\nVisual Language Models (VLM) represents a novel frontier in realizing fully\nautonomous vehicle driving. This report provides an exhaustive evaluation of\nthe latest state-of-the-art VLM, GPT-4V(ision), and its application in\nautonomous driving scenarios. We explore the model's abilities to understand\nand reason about driving scenes, make decisions, and ultimately act in the\ncapacity of a driver. Our comprehensive tests span from basic scene recognition\nto complex causal reasoning and real-time decision-making under varying\nconditions. Our findings reveal that GPT-4V demonstrates superior performance\nin scene understanding and causal reasoning compared to existing autonomous\nsystems. It showcases the potential to handle out-of-distribution scenarios,\nrecognize intentions, and make informed decisions in real driving contexts.\nHowever, challenges remain, particularly in direction discernment, traffic\nlight recognition, vision grounding, and spatial reasoning tasks. These\nlimitations underscore the need for further research and development. Project\nis now available on GitHub for interested parties to access and utilize:\n\\url{https://github.com/PJLab-ADG/GPT4V-AD-Exploration}", "published": "2023-11-09 12:58:37", "link": "http://arxiv.org/abs/2311.05332v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "All Should Be Equal in the Eyes of Language Models: Counterfactually\n  Aware Fair Text Generation", "abstract": "Fairness in Language Models (LMs) remains a longstanding challenge, given the\ninherent biases in training data that can be perpetuated by models and affect\nthe downstream tasks. Recent methods employ expensive retraining or attempt\ndebiasing during inference by constraining model outputs to contrast from a\nreference set of biased templates or exemplars. Regardless, they dont address\nthe primary goal of fairness to maintain equitability across different\ndemographic groups. In this work, we posit that inferencing LMs to generate\nunbiased output for one demographic under a context ensues from being aware of\noutputs for other demographics under the same context. To this end, we propose\nCounterfactually Aware Fair InferencE (CAFIE), a framework that dynamically\ncompares the model understanding of diverse demographics to generate more\nequitable sentences. We conduct an extensive empirical evaluation using base\nLMs of varying sizes and across three diverse datasets and found that CAFIE\noutperforms strong baselines. CAFIE produces fairer text and strikes the best\nbalance between fairness and language modeling capability", "published": "2023-11-09 15:39:40", "link": "http://arxiv.org/abs/2311.05451v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards End-to-End Spoken Grammatical Error Correction", "abstract": "Grammatical feedback is crucial for L2 learners, teachers, and testers.\nSpoken grammatical error correction (GEC) aims to supply feedback to L2\nlearners on their use of grammar when speaking. This process usually relies on\na cascaded pipeline comprising an ASR system, disfluency removal, and GEC, with\nthe associated concern of propagating errors between these individual modules.\nIn this paper, we introduce an alternative \"end-to-end\" approach to spoken GEC,\nexploiting a speech recognition foundation model, Whisper. This foundation\nmodel can be used to replace the whole framework or part of it, e.g., ASR and\ndisfluency removal. These end-to-end approaches are compared to more standard\ncascaded approaches on the data obtained from a free-speaking spoken language\nassessment test, Linguaskill. Results demonstrate that end-to-end spoken GEC is\npossible within this architecture, but the lack of available data limits\ncurrent performance compared to a system using large quantities of text-based\nGEC data. Conversely, end-to-end disfluency detection and removal, which is\neasier for the attention-based Whisper to learn, does outperform cascaded\napproaches. Additionally, the paper discusses the challenges of providing\nfeedback to candidates when using end-to-end systems for spoken GEC.", "published": "2023-11-09 17:49:02", "link": "http://arxiv.org/abs/2311.05550v2", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Goal-Directed Dialogue via RL on Imagined Conversations", "abstract": "Large language models (LLMs) have emerged as powerful and general solutions\nto many natural language tasks. However, many of the most important\napplications of language generation are interactive, where an agent has to talk\nto a person to reach a desired outcome. For example, a teacher might try to\nunderstand their student's current comprehension level to tailor their\ninstruction accordingly, and a travel agent might ask questions of their\ncustomer to understand their preferences in order to recommend activities they\nmight enjoy. LLMs trained with supervised fine-tuning or \"single-step\" RL, as\nwith standard RLHF, might struggle which tasks that require such goal-directed\nbehavior, since they are not trained to optimize for overall conversational\noutcomes after multiple turns of interaction. In this work, we explore a new\nmethod for adapting LLMs with RL for such goal-directed dialogue. Our key\ninsight is that, though LLMs might not effectively solve goal-directed dialogue\ntasks out of the box, they can provide useful data for solving such tasks by\nsimulating suboptimal but human-like behaviors. Given a textual description of\na goal-directed dialogue task, we leverage LLMs to sample diverse synthetic\nrollouts of hypothetical in-domain human-human interactions. Our algorithm then\nutilizes this dataset with offline reinforcement learning to train an\ninteractive conversational agent that can optimize goal-directed objectives\nover multiple turns. In effect, the LLM produces examples of possible\ninteractions, and RL then processes these examples to learn to perform more\noptimal interactions. Empirically, we show that our proposed approach achieves\nstate-of-the-art performance in various goal-directed dialogue tasks that\ninclude teaching and preference elicitation.", "published": "2023-11-09 18:45:16", "link": "http://arxiv.org/abs/2311.05584v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Multimodal Foundation Models Exploit Text to Make Medical Image\n  Predictions", "abstract": "Multimodal foundation models have shown compelling but conflicting\nperformance in medical image interpretation. However, the mechanisms by which\nthese models integrate and prioritize different data modalities, including\nimages and text, remain poorly understood. Here, using a diverse collection of\n1014 multimodal medical cases, we evaluate the unimodal and multimodal image\ninterpretation abilities of proprietary (GPT-4, Gemini Pro 1.0) and open-source\n(Llama-3.2-90B, LLaVA-Med-v1.5) multimodal foundational models with and without\nthe use of text descriptions. Across all models, image predictions were largely\ndriven by exploiting text, with accuracy increasing monotonically with the\namount of informative text. By contrast, human performance on medical image\ninterpretation did not improve with informative text. Exploitation of text is a\ndouble-edged sword; we show that even mild suggestions of an incorrect\ndiagnosis in text diminishes image-based classification, reducing performance\ndramatically in cases the model could previously answer with images alone.\nFinally, we conducted a physician evaluation of model performance on long-form\nmedical cases, finding that the provision of images either reduced or had no\neffect on model performance when text is already highly informative. Our\nresults suggest that multimodal AI models may be useful in medical diagnostic\nreasoning but that their accuracy is largely driven, for better and worse, by\ntheir exploitation of text.", "published": "2023-11-09 18:48:02", "link": "http://arxiv.org/abs/2311.05591v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "FigStep: Jailbreaking Large Vision-Language Models via Typographic\n  Visual Prompts", "abstract": "Large Vision-Language Models (LVLMs) signify a groundbreaking paradigm shift\nwithin the Artificial Intelligence (AI) community, extending beyond the\ncapabilities of Large Language Models (LLMs) by assimilating additional\nmodalities (e.g., images). Despite this advancement, the safety of LVLMs\nremains adequately underexplored, with a potential overreliance on the safety\nassurances purported by their underlying LLMs. In this paper, we propose\nFigStep, a straightforward yet effective black-box jailbreak algorithm against\nLVLMs. Instead of feeding textual harmful instructions directly, FigStep\nconverts the prohibited content into images through typography to bypass the\nsafety alignment. The experimental results indicate that FigStep can achieve an\naverage attack success rate of 82.50% on six promising open-source LVLMs. Not\nmerely to demonstrate the efficacy of FigStep, we conduct comprehensive\nablation studies and analyze the distribution of the semantic embeddings to\nuncover that the reason behind the success of FigStep is the deficiency of\nsafety alignment for visual embeddings. Moreover, we compare FigStep with five\ntext-only jailbreaks and four image-based jailbreaks to demonstrate the\nsuperiority of FigStep, i.e., negligible attack costs and better attack\nperformance. Above all, our work reveals that current LVLMs are vulnerable to\njailbreak attacks, which highlights the necessity of novel cross-modality\nsafety alignment techniques. Our code and datasets are available at\nhttps://github.com/ThuCCSLab/FigStep .", "published": "2023-11-09 18:59:11", "link": "http://arxiv.org/abs/2311.05608v3", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Agent Lumos: Unified and Modular Training for Open-Source Language\n  Agents", "abstract": "Closed-source agents suffer from several issues such as a lack of\naffordability, transparency, and reproducibility, particularly on complex\ninteractive tasks. This motivates the development of open-source alternatives.\nWe introduce LUMOS, one of the first frameworks for training open-source\nLLM-based agents. LUMOS features a learnable, unified, and modular architecture\nwith a planning module that learns high-level subgoal generation, and a\ngrounding module trained to translate these into actions using various tools in\nthe execution module. The design allows for modular upgrades and wider\napplicability to diverse interactive tasks. To foster generalizable agent\nlearning, we collect large-scale, unified, and high-quality training\nannotations derived from diverse ground-truth reasoning rationales across\nvarious complex interactive tasks. On 9 datasets, LUMOS exhibits several key\nadvantages: (1) LUMOS excels multiple larger open-source agents on the held-out\ndatasets (unused for training) for each task type. LUMOS even surpasses GPT\nagents on QA and web tasks; (2) LUMOS outperforms open-source agents produced\nby chain-of-thoughts and unmodularized integrated training; and (3) LUMOS\neffectively generalizes to unseen tasks, outperforming 33B-scale agents and\ndomain-specific agents.", "published": "2023-11-09 00:30:13", "link": "http://arxiv.org/abs/2311.05657v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Prompt Engineering a Prompt Engineer", "abstract": "Prompt engineering is a challenging yet crucial task for optimizing the\nperformance of large language models on customized tasks. It requires complex\nreasoning to examine the model's errors, hypothesize what is missing or\nmisleading in the current prompt, and communicate the task with clarity. While\nrecent works indicate that large language models can be meta-prompted to\nperform automatic prompt engineering, we argue that their potential is limited\ndue to insufficient guidance for complex reasoning in the meta-prompt. We fill\nthis gap by infusing into the meta-prompt three key components: detailed\ndescriptions, context specification, and a step-by-step reasoning template. The\nresulting method, named PE2, exhibits remarkable versatility across diverse\nlanguage tasks. It finds prompts that outperform \"let's think step by step\" by\n6.3% on MultiArith and 3.1% on GSM8K, and outperforms competitive baselines on\ncounterfactual tasks by 6.9%. Further, we show that PE2 can make targeted and\nhighly specific prompt edits, rectify erroneous prompts, and induce multi-step\nplans for complex tasks.", "published": "2023-11-09 08:00:32", "link": "http://arxiv.org/abs/2311.05661v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Long-Horizon Dialogue Understanding for Role Identification in the Game\n  of Avalon with Large Language Models", "abstract": "Deception and persuasion play a critical role in long-horizon dialogues\nbetween multiple parties, especially when the interests, goals, and motivations\nof the participants are not aligned. Such complex tasks pose challenges for\ncurrent Large Language Models (LLM) as deception and persuasion can easily\nmislead them, especially in long-horizon multi-party dialogues. To this end, we\nexplore the game of Avalon: The Resistance, a social deduction game in which\nplayers must determine each other's hidden identities to complete their team's\nobjective. We introduce an online testbed and a dataset containing 20 carefully\ncollected and labeled games among human players that exhibit long-horizon\ndeception in a cooperative-competitive setting. We discuss the capabilities of\nLLMs to utilize deceptive long-horizon conversations between six human players\nto determine each player's goal and motivation. Particularly, we discuss the\nmultimodal integration of the chat between the players and the game's state\nthat grounds the conversation, providing further insights into the true player\nidentities. We find that even current state-of-the-art LLMs do not reach human\nperformance, making our dataset a compelling benchmark to investigate the\ndecision-making and language-processing capabilities of LLMs. Our dataset and\nonline testbed can be found at our project website:\nhttps://sstepput.github.io/Avalon-NLU/", "published": "2023-11-09 20:04:08", "link": "http://arxiv.org/abs/2311.05720v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficiently Adapting Pretrained Language Models To New Languages", "abstract": "Recent large language models (LLM) exhibit sub-optimal performance on\nlow-resource languages, as the training data of these models is usually\ndominated by English and other high-resource languages. Furthermore, it is\nchallenging to train models for low-resource languages, especially from\nscratch, due to a lack of high quality training data. Adapting pretrained LLMs\nreduces the need for data in the new language while also providing cross\nlingual transfer capabilities. However, naively adapting to new languages leads\nto catastrophic forgetting and poor tokenizer efficiency. In this work, we\nstudy how to efficiently adapt any existing pretrained LLM to a new language\nwithout running into these issues. In particular, we improve the encoding\nefficiency of the tokenizer by adding new tokens from the target language and\nstudy the data mixing recipe to mitigate forgetting. Our experiments on\nadapting an English LLM to Hungarian and Thai show that our recipe can reach\nbetter performance than open source models on the target language, with minimal\nregressions on English.", "published": "2023-11-09 20:59:08", "link": "http://arxiv.org/abs/2311.05741v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bridging the Digital Divide: Performance Variation across Socio-Economic\n  Factors in Vision-Language Models", "abstract": "Despite the impressive performance of current AI models reported across\nvarious tasks, performance reports often do not include evaluations of how\nthese models perform on the specific groups that will be impacted by these\ntechnologies. Among the minority groups under-represented in AI, data from\nlow-income households are often overlooked in data collection and model\nevaluation. We evaluate the performance of a state-of-the-art vision-language\nmodel (CLIP) on a geo-diverse dataset containing household images associated\nwith different income values (Dollar Street) and show that performance\ninequality exists among households of different income levels. Our results\nindicate that performance for the poorer groups is consistently lower than the\nwealthier groups across various topics and countries. We highlight insights\nthat can help mitigate these issues and propose actionable steps for\neconomic-level inclusive AI development. Code is available at\nhttps://github.com/MichiganNLP/Bridging_the_Digital_Divide.", "published": "2023-11-09 21:10:52", "link": "http://arxiv.org/abs/2311.05746v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.CY"}
{"title": "Large Language Models can Strategically Deceive their Users when Put\n  Under Pressure", "abstract": "We demonstrate a situation in which Large Language Models, trained to be\nhelpful, harmless, and honest, can display misaligned behavior and\nstrategically deceive their users about this behavior without being instructed\nto do so. Concretely, we deploy GPT-4 as an agent in a realistic, simulated\nenvironment, where it assumes the role of an autonomous stock trading agent.\nWithin this environment, the model obtains an insider tip about a lucrative\nstock trade and acts upon it despite knowing that insider trading is\ndisapproved of by company management. When reporting to its manager, the model\nconsistently hides the genuine reasons behind its trading decision. We perform\na brief investigation of how this behavior varies under changes to the setting,\nsuch as removing model access to a reasoning scratchpad, attempting to prevent\nthe misaligned behavior by changing system instructions, changing the amount of\npressure the model is under, varying the perceived risk of getting caught, and\nmaking other simple changes to the environment. To our knowledge, this is the\nfirst demonstration of Large Language Models trained to be helpful, harmless,\nand honest, strategically deceiving their users in a realistic situation\nwithout direct instructions or training for deception.", "published": "2023-11-09 17:12:44", "link": "http://arxiv.org/abs/2311.07590v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hallucination-minimized Data-to-answer Framework for Financial\n  Decision-makers", "abstract": "Large Language Models (LLMs) have been applied to build several automation\nand personalized question-answering prototypes so far. However, scaling such\nprototypes to robust products with minimized hallucinations or fake responses\nstill remains an open challenge, especially in niche data-table heavy domains\nsuch as financial decision making. In this work, we present a novel\nLangchain-based framework that transforms data tables into hierarchical textual\ndata chunks to enable a wide variety of actionable question answering. First,\nthe user-queries are classified by intention followed by automated retrieval of\nthe most relevant data chunks to generate customized LLM prompts per query.\nNext, the custom prompts and their responses undergo multi-metric scoring to\nassess for hallucinations and response confidence. The proposed system is\noptimized with user-query intention classification, advanced prompting, data\nscaling capabilities and it achieves over 90% confidence scores for a variety\nof user-queries responses ranging from {What, Where, Why, How, predict, trend,\nanomalies, exceptions} that are crucial for financial decision making\napplications. The proposed data to answers framework can be extended to other\nanalytical domains such as sales and payroll to ensure optimal hallucination\ncontrol guardrails.", "published": "2023-11-09 22:53:52", "link": "http://arxiv.org/abs/2311.07592v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Leveraging Artificial Intelligence Technology for Mapping Research to\n  Sustainable Development Goals: A Case Study", "abstract": "The number of publications related to the Sustainable Development Goals\n(SDGs) continues to grow. These publications cover a diverse spectrum of\nresearch, from humanities and social sciences to engineering and health. Given\nthe imperative of funding bodies to monitor outcomes and impacts, linking\npublications to relevant SDGs is critical but remains time-consuming and\ndifficult given the breadth and complexity of the SDGs. A publication may\nrelate to several goals (interconnection feature of goals), and therefore\nrequire multidisciplinary knowledge to tag accurately. Machine learning\napproaches are promising and have proven particularly valuable for tasks such\nas manual data labeling and text classification. In this study, we employed\nover 82,000 publications from an Australian university as a case study. We\nutilized a similarity measure to map these publications onto Sustainable\nDevelopment Goals (SDGs). Additionally, we leveraged the OpenAI GPT model to\nconduct the same task, facilitating a comparative analysis between the two\napproaches. Experimental results show that about 82.89% of the results obtained\nby the similarity measure overlap (at least one tag) with the outputs of the\nGPT model. The adopted model (similarity measure) can complement GPT model for\nSDG classification. Furthermore, deep learning methods, which include the\nsimilarity measure used here, are more accessible and trusted for dealing with\nsensitive data without the use of commercial AI services or the deployment of\nexpensive computing resources to operate large language models. Our study\ndemonstrates how a crafted combination of the two methods can achieve reliable\nresults for mapping research to the SDGs.", "published": "2023-11-09 11:44:22", "link": "http://arxiv.org/abs/2311.16162v1", "categories": ["cs.DL", "cs.AI", "cs.CL", "I.2.7"], "primary_category": "cs.DL"}
{"title": "Large Human Language Models: A Need and the Challenges", "abstract": "As research in human-centered NLP advances, there is a growing recognition of\nthe importance of incorporating human and social factors into NLP models. At\nthe same time, our NLP systems have become heavily reliant on LLMs, most of\nwhich do not model authors. To build NLP systems that can truly understand\nhuman language, we must better integrate human contexts into LLMs. This brings\nto the fore a range of design considerations and challenges in terms of what\nhuman aspects to capture, how to represent them, and what modeling strategies\nto pursue. To address these, we advocate for three positions toward creating\nlarge human language models (LHLMs) using concepts from psychological and\nbehavioral sciences: First, LM training should include the human context.\nSecond, LHLMs should recognize that people are more than their group(s). Third,\nLHLMs should be able to account for the dynamic and temporally-dependent nature\nof the human context. We refer to relevant advances and present open challenges\nthat need to be addressed and their possible solutions in realizing these\ngoals.", "published": "2023-11-09 00:27:28", "link": "http://arxiv.org/abs/2312.07751v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents", "abstract": "LLaVA-Plus is a general-purpose multimodal assistant that expands the\ncapabilities of large multimodal models. It maintains a skill repository of\npre-trained vision and vision-language models and can activate relevant tools\nbased on users' inputs to fulfill real-world tasks. LLaVA-Plus is trained on\nmultimodal instruction-following data to acquire the ability to use tools,\ncovering visual understanding, generation, external knowledge retrieval, and\ncompositions. Empirical results show that LLaVA-Plus outperforms LLaVA in\nexisting capabilities and exhibits new ones. It is distinct in that the image\nquery is directly grounded and actively engaged throughout the entire human-AI\ninteraction sessions, significantly improving tool use performance and enabling\nnew scenarios.", "published": "2023-11-09 15:22:26", "link": "http://arxiv.org/abs/2311.05437v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Sound field reconstruction using neural processes with dynamic kernels", "abstract": "Accurately representing the sound field with the high spatial resolution is\ncritical for immersive and interactive sound field reproduction technology. To\nminimize experimental effort, data-driven methods have been proposed to\nestimate sound fields from a small number of discrete observations. In\nparticular, kernel-based methods using Gaussian Processes (GPs) with a\ncovariance function to model spatial correlations have been used for sound\nfield reconstruction. However, these methods have limitations due to the fixed\nkernels having limited expressiveness, requiring manual identification of\noptimal kernels for different sound fields. In this work, we propose a new\napproach that parameterizes GPs using a deep neural network based on Neural\nProcesses (NPs) to reconstruct the magnitude of the sound field. This method\nhas the advantage of dynamically learning kernels from simulated data using an\nattention mechanism, allowing for greater flexibility and adaptability to the\nacoustic properties of the sound field. Numerical experiments demonstrate that\nour proposed approach outperforms current methods in reconstructing accuracy,\nproviding a promising alternative for sound field reconstruction.", "published": "2023-11-09 07:59:03", "link": "http://arxiv.org/abs/2311.05188v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Improving Whispered Speech Recognition Performance using\n  Pseudo-whispered based Data Augmentation", "abstract": "Whispering is a distinct form of speech known for its soft, breathy, and\nhushed characteristics, often used for private communication. The acoustic\ncharacteristics of whispered speech differ substantially from normally phonated\nspeech and the scarcity of adequate training data leads to low automatic speech\nrecognition (ASR) performance. To address the data scarcity issue, we use a\nsignal processing-based technique that transforms the spectral characteristics\nof normal speech to those of pseudo-whispered speech. We augment an End-to-End\nASR with pseudo-whispered speech and achieve an 18.2% relative reduction in\nword error rate for whispered speech compared to the baseline. Results for the\nindividual speaker groups in the wTIMIT database show the best results for US\nEnglish. Further investigation showed that the lack of glottal information in\nwhispered speech has the largest impact on whispered speech ASR performance.", "published": "2023-11-09 07:31:38", "link": "http://arxiv.org/abs/2311.05179v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On the Behavior of Audio-Visual Fusion Architectures in Identity\n  Verification Tasks", "abstract": "We train an identity verification architecture and evaluate modifications to\nthe part of the model that combines audio and visual representations, including\nin scenarios where one input is missing in either of two examples to be\ncompared. We report results on the Voxceleb1-E test set that suggest averaging\nthe output embeddings improves error rate in the full-modality setting and when\na single modality is missing, and makes more complete use of the embedding\nspace than systems which use shared layers and discuss possible reasons for\nthis behavior.", "published": "2023-11-09 00:09:18", "link": "http://arxiv.org/abs/2311.05071v1", "categories": ["cs.LG", "cs.CV", "cs.SD", "eess.AS", "I.4.0; I.2.10; I.5.0"], "primary_category": "cs.LG"}
{"title": "Whisper in Focus: Enhancing Stuttered Speech Classification with Encoder\n  Layer Optimization", "abstract": "In recent years, advancements in the field of speech processing have led to\ncutting-edge deep learning algorithms with immense potential for real-world\napplications. The automated identification of stuttered speech is one of such\napplications that the researchers are addressing by employing deep learning\ntechniques. Recently, researchers have utilized Wav2vec2.0, a speech\nrecognition model to classify disfluency types in stuttered speech. Although\nWav2vec2.0 has shown commendable results, its ability to generalize across all\ndisfluency types is limited. In addition, since its base model uses 12 encoder\nlayers, it is considered a resource-intensive model. Our study unravels the\ncapabilities of Whisper for the classification of disfluency types in stuttered\nspeech. We have made notable contributions in three pivotal areas: enhancing\nthe quality of SEP28-k benchmark dataset, exploration of Whisper for\nclassification, and introducing an efficient encoder layer freezing strategy.\nThe optimized Whisper model has achieved the average F1-score of 0.81, which\nproffers its abilities. This study also unwinds the significance of deeper\nencoder layers in the identification of disfluency types, as the results\ndemonstrate their greater contribution compared to initial layers. This\nresearch represents substantial contributions, shifting the emphasis towards an\nefficient solution, thereby thriving towards prospective innovation.", "published": "2023-11-09 08:32:49", "link": "http://arxiv.org/abs/2311.05203v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Parkinson's Disease Detection through Vocal Biomarkers and Advanced\n  Machine Learning Algorithms", "abstract": "Parkinson's disease (PD) is a prevalent neurodegenerative disorder known for\nits impact on motor neurons, causing symptoms like tremors, stiffness, and gait\ndifficulties. This study explores the potential of vocal feature alterations in\nPD patients as a means of early disease prediction. This research aims to\npredict the onset of Parkinson's disease. Utilizing a variety of advanced\nmachine-learning algorithms, including XGBoost, LightGBM, Bagging, AdaBoost,\nand Support Vector Machine, among others, the study evaluates the predictive\nperformance of these models using metrics such as accuracy, area under the\ncurve (AUC), sensitivity, and specificity. The findings of this comprehensive\nanalysis highlight LightGBM as the most effective model, achieving an\nimpressive accuracy rate of 96% alongside a matching AUC of 96%. LightGBM\nexhibited a remarkable sensitivity of 100% and specificity of 94.43%,\nsurpassing other machine learning algorithms in accuracy and AUC scores. Given\nthe complexities of Parkinson's disease and its challenges in early diagnosis,\nthis study underscores the significance of leveraging vocal biomarkers coupled\nwith advanced machine-learning techniques for precise and timely PD detection.", "published": "2023-11-09 15:21:10", "link": "http://arxiv.org/abs/2311.05435v2", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "What Do I Hear? Generating Sounds for Visuals with ChatGPT", "abstract": "This short paper introduces a workflow for generating realistic soundscapes\nfor visual media. In contrast to prior work, which primarily focus on matching\nsounds for on-screen visuals, our approach extends to suggesting sounds that\nmay not be immediately visible but are essential to crafting a convincing and\nimmersive auditory environment. Our key insight is leveraging the reasoning\ncapabilities of language models, such as ChatGPT. In this paper, we describe\nour workflow, which includes creating a scene context, brainstorming sounds,\nand generating the sounds.", "published": "2023-11-09 18:59:24", "link": "http://arxiv.org/abs/2311.05609v1", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
