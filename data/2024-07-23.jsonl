{"title": "Automated Market Making and Decentralized Finance", "abstract": "Automated market makers (AMMs) are a new type of trading venues which are\nrevolutionising the way market participants interact. At present, the majority\nof AMMs are constant function market makers (CFMMs) where a deterministic\ntrading function determines how markets are cleared. Within CFMMs, we focus on\nconstant product market makers (CPMMs) which implements the concentrated\nliquidity (CL) feature. In this thesis we formalise and study the trading\nmechanism of CPMMs with CL, and we develop liquidity provision and liquidity\ntaking strategies. Our models are motivated and tested with market data.\n  We derive optimal strategies for liquidity takers (LTs) who trade orders of\nlarge size and execute statistical arbitrages. First, we consider an LT who\ntrades in a CPMM with CL and uses the dynamics of prices in competing venues as\nmarket signals. We use Uniswap v3 data to study price, liquidity, and trading\ncost dynamics, and to motivate the model. Next, we consider an LT who trades a\nbasket of crypto-currencies whose constituents co-move. We use market data to\nstudy lead-lag effects, spillover effects, and causality between trading\nvenues.\n  We derive optimal strategies for strategic liquidity providers (LPs) who\nprovide liquidity in CPMM with CL. First, we use stochastic control tools to\nderive a self-financing and closed-form optimal liquidity provision strategy\nwhere the width of the LP's liquidity range is determined by the profitability\nof the pool, the dynamics of the LP's position, and concentration risk. Next,\nwe use a model-free approach to solve the problem of an LP who provides\nliquidity in multiple CPMMs with CL. We do not specify a model for the\nstochastic processes observed by LPs, and use a long short-term memory (LSTM)\nneural network to approximate the optimal liquidity provision strategy.", "published": "2024-07-23 23:38:19", "link": "http://arxiv.org/abs/2407.16885v1", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "Stablecoin Runs and Disclosure Policy in the Presence of Large Sales", "abstract": "Stablecoins have historically depegged due from par to large sales, possibly\nof speculative nature, or poor reserve asset quality. Using a global game which\naddresses both concerns, we show that the selling pressure on stablecoin\nholders increases in the presence of a large sale. While precise public\nknowledge reduces (increases) the probability of a run when fundamentals are\nstrong (weak), interestingly, more precise private signals increase (reduce)\nthe probability of a run when fundamentals are strong (weak), potentially\nexplaining the stability of opaque stablecoins. The total run probability can\nbe decomposed into components representing risks from large sales and poor\ncollateral. By analyzing how these risk components vary with respect to\ninformation uncertainty and fundamentals, we can split the fundamental space\ninto regions based on the type of risk a stablecoin issuer is more prone to. We\nsuggest testable implications and connect our model's implications to\nreal-world applications, including depegging events and the no-questions-asked\nproperty of money.", "published": "2024-07-23 19:37:02", "link": "http://arxiv.org/abs/2408.07227v1", "categories": ["q-fin.TR", "econ.TH"], "primary_category": "q-fin.TR"}
{"title": "The Hybrid Forecast of S&P 500 Volatility ensembled from VIX, GARCH and LSTM models", "abstract": "Predicting the S&P 500 index volatility is crucial for investors and\nfinancial analysts as it helps assess market risk and make informed investment\ndecisions. Volatility represents the level of uncertainty or risk related to\nthe size of changes in a security's value, making it an essential indicator for\nfinancial planning. This study explores four methods to improve the accuracy of\nvolatility forecasts for the S&P 500: the established GARCH model, known for\ncapturing historical volatility patterns; an LSTM network that utilizes past\nvolatility and log returns; a hybrid LSTM-GARCH model that combines the\nstrengths of both approaches; and an advanced version of the hybrid model that\nalso factors in the VIX index to gauge market sentiment. This analysis is based\non a daily dataset that includes S&P 500 and VIX index data, covering the\nperiod from January 3, 2000, to December 21, 2023. Through rigorous testing and\ncomparison, we found that machine learning approaches, particularly the hybrid\nLSTM models, significantly outperform the traditional GARCH model. Including\nthe VIX index in the hybrid model further enhances its forecasting ability by\nincorporating real-time market sentiment. The results of this study offer\nvaluable insights for achieving more accurate volatility predictions, enabling\nbetter risk management and strategic investment decisions in the volatile\nenvironment of the S&P 500.", "published": "2024-07-23 18:28:16", "link": "http://arxiv.org/abs/2407.16780v1", "categories": ["q-fin.TR", "q-fin.PM", "q-fin.ST", "stat.ML"], "primary_category": "q-fin.TR"}
{"title": "The Negative Drift of a Limit Order Fill", "abstract": "Market making refers to a form of trading in financial markets characterized\nby passive orders which add liquidity to limit order books. Market makers are\nimportant for the proper functioning of financial markets worldwide. Given the\nimportance, financial mathematics has endeavored to derive optimal strategies\nfor placing limit orders in this context. This paper identifies a key\ndiscrepancy between popular model assumptions and the realities of real\nmarkets, specifically regarding the dynamics around limit order fills.\nTraditionally, market making models rely on an assumption of low-cost random\nfills, when in reality we observe a high-cost non-random fill behavior. Namely,\nlimit order fills are caused by and coincide with adverse price movements,\nwhich create a drag on the market maker's profit and loss. We refer to this\nphenomenon as \"the negative drift\" associated with limit order fills. We\ndescribe a discrete market model and prove theoretically that the negative\ndrift exists. We also provide a detailed empirical simulation using one of the\nmost traded financial instruments in the world, the 10 Year US Treasury Bond\nfutures, which also confirms its existence. To our knowledge, this is the first\npaper to describe and prove this phenomenon in such detail.", "published": "2024-07-23 14:39:42", "link": "http://arxiv.org/abs/2407.16527v1", "categories": ["q-fin.MF", "q-fin.ST", "q-fin.TR"], "primary_category": "q-fin.MF"}
{"title": "Reinforcement Learning Pair Trading: A Dynamic Scaling approach", "abstract": "Cryptocurrency is a cryptography-based digital asset with extremely volatile\nprices. Around USD 70 billion worth of cryptocurrency is traded daily on\nexchanges. Trading cryptocurrency is difficult due to the inherent volatility\nof the crypto market. This study investigates whether Reinforcement Learning\n(RL) can enhance decision-making in cryptocurrency algorithmic trading compared\nto traditional methods. In order to address this question, we combined\nreinforcement learning with a statistical arbitrage trading technique, pair\ntrading, which exploits the price difference between statistically correlated\nassets. We constructed RL environments and trained RL agents to determine when\nand how to trade pairs of cryptocurrencies. We developed new reward shaping and\nobservation/action spaces for reinforcement learning. We performed experiments\nwith the developed reinforcement learner on pairs of BTC-GBP and BTC-EUR data\nseparated by 1 min intervals (n=263,520). The traditional non-RL pair trading\ntechnique achieved an annualized profit of 8.33%, while the proposed RL-based\npair trading technique achieved annualized profits from 9.94% to 31.53%,\ndepending upon the RL learner. Our results show that RL can significantly\noutperform manual and traditional pair trading techniques when applied to\nvolatile markets such as~cryptocurrencies.", "published": "2024-07-23 00:16:27", "link": "http://arxiv.org/abs/2407.16103v2", "categories": ["q-fin.CP", "cs.LG", "q-fin.TR", "91-08"], "primary_category": "q-fin.CP"}
{"title": "CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for\n  Literature Review Support", "abstract": "Literature review requires researchers to synthesize a large amount of\ninformation and is increasingly challenging as the scientific literature\nexpands. In this work, we investigate the potential of LLMs for producing\nhierarchical organizations of scientific studies to assist researchers with\nliterature review. We define hierarchical organizations as tree structures\nwhere nodes refer to topical categories and every node is linked to the studies\nassigned to that category. Our naive LLM-based pipeline for hierarchy\ngeneration from a set of studies produces promising yet imperfect hierarchies,\nmotivating us to collect CHIME, an expert-curated dataset for this task focused\non biomedicine. Given the challenging and time-consuming nature of building\nhierarchies from scratch, we use a human-in-the-loop process in which experts\ncorrect errors (both links between categories and study assignment) in\nLLM-generated hierarchies. CHIME contains 2,174 LLM-generated hierarchies\ncovering 472 topics, and expert-corrected hierarchies for a subset of 100\ntopics. Expert corrections allow us to quantify LLM performance, and we find\nthat while they are quite good at generating and organizing categories, their\nassignment of studies to categories could be improved. We attempt to train a\ncorrector model with human feedback which improves study assignment by 12.6 F1\npoints. We release our dataset and models to encourage research on developing\nbetter assistive tools for literature review.", "published": "2024-07-23 03:18:00", "link": "http://arxiv.org/abs/2407.16148v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DDK: Distilling Domain Knowledge for Efficient Large Language Models", "abstract": "Despite the advanced intelligence abilities of large language models (LLMs)\nin various applications, they still face significant computational and storage\ndemands. Knowledge Distillation (KD) has emerged as an effective strategy to\nimprove the performance of a smaller LLM (i.e., the student model) by\ntransferring knowledge from a high-performing LLM (i.e., the teacher model).\nPrevailing techniques in LLM distillation typically use a black-box model API\nto generate high-quality pretrained and aligned datasets, or utilize white-box\ndistillation by altering the loss function to better transfer knowledge from\nthe teacher LLM. However, these methods ignore the knowledge differences\nbetween the student and teacher LLMs across domains. This results in excessive\nfocus on domains with minimal performance gaps and insufficient attention to\ndomains with large gaps, reducing overall performance. In this paper, we\nintroduce a new LLM distillation framework called DDK, which dynamically\nadjusts the composition of the distillation dataset in a smooth manner\naccording to the domain performance differences between the teacher and student\nmodels, making the distillation process more stable and effective. Extensive\nevaluations show that DDK significantly improves the performance of student\nmodels, outperforming both continuously pretrained baselines and existing\nknowledge distillation methods by a large margin.", "published": "2024-07-23 03:47:28", "link": "http://arxiv.org/abs/2407.16154v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robust Privacy Amidst Innovation with Large Language Models Through a\n  Critical Assessment of the Risks", "abstract": "This study examines integrating EHRs and NLP with large language models\n(LLMs) to improve healthcare data management and patient care. It focuses on\nusing advanced models to create secure, HIPAA-compliant synthetic patient notes\nfor biomedical research. The study used de-identified and re-identified MIMIC\nIII datasets with GPT-3.5, GPT-4, and Mistral 7B to generate synthetic notes.\nText generation employed templates and keyword extraction for contextually\nrelevant notes, with one-shot generation for comparison. Privacy assessment\nchecked PHI occurrence, while text utility was tested using an ICD-9 coding\ntask. Text quality was evaluated with ROUGE and cosine similarity metrics to\nmeasure semantic similarity with source notes. Analysis of PHI occurrence and\ntext utility via the ICD-9 coding task showed that the keyword-based method had\nlow risk and good performance. One-shot generation showed the highest PHI\nexposure and PHI co-occurrence, especially in geographic location and date\ncategories. The Normalized One-shot method achieved the highest classification\naccuracy. Privacy analysis revealed a critical balance between data utility and\nprivacy protection, influencing future data use and sharing. Re-identified data\nconsistently outperformed de-identified data. This study demonstrates the\neffectiveness of keyword-based methods in generating privacy-protecting\nsynthetic clinical notes that retain data usability, potentially transforming\nclinical data-sharing practices. The superior performance of re-identified over\nde-identified data suggests a shift towards methods that enhance utility and\nprivacy by using dummy PHIs to perplex privacy attacks.", "published": "2024-07-23 04:20:14", "link": "http://arxiv.org/abs/2407.16166v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Progressively Modality Freezing for Multi-Modal Entity Alignment", "abstract": "Multi-Modal Entity Alignment aims to discover identical entities across\nheterogeneous knowledge graphs. While recent studies have delved into fusion\nparadigms to represent entities holistically, the elimination of features\nirrelevant to alignment and modal inconsistencies is overlooked, which are\ncaused by inherent differences in multi-modal features. To address these\nchallenges, we propose a novel strategy of progressive modality freezing,\ncalled PMF, that focuses on alignmentrelevant features and enhances multi-modal\nfeature fusion. Notably, our approach introduces a pioneering cross-modal\nassociation loss to foster modal consistency. Empirical evaluations across nine\ndatasets confirm PMF's superiority, demonstrating stateof-the-art performance\nand the rationale for freezing modalities. Our code is available at\nhttps://github.com/ninibymilk/PMF-MMEA.", "published": "2024-07-23 04:22:30", "link": "http://arxiv.org/abs/2407.16168v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structural Optimization Ambiguity and Simplicity Bias in Unsupervised\n  Neural Grammar Induction", "abstract": "Neural parameterization has significantly advanced unsupervised grammar\ninduction. However, training these models with a traditional likelihood loss\nfor all possible parses exacerbates two issues: 1) $\\textit{structural\noptimization ambiguity}$ that arbitrarily selects one among structurally\nambiguous optimal grammars despite the specific preference of gold parses, and\n2) $\\textit{structural simplicity bias}$ that leads a model to underutilize\nrules to compose parse trees. These challenges subject unsupervised neural\ngrammar induction (UNGI) to inevitable prediction errors, high variance, and\nthe necessity for extensive grammars to achieve accurate predictions. This\npaper tackles these issues, offering a comprehensive analysis of their origins.\nAs a solution, we introduce $\\textit{sentence-wise parse-focusing}$ to reduce\nthe parse pool per sentence for loss evaluation, using the structural bias from\npre-trained parsers on the same dataset. In unsupervised parsing benchmark\ntests, our method significantly improves performance while effectively reducing\nvariance and bias toward overly simplistic parses. Our research promotes\nlearning more compact, accurate, and consistent explicit grammars, facilitating\nbetter interpretability.", "published": "2024-07-23 04:57:03", "link": "http://arxiv.org/abs/2407.16181v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph-Structured Speculative Decoding", "abstract": "Speculative decoding has emerged as a promising technique to accelerate the\ninference of Large Language Models (LLMs) by employing a small language model\nto draft a hypothesis sequence, which is then validated by the LLM. The\neffectiveness of this approach heavily relies on the balance between\nperformance and efficiency of the draft model. In our research, we focus on\nenhancing the proportion of draft tokens that are accepted to the final output\nby generating multiple hypotheses instead of just one. This allows the LLM more\noptions to choose from and select the longest sequence that meets its\nstandards. Our analysis reveals that hypotheses produced by the draft model\nshare many common token sequences, suggesting a potential for optimizing\ncomputation. Leveraging this observation, we introduce an innovative approach\nutilizing a directed acyclic graph (DAG) to manage the drafted hypotheses. This\nstructure enables us to efficiently predict and merge recurring token\nsequences, vastly reducing the computational demands of the draft model. We\nterm this approach Graph-structured Speculative Decoding (GSD). We apply GSD\nacross a range of LLMs, including a 70-billion parameter LLaMA-2 model, and\nobserve a remarkable speedup of 1.73$\\times$ to 1.96$\\times$, significantly\nsurpassing standard speculative decoding.", "published": "2024-07-23 06:21:24", "link": "http://arxiv.org/abs/2407.16207v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Survey of LLM Alignment Techniques: RLHF, RLAIF, PPO,\n  DPO and More", "abstract": "With advancements in self-supervised learning, the availability of trillions\ntokens in a pre-training corpus, instruction fine-tuning, and the development\nof large Transformers with billions of parameters, large language models (LLMs)\nare now capable of generating factual and coherent responses to human queries.\nHowever, the mixed quality of training data can lead to the generation of\nundesired responses, presenting a significant challenge. Over the past two\nyears, various methods have been proposed from different perspectives to\nenhance LLMs, particularly in aligning them with human expectation. Despite\nthese efforts, there has not been a comprehensive survey paper that categorizes\nand details these approaches. In this work, we aim to address this gap by\ncategorizing these papers into distinct topics and providing detailed\nexplanations of each alignment method, thereby helping readers gain a thorough\nunderstanding of the current state of the field.", "published": "2024-07-23 06:45:52", "link": "http://arxiv.org/abs/2407.16216v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do LLMs Know When to NOT Answer? Investigating Abstention Abilities of\n  Large Language Models", "abstract": "Abstention Ability (AA) is a critical aspect of Large Language Model (LLM)\nreliability, referring to an LLM's capability to withhold responses when\nuncertain or lacking a definitive answer, without compromising performance.\nAlthough previous studies have attempted to improve AA, they lack a\nstandardised evaluation method and remain unsuitable for black-box models where\ntoken prediction probabilities are inaccessible. This makes comparative\nanalysis challenging, especially for state-of-the-art closed-source commercial\nLLMs. This paper bridges this gap by introducing a black-box evaluation\napproach and a new dataset, Abstain-QA, crafted to rigorously assess AA across\nvaried question types (answerable and unanswerable), domains (well-represented\nand under-represented), and task types (fact centric and reasoning). We also\npropose a new confusion matrix, the ''Answerable-Unanswerable Confusion\nMatrix'' (AUCM) which serves as the basis for evaluating AA, by offering a\nstructured and precise approach for assessment. Finally, we explore the impact\nof three prompting strategies-Strict Prompting, Verbal Confidence Thresholding,\nand Chain-of-Thought (CoT)-on improving AA. Our results indicate that even\npowerful models like GPT-4, Mixtral 8x22b encounter difficulties with\nabstention; however, strategic approaches such as Strict prompting and CoT can\nenhance this capability.", "published": "2024-07-23 06:56:54", "link": "http://arxiv.org/abs/2407.16221v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PreAlign: Boosting Cross-Lingual Transfer by Early Establishment of\n  Multilingual Alignment", "abstract": "Large language models demonstrate reasonable multilingual abilities, despite\npredominantly English-centric pretraining. However, the spontaneous\nmultilingual alignment in these models is shown to be weak, leading to\nunsatisfactory cross-lingual transfer and knowledge sharing. Previous works\nattempt to address this issue by explicitly injecting multilingual alignment\ninformation during or after pretraining. Thus for the early stage in\npretraining, the alignment is weak for sharing information or knowledge across\nlanguages. In this paper, we propose PreAlign, a framework that establishes\nmultilingual alignment prior to language model pretraining. PreAlign injects\nmultilingual alignment by initializing the model to generate similar\nrepresentations of aligned words and preserves this alignment using a\ncode-switching strategy during pretraining. Extensive experiments in a\nsynthetic English to English-Clone setting demonstrate that PreAlign\nsignificantly outperforms standard multilingual joint training in language\nmodeling, zero-shot cross-lingual transfer, and cross-lingual knowledge\napplication. Further experiments in real-world scenarios further validate\nPreAlign's effectiveness across various model sizes.", "published": "2024-07-23 06:59:53", "link": "http://arxiv.org/abs/2407.16222v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Effectiveness and Consistency of Task Selection in\n  Intermediate-Task Transfer Learning", "abstract": "Identifying beneficial tasks to transfer from is a critical step toward\nsuccessful intermediate-task transfer learning. In this work, we experiment\nwith 130 source-target task combinations and demonstrate that the transfer\nperformance exhibits severe variance across different source tasks and training\nseeds, highlighting the crucial role of intermediate-task selection in a\nbroader context. We compare four representative task selection methods in a\nunified setup, focusing on their effectiveness and consistency. Compared to\nembedding-free methods and text embeddings, task embeddings constructed from\nfine-tuned weights can better estimate task transferability by improving task\nprediction scores from 2.59% to 3.96%. Despite their strong performance, we\nobserve that the task embeddings do not consistently demonstrate superiority\nfor tasks requiring reasoning abilities. Furthermore, we introduce a novel\nmethod that measures pairwise token similarity using maximum inner product\nsearch, leading to the highest performance in task prediction. Our findings\nsuggest that token-wise similarity is better predictive for predicting\ntransferability compared to averaging weights.", "published": "2024-07-23 07:31:43", "link": "http://arxiv.org/abs/2407.16245v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Binary Gender: Evaluating Gender-Inclusive Machine Translation\n  with Ambiguous Attitude Words", "abstract": "Gender bias has been a focal point in the study of bias in machine\ntranslation and language models. Existing machine translation gender bias\nevaluations are primarily focused on male and female genders, limiting the\nscope of the evaluation. To assess gender bias accurately, these studies often\nrely on calculating the accuracy of gender pronouns or the masculine and\nfeminine attributes of grammatical gender via the stereotypes triggered by\noccupations or sentiment words ({\\em i.e.}, clear positive or negative\nattitude), which cannot extend to non-binary groups. This study presents a\nbenchmark AmbGIMT (Gender-Inclusive Machine Translation with Ambiguous attitude\nwords), which assesses gender bias beyond binary gender. Meanwhile, we propose\na novel process to evaluate gender bias based on the Emotional Attitude Score\n(EAS), which is used to quantify ambiguous attitude words. In evaluating three\nrecent and effective open-source LLMs and one powerful multilingual\ntranslation-specific model, our main observations are: (1) The translation\nperformance within non-binary gender contexts is markedly inferior in terms of\ntranslation quality and exhibits more negative attitudes than binary-gender\ncontexts. (2) The analysis experiments indicate that incorporating constraint\ncontext in prompts for gender identity terms can substantially reduce\ntranslation bias, while the bias remains evident despite the presence of the\nconstraints. The code is publicly available at\n\\url{https://github.com/pppa2019/ambGIMT}.", "published": "2024-07-23 08:13:51", "link": "http://arxiv.org/abs/2407.16266v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FACTTRACK: Time-Aware World State Tracking in Story Outlines", "abstract": "While accurately detecting and correcting factual contradictions in language\nmodel outputs has become increasingly important as their capabilities improve,\ndoing so is highly challenging. We propose a novel method, FACTTRACK, for\ntracking atomic facts and addressing factual contradictions. Crucially,\nFACTTRACK also maintains time-aware validity intervals for each fact, allowing\nfor change over time. At a high level, FACTTRACK consists of a four-step\npipeline to update a world state data structure for each new event: (1)\ndecompose the event into directional atomic facts; (2) determine the validity\ninterval of each atomic fact using the world state; (3) detect contradictions\nwith existing facts in the world state; and finally (4) add new facts to the\nworld state and update existing atomic facts. When we apply FACTTRACK to\ncontradiction detection on structured story outlines, we find that FACTTRACK\nusing LLaMA2-7B-Chat substantially outperforms a fair baseline using\nLLaMA2-7B-Chat, and achieves performance comparable to a GPT4 baseline.\nMoreover, when using GPT4, FACTTRACK significantly outperforms the GPT4\nbaseline.", "published": "2024-07-23 09:50:14", "link": "http://arxiv.org/abs/2407.16347v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TookaBERT: A Step Forward for Persian NLU", "abstract": "The field of natural language processing (NLP) has seen remarkable\nadvancements, thanks to the power of deep learning and foundation models.\nLanguage models, and specifically BERT, have been key players in this progress.\nIn this study, we trained and introduced two new BERT models using Persian\ndata. We put our models to the test, comparing them to seven existing models\nacross 14 diverse Persian natural language understanding (NLU) tasks. The\nresults speak for themselves: our larger model outperforms the competition,\nshowing an average improvement of at least +2.8 points. This highlights the\neffectiveness and potential of our new BERT models for Persian NLU tasks.", "published": "2024-07-23 11:12:47", "link": "http://arxiv.org/abs/2407.16382v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FairFlow: An Automated Approach to Model-based Counterfactual Data\n  Augmentation For NLP", "abstract": "Despite the evolution of language models, they continue to portray harmful\nsocietal biases and stereotypes inadvertently learned from training data. These\ninherent biases often result in detrimental effects in various applications.\nCounterfactual Data Augmentation (CDA), which seeks to balance demographic\nattributes in training data, has been a widely adopted approach to mitigate\nbias in natural language processing. However, many existing CDA approaches rely\non word substitution techniques using manually compiled word-pair dictionaries.\nThese techniques often lead to out-of-context substitutions, resulting in\npotential quality issues. The advancement of model-based techniques, on the\nother hand, has been challenged by the need for parallel training data. Works\nin this area resort to manually generated parallel data that are expensive to\ncollect and are consequently limited in scale. This paper proposes FairFlow, an\nautomated approach to generating parallel data for training counterfactual text\ngenerator models that limits the need for human intervention. Furthermore, we\nshow that FairFlow significantly overcomes the limitations of dictionary-based\nword-substitution approaches whilst maintaining good performance.", "published": "2024-07-23 12:29:37", "link": "http://arxiv.org/abs/2407.16431v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing LLM's Cognition via Structurization", "abstract": "When reading long-form text, human cognition is complex and structurized.\nWhile large language models (LLMs) process input contexts through a causal and\nsequential perspective, this approach can potentially limit their ability to\nhandle intricate and complex inputs effectively. To enhance LLM's cognition\ncapability, this paper presents a novel concept of context structurization.\nSpecifically, we transform the plain, unordered contextual sentences into\nwell-ordered and hierarchically structurized elements. By doing so, LLMs can\nbetter grasp intricate and extended contexts through precise attention and\ninformation-seeking along the organized structures. Extensive evaluations are\nconducted across various model architectures and sizes (including a series of\nauto-regressive LLMs as well as BERT-like masking models) on a diverse set of\nNLP tasks (e.g., context-based question-answering, exhaustive hallucination\nevaluation, and passage-level dense retrieval). Empirical results show\nconsistent and significant performance gains afforded by a single-round\nstructurization. In particular, we boost the open-sourced LLaMA2-70B model to\nachieve comparable performance against GPT-3.5-Turbo as the hallucination\nevaluator. Besides, we show the feasibility of distilling advanced LLMs'\nlanguage processing abilities to a smaller yet effective StruXGPT-7B to execute\nstructurization, addressing the practicality of our approach. Code is available\nat https://github.com/alibaba/struxgpt.", "published": "2024-07-23 12:33:58", "link": "http://arxiv.org/abs/2407.16434v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing In-context Learning and Fine-tuning for Topic Classification\n  of German Web Data", "abstract": "Researchers in the political and social sciences often rely on classification\nmodels to analyze trends in information consumption by examining browsing\nhistories of millions of webpages. Automated scalable methods are necessary due\nto the impracticality of manual labeling. In this paper, we model the detection\nof topic-related content as a binary classification task and compare the\naccuracy of fine-tuned pre-trained encoder models against in-context learning\nstrategies. Using only a few hundred annotated data points per topic, we detect\ncontent related to three German policies in a database of scraped webpages. We\ncompare multilingual and monolingual models, as well as zero and few-shot\napproaches, and investigate the impact of negative sampling strategies and the\ncombination of URL & content-based features. Our results show that a small\nsample of annotated data is sufficient to train an effective classifier.\nFine-tuning encoder-based models yields better results than in-context\nlearning. Classifiers using both URL & content-based features perform best,\nwhile using URLs alone provides adequate results when content is unavailable.", "published": "2024-07-23 14:31:59", "link": "http://arxiv.org/abs/2407.16516v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AMONGAGENTS: Evaluating Large Language Models in the Interactive\n  Text-Based Social Deduction Game", "abstract": "Strategic social deduction games serve as valuable testbeds for evaluating\nthe understanding and inference skills of language models, offering crucial\ninsights into social science, artificial intelligence, and strategic gaming.\nThis paper focuses on creating proxies of human behavior in simulated\nenvironments, with Among Us utilized as a tool for studying simulated human\nbehavior. The study introduces a text-based game environment, named\nAmongAgents, that mirrors the dynamics of Among Us. Players act as crew members\naboard a spaceship, tasked with identifying impostors who are sabotaging the\nship and eliminating the crew. Within this environment, the behavior of\nsimulated language agents is analyzed. The experiments involve diverse game\nsequences featuring different configurations of Crewmates and Impostor\npersonality archetypes. Our work demonstrates that state-of-the-art large\nlanguage models (LLMs) can effectively grasp the game rules and make decisions\nbased on the current context. This work aims to promote further exploration of\nLLMs in goal-oriented games with incomplete information and complex action\nspaces, as these settings offer valuable opportunities to assess language model\nperformance in socially driven scenarios.", "published": "2024-07-23 14:34:38", "link": "http://arxiv.org/abs/2407.16521v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quantifying the Role of Textual Predictability in Automatic Speech\n  Recognition", "abstract": "A long-standing question in automatic speech recognition research is how to\nattribute errors to the ability of a model to model the acoustics, versus its\nability to leverage higher-order context (lexicon, morphology, syntax,\nsemantics). We validate a novel approach which models error rates as a function\nof relative textual predictability, and yields a single number, $k$, which\nmeasures the effect of textual predictability on the recognizer. We use this\nmethod to demonstrate that a Wav2Vec 2.0-based model makes greater stronger use\nof textual context than a hybrid ASR model, in spite of not using an explicit\nlanguage model, and also use it to shed light on recent results demonstrating\npoor performance of standard ASR systems on African-American English. We\ndemonstrate that these mostly represent failures of acoustic--phonetic\nmodelling. We show how this approach can be used straightforwardly in\ndiagnosing and improving ASR.", "published": "2024-07-23 14:47:25", "link": "http://arxiv.org/abs/2407.16537v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Retrieve, Generate, Evaluate: A Case Study for Medical Paraphrases\n  Generation with Small Language Models", "abstract": "Recent surge in the accessibility of large language models (LLMs) to the\ngeneral population can lead to untrackable use of such models for\nmedical-related recommendations. Language generation via LLMs models has two\nkey problems: firstly, they are prone to hallucination and therefore, for any\nmedical purpose they require scientific and factual grounding; secondly, LLMs\npose tremendous challenge to computational resources due to their gigantic\nmodel size. In this work, we introduce pRAGe, a pipeline for Retrieval\nAugmented Generation and evaluation of medical paraphrases generation using\nSmall Language Models (SLM). We study the effectiveness of SLMs and the impact\nof external knowledge base for medical paraphrase generation in French.", "published": "2024-07-23 15:17:11", "link": "http://arxiv.org/abs/2407.16565v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TLCR: Token-Level Continuous Reward for Fine-grained Reinforcement\n  Learning from Human Feedback", "abstract": "Reinforcement Learning from Human Feedback (RLHF) leverages human preference\ndata to train language models to align more closely with human essence. These\nhuman preference data, however, are labeled at the sequence level, creating a\nmismatch between sequence-level preference labels and tokens, which are\nautoregressively generated from the language model. Although several recent\napproaches have tried to provide token-level (i.e., dense) rewards for each\nindividual token, these typically rely on predefined discrete reward values\n(e.g., positive: +1, negative: -1, neutral: 0), failing to account for varying\ndegrees of preference inherent to each token. To address this limitation, we\nintroduce TLCR (Token-Level Continuous Reward) for RLHF, which incorporates a\ndiscriminator trained to distinguish positive and negative tokens, and the\nconfidence of the discriminator is used to assign continuous rewards to each\ntoken considering the context. Extensive experiments show that our proposed\nTLCR leads to consistent performance improvements over previous sequence-level\nor token-level discrete rewards on open-ended generation benchmarks.", "published": "2024-07-23 15:27:37", "link": "http://arxiv.org/abs/2407.16574v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Shared Imagination: LLMs Hallucinate Alike", "abstract": "Despite the recent proliferation of large language models (LLMs), their\ntraining recipes -- model architecture, pre-training data and optimization\nalgorithm -- are often very similar. This naturally raises the question of the\nsimilarity among the resulting models. In this paper, we propose a novel\nsetting, imaginary question answering (IQA), to better understand model\nsimilarity. In IQA, we ask one model to generate purely imaginary questions\n(e.g., on completely made-up concepts in physics) and prompt another model to\nanswer. Surprisingly, despite the total fictionality of these questions, all\nmodels can answer each other's questions with remarkable success, suggesting a\n\"shared imagination space\" in which these models operate during such\nhallucinations. We conduct a series of investigations into this phenomenon and\ndiscuss implications on model homogeneity, hallucination, and computational\ncreativity.", "published": "2024-07-23 16:06:22", "link": "http://arxiv.org/abs/2407.16604v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Change Characterization with LLMs using Rhetorics", "abstract": "Languages continually evolve in response to societal events, resulting in new\nterms and shifts in meanings. These changes have significant implications for\ncomputer applications, including automatic translation and chatbots, making it\nessential to characterize them accurately. The recent development of LLMs has\nnotably advanced natural language understanding, particularly in sense\ninference and reasoning. In this paper, we investigate the potential of LLMs in\ncharacterizing three types of semantic change: dimension, relation, and\norientation. We achieve this by combining LLMs' Chain-of-Thought with\nrhetorical devices and conducting an experimental assessment of our approach\nusing newly created datasets. Our results highlight the effectiveness of LLMs\nin capturing and analyzing semantic changes, providing valuable insights to\nimprove computational linguistic applications.", "published": "2024-07-23 16:32:49", "link": "http://arxiv.org/abs/2407.16624v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models Automatically Jailbreak GPT-4V?", "abstract": "GPT-4V has attracted considerable attention due to its extraordinary capacity\nfor integrating and processing multimodal information. At the same time, its\nability of face recognition raises new safety concerns of privacy leakage.\nDespite researchers' efforts in safety alignment through RLHF or preprocessing\nfilters, vulnerabilities might still be exploited. In our study, we introduce\nAutoJailbreak, an innovative automatic jailbreak technique inspired by prompt\noptimization. We leverage Large Language Models (LLMs) for red-teaming to\nrefine the jailbreak prompt and employ weak-to-strong in-context learning\nprompts to boost efficiency. Furthermore, we present an effective search method\nthat incorporates early stopping to minimize optimization time and token\nexpenditure. Our experiments demonstrate that AutoJailbreak significantly\nsurpasses conventional methods, achieving an Attack Success Rate (ASR)\nexceeding 95.3\\%. This research sheds light on strengthening GPT-4V security,\nunderscoring the potential for LLMs to be exploited in compromising GPT-4V\nintegrity.", "published": "2024-07-23 17:50:45", "link": "http://arxiv.org/abs/2407.16686v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explanation Regularisation through the Lens of Attributions", "abstract": "Explanation regularisation (ER) has been introduced as a way to guide text\nclassifiers to form their predictions relying on input tokens that humans\nconsider plausible. This is achieved by introducing an auxiliary explanation\nloss that measures how well the output of an input attribution technique for\nthe model agrees with human-annotated rationales. The guidance appears to\nbenefit performance in out-of-domain (OOD) settings, presumably due to an\nincreased reliance on \"plausible\" tokens. However, previous work has\nunder-explored the impact of guidance on that reliance, particularly when\nreliance is measured using attribution techniques different from those used to\nguide the model. In this work, we seek to close this gap, and also explore the\nrelationship between reliance on plausible features and OOD performance. We\nfind that the connection between ER and the ability of a classifier to rely on\nplausible features has been overstated and that a stronger reliance on\nplausible tokens does not seem to be the cause for OOD improvements.", "published": "2024-07-23 17:56:32", "link": "http://arxiv.org/abs/2407.16693v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structure-aware Domain Knowledge Injection for Large Language Models", "abstract": "This paper introduces a pioneering methodology, termed StructTuning, to\nefficiently transform foundation Large Language Models (LLMs) into domain\nspecialists. It significantly reduces the training corpus needs to a mere 5%\nwhile achieving an impressive 100% of traditional knowledge injection\nperformance. Motivated by structured human education, we propose a novel\ntwo-stage strategy for knowledge injection and alignment: Structure-aware\nContinual Pre-Training (SCPT) and Structure-aware Supervised Fine-Tuning\n(SSFT). In the SCPT phase, we automatically extract the domain knowledge\ntaxonomy and reorganize the training corpora, enabling LLMs to effectively link\ntextual segments to targeted knowledge points within the taxonomy. In the SSFT\nphase, we explicitly prompt models to elucidate the underlying knowledge\nstructure in their outputs, leveraging the structured domain insight to address\npractical problems. Our ultimate method was extensively evaluated across model\narchitectures and scales on LongBench and MMedBench datasets, demonstrating\nsuperior performance against other knowledge injection methods. We also\nexplored our method's scalability across different training corpus sizes,\nlaying the foundation to enhance domain-specific LLMs with better data\nutilization.", "published": "2024-07-23 12:38:48", "link": "http://arxiv.org/abs/2407.16724v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey of Text Style Transfer: Applications and Ethical Implications", "abstract": "Text style transfer (TST) is an important task in controllable text\ngeneration, which aims to control selected attributes of language use, such as\npoliteness, formality, or sentiment, without altering the style-independent\ncontent of the text. The field has received considerable research attention in\nrecent years and has already been covered in several reviews, but the focus has\nmostly been on the development of new algorithms and learning from different\ntypes of data (supervised, unsupervised, out-of-domain, etc.) and not so much\non the application side. However, TST-related technologies are gradually\nreaching a production- and deployment-ready level, and therefore, the inclusion\nof the application perspective in TST research becomes crucial. Similarly, the\noften overlooked ethical considerations of TST technology have become a\npressing issue. This paper presents a comprehensive review of TST applications\nthat have been researched over the years, using both traditional linguistic\napproaches and more recent deep learning methods. We discuss current\nchallenges, future research directions, and ethical implications of TST\napplications in text generation. By providing a holistic overview of the\nlandscape of TST applications, we hope to stimulate further research and\ncontribute to a better understanding of the potential as well as ethical\nconsiderations associated with TST.", "published": "2024-07-23 17:15:23", "link": "http://arxiv.org/abs/2407.16737v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "$\\textit{BenchIE}^{FL}$ : A Manually Re-Annotated Fact-Based Open\n  Information Extraction Benchmark", "abstract": "Open Information Extraction (OIE) is a field of natural language processing\nthat aims to present textual information in a format that allows it to be\norganized, analyzed and reflected upon. Numerous OIE systems are developed,\nclaiming ever-increasing performance, marking the need for objective\nbenchmarks. BenchIE is the latest reference we know of. Despite being very well\nthought out, we noticed a number of issues we believe are limiting. Therefore,\nwe propose $\\textit{BenchIE}^{FL}$, a new OIE benchmark which fully enforces\nthe principles of BenchIE while containing fewer errors, omissions and\nshortcomings when candidate facts are matched towards reference ones.\n$\\textit{BenchIE}^{FL}$ allows insightful conclusions to be drawn on the actual\nperformance of OIE extractors.", "published": "2024-07-23 22:04:04", "link": "http://arxiv.org/abs/2407.16860v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Artificial Intelligence in Extracting Diagnostic Data from Dental\n  Records", "abstract": "This research addresses the issue of missing structured data in dental\nrecords by extracting diagnostic information from unstructured text. The\nupdated periodontology classification system's complexity has increased\nincomplete or missing structured diagnoses. To tackle this, we use advanced AI\nand NLP methods, leveraging GPT-4 to generate synthetic notes for fine-tuning a\nRoBERTa model. This significantly enhances the model's ability to understand\nmedical and dental language. We evaluated the model using 120 randomly selected\nclinical notes from two datasets, demonstrating its improved diagnostic\nextraction accuracy. The results showed high accuracy in diagnosing periodontal\nstatus, stage, and grade, with Site 1 scoring 0.99 and Site 2 scoring 0.98. In\nthe subtype category, Site 2 achieved perfect scores, outperforming Site 1.\nThis method enhances extraction accuracy and broadens its use across dental\ncontexts. The study underscores AI and NLP's transformative impact on\nhealthcare delivery and management. Integrating AI and NLP technologies\nenhances documentation and simplifies administrative tasks by precisely\nextracting complex clinical information. This approach effectively addresses\nchallenges in dental diagnostics. Using synthetic training data from LLMs\noptimizes the training process, improving accuracy and efficiency in\nidentifying periodontal diagnoses from clinical notes. This innovative method\nholds promise for broader healthcare applications, potentially improving\npatient care quality.", "published": "2024-07-23 04:05:48", "link": "http://arxiv.org/abs/2407.21050v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Active Inference Strategy for Prompting Reliable Responses from Large\n  Language Models in Medical Practice", "abstract": "Continuing advances in Large Language Models (LLMs) in artificial\nintelligence offer important capacities in intuitively accessing and using\nmedical knowledge in many contexts, including education and training as well as\nassessment and treatment. Most of the initial literature on LLMs in medicine\nhas emphasized that LLMs are unsuitable for medical use because they are\nnon-deterministic, may provide incorrect or harmful responses, and cannot be\nregulated to assure quality control. If these issues could be corrected,\noptimizing LLM technology could benefit patients and physicians by providing\naffordable, point-of-care medical knowledge. Our proposed framework refines LLM\nresponses by restricting their primary knowledge base to domain-specific\ndatasets containing validated medical information. Additionally, we introduce\nan actor-critic LLM prompting protocol based on active inference principles of\nhuman cognition, where a Therapist agent initially responds to patient queries,\nand a Supervisor agent evaluates and adjusts responses to ensure accuracy and\nreliability. We conducted a validation study where expert cognitive behaviour\ntherapy for insomnia (CBT-I) therapists evaluated responses from the LLM in a\nblind format. Experienced human CBT-I therapists assessed responses to 100\npatient queries, comparing LLM-generated responses with appropriate and\ninappropriate responses crafted by experienced CBT-I therapists. Results showed\nthat LLM responses received high ratings from the CBT-I therapists, often\nexceeding those of therapist-generated appropriate responses. This structured\napproach aims to integrate advanced LLM technology into medical applications,\nmeeting regulatory requirements for establishing the safe and effective use of\nspecial purpose validated LLMs in medicine.", "published": "2024-07-23 05:00:18", "link": "http://arxiv.org/abs/2407.21051v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Polysemy Evolution Using Semantic Cells", "abstract": "The senses of words evolve. The sense of the same word may change from today\nto tomorrow, and multiple senses of the same word may be the result of the\nevolution of each other, that is, they may be parents and children. If we view\nJuba as an evolving ecosystem, the paradigm of learning the correct answer,\nwhich does not move with the sense of a word, is no longer valid. This paper is\na case study that shows that word polysemy is an evolutionary consequence of\nthe modification of Semantic Cells, which has al-ready been presented by the\nauthor, by introducing a small amount of diversity in its initial state as an\nexample of analyzing the current set of short sentences. In particular, the\nanalysis of a sentence sequence of 1000 sentences in some order for each of the\nfour senses of the word Spring, collected using Chat GPT, shows that the word\nacquires the most polysemy monotonically in the analysis when the senses are\narranged in the order in which they have evolved. In other words, we present a\nmethod for analyzing the dynamism of a word's acquiring polysemy with evolution\nand, at the same time, a methodology for viewing polysemy from an evolutionary\nframework rather than a learning-based one.", "published": "2024-07-23 00:52:12", "link": "http://arxiv.org/abs/2407.16110v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Finetuning Generative Large Language Models with Discrimination\n  Instructions for Knowledge Graph Completion", "abstract": "Traditional knowledge graph (KG) completion models learn embeddings to\npredict missing facts. Recent works attempt to complete KGs in a\ntext-generation manner with large language models (LLMs). However, they need to\nground the output of LLMs to KG entities, which inevitably brings errors. In\nthis paper, we present a finetuning framework, DIFT, aiming to unleash the KG\ncompletion ability of LLMs and avoid grounding errors. Given an incomplete\nfact, DIFT employs a lightweight model to obtain candidate entities and\nfinetunes an LLM with discrimination instructions to select the correct one\nfrom the given candidates. To improve performance while reducing instruction\ndata, DIFT uses a truncated sampling method to select useful facts for\nfinetuning and injects KG embeddings into the LLM. Extensive experiments on\nbenchmark datasets demonstrate the effectiveness of our proposed framework.", "published": "2024-07-23 02:25:01", "link": "http://arxiv.org/abs/2407.16127v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "UniMEL: A Unified Framework for Multimodal Entity Linking with Large\n  Language Models", "abstract": "Multimodal Entity Linking (MEL) is a crucial task that aims at linking\nambiguous mentions within multimodal contexts to the referent entities in a\nmultimodal knowledge base, such as Wikipedia. Existing methods focus heavily on\nusing complex mechanisms and extensive model tuning methods to model the\nmultimodal interaction on specific datasets. However, these methods\novercomplicate the MEL task and overlook the visual semantic information, which\nmakes them costly and hard to scale. Moreover, these methods can not solve the\nissues like textual ambiguity, redundancy, and noisy images, which severely\ndegrade their performance. Fortunately, the advent of Large Language Models\n(LLMs) with robust capabilities in text understanding and reasoning,\nparticularly Multimodal Large Language Models (MLLMs) that can process\nmultimodal inputs, provides new insights into addressing this challenge.\nHowever, how to design a universally applicable LLMs-based MEL approach remains\na pressing challenge. To this end, we propose UniMEL, a unified framework which\nestablishes a new paradigm to process multimodal entity linking tasks using\nLLMs. In this framework, we employ LLMs to augment the representation of\nmentions and entities individually by integrating textual and visual\ninformation and refining textual information. Subsequently, we employ the\nembedding-based method for retrieving and re-ranking candidate entities. Then,\nwith only ~0.26% of the model parameters fine-tuned, LLMs can make the final\nselection from the candidate entities. Extensive experiments on three public\nbenchmark datasets demonstrate that our solution achieves state-of-the-art\nperformance, and ablation studies verify the effectiveness of all modules. Our\ncode is available at https://github.com/Javkonline/UniMEL.", "published": "2024-07-23 03:58:08", "link": "http://arxiv.org/abs/2407.16160v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "How to Leverage Personal Textual Knowledge for Personalized\n  Conversational Information Retrieval", "abstract": "Personalized conversational information retrieval (CIR) combines\nconversational and personalizable elements to satisfy various users' complex\ninformation needs through multi-turn interaction based on their backgrounds.\nThe key promise is that the personal textual knowledge base (PTKB) can improve\nthe CIR effectiveness because the retrieval results can be more related to the\nuser's background. However, PTKB is noisy: not every piece of knowledge in PTKB\nis relevant to the specific query at hand. In this paper, we explore and test\nseveral ways to select knowledge from PTKB and use it for query reformulation\nby using a large language model (LLM). The experimental results show the PTKB\nmight not always improve the search results when used alone, but LLM can help\ngenerate a more appropriate personalized query when high-quality guidance is\nprovided.", "published": "2024-07-23 05:34:41", "link": "http://arxiv.org/abs/2407.16192v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A Multi-view Mask Contrastive Learning Graph Convolutional Neural\n  Network for Age Estimation", "abstract": "The age estimation task aims to use facial features to predict the age of\npeople and is widely used in public security, marketing, identification, and\nother fields. However, the features are mainly concentrated in facial\nkeypoints, and existing CNN and Transformer-based methods have inflexibility\nand redundancy for modeling complex irregular structures. Therefore, this paper\nproposes a Multi-view Mask Contrastive Learning Graph Convolutional Neural\nNetwork (MMCL-GCN) for age estimation. Specifically, the overall structure of\nthe MMCL-GCN network contains a feature extraction stage and an age estimation\nstage. In the feature extraction stage, we introduce a graph structure to\nconstruct face images as input and then design a Multi-view Mask Contrastive\nLearning (MMCL) mechanism to learn complex structural and semantic information\nabout face images. The learning mechanism employs an asymmetric siamese network\narchitecture, which utilizes an online encoder-decoder structure to reconstruct\nthe missing information from the original graph and utilizes the target encoder\nto learn latent representations for contrastive learning. Furthermore, to\npromote the two learning mechanisms better compatible and complementary, we\nadopt two augmentation strategies and optimize the joint losses. In the age\nestimation stage, we design a Multi-layer Extreme Learning Machine (ML-IELM)\nwith identity mapping to fully use the features extracted by the online\nencoder. Then, a classifier and a regressor were constructed based on ML-IELM,\nwhich were used to identify the age grouping interval and accurately estimate\nthe final age. Extensive experiments show that MMCL-GCN can effectively reduce\nthe error of age estimation on benchmark datasets such as Adience, MORPH-II,\nand LAP-2016.", "published": "2024-07-23 07:17:46", "link": "http://arxiv.org/abs/2407.16234v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Psychomatics -- A Multidisciplinary Framework for Understanding\n  Artificial Minds", "abstract": "Although LLMs and other artificial intelligence systems demonstrate cognitive\nskills similar to humans, like concept learning and language acquisition, the\nway they process information fundamentally differs from biological cognition.\nTo better understand these differences this paper introduces Psychomatics, a\nmultidisciplinary framework bridging cognitive science, linguistics, and\ncomputer science. It aims to better understand the high-level functioning of\nLLMs, focusing specifically on how LLMs acquire, learn, remember, and use\ninformation to produce their outputs. To achieve this goal, Psychomatics will\nrely on a comparative methodology, starting from a theory-driven research\nquestion - is the process of language development and use different in humans\nand LLMs? - drawing parallels between LLMs and biological systems. Our analysis\nshows how LLMs can map and manipulate complex linguistic patterns in their\ntraining data. Moreover, LLMs can follow Grice's Cooperative Principle to\nprovide relevant and informative responses. However, human cognition draws from\nmultiple sources of meaning, including experiential, emotional, and imaginative\nfacets, which transcend mere language processing and are rooted in our social\nand developmental trajectories. Moreover, current LLMs lack physical\nembodiment, reducing their ability to make sense of the intricate interplay\nbetween perception, action, and cognition that shapes human understanding and\nexpression. Ultimately, Psychomatics holds the potential to yield\ntransformative insights into the nature of language, cognition, and\nintelligence, both artificial and biological. Moreover, by drawing parallels\nbetween LLMs and human cognitive processes, Psychomatics can inform the\ndevelopment of more robust and human-like AI systems.", "published": "2024-07-23 12:53:41", "link": "http://arxiv.org/abs/2407.16444v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Machine Translation Hallucination Detection for Low and High Resource\n  Languages using Large Language Models", "abstract": "Recent advancements in massively multilingual machine translation systems\nhave significantly enhanced translation accuracy; however, even the best\nperforming systems still generate hallucinations, severely impacting user\ntrust. Detecting hallucinations in Machine Translation (MT) remains a critical\nchallenge, particularly since existing methods excel with High-Resource\nLanguages (HRLs) but exhibit substantial limitations when applied to\nLow-Resource Languages (LRLs). This paper evaluates sentence-level\nhallucination detection approaches using Large Language Models (LLMs) and\nsemantic similarity within massively multilingual embeddings. Our study spans\n16 language directions, covering HRLs, LRLs, with diverse scripts. We find that\nthe choice of model is essential for performance. On average, for HRLs,\nLlama3-70B outperforms the previous state of the art by as much as 0.16 MCC\n(Matthews Correlation Coefficient). However, for LRLs we observe that Claude\nSonnet outperforms other LLMs on average by 0.03 MCC. The key takeaway from our\nstudy is that LLMs can achieve performance comparable or even better than\npreviously proposed models, despite not being explicitly trained for any\nmachine translation task. However, their advantage is less significant for\nLRLs.", "published": "2024-07-23 13:40:54", "link": "http://arxiv.org/abs/2407.16470v3", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Comparative Study on Patient Language across Therapeutic Domains for\n  Effective Patient Voice Classification in Online Health Discussions", "abstract": "There exists an invisible barrier between healthcare professionals'\nperception of a patient's clinical experience and the reality. This barrier may\nbe induced by the environment that hinders patients from sharing their\nexperiences openly with healthcare professionals. As patients are observed to\ndiscuss and exchange knowledge more candidly on social media, valuable insights\ncan be leveraged from these platforms. However, the abundance of non-patient\nposts on social media necessitates filtering out such irrelevant content to\ndistinguish the genuine voices of patients, a task we refer to as patient voice\nclassification. In this study, we analyse the importance of linguistic\ncharacteristics in accurately classifying patient voices. Our findings\nunderscore the essential role of linguistic and statistical text similarity\nanalysis in identifying common patterns among patient groups. These results\nallude to even starker differences in the way patients express themselves at a\ndisease level and across various therapeutic domains. Additionally, we\nfine-tuned a pre-trained Language Model on the combined datasets with similar\nlinguistic patterns, resulting in a highly accurate automatic patient voice\nclassification. Being the pioneering study on the topic, our focus on\nextracting authentic patient experiences from social media stands as a crucial\nstep towards advancing healthcare standards and fostering a patient-centric\napproach.", "published": "2024-07-23 15:51:46", "link": "http://arxiv.org/abs/2407.16593v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Data Mixture Inference: What do BPE Tokenizers Reveal about their\n  Training Data?", "abstract": "The pretraining data of today's strongest language models is opaque; in\nparticular, little is known about the proportions of various domains or\nlanguages represented. In this work, we tackle a task which we call data\nmixture inference, which aims to uncover the distributional make-up of training\ndata. We introduce a novel attack based on a previously overlooked source of\ninformation: byte-pair encoding (BPE) tokenizers, used by the vast majority of\nmodern language models. Our key insight is that the ordered list of merge rules\nlearned by a BPE tokenizer naturally reveals information about the token\nfrequencies in its training data. Given a tokenizer's merge list along with\nexample data for each category of interest, we formulate a linear program that\nsolves for the proportion of each category in the tokenizer's training set. In\ncontrolled experiments, we show that our attack recovers mixture ratios with\nhigh precision for tokenizers trained on known mixtures of natural languages,\nprogramming languages, and data sources. We then apply our approach to\noff-the-shelf tokenizers released with recent LMs. We confirm much publicly\ndisclosed information about these models, and also make several new inferences:\nGPT-4o and Mistral NeMo's tokenizers are much more multilingual than their\npredecessors, training on 39% and 47% non-English language data, respectively;\nLlama 3 extends GPT-3.5's tokenizer primarily for multilingual (48%) use;\nGPT-3.5's and Claude's tokenizers are trained on predominantly code (~60%). We\nhope our work sheds light on current design practices for pretraining data, and\ninspires continued research into data mixture inference for LMs.", "published": "2024-07-23 16:13:22", "link": "http://arxiv.org/abs/2407.16607v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards scalable efficient on-device ASR with transfer learning", "abstract": "Multilingual pretraining for transfer learning significantly boosts the\nrobustness of low-resource monolingual ASR models. This study systematically\ninvestigates three main aspects: (a) the impact of transfer learning on model\nperformance during initial training or fine-tuning, (b) the influence of\ntransfer learning across dataset domains and languages, and (c) the effect on\nrare-word recognition compared to non-rare words. Our finding suggests that\nRNNT-loss pretraining, followed by monolingual fine-tuning with Minimum Word\nError Rate (MinWER) loss, consistently reduces Word Error Rates (WER) across\nlanguages like Italian and French. WER Reductions (WERR) reach 36.2% and 42.8%\ncompared to monolingual baselines for MLS and in-house datasets. Out-of-domain\npretraining leads to 28% higher WERR than in-domain pretraining. Both rare and\nnon-rare words benefit, with rare words showing greater improvements with\nout-of-domain pretraining, and non-rare words with in-domain pretraining.", "published": "2024-07-23 17:29:02", "link": "http://arxiv.org/abs/2407.16664v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "APTNESS: Incorporating Appraisal Theory and Emotion Support Strategies\n  for Empathetic Response Generation", "abstract": "Empathetic response generation is designed to comprehend the emotions of\nothers and select the most appropriate strategies to assist them in resolving\nemotional challenges. Empathy can be categorized into cognitive empathy and\naffective empathy. The former pertains to the ability to understand and discern\nthe emotional issues and situations of others, while the latter involves the\ncapacity to provide comfort. To enhance one's empathetic abilities, it is\nessential to develop both these aspects. Therefore, we develop an innovative\nframework that combines retrieval augmentation and emotional support strategy\nintegration. Our framework starts with the introduction of a comprehensive\nemotional palette for empathy. We then apply appraisal theory to decompose this\npalette and create a database of empathetic responses. This database serves as\nan external resource and enhances the LLM's empathy by integrating semantic\nretrieval mechanisms. Moreover, our framework places a strong emphasis on the\nproper articulation of response strategies. By incorporating emotional support\nstrategies, we aim to enrich the model's capabilities in both cognitive and\naffective empathy, leading to a more nuanced and comprehensive empathetic\nresponse. Finally, we extract datasets ED and ET from the empathetic dialogue\ndataset \\textsc{EmpatheticDialogues} and ExTES based on dialogue length.\nExperiments demonstrate that our framework can enhance the empathy ability of\nLLMs from both cognitive and affective empathy perspectives. Our code is\nreleased at https://github.com/CAS-SIAT-XinHai/APTNESS.", "published": "2024-07-23 02:23:37", "link": "http://arxiv.org/abs/2407.21048v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Table-Filling via Mean Teacher for Cross-domain Aspect Sentiment Triplet\n  Extraction", "abstract": "Cross-domain Aspect Sentiment Triplet Extraction (ASTE) aims to extract\nfine-grained sentiment elements from target domain sentences by leveraging the\nknowledge acquired from the source domain. Due to the absence of labeled data\nin the target domain, recent studies tend to rely on pre-trained language\nmodels to generate large amounts of synthetic data for training purposes.\nHowever, these approaches entail additional computational costs associated with\nthe generation process. Different from them, we discover a striking resemblance\nbetween table-filling methods in ASTE and two-stage Object Detection (OD) in\ncomputer vision, which inspires us to revisit the cross-domain ASTE task and\napproach it from an OD standpoint. This allows the model to benefit from the OD\nextraction paradigm and region-level alignment. Building upon this premise, we\npropose a novel method named \\textbf{T}able-\\textbf{F}illing via \\textbf{M}ean\n\\textbf{T}eacher (TFMT). Specifically, the table-filling methods encode the\nsentence into a 2D table to detect word relations, while TFMT treats the table\nas a feature map and utilizes a region consistency to enhance the quality of\nthose generated pseudo labels. Additionally, considering the existence of the\ndomain gap, a cross-domain consistency based on Maximum Mean Discrepancy is\ndesigned to alleviate domain shift problems. Our method achieves\nstate-of-the-art performance with minimal parameters and computational costs,\nmaking it a strong baseline for cross-domain ASTE.", "published": "2024-07-23 09:04:08", "link": "http://arxiv.org/abs/2407.21052v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge Models for Cancer Clinical Practice Guidelines : Construction,\n  Management and Usage in Question Answering", "abstract": "An automated knowledge modeling algorithm for Cancer Clinical Practice\nGuidelines (CPGs) extracts the knowledge contained in the CPG documents and\ntransforms it into a programmatically interactable, easy-to-update structured\nmodel with minimal human intervention. The existing automated algorithms have\nminimal scope and cannot handle the varying complexity of the knowledge content\nin the CPGs for different cancer types. This work proposes an improved\nautomated knowledge modeling algorithm to create knowledge models from the\nNational Comprehensive Cancer Network (NCCN) CPGs in Oncology for different\ncancer types. The proposed algorithm has been evaluated with NCCN CPGs for four\ndifferent cancer types. We also proposed an algorithm to compare the knowledge\nmodels for different versions of a guideline to discover the specific changes\nintroduced in the treatment protocol of a new version. We created a\nquestion-answering (Q&A) framework with the guideline knowledge models as the\naugmented knowledge base to study our ability to query the knowledge models. We\ncompiled a set of 32 question-answer pairs derived from two reliable data\nsources for the treatment of Non-Small Cell Lung Cancer (NSCLC) to evaluate the\nQ&A framework. The framework was evaluated against the question-answer pairs\nfrom one data source, and it can generate the answers with 54.5% accuracy from\nthe treatment algorithm and 81.8% accuracy from the discussion part of the NCCN\nNSCLC guideline knowledge model.", "published": "2024-07-23 11:26:40", "link": "http://arxiv.org/abs/2407.21053v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Artificial Agency and Large Language Models", "abstract": "The arrival of Large Language Models (LLMs) has stirred up philosophical\ndebates about the possibility of realizing agency in an artificial manner. In\nthis work we contribute to the debate by presenting a theoretical model that\ncan be used as a threshold conception for artificial agents. The model defines\nagents as systems whose actions and goals are always influenced by a dynamic\nframework of factors that consists of the agent's accessible history, its\nadaptive repertoire and its external environment. This framework, in turn, is\ninfluenced by the actions that the agent takes and the goals that it forms. We\nshow with the help of the model that state-of-the-art LLMs are not agents yet,\nbut that there are elements to them that suggest a way forward. The paper\nargues that a combination of the agent architecture presented in Park et al.\n(2023) together with the use of modules like the Coscientist in Boiko et al.\n(2023) could potentially be a way to realize agency in an artificial manner. We\nend the paper by reflecting on the obstacles one might face in building such an\nartificial agent and by presenting possible directions for future research.", "published": "2024-07-23 05:32:00", "link": "http://arxiv.org/abs/2407.16190v2", "categories": ["cs.AI", "cs.CL", "cs.ET"], "primary_category": "cs.AI"}
{"title": "LLMs can be Dangerous Reasoners: Analyzing-based Jailbreak Attack on\n  Large Language Models", "abstract": "The rapid development of Large Language Models (LLMs) has brought significant\nadvancements across various tasks. However, despite these achievements, LLMs\nstill exhibit inherent safety vulnerabilities, especially when confronted with\njailbreak attacks. Existing jailbreak methods suffer from two main limitations:\nreliance on complicated prompt engineering and iterative optimization, which\nlead to low attack success rate (ASR) and attack efficiency (AE). In this work,\nwe propose an efficient jailbreak attack method, Analyzing-based Jailbreak\n(ABJ), which leverages the advanced reasoning capability of LLMs to\nautonomously generate harmful content, revealing their underlying safety\nvulnerabilities during complex reasoning process. We conduct comprehensive\nexperiments on ABJ across various open-source and closed-source LLMs. In\nparticular, ABJ achieves high ASR (82.1% on GPT-4o-2024-11-20) with exceptional\nAE among all target LLMs, showcasing its remarkable attack effectiveness,\ntransferability, and efficiency. Our findings underscore the urgent need to\nprioritize and improve the safety of LLMs to mitigate the risks of misuse.", "published": "2024-07-23 06:14:41", "link": "http://arxiv.org/abs/2407.16205v5", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "LawLuo: A Multi-Agent Collaborative Framework for Multi-Round Chinese\n  Legal Consultation", "abstract": "Legal Large Language Models (LLMs) have shown promise in providing legal\nconsultations to non-experts. However, most existing Chinese legal consultation\nmodels are based on single-agent systems, which differ from real-world legal\nconsultations, where multiple professionals collaborate to offer more tailored\nresponses. To better simulate real consultations, we propose LawLuo, a\nmulti-agent framework for multi-turn Chinese legal consultations. LawLuo\nincludes four agents: the receptionist agent, which assesses user intent and\nselects a lawyer agent; the lawyer agent, which interacts with the user; the\nsecretary agent, which organizes conversation records and generates\nconsultation reports; and the boss agent, which evaluates the performance of\nthe lawyer and secretary agents to ensure optimal results. These agents'\ninteractions mimic the operations of real law firms. To train them to follow\ndifferent legal instructions, we developed distinct fine-tuning datasets. We\nalso introduce a case graph-based RAG to help the lawyer agent address vague\nuser inputs. Experimental results show that LawLuo outperforms baselines in\ngenerating more personalized and professional responses, handling ambiguous\nqueries, and following legal instructions in multi-turn conversations. Our full\ncode and constructed datasets will be open-sourced upon paper acceptance.", "published": "2024-07-23 07:40:41", "link": "http://arxiv.org/abs/2407.16252v3", "categories": ["cs.CL", "cs.AI", "cs.CV", "I.2.1"], "primary_category": "cs.CL"}
{"title": "PrimeGuard: Safe and Helpful LLMs through Tuning-Free Routing", "abstract": "Deploying language models (LMs) necessitates outputs to be both high-quality\nand compliant with safety guidelines. Although Inference-Time Guardrails (ITG)\noffer solutions that shift model output distributions towards compliance, we\nfind that current methods struggle in balancing safety with helpfulness. ITG\nMethods that safely address non-compliant queries exhibit lower helpfulness\nwhile those that prioritize helpfulness compromise on safety. We refer to this\ntrade-off as the guardrail tax, analogous to the alignment tax. To address\nthis, we propose PrimeGuard, a novel ITG method that utilizes structured\ncontrol flow.\n  PrimeGuard routes requests to different self-instantiations of the LM with\nvarying instructions, leveraging its inherent instruction-following\ncapabilities and in-context learning. Our tuning-free approach dynamically\ncompiles system-designer guidelines for each query. We construct and release\nsafe-eval, a diverse red-team safety benchmark. Extensive evaluations\ndemonstrate that PrimeGuard, without fine-tuning, overcomes the guardrail tax\nby (1) significantly increasing resistance to iterative jailbreak attacks and\n(2) achieving state-of-the-art results in safety guardrailing while (3)\nmatching helpfulness scores of alignment-tuned models. Extensive evaluations\ndemonstrate that PrimeGuard, without fine-tuning, outperforms all competing\nbaselines and overcomes the guardrail tax by improving the fraction of safe\nresponses from 61% to 97% and increasing average helpfulness scores from 4.17\nto 4.29 on the largest models, while reducing attack success rate from 100% to\n8%.\n  PrimeGuard implementation is available at\nhttps://github.com/dynamofl/PrimeGuard and safe-eval dataset is available at\nhttps://huggingface.co/datasets/dynamoai/safe_eval.", "published": "2024-07-23 09:14:27", "link": "http://arxiv.org/abs/2407.16318v1", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.SE"], "primary_category": "cs.AI"}
{"title": "Evolutionary Prompt Design for LLM-Based Post-ASR Error Correction", "abstract": "Building upon the strength of modern large language models (LLMs), generative\nerror correction (GEC) has emerged as a promising paradigm that can elevate the\nperformance of modern automatic speech recognition (ASR) systems. One\nrepresentative approach is to leverage in-context learning to prompt LLMs so\nthat a better hypothesis can be generated by the LLMs based on a\ncarefully-designed prompt and an $N$-best list of hypotheses produced by ASR\nsystems. However, it is yet unknown whether the existing prompts are the most\neffective ones for the task of post-ASR error correction. In this context, this\npaper first explores alternative prompts to identify an initial set of\neffective prompts, and then proposes to employ an evolutionary prompt\noptimization algorithm to refine the initial prompts. Evaluations results on\nthe CHiME-4 subset of the Task $1$ of the SLT $2024$ GenSEC challenge show the\neffectiveness and potential of the proposed algorithms.", "published": "2024-07-23 10:38:49", "link": "http://arxiv.org/abs/2407.16370v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Imperfect Vision Encoders: Efficient and Robust Tuning for\n  Vision-Language Models", "abstract": "Vision language models (VLMs) demonstrate impressive capabilities in visual\nquestion answering and image captioning, acting as a crucial link between\nvisual and language models. However, existing open-source VLMs heavily rely on\npretrained and frozen vision encoders (such as CLIP). Despite CLIP's robustness\nacross diverse domains, it still exhibits non-negligible image understanding\nerrors. These errors propagate to the VLM responses, resulting in sub-optimal\nperformance. In our work, we propose an efficient and robust method for\nupdating vision encoders within VLMs. Our approach selectively and locally\nupdates encoders, leading to substantial performance improvements on data where\nprevious mistakes occurred, while maintaining overall robustness. Furthermore,\nwe demonstrate the effectiveness of our method during continual few-shot\nupdates. Theoretical grounding, generality, and computational efficiency\ncharacterize our approach.", "published": "2024-07-23 14:39:40", "link": "http://arxiv.org/abs/2407.16526v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Lawma: The Power of Specialization for Legal Tasks", "abstract": "Annotation and classification of legal text are central components of\nempirical legal research. Traditionally, these tasks are often delegated to\ntrained research assistants. Motivated by the advances in language modeling,\nempirical legal scholars are increasingly turning to prompting commercial\nmodels, hoping that it will alleviate the significant cost of human annotation.\nDespite growing use, our understanding of how to best utilize large language\nmodels for legal tasks remains limited. We conduct a comprehensive study of 260\nlegal text classification tasks, nearly all new to the machine learning\ncommunity. Starting from GPT-4 as a baseline, we show that it has non-trivial\nbut highly varied zero-shot accuracy, often exhibiting performance that may be\ninsufficient for legal work. We then demonstrate that a lightly fine-tuned\nLlama 3 model vastly outperforms GPT-4 on almost all tasks, typically by\ndouble-digit percentage points. We find that larger models respond better to\nfine-tuning than smaller models. A few tens to hundreds of examples suffice to\nachieve high classification accuracy. Notably, we can fine-tune a single model\non all 260 tasks simultaneously at a small loss in accuracy relative to having\na separate model for each task. Our work points to a viable alternative to the\npredominant practice of prompting commercial models. For concrete legal tasks\nwith some available labeled data, researchers are better off using a fine-tuned\nopen-source model.", "published": "2024-07-23 16:23:04", "link": "http://arxiv.org/abs/2407.16615v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Course-Correction: Safety Alignment Using Synthetic Preferences", "abstract": "The risk of harmful content generated by large language models (LLMs) becomes\na critical concern. This paper presents a systematic study on assessing and\nimproving LLMs' capability to perform the task of \\textbf{course-correction},\n\\ie, the model can steer away from generating harmful content autonomously. To\nstart with, we introduce the \\textsc{C$^2$-Eval} benchmark for quantitative\nassessment and analyze 10 popular LLMs, revealing varying proficiency of\ncurrent safety-tuned LLMs in course-correction. To improve, we propose\nfine-tuning LLMs with preference learning, emphasizing the preference for\ntimely course-correction. Using an automated pipeline, we create\n\\textsc{C$^2$-Syn}, a synthetic dataset with 750K pairwise preferences, to\nteach models the concept of timely course-correction through data-driven\npreference learning. Experiments on 2 LLMs, \\textsc{Llama2-Chat 7B} and\n\\textsc{Qwen2 7B}, show that our method effectively enhances course-correction\nskills without affecting general performance. Additionally, it effectively\nimproves LLMs' safety, particularly in resisting jailbreak attacks.", "published": "2024-07-23 16:54:28", "link": "http://arxiv.org/abs/2407.16637v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RedAgent: Red Teaming Large Language Models with Context-aware\n  Autonomous Language Agent", "abstract": "Recently, advanced Large Language Models (LLMs) such as GPT-4 have been\nintegrated into many real-world applications like Code Copilot. These\napplications have significantly expanded the attack surface of LLMs, exposing\nthem to a variety of threats. Among them, jailbreak attacks that induce toxic\nresponses through jailbreak prompts have raised critical safety concerns. To\nidentify these threats, a growing number of red teaming approaches simulate\npotential adversarial scenarios by crafting jailbreak prompts to test the\ntarget LLM. However, existing red teaming methods do not consider the unique\nvulnerabilities of LLM in different scenarios, making it difficult to adjust\nthe jailbreak prompts to find context-specific vulnerabilities. Meanwhile,\nthese methods are limited to refining jailbreak templates using a few mutation\noperations, lacking the automation and scalability to adapt to different\nscenarios. To enable context-aware and efficient red teaming, we abstract and\nmodel existing attacks into a coherent concept called \"jailbreak strategy\" and\npropose a multi-agent LLM system named RedAgent that leverages these strategies\nto generate context-aware jailbreak prompts. By self-reflecting on contextual\nfeedback in an additional memory buffer, RedAgent continuously learns how to\nleverage these strategies to achieve effective jailbreaks in specific contexts.\nExtensive experiments demonstrate that our system can jailbreak most black-box\nLLMs in just five queries, improving the efficiency of existing red teaming\nmethods by two times. Additionally, RedAgent can jailbreak customized LLM\napplications more efficiently. By generating context-aware jailbreak prompts\ntowards applications on GPTs, we discover 60 severe vulnerabilities of these\nreal-world applications with only two queries per vulnerability. We have\nreported all found issues and communicated with OpenAI and Meta for bug fixes.", "published": "2024-07-23 17:34:36", "link": "http://arxiv.org/abs/2407.16667v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Stress-Testing Long-Context Language Models with Lifelong ICL and Task\n  Haystack", "abstract": "We introduce Lifelong ICL, a problem setting that challenges long-context\nlanguage models (LMs) to learn a sequence of language tasks through in-context\nlearning (ICL). We further introduce Task Haystack, an evaluation suite\ndedicated to assessing and diagnosing how long-context LMs utilizes contexts in\nLifelong ICL. When given a task instruction and test inputs, long-context LMs\nare expected to leverage the relevant demonstrations in the Lifelong ICL\nprompt, avoid distraction and interference from other tasks, and achieve test\naccuracies that are not significantly worse than those of the Single-task ICL\nbaseline.\n  Task Haystack draws inspiration from the widely-adopted\n\"needle-in-a-haystack\" (NIAH) evaluation, but presents distinct new challenges.\nIt requires models (1) to utilize the contexts at a deeper level, rather than\nresorting to simple copying and pasting; (2) to navigate through long streams\nof evolving topics and tasks, proxying the complexities and dynamism of\ncontexts in real-world scenarios. Additionally, Task Haystack inherits the\ncontrollability of NIAH, providing model developers with tools and\nvisualizations to identify model vulnerabilities effectively.\n  We benchmark 14 long-context LMs using Task Haystack, finding that frontier\nmodels like GPT-4o still struggle with the setting, failing on 15% of cases on\naverage. Most open-weight models further lack behind by a large margin, with\nfailure rates reaching up to 61%. In our controlled analysis, we identify\nfactors such as distraction and recency bias as contributors to these failure\ncases. Further, performance declines when task instructions are paraphrased at\ntest time or when ICL demonstrations are repeated excessively, raising concerns\nabout the robustness, instruction understanding, and true context utilization\nof long-context LMs.", "published": "2024-07-23 17:57:41", "link": "http://arxiv.org/abs/2407.16695v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "OpenHands: An Open Platform for AI Software Developers as Generalist\n  Agents", "abstract": "Software is one of the most powerful tools that we humans have at our\ndisposal; it allows a skilled programmer to interact with the world in complex\nand profound ways. At the same time, thanks to improvements in large language\nmodels (LLMs), there has also been a rapid development in AI agents that\ninteract with and affect change in their surrounding environments. In this\npaper, we introduce OpenHands (f.k.a. OpenDevin), a platform for the\ndevelopment of powerful and flexible AI agents that interact with the world in\nsimilar ways to those of a human developer: by writing code, interacting with a\ncommand line, and browsing the web. We describe how the platform allows for the\nimplementation of new agents, safe interaction with sandboxed environments for\ncode execution, coordination between multiple agents, and incorporation of\nevaluation benchmarks. Based on our currently incorporated benchmarks, we\nperform an evaluation of agents over 15 challenging tasks, including software\nengineering (e.g., SWE-BENCH) and web browsing (e.g., WEBARENA), among others.\nReleased under the permissive MIT license, OpenHands is a community project\nspanning academia and industry with more than 2.1K contributions from over 188\ncontributors.", "published": "2024-07-23 17:50:43", "link": "http://arxiv.org/abs/2407.16741v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "VisMin: Visual Minimal-Change Understanding", "abstract": "Fine-grained understanding of objects, attributes, and relationships between\nobjects is crucial for visual-language models (VLMs). Existing benchmarks\nprimarily focus on evaluating VLMs' capability to distinguish between two very\nsimilar captions given an image. In this paper, we introduce a new, challenging\nbenchmark termed Visual Minimal-Change Understanding (VisMin), which requires\nmodels to predict the correct image-caption match given two images and two\ncaptions. The image pair and caption pair contain minimal changes, i.e., only\none aspect changes at a time from among the following: object, attribute,\ncount, and spatial relation. These changes test the models' understanding of\nobjects, attributes (such as color, material, shape), counts, and spatial\nrelationships between objects. We built an automatic framework using large\nlanguage models and diffusion models, followed by a rigorous 4-step\nverification process by human annotators. Empirical experiments reveal that\ncurrent VLMs exhibit notable deficiencies in understanding spatial\nrelationships and counting abilities. We also generate a large-scale training\ndataset to finetune CLIP and Idefics2, showing significant improvements in\nfine-grained understanding across benchmarks and in CLIP's general image-text\nalignment. We release all resources, including the benchmark, training data,\nand finetuned model checkpoints, at https://vismin.net/.", "published": "2024-07-23 18:10:43", "link": "http://arxiv.org/abs/2407.16772v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive\n  Study and Hybrid Approach", "abstract": "Retrieval Augmented Generation (RAG) has been a powerful tool for Large\nLanguage Models (LLMs) to efficiently process overly lengthy contexts. However,\nrecent LLMs like Gemini-1.5 and GPT-4 show exceptional capabilities to\nunderstand long contexts directly. We conduct a comprehensive comparison\nbetween RAG and long-context (LC) LLMs, aiming to leverage the strengths of\nboth. We benchmark RAG and LC across various public datasets using three latest\nLLMs. Results reveal that when resourced sufficiently, LC consistently\noutperforms RAG in terms of average performance. However, RAG's significantly\nlower cost remains a distinct advantage. Based on this observation, we propose\nSelf-Route, a simple yet effective method that routes queries to RAG or LC\nbased on model self-reflection. Self-Route significantly reduces the\ncomputation cost while maintaining a comparable performance to LC. Our findings\nprovide a guideline for long-context applications of LLMs using RAG and LC.", "published": "2024-07-23 20:51:52", "link": "http://arxiv.org/abs/2407.16833v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MLLM-CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs", "abstract": "The ability to compare objects, scenes, or situations is crucial for\neffective decision-making and problem-solving in everyday life. For instance,\ncomparing the freshness of apples enables better choices during grocery\nshopping while comparing sofa designs helps optimize the aesthetics of our\nliving space. Despite its significance, the comparative capability is largely\nunexplored in artificial general intelligence (AGI). In this paper, we\nintroduce MLLM-CompBench, a benchmark designed to evaluate the comparative\nreasoning capability of multimodal large language models (MLLMs).\nMLLM-CompBench mines and pairs images through visually oriented questions\ncovering eight dimensions of relative comparison: visual attribute, existence,\nstate, emotion, temporality, spatiality, quantity, and quality. We curate a\ncollection of around 40K image pairs using metadata from diverse vision\ndatasets and CLIP similarity scores. These image pairs span a broad array of\nvisual domains, including animals, fashion, sports, and both outdoor and indoor\nscenes. The questions are carefully crafted to discern relative characteristics\nbetween two images and are labeled by human annotators for accuracy and\nrelevance. We use MLLM-CompBench to evaluate recent MLLMs, including\nGPT-4V(ision), Gemini-Pro, and LLaVA-1.6. Our results reveal notable\nshortcomings in their comparative abilities. We believe MLLM-COMPBENCH not only\nsheds light on these limitations but also establishes a solid foundation for\nfuture enhancements in the comparative capability of MLLMs.", "published": "2024-07-23 21:02:38", "link": "http://arxiv.org/abs/2407.16837v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Generation Constraint Scaling Can Mitigate Hallucination", "abstract": "Addressing the issue of hallucinations in large language models (LLMs) is a\ncritical challenge. As the cognitive mechanisms of hallucination have been\nrelated to memory, here we explore hallucination for LLM that is enabled with\nexplicit memory mechanisms. We empirically demonstrate that by simply scaling\nthe readout vector that constrains generation in a memory-augmented LLM\ndecoder, hallucination mitigation can be achieved in a training-free manner.\nOur method is geometry-inspired and outperforms a state-of-the-art LLM editing\nmethod on the task of generation of Wikipedia-like biography entries both in\nterms of generation quality and runtime complexity.", "published": "2024-07-23 23:58:19", "link": "http://arxiv.org/abs/2407.16908v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating Long Range Dependency Handling in Code Generation Models\n  using Multi-Step Key Retrieval", "abstract": "As language models support larger and larger context sizes, evaluating their\nability to make effective use of that context becomes increasingly important.\nWe analyze the ability of several code generation models to handle long range\ndependencies using a suite of multi-step key retrieval tasks in context windows\nup to 8k tokens in length. The tasks progressively increase in difficulty and\nallow more nuanced evaluation of model capabilities than tests like the popular\nneedle-in-the-haystack test. We find that performance degrades significantly\n(up to 2x) when a function references another function that is defined later in\nthe prompt. We also observe that models that use sliding window attention\nmechanisms have difficulty handling references further than the size of a\nsingle window. We perform simple prompt modifications using call graph\ninformation to improve multi-step retrieval performance up to 3x. Our analysis\nhighlights different facets of long-context performance and is suggestive of\nprompt construction strategies for code completion tools", "published": "2024-07-23 02:45:22", "link": "http://arxiv.org/abs/2407.21049v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deep Learning based Key Information Extraction from Business Documents:\n  Systematic Literature Review", "abstract": "Extracting key information from documents represents a large portion of\nbusiness workloads and therefore offers a high potential for efficiency\nimprovements and process automation. With recent advances in deep learning, a\nplethora of deep learning-based approaches for Key Information Extraction have\nbeen proposed under the umbrella term Document Understanding that enable the\nprocessing of complex business documents. The goal of this systematic\nliterature review is an in-depth analysis of existing approaches in this domain\nand the identification of opportunities for further research. To this end, 96\napproaches published between 2017 and 2023 are analyzed in this study.", "published": "2024-07-23 08:15:55", "link": "http://arxiv.org/abs/2408.06345v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "A.1; I.2.7; I.4.9; I.7.5"], "primary_category": "cs.IR"}
{"title": "The CHiME-8 DASR Challenge for Generalizable and Array Agnostic Distant\n  Automatic Speech Recognition and Diarization", "abstract": "This paper presents the CHiME-8 DASR challenge which carries on from the\nprevious edition CHiME-7 DASR (C7DASR) and the past CHiME-6 challenge. It\nfocuses on joint multi-channel distant speech recognition (DASR) and\ndiarization with one or more, possibly heterogeneous, devices. The main goal is\nto spur research towards meeting transcription approaches that can generalize\nacross arbitrary number of speakers, diverse settings (formal vs. informal\nconversations), meeting duration, wide-variety of acoustic scenarios and\ndifferent recording configurations. Novelties with respect to C7DASR include:\ni) the addition of NOTSOFAR-1, an additional office/corporate meeting scenario,\nii) a manually corrected Mixer 6 development set, iii) a new track in which we\nallow the use of large-language models (LLM) iv) a jury award mechanism to\nencourage participants to explore also more practical and innovative solutions.\nTo lower the entry barrier for participants, we provide a standalone toolkit\nfor downloading and preparing such datasets as well as performing text\nnormalization and scoring their submissions. Furthermore, this year we also\nprovide two baseline systems, one directly inherited from C7DASR and based on\nESPnet and another one developed on NeMo and based on NeMo team submission in\nlast year C7DASR. Baseline system results suggest that the addition of the\nNOTSOFAR-1 scenario significantly increases the task's difficulty due to its\nhigh number of speakers and very short duration.", "published": "2024-07-23 12:54:32", "link": "http://arxiv.org/abs/2407.16447v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Distortion Recovery: A Two-Stage Method for Guitar Effect Removal", "abstract": "Removing audio effects from electric guitar recordings makes it easier for\npost-production and sound editing. An audio distortion recovery model not only\nimproves the clarity of the guitar sounds but also opens up new opportunities\nfor creative adjustments in mixing and mastering. While progress have been made\nin creating such models, previous efforts have largely focused on synthetic\ndistortions that may be too simplistic to accurately capture the complexities\nseen in real-world recordings.\n  In this paper, we tackle the task by using a dataset of guitar recordings\nrendered with commercial-grade audio effect VST plugins. Moreover, we introduce\na novel two-stage methodology for audio distortion recovery. The idea is to\nfirstly process the audio signal in the Mel-spectrogram domain in the first\nstage, and then use a neural vocoder to generate the pristine original guitar\nsound from the processed Mel-spectrogram in the second stage. We report a set\nof experiments demonstrating the effectiveness of our approach over existing\nmethods, through both subjective and objective evaluation metrics.", "published": "2024-07-23 16:55:13", "link": "http://arxiv.org/abs/2407.16639v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Synth4Kws: Synthesized Speech for User Defined Keyword Spotting in Low\n  Resource Environments", "abstract": "One of the challenges in developing a high quality custom keyword spotting\n(KWS) model is the lengthy and expensive process of collecting training data\ncovering a wide range of languages, phrases and speaking styles. We introduce\nSynth4Kws - a framework to leverage Text to Speech (TTS) synthesized data for\ncustom KWS in different resource settings. With no real data, we found\nincreasing TTS phrase diversity and utterance sampling monotonically improves\nmodel performance, as evaluated by EER and AUC metrics over 11k utterances of\nthe speech command dataset. In low resource settings, with 50k real utterances\nas a baseline, we found using optimal amounts of TTS data can improve EER by\n30.1% and AUC by 46.7%. Furthermore, we mix TTS data with varying amounts of\nreal data and interpolate the real data needed to achieve various quality\ntargets. Our experiments are based on English and single word utterances but\nthe findings generalize to i18n languages and other keyword types.", "published": "2024-07-23 21:05:44", "link": "http://arxiv.org/abs/2407.16840v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "On the Utility of Speech and Audio Foundation Models for Marmoset Call\n  Analysis", "abstract": "Marmoset monkeys encode vital information in their calls and serve as a\nsurrogate model for neuro-biologists to understand the evolutionary origins of\nhuman vocal communication. Traditionally analyzed with signal processing-based\nfeatures, recent approaches have utilized self-supervised models pre-trained on\nhuman speech for feature extraction, capitalizing on their ability to learn a\nsignal's intrinsic structure independently of its acoustic domain. However, the\nutility of such foundation models remains unclear for marmoset call analysis in\nterms of multi-class classification, bandwidth, and pre-training domain. This\nstudy assesses feature representations derived from speech and general audio\ndomains, across pre-training bandwidths of 4, 8, and 16 kHz for marmoset\ncall-type and caller classification tasks. Results show that models with higher\nbandwidth improve performance, and pre-training on speech or general audio\nyields comparable results, improving over a spectral baseline.", "published": "2024-07-23 12:00:44", "link": "http://arxiv.org/abs/2407.16417v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Coarse-to-Fine Proposal Refinement Framework for Audio Temporal Forgery\n  Detection and Localization", "abstract": "Recently, a novel form of audio partial forgery has posed challenges to its\nforensics, requiring advanced countermeasures to detect subtle forgery\nmanipulations within long-duration audio. However, existing countermeasures\nstill serve a classification purpose and fail to perform meaningful analysis of\nthe start and end timestamps of partial forgery segments. To address this\nchallenge, we introduce a novel coarse-to-fine proposal refinement framework\n(CFPRF) that incorporates a frame-level detection network (FDN) and a proposal\nrefinement network (PRN) for audio temporal forgery detection and localization.\nSpecifically, the FDN aims to mine informative inconsistency cues between real\nand fake frames to obtain discriminative features that are beneficial for\nroughly indicating forgery regions. The PRN is responsible for predicting\nconfidence scores and regression offsets to refine the coarse-grained proposals\nderived from the FDN. To learn robust discriminative features, we devise a\ndifference-aware feature learning (DAFL) module guided by contrastive\nrepresentation learning to enlarge the sensitive differences between different\nframes induced by minor manipulations. We further design a boundary-aware\nfeature enhancement (BAFE) module to capture the contextual information of\nmultiple transition boundaries and guide the interaction between boundary\ninformation and temporal features via a cross-attention mechanism. Extensive\nexperiments show that our CFPRF achieves state-of-the-art performance on\nvarious datasets, including LAV-DF, ASVS2019PS, and HAD.", "published": "2024-07-23 15:07:52", "link": "http://arxiv.org/abs/2407.16554v1", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS", "68T07, 68T10", "I.2; I.5"], "primary_category": "cs.MM"}
{"title": "Audio Prompt Adapter: Unleashing Music Editing Abilities for\n  Text-to-Music with Lightweight Finetuning", "abstract": "Text-to-music models allow users to generate nearly realistic musical audio\nwith textual commands. However, editing music audios remains challenging due to\nthe conflicting desiderata of performing fine-grained alterations on the audio\nwhile maintaining a simple user interface. To address this challenge, we\npropose Audio Prompt Adapter (or AP-Adapter), a lightweight addition to\npretrained text-to-music models. We utilize AudioMAE to extract features from\nthe input audio, and construct attention-based adapters to feedthese features\ninto the internal layers of AudioLDM2, a diffusion-based text-to-music model.\nWith 22M trainable parameters, AP-Adapter empowers users to harness both global\n(e.g., genre and timbre) and local (e.g., melody) aspects of music, using the\noriginal audio and a short text as inputs. Through objective and subjective\nstudies, we evaluate AP-Adapter on three tasks: timbre transfer, genre\ntransfer, and accompaniment generation. Additionally, we demonstrate its\neffectiveness on out-of-domain audios containing unseen instruments during\ntraining.", "published": "2024-07-23 15:16:18", "link": "http://arxiv.org/abs/2407.16564v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Synthesizer Sound Matching Using Audio Spectrogram Transformers", "abstract": "Systems for synthesizer sound matching, which automatically set the\nparameters of a synthesizer to emulate an input sound, have the potential to\nmake the process of synthesizer programming faster and easier for novice and\nexperienced musicians alike, whilst also affording new means of interaction\nwith synthesizers. Considering the enormous variety of synthesizers in the\nmarketplace, and the complexity of many of them, general-purpose sound matching\nsystems that function with minimal knowledge or prior assumptions about the\nunderlying synthesis architecture are particularly desirable. With this in\nmind, we introduce a synthesizer sound matching model based on the Audio\nSpectrogram Transformer. We demonstrate the viability of this model by training\non a large synthetic dataset of randomly generated samples from the popular\nMassive synthesizer. We show that this model can reconstruct parameters of\nsamples generated from a set of 16 parameters, highlighting its improved\nfidelity relative to multi-layer perceptron and convolutional neural network\nbaselines. We also provide audio examples demonstrating the out-of-domain model\nperformance in emulating vocal imitations, and sounds from other synthesizers\nand musical instruments.", "published": "2024-07-23 16:58:14", "link": "http://arxiv.org/abs/2407.16643v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Automatic Equalization for Individual Instrument Tracks Using\n  Convolutional Neural Networks", "abstract": "We propose a novel approach for the automatic equalization of individual\nmusical instrument tracks. Our method begins by identifying the instrument\npresent within a source recording in order to choose its corresponding ideal\nspectrum as a target. Next, the spectral difference between the recording and\nthe target is calculated, and accordingly, an equalizer matching model is used\nto predict settings for a parametric equalizer. To this end, we build upon a\ndifferentiable parametric equalizer matching neural network, demonstrating\nimprovements relative to previously established state-of-the-art. Unlike past\napproaches, we show how our system naturally allows real-world audio data to be\nleveraged during the training of our matching model, effectively generating\nsuitably produced training targets in an automated manner mirroring conditions\nat inference time. Consequently, we illustrate how fine-tuning our matching\nmodel on such examples considerably improves parametric equalizer matching\nperformance in real-world scenarios, decreasing mean absolute error by 24%\nrelative to methods relying solely on random parameter sampling techniques as a\nself-supervised learning strategy. We perform listening tests, and demonstrate\nthat our proposed automatic equalization solution subjectively enhances the\ntonal characteristics for recordings of common instrument types.", "published": "2024-07-23 17:55:25", "link": "http://arxiv.org/abs/2407.16691v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
