{"title": "Detecting and ordering adjectival scalemates", "abstract": "This paper presents a pattern-based method that can be used to infer\nadjectival scales, such as <lukewarm, warm, hot>, from a corpus. Specifically,\nthe proposed method uses lexical patterns to automatically identify and order\npairs of scalemates, followed by a filtering phase in which unrelated pairs are\ndiscarded. For the filtering phase, several different similarity measures are\nimplemented and compared. The model presented in this paper is evaluated using\nthe current standard, along with a novel evaluation set, and shown to be at\nleast as good as the current state-of-the-art.", "published": "2015-04-30 07:27:56", "link": "http://arxiv.org/abs/1504.08102v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Texts in, meaning out: neural language models in semantic similarity\n  task for Russian", "abstract": "Distributed vector representations for natural language vocabulary get a lot\nof attention in contemporary computational linguistics. This paper summarizes\nthe experience of applying neural network language models to the task of\ncalculating semantic similarity for Russian. The experiments were performed in\nthe course of Russian Semantic Similarity Evaluation track, where our models\ntook from the 2nd to the 5th position, depending on the task.\n  We introduce the tools and corpora used, comment on the nature of the shared\ntask and describe the achieved results. It was found out that Continuous\nSkip-gram and Continuous Bag-of-words models, previously successfully applied\nto English material, can be used for semantic modeling of Russian as well.\nMoreover, we show that texts in Russian National Corpus (RNC) provide an\nexcellent training material for such models, outperforming other, much larger\ncorpora. It is especially true for semantic relatedness tasks (although\nstacking models trained on larger corpora on top of RNC models improves\nperformance even more).\n  High-quality semantic vectors learned in such a way can be used in a variety\nof linguistic tasks and promise an exciting field for further study.", "published": "2015-04-30 12:03:10", "link": "http://arxiv.org/abs/1504.08183v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Concept-level Emotion Cause in Microblogging", "abstract": "In this paper, we propose a Concept-level Emotion Cause Model (CECM), instead\nof the mere word-level models, to discover causes of microblogging users'\ndiversified emotions on specific hot event. A modified topic-supervised biterm\ntopic model is utilized in CECM to detect emotion topics' in event-related\ntweets, and then context-sensitive topical PageRank is utilized to detect\nmeaningful multiword expressions as emotion causes. Experimental results on a\ndataset from Sina Weibo, one of the largest microblogging websites in China,\nshow CECM can better detect emotion causes than baseline methods.", "published": "2015-04-30 00:35:32", "link": "http://arxiv.org/abs/1504.08050v1", "categories": ["cs.CL", "cs.AI", "68P20", "H.2.8"], "primary_category": "cs.CL"}
{"title": "Parsing Linear Context-Free Rewriting Systems with Fast Matrix\n  Multiplication", "abstract": "We describe a matrix multiplication recognition algorithm for a subset of\nbinary linear context-free rewriting systems (LCFRS) with running time\n$O(n^{\\omega d})$ where $M(m) = O(m^{\\omega})$ is the running time for $m\n\\times m$ matrix multiplication and $d$ is the \"contact rank\" of the LCFRS --\nthe maximal number of combination and non-combination points that appear in the\ngrammar rules. We also show that this algorithm can be used as a subroutine to\nget a recognition algorithm for general binary LCFRS with running time\n$O(n^{\\omega d + 1})$. The currently best known $\\omega$ is smaller than\n$2.38$. Our result provides another proof for the best known result for parsing\nmildly context sensitive formalisms such as combinatory categorial grammars,\nhead grammars, linear indexed grammars, and tree adjoining grammars, which can\nbe parsed in time $O(n^{4.76})$. It also shows that inversion transduction\ngrammars can be parsed in time $O(n^{5.76})$. In addition, binary LCFRS\nsubsumes many other formalisms and types of grammars, for some of which we also\nimprove the asymptotic complexity of parsing.", "published": "2015-04-30 18:53:06", "link": "http://arxiv.org/abs/1504.08342v3", "categories": ["cs.CL", "cs.FL"], "primary_category": "cs.CL"}
