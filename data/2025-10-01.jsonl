{"title": "One More Question is Enough, Expert Question Decomposition (EQD) Model for Domain Quantitative Reasoning", "abstract": "Domain-specific quantitative reasoning remains a major challenge for large\nlanguage models (LLMs), especially in fields requiring expert knowledge and\ncomplex question answering (QA). In this work, we propose Expert Question\nDecomposition (EQD), an approach designed to balance the use of domain\nknowledge with computational efficiency. EQD is built on a two-step fine-tuning\nframework and guided by a reward function that measures the effectiveness of\ngenerated sub-questions in improving QA outcomes. It requires only a few\nthousand training examples and a single A100 GPU for fine-tuning, with\ninference time comparable to zero-shot prompting. Beyond its efficiency, EQD\noutperforms state-of-the-art domain-tuned models and advanced prompting\nstrategies. We evaluate EQD in the financial domain, characterized by\nspecialized knowledge and complex quantitative reasoning, across four benchmark\ndatasets. Our method consistently improves QA performance by 0.6% to 10.5%\nacross different LLMs. Our analysis reveals an important insight: in\ndomain-specific QA, a single supporting question often provides greater benefit\nthan detailed guidance steps.", "published": "2025-10-01 23:45:45", "link": "http://arxiv.org/abs/2510.01526v1", "categories": ["cs.CL", "q-fin.CP"], "primary_category": "cs.CL"}
{"title": "From Videos to Indexed Knowledge Graphs -- Framework to Marry Methods for Multimodal Content Analysis and Understanding", "abstract": "Analysis of multi-modal content can be tricky, computationally expensive, and\nrequire a significant amount of engineering efforts. Lots of work with\npre-trained models on static data is out there, yet fusing these opensource\nmodels and methods with complex data such as videos is relatively challenging.\nIn this paper, we present a framework that enables efficiently prototyping\npipelines for multi-modal content analysis. We craft a candidate recipe for a\npipeline, marrying a set of pre-trained models, to convert videos into a\ntemporal semi-structured data format. We translate this structure further to a\nframe-level indexed knowledge graph representation that is query-able and\nsupports continual learning, enabling the dynamic incorporation of new\ndomain-specific knowledge through an interactive medium.", "published": "2025-10-01 23:20:15", "link": "http://arxiv.org/abs/2510.01513v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.CV"}
{"title": "Extracting O*NET Features from the NLx Corpus to Build Public Use Aggregate Labor Market Data", "abstract": "Data from online job postings are difficult to access and are not built in a\nstandard or transparent manner. Data included in the standard taxonomy and\noccupational information database (O*NET) are updated infrequently and based on\nsmall survey samples. We adopt O*NET as a framework for building natural\nlanguage processing tools that extract structured information from job\npostings. We publish the Job Ad Analysis Toolkit (JAAT), a collection of\nopen-source tools built for this purpose, and demonstrate its reliability and\naccuracy in out-of-sample and LLM-as-a-Judge testing. We extract more than 10\nbillion data points from more than 155 million online job ads provided by the\nNational Labor Exchange (NLx) Research Hub, including O*NET tasks, occupation\ncodes, tools, and technologies, as well as wages, skills, industry, and more\nfeatures. We describe the construction of a dataset of occupation, state, and\nindustry level features aggregated by monthly active jobs from 2015 - 2025. We\nillustrate the potential for research and future uses in education and\nworkforce development.", "published": "2025-10-01 21:27:11", "link": "http://arxiv.org/abs/2510.01470v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "A-VERT: Agnostic Verification with Embedding Ranking Targets", "abstract": "The automatic evaluation of Language Model (LM) responses is a critical piece\nin the development of benchmarks and metrics, both for model training and\nquality assessment of production model endpoints. The current approaches to\nresponse classification relies on methods that are too expensive (i.e.\nLLM-as-a-Judge) or that are far from real-world conditions (string-matching,\nlogprob). In this paper, a structure-free evaluation method is presented. The\nmethod makes use of semantic embedding distances to match target candidates\nwith arbitrary LM-generated text, resulting in a robust classification of the\nresponse at a relatively low compute cost (embedding models of less than $10B$\nparameters). The results show a regression score of ~0.97 and an accuracy of\n~96% against human annotators, tested over 3 data sets and 3 different LM\narchitectures.", "published": "2025-10-01 21:26:03", "link": "http://arxiv.org/abs/2510.01469v1", "categories": ["cs.CL", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM Reasoning", "abstract": "Since the release of Deepseek-R1, reinforcement learning with verifiable\nrewards (RLVR) has become a central approach for training large language models\n(LLMs) on reasoning tasks. Recent work has largely focused on modifying loss\nfunctions to make RLVR more efficient and effective. In this paper, motivated\nby studies of overthinking in LLMs, we propose Length-aware Sampling for Policy\nOptimization (LSPO), a novel meta-RLVR algorithm that dynamically selects\ntraining data at each step based on the average response length. We evaluate\nLSPO across multiple base models and datasets, demonstrating that it\nconsistently improves learning effectiveness. In addition, we conduct a\ndetailed ablation study to examine alternative ways of incorporating length\nsignals into dynamic sampling, offering further insights and highlighting\npromising directions for future research.", "published": "2025-10-01 20:57:22", "link": "http://arxiv.org/abs/2510.01459v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning", "abstract": "Reinforcement learning with verifiable rewards (RLVR) improves reasoning in\nlarge language models (LLMs) but struggles with exploration, an issue that\nstill persists for multimodal LLMs (MLLMs). Current methods treat the visual\ninput as a fixed, deterministic condition, overlooking a critical source of\nambiguity and struggling to build policies robust to plausible visual\nvariations. We introduce $\\textbf{VOGUE (Visual Uncertainty Guided\nExploration)}$, a novel method that shifts exploration from the output (text)\nto the input (visual) space. By treating the image as a stochastic context,\nVOGUE quantifies the policy's sensitivity to visual perturbations using the\nsymmetric KL divergence between a \"raw\" and \"noisy\" branch, creating a direct\nsignal for uncertainty-aware exploration. This signal shapes the learning\nobjective via an uncertainty-proportional bonus, which, combined with a\ntoken-entropy bonus and an annealed sampling schedule, effectively balances\nexploration and exploitation. Implemented within GRPO on two model scales\n(Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three\nvisual math benchmarks and 3.7% on three general-domain reasoning benchmarks,\nwhile simultaneously increasing pass@4 performance and mitigating the\nexploration decay commonly observed in RL fine-tuning. Our work shows that\ngrounding exploration in the inherent uncertainty of visual inputs is an\neffective strategy for improving multimodal reasoning.", "published": "2025-10-01 20:32:08", "link": "http://arxiv.org/abs/2510.01444v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Optimal Stopping vs Best-of-$N$ for Inference Time Optimization", "abstract": "Large language model (LLM) generation often requires balancing output quality\nagainst inference cost, especially when using multiple generations. We\nintroduce a new framework for inference-time optimization based on the\nclassical Pandora's Box problem. Viewing each generation as opening a costly\n\"box\" with random reward, we develop algorithms that decide when to stop\ngenerating without knowing the underlying reward distribution. Our first\ncontribution is a UCB-style Pandora's Box algorithm, which achieves performance\nthat is provably close to Weitzman's algorithm, the optimal strategy when the\ndistribution is known. We further adapt this method to practical LLM settings\nby addressing reward scaling across prompts via a Bradley-Terry inspired\ntransformation. This leads to an adaptive inference-time optimization method\nthat normalizes rewards and learns stopping thresholds on the fly. Experiments\non the AlpacaFarm and HH-RLHF datasets, using multiple LLM-reward model pairs,\nshow that our adaptive strategy can obtain the same performance as non-adaptive\nBest-of-N sampling while requiring 15-35 percent fewer generations on average.\nOur results establish a principled bridge between optimal stopping theory and\ninference-time scaling, providing both theoretical performance bounds and\npractical efficiency gains for LLM deployment.", "published": "2025-10-01 19:25:59", "link": "http://arxiv.org/abs/2510.01394v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "TAG-EQA: Text-And-Graph for Event Question Answering via Structured Prompting Strategies", "abstract": "Large language models (LLMs) excel at general language tasks but often\nstruggle with event-based questions-especially those requiring causal or\ntemporal reasoning. We introduce TAG-EQA (Text-And-Graph for Event Question\nAnswering), a prompting framework that injects causal event graphs into LLM\ninputs by converting structured relations into natural-language statements.\nTAG-EQA spans nine prompting configurations, combining three strategies\n(zero-shot, few-shot, chain-of-thought) with three input modalities (text-only,\ngraph-only, text+graph), enabling a systematic analysis of when and how\nstructured knowledge aids inference. On the TORQUESTRA benchmark, TAG-EQA\nimproves accuracy by 5% on average over text-only baselines, with gains up to\n12% in zero-shot settings and 18% when graph-augmented CoT prompting is\neffective. While performance varies by model and configuration, our findings\nshow that causal graphs can enhance event reasoning in LLMs without\nfine-tuning, offering a flexible way to encode structure in prompt-based QA.", "published": "2025-10-01 19:23:41", "link": "http://arxiv.org/abs/2510.01391v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-tuning with RAG for Improving LLM Learning of New Skills", "abstract": "Large language model (LLM) agents deployed for multi-step tasks frequently\nfail in predictable ways: attempting actions with unmet preconditions, issuing\nredundant commands, or mishandling environment constraints. While\nretrieval-augmented generation (RAG) can improve performance by providing\nruntime guidance, it requires maintaining external knowledge databases and adds\ncomputational overhead at every deployment. We propose a simple pipeline that\nconverts inference-time retrieval into learned competence through distillation.\nOur approach: (1) extracts compact, reusable hints from agent failures, (2)\nuses these hints to generate improved teacher trajectories via one-shot\nretrieval at episode start, and (3) trains student models on these trajectories\nwith hint strings removed, forcing internalization rather than memorization.\nAcross two interactive benchmarks, ALFWorld (household tasks) and WebShop\n(online shopping), distilled students consistently outperform baseline agents,\nachieving up to 91% success on ALFWorld (vs. 79% for baselines) and improving\nWebShop scores to 72 (vs. 61 for baselines), while using 10-60% fewer tokens\nthan retrieval-augmented teachers depending on the environment. The approach\ngeneralizes across model scales (7B/14B parameters) and agent architectures\n(ReAct/StateAct), demonstrating that retrieval benefits can be effectively\ninternalized through targeted fine-tuning without permanent runtime\ndependencies.", "published": "2025-10-01 19:03:48", "link": "http://arxiv.org/abs/2510.01375v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort", "abstract": "Reward hacking, where a reasoning model exploits loopholes in a reward\nfunction to achieve high rewards without solving the intended task, poses a\nsignificant threat. This behavior may be explicit, i.e. verbalized in the\nmodel's chain-of-thought (CoT), or implicit, where the CoT appears benign thus\nbypasses CoT monitors. To detect implicit reward hacking, we propose TRACE\n(Truncated Reasoning AUC Evaluation). Our key observation is that hacking\noccurs when exploiting the loophole is easier than solving the actual task.\nThis means that the model is using less `effort' than required to achieve high\nreward. TRACE quantifies effort by measuring how early a model's reasoning\nbecomes sufficient to pass a verifier. We progressively truncate a model's CoT\nat various lengths, force the model to answer, and measure the verifier-passing\nrate at each cutoff. A hacking model, which takes a shortcut, will achieve a\nhigh passing rate with only a small fraction of its CoT, yielding a large area\nunder the accuracy-vs-length curve. TRACE achieves over 65% gains over our\nstrongest 72B CoT monitor in math reasoning, and over 30% gains over a 32B\nmonitor in coding. We further show that TRACE can discover unknown loopholes\nduring training. Overall, TRACE offers a scalable unsupervised approach for\noversight where current monitoring methods prove ineffective.", "published": "2025-10-01 18:49:45", "link": "http://arxiv.org/abs/2510.01367v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents", "abstract": "Multiple prompt injection attacks have been proposed against web agents. At\nthe same time, various methods have been developed to detect general prompt\ninjection attacks, but none have been systematically evaluated for web agents.\nIn this work, we bridge this gap by presenting the first comprehensive\nbenchmark study on detecting prompt injection attacks targeting web agents. We\nbegin by introducing a fine-grained categorization of such attacks based on the\nthreat model. We then construct datasets containing both malicious and benign\nsamples: malicious text segments generated by different attacks, benign text\nsegments from four categories, malicious images produced by attacks, and benign\nimages from two categories. Next, we systematize both text-based and\nimage-based detection methods. Finally, we evaluate their performance across\nmultiple scenarios. Our key findings show that while some detectors can\nidentify attacks that rely on explicit textual instructions or visible image\nperturbations with moderate to high accuracy, they largely fail against attacks\nthat omit explicit instructions or employ imperceptible perturbations. Our\ndatasets and code are released at:\nhttps://github.com/Norrrrrrr-lyn/WAInjectBench.", "published": "2025-10-01 18:34:06", "link": "http://arxiv.org/abs/2510.01354v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "MEMTRACK: Evaluating Long-Term Memory and State Tracking in Multi-Platform Dynamic Agent Environments", "abstract": "Recent works on context and memory benchmarking have primarily focused on\nconversational instances but the need for evaluating memory in dynamic\nenterprise environments is crucial for its effective application. We introduce\nMEMTRACK, a benchmark designed to evaluate long-term memory and state tracking\nin multi-platform agent environments. MEMTRACK models realistic organizational\nworkflows by integrating asynchronous events across multiple communication and\nproductivity platforms such as Slack, Linear and Git. Each benchmark instance\nprovides a chronologically platform-interleaved timeline, with noisy,\nconflicting, cross-referring information as well as potential\ncodebase/file-system comprehension and exploration. Consequently, our benchmark\ntests memory capabilities such as acquistion, selection and conflict\nresolution. We curate the MEMTRACK dataset through both manual expert driven\ndesign and scalable agent based synthesis, generating ecologically valid\nscenarios grounded in real world software development processes. We introduce\npertinent metrics for Correctness, Efficiency, and Redundancy that capture the\neffectiveness of memory mechanisms beyond simple QA performance. Experiments\nacross SoTA LLMs and memory backends reveal challenges in utilizing memory\nacross long horizons, handling cross-platform dependencies, and resolving\ncontradictions. Notably, the best performing GPT-5 model only achieves a 60\\%\nCorrectness score on MEMTRACK. This work provides an extensible framework for\nadvancing evaluation research for memory-augmented agents, beyond existing\nfocus on conversational setups, and sets the stage for multi-agent,\nmulti-platform memory benchmarking in complex organizational settings", "published": "2025-10-01 18:34:03", "link": "http://arxiv.org/abs/2510.01353v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Aristotle: IMO-level Automated Theorem Proving", "abstract": "We introduce Aristotle, an AI system that combines formal verification with\ninformal reasoning, achieving gold-medal-equivalent performance on the 2025\nInternational Mathematical Olympiad problems. Aristotle integrates three main\ncomponents: a Lean proof search system, an informal reasoning system that\ngenerates and formalizes lemmas, and a dedicated geometry solver. Our system\ndemonstrates state-of-the-art performance with favorable scaling properties for\nautomated theorem proving.", "published": "2025-10-01 18:21:13", "link": "http://arxiv.org/abs/2510.01346v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "HiSpec: Hierarchical Speculative Decoding for LLMs", "abstract": "Speculative decoding accelerates LLM inference by using a smaller draft model\nto speculate tokens that a larger target model verifies. Verification is often\nthe bottleneck (e.g. verification is $4\\times$ slower than token generation\nwhen a 3B model speculates for a 70B target model), but most prior works focus\nonly on accelerating drafting. $\\textit{``Intermediate\"}$ verification reduces\nverification time by discarding inaccurate draft tokens early, but existing\nmethods incur substantial training overheads in incorporating the intermediate\nverifier, increase the memory footprint to orchestrate the intermediate\nverification step, and compromise accuracy by relying on approximate\nheuristics.\n  We propose $\\underline{\\textit{Hi}}\\textit{erarchical\n}\\underline{\\textit{Spec}}\\textit{ulative Decoding (HiSpec)}$, a framework for\nhigh-throughput speculative decoding that exploits $\\textit{early-exit (EE)\nmodels}$ for low-overhead intermediate verification. EE models allow tokens to\nexit early by skipping layer traversal and are explicitly trained so that\nhidden states at selected layers can be interpreted, making them uniquely\nsuited for intermediate verification without drastically increasing compute and\nmemory overheads. To improve resource-efficiency even further, we design a\nmethodology that enables HiSpec to re-use key-value caches and hidden states\nbetween the draft, intermediate verifier, and target models. To maintain\naccuracy, HiSpec periodically validates the draft tokens accepted by the\nintermediate verifier against the target model. Our evaluations using various\nrepresentative benchmarks and models show that HiSpec improves throughput by\n1.28$\\times$ on average and by up to 2.01$\\times$ compared to the baseline\nsingle-layer speculation without compromising accuracy.", "published": "2025-10-01 18:04:14", "link": "http://arxiv.org/abs/2510.01336v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BroRL: Scaling Reinforcement Learning via Broadened Exploration", "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key\ningredient for unlocking complex reasoning capabilities in large language\nmodels. Recent work ProRL has shown promise in scaling RL by increasing the\nnumber of training steps. However, performance plateaus after thousands of\nsteps, with clear diminishing returns from allocating more computation to\nadditional training. In this work, we investigate a complementary paradigm for\nscaling RL, BroR-Lincreasing the number of rollouts per example to hundreds to\nexhaustively Broaden exploration, which yields continuous performance gains\nbeyond the saturation point observed in ProRL when scaling the number of\ntraining steps. Our approach is motivated by a mass balance equation analysis\nallowing us to characterize the rate of change in probability mass for correct\nand incorrect tokens during the reinforcement process. We show that under a\none-step RL assumption, sampled rollout tokens always contribute to\ncorrect-mass expansion, while unsampled tokens outside rollouts may lead to\ngains or losses depending on their distribution and the net reward balance.\nImportantly, as the number of rollouts per example N increases, the effect of\nunsampled terms diminishes, ensuring overall correct-mass expansion. To\nvalidate our theoretical analysis, we conduct simulations under more relaxed\nconditions and find that a sufficiently large rollout size N-corresponding to\nample exploration-guarantees an increase in the probability mass of all correct\ntokens. Empirically, BroRL revives models saturated after 3K ProRL training\nsteps and demonstrates robust, continuous improvement, achieving\nstate-of-the-art results for the 1.5B model across diverse benchmarks.", "published": "2025-10-01 17:59:02", "link": "http://arxiv.org/abs/2510.01180v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models", "abstract": "Although current large Vision-Language Models (VLMs) have advanced in\nmultimodal understanding and reasoning, their fundamental perceptual and\nreasoning abilities remain limited. Specifically, even on simple jigsaw tasks,\nexisting VLMs perform near randomly, revealing deficiencies in core perception\nand reasoning capabilities. While high-quality vision-language data can enhance\nthese capabilities, its scarcity and limited scalability impose significant\nconstraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction\nLearning for Enhancing visual perception and reasoning in VLMs. AGILE\nformulates jigsaw solving as an interactive process, enabling the model to\nprogressively engage with the environment. At each step, the model generates\nexecutable code to perform an action based on the current state, while the\nenvironment provides fine-grained visual feedback to guide task completion.\nThrough this iterative cycle of observation and interaction, the model\nincrementally improves its perceptual and reasoning capabilities via\nexploration and feedback. Experimental results show that AGILE not only\nsubstantially boosts performance on jigsaw tasks of varying complexity (e.g.,\nincreasing accuracy from 9.5% to 82.8% under the 2 $\\times$ 2 setting) but also\ndemonstrates strong generalization across 9 general vision tasks, achieving an\naverage improvement of 3.1%. These results indicate notable enhancements in\nboth perceptual and reasoning abilities. This work opens a new avenue for\nadvancing reasoning and generalization in multimodal models and provides an\nefficient, scalable solution to the scarcity of multimodal reinforcement\nlearning data. The code and datasets is available at\nhttps://github.com/yuzeng0-0/AGILE .", "published": "2025-10-01 17:58:05", "link": "http://arxiv.org/abs/2510.01304v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments", "abstract": "Large Language Model (LLM) agents are rapidly emerging as powerful systems\nfor automating tasks across domains. Yet progress in the open-source community\nis constrained by the lack of high quality permissively licensed tool-agentic\ntraining data. Existing datasets are often limited in diversity, realism, and\ncomplexity, particularly regarding multi-tool and multi-turn interactions. To\naddress this gap, we introduce Toucan, the largest publicly available\ntool-agentic dataset to date, containing 1.5 million trajectories synthesized\nfrom nearly 500 real-world Model Context Protocols (MCPs). Unlike prior work,\nToucan leverages authentic MCP environments to generate diverse, realistic, and\nchallenging tasks with trajectories involving real tool execution. Our pipeline\nfirst produces a broad spectrum of tool-use queries using five distinct models,\napplies model-based quality filtering, and then generates agentic trajectories\nwith three teacher models using two agentic frameworks. Rigorous rule-based and\nmodel-based validation ensures high-quality outputs. We also introduce three\nextension mechanisms to further diversify tasks and simulate multi-turn\nconversations. Models fine-tuned on Toucan outperform larger closed-source\ncounterparts on the BFCL V3 benchmark and push the Pareto frontier forward on\nMCP-Universe Bench.", "published": "2025-10-01 17:58:03", "link": "http://arxiv.org/abs/2510.01179v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Code2Video: A Code-centric Paradigm for Educational Video Generation", "abstract": "While recent generative models advance pixel-space video synthesis, they\nremain limited in producing professional educational videos, which demand\ndisciplinary knowledge, precise visual structures, and coherent transitions,\nlimiting their applicability in educational scenarios. Intuitively, such\nrequirements are better addressed through the manipulation of a renderable\nenvironment, which can be explicitly controlled via logical commands (e.g.,\ncode). In this work, we propose Code2Video, a code-centric agent framework for\ngenerating educational videos via executable Python code. The framework\ncomprises three collaborative agents: (i) Planner, which structures lecture\ncontent into temporally coherent flows and prepares corresponding visual\nassets; (ii) Coder, which converts structured instructions into executable\nPython codes while incorporating scope-guided auto-fix to enhance efficiency;\nand (iii) Critic, which leverages vision-language models (VLM) with visual\nanchor prompts to refine spatial layout and ensure clarity. To support\nsystematic evaluation, we build MMMC, a benchmark of professionally produced,\ndiscipline-specific educational videos. We evaluate MMMC across diverse\ndimensions, including VLM-as-a-Judge aesthetic scores, code efficiency, and\nparticularly, TeachQuiz, a novel end-to-end metric that quantifies how well a\nVLM, after unlearning, can recover knowledge by watching the generated videos.\nOur results demonstrate the potential of Code2Video as a scalable,\ninterpretable, and controllable approach, achieving 40% improvement over direct\ncode generation and producing videos comparable to human-crafted tutorials. The\ncode and datasets are available at https://github.com/showlab/Code2Video.", "published": "2025-10-01 17:56:48", "link": "http://arxiv.org/abs/2510.01174v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.HC", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Energy-Regularized Sequential Model Editing on Hyperspheres", "abstract": "Large language models (LLMs) require constant updates to remain aligned with\nevolving real-world knowledge. Model editing offers a lightweight alternative\nto retraining, but sequential editing often destabilizes representations and\ninduces catastrophic forgetting. In this work, we seek to better understand and\nmitigate performance degradation caused by sequential editing. We hypothesize\nthat hyperspherical uniformity, a property that maintains uniform distribution\nof neuron weights on a hypersphere, helps the model remain stable, retain prior\nknowledge, while still accommodate new updates. We use Hyperspherical Energy\n(HE) to quantify neuron uniformity during editing, and examine its correlation\nwith editing performance. Empirical studies across widely used editing methods\nreveals a strong correlation between HE dynamics and editing performance, with\nediting failures consistently coinciding with high HE fluctuations. We further\ntheoretically prove that HE dynamics impose a lower bound on the degradation of\npretrained knowledge, highlighting why HE stability is crucial for knowledge\nretention. Motivated by these insights, we propose SPHERE (Sparse Projection\nfor Hyperspherical Energy-Regularized Editing), an HE-driven regularization\nstrategy that stabilizes neuron weight distributions, ultimately preserving\nprior knowledge while enabling reliable sequential updates. Specifically,\nSPHERE identifies a sparse space complementary to the principal hyperspherical\ndirections of the pretrained weight matrices and projects new knowledge onto\nit, attenuating perturbations on the principal directions. Extensive\nexperiments on LLaMA3 (8B) and Qwen2.5 (7B) show that SPHERE outperforms the\nbest baseline in editing capability by an average of 16.41%, while most\nfaithfully preserving general model performance, thereby offering a principled\npath toward reliable large-scale knowledge editing.", "published": "2025-10-01 17:55:43", "link": "http://arxiv.org/abs/2510.01172v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity", "abstract": "Post-training alignment often reduces LLM diversity, leading to a phenomenon\nknown as mode collapse. Unlike prior work that attributes this effect to\nalgorithmic limitations, we identify a fundamental, pervasive data-level\ndriver: typicality bias in preference data, whereby annotators systematically\nfavor familiar text as a result of well-established findings in cognitive\npsychology. We formalize this bias theoretically, verify it on preference\ndatasets empirically, and show that it plays a central role in mode collapse.\nMotivated by this analysis, we introduce Verbalized Sampling, a simple,\ntraining-free prompting strategy to circumvent mode collapse. VS prompts the\nmodel to verbalize a probability distribution over a set of responses (e.g.,\n``Generate 5 jokes about coffee and their corresponding probabilities'').\nComprehensive experiments show that VS significantly improves performance\nacross creative writing (poems, stories, jokes), dialogue simulation,\nopen-ended QA, and synthetic data generation, without sacrificing factual\naccuracy and safety. For instance, in creative writing, VS increases diversity\nby 1.6-2.1x over direct prompting. We further observe an emergent trend that\nmore capable models benefit more from VS. In sum, our work provides a new\ndata-centric perspective on mode collapse and a practical inference-time remedy\nthat helps unlock pre-trained generative diversity.", "published": "2025-10-01 17:55:37", "link": "http://arxiv.org/abs/2510.01171v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Simultaneous Multi-objective Alignment Across Verifiable and Non-verifiable Rewards", "abstract": "Aligning large language models to human preferences is inherently\nmultidimensional, yet most pipelines collapse heterogeneous signals into a\nsingle optimizeable objective. We seek to answer what it would take to\nsimultaneously align a model across various domains spanning those with:\nverifiable rewards (mathematical accuracy), non-verifiable subjective\npreferences (human values), and complex interactive scenarios (multi-turn AI\ntutoring dialogues). Such multi-objective reinforcement learning setups are\noften plagued by the individual objectives being at odds with each other,\nresulting in inefficient training and little user control during inference. We\npropose a unified framework that: (i) standardizes {process reward model} (PRM)\ntraining across both verifiable and non-verifiable settings to better supervise\nmodels' chain-of-thought reasoning; (ii) performs {multi-objective alignment}\nby training the LLM with our $\\textbf{M}$ulti-$\\textbf{A}$ction-$\\textbf{H}$ead\n$\\textbf{DPO}$ (MAH-DPO) and a vectorized reward where the dimensions of the\nvector correspond to the various objectives instead of a single scalar; and\n(iii) demonstrates how such a system provides fine-grained inference-time user\ncontrol. Experiments across math reasoning, value alignment, and multi-turn\ndialogue show that our framework improves performance across multiple\nobjectives simultaneously, while minimizing cross-objective trade-offs and\nenabling flexible inference time user control. The code can be found at\nhttps://github.com/pearls-lab/multiobj-align.", "published": "2025-10-01 17:54:15", "link": "http://arxiv.org/abs/2510.01167v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "GRAD: Generative Retrieval-Aligned Demonstration Sampler for Efficient Few-Shot Reasoning", "abstract": "Large Language Models (LLMs) achieve strong performance across diverse tasks,\nbut their effectiveness often depends on the quality of the provided context.\nRetrieval-Augmented Generation (RAG) enriches prompts with external\ninformation, but its reliance on static databases constrains adaptability and\ncan result in irrelevant demonstrations. In this work, we propose a Generative\nRetrieval-Aligned Demonstrator (GRAD), a dynamic demonstration-based approach\nwhere an LLM model is trained to generate input-specific concise\ndemonstrations. By tailoring demonstrations to each input, our method offers\nbetter contextual support than traditional RAG approaches. We demonstrate the\nsuperiority of GRAD under budget constraints, where we limit both the number of\ntokens used per demonstration and the number of tokens used for the final\noutput. Trained solely on a math dataset, GRAD consistently outperforms strong\nbaselines on Qwen2.5-14B across mathematical reasoning and advanced STEM\nquestions, highlighting GRAD's robust generalization to out-of-distribution\n(OOD) domains such as physics, chemistry, and computer science. Furthermore, we\nshow that demonstrations generated by trained smaller models can effectively\nguide larger target models, reducing training costs while maintaining\ncompetitive accuracy. Overall, this work introduces a scalable demonstration\ngenerator model presenting the first step toward a dynamic few-shot learning\nparadigm in resource-constrained settings. We release the code used for the\nproject.", "published": "2025-10-01 17:52:41", "link": "http://arxiv.org/abs/2510.01165v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Social Welfare Function Leaderboard: When LLM Agents Allocate Social Welfare", "abstract": "Large language models (LLMs) are increasingly entrusted with high-stakes\ndecisions that affect human welfare. However, the principles and values that\nguide these models when distributing scarce societal resources remain largely\nunexamined. To address this, we introduce the Social Welfare Function (SWF)\nBenchmark, a dynamic simulation environment where an LLM acts as a sovereign\nallocator, distributing tasks to a heterogeneous community of recipients. The\nbenchmark is designed to create a persistent trade-off between maximizing\ncollective efficiency (measured by Return on Investment) and ensuring\ndistributive fairness (measured by the Gini coefficient). We evaluate 20\nstate-of-the-art LLMs and present the first leaderboard for social welfare\nallocation. Our findings reveal three key insights: (i) A model's general\nconversational ability, as measured by popular leaderboards, is a poor\npredictor of its allocation skill. (ii) Most LLMs exhibit a strong default\nutilitarian orientation, prioritizing group productivity at the expense of\nsevere inequality. (iii) Allocation strategies are highly vulnerable, easily\nperturbed by output-length constraints and social-influence framing. These\nresults highlight the risks of deploying current LLMs as societal\ndecision-makers and underscore the need for specialized benchmarks and targeted\nalignment for AI governance.", "published": "2025-10-01 17:52:31", "link": "http://arxiv.org/abs/2510.01164v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Backdoor Attacks Against Speech Language Models", "abstract": "Large Language Models (LLMs) and their multimodal extensions are becoming\nincreasingly popular. One common approach to enable multimodality is to cascade\ndomain-specific encoders with an LLM, making the resulting model inherit\nvulnerabilities from all of its components. In this work, we present the first\nsystematic study of audio backdoor attacks against speech language models. We\ndemonstrate its effectiveness across four speech encoders and three datasets,\ncovering four tasks: automatic speech recognition (ASR), speech emotion\nrecognition, and gender and age prediction. The attack consistently achieves\nhigh success rates, ranging from 90.76% to 99.41%. To better understand how\nbackdoors propagate, we conduct a component-wise analysis to identify the most\nvulnerable stages of the pipeline. Finally, we propose a fine-tuning-based\ndefense that mitigates the threat of poisoned pretrained encoders.", "published": "2025-10-01 17:45:04", "link": "http://arxiv.org/abs/2510.01157v1", "categories": ["cs.CL", "cs.CR", "cs.SD"], "primary_category": "cs.CL"}
{"title": "Pay-Per-Search Models are Abstention Models", "abstract": "LLMs cannot reliably recognize their parametric knowledge boundaries and\noften hallucinate answers to outside-of-boundary questions. In contrast, humans\nrecognize their limitations and can either seek external help for such\nquestions or abstain. In this paper, we introduce MASH (Modeling Abstention via\nSelective Help-seeking), a training framework that readily extracts abstentions\nfrom LLMs. Our key idea is that any external help-seeking by an LLM, i.e.\nsearch tool use, can serve as a proxy for abstention if the external help\n(search) is appropriately penalized while simultaneously rewarding answer\naccuracy. MASH operationalizes this idea using reinforcement learning with a\npay-per-search reward.\n  We run experiments on three knowledge-intensive QA datasets. Our results show\nthat MASH substantially improves upon the selective help-seeking performance of\nprior efficient search approaches; on multi-hop datasets, MASH improves answer\naccuracy by 7.6%. Furthermore, MASH demonstrates strong off-the-shelf\nabstention -- it can distinguish between unanswerable/answerable questions and\nselectively generate responses for answerable questions -- showcasing behavior\nanalogous to specialized abstention approaches. We emphasize that contrary to\nprior abstention methods, MASH does not require pre-determining knowledge\nboundaries to construct training data. Instead, MASH's abstentions are a\nby-product of training for the auxiliary selective help-seeking task. Overall,\nwe show that MASH training effectively aligns search tool use with parametric\nknowledge, which can be successfully leveraged for making abstention decisions.", "published": "2025-10-01 17:41:54", "link": "http://arxiv.org/abs/2510.01152v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "mR3: Multilingual Rubric-Agnostic Reward Reasoning Models", "abstract": "Evaluation using Large Language Model (LLM) judges has been widely adopted in\nEnglish and shown to be effective for automatic evaluation. However, their\nperformance does not generalize well to non-English settings, and it remains\nunclear what constitutes effective multilingual training for such judges. In\nthis paper, we introduce mR3, a massively multilingual, rubric-agnostic reward\nreasoning model trained on 72 languages, achieving the broadest language\ncoverage in reward modeling to date. We present a comprehensive study of data\nand curriculum selection for training to identify effective strategies and data\nsources for building high-quality reward models, including the integration of\ntarget-language reasoning datasets. Our approach attains state-of-the-art\nperformance on multilingual reward model benchmarks, surpassing much larger\nmodels (i.e., GPT-OSS-120B) while being up to 9x smaller, and its effectiveness\nis further confirmed through extensive ablation studies. Our models, data, and\ncode are available as open source at https://github.com/rubricreward/mr3.", "published": "2025-10-01 17:36:59", "link": "http://arxiv.org/abs/2510.01146v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automatic Speech Recognition (ASR) for African Low-Resource Languages: A Systematic Literature Review", "abstract": "ASR has achieved remarkable global progress, yet African low-resource\nlanguages remain rigorously underrepresented, producing barriers to digital\ninclusion across the continent with more than +2000 languages. This systematic\nliterature review (SLR) explores research on ASR for African languages with a\nfocus on datasets, models and training methods, evaluation techniques,\nchallenges, and recommends future directions. We employ the PRISMA 2020\nprocedures and search DBLP, ACM Digital Library, Google Scholar, Semantic\nScholar, and arXiv for studies published between January 2020 and July 2025. We\ninclude studies related to ASR datasets, models or metrics for African\nlanguages, while excluding non-African, duplicates, and low-quality studies\n(score <3/5). We screen 71 out of 2,062 records and we record a total of 74\ndatasets across 111 languages, encompassing approximately 11,206 hours of\nspeech. Fewer than 15% of research provided reproducible materials, and dataset\nlicensing is not clear. Self-supervised and transfer learning techniques are\npromising, but are hindered by limited pre-training data, inadequate coverage\nof dialects, and the availability of resources. Most of the researchers use\nWord Error Rate (WER), with very minimal use of linguistically informed scores\nsuch as Character Error Rate (CER) or Diacritic Error Rate (DER), and thus with\nlimited application in tonal and morphologically rich languages. The existing\nevidence on ASR systems is inconsistent, hindered by issues like dataset\navailability, poor annotations, licensing uncertainties, and limited\nbenchmarking. Nevertheless, the rise of community-driven initiatives and\nmethodological advancements indicates a pathway for improvement. Sustainable\ndevelopment for this area will also include stakeholder partnership, creation\nof ethically well-balanced datasets, use of lightweight modelling techniques,\nand active benchmarking.", "published": "2025-10-01 17:36:06", "link": "http://arxiv.org/abs/2510.01145v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompt Curriculum Learning for Efficient LLM Post-Training", "abstract": "We introduce Prompt Curriculum Learning (PCL), a lightweight reinforcement\nlearning (RL) algorithm that selects intermediate-difficulty prompts using a\nlearned value model to post-train language models. Since post-training LLMs via\nRL remains sensitive to batching and prompt selection strategies, we first\nconduct a series of systematic experiments where we (1) determine the optimal\ntraining batch size that balances generation efficiency and gradient quality\nand (2) establish the importance of focusing on prompts of intermediate\ndifficulty for the policy. We build upon these results to design PCL, which\nidentifies prompts of intermediate difficulty for the current policy in an\non-policy manner by using a value model that is concurrently updated based on\nthe current policy. By focusing on informative prompts that yield high\neffective ratios, PCL achieves either the highest performance or requires\nsignificantly less time to reach comparable performance to its counterparts.\nCompared to rollout-based filtering methods, PCL avoids costly rollouts and\nachieves $12.1\\times$ and $16.9\\times$ faster speed on identifying\nintermediate-difficulty prompts when training on MATH and DeepScaleR,\nrespectively. We further demonstrate that our value model accurately predicts\nprompt difficulty and allows PCL to focus on progressively more challenging\nprompts during RL. Our results present a new methodology that delivers improved\ntradeoff between upper-bound performance and efficiency for reasoning-focused\nRL.", "published": "2025-10-01 17:24:28", "link": "http://arxiv.org/abs/2510.01135v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning", "abstract": "We study what actually works and what doesn't for training large language\nmodels as agents via multi-turn reinforcement learning. Despite rapid progress,\nexisting frameworks and definitions are fragmented, and there is no systematic\nformulation or analysis of which design choices matter across tasks. We address\nthis gap by first breaking down the design space into three inter-related\npillars -- environment, reward, and policy -- and empirically derive a recipe\nfor training LLM agents in situated textual domains. In particular, we test\nTextWorld and ALFWorld, popular domains for testing situated embodied\nreasoning, as well as SWE-Gym for more software engineering style tasks. (i)\nFor the environment, we analyze the impacts of task complexity in terms of\nsizes of the state and action spaces as well as optimal solution length,\nfinding that even simple environments within a domain can provide signal on how\nwell an agent can generalize to more complex tasks. (ii) For the reward, we\nablate relative reward sparsity, observing that while dense turn-level rewards\naccelerate training, performance and stability is highly dependent on the\nchoice of RL algorithm. (iii) And for the agent's policy, we explore the\ninterplay between reward sparsity and biased (PPO, GRPO) and unbiased (RLOO)\npolicy gradient methods in addition to showing how to find the optimal\nSupervised Fine-tuning (SFT) to RL training ratio given a fixed budget. We\ndistill these findings into a training recipe that guides co-design across the\nthree pillars, facilitating research and practical efforts in multi-turn\nagentic RL. Code: https://github.com/pearls-lab/meow-tea-taro", "published": "2025-10-01 17:23:04", "link": "http://arxiv.org/abs/2510.01132v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Research on the Integration of Embodied Intelligence and Reinforcement Learning in Textual Domains", "abstract": "This article addresses embodied intelligence and reinforcement learning\nintegration in the field of text processing, aiming to enhance text handling\nwith more intelligence on the basis of embodied intelligence's perception and\naction superiority and reinforcement learning's decision optimization\ncapability. Through detailed theoretical explanation and experimental\nexploration, a novel integration model is introduced. This model has been\ndemonstrated to be very effective in a wide range oftext processing tasks,\nvalidating its applicative potential", "published": "2025-10-01 16:21:04", "link": "http://arxiv.org/abs/2510.01076v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WALT: Web Agents that Learn Tools", "abstract": "Web agents promise to automate complex browser tasks, but current methods\nremain brittle -- relying on step-by-step UI interactions and heavy LLM\nreasoning that break under dynamic layouts and long horizons. Humans, by\ncontrast, exploit website-provided functionality through high-level operations\nlike search, filter, and sort. We introduce WALT (Web Agents that Learn Tools),\na framework that reverse-engineers latent website functionality into reusable\ninvocable tools. Rather than hypothesizing ad-hoc skills, WALT exposes robust\nimplementations of automations already designed into websites -- spanning\ndiscovery (search, filter, sort), communication (post, comment, upvote), and\ncontent management (create, edit, delete). Tools abstract away low-level\nexecution: instead of reasoning about how to click and type, agents simply call\nsearch(query) or create(listing). This shifts the computational burden from\nfragile step-by-step reasoning to reliable tool invocation. On VisualWebArena\nand WebArena, WALT achieves higher success with fewer steps and less\nLLM-dependent reasoning, establishing a robust and generalizable paradigm for\nbrowser automation.", "published": "2025-10-01 23:41:47", "link": "http://arxiv.org/abs/2510.01524v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Aligning Video Models with Human Social Judgments via Behavior-Guided Fine-Tuning", "abstract": "Humans intuitively perceive complex social signals in visual scenes, yet it\nremains unclear whether state-of-the-art AI models encode the same similarity\nstructure. We study (Q1) whether modern video and language models capture\nhuman-perceived similarity in social videos, and (Q2) how to instill this\nstructure into models using human behavioral data. To address this, we\nintroduce a new benchmark of over 49,000 odd-one-out similarity judgments on\n250 three-second video clips of social interactions, and discover a modality\ngap: despite the task being visual, caption-based language embeddings align\nbetter with human similarity than any pretrained video model. We close this gap\nby fine-tuning a TimeSformer video model on these human judgments with our\nnovel hybrid triplet-RSA objective using low-rank adaptation (LoRA), aligning\npairwise distances to human similarity. This fine-tuning protocol yields\nsignificantly improved alignment with human perceptions on held-out videos in\nterms of both explained variance and odd-one-out triplet accuracy. Variance\npartitioning shows that the fine-tuned video model increases shared variance\nwith language embeddings and explains additional unique variance not captured\nby the language model. Finally, we test transfer via linear probes and find\nthat human-similarity fine-tuning strengthens the encoding of social-affective\nattributes (intimacy, valence, dominance, communication) relative to the\npretrained baseline. Overall, our findings highlight a gap in pretrained video\nmodels' social recognition and demonstrate that behavior-guided fine-tuning\nshapes video representations toward human social perception.", "published": "2025-10-01 22:29:55", "link": "http://arxiv.org/abs/2510.01502v1", "categories": ["q-bio.NC", "cs.CV", "cs.LG"], "primary_category": "q-bio.NC"}
{"title": "AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging", "abstract": "While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic\naneurysms (AAA), the required iodinated contrast agents pose significant risks,\nincluding nephrotoxicity, patient allergies, and environmental harm. To reduce\ncontrast agent use, recent deep learning methods have focused on generating\nsynthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a\nmulti-stage pipeline that first generates images and then performs\nsegmentation, which leads to error accumulation and fails to leverage shared\nsemantic and anatomical structures. To address this, we propose a unified deep\nlearning framework that generates synthetic CECT images from NCCT scans while\nsimultaneously segmenting the aortic lumen and thrombus. Our approach\nintegrates conditional diffusion models (CDM) with multi-task learning,\nenabling end-to-end joint optimization of image synthesis and anatomical\nsegmentation. Unlike previous multitask diffusion models, our approach requires\nno initial predictions (e.g., a coarse segmentation mask), shares both encoder\nand decoder parameters across tasks, and employs a semi-supervised training\nstrategy to learn from scans with missing segmentation labels, a common\nconstraint in real-world clinical data. We evaluated our method on a cohort of\n264 patients, where it consistently outperformed state-of-the-art single-task\nand multi-stage models. For image synthesis, our model achieved a PSNR of 25.61\ndB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation,\nit improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus\nDice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to\nmore accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm\nfrom 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to\nnnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.", "published": "2025-10-01 22:19:27", "link": "http://arxiv.org/abs/2510.01498v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Purrception: Variational Flow Matching for Vector-Quantized Image Generation", "abstract": "We introduce Purrception, a variational flow matching approach for\nvector-quantized image generation that provides explicit categorical\nsupervision while maintaining continuous transport dynamics. Our method adapts\nVariational Flow Matching to vector-quantized latents by learning categorical\nposteriors over codebook indices while computing velocity fields in the\ncontinuous embedding space. This combines the geometric awareness of continuous\nmethods with the discrete supervision of categorical approaches, enabling\nuncertainty quantification over plausible codes and temperature-controlled\ngeneration. We evaluate Purrception on ImageNet-1k 256x256 generation. Training\nconverges faster than both continuous flow matching and discrete flow matching\nbaselines while achieving competitive FID scores with state-of-the-art models.\nThis demonstrates that Variational Flow Matching can effectively bridge\ncontinuous transport and discrete supervision for improved training efficiency\nin image generation.", "published": "2025-10-01 21:41:30", "link": "http://arxiv.org/abs/2510.01478v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories", "abstract": "Data-efficient learning aims to eliminate redundancy in large training\ndatasets by training models on smaller subsets of the most informative\nexamples. While data selection has been extensively explored for vision models\nand large language models (LLMs), it remains underexplored for Large\nVision-Language Models (LVLMs). Notably, none of existing methods can\noutperform random selection at different subset sizes. In this work, we propose\nthe first principled method for data-efficient instruction tuning of LVLMs. We\nprove that examples with similar cross-modal attention matrices during\ninstruction tuning have similar gradients. Thus, they influence model\nparameters in a similar manner and convey the same information to the model\nduring training. Building on this insight, we propose XMAS, which clusters\nexamples based on the trajectories of the top singular values of their\nattention matrices obtained from fine-tuning a small proxy LVLM. By sampling a\nbalanced subset from these clusters, XMAS effectively removes redundancy in\nlarge-scale LVLM training data. Extensive experiments show that XMAS can\ndiscard 50% of the LLaVA-665k dataset and 85% of the Vision-Flan dataset while\nfully preserving performance of LLaVA-1.5-7B on 10 downstream benchmarks and\nspeeding up its training by 1.2x. This is 30% more data reduction compared to\nthe best baseline for LLaVA-665k. The project's website can be found at\nhttps://bigml-cs-ucla.github.io/XMAS-project-page/.", "published": "2025-10-01 20:47:29", "link": "http://arxiv.org/abs/2510.01454v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "GeoSURGE: Geo-localization using Semantic Fusion with Hierarchy of Geographic Embeddings", "abstract": "Worldwide visual geo-localization seeks to determine the geographic location\nof an image anywhere on Earth using only its visual content. Learned\nrepresentations of geography for visual geo-localization remain an active\nresearch topic despite much progress. We formulate geo-localization as aligning\nthe visual representation of the query image with a learned geographic\nrepresentation. Our novel geographic representation explicitly models the world\nas a hierarchy of geographic embeddings. Additionally, we introduce an approach\nto efficiently fuse the appearance features of the query image with its\nsemantic segmentation map, forming a robust visual representation. Our main\nexperiments demonstrate improved all-time bests in 22 out of 25 metrics\nmeasured across five benchmark datasets compared to prior state-of-the-art\n(SOTA) methods and recent Large Vision-Language Models (LVLMs). Additional\nablation studies support the claim that these gains are primarily driven by the\ncombination of geographic and visual representations.", "published": "2025-10-01 20:39:48", "link": "http://arxiv.org/abs/2510.01448v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "On the Role of Domain Experts in Creating Effective Tutoring Systems", "abstract": "The role that highly curated knowledge, provided by domain experts, could\nplay in creating effective tutoring systems is often overlooked within the AI\nfor education community. In this paper, we highlight this topic by discussing\ntwo ways such highly curated expert knowledge could help in creating novel\neducational systems. First, we will look at how one could use explainable AI\n(XAI) techniques to automatically create lessons. Most existing XAI methods are\nprimarily aimed at debugging AI systems. However, we will discuss how one could\nuse expert specified rules about solving specific problems along with novel XAI\ntechniques to automatically generate lessons that could be provided to\nlearners. Secondly, we will see how an expert specified curriculum for learning\na target concept can help develop adaptive tutoring systems, that can not only\nprovide a better learning experience, but could also allow us to use more\nefficient algorithms to create these systems. Finally, we will highlight the\nimportance of such methods using a case study of creating a tutoring system for\npollinator identification, where such knowledge could easily be elicited from\nexperts.", "published": "2025-10-01 20:12:57", "link": "http://arxiv.org/abs/2510.01432v1", "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Ultra-Efficient Decoding for End-to-End Neural Compression and Reconstruction", "abstract": "Image compression and reconstruction are crucial for various digital\napplications. While contemporary neural compression methods achieve impressive\ncompression rates, the adoption of such technology has been largely hindered by\nthe complexity and large computational costs of the convolution-based decoders\nduring data reconstruction. To address the decoder bottleneck in neural\ncompression, we develop a new compression-reconstruction framework based on\nincorporating low-rank representation in an autoencoder with vector\nquantization. We demonstrated that performing a series of computationally\nefficient low-rank operations on the learned latent representation of images\ncan efficiently reconstruct the data with high quality. Our approach\ndramatically reduces the computational overhead in the decoding phase of neural\ncompression/reconstruction, essentially eliminating the decoder compute\nbottleneck while maintaining high fidelity of image outputs.", "published": "2025-10-01 19:42:59", "link": "http://arxiv.org/abs/2510.01407v1", "categories": ["cs.LG", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Exploring one-dimensional, binary, radius-2 cellular automata, over cyclic configurations, in terms of their ability to solve decision problems by distributed consensus", "abstract": "Probing the ability of automata networks to solve decision problems has\nreceived a continuous attention in the literature, and specially with the\nautomata reaching the answer by distributed consensus, i.e., their all taking\non a same state, out of two. In the case of binary automata networks,\nregardless of the kind of update employed, the networks should display only two\npossible attractors, the fixed points $0^L$ and $1^L$, for all cyclic\nconfigurations of size $L$. A previous investigation into the space of\none-dimensional, binary, radius-2 cellular automata identified a restricted\nsubset of rules as potential solvers of decision problems, but the reported\nresults were incomplete and lacked sufficient detail for replication. To\naddress this gap, we conducted a comprehensive reevaluation of the entire\nradius-2 rule space, by filtering it with all configuration sizes from 5 to 20,\naccording to their basins of attraction being formed by only the two expected\nfixed points. A set of over fifty-four thousand potential decision problem\nsolvers were then obtained. Among these, more than forty-five thousand were\nassociated with 3 well-defined decision problems, and precise formal\nexplanations were provided for over forty thousand of them. The remaining\ncandidate rules suggest additional problem classes yet to be fully\ncharacterised. Overall, this work substantially extends the understanding of\nradius-2 cellular automata, offering a more complete picture of their capacity\nto solve decision problems by consensus.", "published": "2025-10-01 15:43:29", "link": "http://arxiv.org/abs/2510.01040v1", "categories": ["cs.DM"], "primary_category": "cs.DM"}
{"title": "Cube Height, Cube Width and Related Extremal Problems for Posets", "abstract": "Given a poset $P$, a family $\\mathcal{S}=\\{S_x:x\\in P\\}$ of sets indexed by\nthe elements of $P$ is called an inclusion representation of $P$ if $x\\leqslant\ny$ in $P$ if and only if $S_x\\subseteq S_y$. The cube height of a poset is the\nleast non-negative integer $h$ such that $P$ has an inclusion representation\nfor which every set has size at most $h$. In turn, the cube width of $P$ is the\nleast non-negative integer $w$ for which there is an inclusion representation\n$\\mathcal{S}$ of $P$ such that $|\\bigcup\\mathcal{S}|=w$ and every set in\n$\\mathcal{S}$ has size at most the cube height of $P$. In this paper, we show\nthat the cube width of a poset never exceeds the size of its ground set, and we\ncharacterize those posets for which this inequality is tight. Our research\nprompted us to investigate related extremal problems for posets and inclusion\nrepresentations. Accordingly, the results for cube width are obtained as\nextensions of more comprehensive results that we believe to be of independent\ninterest.", "published": "2025-10-01 14:08:10", "link": "http://arxiv.org/abs/2510.00928v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "Throttling for metric dimension and its variants", "abstract": "Metric dimension is a graph parameter that has been applied to robot\nnavigation and finding low-dimensional vector embeddings. Throttling entails\nminimizing the sum of two available resources when solving certain graph\nproblems. In this paper, we introduce throttling for metric dimension, edge\nmetric dimension, and mixed metric dimension. In the context of vector\nembeddings, metric dimension throttling finds a low-dimensional, low-magnitude\nembedding with integer coordinates. We show that computing the throttling\nnumber is NP-hard for all three variants. We give formulas for the throttling\nnumbers of special families of graphs, and characterize graphs with extremal\nthrottling numbers. We also prove that the minimum possible throttling number\nof a graph of order $n$ is $\\Theta\\left(\\frac{\\log{n}}{\\log{\\log{n}}}\\right)$,\nwhile the minimum possible throttling number of a tree of order $n$ is\n$\\Theta(n^{1/3})$ or $\\Theta(n^{1/2})$ depending on the variant of metric\ndimension.", "published": "2025-10-01 05:27:27", "link": "http://arxiv.org/abs/2510.00530v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "MetaSynth: Multi-Agent Metadata Generation from Implicit Feedback in Black-Box Systems", "abstract": "Meta titles and descriptions strongly shape engagement in search and\nrecommendation platforms, yet optimizing them remains challenging. Search\nengine ranking models are black box environments, explicit labels are\nunavailable, and feedback such as click-through rate (CTR) arrives only\npost-deployment. Existing template, LLM, and retrieval-augmented approaches\neither lack diversity, hallucinate attributes, or ignore whether candidate\nphrasing has historically succeeded in ranking. This leaves a gap in directly\nleveraging implicit signals from observable outcomes. We introduce MetaSynth, a\nmulti-agent retrieval-augmented generation framework that learns from implicit\nsearch feedback. MetaSynth builds an exemplar library from top-ranked results,\ngenerates candidate snippets conditioned on both product content and exemplars,\nand iteratively refines outputs via evaluator-generator loops that enforce\nrelevance, promotional strength, and compliance. On both proprietary e-commerce\ndata and the Amazon Reviews corpus, MetaSynth outperforms strong baselines\nacross NDCG, MRR, and rank metrics. Large-scale A/B tests further demonstrate\n10.26% CTR and 7.51% clicks. Beyond metadata, this work contributes a general\nparadigm for optimizing content in black-box systems using implicit signals.", "published": "2025-10-01 23:41:39", "link": "http://arxiv.org/abs/2510.01523v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "ModernVBERT: Towards Smaller Visual Document Retrievers", "abstract": "Multimodal embedding models are gaining prevalence, notably for document\nretrieval as efficient alternatives to text-only pipelines. These models are\ntypically built by finetuning large vision-language decoders (VLMs) with\ncontrastive losses on text-image pairs. In this work, we show that, while\ncost-efficient, this repurposing approach often bottlenecks retrieval\nperformance. Through controlled experiments, we establish a principled recipe\nfor improving visual document retrieval models. We notably measure the impact\nof attention masking, image resolution, modality alignment data regimes, and\nlate interaction centered contrastive objectives which emerge as central\nperformance factors. Building on these insights, we release ModernVBERT, a\ncompact 250M-parameter vision-language encoder that outperforms models up to 10\ntimes larger when finetuned on document retrieval tasks. Models and code are\nmade available at https://huggingface.co/ModernVBERT.", "published": "2025-10-01 17:41:17", "link": "http://arxiv.org/abs/2510.01149v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Deep Learning-Based Approach for Improving Relational Aggregated Search", "abstract": "Due to an information explosion on the internet, there is a need for the\ndevelopment of aggregated search systems that can boost the retrieval and\nmanagement of content in various formats. To further improve the clustering of\nArabic text data in aggregated search environments, this research investigates\nthe application of advanced natural language processing techniques, namely\nstacked autoencoders and AraBERT embeddings. By transcending the limitations of\ntraditional search engines, which are imprecise, not contextually relevant, and\nnot personalized, we offer more enriched, context-aware characterizations of\nsearch results, so we used a K-means clustering algorithm to discover\ndistinctive features and relationships in these results, we then used our\napproach on different Arabic queries to evaluate its effectiveness. Our model\nillustrates that using stacked autoencoders in representation learning suits\nclustering tasks and can significantly improve clustering search results. It\nalso demonstrates improved accuracy and relevance of search results.", "published": "2025-10-01 14:37:38", "link": "http://arxiv.org/abs/2510.00966v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "Bridging Language Gaps: Advances in Cross-Lingual Information Retrieval with Multilingual LLMs", "abstract": "Cross-lingual information retrieval (CLIR) addresses the challenge of\nretrieving relevant documents written in languages different from that of the\noriginal query. Research in this area has typically framed the task as\nmonolingual retrieval augmented by translation, treating retrieval methods and\ncross-lingual capabilities in isolation. Both monolingual and cross-lingual\nretrieval usually follow a pipeline of query expansion, ranking, re-ranking\nand, increasingly, question answering. Recent advances, however, have shifted\nfrom translation-based methods toward embedding-based approaches and leverage\nmultilingual large language models (LLMs), for which aligning representations\nacross languages remains a central challenge. The emergence of cross-lingual\nembeddings and multilingual LLMs has introduced a new paradigm, offering\nimproved retrieval performance and enabling answer generation. This survey\nprovides a comprehensive overview of developments from early translation-based\nmethods to state-of-the-art embedding-driven and generative techniques. It\npresents a structured account of core CLIR components, evaluation practices,\nand available resources. Persistent challenges such as data imbalance and\nlinguistic variation are identified, while promising directions are suggested\nfor advancing equitable and effective cross-lingual information retrieval. By\nsituating CLIR within the broader landscape of information retrieval and\nmultilingual language processing, this work not only reviews current\ncapabilities but also outlines future directions for building retrieval systems\nthat are robust, inclusive, and adaptable.", "published": "2025-10-01 13:50:05", "link": "http://arxiv.org/abs/2510.00908v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "On Listwise Reranking for Corpus Feedback", "abstract": "Reranker improves retrieval performance by capturing document interactions.\nAt one extreme, graph-aware adaptive retrieval (GAR) represents an\ninformation-rich regime, requiring a pre-computed document similarity graph in\nreranking. However, as such graphs are often unavailable, or incur quadratic\nmemory costs even when available, graph-free rerankers leverage large language\nmodel (LLM) calls to achieve competitive performance. We introduce L2G, a novel\nframework that implicitly induces document graphs from listwise reranker logs.\nBy converting reranker signals into a graph structure, L2G enables scalable\ngraph-based retrieval without the overhead of explicit graph computation.\nResults on the TREC-DL and BEIR subset show that L2G matches the effectiveness\nof oracle-based graph methods, while incurring zero additional LLM calls.", "published": "2025-10-01 13:34:02", "link": "http://arxiv.org/abs/2510.00887v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Erase to Improve: Erasable Reinforcement Learning for Search-Augmented LLMs", "abstract": "While search-augmented large language models (LLMs) exhibit impressive\ncapabilities, their reliability in complex multi-hop reasoning remains limited.\nThis limitation arises from three fundamental challenges: decomposition errors,\nwhere tasks are incorrectly broken down; retrieval missing, where key evidence\nfails to be retrieved; and reasoning errors, where flawed logic propagates\nthrough the reasoning chain. A single failure in any of these stages can derail\nthe final answer. We propose Erasable Reinforcement Learning (ERL), a novel\nframework that transforms fragile reasoning into a robust process. ERL\nexplicitly identifies faulty steps, erases them, and regenerates reasoning in\nplace, preventing defective logic from propagating through the reasoning chain.\nThis targeted correction mechanism turns brittle reasoning into a more\nresilient process. Models trained with ERL, termed ESearch, achieve substantial\nimprovements on HotpotQA, MuSiQue, 2Wiki, and Bamboogle, with the 3B model\nachieving +8.48% EM and +11.56% F1, and the 7B model achieving +5.38% EM and\n+7.22% F1 over previous state-of-the-art(SOTA) results. These findings suggest\nthat erasable reinforcement learning provides a powerful paradigm shift for\nrobust multi-step reasoning in LLMs.", "published": "2025-10-01 13:10:36", "link": "http://arxiv.org/abs/2510.00861v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "AttentionDep: Domain-Aware Attention for Explainable Depression Severity Assessment", "abstract": "In today's interconnected society, social media platforms provide a window\ninto individuals' thoughts, emotions, and mental states. This paper explores\nthe use of platforms like Facebook, X (formerly Twitter), and Reddit for\ndepression severity detection. We propose AttentionDep, a domain-aware\nattention model that drives explainable depression severity estimation by\nfusing contextual and domain knowledge. Posts are encoded hierarchically using\nunigrams and bigrams, with attention mechanisms highlighting clinically\nrelevant tokens. Domain knowledge from a curated mental health knowledge graph\nis incorporated through a cross-attention mechanism, enriching the contextual\nfeatures. Finally, depression severity is predicted using an ordinal regression\nframework that respects the clinical-relevance and natural ordering of severity\nlevels. Our experiments demonstrate that AttentionDep outperforms\nstate-of-the-art baselines by over 5% in graded F1 score across datasets, while\nproviding interpretable insights into its predictions. This work advances the\ndevelopment of trustworthy and transparent AI systems for mental health\nassessment from social media.", "published": "2025-10-01 09:20:53", "link": "http://arxiv.org/abs/2510.00706v1", "categories": ["cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.AI"}
{"title": "ALARB: An Arabic Legal Argument Reasoning Benchmark", "abstract": "We introduce ALARB, a dataset and suite of tasks designed to evaluate the\nreasoning capabilities of large language models (LLMs) within the Arabic legal\ndomain. While existing Arabic benchmarks cover some knowledge-intensive tasks\nsuch as retrieval and understanding, substantial datasets focusing specifically\non multistep reasoning for Arabic LLMs, especially in open-ended contexts, are\nlacking. The dataset comprises over 13K commercial court cases from Saudi\nArabia, with each case including the facts presented, the reasoning of the\ncourt, the verdict, as well as the cited clauses extracted from the regulatory\ndocuments. We define a set of challenging tasks leveraging this dataset and\nreflecting the complexity of real-world legal reasoning, including verdict\nprediction, completion of reasoning chains in multistep legal arguments, and\nidentification of relevant regulations based on case facts. We benchmark a\nrepresentative selection of current open and closed Arabic LLMs on these tasks\nand demonstrate the dataset's utility for instruction tuning. Notably, we show\nthat instruction-tuning a modest 12B parameter model using ALARB significantly\nenhances its performance in verdict prediction and Arabic verdict generation,\nreaching a level comparable to that of GPT-4o.", "published": "2025-10-01 09:15:41", "link": "http://arxiv.org/abs/2510.00694v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Milco: Learned Sparse Retrieval Across Languages via a Multilingual Connector", "abstract": "Learned Sparse Retrieval (LSR) combines the efficiency of bi-encoders with\nthe transparency of lexical matching, but existing approaches struggle to scale\nbeyond English. We introduce MILCO, an LSR architecture that maps queries and\ndocuments from different languages into a shared English lexical space via a\nmultilingual connector. MILCO is trained with a specialized two-stage regime\nthat combines Sparse Alignment Pretraining with contrastive training to provide\nrepresentation transparency and effectiveness while mitigating semantic\ncollapse. Motivated by the observation that uncommon entities are often lost\nwhen projected into English, we propose a new LexEcho head, which enhances\nrobustness by augmenting the English lexical representation with a\nsource-language view obtained through a special [ECHO] token. MILCO achieves\nstate-of-the-art multilingual and cross-lingual LSR performance, outperforming\nleading dense, sparse, and multi-vector baselines such as BGE-M3 and\nQwen3-Embed on standard multilingual benchmarks, while supporting dynamic\nefficiency through post-hoc pruning. Notably, when using mass-based pruning to\nreduce document representations to only 30 active dimensions on average, MILCO\n560M outperforms the similarly-sized Qwen3-Embed 0.6B with 1024 dimensions.", "published": "2025-10-01 08:58:25", "link": "http://arxiv.org/abs/2510.00671v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Isogeny Graphs in Superposition and Quantum Onion Routing", "abstract": "Onion routing provides anonymity by layering encryption so that no relay can\nlink sender to destination. A quantum analogue faces a core obstacle: layered\nquantum encryption generally requires symmetric encryption schemes, whereas\nclassically one would rely on public-key encryption. We propose a\nsymmetric-encryption-based quantum onion routing (QOR) scheme by instantiating\neach layer with the abelian ideal class group action from the Theory of Complex\nMultiplication. Session keys are established locally via a Diffie-Hellman key\nexchange between neighbors in the chain of communication. Furthermore, we\npropose a novel ''non-local'' key exchange between the sender and receiver. The\nunderlying problem remains hard even for quantum adversaries and underpins the\nsecurity of current post-quantum schemes. We connect our construction to\nisogeny graphs and their association schemes, using the Bose-Mesner algebra to\nformalize commutativity and guide implementation. We give two implementation\npaths: (i) a universal quantum oracle evaluating the class group action with\npolynomially many quantum resources, and (ii) an intrinsically quantum approach\nvia continuous-time quantum walks (CTQWs), outlined here and developed in a\ncompanion paper. A small Qiskit example illustrates the mechanics (by design,\nnot the efficiency) of the QOR.", "published": "2025-10-01 21:15:22", "link": "http://arxiv.org/abs/2510.01464v1", "categories": ["quant-ph", "cs.IT", "math.IT", "11G15, 05E30, 81P45, 81P94, 68Q12, 94A60, 11R29, 14K02", "C.2.2; E.3; F.1.2"], "primary_category": "quant-ph"}
{"title": "Layered Normalized Min-Sum Decoding with Bit Flipping for FDPC Codes", "abstract": "Fair-density parity-check (FDPC) codes have been recently introduced\ndemonstrating improved performance compared to low-density parity-check (LDPC)\ncodes standardized in 5G systems particularly in high-rate regimes. In this\npaper, we introduce a layered normalized min-sum (LNMS) message-passing\ndecoding algorithm for the FDPC codes. We also introduce a syndrome-guided bit\nflipping (SGBF) method to enhance the error-correction performance of our\nproposed decoder. The LNMS decoder leverages conflict graph coloring for\nefficient layered scheduling, enabling faster convergence by grouping\nnon-conflicting check nodes and updating variable nodes immediately after each\nlayer. In the event of decoding failure, the SGBF method is activated,\nutilizing a novel reliability metric that combines log-likelihood ratio (LLR)\nmagnitudes and syndrome-derived error counts to identify the least reliable\nbits. A set of candidate sequences is then generated by performing single-bit\nflips at these positions, with each candidate re-decoded via LNMS. The optimal\ncandidate is selected based on the minimum syndrome weight. Extensive\nsimulation results demonstrate the superiority of the proposed decoder.\nNumerical simulations on FDPC$(256,192)$ code with a bit-flipping set size of\n$T = 128$ and a maximum of $5$ iterations demonstrate that the proposed decoder\nachieves approximately a $0.5\\,\\mathrm{dB}$ coding gain over standalone LNMS\ndecoding at a frame error rate (FER) of $10^{-3}$, while providing coding gains\nof $0.75-1.5\\,\\mathrm{dB}$ over other state-of-the-art codes including polar\ncodes and 5G-LDPC codes at the same length and rate and also under belief\npropagation decoding.", "published": "2025-10-01 15:26:28", "link": "http://arxiv.org/abs/2510.01019v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Optimizing Version AoI in Energy-Harvesting IoT: Model-Based and Learning-Based Approaches", "abstract": "Efficient data transmission in resource-constrained Internet of Things (IoT)\nsystems requires semantics-aware management that maximizes the delivery of\ntimely and informative data. This paper investigates the optimization of the\nsemantic metric Version Age of Information (VAoI) in a status update system\ncomprising an energy-harvesting (EH) sensor and a destination monitoring node.\nWe consider three levels of knowledge about the system model -- fully known,\npartially known, and unknown -- and propose corresponding optimization\nstrategies: model-based, estimation-based, and model-free methods. By employing\nMarkov Decision Process (MDP) and Reinforcement Learning (RL) frameworks, we\nanalyze performance trade-offs under varying degrees of model information. Our\nfindings provide guidance for designing efficient and adaptive semantics-aware\npolicies in both known and unknown IoT environments.", "published": "2025-10-01 13:45:02", "link": "http://arxiv.org/abs/2510.00904v1", "categories": ["cs.NI", "cs.IT", "math.IT"], "primary_category": "cs.NI"}
{"title": "On Estimating the Quantum Tsallis Relative Entropy", "abstract": "The relative entropy between quantum states quantifies their\ndistinguishability. The estimation of certain relative entropies has been\ninvestigated in the literature, e.g., the von Neumann relative entropy and\nsandwiched R\\'enyi relative entropy. In this paper, we present a comprehensive\nstudy of the estimation of the quantum Tsallis relative entropy. We show that\nfor any constant $\\alpha \\in (0, 1)$, the $\\alpha$-Tsallis relative entropy\nbetween two quantum states of rank $r$ can be estimated with sample complexity\n$\\operatorname{poly}(r)$, which can be made more efficient if we know their\nstate-preparation circuits. As an application, we obtain an approach to\ntolerant quantum state certification with respect to the quantum Hellinger\ndistance with sample complexity $\\widetilde{O}(r^{3.5})$, which exponentially\noutperforms the folklore approach based on quantum state tomography when $r$ is\npolynomial in the number of qubits. In addition, we show that the quantum state\ndistinguishability problems with respect to the quantum $\\alpha$-Tsallis\nrelative entropy and quantum Hellinger distance are $\\mathsf{QSZK}$-complete in\na certain regime, and they are $\\mathsf{BQP}$-complete in the low-rank case.", "published": "2025-10-01 10:38:59", "link": "http://arxiv.org/abs/2510.00752v1", "categories": ["quant-ph", "cs.CC", "cs.IT", "math.IT"], "primary_category": "quant-ph"}
{"title": "OTFS for Joint Radar and Communication: Algorithms, Prototypes, and Experiments", "abstract": "We propose an Joint Radar and Communication (JRC) system that utilizes the\nOrthogonal Time Frequency Space (OTFS) signals. The system features a fast\nradar sensing algorithm for detecting target range and speed by using the OTFS\ncommunication signals, and a self-interference cancellation for enhanced\nmulti-target separation. In addition to target detection, we propose methods\nfor monitoring human vital signs, such as breathing rate and heartbeat.\nFurthermore, we explore two approaches for distinguishing between human and\nnonhuman targets: one based on signal processing and the other based on machine\nlearning. We have developed a prototype JRC system using the software-defined\nradio (SDR) technology. Experimental results are shown to demonstrate the\neffectiveness of the prototype in detecting range, speed, and vital signs in\nboth human and mobile robot scenarios, as well as in distinguishing between\nhuman and non-human targets.", "published": "2025-10-01 08:53:51", "link": "http://arxiv.org/abs/2510.00668v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "On the Achievable Performance in the presence of Multiple Path Interference for Intra Data Center applications", "abstract": "An accurate analytical form of the achievable bit error rate in the presence\nof multipath interference (MPI) is proposed for PAM4 for the first time, taking\ninto account an ideal MPI estimate and compensation.", "published": "2025-10-01 08:14:49", "link": "http://arxiv.org/abs/2510.00638v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Stable Phase Retrieval: Optimal Rates in Poisson and Heavy-tailed Models", "abstract": "We investigate stable recovery guarantees for phase retrieval under two\nrealistic and challenging noise models: the Poisson model and the heavy-tailed\nmodel. Our analysis covers both nonconvex least squares (NCVX-LS) and convex\nleast squares (CVX-LS) estimators. For the Poisson model, we demonstrate that\nin the high-energy regime where the true signal $pmb{x}$ exceeds a certain\nenergy threshold, both estimators achieve a signal-independent, minimax optimal\nerror rate $\\mathcal{O}(\\sqrt{\\frac{n}{m}})$, with $n$ denoting the signal\ndimension and $m$ the number of sampling vectors. In contrast, in the\nlow-energy regime, the NCVX-LS estimator attains an error rate of\n$\\mathcal{O}(\\|\\pmb{x}\\|^{1/4}_2\\cdot(\\frac{n}{m})^{1/4})$, which decreases as\nthe energy of signal $\\pmb{x}$ diminishes and remains nearly optimal with\nrespect to the oversampling ratio. This demonstrates a signal-energy-adaptive\nbehavior in the Poisson setting. For the heavy-tailed model with noise having a\nfinite $q$-th moment ($q>2$), both estimators attain the minimax optimal error\nrate $\\mathcal{O}( \\frac{\\| \\xi \\|_{L_q}}{\\| \\pmb{x} \\|_2} \\cdot\n\\sqrt{\\frac{n}{m}} )$ in the high-energy regime, while the NCVX-LS estimator\nfurther achieves the minimax optimal rate $\\mathcal{O}( \\sqrt{\\|\\xi\n\\|_{L_q}}\\cdot (\\frac{n}{m})^{1/4} )$ in the low-energy regime. Our analysis\nbuilds on two key ideas: the use of multiplier inequalities to handle noise\nthat may exhibit dependence on the sampling vectors, and a novel interpretation\nof Poisson noise as sub-exponential in the high-energy regime yet heavy-tailed\nin the low-energy regime. These insights form the foundation of a unified\nanalytical framework, which we further apply to a range of related problems,\nincluding sparse phase retrieval, low-rank PSD matrix recovery, and random\nblind deconvolution.", "published": "2025-10-01 06:11:53", "link": "http://arxiv.org/abs/2510.00551v1", "categories": ["math.ST", "cs.IT", "math.IT", "stat.TH"], "primary_category": "math.ST"}
{"title": "Spectral Scaling Laws in Language Models: How Effectively Do Feed-Forward Networks Use Their Latent Space?", "abstract": "As large language models (LLMs) scale, the question is not only how large\nthey become, but how much of their capacity is effectively utilized. Existing\nscaling laws relate model size to loss, yet overlook how components exploit\ntheir latent space. We study feed-forward networks (FFNs) and recast width\nselection as a spectral utilization problem. Using a lightweight diagnostic\nsuite -- Hard Rank (participation ratio), Soft Rank (Shannon rank), Spectral\nConcentration, and the composite Spectral Utilization Index (SUI) -- we\nquantify how many latent directions are meaningfully activated across LLaMA,\nGPT-2, and nGPT families. Our key finding is an asymmetric spectral scaling\nlaw: soft rank follows an almost perfect power law with FFN width, while hard\nrank grows only sublinearly and with high variance. This asymmetry suggests\nthat widening FFNs mostly adds low-energy tail directions, while dominant-mode\nsubspaces saturate early. Moreover, at larger widths, variance further\ncollapses into a narrow subspace, leaving much of the latent space\nunder-utilized. These results recast FFN width selection as a principled\ntrade-off between tail capacity and dominant-mode capacity, offering concrete\nguidance for inference-efficient LLM design.", "published": "2025-10-01 05:38:21", "link": "http://arxiv.org/abs/2510.00537v1", "categories": ["cs.LG", "cs.IT", "math.IT"], "primary_category": "cs.LG"}
{"title": "A universal compression theory: Lottery ticket hypothesis and superpolynomial scaling laws", "abstract": "When training large-scale models, the performance typically scales with the\nnumber of parameters and the dataset size according to a slow power law. A\nfundamental theoretical and practical question is whether comparable\nperformance can be achieved with significantly smaller models and substantially\nless data. In this work, we provide a positive and constructive answer. We\nprove that a generic permutation-invariant function of $d$ objects can be\nasymptotically compressed into a function of $\\operatorname{polylog} d$ objects\nwith vanishing error. This theorem yields two key implications: (Ia) a large\nneural network can be compressed to polylogarithmic width while preserving its\nlearning dynamics; (Ib) a large dataset can be compressed to polylogarithmic\nsize while leaving the loss landscape of the corresponding model unchanged.\n(Ia) directly establishes a proof of the \\textit{dynamical} lottery ticket\nhypothesis, which states that any ordinary network can be strongly compressed\nsuch that the learning dynamics and result remain unchanged. (Ib) shows that a\nneural scaling law of the form $L\\sim d^{-\\alpha}$ can be boosted to an\narbitrarily fast power law decay, and ultimately to $\\exp(-\\alpha'\n\\sqrt[m]{d})$.", "published": "2025-10-01 04:35:23", "link": "http://arxiv.org/abs/2510.00504v1", "categories": ["stat.ML", "cond-mat.dis-nn", "cs.IT", "cs.LG", "math.IT"], "primary_category": "stat.ML"}
{"title": "DeMuon: A Decentralized Muon for Matrix Optimization over Graphs", "abstract": "In this paper, we propose DeMuon, a method for decentralized matrix\noptimization over a given communication topology. DeMuon incorporates matrix\northogonalization via Newton-Schulz iterations-a technique inherited from its\ncentralized predecessor, Muon-and employs gradient tracking to mitigate\nheterogeneity among local functions. Under heavy-tailed noise conditions and\nadditional mild assumptions, we establish the iteration complexity of DeMuon\nfor reaching an approximate stochastic stationary point. This complexity result\nmatches the best-known complexity bounds of centralized algorithms in terms of\ndependence on the target tolerance. To the best of our knowledge, DeMuon is the\nfirst direct extension of Muon to decentralized optimization over graphs with\nprovable complexity guarantees. We conduct preliminary numerical experiments on\ndecentralized transformer pretraining over graphs with varying degrees of\nconnectivity. Our numerical results demonstrate a clear margin of improvement\nof DeMuon over other popular decentralized algorithms across different network\ntopologies.", "published": "2025-10-01 19:06:11", "link": "http://arxiv.org/abs/2510.01377v1", "categories": ["math.OC", "cs.AI", "cs.LG", "cs.MA", "cs.SY", "eess.SY"], "primary_category": "math.OC"}
{"title": "Partial Resilient Leader-Follower Consensus in Time-Varying Graphs", "abstract": "This work studies resilient leader-follower consensus with a bounded number\nof adversaries. Existing approaches typically require robustness conditions of\nthe entire network to guarantee resilient consensus. However, the behavior of\nsuch systems when these conditions are not fully met remains unexplored. To\naddress this gap, we introduce the notion of partial leader-follower consensus,\nin which a subset of non-adversarial followers successfully tracks the leader's\nreference state despite insufficient robustness. We propose a novel distributed\nalgorithm - the Bootstrap Percolation and Mean Subsequence Reduced (BP-MSR)\nalgorithm - and establish sufficient conditions for individual followers to\nachieve consensus via the BP-MSR algorithm in arbitrary time-varying graphs. We\nvalidate our findings through simulations, demonstrating that our method\nguarantees partial leader-follower consensus, even when standard resilient\nconsensus algorithms fail.", "published": "2025-10-01 17:35:02", "link": "http://arxiv.org/abs/2510.01144v1", "categories": ["cs.MA", "cs.SY", "eess.SY"], "primary_category": "cs.MA"}
{"title": "Exploring Network-Knowledge Graph Duality: A Case Study in Agentic Supply Chain Risk Analysis", "abstract": "Large Language Models (LLMs) struggle with the complex, multi-modal, and\nnetwork-native data underlying financial risk. Standard Retrieval-Augmented\nGeneration (RAG) oversimplifies relationships, while specialist models are\ncostly and static. We address this gap with an LLM-centric agent framework for\nsupply chain risk analysis. Our core contribution is to exploit the inherent\nduality between networks and knowledge graphs (KG). We treat the supply chain\nnetwork as a KG, allowing us to use structural network science principles for\nretrieval. A graph traverser, guided by network centrality scores, efficiently\nextracts the most economically salient risk paths. An agentic architecture\norchestrates this graph retrieval alongside data from numerical factor tables\nand news streams. Crucially, it employs novel ``context shells'' -- descriptive\ntemplates that embed raw figures in natural language -- to make quantitative\ndata fully intelligible to the LLM. This lightweight approach enables the model\nto generate concise, explainable, and context-rich risk narratives in real-time\nwithout costly fine-tuning or a dedicated graph database.", "published": "2025-10-01 17:02:14", "link": "http://arxiv.org/abs/2510.01115v1", "categories": ["cs.AI", "cs.MA", "econ.TH", "physics.soc-ph"], "primary_category": "cs.AI"}
{"title": "SimCity: Multi-Agent Urban Development Simulation with Rich Interactions", "abstract": "Large Language Models (LLMs) open new possibilities for constructing\nrealistic and interpretable macroeconomic simulations. We present SimCity, a\nmulti-agent framework that leverages LLMs to model an interpretable\nmacroeconomic system with heterogeneous agents and rich interactions. Unlike\nclassical equilibrium models that limit heterogeneity for tractability, or\ntraditional agent-based models (ABMs) that rely on hand-crafted decision rules,\nSimCity enables flexible, adaptive behavior with transparent natural-language\nreasoning. Within SimCity, four core agent types (households, firms, a central\nbank, and a government) deliberate and participate in a frictional labor\nmarket, a heterogeneous goods market, and a financial market. Furthermore, a\nVision-Language Model (VLM) determines the geographic placement of new firms\nand renders a mapped virtual city, allowing us to study both macroeconomic\nregularities and urban expansion dynamics within a unified environment. To\nevaluate the framework, we compile a checklist of canonical macroeconomic\nphenomena, including price elasticity of demand, Engel's Law, Okun's Law, the\nPhillips Curve, and the Beveridge Curve, and show that SimCity naturally\nreproduces these empirical patterns while remaining robust across simulation\nruns.", "published": "2025-10-01 10:27:01", "link": "http://arxiv.org/abs/2510.01297v1", "categories": ["cs.MA"], "primary_category": "cs.MA"}
{"title": "Stochastic Self-Organization in Multi-Agent Systems", "abstract": "Multi-agent systems (MAS) based on Large Language Models (LLMs) have the\npotential to solve tasks that are beyond the reach of any single LLM. However,\nthis potential can only be realized when the collaboration mechanism between\nagents is optimized. Specifically, optimizing the communication structure\nbetween agents is critical for fruitful collaboration. Most existing approaches\nrely on fixed topologies, pretrained graph generators, optimization over edges,\nor employ external LLM judges, thereby adding to the complexity. In this work,\nwe introduce a response-conditioned framework that adapts communication\non-the-fly. Agents independently generate responses to the user query and\nassess peer contributions using an approximation of the Shapley value. A\ndirected acyclic graph (DAG) is then constructed to regulate the propagation of\nthe responses among agents, which ensures stable and efficient message\ntransmission from high-contributing agents to others. This graph is dynamically\nupdated based on the agent responses from the previous collaboration round.\nSince the proposed framework enables the self-organization of agents without\nadditional supervision or training, we refer to it as SelfOrg. The SelfOrg\nframework goes beyond task- and query-level optimization and takes into account\nthe stochastic nature of agent responses. Experiments with both strong and weak\nLLM backends demonstrate robust performance, with significant gains in the weak\nregime where prior methods collapse. We also theoretically show that multiple\nagents increase the chance of correctness and that the correct responses\nnaturally dominate the information flow.", "published": "2025-10-01 09:08:04", "link": "http://arxiv.org/abs/2510.00685v1", "categories": ["cs.MA", "cs.CL", "cs.LG"], "primary_category": "cs.MA"}
{"title": "Shared Object Manipulation with a Team of Collaborative Quadrupeds", "abstract": "Utilizing teams of multiple robots is advantageous for handling bulky\nobjects. Many related works focus on multi-manipulator systems, which are\nlimited by workspace constraints. In this paper, we extend a classical hybrid\nmotion-force controller to a team of legged manipulator systems, enabling\ncollaborative loco-manipulation of rigid objects with a force-closed grasp. Our\nnovel approach allows the robots to flexibly coordinate their movements,\nachieving efficient and stable object co-manipulation and transport, validated\nthrough extensive simulations and real-world experiments.", "published": "2025-10-01 09:04:38", "link": "http://arxiv.org/abs/2510.00682v1", "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY"], "primary_category": "cs.RO"}
{"title": "The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation", "abstract": "As Large Language Models (LLMs) transition from static tools to autonomous\nagents, traditional evaluation benchmarks that measure performance on\ndownstream tasks are becoming insufficient. These methods fail to capture the\nemergent social and cognitive dynamics that arise when agents communicate,\npersuade, and collaborate in interactive environments. To address this gap, we\nintroduce a novel evaluation framework that uses multi-agent debate as a\ncontrolled \"social laboratory\" to discover and quantify these behaviors. In our\nframework, LLM-based agents, instantiated with distinct personas and\nincentives, deliberate on a wide range of challenging topics under the\nsupervision of an LLM moderator. Our analysis, enabled by a new suite of\npsychometric and semantic metrics, reveals several key findings. Across\nhundreds of debates, we uncover a powerful and robust emergent tendency for\nagents to seek consensus, consistently reaching high semantic agreement ({\\mu}\n> 0.88) even without explicit instruction and across sensitive topics. We show\nthat assigned personas induce stable, measurable psychometric profiles,\nparticularly in cognitive effort, and that the moderators persona can\nsignificantly alter debate outcomes by structuring the environment, a key\nfinding for external AI alignment. This work provides a blueprint for a new\nclass of dynamic, psychometrically grounded evaluation protocols designed for\nthe agentic setting, offering a crucial methodology for understanding and\nshaping the social behaviors of the next generation of AI agents. We have\nreleased the code and results at\nhttps://github.com/znreza/multi-agent-LLM-eval-for-debate.", "published": "2025-10-01 07:10:28", "link": "http://arxiv.org/abs/2510.01295v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Capital Games and Growth Equilibria", "abstract": "We examine formal games that we call \"capital games\" in which player payoffs\nare known, but their payoffs are not guaranteed to be von Neumann-Morgenstern\nutilities. In capital games, the dynamics of player payoffs determine their\nutility functions. Different players can have different payoff dynamics. We\nmake no assumptions about where these dynamics come from, but implicitly assume\nthat they come from the players' actions and interactions over time. We define\nan equilibrium concept called \"growth equilibrium\" and show a correspondence\nbetween the growth equilibria of capital games and the Nash equilibria of\nstandard games.", "published": "2025-10-01 03:50:31", "link": "http://arxiv.org/abs/2510.00472v1", "categories": ["cs.GT", "cs.MA", "econ.TH"], "primary_category": "cs.GT"}
{"title": "Cloud Investigation Automation Framework (CIAF): An AI-Driven Approach to Cloud Forensics", "abstract": "Large Language Models (LLMs) have gained prominence in domains including\ncloud security and forensics. Yet cloud forensic investigations still rely on\nmanual analysis, making them time-consuming and error-prone. LLMs can mimic\nhuman reasoning, offering a pathway to automating cloud log analysis. To\naddress this, we introduce the Cloud Investigation Automation Framework (CIAF),\nan ontology-driven framework that systematically investigates cloud forensic\nlogs while improving efficiency and accuracy. CIAF standardizes user inputs\nthrough semantic validation, eliminating ambiguity and ensuring consistency in\nlog interpretation. This not only enhances data quality but also provides\ninvestigators with reliable, standardized information for decision-making. To\nevaluate security and performance, we analyzed Microsoft Azure logs containing\nransomware-related events. By simulating attacks and assessing CIAF's impact,\nresults showed significant improvement in ransomware detection, achieving\nprecision, recall, and F1 scores of 93 percent. CIAF's modular, adaptable\ndesign extends beyond ransomware, making it a robust solution for diverse\ncyberattacks. By laying the foundation for standardized forensic methodologies\nand informing future AI-driven automation, this work underscores the role of\ndeterministic prompt engineering and ontology-based validation in enhancing\ncloud forensic investigations. These advancements improve cloud security while\npaving the way for efficient, automated forensic workflows.", "published": "2025-10-01 03:05:47", "link": "http://arxiv.org/abs/2510.00452v1", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.CR"}
{"title": "A Call to Action for a Secure-by-Design Generative AI Paradigm", "abstract": "Large language models have gained widespread prominence, yet their\nvulnerability to prompt injection and other adversarial attacks remains a\ncritical concern. This paper argues for a security-by-design AI paradigm that\nproactively mitigates LLM vulnerabilities while enhancing performance. To\nachieve this, we introduce PromptShield, an ontology-driven framework that\nensures deterministic and secure prompt interactions. It standardizes user\ninputs through semantic validation, eliminating ambiguity and mitigating\nadversarial manipulation. To assess PromptShield's security and performance\ncapabilities, we conducted an experiment on an agent-based system to analyze\ncloud logs within Amazon Web Services (AWS), containing 493 distinct events\nrelated to malicious activities and anomalies. By simulating prompt injection\nattacks and assessing the impact of deploying PromptShield, our results\ndemonstrate a significant improvement in model security and performance,\nachieving precision, recall, and F1 scores of approximately 94%. Notably, the\nontology-based framework not only mitigates adversarial threats but also\nenhances the overall performance and reliability of the system. Furthermore,\nPromptShield's modular and adaptable design ensures its applicability beyond\ncloud security, making it a robust solution for safeguarding generative AI\napplications across various domains. By laying the groundwork for AI safety\nstandards and informing future policy development, this work stimulates a\ncrucial dialogue on the pivotal role of deterministic prompt engineering and\nontology-based validation in ensuring the safe and responsible deployment of\nLLMs in high-stakes environments.", "published": "2025-10-01 03:05:07", "link": "http://arxiv.org/abs/2510.00451v1", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.CR"}
{"title": "Conflict-Based Search as a Protocol: A Multi-Agent Motion Planning Protocol for Heterogeneous Agents, Solvers, and Independent Tasks", "abstract": "Imagine the future construction site, hospital, office, or even sophisticated\nhousehold with dozens of robots bought from different manufacturers. How can we\nenable these different systems to effectively move in a shared environment,\ngiven that each robot may have its own independent motion planning system? This\nwork shows how we can get efficient collision-free movements between\nalgorithmically heterogeneous agents by using Conflict-Based Search (Sharon et\nal. 2015) as a protocol. At its core, the CBS Protocol requires one specific\nsingle-agent motion planning API; finding a collision-free path that satisfies\ncertain space-time constraints. Given such an API, CBS uses a central planner\nto find collision-free paths - independent of how the API is implemented. We\nshow how this protocol enables multi-agent motion planning for a heterogeneous\nteam of agents completing independent tasks with a variety of single-agent\nplanners including: Heuristic Search (e.g., A*), Sampling Based Search (e.g.,\nRRT), Optimization (e.g., Direct Collocation), Diffusion, and Reinforcement\nLearning.", "published": "2025-10-01 02:07:18", "link": "http://arxiv.org/abs/2510.00425v1", "categories": ["cs.MA", "cs.RO"], "primary_category": "cs.MA"}
{"title": "Physics-Informed Neural Controlled Differential Equations for Scalable Long Horizon Multi-Agent Motion Forecasting", "abstract": "Long-horizon motion forecasting for multiple autonomous robots is challenging\ndue to non-linear agent interactions, compounding prediction errors, and\ncontinuous-time evolution of dynamics. Learned dynamics of such a system can be\nuseful in various applications such as travel time prediction,\nprediction-guided planning and generative simulation. In this work, we aim to\ndevelop an efficient trajectory forecasting model conditioned on multi-agent\ngoals. Motivated by the recent success of physics-guided deep learning for\npartially known dynamical systems, we develop a model based on neural\nControlled Differential Equations (CDEs) for long-horizon motion forecasting.\nUnlike discrete-time methods such as RNNs and transformers, neural CDEs operate\nin continuous time, allowing us to combine physics-informed constraints and\nbiases to jointly model multi-robot dynamics. Our approach, named PINCoDE\n(Physics-Informed Neural Controlled Differential Equations), learns\ndifferential equation parameters that can be used to predict the trajectories\nof a multi-agent system starting from an initial condition. PINCoDE is\nconditioned on future goals and enforces physics constraints for robot motion\nover extended periods of time. We adopt a strategy that scales our model from\n10 robots to 100 robots without the need for additional model parameters, while\nproducing predictions with an average ADE below 0.5 m for a 1-minute horizon.\nFurthermore, progressive training with curriculum learning for our PINCoDE\nmodel results in a 2.7X reduction of forecasted pose error over 4 minute\nhorizons compared to analytical models.", "published": "2025-10-01 01:27:07", "link": "http://arxiv.org/abs/2510.00401v1", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.RO"}
{"title": "Exponential convergence of a distributed divide-and-conquer algorithm for constrained convex optimization on networks", "abstract": "We propose a divide-and-conquer (DAC) algorithm for constrained convex\noptimization over networks, where the global objective is the sum of local\nobjectives attached to individual agents. The algorithm is fully distributed:\neach iteration solves local subproblems around selected fusion centers and\ncoordinates only with neighboring fusion centers. Under standard assumptions of\nsmoothness, strong convexity, and locality on the objective function, together\nwith polynomial growth conditions on the underlying graph, we establish\nexponential convergence of the DAC iterations and derive explicit bounds for\nboth exact and inexact local solvers. Numerical experiments on three\nrepresentative losses ($L_2$ distance, quadratic, and entropy) confirm the\ntheory and demonstrate scalability and effectiveness.", "published": "2025-10-01 23:12:04", "link": "http://arxiv.org/abs/2510.01511v1", "categories": ["math.OC", "cs.DC", "cs.NA", "math.NA"], "primary_category": "math.OC"}
{"title": "Improving Runtime Performance of Tensor Computations using Rust From Python", "abstract": "In this work, we investigate improving the runtime performance of key\ncomputational kernels in the Python Tensor Toolbox (pyttb), a package for\nanalyzing tensor data across a wide variety of applications. Recent runtime\nperformance improvements have been demonstrated using Rust, a compiled\nlanguage, from Python via extension modules leveraging the Python C API --\ne.g., web applications, data parsing, data validation, etc. Using this same\napproach, we study the runtime performance of key tensor kernels of increasing\ncomplexity, from simple kernels involving sums of products over data accessed\nthrough single and nested loops to more advanced tensor multiplication kernels\nthat are key in low-rank tensor decomposition and tensor regression algorithms.\nIn numerical experiments involving synthetically generated tensor data of\nvarious sizes and these tensor kernels, we demonstrate consistent improvements\nin runtime performance when using Rust from Python over 1) using Python alone,\n2) using Python and the Numba just-in-time Python compiler (for loop-based\nkernels), and 3) using the NumPy Python package for scientific computing (for\npyttb kernels).", "published": "2025-10-01 22:14:17", "link": "http://arxiv.org/abs/2510.01495v1", "categories": ["cs.MS", "cs.NA", "math.NA", "G.4"], "primary_category": "cs.MS"}
{"title": "Deep Learning Accelerated Algebraic Multigrid Methods for Polytopal Discretizations of Second-Order Differential Problems", "abstract": "Algebraic Multigrid (AMG) methods are state-of-the-art algebraic solvers for\npartial differential equations. Still, their efficiency depends heavily on the\nchoice of suitable parameters and/or ingredients. Paradigmatic examples include\nthe so-called strong threshold parameter $\\theta$, which controls the algebraic\ncoarse-grid hierarchy, as well as the smoother, i.e., the relaxation methods\nused on the fine grid to damp out high-frequency errors. In AMG, since the\ncoarse grids are constructed algebraically (without geometric intuition), the\nsmoother's performance is even more critical. For the linear systems stemming\nfrom polytopal discretizations, such as Polytopal Discontinuous Galerkin\n(PolyDG) and Virtual Element Methods (VEM), AMG sensitivity to such choices is\neven more critical due to the significant variability of the underlying meshes,\nwhich results in algebraic systems with different sparsity patterns. We propose\na novel deep learning approach that automatically tunes the strong threshold\nparameter, as well as the smoother choice in AMG solvers, for linear systems of\nequations arising from polytopal discretizations, thereby maximizing AMG\nperformance. We interpret the sparse matrix resulting from polytopal\ndiscretization as a grayscale image, and by applying pooling, our neural\nnetwork extracts compact features that preserve the necessary information at a\nlow computational cost. We test various differential problems in both two- and\nthree-dimensional settings, with heterogeneous coefficients and\npolygonal/polyhedral meshes, and demonstrate that the proposed approach\ngeneralizes well. In practice, we demonstrate that we can reduce AMG solver\ntime by up to $27\\%$ with minimal changes to existing PolyDG and VEM codes.", "published": "2025-10-01 20:24:35", "link": "http://arxiv.org/abs/2510.01442v1", "categories": ["math.NA", "cs.NA", "65N22, 65N30, 65N55, 68T01"], "primary_category": "math.NA"}
{"title": "A first-order method for constrained nonconvex--nonconcave minimax problems under a local Kurdyka-\u0141ojasiewicz condition", "abstract": "We study a class of constrained nonconvex--nonconcave minimax problems in\nwhich the inner maximization involves potentially complex constraints. Under\nthe assumption that the inner problem of a novel lifted minimax problem\nsatisfies a local Kurdyka-{\\L}ojasiewicz (KL) condition, we show that the\nmaximal function of the original problem enjoys a local H\\\"older smoothness\nproperty. We also propose a sequential convex programming (SCP) method for\nsolving constrained optimization problems and establish its convergence rate\nunder a local KL condition. Leveraging these results, we develop an inexact\nproximal gradient method for the original minimax problem, where the inexact\ngradient of the maximal function is computed via the SCP method applied to a\nlocally KL-structured subproblem. Finally, we establish complexity guarantees\nfor the proposed method in computing an approximate stationary point of the\noriginal minimax problem.", "published": "2025-10-01 17:54:27", "link": "http://arxiv.org/abs/2510.01168v1", "categories": ["math.OC", "cs.LG", "cs.NA", "math.NA", "stat.ML", "90C26, 90C30, 90C47, 90C99, 65K05"], "primary_category": "math.OC"}
{"title": "The exterior derivative and the mean value equality in $\\mathbb{R}^n$", "abstract": "This survey revisits classical results in vector calculus and analysis by\nexploring a generalised perspective on the exterior derivative, interpreting it\nas a measure of \"infinitesimal flux\". This viewpoint leads to a\nhigher-dimensional analogue of the Mean Value Theorem, valid for differential\n$k$-forms, and provides a natural formulation of Stokes' theorem that mirrors\nthe exact hypotheses of the Fundamental Theorem of Calculus - without requiring\nfull $C^1$ smoothness of the differential form.\n  As a numerical application, we propose an algorithm for exterior\ndifferentiation in $\\mathbb{R}^n$ that relies solely on black-box access to the\ndifferential form, offering a practical tool for computation without the need\nfor mesh discretization or explicit symbolic expressions.", "published": "2025-10-01 15:05:23", "link": "http://arxiv.org/abs/2510.00999v1", "categories": ["math.DG", "cs.NA", "math.NA", "26B05, 58A10, 65D25"], "primary_category": "math.DG"}
{"title": "Implementation techniques for multigrid solvers for high-order Discontinuous Galerkin methods", "abstract": "Matrix-free geometric multigrid solvers for elliptic PDEs that have been\ndiscretised with Higher-order Discontinuous Galerkin (DG) methods are ideally\nsuited to exploit state-of-the-art computer architectures. Higher polynomial\ndegrees offer exponential convergence, while the workload fits to vector units,\nis straightforward to parallelise, and exhibits high arithmetic intensity. Yet,\nDG methods such as the interior penalty DG discreisation do not magically\nguarantee high performance: they require non-local memory access due to\ncoupling between neighbouring cells and break down into compute steps of widely\nvarying costs and compute character. We address these limitations by developing\nefficient execution strategies for $hp$-multigrid. Separating cell- and\nfacet-operations by introducing auxiliary facet variables localizes data\naccess, reduces the need for frequent synchronization, and enables overlap of\ncomputation and communication. Loop fusion results in a single-touch scheme\nwhich reads (cell) data only once per smoothing step. We interpret the\nresulting execution strategies in the context of a task formalism, which\nexposes additional concurreny. The target audience of this paper are\npractitioners in Scientific Computing who are not necessarily experts on\nmultigrid or familiar with sophisticated discretisation techniques. By\ndiscussing implementation techniques for a powerful solver algorithm we aim to\nmake it accessible to the wider community.", "published": "2025-10-01 15:04:21", "link": "http://arxiv.org/abs/2510.00998v1", "categories": ["math.NA", "cs.NA", "35J05, 35J55, 65N30, 65N55, 65Y05, 65Y20", "G.1.3; G.1.8"], "primary_category": "math.NA"}
{"title": "Digital Twins: McKean-Pontryagin Control for Partially Observed Physical Twins", "abstract": "Optimal control for fully observed diffusion processes is well established\nand has led to numerous numerical implementations based on, for example,\nBellman's principle, model free reinforcement learning, Pontryagin's maximum\nprinciple, and model predictive control. On the contrary, much fewer algorithms\nare available for optimal control of partially observed processes. However,\nthis scenario is central to the digital twin paradigm where a physical twin is\npartially observed and control laws are derived based on a digital twin. In\nthis paper, we contribute to this challenge by combining data assimilation in\nthe form of the ensemble Kalman filter with the recently proposed\nMcKean-Pontryagin approach to stochastic optimal control. We derive forward\nevolving mean-field evolution equations for states and co-states which\nsimultaneously allow for an online assimilation of data as well as an online\ncomputation of control laws. The proposed methodology is therefore perfectly\nsuited for real time applications of digital twins. We present numerical\nresults for a controlled Lorenz-63 system and an inverted pendulum.", "published": "2025-10-01 14:15:42", "link": "http://arxiv.org/abs/2510.00937v1", "categories": ["math.OC", "cs.NA", "math.NA"], "primary_category": "math.OC"}
{"title": "A semi-Lagrangian method for solving state constraint Mean Field Games in Macroeconomics", "abstract": "We study continuous-time heterogeneous agent models cast as Mean Field Games,\nin the Aiyagari-Bewley-Huggett framework. The model couples a\nHamilton-Jacobi-Bellman equation for individual optimization with a\nFokker-Planck-Kolmogorov equation for the wealth distribution. We establish a\ncomparison principle for constrained viscosity solutions of the HJB equation\nand propose a semi-Lagrangian (SL) scheme for its numerical solution, proving\nconvergence via the Barles-Souganidis method. A policy iteration algorithm\nhandles state constraints, and a dual SL scheme is used for the FPK equation.\nNumerical methods are presented in a fully discrete, implementable form.", "published": "2025-10-01 11:00:36", "link": "http://arxiv.org/abs/2510.00768v1", "categories": ["math.OC", "cs.NA", "math.NA"], "primary_category": "math.OC"}
{"title": "Approximation of differential entropy in Bayesian optimal experimental design", "abstract": "Bayesian optimal experimental design provides a principled framework for\nselecting experimental settings that maximize obtained information. In this\nwork, we focus on estimating the expected information gain in the setting where\nthe differential entropy of the likelihood is either independent of the design\nor can be evaluated explicitly. This reduces the problem to maximum entropy\nestimation, alleviating several challenges inherent in expected information\ngain computation.\n  Our study is motivated by large-scale inference problems, such as inverse\nproblems, where the computational cost is dominated by expensive likelihood\nevaluations. We propose a computational approach in which the evidence density\nis approximated by a Monte Carlo or quasi-Monte Carlo surrogate, while the\ndifferential entropy is evaluated using standard methods without additional\nlikelihood evaluations. We prove that this strategy achieves convergence rates\nthat are comparable to, or better than, state-of-the-art methods for full\nexpected information gain estimation, particularly when the cost of entropy\nevaluation is negligible. Moreover, our approach relies only on mild smoothness\nof the forward map and avoids stronger technical assumptions required in\nearlier work. We also present numerical experiments, which confirm our\ntheoretical findings.", "published": "2025-10-01 10:17:07", "link": "http://arxiv.org/abs/2510.00734v1", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA", "stat.CO"], "primary_category": "stat.ML"}
{"title": "Carleman Linearization of Parabolic PDEs: Well-posedness, convergence, and efficient numerical methods", "abstract": "We explore how the analysis of the Carleman linearization can be extended to\ndynamical systems on infinite-dimensional Hilbert spaces with quadratic\nnonlinearities. We demonstrate the well-posedness and convergence of the\ntruncated Carleman linearization under suitable assumptions on the dynamical\nsystem, which encompass common parabolic semi-linear partial differential\nequations such as the Navier-Stokes equations and nonlinear\ndiffusion-advection-reaction equations. Upon discretization, we show that the\ntotal approximation error of the linearization decomposes into two independent\ncomponents: the discretization error and the linearization error. This\ndecomposition yields a convergence radius and convergence rate for the\ndiscretized linearization that are independent of the discretization. We thus\njustify the application of the linearization to parabolic PDE problems.\nFurthermore, it motivates the use of non-standard structure-exploiting\nnumerical methods, such as sparse grids, taming the curse of dimensionality\nassociated with the Carleman linearization. Finally, we verify the results with\nnumerical experiments.", "published": "2025-10-01 09:57:03", "link": "http://arxiv.org/abs/2510.00722v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Symbolic and High-Accuracy Solutions to Differential and Integral Problems via a Novel Recursive Inverse Laplace Method", "abstract": "In this paper, we introduce a novel semi-analytical method for solving a\nbroad class of initial value problems involving differential,\nintegro-differential, and delay equations, including those with fractional and\nvariable-order derivatives. The proposed approach is based on the inverse\nLaplace transform, applied initially - unlike traditional Laplace-based\ntechniques which begin with a forward transformation. By assuming the unknown\nsolution is the Laplace transform of an auxiliary function, the method\nreformulates the problem in the time domain and recursively solves for this\nfunction using symbolic operations. The final solution is then obtained by\napplying the Laplace transform to the result. This strategy enables the\nconstruction of symbolic solutions as generalized logarithmic-power series with\narbitrary accuracy, and naturally accommodates complex terms. The method is\nhighly versatile and demonstrates superior speed and precision across a wide\nrange of linear and nonlinear problems, including singular, fractional, and\nchaotic systems. Several benchmark examples are provided to validate the\nreliability and efficiency of the proposed technique compared to classical\nnumerical methods. The results confirm that the new method offers a powerful\nand flexible framework for symbolic computation of initial value problems.", "published": "2025-10-01 09:53:06", "link": "http://arxiv.org/abs/2510.00719v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "A Computationally Efficient Finite Element Method for Shape Reconstruction of Inverse Conductivity Problems", "abstract": "The inverse conductivity problem aims at determining the unknown conductivity\ninside a bounded domain from boundary measurements. In practical applications,\nalgorithms based on minimizing a regularized residual functional subject to PDE\nconstraints have been widely used to deal with this problem. However, such\napproaches typically require repeated iterations and solving the forward\nproblem at each iteration, which leads to a heavy computational cost. To\naddress this issue, we first reformulate the inverse conductivity problem as a\nminimization problem involving a regularized residual functional. We then\ntransform this minimization problem into a variational problem and establish\nthe equivalence between them. This reformulation enables the employment of the\nfinite element method to reconstruct the shape of the object from finitely many\nmeasurements. Notably, the proposed approach allows us to identify the object\ndirectly without requiring any iterative procedure. {\\it A prior} error\nestimates are rigorously established to demonstrate the theoretical soundness\nof the finite element method. Based on these estimates, we provide a criterion\nfor selecting the regularization parameter. Additionally, several numerical\nexamples are presented to verify the feasibility of the proposed approach in\nshape reconstruction.", "published": "2025-10-01 07:26:10", "link": "http://arxiv.org/abs/2510.00597v1", "categories": ["math.NA", "cs.NA", "math-ph", "math.MP"], "primary_category": "math.NA"}
{"title": "Time-marching multi-level variational multiscale tensor decomposition algorithm for heat conduction with moving heat source", "abstract": "In this paper, we propose a time-marching multi-level Variational\nMultiscale-Tensor Decomposition (VMS-TD) algorithm to solve the heat equation\nwith a moving heat source model that arises from additive manufacturing. First,\nwe take a second-order centered difference for time semi-discretization. The\ntemperature field is decomposed according to multiple space resolution levels,\neach represented by the TD method. Then we adopt the VMS formulation [T.J.R.\nHughes, G.R. Feijoo, L. Mazzei, J.B. Quincy. Comput. Methods Appl. Mech. Engrg.\n166:3-24 (1998)] for the resulting elliptic problem to obtain a Galerkin weak\nform, and design VMS-TD algorithm to solve it. Furthermore, to comply with the\nTD solution scheme, special inter-scale data transfers are made at the scale\ninterface and moving fine-scale subdomains. Numerical results demonstrate that\nthe multi-level VMS-TD algorithm is much more efficient than the fully resolved\nTD algorithm, let alone traditional direct numerical simulation methods such as\nfinite difference or finite element analysis. Compared with the well-known\nmulti-grid methods or more recent GO-MELT framework [J.P. Leonor, G.J. Wagner.\nComput. Methods Appl. Mech. Engrg, 426:116977 (2024)], the three-level VMS-TD\nuses much smaller degrees of freedom to reach accurate results. A\nmulti-time-scale extension of VMS-TD algorithm is also proposed.", "published": "2025-10-01 04:56:43", "link": "http://arxiv.org/abs/2510.00516v1", "categories": ["math.NA", "cs.NA", "65M60, 80M10"], "primary_category": "math.NA"}
{"title": "A multi-resolution limiter for the Runge-Kutta discontinuous Galerkin method", "abstract": "We propose a novel multi-resolution (MR) limiter for the Runge-Kutta\ndiscontinuous Galerkin (RKDG) method for solving hyperbolic conservation laws\non a general unstructured mesh. Unlike classical limiters, which detects only\nsolution discontinuities to dichotomize cells into good or troubled, the\nproposed MR limiter also takes into account the derivative discontinuities to\ndivide cells into several groups. The method operates by performing a\nsuccessive comparison of the local DG polynomial's derivatives, from high-order\nto low-order, against a baseline constructed from neighboring cell averages. If\na $k$th-order derivative of the DG polynomial is larger than the baseline,\n  then we reduce the order to $k-1$ and set the corresponding $k$th-order terms\nto be 0; Otherwise, the remaining $k$th-order DG polynomial is used to\nrepresent the final solution. Only if all the derivatives are larger than the\nbaseline, a TVD slope limiter is used to reconstruct the solution. In this\nmanner, the limiter dynamically selects an optimal polynomial suited to the\nlocal solution smoothness without problem-dependent parameter to tune. Notably,\nit also possesses a scale-invariance property that is absent in most classical\nlimiters. A series of numerical examples demonstrate the accuracy and\nrobustness of the proposed MR limiter.", "published": "2025-10-01 04:47:36", "link": "http://arxiv.org/abs/2510.00511v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Mathematical and numerical analysis of quantum signal processing", "abstract": "Quantum signal processing (QSP) provides a representation of scalar\npolynomials of degree $d$ as products of matrices in $\\mathrm{SU}(2)$,\nparameterized by $(d+1)$ real numbers known as phase factors. QSP is the\nmathematical foundation of quantum singular value transformation (QSVT), which\nis often regarded as one of the most important quantum algorithms of the past\ndecade, with a wide range of applications in scientific computing, from\nHamiltonian simulation to solving linear systems of equations and eigenvalue\nproblems. In this article we survey recent advances in the mathematical and\nnumerical analysis of QSP. In particular, we focus on its generalization beyond\npolynomials, the computational complexity of algorithms for phase factor\nevaluation, and the numerical stability of such algorithms. The resolution to\nsome of these problems relies on an unexpected interplay between QSP, nonlinear\nFourier analysis on $\\mathrm{SU}(2)$, fast polynomial multiplications, and\nGaussian elimination for matrices with displacement structure.", "published": "2025-10-01 02:49:50", "link": "http://arxiv.org/abs/2510.00443v1", "categories": ["quant-ph", "cs.NA", "math.CA", "math.NA", "68Q12, 81P68, 65T99, 65F05, 42C99"], "primary_category": "quant-ph"}
{"title": "Blow-up of solutions for discrete semilinear wave equation with the scale-invariant damping", "abstract": "We consider the blow-up problem for discretized scale-invariant nonlinear\ndissipative wave equations. It is known that the critical exponents for\nundiscretized equations (continuous equations) are given by Fujita and Strauss\nexponents depending on the space dimensions. Our purpose is to obtain results\nfor the discretized equations that correspond to those shown for the continuous\none. The proof is based on Matsuya [6], who showed the blow-up problem for\ndiscrete semilinear wave equations without dissipative terms, and we found that\nthe result is sharp in the case of one and two space dimensions compared to the\ncontinuous equations.", "published": "2025-10-01 02:43:46", "link": "http://arxiv.org/abs/2510.00439v1", "categories": ["math.AP", "cs.NA", "math.NA"], "primary_category": "math.AP"}
{"title": "Numerical analysis of 2D Navier--Stokes equations with nonsmooth initial value in the critical space", "abstract": "This paper addresses the numerical solution of the two-dimensional\nNavier--Stokes (NS) equations with nonsmooth initial data in the $L^2$ space,\nwhich is the critical space for the two-dimensional NS equations to be\nwell-posed. In this case, the solutions of the NS equations exhibit certain\nsingularities at $t=0$, e.g., the $H^s$ norm of the solution blows up as\n$t\\rightarrow 0$ when $s>0$. To date, the best convergence result proved in the\nliterature are first-order accuracy in both time and space for the\nsemi-implicit Euler time-stepping scheme and divergence-free finite elements\n(even high-order finite elements are used), while numerical results demonstrate\nthat second-order convergence in time and space may be achieved. Therefore,\nthere is still a gap between numerical analysis and numerical computation for\nthe NS equations with $L^2$ initial data. The primary challenge to realizing\nhigh-order convergence is the insufficient regularity in the solutions due to\nthe rough initial condition and the nonlinearity of the equations. In this\nwork, we propose a fully discrete numerical scheme that utilizes the\nTaylor--Hood or Stokes-MINI finite element method for spatial discretization\nand an implicit-explicit Runge--Kutta time-stepping method in conjunction with\ngraded stepsizes. By employing discrete semigroup techniques, sharp regularity\nestimates, negative norm estimates and the $L^2$ projection onto the\ndivergence-free Raviart--Thomas element space, we prove that the proposed\nscheme attains second-order convergence in both space and time. Numerical\nexamples are presented to support the theoretical analysis. In particular, the\nconvergence in space is at most second order even higher-order finite elements\nare used. This shows the sharpness of the convergence order proved in this\narticle.", "published": "2025-10-01 01:08:01", "link": "http://arxiv.org/abs/2510.00393v1", "categories": ["math.NA", "cs.NA", "65M12, 65M15, 76D05"], "primary_category": "math.NA"}
{"title": "Zero variance self-normalized importance sampling via estimating equations", "abstract": "In ordinary importance sampling with a nonnegative integrand there exists an\nimportance sampling strategy with zero variance. Practical sampling strategies\nare often based on approximating that optimal solution, potentially approaching\nzero variance. There is a positivisation extension of that method to handle\nintegrands that take both positive and negative values. Self-normalized\nimportance sampling uses a ratio estimate, for which the optimal sampler does\nnot have zero variance and so zero variance cannot even be approached in\npractice. Strategies that separately estimate the numerator and denominator of\nthat ratio can approach zero variance. This paper develops another zero\nvariance solution for self-normalized importance sampling. The first step is to\nwrite the desired expectation as the zero of an estimating equation using\nFieller's technique. Then we apply the positivisation strategy to the\nestimating equation. This paper give conditions for existence and uniqueness of\nthe sample solution to the estimating equation. Then it give conditions for\nconsistency and asymptotic normality and an expression for the asymptotic\nvariance. The sample size multiplied by the variance of the asymptotic formula\nbecomes arbitrarily close to zero for certain sampling strategies.", "published": "2025-10-01 01:01:36", "link": "http://arxiv.org/abs/2510.00389v1", "categories": ["math.ST", "cs.NA", "math.NA", "stat.CO", "stat.TH"], "primary_category": "math.ST"}
{"title": "Can Machine Learning Algorithms Outperform Traditional Models for Option Pricing?", "abstract": "This study investigates the application of machine learning techniques,\nspecifically Neural Networks, Random Forests, and CatBoost for option pricing,\nin comparison to traditional models such as Black-Scholes and Heston Model.\nUsing both synthetically generated data and real market option data, each model\nis evaluated in predicting the option price. The results show that machine\nlearning models can capture complex, non-linear relationships in option prices\nand, in several cases, outperform both Black-Scholes and Heston models. These\nfindings highlight the potential of data-driven methods to improve pricing\naccuracy and better reflect market dynamics.", "published": "2025-10-01 20:37:18", "link": "http://arxiv.org/abs/2510.01446v1", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "DiffKnock: Diffusion-based Knockoff Statistics for Neural Networks Inference", "abstract": "We introduce DiffKnock, a diffusion-based knockoff framework for\nhigh-dimensional feature selection with finite-sample false discovery rate\n(FDR) control. DiffKnock addresses two key limitations of existing knockoff\nmethods: preserving complex feature dependencies and detecting non-linear\nassociations. Our approach trains diffusion models to generate valid knockoffs\nand uses neural network--based gradient and filter statistics to construct\nantisymmetric feature importance measures. Through simulations, we showed that\nDiffKnock achieved higher power than autoencoder-based knockoffs while\nmaintaining target FDR, indicating its superior performance in scenarios\ninvolving complex non-linear architectures. Applied to murine single-cell\nRNA-seq data of LPS-stimulated macrophages, DiffKnock identifies canonical\nNF-$\\kappa$B target genes (Ccl3, Hmox1) and regulators (Fosb, Pdgfb). These\nresults highlight that, by combining the flexibility of deep generative models\nwith rigorous statistical guarantees, DiffKnock is a powerful and reliable tool\nfor analyzing single-cell RNA-seq data, as well as high-dimensional and\nstructured data in other domains.", "published": "2025-10-01 19:54:23", "link": "http://arxiv.org/abs/2510.01418v1", "categories": ["stat.ME", "stat.AP", "stat.ML"], "primary_category": "stat.ME"}
{"title": "Risk Phase Transitions in Spiked Regression: Alignment Driven Benign and Catastrophic Overfitting", "abstract": "This paper analyzes the generalization error of minimum-norm interpolating\nsolutions in linear regression using spiked covariance data models. The paper\ncharacterizes how varying spike strengths and target-spike alignments can\naffect risk, especially in overparameterized settings. The study presents an\nexact expression for the generalization error, leading to a comprehensive\nclassification of benign, tempered, and catastrophic overfitting regimes based\non spike strength, the aspect ratio $c=d/n$ (particularly as $c \\to \\infty$),\nand target alignment. Notably, in well-specified aligned problems, increasing\nspike strength can surprisingly induce catastrophic overfitting before\nachieving benign overfitting. The paper also reveals that target-spike\nalignment is not always advantageous, identifying specific, sometimes\ncounterintuitive, conditions for its benefit or detriment. Alignment with the\nspike being detrimental is empirically demonstrated to persist in nonlinear\nmodels.", "published": "2025-10-01 19:51:47", "link": "http://arxiv.org/abs/2510.01414v1", "categories": ["stat.ML", "cs.AI", "cs.LG"], "primary_category": "stat.ML"}
{"title": "A theoretical framework for M-posteriors: frequentist guarantees and robustness properties", "abstract": "We provide a theoretical framework for a wide class of generalized posteriors\nthat can be viewed as the natural Bayesian posterior counterpart of the class\nof M-estimators in the frequentist world. We call the members of this class\nM-posteriors and show that they are asymptotically normally distributed under\nmild conditions on the M-estimation loss and the prior. In particular, an\nM-posterior contracts in probability around a normal distribution centered at\nan M-estimator, showing frequentist consistency and suggesting some degree of\nrobustness depending on the reference M-estimator. We formalize the robustness\nproperties of the M-posteriors by a new characterization of the posterior\ninfluence function and a novel definition of breakdown point adapted for\nposterior distributions. We illustrate the wide applicability of our theory in\nvarious popular models and illustrate their empirical relevance in some\nnumerical examples.", "published": "2025-10-01 18:37:05", "link": "http://arxiv.org/abs/2510.01358v1", "categories": ["math.ST", "stat.ML", "stat.TH"], "primary_category": "math.ST"}
{"title": "To Augment or Not to Augment? Diagnosing Distributional Symmetry Breaking", "abstract": "Symmetry-aware methods for machine learning, such as data augmentation and\nequivariant architectures, encourage correct model behavior on all\ntransformations (e.g. rotations or permutations) of the original dataset. These\nmethods can improve generalization and sample efficiency, under the assumption\nthat the transformed datapoints are highly probable, or \"important\", under the\ntest distribution. In this work, we develop a method for critically evaluating\nthis assumption. In particular, we propose a metric to quantify the amount of\nanisotropy, or symmetry-breaking, in a dataset, via a two-sample neural\nclassifier test that distinguishes between the original dataset and its\nrandomly augmented equivalent. We validate our metric on synthetic datasets,\nand then use it to uncover surprisingly high degrees of alignment in several\nbenchmark point cloud datasets. We show theoretically that distributional\nsymmetry-breaking can actually prevent invariant methods from performing\noptimally even when the underlying labels are truly invariant, as we show for\ninvariant ridge regression in the infinite feature limit. Empirically, we find\nthat the implication for symmetry-aware methods is dataset-dependent:\nequivariant methods still impart benefits on some anisotropic datasets, but not\nothers. Overall, these findings suggest that understanding equivariance -- both\nwhen it works, and why -- may require rethinking symmetry biases in the data.", "published": "2025-10-01 18:26:33", "link": "http://arxiv.org/abs/2510.01349v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "LVTINO: LAtent Video consisTency INverse sOlver for High Definition Video Restoration", "abstract": "Computational imaging methods increasingly rely on powerful generative\ndiffusion models to tackle challenging image restoration tasks. In particular,\nstate-of-the-art zero-shot image inverse solvers leverage distilled\ntext-to-image latent diffusion models (LDMs) to achieve unprecedented accuracy\nand perceptual quality with high computational efficiency. However, extending\nthese advances to high-definition video restoration remains a significant\nchallenge, due to the need to recover fine spatial detail while capturing\nsubtle temporal dependencies. Consequently, methods that naively apply\nimage-based LDM priors on a frame-by-frame basis often result in temporally\ninconsistent reconstructions. We address this challenge by leveraging recent\nadvances in Video Consistency Models (VCMs), which distill video latent\ndiffusion models into fast generators that explicitly capture temporal\ncausality. Building on this foundation, we propose LVTINO, the first zero-shot\nor plug-and-play inverse solver for high definition video restoration with\npriors encoded by VCMs. Our conditioning mechanism bypasses the need for\nautomatic differentiation and achieves state-of-the-art video reconstruction\nquality with only a few neural function evaluations, while ensuring strong\nmeasurement consistency and smooth temporal transitions across frames.\nExtensive experiments on a diverse set of video inverse problems show\nsignificant perceptual improvements over current state-of-the-art methods that\napply image LDMs frame by frame, establishing a new benchmark in both\nreconstruction fidelity and computational efficiency.", "published": "2025-10-01 18:10:08", "link": "http://arxiv.org/abs/2510.01339v1", "categories": ["cs.CV", "stat.ML"], "primary_category": "cs.CV"}
{"title": "On the Identifiability of Latent Action Policies", "abstract": "We study the identifiability of latent action policy learning (LAPO), a\nframework introduced recently to discover representations of actions from video\ndata. We formally describe desiderata for such representations, their\nstatistical benefits and potential sources of unidentifiability. Finally, we\nprove that an entropy-regularized LAPO objective identifies action\nrepresentations satisfying our desiderata, under suitable conditions. Our\nanalysis provides an explanation for why discrete action representations\nperform well in practice.", "published": "2025-10-01 18:09:24", "link": "http://arxiv.org/abs/2510.01337v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Continuously Augmented Discrete Diffusion model for Categorical Generative Modeling", "abstract": "Standard discrete diffusion models treat all unobserved states identically by\nmapping them to an absorbing [MASK] token. This creates an 'information void'\nwhere semantic information that could be inferred from unmasked tokens is lost\nbetween denoising steps. We introduce Continuously Augmented Discrete Diffusion\n(CADD), a framework that augments the discrete state space with a paired\ndiffusion in a continuous latent space. This yields graded, gradually corrupted\nstates in which masked tokens are represented by noisy yet informative latent\nvectors rather than collapsed 'information voids'. At each reverse step, CADD\nmay leverage the continuous latent as a semantic hint to guide discrete\ndenoising. The design is clean and compatible with existing discrete diffusion\ntraining. At sampling time, the strength and choice of estimator for the\ncontinuous latent vector enables a controlled trade-off between mode-coverage\n(generating diverse outputs) and mode-seeking (generating contextually precise\noutputs) behaviors. Empirically, we demonstrate CADD improves generative\nquality over mask-based diffusion across text generation, image synthesis, and\ncode modeling, with consistent gains on both qualitative and quantitative\nmetrics against strong discrete baselines.", "published": "2025-10-01 18:00:56", "link": "http://arxiv.org/abs/2510.01329v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "On the Benefits of Weight Normalization for Overparameterized Matrix Sensing", "abstract": "While normalization techniques are widely used in deep learning, their\ntheoretical understanding remains relatively limited. In this work, we\nestablish the benefits of (generalized) weight normalization (WN) applied to\nthe overparameterized matrix sensing problem. We prove that WN with Riemannian\noptimization achieves linear convergence, yielding an exponential speedup over\nstandard methods that do not use WN. Our analysis further demonstrates that\nboth iteration and sample complexity improve polynomially as the level of\noverparameterization increases. To the best of our knowledge, this work\nprovides the first characterization of how WN leverages overparameterization\nfor faster convergence in matrix sensing.", "published": "2025-10-01 17:56:58", "link": "http://arxiv.org/abs/2510.01175v1", "categories": ["cs.LG", "eess.SP", "math.OC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "How Does the Pretraining Distribution Shape In-Context Learning? Task Selection, Generalization, and Robustness", "abstract": "The emergence of in-context learning (ICL) in large language models (LLMs)\nremains poorly understood despite its consistent effectiveness, enabling models\nto adapt to new tasks from only a handful of examples. To clarify and improve\nthese capabilities, we characterize how the statistical properties of the\npretraining distribution (e.g., tail behavior, coverage) shape ICL on numerical\ntasks. We develop a theoretical framework that unifies task selection and\ngeneralization, extending and sharpening earlier results, and show how\ndistributional properties govern sample efficiency, task retrieval, and\nrobustness. To this end, we generalize Bayesian posterior consistency and\nconcentration results to heavy-tailed priors and dependent sequences, better\nreflecting the structure of LLM pretraining data. We then empirically study how\nICL performance varies with the pretraining distribution on challenging tasks\nsuch as stochastic differential equations and stochastic processes with memory.\nTogether, these findings suggest that controlling key statistical properties of\nthe pretraining distribution is essential for building ICL-capable and reliable\nLLMs.", "published": "2025-10-01 17:52:29", "link": "http://arxiv.org/abs/2510.01163v1", "categories": ["cs.LG", "stat.ML", "G.3; I.2.6"], "primary_category": "cs.LG"}
{"title": "Theory of Scaling Laws for In-Context Regression: Depth, Width, Context and Time", "abstract": "We study in-context learning (ICL) of linear regression in a deep linear\nself-attention model, characterizing how performance depends on various\ncomputational and statistical resources (width, depth, number of training\nsteps, batch size and data per context). In a joint limit where data dimension,\ncontext length, and residual stream width scale proportionally, we analyze the\nlimiting asymptotics for three ICL settings: (1) isotropic covariates and tasks\n(ISO), (2) fixed and structured covariance (FS), and (3) where covariances are\nrandomly rotated and structured (RRS). For ISO and FS settings, we find that\ndepth only aids ICL performance if context length is limited. Alternatively, in\nthe RRS setting where covariances change across contexts, increasing the depth\nleads to significant improvements in ICL, even at infinite context length. This\nprovides a new solvable toy model of neural scaling laws which depends on both\nwidth and depth of a transformer and predicts an optimal transformer shape as a\nfunction of compute. This toy model enables computation of exact asymptotics\nfor the risk as well as derivation of powerlaws under source/capacity\nconditions for the ICL tasks.", "published": "2025-10-01 16:45:04", "link": "http://arxiv.org/abs/2510.01098v1", "categories": ["stat.ML", "cond-mat.dis-nn", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Optimal placement of wind farms via quantile constraint learning", "abstract": "Wind farm placement arranges the size and the location of multiple wind farms\nwithin a given region. The power output is highly related to the wind speed on\nspatial and temporal levels, which can be modeled by advanced data-driven\napproaches. To this end, we use a probabilistic neural network as a surrogate\nthat accounts for the spatiotemporal correlations of wind speed. This neural\nnetwork uses ReLU activation functions so that it can be reformulated as\nmixed-integer linear set of constraints (constraint learning). We embed these\nconstraints into the placement decision problem, formulated as a two-stage\nstochastic optimization problem. Specifically, conditional quantiles of the\ntotal electricity production are regarded as recursive decisions in the second\nstage. We use real high-resolution regional data from a northern region in\nSpain. We validate that the constraint learning approach outperforms the\nclassical bilinear interpolation method. Numerical experiments are implemented\non risk-averse investors. The results indicate that risk-averse investors\nconcentrate on dominant sites with strong wind, while exhibiting spatial\ndiversification and sensitive capacity spread in non-dominant sites.\nFurthermore, we show that if we introduce transmission line costs in the\nproblem, risk-averse investors favor locations closer to the substations. On\nthe contrary, risk-neutral investors are willing to move to further locations\nto achieve higher expected profits. Our results conclude that the proposed\nnovel approach is able to tackle a portfolio of regional wind farm placements\nand further provide guidance for risk-averse investors.", "published": "2025-10-01 16:38:56", "link": "http://arxiv.org/abs/2510.01093v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Low Rank Gradients and Where to Find Them", "abstract": "This paper investigates low-rank structure in the gradients of the training\nloss for two-layer neural networks while relaxing the usual isotropy\nassumptions on the training data and parameters. We consider a spiked data\nmodel in which the bulk can be anisotropic and ill-conditioned, we do not\nrequire independent data and weight matrices and we also analyze both the\nmean-field and neural-tangent-kernel scalings. We show that the gradient with\nrespect to the input weights is approximately low rank and is dominated by two\nrank-one terms: one aligned with the bulk data-residue , and another aligned\nwith the rank one spike in the input data. We characterize how properties of\nthe training data, the scaling regime and the activation function govern the\nbalance between these two components. Additionally, we also demonstrate that\nstandard regularizers, such as weight decay, input noise and Jacobian\npenalties, also selectively modulate these components. Experiments on synthetic\nand real data corroborate our theoretical predictions.", "published": "2025-10-01 16:20:19", "link": "http://arxiv.org/abs/2510.01303v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Equivariant Geometric Scattering Networks via Vector Diffusion Wavelets", "abstract": "We introduce a novel version of the geometric scattering transform for\ngeometric graphs containing scalar and vector node features. This new\nscattering transform has desirable symmetries with respect to rigid-body\nroto-translations (i.e., $SE(3)$-equivariance) and may be incorporated into a\ngeometric GNN framework. We empirically show that our equivariant\nscattering-based GNN achieves comparable performance to other equivariant\nmessage-passing-based GNNs at a fraction of the parameter count.", "published": "2025-10-01 15:28:45", "link": "http://arxiv.org/abs/2510.01022v1", "categories": ["cs.LG", "eess.SP", "stat.ML"], "primary_category": "cs.LG"}
{"title": "The Good, the Bad, and the Sampled: a No-Regret Approach to Safe Online Classification", "abstract": "We study the problem of sequentially testing individuals for a binary disease\noutcome whose true risk is governed by an unknown logistic model. At each\nround, a patient arrives with feature vector $x_t$, and the decision maker may\neither pay to administer a (noiseless) diagnostic test--revealing the true\nlabel--or skip testing and predict the patient's disease status based on their\nfeature vector and prior history. Our goal is to minimize the total number of\ncostly tests required while guaranteeing that the fraction of\nmisclassifications does not exceed a prespecified error tolerance $\\alpha$,\nwith probability at least $1-\\delta$. To address this, we develop a novel\nalgorithm that interleaves label-collection and distribution estimation to\nestimate both $\\theta^{*}$ and the context distribution $P$, and computes a\nconservative, data-driven threshold $\\tau_t$ on the logistic score\n$|x_t^\\top\\theta|$ to decide when testing is necessary. We prove that, with\nprobability at least $1-\\delta$, our procedure does not exceed the target\nmisclassification rate, and requires only $O(\\sqrt{T})$ excess tests compared\nto the oracle baseline that knows both $\\theta^{*}$ and the patient feature\ndistribution $P$. This establishes the first no-regret guarantees for\nerror-constrained logistic testing, with direct applications to cost-sensitive\nmedical screening. Simulations corroborate our theoretical guarantees, showing\nthat in practice our procedure efficiently estimates $\\theta^{*}$ while\nretaining safety guarantees, and does not require too many excess tests.", "published": "2025-10-01 15:28:00", "link": "http://arxiv.org/abs/2510.01020v1", "categories": ["cs.LG", "cs.AI", "math.ST", "stat.ML", "stat.TH"], "primary_category": "cs.LG"}
{"title": "False Discovery Rate Control via Bayesian Mirror Statistic", "abstract": "Simultaneously performing variable selection and inference in\nhigh-dimensional models is an open challenge in statistics and machine\nlearning. The increasing availability of vast amounts of variables requires the\nadoption of specific statistical procedures to accurately select the most\nimportant predictors in a high-dimensional space, while being able to control\nsome form of selection error. In this work we adapt the Mirror Statistic\napproach to False Discovery Rate (FDR) control into a Bayesian modelling\nframework. The Mirror Statistic, developed in the classic frequentist\nstatistical framework, is a flexible method to control FDR, which only requires\nmild model assumptions, but requires two sets of independent regression\ncoefficient estimates, usually obtained after splitting the original dataset.\nHere we propose to rely on a Bayesian formulation of the model and use the\nposterior distributions of the coefficients of interest to build the Mirror\nStatistic and effectively control the FDR without the need to split the data.\nMoreover, the method is very flexible since it can be used with continuous and\ndiscrete outcomes and more complex predictors, such as with mixed models. We\nkeep the approach scalable to high-dimensions by relying on Automatic\nDifferentiation Variational Inference and fully continuous prior choices.", "published": "2025-10-01 13:24:50", "link": "http://arxiv.org/abs/2510.00875v1", "categories": ["stat.ME", "stat.ML"], "primary_category": "stat.ME"}
{"title": "Non-Euclidean Broximal Point Method: A Blueprint for Geometry-Aware Optimization", "abstract": "The recently proposed Broximal Point Method (BPM) [Gruntkowska et al., 2025]\noffers an idealized optimization framework based on iteratively minimizing the\nobjective function over norm balls centered at the current iterate. It enjoys\nstriking global convergence guarantees, converging linearly and in a finite\nnumber of steps for proper, closed and convex functions. However, its\ntheoretical analysis has so far been confined to the Euclidean geometry. At the\nsame time, emerging trends in deep learning optimization, exemplified by\nalgorithms such as Muon [Jordan et al., 2024] and Scion [Pethick et al., 2025],\ndemonstrate the practical advantages of minimizing over balls defined via\nnon-Euclidean norms which better align with the underlying geometry of the\nassociated loss landscapes. In this note, we ask whether the convergence theory\nof BPM can be extended to this more general, non-Euclidean setting. We give a\npositive answer, showing that most of the elegant guarantees of the original\nmethod carry over to arbitrary norm geometries. Along the way, we clarify which\nproperties are preserved and which necessarily break down when leaving the\nEuclidean realm. Our analysis positions Non-Euclidean BPM as a conceptual\nblueprint for understanding a broad class of geometry-aware optimization\nalgorithms, shedding light on the principles behind their practical\neffectiveness.", "published": "2025-10-01 12:32:52", "link": "http://arxiv.org/abs/2510.00823v1", "categories": ["math.OC", "cs.LG", "stat.ML"], "primary_category": "math.OC"}
{"title": "Learn to Guide Your Diffusion Model", "abstract": "Classifier-free guidance (CFG) is a widely used technique for improving the\nperceptual quality of samples from conditional diffusion models. It operates by\nlinearly combining conditional and unconditional score estimates using a\nguidance weight $\\omega$. While a large, static weight can markedly improve\nvisual results, this often comes at the cost of poorer distributional\nalignment. In order to better approximate the target conditional distribution,\nwe instead learn guidance weights $\\omega_{c,(s,t)}$, which are continuous\nfunctions of the conditioning $c$, the time $t$ from which we denoise, and the\ntime $s$ towards which we denoise. We achieve this by minimizing the\ndistributional mismatch between noised samples from the true conditional\ndistribution and samples from the guided diffusion process. We extend our\nframework to reward guided sampling, enabling the model to target distributions\ntilted by a reward function $R(x_0,c)$, defined on clean data and a\nconditioning $c$. We demonstrate the effectiveness of our methodology on\nlow-dimensional toy examples and high-dimensional image settings, where we\nobserve improvements in Fr\\'echet inception distance (FID) for image\ngeneration. In text-to-image applications, we observe that employing a reward\nfunction given by the CLIP score leads to guidance weights that improve\nimage-prompt alignment.", "published": "2025-10-01 12:21:48", "link": "http://arxiv.org/abs/2510.00815v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Error Feedback for Muon and Friends", "abstract": "Recent optimizers like Muon, Scion, and Gluon have pushed the frontier of\nlarge-scale deep learning by exploiting layer-wise linear minimization oracles\n(LMOs) over non-Euclidean norm balls, capturing neural network structure in\nways traditional algorithms cannot. Yet, no principled distributed framework\nexists for these methods, and communication bottlenecks remain unaddressed. The\nvery few distributed variants are heuristic, with no convergence guarantees in\nsight. We introduce EF21-Muon, the first communication-efficient, non-Euclidean\nLMO-based optimizer with rigorous convergence guarantees. EF21-Muon supports\nstochastic gradients, momentum, and bidirectional compression with error\nfeedback-marking the first extension of error feedback beyond the Euclidean\nsetting. It recovers Muon/Scion/Gluon when compression is off and specific\nnorms are chosen, providing the first efficient distributed implementation of\nthis powerful family. Our theory covers non-Euclidean smooth and the more\ngeneral $(L^0, L^1)$-smooth setting, matching best-known Euclidean rates and\nenabling faster convergence under suitable norm choices. We further extend the\nanalysis to layer-wise (generalized) smoothness regimes, capturing the\nanisotropic structure of deep networks. Experiments on NanoGPT benchmarking\nEF21-Muon against uncompressed Muon/Scion/Gluon demonstrate up to $7\\times$\ncommunication savings with no accuracy degradation.", "published": "2025-10-01 08:20:08", "link": "http://arxiv.org/abs/2510.00643v1", "categories": ["cs.LG", "math.OC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Guaranteed Noisy CP Tensor Recovery via Riemannian Optimization on the Segre Manifold", "abstract": "Recovering a low-CP-rank tensor from noisy linear measurements is a central\nchallenge in high-dimensional data analysis, with applications spanning tensor\nPCA, tensor regression, and beyond. We exploit the intrinsic geometry of\nrank-one tensors by casting the recovery task as an optimization problem over\nthe Segre manifold, the smooth Riemannian manifold of rank-one tensors. This\ngeometric viewpoint yields two powerful algorithms: Riemannian Gradient Descent\n(RGD) and Riemannian Gauss-Newton (RGN), each of which preserves feasibility at\nevery iteration. Under mild noise assumptions, we prove that RGD converges at a\nlocal linear rate, while RGN exhibits an initial local quadratic convergence\nphase that transitions to a linear rate as the iterates approach the\nstatistical noise floor. Extensive synthetic experiments validate these\nconvergence guarantees and demonstrate the practical effectiveness of our\nmethods.", "published": "2025-10-01 06:44:52", "link": "http://arxiv.org/abs/2510.00569v1", "categories": ["stat.ML", "cs.LG", "math.OC", "math.ST", "stat.ME", "stat.TH", "90C26 (Primary) 15A69, 62F10, 62J05, 62H25 (Secondary)"], "primary_category": "stat.ML"}
{"title": "Mathematical Theory of Collinearity Effects on Machine Learning Variable Importance Measures", "abstract": "In many machine learning problems, understanding variable importance is a\ncentral concern. Two common approaches are Permute-and-Predict (PaP), which\nrandomly permutes a feature in a validation set, and Leave-One-Covariate-Out\n(LOCO), which retrains models after permuting a training feature. Both methods\ndeem a variable important if predictions with the original data substantially\noutperform those with permutations. In linear regression, empirical studies\nhave linked PaP to regression coefficients and LOCO to $t$-statistics, but a\nformal theory has been lacking. We derive closed-form expressions for both\nmeasures, expressed using square-root transformations. PaP is shown to be\nproportional to the coefficient and predictor variability: $\\text{PaP}_i =\n\\beta_i \\sqrt{2\\operatorname{Var}(\\mathbf{x}^v_i)}$, while LOCO is proportional\nto the coefficient but dampened by collinearity (captured by $\\Delta$):\n$\\text{LOCO}_i = \\beta_i (1 -\\Delta)\\sqrt{1 + c}$. These derivations explain\nwhy PaP is largely unaffected by multicollinearity, whereas LOCO is highly\nsensitive to it. Monte Carlo simulations confirm these findings across varying\nlevels of collinearity. Although derived for linear regression, we also show\nthat these results provide reasonable approximations for models like Random\nForests. Overall, this work establishes a theoretical basis for two widely used\nimportance measures, helping analysts understand how they are affected by the\ntrue coefficients, dimension, and covariance structure. This work bridges\nempirical evidence and theory, enhancing the interpretability and application\nof variable importance measures.", "published": "2025-10-01 06:18:57", "link": "http://arxiv.org/abs/2510.00557v1", "categories": ["math.ST", "stat.ML", "stat.TH", "62R07, 62F07, 68T09, 62J05, 62F40"], "primary_category": "math.ST"}
{"title": "Bayesian Neural Networks for Functional ANOVA model", "abstract": "With the increasing demand for interpretability in machine learning,\nfunctional ANOVA decomposition has gained renewed attention as a principled\ntool for breaking down high-dimensional function into low-dimensional\ncomponents that reveal the contributions of different variable groups.\nRecently, Tensor Product Neural Network (TPNN) has been developed and applied\nas basis functions in the functional ANOVA model, referred to as ANOVA-TPNN. A\ndisadvantage of ANOVA-TPNN, however, is that the components to be estimated\nmust be specified in advance, which makes it difficult to incorporate\nhigher-order TPNNs into the functional ANOVA model due to computational and\nmemory constraints. In this work, we propose Bayesian-TPNN, a Bayesian\ninference procedure for the functional ANOVA model with TPNN basis functions,\nenabling the detection of higher-order components with reduced computational\ncost compared to ANOVA-TPNN. We develop an efficient MCMC algorithm and\ndemonstrate that Bayesian-TPNN performs well by analyzing multiple benchmark\ndatasets. Theoretically, we prove that the posterior of Bayesian-TPNN is\nconsistent.", "published": "2025-10-01 06:04:11", "link": "http://arxiv.org/abs/2510.00545v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Private Realizable-to-Agnostic Transformation with Near-Optimal Sample Complexity", "abstract": "The realizable-to-agnostic transformation (Beimel et al., 2015; Alon et al.,\n2020) provides a general mechanism to convert a private learner in the\nrealizable setting (where the examples are labeled by some function in the\nconcept class) to a private learner in the agnostic setting (where no\nassumptions are imposed on the data). Specifically, for any concept class\n$\\mathcal{C}$ and error parameter $\\alpha$, a private realizable learner for\n$\\mathcal{C}$ can be transformed into a private agnostic learner while only\nincreasing the sample complexity by\n$\\widetilde{O}(\\mathrm{VC}(\\mathcal{C})/\\alpha^2)$, which is essentially tight\nassuming a constant privacy parameter $\\varepsilon = \\Theta(1)$. However, when\n$\\varepsilon$ can be arbitrary, one has to apply the standard\nprivacy-amplification-by-subsampling technique (Kasiviswanathan et al., 2011),\nresulting in a suboptimal extra sample complexity of\n$\\widetilde{O}(\\mathrm{VC}(\\mathcal{C})/\\alpha^2\\varepsilon)$ that involves a\n$1/\\varepsilon$ factor.\n  In this work, we give an improved construction that eliminates the dependence\non $\\varepsilon$, thereby achieving a near-optimal extra sample complexity of\n$\\widetilde{O}(\\mathrm{VC}(\\mathcal{C})/\\alpha^2)$ for any $\\varepsilon\\le 1$.\nMoreover, our result reveals that in private agnostic learning, the privacy\ncost is only significant for the realizable part. We also leverage our\ntechnique to obtain a nearly tight sample complexity bound for the private\nprediction problem, resolving an open question posed by Dwork and Feldman\n(2018) and Dagan and Feldman (2020).", "published": "2025-10-01 04:49:43", "link": "http://arxiv.org/abs/2510.01291v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "On the joint observability of flow fields and particle properties from Lagrangian trajectories: evidence from neural data assimilation", "abstract": "We numerically investigate the joint observability of flow states and unknown\nparticle properties from Lagrangian particle tracking (LPT) data. LPT offers\ntime-resolved, volumetric measurements of particle trajectories, but\nexperimental tracks are spatially sparse, potentially noisy, and may be further\ncomplicated by inertial transport, raising the question of whether both\nEulerian fields and particle characteristics can be reliably inferred. To\naddress this, we develop a data assimilation framework that couples an Eulerian\nflow representation with Lagrangian particle models, enabling the simultaneous\ninference of carrier fields and particle properties under the governing\nequations of disperse multiphase flow. Using this approach, we establish\nempirical existence proofs of joint observability across three representative\nregimes. In a turbulent boundary layer with noisy tracer tracks (St to 0), flow\nstates and true particle positions are jointly observable. In homogeneous\nisotropic turbulence seeded with inertial particles (St ~ 1-5), we demonstrate\nsimultaneous recovery of flow states and particle diameters, showing the\nfeasibility of implicit particle characterization. In a compressible,\nshock-dominated flow, we report the first joint reconstructions of velocity,\npressure, density, and inertial particle properties (diameter and density),\nhighlighting both the potential and certain limits of observability in\nsupersonic regimes. Systematic sensitivity studies further reveal how seeding\ndensity, noise level, and Stokes number govern reconstruction accuracy,\nyielding practical guidelines for experimental design. Taken together, these\nresults show that the scope of LPT could be broadened to multiphase and\nhigh-speed flows, in which tracer and measurement fidelity are limited.", "published": "2025-10-01 04:00:52", "link": "http://arxiv.org/abs/2510.00479v1", "categories": ["physics.flu-dyn", "stat.ML"], "primary_category": "physics.flu-dyn"}
{"title": "On the Adversarial Robustness of Learning-based Conformal Novelty Detection", "abstract": "This paper studies the adversarial robustness of conformal novelty detection.\nIn particular, we focus on AdaDetect, a powerful learning-based framework for\nnovelty detection with finite-sample false discovery rate (FDR) control. While\nAdaDetect provides rigorous statistical guarantees under benign conditions, its\nbehavior under adversarial perturbations remains unexplored. We first formulate\nan oracle attack setting that quantifies the worst-case degradation of FDR,\nderiving an upper bound that characterizes the statistical cost of attacks.\nThis idealized formulation directly motivates a practical and effective attack\nscheme that only requires query access to AdaDetect's output labels. Coupling\nthese formulations with two popular and complementary black-box adversarial\nalgorithms, we systematically evaluate the vulnerability of AdaDetect on\nsynthetic and real-world datasets. Our results show that adversarial\nperturbations can significantly increase the FDR while maintaining high\ndetection power, exposing fundamental limitations of current error-controlled\nnovelty detection methods and motivating the development of more robust\nalternatives.", "published": "2025-10-01 03:29:11", "link": "http://arxiv.org/abs/2510.00463v1", "categories": ["stat.ML", "cs.LG", "eess.SP", "stat.ME"], "primary_category": "stat.ML"}
{"title": "Robust Spatiotemporally Contiguous Anomaly Detection Using Tensor Decomposition", "abstract": "Anomaly detection in spatiotemporal data is a challenging problem encountered\nin a variety of applications, including video surveillance, medical imaging\ndata, and urban traffic monitoring. Existing anomaly detection methods focus\nmainly on point anomalies and cannot deal with temporal and spatial\ndependencies that arise in spatio-temporal data. Tensor-based anomaly detection\nmethods have been proposed to address this problem. Although existing methods\ncan capture dependencies across different modes, they are primarily supervised\nand do not account for the specific structure of anomalies. Moreover, these\nmethods focus mainly on extracting anomalous features without providing any\nstatistical confidence. In this paper, we introduce an unsupervised\ntensor-based anomaly detection method that simultaneously considers the sparse\nand spatiotemporally smooth nature of anomalies. The anomaly detection problem\nis formulated as a regularized robust low-rank + sparse tensor decomposition\nwhere the total variation of the tensor with respect to the underlying spatial\nand temporal graphs quantifies the spatiotemporal smoothness of the anomalies.\nOnce the anomalous features are extracted, we introduce a statistical anomaly\nscoring framework that accounts for local spatio-temporal dependencies. The\nproposed framework is evaluated on both synthetic and real data.", "published": "2025-10-01 03:25:44", "link": "http://arxiv.org/abs/2510.00460v1", "categories": ["cs.LG", "stat.ME", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Progressively Sampled Equality-Constrained Optimization", "abstract": "An algorithm is proposed, analyzed, and tested for solving continuous\nnonlinear-equality-constrained optimization problems where the constraints are\ndefined by an expectation or an average over a large (finite) number of terms.\nThe main idea of the algorithm is to solve a sequence of equality-constrained\nproblems, each involving a finite sample of constraint-function terms, over\nwhich the sample set grows progressively. Under assumptions about the\nconstraint functions and their first- and second-order derivatives that are\nreasonable in some real-world settings of interest, it is shown that -- with a\nsufficiently large initial sample -- solving a sequence of problems defined\nthrough progressive sampling yields a better worst-case sample complexity bound\ncompared to solving a single problem with a full set of samples. The results of\nnumerical experiments with a set of test problems demonstrate that the proposed\napproach can be effective in practice.", "published": "2025-10-01 01:58:17", "link": "http://arxiv.org/abs/2510.00417v1", "categories": ["math.OC", "cs.LG", "stat.ML"], "primary_category": "math.OC"}
{"title": "Train on Validation (ToV): Fast data selection with applications to fine-tuning", "abstract": "State-of-the-art machine learning often follows a two-stage process:\n$(i)$~pre-training on large, general-purpose datasets; $(ii)$~fine-tuning on\ntask-specific data. In fine-tuning, selecting training examples that closely\nreflect the target distribution is crucial. However, it is often the case that\nonly a few samples are available from the target distribution. Existing data\nselection methods treat these target samples as a validation set and estimate\nthe effect of adding or removing a single sample from the training pool by\nperforming inference on the validation set.\n  We propose a simpler and faster alternative that inverts the usual role of\ntrain and validation: we perform inference on the training pool before and\nafter fine-tuning on the validation set. We then select samples whose\npredictions change the most. Our key insight is that the training samples most\naffected by fine-tuning on a small validation set tend to be the most\nbeneficial for reducing test loss on the target distribution. Experiments on\ninstruction tuning and named entity recognition tasks show that, in most cases,\nour method achieves lower test log-loss than state-of-the-art approaches. We\nsupport our findings with theoretical analysis.", "published": "2025-10-01 00:55:39", "link": "http://arxiv.org/abs/2510.00386v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "CINDES: Classification induced neural density estimator and simulator", "abstract": "Neural network-based methods for (un)conditional density estimation have\nrecently gained substantial attention, as various neural density estimators\nhave outperformed classical approaches in real-data experiments. Despite these\nempirical successes, implementation can be challenging due to the need to\nensure non-negativity and unit-mass constraints, and theoretical understanding\nremains limited. In particular, it is unclear whether such estimators can\nadaptively achieve faster convergence rates when the underlying density\nexhibits a low-dimensional structure. This paper addresses these gaps by\nproposing a structure-agnostic neural density estimator that is (i)\nstraightforward to implement and (ii) provably adaptive, attaining faster rates\nwhen the true density admits a low-dimensional composition structure. Another\nkey contribution of our work is to show that the proposed estimator integrates\nnaturally into generative sampling pipelines, most notably score-based\ndiffusion models, where it achieves provably faster convergence when the\nunderlying density is structured. We validate its performance through extensive\nsimulations and a real-data application.", "published": "2025-10-01 00:21:37", "link": "http://arxiv.org/abs/2510.00367v1", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH", "62G08"], "primary_category": "stat.ML"}
{"title": "RealClass: A Framework for Classroom Speech Simulation with Public Datasets and Game Engines", "abstract": "The scarcity of large-scale classroom speech data has hindered the\ndevelopment of AI-driven speech models for education. Classroom datasets remain\nlimited and not publicly available, and the absence of dedicated classroom\nnoise or Room Impulse Response (RIR) corpora prevents the use of standard data\naugmentation techniques.\n  In this paper, we introduce a scalable methodology for synthesizing classroom\nnoise and RIRs using game engines, a versatile framework that can extend to\nother domains beyond the classroom. Building on this methodology, we present\nRealClass, a dataset that combines a synthesized classroom noise corpus with a\nclassroom speech dataset compiled from publicly available corpora. The speech\ndata pairs a children's speech corpus with instructional speech extracted from\nYouTube videos to approximate real classroom interactions in clean conditions.\nExperiments on clean and noisy speech show that RealClass closely approximates\nreal classroom speech, making it a valuable asset in the absence of abundant\nreal classroom speech.", "published": "2025-10-01 21:04:51", "link": "http://arxiv.org/abs/2510.01462v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning Time-Graph Frequency Representation for Monaural Speech Enhancement", "abstract": "The Graph Fourier Transform (GFT) has recently demonstrated promising results\nin speech enhancement. However, existing GFT-based speech enhancement\napproaches often employ fixed graph topologies to build the graph Fourier\nbasis, whose the representation lacks the adaptively and flexibility. In\naddition, they suffer from the numerical errors and instability introduced by\nmatrix inversion in GFT based on both Singular Value Decomposition (GFT-SVD)\nand Eigen Vector Decomposition (GFT-EVD). Motivated by these limitations, this\npaper propose a simple yet effective learnable GFT-SVD framework for speech\nenhancement. Specifically, we leverage graph shift operators to construct a\nlearnable graph topology and define a learnable graph Fourier basis by the\nsingular value matrices using 1-D convolution (Conv-1D) neural layer. This\neliminates the need for matrix inversion, thereby avoiding the associated\nnumerical errors and stability problem.", "published": "2025-10-01 17:22:14", "link": "http://arxiv.org/abs/2510.01130v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Spiralformer: Low Latency Encoder for Streaming Speech Recognition with Circular Layer Skipping and Early Exiting", "abstract": "For streaming speech recognition, a Transformer-based encoder has been widely\nused with block processing. Although many studies addressed improving emission\nlatency of transducers, little work has been explored for improving encoding\nlatency of the block processing. We seek to reduce latency by frequently\nemitting a chunk with a small shift rather than scarce large-chunk emissions,\nresulting in higher computational costs. To efficiently compute with the small\nchunk shift, we propose a new encoder, Spiralformer, tailored for block\nprocessing by combining layer dropping and early exiting. We skip layer\ncomputation in a cyclic manner and shift the computed layer in each block\nspirally, which completes computation for all the layers over the block\nprocessing. Experimentally, we observed that our method achieved 21.6%\nreduction in the averaged token emission delay in Librispeech, and 7.0% in CSJ,\ncompared with the baseline with similar computational cost and word error\nrates.", "published": "2025-10-01 14:56:45", "link": "http://arxiv.org/abs/2510.00982v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CL-UZH submission to the NIST SRE 2024 Speaker Recognition Evaluation", "abstract": "The CL-UZH team submitted one system each for the fixed and open conditions\nof the NIST SRE 2024 challenge. For the closed-set condition, results for the\naudio-only trials were achieved using the X-vector system developed with Kaldi.\nFor the audio-visual results we used only models developed for the visual\nmodality. Two sets of results were submitted for the open-set and closed-set\nconditions, one based on a pretrained model using the VoxBlink2 and VoxCeleb2\ndatasets. An Xvector-based model was trained from scratch using the CTS\nsuperset dataset for the closed set. In addition to the submission of the\nresults of the SRE24 evaluation to the competition website, we talked about the\nperformance of the proposed systems on the SRE24 evaluation in this report.", "published": "2025-10-01 14:27:00", "link": "http://arxiv.org/abs/2510.00952v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Robust Proactive Communication Strategy for Distributed Active Noise Control Systems", "abstract": "Distributed multichannel active noise control (DMCANC) systems assign the\nhigh computational load of conventional centralized algorithms across multiple\nprocessing nodes, leveraging inter-node communication to collaboratively\nsuppress unwanted noise. However, communication overhead can undermine\nalgorithmic stability and degrade overall performance. To address this\nchallenge, we propose a robust communication framework that integrates\nadaptive-fixed-filter switching and the mixed-gradient combination strategy. In\nthis approach, each node independently executes a single-channel filtered\nreference least mean square (FxLMS) algorithm while monitoring real-time noise\nreduction levels. When the current noise reduction performance degrades\ncompared to the previous state, the node halts its adaptive algorithm, switches\nto a fixed filter, and simultaneously initiates a communication request. The\nexchanged information comprises the difference between the current control\nfilter and the filter at the time of the last communication, equivalent to the\naccumulated gradient sum during non-communication intervals. Upon receiving\nneighboring cumulative gradients, the node employs a mixed-gradient combination\nmethod to update its control filter, subsequently reverting to the adaptive\nmode. This proactive communication strategy and adaptive-fixed switching\nmechanism ensure system robustness by mitigating instability risks caused by\ncommunication issues. Simulations demonstrate that the proposed method achieves\nnoise reduction performance comparable to centralized algorithms while\nmaintaining stability under communication constraints, highlighting its\npractical applicability in real-world distributed ANC scenarios.", "published": "2025-10-01 14:14:53", "link": "http://arxiv.org/abs/2510.00934v1", "categories": ["eess.SP", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Reconstruction of the Complete Vocal Tract Contour Through Acoustic to Articulatory Inversion Using Real-Time MRI Data", "abstract": "Acoustic to articulatory inversion has often been limited to a small part of\nthe vocal tract because the data are generally EMA (ElectroMagnetic\nArticulography) data requiring sensors to be glued to easily accessible\narticulators. The presented acoustic to articulation model focuses on the\ninversion of the entire vocal tract from the glottis, the complete tongue, the\nvelum, to the lips. It relies on a realtime dynamic MRI database of more than 3\nhours of speech. The data are the denoised speech signal and the automatically\nsegmented articulator contours. Several bidirectional LSTM-based approaches\nhave been used, either inverting each articulator individually or inverting all\narticulators simultaneously. To our knowledge, this is the first complete\ninversion of the vocal tract. The average RMSE precision on the test set is\n1.65 mm to be compared with the pixel size which is 1.62 mm.", "published": "2025-10-01 13:55:56", "link": "http://arxiv.org/abs/2510.00914v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "UniverSR: Unified and Versatile Audio Super-Resolution via Vocoder-Free Flow Matching", "abstract": "In this paper, we present a vocoder-free framework for audio super-resolution\nthat employs a flow matching generative model to capture the conditional\ndistribution of complex-valued spectral coefficients. Unlike conventional\ntwo-stage diffusion-based approaches that predict a mel-spectrogram and then\nrely on a pre-trained neural vocoder to synthesize waveforms, our method\ndirectly reconstructs waveforms via the inverse Short-Time Fourier Transform\n(iSTFT), thereby eliminating the dependence on a separate vocoder. This design\nnot only simplifies end-to-end optimization but also overcomes a critical\nbottleneck of two-stage pipelines, where the final audio quality is\nfundamentally constrained by vocoder performance. Experiments show that our\nmodel consistently produces high-fidelity 48 kHz audio across diverse\nupsampling factors, achieving state-of-the-art performance on both speech and\ngeneral audio datasets.", "published": "2025-10-01 11:04:53", "link": "http://arxiv.org/abs/2510.00771v1", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "From Scores to Preferences: Redefining MOS Benchmarking for Speech Quality Reward Modeling", "abstract": "Assessing the perceptual quality of synthetic speech is crucial for guiding\nthe development and refinement of speech generation models. However, it has\ntraditionally relied on human subjective ratings such as the Mean Opinion Score\n(MOS), which depend on manual annotations and often suffer from inconsistent\nrating standards and poor reproducibility. To address these limitations, we\nintroduce MOS-RMBench, a unified benchmark that reformulates diverse MOS\ndatasets into a preference-comparison setting, enabling rigorous evaluation\nacross different datasets. Building on MOS-RMBench, we systematically construct\nand evaluate three paradigms for reward modeling: scalar reward models,\nsemi-scalar reward models, and generative reward models (GRMs). Our experiments\nreveal three key findings: (1) scalar models achieve the strongest overall\nperformance, consistently exceeding 74% accuracy; (2) most models perform\nconsiderably worse on synthetic speech than on human speech; and (3) all models\nstruggle on pairs with very small MOS differences. To improve performance on\nthese challenging pairs, we propose a MOS-aware GRM that incorporates an\nMOS-difference-based reward function, enabling the model to adaptively scale\nrewards according to the difficulty of each sample pair. Experimental results\nshow that the MOS-aware GRM significantly improves fine-grained quality\ndiscrimination and narrows the gap with scalar models on the most challenging\ncases. We hope this work will establish both a benchmark and a methodological\nframework to foster more rigorous and scalable research in automatic speech\nquality assessment.", "published": "2025-10-01 10:27:51", "link": "http://arxiv.org/abs/2510.00743v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation", "abstract": "Recently, an increasing number of multimodal (text and audio) benchmarks have\nemerged, primarily focusing on evaluating models' understanding capability.\nHowever, exploration into assessing generative capabilities remains limited,\nespecially for open-ended long-form content generation. Significant challenges\nlie in no reference standard answer, no unified evaluation metrics and\nuncontrollable human judgments. In this work, we take podcast-like audio\ngeneration as a starting point and propose PodEval, a comprehensive and\nwell-designed open-source evaluation framework. In this framework: 1) We\nconstruct a real-world podcast dataset spanning diverse topics, serving as a\nreference for human-level creative quality. 2) We introduce a multimodal\nevaluation strategy and decompose the complex task into three dimensions: text,\nspeech and audio, with different evaluation emphasis on \"Content\" and \"Format\".\n3) For each modality, we design corresponding evaluation methods, involving\nboth objective metrics and subjective listening test. We leverage\nrepresentative podcast generation systems (including open-source, close-source,\nand human-made) in our experiments. The results offer in-depth analysis and\ninsights into podcast generation, demonstrating the effectiveness of PodEval in\nevaluating open-ended long-form audio. This project is open-source to\nfacilitate public use: https://github.com/yujxx/PodEval.", "published": "2025-10-01 04:08:08", "link": "http://arxiv.org/abs/2510.00485v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SAGE-Music: Low-Latency Symbolic Music Generation via Attribute-Specialized Key-Value Head Sharing", "abstract": "Low-latency symbolic music generation is essential for real-time\nimprovisation and human-AI co-creation. Existing transformer-based models,\nhowever, face a trade-off between inference speed and musical quality.\nTraditional acceleration techniques such as embedding pooling significantly\ndegrade quality, while recently proposed Byte Pair Encoding (BPE) methods -\nthough effective on single-track piano data - suffer large performance drops in\nmulti-track settings, as revealed by our analysis. We propose\nAttribute-Specialized Key-Value Head Sharing (AS-KVHS), adapted to music's\nstructured symbolic representation, achieving about 30% inference speedup with\nonly a negligible (about 0.4%) quality drop in objective evaluations and slight\nimprovements in subjective listening tests. Our main contributions are (1) the\nfirst systematic study of BPE's generalizability in multi-track symbolic music,\nand (2) the introduction of AS-KVHS for low-latency symbolic music generation.\nBeyond these, we also release SAGE-Music, an open-source benchmark that matches\nor surpasses state-of-the-art models in generation quality.", "published": "2025-10-01 01:11:43", "link": "http://arxiv.org/abs/2510.00395v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Meta-Learning-Driven Resource Optimization in Full-Duplex ISAC with Movable Antennas", "abstract": "This paper investigates a full-duplex (FD) scenario where a base station (BS)\nequipped with movable antennas (MAs) simultaneously provides communication\nservices to a set of downlink (DL) and uplink (UL) users while also enabling\nsensing functionalities for target detection, thereby supporting integrated\nsensing and communication (ISAC) technology. Additionally, a receiving BS, also\nequipped with MAs (denoted as BS R), is responsible for capturing the reflected\necho. To optimize this setup, we formulate an optimization problem aimed at\nmaximizing the signal-to-noise and interference ratio (SINR) of the captured\necho. This is achieved by jointly optimizing the transmit beamforming vectors\nat the FD BS, the receiving beamforming vectors at both the FD BS and BS R, the\nUL users' transmit power, and the MAs' positions at both BSs, all while\nsatisfying the quality-of-service (QoS) requirements for both sensing and\ncommunication. Given the non-convex nature of the problem and the high coupling\nbetween the variables, we employ a gradient-based meta-learning (GML) approach\ntailored for large-scale optimization. Numerical results demonstrate the\neffectiveness of the proposed meta-learning approach, achieving results within\n99% of the optimal solution. Furthermore, the MA-based scheme outperforms\nseveral benchmark approaches, highlighting its advantages in practical ISAC\napplications.", "published": "2025-10-01 20:20:21", "link": "http://arxiv.org/abs/2510.01437v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "A Drone-mounted Magnetometer System for Automatic Interference Removal and Landmine Detection", "abstract": "Landmines have been extensively used in conflict zones as an indiscriminate\nweapon to control military movements, often remaining active long after\nhostilities have ended. Their presence poses a persistent danger to civilians,\nhindering post-war recovery efforts, causing injuries or death, and restricting\naccess to essential land for agriculture and infrastructure. Unmanned aerial\nvehicles (UAV) equipped with magnetometers are commonly used to detect remnant\nhidden landmines but come with significant technical challenges due to magnetic\nfield interference from UAV electronics such as motors. We propose the use of a\nframe-mounted UAV-borne two-magnetometer payload to perform a two-step\nautomated interference removal and landmine detection analysis. The first step\nremoves interference via the Wavelet-Adaptive Interference Cancellation for\nUnderdetermined Platform (WAIC-UP) method designed for spaceflight\nmagnetometers. The second method uses the Rapid Unsupervised Detection of\nEvents (RUDE) algorithm to detect landmine signatures. This two-step\nWAIC-UP/RUDE approach with multiple magnetometers achieves high-fidelity\nordinance detection at a low computational cost and simplifies the design of\nmagnetic survey payloads. We validate the method through a Monte Carlo\nsimulation of randomized landmine placements in a 10 x 10 m square grid and\ndrone motor interference. Additionally, we assess the efficacy of the algorithm\nby varying the drone's altitude, examining its performance at different heights\nabove the ground.", "published": "2025-10-01 19:54:17", "link": "http://arxiv.org/abs/2510.01417v1", "categories": ["eess.SP", "physics.ins-det"], "primary_category": "eess.SP"}
{"title": "Delay-Augmented Stacked Intelligent Surfaces: Potential, Challenges, and Opportunities", "abstract": "Stacked intelligent surfaces (SIS)s have been proposed recently as an\nenabling technology for Holographic Multiple Input Multiple Output (HMIMO) and\nUltra-massive MIMO (umMIMO) technologies. Their utility can extend beyond\nspatial wave-domain processing of signals if they are enhanced with\nstrategically-tuned symbol-duration level delays to enable temporal processing\nas well. In this work, we introduce the idea of a delay-augmented SIS (DA-SIS).\nWe shed light on the feasibility of realizing delay units in an SIS. Then, we\ndiscuss the relevance of the proposed DA-SIS and present a use case that\nillustrates its potential, wherein the DA-SIS serves as an analog equalizer\nthat aids in eliminating multi-path-induced inter-symbol-interference (ISI). We\nshow how the number of elements affect the equalization process using the bit\nerror rate (BER) as a metric, and demonstrate the potential of the DA-SIS in\nequalization via comparing with digital equalizers as a benchmark. Finally, we\npresent opportunities and future research directions that can be undertaken to\nbring this idea to fruition.", "published": "2025-10-01 19:46:57", "link": "http://arxiv.org/abs/2510.01411v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Satellite Assignment Policy Learning for Coexistence in LEO Networks", "abstract": "Unlike in terrestrial cellular networks, certain frequency bands for\nlow-earth orbit (LEO) satellite systems have thus far been allocated on a\nnon-exclusive basis. In this context, systems that launch their satellites\nearlier (referred to as primary systems) are given spectrum access priority\nover those that launch later, known as secondary systems. For a secondary\nsystem to function, it is expected to either coordinate with primary systems or\nensure that it does not cause excessive interference to primary ground users.\nReliably meeting this interference constraint requires real-time knowledge of\nthe receive beams of primary users, which in turn depends on the primary\nsatellite-to-primary user associations. However, in practice, primary systems\nhave thus far not publicly disclosed their satellite assignment policies;\ntherefore, it becomes essential for secondary systems to develop methods to\ninfer such policies. Assuming there is limited historical data indicating which\nprimary satellites have served which primary users, we propose an end-to-end\ngraph structure learning-based algorithm for learning highest elevation primary\nsatellite assignment policies, that, upon deployment, can directly map the\nprimary satellite coordinates into assignment decisions for the primary users.\nSimulation results show that our method can outperform the best baseline,\nachieving approximately a 15% improvement in prediction accuracy.", "published": "2025-10-01 19:45:18", "link": "http://arxiv.org/abs/2510.01408v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "A Control Theory inspired Exploration Method for a Linear Bandit driven by a Linear Gaussian Dynamical System", "abstract": "The paper introduces a linear bandit environment where the reward is the\noutput of a known Linear Gaussian Dynamical System (LGDS). In this environment,\nwe address the fundamental challenge of balancing exploration -- gathering\ninformation about the environment -- and exploitation -- selecting to the\naction with the highest predicted reward. We propose two algorithms, Kalman\nfilter Upper Confidence Bound (Kalman-UCB) and Information filter Directed\nExploration Action-selection (IDEA). Kalman-UCB uses the principle of optimism\nin the face of uncertainty. IDEA selects actions that maximize the combination\nof the predicted reward and a term that quantifies how much an action minimizes\nthe error of the Kalman filter state prediction, which depends on the LGDS\nproperty called observability. IDEA is motivated by applications such as\nhyperparameter optimization in machine learning. A major problem encountered in\nhyperparameter optimization is the large action spaces, which hinder the\nperformance of methods inspired by principle of optimism in the face of\nuncertainty as they need to explore each action to lower reward prediction\nuncertainty. To predict if either Kalman-UCB or IDEA will perform better, a\nmetric based on the LGDS properties is provided. This metric is validated with\nnumerical results across a variety of randomly generated environments.", "published": "2025-10-01 18:48:24", "link": "http://arxiv.org/abs/2510.01364v1", "categories": ["eess.SY", "cs.SY", "eess.SP"], "primary_category": "eess.SY"}
{"title": "Graph Neural Networks in Large Scale Wireless Communication Networks: Scalability Across Random Geometric Graphs", "abstract": "The growing complexity of wireless systems has accelerated the move from\ntraditional methods to learning-based solutions. Graph Neural Networks (GNNs)\nare especially well-suited here, since wireless networks can be naturally\nrepresented as graphs. A key property of GNNs is transferability: models\ntrained on one graph often generalize to much larger graphs with little\nperformance loss. While empirical studies have shown that GNN-based wireless\npolicies transfer effectively, existing theoretical guarantees do not capture\nthis phenomenon. Most works focus on dense graphs where node degrees scale with\nnetwork size, an assumption that fails in wireless systems. In this work, we\nprovide a formal theoretical foundation for transferability on Random Geometric\nGraphs (RGGs), a sparse and widely used model of wireless networks. We further\nvalidate our results through numerical experiments on power allocation, a\nfundamental resource management task.", "published": "2025-10-01 13:38:55", "link": "http://arxiv.org/abs/2510.00896v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Agentic AI meets Neural Architecture Search: Proactive Traffic Prediction for AI-RAN", "abstract": "Next-generation wireless networks require intelligent traffic prediction to\nenable autonomous resource management and handle diverse, dynamic service\ndemands. The Open Radio Access Network (O-RAN) framework provides a promising\nfoundation for embedding machine learning intelligence through its\ndisaggregated architecture and programmable interfaces. This work applies a\nNeural Architecture Search (NAS)-based framework that dynamically selects and\norchestrates efficient Long Short-Term Memory (LSTM) architectures for traffic\nprediction in O-RAN environments. Our approach leverages the O-RAN paradigm by\nseparating architecture optimisation (via non-RT RIC rApps) from real-time\ninference (via near-RT RIC xApps), enabling adaptive model deployment based on\ntraffic conditions and resource constraints. Experimental evaluation across six\nLSTM architectures demonstrates that lightweight models achieve $R^2 \\approx\n0.91$--$0.93$ with high efficiency for regular traffic, while complex models\nreach near-perfect accuracy ($R^2 = 0.989$--$0.996$) during critical scenarios.\nOur NAS-based orchestration achieves a 70-75\\% reduction in computational\ncomplexity compared to static high-performance models, while maintaining high\nprediction accuracy when required, thereby enabling scalable deployment in\nreal-world edge environments.", "published": "2025-10-01 13:05:59", "link": "http://arxiv.org/abs/2510.00851v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Effectiveness of Reconfigurable Intelligent Surface in Multipath Fading Channel", "abstract": "A method of simulating a single-input single-output reconfigurable\nintelligent surface (RIS) assisted channel is presented using three channel\nblack boxes to represent the direct signal path, the transmit path to the RIS\nand the reflected path from the RIS. The complex coefficients for each channel\nbox is obtained by ray tracing in a scenario with geographic terrain\ninformation that also contains approximate building shapes. The electrical\ncharacteristics of the ground and building walls were also accounted for in the\nray tracing function. Simulations were conducted with reflected rays only and\nreflected rays together with diffracted rays. The received power exhibits\nvariations typical of multipath fading environments. In the best locations, the\nRIS-assisted channel simulation result agrees well with theoretical models, the\nperformance increasing by the RIS size squared as the number of RIS elements is\nincreased. In the simplified theoretical model where the transmitter and\nreceiver are inline and the RIS orthogonal but much closer than the distance\nbetween the former elements, the simulation results also corroborate best\ndeployment close the transmitter or the receiver with a U-shaped drop between\nthem.", "published": "2025-10-01 12:47:31", "link": "http://arxiv.org/abs/2510.00838v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Benchmarking Machine Learning Models for Fault Classification and Localization in Power System Protection", "abstract": "The increasing integration of distributed energy resources (DERs),\nparticularly renewables, poses significant challenges for power system\nprotection, with fault classification (FC) and fault localization (FL) being\namong the most critical tasks. Conventional protection schemes, based on fixed\nthresholds, cannot reliably identify and localize short circuits with the\nincreasing complexity of the grid under dynamic conditions. Machine learning\n(ML) offers a promising alternative; however, systematic benchmarks across\nmodels and settings remain limited. This work presents, for the first time, a\ncomparative benchmarking study of classical ML models for FC and FL in power\nsystem protection based on EMT data. Using voltage and current waveforms\nsegmented into sliding windows of 10 ms to 50 ms, we evaluate models under\nrealistic real-time constraints. Performance is assessed in terms of accuracy,\nrobustness to window size, and runtime efficiency. The best-performing FC model\nachieved an F1 score of 0.992$\\pm$0.001, while the top FL model reached an R2\nof 0.806$\\pm$0.008 with a mean processing time of 0.563 ms.", "published": "2025-10-01 12:44:14", "link": "http://arxiv.org/abs/2510.00831v1", "categories": ["cs.AI", "cs.LG", "eess.SP"], "primary_category": "cs.AI"}
{"title": "Null-Shaping for Interference Mitigation in LEO Satellites Under Location Uncertainty", "abstract": "Radio frequency interference (RFI) poses a growing challenge to satellite\ncommunications, particularly in uplink channels of Low Earth Orbit (LEO)\nsystems, due to increasing spectrum congestion and uncertainty in the location\nof terrestrial interferers. This paper addresses the impact of RFI source\nposition uncertainty on beamforming-based interference mitigation. First, we\nanalytically characterize how geographic uncertainty in RFI location translates\ninto angular deviation as observed from the satellite. Building on this, we\npropose a robust null-shaping framework to increase resilience in the\ncommunication links by incorporating the probability density function (PDF) of\nthe RFI location uncertainty into the beamforming design via stochastic\noptimization. This allows adaptive shaping of the antenna array's nulling\npattern to enhance interference suppression under uncertainty. Extensive Monte\nCarlo simulations, incorporating realistic satellite orbital dynamics and\nvarious RFI scenarios, demonstrate that the proposed approach achieves\nsignificantly improved mitigation performance compared to conventional\ndeterministic designs.", "published": "2025-10-01 12:25:44", "link": "http://arxiv.org/abs/2510.00816v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Machine Learning-based Path Loss Prediction in Suburban Environment in the Sub-6 GHz Band", "abstract": "Accurate path loss (PL) prediction is crucial for successful network\nplanning, antenna design, and performance optimization in wireless\ncommunication systems. Several conventional approaches for PL prediction have\nbeen adopted, but they have been demonstrated to lack flexibility and accuracy.\nIn this work, we investigate the effectiveness of Machine Learning (ML) models\nin predicting PL, particularly for the sub-6 GHz band in a suburban campus of\nKing Abdullah University of Science and Technology (KAUST). For training\npurposes, we generate synthetic datasets using the ray-tracing simulation\ntechnique. The feasibility and accuracy of the ML-based PL models are verified\nand validated using both synthetic and measurement datasets. The random forest\nregression (RFR) and the K-nearest neighbors (KNN) algorithms provide the best\nPL prediction accuracy compared to other ML models. In addition, we compare the\nperformance of the developed ML-based PL models with the traditional\npropagation models, including COST-231 Hata, Longley-Rice, and Close-in models.\nThe results show the superiority of the ML-based PL models compared to\nconventional models. Therefore, the ML approach using the ray-tracing technique\ncan provide a promising and cost-effective solution for predicting and modeling\nradio wave propagation in various scenarios in a flexible manner.", "published": "2025-10-01 09:17:03", "link": "http://arxiv.org/abs/2510.00696v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Radiation Pattern Reconfigurable FAS-Empowered Interference-Resilient UAV Communication", "abstract": "The widespread use of uncrewed aerial vehicles (UAVs) has propelled the\ndevelopment of advanced techniques on countering unauthorized UAV flights.\nHowever, the resistance of legal UAVs to illegal interference remains\nunder-addressed. This paper proposes radiation pattern reconfigurable fluid\nantenna systems (RPR-FAS)-empowered interference-resilient UAV communication\nscheme. This scheme integrates the reconfigurable pixel antenna technology,\nwhich provides each antenna with an adjustable radiation pattern. Therefore,\nRPR-FAS can enhance the angular resolution of a UAV with a limited number of\nantennas, thereby improving spectral efficiency (SE) and interference\nresilience. Specifically, we first design dedicated radiation pattern adapted\nfrom 3GPP-TR-38.901, where the beam direction and half power beamwidth are\ntailored for UAV communications. Furthermore, we propose a low-storage-overhead\northogonal matching pursuit multiple measurement vectors algorithm, which\naccurately estimates the angle-of-arrival (AoA) of the communication link, even\nin the single antenna case. Particularly, by utilizing the Fourier transform to\nthe radiation pattern gain matrix, we design a dimension-reduction technique to\nachieve 1--2 order-of-magnitude reduction in storage requirements. Meanwhile,\nwe propose a maximum likelihood interference AoA estimation method based on the\nlaw of large numbers, so that the SE can be further improved. Finally,\nalternating optimization is employed to obtain the optimal uplink radiation\npattern and combiner, while an exhaustive search is applied to determine the\noptimal downlink pattern, complemented by the water-filling algorithm for\nbeamforming. Comprehensive simulations demonstrate that the proposed schemes\noutperform traditional methods in terms of angular sensing precision and\nspectral efficiency.", "published": "2025-10-01 07:01:03", "link": "http://arxiv.org/abs/2510.00581v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Geometric Spatio-Spectral Total Variation for Hyperspectral Image Denoising and Destriping", "abstract": "This article proposes a novel regularization method, named Geometric\nSpatio-Spectral Total Variation (GeoSSTV), for hyperspectral (HS) image\ndenoising and destriping. HS images are inevitably affected by various types of\nnoise due to the measurement equipment and environment. Total Variation\n(TV)-based regularization methods that model the spatio-spectral piecewise\nsmoothness inherent in HS images are promising approaches for HS image\ndenoising and destriping. However, existing TV-based methods are based on\nclassical anisotropic and isotropic TVs, which cause staircase artifacts and\nlack rotation invariance, respectively, making it difficult to accurately\nrecover round structures and oblique edges. To address this issue, GeoSSTV\nintroduces a geometrically consistent formulation of TV that measures\nvariations across all directions in a Euclidean manner. Through this\nformulation, GeoSSTV removes noise while preserving round structures and\noblique edges. Furthermore, we formulate the HS image denoising problem as a\nconstrained convex optimization problem involving GeoSSTV and develop an\nefficient algorithm based on a preconditioned primal-dual splitting method.\nExperimental results on HS images contaminated with mixed noise demonstrate the\nsuperiority of the proposed method over existing approaches.", "published": "2025-10-01 06:27:10", "link": "http://arxiv.org/abs/2510.00562v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Investigation of Using Non-Contact Electrodes for Fetal ECG Monitoring", "abstract": "Regular physiological monitoring of maternal and fetal parameters is\nindispensable for ensuring safe outcomes during pregnancy and parturition.\nFetal electrocardiogram (fECG) assessment is crucial to detect fetal distress\nand developmental anomalies. Given challenges of prenatal care due to the lack\nof medical professionals and the limit of accessibility, especially in remote\nand resource-poor areas, we develop a fECG monitoring system using novel\nnon-contact electrodes (NCE) to record the fetal/maternal ECG (f/mECG) signals\nthrough clothes, thereby improving the comfort during measurement. The system\nis designed to be incorporated inside a maternity belt with data acquisition,\ndata transmission module as well as novel NCEs. Thorough characterizations were\ncarried out to evaluate the novel NCE against traditional wet electrodes (i.e.,\nAg/AgCl electrodes), showing comparable performance. A successful {preliminary\npilot feasibility study} conducted with pregnant women (n = 10) between 25 and\n32 weeks of gestation demonstrates the system's performance, usability and\nsafety.", "published": "2025-10-01 06:11:31", "link": "http://arxiv.org/abs/2510.00550v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "A Point Process Model of Skin Conductance Responses in a Stroop Task for Predicting Depression and Suicidal Ideation", "abstract": "Accurate identification of mental health biomarkers can enable earlier\ndetection and objective assessment of compromised mental well-being. In this\nstudy, we analyze electrodermal activity recorded during an Emotional Stroop\ntask to capture sympathetic arousal dynamics associated with depression and\nsuicidal ideation. We model the timing of skin conductance responses as a point\nprocess whose conditional intensity is modulated by task-based covariates,\nincluding stimulus valence, reaction time, and response accuracy. The resulting\nsubject-specific parameter vector serves as input to a machine learning\nclassifier for distinguishing individuals with and without depression. Our\nresults show that the model parameters encode meaningful physiological\ndifferences associated with depressive symptomatology and yield superior\nclassification performance compared to conventional feature extraction methods.", "published": "2025-10-01 02:02:16", "link": "http://arxiv.org/abs/2510.00422v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Semantic-Driven AI Agent Communications: Challenges and Solutions", "abstract": "With the rapid growth of intelligent services, communication targets are\nshifting from humans to artificial intelligent (AI) agents, which require new\nparadigms to enable real-time perception, decision-making, and collaboration.\nSemantic communication, which conveys task-relevant meaning rather than raw\ndata, offers a promising solution. However, its practical deployment remains\nconstrained by dynamic environments and limited resources. To address these\nissues, this article proposes a semantic-driven AI agent communication\nframework and develops three enabling techniques. First, semantic adaptation\ntransmission applies fine-tuning with real or generative samples to efficiently\nadapt models to varying environments. Second, semantic lightweight transmission\nincorporates pruning, quantization, and perception-aware sampling to reduce\nmodel complexity and alleviate computational burden on edge agents. Third,\nsemantic self-evolution control employs distributed hierarchical\ndecision-making to optimize multi-dimensional resources, enabling robust\nmulti-agent collaboration in dynamic environments. Simulation results show that\nthe proposed solutions achieve faster convergence and stronger robustness,\nwhile the proposed distributed hierarchical optimization method significantly\noutperforms conventional decision-making schemes, highlighting its potential\nfor AI agent communication networks.", "published": "2025-10-01 00:52:37", "link": "http://arxiv.org/abs/2510.00381v1", "categories": ["cs.AI", "eess.SP"], "primary_category": "cs.AI"}
{"title": "Ultra-Efficient Decoding for End-to-End Neural Compression and Reconstruction", "abstract": "Image compression and reconstruction are crucial for various digital\napplications. While contemporary neural compression methods achieve impressive\ncompression rates, the adoption of such technology has been largely hindered by\nthe complexity and large computational costs of the convolution-based decoders\nduring data reconstruction. To address the decoder bottleneck in neural\ncompression, we develop a new compression-reconstruction framework based on\nincorporating low-rank representation in an autoencoder with vector\nquantization. We demonstrated that performing a series of computationally\nefficient low-rank operations on the learned latent representation of images\ncan efficiently reconstruct the data with high quality. Our approach\ndramatically reduces the computational overhead in the decoding phase of neural\ncompression/reconstruction, essentially eliminating the decoder compute\nbottleneck while maintaining high fidelity of image outputs.", "published": "2025-10-01 19:42:59", "link": "http://arxiv.org/abs/2510.01407v2", "categories": ["cs.LG", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Satellite Assignment Policy Learning for Coexistence in LEO Networks", "abstract": "Unlike in terrestrial cellular networks, certain frequency bands for\nlow-earth orbit (LEO) satellite systems have thus far been allocated on a\nnon-exclusive basis. In this context, systems that launch their satellites\nearlier (referred to as primary systems) are given spectrum access priority\nover those that launch later, known as secondary systems. For a secondary\nsystem to function, it is expected to either coordinate with primary systems or\nensure that it does not cause excessive interference to primary ground users.\nReliably meeting this interference constraint requires real-time knowledge of\nthe receive beams of primary users, which in turn depends on the primary\nsatellite-to-primary user associations. However, in practice, primary systems\nhave thus far not publicly disclosed their satellite assignment policies;\ntherefore, it becomes essential for secondary systems to develop methods to\ninfer such policies. Assuming there is limited historical data indicating which\nprimary satellites have served which primary users, we propose an end-to-end\ngraph structure learning-based algorithm for learning highest elevation primary\nsatellite assignment policies, that, upon deployment, can directly map the\nprimary satellite coordinates into assignment decisions for the primary users.\nSimulation results show that our method can outperform the best baseline,\nachieving approximately a 15% improvement in prediction accuracy.", "published": "2025-10-01 19:45:18", "link": "http://arxiv.org/abs/2510.01408v2", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Radiation Pattern Reconfigurable FAS-Empowered Interference-Resilient UAV Communication", "abstract": "The widespread use of uncrewed aerial vehicles (UAVs) has propelled the\ndevelopment of advanced techniques on countering unauthorized UAV flights.\nHowever, the resistance of legal UAVs to illegal interference remains\nunder-addressed. This paper proposes radiation pattern reconfigurable fluid\nantenna systems (RPR-FAS)-empowered interference-resilient UAV communication\nscheme. This scheme integrates the reconfigurable pixel antenna technology,\nwhich provides each antenna with an adjustable radiation pattern. Therefore,\nRPR-FAS can enhance the angular resolution of a UAV with a limited number of\nantennas, thereby improving spectral efficiency (SE) and interference\nresilience. Specifically, we first design dedicated radiation pattern adapted\nfrom 3GPP-TR-38.901, where the beam direction and half power beamwidth are\ntailored for UAV communications. Furthermore, we propose a low-storage-overhead\northogonal matching pursuit multiple measurement vectors algorithm, which\naccurately estimates the angle-of-arrival (AoA) of the communication link, even\nin the single antenna case. Particularly, by utilizing the Fourier transform to\nthe radiation pattern gain matrix, we design a dimension-reduction technique to\nachieve 1--2 order-of-magnitude reduction in storage requirements. Meanwhile,\nwe propose a maximum likelihood interference AoA estimation method based on the\nlaw of large numbers, so that the SE can be further improved. Finally,\nalternating optimization is employed to obtain the optimal uplink radiation\npattern and combiner, while an exhaustive search is applied to determine the\noptimal downlink pattern, complemented by the water-filling algorithm for\nbeamforming. Comprehensive simulations demonstrate that the proposed schemes\noutperform traditional methods in terms of angular sensing precision and\nspectral efficiency.", "published": "2025-10-01 07:01:03", "link": "http://arxiv.org/abs/2510.00581v2", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Probability distributions over CSS codes: two-universality, QKD hashing, collision bounds, security", "abstract": "We characterize novel probability distributions for CSS codes. Such classes\nof error correcting codes, originally introduced by Calderbank, Shor, and\nSteane, are of great significance in advancing the fidelity of Quantum\ncomputation, with implications for future near term applications. Within the\ncontext of Quantum key distribution, such codes, as examined by Ostrev in\narXiv: 2109.06709 along with two-universal hashing protocols, have greatly\nsimplified Quantum phases of computation for unconditional security. To further\nexamine novel applications of two-universal hashing protocols, particularly\nthrough the structure of parity check matrices, we demonstrate how being able\nto efficiently compute functions of the parity check matrices relates to\nmarginals of a suitably defined probability measure supported over random\nmatrices. The security of the two-universal QKD hashing protocol will be shown\nto depend upon the computation of purified states of random matrices, which\nrelates to probabilistic collision bounds between two hashing functions.\nCentral to our approach are the introduction of novel real, simulator, and\nideal, isometries, hence allowing for efficient computations of functions of\nthe two parity check matrices. As a result of being able to perform such\ncomputations involving parity check matrices, the security of the two-universal\nhashing protocol is a factor of $2^{ \\frac{5}{2} ( 5 - \\frac{3}{2} ) +\n\\mathrm{log}_2 \\sqrt{C}}$ less secure, for some strictly positive constant $C$.", "published": "2025-10-01 21:52:00", "link": "http://arxiv.org/abs/2510.02402v1", "categories": ["quant-ph", "cs.IT", "math.IT", "math.PR", "81P02, 81Q02"], "primary_category": "quant-ph"}
{"title": "Linear RNNs for autoregressive generation of long music samples", "abstract": "Directly learning to generate audio waveforms in an autoregressive manner is\na challenging task, due to the length of the raw sequences and the existence of\nimportant structure on many different timescales. Traditional approaches based\non recurrent neural networks, as well as causal convolutions and\nself-attention, have only had limited success on this task. However, recent\nwork has shown that deep state space models, also referred to as linear RNNs,\ncan be highly efficient in this context. In this work, we push the boundaries\nof linear RNNs applied to raw audio modeling, investigating the effects of\ndifferent architectural choices and using context-parallelism to enable\ntraining on sequences up to one minute (1M tokens) in length. We present a\nmodel, HarmonicRNN, which attains state of the art log-likelihoods and\nperceptual metrics on small-scale datasets.", "published": "2025-10-01 17:26:54", "link": "http://arxiv.org/abs/2510.02401v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "When Voice Matters: Evidence of Gender Disparity in Positional Bias of SpeechLLMs", "abstract": "The rapid development of SpeechLLM-based conversational AI systems has\ncreated a need for robustly benchmarking these efforts, including aspects of\nfairness and bias. At present, such benchmarks typically rely on multiple\nchoice question answering (MCQA). In this paper, we present the first\ntoken-level probabilistic evaluation and response-based study of several issues\naffecting the use of MCQA in SpeechLLM benchmarking: 1) we examine how model\ntemperature and prompt design affect gender and positional bias on an MCQA\ngender-bias benchmark; 2) we examine how these biases are affected by the\ngender of the input voice; and 3) we study to what extent observed trends carry\nover to a second gender-bias benchmark. Our results show that concerns about\npositional bias from the text domain are equally valid in the speech domain. We\nalso find the effect to be stronger for female voices than for male voices. To\nour knowledge, this is the first study to isolate positional bias effects in\nSpeechLLM-based gender-bias benchmarks. We conclude that current MCQA\nbenchmarks do not account for speech-based bias and alternative strategies are\nneeded to ensure fairness towards all users.", "published": "2025-10-01 10:49:12", "link": "http://arxiv.org/abs/2510.02398v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A first-order method for constrained nonconvex--nonconcave minimax problems under a local Kurdyka-\u0141ojasiewicz condition", "abstract": "We study a class of constrained nonconvex--nonconcave minimax problems in\nwhich the inner maximization involves potentially complex constraints. Under\nthe assumption that the inner problem of a novel lifted minimax problem\nsatisfies a local Kurdyka-{\\L}ojasiewicz (KL) condition, we show that the\nmaximal function of the original problem enjoys a local H\\\"older smoothness\nproperty. We also propose a sequential convex programming (SCP) method for\nsolving constrained optimization problems and establish its convergence rate\nunder a local KL condition. Leveraging these results, we develop an inexact\nproximal gradient method for the original minimax problem, where the inexact\ngradient of the maximal function is computed via the SCP method applied to a\nlocally KL-structured subproblem. Finally, we establish complexity guarantees\nfor the proposed method in computing an approximate stationary point of the\noriginal minimax problem.", "published": "2025-10-01 17:54:27", "link": "http://arxiv.org/abs/2510.01168v2", "categories": ["math.OC", "cs.LG", "cs.NA", "math.NA", "stat.ML", "90C26, 90C30, 90C47, 90C99, 65K05"], "primary_category": "math.OC"}
{"title": "Cluster Analysis for Globally Coupled Map using Optimal Transport Distance and Complexity of Attractor-ruin", "abstract": "In this paper, we show the results of the strength of attractorruins for a\nglobally coupled map. The globally coupled map (GCM) is a discrete dynamical\nsystem, and here we consider a model in which the logistic map is globally\ncoupled. An attractor-ruin is a set in which the attractor is destabilized by a\nchange in parameters, which is characterized by a Milnor attractor.\nIntermittent phenomena called chaotic itinerancy, in which orbits transition\nbetween attractor-ruin, have been observed in various complex systems, and\ntheir onset mechanisms and statistical properties have attracted attention. In\nthis study, the instability of orbits of GCM is analyzed from the perspective\nof clustering using the optimal transport distance, and the strength of\nattractor-ruins is numerically evaluated by applying this method. As a result,\nit was found that the strength of various attractor-ruins is high in the\nparameter region called the partially ordered phase, where chaotic itinerancy\noccurs.", "published": "2025-10-01 05:41:27", "link": "http://arxiv.org/abs/2510.00538v1", "categories": ["math.DS", "cs.NA", "math.NA", "37D45, 39A33, 62H30"], "primary_category": "math.DS"}
{"title": "CL-UZH submission to the NIST SRE 2024 Speaker Recognition Evaluation", "abstract": "The CL-UZH team submitted one system each for the fixed and open conditions\nof the NIST SRE 2024 challenge. For the closed-set condition, results for the\naudio-only trials were achieved using the X-vector system developed with Kaldi.\nFor the audio-visual results we used only models developed for the visual\nmodality. Two sets of results were submitted for the open-set and closed-set\nconditions, one based on a pretrained model using the VoxBlink2 and VoxCeleb2\ndatasets. An Xvector-based model was trained from scratch using the CTS\nsuperset dataset for the closed set. In addition to the submission of the\nresults of the SRE24 evaluation to the competition website, we talked about the\nperformance of the proposed systems on the SRE24 evaluation in this report.", "published": "2025-10-01 14:27:00", "link": "http://arxiv.org/abs/2510.00952v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
