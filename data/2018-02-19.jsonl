{"title": "Before Name-calling: Dynamics and Triggers of Ad Hominem Fallacies in\n  Web Argumentation", "abstract": "Arguing without committing a fallacy is one of the main requirements of an\nideal debate. But even when debating rules are strictly enforced and fallacious\narguments punished, arguers often lapse into attacking the opponent by an ad\nhominem argument. As existing research lacks solid empirical investigation of\nthe typology of ad hominem arguments as well as their potential causes, this\npaper fills this gap by (1) performing several large-scale annotation studies,\n(2) experimenting with various neural architectures and validating our working\nhypotheses, such as controversy or reasonableness, and (3) providing linguistic\ninsights into triggers of ad hominem using explainable neural network\narchitectures.", "published": "2018-02-19 12:57:56", "link": "http://arxiv.org/abs/1802.06613v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tied Multitask Learning for Neural Speech Translation", "abstract": "We explore multitask models for neural translation of speech, augmenting them\nin order to reflect two intuitive notions. First, we introduce a model where\nthe second task decoder receives information from the decoder of the first\ntask, since higher-level intermediate representations should provide useful\ninformation. Second, we apply regularization that encourages transitivity and\ninvertibility. We show that the application of these notions on jointly trained\nmodels improves performance on the tasks of low-resource speech transcription\nand translation. It also leads to better performance when using attention\ninformation for word discovery over unsegmented input.", "published": "2018-02-19 14:49:42", "link": "http://arxiv.org/abs/1802.06655v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Question Generation from Knowledge Graphs for Unseen\n  Predicates and Entity Types", "abstract": "We present a neural model for question generation from knowledge base triples\nin a \"Zero-Shot\" setup, that is generating questions for triples containing\npredicates, subject types or object types that were not seen at training time.\nOur model leverages triples occurrences in the natural language corpus in an\nencoder-decoder architecture, paired with an original part-of-speech copy\naction mechanism to generate questions. Benchmark and human evaluation show\nthat our model sets a new state-of-the-art for zero-shot QG.", "published": "2018-02-19 20:43:53", "link": "http://arxiv.org/abs/1802.06842v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Word Vectors for 157 Languages", "abstract": "Distributed word representations, or word vectors, have recently been applied\nto many tasks in natural language processing, leading to state-of-the-art\nperformance. A key ingredient to the successful application of these\nrepresentations is to train them on very large corpora, and use these\npre-trained models in downstream tasks. In this paper, we describe how we\ntrained such high quality word representations for 157 languages. We used two\nsources of data to train these models: the free online encyclopedia Wikipedia\nand data from the common crawl project. We also introduce three new word\nanalogy datasets to evaluate these word vectors, for French, Hindi and Polish.\nFinally, we evaluate our pre-trained word vectors on 10 languages for which\nevaluation datasets exists, showing very strong performance compared to\nprevious models.", "published": "2018-02-19 22:32:47", "link": "http://arxiv.org/abs/1802.06893v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Hidden Markov Models from Pairwise Co-occurrences with\n  Application to Topic Modeling", "abstract": "We present a new algorithm for identifying the transition and emission\nprobabilities of a hidden Markov model (HMM) from the emitted data.\nExpectation-maximization becomes computationally prohibitive for long\nobservation records, which are often required for identification. The new\nalgorithm is particularly suitable for cases where the available sample size is\nlarge enough to accurately estimate second-order output probabilities, but not\nhigher-order ones. We show that if one is only able to obtain a reliable\nestimate of the pairwise co-occurrence probabilities of the emissions, it is\nstill possible to uniquely identify the HMM if the emission probability is\n\\emph{sufficiently scattered}. We apply our method to hidden topic Markov\nmodeling, and demonstrate that we can learn topics with higher quality if\ndocuments are modeled as observations of HMMs sharing the same emission (topic)\nprobability, compared to the simple but widely used bag-of-words model.", "published": "2018-02-19 22:33:56", "link": "http://arxiv.org/abs/1802.06894v2", "categories": ["cs.CL", "cs.LG", "eess.SP", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative\n  Refinement", "abstract": "We propose a conditional non-autoregressive neural sequence model based on\niterative refinement. The proposed model is designed based on the principles of\nlatent variable models and denoising autoencoders, and is generally applicable\nto any sequence generation task. We extensively evaluate the proposed model on\nmachine translation (En-De and En-Ro) and image caption generation, and observe\nthat it significantly speeds up decoding while maintaining the generation\nquality comparable to the autoregressive counterpart.", "published": "2018-02-19 22:57:54", "link": "http://arxiv.org/abs/1802.06901v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Neural Multi-sequence Alignment TeCHnique (NeuMATCH)", "abstract": "The alignment of heterogeneous sequential data (video to text) is an\nimportant and challenging problem. Standard techniques for this task, including\nDynamic Time Warping (DTW) and Conditional Random Fields (CRFs), suffer from\ninherent drawbacks. Mainly, the Markov assumption implies that, given the\nimmediate past, future alignment decisions are independent of further history.\nThe separation between similarity computation and alignment decision also\nprevents end-to-end training. In this paper, we propose an end-to-end neural\narchitecture where alignment actions are implemented as moving data between\nstacks of Long Short-term Memory (LSTM) blocks. This flexible architecture\nsupports a large variety of alignment tasks, including one-to-one, one-to-many,\nskipping unmatched elements, and (with extensions) non-monotonic alignment.\nExtensive experiments on semi-synthetic and real datasets show that our\nalgorithm outperforms state-of-the-art baselines.", "published": "2018-02-19 06:51:01", "link": "http://arxiv.org/abs/1803.00057v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Voice Impersonation using Generative Adversarial Networks", "abstract": "Voice impersonation is not the same as voice transformation, although the\nlatter is an essential element of it. In voice impersonation, the resultant\nvoice must convincingly convey the impression of having been naturally produced\nby the target speaker, mimicking not only the pitch and other perceivable\nsignal qualities, but also the style of the target speaker. In this paper, we\npropose a novel neural network based speech quality- and style- mimicry\nframework for the synthesis of impersonated voices. The framework is built upon\na fast and accurate generative adversarial network model. Given spectrographic\nrepresentations of source and target speakers' voices, the model learns to\nmimic the target speaker's voice quality and style, regardless of the\nlinguistic content of either's voice, generating a synthetic spectrogram from\nwhich the time domain signal is reconstructed using the Griffin-Lim method. In\neffect, this model reframes the well-known problem of style-transfer for images\nas the problem of style-transfer for speech signals, while intrinsically\naddressing the problem of durational variability of speech sounds. Experiments\ndemonstrate that the model can generate extremely convincing samples of\nimpersonated speech. It is even able to impersonate voices across different\ngenders effectively. Results are qualitatively evaluated using standard\nprocedures for evaluating synthesized voices.", "published": "2018-02-19 20:41:47", "link": "http://arxiv.org/abs/1802.06840v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech Enhancement in Adverse Environments Based on Non-stationary\n  Noise-driven Spectral Subtraction and SNR-dependent Phase Compensation", "abstract": "A two-step enhancement method based on spectral subtraction and phase\nspectrum compensation is presented in this paper for noisy speeches in adverse\nenvironments involving non-stationary noise and medium to low levels of SNR.\nThe magnitude of the noisy speech spectrum is modified in the first step of the\nproposed method by a spectral subtraction approach, where a new noise\nestimation method based on the low frequency information of the noisy speech is\nintroduced. We argue that this method of noise estimation is capable of\nestimating the non-stationary noise accurately. The phase spectrum of the noisy\nspeech is modified in the second step consisting of phase spectrum\ncompensation, where an SNR-dependent approach is incorporated to determine the\namount of compensation to be imposed on the phase spectrum. A modified complex\nspectrum is obtained by aggregating the magnitude from the spectral subtraction\nstep and modified phase spectrum from the phase compensation step, which is\nfound to be a better representation of enhanced speech spectrum. Speech files\navailable in the NOIZEUS database are used to carry extensive simulations for\nevaluation of the proposed method.", "published": "2018-02-19 03:57:15", "link": "http://arxiv.org/abs/1803.00396v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
