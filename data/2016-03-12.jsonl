{"title": "Neural Discourse Relation Recognition with Semantic Memory", "abstract": "Humans comprehend the meanings and relations of discourses heavily relying on\ntheir semantic memory that encodes general knowledge about concepts and facts.\nInspired by this, we propose a neural recognizer for implicit discourse\nrelation analysis, which builds upon a semantic memory that stores knowledge in\na distributed fashion. We refer to this recognizer as SeMDER. Starting from\nword embeddings of discourse arguments, SeMDER employs a shallow encoder to\ngenerate a distributed surface representation for a discourse. A semantic\nencoder with attention to the semantic memory matrix is further established\nover surface representations. It is able to retrieve a deep semantic meaning\nrepresentation for the discourse from the memory. Using the surface and\nsemantic representations as input, SeMDER finally predicts implicit discourse\nrelations via a neural recognizer. Experiments on the benchmark data set show\nthat SeMDER benefits from the semantic memory and achieves substantial\nimprovements of 2.56\\% on average over current state-of-the-art baselines in\nterms of F1-score.", "published": "2016-03-12 08:54:16", "link": "http://arxiv.org/abs/1603.03873v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Variational Neural Discourse Relation Recognizer", "abstract": "Implicit discourse relation recognition is a crucial component for automatic\ndiscourselevel analysis and nature language understanding. Previous studies\nexploit discriminative models that are built on either powerful manual features\nor deep discourse representations. In this paper, instead, we explore\ngenerative models and propose a variational neural discourse relation\nrecognizer. We refer to this model as VarNDRR. VarNDRR establishes a directed\nprobabilistic model with a latent continuous variable that generates both a\ndiscourse and the relation between the two arguments of the discourse. In order\nto perform efficient inference and learning, we introduce neural discourse\nrelation models to approximate the prior and posterior distributions of the\nlatent variable, and employ these approximated distributions to optimize a\nreparameterized variational lower bound. This allows VarNDRR to be trained with\nstandard stochastic gradient methods. Experiments on the benchmark data set\nshow that VarNDRR can achieve comparable results against stateof- the-art\nbaselines without using any manual features.", "published": "2016-03-12 09:11:30", "link": "http://arxiv.org/abs/1603.03876v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sequential Short-Text Classification with Recurrent and Convolutional\n  Neural Networks", "abstract": "Recent approaches based on artificial neural networks (ANNs) have shown\npromising results for short-text classification. However, many short texts\noccur in sequences (e.g., sentences in a document or utterances in a dialog),\nand most existing ANN-based systems do not leverage the preceding short texts\nwhen classifying a subsequent one. In this work, we present a model based on\nrecurrent neural networks and convolutional neural networks that incorporates\nthe preceding short texts. Our model achieves state-of-the-art results on three\ndifferent datasets for dialog act prediction.", "published": "2016-03-12 00:02:51", "link": "http://arxiv.org/abs/1603.03827v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
