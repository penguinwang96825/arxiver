{"title": "Automatic Code Generation using Pre-Trained Language Models", "abstract": "Recent advancements in natural language processing \\cite{gpt2} \\cite{BERT}\nhave led to near-human performance in multiple natural language tasks. In this\npaper, we seek to understand whether similar techniques can be applied to a\nhighly structured environment with strict syntax rules. Specifically, we\npropose an end-to-end machine learning model for code generation in the Python\nlanguage built on-top of pre-trained language models. We demonstrate that a\nfine-tuned model can perform well in code generation tasks, achieving a BLEU\nscore of 0.22, an improvement of 46\\% over a reasonable sequence-to-sequence\nbaseline. All results and related code used for training and data processing\nare available on GitHub.", "published": "2021-02-21 07:21:26", "link": "http://arxiv.org/abs/2102.10535v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Web-based Application for Detecting Indonesian Clickbait Headlines using\n  IndoBERT", "abstract": "With increasing usage of clickbaits in Indonesian Online News, newsworthy\narticles sometimes get buried among clickbaity news. A reliable and lightweight\ntool is needed to detect such clickbaits on-the-go. Leveraging state-of-the-art\nnatural language processing model BERT, a RESTful API based application is\ndeveloped. This study offloaded the computing resources needed to train the\nmodel on the cloud server, while the client-side application only needs to send\na request to the API and the cloud server will handle the rest. This study\nproposed the design and developed a web-based application to detect clickbait\nin Indonesian using IndoBERT as a language model. The application usage is\ndiscussed and available for public use with a performance of mean ROC-AUC of\n89%.", "published": "2021-02-21 13:28:52", "link": "http://arxiv.org/abs/2102.10601v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pre-Training BERT on Arabic Tweets: Practical Considerations", "abstract": "Pretraining Bidirectional Encoder Representations from Transformers (BERT)\nfor downstream NLP tasks is a non-trival task. We pretrained 5 BERT models that\ndiffer in the size of their training sets, mixture of formal and informal\nArabic, and linguistic preprocessing. All are intended to support Arabic\ndialects and social media. The experiments highlight the centrality of data\ndiversity and the efficacy of linguistically aware segmentation. They also\nhighlight that more data or more training step do not necessitate better\nmodels. Our new models achieve new state-of-the-art results on several\ndownstream tasks. The resulting models are released to the community under the\nname QARiB.", "published": "2021-02-21 20:51:33", "link": "http://arxiv.org/abs/2102.10684v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Detecting Compliance of Privacy Policies with Data Protection Laws", "abstract": "Privacy Policies are the legal documents that describe the practices that an\norganization or company has adopted in the handling of the personal data of its\nusers. But as policies are a legal document, they are often written in\nextensive legal jargon that is difficult to understand. Though work has been\ndone on privacy policies but none that caters to the problem of verifying if a\ngiven privacy policy adheres to the data protection laws of a given country or\nstate. We aim to bridge that gap by providing a framework that analyzes privacy\npolicies in light of various data protection laws, such as the General Data\nProtection Regulation (GDPR). To achieve that, firstly we labeled both the\nprivacy policies and laws. Then a correlation scheme is developed to map the\ncontents of a privacy policy to the appropriate segments of law that a policy\nmust conform to. Then we check the compliance of privacy policy's text with the\ncorresponding text of the law using NLP techniques. By using such a tool, users\nwould be better equipped to understand how their personal data is managed. For\nnow, we have provided a mapping for the GDPR and PDPA, but other laws can\neasily be incorporated in the already built pipeline.", "published": "2021-02-21 09:15:15", "link": "http://arxiv.org/abs/2102.12362v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Pruning the Index Contents for Memory Efficient Open-Domain QA", "abstract": "This work presents a novel pipeline that demonstrates what is achievable with\na combined effort of state-of-the-art approaches. Specifically, it proposes the\nnovel R2-D2 (Rank twice, reaD twice) pipeline composed of retriever, passage\nreranker, extractive reader, generative reader and a simple way to combine\nthem. Furthermore, previous work often comes with a massive index of external\ndocuments that scales in the order of tens of GiB. This work presents a simple\napproach for pruning the contents of a massive index such that the open-domain\nQA system altogether with index, OS, and library components fits into 6GiB\ndocker image while retaining only 8% of original index contents and losing only\n3% EM accuracy.", "published": "2021-02-21 21:56:38", "link": "http://arxiv.org/abs/2102.10697v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Anomaly Detection in Audio with Concept Drift using Adaptive Huffman\n  Coding", "abstract": "When detecting anomalies in audio, it can often be necessary to consider\nconcept drift: the distribution of the data may drift over time because of\ndynamically changing environments, and anomalies may become normal as time\nelapses. We propose to use adaptive Huffman coding for anomaly detection in\naudio with concept drift. Compared with the existing method of adaptive\nGaussian mixture modeling (AGMM), adaptive Huffman coding does not require a\npriori information about the clusters and can adjust the number of clusters\ndynamically depending on the amount of variation in the audio. To control the\nsize of the Huffman tree, we propose to merge clusters that are close to each\nother instead of replacing rare clusters with new data. This reduces redundancy\nin the Huffman tree while ensuring that it never forgets past information. On a\ndataset of audio with concept drift which we have curated ourselves, our\nproposed method achieves higher area under the curve (AUC) compared with AGMM\nand fixed-length Huffman trees. The proposed approach is also time-efficient\nand can be easily extended to other types of time series data (e.g., video).", "published": "2021-02-21 05:24:20", "link": "http://arxiv.org/abs/2102.10515v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
