{"title": "Using Adversarial Attacks to Reveal the Statistical Bias in Machine\n  Reading Comprehension Models", "abstract": "Pre-trained language models have achieved human-level performance on many\nMachine Reading Comprehension (MRC) tasks, but it remains unclear whether these\nmodels truly understand language or answer questions by exploiting statistical\nbiases in datasets. Here, we demonstrate a simple yet effective method to\nattack MRC models and reveal the statistical biases in these models. We apply\nthe method to the RACE dataset, for which the answer to each MRC question is\nselected from 4 options. It is found that several pre-trained language models,\nincluding BERT, ALBERT, and RoBERTa, show consistent preference to some\noptions, even when these options are irrelevant to the question. When\ninterfered by these irrelevant options, the performance of MRC models can be\nreduced from human-level performance to the chance-level performance. Human\nreaders, however, are not clearly affected by these irrelevant options.\nFinally, we propose an augmented training method that can greatly reduce\nmodels' statistical biases.", "published": "2021-05-24 07:35:56", "link": "http://arxiv.org/abs/2105.11136v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-Preserving Text Simplification", "abstract": "We present a context-preserving text simplification (TS) approach that\nrecursively splits and rephrases complex English sentences into a semantic\nhierarchy of simplified sentences. Using a set of linguistically principled\ntransformation patterns, input sentences are converted into a hierarchical\nrepresentation in the form of core sentences and accompanying contexts that are\nlinked via rhetorical relations. Hence, as opposed to previously proposed\nsentence splitting approaches, which commonly do not take into account\ndiscourse-level aspects, our TS approach preserves the semantic relationship of\nthe decomposed constituents in the output. A comparative analysis with the\nannotations contained in the RST-DT shows that we are able to capture the\ncontextual hierarchy between the split sentences with a precision of 89% and\nreach an average precision of 69% for the classification of the rhetorical\nrelations that hold between them.", "published": "2021-05-24 09:54:56", "link": "http://arxiv.org/abs/2105.11178v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Standard Criteria for human evaluation of Chatbots: A Survey", "abstract": "Human evaluation is becoming a necessity to test the performance of Chatbots.\nHowever, off-the-shelf settings suffer the severe reliability and replication\nissues partly because of the extremely high diversity of criteria. It is high\ntime to come up with standard criteria and exact definitions. To this end, we\nconduct a through investigation of 105 papers involving human evaluation for\nChatbots. Deriving from this, we propose five standard criteria along with\nprecise definitions.", "published": "2021-05-24 10:49:04", "link": "http://arxiv.org/abs/2105.11197v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "StructuralLM: Structural Pre-training for Form Understanding", "abstract": "Large pre-trained language models achieve state-of-the-art results when\nfine-tuned on downstream NLP tasks. However, they almost exclusively focus on\ntext-only representation, while neglecting cell-level layout information that\nis important for form image understanding. In this paper, we propose a new\npre-training approach, StructuralLM, to jointly leverage cell and layout\ninformation from scanned documents. Specifically, we pre-train StructuralLM\nwith two new designs to make the most of the interactions of cell and layout\ninformation: 1) each cell as a semantic unit; 2) classification of cell\npositions. The pre-trained StructuralLM achieves new state-of-the-art results\nin different types of downstream tasks, including form understanding (from\n78.95 to 85.14), document visual question answering (from 72.59 to 83.94) and\ndocument image classification (from 94.43 to 96.08).", "published": "2021-05-24 11:33:20", "link": "http://arxiv.org/abs/2105.11210v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "De-identification of Privacy-related Entities in Job Postings", "abstract": "De-identification is the task of detecting privacy-related entities in text,\nsuch as person names, emails and contact data. It has been well-studied within\nthe medical domain. The need for de-identification technology is increasing, as\nprivacy-preserving data handling is in high demand in many domains. In this\npaper, we focus on job postings. We present JobStack, a new corpus for\nde-identification of personal data in job vacancies on Stackoverflow. We\nintroduce baselines, comparing Long-Short Term Memory (LSTM) and Transformer\nmodels. To improve upon these baselines, we experiment with contextualized\nembeddings and distantly related auxiliary data via multi-task learning. Our\nresults show that auxiliary data improves de-identification performance.\nSurprisingly, vanilla BERT turned out to be more effective than a BERT model\ntrained on other portions of Stackoverflow.", "published": "2021-05-24 12:01:22", "link": "http://arxiv.org/abs/2105.11223v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distantly-Supervised Long-Tailed Relation Extraction Using Constraint\n  Graphs", "abstract": "Label noise and long-tailed distributions are two major challenges in\ndistantly supervised relation extraction. Recent studies have shown great\nprogress on denoising, but paid little attention to the problem of long-tailed\nrelations. In this paper, we introduce a constraint graph to model the\ndependencies between relation labels. On top of that, we further propose a\nnovel constraint graph-based relation extraction framework(CGRE) to handle the\ntwo challenges simultaneously. CGRE employs graph convolution networks to\npropagate information from data-rich relation nodes to data-poor relation\nnodes, and thus boosts the representation learning of long-tailed relations. To\nfurther improve the noise immunity, a constraint-aware attention module is\ndesigned in CGRE to integrate the constraint information. Extensive\nexperimental results indicate that CGRE achieves significant improvements over\nthe previous methods for both denoising and long-tailed relation extraction.\nThe pre-processed datasets and source code are publicly available at\nhttps://github.com/tmliang/CGRE.", "published": "2021-05-24 12:02:32", "link": "http://arxiv.org/abs/2105.11225v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Text Classification with Heterogeneous Graph Neural\n  Network", "abstract": "Cross-lingual text classification aims at training a classifier on the source\nlanguage and transferring the knowledge to target languages, which is very\nuseful for low-resource languages. Recent multilingual pretrained language\nmodels (mPLM) achieve impressive results in cross-lingual classification tasks,\nbut rarely consider factors beyond semantic similarity, causing performance\ndegradation between some language pairs. In this paper we propose a simple yet\neffective method to incorporate heterogeneous information within and across\nlanguages for cross-lingual text classification using graph convolutional\nnetworks (GCN). In particular, we construct a heterogeneous graph by treating\ndocuments and words as nodes, and linking nodes with different relations, which\ninclude part-of-speech roles, semantic similarity, and document translations.\nExtensive experiments show that our graph-based method significantly\noutperforms state-of-the-art models on all tasks, and also achieves consistent\nperformance gain over baselines in low-resource settings where external tools\nlike translators are unavailable.", "published": "2021-05-24 12:45:42", "link": "http://arxiv.org/abs/2105.11246v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PTR: Prompt Tuning with Rules for Text Classification", "abstract": "Fine-tuned pre-trained language models (PLMs) have achieved awesome\nperformance on almost all NLP tasks. By using additional prompts to fine-tune\nPLMs, we can further stimulate the rich knowledge distributed in PLMs to better\nserve downstream tasks. Prompt tuning has achieved promising results on some\nfew-class classification tasks such as sentiment classification and natural\nlanguage inference. However, manually designing lots of language prompts is\ncumbersome and fallible. For those auto-generated prompts, it is also expensive\nand time-consuming to verify their effectiveness in non-few-shot scenarios.\nHence, it is still challenging for prompt tuning to address many-class\nclassification tasks. To this end, we propose prompt tuning with rules (PTR)\nfor many-class text classification and apply logic rules to construct prompts\nwith several sub-prompts. In this way, PTR is able to encode prior knowledge of\neach class into prompt tuning. We conduct experiments on relation\nclassification, a typical and complicated many-class classification task, and\nthe results show that PTR can significantly and consistently outperform\nexisting state-of-the-art baselines. This indicates that PTR is a promising\napproach to take advantage of both human prior knowledge and PLMs for those\ncomplicated classification tasks.", "published": "2021-05-24 13:24:02", "link": "http://arxiv.org/abs/2105.11259v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing perceived organizational leadership styles through twitter\n  text mining", "abstract": "We propose a text classification tool based on support vector machines for\nthe assessment of organizational leadership styles, as appearing to Twitter\nusers. We collected Twitter data over 51 days, related to the first 30 Italian\norganizations in the 2015 ranking of Forbes Global 2000-out of which we\nselected the five with the most relevant volumes of tweets. We analyzed the\ncommunication of the company leaders, together with the dialogue among the\nstakeholders of each company, to understand the association with perceived\nleadership styles and dimensions. To assess leadership profiles, we referred to\nthe 10-factor model developed by Barchiesi and La Bella in 2007. We maintain\nthe distinctiveness of the approach we propose, as it allows a rapid assessment\nof the perceived leadership capabilities of an enterprise, as they emerge from\nits social media interactions. It can also be used to show how companies\nrespond and manage their communication when specific events take place, and to\nassess their stakeholder's reactions.", "published": "2021-05-24 13:45:51", "link": "http://arxiv.org/abs/2105.11276v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Introducing the Talk Markup Language (TalkML):Adding a little social\n  intelligence to industrial speech interfaces", "abstract": "Virtual Personal Assistants like Siri have great potential but such\ndevelopments hit the fundamental problem of how to make computational devices\nthat understand human speech. Natural language understanding is one of the more\ndisappointing failures of AI research and it seems there is something we\ncomputer scientists don't get about the nature of language. Of course\nphilosophers and linguists think quite differently about language and this\npaper describes how we have taken ideas from other disciplines and implemented\nthem. The background to the work is to take seriously the notion of language as\naction and look at what people actually do with language using the techniques\nof Conversation Analysis. The observation has been that human communication is\n(behind the scenes) about the management of social relations as well as the\n(foregrounded) passing of information. To claim this is one thing but to\nimplement it requires a mechanism. The mechanism described here is based on the\nnotion of language being intentional - we think intentionally, talk about them\nand recognise them in others - and cooperative in that we are compelled to help\nout. The way we are compelled points to a solution to the ever present problem\nof keeping the human on topic. The approach has led to a recent success in\nwhich we significantly improve user satisfaction independent of task\ncompletion. Talk Markup Language (TalkML) is a draft alternative to VoiceXML\nthat, we propose, greatly simplifies the scripting of interaction by providing\ndefault behaviours for no input and not recognised speech events.", "published": "2021-05-24 14:25:35", "link": "http://arxiv.org/abs/2105.11294v1", "categories": ["cs.CL", "I.2.7; J.5; H.5.2"], "primary_category": "cs.CL"}
{"title": "DaN+: Danish Nested Named Entities and Lexical Normalization", "abstract": "This paper introduces DaN+, a new multi-domain corpus and annotation\nguidelines for Danish nested named entities (NEs) and lexical normalization to\nsupport research on cross-lingual cross-domain learning for a less-resourced\nlanguage. We empirically assess three strategies to model the two-layer Named\nEntity Recognition (NER) task. We compare transfer capabilities from German\nversus in-language annotation from scratch. We examine language-specific versus\nmultilingual BERT, and study the effect of lexical normalization on NER. Our\nresults show that 1) the most robust strategy is multi-task learning which is\nrivaled by multi-label decoding, 2) BERT-based NER models are sensitive to\ndomain shifts, and 3) in-language BERT and lexical normalization are the most\nbeneficial on the least canonical data. Our results also show that an\nout-of-domain setup remains challenging, while performance on news plateaus\nquickly. This highlights the importance of cross-domain evaluation of\ncross-lingual transfer.", "published": "2021-05-24 14:35:21", "link": "http://arxiv.org/abs/2105.11301v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RobeCzech: Czech RoBERTa, a monolingual contextualized language\n  representation model", "abstract": "We present RobeCzech, a monolingual RoBERTa language representation model\ntrained on Czech data. RoBERTa is a robustly optimized Transformer-based\npretraining approach. We show that RobeCzech considerably outperforms\nequally-sized multilingual and Czech-trained contextualized language\nrepresentation models, surpasses current state of the art in all five evaluated\nNLP tasks and reaches state-of-the-art results in four of them. The RobeCzech\nmodel is released publicly at https://hdl.handle.net/11234/1-3691 and\nhttps://huggingface.co/ufal/robeczech-base.", "published": "2021-05-24 14:50:04", "link": "http://arxiv.org/abs/2105.11314v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VANiLLa : Verbalized Answers in Natural Language at Large Scale", "abstract": "In the last years, there have been significant developments in the area of\nQuestion Answering over Knowledge Graphs (KGQA). Despite all the notable\nadvancements, current KGQA datasets only provide the answers as the direct\noutput result of the formal query, rather than full sentences incorporating\nquestion context. For achieving coherent answers sentence with the question's\nvocabulary, template-based verbalization so are usually employed for a better\nrepresentation of answers, which in turn require extensive expert intervention.\nThus, making way for machine learning approaches; however, there is a scarcity\nof datasets that empower machine learning models in this area. Hence, we\nprovide the VANiLLa dataset which aims at reducing this gap by offering answers\nin natural language sentences. The answer sentences in this dataset are\nsyntactically and semantically closer to the question than to the triple fact.\nOur dataset consists of over 100k simple questions adapted from the CSQA and\nSimpleQuestionsWikidata datasets and generated using a semi-automatic\nframework. We also present results of training our dataset on multiple baseline\nmodels adapted from current state-of-the-art Natural Language Generation (NLG)\narchitectures. We believe that this dataset will allow researchers to focus on\nfinding suitable methodologies and architectures for answer verbalization.", "published": "2021-05-24 16:57:54", "link": "http://arxiv.org/abs/2105.11407v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Diacritics Restoration using BERT with Analysis on Czech language", "abstract": "We propose a new architecture for diacritics restoration based on\ncontextualized embeddings, namely BERT, and we evaluate it on 12 languages with\ndiacritics. Furthermore, we conduct a detailed error analysis on Czech, a\nmorphologically rich language with a high level of diacritization. Notably, we\nmanually annotate all mispredictions, showing that roughly 44% of them are\nactually not errors, but either plausible variants (19%), or the system\ncorrections of erroneous data (25%). Finally, we categorize the real errors in\ndetail. We release the code at\nhttps://github.com/ufal/bert-diacritics-restoration.", "published": "2021-05-24 16:58:27", "link": "http://arxiv.org/abs/2105.11408v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prevent the Language Model from being Overconfident in Neural Machine\n  Translation", "abstract": "The Neural Machine Translation (NMT) model is essentially a joint language\nmodel conditioned on both the source sentence and partial translation.\nTherefore, the NMT model naturally involves the mechanism of the Language Model\n(LM) that predicts the next token only based on partial translation. Despite\nits success, NMT still suffers from the hallucination problem, generating\nfluent but inadequate translations. The main reason is that NMT pays excessive\nattention to the partial translation while neglecting the source sentence to\nsome extent, namely overconfidence of the LM. Accordingly, we define the Margin\nbetween the NMT and the LM, calculated by subtracting the predicted probability\nof the LM from that of the NMT model for each token. The Margin is negatively\ncorrelated to the overconfidence degree of the LM. Based on the property, we\npropose a Margin-based Token-level Objective (MTO) and a Margin-based\nSentencelevel Objective (MSO) to maximize the Margin for preventing the LM from\nbeing overconfident. Experiments on WMT14 English-to-German, WMT19\nChinese-to-English, and WMT14 English-to-French translation tasks demonstrate\nthe effectiveness of our approach, with 1.36, 1.50, and 0.63 BLEU improvements,\nrespectively, compared to the Transformer baseline. The human evaluation\nfurther verifies that our approaches improve translation adequacy as well as\nfluency.", "published": "2021-05-24 05:34:09", "link": "http://arxiv.org/abs/2105.11098v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Abusive Language Detection in Heterogeneous Contexts: Dataset Collection\n  and the Role of Supervised Attention", "abstract": "Abusive language is a massive problem in online social platforms. Existing\nabusive language detection techniques are particularly ill-suited to comments\ncontaining heterogeneous abusive language patterns, i.e., both abusive and\nnon-abusive parts. This is due in part to the lack of datasets that explicitly\nannotate heterogeneity in abusive language. We tackle this challenge by\nproviding an annotated dataset of abusive language in over 11,000 comments from\nYouTube. We account for heterogeneity in this dataset by separately annotating\nboth the comment as a whole and the individual sentences that comprise each\ncomment. We then propose an algorithm that uses a supervised attention\nmechanism to detect and categorize abusive content using multi-task learning.\nWe empirically demonstrate the challenges of using traditional techniques on\nheterogeneous content and the comparative gains in performance of the proposed\napproach over state-of-the-art methods.", "published": "2021-05-24 06:50:19", "link": "http://arxiv.org/abs/2105.11119v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "One2Set: Generating Diverse Keyphrases as a Set", "abstract": "Recently, the sequence-to-sequence models have made remarkable progress on\nthe task of keyphrase generation (KG) by concatenating multiple keyphrases in a\npredefined order as a target sequence during training. However, the keyphrases\nare inherently an unordered set rather than an ordered sequence. Imposing a\npredefined order will introduce wrong bias during training, which can highly\npenalize shifts in the order between keyphrases. In this work, we propose a new\ntraining paradigm One2Set without predefining an order to concatenate the\nkeyphrases. To fit this paradigm, we propose a novel model that utilizes a\nfixed set of learned control codes as conditions to generate a set of\nkeyphrases in parallel. To solve the problem that there is no correspondence\nbetween each prediction and target during training, we propose a $K$-step\ntarget assignment mechanism via bipartite matching, which greatly increases the\ndiversity and reduces the duplication ratio of generated keyphrases. The\nexperimental results on multiple benchmarks demonstrate that our approach\nsignificantly outperforms the state-of-the-art methods.", "published": "2021-05-24 07:29:47", "link": "http://arxiv.org/abs/2105.11134v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Retrieval Enhanced Model for Commonsense Generation", "abstract": "Commonsense generation is a challenging task of generating a plausible\nsentence describing an everyday scenario using provided concepts. Its\nrequirement of reasoning over commonsense knowledge and compositional\ngeneralization ability even puzzles strong pre-trained language generation\nmodels. We propose a novel framework using retrieval methods to enhance both\nthe pre-training and fine-tuning for commonsense generation. We retrieve\nprototype sentence candidates by concept matching and use them as auxiliary\ninput. For fine-tuning, we further boost its performance with a trainable\nsentence retriever. We demonstrate experimentally on the large-scale CommonGen\nbenchmark that our approach achieves new state-of-the-art results.", "published": "2021-05-24 09:49:17", "link": "http://arxiv.org/abs/2105.11174v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hater-O-Genius Aggression Classification using Capsule Networks", "abstract": "Contending hate speech in social media is one of the most challenging social\nproblems of our time. There are various types of anti-social behavior in social\nmedia. Foremost of them is aggressive behavior, which is causing many social\nissues such as affecting the social lives and mental health of social media\nusers. In this paper, we propose an end-to-end ensemble-based architecture to\nautomatically identify and classify aggressive tweets. Tweets are classified\ninto three categories - Covertly Aggressive, Overtly Aggressive, and\nNon-Aggressive. The proposed architecture is an ensemble of smaller subnetworks\nthat are able to characterize the feature embeddings effectively. We\ndemonstrate qualitatively that each of the smaller subnetworks is able to learn\nunique features. Our best model is an ensemble of Capsule Networks and results\nin a 65.2% F1 score on the Facebook test set, which results in a performance\ngain of 0.95% over the TRAC-2018 winners. The code and the model weights are\npublicly available at\nhttps://github.com/parthpatwa/Hater-O-Genius-Aggression-Classification-using-Capsule-Networks.", "published": "2021-05-24 11:53:58", "link": "http://arxiv.org/abs/2105.11219v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Few-Shot Upsampling for Protest Size Detection", "abstract": "We propose a new task and dataset for a common problem in social science\nresearch: \"upsampling\" coarse document labels to fine-grained labels or spans.\nWe pose the problem in a question answering format, with the answers providing\nthe fine-grained labels. We provide a benchmark dataset and baselines on a\nsocially impactful task: identifying the exact crowd size at protests and\ndemonstrations in the United States given only order-of-magnitude information\nabout protest attendance, a very small sample of fine-grained examples, and\nEnglish-language news text. We evaluate several baseline models, including\nzero-shot results from rule-based and question-answering models, few-shot\nmodels fine-tuned on a small set of documents, and weakly supervised models\nusing a larger set of coarsely-labeled documents. We find that our rule-based\nmodel initially outperforms a zero-shot pre-trained transformer language model\nbut that further fine-tuning on a very small subset of 25 examples\nsubstantially improves out-of-sample performance. We also demonstrate a method\nfor fine-tuning the transformer span on only the coarse labels that performs\nsimilarly to our rule-based approach. This work will contribute to social\nscientists' ability to generate data to understand the causes and successes of\ncollective action.", "published": "2021-05-24 13:27:23", "link": "http://arxiv.org/abs/2105.11260v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Editorial introduction: The power of words and networks", "abstract": "According to Freud \"words were originally magic and to this day words have\nretained much of their ancient magical power\". By words, behaviors are\ntransformed and problems are solved. The way we use words reveals our\nintentions, goals and values. Novel tools for text analysis help understand the\nmagical power of words. This power is multiplied, if it is combined with the\nstudy of social networks, i.e. with the analysis of relationships among social\nunits. This special issue of the International Journal of Information\nManagement, entitled \"Combining Social Network Analysis and Text Mining: from\nTheory to Practice\", includes heterogeneous and innovative research at the\nnexus of text mining and social network analysis. It aims to enrich work at the\nintersection of these fields, which still lags behind in theoretical,\nempirical, and methodological foundations. The nine articles accepted for\ninclusion in this special issue all present methods and tools that have\nbusiness applications. They are summarized in this editorial introduction.", "published": "2021-05-24 13:29:17", "link": "http://arxiv.org/abs/2105.11263v1", "categories": ["cs.CL", "cs.SI", "I.2.7; J.4; H.4.0"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation with Monolingual Translation Memory", "abstract": "Prior work has proved that Translation memory (TM) can boost the performance\nof Neural Machine Translation (NMT). In contrast to existing work that uses\nbilingual corpus as TM and employs source-side similarity search for memory\nretrieval, we propose a new framework that uses monolingual memory and performs\nlearnable memory retrieval in a cross-lingual manner. Our framework has unique\nadvantages. First, the cross-lingual memory retriever allows abundant\nmonolingual data to be TM. Second, the memory retriever and NMT model can be\njointly optimized for the ultimate translation goal. Experiments show that the\nproposed method obtains substantial improvements. Remarkably, it even\noutperforms strong TM-augmented NMT baselines using bilingual TM. Owning to the\nability to leverage monolingual data, our model also demonstrates effectiveness\nin low-resource and domain adaptation scenarios.", "published": "2021-05-24 13:35:19", "link": "http://arxiv.org/abs/2105.11269v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Neural Language Models for Nineteenth-Century English", "abstract": "We present four types of neural language models trained on a large historical\ndataset of books in English, published between 1760-1900 and comprised of ~5.1\nbillion tokens. The language model architectures include static (word2vec and\nfastText) and contextualized models (BERT and Flair). For each architecture, we\ntrained a model instance using the whole dataset. Additionally, we trained\nseparate instances on text published before 1850 for the two static models, and\nfour instances considering different time slices for BERT. Our models have\nalready been used in various downstream tasks where they consistently improved\nperformance. In this paper, we describe how the models have been created and\noutline their reuse potential.", "published": "2021-05-24 14:57:34", "link": "http://arxiv.org/abs/2105.11321v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Classifying Math KCs via Task-Adaptive Pre-Trained BERT", "abstract": "Educational content labeled with proper knowledge components (KCs) are\nparticularly useful to teachers or content organizers. However, manually\nlabeling educational content is labor intensive and error-prone. To address\nthis challenge, prior research proposed machine learning based solutions to\nauto-label educational content with limited success. In this work, we\nsignificantly improve prior research by (1) expanding the input types to\ninclude KC descriptions, instructional video titles, and problem descriptions\n(i.e., three types of prediction task), (2) doubling the granularity of the\nprediction from 198 to 385 KC labels (i.e., more practical setting but much\nharder multinomial classification problem), (3) improving the prediction\naccuracies by 0.5-2.3% using Task-adaptive Pre-trained BERT, outperforming six\nbaselines, and (4) proposing a simple evaluation measure by which we can\nrecover 56-73% of mispredicted KC labels. All codes and data sets in the\nexperiments are available at:https://github.com/tbs17/TAPT-BERT", "published": "2021-05-24 15:27:33", "link": "http://arxiv.org/abs/2105.11343v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "IITP at AILA 2019: System Report for Artificial Intelligence for Legal\n  Assistance Shared Task", "abstract": "In this article, we present a description of our systems as a part of our\nparticipation in the shared task namely Artificial Intelligence for Legal\nAssistance (AILA 2019). This is an integral event of Forum for Information\nRetrieval Evaluation-2019. The outcomes of this track would be helpful for the\nautomation of the working process of the Indian Judiciary System. The manual\nworking procedures and documentation at any level (from lower to higher court)\nof the judiciary system are very complex in nature. The systems produced as a\npart of this track would assist the law practitioners. It would be helpful for\ncommon men too. This kind of track also opens the path of research of Natural\nLanguage Processing (NLP) in the judicial domain. This track defined two\nproblems such as Task 1: Identifying relevant prior cases for a given situation\nand Task 2: Identifying the most relevant statutes for a given situation. We\ntackled both of them. Our proposed approaches are based on BM25 and Doc2Vec. As\nper the results declared by the task organizers, we are in 3rd and a modest\nposition in Task 1 and Task 2 respectively.", "published": "2021-05-24 15:31:24", "link": "http://arxiv.org/abs/2105.11347v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "View Distillation with Unlabeled Data for Extracting Adverse Drug\n  Effects from User-Generated Data", "abstract": "We present an algorithm based on multi-layer transformers for identifying\nAdverse Drug Reactions (ADR) in social media data. Our model relies on the\nproperties of the problem and the characteristics of contextual word embeddings\nto extract two views from documents. Then a classifier is trained on each view\nto label a set of unlabeled documents to be used as an initializer for a new\nclassifier in the other view. Finally, the initialized classifier in each view\nis further trained using the initial training examples. We evaluated our model\nin the largest publicly available ADR dataset. The experiments testify that our\nmodel significantly outperforms the transformer-based models pretrained on\ndomain-specific data.", "published": "2021-05-24 15:38:08", "link": "http://arxiv.org/abs/2105.11354v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reproducibility Report: Contextualizing Hate Speech Classifiers with\n  Post-hoc Explanation", "abstract": "The presented report evaluates Contextualizing Hate Speech Classifiers with\nPost-hoc Explanation paper within the scope of ML Reproducibility Challenge\n2020. Our work focuses on both aspects constituting the paper: the method\nitself and the validity of the stated results. In the following sections, we\nhave described the paper, related works, algorithmic frameworks, our\nexperiments and evaluations.", "published": "2021-05-24 17:02:24", "link": "http://arxiv.org/abs/2105.11412v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "i-Pulse: A NLP based novel approach for employee engagement in logistics\n  organization", "abstract": "Although most logistics and freight forwarding organizations, in one way or\nanother, claim to have core values. The engagement of employees is a vast\nstructure that affects almost every part of the company's core environmental\nvalues. There is little theoretical knowledge about the relationship between\nfirms and the engagement of employees. Based on research literature, this paper\naims to provide a novel approach for insight around employee engagement in a\nlogistics organization by implementing deep natural language processing\nconcepts. The artificial intelligence-enabled solution named Intelligent Pulse\n(I-Pulse) can evaluate hundreds and thousands of pulse survey comments and\nprovides the actionable insights and gist of employee feedback. I-Pulse allows\nthe stakeholders to think in new ways in their organization, helping them to\nhave a powerful influence on employee engagement, retention, and efficiency.\nThis study is of corresponding interest to researchers and practitioners.", "published": "2021-05-24 07:20:40", "link": "http://arxiv.org/abs/2106.07341v1", "categories": ["cs.SI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Unsupervised Speech Recognition", "abstract": "Despite rapid progress in the recent past, current speech recognition systems\nstill require labeled training data which limits this technology to a small\nfraction of the languages spoken around the globe. This paper describes\nwav2vec-U, short for wav2vec Unsupervised, a method to train speech recognition\nmodels without any labeled data. We leverage self-supervised speech\nrepresentations to segment unlabeled audio and learn a mapping from these\nrepresentations to phonemes via adversarial training. The right representations\nare key to the success of our method. Compared to the best previous\nunsupervised work, wav2vec-U reduces the phoneme error rate on the TIMIT\nbenchmark from 26.1 to 11.3. On the larger English Librispeech benchmark,\nwav2vec-U achieves a word error rate of 5.9 on test-other, rivaling some of the\nbest published systems trained on 960 hours of labeled data from only two years\nago. We also experiment on nine other languages, including low-resource\nlanguages such as Kyrgyz, Swahili and Tatar.", "published": "2021-05-24 04:10:47", "link": "http://arxiv.org/abs/2105.11084v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Self-Attention Networks Can Process Bounded Hierarchical Languages", "abstract": "Despite their impressive performance in NLP, self-attention networks were\nrecently proved to be limited for processing formal languages with hierarchical\nstructure, such as $\\mathsf{Dyck}_k$, the language consisting of well-nested\nparentheses of $k$ types. This suggested that natural language can be\napproximated well with models that are too weak for formal languages, or that\nthe role of hierarchy and recursion in natural language might be limited. We\nqualify this implication by proving that self-attention networks can process\n$\\mathsf{Dyck}_{k, D}$, the subset of $\\mathsf{Dyck}_{k}$ with depth bounded by\n$D$, which arguably better captures the bounded hierarchical structure of\nnatural language. Specifically, we construct a hard-attention network with\n$D+1$ layers and $O(\\log k)$ memory size (per token per layer) that recognizes\n$\\mathsf{Dyck}_{k, D}$, and a soft-attention network with two layers and\n$O(\\log k)$ memory size that generates $\\mathsf{Dyck}_{k, D}$. Experiments show\nthat self-attention networks trained on $\\mathsf{Dyck}_{k, D}$ generalize to\nlonger inputs with near-perfect accuracy, and also verify the theoretical\nmemory advantage of self-attention networks over recurrent networks.", "published": "2021-05-24 06:42:58", "link": "http://arxiv.org/abs/2105.11115v3", "categories": ["cs.CL", "cs.AI", "cs.FL"], "primary_category": "cs.CL"}
{"title": "True Few-Shot Learning with Language Models", "abstract": "Pretrained language models (LMs) perform well on many tasks even when\nlearning from a few examples, but prior work uses many held-out examples to\ntune various aspects of learning, such as hyperparameters, training objectives,\nand natural language templates (\"prompts\"). Here, we evaluate the few-shot\nability of LMs when such held-out examples are unavailable, a setting we call\ntrue few-shot learning. We test two model selection criteria, cross-validation\nand minimum description length, for choosing LM prompts and hyperparameters in\nthe true few-shot setting. On average, both marginally outperform random\nselection and greatly underperform selection based on held-out examples.\nMoreover, selection criteria often prefer models that perform significantly\nworse than randomly-selected ones. We find similar results even when taking\ninto account our uncertainty in a model's true performance during selection, as\nwell as when varying the amount of computation and number of examples used for\nselection. Overall, our findings suggest that prior work significantly\noverestimated the true few-shot ability of LMs given the difficulty of few-shot\nmodel selection.", "published": "2021-05-24 17:55:51", "link": "http://arxiv.org/abs/2105.11447v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "The advent and fall of a vocabulary learning bias from communicative\n  efficiency", "abstract": "Biosemiosis is a process of choice-making between simultaneously alternative\noptions. It is well-known that, when sufficiently young children encounter a\nnew word, they tend to interpret it as pointing to a meaning that does not have\na word yet in their lexicon rather than to a meaning that already has a word\nattached. In previous research, the strategy was shown to be optimal from an\ninformation theoretic standpoint. In that framework, interpretation is\nhypothesized to be driven by the minimization of a cost function: the option of\nleast communication cost is chosen. However, the information theoretic model\nemployed in that research neither explains the weakening of that vocabulary\nlearning bias in older children or polylinguals nor reproduces Zipf's\nmeaning-frequency law, namely the non-linear relationship between the number of\nmeanings of a word and its frequency. Here we consider a generalization of the\nmodel that is channeled to reproduce that law. The analysis of the new model\nreveals regions of the phase space where the bias disappears consistently with\nthe weakening or loss of the bias in older children or polylinguals. The model\nis abstract enough to support future research on other levels of life that are\nrelevant to biosemiotics. In the deep learning era, the model is a transparent\nlow-dimensional tool for future experimental research and illustrates the\npredictive power of a theoretical framework originally designed to shed light\non the origins of Zipf's rank-frequency law.", "published": "2021-05-24 20:13:27", "link": "http://arxiv.org/abs/2105.11519v3", "categories": ["cs.CL", "cs.IT", "math.IT", "physics.soc-ph"], "primary_category": "cs.CL"}
