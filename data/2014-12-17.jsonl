{"title": "Computational Model to Generate Case-Inflected Forms of Masculine Nouns\n  for Word Search in Sanskrit E-Text", "abstract": "The problem of word search in Sanskrit is inseparable from complexities that\ninclude those caused by euphonic conjunctions and case-inflections. The\ncase-inflectional forms of a noun normally number 24 owing to the fact that in\nSanskrit there are eight cases and three numbers-singular, dual and plural. The\ntraditional method of generating these inflectional forms is rather elaborate\nowing to the fact that there are differences in the forms generated between\neven very similar words and there are subtle nuances involved. Further, it\nwould be a cumbersome exercise to generate and search for 24 forms of a word\nduring a word search in a large text, using the currently available\ncase-inflectional form generators. This study presents a new approach to\ngenerating case-inflectional forms that is simpler to compute. Further, an\noptimized model that is sufficient for generating only those word forms that\nare required in a word search and is more than 80% efficient compared to the\ncomplete case-inflectional forms generator, is presented in this study for the\nfirst time.", "published": "2014-12-17 16:56:43", "link": "http://arxiv.org/abs/1412.5477v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Network Topic Model: A Simple but General Solution for Short and\n  Imbalanced Texts", "abstract": "The short text has been the prevalent format for information of Internet in\nrecent decades, especially with the development of online social media, whose\nmillions of users generate a vast number of short messages everyday. Although\nsophisticated signals delivered by the short text make it a promising source\nfor topic modeling, its extreme sparsity and imbalance brings unprecedented\nchallenges to conventional topic models like LDA and its variants. Aiming at\npresenting a simple but general solution for topic modeling in short texts, we\npresent a word co-occurrence network based model named WNTM to tackle the\nsparsity and imbalance simultaneously. Different from previous approaches, WNTM\nmodels the distribution over topics for each word instead of learning topics\nfor each document, which successfully enhance the semantic density of data\nspace without importing too much time or space complexity. Meanwhile, the rich\ncontextual information preserved in the word-word space also guarantees its\nsensitivity in identifying rare topics with convincing quality. Furthermore,\nemploying the same Gibbs sampling with LDA makes WNTM easily to be extended to\nvarious application scenarios. Extensive validations on both short and normal\ntexts testify the outperformance of WNTM as compared to baseline methods. And\nfinally we also demonstrate its potential in precisely discovering newly\nemerging topics or unexpected events in Weibo at pretty early stages.", "published": "2014-12-17 14:18:52", "link": "http://arxiv.org/abs/1412.5404v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Extended Recommendation Framework: Generating the Text of a User Review\n  as a Personalized Summary", "abstract": "We propose to augment rating based recommender systems by providing the user\nwith additional information which might help him in his choice or in the\nunderstanding of the recommendation. We consider here as a new task, the\ngeneration of personalized reviews associated to items. We use an extractive\nsummary formulation for generating these reviews. We also show that the two\ninformation sources, ratings and items could be used both for estimating\nratings and for generating summaries, leading to improved performance for each\nsystem compared to the use of a single source. Besides these two contributions,\nwe show how a personalized polarity classifier can integrate the rating and\ntextual aspects. Overall, the proposed system offers the user three\npersonalized hints for a recommendation: rating, text and polarity. We evaluate\nthese three components on two datasets using appropriate measures for each\ntask.", "published": "2014-12-17 15:46:28", "link": "http://arxiv.org/abs/1412.5448v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Effective sampling for large-scale automated writing evaluation systems", "abstract": "Automated writing evaluation (AWE) has been shown to be an effective\nmechanism for quickly providing feedback to students. It has already seen wide\nadoption in enterprise-scale applications and is starting to be adopted in\nlarge-scale contexts. Training an AWE model has historically required a single\nbatch of several hundred writing examples and human scores for each of them.\nThis requirement limits large-scale adoption of AWE since human-scoring essays\nis costly. Here we evaluate algorithms for ensuring that AWE models are\nconsistently trained using the most informative essays. Our results show how to\nminimize training set sizes while maximizing predictive performance, thereby\nreducing cost without unduly sacrificing accuracy. We conclude with a\ndiscussion of how to integrate this approach into large-scale AWE systems.", "published": "2014-12-17 22:41:14", "link": "http://arxiv.org/abs/1412.5659v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Entity-Augmented Distributional Semantics for Discourse Relations", "abstract": "Discourse relations bind smaller linguistic elements into coherent texts.\nHowever, automatically identifying discourse relations is difficult, because it\nrequires understanding the semantics of the linked sentences. A more subtle\nchallenge is that it is not enough to represent the meaning of each sentence of\na discourse relation, because the relation may depend on links between\nlower-level elements, such as entity mentions. Our solution computes\ndistributional meaning representations by composition up the syntactic parse\ntree. A key difference from previous work on compositional distributional\nsemantics is that we also compute representations for entity mentions, using a\nnovel downward compositional pass. Discourse relations are predicted not only\nfrom the distributional representations of the sentences, but also of their\ncoreferent entity mentions. The resulting system obtains substantial\nimprovements over the previous state-of-the-art in predicting implicit\ndiscourse relations in the Penn Discourse Treebank.", "published": "2014-12-17 23:26:48", "link": "http://arxiv.org/abs/1412.5673v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Ensemble of Generative and Discriminative Techniques for Sentiment\n  Analysis of Movie Reviews", "abstract": "Sentiment analysis is a common task in natural language processing that aims\nto detect polarity of a text document (typically a consumer review). In the\nsimplest settings, we discriminate only between positive and negative\nsentiment, turning the task into a standard binary classification problem. We\ncompare several ma- chine learning approaches to this problem, and combine them\nto achieve the best possible results. We show how to use for this task the\nstandard generative lan- guage models, which are slightly complementary to the\nstate of the art techniques. We achieve strong results on a well-known dataset\nof IMDB movie reviews. Our results are easily reproducible, as we publish also\nthe code needed to repeat the experiments. This should simplify further advance\nof the state of the art, as other researchers can combine their techniques with\nours with little effort.", "published": "2014-12-17 11:02:04", "link": "http://arxiv.org/abs/1412.5335v7", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Deep Speech: Scaling up end-to-end speech recognition", "abstract": "We present a state-of-the-art speech recognition system developed using\nend-to-end deep learning. Our architecture is significantly simpler than\ntraditional speech systems, which rely on laboriously engineered processing\npipelines; these traditional systems also tend to perform poorly when used in\nnoisy environments. In contrast, our system does not need hand-designed\ncomponents to model background noise, reverberation, or speaker variation, but\ninstead directly learns a function that is robust to such effects. We do not\nneed a phoneme dictionary, nor even the concept of a \"phoneme.\" Key to our\napproach is a well-optimized RNN training system that uses multiple GPUs, as\nwell as a set of novel data synthesis techniques that allow us to efficiently\nobtain a large amount of varied data for training. Our system, called Deep\nSpeech, outperforms previously published results on the widely studied\nSwitchboard Hub5'00, achieving 16.0% error on the full test set. Deep Speech\nalso handles challenging noisy environments better than widely used,\nstate-of-the-art commercial speech systems.", "published": "2014-12-17 20:39:45", "link": "http://arxiv.org/abs/1412.5567v2", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
