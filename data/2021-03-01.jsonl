{"title": "Combat COVID-19 Infodemic Using Explainable Natural Language Processing\n  Models", "abstract": "Misinformation of COVID-19 is prevalent on social media as the pandemic\nunfolds, and the associated risks are extremely high. Thus, it is critical to\ndetect and combat such misinformation. Recently, deep learning models using\nnatural language processing techniques, such as BERT (Bidirectional Encoder\nRepresentations from Transformers), have achieved great successes in detecting\nmisinformation. In this paper, we proposed an explainable natural language\nprocessing model based on DistilBERT and SHAP (Shapley Additive exPlanations)\nto combat misinformation about COVID-19 due to their efficiency and\neffectiveness. First, we collected a dataset of 984 claims about COVID-19 with\nfact checking. By augmenting the data using back-translation, we doubled the\nsample size of the dataset and the DistilBERT model was able to obtain good\nperformance (accuracy: 0.972; areas under the curve: 0.993) in detecting\nmisinformation about COVID-19. Our model was also tested on a larger dataset\nfor AAAI2021 - COVID-19 Fake News Detection Shared Task and obtained good\nperformance (accuracy: 0.938; areas under the curve: 0.985). The performance on\nboth datasets was better than traditional machine learning models. Second, in\norder to boost public trust in model prediction, we employed SHAP to improve\nmodel explainability, which was further evaluated using a between-subjects\nexperiment with three conditions, i.e., text (T), text+SHAP explanation (TSE),\nand text+SHAP explanation+source and evidence (TSESE). The participants were\nsignificantly more likely to trust and share information related to COVID-19 in\nthe TSE and TSESE conditions than in the T condition. Our results provided good\nimplications in detecting misinformation about COVID-19 and improving public\ntrust.", "published": "2021-03-01 04:28:39", "link": "http://arxiv.org/abs/2103.00747v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Long Document Summarization in a Low Resource Setting using Pretrained\n  Language Models", "abstract": "Abstractive summarization is the task of compressing a long document into a\ncoherent short document while retaining salient information. Modern abstractive\nsummarization methods are based on deep neural networks which often require\nlarge training datasets. Since collecting summarization datasets is an\nexpensive and time-consuming task, practical industrial settings are usually\nlow-resource. In this paper, we study a challenging low-resource setting of\nsummarizing long legal briefs with an average source document length of 4268\nwords and only 120 available (document, summary) pairs. To account for data\nscarcity, we used a modern pretrained abstractive summarizer BART (Lewis et\nal., 2020), which only achieves 17.9 ROUGE-L as it struggles with long\ndocuments. We thus attempt to compress these long documents by identifying\nsalient sentences in the source which best ground the summary, using a novel\nalgorithm based on GPT-2 (Radford et al., 2019) language model perplexity\nscores, that operates within the low resource regime. On feeding the compressed\ndocuments to BART, we observe a 6.0 ROUGE-L improvement. Our method also beats\nseveral competitive salience detection baselines. Furthermore, the identified\nsalient sentences tend to agree with an independent human labeling by domain\nexperts.", "published": "2021-03-01 04:43:55", "link": "http://arxiv.org/abs/2103.00751v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "M6: A Chinese Multimodal Pretrainer", "abstract": "In this work, we construct the largest dataset for multimodal pretraining in\nChinese, which consists of over 1.9TB images and 292GB texts that cover a wide\nrange of domains. We propose a cross-modal pretraining method called M6,\nreferring to Multi-Modality to Multi-Modality Multitask Mega-transformer, for\nunified pretraining on the data of single modality and multiple modalities. We\nscale the model size up to 10 billion and 100 billion parameters, and build the\nlargest pretrained model in Chinese. We apply the model to a series of\ndownstream applications, and demonstrate its outstanding performance in\ncomparison with strong baselines. Furthermore, we specifically design a\ndownstream task of text-guided image generation, and show that the finetuned M6\ncan create high-quality images with high resolution and abundant details.", "published": "2021-03-01 07:46:27", "link": "http://arxiv.org/abs/2103.00823v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Vy\u0101karana: A Colorless Green Benchmark for Syntactic Evaluation in\n  Indic Languages", "abstract": "While there has been significant progress towards developing NLU resources\nfor Indic languages, syntactic evaluation has been relatively less explored.\nUnlike English, Indic languages have rich morphosyntax, grammatical genders,\nfree linear word-order, and highly inflectional morphology. In this paper, we\nintroduce Vy\\=akarana: a benchmark of Colorless Green sentences in Indic\nlanguages for syntactic evaluation of multilingual language models. The\nbenchmark comprises four syntax-related tasks: PoS Tagging, Syntax Tree-depth\nPrediction, Grammatical Case Marking, and Subject-Verb Agreement. We use the\ndatasets from the evaluation tasks to probe five multilingual language models\nof varying architectures for syntax in Indic languages. Due to its prevalence,\nwe also include a code-switching setting in our experiments. Our results show\nthat the token-level and sentence-level representations from the Indic language\nmodels (IndicBERT and MuRIL) do not capture the syntax in Indic languages as\nefficiently as the other highly multilingual language models. Further, our\nlayer-wise probing experiments reveal that while mBERT, DistilmBERT, and XLM-R\nlocalize the syntax in middle layers, the Indic language models do not show\nsuch syntactic localization.", "published": "2021-03-01 09:07:58", "link": "http://arxiv.org/abs/2103.00854v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting MARBERT for Improved Arabic Dialect Identification: Submission\n  to the NADI 2021 Shared Task", "abstract": "In this paper, we tackle the Nuanced Arabic Dialect Identification (NADI)\nshared task (Abdul-Mageed et al., 2021) and demonstrate state-of-the-art\nresults on all of its four subtasks. Tasks are to identify the geographic\norigin of short Dialectal (DA) and Modern Standard Arabic (MSA) utterances at\nthe levels of both country and province. Our final model is an ensemble of\nvariants built on top of MARBERT that achieves an F1-score of 34.03% for DA at\nthe country-level development set -- an improvement of 7.63% from previous\nwork.", "published": "2021-03-01 15:19:56", "link": "http://arxiv.org/abs/2103.01065v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis of Users' Reviews on COVID-19 Contact Tracing Apps\n  with a Benchmark Dataset", "abstract": "Contact tracing has been globally adopted in the fight to control the\ninfection rate of COVID-19. Thanks to digital technologies, such as smartphones\nand wearable devices, contacts of COVID-19 patients can be easily traced and\ninformed about their potential exposure to the virus. To this aim, several\ninteresting mobile applications have been developed. However, there are\never-growing concerns over the working mechanism and performance of these\napplications. The literature already provides some interesting exploratory\nstudies on the community's response to the applications by analyzing\ninformation from different sources, such as news and users' reviews of the\napplications. However, to the best of our knowledge, there is no existing\nsolution that automatically analyzes users' reviews and extracts the evoked\nsentiments. In this work, we propose a pipeline starting from manual annotation\nvia a crowd-sourcing study and concluding on the development and training of AI\nmodels for automatic sentiment analysis of users' reviews. In total, we employ\neight different methods achieving up to an average F1-Scores 94.8% indicating\nthe feasibility of automatic sentiment analysis of users' reviews on the\nCOVID-19 contact tracing applications. We also highlight the key advantages,\ndrawbacks, and users' concerns over the applications. Moreover, we also collect\nand annotate a large-scale dataset composed of 34,534 reviews manually\nannotated from the contract tracing applications of 46 distinct countries. The\npresented analysis and the dataset are expected to provide a baseline/benchmark\nfor future research in the domain.", "published": "2021-03-01 18:43:10", "link": "http://arxiv.org/abs/2103.01196v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Effectiveness of Dataset Embeddings in Mono-lingual,Multi-lingual\n  and Zero-shot Conditions", "abstract": "Recent complementary strands of research have shown that leveraging\ninformation on the data source through encoding their properties into\nembeddings can lead to performance increase when training a single model on\nheterogeneous data sources. However, it remains unclear in which situations\nthese dataset embeddings are most effective, because they are used in a large\nvariety of settings, languages and tasks. Furthermore, it is usually assumed\nthat gold information on the data source is available, and that the test data\nis from a distribution seen during training. In this work, we compare the\neffect of dataset embeddings in mono-lingual settings, multi-lingual settings,\nand with predicted data source label in a zero-shot setting. We evaluate on\nthree morphosyntactic tasks: morphological tagging, lemmatization, and\ndependency parsing, and use 104 datasets, 66 languages, and two different\ndataset grouping strategies. Performance increases are highest when the\ndatasets are of the same language, and we know from which distribution the\ntest-instance is drawn. In contrast, for setups where the data is from an\nunseen distribution, performance increase vanishes.", "published": "2021-03-01 19:34:32", "link": "http://arxiv.org/abs/2103.01273v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DEUS: A Data-driven Approach to Estimate User Satisfaction in Multi-turn\n  Dialogues", "abstract": "Digital assistants are experiencing rapid growth due to their ability to\nassist users with day-to-day tasks where most dialogues are happening\nmulti-turn. However, evaluating multi-turn dialogues remains challenging,\nespecially at scale. We suggest a context-sensitive method to estimate the\nturn-level satisfaction for dialogue considering various types of user\npreferences. The costs of interactions between users and dialogue systems are\nformulated using a budget consumption concept. We assume users have an initial\ninteraction budget for a dialogue formed based on the task complexity and that\neach turn has a cost. When the task is completed, or the budget has been\nexhausted, users quit the dialogue. We demonstrate our method's effectiveness\nby extensive experimentation with a simulated dialogue platform and real\nmulti-turn dialogues.", "published": "2021-03-01 20:00:28", "link": "http://arxiv.org/abs/2103.01287v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ToxCCIn: Toxic Content Classification with Interpretability", "abstract": "Despite the recent successes of transformer-based models in terms of\neffectiveness on a variety of tasks, their decisions often remain opaque to\nhumans. Explanations are particularly important for tasks like offensive\nlanguage or toxicity detection on social media because a manual appeal process\nis often in place to dispute automatically flagged content. In this work, we\npropose a technique to improve the interpretability of these models, based on a\nsimple and powerful assumption: a post is at least as toxic as its most toxic\nspan. We incorporate this assumption into transformer models by scoring a post\nbased on the maximum toxicity of its spans and augmenting the training process\nto identify correct spans. We find this approach effective and can produce\nexplanations that exceed the quality of those provided by Logistic Regression\nanalysis (often regarded as a highly-interpretable model), according to a human\nstudy.", "published": "2021-03-01 22:17:10", "link": "http://arxiv.org/abs/2103.01328v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emotion Dynamics in Movie Dialogues", "abstract": "Emotion dynamics is a framework for measuring how an individual's emotions\nchange over time. It is a powerful tool for understanding how we behave and\ninteract with the world. In this paper, we introduce a framework to track\nemotion dynamics through one's utterances. Specifically we introduce a number\nof utterance emotion dynamics (UED) metrics inspired by work in Psychology. We\nuse this approach to trace emotional arcs of movie characters. We analyze\nthousands of such character arcs to test hypotheses that inform our broader\nunderstanding of stories. Notably, we show that there is a tendency for\ncharacters to use increasingly more negative words and become increasingly\nemotionally discordant with each other until about 90 percent of the narrative\nlength. UED also has applications in behavior studies, social sciences, and\npublic health.", "published": "2021-03-01 23:02:16", "link": "http://arxiv.org/abs/2103.01345v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unbiased Sentence Encoder For Large-Scale Multi-lingual Search Engines", "abstract": "In this paper, we present a multi-lingual sentence encoder that can be used\nin search engines as a query and document encoder. This embedding enables a\nsemantic similarity score between queries and documents that can be an\nimportant feature in document ranking and relevancy. To train such a customized\nsentence encoder, it is beneficial to leverage users search data in the form of\nquery-document clicked pairs however, we must avoid relying too much on search\nclick data as it is biased and does not cover many unseen cases. The search\ndata is heavily skewed towards short queries and for long queries is small and\noften noisy. The goal is to design a universal multi-lingual encoder that works\nfor all cases and covers both short and long queries. We select a number of\npublic NLI datasets in different languages and translation data and together\nwith user search data we train a language model using a multi-task approach. A\nchallenge is that these datasets are not homogeneous in terms of content, size\nand the balance ratio. While the public NLI datasets are usually two-sentence\nbased with the same portion of positive and negative pairs, the user search\ndata can contain multi-sentence documents and only positive pairs. We show how\nmulti-task training enables us to leverage all these datasets and exploit\nknowledge sharing across these tasks.", "published": "2021-03-01 07:19:16", "link": "http://arxiv.org/abs/2106.07719v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BERT-based knowledge extraction method of unstructured domain text", "abstract": "With the development and business adoption of knowledge graph, there is an\nincreasing demand for extracting entities and relations of knowledge graphs\nfrom unstructured domain documents. This makes the automatic knowledge\nextraction for domain text quite meaningful. This paper proposes a knowledge\nextraction method based on BERT, which is used to extract knowledge points from\nunstructured specific domain texts (such as insurance clauses in the insurance\nindustry) automatically to save manpower of knowledge graph construction.\nDifferent from the commonly used methods which are based on rules, templates or\nentity extraction models, this paper converts the domain knowledge points into\nquestion and answer pairs and uses the text around the answer in documents as\nthe context. The method adopts a BERT-based model similar to BERT's SQuAD\nreading comprehension task. The model is fine-tuned. And it is used to directly\nextract knowledge points from more insurance clauses. According to the test\nresults, the model performance is good.", "published": "2021-03-01 03:24:35", "link": "http://arxiv.org/abs/2103.00728v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RAGA: Relation-aware Graph Attention Networks for Global Entity\n  Alignment", "abstract": "Entity alignment (EA) is the task to discover entities referring to the same\nreal-world object from different knowledge graphs (KGs), which is the most\ncrucial step in integrating multi-source KGs. The majority of the existing\nembeddings-based entity alignment methods embed entities and relations into a\nvector space based on relation triples of KGs for local alignment. As these\nmethods insufficiently consider the multiple relations between entities, the\nstructure information of KGs has not been fully leveraged. In this paper, we\npropose a novel framework based on Relation-aware Graph Attention Networks to\ncapture the interactions between entities and relations. Our framework adopts\nthe self-attention mechanism to spread entity information to the relations and\nthen aggregate relation information back to entities. Furthermore, we propose a\nglobal alignment algorithm to make one-to-one entity alignments with a\nfine-grained similarity matrix. Experiments on three real-world cross-lingual\ndatasets show that our framework outperforms the state-of-the-art methods.", "published": "2021-03-01 06:30:51", "link": "http://arxiv.org/abs/2103.00791v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deep Bag-of-Sub-Emotions for Depression Detection in Social Media", "abstract": "This paper presents the Deep Bag-of-Sub-Emotions (DeepBoSE), a novel deep\nlearning model for depression detection in social media. The model is\nformulated such that it internally computes a differentiable Bag-of-Features\n(BoF) representation that incorporates emotional information. This is achieved\nby a reinterpretation of classical weighting schemes like term\nfrequency-inverse document frequency into probabilistic deep learning\noperations. An important advantage of the proposed method is that it can be\ntrained under the transfer learning paradigm, which is useful to enhance\nconventional BoF models that cannot be directly integrated into deep learning\narchitectures. Experiments were performed in the eRisk17 and eRisk18 datasets\nfor the depression detection task; results show that DeepBoSE outperforms\nconventional BoF representations and it is competitive with the state of the\nart, achieving a F1-score over the positive class of 0.64 in eRisk17 and 0.65\nin eRisk18.", "published": "2021-03-01 22:39:47", "link": "http://arxiv.org/abs/2103.01334v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Token-Modification Adversarial Attacks for Natural Language Processing:\n  A Survey", "abstract": "Many adversarial attacks target natural language processing systems, most of\nwhich succeed through modifying the individual tokens of a document. Despite\nthe apparent uniqueness of each of these attacks, fundamentally they are simply\na distinct configuration of four components: a goal function, allowable\ntransformations, a search method, and constraints. In this survey, we\nsystematically present the different components used throughout the literature,\nusing an attack-independent framework which allows for easy comparison and\ncategorisation of components. Our work aims to serve as a comprehensive guide\nfor newcomers to the field and to spark targeted research into refining the\nindividual attack components.", "published": "2021-03-01 01:00:09", "link": "http://arxiv.org/abs/2103.00676v3", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Reasoning Paths over Semantic Graphs for Video-grounded\n  Dialogues", "abstract": "Compared to traditional visual question answering, video-grounded dialogues\nrequire additional reasoning over dialogue context to answer questions in a\nmulti-turn setting. Previous approaches to video-grounded dialogues mostly use\ndialogue context as a simple text input without modelling the inherent\ninformation flows at the turn level. In this paper, we propose a novel\nframework of Reasoning Paths in Dialogue Context (PDC). PDC model discovers\ninformation flows among dialogue turns through a semantic graph constructed\nbased on lexical components in each question and answer. PDC model then learns\nto predict reasoning paths over this semantic graph. Our path prediction model\npredicts a path from the current turn through past dialogue turns that contain\nadditional visual cues to answer the current question. Our reasoning model\nsequentially processes both visual and textual information through this\nreasoning path and the propagated features are used to generate the answer. Our\nexperimental results demonstrate the effectiveness of our method and provide\nadditional insights on how models use semantic dependencies in a dialogue\ncontext to retrieve visual cues.", "published": "2021-03-01 07:39:26", "link": "http://arxiv.org/abs/2103.00820v2", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "AdaSpeech: Adaptive Text to Speech for Custom Voice", "abstract": "Custom voice, a specific text to speech (TTS) service in commercial speech\nplatforms, aims to adapt a source TTS model to synthesize personal voice for a\ntarget speaker using few speech data. Custom voice presents two unique\nchallenges for TTS adaptation: 1) to support diverse customers, the adaptation\nmodel needs to handle diverse acoustic conditions that could be very different\nfrom source speech data, and 2) to support a large number of customers, the\nadaptation parameters need to be small enough for each target speaker to reduce\nmemory usage while maintaining high voice quality. In this work, we propose\nAdaSpeech, an adaptive TTS system for high-quality and efficient customization\nof new voices. We design several techniques in AdaSpeech to address the two\nchallenges in custom voice: 1) To handle different acoustic conditions, we use\ntwo acoustic encoders to extract an utterance-level vector and a sequence of\nphoneme-level vectors from the target speech during training; in inference, we\nextract the utterance-level vector from a reference speech and use an acoustic\npredictor to predict the phoneme-level vectors. 2) To better trade off the\nadaptation parameters and voice quality, we introduce conditional layer\nnormalization in the mel-spectrogram decoder of AdaSpeech, and fine-tune this\npart in addition to speaker embedding for adaptation. We pre-train the source\nTTS model on LibriTTS datasets and fine-tune it on VCTK and LJSpeech datasets\n(with different acoustic conditions from LibriTTS) with few adaptation data,\ne.g., 20 sentences, about 1 minute speech. Experiment results show that\nAdaSpeech achieves much better adaptation quality than baseline methods, with\nonly about 5K specific parameters for each speaker, which demonstrates its\neffectiveness for custom voice. Audio samples are available at\nhttps://speechresearch.github.io/adaspeech/.", "published": "2021-03-01 13:28:59", "link": "http://arxiv.org/abs/2103.00993v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "OmniNet: Omnidirectional Representations from Transformers", "abstract": "This paper proposes Omnidirectional Representations from Transformers\n(OmniNet). In OmniNet, instead of maintaining a strictly horizontal receptive\nfield, each token is allowed to attend to all tokens in the entire network.\nThis process can also be interpreted as a form of extreme or intensive\nattention mechanism that has the receptive field of the entire width and depth\nof the network. To this end, the omnidirectional attention is learned via a\nmeta-learner, which is essentially another self-attention based model. In order\nto mitigate the computationally expensive costs of full receptive field\nattention, we leverage efficient self-attention models such as kernel-based\n(Choromanski et al.), low-rank attention (Wang et al.) and/or Big Bird (Zaheer\net al.) as the meta-learner. Extensive experiments are conducted on\nautoregressive language modeling (LM1B, C4), Machine Translation, Long Range\nArena (LRA), and Image Recognition. The experiments show that OmniNet achieves\nconsiderable improvements across these tasks, including achieving\nstate-of-the-art performance on LM1B, WMT'14 En-De/En-Fr, and Long Range Arena.\nMoreover, using omnidirectional representation in Vision Transformers leads to\nsignificant improvements on image recognition tasks on both few-shot learning\nand fine-tuning setups.", "published": "2021-03-01 15:31:54", "link": "http://arxiv.org/abs/2103.01075v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "The Healthy States of America: Creating a Health Taxonomy with Social\n  Media", "abstract": "Since the uptake of social media, researchers have mined online discussions\nto track the outbreak and evolution of specific diseases or chronic conditions\nsuch as influenza or depression. To broaden the set of diseases under study, we\ndeveloped a Deep Learning tool for Natural Language Processing that extracts\nmentions of virtually any medical condition or disease from unstructured social\nmedia text. With that tool at hand, we processed Reddit and Twitter posts,\nanalyzed the clusters of the two resulting co-occurrence networks of\nconditions, and discovered that they correspond to well-defined categories of\nmedical conditions. This resulted in the creation of the first comprehensive\ntaxonomy of medical conditions automatically derived from online discussions.\nWe validated the structure of our taxonomy against the official International\nStatistical Classification of Diseases and Related Health Problems (ICD-11),\nfinding matches of our clusters with 20 official categories, out of 22. Based\non the mentions of our taxonomy's sub-categories on Reddit posts geo-referenced\nin the U.S., we were then able to compute disease-specific health scores. As\nopposed to counts of disease mentions or counts with no knowledge of our\ntaxonomy's structure, we found that our disease-specific health scores are\ncausally linked with the officially reported prevalence of 18 conditions.", "published": "2021-03-01 18:07:47", "link": "http://arxiv.org/abs/2103.01169v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Generative Adversarial Transformers", "abstract": "We introduce the GANformer, a novel and efficient type of transformer, and\nexplore it for the task of visual generative modeling. The network employs a\nbipartite structure that enables long-range interactions across the image,\nwhile maintaining computation of linear efficiency, that can readily scale to\nhigh-resolution synthesis. It iteratively propagates information from a set of\nlatent variables to the evolving visual features and vice versa, to support the\nrefinement of each in light of the other and encourage the emergence of\ncompositional representations of objects and scenes. In contrast to the classic\ntransformer architecture, it utilizes multiplicative integration that allows\nflexible region-based modulation, and can thus be seen as a generalization of\nthe successful StyleGAN network. We demonstrate the model's strength and\nrobustness through a careful evaluation over a range of datasets, from\nsimulated multi-object environments to rich real-world indoor and outdoor\nscenes, showing it achieves state-of-the-art results in terms of image quality\nand diversity, while enjoying fast learning and better data-efficiency. Further\nqualitative and quantitative experiments offer us an insight into the model's\ninner workings, revealing improved interpretability and stronger\ndisentanglement, and illustrating the benefits and efficacy of our approach. An\nimplementation of the model is available at\nhttps://github.com/dorarad/gansformer.", "published": "2021-03-01 18:54:04", "link": "http://arxiv.org/abs/2103.01209v4", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Cryptonite: A Cryptic Crossword Benchmark for Extreme Ambiguity in\n  Language", "abstract": "Current NLP datasets targeting ambiguity can be solved by a native speaker\nwith relative ease. We present Cryptonite, a large-scale dataset based on\ncryptic crosswords, which is both linguistically complex and naturally sourced.\nEach example in Cryptonite is a cryptic clue, a short phrase or sentence with a\nmisleading surface reading, whose solving requires disambiguating semantic,\nsyntactic, and phonetic wordplays, as well as world knowledge. Cryptic clues\npose a challenge even for experienced solvers, though top-tier experts can\nsolve them with almost 100% accuracy. Cryptonite is a challenging task for\ncurrent models; fine-tuning T5-Large on 470k cryptic clues achieves only 7.6%\naccuracy, on par with the accuracy of a rule-based clue solver (8.6%).", "published": "2021-03-01 19:01:01", "link": "http://arxiv.org/abs/2103.01242v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Roosterize: Suggesting Lemma Names for Coq Verification Projects Using\n  Deep Learning", "abstract": "Naming conventions are an important concern in large verification projects\nusing proof assistants, such as Coq. In particular, lemma names are used by\nproof engineers to effectively understand and modify Coq code. However,\nproviding accurate and informative lemma names is a complex task, which is\ncurrently often carried out manually. Even when lemma naming is automated using\nrule-based tools, generated names may fail to adhere to important conventions\nnot specified explicitly. We demonstrate a toolchain, dubbed Roosterize, which\nautomatically suggests lemma names in Coq projects. Roosterize leverages a\nneural network model trained on existing Coq code, thus avoiding manual\nspecification of naming conventions. To allow proof engineers to conveniently\naccess suggestions from Roosterize during Coq project development, we\nintegrated the toolchain into the popular Visual Studio Code editor. Our\nevaluation shows that Roosterize substantially outperforms strong baselines for\nsuggesting lemma names and is useful in practice. The demo video for Roosterize\ncan be viewed at: https://youtu.be/HZ5ac7Q14rc.", "published": "2021-03-01 23:07:44", "link": "http://arxiv.org/abs/2103.01346v2", "categories": ["cs.PL", "cs.CL", "cs.SE"], "primary_category": "cs.PL"}
{"title": "BERT based patent novelty search by training claims to their own\n  description", "abstract": "In this paper we present a method to concatenate patent claims to their own\ndescription. By applying this method, BERT trains suitable descriptions for\nclaims. Such a trained BERT (claim-to-description- BERT) could be able to\nidentify novelty relevant descriptions for patents. In addition, we introduce a\nnew scoring scheme, relevance scoring or novelty scoring, to process the output\nof BERT in a meaningful way. We tested the method on patent applications by\ntraining BERT on the first claims of patents and corresponding descriptions.\nBERT's output has been processed according to the relevance score and the\nresults compared with the cited X documents in the search reports. The test\nshowed that BERT has scored some of the cited X documents as highly relevant.", "published": "2021-03-01 16:54:50", "link": "http://arxiv.org/abs/2103.01126v4", "categories": ["stat.ML", "cs.CL", "cs.LG", "econ.EM", "math.ST", "stat.TH"], "primary_category": "stat.ML"}
{"title": "Fast threshold optimization for multi-label audio tagging using\n  Surrogate gradient learning", "abstract": "Multi-label audio tagging consists of assigning sets of tags to audio\nrecordings. At inference time, thresholds are applied on the confidence scores\noutputted by a probabilistic classifier, in order to decide which classes are\ndetected active. In this work, we consider having at disposal a trained\nclassifier and we seek to automatically optimize the decision thresholds\naccording to a performance metric of interest, in our case F-measure\n(micro-F1). We propose a new method, called SGL-Thresh for Surrogate Gradient\nLearning of Thresholds, that makes use of gradient descent. Since F1 is not\ndifferentiable, we propose to approximate the thresholding operation gradients\nwith the gradients of a sigmoid function. We report experiments on three\ndatasets, using state-of-the-art pre-trained deep neural networks. In all\ncases, SGL-Thresh outperformed three other approaches: a default threshold\nvalue (defThresh), an heuristic search algorithm and a method estimating F1\ngradients numerically. It reached 54.9\\% F1 on AudioSet eval, compared to 50.7%\nwith defThresh. SGL-Thresh is very fast and scalable to a large number of tags.\nTo facilitate reproducibility, data and source code in Pytorch are available\nonline: https://github.com/topel/SGL-Thresh", "published": "2021-03-01 08:05:07", "link": "http://arxiv.org/abs/2103.00833v1", "categories": ["cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
{"title": "Comparing acoustic analyses of speech data collected remotely", "abstract": "Face-to-face speech data collection has been next to impossible globally due\nto COVID-19 restrictions. To address this problem, simultaneous recordings of\nthree repetitions of the cardinal vowels were made using a Zoom H6 Handy\nRecorder with external microphone (henceforth H6) and compared with two\nalternatives accessible to potential participants at home: the Zoom meeting\napplication (henceforth Zoom) and two lossless mobile phone applications\n(Awesome Voice Recorder, and Recorder; henceforth Phone). F0 was tracked\naccurately by all devices; however, for formant analysis (F1, F2, F3) Phone\nperformed better than Zoom, i.e. more similarly to H6, though data extraction\nmethod (VoiceSauce, Praat) also resulted in differences. In addition, Zoom\nrecordings exhibited unexpected drops in intensity. The results suggest that\nlossless format phone recordings present a viable option for at least some\nphonetic studies.", "published": "2021-03-01 15:12:32", "link": "http://arxiv.org/abs/2103.01059v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Unsupervised Classification of Voiced Speech and Pitch Tracking Using\n  Forward-Backward Kalman Filtering", "abstract": "The detection of voiced speech, the estimation of the fundamental frequency,\nand the tracking of pitch values over time are crucial subtasks for a variety\nof speech processing techniques. Many different algorithms have been developed\nfor each of the three subtasks. We present a new algorithm that integrates the\nthree subtasks into a single procedure. The algorithm can be applied to\npre-recorded speech utterances in the presence of considerable amounts of\nbackground noise. We combine a collection of standard metrics, such as the\nzero-crossing rate, for example, to formulate an unsupervised voicing\nclassifier. The estimation of pitch values is accomplished with a hybrid\nautocorrelation-based technique. We propose a forward-backward Kalman filter to\nsmooth the estimated pitch contour. In experiments, we are able to show that\nthe proposed method compares favorably with current, state-of-the-art pitch\ndetection algorithms.", "published": "2021-03-01 18:13:23", "link": "http://arxiv.org/abs/2103.01173v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Contrastive Separative Coding for Self-supervised Representation\n  Learning", "abstract": "To extract robust deep representations from long sequential modeling of\nspeech data, we propose a self-supervised learning approach, namely Contrastive\nSeparative Coding (CSC). Our key finding is to learn such representations by\nseparating the target signal from contrastive interfering signals. First, a\nmulti-task separative encoder is built to extract shared separable and\ndiscriminative embedding; secondly, we propose a powerful cross-attention\nmechanism performed over speaker representations across various interfering\nconditions, allowing the model to focus on and globally aggregate the most\ncritical information to answer the \"query\" (current bottom-up embedding) while\npaying less attention to interfering, noisy, or irrelevant parts; lastly, we\nform a new probabilistic contrastive loss which estimates and maximizes the\nmutual information between the representations and the global speaker vector.\nWhile most prior unsupervised methods have focused on predicting the future,\nneighboring, or missing samples, we take a different perspective of predicting\nthe interfered samples. Moreover, our contrastive separative loss is free from\nnegative sampling. The experiment demonstrates that our approach can learn\nuseful representations achieving a strong speaker verification performance in\nadverse conditions.", "published": "2021-03-01 07:32:00", "link": "http://arxiv.org/abs/2103.00816v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Sandglasset: A Light Multi-Granularity Self-attentive Network For\n  Time-Domain Speech Separation", "abstract": "One of the leading single-channel speech separation (SS) models is based on a\nTasNet with a dual-path segmentation technique, where the size of each segment\nremains unchanged throughout all layers. In contrast, our key finding is that\nmulti-granularity features are essential for enhancing contextual modeling and\ncomputational efficiency. We introduce a self-attentive network with a novel\nsandglass-shape, namely Sandglasset, which advances the state-of-the-art (SOTA)\nSS performance at significantly smaller model size and computational cost.\nForward along each block inside Sandglasset, the temporal granularity of the\nfeatures gradually becomes coarser until reaching half of the network blocks,\nand then successively turns finer towards the raw signal level. We also unfold\nthat residual connections between features with the same granularity are\ncritical for preserving information after passing through the bottleneck layer.\nExperiments show our Sandglasset with only 2.3M parameters has achieved the\nbest results on two benchmark SS datasets -- WSJ0-2mix and WSJ0-3mix, where the\nSI-SNRi scores have been improved by absolute 0.8 dB and 2.4 dB, respectively,\ncomparing to the prior SOTA results.", "published": "2021-03-01 07:36:09", "link": "http://arxiv.org/abs/2103.00819v2", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
