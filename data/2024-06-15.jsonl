{"title": "Statistical arbitrage in multi-pair trading strategy based on graph clustering algorithms in US equities market", "abstract": "The study seeks to develop an effective strategy based on the novel framework\nof statistical arbitrage based on graph clustering algorithms. Amalgamation of\nquantitative and machine learning methods, including the Kelly criterion, and\nan ensemble of machine learning classifiers have been used to improve\nrisk-adjusted returns and increase immunity to transaction costs over existing\napproaches. The study seeks to provide an integrated approach to optimal signal\ndetection and risk management. As a part of this approach, innovative ways of\noptimizing take profit and stop loss functions for daily frequency trading\nstrategies have been proposed and tested. All of the tested approaches\noutperformed appropriate benchmarks. The best combinations of the techniques\nand parameters demonstrated significantly better performance metrics than the\nrelevant benchmarks. The results have been obtained under the assumption of\nrealistic transaction costs, but are sensitive to changes in some key\nparameters.", "published": "2024-06-15 17:25:32", "link": "http://arxiv.org/abs/2406.10695v1", "categories": ["q-fin.PM", "q-fin.TR", "stat.ML"], "primary_category": "q-fin.PM"}
{"title": "CancerLLM: A Large Language Model in Cancer Domain", "abstract": "Medical Large Language Models (LLMs) have demonstrated impressive performance\non a wide variety of medical NLP tasks; however, there still lacks a LLM\nspecifically designed for phenotyping identification and diagnosis in cancer\ndomain. Moreover, these LLMs typically have several billions of parameters,\nmaking them computationally expensive for healthcare systems. Thus, in this\nstudy, we propose CancerLLM, a model with 7 billion parameters and a\nMistral-style architecture, pre-trained on nearly 2.7M clinical notes and over\n515K pathology reports covering 17 cancer types, followed by fine-tuning on two\ncancer-relevant tasks, including cancer phenotypes extraction and cancer\ndiagnosis generation. Our evaluation demonstrated that the CancerLLM achieves\nstate-of-the-art results with F1 score of 91.78% on phenotyping extraction and\n86.81% on disganois generation. It outperformed existing LLMs, with an average\nF1 score improvement of 9.23%. Additionally, the CancerLLM demonstrated its\nefficiency on time and GPU usage, and robustness comparing with other LLMs. We\ndemonstrated that CancerLLM can potentially provide an effective and robust\nsolution to advance clinical research and practice in cancer domain", "published": "2024-06-15 01:02:48", "link": "http://arxiv.org/abs/2406.10459v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Personalized Pieces: Efficient Personalized Large Language Models\n  through Collaborative Efforts", "abstract": "Personalized large language models (LLMs) aim to tailor interactions,\ncontent, and recommendations to individual user preferences. While\nparameter-efficient fine-tuning (PEFT) methods excel in performance and\ngeneralization, they are costly and limit communal benefits when used\nindividually. To this end, we introduce Personalized Pieces (Per-Pcs), a\nframework that allows users to safely share and assemble personalized PEFT\nefficiently with collaborative efforts. Per-Pcs involves selecting sharers,\nbreaking their PEFT into pieces, and training gates for each piece. These\npieces are added to a pool, from which target users can select and assemble\npersonalized PEFT using their history data. This approach preserves privacy and\nenables fine-grained user modeling without excessive storage and computation\ndemands. Experimental results show Per-Pcs outperforms non-personalized and\nPEFT retrieval baselines, offering performance comparable to OPPU with\nsignificantly lower resource use across six tasks. Further analysis highlights\nPer-Pcs's robustness concerning sharer count and selection strategy, pieces\nsharing ratio, and scalability in computation time and storage space. Per-Pcs's\nmodularity promotes safe sharing, making LLM personalization more efficient,\neffective, and widely accessible through collaborative efforts.", "published": "2024-06-15 02:26:18", "link": "http://arxiv.org/abs/2406.10471v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Large Language Models Discriminate in Hiring Decisions on the Basis\n  of Race, Ethnicity, and Gender?", "abstract": "We examine whether large language models (LLMs) exhibit race- and\ngender-based name discrimination in hiring decisions, similar to classic\nfindings in the social sciences (Bertrand and Mullainathan, 2004). We design a\nseries of templatic prompts to LLMs to write an email to a named job applicant\ninforming them of a hiring decision. By manipulating the applicant's first\nname, we measure the effect of perceived race, ethnicity, and gender on the\nprobability that the LLM generates an acceptance or rejection email. We find\nthat the hiring decisions of LLMs in many settings are more likely to favor\nWhite applicants over Hispanic applicants. In aggregate, the groups with the\nhighest and lowest acceptance rates respectively are masculine White names and\nmasculine Hispanic names. However, the comparative acceptance rates by group\nvary under different templatic settings, suggesting that LLMs' race- and\ngender-sensitivity may be idiosyncratic and prompt-sensitive.", "published": "2024-06-15 03:31:16", "link": "http://arxiv.org/abs/2406.10486v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CroPrompt: Cross-task Interactive Prompting for Zero-shot Spoken\n  Language Understanding", "abstract": "Slot filling and intent detection are two highly correlated tasks in spoken\nlanguage understanding (SLU). Recent SLU research attempts to explore zero-shot\nprompting techniques in large language models to alleviate the data scarcity\nproblem. Nevertheless, the existing prompting work ignores the cross-task\ninteraction information for SLU, which leads to sub-optimal performance. To\nsolve this problem, we present the pioneering work of Cross-task Interactive\nPrompting (CroPrompt) for SLU, which enables the model to interactively\nleverage the information exchange across the correlated tasks in SLU.\nAdditionally, we further introduce a multi-task self-consistency mechanism to\nmitigate the error propagation caused by the intent information injection. We\nconduct extensive experiments on the standard SLU benchmark and the results\nreveal that CroPrompt consistently outperforms the existing prompting\napproaches. In addition, the multi-task self-consistency mechanism can\neffectively ease the error propagation issue, thereby enhancing the\nperformance. We hope this work can inspire more research on cross-task\nprompting for SLU.", "published": "2024-06-15 04:54:56", "link": "http://arxiv.org/abs/2406.10505v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Facts-and-Feelings: Capturing both Objectivity and Subjectivity in\n  Table-to-Text Generation", "abstract": "Table-to-text generation, a long-standing challenge in natural language\ngeneration, has remained unexplored through the lens of subjectivity.\nSubjectivity here encompasses the comprehension of information derived from the\ntable that cannot be described solely by objective data. Given the absence of\npre-existing datasets, we introduce the Ta2TS dataset with 3849 data instances.\nWe perform the task of fine-tuning sequence-to-sequence models on the\nlinearized tables and prompting on popular large language models. We analyze\nthe results from a quantitative and qualitative perspective to ensure the\ncapture of subjectivity and factual consistency. The analysis shows the\nfine-tuned LMs can perform close to the prompted LLMs. Both the models can\ncapture the tabular data, generating texts with 85.15% BERTScore and 26.28%\nMeteor score. To the best of our knowledge, we provide the first-of-its-kind\ndataset on tables with multiple genres and subjectivity included and present\nthe first comprehensive analysis and comparison of different LLM performances\non this task.", "published": "2024-06-15 08:41:44", "link": "http://arxiv.org/abs/2406.10560v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "We Care: Multimodal Depression Detection and Knowledge Infused Mental\n  Health Therapeutic Response Generation", "abstract": "The detection of depression through non-verbal cues has gained significant\nattention. Previous research predominantly centred on identifying depression\nwithin the confines of controlled laboratory environments, often with the\nsupervision of psychologists or counsellors. Unfortunately, datasets generated\nin such controlled settings may struggle to account for individual behaviours\nin real-life situations. In response to this limitation, we present the\nExtended D-vlog dataset, encompassing a collection of 1, 261 YouTube vlogs.\nAdditionally, the emergence of large language models (LLMs) like GPT3.5, and\nGPT4 has sparked interest in their potential they can act like mental health\nprofessionals. Yet, the readiness of these LLM models to be used in real-life\nsettings is still a concern as they can give wrong responses that can harm the\nusers. We introduce a virtual agent serving as an initial contact for mental\nhealth patients, offering Cognitive Behavioral Therapy (CBT)-based responses.\nIt comprises two core functions: 1. Identifying depression in individuals, and\n2. Delivering CBT-based therapeutic responses. Our Mistral model achieved\nimpressive scores of 70.1% and 30.9% for distortion assessment and\nclassification, along with a Bert score of 88.7%. Moreover, utilizing the TVLT\nmodel on our Multimodal Extended D-vlog Dataset yielded outstanding results,\nwith an impressive F1-score of 67.8%", "published": "2024-06-15 08:41:46", "link": "http://arxiv.org/abs/2406.10561v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Concentrate Attention: Towards Domain-Generalizable Prompt Optimization\n  for Language Models", "abstract": "Recent advances in prompt optimization have notably enhanced the performance\nof pre-trained language models (PLMs) on downstream tasks. However, the\npotential of optimized prompts on domain generalization has been\nunder-explored. To explore the nature of prompt generalization on unknown\ndomains, we conduct pilot experiments and find that (i) Prompts gaining more\nattention weight from PLMs' deep layers are more generalizable and (ii) Prompts\nwith more stable attention distributions in PLMs' deep layers are more\ngeneralizable. Thus, we offer a fresh objective towards domain-generalizable\nprompts optimization named \"Concentration\", which represents the \"lookback\"\nattention from the current decoding token to the prompt tokens, to increase the\nattention strength on prompts and reduce the fluctuation of attention\ndistribution. We adapt this new objective to popular soft prompt and hard\nprompt optimization methods, respectively. Extensive experiments demonstrate\nthat our idea improves comparison prompt optimization methods by 1.42% for soft\nprompt generalization and 2.16% for hard prompt generalization in accuracy on\nthe multi-source domain generalization setting, while maintaining satisfying\nin-domain performance. The promising results validate the effectiveness of our\nproposed prompt optimization objective and provide key insights into\ndomain-generalizable prompts.", "published": "2024-06-15 10:02:46", "link": "http://arxiv.org/abs/2406.10584v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BlockPruner: Fine-grained Pruning for Large Language Models", "abstract": "With the rapid growth in the size and complexity of large language models\n(LLMs), the costs associated with their training and inference have escalated\nsignificantly. Research indicates that certain layers in LLMs harbor\nsubstantial redundancy, and pruning these layers has minimal impact on the\noverall performance. While various layer pruning methods have been developed\nbased on this insight, they generally overlook the finer-grained redundancies\nwithin the layers themselves. In this paper, we delve deeper into the\narchitecture of LLMs and demonstrate that finer-grained pruning can be achieved\nby targeting redundancies in multi-head attention (MHA) and multi-layer\nperceptron (MLP) blocks. We propose a novel, training-free structured pruning\napproach called BlockPruner. Unlike existing layer pruning methods, BlockPruner\nsegments each Transformer layer into MHA and MLP blocks. It then assesses the\nimportance of these blocks using perplexity measures and applies a heuristic\nsearch for iterative pruning. We applied BlockPruner to LLMs of various sizes\nand architectures and validated its performance across a wide range of\ndownstream tasks. Experimental results show that BlockPruner achieves more\ngranular and effective pruning compared to state-of-the-art baselines.", "published": "2024-06-15 11:03:33", "link": "http://arxiv.org/abs/2406.10594v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Hardness of Faithful Chain-of-Thought Reasoning in Large Language\n  Models", "abstract": "As Large Language Models (LLMs) are increasingly being employed in real-world\napplications in critical domains such as healthcare, it is important to ensure\nthat the Chain-of-Thought (CoT) reasoning generated by these models faithfully\ncaptures their underlying behavior.\n  While LLMs are known to generate CoT reasoning that is appealing to humans,\nprior studies have shown that these explanations do not accurately reflect the\nactual behavior of the underlying LLMs. In this work, we explore the promise of\nthree broad approaches commonly employed to steer the behavior of LLMs to\nenhance the faithfulness of the CoT reasoning generated by LLMs: in-context\nlearning, fine-tuning, and activation editing. Specifically, we introduce novel\nstrategies for in-context learning, fine-tuning, and activation editing aimed\nat improving the faithfulness of the CoT reasoning. We then carry out extensive\nempirical analyses with multiple benchmark datasets to explore the promise of\nthese strategies. Our analyses indicate that these strategies offer limited\nsuccess in improving the faithfulness of the CoT reasoning, with only slight\nperformance enhancements in controlled scenarios. Activation editing\ndemonstrated minimal success, while fine-tuning and in-context learning\nachieved marginal improvements that failed to generalize across diverse\nreasoning and truthful question-answering benchmarks. In summary, our work\nunderscores the inherent difficulty in eliciting faithful CoT reasoning from\nLLMs, suggesting that the current array of approaches may not be sufficient to\naddress this complex challenge.", "published": "2024-06-15 13:16:44", "link": "http://arxiv.org/abs/2406.10625v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DIEKAE: Difference Injection for Efficient Knowledge Augmentation and\n  Editing of Large Language Models", "abstract": "Pretrained Language Models (PLMs) store extensive knowledge within their\nweights, enabling them to recall vast amount of information. However, relying\non this parametric knowledge brings some limitations such as outdated\ninformation or gaps in the training data. This work addresses these problems by\ndistinguish between two separate solutions: knowledge editing and knowledge\naugmentation. We introduce Difference Injection for Efficient Knowledge\nAugmentation and Editing (DIEK\\AE), a new method that decouples knowledge\nprocessing from the PLM (LLaMA2-7B, in particular) by adopting a series of\nencoders. These encoders handle external knowledge and inject it into the PLM\nlayers, significantly reducing computational costs and improving performance of\nthe PLM. We propose a novel training technique for these encoders that does not\nrequire back-propagation through the PLM, thus greatly reducing the memory and\ntime required to train them. Our findings demonstrate how our method is faster\nand more efficient compared to multiple baselines in knowledge augmentation and\nediting during both training and inference. We have released our code and data\nat https://github.com/alessioGalatolo/DIEKAE.", "published": "2024-06-15 14:57:39", "link": "http://arxiv.org/abs/2406.10660v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Augmenting Biomedical Named Entity Recognition with General-domain\n  Resources", "abstract": "Training a neural network-based biomedical named entity recognition (BioNER)\nmodel usually requires extensive and costly human annotations. While several\nstudies have employed multi-task learning with multiple BioNER datasets to\nreduce human effort, this approach does not consistently yield performance\nimprovements and may introduce label ambiguity in different biomedical corpora.\nWe aim to tackle those challenges through transfer learning from easily\naccessible resources with fewer concept overlaps with biomedical datasets. We\nproposed GERBERA, a simple-yet-effective method that utilized general-domain\nNER datasets for training. We performed multi-task learning to train a\npre-trained biomedical language model with both the target BioNER dataset and\nthe general-domain dataset. Subsequently, we fine-tuned the models specifically\nfor the BioNER dataset. We systematically evaluated GERBERA on five datasets of\neight entity types, collectively consisting of 81,410 instances. Despite using\nfewer biomedical resources, our models demonstrated superior performance\ncompared to baseline models trained with additional BioNER datasets.\nSpecifically, our models consistently outperformed the baseline models in six\nout of eight entity types, achieving an average improvement of 0.9% over the\nbest baseline performance across eight entities. Our method was especially\neffective in amplifying performance on BioNER datasets characterized by limited\ndata, with a 4.7% improvement in F1 scores on the JNLPBA-RNA dataset. This\nstudy introduces a new training method that leverages cost-effective\ngeneral-domain NER datasets to augment BioNER models. This approach\nsignificantly improves BioNER model performance, making it a valuable asset for\nscenarios with scarce or costly biomedical datasets.", "published": "2024-06-15 15:28:02", "link": "http://arxiv.org/abs/2406.10671v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MIND: Multimodal Shopping Intention Distillation from Large\n  Vision-language Models for E-commerce Purchase Understanding", "abstract": "Improving user experience and providing personalized search results in\nE-commerce platforms heavily rely on understanding purchase intention. However,\nexisting methods for acquiring large-scale intentions bank on distilling large\nlanguage models with human annotation for verification. Such an approach tends\nto generate product-centric intentions, overlook valuable visual information\nfrom product images, and incurs high costs for scalability. To address these\nissues, we introduce MIND, a multimodal framework that allows Large\nVision-Language Models (LVLMs) to infer purchase intentions from multimodal\nproduct metadata and prioritize human-centric ones. Using Amazon Review data,\nwe apply MIND and create a multimodal intention knowledge base, which contains\n1,264,441 million intentions derived from 126,142 co-buy shopping records\nacross 107,215 products. Extensive human evaluations demonstrate the high\nplausibility and typicality of our obtained intentions and validate the\neffectiveness of our distillation framework and filtering mechanism. Additional\nexperiments reveal that our obtained intentions significantly enhance large\nlanguage models in two intention comprehension tasks.", "published": "2024-06-15 17:56:09", "link": "http://arxiv.org/abs/2406.10701v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models as Interpolated and Extrapolated Event Predictors", "abstract": "Salient facts of sociopolitical events are distilled into quadruples\nfollowing a format of subject, relation, object, and timestamp. Machine\nlearning methods, such as graph neural networks (GNNs) and recurrent neural\nnetworks (RNNs), have been built to make predictions and infer relations on the\nquadruple-based knowledge graphs (KGs). In many applications, quadruples are\nextended to quintuples with auxiliary attributes such as text summaries that\ndescribe the quadruple events. In this paper, we comprehensively investigate\nhow large language models (LLMs) streamline the design of event prediction\nframeworks using quadruple-based or quintuple-based data while maintaining\ncompetitive accuracy. We propose LEAP, a unified framework that leverages large\nlanguage models as event predictors. Specifically, we develop multiple prompt\ntemplates to frame the object prediction (OP) task as a standard\nquestion-answering (QA) task, suitable for instruction fine-tuning with an\nencoder-decoder LLM. For multi-event forecasting (MEF) task, we design a simple\nyet effective prompt template for each event quintuple. This novel approach\nremoves the need for GNNs and RNNs, instead utilizing an encoder-only LLM to\ngenerate fixed intermediate embeddings, which are processed by a customized\ndownstream head with a self-attention mechanism to predict potential relation\noccurrences in the future. Extensive experiments on multiple real-world\ndatasets using various evaluation metrics validate the effectiveness of our\napproach.", "published": "2024-06-15 04:09:31", "link": "http://arxiv.org/abs/2406.10492v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reactor Mk.1 performances: MMLU, HumanEval and BBH test results", "abstract": "The paper presents the performance results of Reactor Mk.1, ARCs flagship\nlarge language model, through a benchmarking process analysis. The model\nutilizes the Lychee AI engine and possesses less than 100 billion parameters,\nresulting in a combination of efficiency and potency. The Reactor Mk.1\noutperformed models such as GPT-4o, Claude Opus, and Llama 3, with achieved\nscores of 92% on the MMLU dataset, 91% on HumanEval dataset, and 88% on BBH\ndataset. It excels in both managing difficult jobs and reasoning, establishing\nas a prominent AI solution in the present cutting-edge AI technology.", "published": "2024-06-15 05:52:32", "link": "http://arxiv.org/abs/2406.10515v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Multilingual Large Language Models and Curse of Multilinguality", "abstract": "Multilingual Large Language Models (LLMs) have gained large popularity among\nNatural Language Processing (NLP) researchers and practitioners. These models,\ntrained on huge datasets, show proficiency across various languages and\ndemonstrate effectiveness in numerous downstream tasks. This paper navigates\nthe landscape of multilingual LLMs, providing an introductory overview of their\ntechnical aspects. It explains underlying architectures, objective functions,\npre-training data sources, and tokenization methods. This work explores the\nunique features of different model types: encoder-only (mBERT, XLM-R),\ndecoder-only (XGLM, PALM, BLOOM, GPT-3), and encoder-decoder models (mT5,\nmBART). Additionally, it addresses one of the significant limitations of\nmultilingual LLMs - the curse of multilinguality - and discusses current\nattempts to overcome it.", "published": "2024-06-15 11:31:39", "link": "http://arxiv.org/abs/2406.10602v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "StrucText-Eval: Evaluating Large Language Model's Reasoning Ability in\n  Structure-Rich Text", "abstract": "The effective utilization of structured data, integral to corporate data\nstrategies, has been challenged by the rise of large language models (LLMs)\ncapable of processing unstructured information. This shift prompts the\nquestion: can LLMs interpret structured data directly in its unstructured form?\nWe propose an automatic evaluation data generation method for assessing LLMs'\nreasoning capabilities on structure-rich text to explore this. Our approach\nsupports 8 structured languages and 29 tasks, generating data with adjustable\ncomplexity through controllable nesting and structural width. We introduce\nStrucText-Eval, a benchmark containing 5,800 pre-generated and annotated\nsamples designed to evaluate how well LLMs understand and reason through\nstructured text. StrucText-Eval is divided into two suites: a regular Test\nsuite (3,712 samples) and a Test-Hard suite (2,088 samples), the latter\nemphasizing the gap between human and model performance on more complex tasks.\nExperimental results show that while open-source LLMs achieve a maximum\naccuracy of 74.9\\% on the standard dataset, their performance drops\nsignificantly to 45.8\\% on the harder dataset. In contrast, human participants\nreach an accuracy of 92.6\\% on StrucText-Eval-Hard, highlighting LLMs' current\nlimitations in handling intricate structural information. The benchmark and\ngeneration codes are open sourced in\n\\url{https://github.com/MikeGu721/StrucText-Eval}", "published": "2024-06-15 12:48:00", "link": "http://arxiv.org/abs/2406.10621v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SyntheT2C: Generating Synthetic Data for Fine-Tuning Large Language\n  Models on the Text2Cypher Task", "abstract": "Integrating Large Language Models (LLMs) with existing Knowledge Graph (KG)\ndatabases presents a promising avenue for enhancing LLMs' efficacy and\nmitigating their \"hallucinations\". Given that most KGs reside in graph\ndatabases accessible solely through specialized query languages (e.g., Cypher),\nit is critical to connect LLMs with KG databases by automating the translation\nof natural language into Cypher queries (termed as \"Text2Cypher\" task). Prior\nefforts tried to bolster LLMs' proficiency in Cypher generation through\nSupervised Fine-Tuning (SFT). However, these explorations are hindered by the\nlack of annotated datasets of Query-Cypher pairs, resulting from the\nlabor-intensive and domain-specific nature of such annotation. In this study,\nwe propose SyntheT2C, a methodology for constructing a synthetic Query-Cypher\npair dataset, comprising two distinct pipelines: (1) LLM-based prompting and\n(2) template-filling. SyntheT2C is applied to two medical KG databases,\nculminating in the creation of a synthetic dataset, MedT2C. Comprehensive\nexperiments demonstrate that the MedT2C dataset effectively enhances the\nperformance of backbone LLMs on Text2Cypher task via SFT. Both the SyntheT2C\ncodebase and the MedT2C dataset are released in\nhttps://github.com/ZGChung/SyntheT2C.", "published": "2024-06-15 18:43:49", "link": "http://arxiv.org/abs/2406.10710v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "SparseCL: Sparse Contrastive Learning for Contradiction Retrieval", "abstract": "Contradiction retrieval refers to identifying and extracting documents that\nexplicitly disagree with or refute the content of a query, which is important\nto many downstream applications like fact checking and data cleaning. To\nretrieve contradiction argument to the query from large document corpora,\nexisting methods such as similarity search and crossencoder models exhibit\nsignificant limitations. The former struggles to capture the essence of\ncontradiction due to its inherent nature of favoring similarity, while the\nlatter suffers from computational inefficiency, especially when the size of\ncorpora is large. To address these challenges, we introduce a novel approach:\nSparseCL that leverages specially trained sentence embeddings designed to\npreserve subtle, contradictory nuances between sentences. Our method utilizes a\ncombined metric of cosine similarity and a sparsity function to efficiently\nidentify and retrieve documents that contradict a given query. This approach\ndramatically enhances the speed of contradiction detection by reducing the need\nfor exhaustive document comparisons to simple vector calculations. We validate\nour model using the Arguana dataset, a benchmark dataset specifically geared\ntowards contradiction retrieval, as well as synthetic contradictions generated\nfrom the MSMARCO and HotpotQA datasets using GPT-4. Our experiments demonstrate\nthe efficacy of our approach not only in contradiction retrieval with more than\n30% accuracy improvements on MSMARCO and HotpotQA across different model\narchitectures but also in applications such as cleaning corrupted corpora to\nrestore high-quality QA retrieval. This paper outlines a promising direction\nfor improving the accuracy and efficiency of contradiction retrieval in\nlarge-scale text corpora.", "published": "2024-06-15 21:57:03", "link": "http://arxiv.org/abs/2406.10746v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Intertwining CP and NLP: The Generation of Unreasonably Constrained\n  Sentences", "abstract": "Constrained text generation remains a challenging task, particularly when\ndealing with hard constraints. Traditional NLP approaches prioritize generating\nmeaningful and coherent output. Also, the current state-of-the-art methods\noften lack the expressiveness and constraint satisfaction capabilities to\nhandle such tasks effectively. Recently, an approach for generating constrained\nsentences in CP has been proposed in (Bonlarron et al, 2023). This ad-hoc model\nto solve the sentences generation problem under MNREAD rules proved\nneithertheless to be computationaly and structuraly unsuitable to deal with\nother more constrained problems. In this paper, a novel more generic approach\nis introduced to tackle many of these previously untractable problems, and\nillustrated here with the quite untractable sentences generation problem\nfollowing RADNER rules.\n  More precisely, this paper presents the CPTextGen Framework. This framework\nconsiders a constrained text generation problem as a discrete combinatorial\noptimization problem. It is solved by a constraint programming method that\ncombines linguistic properties (e.g., n-grams or language level) with other\nmore classical constraints (e.g., the number of characters, syllables).\nEventually, a curation phase allows for selecting the best-generated sentences\naccording to perplexity using an LLM.\n  The effectiveness of this approach is demonstrated by tackling a new, more\ntediously constrained text generation problem: the iconic RADNER sentences\nproblem. This problem aims to generate sentences respecting a set of quite\nstrict rules defined by their use in vision and clinical research. Thanks to\nour CP-based approach, many new strongly constrained sentences have been\nsuccessfully generated. This highlights our approach's potential to handle\nunreasonably constrained text generation scenarios.", "published": "2024-06-15 17:40:49", "link": "http://arxiv.org/abs/2406.15473v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TokenRec: Learning to Tokenize ID for LLM-based Generative\n  Recommendation", "abstract": "There is a growing interest in utilizing large-scale language models (LLMs)\nto advance next-generation Recommender Systems (RecSys), driven by their\noutstanding language understanding and in-context learning capabilities. In\nthis scenario, tokenizing (i.e., indexing) users and items becomes essential\nfor ensuring a seamless alignment of LLMs with recommendations. While several\nstudies have made progress in representing users and items through textual\ncontents or latent representations, challenges remain in efficiently capturing\nhigh-order collaborative knowledge into discrete tokens that are compatible\nwith LLMs. Additionally, the majority of existing tokenization approaches often\nface difficulties in generalizing effectively to new/unseen users or items that\nwere not in the training corpus. To address these challenges, we propose a\nnovel framework called TokenRec, which introduces not only an effective ID\ntokenization strategy but also an efficient retrieval paradigm for LLM-based\nrecommendations. Specifically, our tokenization strategy, Masked\nVector-Quantized (MQ) Tokenizer, involves quantizing the masked user/item\nrepresentations learned from collaborative filtering into discrete tokens, thus\nachieving a smooth incorporation of high-order collaborative knowledge and a\ngeneralizable tokenization of users and items for LLM-based RecSys. Meanwhile,\nour generative retrieval paradigm is designed to efficiently recommend top-$K$\nitems for users to eliminate the need for the time-consuming auto-regressive\ndecoding and beam search processes used by LLMs, thus significantly reducing\ninference time. Comprehensive experiments validate the effectiveness of the\nproposed methods, demonstrating that TokenRec outperforms competitive\nbenchmarks, including both traditional recommender systems and emerging\nLLM-based recommender systems.", "published": "2024-06-15 00:07:44", "link": "http://arxiv.org/abs/2406.10450v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "From Words to Worlds: Transforming One-line Prompt into Immersive\n  Multi-modal Digital Stories with Communicative LLM Agent", "abstract": "Digital storytelling, essential in entertainment, education, and marketing,\nfaces challenges in production scalability and flexibility. The StoryAgent\nframework, introduced in this paper, utilizes Large Language Models and\ngenerative tools to automate and refine digital storytelling. Employing a\ntop-down story drafting and bottom-up asset generation approach, StoryAgent\ntackles key issues such as manual intervention, interactive scene\norchestration, and narrative consistency. This framework enables efficient\nproduction of interactive and consistent narratives across multiple modalities,\ndemocratizing content creation and enhancing engagement. Our results\ndemonstrate the framework's capability to produce coherent digital stories\nwithout reference videos, marking a significant advancement in automated\ndigital storytelling.", "published": "2024-06-15 03:03:43", "link": "http://arxiv.org/abs/2406.10478v2", "categories": ["cs.CL", "cs.AI", "cs.GR"], "primary_category": "cs.CL"}
{"title": "Task Facet Learning: A Structured Approach to Prompt Optimization", "abstract": "Given a task in the form of a basic description and its training examples,\nprompt optimization is the problem of synthesizing the given information into a\ntext prompt for a large language model (LLM). Humans solve this problem by also\nconsidering the different facets that define a task (e.g., counter-examples,\nexplanations, analogies) and including them in the prompt. However, it is\nunclear whether existing algorithmic approaches, based on iteratively editing a\ngiven prompt or automatically selecting a few in-context examples, can cover\nthe multiple facets required to solve a complex task. In this work, we view\nprompt optimization as that of learning multiple facets of a task from a set of\ntraining examples. We identify and exploit structure in the prompt optimization\nproblem -- first, we find that prompts can be broken down into loosely coupled\nsemantic sections that have a relatively independent effect on the prompt's\nperformance; second, we cluster the input space and use clustered batches so\nthat the optimization procedure can learn the different facets of a task across\nbatches. The resulting algorithm, UniPrompt, consists of a generative model to\ngenerate initial candidates for each prompt section; and a feedback mechanism\nthat aggregates suggested edits from multiple mini-batches into a conceptual\ndescription for the section. Empirical evaluation on multiple datasets and a\nreal-world task shows that prompts generated using UniPrompt obtain higher\naccuracy than human-tuned prompts and those from state-of-the-art methods. In\nparticular, our algorithm can generate long, complex prompts that existing\nmethods are unable to generate. Code for UniPrompt will be available at\n\\url{https://aka.ms/uniprompt}.", "published": "2024-06-15 04:54:26", "link": "http://arxiv.org/abs/2406.10504v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Benchmarking Children's ASR with Supervised and Self-supervised Speech\n  Foundation Models", "abstract": "Speech foundation models (SFMs) have achieved state-of-the-art results for\nvarious speech tasks in supervised (e.g. Whisper) or self-supervised systems\n(e.g. WavLM). However, the performance of SFMs for child ASR has not been\nsystematically studied. In addition, there is no benchmark for child ASR with\nstandard evaluations, making the comparisons of novel ideas difficult. In this\npaper, we initiate and present a comprehensive benchmark on several child\nspeech databases based on various SFMs (Whisper, Wav2vec2.0, HuBERT, and\nWavLM). Moreover, we investigate finetuning strategies by comparing various\ndata augmentation and parameter-efficient finetuning (PEFT) methods. We observe\nthat the behaviors of these methods are different when the model size\nincreases. For example, PEFT matches the performance of full finetuning for\nlarge models but worse for small models. To stabilize finetuning using\naugmented data, we propose a perturbation invariant finetuning (PIF) loss as a\nregularization.", "published": "2024-06-15 05:13:19", "link": "http://arxiv.org/abs/2406.10507v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Humor in AI: Massive Scale Crowd-Sourced Preferences and Benchmarks for\n  Cartoon Captioning", "abstract": "We present a novel multimodal preference dataset for creative tasks,\nconsisting of over 250 million human ratings on more than 2.2 million captions,\ncollected through crowdsourcing rating data for The New Yorker's weekly cartoon\ncaption contest over the past eight years. This unique dataset supports the\ndevelopment and evaluation of multimodal large language models and\npreference-based fine-tuning algorithms for humorous caption generation. We\npropose novel benchmarks for judging the quality of model-generated captions,\nutilizing both GPT4 and human judgments to establish ranking-based evaluation\nstrategies. Our experimental results highlight the limitations of current\nfine-tuning methods, such as RLHF and DPO, when applied to creative tasks.\nFurthermore, we demonstrate that even state-of-the-art models like GPT4 and\nClaude currently underperform top human contestants in generating humorous\ncaptions. As we conclude this extensive data collection effort, we release the\nentire preference dataset to the research community, fostering further\nadvancements in AI humor generation and evaluation.", "published": "2024-06-15 06:26:25", "link": "http://arxiv.org/abs/2406.10522v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Lightweight Audio Segmentation for Long-form Speech Translation", "abstract": "Speech segmentation is an essential part of speech translation (ST) systems\nin real-world scenarios. Since most ST models are designed to process speech\nsegments, long-form audio must be partitioned into shorter segments before\ntranslation. Recently, data-driven approaches for the speech segmentation task\nhave been developed. Although the approaches improve overall translation\nquality, a performance gap exists due to a mismatch between the models and ST\nsystems. In addition, the prior works require large self-supervised speech\nmodels, which consume significant computational resources. In this work, we\npropose a segmentation model that achieves better speech translation quality\nwith a small model size. We propose an ASR-with-punctuation task as an\neffective pre-training strategy for the segmentation model. We also show that\nproper integration of the speech segmentation model into the underlying ST\nsystem is critical to improve overall translation quality at inference time.", "published": "2024-06-15 08:02:15", "link": "http://arxiv.org/abs/2406.10549v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Large Language Model Enhanced Clustering for News Event Detection", "abstract": "The news landscape is continuously evolving, with an ever-increasing volume\nof information from around the world. Automated event detection within this\nvast data repository is essential for monitoring, identifying, and categorizing\nsignificant news occurrences across diverse platforms. This paper presents an\nevent detection framework that leverages Large Language Models (LLMs) combined\nwith clustering analysis to detect news events from the Global Database of\nEvents, Language, and Tone (GDELT). The framework enhances event clustering\nthrough both pre-event detection tasks (keyword extraction and text embedding)\nand post-event detection tasks (event summarization and topic labelling). We\nalso evaluate the impact of various textual embeddings on the quality of\nclustering outcomes, ensuring robust news categorization. Additionally, we\nintroduce a novel Cluster Stability Assessment Index (CSAI) to assess the\nvalidity and robustness of clustering results. CSAI utilizes multiple feature\nvectors to provide a new way of measuring clustering quality. Our experiments\nindicate that the use of LLM embedding in the event detection framework has\nsignificantly improved the results, demonstrating greater robustness in terms\nof CSAI scores. Moreover, post-event detection tasks generate meaningful\ninsights, facilitating effective interpretation of event clustering results.\nOverall, our experimental results indicate that the proposed framework offers\nvaluable insights and could enhance the accuracy in news analysis and\nreporting.", "published": "2024-06-15 08:13:47", "link": "http://arxiv.org/abs/2406.10552v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bypass Back-propagation: Optimization-based Structural Pruning for Large\n  Language Models via Policy Gradient", "abstract": "In contrast to moderate-size neural network pruning, structural weight\npruning on the Large-Language Models (LLMs) imposes a novel challenge on the\nefficiency of the pruning algorithms, due to the heavy computation/memory\ndemands of the LLMs. Recent efficient LLM pruning methods typically operate at\nthe post-training phase without the expensive weight finetuning, however, their\npruning criteria often rely on heuristically hand-crafted metrics, potentially\nleading to suboptimal performance. We instead propose a novel\noptimization-based structural pruning that learns the pruning masks in a\nprobabilistic space directly by optimizing the loss of the pruned model. To\npreserve the efficiency, our method eliminates the back-propagation through the\nLLM per se during the optimization, requiring only the forward pass of the LLM.\nWe achieve this by learning an underlying Bernoulli distribution to sample\nbinary pruning masks, where we decouple the Bernoulli parameters from the LLM\nloss, thus facilitating an efficient optimization via a policy gradient\nestimator without back-propagation. As a result, our method is able to 1)\noperate at structural granularities of channels, heads, and layers, 2) support\nglobal and heterogeneous pruning (i.e., our method automatically determines\ndifferent redundancy for different layers), and 3) optionally initialize with a\nmetric-based method (for our Bernoulli distributions). Extensive experiments on\nLLaMA, LLaMA-2, LLaMA-3, Vicuna, and Mistral using the C4 and WikiText2\ndatasets demonstrate that our method operates for 2.7 hours with around 35GB\nmemory for the 13B models on a single A100 GPU, and our pruned models\noutperform the state-of-the-arts w.r.t. both perplexity and the majority of\nvarious zero-shot tasks. Codes will be released.", "published": "2024-06-15 09:31:03", "link": "http://arxiv.org/abs/2406.10576v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Emerging Safety Attack and Defense in Federated Instruction Tuning of\n  Large Language Models", "abstract": "Federated learning (FL) enables multiple parties to collaboratively fine-tune\nan large language model (LLM) without the need of direct data sharing. Ideally,\nby training on decentralized data that is aligned with human preferences and\nsafety principles, federated instruction tuning can result in an LLM that could\nbehave in a helpful and safe manner. In this paper, we for the first time\nreveal the vulnerability of safety alignment in FedIT by proposing a simple,\nstealthy, yet effective safety attack method. Specifically, the malicious\nclients could automatically generate attack data without involving manual\nefforts and attack the FedIT system by training their local LLMs on such attack\ndata. Unfortunately, this proposed safety attack not only can compromise the\nsafety alignment of LLM trained via FedIT, but also can not be effectively\ndefended against by many existing FL defense methods. Targeting this, we\nfurther propose a post-hoc defense method, which could rely on a fully\nautomated pipeline: generation of defense data and further fine-tuning of the\nLLM. Extensive experiments show that our safety attack method can significantly\ncompromise the LLM's safety alignment (e.g., reduce safety rate by 70\\%), which\ncan not be effectively defended by existing defense methods (at most 4\\%\nabsolute improvement), while our safety defense method can significantly\nenhance the attacked LLM's safety alignment (at most 69\\% absolute\nimprovement).", "published": "2024-06-15 13:24:22", "link": "http://arxiv.org/abs/2406.10630v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.MA"], "primary_category": "cs.CL"}
{"title": "CoLoR-Filter: Conditional Loss Reduction Filtering for Targeted Language\n  Model Pre-training", "abstract": "Selecting high-quality data for pre-training is crucial in shaping the\ndownstream task performance of language models. A major challenge lies in\nidentifying this optimal subset, a problem generally considered intractable,\nthus necessitating scalable and effective heuristics. In this work, we propose\na data selection method, CoLoR-Filter (Conditional Loss Reduction Filtering),\nwhich leverages an empirical Bayes-inspired approach to derive a simple and\ncomputationally efficient selection criterion based on the relative loss values\nof two auxiliary models.\n  In addition to the modeling rationale, we evaluate CoLoR-Filter empirically\non two language modeling tasks: (1) selecting data from C4 for domain\nadaptation to evaluation on Books and (2) selecting data from C4 for a suite of\ndownstream multiple-choice question answering tasks. We demonstrate favorable\nscaling both as we subselect more aggressively and using small auxiliary models\nto select data for large target models. As one headline result, CoLoR-Filter\ndata selected using a pair of 150m parameter auxiliary models can train a 1.2b\nparameter target model to match a 1.2b parameter model trained on 25b randomly\nselected tokens with 25x less data for Books and 11x less data for the\ndownstream tasks.\n  Code: https://github.com/davidbrandfonbrener/color-filter-olmo\n  Filtered data:\nhttps://huggingface.co/datasets/davidbrandfonbrener/color-filtered-c4", "published": "2024-06-15 15:28:02", "link": "http://arxiv.org/abs/2406.10670v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "How Should We Extract Discrete Audio Tokens from Self-Supervised Models?", "abstract": "Discrete audio tokens have recently gained attention for their potential to\nbridge the gap between audio and language processing. Ideal audio tokens must\npreserve content, paralinguistic elements, speaker identity, and many other\naudio details. Current audio tokenization methods fall into two categories:\nSemantic tokens, acquired through quantization of Self-Supervised Learning\n(SSL) models, and Neural compression-based tokens (codecs). Although previous\nstudies have benchmarked codec models to identify optimal configurations, the\nideal setup for quantizing pretrained SSL models remains unclear. This paper\nexplores the optimal configuration of semantic tokens across discriminative and\ngenerative tasks. We propose a scalable solution to train a universal vocoder\nacross multiple SSL layers. Furthermore, an attention mechanism is employed to\nidentify task-specific influential layers, enhancing the adaptability and\nperformance of semantic tokens in diverse audio applications.", "published": "2024-06-15 20:43:07", "link": "http://arxiv.org/abs/2406.10735v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Evaluating the Generalization Ability of Quantized LLMs: Benchmark,\n  Analysis, and Toolbox", "abstract": "Large language models (LLMs) have exhibited exciting progress in multiple\nscenarios, while the huge computational demands hinder their deployments in\nlots of real-world applications. As an effective means to reduce memory\nfootprint and inference cost, quantization also faces challenges in performance\ndegradation at low bit-widths. Understanding the impact of quantization on LLM\ncapabilities, especially the generalization ability, is crucial. However, the\ncommunity's main focus remains on the algorithms and models of quantization,\nwith insufficient attention given to whether the quantized models can retain\nthe strong generalization abilities of LLMs. In this work, we fill this gap by\nproviding a comprehensive benchmark suite for this research topic, including an\nevaluation system, detailed analyses, and a general toolbox. Specifically,\nbased on the dominant pipeline in LLM quantization, we primarily explore the\nimpact of calibration data distribution on the generalization of quantized LLMs\nand conduct the benchmark using more than 40 datasets within two main\nscenarios. Based on this benchmark, we conduct extensive experiments with two\nwell-known LLMs (English and Chinese) and four quantization algorithms to\ninvestigate this topic in-depth, yielding several counter-intuitive and\nvaluable findings, e.g., models quantized using a calibration set with the same\ndistribution as the test data are not necessarily optimal. Besides, to\nfacilitate future research, we also release a modular-designed toolbox, which\ndecouples the overall pipeline into several separate components, e.g., base LLM\nmodule, dataset module, quantizer module, etc. and allows subsequent\nresearchers to easily assemble their methods through a simple configuration.\nOur benchmark suite is publicly available at\nhttps://github.com/TsingmaoAI/MI-optimize", "published": "2024-06-15 12:02:14", "link": "http://arxiv.org/abs/2406.12928v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MMLU-SR: A Benchmark for Stress-Testing Reasoning Capability of Large\n  Language Models", "abstract": "We propose MMLU-SR, a novel dataset designed to measure the true\ncomprehension abilities of Large Language Models (LLMs) by challenging their\nperformance in question-answering tasks with modified terms. We reasoned that\nan agent that \"truly\" understands a concept can still evaluate it when key\nterms are replaced by suitably defined alternate terms, and sought to\ndifferentiate such comprehension from mere text replacement. In our study, we\nmodified standardized test questions by replacing a key term with a dummy word\nalong with its definition. The key term could be in the context of questions,\nanswers, or both questions and answers. Notwithstanding the high scores\nachieved by recent popular LLMs on the MMLU leaderboard, we found a substantial\nreduction in model performance after such replacement, suggesting poor\ncomprehension. This new benchmark provides a rigorous benchmark for testing\ntrue model comprehension, and poses a challenge to the broader scientific\ncommunity.", "published": "2024-06-15 05:35:47", "link": "http://arxiv.org/abs/2406.15468v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mental Disorder Classification via Temporal Representation of Text", "abstract": "Mental disorders pose a global challenge, aggravated by the shortage of\nqualified mental health professionals. Mental disorder prediction from social\nmedia posts by current LLMs is challenging due to the complexities of\nsequential text data and the limited context length of language models. Current\nlanguage model-based approaches split a single data instance into multiple\nchunks to compensate for limited context size. The predictive model is then\napplied to each chunk individually, and the most voted output is selected as\nthe final prediction. This results in the loss of inter-post dependencies and\nimportant time variant information, leading to poor performance. We propose a\nnovel framework which first compresses the large sequence of chronologically\nordered social media posts into a series of numbers. We then use this time\nvariant representation for mental disorder classification. We demonstrate the\ngeneralization capabilities of our framework by outperforming the current SOTA\nin three different mental conditions: depression, self-harm, and anorexia, with\nan absolute improvement of 5% in the F1 score. We investigate the situation\nwhere current data instances fall within the context length of language models\nand present empirical results highlighting the importance of temporal\nproperties of textual data. Furthermore, we utilize the proposed framework for\na cross-domain study, exploring commonalities across disorders and the\npossibility of inter-domain data usage.", "published": "2024-06-15 10:53:21", "link": "http://arxiv.org/abs/2406.15470v2", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Improving Large Models with Small models: Lower Costs and Better\n  Performance", "abstract": "Pretrained large models (PLMs), such as ChatGPT, have demonstrated remarkable\nperformance across diverse tasks. However, the significant computational\nrequirements of PLMs have discouraged most product teams from running or\nfine-tuning them. In such cases, to harness the exceptional performance of\nPLMs, one must rely on expensive APIs, thereby exacerbating the economic\nburden. Despite the overall inferior performance of small models, in specific\ndistributions, they can achieve comparable or even superior results.\nConsequently, some input can be processed exclusively by small models. On the\nother hand, certain tasks can be broken down into multiple subtasks, some of\nwhich can be completed without powerful capabilities. Under these\ncircumstances, small models can handle the simple subtasks, allowing large\nmodels to focus on challenging subtasks, thus improving the performance. We\npropose Data Shunt$^+$ (DS$^+$), a general paradigm for collaboration of small\nand large models. DS$^+$ not only substantially reduces the cost associated\nwith querying large models but also effectively improves large models'\nperformance. For instance, ChatGPT achieves an accuracy of $94.43\\%$ on Amazon\nProduct sentiment analysis, and DS$^+$ achieves an accuracy of $95.64\\%$, while\nthe cost has been reduced to only $31.18\\%$. Besides, experiments also prove\nthat the proposed collaborative-based paradigm can better inject specific task\nknowledge into PLMs compared to fine-tuning.", "published": "2024-06-15 14:44:43", "link": "http://arxiv.org/abs/2406.15471v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hyperbolic sentence representations for solving Textual Entailment", "abstract": "Hyperbolic spaces have proven to be suitable for modeling data of\nhierarchical nature. As such we use the Poincare ball to embed sentences with\nthe goal of proving how hyperbolic spaces can be used for solving Textual\nEntailment. To this end, apart from the standard datasets used for evaluating\ntextual entailment, we developed two additional datasets. We evaluate against\nbaselines of various backgrounds, including LSTMs, Order Embeddings and\nEuclidean Averaging, which comes as a natural counterpart to representing\nsentences into the Euclidean space. We consistently outperform the baselines on\nthe SICK dataset and are second only to Order Embeddings on the SNLI dataset,\nfor the binary classification version of the entailment task.", "published": "2024-06-15 15:39:43", "link": "http://arxiv.org/abs/2406.15472v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cutting through the noise to motivate people: A comprehensive analysis\n  of COVID-19 social media posts de/motivating vaccination", "abstract": "The COVID-19 pandemic exposed significant weaknesses in the healthcare\ninformation system. The overwhelming volume of misinformation on social media\nand other socioeconomic factors created extraordinary challenges to motivate\npeople to take proper precautions and get vaccinated. In this context, our work\nexplored a novel direction by analyzing an extensive dataset collected over two\nyears, identifying the topics de/motivating the public about COVID-19\nvaccination. We analyzed these topics based on time, geographic location, and\npolitical orientation. We noticed that while the motivating topics remain the\nsame over time and geographic location, the demotivating topics change rapidly.\nWe also identified that intrinsic motivation, rather than external mandate, is\nmore advantageous to inspire the public. This study addresses scientific\ncommunication and public motivation in social media. It can help public health\nofficials, policymakers, and social media platforms develop more effective\nmessaging strategies to cut through the noise of misinformation and educate the\npublic about scientific findings.", "published": "2024-06-15 02:36:11", "link": "http://arxiv.org/abs/2407.03190v2", "categories": ["cs.CY", "cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CY"}
{"title": "Transformative Influence of LLM and AI Tools in Student Social Media\n  Engagement: Analyzing Personalization, Communication Efficiency, and\n  Collaborative Learning", "abstract": "The advent of Large Language Models (LLMs) and Artificial Intelligence (AI)\ntools has revolutionized various facets of our lives, particularly in the realm\nof social media. For students, these advancements have unlocked unprecedented\nopportunities for learning, collaboration, and personal growth. AI-driven\napplications are transforming how students interact with social media, offering\npersonalized content and recommendations, and enabling smarter, more efficient\ncommunication. Recent studies utilizing data from UniversityCube underscore the\nprofound impact of AI tools on students' academic and social experiences. These\nstudies reveal that students engaging with AI-enhanced social media platforms\nreport higher academic performance, enhanced critical thinking skills, and\nincreased engagement in collaborative projects.\n  Moreover, AI tools assist in filtering out distracting content, allowing\nstudents to concentrate more on educational materials and pertinent\ndiscussions. The integration of LLMs in social media has further facilitated\nimproved peer-to-peer communication and mentorship opportunities. AI algorithms\neffectively match students based on shared academic interests and career goals,\nfostering a supportive and intellectually stimulating online community, thereby\ncontributing to increased student satisfaction and retention rates.\n  In this article, we delve into the data provided by UniversityCube to explore\nhow LLMs and AI tools are specifically transforming social media for students.\nThrough case studies and statistical analyses, we offer a comprehensive\nunderstanding of the educational and social benefits these technologies offer.\nOur exploration highlights the potential of AI-driven tools to create a more\nenriched, efficient, and supportive educational environment for students in the\ndigital age.", "published": "2024-06-15 01:05:56", "link": "http://arxiv.org/abs/2407.15012v2", "categories": ["cs.CY", "cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CY"}
{"title": "GTR-Voice: Articulatory Phonetics Informed Controllable Expressive\n  Speech Synthesis", "abstract": "Expressive speech synthesis aims to generate speech that captures a wide\nrange of para-linguistic features, including emotion and articulation, though\ncurrent research primarily emphasizes emotional aspects over the nuanced\narticulatory features mastered by professional voice actors. Inspired by this,\nwe explore expressive speech synthesis through the lens of articulatory\nphonetics. Specifically, we define a framework with three dimensions:\nGlottalization, Tenseness, and Resonance (GTR), to guide the synthesis at the\nvoice production level. With this framework, we record a high-quality speech\ndataset named GTR-Voice, featuring 20 Chinese sentences articulated by a\nprofessional voice actor across 125 distinct GTR combinations. We verify the\nframework and GTR annotations through automatic classification and listening\ntests, and demonstrate precise controllability along the GTR dimensions on two\nfine-tuned expressive TTS models. We open-source the dataset and TTS models.", "published": "2024-06-15 05:37:04", "link": "http://arxiv.org/abs/2406.10514v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AVR: Synergizing Foundation Models for Audio-Visual Humor Detection", "abstract": "In this work, we present, AVR application for audio-visual humor detection.\nWhile humor detection has traditionally centered around textual analysis,\nrecent advancements have spotlighted multimodal approaches. However, these\nmethods lean on textual cues as a modality, necessitating the use of ASR\nsystems for transcribing the audio-data. This heavy reliance on ASR accuracy\ncan pose challenges in real-world applications. To address this bottleneck, we\npropose an innovative audio-visual humor detection system that circumvents\ntextual reliance, eliminating the need for ASR models. Instead, the proposed\napproach hinges on the intricate interplay between audio and visual content for\neffective humor detection.", "published": "2024-06-15 00:02:41", "link": "http://arxiv.org/abs/2406.10448v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SOA: Reducing Domain Mismatch in SSL Pipeline by Speech Only Adaptation\n  for Low Resource ASR", "abstract": "Recently, speech foundation models have gained popularity due to their\nsuperiority in finetuning downstream ASR tasks. However, models finetuned on\ncertain domains, such as LibriSpeech (adult read speech), behave poorly on\nother domains (child or noisy speech). One solution could be collecting as much\nlabeled and diverse data as possible for joint finetuning on various domains.\nHowever, collecting target domain speech-text paired data and retraining the\nmodel is often costly and computationally expensive. In this paper, we\nintroduce a simple yet effective method, speech only adaptation (SOA), based on\nspeech foundation models (Wav2vec 2.0), which requires only speech input data\nfrom the target domain. Specifically, the Wav2vec 2.0 feature encoder is\ncontinually pretrained with the Wav2vec 2.0 loss on both the source and target\ndomain data for domain adaptation, while the contextual encoder is frozen.\nCompared to a source domain finetuned model with the feature encoder being\nfrozen during training, we find that replacing the frozen feature encoder with\nthe adapted one provides significant WER improvements to the target domain\nwhile preserving the performance of the source domain. The effectiveness of SOA\nis examined on various low resource or domain mismatched ASR settings,\nincluding adult-child and clean-noisy speech.", "published": "2024-06-15 05:28:56", "link": "http://arxiv.org/abs/2406.10512v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Double Multi-Head Attention Multimodal System for Odyssey 2024 Speech\n  Emotion Recognition Challenge", "abstract": "As computer-based applications are becoming more integrated into our daily\nlives, the importance of Speech Emotion Recognition (SER) has increased\nsignificantly. Promoting research with innovative approaches in SER, the\nOdyssey 2024 Speech Emotion Recognition Challenge was organized as part of the\nOdyssey 2024 Speaker and Language Recognition Workshop. In this paper we\ndescribe the Double Multi-Head Attention Multimodal System developed for this\nchallenge. Pre-trained self-supervised models were used to extract informative\nacoustic and text features. An early fusion strategy was adopted, where a\nMulti-Head Attention layer transforms these mixed features into complementary\ncontextualized representations. A second attention mechanism is then applied to\npool these representations into an utterance-level vector. Our proposed system\nachieved the third position in the categorical task ranking with a 34.41%\nMacro-F1 score, where 31 teams participated in total.", "published": "2024-06-15 11:11:06", "link": "http://arxiv.org/abs/2406.10598v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Emotion Recognition Using CNN and Its Use Case in Digital\n  Healthcare", "abstract": "The process of identifying human emotion and affective states from speech is\nknown as speech emotion recognition (SER). This is based on the observation\nthat tone and pitch in the voice frequently convey underlying emotion. Speech\nrecognition includes the ability to recognize emotions, which is becoming\nincreasingly popular and in high demand. With the help of appropriate factors\n(such modalities, emotions, intensities, repetitions, etc.) found in the data,\nmy research seeks to use the Convolutional Neural Network (CNN) to distinguish\nemotions from audio recordings and label them in accordance with the range of\ndifferent emotions. I have developed a machine learning model to identify\nemotions from supplied audio files with the aid of machine learning methods.\nThe evaluation is mostly focused on precision, recall, and F1 score, which are\ncommon machine learning metrics. To properly set up and train the machine\nlearning framework, the main objective is to investigate the influence and\ncross-relation of all input and output parameters. To improve the ability to\nrecognize intentions, a key condition for communication, I have evaluated\nemotions using my specialized machine learning algorithm via voice that would\naddress the emotional state from voice with the help of digital healthcare,\nbridging the gap between human and artificial intelligence (AI).", "published": "2024-06-15 21:33:03", "link": "http://arxiv.org/abs/2406.10741v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MINT: a Multi-modal Image and Narrative Text Dubbing Dataset for Foley\n  Audio Content Planning and Generation", "abstract": "Foley audio, critical for enhancing the immersive experience in multimedia\ncontent, faces significant challenges in the AI-generated content (AIGC)\nlandscape. Despite advancements in AIGC technologies for text and image\ngeneration, the foley audio dubbing remains rudimentary due to difficulties in\ncross-modal scene matching and content correlation. Current text-to-audio\ntechnology, which relies on detailed and acoustically relevant textual\ndescriptions, falls short in practical video dubbing applications. Existing\ndatasets like AudioSet, AudioCaps, Clotho, Sound-of-Story, and WavCaps do not\nfully meet the requirements for real-world foley audio dubbing task. To address\nthis, we introduce the Multi-modal Image and Narrative Text Dubbing Dataset\n(MINT), designed to enhance mainstream dubbing tasks such as literary story\naudiobooks dubbing, image/silent video dubbing. Besides, to address the\nlimitations of existing TTA technology in understanding and planning complex\nprompts, a Foley Audio Content Planning, Generation, and Alignment (CPGA)\nframework is proposed, which includes a content planning module leveraging\nlarge language models for complex multi-modal prompts comprehension.\nAdditionally, the training process is optimized using Proximal Policy\nOptimization based reinforcement learning, significantly improving the\nalignment and auditory realism of generated foley audio. Experimental results\ndemonstrate that our approach significantly advances the field of foley audio\ndubbing, providing robust solutions for the challenges of multi-modal dubbing.\nEven when utilizing the relatively lightweight GPT-2 model, our framework\noutperforms open-source multimodal large models such as LLaVA, DeepSeek-VL, and\nMoondream2. The dataset is available at https://github.com/borisfrb/MINT .", "published": "2024-06-15 10:47:36", "link": "http://arxiv.org/abs/2406.10591v1", "categories": ["eess.AS", "cs.AI", "cs.CV", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
