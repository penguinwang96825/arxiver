{"title": "Multi-Source Neural Machine Translation with Data Augmentation", "abstract": "Multi-source translation systems translate from multiple languages to a\nsingle target language. By using information from these multiple sources, these\nsystems achieve large gains in accuracy. To train these systems, it is\nnecessary to have corpora with parallel text in multiple sources and the target\nlanguage. However, these corpora are rarely complete in practice due to the\ndifficulty of providing human translations in all of the relevant languages. In\nthis paper, we propose a data augmentation approach to fill such incomplete\nparts using multi-source neural machine translation (NMT). In our experiments,\nresults varied over different language combinations but significant gains were\nobserved when using a source language similar to the target language.", "published": "2018-10-16 06:00:37", "link": "http://arxiv.org/abs/1810.06826v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Creating a New Persian Poet Based on Machine Learning", "abstract": "In this article we describe an application of Machine Learning (ML) and\nLinguistic Modeling to generate persian poems. In fact we teach machine by\nreading and learning persian poems to generate fake poems in the same style of\nthe original poems. As two well known poets we used Hafez (1310-1390) and Saadi\n(1210-1292) poems. First we feed the machine with Hafez poems to generate fake\npoems with the same style and then we feed the machine with the both Hafez and\nSaadi poems to generate a new style poems which is combination of these two\npoets styles with emotional (Hafez) and rational (Saadi) elements. This idea of\ncombination of different styles with ML opens new gates for extending the\ntreasure of past literature of different cultures. Results show with enough\nmemory, processing power and time it is possible to generate reasonable good\npoems.", "published": "2018-10-16 09:37:24", "link": "http://arxiv.org/abs/1810.06898v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Morphological Tagging for Estonian", "abstract": "We develop neural morphological tagging and disambiguation models for\nEstonian. First, we experiment with two neural architectures for morphological\ntagging - a standard multiclass classifier which treats each morphological tag\nas a single unit, and a sequence model which handles the morphological tags as\nsequences of morphological category values. Secondly, we complement these\nmodels with the analyses generated by a rule-based Estonian morphological\nanalyser (MA) VABAMORF , thus performing a soft morphological disambiguation.\nWe compare two ways of supplementing a neural morphological tagger with the MA\noutputs: firstly, by adding the combined analyses embeddings to the word\nrepresentation input to the neural tagging model, and secondly, by adopting an\nattention mechanism to focus on the most relevant analyses generated by the MA.\nExperiments on three Estonian datasets show that our neural architectures\nconsistently outperform the non-neural baselines, including HMM-disambiguated\nVABAMORF, while augmenting models with MA outputs results in a further\nperformance boost for both models.", "published": "2018-10-16 10:00:06", "link": "http://arxiv.org/abs/1810.06908v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "INFODENS: An Open-source Framework for Learning Text Representations", "abstract": "The advent of representation learning methods enabled large performance gains\non various language tasks, alleviating the need for manual feature engineering.\nWhile engineered representations are usually based on some linguistic\nunderstanding and are therefore more interpretable, learned representations are\nharder to interpret. Empirically studying the complementarity of both\napproaches can provide more linguistic insights that would help reach a better\ncompromise between interpretability and performance. We present INFODENS, a\nframework for studying learned and engineered representations of text in the\ncontext of text classification tasks. It is designed to simplify the tasks of\nfeature engineering as well as provide the groundwork for extracting learned\nfeatures and combining both approaches. INFODENS is flexible, extensible, with\na short learning curve, and is easy to integrate with many of the available and\nwidely used natural language processing tools.", "published": "2018-10-16 15:44:53", "link": "http://arxiv.org/abs/1810.07091v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The CoNLL--SIGMORPHON 2018 Shared Task: Universal Morphological\n  Reinflection", "abstract": "The CoNLL--SIGMORPHON 2018 shared task on supervised learning of\nmorphological generation featured data sets from 103 typologically diverse\nlanguages. Apart from extending the number of languages involved in earlier\nsupervised tasks of generating inflected forms, this year the shared task also\nfeatured a new second task which asked participants to inflect words in\nsentential context, similar to a cloze task. This second task featured seven\nlanguages. Task 1 received 27 submissions and task 2 received 6 submissions.\nBoth tasks featured a low, medium, and high data condition. Nearly all\nsubmissions featured a neural component and built on highly-ranked systems from\nthe earlier 2017 shared task. In the inflection task (task 1), 41 of the 52\nlanguages present in last year's inflection task showed improvement by the best\nsystems in the low-resource setting. The cloze task (task 2) proved to be\ndifficult, and few submissions managed to consistently improve upon both a\nsimple neural baseline system and a lemma-repeating baseline.", "published": "2018-10-16 16:39:49", "link": "http://arxiv.org/abs/1810.07125v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Subword Semantic Hashing for Intent Classification on Small Datasets", "abstract": "In this paper, we introduce the use of Semantic Hashing as embedding for the\ntask of Intent Classification and achieve state-of-the-art performance on three\nfrequently used benchmarks. Intent Classification on a small dataset is a\nchallenging task for data-hungry state-of-the-art Deep Learning based systems.\nSemantic Hashing is an attempt to overcome such a challenge and learn robust\ntext classification. Current word embedding based are dependent on\nvocabularies. One of the major drawbacks of such methods is out-of-vocabulary\nterms, especially when having small training datasets and using a wider\nvocabulary. This is the case in Intent Classification for chatbots, where\ntypically small datasets are extracted from internet communication. Two\nproblems arise by the use of internet communication. First, such datasets miss\na lot of terms in the vocabulary to use word embeddings efficiently. Second,\nusers frequently make spelling errors. Typically, the models for intent\nclassification are not trained with spelling errors and it is difficult to\nthink about ways in which users will make mistakes. Models depending on a word\nvocabulary will always face such issues. An ideal classifier should handle\nspelling errors inherently. With Semantic Hashing, we overcome these challenges\nand achieve state-of-the-art results on three datasets: AskUbuntu, Chatbot, and\nWeb Application. Our benchmarks are available online:\nhttps://github.com/kumar-shridhar/Know-Your-Intent", "published": "2018-10-16 17:25:22", "link": "http://arxiv.org/abs/1810.07150v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Strategies for Language Identification in Code-Mixed Low Resource\n  Languages", "abstract": "In recent years, substantial work has been done on language tagging of\ncode-mixed data, but most of them use large amounts of data to build their\nmodels. In this article, we present three strategies to build a word level\nlanguage tagger for code-mixed data using very low resources. Each of them\nsecured an accuracy higher than our baseline model, and the best performing\nsystem got an accuracy around 91%. Combining all, the ensemble system achieved\nan accuracy of around 92.6%.", "published": "2018-10-16 17:35:31", "link": "http://arxiv.org/abs/1810.07156v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A survey of automatic de-identification of longitudinal clinical\n  narratives", "abstract": "Use of medical data, also known as electronic health records, in research\nhelps develop and advance medical science. However, protecting patient\nconfidentiality and identity while using medical data for analysis is crucial.\nMedical data can be in the form of tabular structures (i.e. tables), free-form\nnarratives, and images. This study focuses on medical data in the free form\nlongitudinal text. De-identification of electronic health records provides the\nopportunity to use such data for research without it affecting patient privacy,\nand avoids the need for individual patient consent. In recent years there is\nincreasing interest in developing an accurate, robust and adaptable automatic\nde-identification system for electronic health records. This is mainly due to\nthe dilemma between the availability of an abundance of health data, and the\ninability to use such data in research due to legal and ethical restrictions.\nDe-identification tracks in competitions such as the 2014 i2b2 UTHealth and the\n2016 CEGS N-GRID shared tasks have provided a great platform to advance this\narea. The primary reasons for this include the open source nature of the\ndataset and the fact that raw psychiatric data were used for 2016 competitions.\nThis study focuses on noticeable trend changes in the techniques used in the\ndevelopment of automatic de-identification for longitudinal clinical\nnarratives. More specifically, the shift from using conditional random fields\n(CRF) based systems only or rules (regular expressions, dictionary or\ncombinations) based systems only, to hybrid models (combining CRF and rules),\nand more recently to deep learning based systems. We review the literature and\nresults that arose from the 2014 and the 2016 competitions and discuss the\noutcomes of these systems. We also provide a list of research questions that\nemerged from this survey.", "published": "2018-10-16 00:26:39", "link": "http://arxiv.org/abs/1810.06765v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Named Entity Analysis and Extraction with Uncommon Words", "abstract": "Most previous research treats named entity extraction and classification as\nan end-to-end task. We argue that the two sub-tasks should be addressed\nseparately. Entity extraction lies at the level of syntactic analysis while\nentity classification lies at the level of semantic analysis. According to Noam\nChomsky's \"Syntactic Structures,\" pp. 93-94 (Chomsky 1957), syntax is not\nappealed to semantics and semantics does not affect syntax. We analyze two\nbenchmark datasets for the characteristics of named entities, finding that\nuncommon words can distinguish named entities from common text; where uncommon\nwords are the words that hardly appear in common text and they are mainly the\nproper nouns. Experiments validate that lexical and syntactic features achieve\nstate-of-the-art performance on entity extraction and that semantic features do\nnot further improve the extraction performance, in both of our model and the\nstate-of-the-art baselines. With Chomsky's view, we also explain the failure of\njoint syntactic and semantic parsings in other works.", "published": "2018-10-16 05:40:03", "link": "http://arxiv.org/abs/1810.06818v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Automatic event detection in microblogs using incremental machine\n  learning", "abstract": "The global popularity of microblogs has led to an increasing accumulation of\nlarge volumes of text data on microblogging platforms such as Twitter. These\ncorpora are untapped resources to understand social expressions on diverse\nsubjects. Microblog analysis aims to unlock the value of such expressions by\ndiscovering insights and events of significance hidden among swathes of text.\nBesides velocity; diversity of content, brevity, absence of structure and\ntime-sensitivity are key challenges in microblog analysis. In this paper, we\npropose an unsupervised incremental machine learning and event detection\ntechnique to address these challenges. The proposed technique separates a\nmicroblog discussion into topics to address the key problem of diversity. It\nmaintains a record of the evolution of each topic over time. Brevity,\ntime-sensitivity and unstructured nature are addressed by these individual\ntopic pathways which contribute to generate a temporal, topic-driven structure\nof a microblog discussion. The proposed event detection method continuously\nmonitors these topic pathways using multiple domain-independent event\nindicators for events of significance. The autonomous nature of topic\nseparation, topic pathway generation, new topic identification and event\ndetection, appropriates the proposed technique for extensive applications in\nmicroblog analysis. We demonstrate these capabilities on tweets containing\n#microsoft and tweets containing #obama.", "published": "2018-10-16 21:56:25", "link": "http://arxiv.org/abs/1811.05757v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "abstract": "This paper proposes a neural sequence-to-sequence text-to-speech (TTS) model\nwhich can control latent attributes in the generated speech that are rarely\nannotated in the training data, such as speaking style, accent, background\nnoise, and recording conditions. The model is formulated as a conditional\ngenerative model based on the variational autoencoder (VAE) framework, with two\nlevels of hierarchical latent variables. The first level is a categorical\nvariable, which represents attribute groups (e.g. clean/noisy) and provides\ninterpretability. The second level, conditioned on the first, is a multivariate\nGaussian variable, which characterizes specific attribute configurations (e.g.\nnoise level, speaking rate) and enables disentangled fine-grained control over\nthese attributes. This amounts to using a Gaussian mixture model (GMM) for the\nlatent distribution. Extensive evaluation demonstrates its ability to control\nthe aforementioned attributes. In particular, we train a high-quality\ncontrollable TTS model on real found data, which is capable of inferring\nspeaker and style attributes from a noisy utterance and use it to synthesize\nclean speech with controllable speaking style.", "published": "2018-10-16 18:20:02", "link": "http://arxiv.org/abs/1810.07217v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Exploring Sentence Vector Spaces through Automatic Summarization", "abstract": "Given vector representations for individual words, it is necessary to compute\nvector representations of sentences for many applications in a compositional\nmanner, often using artificial neural networks.\n  Relatively little work has explored the internal structure and properties of\nsuch sentence vectors. In this paper, we explore the properties of sentence\nvectors in the context of automatic summarization. In particular, we show that\ncosine similarity between sentence vectors and document vectors is strongly\ncorrelated with sentence importance and that vector semantics can identify and\ncorrect gaps between the sentences chosen so far and the document. In addition,\nwe identify specific dimensions which are linked to effective summaries. To our\nknowledge, this is the first time specific dimensions of sentence embeddings\nhave been connected to sentence properties. We also compare the features of\ndifferent methods of sentence embeddings. Many of these insights have\napplications in uses of sentence embeddings far beyond summarization.", "published": "2018-10-16 23:57:37", "link": "http://arxiv.org/abs/1810.07320v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Multi-Task Deep Learning for Legal Document Translation, Summarization\n  and Multi-Label Classification", "abstract": "The digitalization of the legal domain has been ongoing for a couple of\nyears. In that process, the application of different machine learning (ML)\ntechniques is crucial. Tasks such as the classification of legal documents or\ncontract clauses as well as the translation of those are highly relevant. On\nthe other side, digitized documents are barely accessible in this field,\nparticularly in Germany. Today, deep learning (DL) is one of the hot topics\nwith many publications and various applications. Sometimes it provides results\noutperforming the human level. Hence this technique may be feasible for the\nlegal domain as well. However, DL requires thousands of samples to provide\ndecent results. A potential solution to this problem is multi-task DL to enable\ntransfer learning. This approach may be able to overcome the data scarcity\nproblem in the legal domain, specifically for the German language. We applied\nthe state of the art multi-task model on three tasks: translation,\nsummarization, and multi-label classification. The experiments were conducted\non legal document corpora utilizing several task combinations as well as\nvarious model parameters. The goal was to find the optimal configuration for\nthe tasks at hand within the legal domain. The multi-task DL approach\noutperformed the state of the art results in all three tasks. This opens a new\ndirection to integrate DL technology more efficiently in the legal domain.", "published": "2018-10-16 08:54:50", "link": "http://arxiv.org/abs/1810.07513v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Fine-tuning on Clean Data for End-to-End Speech Translation: FBK @ IWSLT\n  2018", "abstract": "This paper describes FBK's submission to the end-to-end English-German speech\ntranslation task at IWSLT 2018. Our system relies on a state-of-the-art model\nbased on LSTMs and CNNs, where the CNNs are used to reduce the temporal\ndimension of the audio input, which is in general much higher than machine\ntranslation input. Our model was trained only on the audio-to-text parallel\ndata released for the task, and fine-tuned on cleaned subsets of the original\ntraining corpus. The addition of weight normalization and label smoothing\nimproved the baseline system by 1.0 BLEU point on our validation set. The final\nsubmission also featured checkpoint averaging within a training run and\nensemble decoding of models trained during multiple runs. On test data, our\nbest single model obtained a BLEU score of 9.7, while the ensemble obtained a\nBLEU score of 10.24.", "published": "2018-10-16 09:54:37", "link": "http://arxiv.org/abs/1810.07652v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Sequence-to-Sequence Acoustic Modeling for Voice Conversion", "abstract": "In this paper, a neural network named Sequence-to-sequence ConvErsion NeTwork\n(SCENT) is presented for acoustic modeling in voice conversion. At training\nstage, a SCENT model is estimated by aligning the feature sequences of source\nand target speakers implicitly using attention mechanism. At conversion stage,\nacoustic features and durations of source utterances are converted\nsimultaneously using the unified acoustic model. Mel-scale spectrograms are\nadopted as acoustic features which contain both excitation and vocal tract\ndescriptions of speech signals. The bottleneck features extracted from source\nspeech using an automatic speech recognition (ASR) model are appended as\nauxiliary input. A WaveNet vocoder conditioned on Mel-spectrograms is built to\nreconstruct waveforms from the outputs of the SCENT model. It is worth noting\nthat our proposed method can achieve appropriate duration conversion which is\ndifficult in conventional methods. Experimental results show that our proposed\nmethod obtained better objective and subjective performance than the baseline\nmethods using Gaussian mixture models (GMM) and deep neural networks (DNN) as\nacoustic models. This proposed method also outperformed our previous work which\nachieved the top rank in Voice Conversion Challenge 2018. Ablation tests\nfurther confirmed the effectiveness of several components in our proposed\nmethod.", "published": "2018-10-16 08:11:32", "link": "http://arxiv.org/abs/1810.06865v5", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sound event detection using weakly-labeled semi-supervised data with\n  GCRNNS, VAT and Self-Adaptive Label Refinement", "abstract": "In this paper, we present a gated convolutional recurrent neural network\nbased approach to solve task 4, large-scale weakly labelled semi-supervised\nsound event detection in domestic environments, of the DCASE 2018 challenge.\nGated linear units and a temporal attention layer are used to predict the onset\nand offset of sound events in 10s long audio clips. Whereby for training only\nweakly-labelled data is used. Virtual adversarial training is used for\nregularization, utilizing both labelled and unlabeled data. Furthermore, we\nintroduce self-adaptive label refinement, a method which allows unsupervised\nadaption of our trained system to refine the accuracy of frame-level class\npredictions. The proposed system reaches an overall macro averaged event-based\nF-score of 34.6%, resulting in a relative improvement of 20.5% over the\nbaseline system.", "published": "2018-10-16 09:36:25", "link": "http://arxiv.org/abs/1810.06897v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep neural network based i-vector mapping for speaker verification\n  using short utterances", "abstract": "Text-independent speaker recognition using short utterances is a highly\nchallenging task due to the large variation and content mismatch between short\nutterances. I-vector based systems have become the standard in speaker\nverification applications, but they are less effective with short utterances.\nIn this paper, we first compare two state-of-the-art universal background model\ntraining methods for i-vector modeling using full-length and short utterance\nevaluation tasks. The two methods are Gaussian mixture model (GMM) based and\ndeep neural network (DNN) based methods. The results indicate that the\nI-vector_DNN system outperforms the I-vector_GMM system under various\ndurations. However, the performances of both systems degrade significantly as\nthe duration of the utterances decreases. To address this issue, we propose two\nnovel nonlinear mapping methods which train DNN models to map the i-vectors\nextracted from short utterances to their corresponding long-utterance\ni-vectors. The mapped i-vector can restore missing information and reduce the\nvariance of the original short-utterance i-vectors. The proposed methods both\nmodel the joint representation of short and long utterance i-vectors by using\nautoencoder. Experimental results using the NIST SRE 2010 dataset show that\nboth methods provide significant improvement and result in a max of 28.43%\nrelative improvement in Equal Error Rates from a baseline system, when using\ndeep encoder with residual blocks and adding an additional phoneme vector. When\nfurther testing the best-validated models of SRE10 on the Speaker In The Wild\ndataset, the methods result in a 23.12% improvement on arbitrary-duration (1-5\ns) short-utterance conditions.", "published": "2018-10-16 23:16:38", "link": "http://arxiv.org/abs/1810.07309v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
