{"title": "Knowledge-Prompted Estimator: A Novel Approach to Explainable Machine\n  Translation Assessment", "abstract": "Cross-lingual Machine Translation (MT) quality estimation plays a crucial\nrole in evaluating translation performance. GEMBA, the first MT quality\nassessment metric based on Large Language Models (LLMs), employs one-step\nprompting to achieve state-of-the-art (SOTA) in system-level MT quality\nestimation; however, it lacks segment-level analysis. In contrast,\nChain-of-Thought (CoT) prompting outperforms one-step prompting by offering\nimproved reasoning and explainability. In this paper, we introduce\nKnowledge-Prompted Estimator (KPE), a CoT prompting method that combines three\none-step prompting techniques, including perplexity, token-level similarity,\nand sentence-level similarity. This method attains enhanced performance for\nsegment-level estimation compared with previous deep learning models and\none-step prompting approaches. Furthermore, supplementary experiments on\nword-level visualized alignment demonstrate that our KPE method significantly\nimproves token alignment compared with earlier models and provides better\ninterpretability for MT quality estimation. Code will be released upon\npublication.", "published": "2023-06-13 01:18:32", "link": "http://arxiv.org/abs/2306.07486v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HAUSER: Towards Holistic and Automatic Evaluation of Simile Generation", "abstract": "Similes play an imperative role in creative writing such as story and\ndialogue generation. Proper evaluation metrics are like a beacon guiding the\nresearch of simile generation (SG). However, it remains under-explored as to\nwhat criteria should be considered, how to quantify each criterion into\nmetrics, and whether the metrics are effective for comprehensive, efficient,\nand reliable SG evaluation. To address the issues, we establish HAUSER, a\nholistic and automatic evaluation system for the SG task, which consists of\nfive criteria from three perspectives and automatic metrics for each criterion.\nThrough extensive experiments, we verify that our metrics are significantly\nmore correlated with human ratings from each perspective compared with prior\nautomatic metrics.", "published": "2023-06-13 06:06:01", "link": "http://arxiv.org/abs/2306.07554v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question Decomposition Tree for Answering Complex Questions over\n  Knowledge Bases", "abstract": "Knowledge base question answering (KBQA) has attracted a lot of interest in\nrecent years, especially for complex questions which require multiple facts to\nanswer. Question decomposition is a promising way to answer complex questions.\nExisting decomposition methods split the question into sub-questions according\nto a single compositionality type, which is not sufficient for questions\ninvolving multiple compositionality types. In this paper, we propose Question\nDecomposition Tree (QDT) to represent the structure of complex questions.\nInspired by recent advances in natural language generation (NLG), we present a\ntwo-staged method called Clue-Decipher to generate QDT. It can leverage the\nstrong ability of NLG model and simultaneously preserve the original questions.\nTo verify that QDT can enhance KBQA task, we design a decomposition-based KBQA\nsystem called QDTQA. Extensive experiments show that QDTQA outperforms previous\nstate-of-the-art methods on ComplexWebQuestions dataset. Besides, our\ndecomposition method improves an existing KBQA system by 12% and sets a new\nstate-of-the-art on LC-QuAD 1.0.", "published": "2023-06-13 07:44:29", "link": "http://arxiv.org/abs/2306.07597v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Soft Language Clustering for Multilingual Model Pre-training", "abstract": "Multilingual pre-trained language models have demonstrated impressive\n(zero-shot) cross-lingual transfer abilities, however, their performance is\nhindered when the target language has distant typology from source languages or\nwhen pre-training data is limited in size. In this paper, we propose XLM-P,\nwhich contextually retrieves prompts as flexible guidance for encoding\ninstances conditionally. Our XLM-P enables (1) lightweight modeling of\nlanguage-invariant and language-specific knowledge across languages, and (2)\neasy integration with other multilingual pre-training methods. On the tasks of\nXTREME including text classification, sequence labeling, question answering,\nand sentence retrieval, both base- and large-size language models pre-trained\nwith our proposed method exhibit consistent performance improvement.\nFurthermore, it provides substantial advantages for low-resource languages in\nunsupervised sentence retrieval and for target languages that differ greatly\nfrom the source language in cross-lingual transfer.", "published": "2023-06-13 08:08:08", "link": "http://arxiv.org/abs/2306.07610v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rank-Aware Negative Training for Semi-Supervised Text Classification", "abstract": "Semi-supervised text classification-based paradigms (SSTC) typically employ\nthe spirit of self-training. The key idea is to train a deep classifier on\nlimited labeled texts and then iteratively predict the unlabeled texts as their\npseudo-labels for further training. However, the performance is largely\naffected by the accuracy of pseudo-labels, which may not be significant in\nreal-world scenarios. This paper presents a Rank-aware Negative Training (RNT)\nframework to address SSTC in learning with noisy label manner. To alleviate the\nnoisy information, we adapt a reasoning with uncertainty-based approach to rank\nthe unlabeled texts based on the evidential support received from the labeled\ntexts. Moreover, we propose the use of negative training to train RNT based on\nthe concept that ``the input instance does not belong to the complementary\nlabel''. A complementary label is randomly selected from all labels except the\nlabel on-target. Intuitively, the probability of a true label serving as a\ncomplementary label is low and thus provides less noisy information during the\ntraining, resulting in better performance on the test data. Finally, we\nevaluate the proposed solution on various text classification benchmark\ndatasets. Our extensive experiments show that it consistently overcomes the\nstate-of-the-art alternatives in most scenarios and achieves competitive\nperformance in the others. The code of RNT is publicly available\nat:https://github.com/amurtadha/RNT.", "published": "2023-06-13 08:41:36", "link": "http://arxiv.org/abs/2306.07621v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hybrid lemmatization in HuSpaCy", "abstract": "Lemmatization is still not a trivial task for morphologically rich languages.\nPrevious studies showed that hybrid architectures usually work better for these\nlanguages and can yield great results. This paper presents a hybrid lemmatizer\nutilizing both a neural model, dictionaries and hand-crafted rules. We\nintroduce a hybrid architecture along with empirical results on a widely used\nHungarian dataset. The presented methods are published as three HuSpaCy models.", "published": "2023-06-13 09:15:40", "link": "http://arxiv.org/abs/2306.07636v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Is Anisotropy Inherent to Transformers?", "abstract": "The representation degeneration problem is a phenomenon that is widely\nobserved among self-supervised learning methods based on Transformers. In NLP,\nit takes the form of anisotropy, a singular property of hidden representations\nwhich makes them unexpectedly close to each other in terms of angular distance\n(cosine-similarity). Some recent works tend to show that anisotropy is a\nconsequence of optimizing the cross-entropy loss on long-tailed distributions\nof tokens. We show in this paper that anisotropy can also be observed\nempirically in language models with specific objectives that should not suffer\ndirectly from the same consequences. We also show that the anisotropy problem\nextends to Transformers trained on other modalities. Our observations tend to\ndemonstrate that anisotropy might actually be inherent to Transformers-based\nmodels.", "published": "2023-06-13 09:54:01", "link": "http://arxiv.org/abs/2306.07656v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NAVER LABS Europe's Multilingual Speech Translation Systems for the\n  IWSLT 2023 Low-Resource Track", "abstract": "This paper presents NAVER LABS Europe's systems for Tamasheq-French and\nQuechua-Spanish speech translation in the IWSLT 2023 Low-Resource track. Our\nwork attempts to maximize translation quality in low-resource settings using\nmultilingual parameter-efficient solutions that leverage strong pre-trained\nmodels. Our primary submission for Tamasheq outperforms the previous state of\nthe art by 7.5 BLEU points on the IWSLT 2022 test set, and achieves 23.6 BLEU\non this year's test set, outperforming the second best participant by 7.7\npoints. For Quechua, we also rank first and achieve 17.7 BLEU, despite having\nonly two hours of translation data. Finally, we show that our proposed\nmultilingual architecture is also competitive for high-resource languages,\noutperforming the best unconstrained submission to the IWSLT 2021 Multilingual\ntrack, despite using much less training data and compute.", "published": "2023-06-13 13:22:30", "link": "http://arxiv.org/abs/2306.07763v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adversarial Capsule Networks for Romanian Satire Detection and Sentiment\n  Analysis", "abstract": "Satire detection and sentiment analysis are intensively explored natural\nlanguage processing (NLP) tasks that study the identification of the satirical\ntone from texts and extracting sentiments in relationship with their targets.\nIn languages with fewer research resources, an alternative is to produce\nartificial examples based on character-level adversarial processes to overcome\ndataset size limitations. Such samples are proven to act as a regularization\nmethod, thus improving the robustness of models. In this work, we improve the\nwell-known NLP models (i.e., Convolutional Neural Networks, Long Short-Term\nMemory (LSTM), Bidirectional LSTM, Gated Recurrent Units (GRUs), and\nBidirectional GRUs) with adversarial training and capsule networks. The\nfine-tuned models are used for satire detection and sentiment analysis tasks in\nthe Romanian language. The proposed framework outperforms the existing methods\nfor the two tasks, achieving up to 99.08% accuracy, thus confirming the\nimprovements added by the capsule layers and the adversarial training in NLP\napproaches.", "published": "2023-06-13 15:23:44", "link": "http://arxiv.org/abs/2306.07845v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Questioning the Survey Responses of Large Language Models", "abstract": "Surveys have recently gained popularity as a tool to study large language\nmodels. By comparing survey responses of models to those of human reference\npopulations, researchers aim to infer the demographics, political opinions, or\nvalues best represented by current language models. In this work, we critically\nexamine this methodology on the basis of the well-established American\nCommunity Survey by the U.S. Census Bureau. Evaluating 43 different language\nmodels using de-facto standard prompting methodologies, we establish two\ndominant patterns. First, models' responses are governed by ordering and\nlabeling biases, for example, towards survey responses labeled with the letter\n\"A\". Second, when adjusting for these systematic biases through randomized\nanswer ordering, models across the board trend towards uniformly random survey\nresponses, irrespective of model size or pre-training data. As a result, in\ncontrast to conjectures from prior work, survey-derived alignment measures\noften permit a simple explanation: models consistently appear to better\nrepresent subgroups whose aggregate statistics are closest to uniform for any\nsurvey under consideration.", "published": "2023-06-13 17:48:27", "link": "http://arxiv.org/abs/2306.07951v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Resources for Brewing BEIR: Reproducible Reference Models and an\n  Official Leaderboard", "abstract": "BEIR is a benchmark dataset for zero-shot evaluation of information retrieval\nmodels across 18 different domain/task combinations. In recent years, we have\nwitnessed the growing popularity of a representation learning approach to\nbuilding retrieval models, typically using pretrained transformers in a\nsupervised setting. This naturally begs the question: How effective are these\nmodels when presented with queries and documents that differ from the training\ndata? Examples include searching in different domains (e.g., medical or legal\ntext) and with different types of queries (e.g., keywords vs. well-formed\nquestions). While BEIR was designed to answer these questions, our work\naddresses two shortcomings that prevent the benchmark from achieving its full\npotential: First, the sophistication of modern neural methods and the\ncomplexity of current software infrastructure create barriers to entry for\nnewcomers. To this end, we provide reproducible reference implementations that\ncover the two main classes of approaches: learned dense and sparse models.\nSecond, there does not exist a single authoritative nexus for reporting the\neffectiveness of different models on BEIR, which has led to difficulty in\ncomparing different methods. To remedy this, we present an official\nself-service BEIR leaderboard that provides fair and consistent comparisons of\nretrieval models. By addressing both shortcomings, our work facilitates future\nexplorations in a range of interesting research questions that BEIR enables.", "published": "2023-06-13 00:26:18", "link": "http://arxiv.org/abs/2306.07471v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Large Language Models Sometimes Generate Purely Negatively-Reinforced\n  Text", "abstract": "When using adversarial training, it is common practice to train against the\nmost egregious failures. However, this might imply using examples with\nsensitive information (such as leaked passwords or security vulnerabilities) as\ntraining data. One might assume that language models trained with gradient\ndescent never generate text snippets which were only present in examples\nassociated with the lowest possible reward. In this paper, we show that this\nassumption is wrong: in some situations, large language models do learn from\nsuch negatively-reinforced examples. We present a specific training setup that\nenables Pythia-160M to guess passwords 13% more often than it would by guessing\nrandomly, despite only showing it these passwords on examples where the model\nis incentivized to not output these passwords. Our code is available at\nwww.github.com/FabienRoger/Learning-From-Negative-Examples", "published": "2023-06-13 06:40:37", "link": "http://arxiv.org/abs/2306.07567v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SqueezeLLM: Dense-and-Sparse Quantization", "abstract": "Generative Large Language Models (LLMs) have demonstrated remarkable results\nfor a wide range of tasks. However, deploying these models for inference has\nbeen a significant challenge due to their unprecedented resource requirements.\nThis has forced existing deployment frameworks to use multi-GPU inference\npipelines, which are often complex and costly, or to use smaller and less\nperformant models. In this work, we demonstrate that the main bottleneck for\ngenerative inference with LLMs is memory bandwidth, rather than compute,\nspecifically for single batch inference. While quantization has emerged as a\npromising solution by representing weights with reduced precision, previous\nefforts have often resulted in notable performance degradation. To address\nthis, we introduce SqueezeLLM, a post-training quantization framework that not\nonly enables lossless compression to ultra-low precisions of up to 3-bit, but\nalso achieves higher quantization performance under the same memory constraint.\nOur framework incorporates two novel ideas: (i) sensitivity-based non-uniform\nquantization, which searches for the optimal bit precision assignment based on\nsecond-order information; and (ii) the Dense-and-Sparse decomposition that\nstores outliers and sensitive weight values in an efficient sparse format. When\napplied to the LLaMA models, our 3-bit quantization significantly reduces the\nperplexity gap from the FP16 baseline by up to 2.1x as compared to the\nstate-of-the-art methods with the same memory requirement. Furthermore, when\ndeployed on an A6000 GPU, our quantized models achieve up to 2.3x speedup\ncompared to the baseline. Our code is available at\nhttps://github.com/SqueezeAILab/SqueezeLLM.", "published": "2023-06-13 08:57:54", "link": "http://arxiv.org/abs/2306.07629v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tokenization with Factorized Subword Encoding", "abstract": "In recent years, language models have become increasingly larger and more\ncomplex. However, the input representations for these models continue to rely\non simple and greedy subword tokenization methods. In this paper, we propose a\nnovel tokenization method that factorizes subwords onto discrete triplets using\na VQ-VAE model. The effectiveness of the proposed tokenization method, referred\nto as the Factorizer, is evaluated on language modeling and morpho-syntactic\ntasks for 7 diverse languages. Results indicate that this method is more\nappropriate and robust for morphological tasks than the commonly used byte-pair\nencoding (BPE) tokenization algorithm.", "published": "2023-06-13 13:27:34", "link": "http://arxiv.org/abs/2306.07764v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Cloud-based Machine Learning Pipeline for the Efficient Extraction of\n  Insights from Customer Reviews", "abstract": "The efficiency of natural language processing has improved dramatically with\nthe advent of machine learning models, particularly neural network-based\nsolutions. However, some tasks are still challenging, especially when\nconsidering specific domains. In this paper, we present a cloud-based system\nthat can extract insights from customer reviews using machine learning methods\nintegrated into a pipeline. For topic modeling, our composite model uses\ntransformer-based neural networks designed for natural language processing,\nvector embedding-based keyword extraction, and clustering. The elements of our\nmodel have been integrated and further developed to meet better the\nrequirements of efficient information extraction, topic modeling of the\nextracted information, and user needs. Furthermore, our system can achieve\nbetter results than this task's existing topic modeling and keyword extraction\nsolutions. Our approach is validated and compared with other state-of-the-art\nmethods using publicly available datasets for benchmarking.", "published": "2023-06-13 14:07:52", "link": "http://arxiv.org/abs/2306.07786v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NoCoLA: The Norwegian Corpus of Linguistic Acceptability", "abstract": "While there has been a surge of large language models for Norwegian in recent\nyears, we lack any tool to evaluate their understanding of grammaticality. We\npresent two new Norwegian datasets for this task. NoCoLA_class is a supervised\nbinary classification task where the goal is to discriminate between acceptable\nand non-acceptable sentences. On the other hand, NoCoLA_zero is a purely\ndiagnostic task for evaluating the grammatical judgement of a language model in\na completely zero-shot manner, i.e. without any further training. In this\npaper, we describe both datasets in detail, show how to use them for different\nflavors of language models, and conduct a comparative study of the existing\nNorwegian language models.", "published": "2023-06-13 14:11:19", "link": "http://arxiv.org/abs/2306.07790v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Monolingual and Cross-Lingual Knowledge Transfer for Topic\n  Classification", "abstract": "This article investigates the knowledge transfer from the RuQTopics dataset.\nThis Russian topical dataset combines a large sample number (361,560\nsingle-label, 170,930 multi-label) with extensive class coverage (76 classes).\nWe have prepared this dataset from the \"Yandex Que\" raw data. By evaluating the\nRuQTopics - trained models on the six matching classes of the Russian MASSIVE\nsubset, we have proved that the RuQTopics dataset is suitable for real-world\nconversational tasks, as the Russian-only models trained on this dataset\nconsistently yield an accuracy around 85\\% on this subset. We also have figured\nout that for the multilingual BERT, trained on the RuQTopics and evaluated on\nthe same six classes of MASSIVE (for all MASSIVE languages), the language-wise\naccuracy closely correlates (Spearman correlation 0.773 with p-value 2.997e-11)\nwith the approximate size of the pretraining BERT's data for the corresponding\nlanguage. At the same time, the correlation of the language-wise accuracy with\nthe linguistical distance from Russian is not statistically significant.", "published": "2023-06-13 14:19:45", "link": "http://arxiv.org/abs/2306.07797v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use\n  Large Language Models for Text Production Tasks", "abstract": "Large language models (LLMs) are remarkable data annotators. They can be used\nto generate high-fidelity supervised training data, as well as survey and\nexperimental data. With the widespread adoption of LLMs, human gold--standard\nannotations are key to understanding the capabilities of LLMs and the validity\nof their results. However, crowdsourcing, an important, inexpensive way to\nobtain human annotations, may itself be impacted by LLMs, as crowd workers have\nfinancial incentives to use LLMs to increase their productivity and income. To\ninvestigate this concern, we conducted a case study on the prevalence of LLM\nusage by crowd workers. We reran an abstract summarization task from the\nliterature on Amazon Mechanical Turk and, through a combination of keystroke\ndetection and synthetic text classification, estimate that 33-46% of crowd\nworkers used LLMs when completing the task. Although generalization to other,\nless LLM-friendly tasks is unclear, our results call for platforms,\nresearchers, and crowd workers to find new ways to ensure that human data\nremain human, perhaps using the methodology proposed here as a stepping stone.\nCode/data: https://github.com/epfl-dlab/GPTurk", "published": "2023-06-13 16:46:24", "link": "http://arxiv.org/abs/2306.07899v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Massively Multilingual Corpus of Sentiment Datasets and Multi-faceted\n  Sentiment Classification Benchmark", "abstract": "Despite impressive advancements in multilingual corpora collection and model\ntraining, developing large-scale deployments of multilingual models still\npresents a significant challenge. This is particularly true for language tasks\nthat are culture-dependent. One such example is the area of multilingual\nsentiment analysis, where affective markers can be subtle and deeply ensconced\nin culture. This work presents the most extensive open massively multilingual\ncorpus of datasets for training sentiment models. The corpus consists of 79\nmanually selected datasets from over 350 datasets reported in the scientific\nliterature based on strict quality criteria. The corpus covers 27 languages\nrepresenting 6 language families. Datasets can be queried using several\nlinguistic and functional features. In addition, we present a multi-faceted\nsentiment classification benchmark summarizing hundreds of experiments\nconducted on different base models, training objectives, dataset collections,\nand fine-tuning strategies.", "published": "2023-06-13 16:54:13", "link": "http://arxiv.org/abs/2306.07902v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WebGLM: Towards An Efficient Web-Enhanced Question Answering System with\n  Human Preferences", "abstract": "We present WebGLM, a web-enhanced question-answering system based on the\nGeneral Language Model (GLM). Its goal is to augment a pre-trained large\nlanguage model (LLM) with web search and retrieval capabilities while being\nefficient for real-world deployments. To achieve this, we develop WebGLM with\nstrategies for the LLM-augmented retriever, bootstrapped generator, and human\npreference-aware scorer. Specifically, we identify and address the limitations\nof WebGPT (OpenAI), through which WebGLM is enabled with accuracy, efficiency,\nand cost-effectiveness advantages. In addition, we propose systematic criteria\nfor evaluating web-enhanced QA systems. We conduct multi-dimensional human\nevaluation and quantitative ablation studies, which suggest the outperformance\nof the proposed WebGLM designs over existing systems. WebGLM with the\n10-billion-parameter GLM (10B) is shown to perform better than the\nsimilar-sized WebGPT (13B) and even comparably to WebGPT (175B) in human\nevaluation. The code, demo, and data are at\n\\url{https://github.com/THUDM/WebGLM}.", "published": "2023-06-13 16:57:53", "link": "http://arxiv.org/abs/2306.07906v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "arXiVeri: Automatic table verification with GPT", "abstract": "Without accurate transcription of numerical data in scientific documents, a\nscientist cannot draw accurate conclusions. Unfortunately, the process of\ncopying numerical data from one paper to another is prone to human error. In\nthis paper, we propose to meet this challenge through the novel task of\nautomatic table verification (AutoTV), in which the objective is to verify the\naccuracy of numerical data in tables by cross-referencing cited sources. To\nsupport this task, we propose a new benchmark, arXiVeri, which comprises\ntabular data drawn from open-access academic papers on arXiv. We introduce\nmetrics to evaluate the performance of a table verifier in two key areas: (i)\ntable matching, which aims to identify the source table in a cited document\nthat corresponds to a target table, and (ii) cell matching, which aims to\nlocate shared cells between a target and source table and identify their row\nand column indices accurately. By leveraging the flexible capabilities of\nmodern large language models (LLMs), we propose simple baselines for table\nverification. Our findings highlight the complexity of this task, even for\nstate-of-the-art LLMs like OpenAI's GPT-4. The code and benchmark will be made\npublicly available.", "published": "2023-06-13 17:59:57", "link": "http://arxiv.org/abs/2306.07968v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Curatr: A Platform for Semantic Analysis and Curation of Historical\n  Literary Texts", "abstract": "The increasing availability of digital collections of historical and\ncontemporary literature presents a wealth of possibilities for new research in\nthe humanities. The scale and diversity of such collections however, presents\nparticular challenges in identifying and extracting relevant content. This\npaper presents Curatr, an online platform for the exploration and curation of\nliterature with machine learning-supported semantic search, designed within the\ncontext of digital humanities scholarship. The platform provides a text mining\nworkflow that combines neural word embeddings with expert domain knowledge to\nenable the generation of thematic lexicons, allowing researches to curate\nrelevant sub-corpora from a large corpus of 18th and 19th century digitised\ntexts.", "published": "2023-06-13 15:15:31", "link": "http://arxiv.org/abs/2306.08020v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FLamE: Few-shot Learning from Natural Language Explanations", "abstract": "Natural language explanations have the potential to provide rich information\nthat in principle guides model reasoning. Yet, recent work by Lampinen et al.\n(2022) has shown limited utility of natural language explanations in improving\nclassification. To effectively learn from explanations, we present FLamE, a\ntwo-stage few-shot learning framework that first generates explanations using\nGPT-3, and then finetunes a smaller model (e.g., RoBERTa) with generated\nexplanations. Our experiments on natural language inference demonstrate\neffectiveness over strong baselines, increasing accuracy by 17.6% over GPT-3\nBabbage and 5.7% over GPT-3 Davinci in e-SNLI. Despite improving classification\nperformance, human evaluation surprisingly reveals that the majority of\ngenerated explanations does not adequately justify classification decisions.\nAdditional analyses point to the important role of label-specific cues (e.g.,\n\"not know\" for the neutral label) in generated explanations.", "published": "2023-06-13 18:01:46", "link": "http://arxiv.org/abs/2306.08042v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AutoML in the Age of Large Language Models: Current Challenges, Future\n  Opportunities and Risks", "abstract": "The fields of both Natural Language Processing (NLP) and Automated Machine\nLearning (AutoML) have achieved remarkable results over the past years. In NLP,\nespecially Large Language Models (LLMs) have experienced a rapid series of\nbreakthroughs very recently. We envision that the two fields can radically push\nthe boundaries of each other through tight integration. To showcase this\nvision, we explore the potential of a symbiotic relationship between AutoML and\nLLMs, shedding light on how they can benefit each other. In particular, we\ninvestigate both the opportunities to enhance AutoML approaches with LLMs from\ndifferent perspectives and the challenges of leveraging AutoML to further\nimprove LLMs. To this end, we survey existing work, and we critically assess\nrisks. We strongly believe that the integration of the two fields has the\npotential to disrupt both fields, NLP and AutoML. By highlighting conceivable\nsynergies, but also risks, we aim to foster further exploration at the\nintersection of AutoML and LLMs.", "published": "2023-06-13 19:51:22", "link": "http://arxiv.org/abs/2306.08107v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "PersonaPKT: Building Personalized Dialogue Agents via\n  Parameter-efficient Knowledge Transfer", "abstract": "Personalized dialogue agents (DAs) powered by large pre-trained language\nmodels (PLMs) often rely on explicit persona descriptions to maintain\npersonality consistency. However, such descriptions may not always be available\nor may pose privacy concerns. To tackle this bottleneck, we introduce\nPersonaPKT, a lightweight transfer learning approach that can build\npersona-consistent dialogue models without explicit persona descriptions. By\nrepresenting each persona as a continuous vector, PersonaPKT learns implicit\npersona-specific features directly from a small number of dialogue samples\nproduced by the same persona, adding less than 0.1% trainable parameters for\neach persona on top of the PLM backbone. Empirical results demonstrate that\nPersonaPKT effectively builds personalized DAs with high storage efficiency,\noutperforming various baselines in terms of persona consistency while\nmaintaining good response generation quality. In addition, it enhances privacy\nprotection by avoiding explicit persona descriptions. Overall, PersonaPKT is an\neffective solution for creating personalized DAs that respect user privacy.", "published": "2023-06-13 20:47:29", "link": "http://arxiv.org/abs/2306.08126v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large-scale Language Model Rescoring on Long-form Data", "abstract": "In this work, we study the impact of Large-scale Language Models (LLM) on\nAutomated Speech Recognition (ASR) of YouTube videos, which we use as a source\nfor long-form ASR. We demonstrate up to 8\\% relative reduction in Word Error\nEate (WER) on US English (en-us) and code-switched Indian English (en-in)\nlong-form ASR test sets and a reduction of up to 30\\% relative on Salient Term\nError Rate (STER) over a strong first-pass baseline that uses a maximum-entropy\nbased language model. Improved lattice processing that results in a lattice\nwith a proper (non-tree) digraph topology and carrying context from the 1-best\nhypothesis of the previous segment(s) results in significant wins in rescoring\nwith LLMs. We also find that the gains in performance from the combination of\nLLMs trained on vast quantities of available data (such as C4) and conventional\nneural LMs is additive and significantly outperforms a strong first-pass\nbaseline with a maximum entropy LM.\n  Copyright 2023 IEEE. Personal use of this material is permitted. Permission\nfrom IEEE must be obtained for all other uses, in any current or future media,\nincluding reprinting/republishing this material for advertising or promotional\npurposes, creating new collective works, for resale or redistribution to\nservers or lists, or reuse of any copyrighted component of this work in other\nworks.", "published": "2023-06-13 20:54:12", "link": "http://arxiv.org/abs/2306.08133v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Improving Opinion-based Question Answering Systems Through Label Error\n  Detection and Overwrite", "abstract": "Label error is a ubiquitous problem in annotated data. Large amounts of label\nerror substantially degrades the quality of deep learning models. Existing\nmethods to tackle the label error problem largely focus on the classification\ntask, and either rely on task specific architecture or require non-trivial\nadditional computations, which is undesirable or even unattainable for industry\nusage. In this paper, we propose LEDO: a model-agnostic and computationally\nefficient framework for Label Error Detection and Overwrite. LEDO is based on\nMonte Carlo Dropout combined with uncertainty metrics, and can be easily\ngeneralized to multiple tasks and data sets. Applying LEDO to an industry\nopinion-based question answering system demonstrates it is effective at\nimproving accuracy in all the core models. Specifically, LEDO brings 1.1% MRR\ngain for the retrieval model, 1.5% PR AUC improvement for the machine reading\ncomprehension model, and 0.9% rise in the Average Precision for the ranker, on\ntop of the strong baselines with a large-scale social media dataset.\nImportantly, LEDO is computationally efficient compared to methods that require\nloss function change, and cost-effective as the resulting data can be used in\nthe same continuous training pipeline for production. Further analysis shows\nthat these gains come from an improved decision boundary after cleaning the\nlabel errors existed in the training data.", "published": "2023-06-13 02:20:58", "link": "http://arxiv.org/abs/2306.07499v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adding guardrails to advanced chatbots", "abstract": "Generative AI models continue to become more powerful. The launch of ChatGPT\nin November 2022 has ushered in a new era of AI. ChatGPT and other similar\nchatbots have a range of capabilities, from answering student homework\nquestions to creating music and art. There are already concerns that humans may\nbe replaced by chatbots for a variety of jobs. Because of the wide spectrum of\ndata chatbots are built on, we know that they will have human errors and human\nbiases built into them. These biases may cause significant harm and/or inequity\ntoward different subpopulations. To understand the strengths and weakness of\nchatbot responses, we present a position paper that explores different use\ncases of ChatGPT to determine the types of questions that are answered fairly\nand the types that still need improvement. We find that ChatGPT is a fair\nsearch engine for the tasks we tested; however, it has biases on both text\ngeneration and code generation. We find that ChatGPT is very sensitive to\nchanges in the prompt, where small changes lead to different levels of\nfairness. This suggests that we need to immediately implement \"corrections\" or\nmitigation strategies in order to improve fairness of these systems. We suggest\ndifferent strategies to improve chatbots and also advocate for an impartial\nreview panel that has access to the model parameters to measure the levels of\ndifferent types of biases and then recommends safeguards that move toward\nresponses that are less discriminatory and more accurate.", "published": "2023-06-13 02:23:04", "link": "http://arxiv.org/abs/2306.07500v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Noisy Positive-Unlabeled Learning with Self-Training for Speculative\n  Knowledge Graph Reasoning", "abstract": "This paper studies speculative reasoning task on real-world knowledge graphs\n(KG) that contain both \\textit{false negative issue} (i.e., potential true\nfacts being excluded) and \\textit{false positive issue} (i.e., unreliable or\noutdated facts being included). State-of-the-art methods fall short in the\nspeculative reasoning ability, as they assume the correctness of a fact is\nsolely determined by its presence in KG, making them vulnerable to false\nnegative/positive issues. The new reasoning task is formulated as a noisy\nPositive-Unlabeled learning problem. We propose a variational framework, namely\nnPUGraph, that jointly estimates the correctness of both collected and\nuncollected facts (which we call \\textit{label posterior}) and updates model\nparameters during training. The label posterior estimation facilitates\nspeculative reasoning from two perspectives. First, it improves the robustness\nof a label posterior-aware graph encoder against false positive links. Second,\nit identifies missing facts to provide high-quality grounds of reasoning. They\nare unified in a simple yet effective self-training procedure. Empirically,\nextensive experiments on three benchmark KG and one Twitter dataset with\nvarious degrees of false negative/positive cases demonstrate the effectiveness\nof nPUGraph.", "published": "2023-06-13 02:43:21", "link": "http://arxiv.org/abs/2306.07512v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SI"], "primary_category": "cs.LG"}
{"title": "TART: A plug-and-play Transformer module for task-agnostic reasoning", "abstract": "Large language models (LLMs) exhibit in-context learning abilities which\nenable the same model to perform several tasks without any task-specific\ntraining. In contrast, traditional adaptation approaches, such as fine-tuning,\nmodify the underlying models for each specific task. In-context learning,\nhowever, consistently underperforms task-specific tuning approaches even when\npresented with the same examples. While most existing approaches (e.g., prompt\nengineering) focus on the LLM's learned representations to patch this\nperformance gap, our analysis actually reveal that LLM representations contain\nsufficient information to make good predictions. As such, we focus on the LLM's\nreasoning abilities and demonstrate that this performance gap exists due to\ntheir inability to perform simple probabilistic reasoning tasks. This raises an\nintriguing question: Are LLMs actually capable of learning how to reason in a\ntask-agnostic manner? We answer this in the affirmative and propose TART which\ngenerically improves an LLM's reasoning abilities using a synthetically trained\nTransformer-based reasoning module. TART trains this reasoning module in a\ntask-agnostic manner using only synthetic logistic regression tasks and\ncomposes it with an arbitrary real-world pre-trained model without any\nadditional training. With a single inference module, TART improves performance\nacross different model families (GPT-Neo, Pythia, BLOOM), model sizes (100M -\n6B), tasks (14 NLP binary classification tasks), and even across different\nmodalities (audio and vision). Additionally, on the RAFT Benchmark, TART\nimproves GPT-Neo (125M)'s performance such that it outperforms BLOOM (176B),\nand is within 4% of GPT-3 (175B). Our code and models are available at\nhttps://github.com/HazyResearch/TART .", "published": "2023-06-13 04:37:00", "link": "http://arxiv.org/abs/2306.07536v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Human-Like Intuitive Behavior and Reasoning Biases Emerged in Language\n  Models -- and Disappeared in GPT-4", "abstract": "Large language models (LLMs) are currently at the forefront of intertwining\nAI systems with human communication and everyday life. Therefore, it is of\ngreat importance to evaluate their emerging abilities. In this study, we show\nthat LLMs, most notably GPT-3, exhibit behavior that strikingly resembles\nhuman-like intuition -- and the cognitive errors that come with it. However,\nLLMs with higher cognitive capabilities, in particular ChatGPT and GPT-4,\nlearned to avoid succumbing to these errors and perform in a hyperrational\nmanner. For our experiments, we probe LLMs with the Cognitive Reflection Test\n(CRT) as well as semantic illusions that were originally designed to\ninvestigate intuitive decision-making in humans. Moreover, we probe how sturdy\nthe inclination for intuitive-like decision-making is. Our study demonstrates\nthat investigating LLMs with methods from psychology has the potential to\nreveal otherwise unknown emergent traits.", "published": "2023-06-13 08:43:13", "link": "http://arxiv.org/abs/2306.07622v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Modality Adaption or Regularization? A Case Study on End-to-End Speech\n  Translation", "abstract": "Pre-training and fine-tuning is a paradigm for alleviating the data scarcity\nproblem in end-to-end speech translation (E2E ST). The commonplace \"modality\ngap\" between speech and text data often leads to inconsistent inputs between\npre-training and fine-tuning. However, we observe that this gap occurs in the\nearly stages of fine-tuning, but does not have a major impact on the final\nperformance. On the other hand, we find that there has another gap, which we\ncall the \"capacity gap\": high resource tasks (such as ASR and MT) always\nrequire a large model to fit, when the model is reused for a low resource task\n(E2E ST), it will get a sub-optimal performance due to the over-fitting. In a\ncase study, we find that the regularization plays a more important role than\nthe well-designed modality adaption method, which achieves 29.0 for en-de and\n40.3 for en-fr on the MuST-C dataset. Code and models are available at\nhttps://github.com/hannlp/TAB.", "published": "2023-06-13 09:42:48", "link": "http://arxiv.org/abs/2306.07650v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Rethink the Effectiveness of Text Data Augmentation: An Empirical\n  Analysis", "abstract": "In recent years, language models (LMs) have made remarkable progress in\nadvancing the field of natural language processing (NLP). However, the impact\nof data augmentation (DA) techniques on the fine-tuning (FT) performance of\nthese LMs has been a topic of ongoing debate. In this study, we evaluate the\neffectiveness of three different FT methods in conjugation with\nback-translation across an array of 7 diverse NLP tasks, including\nclassification and regression types, covering single-sentence and sentence-pair\ntasks. Contrary to prior assumptions that DA does not contribute to the\nenhancement of LMs' FT performance, our findings reveal that continued\npre-training on augmented data can effectively improve the FT performance of\nthe downstream tasks. In the most favourable case, continued pre-training\nimproves the performance of FT by more than 10% in the few-shot learning\nsetting. Our finding highlights the potential of DA as a powerful tool for\nbolstering LMs' performance.", "published": "2023-06-13 10:14:58", "link": "http://arxiv.org/abs/2306.07664v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ChatGPT vs Human-authored Text: Insights into Controllable Text\n  Summarization and Sentence Style Transfer", "abstract": "Large-scale language models, like ChatGPT, have garnered significant media\nattention and stunned the public with their remarkable capacity for generating\ncoherent text from short natural language prompts. In this paper, we aim to\nconduct a systematic inspection of ChatGPT's performance in two controllable\ngeneration tasks, with respect to ChatGPT's ability to adapt its output to\ndifferent target audiences (expert vs. layman) and writing styles (formal vs.\ninformal). Additionally, we evaluate the faithfulness of the generated text,\nand compare the model's performance with human-authored texts. Our findings\nindicate that the stylistic variations produced by humans are considerably\nlarger than those demonstrated by ChatGPT, and the generated texts diverge from\nhuman samples in several characteristics, such as the distribution of word\ntypes. Moreover, we observe that ChatGPT sometimes incorporates factual errors\nor hallucinations when adapting the text to suit a specific style.", "published": "2023-06-13 14:21:35", "link": "http://arxiv.org/abs/2306.07799v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio\n  Pretraining for Accurate Speech Emotion Recognition", "abstract": "Contrastive cross-modality pretraining has recently exhibited impressive\nsuccess in diverse fields, whereas there is limited research on their merits in\nspeech emotion recognition (SER). In this paper, we propose GEmo-CLAP, a kind\nof gender-attribute-enhanced contrastive language-audio pretraining (CLAP)\nmethod for SER. Specifically, we first construct an effective emotion CLAP\n(Emo-CLAP) for SER, using pre-trained text and audio encoders. Second, given\nthe significance of gender information in SER, two novel multi-task learning\nbased GEmo-CLAP (ML-GEmo-CLAP) and soft label based GEmo-CLAP (SL-GEmo-CLAP)\nmodels are further proposed to incorporate gender information of speech\nsignals, forming more reasonable objectives. Experiments on IEMOCAP indicate\nthat our proposed two GEmo-CLAPs consistently outperform Emo-CLAP with\ndifferent pre-trained models. Remarkably, the proposed WavLM-based SL-GEmo-CLAP\nobtains the best WAR of 83.16\\%, which performs better than state-of-the-art\nSER methods.", "published": "2023-06-13 15:28:10", "link": "http://arxiv.org/abs/2306.07848v10", "categories": ["cs.CL", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ReadProbe: A Demo of Retrieval-Enhanced Large Language Models to Support\n  Lateral Reading", "abstract": "With the rapid growth and spread of online misinformation, people need tools\nto help them evaluate the credibility and accuracy of online information.\nLateral reading, a strategy that involves cross-referencing information with\nmultiple sources, may be an effective approach to achieving this goal. In this\npaper, we present ReadProbe, a tool to support lateral reading, powered by\ngenerative large language models from OpenAI and the Bing search engine. Our\ntool is able to generate useful questions for lateral reading, scour the web\nfor relevant documents, and generate well-attributed answers to help people\nbetter evaluate online information. We made a web-based application to\ndemonstrate how ReadProbe can help reduce the risk of being misled by false\ninformation. The code is available at\nhttps://github.com/DakeZhang1998/ReadProbe. An earlier version of our tool won\nthe first prize in a national AI misinformation hackathon.", "published": "2023-06-13 16:10:10", "link": "http://arxiv.org/abs/2306.07875v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.IR"}
{"title": "BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory\n  Information", "abstract": "Automated reasoning with unstructured natural text is a key requirement for\nmany potential applications of NLP and for developing robust AI systems.\nRecently, Language Models (LMs) have demonstrated complex reasoning capacities\neven without any finetuning. However, existing evaluation for automated\nreasoning assumes access to a consistent and coherent set of information over\nwhich models reason. When reasoning in the real-world, the available\ninformation is frequently inconsistent or contradictory, and therefore models\nneed to be equipped with a strategy to resolve such conflicts when they arise.\nOne widely-applicable way of resolving conflicts is to impose preferences over\ninformation sources (e.g., based on source credibility or information recency)\nand adopt the source with higher preference. In this paper, we formulate the\nproblem of reasoning with contradictory information guided by preferences over\nsources as the classical problem of defeasible reasoning, and develop a dataset\ncalled BoardgameQA for measuring the reasoning capacity of LMs in this setting.\nBoardgameQA also incorporates reasoning with implicit background knowledge, to\nbetter reflect reasoning problems in downstream applications. We benchmark\nvarious LMs on BoardgameQA and the results reveal a significant gap in the\nreasoning capacity of state-of-the-art LMs on this problem, showing that\nreasoning with conflicting information does not surface out-of-the-box in LMs.\nWhile performance can be improved with finetuning, it nevertheless remains\npoor.", "published": "2023-06-13 17:39:20", "link": "http://arxiv.org/abs/2306.07934v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MOFI: Learning Image Representations from Noisy Entity Annotated Images", "abstract": "We present MOFI, Manifold OF Images, a new vision foundation model designed\nto learn image representations from noisy entity annotated images. MOFI differs\nfrom previous work in two key aspects: (i) pre-training data, and (ii) training\nrecipe. Regarding data, we introduce a new approach to automatically assign\nentity labels to images from noisy image-text pairs. Our approach involves\nemploying a named entity recognition model to extract entities from the\nalt-text, and then using a CLIP model to select the correct entities as labels\nof the paired image. It's a simple, cost-effective method that can scale to\nhandle billions of web-mined image-text pairs. Through this method, we have\ncreated Image-to-Entities (I2E), a new dataset with 1 billion images and 2\nmillion distinct entities, covering rich visual concepts in the wild. Building\nupon the I2E dataset, we study different training recipes like supervised\npre-training, contrastive pre-training, and multi-task learning. For\ncontrastive pre-training, we treat entity names as free-form text, and further\nenrich them with entity descriptions. Experiments show that supervised\npre-training with large-scale fine-grained entity labels is highly effective\nfor image retrieval tasks, and multi-task training further improves the\nperformance. The final MOFI model achieves 86.66% mAP on the challenging\nGPR1200 dataset, surpassing the previous state-of-the-art performance of 72.19%\nfrom OpenAI's CLIP model. Further experiments on zero-shot and linear probe\nimage classification also show that MOFI outperforms a CLIP model trained on\nthe original image-text data, demonstrating the effectiveness of the I2E\ndataset in learning strong image representations. We release our code and model\nweights at https://github.com/apple/ml-mofi.", "published": "2023-06-13 17:51:18", "link": "http://arxiv.org/abs/2306.07952v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A Novel Scheme to classify Read and Spontaneous Speech", "abstract": "The COVID-19 pandemic has led to an increased use of remote telephonic\ninterviews, making it important to distinguish between scripted and spontaneous\nspeech in audio recordings. In this paper, we propose a novel scheme for\nidentifying read and spontaneous speech. Our approach uses a pre-trained\nDeepSpeech audio-to-alphabet recognition engine to generate a sequence of\nalphabets from the audio. From these alphabets, we derive features that allow\nus to discriminate between read and spontaneous speech. Our experimental\nresults show that even a small set of self-explanatory features can effectively\nclassify the two types of speech very effectively.", "published": "2023-06-13 11:16:52", "link": "http://arxiv.org/abs/2306.08012v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CipherSniffer: Classifying Cipher Types", "abstract": "Ciphers are a powerful tool for encrypting communication. There are many\ndifferent cipher types, which makes it computationally expensive to solve a\ncipher using brute force. In this paper, we frame the decryption task as a\nclassification problem. We first create a dataset of transpositions,\nsubstitutions, text reversals, word reversals, sentence shifts, and unencrypted\ntext. Then, we evaluate the performance of various tokenizer-model combinations\non this task.", "published": "2023-06-13 20:18:24", "link": "http://arxiv.org/abs/2306.08116v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Beyond Black Box AI-Generated Plagiarism Detection: From Sentence to\n  Document Level", "abstract": "The increasing reliance on large language models (LLMs) in academic writing\nhas led to a rise in plagiarism. Existing AI-generated text classifiers have\nlimited accuracy and often produce false positives. We propose a novel approach\nusing natural language processing (NLP) techniques, offering quantifiable\nmetrics at both sentence and document levels for easier interpretation by human\nevaluators. Our method employs a multi-faceted approach, generating multiple\nparaphrased versions of a given question and inputting them into the LLM to\ngenerate answers. By using a contrastive loss function based on cosine\nsimilarity, we match generated sentences with those from the student's\nresponse. Our approach achieves up to 94% accuracy in classifying human and AI\ntext, providing a robust and adaptable solution for plagiarism detection in\nacademic settings. This method improves with LLM advancements, reducing the\nneed for new model training or reconfiguration, and offers a more transparent\nway of evaluating and detecting AI-generated text.", "published": "2023-06-13 20:34:55", "link": "http://arxiv.org/abs/2306.08122v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AVIS: Autonomous Visual Information Seeking with Large Language Model\n  Agent", "abstract": "In this paper, we propose an autonomous information seeking visual question\nanswering framework, AVIS. Our method leverages a Large Language Model (LLM) to\ndynamically strategize the utilization of external tools and to investigate\ntheir outputs, thereby acquiring the indispensable knowledge needed to provide\nanswers to the posed questions. Responding to visual questions that necessitate\nexternal knowledge, such as \"What event is commemorated by the building\ndepicted in this image?\", is a complex task. This task presents a combinatorial\nsearch space that demands a sequence of actions, including invoking APIs,\nanalyzing their responses, and making informed decisions. We conduct a user\nstudy to collect a variety of instances of human decision-making when faced\nwith this task. This data is then used to design a system comprised of three\ncomponents: an LLM-powered planner that dynamically determines which tool to\nuse next, an LLM-powered reasoner that analyzes and extracts key information\nfrom the tool outputs, and a working memory component that retains the acquired\ninformation throughout the process. The collected user behavior serves as a\nguide for our system in two key ways. First, we create a transition graph by\nanalyzing the sequence of decisions made by users. This graph delineates\ndistinct states and confines the set of actions available at each state.\nSecond, we use examples of user decision-making to provide our LLM-powered\nplanner and reasoner with relevant contextual instances, enhancing their\ncapacity to make informed decisions. We show that AVIS achieves\nstate-of-the-art results on knowledge-intensive visual question answering\nbenchmarks such as Infoseek and OK-VQA.", "published": "2023-06-13 20:50:22", "link": "http://arxiv.org/abs/2306.08129v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Sociodemographic Bias in Language Models: A Survey and Forward Path", "abstract": "Sociodemographic bias in language models (LMs) has the potential for harm\nwhen deployed in real-world settings. This paper presents a comprehensive\nsurvey of the past decade of research on sociodemographic bias in LMs,\norganized into a typology that facilitates examining the different aims: types\nof bias, quantifying bias, and debiasing techniques. We track the evolution of\nthe latter two questions, then identify current trends and their limitations,\nas well as emerging techniques. To guide future research towards more effective\nand reliable solutions, and to help authors situate their work within this\nbroad landscape, we conclude with a checklist of open questions.", "published": "2023-06-13 22:07:54", "link": "http://arxiv.org/abs/2306.08158v5", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "INT2.1: Towards Fine-Tunable Quantized Large Language Models with Error\n  Correction through Low-Rank Adaptation", "abstract": "We introduce a method that dramatically reduces fine-tuning VRAM requirements\nand rectifies quantization errors in quantized Large Language Models. First, we\ndevelop an extremely memory-efficient fine-tuning (EMEF) method for quantized\nmodels using Low-Rank Adaptation (LoRA), and drawing upon it, we construct an\nerror-correcting algorithm designed to minimize errors induced by the\nquantization process. Our method reduces the memory requirements by up to 5.6\ntimes, which enables fine-tuning a 7 billion parameter Large Language Model\n(LLM) on consumer laptops. At the same time, we propose a Low-Rank Error\nCorrection (LREC) method that exploits the added LoRA layers to ameliorate the\ngap between the quantized model and its float point counterpart. Our error\ncorrection framework leads to a fully functional INT2 quantized LLM with the\ncapacity to generate coherent English text. To the best of our knowledge, this\nis the first INT2 Large Language Model that has been able to reach such a\nperformance. The overhead of our method is merely a 1.05 times increase in\nmodel size, which translates to an effective precision of INT2.1. Also, our\nmethod readily generalizes to other quantization standards, such as INT3, INT4,\nand INT8, restoring their lost performance, which marks a significant milestone\nin the field of model quantization. The strategies delineated in this paper\nhold promising implications for the future development and optimization of\nquantized models, marking a pivotal shift in the landscape of low-resource\nmachine learning computations.", "published": "2023-06-13 22:25:35", "link": "http://arxiv.org/abs/2306.08162v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Detect Depression from Social Networks with Sentiment Knowledge Sharing", "abstract": "Social network plays an important role in propagating people's viewpoints,\nemotions, thoughts, and fears. Notably, following lockdown periods during the\nCOVID-19 pandemic, the issue of depression has garnered increasing attention,\nwith a significant portion of individuals resorting to social networks as an\noutlet for expressing emotions. Using deep learning techniques to discern\npotential signs of depression from social network messages facilitates the\nearly identification of mental health conditions. Current efforts in detecting\ndepression through social networks typically rely solely on analyzing the\ntextual content, overlooking other potential information. In this work, we\nconduct a thorough investigation that unveils a strong correlation between\ndepression and negative emotional states. The integration of such associations\nas external knowledge can provide valuable insights for detecting depression.\nAccordingly, we propose a multi-task training framework, DeSK, which utilizes\nshared sentiment knowledge to enhance the efficacy of depression detection.\nExperiments conducted on both Chinese and English datasets demonstrate the\ncross-lingual effectiveness of DeSK.", "published": "2023-06-13 05:16:18", "link": "http://arxiv.org/abs/2306.14903v1", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion\n  and Adversarial Training with Large Speech Language Models", "abstract": "In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that\nleverages style diffusion and adversarial training with large speech language\nmodels (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its\npredecessor by modeling styles as a latent random variable through diffusion\nmodels to generate the most suitable style for the text without requiring\nreference speech, achieving efficient latent diffusion while benefiting from\nthe diverse speech synthesis offered by diffusion models. Furthermore, we\nemploy large pre-trained SLMs, such as WavLM, as discriminators with our novel\ndifferentiable duration modeling for end-to-end training, resulting in improved\nspeech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker\nLJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by\nnative English speakers. Moreover, when trained on the LibriTTS dataset, our\nmodel outperforms previous publicly available models for zero-shot speaker\nadaptation. This work achieves the first human-level TTS on both single and\nmultispeaker datasets, showcasing the potential of style diffusion and\nadversarial training with large SLMs. The audio demos and source code are\navailable at https://styletts2.github.io/.", "published": "2023-06-13 11:04:43", "link": "http://arxiv.org/abs/2306.07691v2", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Zero-Shot Detection of Low Prevalence Chest Pathologies using\n  Domain Pre-trained Language Models", "abstract": "Recent advances in zero-shot learning have enabled the use of paired\nimage-text data to replace structured labels, replacing the need for expert\nannotated datasets. Models such as CLIP-based CheXzero utilize these\nadvancements in the domain of chest X-ray interpretation. We hypothesize that\ndomain pre-trained models such as CXR-BERT, BlueBERT, and ClinicalBERT offer\nthe potential to improve the performance of CLIP-like models with specific\ndomain knowledge by replacing BERT weights at the cost of breaking the original\nmodel's alignment. We evaluate the performance of zero-shot classification\nmodels with domain-specific pre-training for detecting low-prevalence\npathologies. Even though replacing the weights of the original CLIP-BERT\ndegrades model performance on commonly found pathologies, we show that\npre-trained text towers perform exceptionally better on low-prevalence\ndiseases. This motivates future ensemble models with a combination of\ndifferently trained language models for maximal performance.", "published": "2023-06-13 06:26:54", "link": "http://arxiv.org/abs/2306.08000v1", "categories": ["physics.med-ph", "cs.CL", "cs.CV", "cs.LG", "eess.IV"], "primary_category": "physics.med-ph"}
{"title": "h2oGPT: Democratizing Large Language Models", "abstract": "Applications built on top of Large Language Models (LLMs) such as GPT-4\nrepresent a revolution in AI due to their human-level capabilities in natural\nlanguage processing. However, they also pose many significant risks such as the\npresence of biased, private, or harmful text, and the unauthorized inclusion of\ncopyrighted material.\n  We introduce h2oGPT, a suite of open-source code repositories for the\ncreation and use of LLMs based on Generative Pretrained Transformers (GPTs).\nThe goal of this project is to create the world's best truly open-source\nalternative to closed-source approaches. In collaboration with and as part of\nthe incredible and unstoppable open-source community, we open-source several\nfine-tuned h2oGPT models from 7 to 40 Billion parameters, ready for commercial\nuse under fully permissive Apache 2.0 licenses. Included in our release is\n100\\% private document search using natural language.\n  Open-source language models help boost AI development and make it more\naccessible and trustworthy. They lower entry hurdles, allowing people and\ngroups to tailor these models to their needs. This openness increases\ninnovation, transparency, and fairness. An open-source strategy is needed to\nshare AI benefits fairly, and H2O.ai will continue to democratize AI and LLMs.", "published": "2023-06-13 22:19:53", "link": "http://arxiv.org/abs/2306.08161v2", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for\n  Large Language Models", "abstract": "Large Language Models (LLMs), with their remarkable task-handling\ncapabilities and innovative outputs, have catalyzed significant advancements\nacross a spectrum of fields. However, their proficiency within specialized\ndomains such as biomolecular studies remains limited. To address this\nchallenge, we introduce Mol-Instructions, a comprehensive instruction dataset\ndesigned for the biomolecular domain. Mol-Instructions encompasses three key\ncomponents: molecule-oriented instructions, protein-oriented instructions, and\nbiomolecular text instructions. Each component aims to improve the\nunderstanding and prediction capabilities of LLMs concerning biomolecular\nfeatures and behaviors. Through extensive instruction tuning experiments on\nLLMs, we demonstrate the effectiveness of Mol-Instructions in enhancing large\nmodels' performance in the intricate realm of biomolecular studies, thus\nfostering progress in the biomolecular research community. Mol-Instructions is\npublicly available for ongoing research and will undergo regular updates to\nenhance its applicability.", "published": "2023-06-13 14:35:34", "link": "http://arxiv.org/abs/2306.08018v5", "categories": ["q-bio.QM", "cs.AI", "cs.CE", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "q-bio.QM"}
{"title": "Cognitive performance in open-plan office acoustic simulations: Effects\n  of room acoustics and semantics but not spatial separation of sound sources", "abstract": "The irrelevant sound effect (ISE) characterizes short-term memory performance\nimpairment during irrelevant sounds relative to quiet. Irrelevant sound\npresentation in most laboratory-based ISE studies has been rather limited to\nrepresent complex scenarios including open-plan offices (OPOs) and not many\nstudies have considered serial recall of heard information. This paper\ninvestigates ISE using an auditory-verbal serial recall task, wherein\nperformance was evaluated for relevant factors in simulating OPO acoustics: the\nirrelevant sounds including the semanticity of speech, reproduction methods\nover headphones, and room acoustics. Results (Experiments 1 and 2) show that\nISE was exhibited in most conditions with anechoic (irrelevant) nonspeech\nsounds with/without speech, but the effect was substantially higher with\nmeaningful speech compared to foreign speech, suggesting a semantic effect.\nPerformance differences in conditions with diotic and binaural reproductions\nwere not statistically robust, suggesting limited role of spatial separation of\nsources. In Experiment 3, statistically robust ISE were exhibited for binaural\nroom acoustic conditions with mid-frequency reverberation times, T30 (s) = 0.4,\n0.8, 1.1, suggesting cognitive impairment regardless of sound absorption\nrepresentative of OPOs. Performance differences in T30 = 0.4 s relative to T30\n= 0.8 and 1.1 s conditions were statistically robust. This emphasizes the\nbenefits for cognitive performance with increased sound absorption, reinforcing\nextant room acoustic design recommendations. Performance differences in T30 =\n0.8 s vs. 1.1 s were not statistically robust. Collectively, these results\nsuggest that certain findings from ISE studies with idiosyncratic acoustics may\nnot translate well to complex OPO acoustic environments.", "published": "2023-06-13 18:17:12", "link": "http://arxiv.org/abs/2306.08051v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Speaker Verification Across Ages: Investigating Deep Speaker Embedding\n  Sensitivity to Age Mismatch in Enrollment and Test Speech", "abstract": "In this paper, we study the impact of the ageing on modern deep speaker\nembedding based automatic speaker verification (ASV) systems. We have selected\ntwo different datasets to examine ageing on the state-of-the-art ECAPA-TDNN\nsystem. The first dataset, used for addressing short-term ageing (up to 10\nyears time difference between enrollment and test) under uncontrolled\nconditions, is VoxCeleb. The second dataset, used for addressing long-term\nageing effect (up to 40 years difference) of Finnish speakers under a more\ncontrolled setup, is Longitudinal Corpus of Finnish Spoken in Helsinki (LCFSH).\nOur study provides new insights into the impact of speaker ageing on modern ASV\nsystems. Specifically, we establish a quantitative measure between ageing and\nASV scores. Further, our research indicates that ageing affects female English\nspeakers to a greater degree than male English speakers, while in the case of\nFinnish, it has a greater impact on male speakers than female speakers.", "published": "2023-06-13 02:23:55", "link": "http://arxiv.org/abs/2306.07501v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "UniCATS: A Unified Context-Aware Text-to-Speech Framework with\n  Contextual VQ-Diffusion and Vocoding", "abstract": "The utilization of discrete speech tokens, divided into semantic tokens and\nacoustic tokens, has been proven superior to traditional acoustic feature\nmel-spectrograms in terms of naturalness and robustness for text-to-speech\n(TTS) synthesis. Recent popular models, such as VALL-E and SPEAR-TTS, allow\nzero-shot speaker adaptation through auto-regressive (AR) continuation of\nacoustic tokens extracted from a short speech prompt. However, these AR models\nare restricted to generate speech only in a left-to-right direction, making\nthem unsuitable for speech editing where both preceding and following contexts\nare provided. Furthermore, these models rely on acoustic tokens, which have\naudio quality limitations imposed by the performance of audio codec models. In\nthis study, we propose a unified context-aware TTS framework called UniCATS,\nwhich is capable of both speech continuation and editing. UniCATS comprises two\ncomponents, an acoustic model CTX-txt2vec and a vocoder CTX-vec2wav.\nCTX-txt2vec employs contextual VQ-diffusion to predict semantic tokens from the\ninput text, enabling it to incorporate the semantic context and maintain\nseamless concatenation with the surrounding context. Following that,\nCTX-vec2wav utilizes contextual vocoding to convert these semantic tokens into\nwaveforms, taking into consideration the acoustic context. Our experimental\nresults demonstrate that CTX-vec2wav outperforms HifiGAN and AudioLM in terms\nof speech resynthesis from semantic tokens. Moreover, we show that UniCATS\nachieves state-of-the-art performance in both speech continuation and editing.", "published": "2023-06-13 05:38:34", "link": "http://arxiv.org/abs/2306.07547v6", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Statistical Beamformer Exploiting Non-stationarity and Sparsity with\n  Spatially Constrained ICA for Robust Speech Recognition", "abstract": "In this paper, we present a statistical beamforming algorithm as a\npre-processing step for robust automatic speech recognition (ASR). By modeling\nthe target speech as a non-stationary Laplacian distribution, a mask-based\nstatistical beamforming algorithm is proposed to exploit both its output and\nmasked input variance for robust estimation of the beamformer. In addition, we\nalso present a method for steering vector estimation (SVE) based on a noise\npower ratio obtained from the target and noise outputs in independent component\nanalysis (ICA). To update the beamformer in the same ICA framework, we derive\nICA with distortionless and null constraints on target speech, which yields\nbeamformed speech at the target output and noises at the other outputs,\nrespectively. The demixing weights for the target output result in a\nstatistical beamformer with the weighted spatial covariance matrix (wSCM) using\na weighting function characterized by a source model. To enhance the SVE, the\nstrict null constraints imposed by the Lagrange multiplier methods are relaxed\nby generalized penalties with weight parameters, while the strict\ndistortionless constraints are maintained. Furthermore, we derive an online\nalgorithm based on an optimization technique of recursive least squares (RLS)\nfor practical applications. Experimental results on various environments using\nCHiME-4 and LibriCSS datasets demonstrate the effectiveness of the presented\nalgorithm compared to conventional beamforming and blind source extraction\n(BSE) based on ICA on both batch and online processing.", "published": "2023-06-13 06:21:59", "link": "http://arxiv.org/abs/2306.07562v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unlocking Foundation Models for Privacy-Enhancing Speech Understanding:\n  An Early Study on Low Resource Speech Training Leveraging Label-guided\n  Synthetic Speech Content", "abstract": "Automatic Speech Understanding (ASU) leverages the power of deep learning\nmodels for accurate interpretation of human speech, leading to a wide range of\nspeech applications that enrich the human experience. However, training a\nrobust ASU model requires the curation of a large number of speech samples,\ncreating risks for privacy breaches. In this work, we investigate using\nfoundation models to assist privacy-enhancing speech computing. Unlike\nconventional works focusing primarily on data perturbation or distributed\nalgorithms, our work studies the possibilities of using pre-trained generative\nmodels to synthesize speech content as training data with just label guidance.\nWe show that zero-shot learning with training label-guided synthetic speech\ncontent remains a challenging task. On the other hand, our results demonstrate\nthat the model trained with synthetic speech samples provides an effective\ninitialization point for low-resource ASU training. This result reveals the\npotential to enhance privacy by reducing user data collection but using\nlabel-guided synthetic speech content.", "published": "2023-06-13 14:13:08", "link": "http://arxiv.org/abs/2306.07791v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Quantifying Spatial Audio Quality Impairment", "abstract": "Spatial audio quality is a highly multifaceted concept, with many\ninteractions between environmental, geometrical, anatomical, psychological, and\ncontextual considerations. Methods for characterization or evaluation of the\ngeometrical components of spatial audio quality, however, remain scarce,\ndespite being perhaps the least subjective aspect of spatial audio quality to\nquantify. By considering interchannel time and level differences relative to a\nreference signal, it is possible to construct a signal model to isolate some of\nthe spatial distortion. By using a combination of least-square optimization and\nheuristics, we propose a signal decomposition method to isolate the spatial\nerror from a processed signal, in terms of interchannel gain leakages and\nchanges in relative delays. This allows the computation of simple energy-ratio\nmetrics, providing objective measures of spatial and non-spatial signal\nqualities, with minimal assumptions and no dataset dependency. Experiments\ndemonstrate the robustness of the method against common spatial signal\ndegradation introduced by, e.g., audio compression and music source separation.\nImplementation is available at https://github.com/karnwatcharasupat/spauq.", "published": "2023-06-13 18:18:09", "link": "http://arxiv.org/abs/2306.08053v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Efficient Adapters for Giant Speech Models", "abstract": "Large pre-trained speech models are widely used as the de-facto paradigm,\nespecially in scenarios when there is a limited amount of labeled data\navailable. However, finetuning all parameters from the self-supervised learned\nmodel can be computationally expensive, and becomes infeasiable as the size of\nthe model and the number of downstream tasks scales. In this paper, we propose\na novel approach called Two Parallel Adapter (TPA) that is inserted into the\nconformer-based model pre-trained model instead. TPA is based on systematic\nstudies of the residual adapter, a popular approach for finetuning a subset of\nparameters. We evaluate TPA on various public benchmarks and experiment results\ndemonstrates its superior performance, which is close to the full finetuning on\ndifferent datasets and speech tasks. These results show that TPA is an\neffective and efficient approach for serving large pre-trained speech models.\nAblation studies show that TPA can also be pruned, especially for lower blocks.", "published": "2023-06-13 20:51:00", "link": "http://arxiv.org/abs/2306.08131v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "PauseSpeech: Natural Speech Synthesis via Pre-trained Language Model and\n  Pause-based Prosody Modeling", "abstract": "Although text-to-speech (TTS) systems have significantly improved, most TTS\nsystems still have limitations in synthesizing speech with appropriate\nphrasing. For natural speech synthesis, it is important to synthesize the\nspeech with a phrasing structure that groups words into phrases based on\nsemantic information. In this paper, we propose PuaseSpeech, a speech synthesis\nsystem with a pre-trained language model and pause-based prosody modeling.\nFirst, we introduce a phrasing structure encoder that utilizes a context\nrepresentation from the pre-trained language model. In the phrasing structure\nencoder, we extract a speaker-dependent syntactic representation from the\ncontext representation and then predict a pause sequence that separates the\ninput text into phrases. Furthermore, we introduce a pause-based word encoder\nto model word-level prosody based on pause sequence. Experimental results show\nPauseSpeech outperforms previous models in terms of naturalness. Furthermore,\nin terms of objective evaluations, we can observe that our proposed methods\nhelp the model decrease the distance between ground-truth and synthesized\nspeech. Audio samples are available at\nhttps://jisang93.github.io/pausespeech-demo/.", "published": "2023-06-13 01:36:55", "link": "http://arxiv.org/abs/2306.07489v1", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Malafide: a novel adversarial convolutive noise attack against deepfake\n  and spoofing detection systems", "abstract": "We present Malafide, a universal adversarial attack against automatic speaker\nverification (ASV) spoofing countermeasures (CMs). By introducing convolutional\nnoise using an optimised linear time-invariant filter, Malafide attacks can be\nused to compromise CM reliability while preserving other speech attributes such\nas quality and the speaker's voice. In contrast to other adversarial attacks\nproposed recently, Malafide filters are optimised independently of the input\nutterance and duration, are tuned instead to the underlying spoofing attack,\nand require the optimisation of only a small number of filter coefficients.\nEven so, they degrade CM performance estimates by an order of magnitude, even\nin black-box settings, and can also be configured to overcome integrated CM and\nASV subsystems. Integrated solutions that use self-supervised learning CMs,\nhowever, are more robust, under both black-box and white-box settings.", "published": "2023-06-13 09:52:44", "link": "http://arxiv.org/abs/2306.07655v1", "categories": ["eess.AS", "cs.CR", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Contrastive Learning-Based Audio to Lyrics Alignment for Multiple\n  Languages", "abstract": "Lyrics alignment gained considerable attention in recent years.\nState-of-the-art systems either re-use established speech recognition toolkits,\nor design end-to-end solutions involving a Connectionist Temporal\nClassification (CTC) loss. However, both approaches suffer from specific\nweaknesses: toolkits are known for their complexity, and CTC systems use a loss\ndesigned for transcription which can limit alignment accuracy. In this paper,\nwe use instead a contrastive learning procedure that derives cross-modal\nembeddings linking the audio and text domains. This way, we obtain a novel\nsystem that is simple to train end-to-end, can make use of weakly annotated\ntraining data, jointly learns a powerful text model, and is tailored to\nalignment. The system is not only the first to yield an average absolute error\nbelow 0.2 seconds on the standard Jamendo dataset but it is also robust to\nother languages, even when trained on English data only. Finally, we release\nword-level alignments for the JamendoLyrics Multi-Lang dataset.", "published": "2023-06-13 13:01:02", "link": "http://arxiv.org/abs/2306.07744v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Unsupervised speech enhancement with deep dynamical generative speech\n  and noise models", "abstract": "This work builds on a previous work on unsupervised speech enhancement using\na dynamical variational autoencoder (DVAE) as the clean speech model and\nnon-negative matrix factorization (NMF) as the noise model. We propose to\nreplace the NMF noise model with a deep dynamical generative model (DDGM)\ndepending either on the DVAE latent variables, or on the noisy observations, or\non both. This DDGM can be trained in three configurations: noise-agnostic,\nnoise-dependent and noise adaptation after noise-dependent training.\nExperimental results show that the proposed method achieves competitive\nperformance compared to state-of-the-art unsupervised speech enhancement\nmethods, while the noise-dependent training configuration yields a much more\ntime-efficient inference process.", "published": "2023-06-13 14:52:35", "link": "http://arxiv.org/abs/2306.07820v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Domain Information Control at Inference Time for Acoustic Scene\n  Classification", "abstract": "Domain shift is considered a challenge in machine learning as it causes\nsignificant degradation of model performance. In the Acoustic Scene\nClassification task (ASC), domain shift is mainly caused by different recording\ndevices. Several studies have already targeted domain generalization to improve\nthe performance of ASC models on unseen domains, such as new devices. Recently,\nthe Controllable Gate Adapter ConGater has been proposed in Natural Language\nProcessing to address the biased training data problem. ConGater allows\ncontrolling the debiasing process at inference time. ConGater's main advantage\nis the continuous and selective debiasing of a trained model, during inference.\nIn this work, we adapt ConGater to the audio spectrogram transformer for an\nacoustic scene classification task. We show that ConGater can be used to\nselectively adapt the learned representations to be invariant to device domain\nshifts such as recording devices. Our analysis shows that ConGater can\nprogressively remove device information from the learned representations and\nimprove the model generalization, especially under domain shift conditions\n(e.g. unseen devices). We show that information removal can be extended to both\ndevice and location domain. Finally, we demonstrate ConGater's ability to\nenhance specific device performance without further training.", "published": "2023-06-13 10:58:05", "link": "http://arxiv.org/abs/2306.08010v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DCTX-Conformer: Dynamic context carry-over for low latency unified\n  streaming and non-streaming Conformer ASR", "abstract": "Conformer-based end-to-end models have become ubiquitous these days and are\ncommonly used in both streaming and non-streaming automatic speech recognition\n(ASR). Techniques like dual-mode and dynamic chunk training helped unify\nstreaming and non-streaming systems. However, there remains a performance gap\nbetween streaming with a full and limited past context. To address this issue,\nwe propose the integration of a novel dynamic contextual carry-over mechanism\nin a state-of-the-art (SOTA) unified ASR system. Our proposed dynamic context\nConformer (DCTX-Conformer) utilizes a non-overlapping contextual carry-over\nmechanism that takes into account both the left context of a chunk and one or\nmore preceding context embeddings. We outperform the SOTA by a relative 25.0%\nword error rate, with a negligible latency impact due to the additional context\nembeddings.", "published": "2023-06-13 23:42:53", "link": "http://arxiv.org/abs/2306.08175v2", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
