{"title": "Semantic Specialization for Knowledge-based Word Sense Disambiguation", "abstract": "A promising approach for knowledge-based Word Sense Disambiguation (WSD) is\nto select the sense whose contextualized embeddings computed for its definition\nsentence are closest to those computed for a target word in a given sentence.\nThis approach relies on the similarity of the \\textit{sense} and\n\\textit{context} embeddings computed by a pre-trained language model. We\npropose a semantic specialization for WSD where contextualized embeddings are\nadapted to the WSD task using solely lexical knowledge. The key idea is, for a\ngiven sense, to bring semantically related senses and contexts closer and send\ndifferent/unrelated senses farther away. We realize this idea as the joint\noptimization of the Attract-Repel objective for sense pairs and the\nself-training objective for context-sense pairs while controlling deviations\nfrom the original embeddings. The proposed method outperformed previous studies\nthat adapt contextualized embeddings. It achieved state-of-the-art performance\non knowledge-based WSD when combined with the reranking heuristic that uses the\nsense inventory. We found that the similarity characteristics of specialized\nembeddings conform to the key idea. We also found that the (dis)similarity of\nembeddings between the related/different/unrelated senses correlates well with\nthe performance of WSD.", "published": "2023-04-22 07:40:23", "link": "http://arxiv.org/abs/2304.11340v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Romanian Multiword Expression Detection Using Multilingual Adversarial\n  Training and Lateral Inhibition", "abstract": "Multiword expressions are a key ingredient for developing large-scale and\nlinguistically sound natural language processing technology. This paper\ndescribes our improvements in automatically identifying Romanian multiword\nexpressions on the corpus released for the PARSEME v1.2 shared task. Our\napproach assumes a multilingual perspective based on the recently introduced\nlateral inhibition layer and adversarial training to boost the performance of\nthe employed multilingual language models. With the help of these two methods,\nwe improve the F1-score of XLM-RoBERTa by approximately 2.7% on unseen\nmultiword expressions, the main task of the PARSEME 1.2 edition. In addition,\nour results can be considered SOTA performance, as they outperform the previous\nresults on Romanian obtained by the participants in this competition.", "published": "2023-04-22 09:10:49", "link": "http://arxiv.org/abs/2304.11350v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformer-Based Language Model Surprisal Predicts Human Reading Times\n  Best with About Two Billion Training Tokens", "abstract": "Recent psycholinguistic studies have drawn conflicting conclusions about the\nrelationship between the quality of a language model and the ability of its\nsurprisal estimates to predict human reading times, which has been speculated\nto be due to the large gap in both the amount of training data and model\ncapacity across studies. The current work aims to consolidate these findings by\nevaluating surprisal estimates from Transformer-based language model variants\nthat vary systematically in the amount of training data and model capacity on\ntheir ability to predict human reading times. The results show that surprisal\nestimates from most variants with contemporary model capacities provide the\nbest fit after seeing about two billion training tokens, after which they begin\nto diverge from humanlike expectations. Additionally, newly-trained smaller\nmodel variants reveal a 'tipping point' at convergence, after which the\ndecrease in language model perplexity begins to result in poorer fits to human\nreading times. These results suggest that the massive amount of training data\nis mainly responsible for the poorer fit achieved by surprisal from larger\npre-trained language models, and that a certain degree of model capacity is\nnecessary for Transformer-based language models to capture humanlike\nexpectations.", "published": "2023-04-22 12:50:49", "link": "http://arxiv.org/abs/2304.11389v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LaMP: When Large Language Models Meet Personalization", "abstract": "This paper highlights the importance of personalization in large language\nmodels and introduces the LaMP benchmark -- a novel benchmark for training and\nevaluating language models for producing personalized outputs. LaMP offers a\ncomprehensive evaluation framework with diverse language tasks and multiple\nentries for each user profile. It consists of seven personalized tasks,\nspanning three text classification and four text generation tasks. We\nadditionally propose two retrieval augmentation approaches that retrieve\npersonal items from each user profile for personalizing language model outputs.\nTo this aim, we study various retrieval models, including term matching,\nsemantic matching, and time-aware methods. Extensive experiments on LaMP for\nzero-shot and fine-tuned language models demonstrate the efficacy of the\nproposed retrieval augmentation approach and highlight the impact of\npersonalization in various natural language tasks.", "published": "2023-04-22 13:42:04", "link": "http://arxiv.org/abs/2304.11406v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dialectical language model evaluation: An initial appraisal of the\n  commonsense spatial reasoning abilities of LLMs", "abstract": "Language models have become very popular recently and many claims have been\nmade about their abilities, including for commonsense reasoning. Given the\nincreasingly better results of current language models on previous static\nbenchmarks for commonsense reasoning, we explore an alternative dialectical\nevaluation. The goal of this kind of evaluation is not to obtain an aggregate\nperformance value but to find failures and map the boundaries of the system.\nDialoguing with the system gives the opportunity to check for consistency and\nget more reassurance of these boundaries beyond anecdotal evidence. In this\npaper we conduct some qualitative investigations of this kind of evaluation for\nthe particular case of spatial reasoning (which is a fundamental aspect of\ncommonsense reasoning). We conclude with some suggestions for future work both\nto improve the capabilities of language models and to systematise this kind of\ndialectical evaluation.", "published": "2023-04-22 06:28:46", "link": "http://arxiv.org/abs/2304.11164v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SAILER: Structure-aware Pre-trained Language Model for Legal Case\n  Retrieval", "abstract": "Legal case retrieval, which aims to find relevant cases for a query case,\nplays a core role in the intelligent legal system. Despite the success that\npre-training has achieved in ad-hoc retrieval tasks, effective pre-training\nstrategies for legal case retrieval remain to be explored. Compared with\ngeneral documents, legal case documents are typically long text sequences with\nintrinsic logical structures. However, most existing language models have\ndifficulty understanding the long-distance dependencies between different\nstructures. Moreover, in contrast to the general retrieval, the relevance in\nthe legal domain is sensitive to key legal elements. Even subtle differences in\nkey legal elements can significantly affect the judgement of relevance.\nHowever, existing pre-trained language models designed for general purposes\nhave not been equipped to handle legal elements.\n  To address these issues, in this paper, we propose SAILER, a new\nStructure-Aware pre-traIned language model for LEgal case Retrieval. It is\nhighlighted in the following three aspects: (1) SAILER fully utilizes the\nstructural information contained in legal case documents and pays more\nattention to key legal elements, similar to how legal experts browse legal case\ndocuments. (2) SAILER employs an asymmetric encoder-decoder architecture to\nintegrate several different pre-training objectives. In this way, rich semantic\ninformation across tasks is encoded into dense vectors. (3) SAILER has powerful\ndiscriminative ability, even without any legal annotation data. It can\ndistinguish legal cases with different charges accurately. Extensive\nexperiments over publicly available legal benchmarks demonstrate that our\napproach can significantly outperform previous state-of-the-art methods in\nlegal case retrieval.", "published": "2023-04-22 10:47:01", "link": "http://arxiv.org/abs/2304.11370v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A bounded rationality account of dependency length minimization in Hindi", "abstract": "The principle of DEPENDENCY LENGTH MINIMIZATION, which seeks to keep\nsyntactically related words close in a sentence, is thought to universally\nshape the structure of human languages for effective communication. However,\nthe extent to which dependency length minimization is applied in human language\nsystems is not yet fully understood. Preverbally, the placement of\nlong-before-short constituents and postverbally, short-before-long constituents\nare known to minimize overall dependency length of a sentence. In this study,\nwe test the hypothesis that placing only the shortest preverbal constituent\nnext to the main-verb explains word order preferences in Hindi (a SOV language)\nas opposed to the global minimization of dependency length. We characterize\nthis approach as a least-effort strategy because it is a cost-effective way to\nshorten all dependencies between the verb and its preverbal dependencies. As\nsuch, this approach is consistent with the bounded-rationality perspective\naccording to which decision making is governed by \"fast but frugal\" heuristics\nrather than by a search for optimal solutions. Consistent with this idea, our\nresults indicate that actual corpus sentences in the Hindi-Urdu Treebank corpus\nare better explained by the least effort strategy than by global minimization\nof dependency lengths. Additionally, for the task of distinguishing corpus\nsentences from counterfactual variants, we find that the dependency length and\nconstituent length of the constituent closest to the main verb are much better\npredictors of whether a sentence appeared in the corpus than total dependency\nlength. Overall, our findings suggest that cognitive resource constraints play\na crucial role in shaping natural languages.", "published": "2023-04-22 13:53:50", "link": "http://arxiv.org/abs/2304.11410v1", "categories": ["cs.CL", "econ.TH"], "primary_category": "cs.CL"}
{"title": "L3Cube-IndicSBERT: A simple approach for learning cross-lingual sentence\n  representations using multilingual BERT", "abstract": "The multilingual Sentence-BERT (SBERT) models map different languages to\ncommon representation space and are useful for cross-language similarity and\nmining tasks. We propose a simple yet effective approach to convert vanilla\nmultilingual BERT models into multilingual sentence BERT models using synthetic\ncorpus. We simply aggregate translated NLI or STS datasets of the low-resource\ntarget languages together and perform SBERT-like fine-tuning of the vanilla\nmultilingual BERT model. We show that multilingual BERT models are inherent\ncross-lingual learners and this simple baseline fine-tuning approach without\nexplicit cross-lingual training yields exceptional cross-lingual properties. We\nshow the efficacy of our approach on 10 major Indic languages and also show the\napplicability of our approach to non-Indic languages German and French. Using\nthis approach, we further present L3Cube-IndicSBERT, the first multilingual\nsentence representation model specifically for Indian languages Hindi, Marathi,\nKannada, Telugu, Malayalam, Tamil, Gujarati, Odia, Bengali, and Punjabi. The\nIndicSBERT exhibits strong cross-lingual capabilities and performs\nsignificantly better than alternatives like LaBSE, LASER, and\nparaphrase-multilingual-mpnet-base-v2 on Indic cross-lingual and monolingual\nsentence similarity tasks. We also release monolingual SBERT models for each of\nthe languages and show that IndicSBERT performs competitively with its\nmonolingual counterparts. These models have been evaluated using embedding\nsimilarity scores and classification accuracy.", "published": "2023-04-22 15:45:40", "link": "http://arxiv.org/abs/2304.11434v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Boosting Theory-of-Mind Performance in Large Language Models via\n  Prompting", "abstract": "Large language models (LLMs) excel in many tasks in 2023, but they still face\nchallenges in complex reasoning. Theory-of-mind (ToM) tasks, which require\nunderstanding agents' beliefs, goals, and mental states, are essential for\ncommon-sense reasoning involving humans, making it crucial to enhance LLM\nperformance in this area. This study measures the ToM performance of GPT-4 and\nthree GPT-3.5 variants (Davinci-2, Davinci-3, GPT-3.5-Turbo), and investigates\nthe effectiveness of in-context learning in improving their ToM comprehension.\nWe evaluated prompts featuring two-shot chain of thought reasoning and\nstep-by-step thinking instructions. We found that LLMs trained with\nReinforcement Learning from Human Feedback (RLHF) (all models excluding\nDavinci-2) improved their ToM accuracy via in-context learning. GPT-4 performed\nbest in zero-shot settings, reaching nearly 80% ToM accuracy, but still fell\nshort of the 87% human accuracy on the test set. However, when supplied with\nprompts for in-context learning, all RLHF-trained LLMs exceeded 80% ToM\naccuracy, with GPT-4 reaching 100%. These results demonstrate that appropriate\nprompting enhances LLM ToM reasoning, and they underscore the context-dependent\nnature of LLM cognitive capacities.", "published": "2023-04-22 22:50:50", "link": "http://arxiv.org/abs/2304.11490v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "On the Identification of the Energy related Issues from the App Reviews", "abstract": "The energy inefficiency of the apps can be a major issue for the app users\nwhich is discussed on App Stores extensively. Previous research has shown the\nimportance of investigating the energy related app reviews to identify the\nmajor causes or categories of energy related user feedback. However, there is\nno study that efficiently extracts the energy related app reviews\nautomatically. In this paper, we empirically study different techniques for\nautomatic extraction of the energy related user feedback. We compare the\naccuracy, F1-score and run time of numerous machine-learning models with\nrelevant feature combinations and relatively modern Neural Network-based\nmodels. In total, 60 machine learning models are compared to 30 models that we\nbuild using six neural network architectures and three word embedding models.\nWe develop a visualization tool for this study through which a developer can\ntraverse through this large-scale result set. The results show that neural\nnetworks outperform the other machine learning techniques and can achieve the\nhighest F1-score of 0.935. To replicate the research results, we have open\nsourced the interactive visualization tool. After identifying the best results\nand extracting the energy related reviews, we further compare various\ntechniques to help the developers automatically investigate the emerging issues\nthat might be responsible for energy inefficiency of the apps. We experiment\nthe previously used string matching with results obtained from applying two of\nthe state-of-the-art topic modeling algorithms, OBTM and AOLDA. Finally, we run\na qualitative study performed in collaboration with developers and students\nfrom different institutions to determine their preferences for identifying\nnecessary topics from previously categorized reviews, which shows OBTM produces\nthe most helpful results.", "published": "2023-04-22 01:54:30", "link": "http://arxiv.org/abs/2304.11292v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "(Vector) Space is Not the Final Frontier: Product Search as Program\n  Synthesis", "abstract": "As ecommerce continues growing, huge investments in ML and NLP for\nInformation Retrieval are following. While the vector space model dominated\nretrieval modelling in product search - even as vectorization itself greatly\nchanged with the advent of deep learning -, our position paper argues in a\ncontrarian fashion that program synthesis provides significant advantages for\nmany queries and a significant number of players in the market. We detail the\nindustry significance of the proposed approach, sketch implementation details,\nand address common objections drawing from our experience building a similar\nsystem at Tooso.", "published": "2023-04-22 20:00:06", "link": "http://arxiv.org/abs/2304.11473v2", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Understanding Lexical Biases when Identifying Gang-related Social Media\n  Communications", "abstract": "Individuals involved in gang-related activity use mainstream social media\nincluding Facebook and Twitter to express taunts and threats as well as grief\nand memorializing. However, identifying the impact of gang-related activity in\norder to serve community member needs through social media sources has a unique\nset of challenges. This includes the difficulty of ethically identifying\ntraining data of individuals impacted by gang activity and the need to account\nfor a non-standard language style commonly used in the tweets from these\nindividuals. Our study provides evidence of methods where natural language\nprocessing tools can be helpful in efficiently identifying individuals who may\nbe in need of community care resources such as counselors, conflict mediators,\nor academic/professional training programs. We demonstrate that our binary\nlogistic classifier outperforms baseline standards in identifying individuals\nimpacted by gang-related violence using a sample of gang-related tweets\nassociated with Chicago. We ultimately found that the language of a tweet is\nhighly relevant and that uses of ``big data'' methods or machine learning\nmodels need to better understand how language impacts the model's performance\nand how it discriminates among populations.", "published": "2023-04-22 21:51:49", "link": "http://arxiv.org/abs/2304.11485v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI", "J.4; K.4.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Transcending the \"Male Code\": Implicit Masculine Biases in NLP Contexts", "abstract": "Critical scholarship has elevated the problem of gender bias in data sets\nused to train virtual assistants (VAs). Most work has focused on explicit\nbiases in language, especially against women, girls, femme-identifying people,\nand genderqueer folk; implicit associations through word embeddings; and\nlimited models of gender and masculinities, especially toxic masculinities,\nconflation of sex and gender, and a sex/gender binary framing of the masculine\nas diametric to the feminine. Yet, we must also interrogate how masculinities\nare \"coded\" into language and the assumption of \"male\" as the linguistic\ndefault: implicit masculine biases. To this end, we examined two natural\nlanguage processing (NLP) data sets. We found that when gendered language was\npresent, so were gender biases and especially masculine biases. Moreover, these\nbiases related in nuanced ways to the NLP context. We offer a new dictionary\ncalled AVA that covers ambiguous associations between gendered language and the\nlanguage of VAs.", "published": "2023-04-22 03:53:24", "link": "http://arxiv.org/abs/2304.12810v1", "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "\"I'm\" Lost in Translation: Pronoun Missteps in Crowdsourced Data Sets", "abstract": "As virtual assistants continue to be taken up globally, there is an\never-greater need for these speech-based systems to communicate naturally in a\nvariety of languages. Crowdsourcing initiatives have focused on multilingual\ntranslation of big, open data sets for use in natural language processing\n(NLP). Yet, language translation is often not one-to-one, and biases can\ntrickle in. In this late-breaking work, we focus on the case of pronouns\ntranslated between English and Japanese in the crowdsourced Tatoeba database.\nWe found that masculine pronoun biases were present overall, even though\nplurality in language was accounted for in other ways. Importantly, we detected\nbiases in the translation process that reflect nuanced reactions to the\npresence of feminine, neutral, and/or non-binary pronouns. We raise the issue\nof translation bias for pronouns and offer a practical solution to embed\nplurality in NLP data sets.", "published": "2023-04-22 09:27:32", "link": "http://arxiv.org/abs/2304.13557v1", "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Recurrent Neural Networks and Long Short-Term Memory Networks: Tutorial\n  and Survey", "abstract": "This is a tutorial paper on Recurrent Neural Network (RNN), Long Short-Term\nMemory Network (LSTM), and their variants. We start with a dynamical system and\nbackpropagation through time for RNN. Then, we discuss the problems of gradient\nvanishing and explosion in long-term dependencies. We explain close-to-identity\nweight matrix, long delays, leaky units, and echo state networks for solving\nthis problem. Then, we introduce LSTM gates and cells, history and variants of\nLSTM, and Gated Recurrent Units (GRU). Finally, we introduce bidirectional RNN,\nbidirectional LSTM, and the Embeddings from Language Model (ELMo) network, for\nprocessing a sequence in both directions.", "published": "2023-04-22 18:22:10", "link": "http://arxiv.org/abs/2304.11461v1", "categories": ["cs.LG", "cs.CL", "cs.NE", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Can Voice Assistants Sound Cute? Towards a Model of Kawaii Vocalics", "abstract": "The Japanese notion of \"kawaii\" or expressions of cuteness, vulnerability,\nand/or charm is a global cultural export. Work has explored kawaii-ness as a\ndesign feature and factor of user experience in the visual appearance,\nnonverbal behaviour, and sound of robots and virtual characters. In this\ninitial work, we consider whether voices can be kawaii by exploring the vocal\nqualities of voice assistant speech, i.e., kawaii vocalics. Drawing from an\nage-inclusive model of kawaii, we ran a user perceptions study on the\nkawaii-ness of younger- and older-sounding Japanese computer voices. We found\nthat kawaii-ness intersected with perceptions of gender and age, i.e., gender\nambiguous and girlish, as well as VA features, i.e., fluency and artificiality.\nWe propose an initial model of kawaii vocalics to be validated through the\nidentification and study of vocal qualities, cognitive appraisals, behavioural\nresponses, and affective reports.", "published": "2023-04-22 01:47:19", "link": "http://arxiv.org/abs/2304.12809v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.SD"], "primary_category": "cs.HC"}
{"title": "Lightweight Toxicity Detection in Spoken Language: A Transformer-based\n  Approach for Edge Devices", "abstract": "Toxicity is a prevalent social behavior that involves the use of hate speech,\noffensive language, bullying, and abusive speech. While text-based approaches\nfor toxicity detection are common, there is limited research on processing\nspeech signals in the physical world. Detecting toxicity in the physical world\nis challenging due to the difficulty of integrating AI-capable computers into\nthe environment. We propose a lightweight transformer model based on wav2vec2.0\nand optimize it using techniques such as quantization and knowledge\ndistillation. Our model uses multitask learning and achieves an average macro\nF1-score of 90.3\\% and a weighted accuracy of 88\\%, outperforming\nstate-of-the-art methods on DeToxy-B and a public dataset. Our results show\nthat quantization reduces the model size by almost 4 times and RAM usage by\n3.3\\%, with only a 1\\% F1 score decrease. Knowledge distillation reduces the\nmodel size by 3.7 times, RAM usage by 1.9, and inference time by 2 times, but\ndecreases accuracy by 8\\%. Combining both techniques reduces the model size by\n14.6 times and RAM usage by around 4.3 times, with a two-fold inference time\nimprovement. Our compact model is the first end-to-end speech-based toxicity\ndetection model based on a lightweight transformer model suitable for\ndeployment in physical spaces. The results show its feasibility for toxicity\ndetection on edge devices in real-world environments.", "published": "2023-04-22 13:45:38", "link": "http://arxiv.org/abs/2304.11408v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Dis/Immersion in Mindfulness Meditation with a Wandering Voice Assistant", "abstract": "Mindfulness meditation is a validated means of helping people manage stress.\nVoice-based virtual assistants (VAs) in smart speakers, smartphones, and smart\nenvironments can assist people in carrying out mindfulness meditation through\nguided experiences. However, the common fixed location embodiment of VAs makes\nit difficult to provide intuitive support. In this work, we explored the novel\nembodiment of a \"wandering voice\" that is co-located with the user and \"moves\"\nwith the task. We developed a multi-speaker VA embedded in a yoga mat that\nchanges location along the body according to the meditation experience. We\nconducted a qualitative user study in two sessions, comparing a typical fixed\nsmart speaker to the wandering VA embodiment. Thick descriptions from\ninterviews with twelve people revealed sometimes simultaneous experiences of\nimmersion and dis-immersion. We offer design implications for \"wandering\nvoices\" and a new paradigm for VA embodiment that may extend to guidance tasks\nin other contexts.", "published": "2023-04-22 01:10:21", "link": "http://arxiv.org/abs/2304.11286v1", "categories": ["cs.HC", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "A Comparative Study of Pre-trained Speech and Audio Embeddings for\n  Speech Emotion Recognition", "abstract": "Pre-trained models (PTMs) have shown great promise in the speech and audio\ndomain. Embeddings leveraged from these models serve as inputs for learning\nalgorithms with applications in various downstream tasks. One such crucial task\nis Speech Emotion Recognition (SER) which has a wide range of applications,\nincluding dynamic analysis of customer calls, mental health assessment, and\npersonalized language learning. PTM embeddings have helped advance SER,\nhowever, a comprehensive comparison of these PTM embeddings that consider\nmultiple facets such as embedding model architecture, data used for\npre-training, and the pre-training procedure being followed is missing. A\nthorough comparison of PTM embeddings will aid in the faster and more efficient\ndevelopment of models and enable their deployment in real-world scenarios. In\nthis work, we exploit this research gap and perform a comparative analysis of\nembeddings extracted from eight speech and audio PTMs (wav2vec 2.0, data2vec,\nwavLM, UniSpeech-SAT, wav2clip, YAMNet, x-vector, ECAPA). We perform an\nextensive empirical analysis with four speech emotion datasets (CREMA-D, TESS,\nSAVEE, Emo-DB) by training three algorithms (XGBoost, Random Forest, FCN) on\nthe derived embeddings. The results of our study indicate that the best\nperformance is achieved by algorithms trained on embeddings derived from PTMs\ntrained for speaker recognition followed by wav2clip and UniSpeech-SAT. This\ncan relay that the top performance by embeddings from speaker recognition PTMs\nis most likely due to the model taking up information about numerous speech\nfeatures such as tone, accent, pitch, and so on during its speaker recognition\ntraining. Insights from this work will assist future studies in their selection\nof embeddings for applications related to SER.", "published": "2023-04-22 19:56:35", "link": "http://arxiv.org/abs/2304.11472v1", "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "eess.AS"}
