{"title": "Span-Based Constituency Parsing with a Structure-Label System and\n  Provably Optimal Dynamic Oracles", "abstract": "Parsing accuracy using efficient greedy transition systems has improved\ndramatically in recent years thanks to neural networks. Despite striking\nresults in dependency parsing, however, neural models have not surpassed\nstate-of-the-art approaches in constituency parsing. To remedy this, we\nintroduce a new shift-reduce system whose stack contains merely sentence spans,\nrepresented by a bare minimum of LSTM features. We also design the first\nprovably optimal dynamic oracle for constituency parsing, which runs in\namortized O(1) time, compared to O(n^3) oracles for standard dependency\nparsing. Training with this oracle, we achieve the best F1 scores on both\nEnglish and French of any parser that does not use reranking or external data.", "published": "2016-12-20 01:23:00", "link": "http://arxiv.org/abs/1612.06475v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Different Dimensions of Attention for Uncertainty Detection", "abstract": "Neural networks with attention have proven effective for many natural\nlanguage processing tasks. In this paper, we develop attention mechanisms for\nuncertainty detection. In particular, we generalize standardly used attention\nmechanisms by introducing external attention and sequence-preserving attention.\nThese novel architectures differ from standard approaches in that they use\nexternal resources to compute attention weights and preserve sequence\ninformation. We compare them to other configurations along different dimensions\nof attention. Our novel architectures set the new state of the art on a\nWikipedia benchmark dataset and perform similar to the state-of-the-art model\non a biomedical benchmark which uses a large set of linguistic features.", "published": "2016-12-20 08:49:59", "link": "http://arxiv.org/abs/1612.06549v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Dialogue Act Induction using Gaussian Mixtures", "abstract": "This paper introduces a new unsupervised approach for dialogue act induction.\nGiven the sequence of dialogue utterances, the task is to assign them the\nlabels representing their function in the dialogue.\n  Utterances are represented as real-valued vectors encoding their meaning. We\nmodel the dialogue as Hidden Markov model with emission probabilities estimated\nby Gaussian mixtures. We use Gibbs sampling for posterior inference.\n  We present the results on the standard Switchboard-DAMSL corpus. Our\nalgorithm achieves promising results compared with strong supervised baselines\nand outperforms other unsupervised algorithms.", "published": "2016-12-20 09:52:36", "link": "http://arxiv.org/abs/1612.06572v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grammar rules for the isiZulu complex verb", "abstract": "The isiZulu verb is known for its morphological complexity, which is a\nsubject for on-going linguistics research, as well as for prospects of\ncomputational use, such as controlled natural language interfaces, machine\ntranslation, and spellcheckers. To this end, we seek to answer the question as\nto what the precise grammar rules for the isiZulu complex verb are (and, by\nextension, the Bantu verb morphology). To this end, we iteratively specify the\ngrammar as a Context Free Grammar, and evaluate it computationally. The grammar\npresented in this paper covers the subject and object concords, negation,\npresent tense, aspect, mood, and the causative, applicative, stative, and the\nreciprocal verbal extensions, politeness, the wh-question modifiers, and aspect\ndoubling, ensuring their correct order as they appear in verbs. The grammar\nconforms to specification.", "published": "2016-12-20 10:10:34", "link": "http://arxiv.org/abs/1612.06581v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Inferring the location of authors from words in their texts", "abstract": "For the purposes of computational dialectology or other geographically bound\ntext analysis tasks, texts must be annotated with their or their authors'\nlocation. Many texts are locatable through explicit labels but most have no\nexplicit annotation of place. This paper describes a series of experiments to\ndetermine how positionally annotated microblog posts can be used to learn\nlocation-indicating words which then can be used to locate blog texts and their\nauthors. A Gaussian distribution is used to model the locational qualities of\nwords. We introduce the notion of placeness to describe how locational words\nare.\n  We find that modelling word distributions to account for several locations\nand thus several Gaussian distributions per word, defining a filter which picks\nout words with high placeness based on their local distributional context, and\naggregating locational information in a centroid for each text gives the most\nuseful results. The results are applied to data in the Swedish language.", "published": "2016-12-20 14:13:38", "link": "http://arxiv.org/abs/1612.06671v1", "categories": ["cs.CL", "H.3.1; I.2.7"], "primary_category": "cs.CL"}
{"title": "Stateology: State-Level Interactive Charting of Language, Feelings, and\n  Values", "abstract": "People's personality and motivations are manifest in their everyday language\nusage. With the emergence of social media, ample examples of such usage are\nprocurable. In this paper, we aim to analyze the vocabulary used by close to\n200,000 Blogger users in the U.S. with the purpose of geographically portraying\nvarious demographic, linguistic, and psychological dimensions at the state\nlevel. We give a description of a web-based tool for viewing maps that depict\nvarious characteristics of the social media users as derived from this large\nblog dataset of over two billion words.", "published": "2016-12-20 14:44:19", "link": "http://arxiv.org/abs/1612.06685v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SCDV : Sparse Composite Document Vectors using soft clustering over\n  distributional representations", "abstract": "We present a feature vector formation technique for documents - Sparse\nComposite Document Vector (SCDV) - which overcomes several shortcomings of the\ncurrent distributional paragraph vector representations that are widely used\nfor text representation. In SCDV, word embedding's are clustered to capture\nmultiple semantic contexts in which words occur. They are then chained together\nto form document topic-vectors that can express complex, multi-topic documents.\nThrough extensive experiments on multi-class and multi-label classification\ntasks, we outperform the previous state-of-the-art method, NTSG (Liu et al.,\n2015a). We also show that SCDV embedding's perform well on heterogeneous tasks\nlike Topic Coherence, context-sensitive Learning and Information Retrieval.\nMoreover, we achieve significant reduction in training and prediction times\ncompared to other representation methods. SCDV achieves best of both worlds -\nbetter performance with lower time and space complexity.", "published": "2016-12-20 17:38:57", "link": "http://arxiv.org/abs/1612.06778v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "User Bias Removal in Review Score Prediction", "abstract": "Review score prediction of text reviews has recently gained a lot of\nattention in recommendation systems. A major problem in models for review score\nprediction is the presence of noise due to user-bias in review scores. We\npropose two simple statistical methods to remove such noise and improve review\nscore prediction. Compared to other methods that use multiple classifiers, one\nfor each user, our model uses a single global classifier to predict review\nscores. We empirically evaluate our methods on two major categories\n(\\textit{Electronics} and \\textit{Movies and TV}) of the SNAP published Amazon\ne-Commerce Reviews data-set and Amazon \\textit{Fine Food} reviews data-set. We\nobtain improved review score prediction for three commonly used text feature\nrepresentations.", "published": "2016-12-20 19:39:59", "link": "http://arxiv.org/abs/1612.06821v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fast Domain Adaptation for Neural Machine Translation", "abstract": "Neural Machine Translation (NMT) is a new approach for automatic translation\nof text from one human language into another. The basic concept in NMT is to\ntrain a large Neural Network that maximizes the translation performance on a\ngiven parallel corpus. NMT is gaining popularity in the research community\nbecause it outperformed traditional SMT approaches in several translation tasks\nat WMT and other evaluation tasks/benchmarks at least for some language pairs.\nHowever, many of the enhancements in SMT over the years have not been\nincorporated into the NMT framework. In this paper, we focus on one such\nenhancement namely domain adaptation. We propose an approach for adapting a NMT\nsystem to a new domain. The main idea behind domain adaptation is that the\navailability of large out-of-domain training data and a small in-domain\ntraining data. We report significant gains with our proposed method in both\nautomatic metrics and a human subjective evaluation metric on two language\npairs. With our adaptation method, we show large improvement on the new domain\nwhile the performance of our general domain only degrades slightly. In\naddition, our approach is fast enough to adapt an already trained system to a\nnew domain within few hours without the need to retrain the NMT model on the\ncombined data which usually takes several days/weeks depending on the volume of\nthe data.", "published": "2016-12-20 22:07:51", "link": "http://arxiv.org/abs/1612.06897v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Generation of Grounded Visual Questions", "abstract": "In this paper, we propose the first model to be able to generate visually\ngrounded questions with diverse types for a single image. Visual question\ngeneration is an emerging topic which aims to ask questions in natural language\nbased on visual input. To the best of our knowledge, it lacks automatic methods\nto generate meaningful questions with various types for the same visual input.\nTo circumvent the problem, we propose a model that automatically generates\nvisually grounded questions with varying types. Our model takes as input both\nimages and the captions generated by a dense caption model, samples the most\nprobable question types, and generates the questions in sequel. The\nexperimental results on two real world datasets show that our model outperforms\nthe strongest baseline in terms of both correctness and diversity with a wide\nmargin.", "published": "2016-12-20 07:20:16", "link": "http://arxiv.org/abs/1612.06530v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary\n  Visual Reasoning", "abstract": "When building artificial intelligence systems that can reason and answer\nquestions about visual data, we need diagnostic tests to analyze our progress\nand discover shortcomings. Existing benchmarks for visual question answering\ncan help, but have strong biases that models can exploit to correctly answer\nquestions without reasoning. They also conflate multiple sources of error,\nmaking it hard to pinpoint model weaknesses. We present a diagnostic dataset\nthat tests a range of visual reasoning abilities. It contains minimal biases\nand has detailed annotations describing the kind of reasoning each question\nrequires. We use this dataset to analyze a variety of modern visual reasoning\nsystems, providing novel insights into their abilities and limitations.", "published": "2016-12-20 21:40:40", "link": "http://arxiv.org/abs/1612.06890v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
