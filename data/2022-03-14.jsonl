{"title": "Can pre-trained Transformers be used in detecting complex sensitive\n  sentences? -- A Monsanto case study", "abstract": "Each and every organisation releases information in a variety of forms\nranging from annual reports to legal proceedings. Such documents may contain\nsensitive information and releasing them openly may lead to the leakage of\nconfidential information. Detection of sentences that contain sensitive\ninformation in documents can help organisations prevent the leakage of valuable\nconfidential information. This is especially challenging when such sentences\ncontain a substantial amount of information or are paraphrased versions of\nknown sensitive content. Current approaches to sensitive information detection\nin such complex settings are based on keyword-based approaches or standard\nmachine learning models. In this paper, we wish to explore whether pre-trained\ntransformer models are well suited to detect complex sensitive information.\nPre-trained transformers are typically trained on an enormous amount of text\nand therefore readily learn grammar, structure and other linguistic features,\nmaking them particularly attractive for this task. Through our experiments on\nthe Monsanto trial data set, we observe that the fine-tuned Bidirectional\nEncoder Representations from Transformers (BERT) transformer model performs\nbetter than traditional models. We experimented with four different categories\nof documents in the Monsanto dataset and observed that BERT achieves better F2\nscores by 24.13\\% to 65.79\\% for GHOST, 30.14\\% to 54.88\\% for TOXIC, 39.22\\%\nfor CHEMI, 53.57\\% for REGUL compared to existing sensitive information\ndetection models.", "published": "2022-03-14 00:17:34", "link": "http://arxiv.org/abs/2203.06793v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KenMeSH: Knowledge-enhanced End-to-end Biomedical Text Labelling", "abstract": "Currently, Medical Subject Headings (MeSH) are manually assigned to every\nbiomedical article published and subsequently recorded in the PubMed database\nto facilitate retrieving relevant information. With the rapid growth of the\nPubMed database, large-scale biomedical document indexing becomes increasingly\nimportant. MeSH indexing is a challenging task for machine learning, as it\nneeds to assign multiple labels to each article from an extremely large\nhierachically organized collection. To address this challenge, we propose\nKenMeSH, an end-to-end model that combines new text features and a dynamic\n\\textbf{K}nowledge-\\textbf{en}hanced mask attention that integrates document\nfeatures with MeSH label hierarchy and journal correlation features to index\nMeSH terms. Experimental results show the proposed method achieves\nstate-of-the-art performance on a number of measures.", "published": "2022-03-14 03:09:56", "link": "http://arxiv.org/abs/2203.06835v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improved Universal Sentence Embeddings with Prompt-based Contrastive\n  Learning and Energy-based Learning", "abstract": "Contrastive learning has been demonstrated to be effective in enhancing\npre-trained language models (PLMs) to derive superior universal sentence\nembeddings. However, existing contrastive methods still have two limitations.\nFirstly, previous works may acquire poor performance under domain shift\nsettings, thus hindering the application of sentence representations in\npractice. We attribute this low performance to the over-parameterization of\nPLMs with millions of parameters. To alleviate it, we propose PromCSE\n(Prompt-based Contrastive Learning for Sentence Embeddings), which only trains\nsmall-scale \\emph{Soft Prompt} (i.e., a set of trainable vectors) while keeping\nPLMs fixed. Secondly, the commonly used NT-Xent loss function of contrastive\nlearning does not fully exploit hard negatives in supervised learning settings.\nTo this end, we propose to integrate an Energy-based Hinge loss to enhance the\npairwise discriminative power, inspired by the connection between the NT-Xent\nloss and the Energy-based Learning paradigm. Empirical results on seven\nstandard semantic textual similarity (STS) tasks and a domain-shifted STS task\nboth show the effectiveness of our method compared with the current\nstate-of-the-art sentence embedding models. Our code is publicly avaliable at\nhttps://github.com/YJiangcm/PromCSE", "published": "2022-03-14 06:07:44", "link": "http://arxiv.org/abs/2203.06875v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PERT: Pre-training BERT with Permuted Language Model", "abstract": "Pre-trained Language Models (PLMs) have been widely used in various natural\nlanguage processing (NLP) tasks, owing to their powerful text representations\ntrained on large-scale corpora. In this paper, we propose a new PLM called PERT\nfor natural language understanding (NLU). PERT is an auto-encoding model (like\nBERT) trained with Permuted Language Model (PerLM). The formulation of the\nproposed PerLM is straightforward. We permute a proportion of the input text,\nand the training objective is to predict the position of the original token.\nMoreover, we also apply whole word masking and N-gram masking to improve the\nperformance of PERT. We carried out extensive experiments on both Chinese and\nEnglish NLU benchmarks. The experimental results show that PERT can bring\nimprovements over various comparable baselines on some of the tasks, while\nothers are not. These results indicate that developing more diverse\npre-training tasks is possible instead of masked language model variants.\nSeveral quantitative studies are carried out to better understand PERT, which\nmight help design PLMs in the future. Resources are available:\nhttps://github.com/ymcui/PERT", "published": "2022-03-14 07:58:34", "link": "http://arxiv.org/abs/2203.06906v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modelling word learning and recognition using visually grounded speech", "abstract": "Background: Computational models of speech recognition often assume that the\nset of target words is already given. This implies that these models do not\nlearn to recognise speech from scratch without prior knowledge and explicit\nsupervision. Visually grounded speech models learn to recognise speech without\nprior knowledge by exploiting statistical dependencies between spoken and\nvisual input. While it has previously been shown that visually grounded speech\nmodels learn to recognise the presence of words in the input, we explicitly\ninvestigate such a model as a model of human speech recognition.\n  Methods: We investigate the time-course of word recognition as simulated by\nthe model using a gating paradigm to test whether its recognition is affected\nby well-known word-competition effects in human speech processing. We\nfurthermore investigate whether vector quantisation, a technique for discrete\nrepresentation learning, aids the model in the discovery and recognition of\nwords.\n  Results/Conclusion: Our experiments show that the model is able to recognise\nnouns in isolation and even learns to properly differentiate between plural and\nsingular nouns. We also find that recognition is influenced by word competition\nfrom the word-initial cohort and neighbourhood density, mirroring word\ncompetition effects in human speech comprehension. Lastly, we find no evidence\nthat vector quantisation is helpful in discovering and recognising words. Our\ngating experiments even show that the vector quantised model requires more of\nthe input sequence for correct recognition.", "published": "2022-03-14 08:59:37", "link": "http://arxiv.org/abs/2203.06937v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "S$^2$SQL: Injecting Syntax to Question-Schema Interaction Graph Encoder\n  for Text-to-SQL Parsers", "abstract": "The task of converting a natural language question into an executable SQL\nquery, known as text-to-SQL, is an important branch of semantic parsing. The\nstate-of-the-art graph-based encoder has been successfully used in this task\nbut does not model the question syntax well. In this paper, we propose\nS$^2$SQL, injecting Syntax to question-Schema graph encoder for Text-to-SQL\nparsers, which effectively leverages the syntactic dependency information of\nquestions in text-to-SQL to improve the performance. We also employ the\ndecoupling constraint to induce diverse relational edge embedding, which\nfurther improves the network's performance. Experiments on the Spider and\nrobustness setting Spider-Syn demonstrate that the proposed approach\noutperforms all existing methods when pre-training models are used, resulting\nin a performance ranks first on the Spider leaderboard.", "published": "2022-03-14 09:49:15", "link": "http://arxiv.org/abs/2203.06958v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interpretability for Language Learners Using Example-Based Grammatical\n  Error Correction", "abstract": "Grammatical Error Correction (GEC) should not focus only on high accuracy of\ncorrections but also on interpretability for language learning. However,\nexisting neural-based GEC models mainly aim at improving accuracy, and their\ninterpretability has not been explored. A promising approach for improving\ninterpretability is an example-based method, which uses similar retrieved\nexamples to generate corrections. In addition, examples are beneficial in\nlanguage learning, helping learners understand the basis of grammatically\nincorrect/correct texts and improve their confidence in writing. Therefore, we\nhypothesize that incorporating an example-based method into GEC can improve\ninterpretability as well as support language learners. In this study, we\nintroduce an Example-Based GEC (EB-GEC) that presents examples to language\nlearners as a basis for a correction result. The examples consist of pairs of\ncorrect and incorrect sentences similar to a given input and its predicted\ncorrection. Experiments demonstrate that the examples presented by EB-GEC help\nlanguage learners decide to accept or refuse suggestions from the GEC output.\nFurthermore, the experiments also show that retrieved examples improve the\naccuracy of corrections.", "published": "2022-03-14 13:15:00", "link": "http://arxiv.org/abs/2203.07085v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Novel Perspective to Look At Attention: Bi-level Attention-based\n  Explainable Topic Modeling for News Classification", "abstract": "Many recent deep learning-based solutions have widely adopted the\nattention-based mechanism in various tasks of the NLP discipline. However, the\ninherent characteristics of deep learning models and the flexibility of the\nattention mechanism increase the models' complexity, thus leading to challenges\nin model explainability. In this paper, to address this challenge, we propose a\nnovel practical framework by utilizing a two-tier attention architecture to\ndecouple the complexity of explanation and the decision-making process. We\napply it in the context of a news article classification task. The experiments\non two large-scaled news corpora demonstrate that the proposed model can\nachieve competitive performance with many state-of-the-art alternatives and\nillustrate its appropriateness from an explainability perspective.", "published": "2022-03-14 15:55:21", "link": "http://arxiv.org/abs/2203.07216v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FairLex: A Multilingual Benchmark for Evaluating Fairness in Legal Text\n  Processing", "abstract": "We present a benchmark suite of four datasets for evaluating the fairness of\npre-trained language models and the techniques used to fine-tune them for\ndownstream tasks. Our benchmarks cover four jurisdictions (European Council,\nUSA, Switzerland, and China), five languages (English, German, French, Italian\nand Chinese) and fairness across five attributes (gender, age, region,\nlanguage, and legal area). In our experiments, we evaluate pre-trained language\nmodels using several group-robust fine-tuning techniques and show that\nperformance group disparities are vibrant in many cases, while none of these\ntechniques guarantee fairness, nor consistently mitigate group disparities.\nFurthermore, we provide a quantitative and qualitative analysis of our results,\nhighlighting open challenges in the development of robustness methods in legal\nNLP.", "published": "2022-03-14 16:10:28", "link": "http://arxiv.org/abs/2203.07228v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoNTACT: A Dutch COVID-19 Adapted BERT for Vaccine Hesitancy and\n  Argumentation Detection", "abstract": "We present CoNTACT: a Dutch language model adapted to the domain of COVID-19\ntweets. The model was developed by continuing the pre-training phase of RobBERT\n(Delobelle, 2020) by using 2.8M Dutch COVID-19 related tweets posted in 2021.\nIn order to test the performance of the model and compare it to RobBERT, the\ntwo models were tested on two tasks: (1) binary vaccine hesitancy detection and\n(2) detection of arguments for vaccine hesitancy. For both tasks, not only\nTwitter but also Facebook data was used to show cross-genre performance. In our\nexperiments, CoNTACT showed statistically significant gains over RobBERT in all\nexperiments for task 1. For task 2, we observed substantial improvements in\nvirtually all classes in all experiments. An error analysis indicated that the\ndomain adaptation yielded better representations of domain-specific\nterminology, causing CoNTACT to make more accurate classification decisions.", "published": "2022-03-14 17:55:32", "link": "http://arxiv.org/abs/2203.07362v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting the Compositional Generalization Abilities of Neural Sequence\n  Models", "abstract": "Compositional generalization is a fundamental trait in humans, allowing us to\neffortlessly combine known phrases to form novel sentences. Recent works have\nclaimed that standard seq-to-seq models severely lack the ability to\ncompositionally generalize. In this paper, we focus on one-shot primitive\ngeneralization as introduced by the popular SCAN benchmark. We demonstrate that\nmodifying the training distribution in simple and intuitive ways enables\nstandard seq-to-seq models to achieve near-perfect generalization performance,\nthereby showing that their compositional generalization abilities were\npreviously underestimated. We perform detailed empirical analysis of this\nphenomenon. Our results indicate that the generalization performance of models\nis highly sensitive to the characteristics of the training data which should be\ncarefully considered while designing such benchmarks in future.", "published": "2022-03-14 18:03:21", "link": "http://arxiv.org/abs/2203.07402v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Visual Knowledge in Language Tasks: An Empirical Study on\n  Intermediate Pre-training for Cross-modal Knowledge Transfer", "abstract": "Pre-trained language models are still far from human performance in tasks\nthat need understanding of properties (e.g. appearance, measurable quantity)\nand affordances of everyday objects in the real world since the text lacks such\ninformation due to reporting bias. In this work, we study whether integrating\nvisual knowledge into a language model can fill the gap. We investigate two\ntypes of knowledge transfer: (1) text knowledge transfer using image captions\nthat may contain enriched visual knowledge and (2) cross-modal knowledge\ntransfer using both images and captions with vision-language training\nobjectives. On 5 downstream tasks that may need visual knowledge to solve the\nproblem, we perform extensive empirical comparisons over the presented\nobjectives. Our experiments show that visual knowledge transfer can improve\nperformance in both low-resource and fully supervised settings.", "published": "2022-03-14 22:02:40", "link": "http://arxiv.org/abs/2203.07519v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Choose Your QA Model Wisely: A Systematic Study of Generative and\n  Extractive Readers for Question Answering", "abstract": "While both extractive and generative readers have been successfully applied\nto the Question Answering (QA) task, little attention has been paid toward the\nsystematic comparison of them. Characterizing the strengths and weaknesses of\nthe two readers is crucial not only for making a more informed reader selection\nin practice but also for developing a deeper understanding to foster further\nresearch on improving readers in a principled manner. Motivated by this goal,\nwe make the first attempt to systematically study the comparison of extractive\nand generative readers for question answering. To be aligned with the\nstate-of-the-art, we explore nine transformer-based large pre-trained language\nmodels (PrLMs) as backbone architectures. Furthermore, we organize our findings\nunder two main categories: (1) keeping the architecture invariant, and (2)\nvarying the underlying PrLMs. Among several interesting findings, it is\nimportant to highlight that (1) the generative readers perform better in long\ncontext QA, (2) the extractive readers perform better in short context while\nalso showing better out-of-domain generalization, and (3) the encoder of\nencoder-decoder PrLMs (e.g., T5) turns out to be a strong extractive reader and\noutperforms the standard choice of encoder-only PrLMs (e.g., RoBERTa). We also\nstudy the effect of multi-task learning on the two types of readers varying the\nunderlying PrLMs and perform qualitative and quantitative diagnosis to provide\nfurther insights into future directions in modeling better readers.", "published": "2022-03-14 22:07:52", "link": "http://arxiv.org/abs/2203.07522v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sense Embeddings are also Biased--Evaluating Social Biases in Static and\n  Contextualised Sense Embeddings", "abstract": "Sense embedding learning methods learn different embeddings for the different\nsenses of an ambiguous word. One sense of an ambiguous word might be socially\nbiased while its other senses remain unbiased. In comparison to the numerous\nprior work evaluating the social biases in pretrained word embeddings, the\nbiases in sense embeddings have been relatively understudied. We create a\nbenchmark dataset for evaluating the social biases in sense embeddings and\npropose novel sense-specific bias evaluation measures. We conduct an extensive\nevaluation of multiple static and contextualised sense embeddings for various\ntypes of social biases using the proposed measures. Our experimental results\nshow that even in cases where no biases are found at word-level, there still\nexist worrying levels of social biases at sense-level, which are often ignored\nby the word-level bias evaluation measures.", "published": "2022-03-14 22:08:37", "link": "http://arxiv.org/abs/2203.07523v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Language Modeling with Sparse all-MLP", "abstract": "All-MLP architectures have attracted increasing interest as an alternative to\nattention-based models. In NLP, recent work like gMLP shows that all-MLPs can\nmatch Transformers in language modeling, but still lag behind in downstream\ntasks. In this work, we analyze the limitations of MLPs in expressiveness, and\npropose sparsely activated MLPs with mixture-of-experts (MoEs) in both feature\nand input (token) dimensions. Such sparse all-MLPs significantly increase model\ncapacity and expressiveness while keeping the compute constant. We address\ncritical challenges in incorporating conditional computation with two routing\nstrategies. The proposed sparse all-MLP improves language modeling perplexity\nand obtains up to 2$\\times$ improvement in training efficiency compared to both\nTransformer-based MoEs (GShard, Switch Transformer, Base Layers and HASH\nLayers) as well as dense Transformers and all-MLPs. Finally, we evaluate its\nzero-shot in-context learning performance on six downstream tasks, and find\nthat it surpasses Transformer-based MoEs and dense Transformers.", "published": "2022-03-14 04:32:19", "link": "http://arxiv.org/abs/2203.06850v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WCL-BBCD: A Contrastive Learning and Knowledge Graph Approach to Named\n  Entity Recognition", "abstract": "Named Entity Recognition task is one of the core tasks of information\nextraction. Word ambiguity and word abbreviation are important reasons for the\nlow recognition rate of named entities. In this paper, we propose a novel named\nentity recognition model WCL-BBCD (Word Contrastive Learning with\nBERT-BiLSTM-CRF-DBpedia), which incorporates the idea of contrastive learning.\nThe model first trains the sentence pairs in the text, calculate similarity\nbetween sentence pairs, and fine-tunes BERT used for the named entity\nrecognition task according to the similarity, so as to alleviate word\nambiguity. Then, the fine-tuned BERT is combined with BiLSTM-CRF to perform the\nnamed entity recognition task. Finally, the recognition results are corrected\nin combination with prior knowledge such as knowledge graphs, so as to\nalleviate the low-recognition-rate problem caused by word abbreviations. The\nresults of experimentals conducted on the CoNLL-2003 English dataset and\nOntoNotes V5 English dataset show that our model outperforms other similar\nmodels on.", "published": "2022-03-14 08:29:58", "link": "http://arxiv.org/abs/2203.06925v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hyperlink-induced Pre-training for Passage Retrieval in Open-domain\n  Question Answering", "abstract": "To alleviate the data scarcity problem in training question answering\nsystems, recent works propose additional intermediate pre-training for dense\npassage retrieval (DPR). However, there still remains a large discrepancy\nbetween the provided upstream signals and the downstream question-passage\nrelevance, which leads to less improvement. To bridge this gap, we propose the\nHyperLink-induced Pre-training (HLP), a method to pre-train the dense retriever\nwith the text relevance induced by hyperlink-based topology within Web\ndocuments. We demonstrate that the hyperlink-based structures of dual-link and\nco-mention can provide effective relevance signals for large-scale pre-training\nthat better facilitate downstream passage retrieval. We investigate the\neffectiveness of our approach across a wide range of open-domain QA datasets\nunder zero-shot, few-shot, multi-hop, and out-of-domain scenarios. The\nexperiments show our HLP outperforms the BM25 by up to 7 points as well as\nother pre-training methods by more than 10 points in terms of top-20 retrieval\naccuracy under the zero-shot scenario. Furthermore, HLP significantly\noutperforms other pre-training methods under the other scenarios.", "published": "2022-03-14 09:09:49", "link": "http://arxiv.org/abs/2203.06942v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "XYLayoutLM: Towards Layout-Aware Multimodal Networks For Visually-Rich\n  Document Understanding", "abstract": "Recently, various multimodal networks for Visually-Rich Document\nUnderstanding(VRDU) have been proposed, showing the promotion of transformers\nby integrating visual and layout information with the text embeddings. However,\nmost existing approaches utilize the position embeddings to incorporate the\nsequence information, neglecting the noisy improper reading order obtained by\nOCR tools. In this paper, we propose a robust layout-aware multimodal network\nnamed XYLayoutLM to capture and leverage rich layout information from proper\nreading orders produced by our Augmented XY Cut. Moreover, a Dilated\nConditional Position Encoding module is proposed to deal with the input\nsequence of variable lengths, and it additionally extracts local layout\ninformation from both textual and visual modalities while generating position\nembeddings. Experiment results show that our XYLayoutLM achieves competitive\nresults on document understanding tasks.", "published": "2022-03-14 09:19:12", "link": "http://arxiv.org/abs/2203.06947v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Towards Unifying the Label Space for Aspect- and Sentence-based\n  Sentiment Analysis", "abstract": "The aspect-based sentiment analysis (ABSA) is a fine-grained task that aims\nto determine the sentiment polarity towards targeted aspect terms occurring in\nthe sentence. The development of the ABSA task is very much hindered by the\nlack of annotated data. To tackle this, the prior works have studied the\npossibility of utilizing the sentiment analysis (SA) datasets to assist in\ntraining the ABSA model, primarily via pretraining or multi-task learning. In\nthis article, we follow this line, and for the first time, we manage to apply\nthe Pseudo-Label (PL) method to merge the two homogeneous tasks. While it seems\nstraightforward to use generated pseudo labels to handle this case of label\ngranularity unification for two highly related tasks, we identify its major\nchallenge in this paper and propose a novel framework, dubbed as\nDual-granularity Pseudo Labeling (DPL). Further, similar to PL, we regard the\nDPL as a general framework capable of combining other prior methods in the\nliterature. Through extensive experiments, DPL has achieved state-of-the-art\nperformance on standard benchmarks surpassing the prior work significantly.", "published": "2022-03-14 13:21:57", "link": "http://arxiv.org/abs/2203.07090v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Bayesian approach to translators' reliability assessment", "abstract": "Translation Quality Assessment (TQA) is a process conducted by human\ntranslators and is widely used, both for estimating the performance of\n(increasingly used) Machine Translation, and for finding an agreement between\ntranslation providers and their customers. While translation scholars are aware\nof the importance of having a reliable way to conduct the TQA process, it seems\nthat there is limited literature that tackles the issue of reliability with a\nquantitative approach. In this work, we consider the TQA as a complex process\nfrom the point of view of physics of complex systems and approach the\nreliability issue from the Bayesian paradigm. Using a dataset of translation\nquality evaluations (in the form of error annotations), produced entirely by\nthe Professional Translation Service Provider Translated SRL, we compare two\nBayesian models that parameterise the following features involved in the TQA\nprocess: the translation difficulty, the characteristics of the translators\ninvolved in producing the translation, and of those assessing its quality - the\nreviewers. We validate the models in an unsupervised setting and show that it\nis possible to get meaningful insights into translators even with just one\nreview per translation; subsequently, we extract information like translators'\nskills and reviewers' strictness, as well as their consistency in their\nrespective roles. Using this, we show that the reliability of reviewers cannot\nbe taken for granted even in the case of expert translators: a translator's\nexpertise can induce a cognitive bias when reviewing a translation produced by\nanother translator. The most expert translators, however, are characterised by\nthe highest level of consistency, both in translating and in assessing the\ntranslation quality.", "published": "2022-03-14 14:29:45", "link": "http://arxiv.org/abs/2203.07135v2", "categories": ["cs.CL", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "CLIP Models are Few-shot Learners: Empirical Studies on VQA and Visual\n  Entailment", "abstract": "CLIP has shown a remarkable zero-shot capability on a wide range of vision\ntasks. Previously, CLIP is only regarded as a powerful visual encoder. However,\nafter being pre-trained by language supervision from a large amount of\nimage-caption pairs, CLIP itself should also have acquired some few-shot\nabilities for vision-language tasks. In this work, we empirically show that\nCLIP can be a strong vision-language few-shot learner by leveraging the power\nof language. We first evaluate CLIP's zero-shot performance on a typical visual\nquestion answering task and demonstrate a zero-shot cross-modality transfer\ncapability of CLIP on the visual entailment task. Then we propose a\nparameter-efficient fine-tuning strategy to boost the few-shot performance on\nthe vqa task. We achieve competitive zero/few-shot results on the visual\nquestion answering and visual entailment tasks without introducing any\nadditional pre-training procedure.", "published": "2022-03-14 15:29:27", "link": "http://arxiv.org/abs/2203.07190v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for\n  Large Language Models", "abstract": "Transformer-based language models have become a key building block for\nnatural language processing. While these models are extremely accurate, they\ncan be too large and computationally intensive to run on standard deployments.\nA variety of compression methods, including distillation, quantization,\nstructured and unstructured pruning are known to decrease model size and\nincrease inference speed, with low accuracy loss. In this context, this paper's\ncontributions are two-fold. We perform an in-depth study of the\naccuracy-compression trade-off for unstructured weight pruning of BERT models.\nWe introduce Optimal BERT Surgeon (oBERT), an efficient and accurate weight\npruning method based on approximate second-order information, which we show to\nyield state-of-the-art results in both stages of language tasks: pre-training\nand fine-tuning. Specifically, oBERT extends existing work on unstructured\nsecond-order pruning by allowing for pruning blocks of weights, and by being\napplicable at the BERT scale. Second, we investigate the impact of this pruning\nmethod when compounding compression approaches to obtain highly compressed but\naccurate models for deployment on edge devices. These models significantly push\nboundaries of the current state-of-the-art sparse BERT models with respect to\nall metrics: model size, inference speed and task accuracy. For example,\nrelative to the dense BERT-base, we obtain 10x model size compression (in MB)\nwith < 1% accuracy drop, 10x CPU-inference speedup with < 2% accuracy drop, and\n29x CPU-inference speedup with < 7.5% accuracy drop. Our code, fully integrated\nwith Transformers and SparseML, is available at\nhttps://github.com/neuralmagic/sparseml/tree/main/research/optimal_BERT_surgeon_oBERT.", "published": "2022-03-14 16:40:31", "link": "http://arxiv.org/abs/2203.07259v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Show Me More Details: Discovering Hierarchies of Procedures from\n  Semi-structured Web Data", "abstract": "Procedures are inherently hierarchical. To \"make videos\", one may need to\n\"purchase a camera\", which in turn may require one to \"set a budget\". While\nsuch hierarchical knowledge is critical for reasoning about complex procedures,\nmost existing work has treated procedures as shallow structures without\nmodeling the parent-child relation. In this work, we attempt to construct an\nopen-domain hierarchical knowledge-base (KB) of procedures based on wikiHow, a\nwebsite containing more than 110k instructional articles, each documenting the\nsteps to carry out a complex procedure. To this end, we develop a simple and\nefficient method that links steps (e.g., \"purchase a camera\") in an article to\nother articles with similar goals (e.g., \"how to choose a camera\"), recursively\nconstructing the KB. Our method significantly outperforms several strong\nbaselines according to automatic evaluation, human judgment, and application to\ndownstream tasks such as instructional video retrieval.\n  A demo with partial data can be found at https://wikihow-hierarchy.github.io.\nThe code and the data are at https://github.com/shuyanzhou/wikihow_hierarchy.", "published": "2022-03-14 16:42:35", "link": "http://arxiv.org/abs/2203.07264v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Diversifying Content Generation for Commonsense Reasoning with Mixture\n  of Knowledge Graph Experts", "abstract": "Generative commonsense reasoning (GCR) in natural language is to reason about\nthe commonsense while generating coherent text. Recent years have seen a surge\nof interest in improving the generation quality of commonsense reasoning tasks.\nNevertheless, these approaches have seldom investigated diversity in the GCR\ntasks, which aims to generate alternative explanations for a real-world\nsituation or predict all possible outcomes. Diversifying GCR is challenging as\nit expects to generate multiple outputs that are not only semantically\ndifferent but also grounded in commonsense knowledge. In this paper, we propose\nMoKGE, a novel method that diversifies the generative reasoning by a mixture of\nexpert (MoE) strategy on commonsense knowledge graphs (KG). A set of knowledge\nexperts seek diverse reasoning on KG to encourage various generation outputs.\nEmpirical experiments demonstrated that MoKGE can significantly improve the\ndiversity while achieving on par performance on accuracy on two GCR benchmarks,\nbased on both automatic and human evaluations.", "published": "2022-03-14 16:57:50", "link": "http://arxiv.org/abs/2203.07285v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sememe Prediction for BabelNet Synsets using Multilingual and Multimodal\n  Information", "abstract": "In linguistics, a sememe is defined as the minimum semantic unit of\nlanguages. Sememe knowledge bases (KBs), which are built by manually annotating\nwords with sememes, have been successfully applied to various NLP tasks.\nHowever, existing sememe KBs only cover a few languages, which hinders the wide\nutilization of sememes. To address this issue, the task of sememe prediction\nfor BabelNet synsets (SPBS) is presented, aiming to build a multilingual sememe\nKB based on BabelNet, a multilingual encyclopedia dictionary. By automatically\npredicting sememes for a BabelNet synset, the words in many languages in the\nsynset would obtain sememe annotations simultaneously. However, previous SPBS\nmethods have not taken full advantage of the abundant information in BabelNet.\nIn this paper, we utilize the multilingual synonyms, multilingual glosses and\nimages in BabelNet for SPBS. We design a multimodal information fusion model to\nencode and combine this information for sememe prediction. Experimental results\nshow the substantial outperformance of our model over previous methods (about\n10 MAP and F1 scores). All the code and data of this paper can be obtained at\nhttps://github.com/thunlp/MSGI.", "published": "2022-03-14 18:37:09", "link": "http://arxiv.org/abs/2203.07426v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ScienceWorld: Is your Agent Smarter than a 5th Grader?", "abstract": "We present ScienceWorld, a benchmark to test agents' scientific reasoning\nabilities in a new interactive text environment at the level of a standard\nelementary school science curriculum. Despite the transformer-based progress\nseen in question-answering and scientific text processing, we find that current\nmodels cannot reason about or explain learned science concepts in novel\ncontexts. For instance, models can easily answer what the conductivity of a\nknown material is but struggle when asked how they would conduct an experiment\nin a grounded environment to find the conductivity of an unknown material. This\nbegs the question of whether current models are simply retrieving answers by\nway of seeing a large number of similar examples or if they have learned to\nreason about concepts in a reusable manner. We hypothesize that agents need to\nbe grounded in interactive environments to achieve such reasoning capabilities.\nOur experiments provide empirical evidence supporting this hypothesis --\nshowing that a 1.5 million parameter agent trained interactively for 100k steps\noutperforms a 11 billion parameter model statically trained for scientific\nquestion-answering and reasoning from millions of expert demonstrations.", "published": "2022-03-14 22:52:34", "link": "http://arxiv.org/abs/2203.07540v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Calibration of Pre-trained Language Models using Mixup Guided by\n  Area Under the Margin and Saliency", "abstract": "A well-calibrated neural model produces confidence (probability outputs)\nclosely approximated by the expected accuracy. While prior studies have shown\nthat mixup training as a data augmentation technique can improve model\ncalibration on image classification tasks, little is known about using mixup\nfor model calibration on natural language understanding (NLU) tasks. In this\npaper, we explore mixup for model calibration on several NLU tasks and propose\na novel mixup strategy for pre-trained language models that improves model\ncalibration further. Our proposed mixup is guided by both the Area Under the\nMargin (AUM) statistic (Pleiss et al., 2020) and the saliency map of each\nsample (Simonyan et al.,2013). Moreover, we combine our mixup strategy with\nmodel miscalibration correction techniques (i.e., label smoothing and\ntemperature scaling) and provide detailed analyses of their impact on our\nproposed mixup. We focus on systematically designing experiments on three NLU\ntasks: natural language inference, paraphrase detection, and commonsense\nreasoning. Our method achieves the lowest expected calibration error compared\nto strong baselines on both in-domain and out-of-domain test samples while\nmaintaining competitive accuracy.", "published": "2022-03-14 23:45:08", "link": "http://arxiv.org/abs/2203.07559v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Grounding Commands for Autonomous Vehicles via Layer Fusion with\n  Region-specific Dynamic Layer Attention", "abstract": "Grounding a command to the visual environment is an essential ingredient for\ninteractions between autonomous vehicles and humans. In this work, we study the\nproblem of language grounding for autonomous vehicles, which aims to localize a\nregion in a visual scene according to a natural language command from a\npassenger. Prior work only employs the top layer representations of a\nvision-and-language pre-trained model to predict the region referred to by the\ncommand. However, such a method omits the useful features encoded in other\nlayers, and thus results in inadequate understanding of the input scene and\ncommand. To tackle this limitation, we present the first layer fusion approach\nfor this task. Since different visual regions may require distinct types of\nfeatures to disambiguate them from each other, we further propose the\nregion-specific dynamic (RSD) layer attention to adaptively fuse the multimodal\ninformation across layers for each region. Extensive experiments on the\nTalk2Car benchmark demonstrate that our approach helps predict more accurate\nregions and outperforms state-of-the-art methods.", "published": "2022-03-14 02:37:11", "link": "http://arxiv.org/abs/2203.06822v1", "categories": ["cs.CV", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "SUPERB-SG: Enhanced Speech processing Universal PERformance Benchmark\n  for Semantic and Generative Capabilities", "abstract": "Transfer learning has proven to be crucial in advancing the state of speech\nand natural language processing research in recent years. In speech, a model\npre-trained by self-supervised learning transfers remarkably well on multiple\ntasks. However, the lack of a consistent evaluation methodology is limiting\ntowards a holistic understanding of the efficacy of such models. SUPERB was a\nstep towards introducing a common benchmark to evaluate pre-trained models\nacross various speech tasks. In this paper, we introduce SUPERB-SG, a new\nbenchmark focused on evaluating the semantic and generative capabilities of\npre-trained models by increasing task diversity and difficulty over SUPERB. We\nuse a lightweight methodology to test the robustness of representations learned\nby pre-trained models under shifts in data domain and quality across different\ntypes of tasks. It entails freezing pre-trained model parameters, only using\nsimple task-specific trainable heads. The goal is to be inclusive of all\nresearchers, and encourage efficient use of computational resources. We also\nshow that the task diversity of SUPERB-SG coupled with limited task supervision\nis an effective recipe for evaluating the generalizability of model\nrepresentation.", "published": "2022-03-14 04:26:40", "link": "http://arxiv.org/abs/2203.06849v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for\n  Pre-trained Language Models", "abstract": "Despite the success, the process of fine-tuning large-scale PLMs brings\nprohibitive adaptation costs. In fact, fine-tuning all the parameters of a\ncolossal model and retaining separate instances for different tasks are\npractically infeasible. This necessitates a new branch of research focusing on\nthe parameter-efficient adaptation of PLMs, dubbed as delta tuning in this\npaper. In contrast with the standard fine-tuning, delta tuning only fine-tunes\na small portion of the model parameters while keeping the rest untouched,\nlargely reducing both the computation and storage costs. Recent studies have\ndemonstrated that a series of delta tuning methods with distinct tuned\nparameter selection could achieve performance on a par with full-parameter\nfine-tuning, suggesting a new promising way of stimulating large-scale PLMs. In\nthis paper, we first formally describe the problem of delta tuning and then\ncomprehensively review recent delta tuning approaches. We also propose a\nunified categorization criterion that divide existing delta tuning methods into\nthree groups: addition-based, specification-based, and reparameterization-based\nmethods. Though initially proposed as an efficient method to steer large\nmodels, we believe that some of the fascinating evidence discovered along with\ndelta tuning could help further reveal the mechanisms of PLMs and even deep\nneural networks. To this end, we discuss the theoretical principles underlying\nthe effectiveness of delta tuning and propose frameworks to interpret delta\ntuning from the perspective of optimization and optimal control, respectively.\nFurthermore, we provide a holistic empirical study of representative methods,\nwhere results on over 100 NLP tasks demonstrate a comprehensive performance\ncomparison of different approaches. The experimental results also cover the\nanalysis of combinatorial, scaling and transferable properties of delta tuning.", "published": "2022-03-14 07:56:32", "link": "http://arxiv.org/abs/2203.06904v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Uncertainty-Aware Text-to-Program for Question Answering on Structured\n  Electronic Health Records", "abstract": "Question Answering on Electronic Health Records (EHR-QA) has a significant\nimpact on the healthcare domain, and it is being actively studied. Previous\nresearch on structured EHR-QA focuses on converting natural language queries\ninto query language such as SQL or SPARQL (NLQ2Query), so the problem scope is\nlimited to pre-defined data types by the specific query language. In order to\nexpand the EHR-QA task beyond this limitation to handle multi-modal medical\ndata and solve complex inference in the future, more primitive systemic\nlanguage is needed. In this paper, we design the program-based model\n(NLQ2Program) for EHR-QA as the first step towards the future direction. We\ntackle MIMICSPARQL*, the graph-based EHR-QA dataset, via a program-based\napproach in a semi-supervised manner in order to overcome the absence of gold\nprograms. Without the gold program, our proposed model shows comparable\nperformance to the previous state-of-the-art model, which is an NLQ2Query model\n(0.9% gain). In addition, for a reliable EHR-QA model, we apply the uncertainty\ndecomposition method to measure the ambiguity in the input question. We\nempirically confirmed data uncertainty is most indicative of the ambiguity in\nthe input question.", "published": "2022-03-14 08:12:16", "link": "http://arxiv.org/abs/2203.06918v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Extracting associations and meanings of objects depicted in artworks\n  through bi-modal deep networks", "abstract": "We present a novel bi-modal system based on deep networks to address the\nproblem of learning associations and simple meanings of objects depicted in\n\"authored\" images, such as fine art paintings and drawings. Our overall system\nprocesses both the images and associated texts in order to learn associations\nbetween images of individual objects, their identities and the abstract\nmeanings they signify. Unlike past deep nets that describe depicted objects and\ninfer predicates, our system identifies meaning-bearing objects (\"signifiers\")\nand their associations (\"signifieds\") as well as basic overall meanings for\ntarget artworks. Our system had precision of 48% and recall of 78% with an F1\nmetric of 0.6 on a curated set of Dutch vanitas paintings, a genre celebrated\nfor its concentration on conveying a meaning of great import at the time of\ntheir execution. We developed and tested our system on fine art paintings but\nour general methods can be applied to other authored images.", "published": "2022-03-14 12:10:48", "link": "http://arxiv.org/abs/2203.07026v2", "categories": ["cs.CV", "cs.CL", "cs.LG", "J.5"], "primary_category": "cs.CV"}
{"title": "Interpretable Dysarthric Speaker Adaptation based on Optimal-Transport", "abstract": "This work addresses the mismatch problem between the distribution of training\ndata (source) and testing data (target), in the challenging context of\ndysarthric speech recognition. We focus on Speaker Adaptation (SA) in command\nspeech recognition, where data from multiple sources (i.e., multiple speakers)\nare available. Specifically, we propose an unsupervised Multi-Source Domain\nAdaptation (MSDA) algorithm based on optimal-transport, called MSDA via\nWeighted Joint Optimal Transport (MSDA-WJDOT). We achieve a Command Error Rate\nrelative reduction of 16% and 7% over the speaker-independent model and the\nbest competitor method, respectively. The strength of the proposed approach is\nthat, differently from any other existing SA method, it offers an interpretable\nmodel that can also be exploited, in this context, to diagnose dysarthria\nwithout any specific training. Indeed, it provides a closeness measure between\nthe target and the source speakers, reflecting their similarity in terms of\nspeech characteristics. Based on the similarity between the target speaker and\nthe healthy/dysarthric source speakers, we then define the healthy/dysarthric\nscore of the target speaker that we leverage to perform dysarthria detection.\nThis approach does not require any additional training and achieves a 95%\naccuracy in the dysarthria diagnosis.", "published": "2022-03-14 14:39:00", "link": "http://arxiv.org/abs/2203.07143v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "RED-ACE: Robust Error Detection for ASR using Confidence Embeddings", "abstract": "ASR Error Detection (AED) models aim to post-process the output of Automatic\nSpeech Recognition (ASR) systems, in order to detect transcription errors.\nModern approaches usually use text-based input, comprised solely of the ASR\ntranscription hypothesis, disregarding additional signals from the ASR model.\nInstead, we propose to utilize the ASR system's word-level confidence scores\nfor improving AED performance. Specifically, we add an ASR Confidence Embedding\n(ACE) layer to the AED model's encoder, allowing us to jointly encode the\nconfidence scores and the transcribed text into a contextualized\nrepresentation. Our experiments show the benefits of ASR confidence scores for\nAED, their complementary effect over the textual signal, as well as the\neffectiveness and robustness of ACE for combining these signals. To foster\nfurther research, we publish a novel AED dataset consisting of ASR outputs on\nthe LibriSpeech corpus with annotated transcription errors.", "published": "2022-03-14 15:13:52", "link": "http://arxiv.org/abs/2203.07172v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large\n  Language Models", "abstract": "Providing natural language instructions in prompts is a useful new paradigm\nfor improving task performance of large language models in a zero-shot setting.\nRecent work has aimed to improve such prompts via manual rewriting or\ngradient-based tuning. However, manual rewriting is time-consuming and requires\nsubjective interpretation, while gradient-based tuning can be extremely\ncomputationally demanding for large models and may not be feasible for\nAPI-based models. In this work, we introduce Gradient-free Instructional Prompt\nSearch (GrIPS), a gradient-free, edit-based search approach for improving task\ninstructions for large language models. GrIPS takes in instructions designed\nfor humans and automatically returns an improved, edited prompt, while allowing\nfor API-based tuning. With InstructGPT models, GrIPS improves the average task\nperformance by up to 4.30 percentage points on eight classification tasks from\nthe Natural Instructions dataset (with similar improvements for OPT, BLOOM, and\nFLAN-T5). We see improvements for both instruction-only prompts and instruction\n+ k-shot examples prompts. Notably, GrIPS outperforms manual rewriting and\npurely example-based prompts while controlling for the available compute and\ndata budget. Further, performance of GrIPS is comparable to select\ngradient-based tuning approaches. Qualitatively, we show our edits can simplify\ninstructions and at times make them incoherent but nonetheless improve\naccuracy. Our code is available at: https://github.com/archiki/GrIPS", "published": "2022-03-14 16:54:46", "link": "http://arxiv.org/abs/2203.07281v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HIE-SQL: History Information Enhanced Network for Context-Dependent\n  Text-to-SQL Semantic Parsing", "abstract": "Recently, context-dependent text-to-SQL semantic parsing which translates\nnatural language into SQL in an interaction process has attracted a lot of\nattention. Previous works leverage context-dependence information either from\ninteraction history utterances or the previous predicted SQL queries but fail\nin taking advantage of both since of the mismatch between natural language and\nlogic-form SQL. In this work, we propose a History Information Enhanced\ntext-to-SQL model (HIE-SQL) to exploit context-dependence information from both\nhistory utterances and the last predicted SQL query. In view of the mismatch,\nwe treat natural language and SQL as two modalities and propose a bimodal\npre-trained model to bridge the gap between them. Besides, we design a\nschema-linking graph to enhance connections from utterances and the SQL query\nto the database schema. We show our history information enhanced methods\nimprove the performance of HIE-SQL by a significant margin, which achieves new\nstate-of-the-art results on the two context-dependent text-to-SQL benchmarks,\nthe SparC and CoSQL datasets, at the writing time.", "published": "2022-03-14 11:58:37", "link": "http://arxiv.org/abs/2203.07376v2", "categories": ["cs.DB", "cs.AI", "cs.CL"], "primary_category": "cs.DB"}
{"title": "A Neural Pairwise Ranking Model for Readability Assessment", "abstract": "Automatic Readability Assessment (ARA), the task of assigning a reading level\nto a text, is traditionally treated as a classification problem in NLP\nresearch. In this paper, we propose the first neural, pairwise ranking approach\nto ARA and compare it with existing classification, regression, and\n(non-neural) ranking methods. We establish the performance of our model by\nconducting experiments with three English, one French and one Spanish datasets.\nWe demonstrate that our approach performs well in monolingual single/cross\ncorpus testing scenarios and achieves a zero-shot cross-lingual ranking\naccuracy of over 80% for both French and Spanish when trained on English data.\nAdditionally, we also release a new parallel bilingual readability dataset in\nEnglish and French. To our knowledge, this paper proposes the first neural\npairwise ranking model for ARA, and shows the first results of cross-lingual,\nzero-shot evaluation of ARA with neural models.", "published": "2022-03-14 19:11:03", "link": "http://arxiv.org/abs/2203.07450v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Uncertainty Estimation for Language Reward Models", "abstract": "Language models can learn a range of capabilities from unsupervised training\non text corpora. However, to solve a particular problem (such as text\nsummarization) it is typically necessary to fine-tune them on a task-specific\ndataset. It is often easier for humans to choose between options than to\nprovide labeled data, and prior work has achieved state-of-the-art performance\nby training a reward model from such preference comparisons. However,\ncollecting a large preference comparison dataset is still expensive -- and the\nlearned reward models are unreliable out-of-distribution. We seek to address\nthese problems via uncertainty estimation, which can improve sample efficiency\nand robustness using active learning and risk-averse reinforcement learning\n(RL). Specifically, we use bootstrap aggregating (bagging) to train an ensemble\nof reward models differing in the initialization of their final layer.\nEnsembles have proved successful in prior applications of active learning, but\nwe find that in our setting ensemble active learning does not outperform random\nsampling. Further experiments show that while the aggregate predictions are\nwell-calibrated, the ensemble's estimated epistemic uncertainty is only weakly\ncorrelated with model error. We suspect this is because the ensemble members\nare fine-tuned from a single model and so are similar to one another. This\nsuggests current pre-training methods will need to be modified to support\nuncertainty estimation, e.g. by training multiple language models.", "published": "2022-03-14 20:13:21", "link": "http://arxiv.org/abs/2203.07472v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "VAST: The Valence-Assessing Semantics Test for Contextualizing Language\n  Models", "abstract": "VAST, the Valence-Assessing Semantics Test, is a novel intrinsic evaluation\ntask for contextualized word embeddings (CWEs). VAST uses valence, the\nassociation of a word with pleasantness, to measure the correspondence of\nword-level LM semantics with widely used human judgments, and examines the\neffects of contextualization, tokenization, and LM-specific geometry. Because\nprior research has found that CWEs from GPT-2 perform poorly on other intrinsic\nevaluations, we select GPT-2 as our primary subject, and include results\nshowing that VAST is useful for 7 other LMs, and can be used in 7 languages.\nGPT-2 results show that the semantics of a word incorporate the semantics of\ncontext in layers closer to model output, such that VAST scores diverge between\nour contextual settings, ranging from Pearson's rho of .55 to .77 in layer 11.\nWe also show that multiply tokenized words are not semantically encoded until\nlayer 8, where they achieve Pearson's rho of .46, indicating the presence of an\nencoding process for multiply tokenized words which differs from that of singly\ntokenized words, for which rho is highest in layer 0. We find that a few\nneurons with values having greater magnitude than the rest mask word-level\nsemantics in GPT-2's top layer, but that word-level semantics can be recovered\nby nullifying non-semantic principal components: Pearson's rho in the top layer\nimproves from .32 to .76. After isolating semantics, we show the utility of\nVAST for understanding LM semantics via improvements over related work on four\nword similarity tasks, with a score of .50 on SimLex-999, better than the\nprevious best of .45 for GPT-2. Finally, we show that 8 of 10 WEAT bias tests,\nwhich compare differences in word embedding associations between groups of\nwords, exhibit more stereotype-congruent biases after isolating semantics,\nindicating that non-semantic structures in LMs also mask biases.", "published": "2022-03-14 21:29:38", "link": "http://arxiv.org/abs/2203.07504v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Contrastive Visual Semantic Pretraining Magnifies the Semantics of\n  Natural Language Representations", "abstract": "We examine the effects of contrastive visual semantic pretraining by\ncomparing the geometry and semantic properties of contextualized English\nlanguage representations formed by GPT-2 and CLIP, a zero-shot multimodal image\nclassifier which adapts the GPT-2 architecture to encode image captions. We\nfind that contrastive visual semantic pretraining significantly mitigates the\nanisotropy found in contextualized word embeddings from GPT-2, such that the\nintra-layer self-similarity (mean pairwise cosine similarity) of CLIP word\nembeddings is under .25 in all layers, compared to greater than .95 in the top\nlayer of GPT-2. CLIP word embeddings outperform GPT-2 on word-level semantic\nintrinsic evaluation tasks, and achieve a new corpus-based state of the art for\nthe RG65 evaluation, at .88. CLIP also forms fine-grained semantic\nrepresentations of sentences, and obtains Spearman's rho = .73 on the\nSemEval-2017 Semantic Textual Similarity Benchmark with no fine-tuning,\ncompared to no greater than rho = .45 in any layer of GPT-2. Finally,\nintra-layer self-similarity of CLIP sentence embeddings decreases as the layer\nindex increases, finishing at .25 in the top layer, while the self-similarity\nof GPT-2 sentence embeddings formed using the EOS token increases\nlayer-over-layer and never falls below .97. Our results indicate that high\nanisotropy is not an inevitable consequence of contextualization, and that\nvisual semantic pretraining is beneficial not only for ordering visual\nrepresentations, but also for encoding useful semantic representations of\nlanguage, both on the word level and the sentence level.", "published": "2022-03-14 21:42:13", "link": "http://arxiv.org/abs/2203.07511v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MDNet: Learning Monaural Speech Enhancement from Deep Prior Gradient", "abstract": "While traditional statistical signal processing model-based methods can\nderive the optimal estimators relying on specific statistical assumptions,\ncurrent learning-based methods further promote the performance upper bound via\ndeep neural networks but at the expense of high encapsulation and lack adequate\ninterpretability. Standing upon the intersection between traditional\nmodel-based methods and learning-based methods, we propose a model-driven\napproach based on the maximum a posteriori (MAP) framework, termed as MDNet,\nfor single-channel speech enhancement. Specifically, the original problem is\nformulated into the joint posterior estimation w.r.t. speech and noise\ncomponents. Different from the manual assumption toward the prior terms, we\npropose to model the prior distribution via networks and thus can learn from\ntraining data. The framework takes the unfolding structure and in each step,\nthe target parameters can be progressively estimated through explicit gradient\ndescent operations. Besides, another network serves as the fusion module to\nfurther refine the previous speech estimation. The experiments are conducted on\nthe WSJ0-SI84 and Interspeech2020 DNS-Challenge datasets, and quantitative\nresults show that the proposed approach outshines previous state-of-the-art\nbaselines.", "published": "2022-03-14 15:19:01", "link": "http://arxiv.org/abs/2203.07179v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TaylorBeamformer: Learning All-Neural Beamformer for Multi-Channel\n  Speech Enhancement from Taylor's Approximation Theory", "abstract": "While existing end-to-end beamformers achieve impressive performance in\nvarious front-end speech processing tasks, they usually encapsulate the whole\nprocess into a black box and thus lack adequate interpretability. As an attempt\nto fill the blank, we propose a novel neural beamformer inspired by Taylor's\napproximation theory called TaylorBeamformer for multi-channel speech\nenhancement. The core idea is that the recovery process can be formulated as\nthe spatial filtering in the neighborhood of the input mixture. Based on that,\nwe decompose it into the superimposition of the 0th-order non-derivative and\nhigh-order derivative terms, where the former serves as the spatial filter and\nthe latter is viewed as the residual noise canceller to further improve the\nspeech quality. To enable end-to-end training, we replace the derivative\noperations with trainable networks and thus can learn from training data.\nExtensive experiments are conducted on the synthesized dataset based on\nLibriSpeech and results show that the proposed approach performs favorably\nagainst the previous advanced baselines.", "published": "2022-03-14 15:34:59", "link": "http://arxiv.org/abs/2203.07195v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Dawn of the transformer era in speech emotion recognition: closing the\n  valence gap", "abstract": "Recent advances in transformer-based architectures which are pre-trained in\nself-supervised manner have shown great promise in several machine learning\ntasks. In the audio domain, such architectures have also been successfully\nutilised in the field of speech emotion recognition (SER). However, existing\nworks have not evaluated the influence of model size and pre-training data on\ndownstream performance, and have shown limited attention to generalisation,\nrobustness, fairness, and efficiency. The present contribution conducts a\nthorough analysis of these aspects on several pre-trained variants of wav2vec\n2.0 and HuBERT that we fine-tuned on the dimensions arousal, dominance, and\nvalence of MSP-Podcast, while additionally using IEMOCAP and MOSI to test\ncross-corpus generalisation. To the best of our knowledge, we obtain the top\nperformance for valence prediction without use of explicit linguistic\ninformation, with a concordance correlation coefficient (CCC) of .638 on\nMSP-Podcast. Furthermore, our investigations reveal that transformer-based\narchitectures are more robust to small perturbations compared to a CNN-based\nbaseline and fair with respect to biological sex groups, but not towards\nindividual speakers. Finally, we are the first to show that their extraordinary\nsuccess on valence is based on implicit linguistic information learnt during\nfine-tuning of the transformer layers, which explains why they perform on-par\nwith recent multimodal approaches that explicitly utilise textual information.\nOur findings collectively paint the following picture: transformer-based\narchitectures constitute the new state-of-the-art in SER, but further advances\nare needed to mitigate remaining robustness and individual speaker issues. To\nmake our findings reproducible, we release the best performing model to the\ncommunity.", "published": "2022-03-14 13:21:47", "link": "http://arxiv.org/abs/2203.07378v4", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
