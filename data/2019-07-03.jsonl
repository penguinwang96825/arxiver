{"title": "Depth Growing for Neural Machine Translation", "abstract": "While very deep neural networks have shown effectiveness for computer vision\nand text classification applications, how to increase the network depth of\nneural machine translation (NMT) models for better translation quality remains\na challenging problem. Directly stacking more blocks to the NMT model results\nin no improvement and even reduces performance. In this work, we propose an\neffective two-stage approach with three specially designed components to\nconstruct deeper NMT models, which result in significant improvements over the\nstrong Transformer baselines on WMT$14$ English$\\to$German and\nEnglish$\\to$French translation tasks\\footnote{Our code is available at\n\\url{https://github.com/apeterswu/Depth_Growing_NMT}}.", "published": "2019-07-03 14:31:45", "link": "http://arxiv.org/abs/1907.01968v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Real-time Claim Detection from News Articles and Retrieval of\n  Semantically-Similar Factchecks", "abstract": "Factchecking has always been a part of the journalistic process. However with\nnewsroom budgets shrinking it is coming under increasing pressure just as the\namount of false information circulating is on the rise. We therefore propose a\nmethod to increase the efficiency of the factchecking process, using the latest\ndevelopments in Natural Language Processing (NLP). This method allows us to\ncompare incoming claims to an existing corpus and return similar, factchecked,\nclaims in a live system-allowing factcheckers to work simultaneously without\nduplicating their work.", "published": "2019-07-03 16:50:50", "link": "http://arxiv.org/abs/1907.02030v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Multi-Party Turn-Taking Models from Dialogue Logs", "abstract": "This paper investigates the application of machine learning (ML) techniques\nto enable intelligent systems to learn multi-party turn-taking models from\ndialogue logs. The specific ML task consists of determining who speaks next,\nafter each utterance of a dialogue, given who has spoken and what was said in\nthe previous utterances. With this goal, this paper presents comparisons of the\naccuracy of different ML techniques such as Maximum Likelihood Estimation\n(MLE), Support Vector Machines (SVM), and Convolutional Neural Networks (CNN)\narchitectures, with and without utterance data. We present three corpora: the\nfirst with dialogues from an American TV situated comedy (chit-chat), the\nsecond with logs from a financial advice multi-bot system and the third with a\ncorpus created from the Multi-Domain Wizard-of-Oz dataset (both are\ntopic-oriented). The results show: (i) the size of the corpus has a very\npositive impact on the accuracy for the content-based deep learning approaches\nand those models perform best in the larger datasets; and (ii) if the dialogue\ndataset is small and topic-oriented (but with few topics), it is sufficient to\nuse an agent-only MLE or SVM models, although slightly higher accuracies can be\nachieved with the use of the content of the utterances with a CNN model.", "published": "2019-07-03 18:08:16", "link": "http://arxiv.org/abs/1907.02090v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Polyphone Disambiguation for Mandarin Chinese Using Conditional Neural\n  Network with Multi-level Embedding Features", "abstract": "This paper describes a conditional neural network architecture for Mandarin\nChinese polyphone disambiguation. The system is composed of a bidirectional\nrecurrent neural network component acting as a sentence encoder to accumulate\nthe context correlations, followed by a prediction network that maps the\npolyphonic character embeddings along with the conditions to corresponding\npronunciations. We obtain the word-level condition from a pre-trained\nword-to-vector lookup table. One goal of polyphone disambiguation is to address\nthe homograph problem existing in the front-end processing of Mandarin Chinese\ntext-to-speech system. Our system achieves an accuracy of 94.69\\% on a publicly\navailable polyphonic character dataset. To further validate our choices on the\nconditional feature, we investigate polyphone disambiguation systems with\nmulti-level conditions respectively. The experimental results show that both\nthe sentence-level and the word-level conditional embedding features are able\nto attain good performance for Mandarin Chinese polyphone disambiguation.", "published": "2019-07-03 05:50:34", "link": "http://arxiv.org/abs/1907.01749v1", "categories": ["cs.CL", "eess.AS", "stat.ML"], "primary_category": "cs.CL"}
{"title": "On the Weaknesses of Reinforcement Learning for Neural Machine\n  Translation", "abstract": "Reinforcement learning (RL) is frequently used to increase performance in\ntext generation tasks, including machine translation (MT), notably through the\nuse of Minimum Risk Training (MRT) and Generative Adversarial Networks (GAN).\nHowever, little is known about what and how these methods learn in the context\nof MT. We prove that one of the most common RL methods for MT does not optimize\nthe expected reward, as well as show that other methods take an infeasibly long\ntime to converge. In fact, our results suggest that RL practices in MT are\nlikely to improve performance only where the pre-trained parameters are already\nclose to yielding the correct translation. Our findings further suggest that\nobserved gains may be due to effects unrelated to the training signal, but\nrather from changes in the shape of the distribution curve.", "published": "2019-07-03 06:15:14", "link": "http://arxiv.org/abs/1907.01752v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-Task Networks With Universe, Group, and Task Feature Learning", "abstract": "We present methods for multi-task learning that take advantage of natural\ngroupings of related tasks. Task groups may be defined along known properties\nof the tasks, such as task domain or language. Such task groups represent\nsupervised information at the inter-task level and can be encoded into the\nmodel. We investigate two variants of neural network architectures that\naccomplish this, learning different feature spaces at the levels of individual\ntasks, task groups, as well as the universe of all tasks: (1) parallel\narchitectures encode each input simultaneously into feature spaces at different\nlevels; (2) serial architectures encode each input successively into feature\nspaces at different levels in the task hierarchy. We demonstrate the methods on\nnatural language understanding (NLU) tasks, where a grouping of tasks into\ndifferent task domains leads to improved performance on ATIS, Snips, and a\nlarge inhouse dataset.", "published": "2019-07-03 08:39:14", "link": "http://arxiv.org/abs/1907.01791v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "End-to-End Speech Recognition with High-Frame-Rate Features Extraction", "abstract": "State-of-the-art end-to-end automatic speech recognition (ASR) extracts\nacoustic features from input speech signal every 10 ms which corresponds to a\nframe rate of 100 frames/second. In this report, we investigate the use of\nhigh-frame-rate features extraction in end-to-end ASR. High frame rates of 200\nand 400 frames/second are used in the features extraction and provide\nadditional information for end-to-end ASR. The effectiveness of high-frame-rate\nfeatures extraction is evaluated independently and in combination with speed\nperturbation based data augmentation. Experiments performed on two speech\ncorpora, Wall Street Journal (WSJ) and CHiME-5, show that using high-frame-rate\nfeatures extraction yields improved performance for end-to-end ASR, both\nindependently and in combination with speed perturbation. On WSJ corpus, the\nrelative reduction of word error rate (WER) yielded by high-frame-rate features\nextraction independently and in combination with speed perturbation are up to\n21.3% and 24.1%, respectively. On CHiME-5 corpus, the corresponding relative\nWER reductions are up to 2.8% and 7.9%, respectively, on the test data recorded\nby microphone arrays and up to 11.8% and 21.2%, respectively, on the test data\nrecorded by binaural microphones.", "published": "2019-07-03 14:14:27", "link": "http://arxiv.org/abs/1907.01957v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Chasing Ghosts: Instruction Following as Bayesian State Tracking", "abstract": "A visually-grounded navigation instruction can be interpreted as a sequence\nof expected observations and actions an agent following the correct trajectory\nwould encounter and perform. Based on this intuition, we formulate the problem\nof finding the goal location in Vision-and-Language Navigation (VLN) within the\nframework of Bayesian state tracking - learning observation and motion models\nconditioned on these expectable events. Together with a mapper that constructs\na semantic spatial map on-the-fly during navigation, we formulate an end-to-end\ndifferentiable Bayes filter and train it to identify the goal by predicting the\nmost likely trajectory through the map according to the instructions. The\nresulting navigation policy constitutes a new approach to instruction following\nthat explicitly models a probability distribution over states, encoding strong\ngeometric and algorithmic priors while enabling greater explainability. Our\nexperiments show that our approach outperforms a strong LingUNet baseline when\npredicting the goal location on the map. On the full VLN task, i.e. navigating\nto the goal location, our approach achieves promising results with less\nreliance on navigation constraints.", "published": "2019-07-03 16:39:05", "link": "http://arxiv.org/abs/1907.02022v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Combining Q&A Pair Quality and Question Relevance Features on\n  Community-based Question Retrieval", "abstract": "The Q&A community has become an important way for people to access knowledge\nand information from the Internet. However, the existing translation based on\nmodels does not consider the query specific semantics when assigning weights to\nquery terms in question retrieval. So we improve the term weighting model based\non the traditional topic translation model and further considering the quality\ncharacteristics of question and answer pairs, this paper proposes a\ncommunitybased question retrieval method that combines question and answer on\nquality and question relevance (T2LM+). We have also proposed a question\nretrieval method based on convolutional neural networks. The results show that\nCompared with the relatively advanced methods, the two methods proposed in this\npaper increase MAP by 4.91% and 6.31%.", "published": "2019-07-03 16:53:28", "link": "http://arxiv.org/abs/1907.02031v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Deep neural network-based classification model for Sentiment Analysis", "abstract": "The growing prosperity of social networks has brought great challenges to the\nsentimental tendency mining of users. As more and more researchers pay\nattention to the sentimental tendency of online users, rich research results\nhave been obtained based on the sentiment classification of explicit texts.\nHowever, research on the implicit sentiment of users is still in its infancy.\nAiming at the difficulty of implicit sentiment classification, a research on\nimplicit sentiment classification model based on deep neural network is carried\nout. Classification models based on DNN, LSTM, Bi-LSTM and CNN were established\nto judge the tendency of the user's implicit sentiment text. Based on the\nBi-LSTM model, the classification model of word-level attention mechanism is\nstudied. The experimental results on the public dataset show that the\nestablished LSTM series classification model and CNN classification model can\nachieve good sentiment classification effect, and the classification effect is\nsignificantly better than the DNN model. The Bi-LSTM based attention mechanism\nclassification model obtained the optimal R value in the positive category\nidentification.", "published": "2019-07-03 17:24:14", "link": "http://arxiv.org/abs/1907.02046v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Use of OWL and Semantic Web Technologies at Pinterest", "abstract": "Pinterest is a popular Web application that has over 250 million active\nusers. It is a visual discovery engine for finding ideas for recipes, fashion,\nweddings, home decoration, and much more. In the last year, the company adopted\nSemantic Web technologies to create a knowledge graph that aims to represent\nthe vast amount of content and users on Pinterest, to help both content\nrecommendation and ads targeting. In this paper, we present the engineering of\nan OWL ontology---the Pinterest Taxonomy---that forms the core of Pinterest's\nknowledge graph, the Pinterest Taste Graph. We describe modeling choices and\nenhancements to WebProt\\'eg\\'e that we used for the creation of the ontology.\nIn two months, eight Pinterest engineers, without prior experience of OWL and\nWebProt\\'eg\\'e, revamped an existing taxonomy of noisy terms into an OWL\nontology. We share our experience and present the key aspects of our work that\nwe believe will be useful for others working in this area.", "published": "2019-07-03 18:58:49", "link": "http://arxiv.org/abs/1907.02106v1", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Interpretable Segmentation of Medical Free-Text Records Based on Word\n  Embeddings", "abstract": "Is it true that patients with similar conditions get similar diagnoses? In\nthis paper we show NLP methods and a unique corpus of documents to validate\nthis claim. We (1) introduce a method for representation of medical visits\nbased on free-text descriptions recorded by doctors, (2) introduce a new method\nfor clustering of patients' visits and (3) present an~application of the\nproposed method on a corpus of 100,000 visits. With the proposed method we\nobtained stable and separated segments of visits which were positively\nvalidated against final medical diagnoses. We show how the presented algorithm\nmay be used to aid doctors during their practice.", "published": "2019-07-03 15:22:04", "link": "http://arxiv.org/abs/1907.04152v3", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Supervised Classifiers for Audio Impairments with Noisy Labels", "abstract": "Voice-over-Internet-Protocol (VoIP) calls are prone to various speech\nimpairments due to environmental and network conditions resulting in bad user\nexperience. A reliable audio impairment classifier helps to identify the cause\nfor bad audio quality. The user feedback after the call can act as the ground\ntruth labels for training a supervised classifier on a large audio dataset.\nHowever, the labels are noisy as most of the users lack the expertise to\nprecisely articulate the impairment in the perceived speech. In this paper, we\nanalyze the effects of massive noise in labels in training dense networks and\nConvolutional Neural Networks (CNN) using engineered features, spectrograms and\nraw audio samples as inputs. We demonstrate that CNN can generalize better on\nthe training data with a large number of noisy labels and gives remarkably\nhigher test performance. The classifiers were trained both on randomly\ngenerated label noise and the label noise introduced by human errors. We also\nshow that training with noisy labels requires a significant increase in the\ntraining dataset size, which is in proportion to the amount of noise in the\nlabels.", "published": "2019-07-03 05:21:06", "link": "http://arxiv.org/abs/1907.01742v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Receptive Field as a Regularizer in Deep Convolutional Neural\n  Networks for Acoustic Scene Classification", "abstract": "Convolutional Neural Networks (CNNs) have had great success in many machine\nvision as well as machine audition tasks. Many image recognition network\narchitectures have consequently been adapted for audio processing tasks.\nHowever, despite some successes, the performance of many of these did not\ntranslate from the image to the audio domain. For example, very deep\narchitectures such as ResNet and DenseNet, which significantly outperform VGG\nin image recognition, do not perform better in audio processing tasks such as\nAcoustic Scene Classification (ASC). In this paper, we investigate the reasons\nwhy such powerful architectures perform worse in ASC compared to simpler models\n(e.g., VGG). To this end, we analyse the receptive field (RF) of these CNNs and\ndemonstrate the importance of the RF to the generalization capability of the\nmodels. Using our receptive field analysis, we adapt both ResNet and DenseNet,\nachieving state-of-the-art performance and eventually outperforming the\nVGG-based models. We introduce systematic ways of adapting the RF in CNNs, and\npresent results on three data sets that show how changing the RF over the time\nand frequency dimensions affects a model's performance. Our experimental\nresults show that very small or very large RFs can cause performance\ndegradation, but deep models can be made to generalize well by carefully\nchoosing an appropriate RF size within a certain range.", "published": "2019-07-03 09:06:42", "link": "http://arxiv.org/abs/1907.01803v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Case Study of Deep-Learned Activations via Hand-Crafted Audio Features", "abstract": "The explainability of Convolutional Neural Networks (CNNs) is a particularly\nchallenging task in all areas of application, and it is notably\nunder-researched in music and audio domain. In this paper, we approach\nexplainability by exploiting the knowledge we have on hand-crafted audio\nfeatures. Our study focuses on a well-defined MIR task, the recognition of\nmusical instruments from user-generated music recordings. We compute the\nsimilarity between a set of traditional audio features and representations\nlearned by CNNs. We also propose a technique for measuring the similarity\nbetween activation maps and audio features which typically presented in the\nform of a matrix, such as chromagrams or spectrograms. We observe that some\nneurons' activations correspond to well-known classical audio features. In\nparticular, for shallow layers, we found similarities between activations and\nharmonic and percussive components of the spectrum. For deeper layers, we\ncompare chromagrams with high-level activation maps as well as loudness and\nonset rate with deep-learned embeddings.", "published": "2019-07-03 09:32:42", "link": "http://arxiv.org/abs/1907.01813v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Convolutional Neural Network-based Speech Enhancement for Cochlear\n  Implant Recipients", "abstract": "Attempts to develop speech enhancement algorithms with improved speech\nintelligibility for cochlear implant (CI) users have met with limited success.\nTo improve speech enhancement methods for CI users, we propose to perform\nspeech enhancement in a cochlear filter-bank feature space, a feature-set\nspecifically designed for CI users based on CI auditory stimuli. We leverage a\nconvolutional neural network (CNN) to extract both stationary and\nnon-stationary components of environmental acoustics and speech. We propose\nthree CNN architectures: (1) vanilla CNN that directly generates the enhanced\nsignal; (2) spectral-subtraction-style CNN (SS-CNN) that first predicts noise\nand then generates the enhanced signal by subtracting noise from the noisy\nsignal; (3) Wiener-style CNN (Wiener-CNN) that generates an optimal mask for\nsuppressing noise. An important problem of the proposed networks is that they\nintroduce considerable delays, which limits their real-time application for CI\nusers. To address this, this study also considers causal variations of these\nnetworks. Our experiments show that the proposed networks (both causal and\nnon-causal forms) achieve significant improvement over existing baseline\nsystems. We also found that causal Wiener-CNN outperforms other networks, and\nleads to the best overall envelope coefficient measure (ECM). The proposed\nalgorithms represent a viable option for implementation on the CCi-MOBILE\nresearch platform as a pre-processor for CI users in naturalistic environments.", "published": "2019-07-03 21:25:21", "link": "http://arxiv.org/abs/1907.02526v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio-Based Search and Rescue with a Drone: Highlights from the IEEE\n  Signal Processing Cup 2019 Student Competition", "abstract": "Unmanned aerial vehicles (UAV), commonly referred to as drones, have raised\nincreasing interest in recent years. Search and rescue scenarios where humans\nin emergency situations need to be quickly found in areas difficult to access\nconstitute an important field of application for this technology. While\nresearch efforts have mostly focused on developing video-based solutions for\nthis task \\cite{lopez2017cvemergency}, UAV-embedded audio-based localization\nhas received relatively less attention. Though, UAVs equipped with a microphone\narray could be of critical help to localize people in emergency situations, in\nparticular when video sensors are limited by a lack of visual feedback due to\nbad lighting conditions or obstacles limiting the field of view. This motivated\nthe topic of the 6th edition of the IEEE Signal Processing Cup (SP Cup): a\nUAV-embedded sound source localization challenge for search and rescue. In this\narticle, we share an overview of the IEEE SP Cup experience including the\ncompetition tasks, participating teams, technical approaches and statistics.", "published": "2019-07-03 09:04:50", "link": "http://arxiv.org/abs/1907.04655v1", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
