{"title": "OntoSeg: a Novel Approach to Text Segmentation using Ontological\n  Similarity", "abstract": "Text segmentation (TS) aims at dividing long text into coherent segments\nwhich reflect the subtopic structure of the text. It is beneficial to many\nnatural language processing tasks, such as Information Retrieval (IR) and\ndocument summarisation. Current approaches to text segmentation are similar in\nthat they all use word-frequency metrics to measure the similarity between two\nregions of text, so that a document is segmented based on the lexical cohesion\nbetween its words. Various NLP tasks are now moving towards the semantic web\nand ontologies, such as ontology-based IR systems, to capture the\nconceptualizations associated with user needs and contents. Text segmentation\nbased on lexical cohesion between words is hence not sufficient anymore for\nsuch tasks. This paper proposes OntoSeg, a novel approach to text segmentation\nbased on the ontological similarity between text blocks. The proposed method\nuses ontological similarity to explore conceptual relations between text\nsegments and a Hierarchical Agglomerative Clustering (HAC) algorithm to\nrepresent the text as a tree-like hierarchy that is conceptually structured.\nThe rich structure of the created tree further allows the segmentation of text\nin a linear fashion at various levels of granularity. The proposed method was\nevaluated on a wellknown dataset, and the results show that using ontological\nsimilarity in text segmentation is very promising. Also we enhance the proposed\nmethod by combining ontological similarity with lexical similarity and the\nresults show an enhancement of the segmentation quality.", "published": "2015-11-26 15:10:18", "link": "http://arxiv.org/abs/1511.08411v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Mechanism of Additive Composition", "abstract": "Additive composition (Foltz et al, 1998; Landauer and Dumais, 1997; Mitchell\nand Lapata, 2010) is a widely used method for computing meanings of phrases,\nwhich takes the average of vector representations of the constituent words. In\nthis article, we prove an upper bound for the bias of additive composition,\nwhich is the first theoretical analysis on compositional frameworks from a\nmachine learning point of view. The bound is written in terms of collocation\nstrength; we prove that the more exclusively two successive words tend to occur\ntogether, the more accurate one can guarantee their additive composition as an\napproximation to the natural phrase vector. Our proof relies on properties of\nnatural language data that are empirically verified, and can be theoretically\nderived from an assumption that the data is generated from a Hierarchical\nPitman-Yor Process. The theory endorses additive composition as a reasonable\noperation for calculating meanings of phrases, and suggests ways to improve\nadditive compositionality, including: transforming entries of distributional\nword vectors by a function that meets a specific condition, constructing a\nnovel type of vector representations to make additive composition sensitive to\nword order, and utilizing singular value decomposition to train word vectors.", "published": "2015-11-26 14:58:17", "link": "http://arxiv.org/abs/1511.08407v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TGSum: Build Tweet Guided Multi-Document Summarization Dataset", "abstract": "The development of summarization research has been significantly hampered by\nthe costly acquisition of reference summaries. This paper proposes an effective\nway to automatically collect large scales of news-related multi-document\nsummaries with reference to social media's reactions. We utilize two types of\nsocial labels in tweets, i.e., hashtags and hyper-links. Hashtags are used to\ncluster documents into different topic sets. Also, a tweet with a hyper-link\noften highlights certain key points of the corresponding document. We\nsynthesize a linked document cluster to form a reference summary which can\ncover most key points. To this aim, we adopt the ROUGE metrics to measure the\ncoverage ratio, and develop an Integer Linear Programming solution to discover\nthe sentence set reaching the upper bound of ROUGE. Since we allow summary\nsentences to be selected from both documents and high-quality tweets, the\ngenerated reference summaries could be abstractive. Both informativeness and\nreadability of the collected summaries are verified by manual judgment. In\naddition, we train a Support Vector Regression summarizer on DUC generic\nmulti-document summarization benchmarks. With the collected data as extra\ntraining resource, the performance of the summarizer improves a lot on all the\ntest sets. We release this dataset for further research.", "published": "2015-11-26 15:22:54", "link": "http://arxiv.org/abs/1511.08417v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A Deep Architecture for Semantic Matching with Multiple Positional\n  Sentence Representations", "abstract": "Matching natural language sentences is central for many applications such as\ninformation retrieval and question answering. Existing deep models rely on a\nsingle sentence representation or multiple granularity representations for\nmatching. However, such methods cannot well capture the contextualized local\ninformation in the matching process. To tackle this problem, we present a new\ndeep architecture to match two sentences with multiple positional sentence\nrepresentations. Specifically, each positional sentence representation is a\nsentence representation at this position, generated by a bidirectional long\nshort term memory (Bi-LSTM). The matching score is finally produced by\naggregating interactions between these different positional sentence\nrepresentations, through $k$-Max pooling and a multi-layer perceptron. Our\nmodel has several advantages: (1) By using Bi-LSTM, rich context of the whole\nsentence is leveraged to capture the contextualized local information in each\npositional sentence representation; (2) By matching with multiple positional\nsentence representations, it is flexible to aggregate different important\ncontextualized local information in a sentence to support the matching; (3)\nExperiments on different tasks such as question answering and sentence\ncompletion demonstrate the superiority of our model.", "published": "2015-11-26 02:57:54", "link": "http://arxiv.org/abs/1511.08277v1", "categories": ["cs.AI", "cs.CL", "cs.NE"], "primary_category": "cs.AI"}
{"title": "Hierarchical classification of e-commerce related social media", "abstract": "In this paper, we attempt to classify tweets into root categories of the\nAmazon browse node hierarchy using a set of tweets with browse node ID labels,\na much larger set of tweets without labels, and a set of Amazon reviews.\nExamining twitter data presents unique challenges in that the samples are short\n(under 140 characters) and often contain misspellings or abbreviations that are\ntrivial for a human to decipher but difficult for a computer to parse. A\nvariety of query and document expansion techniques are implemented in an effort\nto improve information retrieval to modest success.", "published": "2015-11-26 06:57:06", "link": "http://arxiv.org/abs/1511.08299v1", "categories": ["cs.SI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Named Entity Recognition with Bidirectional LSTM-CNNs", "abstract": "Named entity recognition is a challenging task that has traditionally\nrequired large amounts of knowledge in the form of feature engineering and\nlexicons to achieve high performance. In this paper, we present a novel neural\nnetwork architecture that automatically detects word- and character-level\nfeatures using a hybrid bidirectional LSTM and CNN architecture, eliminating\nthe need for most feature engineering. We also propose a novel method of\nencoding partial lexicon matches in neural networks and compare it to existing\napproaches. Extensive evaluation shows that, given only tokenized text and\npublicly available word embeddings, our system is competitive on the CoNLL-2003\ndataset and surpasses the previously reported state of the art performance on\nthe OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed\nfrom publicly-available sources, we establish new state of the art performance\nwith an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing\nsystems that employ heavy feature engineering, proprietary lexicons, and rich\nentity linking information.", "published": "2015-11-26 07:40:33", "link": "http://arxiv.org/abs/1511.08308v5", "categories": ["cs.CL", "cs.LG", "cs.NE", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Regularizing RNNs by Stabilizing Activations", "abstract": "We stabilize the activations of Recurrent Neural Networks (RNNs) by\npenalizing the squared distance between successive hidden states' norms.\n  This penalty term is an effective regularizer for RNNs including LSTMs and\nIRNNs, improving performance on character-level language modeling and phoneme\nrecognition, and outperforming weight noise and dropout.\n  We achieve competitive performance (18.6\\% PER) on the TIMIT phoneme\nrecognition task for RNNs evaluated without beam search or an RNN transducer.\n  With this penalty term, IRNN can achieve similar performance to LSTM on\nlanguage modeling, although adding the penalty term to the LSTM results in\nsuperior performance.\n  Our penalty term also prevents the exponential growth of IRNN's activations\noutside of their training horizon, allowing them to generalize to much longer\nsequences.", "published": "2015-11-26 14:35:27", "link": "http://arxiv.org/abs/1511.08400v7", "categories": ["cs.NE", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.NE"}
