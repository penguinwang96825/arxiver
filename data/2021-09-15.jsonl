{"title": "A Conditional Generative Matching Model for Multi-lingual Reply\n  Suggestion", "abstract": "We study the problem of multilingual automated reply suggestions (RS) model\nserving many languages simultaneously. Multilingual models are often challenged\nby model capacity and severe data distribution skew across languages. While\nprior works largely focus on monolingual models, we propose Conditional\nGenerative Matching models (CGM), optimized within a Variational Autoencoder\nframework to address challenges arising from multi-lingual RS. CGM does so with\nexpressive message conditional priors, mixture densities to enhance\nmulti-lingual data representation, latent alignment for language\ndiscrimination, and effective variational optimization techniques for training\nmulti-lingual RS. The enhancements result in performance that exceed\ncompetitive baselines in relevance (ROUGE score) by more than 10\\% on average,\nand 16\\% for low resource languages. CGM also shows remarkable improvements in\ndiversity (80\\%) illustrating its expressiveness in representation of\nmulti-lingual data.", "published": "2021-09-15 01:54:41", "link": "http://arxiv.org/abs/2109.07046v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ARCH: Efficient Adversarial Regularized Training with Caching", "abstract": "Adversarial regularization can improve model generalization in many natural\nlanguage processing tasks. However, conventional approaches are computationally\nexpensive since they need to generate a perturbation for each sample in each\nepoch. We propose a new adversarial regularization method ARCH (adversarial\nregularization with caching), where perturbations are generated and cached once\nevery several epochs. As caching all the perturbations imposes memory usage\nconcerns, we adopt a K-nearest neighbors-based strategy to tackle this issue.\nThe strategy only requires caching a small amount of perturbations, without\nintroducing additional training time. We evaluate our proposed method on a set\nof neural machine translation and natural language understanding tasks. We\nobserve that ARCH significantly eases the computational burden (saves up to 70%\nof computational time in comparison with conventional approaches). More\nsurprisingly, by reducing the variance of stochastic gradients, ARCH produces a\nnotably better (in most of the tasks) or comparable model generalization. Our\ncode is available at https://github.com/SimiaoZuo/Caching-Adv.", "published": "2021-09-15 02:05:37", "link": "http://arxiv.org/abs/2109.07048v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Text Auto-Completion with Next Phrase Prediction", "abstract": "Language models such as GPT-2 have performed well on constructing\nsyntactically sound sentences for text auto-completion task. However, such\nmodels often require considerable training effort to adapt to specific writing\ndomains (e.g., medical). In this paper, we propose an intermediate training\nstrategy to enhance pre-trained language models' performance in the text\nauto-completion task and fastly adapt them to specific domains. Our strategy\nincludes a novel self-supervised training objective called Next Phrase\nPrediction (NPP), which encourages a language model to complete the partial\nquery with enriched phrases and eventually improve the model's text\nauto-completion performance. Preliminary experiments have shown that our\napproach is able to outperform the baselines in auto-completion for email and\nacademic writing domains.", "published": "2021-09-15 04:26:15", "link": "http://arxiv.org/abs/2109.07067v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformer-based Lexically Constrained Headline Generation", "abstract": "This paper explores a variant of automatic headline generation methods, where\na generated headline is required to include a given phrase such as a company or\na product name. Previous methods using Transformer-based models generate a\nheadline including a given phrase by providing the encoder with additional\ninformation corresponding to the given phrase. However, these methods cannot\nalways include the phrase in the generated headline. Inspired by previous\nRNN-based methods generating token sequences in backward and forward directions\nfrom the given phrase, we propose a simple Transformer-based method that\nguarantees to include the given phrase in the high-quality generated headline.\nWe also consider a new headline generation strategy that takes advantage of the\ncontrollable generation order of Transformer. Our experiments with the Japanese\nNews Corpus demonstrate that our methods, which are guaranteed to include the\nphrase in the generated headline, achieve ROUGE scores comparable to previous\nTransformer-based methods. We also show that our generation strategy performs\nbetter than previous strategies.", "published": "2021-09-15 05:09:46", "link": "http://arxiv.org/abs/2109.07080v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Edge Probing Tasks Reveal Linguistic Knowledge in QA Models?", "abstract": "There have been many efforts to try to understand what grammatical knowledge\n(e.g., ability to understand the part of speech of a token) is encoded in large\npre-trained language models (LM). This is done through `Edge Probing' (EP)\ntests: supervised classification tasks to predict the grammatical properties of\na span (whether it has a particular part of speech) using only the token\nrepresentations coming from the LM encoder. However, most NLP applications\nfine-tune these LM encoders for specific tasks. Here, we ask: if an LM is\nfine-tuned, does the encoding of linguistic information in it change, as\nmeasured by EP tests? Specifically, we focus on the task of Question Answering\n(QA) and conduct experiments on multiple datasets. We find that EP test results\ndo not change significantly when the fine-tuned model performs well or in\nadversarial situations where the model is forced to learn wrong correlations.\nFrom a similar finding, some recent papers conclude that fine-tuning does not\nchange linguistic knowledge in encoders but they do not provide an explanation.\nWe find that EP models themselves are susceptible to exploiting spurious\ncorrelations in the EP datasets. When this dataset bias is corrected, we do see\nan improvement in the EP test results as expected.", "published": "2021-09-15 06:16:12", "link": "http://arxiv.org/abs/2109.07102v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Universality of Deep Contextual Language Models", "abstract": "Deep Contextual Language Models (LMs) like ELMO, BERT, and their successors\ndominate the landscape of Natural Language Processing due to their ability to\nscale across multiple tasks rapidly by pre-training a single model, followed by\ntask-specific fine-tuning. Furthermore, multilingual versions of such models\nlike XLM-R and mBERT have given promising results in zero-shot cross-lingual\ntransfer, potentially enabling NLP applications in many under-served and\nunder-resourced languages. Due to this initial success, pre-trained models are\nbeing used as `Universal Language Models' as the starting point across diverse\ntasks, domains, and languages. This work explores the notion of `Universality'\nby identifying seven dimensions across which a universal model should be able\nto scale, that is, perform equally well or reasonably well, to be useful across\ndiverse settings. We outline the current theoretical and empirical results that\nsupport model performance across these dimensions, along with extensions that\nmay help address some of their current limitations. Through this survey, we lay\nthe foundation for understanding the capabilities and limitations of massive\ncontextual language models and help discern research gaps and directions for\nfuture work to make these LMs inclusive and fair to diverse applications,\nusers, and linguistic phenomena.", "published": "2021-09-15 08:00:33", "link": "http://arxiv.org/abs/2109.07140v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantics of European poetry is shaped by conservative forces: The\n  relationship between poetic meter and meaning in accentual-syllabic verse", "abstract": "Recent advances in cultural analytics and large-scale computational studies\nof art, literature and film often show that long-term change in the features of\nartistic works happens gradually. These findings suggest that conservative\nforces that shape creative domains might be underestimated. To this end, we\nprovide the first large-scale formal evidence of the persistent association\nbetween poetic meter and semantics in 18-19th European literatures, using\nCzech, German and Russian collections with additional data from English poetry\nand early modern Dutch songs. Our study traces this association through a\nseries of clustering experiments using the abstracted semantic features of\n150,000 poems. With the aid of topic modeling we infer semantic features for\nindividual poems. Texts were also lexically simplified across collections to\nincrease generalizability and decrease the sparseness of word frequency\ndistributions. Topics alone enable recognition of the meters in each observed\nlanguage, as may be seen from highly robust clustering of same-meter samples\n(median Adjusted Rand Index between 0.48 and 1). In addition, this study shows\nthat the strength of the association between form and meaning tends to decrease\nover time. This may reflect a shift in aesthetic conventions between the 18th\nand 19th centuries as individual innovation was increasingly favored in\nliterature. Despite this decline, it remains possible to recognize semantics of\nthe meters from past or future, which suggests the continuity of semantic\ntraditions while also revealing the historical variability of conditions across\nlanguages. This paper argues that distinct metrical forms, which are often\ncopied in a language over centuries, also maintain long-term semantic inertia\nin poetry. Our findings, thus, highlight the role of the formal features of\ncultural items in influencing the pace and shape of cultural evolution.", "published": "2021-09-15 08:20:01", "link": "http://arxiv.org/abs/2109.07148v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incorporating Residual and Normalization Layers into Analysis of Masked\n  Language Models", "abstract": "Transformer architecture has become ubiquitous in the natural language\nprocessing field. To interpret the Transformer-based models, their attention\npatterns have been extensively analyzed. However, the Transformer architecture\nis not only composed of the multi-head attention; other components can also\ncontribute to Transformers' progressive performance. In this study, we extended\nthe scope of the analysis of Transformers from solely the attention patterns to\nthe whole attention block, i.e., multi-head attention, residual connection, and\nlayer normalization. Our analysis of Transformer-based masked language models\nshows that the token-to-token interaction performed via attention has less\nimpact on the intermediate representations than previously assumed. These\nresults provide new intuitive explanations of existing reports; for example,\ndiscarding the learned attention patterns tends not to adversely affect the\nperformance. The codes of our experiments are publicly available.", "published": "2021-09-15 08:32:20", "link": "http://arxiv.org/abs/2109.07152v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Language Models be Biomedical Knowledge Bases?", "abstract": "Pre-trained language models (LMs) have become ubiquitous in solving various\nnatural language processing (NLP) tasks. There has been increasing interest in\nwhat knowledge these LMs contain and how we can extract that knowledge,\ntreating LMs as knowledge bases (KBs). While there has been much work on\nprobing LMs in the general domain, there has been little attention to whether\nthese powerful LMs can be used as domain-specific KBs. To this end, we create\nthe BioLAMA benchmark, which is comprised of 49K biomedical factual knowledge\ntriples for probing biomedical LMs. We find that biomedical LMs with recently\nproposed probing methods can achieve up to 18.51% Acc@5 on retrieving\nbiomedical knowledge. Although this seems promising given the task difficulty,\nour detailed analyses reveal that most predictions are highly correlated with\nprompt templates without any subjects, hence producing similar results on each\nrelation and hindering their capabilities to be used as domain-specific KBs. We\nhope that BioLAMA can serve as a challenging benchmark for biomedical factual\nprobing.", "published": "2021-09-15 08:34:56", "link": "http://arxiv.org/abs/2109.07154v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Disentangling Generative Factors in Natural Language with Discrete\n  Variational Autoencoders", "abstract": "The ability of learning disentangled representations represents a major step\nfor interpretable NLP systems as it allows latent linguistic features to be\ncontrolled. Most approaches to disentanglement rely on continuous variables,\nboth for images and text. We argue that despite being suitable for image\ndatasets, continuous variables may not be ideal to model features of textual\ndata, due to the fact that most generative factors in text are discrete. We\npropose a Variational Autoencoder based method which models language features\nas discrete variables and encourages independence between variables for\nlearning disentangled representations. The proposed model outperforms\ncontinuous and discrete baselines on several qualitative and quantitative\nbenchmarks for disentanglement as well as on a text style transfer downstream\napplication.", "published": "2021-09-15 09:10:05", "link": "http://arxiv.org/abs/2109.07169v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformer-based Language Models for Factoid Question Answering at\n  BioASQ9b", "abstract": "In this work, we describe our experiments and participating systems in the\nBioASQ Task 9b Phase B challenge of biomedical question answering. We have\nfocused on finding the ideal answers and investigated multi-task fine-tuning\nand gradual unfreezing techniques on transformer-based language models. For\nfactoid questions, our ALBERT-based systems ranked first in test batch 1 and\nfourth in test batch 2. Our DistilBERT systems outperformed the ALBERT variants\nin test batches 4 and 5 despite having 81% fewer parameters than ALBERT.\nHowever, we observed that gradual unfreezing had no significant impact on the\nmodel's accuracy compared to standard fine-tuning.", "published": "2021-09-15 10:01:06", "link": "http://arxiv.org/abs/2109.07185v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis in Poems in Misurata Sub-dialect -- A Sentiment\n  Detection in an Arabic Sub-dialect", "abstract": "Over the recent decades, there has been a significant increase and\ndevelopment of resources for Arabic natural language processing. This includes\nthe task of exploring Arabic Language Sentiment Analysis (ALSA) from Arabic\nutterances in both Modern Standard Arabic (MSA) and different Arabic dialects.\nThis study focuses on detecting sentiment in poems written in Misurata Arabic\nsub-dialect spoken in Misurata, Libya. The tools used to detect sentiment from\nthe dataset are Sklearn as well as Mazajak sentiment tool 1. Logistic\nRegression, Random Forest, Naive Bayes (NB), and Support Vector Machines (SVM)\nclassifiers are used with Sklearn, while the Convolutional Neural Network (CNN)\nis implemented with Mazajak. The results show that the traditional classifiers\nscore a higher level of accuracy as compared to Mazajak which is built on an\nalgorithm that includes deep learning techniques. More research is suggested to\nanalyze Arabic sub-dialect poetry in order to investigate the aspects that\ncontribute to sentiments in these multi-line texts; for example, the use of\nfigurative language such as metaphors.", "published": "2021-09-15 10:42:39", "link": "http://arxiv.org/abs/2109.07203v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Much do Lyrics Matter? Analysing Lyrical Simplicity Preferences for\n  Individuals At Risk of Depression", "abstract": "Music affects and in some cases reflects one's emotional state. Key to this\ninfluence is lyrics and their meaning in conjunction with the acoustic\nproperties of the track. Recent work has focused on analysing these acoustic\nproperties and showing that individuals prone to depression primarily consume\nlow valence and low energy music. However, no studies yet have explored lyrical\ncontent preferences in relation to online music consumption of such\nindividuals. In the current study, we examine lyrical simplicity, measured as\nthe Compressibility and Absolute Information Content of the text, associated\nwith preferences of individuals at risk for depression. Using the six-month\nlistening history of 541 Last.fm users, we compare lyrical simplicity trends\nfor users grouped as being at risk (At-Risk) of depression from those that are\nnot (No-Risk). Our findings reveal that At-Risk individuals prefer songs with\ngreater information content (lower Compressibility) on average, especially for\nsongs characterised as Sad. Furthermore, we found that At-Risk individuals also\nhave greater variability of Absolute Information Content across their listening\nhistory. We discuss the results in light of existing socio-psychological\nlab-based research on music habits associated with depression and their\nrelevance to naturally occurring online music listening behaviour.", "published": "2021-09-15 11:41:20", "link": "http://arxiv.org/abs/2109.07227v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SWEAT: Scoring Polarization of Topics across Different Corpora", "abstract": "Understanding differences of viewpoints across corpora is a fundamental task\nfor computational social sciences. In this paper, we propose the Sliced Word\nEmbedding Association Test (SWEAT), a novel statistical measure to compute the\nrelative polarization of a topical wordset across two distributional\nrepresentations. To this end, SWEAT uses two additional wordsets, deemed to\nhave opposite valence, to represent two different poles. We validate our\napproach and illustrate a case study to show the usefulness of the introduced\nmeasure.", "published": "2021-09-15 11:57:21", "link": "http://arxiv.org/abs/2109.07231v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Unreasonable Effectiveness of the Baseline: Discussing SVMs in Legal\n  Text Classification", "abstract": "We aim to highlight an interesting trend to contribute to the ongoing debate\naround advances within legal Natural Language Processing. Recently, the focus\nfor most legal text classification tasks has shifted towards large pre-trained\ndeep learning models such as BERT. In this paper, we show that a more\ntraditional approach based on Support Vector Machine classifiers reaches\nsurprisingly competitive performance with BERT-based models on the\nclassification tasks in the LexGLUE benchmark. We also highlight that error\nreduction obtained by using specialised BERT-based models over baselines is\nnoticeably smaller in the legal domain when compared to general language tasks.\nWe present and discuss three hypotheses as potential explanations for these\nresults to support future discussions.", "published": "2021-09-15 12:05:28", "link": "http://arxiv.org/abs/2109.07234v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Regressive Ensemble for Machine Translation Quality Evaluation", "abstract": "This work introduces a simple regressive ensemble for evaluating machine\ntranslation quality based on a set of novel and established metrics. We\nevaluate the ensemble using a correlation to expert-based MQM scores of the WMT\n2021 Metrics workshop. In both monolingual and zero-shot cross-lingual\nsettings, we show a significant performance improvement over single metrics. In\nthe cross-lingual settings, we also demonstrate that an ensemble approach is\nwell-applicable to unseen languages. Furthermore, we identify a strong\nreference-free baseline that consistently outperforms the commonly-used BLEU\nand METEOR measures and significantly improves our ensemble's performance.", "published": "2021-09-15 12:22:52", "link": "http://arxiv.org/abs/2109.07242v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Clinical Information Extraction with Transferred Contextual\n  Embeddings", "abstract": "The Bidirectional Encoder Representations from Transformers (BERT) model has\nachieved the state-of-the-art performance for many natural language processing\n(NLP) tasks. Yet, limited research has been contributed to studying its\neffectiveness when the target domain is shifted from the pre-training corpora,\nfor example, for biomedical or clinical NLP applications. In this paper, we\napplied it to a widely studied a hospital information extraction (IE) task and\nanalyzed its performance under the transfer learning setting. Our application\nbecame the new state-of-the-art result by a clear margin, compared with a range\nof existing IE models. Specifically, on this nursing handover data set, the\nmacro-average F1 score from our model was 0.438, whilst the previous best deep\nlearning models had 0.416. In conclusion, we showed that BERT based\npre-training models can be transferred to health-related documents under mild\nconditions and with a proper fine-tuning process.", "published": "2021-09-15 12:22:57", "link": "http://arxiv.org/abs/2109.07243v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scope resolution of predicted negation cues: A two-step neural\n  network-based approach", "abstract": "Neural network-based methods are the state of the art in negation scope\nresolution. However, they often use the unrealistic assumption that cue\ninformation is completely accurate. Even if this assumption holds, there\nremains a dependency on engineered features from state-of-the-art machine\nlearning methods. The current study adopted a two-step negation resolving\napporach to assess whether a Bidirectional Long Short-Term Memory-based method\ncan be used for cue detection as well, and how inaccurate cue predictions would\naffect the scope resolution performance. Results suggest that this method is\nnot suitable for negation detection. Scope resolution performance is most\nrobust against inaccurate information for models with a recurrent layer only,\ncompared to extensions with a Conditional Random Fields layer or a\npost-processing algorithm. We advocate for more research into the application\nof deep learning on negation detection and the effect of imperfect information\non scope resolution.", "published": "2021-09-15 13:03:09", "link": "http://arxiv.org/abs/2109.07264v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sequence Length is a Domain: Length-based Overfitting in Transformer\n  Models", "abstract": "Transformer-based sequence-to-sequence architectures, while achieving\nstate-of-the-art results on a large number of NLP tasks, can still suffer from\noverfitting during training. In practice, this is usually countered either by\napplying regularization methods (e.g. dropout, L2-regularization) or by\nproviding huge amounts of training data. Additionally, Transformer and other\narchitectures are known to struggle when generating very long sequences. For\nexample, in machine translation, the neural-based systems perform worse on very\nlong sequences when compared to the preceding phrase-based translation\napproaches (Koehn and Knowles, 2017).\n  We present results which suggest that the issue might also be in the mismatch\nbetween the length distributions of the training and validation data combined\nwith the aforementioned tendency of the neural networks to overfit to the\ntraining data. We demonstrate on a simple string editing task and a machine\ntranslation task that the Transformer model performance drops significantly\nwhen facing sequences of length diverging from the length distribution in the\ntraining data. Additionally, we show that the observed drop in performance is\ndue to the hypothesis length corresponding to the lengths seen by the model\nduring training rather than the length of the input sequence.", "published": "2021-09-15 13:25:19", "link": "http://arxiv.org/abs/2109.07276v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Keyphrase Extraction by Jointly Modeling Local and Global\n  Context", "abstract": "Embedding based methods are widely used for unsupervised keyphrase extraction\n(UKE) tasks. Generally, these methods simply calculate similarities between\nphrase embeddings and document embedding, which is insufficient to capture\ndifferent context for a more effective UKE model. In this paper, we propose a\nnovel method for UKE, where local and global contexts are jointly modeled. From\na global view, we calculate the similarity between a certain phrase and the\nwhole document in the vector space as transitional embedding based models do.\nIn terms of the local view, we first build a graph structure based on the\ndocument where phrases are regarded as vertices and the edges are similarities\nbetween vertices. Then, we proposed a new centrality computation method to\ncapture local salient information based on the graph structure. Finally, we\nfurther combine the modeling of global and local context for ranking. We\nevaluate our models on three public benchmarks (Inspec, DUC 2001, SemEval 2010)\nand compare with existing state-of-the-art models. The results show that our\nmodel outperforms most models while generalizing better on input documents with\ndifferent domains and length. Additional ablation study shows that both the\nlocal and global information is crucial for unsupervised keyphrase extraction\ntasks.", "published": "2021-09-15 13:41:10", "link": "http://arxiv.org/abs/2109.07293v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Vision-Language Models `See' when they See Scenes", "abstract": "Images can be described in terms of the objects they contain, or in terms of\nthe types of scene or place that they instantiate. In this paper we address to\nwhat extent pretrained Vision and Language models can learn to align\ndescriptions of both types with images. We compare 3 state-of-the-art models,\nVisualBERT, LXMERT and CLIP. We find that (i) V&L models are susceptible to\nstylistic biases acquired during pretraining; (ii) only CLIP performs\nconsistently well on both object- and scene-level descriptions. A follow-up\nablation study shows that CLIP uses object-level information in the visual\nmodality to align with scene-level textual descriptions.", "published": "2021-09-15 13:57:39", "link": "http://arxiv.org/abs/2109.07301v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Allocating Large Vocabulary Capacity for Cross-lingual Language Model\n  Pre-training", "abstract": "Compared to monolingual models, cross-lingual models usually require a more\nexpressive vocabulary to represent all languages adequately. We find that many\nlanguages are under-represented in recent cross-lingual language models due to\nthe limited vocabulary capacity. To this end, we propose an algorithm VoCap to\ndetermine the desired vocabulary capacity of each language. However, increasing\nthe vocabulary size significantly slows down the pre-training speed. In order\nto address the issues, we propose k-NN-based target sampling to accelerate the\nexpensive softmax. Our experiments show that the multilingual vocabulary\nlearned with VoCap benefits cross-lingual language model pre-training.\nMoreover, k-NN-based target sampling mitigates the side-effects of increasing\nthe vocabulary size while achieving comparable performance and faster\npre-training speed. The code and the pretrained multilingual vocabularies are\navailable at https://github.com/bozheng-hit/VoCapXLM.", "published": "2021-09-15 14:04:16", "link": "http://arxiv.org/abs/2109.07306v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mi\u00f0eind's WMT 2021 submission", "abstract": "We present Mi{\\dh}eind's submission for the English$\\to$Icelandic and\nIcelandic$\\to$English subsets of the 2021 WMT news translation task.\nTransformer-base models are trained for translation on parallel data to\ngenerate backtranslations iteratively. A pretrained mBART-25 model is then\nadapted for translation using parallel data as well as the last backtranslation\niteration. This adapted pretrained model is then used to re-generate\nbacktranslations, and the training of the adapted model is continued.", "published": "2021-09-15 14:56:00", "link": "http://arxiv.org/abs/2109.07343v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Introducing an Abusive Language Classification Framework for Telegram to\n  Investigate the German Hater Community", "abstract": "Since traditional social media platforms continue to ban actors spreading\nhate speech or other forms of abusive languages (a process known as\ndeplatforming), these actors migrate to alternative platforms that do not\nmoderate users content. One popular platform relevant for the German hater\ncommunity is Telegram for which limited research efforts have been made so far.\nThis study aims to develop a broad framework comprising (i) an abusive language\nclassification model for German Telegram messages and (ii) a classification\nmodel for the hatefulness of Telegram channels. For the first part, we use\nexisting abusive language datasets containing posts from other platforms to\ndevelop our classification models. For the channel classification model, we\ndevelop a method that combines channel-specific content information collected\nfrom a topic model with a social graph to predict the hatefulness of channels.\nFurthermore, we complement these two approaches for hate speech detection with\ninsightful results on the evolution of the hater community on Telegram in\nGermany. We also propose methods for conducting scalable network analyses for\nsocial media platforms to the hate speech research community. As an additional\noutput of this study, we provide an annotated abusive language dataset\ncontaining 1,149 annotated Telegram messages.", "published": "2021-09-15 14:58:46", "link": "http://arxiv.org/abs/2109.07346v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The ELITR ECA Corpus", "abstract": "We present the ELITR ECA corpus, a multilingual corpus derived from\npublications of the European Court of Auditors. We use automatic translation\ntogether with Bleualign to identify parallel sentence pairs in all 506\ntranslation directions. The result is a corpus comprising 264k document pairs\nand 41.9M sentence pairs.", "published": "2021-09-15 15:03:27", "link": "http://arxiv.org/abs/2109.07351v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Incremental Transformers: An Empirical Analysis of Transformer\n  Models for Incremental NLU", "abstract": "Incremental processing allows interactive systems to respond based on partial\ninputs, which is a desirable property e.g. in dialogue agents. The currently\npopular Transformer architecture inherently processes sequences as a whole,\nabstracting away the notion of time. Recent work attempts to apply Transformers\nincrementally via restart-incrementality by repeatedly feeding, to an unchanged\nmodel, increasingly longer input prefixes to produce partial outputs. However,\nthis approach is computationally costly and does not scale efficiently for long\nsequences. In parallel, we witness efforts to make Transformers more efficient,\ne.g. the Linear Transformer (LT) with a recurrence mechanism. In this work, we\nexamine the feasibility of LT for incremental NLU in English. Our results show\nthat the recurrent LT model has better incremental performance and faster\ninference speed compared to the standard Transformer and LT with\nrestart-incrementality, at the cost of part of the non-incremental (full\nsequence) quality. We show that the performance drop can be mitigated by\ntraining the model to wait for right context before committing to an output and\nthat training with input prefixes is beneficial for delivering correct partial\noutputs.", "published": "2021-09-15 15:20:29", "link": "http://arxiv.org/abs/2109.07364v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RankNAS: Efficient Neural Architecture Search by Pairwise Ranking", "abstract": "This paper addresses the efficiency challenge of Neural Architecture Search\n(NAS) by formulating the task as a ranking problem. Previous methods require\nnumerous training examples to estimate the accurate performance of\narchitectures, although the actual goal is to find the distinction between\n\"good\" and \"bad\" candidates. Here we do not resort to performance predictors.\nInstead, we propose a performance ranking method (RankNAS) via pairwise\nranking. It enables efficient architecture search using much fewer training\nexamples. Moreover, we develop an architecture selection method to prune the\nsearch space and concentrate on more promising candidates. Extensive\nexperiments on machine translation and language modeling tasks show that\nRankNAS can design high-performance architectures while being orders of\nmagnitude faster than state-of-the-art NAS systems.", "published": "2021-09-15 15:43:08", "link": "http://arxiv.org/abs/2109.07383v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERT is Robust! A Case Against Synonym-Based Adversarial Examples in\n  Text Classification", "abstract": "Deep Neural Networks have taken Natural Language Processing by storm. While\nthis led to incredible improvements across many tasks, it also initiated a new\nresearch field, questioning the robustness of these neural networks by\nattacking them. In this paper, we investigate four word substitution-based\nattacks on BERT. We combine a human evaluation of individual word substitutions\nand a probabilistic analysis to show that between 96% and 99% of the analyzed\nattacks do not preserve semantics, indicating that their success is mainly\nbased on feeding poor data to the model. To further confirm that, we introduce\nan efficient data augmentation procedure and show that many adversarial\nexamples can be prevented by including data similar to the attacks during\ntraining. An additional post-processing step reduces the success rates of\nstate-of-the-art attacks below 5%. Finally, by looking at more reasonable\nthresholds on constraints for word substitutions, we conclude that BERT is a\nlot more robust than research on attacks suggests.", "published": "2021-09-15 16:15:16", "link": "http://arxiv.org/abs/2109.07403v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is \"moby dick\" a Whale or a Bird? Named Entities and Terminology in\n  Speech Translation", "abstract": "Automatic translation systems are known to struggle with rare words. Among\nthese, named entities (NEs) and domain-specific terms are crucial, since errors\nin their translation can lead to severe meaning distortions. Despite their\nimportance, previous speech translation (ST) studies have neglected them, also\ndue to the dearth of publicly available resources tailored to their specific\nevaluation. To fill this gap, we i) present the first systematic analysis of\nthe behavior of state-of-the-art ST systems in translating NEs and terminology,\nand ii) release NEuRoparl-ST, a novel benchmark built from European Parliament\nspeeches annotated with NEs and terminology. Our experiments on the three\nlanguage directions covered by our benchmark (en->es/fr/it) show that ST\nsystems correctly translate 75-80% of terms and 65-70% of NEs, with very low\nperformance (37-40%) on person names.", "published": "2021-09-15 17:15:31", "link": "http://arxiv.org/abs/2109.07439v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WikiGUM: Exhaustive Entity Linking for Wikification in 12 Genres", "abstract": "Previous work on Entity Linking has focused on resources targeting non-nested\nproper named entity mentions, often in data from Wikipedia, i.e. Wikification.\nIn this paper, we present and evaluate WikiGUM, a fully wikified dataset,\ncovering all mentions of named entities, including their non-named and\npronominal mentions, as well as mentions nested within other mentions. The\ndataset covers a broad range of 12 written and spoken genres, most of which\nhave not been included in Entity Linking efforts to date, leading to poor\nperformance by a pretrained SOTA system in our evaluation. The availability of\na variety of other annotations for the same data also enables further research\non entities in context.", "published": "2021-09-15 17:35:24", "link": "http://arxiv.org/abs/2109.07449v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Domain Adaptation of Language Models via Adaptive Tokenization", "abstract": "Contextual embedding-based language models trained on large data sets, such\nas BERT and RoBERTa, provide strong performance across a wide range of tasks\nand are ubiquitous in modern NLP. It has been observed that fine-tuning these\nmodels on tasks involving data from domains different from that on which they\nwere pretrained can lead to suboptimal performance. Recent work has explored\napproaches to adapt pretrained language models to new domains by incorporating\nadditional pretraining using domain-specific corpora and task data. We propose\nan alternative approach for transferring pretrained language models to new\ndomains by adapting their tokenizers. We show that domain-specific subword\nsequences can be efficiently determined directly from divergences in the\nconditional token distributions of the base and domain-specific corpora. In\ndatasets from four disparate domains, we find adaptive tokenization on a\npretrained RoBERTa model provides >97% of the performance benefits of domain\nspecific pretraining. Our approach produces smaller models and less training\nand inference time than other approaches using tokenizer augmentation. While\nadaptive tokenization incurs a 6% increase in model parameters in our\nexperimentation, due to the introduction of 10k new domain-specific tokens, our\napproach, using 64 vCPUs, is 72x faster than further pretraining the language\nmodel on domain-specific corpora on 8 TPUs.", "published": "2021-09-15 17:51:27", "link": "http://arxiv.org/abs/2109.07460v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AnnIE: An Annotation Platform for Constructing Complete Open Information\n  Extraction Benchmark", "abstract": "Open Information Extraction (OIE) is the task of extracting facts from\nsentences in the form of relations and their corresponding arguments in\nschema-free manner. Intrinsic performance of OIE systems is difficult to\nmeasure due to the incompleteness of existing OIE benchmarks: the ground truth\nextractions do not group all acceptable surface realizations of the same fact\nthat can be extracted from a sentence. To measure performance of OIE systems\nmore realistically, it is necessary to manually annotate complete facts (i.e.,\nclusters of all acceptable surface realizations of the same fact) from input\nsentences. We propose AnnIE: an interactive annotation platform that\nfacilitates such challenging annotation tasks and supports creation of complete\nfact-oriented OIE evaluation benchmarks. AnnIE is modular and flexible in order\nto support different use case scenarios (i.e., benchmarks covering different\ntypes of facts). We use AnnIE to build two complete OIE benchmarks: one with\nverb-mediated facts and another with facts encompassing named entities.\nFinally, we evaluate several OIE systems on our complete benchmarks created\nwith AnnIE. Our results suggest that existing incomplete benchmarks are overly\nlenient, and that OIE systems are not as robust as previously reported. We\npublicly release AnnIE under non-restrictive license.", "published": "2021-09-15 17:57:30", "link": "http://arxiv.org/abs/2109.07464v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Limits of Minimal Pairs in Contrastive Evaluation", "abstract": "Minimal sentence pairs are frequently used to analyze the behavior of\nlanguage models. It is often assumed that model behavior on contrastive pairs\nis predictive of model behavior at large. We argue that two conditions are\nnecessary for this assumption to hold: First, a tested hypothesis should be\nwell-motivated, since experiments show that contrastive evaluation can lead to\nfalse positives. Secondly, test data should be chosen such as to minimize\ndistributional discrepancy between evaluation time and deployment time. For a\ngood approximation of deployment-time decoding, we recommend that minimal pairs\nare created based on machine-generated text, as opposed to human-written\nreferences. We present a contrastive evaluation suite for English-German MT\nthat implements this recommendation.", "published": "2021-09-15 17:59:15", "link": "http://arxiv.org/abs/2109.07465v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Making Heads and Tails of Models with Marginal Calibration for Sparse\n  Tagsets", "abstract": "For interpreting the behavior of a probabilistic model, it is useful to\nmeasure a model's calibration--the extent to which it produces reliable\nconfidence scores. We address the open problem of calibration for tagging\nmodels with sparse tagsets, and recommend strategies to measure and reduce\ncalibration error (CE) in such models. We show that several post-hoc\nrecalibration techniques all reduce calibration error across the marginal\ndistribution for two existing sequence taggers. Moreover, we propose tag\nfrequency grouping (TFG) as a way to measure calibration error in different\nfrequency bands. Further, recalibrating each group separately promotes a more\nequitable reduction of calibration error across the tag frequency spectrum.", "published": "2021-09-15 18:00:37", "link": "http://arxiv.org/abs/2109.07494v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dialogue State Tracking with a Language Model using Schema-Driven\n  Prompting", "abstract": "Task-oriented conversational systems often use dialogue state tracking to\nrepresent the user's intentions, which involves filling in values of\npre-defined slots. Many approaches have been proposed, often using\ntask-specific architectures with special-purpose classifiers. Recently, good\nresults have been obtained using more general architectures based on pretrained\nlanguage models. Here, we introduce a new variation of the language modeling\napproach that uses schema-driven prompting to provide task-aware history\nencoding that is used for both categorical and non-categorical slots. We\nfurther improve performance by augmenting the prompting with schema\ndescriptions, a naturally occurring source of in-domain knowledge. Our purely\ngenerative system achieves state-of-the-art performance on MultiWOZ 2.2 and\nachieves competitive performance on two other benchmarks: MultiWOZ 2.1 and M2M.\nThe data and code will be available at\nhttps://github.com/chiahsuan156/DST-as-Prompting.", "published": "2021-09-15 18:11:25", "link": "http://arxiv.org/abs/2109.07506v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text as Causal Mediators: Research Design for Causal Estimates of\n  Differential Treatment of Social Groups via Language Aspects", "abstract": "Using observed language to understand interpersonal interactions is important\nin high-stakes decision making. We propose a causal research design for\nobservational (non-experimental) data to estimate the natural direct and\nindirect effects of social group signals (e.g. race or gender) on speakers'\nresponses with separate aspects of language as causal mediators. We illustrate\nthe promises and challenges of this framework via a theoretical case study of\nthe effect of an advocate's gender on interruptions from justices during U.S.\nSupreme Court oral arguments. We also discuss challenges conceptualizing and\noperationalizing causal variables such as gender and language that comprise of\nmany components, and we articulate technical open challenges such as temporal\ndependence between language mediators in conversational settings.", "published": "2021-09-15 19:15:35", "link": "http://arxiv.org/abs/2109.07542v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning", "abstract": "Named Entity Recognition (NER) in Few-Shot setting is imperative for entity\ntagging in low resource domains. Existing approaches only learn class-specific\nsemantic features and intermediate representations from source domains. This\naffects generalizability to unseen target domains, resulting in suboptimal\nperformances. To this end, we present CONTaiNER, a novel contrastive learning\ntechnique that optimizes the inter-token distribution distance for Few-Shot\nNER. Instead of optimizing class-specific attributes, CONTaiNER optimizes a\ngeneralized objective of differentiating between token categories based on\ntheir Gaussian-distributed embeddings. This effectively alleviates overfitting\nissues originating from training domains. Our experiments in several\ntraditional test domains (OntoNotes, CoNLL'03, WNUT '17, GUM) and a new large\nscale Few-Shot NER dataset (Few-NERD) demonstrate that on average, CONTaiNER\noutperforms previous methods by 3%-13% absolute F1 points while showing\nconsistent performance trends, even in challenging scenarios where previous\napproaches could not achieve appreciable performance.", "published": "2021-09-15 21:41:16", "link": "http://arxiv.org/abs/2109.07589v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparing Feature-Engineering and Feature-Learning Approaches for\n  Multilingual Translationese Classification", "abstract": "Traditional hand-crafted linguistically-informed features have often been\nused for distinguishing between translated and original non-translated texts.\nBy contrast, to date, neural architectures without manual feature engineering\nhave been less explored for this task. In this work, we (i) compare the\ntraditional feature-engineering-based approach to the feature-learning-based\none and (ii) analyse the neural architectures in order to investigate how well\nthe hand-crafted features explain the variance in the neural models'\npredictions. We use pre-trained neural word embeddings, as well as several\nend-to-end neural architectures in both monolingual and multilingual settings\nand compare them to feature-engineering-based SVM classifiers. We show that (i)\nneural architectures outperform other approaches by more than 20 accuracy\npoints, with the BERT-based model performing the best in both the monolingual\nand multilingual settings; (ii) while many individual hand-crafted\ntranslationese features correlate with neural model predictions, feature\nimportance analysis shows that the most important features for neural and\nclassical architectures differ; and (iii) our multilingual experiments provide\nempirical evidence for translationese universals across languages.", "published": "2021-09-15 22:34:48", "link": "http://arxiv.org/abs/2109.07604v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attention Is Indeed All You Need: Semantically Attention-Guided Decoding\n  for Data-to-Text NLG", "abstract": "Ever since neural models were adopted in data-to-text language generation,\nthey have invariably been reliant on extrinsic components to improve their\nsemantic accuracy, because the models normally do not exhibit the ability to\ngenerate text that reliably mentions all of the information provided in the\ninput. In this paper, we propose a novel decoding method that extracts\ninterpretable information from encoder-decoder models' cross-attention, and\nuses it to infer which attributes are mentioned in the generated text, which is\nsubsequently used to rescore beam hypotheses. Using this decoding method with\nT5 and BART, we show on three datasets its ability to dramatically reduce\nsemantic errors in the generated outputs, while maintaining their\nstate-of-the-art quality.", "published": "2021-09-15 01:42:51", "link": "http://arxiv.org/abs/2109.07043v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-Training with Differentiable Teacher", "abstract": "Self-training achieves enormous success in various semi-supervised and\nweakly-supervised learning tasks. The method can be interpreted as a\nteacher-student framework, where the teacher generates pseudo-labels, and the\nstudent makes predictions. The two models are updated alternatingly. However,\nsuch a straightforward alternating update rule leads to training instability.\nThis is because a small change in the teacher may result in a significant\nchange in the student. To address this issue, we propose DRIFT, short for\ndifferentiable self-training, that treats teacher-student as a Stackelberg\ngame. In this game, a leader is always in a more advantageous position than a\nfollower. In self-training, the student contributes to the prediction\nperformance, and the teacher controls the training process by generating\npseudo-labels. Therefore, we treat the student as the leader and the teacher as\nthe follower. The leader procures its advantage by acknowledging the follower's\nstrategy, which involves differentiable pseudo-labels and differentiable sample\nweights. Consequently, the leader-follower interaction can be effectively\ncaptured via Stackelberg gradient, obtained by differentiating the follower's\nstrategy. Experimental results on semi- and weakly-supervised classification\nand named entity recognition tasks show that our model outperforms existing\napproaches by large margins.", "published": "2021-09-15 02:06:13", "link": "http://arxiv.org/abs/2109.07049v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fast Extraction of Word Embedding from Q-contexts", "abstract": "The notion of word embedding plays a fundamental role in natural language\nprocessing (NLP). However, pre-training word embedding for very large-scale\nvocabulary is computationally challenging for most existing methods. In this\nwork, we show that with merely a small fraction of contexts (Q-contexts)which\nare typical in the whole corpus (and their mutual information with words), one\ncan construct high-quality word embedding with negligible errors. Mutual\ninformation between contexts and words can be encoded canonically as a sampling\nstate, thus, Q-contexts can be fast constructed. Furthermore, we present an\nefficient and effective WEQ method, which is capable of extracting word\nembedding directly from these typical contexts. In practical scenarios, our\nalgorithm runs 11$\\sim$13 times faster than well-established methods. By\ncomparing with well-known methods such as matrix factorization, word2vec,\nGloVeand fasttext, we demonstrate that our method achieves comparable\nperformance on a variety of downstream NLP tasks, and in the meanwhile\nmaintains run-time and resource advantages over all these baselines.", "published": "2021-09-15 05:14:31", "link": "http://arxiv.org/abs/2109.07084v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Document-Level Paraphrase Generation with Sentence Rewriting and\n  Reordering", "abstract": "Paraphrase generation is an important task in natural language processing.\nPrevious works focus on sentence-level paraphrase generation, while ignoring\ndocument-level paraphrase generation, which is a more challenging and valuable\ntask. In this paper, we explore the task of document-level paraphrase\ngeneration for the first time and focus on the inter-sentence diversity by\nconsidering sentence rewriting and reordering. We propose CoRPG (Coherence\nRelationship guided Paraphrase Generation), which leverages graph GRU to encode\nthe coherence relationship graph and get the coherence-aware representation for\neach sentence, which can be used for re-arranging the multiple (possibly\nmodified) input sentences. We create a pseudo document-level paraphrase dataset\nfor training CoRPG. Automatic evaluation results show CoRPG outperforms several\nstrong baseline models on the BERTScore and diversity scores. Human evaluation\nalso shows our model can generate document paraphrase with more diversity and\nsemantic preservation.", "published": "2021-09-15 05:53:40", "link": "http://arxiv.org/abs/2109.07095v1", "categories": ["cs.CL", "cs.AI", "68T50 (Primary), 68T07 (Secondary)", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Low-Resource Named Entity Recognition Based on Multi-hop Dependency\n  Trigger", "abstract": "This paper presents a simple and effective approach in low-resource named\nentity recognition (NER) based on multi-hop dependency trigger. Dependency\ntrigger refer to salient nodes relative to a entity in the dependency graph of\na context sentence. Our main observation is that there often exists trigger\nwhich play an important role to recognize the location and type of entity in\nsentence. Previous research has used manual labelling of trigger. Our main\ncontribution is to propose use a syntactic parser to automatically annotate\ntrigger. Experiments on two English datasets (CONLL 2003 and BC5CDR) show that\nthe proposed method is comparable to the previous trigger-based NER model.", "published": "2021-09-15 07:00:40", "link": "http://arxiv.org/abs/2109.07118v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What Does The User Want? Information Gain for Hierarchical Dialogue\n  Policy Optimisation", "abstract": "The dialogue management component of a task-oriented dialogue system is\ntypically optimised via reinforcement learning (RL). Optimisation via RL is\nhighly susceptible to sample inefficiency and instability. The hierarchical\napproach called Feudal Dialogue Management takes a step towards more efficient\nlearning by decomposing the action space. However, it still suffers from\ninstability due to the reward only being provided at the end of the dialogue.\nWe propose the usage of an intrinsic reward based on information gain to\naddress this issue. Our proposed reward favours actions that resolve\nuncertainty or query the user whenever necessary. It enables the policy to\nlearn how to retrieve the users' needs efficiently, which is an integral aspect\nin every task-oriented conversation. Our algorithm, which we call FeudalGain,\nachieves state-of-the-art results in most environments of the PyDial framework,\noutperforming much more complex approaches. We confirm the sample efficiency\nand stability of our algorithm through experiments in simulation and a human\ntrial.", "published": "2021-09-15 07:21:26", "link": "http://arxiv.org/abs/2109.07129v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Beyond Glass-Box Features: Uncertainty Quantification Enhanced Quality\n  Estimation for Neural Machine Translation", "abstract": "Quality Estimation (QE) plays an essential role in applications of Machine\nTranslation (MT). Traditionally, a QE system accepts the original source text\nand translation from a black-box MT system as input. Recently, a few studies\nindicate that as a by-product of translation, QE benefits from the model and\ntraining data's information of the MT system where the translations come from,\nand it is called the \"glass-box QE\". In this paper, we extend the definition of\n\"glass-box QE\" generally to uncertainty quantification with both \"black-box\"\nand \"glass-box\" approaches and design several features deduced from them to\nblaze a new trial in improving QE's performance. We propose a framework to fuse\nthe feature engineering of uncertainty quantification into a pre-trained\ncross-lingual language model to predict the translation quality. Experiment\nresults show that our method achieves state-of-the-art performances on the\ndatasets of WMT 2020 QE shared task.", "published": "2021-09-15 08:05:13", "link": "http://arxiv.org/abs/2109.07141v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning to Match Job Candidates Using Multilingual Bi-Encoder BERT", "abstract": "In this talk, we will show how we used Randstad history of candidate\nplacements to generate labeled CV-vacancy pairs dataset. Afterwards we\nfine-tune a multilingual BERT with bi encoder structure over this dataset, by\nadding a cosine similarity log loss layer. We will explain how using the\nmentioned structure helps us overcome most of the challenges described above,\nand how it enables us to build a maintainable and scalable pipeline to match\nCVs and vacancies. In addition, we show how we gain a better semantic\nunderstanding, and learn to bridge the vocabulary gap. Finally, we highlight\nhow multilingual transformers help us handle cross language barrier and might\nreduce discrimination.", "published": "2021-09-15 08:43:46", "link": "http://arxiv.org/abs/2109.07157v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Adversarial Mixing Policy for Relaxing Locally Linear Constraints in\n  Mixup", "abstract": "Mixup is a recent regularizer for current deep classification networks.\nThrough training a neural network on convex combinations of pairs of examples\nand their labels, it imposes locally linear constraints on the model's input\nspace. However, such strict linear constraints often lead to under-fitting\nwhich degrades the effects of regularization. Noticeably, this issue is getting\nmore serious when the resource is extremely limited. To address these issues,\nwe propose the Adversarial Mixing Policy (AMP), organized in a min-max-rand\nformulation, to relax the Locally Linear Constraints in Mixup. Specifically,\nAMP adds a small adversarial perturbation to the mixing coefficients rather\nthan the examples. Thus, slight non-linearity is injected in-between the\nsynthetic examples and synthetic labels. By training on these data, the deep\nnetworks are further regularized, and thus achieve a lower predictive error\nrate. Experiments on five text classification benchmarks and five backbone\nmodels have empirically shown that our methods reduce the error rate over Mixup\nvariants in a significant margin (up to 31.3%), especially in low-resource\nconditions (up to 17.5%).", "published": "2021-09-15 09:31:59", "link": "http://arxiv.org/abs/2109.07177v1", "categories": ["cs.CL", "cs.LG", "NLP"], "primary_category": "cs.CL"}
{"title": "Multiagent Multimodal Categorization for Symbol Emergence: Emergent\n  Communication via Interpersonal Cross-modal Inference", "abstract": "This paper describes a computational model of multiagent multimodal\ncategorization that realizes emergent communication. We clarify whether the\ncomputational model can reproduce the following functions in a symbol emergence\nsystem, comprising two agents with different sensory modalities playing a\nnaming game. (1) Function for forming a shared lexical system that comprises\nperceptual categories and corresponding signs, formed by agents through\nindividual learning and semiotic communication between agents. (2) Function to\nimprove the categorization accuracy in an agent via semiotic communication with\nanother agent, even when some sensory modalities of each agent are missing. (3)\nFunction that an agent infers unobserved sensory information based on a sign\nsampled from another agent in the same manner as cross-modal inference. We\npropose an interpersonal multimodal Dirichlet mixture (Inter-MDM), which is\nderived by dividing an integrative probabilistic generative model, which is\nobtained by integrating two Dirichlet mixtures (DMs). The Markov chain Monte\nCarlo algorithm realizes emergent communication. The experimental results\ndemonstrated that Inter-MDM enables agents to form multimodal categories and\nappropriately share signs between agents. It is shown that emergent\ncommunication improves categorization accuracy, even when some sensory\nmodalities are missing. Inter-MDM enables an agent to predict unobserved\ninformation based on a shared sign.", "published": "2021-09-15 10:20:54", "link": "http://arxiv.org/abs/2109.07194v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "A Relation-Oriented Clustering Method for Open Relation Extraction", "abstract": "The clustering-based unsupervised relation discovery method has gradually\nbecome one of the important methods of open relation extraction (OpenRE).\nHowever, high-dimensional vectors can encode complex linguistic information\nwhich leads to the problem that the derived clusters cannot explicitly align\nwith the relational semantic classes. In this work, we propose a\nrelation-oriented clustering model and use it to identify the novel relations\nin the unlabeled data. Specifically, to enable the model to learn to cluster\nrelational data, our method leverages the readily available labeled data of\npre-defined relations to learn a relation-oriented representation. We minimize\ndistance between the instance with same relation by gathering the instances\ntowards their corresponding relation centroids to form a cluster structure, so\nthat the learned representation is cluster-friendly. To reduce the clustering\nbias on predefined classes, we optimize the model by minimizing a joint\nobjective on both labeled and unlabeled data. Experimental results show that\nour method reduces the error rate by 29.2% and 15.7%, on two datasets\nrespectively, compared with current SOTA methods.", "published": "2021-09-15 10:46:39", "link": "http://arxiv.org/abs/2109.07205v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dialog speech sentiment classification for imbalanced datasets", "abstract": "Speech is the most common way humans express their feelings, and sentiment\nanalysis is the use of tools such as natural language processing and\ncomputational algorithms to identify the polarity of these feelings. Even\nthough this field has seen tremendous advancements in the last two decades, the\ntask of effectively detecting under represented sentiments in different kinds\nof datasets is still a challenging task. In this paper, we use single and\nbi-modal analysis of short dialog utterances and gain insights on the main\nfactors that aid in sentiment detection, particularly in the underrepresented\nclasses, in datasets with and without inherent sentiment component.\nFurthermore, we propose an architecture which uses a learning rate scheduler\nand different monitoring criteria and provides state-of-the-art results for the\nSWITCHBOARD imbalanced sentiment dataset.", "published": "2021-09-15 11:43:04", "link": "http://arxiv.org/abs/2109.07228v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Mathematical Properties of Integers", "abstract": "Embedding words in high-dimensional vector spaces has proven valuable in many\nnatural language applications. In this work, we investigate whether\nsimilarly-trained embeddings of integers can capture concepts that are useful\nfor mathematical applications. We probe the integer embeddings for mathematical\nknowledge, apply them to a set of numerical reasoning tasks, and show that by\nlearning the representations from mathematical sequence data, we can\nsubstantially improve over number embeddings learned from English text corpora.", "published": "2021-09-15 11:54:28", "link": "http://arxiv.org/abs/2109.07230v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "End-to-End Learning of Flowchart Grounded Task-Oriented Dialogs", "abstract": "We propose a novel problem within end-to-end learning of task-oriented\ndialogs (TOD), in which the dialog system mimics a troubleshooting agent who\nhelps a user by diagnosing their problem (e.g., car not starting). Such dialogs\nare grounded in domain-specific flowcharts, which the agent is supposed to\nfollow during the conversation. Our task exposes novel technical challenges for\nneural TOD, such as grounding an utterance to the flowchart without explicit\nannotation, referring to additional manual pages when user asks a clarification\nquestion, and ability to follow unseen flowcharts at test time. We release a\ndataset (FloDial) consisting of 2,738 dialogs grounded on 12 different\ntroubleshooting flowcharts. We also design a neural model, FloNet, which uses a\nretrieval-augmented generation architecture to train the dialog agent. Our\nexperiments find that FloNet can do zero-shot transfer to unseen flowcharts,\nand sets a strong baseline for future research.", "published": "2021-09-15 12:58:51", "link": "http://arxiv.org/abs/2109.07263v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Transfer of Monolingual Models", "abstract": "Recent studies in zero-shot cross-lingual learning using multilingual models\nhave falsified the previous hypothesis that shared vocabulary and joint\npre-training are the keys to cross-lingual generalization. Inspired by this\nadvancement, we introduce a cross-lingual transfer method for monolingual\nmodels based on domain adaptation. We study the effects of such transfer from\nfour different languages to English. Our experimental results on GLUE show that\nthe transferred models outperform the native English model independently of the\nsource language. After probing the English linguistic knowledge encoded in the\nrepresentations before and after transfer, we find that semantic information is\nretained from the source language, while syntactic information is learned\nduring transfer. Additionally, the results of evaluating the transferred models\nin source language tasks reveal that their performance in the source domain\ndeteriorates after transfer.", "published": "2021-09-15 15:00:53", "link": "http://arxiv.org/abs/2109.07348v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning When to Translate for Streaming Speech", "abstract": "How to find proper moments to generate partial sentence translation given a\nstreaming speech input? Existing approaches waiting-and-translating for a fixed\nduration often break the acoustic units in speech, since the boundaries between\nacoustic units in speech are not even. In this paper, we propose MoSST, a\nsimple yet effective method for translating streaming speech content. Given a\nusually long speech sequence, we develop an efficient monotonic segmentation\nmodule inside an encoder-decoder model to accumulate acoustic information\nincrementally and detect proper speech unit boundaries for the input in speech\ntranslation task. Experiments on multiple translation directions of the MuST-C\ndataset show that MoSST outperforms existing methods and achieves the best\ntrade-off between translation quality (BLEU) and latency. Our code is available\nat https://github.com/dqqcasia/mosst.", "published": "2021-09-15 15:22:10", "link": "http://arxiv.org/abs/2109.07368v4", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Topic Transferable Table Question Answering", "abstract": "Weakly-supervised table question-answering(TableQA) models have achieved\nstate-of-art performance by using pre-trained BERT transformer to jointly\nencoding a question and a table to produce structured query for the question.\nHowever, in practical settings TableQA systems are deployed over table corpora\nhaving topic and word distributions quite distinct from BERT's pretraining\ncorpus. In this work we simulate the practical topic shift scenario by\ndesigning novel challenge benchmarks WikiSQL-TS and WikiTQ-TS, consisting of\ntrain-dev-test splits in five distinct topic groups, based on the popular\nWikiSQL and WikiTableQuestions datasets. We empirically show that, despite\npre-training on large open-domain text, performance of models degrades\nsignificantly when they are evaluated on unseen topics. In response, we propose\nT3QA (Topic Transferable Table Question Answering) a pragmatic adaptation\nframework for TableQA comprising of: (1) topic-specific vocabulary injection\ninto BERT, (2) a novel text-to-text transformer generator (such as T5, GPT2)\nbased natural language question generation pipeline focused on generating topic\nspecific training data, and (3) a logical form reranker. We show that T3QA\nprovides a reasonably good baseline for our topic shift benchmarks. We believe\nour topic split benchmarks will lead to robust TableQA solutions that are\nbetter suited for practical deployment.", "published": "2021-09-15 15:34:39", "link": "http://arxiv.org/abs/2109.07377v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Constraint based Knowledge Base Distillation in End-to-End Task Oriented\n  Dialogs", "abstract": "End-to-End task-oriented dialogue systems generate responses based on dialog\nhistory and an accompanying knowledge base (KB). Inferring those KB entities\nthat are most relevant for an utterance is crucial for response generation.\nExisting state of the art scales to large KBs by softly filtering over\nirrelevant KB information. In this paper, we propose a novel filtering\ntechnique that consists of (1) a pairwise similarity based filter that\nidentifies relevant information by respecting the n-ary structure in a KB\nrecord. and, (2) an auxiliary loss that helps in separating contextually\nunrelated KB information. We also propose a new metric -- multiset entity F1\nwhich fixes a correctness issue in the existing entity F1 metric. Experimental\nresults on three publicly available task-oriented dialog datasets show that our\nproposed approach outperforms existing state-of-the-art models.", "published": "2021-09-15 16:00:10", "link": "http://arxiv.org/abs/2109.07396v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SupCL-Seq: Supervised Contrastive Learning for Downstream Optimized\n  Sequence Representations", "abstract": "While contrastive learning is proven to be an effective training strategy in\ncomputer vision, Natural Language Processing (NLP) is only recently adopting it\nas a self-supervised alternative to Masked Language Modeling (MLM) for\nimproving sequence representations. This paper introduces SupCL-Seq, which\nextends the supervised contrastive learning from computer vision to the\noptimization of sequence representations in NLP. By altering the dropout mask\nprobability in standard Transformer architectures, for every representation\n(anchor), we generate augmented altered views. A supervised contrastive loss is\nthen utilized to maximize the system's capability of pulling together similar\nsamples (e.g., anchors and their altered views) and pushing apart the samples\nbelonging to the other classes. Despite its simplicity, SupCLSeq leads to large\ngains in many sequence classification tasks on the GLUE benchmark compared to a\nstandard BERTbase, including 6% absolute improvement on CoLA, 5.4% on MRPC,\n4.7% on RTE and 2.6% on STSB. We also show consistent gains over self\nsupervised contrastively learned representations, especially in non-semantic\ntasks. Finally we show that these gains are not solely due to augmentation, but\nrather to a downstream optimized sequence representation. Code:\nhttps://github.com/hooman650/SupCL-Seq", "published": "2021-09-15 16:51:18", "link": "http://arxiv.org/abs/2109.07424v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Discriminative and Generative Transformer-based Models For Situation\n  Entity Classification", "abstract": "We re-examine the situation entity (SE) classification task with varying\namounts of available training data. We exploit a Transformer-based variational\nautoencoder to encode sentences into a lower dimensional latent space, which is\nused to generate the text and learn a SE classifier. Test set and cross-genre\nevaluations show that when training data is plentiful, the proposed model can\nimprove over the previous discriminative state-of-the-art models. Our approach\nperforms disproportionately better with smaller amounts of training data, but\nwhen faced with extremely small sets (4 instances per label), generative RNN\nmethods outperform transformers. Our work provides guidance for future efforts\non SE and semantic prediction tasks, and low-label training regimes.", "published": "2021-09-15 17:07:07", "link": "http://arxiv.org/abs/2109.07434v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Should We Be Pre-training? An Argument for End-task Aware Training as an\n  Alternative", "abstract": "In most settings of practical concern, machine learning practitioners know in\nadvance what end-task they wish to boost with auxiliary tasks. However, widely\nused methods for leveraging auxiliary data like pre-training and its\ncontinued-pretraining variant are end-task agnostic: they rarely, if ever,\nexploit knowledge of the target task. We study replacing end-task agnostic\ncontinued training of pre-trained language models with end-task aware training\nof said models. We argue that for sufficiently important end-tasks, the\nbenefits of leveraging auxiliary data in a task-aware fashion can justify\nforgoing the traditional approach of obtaining generic, end-task agnostic\nrepresentations as with (continued) pre-training. On three different\nlow-resource NLP tasks from two domains, we demonstrate that multi-tasking the\nend-task and auxiliary objectives results in significantly better downstream\ntask performance than the widely-used task-agnostic continued pre-training\nparadigm of Gururangan et al. (2020). We next introduce an online meta-learning\nalgorithm that learns a set of multi-task weights to better balance among our\nmultiple auxiliary objectives, achieving further improvements on end-task\nperformance and data efficiency.", "published": "2021-09-15 17:13:18", "link": "http://arxiv.org/abs/2109.07437v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "When Does Translation Require Context? A Data-driven, Multilingual\n  Exploration", "abstract": "Although proper handling of discourse significantly contributes to the\nquality of machine translation (MT), these improvements are not adequately\nmeasured in common translation quality metrics. Recent works in context-aware\nMT attempt to target a small set of discourse phenomena during evaluation,\nhowever not in a fully systematic way. In this paper, we develop the\nMultilingual Discourse-Aware (MuDA) benchmark, a series of taggers that\nidentify and evaluate model performance on discourse phenomena in any given\ndataset. The choice of phenomena is inspired by a novel methodology to\nsystematically identify translations requiring context. We confirm the\ndifficulty of previously studied phenomena while uncovering others that were\npreviously unaddressed. We find that common context-aware MT models make only\nmarginal improvements over context-agnostic models, which suggests these models\ndo not handle these ambiguities effectively. We release code and data for 14\nlanguage pairs to encourage the MT community to focus on accurately capturing\ndiscourse phenomena.", "published": "2021-09-15 17:29:30", "link": "http://arxiv.org/abs/2109.07446v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can Machines Read Coding Manuals Yet? -- A Benchmark for Building Better\n  Language Models for Code Understanding", "abstract": "Code understanding is an increasingly important application of Artificial\nIntelligence. A fundamental aspect of understanding code is understanding text\nabout code, e.g., documentation and forum discussions. Pre-trained language\nmodels (e.g., BERT) are a popular approach for various NLP tasks, and there are\nnow a variety of benchmarks, such as GLUE, to help improve the development of\nsuch models for natural language understanding. However, little is known about\nhow well such models work on textual artifacts about code, and we are unaware\nof any systematic set of downstream tasks for such an evaluation. In this\npaper, we derive a set of benchmarks (BLANCA - Benchmarks for LANguage models\non Coding Artifacts) that assess code understanding based on tasks such as\npredicting the best answer to a question in a forum post, finding related forum\nposts, or predicting classes related in a hierarchy from class documentation.\nWe evaluate the performance of current state-of-the-art language models on\nthese tasks and show that there is a significant improvement on each task from\nfine tuning. We also show that multi-task training over BLANCA tasks helps\nbuild better language models for code understanding.", "published": "2021-09-15 17:42:44", "link": "http://arxiv.org/abs/2109.07452v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Comparing Text Representations: A Theory-Driven Approach", "abstract": "Much of the progress in contemporary NLP has come from learning\nrepresentations, such as masked language model (MLM) contextual embeddings,\nthat turn challenging problems into simple classification tasks. But how do we\nquantify and explain this effect? We adapt general tools from computational\nlearning theory to fit the specific characteristics of text datasets and\npresent a method to evaluate the compatibility between representations and\ntasks. Even though many tasks can be easily solved with simple bag-of-words\n(BOW) representations, BOW does poorly on hard natural language inference\ntasks. For one such task we find that BOW cannot distinguish between real and\nrandomized labelings, while pre-trained MLM representations show 72x greater\ndistinction between real and random labelings than BOW. This method provides a\ncalibrated, quantitative measure of the difficulty of a classification-based\nNLP task, enabling comparisons between representations without requiring\nempirical evaluations that may be sensitive to initializations and\nhyperparameters. The method provides a fresh perspective on the patterns in a\ndataset and the alignment of those patterns with specific labels.", "published": "2021-09-15 17:48:19", "link": "http://arxiv.org/abs/2109.07458v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-Register Projection for Headline Part of Speech Tagging", "abstract": "Part of speech (POS) tagging is a familiar NLP task. State of the art taggers\nroutinely achieve token-level accuracies of over 97% on news body text,\nevidence that the problem is well understood. However, the register of English\nnews headlines, \"headlinese\", is very different from the register of long-form\ntext, causing POS tagging models to underperform on headlines. In this work, we\nautomatically annotate news headlines with POS tags by projecting predicted\ntags from corresponding sentences in news bodies. We train a multi-domain POS\ntagger on both long-form and headline text and show that joint training on both\nregisters improves over training on just one or naively concatenating training\nsets. We evaluate on a newly-annotated corpus of over 5,248 English news\nheadlines from the Google sentence compression corpus, and show that our model\nyields a 23% relative error reduction per token and 19% per headline. In\naddition, we demonstrate that better headline POS tags can improve the\nperformance of a syntax-based open information extraction system. We make POSH,\nthe POS-tagged Headline corpus, available to encourage research in improved NLP\nmodels for news headlines.", "published": "2021-09-15 18:00:02", "link": "http://arxiv.org/abs/2109.07483v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Comparing Euclidean and Hyperbolic Embeddings on the WordNet Nouns\n  Hypernymy Graph", "abstract": "Nickel and Kiela (2017) present a new method for embedding tree nodes in the\nPoincare ball, and suggest that these hyperbolic embeddings are far more\neffective than Euclidean embeddings at embedding nodes in large, hierarchically\nstructured graphs like the WordNet nouns hypernymy tree. This is especially\ntrue in low dimensions (Nickel and Kiela, 2017, Table 1). In this work, we seek\nto reproduce their experiments on embedding and reconstructing the WordNet\nnouns hypernymy graph. Counter to what they report, we find that Euclidean\nembeddings are able to represent this tree at least as well as Poincare\nembeddings, when allowed at least 50 dimensions. We note that this does not\ndiminish the significance of their work given the impressive performance of\nhyperbolic embeddings in very low-dimensional settings. However, given the wide\ninfluence of their work, our aim here is to present an updated and more\naccurate comparison between the Euclidean and hyperbolic embeddings.", "published": "2021-09-15 18:00:05", "link": "http://arxiv.org/abs/2109.07488v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "\"It doesn't look good for a date\": Transforming Critiques into\n  Preferences for Conversational Recommendation Systems", "abstract": "Conversations aimed at determining good recommendations are iterative in\nnature. People often express their preferences in terms of a critique of the\ncurrent recommendation (e.g., \"It doesn't look good for a date\"), requiring\nsome degree of common sense for a preference to be inferred. In this work, we\npresent a method for transforming a user critique into a positive preference\n(e.g., \"I prefer more romantic\") in order to retrieve reviews pertaining to\npotentially better recommendations (e.g., \"Perfect for a romantic dinner\"). We\nleverage a large neural language model (LM) in a few-shot setting to perform\ncritique-to-preference transformation, and we test two methods for retrieving\nrecommendations: one that matches embeddings, and another that fine-tunes an LM\nfor the task. We instantiate this approach in the restaurant domain and\nevaluate it using a new dataset of restaurant critiques. In an ablation study,\nwe show that utilizing critique-to-preference transformation improves\nrecommendations, and that there are at least three general cases that explain\nthis improved performance.", "published": "2021-09-15 21:10:33", "link": "http://arxiv.org/abs/2109.07576v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An influencer-based approach to understanding radical right viral tweets", "abstract": "Radical right influencers routinely use social media to spread highly\ndivisive, disruptive and anti-democratic messages. Assessing and countering the\nchallenge that such content poses is crucial for ensuring that online spaces\nremain open, safe and accessible. Previous work has paid little attention to\nunderstanding factors associated with radical right content that goes viral. We\ninvestigate this issue with a new dataset ROT which provides insight into the\ncontent, engagement and followership of a set of 35 radical right influencers.\nIt includes over 50,000 original entries and over 40 million retweets, quotes,\nreplies and mentions. We use a multilevel model to measure engagement with\ntweets, which are nested in each influencer. We show that it is crucial to\naccount for the influencer-level structure, and find evidence of the importance\nof both influencer- and content-level factors, including the number of\nfollowers each influencer has, the type of content (original posts, quotes and\nreplies), the length and toxicity of content, and whether influencers request\nretweets. We make ROT available for other researchers to use.", "published": "2021-09-15 21:40:25", "link": "http://arxiv.org/abs/2109.07588v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "On the Complementarity of Data Selection and Fine Tuning for Domain\n  Adaptation", "abstract": "Domain adaptation of neural networks commonly relies on three training\nphases: pretraining, selected data training and then fine tuning. Data\nselection improves target domain generalization by training further on\npretraining data identified by relying on a small sample of target domain data.\nThis work examines the benefit of data selection for language modeling and\nmachine translation. Our experiments assess the complementarity of selection\nwith fine tuning and result in practical recommendations: (i) selected data\nmust be similar to the fine-tuning domain but not so much as to erode the\ncomplementary effect of fine-tuning; (ii) there is a trade-off between\nselecting little data for fast but limited progress or much data for slow but\nlong lasting progress; (iii) data selection can be applied early during\npretraining, with performance gains comparable to long pretraining session;\n(iv) data selection from domain classifiers is often more effective than the\npopular contrastive data selection method.", "published": "2021-09-15 21:49:06", "link": "http://arxiv.org/abs/2109.07591v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EfficientBERT: Progressively Searching Multilayer Perceptron via Warm-up\n  Knowledge Distillation", "abstract": "Pre-trained language models have shown remarkable results on various NLP\ntasks. Nevertheless, due to their bulky size and slow inference speed, it is\nhard to deploy them on edge devices. In this paper, we have a critical insight\nthat improving the feed-forward network (FFN) in BERT has a higher gain than\nimproving the multi-head attention (MHA) since the computational cost of FFN is\n2$\\sim$3 times larger than MHA. Hence, to compact BERT, we are devoted to\ndesigning efficient FFN as opposed to previous works that pay attention to MHA.\nSince FFN comprises a multilayer perceptron (MLP) that is essential in BERT\noptimization, we further design a thorough search space towards an advanced MLP\nand perform a coarse-to-fine mechanism to search for an efficient BERT\narchitecture. Moreover, to accelerate searching and enhance model\ntransferability, we employ a novel warm-up knowledge distillation strategy at\neach search stage. Extensive experiments show our searched EfficientBERT is\n6.9$\\times$ smaller and 4.4$\\times$ faster than BERT$\\rm_{BASE}$, and has\ncompetitive performances on GLUE and SQuAD Benchmarks. Concretely,\nEfficientBERT attains a 77.7 average score on GLUE \\emph{test}, 0.7 higher than\nMobileBERT$\\rm_{TINY}$, and achieves an 85.3/74.5 F1 score on SQuAD v1.1/v2.0\n\\emph{dev}, 3.2/2.7 higher than TinyBERT$_4$ even without data augmentation.\nThe code is released at https://github.com/cheneydon/efficient-bert.", "published": "2021-09-15 11:25:39", "link": "http://arxiv.org/abs/2109.07222v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Matching with Transformers in MELT", "abstract": "One of the strongest signals for automated matching of ontologies and\nknowledge graphs are the textual descriptions of the concepts. The methods that\nare typically applied (such as character- or token-based comparisons) are\nrelatively simple, and therefore do not capture the actual meaning of the\ntexts. With the rise of transformer-based language models, text comparison\nbased on meaning (rather than lexical features) is possible. In this paper, we\nmodel the ontology matching task as classification problem and present\napproaches based on transformer models. We further provide an easy to use\nimplementation in the MELT framework which is suited for ontology and knowledge\ngraph matching. We show that a transformer-based filter helps to choose the\ncorrect correspondences given a high-recall alignment and already achieves a\ngood result with simple alignment post-processing methods.", "published": "2021-09-15 16:07:43", "link": "http://arxiv.org/abs/2109.07401v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Challenges in Detoxifying Language Models", "abstract": "Large language models (LM) generate remarkably fluent text and can be\nefficiently adapted across NLP tasks. Measuring and guaranteeing the quality of\ngenerated text in terms of safety is imperative for deploying LMs in the real\nworld; to this end, prior work often relies on automatic evaluation of LM\ntoxicity. We critically discuss this approach, evaluate several toxicity\nmitigation strategies with respect to both automatic and human evaluation, and\nanalyze consequences of toxicity mitigation in terms of model bias and LM\nquality. We demonstrate that while basic intervention strategies can\neffectively optimize previously established automatic metrics on the\nRealToxicityPrompts dataset, this comes at the cost of reduced LM coverage for\nboth texts about, and dialects of, marginalized groups. Additionally, we find\nthat human raters often disagree with high automatic toxicity scores after\nstrong toxicity reduction interventions -- highlighting further the nuances\ninvolved in careful evaluation of LM toxicity.", "published": "2021-09-15 17:27:06", "link": "http://arxiv.org/abs/2109.07445v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "Tied & Reduced RNN-T Decoder", "abstract": "Previous works on the Recurrent Neural Network-Transducer (RNN-T) models have\nshown that, under some conditions, it is possible to simplify its prediction\nnetwork with little or no loss in recognition accuracy (arXiv:2003.07705\n[eess.AS], [2], arXiv:2012.06749 [cs.CL]). This is done by limiting the context\nsize of previous labels and/or using a simpler architecture for its layers\ninstead of LSTMs. The benefits of such changes include reduction in model size,\nfaster inference and power savings, which are all useful for on-device\napplications.\n  In this work, we study ways to make the RNN-T decoder (prediction network +\njoint network) smaller and faster without degradation in recognition\nperformance. Our prediction network performs a simple weighted averaging of the\ninput embeddings, and shares its embedding matrix weights with the joint\nnetwork's output layer (a.k.a. weight tying, commonly used in language modeling\narXiv:1611.01462 [cs.LG]). This simple design, when used in conjunction with\nadditional Edit-based Minimum Bayes Risk (EMBR) training, reduces the RNN-T\nDecoder from 23M parameters to just 2M, without affecting word-error rate\n(WER).", "published": "2021-09-15 18:19:16", "link": "http://arxiv.org/abs/2109.07513v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Prefix-to-SQL: Text-to-SQL Generation from Incomplete User Questions", "abstract": "Existing text-to-SQL research only considers complete questions as the input,\nbut lay-users might strive to formulate a complete question. To build a smarter\nnatural language interface to database systems (NLIDB) that also processes\nincomplete questions, we propose a new task, prefix-to-SQL which takes question\nprefix from users as the input and predicts the intended SQL. We construct a\nnew benchmark called PAGSAS that contains 124K user question prefixes and the\nintended SQL for 5 sub-tasks Advising, GeoQuery, Scholar, ATIS, and Spider.\nAdditionally, we propose a new metric SAVE to measure how much effort can be\nsaved by users. Experimental results show that PAGSAS is challenging even for\nstrong baseline models such as T5. As we observe the difficulty of\nprefix-to-SQL is related to the number of omitted tokens, we incorporate\ncurriculum learning of feeding examples with an increasing number of omitted\ntokens. This improves scores on various sub-tasks by as much as 9% recall\nscores on sub-task GeoQuery in PAGSAS.", "published": "2021-09-15 14:28:18", "link": "http://arxiv.org/abs/2109.13066v3", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Binaural rendering from microphone array signals of arbitrary geometry", "abstract": "A method of binaural rendering from microphone array signals of arbitrary\ngeometry is proposed. To reproduce binaural signals from microphone array\nrecordings at a remote location, a spherical microphone array is generally used\nfor capturing a soundfield. However, owing to the lack of flexibility in the\nmicrophone arrangement, the single spherical array is sometimes impractical for\nestimating a large region of a soundfield. We propose a method based on\nharmonic analysis of infinite order, which allows the use of arbitrarily placed\nmicrophones. In the synthesis of the estimated soundfield, a\nspherical-wave-decomposition-based binaural rendering is also formulated to\ntake into consideration the distance in measuring head-related transfer\nfunctions. We develop and evaluate a composite microphone array consisting of\nmultiple small arrays. Experimental results including those of listening tests\nindicate that our proposed method is robust against change in listening\nposition in the recording area.", "published": "2021-09-15 13:23:21", "link": "http://arxiv.org/abs/2109.07274v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Streaming Transformer Based ASR Under a Framework of\n  Self-supervised Learning", "abstract": "Recently self-supervised learning has emerged as an effective approach to\nimprove the performance of automatic speech recognition (ASR). Under such a\nframework, the neural network is usually pre-trained with massive unlabeled\ndata and then fine-tuned with limited labeled data. However, the non-streaming\narchitecture like bidirectional transformer is usually adopted by the neural\nnetwork to achieve competitive results, which can not be used in streaming\nscenarios. In this paper, we mainly focus on improving the performance of\nstreaming transformer under the self-supervised learning framework.\nSpecifically, we propose a novel two-stage training method during fine-tuning,\nwhich combines knowledge distilling and self-training. The proposed training\nmethod achieves 16.3% relative word error rate (WER) reduction on Librispeech\nnoisy test set. Finally, by only using the 100h clean subset of Librispeech as\nthe labeled data and the rest (860h) as the unlabeled data, our streaming\ntransformer based model obtains competitive WERs 3.5/8.7 on Librispeech\nclean/noisy test sets.", "published": "2021-09-15 14:35:59", "link": "http://arxiv.org/abs/2109.07327v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Accent Identification and Accented Speech Recognition Under a\n  Framework of Self-supervised Learning", "abstract": "Recently, self-supervised pre-training has gained success in automatic speech\nrecognition (ASR). However, considering the difference between speech accents\nin real scenarios, how to identify accents and use accent features to improve\nASR is still challenging. In this paper, we employ the self-supervised\npre-training method for both accent identification and accented speech\nrecognition tasks. For the former task, a standard deviation constraint loss\n(SDC-loss) based end-to-end (E2E) architecture is proposed to identify accents\nunder the same language. As for accented speech recognition task, we design an\naccent-dependent ASR system, which can utilize additional accent input\nfeatures. Furthermore, we propose a frame-level accent feature, which is\nextracted based on the proposed accent identification model and can be\ndynamically adjusted. We pre-train our models using 960 hours unlabeled\nLibriSpeech dataset and fine-tune them on AESRC2020 speech dataset. The\nexperimental results show that our proposed accent-dependent ASR system is\nsignificantly ahead of the AESRC2020 baseline and achieves $6.5\\%$ relative\nword error rate (WER) reduction compared with our accent-independent ASR\nsystem.", "published": "2021-09-15 15:01:55", "link": "http://arxiv.org/abs/2109.07349v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Behavior of Keyword Spotting Networks Under Noisy Conditions", "abstract": "Keyword spotting (KWS) is becoming a ubiquitous need with the advancement in\nartificial intelligence and smart devices. Recent work in this field have\nfocused on several different architectures to achieve good results on datasets\nwith low to moderate noise. However, the performance of these models\ndeteriorates under high noise conditions as shown by our experiments. In our\npaper, we present an extensive comparison between state-of-the-art KWS networks\nunder various noisy conditions. We also suggest adaptive batch normalization as\na technique to improve the performance of the networks when the noise files are\nunknown during the training phase. The results of such high noise\ncharacterization enable future work in developing models that perform better in\nthe aforementioned conditions.", "published": "2021-09-15 10:02:34", "link": "http://arxiv.org/abs/2109.07930v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "BacHMMachine: An Interpretable and Scalable Model for Algorithmic\n  Harmonization for Four-part Baroque Chorales", "abstract": "Algorithmic harmonization - the automated harmonization of a musical piece\ngiven its melodic line - is a challenging problem that has garnered much\ninterest from both music theorists and computer scientists. One genre of\nparticular interest is the four-part Baroque chorales of J.S. Bach. Methods for\nalgorithmic chorale harmonization typically adopt a black-box, \"data-driven\"\napproach: they do not explicitly integrate principles from music theory but\nrely on a complex learning model trained with a large amount of chorale data.\nWe propose instead a new harmonization model, called BacHMMachine, which\nemploys a \"theory-driven\" framework guided by music composition principles,\nalong with a \"data-driven\" model for learning compositional features within\nthis framework. As its name suggests, BacHMMachine uses a novel Hidden Markov\nModel based on key and chord transitions, providing a probabilistic framework\nfor learning key modulations and chordal progressions from a given melodic\nline. This allows for the generation of creative, yet musically coherent\nchorale harmonizations; integrating compositional principles allows for a much\nsimpler model that results in vast decreases in computational burden and\ngreater interpretability compared to state-of-the-art algorithmic harmonization\nmethods, at no penalty to quality of harmonization or musicality. We\ndemonstrate this improvement via comprehensive experiments and Turing tests\ncomparing BacHMMachine to existing methods.", "published": "2021-09-15 23:39:45", "link": "http://arxiv.org/abs/2109.07623v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "FSER: Deep Convolutional Neural Networks for Speech Emotion Recognition", "abstract": "Using mel-spectrograms over conventional MFCCs features, we assess the\nabilities of convolutional neural networks to accurately recognize and classify\nemotions from speech data. We introduce FSER, a speech emotion recognition\nmodel trained on four valid speech databases, achieving a high-classification\naccuracy of 95,05\\%, over 8 different emotion classes: anger, anxiety, calm,\ndisgust, happiness, neutral, sadness, surprise. On each benchmark dataset, FSER\noutperforms the best models introduced so far, achieving a state-of-the-art\nperformance. We show that FSER stays reliable, independently of the language,\nsex identity, and any other external factor. Additionally, we describe how FSER\ncould potentially be used to improve mental and emotional health care and how\nour analysis and findings serve as guidelines and benchmarks for further works\nin the same direction.", "published": "2021-09-15 05:03:24", "link": "http://arxiv.org/abs/2109.07916v1", "categories": ["eess.AS", "cs.CV", "cs.HC"], "primary_category": "eess.AS"}
