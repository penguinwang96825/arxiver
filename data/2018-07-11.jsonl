{"title": "A Dialogue Annotation Scheme for Weight Management Chat using the\n  Trans-Theoretical Model of Health Behavior Change", "abstract": "In this study we collect and annotate human-human role-play dialogues in the\ndomain of weight management. There are two roles in the conversation: the\n\"seeker\" who is looking for ways to lose weight and the \"helper\" who provides\nsuggestions to help the \"seeker\" in their weight loss journey. The chat\ndialogues collected are then annotated with a novel annotation scheme inspired\nby a popular health behavior change theory called \"trans-theoretical model of\nhealth behavior change\". We also build classifiers to automatically predict the\nannotation labels used in our corpus. We find that classification accuracy\nimproves when oracle segmentations of the interlocutors' sentences are provided\ncompared to directly classifying unsegmented sentences.", "published": "2018-07-11 04:45:59", "link": "http://arxiv.org/abs/1807.03948v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Understanding End-of-trip Instructions in a Taxi Ride Scenario", "abstract": "We introduce a dataset containing human-authored descriptions of target\nlocations in an \"end-of-trip in a taxi ride\" scenario. We describe our data\ncollection method and a novel annotation scheme that supports understanding of\nsuch descriptions of target locations. Our dataset contains target location\ndescriptions for both synthetic and real-world images as well as visual\nannotations (ground truth labels, dimensions of vehicles and objects,\ncoordinates of the target location,distance and direction of the target\nlocation from vehicles and objects) that can be used in various visual and\nlanguage tasks. We also perform a pilot experiment on how the corpus could be\napplied to visual reference resolution in this domain.", "published": "2018-07-11 05:06:53", "link": "http://arxiv.org/abs/1807.03950v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An improved neural network model for joint POS tagging and dependency\n  parsing", "abstract": "We propose a novel neural network model for joint part-of-speech (POS)\ntagging and dependency parsing. Our model extends the well-known BIST\ngraph-based dependency parser (Kiperwasser and Goldberg, 2016) by incorporating\na BiLSTM-based tagging component to produce automatically predicted POS tags\nfor the parser. On the benchmark English Penn treebank, our model obtains\nstrong UAS and LAS scores at 94.51% and 92.87%, respectively, producing 1.5+%\nabsolute improvements to the BIST graph-based parser, and also obtaining a\nstate-of-the-art POS tagging accuracy at 97.97%. Furthermore, experimental\nresults on parsing 61 \"big\" Universal Dependencies treebanks from raw texts\nshow that our model outperforms the baseline UDPipe (Straka and Strakov\\'a,\n2017) with 0.8% higher average POS tagging score and 3.6% higher average LAS\nscore. In addition, with our model, we also obtain state-of-the-art downstream\ntask scores for biomedical event extraction and opinion analysis applications.\nOur code is available together with all pre-trained models at:\nhttps://github.com/datquocnguyen/jPTDP", "published": "2018-07-11 05:47:33", "link": "http://arxiv.org/abs/1807.03955v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UniParse: A universal graph-based parsing toolkit", "abstract": "This paper describes the design and use of the graph-based parsing framework\nand toolkit UniParse, released as an open-source python software package.\nUniParse as a framework novelly streamlines research prototyping, development\nand evaluation of graph-based dependency parsing architectures. UniParse does\nthis by enabling highly efficient, sufficiently independent, easily readable,\nand easily extensible implementations for all dependency parser components. We\ndistribute the toolkit with ready-made configurations as re-implementations of\nall current state-of-the-art first-order graph-based parsers, including even\nmore efficient Cython implementations of both encoders and decoders, as well as\nthe required specialised loss functions.", "published": "2018-07-11 10:14:48", "link": "http://arxiv.org/abs/1807.04053v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "JeSemE: A Website for Exploring Diachronic Changes in Word Meaning and\n  Emotion", "abstract": "We here introduce a substantially extended version of JeSemE, an interactive\nwebsite for visually exploring computationally derived time-variant information\non word meanings and lexical emotions assembled from five large diachronic text\ncorpora. JeSemE is designed for scholars in the (digital) humanities as an\nalternative to consulting manually compiled, printed dictionaries for such\ninformation (if available at all). This tool uniquely combines state-of-the-art\ndistributional semantics with a nuanced model of human emotions, two\ninformation streams we deem beneficial for a data-driven interpretation of\ntexts in the humanities.", "published": "2018-07-11 14:12:58", "link": "http://arxiv.org/abs/1807.04148v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linear Transformations for Cross-lingual Semantic Textual Similarity", "abstract": "Cross-lingual semantic textual similarity systems estimate the degree of the\nmeaning similarity between two sentences, each in a different language.\nState-of-the-art algorithms usually employ machine translation and combine vast\namount of features, making the approach strongly supervised, resource rich, and\ndifficult to use for poorly-resourced languages.\n  In this paper, we study linear transformations, which project monolingual\nsemantic spaces into a shared space using bilingual dictionaries. We propose a\nnovel transformation, which builds on the best ideas from prior works. We\nexperiment with unsupervised techniques for sentence similarity based only on\nsemantic spaces and we show they can be significantly improved by the word\nweighting. Our transformation outperforms other methods and together with word\nweighting leads to very promising results on several datasets in different\nlanguages.", "published": "2018-07-11 14:48:02", "link": "http://arxiv.org/abs/1807.04172v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Word Analogies using Linear Transformations between\n  Semantic Spaces", "abstract": "We generalize the word analogy task across languages, to provide a new\nintrinsic evaluation method for cross-lingual semantic spaces. We experiment\nwith six languages within different language families, including English,\nGerman, Spanish, Italian, Czech, and Croatian. State-of-the-art monolingual\nsemantic spaces are transformed into a shared space using dictionaries of word\ntranslations. We compare several linear transformations and rank them for\nexperiments with monolingual (no transformation), bilingual (one semantic space\nis transformed to another), and multilingual (all semantic spaces are\ntransformed onto English space) versions of semantic spaces. We show that\ntested linear transformations preserve relationships between words (word\nanalogies) and lead to impressive results. We achieve average accuracy of\n51.1%, 43.1%, and 38.2% for monolingual, bilingual, and multilingual semantic\nspaces, respectively.", "published": "2018-07-11 14:51:35", "link": "http://arxiv.org/abs/1807.04175v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Seq2Seq2Sentiment: Multimodal Sequence to Sequence Models for Sentiment\n  Analysis", "abstract": "Multimodal machine learning is a core research area spanning the language,\nvisual and acoustic modalities. The central challenge in multimodal learning\ninvolves learning representations that can process and relate information from\nmultiple modalities. In this paper, we propose two methods for unsupervised\nlearning of joint multimodal representations using sequence to sequence\n(Seq2Seq) methods: a \\textit{Seq2Seq Modality Translation Model} and a\n\\textit{Hierarchical Seq2Seq Modality Translation Model}. We also explore\nmultiple different variations on the multimodal inputs and outputs of these\nseq2seq models. Our experiments on multimodal sentiment analysis using the\nCMU-MOSI dataset indicate that our methods learn informative multimodal\nrepresentations that outperform the baselines and achieve improved performance\non multimodal sentiment analysis, specifically in the Bimodal case where our\nmodel is able to improve F1 Score by twelve points. We also discuss future\ndirections for multimodal Seq2Seq methods.", "published": "2018-07-11 01:13:13", "link": "http://arxiv.org/abs/1807.03915v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Neural Chinese Word Segmentation with Dictionary Knowledge", "abstract": "Chinese word segmentation (CWS) is an important task for Chinese NLP.\nRecently, many neural network based methods have been proposed for CWS.\nHowever, these methods require a large number of labeled sentences for model\ntraining, and usually cannot utilize the useful information in Chinese\ndictionary. In this paper, we propose two methods to exploit the dictionary\ninformation for CWS. The first one is based on pseudo labeled data generation,\nand the second one is based on multi-task learning. The experimental results on\ntwo benchmark datasets validate that our approach can effectively improve the\nperformance of Chinese word segmentation, especially when training data is\ninsufficient.", "published": "2018-07-11 04:51:41", "link": "http://arxiv.org/abs/1807.05849v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A Fast-Converged Acoustic Modeling for Korean Speech Recognition: A\n  Preliminary Study on Time Delay Neural Network", "abstract": "In this paper, a time delay neural network (TDNN) based acoustic model is\nproposed to implement a fast-converged acoustic modeling for Korean speech\nrecognition. The TDNN has an advantage in fast-convergence where the amount of\ntraining data is limited, due to subsampling which excludes duplicated weights.\nThe TDNN showed an absolute improvement of 2.12% in terms of character error\nrate compared to feed forward neural network (FFNN) based modelling for Korean\nspeech corpora. The proposed model converged 1.67 times faster than a\nFFNN-based model did.", "published": "2018-07-11 05:34:09", "link": "http://arxiv.org/abs/1807.05855v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "RTF-Based Binaural MVDR Beamformer Exploiting an External Microphone in\n  a Diffuse Noise Field", "abstract": "Besides suppressing all undesired sound sources, an important objective of a\nbinaural noise reduction algorithm for hearing devices is the preservation of\nthe binaural cues, aiming at preserving the spatial perception of the acoustic\nscene. A well-known binaural noise reduction algorithm is the binaural minimum\nvariance distortionless response beamformer, which can be steered using the\nrelative transfer function (RTF) vector of the desired source, relating the\nacoustic transfer functions between the desired source and all microphones to a\nreference microphone. In this paper, we propose a computationally efficient\nmethod to estimate the RTF vector in a diffuse noise field, requiring an\nadditional microphone that is spatially separated from the head-mounted\nmicrophones. Assuming that the spatial coherence between the noise components\nin the head-mounted microphone signals and the additional microphone signal is\nzero, we show that an unbiased estimate of the RTF vector can be obtained.\nBased on real-world recordings, experimental results for several reverberation\ntimes show that the proposed RTF estimator outperforms the widely used RTF\nestimator based on covariance whitening and a simple biased RTF estimator in\nterms of noise reduction and binaural cue preservation performance.", "published": "2018-07-11 11:54:54", "link": "http://arxiv.org/abs/1807.04096v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Efficient keyword spotting using time delay neural networks", "abstract": "This paper describes a novel method of live keyword spotting using a\ntwo-stage time delay neural network. The model is trained using transfer\nlearning: initial training with phone targets from a large speech corpus is\nfollowed by training with keyword targets from a smaller data set. The accuracy\nof the system is evaluated on two separate tasks. The first is the freely\navailable Google Speech Commands dataset. The second is an in-house task\nspecifically developed for keyword spotting. The results show significant\nimprovements in false accept and false reject rates in both clean and noisy\nenvironments when compared with previously known techniques. Furthermore, we\ninvestigate various techniques to reduce computation in terms of\nmultiplications per second of audio. Compared to recently published work, the\nproposed system provides up to 89% savings on computational complexity.", "published": "2018-07-11 21:11:28", "link": "http://arxiv.org/abs/1807.04353v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
