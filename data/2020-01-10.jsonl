{"title": "A Scalable Chatbot Platform Leveraging Online Community Posts: A\n  Proof-of-Concept Study", "abstract": "The development of natural language processing algorithms and the explosive\ngrowth of conversational data are encouraging researches on the human-computer\nconversation. Still, getting qualified conversational data on a large scale is\ndifficult and expensive. In this paper, we verify the feasibility of\nconstructing a data-driven chatbot with processed online community posts by\nusing them as pseudo-conversational data. We argue that chatbots for various\npurposes can be built extensively through the pipeline exploiting the common\nstructure of community posts. Our experiment demonstrates that chatbots created\nalong the pipeline can yield the proper responses.", "published": "2020-01-10 01:45:45", "link": "http://arxiv.org/abs/2001.03278v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Co-evolution of language and agents in referential games", "abstract": "Referential games offer a grounded learning environment for neural agents\nwhich accounts for the fact that language is functionally used to communicate.\nHowever, they do not take into account a second constraint considered to be\nfundamental for the shape of human language: that it must be learnable by new\nlanguage learners.\n  Cogswell et al. (2019) introduced cultural transmission within referential\ngames through a changing population of agents to constrain the emerging\nlanguage to be learnable. However, the resulting languages remain inherently\nbiased by the agents' underlying capabilities.\n  In this work, we introduce Language Transmission Engine to model both\ncultural and architectural evolution in a population of agents. As our core\ncontribution, we empirically show that the optimal situation is to take into\naccount also the learning biases of the language learners and thus let language\nand agents co-evolve. When we allow the agent population to evolve through\narchitectural evolution, we achieve across the board improvements on all\nconsidered metrics and surpass the gains made with cultural transmission. These\nresults stress the importance of studying the underlying agent architecture and\npave the way to investigate the co-evolution of language and agent in language\nemergence studies.", "published": "2020-01-10 09:29:20", "link": "http://arxiv.org/abs/2001.03361v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does syntax need to grow on trees? Sources of hierarchical inductive\n  bias in sequence-to-sequence networks", "abstract": "Learners that are exposed to the same training data might generalize\ndifferently due to differing inductive biases. In neural network models,\ninductive biases could in theory arise from any aspect of the model\narchitecture. We investigate which architectural factors affect the\ngeneralization behavior of neural sequence-to-sequence models trained on two\nsyntactic tasks, English question formation and English tense reinflection. For\nboth tasks, the training set is consistent with a generalization based on\nhierarchical structure and a generalization based on linear order. All\narchitectural factors that we investigated qualitatively affected how models\ngeneralized, including factors with no clear connection to hierarchical\nstructure. For example, LSTMs and GRUs displayed qualitatively different\ninductive biases. However, the only factor that consistently contributed a\nhierarchical bias across tasks was the use of a tree-structured model rather\nthan a model with sequential recurrence, suggesting that human-like syntactic\ngeneralization requires architectural syntactic structure.", "published": "2020-01-10 19:02:52", "link": "http://arxiv.org/abs/2001.03632v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Multi-Task Learn for Better Neural Machine Translation", "abstract": "Scarcity of parallel sentence pairs is a major challenge for training high\nquality neural machine translation (NMT) models in bilingually low-resource\nscenarios, as NMT is data-hungry. Multi-task learning is an elegant approach to\ninject linguistic-related inductive biases into NMT, using auxiliary syntactic\nand semantic tasks, to improve generalisation. The challenge, however, is to\ndevise effective training schedules, prescribing when to make use of the\nauxiliary tasks during the training process to fill the knowledge gaps of the\nmain translation task, a setting referred to as biased-MTL. Current approaches\nfor the training schedule are based on hand-engineering heuristics, whose\neffectiveness vary in different MTL settings. We propose a novel framework for\nlearning the training schedule, ie learning to multi-task learn, for the MTL\nsetting of interest. We formulate the training schedule as a Markov decision\nprocess which paves the way to employ policy learning methods to learn the\nscheduling policy. We effectively and efficiently learn the training schedule\npolicy within the imitation learning framework using an oracle policy algorithm\nthat dynamically sets the importance weights of auxiliary tasks based on their\ncontributions to the generalisability of the main NMT task. Experiments on\nlow-resource NMT settings show the resulting automatically learned training\nschedulers are competitive with the best heuristics, and lead to up to +1.1\nBLEU score improvements.", "published": "2020-01-10 03:12:28", "link": "http://arxiv.org/abs/2001.03294v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Linking Social Media Posts to News with Siamese Transformers", "abstract": "Many computational social science projects examine online discourse\nsurrounding a specific trending topic. These works often involve the\nacquisition of large-scale corpora relevant to the event in question to analyze\naspects of the response to the event. Keyword searches present a\nprecision-recall trade-off and crowd-sourced annotations, while effective, are\ncostly. This work aims to enable automatic and accurate ad-hoc retrieval of\ncomments discussing a trending topic from a large corpus, using only a handful\nof seed news articles.", "published": "2020-01-10 04:39:44", "link": "http://arxiv.org/abs/2001.03303v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Towards Minimal Supervision BERT-based Grammar Error Correction", "abstract": "Current grammatical error correction (GEC) models typically consider the task\nas sequence generation, which requires large amounts of annotated data and\nlimit the applications in data-limited settings. We try to incorporate\ncontextual information from pre-trained language model to leverage annotation\nand benefit multilingual scenarios. Results show strong potential of\nBidirectional Encoder Representations from Transformers (BERT) in grammatical\nerror correction task.", "published": "2020-01-10 15:45:59", "link": "http://arxiv.org/abs/2001.03521v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Machine Learning Approaches for Amharic Parts-of-speech Tagging", "abstract": "Part-of-speech (POS) tagging is considered as one of the basic but necessary\ntools which are required for many Natural Language Processing (NLP)\napplications such as word sense disambiguation, information retrieval,\ninformation processing, parsing, question answering, and machine translation.\nPerformance of the current POS taggers in Amharic is not as good as that of the\ncontemporary POS taggers available for English and other European languages.\nThe aim of this work is to improve POS tagging performance for the Amharic\nlanguage, which was never above 91%. Usage of morphological knowledge, an\nextension of the existing annotated data, feature extraction, parameter tuning\nby applying grid search and the tagging algorithms have been examined and\nobtained significant performance difference from the previous works. We have\nused three different datasets for POS experiments.", "published": "2020-01-10 06:40:49", "link": "http://arxiv.org/abs/2001.03324v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Inductive Document Network Embedding with Topic-Word Attention", "abstract": "Document network embedding aims at learning representations for a structured\ntext corpus i.e. when documents are linked to each other. Recent algorithms\nextend network embedding approaches by incorporating the text content\nassociated with the nodes in their formulations. In most cases, it is hard to\ninterpret the learned representations. Moreover, little importance is given to\nthe generalization to new documents that are not observed within the network.\nIn this paper, we propose an interpretable and inductive document network\nembedding method. We introduce a novel mechanism, the Topic-Word Attention\n(TWA), that generates document representations based on the interplay between\nword and topic representations. We train these word and topic vectors through\nour general model, Inductive Document Network Embedding (IDNE), by leveraging\nthe connections in the document network. Quantitative evaluations show that our\napproach achieves state-of-the-art performance on various networks and we\nqualitatively show that our model produces meaningful and interpretable\nrepresentations of the words, topics and documents.", "published": "2020-01-10 10:14:07", "link": "http://arxiv.org/abs/2001.03369v1", "categories": ["cs.LG", "cs.CL", "cs.IR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Retouchdown: Adding Touchdown to StreetLearn as a Shareable Resource for\n  Language Grounding Tasks in Street View", "abstract": "The Touchdown dataset (Chen et al., 2019) provides instructions by human\nannotators for navigation through New York City streets and for resolving\nspatial descriptions at a given location. To enable the wider research\ncommunity to work effectively with the Touchdown tasks, we are publicly\nreleasing the 29k raw Street View panoramas needed for Touchdown. We follow the\nprocess used for the StreetLearn data release (Mirowski et al., 2019) to check\npanoramas for personally identifiable information and blur them as necessary.\nThese have been added to the StreetLearn dataset and can be obtained via the\nsame process as used previously for StreetLearn. We also provide a reference\nimplementation for both of the Touchdown tasks: vision and language navigation\n(VLN) and spatial description resolution (SDR). We compare our model results to\nthose given in Chen et al. (2019) and show that the panoramas we have added to\nStreetLearn fully support both Touchdown tasks and can be used effectively for\nfurther research and comparison.", "published": "2020-01-10 21:35:28", "link": "http://arxiv.org/abs/2001.03671v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Improving Dysarthric Speech Intelligibility Using Cycle-consistent\n  Adversarial Training", "abstract": "Dysarthria is a motor speech impairment affecting millions of people.\nDysarthric speech can be far less intelligible than those of non-dysarthric\nspeakers, causing significant communication difficulties. The goal of our work\nis to develop a model for dysarthric to healthy speech conversion using\nCycle-consistent GAN. Using 18,700 dysarthric and 8,610 healthy control Korean\nutterances that were recorded for the purpose of automatic recognition of voice\nkeyboard in a previous study, the generator is trained to transform dysarthric\nto healthy speech in the spectral domain, which is then converted back to\nspeech. Objective evaluation using automatic speech recognition of the\ngenerated utterance on a held-out test set shows that the recognition\nperformance is improved compared with the original dysarthic speech after\nperforming adversarial training, as the absolute WER has been lowered by 33.4%.\nIt demonstrates that the proposed GAN-based conversion method is useful for\nimproving dysarthric speech intelligibility.", "published": "2020-01-10 01:40:27", "link": "http://arxiv.org/abs/2001.04260v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
