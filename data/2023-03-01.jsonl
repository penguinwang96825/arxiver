{"title": "DIFFQG: Generating Questions to Summarize Factual Changes", "abstract": "Identifying the difference between two versions of the same article is useful\nto update knowledge bases and to understand how articles evolve. Paired texts\noccur naturally in diverse situations: reporters write similar news stories and\nmaintainers of authoritative websites must keep their information up to date.\nWe propose representing factual changes between paired documents as\nquestion-answer pairs, where the answer to the same question differs between\ntwo versions. We find that question-answer pairs can flexibly and concisely\ncapture the updated contents. Provided with paired documents, annotators\nidentify questions that are answered by one passage but answered differently or\ncannot be answered by the other. We release DIFFQG which consists of 759 QA\npairs and 1153 examples of paired passages with no factual change. These\nquestions are intended to be both unambiguous and information-seeking and\ninvolve complex edits, pushing beyond the capabilities of current question\ngeneration and factual change detection systems. Our dataset summarizes the\nchanges between two versions of the document as questions and answers, studying\nautomatic update summarization in a novel way.", "published": "2023-03-01 05:45:48", "link": "http://arxiv.org/abs/2303.00242v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hidden Markov Transformer for Simultaneous Machine Translation", "abstract": "Simultaneous machine translation (SiMT) outputs the target sequence while\nreceiving the source sequence, and hence learning when to start translating\neach target token is the core challenge for SiMT task. However, it is\nnon-trivial to learn the optimal moment among many possible moments of starting\ntranslating, as the moments of starting translating always hide inside the\nmodel and can only be supervised with the observed target sequence. In this\npaper, we propose a Hidden Markov Transformer (HMT), which treats the moments\nof starting translating as hidden events and the target sequence as the\ncorresponding observed events, thereby organizing them as a hidden Markov\nmodel. HMT explicitly models multiple moments of starting translating as the\ncandidate hidden events, and then selects one to generate the target token.\nDuring training, by maximizing the marginal likelihood of the target sequence\nover multiple moments of starting translating, HMT learns to start translating\nat the moments that target tokens can be generated more accurately. Experiments\non multiple SiMT benchmarks show that HMT outperforms strong baselines and\nachieves state-of-the-art performance.", "published": "2023-03-01 06:28:33", "link": "http://arxiv.org/abs/2303.00257v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language\n  Understanding Tasks", "abstract": "The GPT-3.5 models have demonstrated impressive performance in various\nNatural Language Processing (NLP) tasks, showcasing their strong understanding\nand reasoning capabilities. However, their robustness and abilities to handle\nvarious complexities of the open world have yet to be explored, which is\nespecially crucial in assessing the stability of models and is a key aspect of\ntrustworthy AI. In this study, we perform a comprehensive experimental analysis\nof GPT-3.5, exploring its robustness using 21 datasets (about 116K test\nsamples) with 66 text transformations from TextFlint that cover 9 popular\nNatural Language Understanding (NLU) tasks. Our findings indicate that while\nGPT-3.5 outperforms existing fine-tuned models on some tasks, it still\nencounters significant robustness degradation, such as its average performance\ndropping by up to 35.74\\% and 43.59\\% in natural language inference and\nsentiment analysis tasks, respectively. We also show that GPT-3.5 faces some\nspecific robustness challenges, including robustness instability, prompt\nsensitivity, and number sensitivity. These insights are valuable for\nunderstanding its limitations and guiding future research in addressing these\nchallenges to enhance GPT-3.5's overall performance and generalization\nabilities.", "published": "2023-03-01 07:39:01", "link": "http://arxiv.org/abs/2303.00293v1", "categories": ["cs.CL", "68-06", "I.2"], "primary_category": "cs.CL"}
{"title": "Competence-Based Analysis of Language Models", "abstract": "Despite the recent successes of large, pretrained neural language models\n(LLMs), comparatively little is known about the representations of linguistic\nstructure they learn during pretraining, which can lead to unexpected behaviors\nin response to prompt variation or distribution shift. To better understand\nthese models and behaviors, we introduce a general model analysis framework to\nstudy LLMs with respect to their representation and use of human-interpretable\nlinguistic properties. Our framework, CALM (Competence-based Analysis of\nLanguage Models), is designed to investigate LLM competence in the context of\nspecific tasks by intervening on models' internal representations of different\nlinguistic properties using causal probing, and measuring models' alignment\nunder these interventions with a given ground-truth causal model of the task.\nWe also develop a new approach for performing causal probing interventions\nusing gradient-based adversarial attacks, which can target a broader range of\nproperties and representations than prior techniques. Finally, we carry out a\ncase study of CALM using these interventions to analyze and compare LLM\ncompetence across a variety of lexical inference tasks, showing that CALM can\nbe used to explain behaviors across these tasks.", "published": "2023-03-01 08:53:36", "link": "http://arxiv.org/abs/2303.00333v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inline Citation Classification using Peripheral Context and\n  Time-evolving Augmentation", "abstract": "Citation plays a pivotal role in determining the associations among research\narticles. It portrays essential information in indicative, supportive, or\ncontrastive studies. The task of inline citation classification aids in\nextrapolating these relationships; However, existing studies are still immature\nand demand further scrutiny. Current datasets and methods used for inline\ncitation classification only use citation-marked sentences constraining the\nmodel to turn a blind eye to domain knowledge and neighboring contextual\nsentences. In this paper, we propose a new dataset, named 3Cext, which along\nwith the cited sentences, provides discourse information using the vicinal\nsentences to analyze the contrasting and entailing relationships as well as\ndomain information. We propose PeriCite, a Transformer-based deep neural\nnetwork that fuses peripheral sentences and domain knowledge. Our model\nachieves the state-of-the-art on the 3Cext dataset by +0.09 F1 against the best\nbaseline. We conduct extensive ablations to analyze the efficacy of the\nproposed dataset and model fusion methods.", "published": "2023-03-01 09:11:07", "link": "http://arxiv.org/abs/2303.00344v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Persian Benchmark for Joint Intent Detection and Slot Filling", "abstract": "Natural Language Understanding (NLU) is important in today's technology as it\nenables machines to comprehend and process human language, leading to improved\nhuman-computer interactions and advancements in fields such as virtual\nassistants, chatbots, and language-based AI systems. This paper highlights the\nsignificance of advancing the field of NLU for low-resource languages. With\nintent detection and slot filling being crucial tasks in NLU, the widely used\ndatasets ATIS and SNIPS have been utilized in the past. However, these datasets\nonly cater to the English language and do not support other languages. In this\nwork, we aim to address this gap by creating a Persian benchmark for joint\nintent detection and slot filling based on the ATIS dataset. To evaluate the\neffectiveness of our benchmark, we employ state-of-the-art methods for intent\ndetection and slot filling.", "published": "2023-03-01 10:57:21", "link": "http://arxiv.org/abs/2303.00408v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uzbek text summarization based on TF-IDF", "abstract": "The volume of information is increasing at an incredible rate with the rapid\ndevelopment of the Internet and electronic information services. Due to time\nconstraints, we don't have the opportunity to read all this information. Even\nthe task of analyzing textual data related to one field requires a lot of work.\nThe text summarization task helps to solve these problems. This article\npresents an experiment on summarization task for Uzbek language, the\nmethodology was based on text abstracting based on TF-IDF algorithm. Using this\ndensity function, semantically important parts of the text are extracted. We\nsummarize the given text by applying the n-gram method to important parts of\nthe whole text. The authors used a specially handcrafted corpus called \"School\ncorpus\" to evaluate the performance of the proposed method. The results show\nthat the proposed approach is effective in extracting summaries from Uzbek\nlanguage text and can potentially be used in various applications such as\ninformation retrieval and natural language processing. Overall, this research\ncontributes to the growing body of work on text summarization in\nunder-resourced languages.", "published": "2023-03-01 12:39:46", "link": "http://arxiv.org/abs/2303.00461v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uzbek text's correspondence with the educational potential of pupils: a\n  case study of the School corpus", "abstract": "One of the major challenges of an educational system is choosing appropriate\ncontent considering pupils' age and intellectual potential. In this article the\nexperiment of primary school grades (from 1st to 4th grades) is considered for\nautomatically determining the correspondence of an educational materials\nrecommended for pupils by using the School corpus where it includes the dataset\nof 25 school textbooks confirmed by the Ministry of preschool and school\neducation of the Republic of Uzbekistan. In this case, TF-IDF scores of the\ntexts are determined, they are converted into a vector representation, and the\ngiven educational materials are compared with the corresponding class of the\nSchool corpus using the cosine similarity algorithm. Based on the results of\nthe calculation, it is determined whether the given educational material is\nappropriate or not appropriate for the pupils' educational potential.", "published": "2023-03-01 12:46:00", "link": "http://arxiv.org/abs/2303.00465v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Authorship attribution for Differences between Literary Texts by\n  Bilingual Russian-French and Non-Bilingual French Authors", "abstract": "Do bilingual Russian-French authors of the end of the twentieth century such\nas Andre\\\"i Makine, Val\\'ery Afanassiev, Vladimir F\\'edorovski, Iegor Gran,\nLuba Jurgenson have common stylistic traits in the novels they wrote in French?\nCan we distinguish between them and non-bilingual French writers' texts? Is the\nphenomenon of interference observable in French texts of Russian authors? This\npaper applies authorship attribution methods including Support Vector Machine\n(SVM), $K$-Nearest Neighbors (KNN), Ridge classification, and Neural Network to\nanswer these questions.", "published": "2023-03-01 20:20:05", "link": "http://arxiv.org/abs/2303.13622v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RAMM: Retrieval-augmented Biomedical Visual Question Answering with\n  Multi-modal Pre-training", "abstract": "Vision-and-language multi-modal pretraining and fine-tuning have shown great\nsuccess in visual question answering (VQA). Compared to general domain VQA, the\nperformance of biomedical VQA suffers from limited data. In this paper, we\npropose a retrieval-augmented pretrain-and-finetune paradigm named RAMM for\nbiomedical VQA to overcome the data limitation issue. Specifically, we collect\na new biomedical dataset named PMCPM which offers patient-based image-text\npairs containing diverse patient situations from PubMed. Then, we pretrain the\nbiomedical multi-modal model to learn visual and textual representation for\nimage-text pairs and align these representations with image-text contrastive\nobjective (ITC). Finally, we propose a retrieval-augmented method to better use\nthe limited data. We propose to retrieve similar image-text pairs based on ITC\nfrom pretraining datasets and introduce a novel retrieval-attention module to\nfuse the representation of the image and the question with the retrieved images\nand texts. Experiments demonstrate that our retrieval-augmented\npretrain-and-finetune paradigm obtains state-of-the-art performance on\nMed-VQA2019, Med-VQA2021, VQARAD, and SLAKE datasets. Further analysis shows\nthat the proposed RAMM and PMCPM can enhance biomedical VQA performance\ncompared with previous resources and methods. We will open-source our dataset,\ncodes, and pretrained model.", "published": "2023-03-01 14:21:19", "link": "http://arxiv.org/abs/2303.00534v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MuAViC: A Multilingual Audio-Visual Corpus for Robust Speech Recognition\n  and Robust Speech-to-Text Translation", "abstract": "We introduce MuAViC, a multilingual audio-visual corpus for robust speech\nrecognition and robust speech-to-text translation providing 1200 hours of\naudio-visual speech in 9 languages. It is fully transcribed and covers 6\nEnglish-to-X translation as well as 6 X-to-English translation directions. To\nthe best of our knowledge, this is the first open benchmark for audio-visual\nspeech-to-text translation and the largest open benchmark for multilingual\naudio-visual speech recognition. Our baseline results show that MuAViC is\neffective for building noise-robust speech recognition and translation models.\nWe make the corpus available at https://github.com/facebookresearch/muavic.", "published": "2023-03-01 16:31:01", "link": "http://arxiv.org/abs/2303.00628v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Systematic Analysis of Vocabulary and BPE Settings for Optimal\n  Fine-tuning of NMT: A Case Study of In-domain Translation", "abstract": "The effectiveness of Neural Machine Translation (NMT) models largely depends\non the vocabulary used at training; small vocabularies can lead to\nout-of-vocabulary problems -- large ones, to memory issues. Subword (SW)\ntokenization has been successfully employed to mitigate these issues. The\nchoice of vocabulary and SW tokenization has a significant impact on both\ntraining and fine-tuning an NMT model. Fine-tuning is a common practice in\noptimizing an MT model with respect to new data. However, new data potentially\nintroduces new words (or tokens), which, if not taken into consideration, may\nlead to suboptimal performance. In addition, the distribution of tokens in the\nnew data can differ from the distribution of the original data. As such, the\noriginal SW tokenization model could be less suitable for the new data. Through\na systematic empirical evaluation, in this work we compare different strategies\nfor SW tokenization and vocabulary generation with the ultimate goal to uncover\nan optimal setting for fine-tuning a domain-specific model. Furthermore, we\ndeveloped several (in-domain) models, the best of which achieves 6 BLEU points\nimprovement over the baseline.", "published": "2023-03-01 18:26:47", "link": "http://arxiv.org/abs/2303.00722v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Building High-accuracy Multilingual ASR with Gated Language Experts and\n  Curriculum Training", "abstract": "We propose gated language experts and curriculum training to enhance\nmultilingual transformer transducer models without requiring language\nidentification (LID) input from users during inference. Our method incorporates\na gating mechanism and LID loss, enabling transformer experts to learn\nlanguage-specific information. By combining gated transformer experts with\nshared transformer layers, we construct multilingual transformer blocks and\nutilize linear experts to effectively regularize the joint network. The\ncurriculum training scheme leverages LID to guide the gated experts in\nimproving their respective language performance. Experimental results on a\nbilingual task involving English and Spanish demonstrate significant\nimprovements, with average relative word error reductions of 12.5% and 7.3%\ncompared to the baseline bilingual and monolingual models, respectively.\nNotably, our method achieves performance comparable to the upper-bound model\ntrained and inferred with oracle LID. Extending our approach to trilingual,\nquadrilingual, and pentalingual models reveals similar advantages to those\nobserved in the bilingual models, highlighting its ease of extension to\nmultiple languages.", "published": "2023-03-01 19:20:01", "link": "http://arxiv.org/abs/2303.00786v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and\n  Distillation of Rerankers", "abstract": "Many information retrieval tasks require large labeled datasets for\nfine-tuning. However, such datasets are often unavailable, and their utility\nfor real-world applications can diminish quickly due to domain shifts. To\naddress this challenge, we develop and motivate a method for using large\nlanguage models (LLMs) to generate large numbers of synthetic queries cheaply.\nThe method begins by generating a small number of synthetic queries using an\nexpensive LLM. After that, a much less expensive one is used to create large\nnumbers of synthetic queries, which are used to fine-tune a family of reranker\nmodels. These rerankers are then distilled into a single efficient retriever\nfor use in the target domain. We show that this technique boosts zero-shot\naccuracy in long-tail domains and achieves substantially lower latency than\nstandard reranking methods.", "published": "2023-03-01 20:21:23", "link": "http://arxiv.org/abs/2303.00807v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Soft Prompt Guided Joint Learning for Cross-Domain Sentiment Analysis", "abstract": "Aspect term extraction is a fundamental task in fine-grained sentiment\nanalysis, which aims at detecting customer's opinion targets from reviews on\nproduct or service. The traditional supervised models can achieve promising\nresults with annotated datasets, however, the performance dramatically\ndecreases when they are applied to the task of cross-domain aspect term\nextraction. Existing cross-domain transfer learning methods either directly\ninject linguistic features into Language models, making it difficult to\ntransfer linguistic knowledge to target domain, or rely on the fixed predefined\nprompts, which is time-consuming to construct the prompts over all potential\naspect term spans. To resolve the limitations, we propose a soft prompt-based\njoint learning method for cross domain aspect term extraction in this paper.\nSpecifically, by incorporating external linguistic features, the proposed\nmethod learn domain-invariant representations between source and target domains\nvia multiple objectives, which bridges the gap between domains with varied\ndistributions of aspect terms. Further, the proposed method interpolates a set\nof transferable soft prompts consisted of multiple learnable vectors that are\nbeneficial to detect aspect terms in target domain. Extensive experiments are\nconducted on the benchmark datasets and the experimental results demonstrate\nthe effectiveness of the proposed method for cross-domain aspect terms\nextraction.", "published": "2023-03-01 20:33:37", "link": "http://arxiv.org/abs/2303.00815v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Almanac: Retrieval-Augmented Language Models for Clinical Medicine", "abstract": "Large-language models have recently demonstrated impressive zero-shot\ncapabilities in a variety of natural language tasks such as summarization,\ndialogue generation, and question-answering. Despite many promising\napplications in clinical medicine, adoption of these models in real-world\nsettings has been largely limited by their tendency to generate incorrect and\nsometimes even toxic statements. In this study, we develop Almanac, a large\nlanguage model framework augmented with retrieval capabilities for medical\nguideline and treatment recommendations. Performance on a novel dataset of\nclinical scenarios (n = 130) evaluated by a panel of 5 board-certified and\nresident physicians demonstrates significant increases in factuality (mean of\n18% at p-value < 0.05) across all specialties, with improvements in\ncompleteness and safety. Our results demonstrate the potential for large\nlanguage models to be effective tools in the clinical decision-making process,\nwhile also emphasizing the importance of careful testing and deployment to\nmitigate their shortcomings.", "published": "2023-03-01 02:30:11", "link": "http://arxiv.org/abs/2303.01229v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Frauds Bargain Attack: Generating Adversarial Text Samples via Word\n  Manipulation Process", "abstract": "Recent research has revealed that natural language processing (NLP) models\nare vulnerable to adversarial examples. However, the current techniques for\ngenerating such examples rely on deterministic heuristic rules, which fail to\nproduce optimal adversarial examples. In response, this study proposes a new\nmethod called the Fraud's Bargain Attack (FBA), which uses a randomization\nmechanism to expand the search space and produce high-quality adversarial\nexamples with a higher probability of success. FBA uses the Metropolis-Hasting\nsampler, a type of Markov Chain Monte Carlo sampler, to improve the selection\nof adversarial examples from all candidates generated by a customized\nstochastic process called the Word Manipulation Process (WMP). The WMP method\nmodifies individual words in a contextually-aware manner through insertion,\nremoval, or substitution. Through extensive experiments, this study\ndemonstrates that FBA outperforms other methods in terms of attack success\nrate, imperceptibility and sentence quality.", "published": "2023-03-01 06:04:25", "link": "http://arxiv.org/abs/2303.01234v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can ChatGPT Assess Human Personalities? A General Evaluation Framework", "abstract": "Large Language Models (LLMs) especially ChatGPT have produced impressive\nresults in various areas, but their potential human-like psychology is still\nlargely unexplored. Existing works study the virtual personalities of LLMs but\nrarely explore the possibility of analyzing human personalities via LLMs. This\npaper presents a generic evaluation framework for LLMs to assess human\npersonalities based on Myers Briggs Type Indicator (MBTI) tests. Specifically,\nwe first devise unbiased prompts by randomly permuting options in MBTI\nquestions and adopt the average testing result to encourage more impartial\nanswer generation. Then, we propose to replace the subject in question\nstatements to enable flexible queries and assessments on different subjects\nfrom LLMs. Finally, we re-formulate the question instructions in a manner of\ncorrectness evaluation to facilitate LLMs to generate clearer responses. The\nproposed framework enables LLMs to flexibly assess personalities of different\ngroups of people. We further propose three evaluation metrics to measure the\nconsistency, robustness, and fairness of assessment results from\nstate-of-the-art LLMs including ChatGPT and GPT-4. Our experiments reveal\nChatGPT's ability to assess human personalities, and the average results\ndemonstrate that it can achieve more consistent and fairer assessments in spite\nof lower robustness against prompt biases compared with InstructGPT.", "published": "2023-03-01 06:16:14", "link": "http://arxiv.org/abs/2303.01248v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Understanding Natural Language Understanding Systems. A Critical\n  Analysis", "abstract": "The development of machines that {\\guillemotleft}talk like\nus{\\guillemotright}, also known as Natural Language Understanding (NLU)\nsystems, is the Holy Grail of Artificial Intelligence (AI), since language is\nthe quintessence of human intelligence. The brief but intense life of NLU\nresearch in AI and Natural Language Processing (NLP) is full of ups and downs,\nwith periods of high hopes that the Grail is finally within reach, typically\nfollowed by phases of equally deep despair and disillusion. But never has the\ntrust that we can build {\\guillemotleft}talking machines{\\guillemotright} been\nstronger than the one engendered by the last generation of NLU systems. But is\nit gold all that glitters in AI? do state-of-the-art systems possess something\ncomparable to the human knowledge of language? Are we at the dawn of a new era,\nin which the Grail is finally closer to us? In fact, the latest achievements of\nAI systems have sparkled, or better renewed, an intense scientific debate on\ntheir true language understanding capabilities. Some defend the idea that, yes,\nwe are on the right track, despite the limits that computational models still\nshow. Others are instead radically skeptic and even dismissal: The present\nlimits are not just contingent and temporary problems of NLU systems, but the\nsign of the intrinsic inadequacy of the epistemological and technological\nparadigm grounding them. This paper aims at contributing to such debate by\ncarrying out a critical analysis of the linguistic abilities of the most recent\nNLU systems. I contend that they incorporate important aspects of the way\nlanguage is learnt and processed by humans, but at the same time they lack key\ninterpretive and inferential skills that it is unlikely they can attain unless\nthey are integrated with structured knowledge and the ability to exploit it for\nlanguage use.", "published": "2023-03-01 08:32:55", "link": "http://arxiv.org/abs/2303.04229v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "ToxVis: Enabling Interpretability of Implicit vs. Explicit Toxicity\n  Detection Models with Interactive Visualization", "abstract": "The rise of hate speech on online platforms has led to an urgent need for\neffective content moderation. However, the subjective and multi-faceted nature\nof hateful online content, including implicit hate speech, poses significant\nchallenges to human moderators and content moderation systems. To address this\nissue, we developed ToxVis, a visually interactive and explainable tool for\nclassifying hate speech into three categories: implicit, explicit, and\nnon-hateful. We fine-tuned two transformer-based models using RoBERTa, XLNET,\nand GPT-3 and used deep learning interpretation techniques to provide\nexplanations for the classification results. ToxVis enables users to input\npotentially hateful text and receive a classification result along with a\nvisual explanation of which words contributed most to the decision. By making\nthe classification process explainable, ToxVis provides a valuable tool for\nunderstanding the nuances of hateful content and supporting more effective\ncontent moderation. Our research contributes to the growing body of work aimed\nat mitigating the harms caused by online hate speech and demonstrates the\npotential for combining state-of-the-art natural language processing models\nwith interpretable deep learning techniques to address this critical issue.\nFinally, ToxVis can serve as a resource for content moderators, social media\nplatforms, and researchers working to combat the spread of hate speech online.", "published": "2023-03-01 17:24:15", "link": "http://arxiv.org/abs/2303.09402v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Coarse-to-Fine Covid-19 Segmentation via Vision-Language Alignment", "abstract": "Segmentation of COVID-19 lesions can assist physicians in better diagnosis\nand treatment of COVID-19. However, there are few relevant studies due to the\nlack of detailed information and high-quality annotation in the COVID-19\ndataset. To solve the above problem, we propose C2FVL, a Coarse-to-Fine\nsegmentation framework via Vision-Language alignment to merge text information\ncontaining the number of lesions and specific locations of image information.\nThe introduction of text information allows the network to achieve better\nprediction results on challenging datasets. We conduct extensive experiments on\ntwo COVID-19 datasets including chest X-ray and CT, and the results demonstrate\nthat our proposed method outperforms other state-of-the-art segmentation\nmethods.", "published": "2023-03-01 07:01:29", "link": "http://arxiv.org/abs/2303.00279v1", "categories": ["eess.IV", "cs.CL", "cs.CV", "cs.IR"], "primary_category": "eess.IV"}
{"title": "Modeling Multiple User Interests using Hierarchical Knowledge for\n  Conversational Recommender System", "abstract": "A conversational recommender system (CRS) is a practical application for item\nrecommendation through natural language conversation. Such a system estimates\nuser interests for appropriate personalized recommendations. Users sometimes\nhave various interests in different categories or genres, but existing studies\nassume a unique user interest that can be covered by closely related items. In\nthis work, we propose to model such multiple user interests in CRS. We\ninvestigated its effects in experiments using the ReDial dataset and found that\nthe proposed method can recommend a wider variety of items than that of the\nbaseline CR-Walker.", "published": "2023-03-01 08:15:48", "link": "http://arxiv.org/abs/2303.00311v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "N-best T5: Robust ASR Error Correction using Multiple Input Hypotheses\n  and Constrained Decoding Space", "abstract": "Error correction models form an important part of Automatic Speech\nRecognition (ASR) post-processing to improve the readability and quality of\ntranscriptions. Most prior works use the 1-best ASR hypothesis as input and\ntherefore can only perform correction by leveraging the context within one\nsentence. In this work, we propose a novel N-best T5 model for this task, which\nis fine-tuned from a T5 model and utilizes ASR N-best lists as model input. By\ntransferring knowledge from the pre-trained language model and obtaining richer\ninformation from the ASR decoding space, the proposed approach outperforms a\nstrong Conformer-Transducer baseline. Another issue with standard error\ncorrection is that the generation process is not well-guided. To address this a\nconstrained decoding process, either based on the N-best list or an ASR\nlattice, is used which allows additional information to be propagated.", "published": "2023-03-01 12:32:34", "link": "http://arxiv.org/abs/2303.00456v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Universal Question-Answering Platform for Knowledge Graphs", "abstract": "Knowledge from diverse application domains is organized as knowledge graphs\n(KGs) that are stored in RDF engines accessible in the web via SPARQL\nendpoints. Expressing a well-formed SPARQL query requires information about the\ngraph structure and the exact URIs of its components, which is impractical for\nthe average user. Question answering (QA) systems assist by translating natural\nlanguage questions to SPARQL. Existing QA systems are typically based on\napplication-specific human-curated rules, or require prior information,\nexpensive pre-processing and model adaptation for each targeted KG. Therefore,\nthey are hard to generalize to a broad set of applications and KGs.\n  In this paper, we propose KGQAn, a universal QA system that does not need to\nbe tailored to each target KG. Instead of curated rules, KGQAn introduces a\nnovel formalization of question understanding as a text generation problem to\nconvert a question into an intermediate abstract representation via a neural\nsequence-to-sequence model. We also develop a just-in-time linker that maps at\nquery time the abstract representation to a SPARQL query for a specific KG,\nusing only the publicly accessible APIs and the existing indices of the RDF\nstore, without requiring any pre-processing. Our experiments with several real\nKGs demonstrate that KGQAn is easily deployed and outperforms by a large margin\nthe state-of-the-art in terms of quality of answers and processing time,\nespecially for arbitrary KGs, unseen during the training.", "published": "2023-03-01 15:35:32", "link": "http://arxiv.org/abs/2303.00595v2", "categories": ["cs.AI", "cs.CL", "cs.DB"], "primary_category": "cs.AI"}
{"title": "Synthetic Cross-accent Data Augmentation for Automatic Speech\n  Recognition", "abstract": "The awareness for biased ASR datasets or models has increased notably in\nrecent years. Even for English, despite a vast amount of available training\ndata, systems perform worse for non-native speakers. In this work, we improve\nan accent-conversion model (ACM) which transforms native US-English speech into\naccented pronunciation. We include phonetic knowledge in the ACM training to\nprovide accurate feedback about how well certain pronunciation patterns were\nrecovered in the synthesized waveform. Furthermore, we investigate the\nfeasibility of learned accent representations instead of static embeddings.\nGenerated data was then used to train two state-of-the-art ASR systems. We\nevaluated our approach on native and non-native English datasets and found that\nsynthetically accented data helped the ASR to better understand speech from\nseen accents. This observation did not translate to unseen accents, and it was\nnot observed for a model that had been pre-trained exclusively with native\nspeech.", "published": "2023-03-01 20:05:19", "link": "http://arxiv.org/abs/2303.00802v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Domain-adapted large language models for classifying nuclear medicine\n  reports", "abstract": "With the growing use of transformer-based language models in medicine, it is\nunclear how well these models generalize to nuclear medicine which has\ndomain-specific vocabulary and unique reporting styles. In this study, we\nevaluated the value of domain adaptation in nuclear medicine by adapting\nlanguage models for the purpose of 5-point Deauville score prediction based on\nclinical 18F-fluorodeoxyglucose (FDG) PET/CT reports. We retrospectively\nretrieved 4542 text reports and 1664 images for FDG PET/CT lymphoma exams from\n2008-2018 in our clinical imaging database. Deauville scores were removed from\nthe reports and then the remaining text in the reports was used as the model\ninput. Multiple general-purpose transformer language models were used to\nclassify the reports into Deauville scores 1-5. We then adapted the models to\nthe nuclear medicine domain using masked language modeling and assessed its\nimpact on classification performance. The language models were compared against\nvision models, a multimodal vision language model, and a nuclear medicine\nphysician with seven-fold Monte Carlo cross validation, reported are the mean\nand standard deviations. Domain adaption improved all language models. For\nexample, BERT improved from 61.3% five-class accuracy to 65.7% following domain\nadaptation. The best performing model (domain-adapted RoBERTa) achieved a\nfive-class accuracy of 77.4%, which was better than the physician's performance\n(66%), the best vision model's performance (48.1), and was similar to the\nmultimodal model's performance (77.2). Domain adaptation improved the\nperformance of large language models in interpreting nuclear medicine text\nreports.", "published": "2023-03-01 09:48:39", "link": "http://arxiv.org/abs/2303.01258v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ParrotTTS: Text-to-Speech synthesis by exploiting self-supervised\n  representations", "abstract": "We present ParrotTTS, a modularized text-to-speech synthesis model leveraging\ndisentangled self-supervised speech representations. It can train a\nmulti-speaker variant effectively using transcripts from a single speaker.\nParrotTTS adapts to a new language in low resource setup and generalizes to\nlanguages not seen while training the self-supervised backbone. Moreover,\nwithout training on bilingual or parallel examples, ParrotTTS can transfer\nvoices across languages while preserving the speaker specific characteristics,\ne.g., synthesizing fluent Hindi speech using a French speaker's voice and\naccent. We present extensive results in monolingual and multi-lingual\nscenarios. ParrotTTS outperforms state-of-the-art multi-lingual TTS models\nusing only a fraction of paired data as latter.", "published": "2023-03-01 17:23:12", "link": "http://arxiv.org/abs/2303.01261v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "SpeechPrompt v2: Prompt Tuning for Speech Classification Tasks", "abstract": "Prompt tuning is a technology that tunes a small set of parameters to steer a\npre-trained language model (LM) to directly generate the output for downstream\ntasks. Recently, prompt tuning has demonstrated its storage and computation\nefficiency in both natural language processing (NLP) and speech processing\nfields. These advantages have also revealed prompt tuning as a candidate\napproach to serving pre-trained LM for multiple tasks in a unified manner. For\nspeech processing, SpeechPrompt shows its high parameter efficiency and\ncompetitive performance on a few speech classification tasks. However, whether\nSpeechPrompt is capable of serving a large number of tasks is unanswered. In\nthis work, we propose SpeechPrompt v2, a prompt tuning framework capable of\nperforming a wide variety of speech classification tasks, covering multiple\nlanguages and prosody-related tasks. The experiment result shows that\nSpeechPrompt v2 achieves performance on par with prior works with less than\n0.15M trainable parameters in a unified framework.", "published": "2023-03-01 18:47:41", "link": "http://arxiv.org/abs/2303.00733v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Grounded Decoding: Guiding Text Generation with Grounded Models for\n  Embodied Agents", "abstract": "Recent progress in large language models (LLMs) has demonstrated the ability\nto learn and leverage Internet-scale knowledge through pre-training with\nautoregressive models. Unfortunately, applying such models to settings with\nembodied agents, such as robots, is challenging due to their lack of experience\nwith the physical world, inability to parse non-language observations, and\nignorance of rewards or safety constraints that robots may require. On the\nother hand, language-conditioned robotic policies that learn from interaction\ndata can provide the necessary grounding that allows the agent to be correctly\nsituated in the real world, but such policies are limited by the lack of\nhigh-level semantic understanding due to the limited breadth of the interaction\ndata available for training them. Thus, if we want to make use of the semantic\nknowledge in a language model while still situating it in an embodied setting,\nwe must construct an action sequence that is both likely according to the\nlanguage model and also realizable according to grounded models of the\nenvironment. We frame this as a problem similar to probabilistic filtering:\ndecode a sequence that both has high probability under the language model and\nhigh probability under a set of grounded model objectives. We demonstrate how\nsuch grounded models can be obtained across three simulation and real-world\ndomains, and that the proposed decoding strategy is able to solve complex,\nlong-horizon embodiment tasks in a robotic setting by leveraging the knowledge\nof both models. The project's website can be found at\ngrounded-decoding.github.io.", "published": "2023-03-01 22:58:50", "link": "http://arxiv.org/abs/2303.00855v2", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Leveraging Redundancy in Multiple Audio Signals for Far-Field Speech\n  Recognition", "abstract": "To achieve robust far-field automatic speech recognition (ASR), existing\ntechniques typically employ an acoustic front end (AFE) cascaded with a neural\ntransducer (NT) ASR model. The AFE output, however, could be unreliable, as the\nbeamforming output in AFE is steered to a wrong direction. A promising way to\naddress this issue is to exploit the microphone signals before the beamforming\nstage and after the acoustic echo cancellation (post-AEC) in AFE. We argue that\nboth, post-AEC and AFE outputs, are complementary and it is possible to\nleverage the redundancy between these signals to compensate for potential AFE\nprocessing errors. We present two fusion networks to explore this redundancy\nand aggregate these multi-channel (MC) signals: (1) Frequency-LSTM based, and\n(2) Convolutional Neural Network based fusion networks. We augment the MC\nfusion networks to a conformer transducer model and train it in an end-to-end\nfashion. Our experimental results on commercial virtual assistant tasks\ndemonstrate that using the AFE output and two post-AEC signals with fusion\nnetworks offers up to 25.9% word error rate (WER) relative improvement over the\nmodel using the AFE output only, at the cost of <= 2% parameter increase.", "published": "2023-03-01 17:39:46", "link": "http://arxiv.org/abs/2303.00692v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Ego-noise reduction of a mobile robot using noise spatial covariance\n  matrix learning and minimum variance distortionless response", "abstract": "The performance of speech and events recognition systems significantly\nimproved recently thanks to deep learning methods. However, some of these tasks\nremain challenging when algorithms are deployed on robots due to the unseen\nmechanical noise and electrical interference generated by their actuators while\ntraining the neural networks. Ego-noise reduction as a preprocessing step\ntherefore can help solve this issue when using pre-trained speech and event\nrecognition algorithms on robots. In this paper, we propose a new method to\nreduce ego-noise using only a microphone array and less than two minute of\nnoise recordings. Using Principal Component Analysis (PCA), the best covariance\nmatrix candidate is selected from a dictionary created online during\ncalibration and used with the Minimum Variance Distortionless Response (MVDR)\nbeamformer. Results show that the proposed method runs in real-time, improves\nthe signal-to-distortion ratio (SDR) by up to 10 dB, decreases the word error\nrate (WER) by 55\\% in some cases and increases the Average Precision (AP) of\nevent detection by up to 0.2.", "published": "2023-03-01 21:29:54", "link": "http://arxiv.org/abs/2303.00829v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "PCF: ECAPA-TDNN with Progressive Channel Fusion for Speaker Verification", "abstract": "ECAPA-TDNN is currently the most popular TDNN-series model for speaker\nverification, which refreshed the state-of-the-art(SOTA) performance of TDNN\nmodels. However, one-dimensional convolution has a global receptive field over\nthe feature channel. It destroys the time-frequency relevance of the\nspectrogram. Besides, as ECAPA-TDNN only has five layers, a much shallower\nstructure compared to ResNet restricts the capability to generate deep\nrepresentations. To further improve ECAPA-TDNN, we propose a progressive\nchannel fusion strategy that splits the spectrogram across the feature channel\nand gradually expands the receptive field through the network. Secondly, we\nenlarge the model by extending the depth and adding branches. Our proposed\nmodel achieves EER with 0.718 and minDCF(0.01) with 0.0858 on vox1o, relatively\nimproved 16.1\\% and 19.5\\% compared with ECAPA-TDNN-large.", "published": "2023-03-01 03:12:28", "link": "http://arxiv.org/abs/2303.00204v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Distance-based Weight Transfer from Near-field to Far-field Speaker\n  Verification", "abstract": "The scarcity of labeled far-field speech is a constraint for training\nsuperior far-field speaker verification systems. Fine-tuning the model\npre-trained on large-scale near-field speech substantially outperforms training\nfrom scratch. However, the fine-tuning method suffers from two\nlimitations--catastrophic forgetting and overfitting. In this paper, we propose\na weight transfer regularization(WTR) loss to constrain the distance of the\nweights between the pre-trained model with large-scale near-field speech and\nthe fine-tuned model through a small number of far-field speech. With the WTR\nloss, the fine-tuning process takes advantage of the previously acquired\ndiscriminative ability from the large-scale near-field speech without\ncatastrophic forgetting. Meanwhile, we use the PAC-Bayes generalization theory\nto analyze the generalization bound of the fine-tuned model with the WTR loss.\nThe analysis result indicates that the WTR term makes the fine-tuned model have\na tighter generalization upper bound. Moreover, we explore three kinds of norm\ndistance for weight transfer, which are L1-norm distance, L2-norm distance and\nMax-norm distance. Finally, we evaluate the effectiveness of the WTR loss on\nVoxCeleb (pre-trained dataset) and FFSVC (fine-tuned dataset) datasets.", "published": "2023-03-01 06:38:02", "link": "http://arxiv.org/abs/2303.00264v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CAM++: A Fast and Efficient Network for Speaker Verification Using\n  Context-Aware Masking", "abstract": "Time delay neural network (TDNN) has been proven to be efficient for speaker\nverification. One of its successful variants, ECAPA-TDNN, achieved\nstate-of-the-art performance at the cost of much higher computational\ncomplexity and slower inference speed. This makes it inadequate for scenarios\nwith demanding inference rate and limited computational resources. We are thus\ninterested in finding an architecture that can achieve the performance of\nECAPA-TDNN and the efficiency of vanilla TDNN. In this paper, we propose an\nefficient network based on context-aware masking, namely CAM++, which uses\ndensely connected time delay neural network (D-TDNN) as backbone and adopts a\nnovel multi-granularity pooling to capture contextual information at different\nlevels. Extensive experiments on two public benchmarks, VoxCeleb and CN-Celeb,\ndemonstrate that the proposed architecture outperforms other mainstream speaker\nverification systems with lower computational cost and faster inference speed.", "published": "2023-03-01 08:50:31", "link": "http://arxiv.org/abs/2303.00332v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "First-shot anomaly sound detection for machine condition monitoring: A\n  domain generalization baseline", "abstract": "This paper provides a baseline system for First-shot-compliant unsupervised\nanomaly detection (ASD) for machine condition monitoring. First-shot ASD does\nnot allow systems to do machine-type dependent hyperparameter tuning or tool\nensembling based on the performance metric calculated with the grand truth. To\nshow benchmark performance for First-shot ASD, this paper proposes an anomaly\nsound detection system that works on the domain generalization task in the\nDetection and Classification of Acoustic Scenes and Events (DCASE) 2022\nChallenge Task 2: \"Unsupervised Anomalous Sound Detection for Machine Condition\nMonitoring Applying Domain Generalization Technique\" while complying with the\nFirst-shot requirements introduced in the DCASE 2023 Challenge Task 2\n(DCASE2023T2). A simple autoencoder based implementation combined with\nselective Mahalanobis metric is implemented as a baseline system. The\nperformance evaluation is conducted to set the target benchmark for the\nforthcoming DCASE2023T2. Source code of the baseline system will be available\non GitHub: https://github.com/nttcslab/dcase2023_task2_baseline_ae .", "published": "2023-03-01 12:29:10", "link": "http://arxiv.org/abs/2303.00455v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards domain generalisation in ASR with elitist sampling and ensemble\n  knowledge distillation", "abstract": "Knowledge distillation has widely been used for model compression and domain\nadaptation for speech applications. In the presence of multiple teachers,\nknowledge can easily be transferred to the student by averaging the models\noutput. However, previous research shows that the student do not adapt well\nwith such combination. This paper propose to use an elitist sampling strategy\nat the output of ensemble teacher models to select the best-decoded utterance\ngenerated by completely out-of-domain teacher models for generalizing unseen\ndomain. The teacher models are trained on AMI, LibriSpeech and WSJ while the\nstudent is adapted for the Switchboard data. The results show that with the\nselection strategy based on the individual models posteriors the student model\nachieves a better WER compared to all the teachers and baselines with a minimum\nabsolute improvement of about 8.4 percent. Furthermore, an insights on the\nmodel adaptation with out-of-domain data has also been studied via correlation\nanalysis.", "published": "2023-03-01 14:49:53", "link": "http://arxiv.org/abs/2303.00550v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "audb -- Sharing and Versioning of Audio and Annotation Data in Python", "abstract": "Driven by the need for larger and more diverse datasets to pre-train and\nfine-tune increasingly complex machine learning models, the number of datasets\nis rapidly growing. audb is an open-source Python library that supports\nversioning and documentation of audio datasets. It aims to provide a\nstandardized and simple user-interface to publish, maintain, and access the\nannotations and audio files of a dataset. To efficiently store the data on a\nserver, audb automatically resolves dependencies between versions of a dataset\nand only uploads newly added or altered files when a new version is published.\nThe library supports partial loading of a dataset and local caching for fast\naccess. audb is a lightweight library and can be interfaced from any machine\nlearning library. It supports the management of datasets on a single PC, within\na university or company, or within a whole research community.", "published": "2023-03-01 16:48:22", "link": "http://arxiv.org/abs/2303.00645v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "WhisperX: Time-Accurate Speech Transcription of Long-Form Audio", "abstract": "Large-scale, weakly-supervised speech recognition models, such as Whisper,\nhave demonstrated impressive results on speech recognition across domains and\nlanguages. However, their application to long audio transcription via buffered\nor sliding window approaches is prone to drifting, hallucination & repetition;\nand prohibits batched transcription due to their sequential nature. Further,\ntimestamps corresponding each utterance are prone to inaccuracies and\nword-level timestamps are not available out-of-the-box. To overcome these\nchallenges, we present WhisperX, a time-accurate speech recognition system with\nword-level timestamps utilising voice activity detection and forced phoneme\nalignment. In doing so, we demonstrate state-of-the-art performance on\nlong-form transcription and word segmentation benchmarks. Additionally, we show\nthat pre-segmenting audio with our proposed VAD Cut & Merge strategy improves\ntranscription quality and enables a twelve-fold transcription speedup via\nbatched inference.", "published": "2023-03-01 18:59:13", "link": "http://arxiv.org/abs/2303.00747v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "I Know Your Feelings Before You Do: Predicting Future Affective\n  Reactions in Human-Computer Dialogue", "abstract": "Current Spoken Dialogue Systems (SDSs) often serve as passive listeners that\nrespond only after receiving user speech. To achieve human-like dialogue, we\npropose a novel future prediction architecture that allows an SDS to anticipate\nfuture affective reactions based on its current behaviors before the user\nspeaks. In this work, we investigate two scenarios: speech and laughter. In\nspeech, we propose to predict the user's future emotion based on its temporal\nrelationship with the system's current emotion and its causal relationship with\nthe system's current Dialogue Act (DA). In laughter, we propose to predict the\noccurrence and type of the user's laughter using the system's laughter\nbehaviors in the current turn. Preliminary analysis of human-robot dialogue\ndemonstrated synchronicity in the emotions and laughter displayed by the human\nand robot, as well as DA-emotion causality in their dialogue. This verifies\nthat our architecture can contribute to the development of an anticipatory SDS.", "published": "2023-03-01 00:36:27", "link": "http://arxiv.org/abs/2303.00146v4", "categories": ["cs.HC", "cs.RO", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "DTW-SiameseNet: Dynamic Time Warped Siamese Network for Mispronunciation\n  Detection and Correction", "abstract": "Personal Digital Assistants (PDAs) - such as Siri, Alexa and Google\nAssistant, to name a few - play an increasingly important role to access\ninformation and complete tasks spanning multiple domains, and by diverse groups\nof users. A text-to-speech (TTS) module allows PDAs to interact in a natural,\nhuman-like manner, and play a vital role when the interaction involves people\nwith visual impairments or other disabilities. To cater to the needs of a\ndiverse set of users, inclusive TTS is important to recognize and pronounce\ncorrectly text in different languages and dialects. Despite great progress in\nspeech synthesis, the pronunciation accuracy of named entities in a\nmulti-lingual setting still has a large room for improvement. Existing\napproaches to correct named entity (NE) mispronunciations, like retraining\nGrapheme-to-Phoneme (G2P) models, or maintaining a TTS pronunciation\ndictionary, require expensive annotation of the ground truth pronunciation,\nwhich is also time consuming. In this work, we present a highly-precise,\nPDA-compatible pronunciation learning framework for the task of TTS\nmispronunciation detection and correction. In addition, we also propose a novel\nmispronunciation detection model called DTW-SiameseNet, which employs metric\nlearning with a Siamese architecture for Dynamic Time Warping (DTW) with\ntriplet loss. We demonstrate that a locale-agnostic, privacy-preserving\nsolution to the problem of TTS mispronunciation detection is feasible. We\nevaluate our approach on a real-world dataset, and a corpus of NE\npronunciations of an anonymized audio dataset of person names recorded by\nparticipants from 10 different locales. Human evaluation shows our proposed\napproach improves pronunciation accuracy on average by ~6% compared to strong\nphoneme-based and audio-based baselines.", "published": "2023-03-01 01:53:11", "link": "http://arxiv.org/abs/2303.00171v1", "categories": ["cs.LG", "cs.AI", "eess.AS"], "primary_category": "cs.LG"}
{"title": "On the Audio-visual Synchronization for Lip-to-Speech Synthesis", "abstract": "Most lip-to-speech (LTS) synthesis models are trained and evaluated under the\nassumption that the audio-video pairs in the dataset are perfectly\nsynchronized. In this work, we show that the commonly used audio-visual\ndatasets, such as GRID, TCD-TIMIT, and Lip2Wav, can have data asynchrony\nissues. Training lip-to-speech with such datasets may further cause the model\nasynchrony issue -- that is, the generated speech and the input video are out\nof sync. To address these asynchrony issues, we propose a synchronized\nlip-to-speech (SLTS) model with an automatic synchronization mechanism (ASM) to\ncorrect data asynchrony and penalize model asynchrony. We further demonstrate\nthe limitation of the commonly adopted evaluation metrics for LTS with\nasynchronous test data and introduce an audio alignment frontend before the\nmetrics sensitive to time alignment for better evaluation. We compare our\nmethod with state-of-the-art approaches on conventional and time-aligned\nmetrics to show the benefits of synchronization training.", "published": "2023-03-01 13:35:35", "link": "http://arxiv.org/abs/2303.00502v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Extending DNN-based Multiplicative Masking to Deep Subband Filtering for\n  Improved Dereverberation", "abstract": "In this paper, we present a scheme for extending deep neural network-based\nmultiplicative maskers to deep subband filters for speech restoration in the\ntime-frequency domain. The resulting method can be generically applied to any\ndeep neural network providing masks in the time-frequency domain, while\nrequiring only few more trainable parameters and a computational overhead that\nis negligible for state-of-the-art neural networks. We demonstrate that the\nresulting deep subband filtering scheme outperforms multiplicative masking for\ndereverberation, while leaving the denoising performance virtually the same. We\nargue that this is because deep subband filtering in the time-frequency domain\nfits the subband approximation often assumed in the dereverberation literature,\nwhereas multiplicative masking corresponds to the narrowband approximation\ngenerally employed for denoising.", "published": "2023-03-01 14:10:21", "link": "http://arxiv.org/abs/2303.00529v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "READ Avatars: Realistic Emotion-controllable Audio Driven Avatars", "abstract": "We present READ Avatars, a 3D-based approach for generating 2D avatars that\nare driven by audio input with direct and granular control over the emotion.\nPrevious methods are unable to achieve realistic animation due to the\nmany-to-many nature of audio to expression mappings. We alleviate this issue by\nintroducing an adversarial loss in the audio-to-expression generation process.\nThis removes the smoothing effect of regression-based models and helps to\nimprove the realism and expressiveness of the generated avatars. We note\nfurthermore, that audio should be directly utilized when generating mouth\ninteriors and that other 3D-based methods do not attempt this. We address this\nwith audio-conditioned neural textures, which are resolution-independent. To\nevaluate the performance of our method, we perform quantitative and qualitative\nexperiments, including a user study. We also propose a new metric for comparing\nhow well an actor's emotion is reconstructed in the generated avatar. Our\nresults show that our approach outperforms state of the art audio-driven avatar\ngeneration methods across several metrics. A demo video can be found at\n\\url{https://youtu.be/QSyMl3vV0pA}", "published": "2023-03-01 18:56:43", "link": "http://arxiv.org/abs/2303.00744v1", "categories": ["cs.CV", "cs.GR", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "DISPLACE Challenge: DIarization of SPeaker and LAnguage in\n  Conversational Environments", "abstract": "In multilingual societies, social conversations often involve code-mixed\nspeech. The current speech technology may not be well equipped to extract\ninformation from multi-lingual multi-speaker conversations. The DISPLACE\nchallenge entails a first-of-kind task to benchmark speaker and language\ndiarization on the same data, as the data contains multi-speaker conversations\nin multilingual code-mixed speech. The challenge attempts to highlight\noutstanding issues in speaker diarization (SD) in multilingual settings with\ncode-mixing. Further, language diarization (LD) in multi-speaker settings also\nintroduces new challenges, where the system has to disambiguate speaker\nswitches with code switches. For this challenge, a natural multilingual,\nmulti-speaker conversational dataset is distributed for development and\nevaluation purposes. The systems are evaluated on single-channel far-field\nrecordings. We also release a baseline system and report the highlights of the\nsystem submissions.", "published": "2023-03-01 21:31:17", "link": "http://arxiv.org/abs/2303.00830v3", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
