{"title": "Exploring subgraph complementation to bounded degree graphs", "abstract": "Graph modification problems are computational tasks where the goal is to\nchange an input graph $G$ using operations from a fixed set, in order to make\nthe resulting graph satisfy a target property, which usually entails membership\nto a desired graph class $\\mathcal{C}$. Some well-known examples of operations\ninclude vertex-deletion, edge-deletion, edge-addition and edge-contraction. In\nthis paper we address an operation known as subgraph complement. Given a graph\n$G$ and a subset $S$ of its vertices, the subgraph complement $G \\oplus S$ is\nthe graph resulting of complementing the edge set of the subgraph induced by\n$S$ in $G$. We say that a graph $H$ is a subgraph complement of $G$ if there is\nan $S$ such that $H$ is isomorphic to $G \\oplus S$. For a graph class\n$\\mathcal{C}$, subgraph complementation to $\\mathcal{C}$ is the problem of\ndeciding, for a given graph $G$, whether $G$ has a subgraph complement in\n$\\mathcal{C}$. This problem has been studied and its complexity has been\nsettled for many classes $\\mathcal{C}$ such as $\\mathcal{H}$-free graphs, for\nvarious families $\\mathcal{H}$, and for classes of bounded degeneracy. In this\nwork, we focus on classes graphs of minimum/maximum degree upper/lower bounded\nby some value $k$. In particular, we answer an open question of Antony et al.\n[Information Processing Letters 188, 106530 (2025)], by showing that subgraph\ncomplementation to $\\mathcal{C}$ is NP-complete when $\\mathcal{C}$ is the class\nof graphs of minimum degree at least $k$, if $k$ is part of the input. We also\nshow that subgraph complementation to $k$-regular parameterized by $k$ is\nfixed-parameter tractable.", "published": "2025-02-21 18:57:42", "link": "http://arxiv.org/abs/2502.15675v1", "categories": ["cs.DM", "05C07, 05C76, 05C85"], "primary_category": "cs.DM"}
{"title": "List Decoding Quotient Reed-Muller Codes", "abstract": "Reed-Muller codes consist of evaluations of $n$-variate polynomials over a\nfinite field $\\mathbb{F}$ with degree at most $d$. Much like every linear code,\nReed-Muller codes can be characterized by constraints, where a codeword is\nvalid if and only if it satisfies all \\emph{degree-$d$} constraints.\n  For a subset $\\tilde{X} \\subseteq \\mathbb{F}^n$, we introduce the notion of\n\\emph{$\\tilde{X}$-quotient} Reed-Muller code. A function $F : \\tilde{X}\n\\rightarrow \\mathbb{F}$ is a valid codeword in the quotient code if it\nsatisfies all the constraints of degree-$d$ polynomials \\emph{lying in\n$\\tilde{X}$}. This gives rise to a novel phenomenon: a quotient codeword may\nhave \\emph{many} extensions to original codewords. This weakens the connection\nbetween original codewords and quotient codewords which introduces a richer\nrange of behaviors along with substantial new challenges.\n  Our goal is to answer the following question: what properties of $\\tilde{X}$\nwill imply that the quotient code inherits its distance and list-decoding\nradius from the original code? We address this question using techniques\ndeveloped by Bhowmick and Lovett [BL14], identifying key properties of\n$\\mathbb{F}^n$ used in their proof and extending them to general subsets\n$\\tilde{X} \\subseteq \\mathbb{F}^n$. By introducing a new tool, we overcome the\nnovel challenge in analyzing the quotient code that arises from the weak\nconnection between original and quotient codewords. This enables us to apply\nknown results from additive combinatorics and algebraic geometry [KZ18, KZ19,\nLZ21] to show that when $\\tilde{X}$ is a \\emph{high rank variety},\n$\\tilde{X}$-quotient Reed-Muller codes inherit the distance and list-decoding\nparameters from the original Reed-Muller codes.", "published": "2025-02-21 18:19:47", "link": "http://arxiv.org/abs/2502.15650v1", "categories": ["cs.CC", "cs.DM"], "primary_category": "cs.CC"}
{"title": "An efficient algorithm for generating transmission irregular trees", "abstract": "The transmission of a vertex in a connected graph is the sum of distances\nfrom that vertex to all the other vertices. A connected graph is transmission\nirregular if any two distinct vertices have different transmissions. We present\nan efficient algorithm that generates all the transmission irregular trees up\nto a given order, up to isomorphism.", "published": "2025-02-21 13:26:17", "link": "http://arxiv.org/abs/2502.15453v1", "categories": ["cs.DM", "math.CO", "68R10, 68R05, 05C12, 05C05"], "primary_category": "cs.DM"}
{"title": "Fast and Furious: A study on Monotonicity and Speed in Cops-and-Robber Games", "abstract": "In this paper, we study different variants of the Cops-and-Robber game with\nrespect to cop- and robber\\-/monotonicity.\n  We study a visible and invisible robber and variants where the robber is\nlazy, thus can only move when the cops announce to move on top of him.\n  In all four combinations, we also vary the number $s$ of edges that the\nrobber can traverse in a single round, called speed.\n  We complete the study of the unbounded speed case by showing that, besides\nthe active variants, also the visible lazy variant has both the cop- and\nrobber\\-/monotonicity property.\n  Furthermore, we prove that the cop\\-/monotone invisible lazy copwidth\ncharacterizes path-width, while the non\\-/monotone and robber\\-/monotone is\nknown to characterize tree-width, thus these variants differ even in the\nunbounded speed case.\n  We find that, even with speed restriction, the cop\\-/monotone invisible\ncopwidth and the robber\\-/monotone invisible active copwidth all characterize\npath-width.\n  On the other hand, we show that the path-width of a graph can be arbitrarily\nlarger than the number of cops needed to win the non\\-/monotone invisible\nactive variant.\n  To complete our study of cop\\-/monotone variants, we show that also in the\nvisible variants the cop\\-/monotone copwidth can be arbitrarily larger than the\nnon\\-/monotone.\n  Regarding robber\\-/monotonicity, for all speeds $s\\geq 4$, we give graphs\nwhere the non\\-/monotone and robber\\-/monotone copwidth differ.\n  On the other hand, we prove that there is a function that bounds the\nrobber\\-/monotone copwidth in terms of the non\\-/monotone copwidth and the\nspeed, thus the gap between the variants is bounded.\n  This proof also yields that a graph class has bounded expansion if and only\nif, for every speed $s$, the number of cops needed in any robber\\-/monotone\nlazy variant is bounded by some constant $c(s)$.", "published": "2025-02-21 11:48:03", "link": "http://arxiv.org/abs/2502.15396v1", "categories": ["cs.DM", "math.CO"], "primary_category": "cs.DM"}
{"title": "Approximate weighted 3-coloring", "abstract": "The paper considers the NP-hard graph vertex coloring problem, which differs\nfrom traditional problems in which it is required to color vertices with a\ngiven (or minimal) number of colors so that adjacent vertices have different\ncolors. In the problem under consideration, a simple edge-weighted graph is\ngiven. It is required to color its vertices in 3 colors to minimize the total\nweight of monochromatic (one-color) edges, i.e. edges with the same colors of\ntheir end vertices. This problem is poorly investigated. Previously, we\ndeveloped graph decomposition algorithms that, in particular, allowed us to\nconstruct lower bounds for the optimum, as well as several greedy algorithms.\nIn this paper, several new approximation algorithms are proposed. Among them\nare variable neighborhood search, simulated annealing, genetic algorithm and\ngraph clustering with further finding the optimal coloring in each cluster. A\nnumerical experiment was conducted on random graphs, as well as on real\ncommunication graphs. The characteristics of the algorithms are presented both\nin tables and graphically. The developed algorithms have shown high efficiency.", "published": "2025-02-21 05:19:11", "link": "http://arxiv.org/abs/2502.15216v1", "categories": ["cs.DM"], "primary_category": "cs.DM"}
{"title": "Network topology of the Euro Area interbank market", "abstract": "The rapidly increasing availability of large amounts of granular financial\ndata, paired with the advances of big data related technologies induces the\nneed of suitable analytics that can represent and extract meaningful\ninformation from such data. In this paper we propose a multi-layer network\napproach to distill the Euro Area (EA) banking system in different distinct\nlayers. Each layer of the network represents a specific type of financial\nrelationship between banks, based on various sources of EA granular data\ncollections. The resulting multi-layer network allows one to describe, analyze\nand compare the topology and structure of EA banks from different perspectives,\neventually yielding a more complete picture of the financial market. This\ngranular information representation has the potential to enable researchers and\npractitioners to better apprehend financial system dynamics as well as to\nsupport financial policies to manage and monitor financial risk from a more\nholistic point of view.", "published": "2025-02-21 17:31:31", "link": "http://arxiv.org/abs/2502.15611v1", "categories": ["q-fin.ST", "cs.CE", "stat.CO", "91B70, 05C82, 62P20, 91G80", "G.3; I.5.3; I.6.5; J.4"], "primary_category": "q-fin.ST"}
{"title": "Clustered Network Connectedness: A New Measurement Framework with Application to Global Equity Markets", "abstract": "Network connections, both across and within markets, are central in countless\neconomic contexts. In recent decades, a large literature has developed and\napplied flexible methods for measuring network connectedness and its evolution,\nbased on variance decompositions from vector autoregressions (VARs), as in\nDiebold and Yilmaz (2014). Those VARs are, however, typically identified using\nfull orthogonalization (Sims, 1980), or no orthogonalization (Koop, Pesaran,\nand Potter, 1996; Pesaran and Shin, 1998), which, although useful, are special\nand extreme cases of a more general framework that we develop in this paper. In\nparticular, we allow network nodes to be connected in \"clusters\", such as asset\nclasses, industries, regions, etc., where shocks are orthogonal across clusters\n(Sims style orthogonalized identification) but correlated within clusters\n(Koop-Pesaran-Potter-Shin style generalized identification), so that the\nordering of network nodes is relevant across clusters but irrelevant within\nclusters. After developing the clustered connectedness framework, we apply it\nin a detailed empirical exploration of sixteen country equity markets spanning\nthree global regions.", "published": "2025-02-21 13:33:57", "link": "http://arxiv.org/abs/2502.15458v1", "categories": ["econ.EM", "q-fin.ST", "stat.AP"], "primary_category": "econ.EM"}
{"title": "Multi-Agent Stock Prediction Systems: Machine Learning Models, Simulations, and Real-Time Trading Strategies", "abstract": "This paper presents a comprehensive study on stock price prediction,\nleveragingadvanced machine learning (ML) and deep learning (DL) techniques to\nimprove financial forecasting accuracy. The research evaluates the performance\nof various recurrent neural network (RNN) architectures, including Long\nShort-Term Memory (LSTM) networks, Gated Recurrent Units (GRU), and\nattention-based models. These models are assessed for their ability to capture\ncomplex temporal dependencies inherent in stock market data. Our findings show\nthat attention-based models outperform other architectures, achieving the\nhighest accuracy by capturing both short and long-term dependencies. This study\ncontributes valuable insights into AI-driven financial forecasting, offering\npractical guidance for developing more accurate and efficient trading systems.", "published": "2025-02-21 06:36:16", "link": "http://arxiv.org/abs/2502.15853v1", "categories": ["q-fin.ST", "cs.LG"], "primary_category": "q-fin.ST"}
{"title": "Latent Factor Models Meets Instructions:Goal-conditioned Latent Factor\n  Discovery without Task Supervision", "abstract": "Instruction-following LLMs have recently allowed systems to discover hidden\nconcepts from a collection of unstructured documents based on a natural\nlanguage description of the purpose of the discovery (i.e., goal). Still, the\nquality of the discovered concepts remains mixed, as it depends heavily on\nLLM's reasoning ability and drops when the data is noisy or beyond LLM's\nknowledge. We present Instruct-LF, a goal-oriented latent factor discovery\nsystem that integrates LLM's instruction-following ability with statistical\nmodels to handle large, noisy datasets where LLM reasoning alone falls short.\n  Instruct-LF uses LLMs to propose fine-grained, goal-related properties from\ndocuments, estimates their presence across the dataset, and applies\ngradient-based optimization to uncover hidden factors, where each factor is\nrepresented by a cluster of co-occurring properties. We evaluate latent factors\nproduced by Instruct-LF on movie recommendation, text-world navigation, and\nlegal document categorization tasks. These interpretable representations\nimprove downstream task performance by 5-52% than the best baselines and were\npreferred 1.8 times as often as the best alternative, on average, in human\nevaluation.", "published": "2025-02-21 02:03:08", "link": "http://arxiv.org/abs/2502.15147v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating the Adaptive Robustness with Knowledge Conflicts in\n  LLM-based Multi-Agent Systems", "abstract": "Recent advances in Large Language Models (LLMs) have upgraded them from\nsophisticated text generators to autonomous agents capable of corporation and\ntool use in multi-agent systems (MASs). However, the robustness of these\nLLM-based MASs, especially under knowledge conflicts, remains unclear. In this\npaper, we design four comprehensive metrics to investigate the robustness of\nMASs when facing mild or task-critical knowledge conflicts. We first analyze\nmild knowledge conflicts introduced by heterogeneous agents and find that they\ndo not harm system robustness but instead improve collaborative\ndecision-making. Next, we investigate task-critical knowledge conflicts by\nsynthesizing knowledge conflicts and embedding them into one of the agents. Our\nresults show that these conflicts have surprisingly little to no impact on MAS\nrobustness. Furthermore, we observe that MASs demonstrate certain\nself-repairing capabilities by reducing their reliance on knowledge conflicts\nand adopting alternative solution paths to maintain stability. Finally, we\nconduct ablation studies on the knowledge conflict number, agent number, and\ninteraction rounds, finding that the self-repairing capability of MASs has\nintrinsic limits, and all findings hold consistently across various factors.\nOur code is publicly available at\nhttps://github.com/wbw625/MultiAgentRobustness.", "published": "2025-02-21 02:24:43", "link": "http://arxiv.org/abs/2502.15153v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "mStyleDistance: Multilingual Style Embeddings and their Evaluation", "abstract": "Style embeddings are useful for stylistic analysis and style transfer;\nhowever, only English style embeddings have been made available. We introduce\nMultilingual StyleDistance (mStyleDistance), a multilingual style embedding\nmodel trained using synthetic data and contrastive learning. We train the model\non data from nine languages and create a multilingual STEL-or-Content benchmark\n(Wegmann et al., 2022) that serves to assess the embeddings' quality. We also\nemploy our embeddings in an authorship verification task involving different\nlanguages. Our results show that mStyleDistance embeddings outperform existing\nmodels on these multilingual style benchmarks and generalize well to unseen\nfeatures and languages. We make our model publicly available at\nhttps://huggingface.co/StyleDistance/mstyledistance .", "published": "2025-02-21 03:11:41", "link": "http://arxiv.org/abs/2502.15168v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unveiling Attractor Cycles in Large Language Models: A Dynamical Systems\n  View of Successive Paraphrasing", "abstract": "Dynamical systems theory provides a framework for analyzing iterative\nprocesses and evolution over time. Within such systems, repetitive\ntransformations can lead to stable configurations, known as attractors,\nincluding fixed points and limit cycles. Applying this perspective to large\nlanguage models (LLMs), which iteratively map input text to output text,\nprovides a principled approach to characterizing long-term behaviors.\nSuccessive paraphrasing serves as a compelling testbed for exploring such\ndynamics, as paraphrases re-express the same underlying meaning with linguistic\nvariation. Although LLMs are expected to explore a diverse set of paraphrases\nin the text space, our study reveals that successive paraphrasing converges to\nstable periodic states, such as 2-period attractor cycles, limiting linguistic\ndiversity. This phenomenon is attributed to the self-reinforcing nature of\nLLMs, as they iteratively favour and amplify certain textual forms over others.\nThis pattern persists with increasing generation randomness or alternating\nprompts and LLMs. These findings underscore inherent constraints in LLM\ngenerative capability, while offering a novel dynamical systems perspective for\nstudying their expressive potential.", "published": "2025-02-21 04:46:57", "link": "http://arxiv.org/abs/2502.15208v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "LightMamba: Efficient Mamba Acceleration on FPGA with Quantization and\n  Hardware Co-design", "abstract": "State space models (SSMs) like Mamba have recently attracted much attention.\nCompared to Transformer-based large language models (LLMs), Mamba achieves\nlinear computation complexity with the sequence length and demonstrates\nsuperior performance. However, Mamba is hard to accelerate due to the scattered\nactivation outliers and the complex computation dependency, rendering existing\nLLM accelerators inefficient. In this paper, we propose LightMamba that\nco-designs the quantization algorithm and FPGA accelerator architecture for\nefficient Mamba inference. We first propose an FPGA-friendly post-training\nquantization algorithm that features rotation-assisted quantization and\npower-of-two SSM quantization to reduce the majority of computation to 4-bit.\nWe further design an FPGA accelerator that partially unrolls the Mamba\ncomputation to balance the efficiency and hardware costs. Through computation\nreordering as well as fine-grained tiling and fusion, the hardware utilization\nand memory efficiency of the accelerator get drastically improved. We implement\nLightMamba on Xilinx Versal VCK190 FPGA and achieve 4.65x to 6.06x higher\nenergy efficiency over the GPU baseline. When evaluated on Alveo U280 FPGA,\nLightMamba reaches 93 tokens/s, which is 1.43x that of the GPU baseline.", "published": "2025-02-21 07:23:23", "link": "http://arxiv.org/abs/2502.15260v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Training-free LLM-based Approach to General Chinese Character Error\n  Correction", "abstract": "Chinese spelling correction (CSC) is a crucial task that aims to correct\ncharacter errors in Chinese text. While conventional CSC focuses on character\nsubstitution errors caused by mistyping, two other common types of character\nerrors, missing and redundant characters, have received less attention. These\nerrors are often excluded from CSC datasets during the annotation process or\nignored during evaluation, even when they have been annotated. This issue\nlimits the practicality of the CSC task. To address this issue, we introduce\nthe task of General Chinese Character Error Correction (C2EC), which focuses on\nall three types of character errors. We construct a high-quality C2EC benchmark\nby combining and manually verifying data from CCTC and Lemon datasets. We\nextend the training-free prompt-free CSC method to C2EC by using Levenshtein\ndistance for handling length changes and leveraging an additional prompt-based\nlarge language model (LLM) to improve performance. Experiments show that our\nmethod enables a 14B-parameter LLM to be on par with models nearly 50 times\nlarger on both conventional CSC and C2EC tasks, without any fine-tuning.", "published": "2025-02-21 07:48:54", "link": "http://arxiv.org/abs/2502.15266v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing the Inner Workings of Transformers in Compositional\n  Generalization", "abstract": "The compositional generalization abilities of neural models have been sought\nafter for human-like linguistic competence. The popular method to evaluate such\nabilities is to assess the models' input-output behavior. However, that does\nnot reveal the internal mechanisms, and the underlying competence of such\nmodels in compositional generalization remains unclear. To address this\nproblem, we explore the inner workings of a Transformer model by finding an\nexisting subnetwork that contributes to the generalization performance and by\nperforming causal analyses on how the model utilizes syntactic features. We\nfind that the model depends on syntactic features to output the correct answer,\nbut that the subnetwork with much better generalization performance than the\nwhole model relies on a non-compositional algorithm in addition to the\nsyntactic features. We also show that the subnetwork improves its\ngeneralization performance relatively slowly during the training compared to\nthe in-distribution one, and the non-compositional solution is acquired in the\nearly stages of the training.", "published": "2025-02-21 08:07:53", "link": "http://arxiv.org/abs/2502.15277v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stepwise Informativeness Search for Improving LLM Reasoning", "abstract": "Advances in Large Language Models (LLMs) have significantly improved\nmulti-step reasoning through generating free-text rationales. However, recent\nstudies show that LLMs tend to lose focus over the middle of long contexts.\nThis raises concerns that as reasoning progresses, LLMs may overlook\ninformation in earlier steps when decoding subsequent steps, leading to\ngenerate unreliable and redundant rationales. To address this, we propose\nguiding LLMs to generate more accurate and concise step-by-step rationales by\n(1) proactively referencing information from underutilized prior steps, and (2)\nminimizing redundant information between new and existing steps. We introduce\nstepwise informativeness search, an inference-time tree search framework\nincorporating two selection heuristics: grounding-guided selection which\nprioritizes steps paying higher attention over underutilized steps; and\nnovelty-guided selection which encourages steps with novel conclusions. During\nrationale generation, we use a self-grounding strategy that prompts LLMs to\nexplicitly reference relevant prior steps to provide premises before deduction\nat each step. Experimental results on four reasoning datasets demonstrate that\nour approach improves reasoning accuracy by generating higher-quality\nrationales with reduced errors and redundancy.", "published": "2025-02-21 09:39:27", "link": "http://arxiv.org/abs/2502.15335v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tokenization is Sensitive to Language Variation", "abstract": "Variation in language is ubiquitous and often systematically linked to\nregional, social, and contextual factors. Tokenizers split texts into smaller\nunits and might behave differently for less common linguistic forms. This might\naffect downstream LLM performance differently on two types of tasks: Tasks\nwhere the model should be robust to language variation (e.g., for semantic\ntasks like NLI, labels do not depend on whether a text uses British or American\nspelling) and tasks where the model should be sensitive to language variation\n(e.g., for form-based tasks like authorship verification, labels depend on\nwhether a text uses British or American spelling). We pre-train BERT base\nmodels for the popular Byte-Pair Encoding algorithm to investigate how key\nalgorithmic design choices impact downstream models' performances: fitting\ncorpus, pre-tokenizer and vocabulary size. We find that the best tokenizer\nvaries on the two task types -- with the pre-tokenizer having the biggest\nimpact on performance. Further, we introduce a new approach to estimate\ntokenizer impact on downstream LLM performance, showing significant improvement\nover techniques like R\\'enyi efficiency. We encourage more work on language\nvariation and its relation to tokenizers and thus LLM performance.", "published": "2025-02-21 09:58:54", "link": "http://arxiv.org/abs/2502.15343v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs\n  Complex Reasoning", "abstract": "In-context learning (ICL) can significantly enhance the complex reasoning\ncapabilities of large language models (LLMs), with the key lying in the\nselection and ordering of demonstration examples. Previous methods typically\nrelied on simple features to measure the relevance between examples. We argue\nthat these features are not sufficient to reflect the intrinsic connections\nbetween examples. In this study, we propose a curriculum ICL strategy guided by\nproblem-solving logic. We select demonstration examples by analyzing the\nproblem-solving logic and order them based on curriculum learning.\nSpecifically, we constructed a problem-solving logic instruction set based on\nthe BREAK dataset and fine-tuned a language model to analyze the\nproblem-solving logic of examples. Subsequently, we selected appropriate\ndemonstration examples based on problem-solving logic and assessed their\ndifficulty according to the number of problem-solving steps. In accordance with\nthe principles of curriculum learning, we ordered the examples from easy to\nhard to serve as contextual prompts. Experimental results on multiple\nbenchmarks indicate that our method outperforms previous ICL approaches in\nterms of performance and efficiency, effectively enhancing the complex\nreasoning capabilities of LLMs. Our project will be publicly available\nsubsequently.", "published": "2025-02-21 12:00:10", "link": "http://arxiv.org/abs/2502.15401v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Textual-to-Visual Iterative Self-Verification for Slide Generation", "abstract": "Generating presentation slides is a time-consuming task that urgently\nrequires automation. Due to their limited flexibility and lack of automated\nrefinement mechanisms, existing autonomous LLM-based agents face constraints in\nreal-world applicability. We decompose the task of generating missing\npresentation slides into two key components: content generation and layout\ngeneration, aligning with the typical process of creating academic slides.\nFirst, we introduce a content generation approach that enhances coherence and\nrelevance by incorporating context from surrounding slides and leveraging\nsection retrieval strategies. For layout generation, we propose a\ntextual-to-visual self-verification process using a LLM-based Reviewer +\nRefiner workflow, transforming complex textual layouts into intuitive visual\nformats. This modality transformation simplifies the task, enabling accurate\nand human-like review and refinement. Experiments show that our approach\nsignificantly outperforms baseline methods in terms of alignment, logical flow,\nvisual appeal, and readability.", "published": "2025-02-21 12:21:09", "link": "http://arxiv.org/abs/2502.15412v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MHQA: A Diverse, Knowledge Intensive Mental Health Question Answering\n  Challenge for Language Models", "abstract": "Mental health remains a challenging problem all over the world, with issues\nlike depression, anxiety becoming increasingly common. Large Language Models\n(LLMs) have seen a vast application in healthcare, specifically in answering\nmedical questions. However, there is a lack of standard benchmarking datasets\nfor question answering (QA) in mental health. Our work presents a novel\nmultiple choice dataset, MHQA (Mental Health Question Answering), for\nbenchmarking Language models (LMs). Previous mental health datasets have\nfocused primarily on text classification into specific labels or disorders.\nMHQA, on the other hand, presents question-answering for mental health focused\non four key domains: anxiety, depression, trauma, and obsessive/compulsive\nissues, with diverse question types, namely, factoid, diagnostic, prognostic,\nand preventive. We use PubMed abstracts as the primary source for QA. We\ndevelop a rigorous pipeline for LLM-based identification of information from\nabstracts based on various selection criteria and converting it into QA pairs.\nFurther, valid QA pairs are extracted based on post-hoc validation criteria.\nOverall, our MHQA dataset consists of 2,475 expert-verified gold standard\ninstances called MHQA-gold and ~56.1k pairs pseudo labeled using external\nmedical references. We report F1 scores on different LLMs along with few-shot\nand supervised fine-tuning experiments, further discussing the insights for the\nscores.", "published": "2025-02-21 12:37:58", "link": "http://arxiv.org/abs/2502.15418v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pub-Guard-LLM: Detecting Fraudulent Biomedical Articles with Reliable\n  Explanations", "abstract": "A significant and growing number of published scientific articles is found to\ninvolve fraudulent practices, posing a serious threat to the credibility and\nsafety of research in fields such as medicine. We propose Pub-Guard-LLM, the\nfirst large language model-based system tailored to fraud detection of\nbiomedical scientific articles. We provide three application modes for\ndeploying Pub-Guard-LLM: vanilla reasoning, retrieval-augmented generation, and\nmulti-agent debate. Each mode allows for textual explanations of predictions.\nTo assess the performance of our system, we introduce an open-source benchmark,\nPubMed Retraction, comprising over 11K real-world biomedical articles,\nincluding metadata and retraction labels. We show that, across all modes,\nPub-Guard-LLM consistently surpasses the performance of various baselines and\nprovides more reliable explanations, namely explanations which are deemed more\nrelevant and coherent than those generated by the baselines when evaluated by\nmultiple assessment methods. By enhancing both detection performance and\nexplainability in scientific fraud detection, Pub-Guard-LLM contributes to\nsafeguarding research integrity with a novel, effective, open-source tool.", "published": "2025-02-21 12:54:56", "link": "http://arxiv.org/abs/2502.15429v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mixup Model Merge: Enhancing Model Merging Performance through\n  Randomized Linear Interpolation", "abstract": "Model merging integrates the parameters of multiple models into a unified\nmodel, combining their diverse capabilities. Existing model merging methods are\noften constrained by fixed parameter merging ratios. In this study, we propose\nMixup Model Merge (M$^3$), an innovative approach inspired by the Mixup data\naugmentation technique. This method merges the parameters of two large language\nmodels (LLMs) by randomly generating linear interpolation ratios, allowing for\na more flexible and comprehensive exploration of the parameter space. Extensive\nexperiments demonstrate the superiority of our proposed M$^3$ method in merging\nfine-tuned LLMs: (1) it significantly improves performance across multiple\ntasks, (2) it enhances LLMs' out-of-distribution (OOD) robustness and\nadversarial robustness, (3) it achieves superior results when combined with\nsparsification techniques such as DARE, and (4) it offers a simple yet\nefficient solution that does not require additional computational resources. In\nconclusion, M$^3$ is a simple yet effective model merging method that\nsignificantly enhances the performance of the merged model by randomly\ngenerating contribution ratios for two fine-tuned LLMs. The code is available\nat https://github.com/MLGroupJLU/MixupModelMerge.", "published": "2025-02-21 13:01:26", "link": "http://arxiv.org/abs/2502.15434v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scale-Distribution Decoupling: Enabling Stable and Effective Training of\n  Large Language Models", "abstract": "Training stability is a persistent challenge in the pre-training of large\nlanguage models (LLMs), particularly for architectures such as Post-Norm\nTransformers, which are prone to gradient explosion and dissipation. In this\npaper, we propose Scale-Distribution Decoupling (SDD), a novel approach that\nstabilizes training by explicitly decoupling the scale and distribution of the\nweight matrix in fully-connected layers. SDD applies a normalization mechanism\nto regulate activations and a learnable scaling vector to maintain\nwell-conditioned gradients, effectively preventing $\\textbf{gradient explosion\nand dissipation}$. This separation improves optimization efficiency,\nparticularly in deep networks, by ensuring stable gradient propagation.\nExperimental results demonstrate that our method stabilizes training across\nvarious LLM architectures and outperforms existing techniques in different\nnormalization configurations. Furthermore, the proposed method is lightweight\nand compatible with existing frameworks, making it a practical solution for\nstabilizing LLM training. Code is available at https://github.com/kaihemo/SDD.", "published": "2025-02-21 14:49:34", "link": "http://arxiv.org/abs/2502.15499v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DReSD: Dense Retrieval for Speculative Decoding", "abstract": "Speculative decoding (SD) accelerates Large Language Model (LLM) generation\nby using an efficient draft model to propose the next few tokens, which are\nverified by the LLM in a single forward call, reducing latency while preserving\nits outputs. We focus on retrieval-based SD where the draft model retrieves the\nnext tokens from a non-parametric datastore. Sparse retrieval (REST), which\noperates on the surface form of strings, is currently the dominant paradigm due\nto its simplicity and scalability. However, its effectiveness is limited due to\nthe usage of short contexts and exact string matching. Instead, we introduce\nDense Retrieval for Speculative Decoding (DReSD), a novel framework that uses\napproximate nearest neighbour search with contextualised token embeddings to\nretrieve the most semantically relevant token sequences for SD. Extensive\nexperiments show that DReSD achieves (on average) 87% higher acceptance rates,\n65% longer accepted tokens and 19% faster generation speeds compared to sparse\nretrieval (REST).", "published": "2025-02-21 16:32:28", "link": "http://arxiv.org/abs/2502.15572v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interpreting and Steering LLMs with Mutual Information-based\n  Explanations on Sparse Autoencoders", "abstract": "Large language models (LLMs) excel at handling human queries, but they can\noccasionally generate flawed or unexpected responses. Understanding their\ninternal states is crucial for understanding their successes, diagnosing their\nfailures, and refining their capabilities. Although sparse autoencoders (SAEs)\nhave shown promise for interpreting LLM internal representations, limited\nresearch has explored how to better explain SAE features, i.e., understanding\nthe semantic meaning of features learned by SAE. Our theoretical analysis\nreveals that existing explanation methods suffer from the frequency bias issue,\nwhere they emphasize linguistic patterns over semantic concepts, while the\nlatter is more critical to steer LLM behaviors. To address this, we propose\nusing a fixed vocabulary set for feature interpretations and designing a mutual\ninformation-based objective, aiming to better capture the semantic meaning\nbehind these features. We further propose two runtime steering strategies that\nadjust the learned feature activations based on their corresponding\nexplanations. Empirical results show that, compared to baselines, our method\nprovides more discourse-level explanations and effectively steers LLM behaviors\nto defend against jailbreak attacks. These findings highlight the value of\nexplanations for steering LLM behaviors in downstream applications. We will\nrelease our code and data once accepted.", "published": "2025-02-21 16:36:42", "link": "http://arxiv.org/abs/2502.15576v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chats-Grid: An Iterative Retrieval Q&A Optimization Scheme Leveraging\n  Large Model and Retrieval Enhancement Generation in smart grid", "abstract": "With rapid advancements in artificial intelligence, question-answering (Q&A)\nsystems have become essential in intelligent search engines, virtual\nassistants, and customer service platforms. However, in dynamic domains like\nsmart grids, conventional retrieval-augmented generation(RAG) Q&A systems face\nchallenges such as inadequate retrieval quality, irrelevant responses, and\ninefficiencies in handling large-scale, real-time data streams. This paper\nproposes an optimized iterative retrieval-based Q&A framework called Chats-Grid\ntailored for smart grid environments. In the pre-retrieval phase, Chats-Grid\nadvanced query expansion ensures comprehensive coverage of diverse data\nsources, including sensor readings, meter records, and control system\nparameters. During retrieval, Best Matching 25(BM25) sparse retrieval and BAAI\nGeneral Embedding(BGE) dense retrieval in Chats-Grid are combined to process\nvast, heterogeneous datasets effectively. Post-retrieval, a fine-tuned large\nlanguage model uses prompt engineering to assess relevance, filter irrelevant\nresults, and reorder documents based on contextual accuracy. The model further\ngenerates precise, context-aware answers, adhering to quality criteria and\nemploying a self-checking mechanism for enhanced reliability. Experimental\nresults demonstrate Chats-Grid's superiority over state-of-the-art methods in\nfidelity, contextual recall, relevance, and accuracy by 2.37%, 2.19%, and 3.58%\nrespectively. This framework advances smart grid management by improving\ndecision-making and user interactions, fostering resilient and adaptive smart\ngrid infrastructures.", "published": "2025-02-21 16:47:01", "link": "http://arxiv.org/abs/2502.15583v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SafeInt: Shielding Large Language Models from Jailbreak Attacks via\n  Safety-Aware Representation Intervention", "abstract": "With the widespread real-world deployment of large language models (LLMs),\nensuring their behavior complies with safety standards has become crucial.\nJailbreak attacks exploit vulnerabilities in LLMs to induce undesirable\nbehavior, posing a significant threat to LLM safety. Previous defenses often\nfail to achieve both effectiveness and efficiency simultaneously. Defenses from\na representation perspective offer new insights, but existing interventions\ncannot dynamically adjust representations based on the harmfulness of the\nqueries. To address this limitation while ensuring both effectiveness and\nefficiency, we propose SafeIntervention (SafeInt), a novel defense method that\nshields LLMs from jailbreak attacks through safety-aware representation\nintervention. SafeInt is built on our analysis of the representations of\njailbreak samples. It adjusts representation distributions of jailbreak samples\nthrough intervention to align them with the representations of unsafe samples\nwhile minimizing unnecessary perturbations to jailbreak-irrelevant\nrepresentations. We conduct comprehensive experiments covering six jailbreak\nattacks, two jailbreak datasets, and two utility benchmarks. Experimental\nresults demonstrate that SafeInt outperforms all baselines in defending LLMs\nagainst jailbreak attacks while largely maintaining utility. Additionally, we\nevaluate SafeInt against adaptive attacks and verify its effectiveness in\nmitigating real-time attacks.", "published": "2025-02-21 17:12:35", "link": "http://arxiv.org/abs/2502.15594v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robust Bias Detection in MLMs and its Application to Human Trait Ratings", "abstract": "There has been significant prior work using templates to study bias against\ndemographic attributes in MLMs. However, these have limitations: they overlook\nrandom variability of templates and target concepts analyzed, assume equality\namongst templates, and overlook bias quantification. Addressing these, we\npropose a systematic statistical approach to assess bias in MLMs, using mixed\nmodels to account for random effects, pseudo-perplexity weights for sentences\nderived from templates and quantify bias using statistical effect sizes.\nReplicating prior studies, we match on bias scores in magnitude and direction\nwith small to medium effect sizes. Next, we explore the novel problem of gender\nbias in the context of $\\textit{personality}$ and $\\textit{character}$ traits,\nacross seven MLMs (base and large). We find that MLMs vary; ALBERT is unbiased\nfor binary gender but the most biased for non-binary $\\textit{neo}$, while\nRoBERTa-large is the most biased for binary gender but shows small to no bias\nfor $\\textit{neo}$. There is some alignment of MLM bias and findings in\npsychology (human perspective) - in $\\textit{agreeableness}$ with RoBERTa-large\nand $\\textit{emotional stability}$ with BERT-large. There is general agreement\nfor the remaining 3 personality dimensions: both sides observe at most small\ndifferences across gender. For character traits, human studies on gender bias\nare limited thus comparisons are not feasible.", "published": "2025-02-21 17:18:02", "link": "http://arxiv.org/abs/2502.15600v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LaTIM: Measuring Latent Token-to-Token Interactions in Mamba Models", "abstract": "State space models (SSMs), such as Mamba, have emerged as an efficient\nalternative to transformers for long-context sequence modeling. However,\ndespite their growing adoption, SSMs lack the interpretability tools that have\nbeen crucial for understanding and improving attention-based architectures.\nWhile recent efforts provide insights into Mamba's internal mechanisms, they do\nnot explicitly decompose token-wise contributions, leaving gaps in\nunderstanding how Mamba selectively processes sequences across layers. In this\nwork, we introduce LaTIM, a novel token-level decomposition method for both\nMamba-1 and Mamba-2 that enables fine-grained interpretability. We extensively\nevaluate our method across diverse tasks, including machine translation,\ncopying, and retrieval-based generation, demonstrating its effectiveness in\nrevealing Mamba's token-to-token interaction patterns.", "published": "2025-02-21 17:33:59", "link": "http://arxiv.org/abs/2502.15612v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Close Look at Decomposition-based XAI-Methods for Transformer Language\n  Models", "abstract": "Various XAI attribution methods have been recently proposed for the\ntransformer architecture, allowing for insights into the decision-making\nprocess of large language models by assigning importance scores to input tokens\nand intermediate representations. One class of methods that seems very\npromising in this direction includes decomposition-based approaches, i.e.,\nXAI-methods that redistribute the model's prediction logit through the network,\nas this value is directly related to the prediction. In the previous literature\nwe note though that two prominent methods of this category, namely ALTI-Logit\nand LRP, have not yet been analyzed in juxtaposition and hence we propose to\nclose this gap by conducting a careful quantitative evaluation w.r.t. ground\ntruth annotations on a subject-verb agreement task, as well as various\nqualitative inspections, using BERT, GPT-2 and LLaMA-3 as a testbed. Along the\nway we compare and extend the ALTI-Logit and LRP methods, including the\nrecently proposed AttnLRP variant, from an algorithmic and implementation\nperspective. We further incorporate in our benchmark two widely-used\ngradient-based attribution techniques. Finally, we make our carefullly\nconstructed benchmark dataset for evaluating attributions on language models,\nas well as our code, publicly available in order to foster evaluation of\nXAI-methods on a well-defined common ground.", "published": "2025-02-21 19:09:40", "link": "http://arxiv.org/abs/2502.15886v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modality-Aware Neuron Pruning for Unlearning in Multimodal Large\n  Language Models", "abstract": "Generative models such as Large Language Models (LLMs) and Multimodal Large\nLanguage Models (MLLMs) trained on massive datasets can lead them to memorize\nand inadvertently reveal sensitive information, raising ethical and privacy\nconcerns. While some prior works have explored this issue in the context of\nLLMs, it presents a unique challenge for MLLMs due to the entangled nature of\nknowledge across modalities, making comprehensive unlearning more difficult. To\naddress this challenge, we propose Modality Aware Neuron Unlearning (MANU), a\nnovel unlearning framework for MLLMs designed to selectively clip neurons based\non their relative importance to the targeted forget data, curated for different\nmodalities. Specifically, MANU consists of two stages: important neuron\nselection and selective pruning. The first stage identifies and collects the\nmost influential neurons across modalities relative to the targeted forget\nknowledge, while the second stage is dedicated to pruning those selected\nneurons. MANU effectively isolates and removes the neurons that contribute most\nto the forget data within each modality, while preserving the integrity of\nretained knowledge. Our experiments conducted across various MLLM architectures\nillustrate that MANU can achieve a more balanced and comprehensive unlearning\nin each modality without largely affecting the overall model utility.", "published": "2025-02-21 19:54:46", "link": "http://arxiv.org/abs/2502.15910v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Esethu Framework: Reimagining Sustainable Dataset Governance and\n  Curation for Low-Resource Languages", "abstract": "This paper presents the Esethu Framework, a sustainable data curation\nframework specifically designed to empower local communities and ensure\nequitable benefit-sharing from their linguistic resources. This framework is\nsupported by the Esethu license, a novel community-centric data license. As a\nproof of concept, we introduce the Vuk'uzenzele isiXhosa Speech Dataset\n(ViXSD), an open-source corpus developed under the Esethu Framework and\nLicense. The dataset, containing read speech from native isiXhosa speakers\nenriched with demographic and linguistic metadata, demonstrates how\ncommunity-driven licensing and curation principles can bridge resource gaps in\nautomatic speech recognition (ASR) for African languages while safeguarding the\ninterests of data creators. We describe the framework guiding dataset\ndevelopment, outline the Esethu license provisions, present the methodology for\nViXSD, and present ASR experiments validating ViXSD's usability in building and\nrefining voice-driven applications for isiXhosa.", "published": "2025-02-21 20:12:50", "link": "http://arxiv.org/abs/2502.15916v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Consistency in Large Language Models through Chain of Guidance", "abstract": "Consistency is a fundamental dimension of trustworthiness in Large Language\nModels (LLMs). For humans to be able to trust LLM-based applications, their\noutputs should be consistent when prompted with inputs that carry the same\nmeaning or intent. Despite this need, there is no known mechanism to control\nand guide LLMs to be more consistent at inference time. In this paper, we\nintroduce a novel alignment strategy to maximize semantic consistency in LLM\noutputs. Our proposal is based on Chain of Guidance (CoG), a multistep\nprompting technique that generates highly consistent outputs from LLMs. For\nclosed-book question-answering (Q&A) tasks, when compared to direct prompting,\nthe outputs generated using CoG show improved consistency. While other\napproaches like template-based responses and majority voting may offer\nalternative paths to consistency, our work focuses on exploring the potential\nof guided prompting. We use synthetic data sets comprised of consistent\ninput-output pairs to fine-tune LLMs to produce consistent and correct outputs.\nOur fine-tuned models are more than twice as consistent compared to base models\nand show strong generalization capabilities by producing consistent outputs\nover datasets not used in the fine-tuning process.", "published": "2025-02-21 20:41:37", "link": "http://arxiv.org/abs/2502.15924v1", "categories": ["cs.CL", "I.2.6; I.5.1"], "primary_category": "cs.CL"}
{"title": "AutoMedPrompt: A New Framework for Optimizing LLM Medical Prompts Using\n  Textual Gradients", "abstract": "Large language models (LLMs) have demonstrated increasingly sophisticated\nperformance in medical and other fields of knowledge. Traditional methods of\ncreating specialist LLMs require extensive fine-tuning and training of models\non large datasets. Recently, prompt engineering, instead of fine-tuning, has\nshown potential to boost the performance of general foundation models. However,\nprompting methods such as chain-of-thought (CoT) may not be suitable for all\nsubspecialty, and k-shot approaches may introduce irrelevant tokens into the\ncontext space. We present AutoMedPrompt, which explores the use of textual\ngradients to elicit medically relevant reasoning through system prompt\noptimization. AutoMedPrompt leverages TextGrad's automatic differentiation via\ntext to improve the ability of general foundation LLMs. We evaluated\nAutoMedPrompt on Llama 3, an open-source LLM, using several QA benchmarks,\nincluding MedQA, PubMedQA, and the nephrology subspecialty-specific NephSAP.\nOur results show that prompting with textual gradients outperforms previous\nmethods on open-source LLMs and surpasses proprietary models such as GPT-4,\nClaude 3 Opus, and Med-PaLM 2. AutoMedPrompt sets a new state-of-the-art (SOTA)\nperformance on PubMedQA with an accuracy of 82.6$\\%$, while also outperforming\nprevious prompting strategies on open-sourced models for MedQA (77.7$\\%$) and\nNephSAP (63.8$\\%$).", "published": "2025-02-21 21:17:37", "link": "http://arxiv.org/abs/2502.15944v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse", "abstract": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we propose a new\nstrategy to eliminate such inefficiency, where the KV cache of each document is\nprecomputed independently. During inference, the KV caches of retrieved\ndocuments are concatenated, allowing the model to reuse cached representations\ninstead of recomputing them. To mitigate the performance degradation of LLMs\nwhen using KV caches computed independently for each document, KVLink\nintroduces three key components: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments, and applying mixed-data fine-tuning to enhance performance while\npreserving the model's original capabilities. Experiments across 7 datasets\ndemonstrate that KVLink improves question answering accuracy by an average of\n4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV\ncaches, our approach reduces time-to-first-token by up to 90% compared to\nstandard LLM inference, making it a scalable and efficient solution for context\nreuse.", "published": "2025-02-21 23:34:29", "link": "http://arxiv.org/abs/2502.16002v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Social Genome: Grounded Social Reasoning Abilities of Multimodal Models", "abstract": "Social reasoning abilities are crucial for AI systems to effectively\ninterpret and respond to multimodal human communication and interaction within\nsocial contexts. We introduce Social Genome, the first benchmark for\nfine-grained, grounded social reasoning abilities of multimodal models. Social\nGenome contains 272 videos of interactions and 1,486 human-annotated reasoning\ntraces related to inferences about these interactions. These traces contain\n5,777 reasoning steps that reference evidence from visual cues, verbal cues,\nvocal cues, and external knowledge (contextual knowledge external to videos).\nSocial Genome is also the first modeling challenge to study external knowledge\nin social reasoning. Social Genome computes metrics to holistically evaluate\nsemantic and structural qualities of model-generated social reasoning traces.\nWe demonstrate the utility of Social Genome through experiments with\nstate-of-the-art models, identifying performance gaps and opportunities for\nfuture research to improve the grounded social reasoning abilities of\nmultimodal models.", "published": "2025-02-21 00:05:40", "link": "http://arxiv.org/abs/2502.15109v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unveiling Reasoning Thresholds in Language Models: Scaling, Fine-Tuning,\n  and Interpretability through Attention Maps", "abstract": "This study investigates the in-context learning capabilities of various\ndecoder-only transformer-based language models with different model sizes and\ntraining data, including GPT2, SmolLM2, OpenELM, TinyLlama, Stable LM, and\nGemma 2. We identify a critical parameter threshold (~1.6 billion), beyond\nwhich reasoning performance improves significantly in tasks such as commonsense\nreasoning in multiple-choice question answering and deductive reasoning.\nSpecifically, models above this threshold achieve better success rates in\nchain-of-thought (CoT) prompting for deductive reasoning tasks, especially\nthose requiring longer reasoning chains, such as proof by contradiction and\ndisjunction elimination. To address limitations in sub-threshold models, we\ndemonstrate that fine-tuning with task-specific exemplars substantially\nenhances reasoning performance, enabling accurate CoT generation even without\nadditional exemplars in the prompt for tasks with shorter reasoning chains.\nFinally, our analysis of attention maps reveals that models capable of\ngenerating correct CoTs exhibit higher token-level attention scores on\nsubsequent correct tokens and the correct parts of speech, providing\ninterpretability insights into reasoning processes. These findings collectively\nadvance understanding of reasoning capabilities in decoder-only\ntransformer-based models. The code is available at:\nhttps://github.com/AnnonymousForPapers/CoT_Reasoning_Test.", "published": "2025-02-21 00:48:32", "link": "http://arxiv.org/abs/2502.15120v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Chain-of-Rank: Enhancing Large Language Models for Domain-Specific RAG\n  in Edge Device", "abstract": "Retrieval-augmented generation (RAG) with large language models (LLMs) is\nespecially valuable in specialized domains, where precision is critical. To\nmore specialize the LLMs into a target domain, domain-specific RAG has recently\nbeen developed by allowing the LLM to access the target domain early via\nfinetuning. The domain-specific RAG makes more sense in resource-constrained\nenvironments like edge devices, as they should perform a specific task (e.g.\npersonalization) reliably using only small-scale LLMs. While the\ndomain-specific RAG is well-aligned with edge devices in this respect, it often\nrelies on widely-used reasoning techniques like chain-of-thought (CoT). The\nreasoning step is useful to understand the given external knowledge, and yet it\nis computationally expensive and difficult for small-scale LLMs to learn it.\nTackling this, we propose the Chain of Rank (CoR) which shifts the focus from\nintricate lengthy reasoning to simple ranking of the reliability of input\nexternal documents. Then, CoR reduces computational complexity while\nmaintaining high accuracy, making it particularly suited for\nresource-constrained environments. We attain the state-of-the-art (SOTA)\nresults in benchmarks, and analyze its efficacy.", "published": "2025-02-21 01:28:12", "link": "http://arxiv.org/abs/2502.15134v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do LLMs Make Mistakes Like Students? Exploring Natural Alignment between\n  Language Models and Human Error Patterns", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious educational tasks, yet their alignment with human learning patterns,\nparticularly in predicting which incorrect options students are most likely to\nselect in multiple-choice questions (MCQs), remains underexplored. Our work\ninvestigates the relationship between LLM generation likelihood and student\nresponse distributions in MCQs with a specific focus on distractor selections.\nWe collect a comprehensive dataset of MCQs with real-world student response\ndistributions to explore two fundamental research questions: (1). RQ1 - Do the\ndistractors that students more frequently select correspond to those that LLMs\nassign higher generation likelihood to? (2). RQ2 - When an LLM selects a\nincorrect choice, does it choose the same distractor that most students pick?\nOur experiments reveals moderate correlations between LLM-assigned\nprobabilities and student selection patterns for distractors in MCQs.\nAdditionally, when LLMs make mistakes, they are more likley to select the same\nincorrect answers that commonly mislead students, which is a pattern consistent\nacross both small and large language models. Our work provides empirical\nevidence that despite LLMs' strong performance on generating educational\ncontent, there remains a gap between LLM's underlying reasoning process and\nhuman cognitive processes in identifying confusing distractors. Our findings\nalso have significant implications for educational assessment development. The\nsmaller language models could be efficiently utilized for automated distractor\ngeneration as they demonstrate similar patterns in identifying confusing answer\nchoices as larger language models. This observed alignment between LLMs and\nstudent misconception patterns opens new opportunities for generating\nhigh-quality distractors that complement traditional human-designed\ndistractors.", "published": "2025-02-21 01:43:32", "link": "http://arxiv.org/abs/2502.15140v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Extreme Speech Classification in the Era of LLMs: Exploring Open-Source\n  and Proprietary Models", "abstract": "In recent years, widespread internet adoption and the growth in userbase of\nvarious social media platforms have led to an increase in the proliferation of\nextreme speech online. While traditional language models have demonstrated\nproficiency in distinguishing between neutral text and non-neutral text (i.e.\nextreme speech), categorizing the diverse types of extreme speech presents\nsignificant challenges. The task of extreme speech classification is\nparticularly nuanced, as it requires a deep understanding of socio-cultural\ncontexts to accurately interpret the intent of the language used by the\nspeaker. Even human annotators often disagree on the appropriate classification\nof such content, emphasizing the complex and subjective nature of this task.\nThe use of human moderators also presents a scaling issue, necessitating the\nneed for automated systems for extreme speech classification. The recent launch\nof ChatGPT has drawn global attention to the potential applications of Large\nLanguage Models (LLMs) across a diverse variety of tasks. Trained on vast and\ndiverse corpora, and demonstrating the ability to effectively capture and\nencode contextual information, LLMs emerge as highly promising tools for\ntackling this specific task of extreme speech classification. In this paper, we\nleverage the Indian subset of the extreme speech dataset from Maronikolakis et\nal. (2022) to develop an effective classification framework using LLMs. We\nevaluate open-source Llama models against closed-source OpenAI models, finding\nthat while pre-trained LLMs show moderate efficacy, fine-tuning with\ndomain-specific data significantly enhances performance, highlighting their\nadaptability to linguistic and contextual nuances. Although GPT-based models\noutperform Llama models in zero-shot settings, the performance gap disappears\nafter fine-tuning.", "published": "2025-02-21 02:31:05", "link": "http://arxiv.org/abs/2502.15155v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BP-GPT: Auditory Neural Decoding Using fMRI-prompted LLM", "abstract": "Decoding language information from brain signals represents a vital research\narea within brain-computer interfaces, particularly in the context of\ndeciphering the semantic information from the fMRI signal. Although existing\nwork uses LLM to achieve this goal, their method does not use an end-to-end\napproach and avoids the LLM in the mapping of fMRI-to-text, leaving space for\nthe exploration of the LLM in auditory decoding. In this paper, we introduce a\nnovel method, the Brain Prompt GPT (BP-GPT). By using the brain representation\nthat is extracted from the fMRI as a prompt, our method can utilize GPT-2 to\ndecode fMRI signals into stimulus text. Further, we introduce the text prompt\nand align the fMRI prompt to it. By introducing the text prompt, our BP-GPT can\nextract a more robust brain prompt and promote the decoding of pre-trained LLM.\nWe evaluate our BP-GPT on the open-source auditory semantic decoding dataset\nand achieve a significant improvement up to 4.61 on METEOR and 2.43 on\nBERTScore across all the subjects compared to the state-of-the-art method. The\nexperimental results demonstrate that using brain representation as a prompt to\nfurther drive LLM for auditory neural decoding is feasible and effective. The\ncode is available at https://github.com/1994cxy/BP-GPT.", "published": "2025-02-21 03:13:44", "link": "http://arxiv.org/abs/2502.15172v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Scale-Free Graph-Language Models", "abstract": "Graph-language models (GLMs) have demonstrated great potential in graph-based\nsemi-supervised learning. A typical GLM consists of two key stages: graph\ngeneration and text embedding, which are usually implemented by inferring a\nlatent graph and finetuning a language model (LM), respectively. However, the\nformer often relies on artificial assumptions about the underlying edge\ndistribution, while the latter requires extensive data annotations. To tackle\nthese challenges, this paper introduces a novel GLM that integrates graph\ngeneration and text embedding within a unified framework. Specifically, for\ngraph generation, we leverage an inherent characteristic of real edge\ndistribution--the scale-free property--as a structural prior. We unexpectedly\nfind that this natural property can be effectively approximated by a simple\nk-nearest neighbor (KNN) graph. For text embedding, we develop a graph-based\npseudo-labeler that utilizes scale-free graphs to provide complementary\nsupervision for improved LM finetuning. Extensive experiments on representative\ndatasets validate our findings on the scale-free structural approximation of\nKNN graphs and demonstrate the effectiveness of integrating graph generation\nand text embedding with a real structural prior. Our code is available at\nhttps://github.com/Jianglin954/SFGL.", "published": "2025-02-21 03:41:43", "link": "http://arxiv.org/abs/2502.15189v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TETRIS: Optimal Draft Token Selection for Batch Speculative Decoding", "abstract": "We propose TETRIS, a novel method that optimizes the total throughput of\nbatch speculative decoding in multi-request settings. Unlike existing methods\nthat optimize for a single request or a group of requests as a whole, TETRIS\nactively selects the most promising draft tokens (for every request in a batch)\nto be accepted when verified in parallel, resulting in fewer rejected tokens\nand hence less wasted computing resources. Such an effective resource\nutilization to achieve fast inference in large language models (LLMs) is\nespecially important to service providers with limited inference capacity.\nCompared to baseline speculative decoding, TETRIS yields a consistently higher\nacceptance rate and more effective utilization of the limited inference\ncapacity. We show theoretically and empirically that TETRIS outperforms\nbaseline speculative decoding and existing methods that dynamically select\ndraft tokens, leading to a more efficient batch inference in LLMs.", "published": "2025-02-21 04:19:24", "link": "http://arxiv.org/abs/2502.15197v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A General Pseudonymization Framework for Cloud-Based LLMs: Replacing\n  Privacy Information in Controlled Text Generation", "abstract": "An increasing number of companies have begun providing services that leverage\ncloud-based large language models (LLMs), such as ChatGPT. However, this\ndevelopment raises substantial privacy concerns, as users' prompts are\ntransmitted to and processed by the model providers. Among the various privacy\nprotection methods for LLMs, those implemented during the pre-training and\nfine-tuning phrases fail to mitigate the privacy risks associated with the\nremote use of cloud-based LLMs by users. On the other hand, methods applied\nduring the inference phrase are primarily effective in scenarios where the\nLLM's inference does not rely on privacy-sensitive information. In this paper,\nwe outline the process of remote user interaction with LLMs and, for the first\ntime, propose a detailed definition of a general pseudonymization framework\napplicable to cloud-based LLMs. The experimental results demonstrate that the\nproposed framework strikes an optimal balance between privacy protection and\nutility. The code for our method is available to the public at\nhttps://github.com/Mebymeby/Pseudonymization-Framework.", "published": "2025-02-21 06:15:53", "link": "http://arxiv.org/abs/2502.15233v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Corrections Meet Explanations: A Unified Framework for Explainable\n  Grammatical Error Correction", "abstract": "Grammatical Error Correction (GEC) faces a critical challenge concerning\nexplainability, notably when GEC systems are designed for language learners.\nExisting research predominantly focuses on explaining grammatical errors\nextracted in advance, thus neglecting the relationship between explanations and\ncorrections. To address this gap, we introduce EXGEC, a unified explainable GEC\nframework that integrates explanation and correction tasks in a generative\nmanner, advocating that these tasks mutually reinforce each other. Experiments\nhave been conducted on EXPECT, a recent human-labeled dataset for explainable\nGEC, comprising around 20k samples. Moreover, we detect significant noise\nwithin EXPECT, potentially compromising model training and evaluation.\nTherefore, we introduce an alternative dataset named EXPECT-denoised, ensuring\na more objective framework for training and evaluation. Results on various NLP\nmodels (BART, T5, and Llama3) show that EXGEC models surpass single-task\nbaselines in both tasks, demonstrating the effectiveness of our approach.", "published": "2025-02-21 07:42:33", "link": "http://arxiv.org/abs/2502.15261v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference", "abstract": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance.", "published": "2025-02-21 08:40:07", "link": "http://arxiv.org/abs/2502.15294v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Detecting Future-related Contexts of Entity Mentions", "abstract": "The ability to automatically identify whether an entity is referenced in a\nfuture context can have multiple applications including decision making,\nplanning and trend forecasting. This paper focuses on detecting implicit future\nreferences in entity-centric texts, addressing the growing need for automated\ntemporal analysis in information processing. We first present a novel dataset\nof 19,540 sentences built around popular entities sourced from Wikipedia, which\nconsists of future-related and non-future-related contexts in which those\nentities appear. As a second contribution, we evaluate the performance of\nseveral Language Models including also Large Language Models (LLMs) on the task\nof distinguishing future-oriented content in the absence of explicit temporal\nreferences.", "published": "2025-02-21 09:34:34", "link": "http://arxiv.org/abs/2502.15332v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Constructing a Norm for Children's Scientific Drawing: Distribution\n  Features Based on Semantic Similarity of Large Language Models", "abstract": "The use of children's drawings to examining their conceptual understanding\nhas been proven to be an effective method, but there are two major problems\nwith previous research: 1. The content of the drawings heavily relies on the\ntask, and the ecological validity of the conclusions is low; 2. The\ninterpretation of drawings relies too much on the subjective feelings of the\nresearchers. To address this issue, this study uses the Large Language Model\n(LLM) to identify 1420 children's scientific drawings (covering 9 scientific\nthemes/concepts), and uses the word2vec algorithm to calculate their semantic\nsimilarity. The study explores whether there are consistent drawing\nrepresentations for children on the same theme, and attempts to establish a\nnorm for children's scientific drawings, providing a baseline reference for\nfollow-up children's drawing research. The results show that the representation\nof most drawings has consistency, manifested as most semantic similarity\ngreater than 0.8. At the same time, it was found that the consistency of the\nrepresentation is independent of the accuracy (of LLM's recognition),\nindicating the existence of consistency bias. In the subsequent exploration of\ninfluencing factors, we used Kendall rank correlation coefficient to\ninvestigate the effects of Sample Size, Abstract Degree, and Focus Points on\ndrawings, and used word frequency statistics to explore whether children\nrepresented abstract themes/concepts by reproducing what was taught in class.", "published": "2025-02-21 10:02:15", "link": "http://arxiv.org/abs/2502.15348v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ARS: Automatic Routing Solver with Large Language Models", "abstract": "Real-world Vehicle Routing Problems (VRPs) are characterized by a variety of\npractical constraints, making manual solver design both knowledge-intensive and\ntime-consuming. Although there is increasing interest in automating the design\nof routing algorithms, existing research has explored only a limited array of\nVRP variants and fails to adequately address the complex and prevalent\nconstraints encountered in real-world situations. To fill this gap, this paper\nintroduces RoutBench, a benchmark of 1,000 VRP variants derived from 24\nattributes, for evaluating the effectiveness of automatic routing solvers in\naddressing complex constraints. Along with RoutBench, we present the Automatic\nRouting Solver (ARS), which employs Large Language Model (LLM) agents to\nenhance a backbone algorithm framework by automatically generating\nconstraint-aware heuristic code, based on problem descriptions and several\nrepresentative constraints selected from a database. Our experiments show that\nARS outperforms state-of-the-art LLM-based methods and commonly used solvers,\nautomatically solving 91.67% of common VRPs and achieving at least a 30%\nimprovement across all benchmarks.", "published": "2025-02-21 10:14:55", "link": "http://arxiv.org/abs/2502.15359v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Evaluating Social Biases in LLM Reasoning", "abstract": "In the recent development of AI reasoning, large language models (LLMs) are\ntrained to automatically generate chain-of-thought reasoning steps, which have\ndemonstrated compelling performance on math and coding tasks. However, when\nbias is mixed within the reasoning process to form strong logical arguments, it\ncould cause even more harmful results and further induce hallucinations. In\nthis paper, we have evaluated the 8B and 32B variants of DeepSeek-R1 against\ntheir instruction tuned counterparts on the BBQ dataset, and investigated the\nbias that is elicited out and being amplified through reasoning steps. To the\nbest of our knowledge, this empirical study is the first to assess bias issues\nin LLM reasoning.", "published": "2025-02-21 10:16:07", "link": "http://arxiv.org/abs/2502.15361v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HiFi-KPI: A Dataset for Hierarchical KPI Extraction from Earnings\n  Filings", "abstract": "The U.S. Securities and Exchange Commission (SEC) requires that public\ncompanies file financial reports tagging numbers with the machine readable\ninline eXtensible Business Reporting Language (iXBRL) standard. However, the\nhighly complex and highly granular taxonomy defined by iXBRL limits label\ntransferability across domains. In this paper, we introduce the Hierarchical\nFinancial Key Performance Indicator (HiFi-KPI) dataset, designed to facilitate\nnumerical KPI extraction at specified levels of granularity from unstructured\nfinancial text. Our approach organizes a 218,126-label hierarchy using a\ntaxonomy based grouping method, investigating which taxonomy layer provides the\nmost meaningful structure. HiFi-KPI comprises ~1.8M paragraphs and ~5M\nentities, each linked to a label in the iXBRL-specific calculation and\npresentation taxonomies. We provide baselines using encoder-based approaches\nand structured extraction using Large Language Models (LLMs). To simplify LLM\ninference and evaluation, we additionally release HiFi-KPI Lite, a manually\ncurated subset with four expert-mapped labels. We publicly release all\nartifacts.", "published": "2025-02-21 12:19:08", "link": "http://arxiv.org/abs/2502.15411v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "When Compression Meets Model Compression: Memory-Efficient Double\n  Compression for Large Language Models", "abstract": "Large language models (LLMs) exhibit excellent performance in various tasks.\nHowever, the memory requirements of LLMs present a great challenge when\ndeploying on memory-limited devices, even for quantized LLMs. This paper\nintroduces a framework to compress LLM after quantization further, achieving\nabout 2.2x compression ratio. A compression-aware quantization is first\nproposed to enhance model weight compressibility by re-scaling the model\nparameters before quantization, followed by a pruning method to improve\nfurther. Upon this, we notice that decompression can be a bottleneck during\npractical scenarios. We then give a detailed analysis of the trade-off between\nmemory usage and latency brought by the proposed method. A speed-adaptive\nmethod is proposed to overcome it. The experimental results show inference with\nthe compressed model can achieve a 40% reduction in memory size with negligible\nloss in accuracy and inference speed.", "published": "2025-02-21 13:11:22", "link": "http://arxiv.org/abs/2502.15443v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Binary-Integer-Programming Based Algorithm for Expert Load Balancing in\n  Mixture-of-Experts Models", "abstract": "For pre-training of MoE (Mixture-of-Experts) models, one of the main issues\nis unbalanced expert loads, which may cause routing collapse or increased\ncomputational overhead. Existing methods contain the Loss-Controlled method and\nthe Loss-Free method, where both the unbalanced degrees at first several\ntraining steps are still high and decrease slowly. In this work, we propose\nBIP-Based Balancing, an expert load balancing algorithm based on binary integer\nprogramming (BIP). The algorithm maintains an additional vector q on each MoE\nlayer that can help change the top-K order of s by solving a binary integer\nprogramming with very small time costs. We implement the algorithm on two MoE\nlanguage models: 16-expert (0.3B) and 64-expert (1.1B). The experimental\nresults show that on both models comparing with the Loss-Controlled method and\nthe Loss-Free method, our algorithm trains models with the lowest perplexities,\nwhile saves at least 13% of pre-training time compared with the Loss-Controlled\nmethod. Within our current knowledge, this is the first routing algorithm that\nachieves maintaining load balance status on every expert in every MoE layer\nfrom the first step to the last step during the whole pre-training process,\nwhile the trained MoE models also perform well. The code material of this work\nis available at https://github.com/sunyuanLLM/bip_routing_algorithm.", "published": "2025-02-21 13:25:00", "link": "http://arxiv.org/abs/2502.15451v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Enhancing RWKV-based Language Models for Long-Sequence Text Generation", "abstract": "This paper introduces an enhanced RWKV architecture with adaptive temporal\ngating mechanisms for improved long-context language modeling. We propose two\nprincipal innovations: (1) a position-aware convolutional shift operator that\ncaptures local syntactic patterns while preserving global coherence, and (2) a\nneurally-gated information routing mechanism that dynamically regulates\ninter-token information flow. Through comprehensive experiments on text\ngeneration tasks, our enhanced model demonstrates superior performance compared\nto the baseline RWKV, achieving 96.5 relative improvement in ROUGE-L scores\nwith only 2.95 increased inference latency. Ablation studies validate the\nindividual contributions of each component, while linguistic analysis reveals\nthe model's adaptive attention to syntactic boundaries and entity coherence.\nThe proposed modifications maintain RWKV's linear computational complexity\nwhile significantly enhancing its contextual modeling capabilities,\nestablishing new state-of-the-art performance for recurrent-style architectures\nin long-form text generation.", "published": "2025-02-21 14:18:18", "link": "http://arxiv.org/abs/2502.15485v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models", "abstract": "Large Language Models (LLMs) are increasingly used in tasks requiring\ninterpretive and inferential accuracy. In this paper, we introduce ExpliCa, a\nnew dataset for evaluating LLMs in explicit causal reasoning. ExpliCa uniquely\nintegrates both causal and temporal relations presented in different linguistic\norders and explicitly expressed by linguistic connectives. The dataset is\nenriched with crowdsourced human acceptability ratings. We tested LLMs on\nExpliCa through prompting and perplexity-based metrics. We assessed seven\ncommercial and open-source LLMs, revealing that even top models struggle to\nreach 0.80 accuracy. Interestingly, models tend to confound temporal relations\nwith causal ones, and their performance is also strongly influenced by the\nlinguistic order of the events. Finally, perplexity-based scores and prompting\nperformance are differently affected by model size.", "published": "2025-02-21 14:23:14", "link": "http://arxiv.org/abs/2502.15487v2", "categories": ["cs.CL", "cs.AI", "68T50, 68T07", "I.2.7"], "primary_category": "cs.CL"}
{"title": "PIP-KAG: Mitigating Knowledge Conflicts in Knowledge-Augmented\n  Generation via Parametric Pruning", "abstract": "Knowledge-Augmented Generation (KAG) has shown great promise in updating the\ninternal memory of Large Language Models (LLMs) by integrating external\nknowledge. However, KAG inevitably faces knowledge conflicts when the internal\nmemory contradicts external information. Current approaches to mitigating these\nconflicts mainly focus on improving external knowledge utilization. However,\nthese methods have shown only limited effectiveness in mitigating the knowledge\nconflict problem, as internal knowledge continues to influence the generation\nprocess of LLMs. In this paper, we propose a ParametrIc Pruning-based\nKnowledge-Augmented Generation (PIP-KAG) approach, which prunes internal\nknowledge of LLMs and incorporates a plug-and-play adaptation module to help\nLLMs better leverage external sources. Additionally, we construct the\nCoConflictQA benchmark based on the hallucination of LLMs to better evaluate\ncontextual faithfulness during answering questions. Experimental results on\nCoConflictQA demonstrate that PIP-KAG significantly reduces knowledge conflicts\nand improves context fidelity. Notably, PIP-KAG reduces LLM's parameters by\n13%, enhancing parameter efficiency in LLMs within the KAG framework. All codes\nare available at https://github.com/OpenBMB/PIP-KAG.", "published": "2025-02-21 15:50:41", "link": "http://arxiv.org/abs/2502.15543v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generalizing From Short to Long: Effective Data Synthesis for\n  Long-Context Instruction Tuning", "abstract": "Long-context modelling for large language models (LLMs) has been a key area\nof recent research because many real world use cases require reasoning over\nlonger inputs such as documents. The focus of research into modelling long\ncontext has been on how to model position and there has been little\ninvestigation into other important aspects of language modelling such as\ninstruction tuning. Long context training examples are challenging and\nexpensive to create and use. In this paper, we investigate how to design\ninstruction data for the post-training phase of a long context pre-trained\nmodel: how much and what type of context is needed for optimal and efficient\npost-training. Our controlled study reveals that models instruction-tuned on\nshort contexts can effectively generalize to longer ones, while also\nidentifying other critical factors such as instruction difficulty and context\ncomposition. Based on these findings, we propose context synthesis, a novel\ndata synthesis framework that leverages off-the-shelf LLMs to generate extended\nbackground contexts for high-quality instruction-answer pairs. Experiment\nresults on the document-level benchmark (LongBench) demonstrate that our\nproposed approach outperforms previous instruction synthesis approaches and\ncomes close to the performance of human-annotated long-context instruction\ndata. The project will be available at:\nhttps://github.com/NJUNLP/context-synthesis.", "published": "2025-02-21 17:02:40", "link": "http://arxiv.org/abs/2502.15592v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pastiche Novel Generation Creating: Fan Fiction You Love in Your\n  Favorite Author's Style", "abstract": "Great novels create immersive worlds with rich character arcs,\nwell-structured plots, and nuanced writing styles. However, current novel\ngeneration methods often rely on brief, simplistic story outlines and generate\ndetails using plain, generic language. To bridge this gap, we introduce the\ntask of Pastiche Novel Generation, which requires the generated novels to\nimitate the distinctive features of the original work, including understanding\ncharacter profiles, predicting plausible plot developments, and writing\nconcrete details using vivid, expressive language. To achieve this, we propose\nWriterAgent, a novel generation system designed to master the core aspects of\nliterary pastiche. WriterAgent is trained through a curriculum learning\nparadigm, progressing from low-level stylistic mastery to high-level narrative\ncoherence. Its key tasks include language style learning, character modeling,\nplot planning, and stylish writing, ensuring comprehensive narrative control.\nTo support this, WriterAgent leverages the WriterLoRA framework, an extension\nof LoRA with hierarchical and cumulative task-specific modules, each\nspecializing in a different narrative aspect. We evaluate WriterAgent on\nmultilingual classics like Harry Potter and Dream of the Red Chamber,\ndemonstrating its superiority over baselines in capturing the target author's\nsettings, character dynamics, and writing style to produce coherent, faithful\nnarratives.", "published": "2025-02-21 17:40:42", "link": "http://arxiv.org/abs/2502.15616v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Extraction multi-\u00e9tiquettes de relations en utilisant des couches de\n  Transformer", "abstract": "In this article, we present the BTransformer18 model, a deep learning\narchitecture designed for multi-label relation extraction in French texts. Our\napproach combines the contextual representation capabilities of pre-trained\nlanguage models from the BERT family - such as BERT, RoBERTa, and their French\ncounterparts CamemBERT and FlauBERT - with the power of Transformer encoders to\ncapture long-term dependencies between tokens. Experiments conducted on the\ndataset from the TextMine'25 challenge show that our model achieves superior\nperformance, particularly when using CamemBERT-Large, with a macro F1 score of\n0.654, surpassing the results obtained with FlauBERT-Large. These results\ndemonstrate the effectiveness of our approach for the automatic extraction of\ncomplex relations in intelligence reports.", "published": "2025-02-21 17:42:51", "link": "http://arxiv.org/abs/2502.15619v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Empowering LLMs with Logical Reasoning: A Comprehensive Survey", "abstract": "Large language models (LLMs) have achieved remarkable successes on various\nnatural language tasks. However, recent studies have found that there are still\nsignificant challenges to the logical reasoning abilities of LLMs. This paper\nsummarizes and categorizes the main challenges into two aspects: (1) Logical\nquestion answering, LLMs often fail to generate the correct answer within\ncomplex logical problem which requires sophisticated deductive, inductive or\nabductive reasoning given a collection of premises and constrains. (2) Logical\nconsistency, LLMs are prone to producing responses contradicting themselves\nacross different questions. For example, a state-of-the-art Macaw\nquestion-answering LLM answers Yes to both questions Is a magpie a bird? and\nDoes a bird have wings? but answers No to Does a magpie have wings?. To\nfacilitate this research direction, we comprehensively investigate the most\ncutting-edge methods and propose detailed taxonomies of these methods.\nSpecifically, to accurately answer complex logic questions, previous methods\ncan be categorized based on reliance on external solvers, prompts, pretraining,\nand fine-tuning. To avoid logical contradictions, we discuss concepts and\nsolutions of various logical consistencies, including implication, negation,\ntransitivity, factuality consistency, and their composites. In addition, we\nreview commonly used benchmark datasets and evaluation metrics, and discuss\npromising research directions, such as extensions to modal logic to account for\nuncertainty, and efficient algorithms satisfying multiple logical consistencies\nsimultaneously.", "published": "2025-02-21 18:20:35", "link": "http://arxiv.org/abs/2502.15652v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Machine-generated text detection prevents language model collapse", "abstract": "As Large Language Models (LLMs) become increasingly prevalent, their\ngenerated outputs are proliferating across the web, risking a future where\nmachine-generated content dilutes human-authored text. Since online data is the\nprimary resource for LLM pre-training, subsequent models could be trained on an\nunknown portion of synthetic samples. This will lead to model collapse, a\ndegenerative process whereby LLMs reinforce their own errors, and ultimately\nyield a declining performance. In this study, we investigate the impact of\ndecoding strategy on model collapse, analysing the characteristics of text at\neach model generation, the similarity to human references, and the resulting\nmodel performance. Using the decoding strategies that lead to the most\nsignificant degradation, we evaluate model collapse in more realistic scenarios\nwhere the origin of the data (human or synthetic) is unknown. We train a\nmachine-generated text detector and propose an importance sampling approach to\nalleviate model collapse. Our method is validated on two LLM variants (GPT-2\nand SmolLM2) on the open-ended text generation task. We demonstrate that it can\nnot only prevent model collapse but also improve performance when sufficient\nhuman-authored samples are present.", "published": "2025-02-21 18:22:36", "link": "http://arxiv.org/abs/2502.15654v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AutoToM: Automated Bayesian Inverse Planning and Model Discovery for\n  Open-ended Theory of Mind", "abstract": "Theory of Mind (ToM), the ability to understand people's mental variables\nbased on their behavior, is key to developing socially intelligent agents.\nCurrent approaches to Theory of Mind reasoning either rely on prompting Large\nLanguage Models (LLMs), which are prone to systematic errors, or use rigid,\nhandcrafted Bayesian Theory of Mind (BToM) models, which are more robust but\ncannot generalize across different domains. In this work, we introduce AutoToM,\nan automated Bayesian Theory of Mind method for achieving open-ended machine\nTheory of Mind. AutoToM can operate in any domain, infer any mental variable,\nand conduct robust Theory of Mind reasoning of any order. Given a Theory of\nMind inference problem, AutoToM first proposes an initial BToM model. It then\nconducts automated Bayesian inverse planning based on the proposed model,\nleveraging an LLM as the backend. Based on the uncertainty of the inference, it\niteratively refines the model, by introducing additional mental variables\nand/or incorporating more timesteps in the context. Empirical evaluations\nacross multiple Theory of Mind benchmarks demonstrate that AutoToM consistently\nachieves state-of-the-art performance, offering a scalable, robust, and\ninterpretable approach to machine Theory of Mind.", "published": "2025-02-21 18:57:52", "link": "http://arxiv.org/abs/2502.15676v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Privacy Ripple Effects from Adding or Removing Personal Information in\n  Language Model Training", "abstract": "Due to the sensitive nature of personally identifiable information (PII), its\nowners may have the authority to control its inclusion or request its removal\nfrom large-language model (LLM) training. Beyond this, PII may be added or\nremoved from training datasets due to evolving dataset curation techniques,\nbecause they were newly scraped for retraining, or because they were included\nin a new downstream fine-tuning stage. We find that the amount and ease of PII\nmemorization is a dynamic property of a model that evolves throughout training\npipelines and depends on commonly altered design choices. We characterize three\nsuch novel phenomena: (1) similar-appearing PII seen later in training can\nelicit memorization of earlier-seen sequences in what we call assisted\nmemorization, and this is a significant factor (in our settings, up to 1/3);\n(2) adding PII can increase memorization of other PII significantly (in our\nsettings, as much as $\\approx\\!7.5\\times$); and (3) removing PII can lead to\nother PII being memorized. Model creators should consider these first- and\nsecond-order privacy risks when training models to avoid the risk of new PII\nregurgitation.", "published": "2025-02-21 18:59:14", "link": "http://arxiv.org/abs/2502.15680v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Forecasting Frontier Language Model Agent Capabilities", "abstract": "As Language Models (LMs) increasingly operate as autonomous agents,\naccurately forecasting their capabilities becomes crucial for societal\npreparedness. We evaluate six forecasting methods that predict downstream\ncapabilities of LM agents. We use \"one-step\" approaches that predict benchmark\nscores from input metrics like compute or model release date directly or\n\"two-step\" approaches that first predict an intermediate metric like the\nprincipal component of cross-benchmark performance (PC-1) and human-evaluated\ncompetitive Elo ratings. We evaluate our forecasting methods by backtesting\nthem on a dataset of 38 LMs from the OpenLLM 2 leaderboard. We then use the\nvalidated two-step approach (Release Date$\\to$Elo$\\to$Benchmark) to predict LM\nagent performance for frontier models on three benchmarks: SWE-Bench Verified\n(software development), Cybench (cybersecurity assessment), and RE-Bench (ML\nresearch engineering). Our forecast predicts that by the beginning of 2026,\nnon-specialized LM agents with low capability elicitation will reach a success\nrate of 54% on SWE-Bench Verified, while state-of-the-art LM agents will reach\nan 87% success rate. Our approach does not account for recent advances in\ninference-compute scaling and might thus be too conservative.", "published": "2025-02-21 02:34:17", "link": "http://arxiv.org/abs/2502.15850v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Control Illusion: The Failure of Instruction Hierarchies in Large\n  Language Models", "abstract": "Large language models (LLMs) are increasingly deployed with hierarchical\ninstruction schemes, where certain instructions (e.g., system-level directives)\nare expected to take precedence over others (e.g., user messages). Yet, we lack\na systematic understanding of how effectively these hierarchical control\nmechanisms work. We introduce a systematic evaluation framework based on\nconstraint prioritization to assess how well LLMs enforce instruction\nhierarchies. Our experiments across six state-of-the-art LLMs reveal that\nmodels struggle with consistent instruction prioritization, even for simple\nformatting conflicts. We find that the widely-adopted system/user prompt\nseparation fails to establish a reliable instruction hierarchy, and models\nexhibit strong inherent biases toward certain constraint types regardless of\ntheir priority designation. While controlled prompt engineering and model\nfine-tuning show modest improvements, our results indicate that instruction\nhierarchy enforcement is not robustly realized, calling for deeper\narchitectural innovations beyond surface-level modifications.", "published": "2025-02-21 04:51:37", "link": "http://arxiv.org/abs/2502.15851v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "C3AI: Crafting and Evaluating Constitutions for Constitutional AI", "abstract": "Constitutional AI (CAI) guides LLM behavior using constitutions, but\nidentifying which principles are most effective for model alignment remains an\nopen challenge. We introduce the C3AI framework (\\textit{Crafting Constitutions\nfor CAI models}), which serves two key functions: (1) selecting and structuring\nprinciples to form effective constitutions before fine-tuning; and (2)\nevaluating whether fine-tuned CAI models follow these principles in practice.\nBy analyzing principles from AI and psychology, we found that positively\nframed, behavior-based principles align more closely with human preferences\nthan negatively framed or trait-based principles. In a safety alignment use\ncase, we applied a graph-based principle selection method to refine an existing\nCAI constitution, improving safety measures while maintaining strong general\nreasoning capabilities. Interestingly, fine-tuned CAI models performed well on\nnegatively framed principles but struggled with positively framed ones, in\ncontrast to our human alignment results. This highlights a potential gap\nbetween principle design and model adherence. Overall, C3AI provides a\nstructured and scalable approach to both crafting and evaluating CAI\nconstitutions.", "published": "2025-02-21 10:26:42", "link": "http://arxiv.org/abs/2502.15861v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "LLMs in Mobile Apps: Practices, Challenges, and Opportunities", "abstract": "The integration of AI techniques has become increasingly popular in software\ndevelopment, enhancing performance, usability, and the availability of\nintelligent features. With the rise of large language models (LLMs) and\ngenerative AI, developers now have access to a wealth of high-quality\nopen-source models and APIs from closed-source providers, enabling easier\nexperimentation and integration of LLMs into various systems. This has also\nopened new possibilities in mobile application (app) development, allowing for\nmore personalized and intelligent apps. However, integrating LLM into mobile\napps might present unique challenges for developers, particularly regarding\nmobile device constraints, API management, and code infrastructure. In this\nproject, we constructed a comprehensive dataset of 149 LLM-enabled Android apps\nand conducted an exploratory analysis to understand how LLMs are deployed and\nused within mobile apps. This analysis highlights key characteristics of the\ndataset, prevalent integration strategies, and common challenges developers\nface. Our findings provide valuable insights for future research and tooling\ndevelopment aimed at enhancing LLM-enabled mobile apps.", "published": "2025-02-21 19:53:43", "link": "http://arxiv.org/abs/2502.15908v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Self-Taught Agentic Long Context Understanding", "abstract": "Answering complex, long-context questions remains a major challenge for large\nlanguage models (LLMs) as it requires effective question clarifications and\ncontext retrieval. We propose Agentic Long-Context Understanding (AgenticLU), a\nframework designed to enhance an LLM's understanding of such queries by\nintegrating targeted self-clarification with contextual grounding within an\nagentic workflow. At the core of AgenticLU is Chain-of-Clarifications (CoC),\nwhere models refine their understanding through self-generated clarification\nquestions and corresponding contextual groundings. By scaling inference as a\ntree search where each node represents a CoC step, we achieve 97.8% answer\nrecall on NarrativeQA with a search depth of up to three and a branching factor\nof eight. To amortize the high cost of this search process to training, we\nleverage the preference pairs for each step obtained by the CoC workflow and\nperform two-stage model finetuning: (1) supervised finetuning to learn\neffective decomposition strategies, and (2) direct preference optimization to\nenhance reasoning quality. This enables AgenticLU models to generate\nclarifications and retrieve relevant context effectively and efficiently in a\nsingle inference pass. Extensive experiments across seven long-context tasks\ndemonstrate that AgenticLU significantly outperforms state-of-the-art prompting\nmethods and specialized long-context LLMs, achieving robust multi-hop reasoning\nwhile sustaining consistent performance as context length grows.", "published": "2025-02-21 20:29:36", "link": "http://arxiv.org/abs/2502.15920v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CVE-LLM : Ontology-Assisted Automatic Vulnerability Evaluation Using\n  Large Language Models", "abstract": "The National Vulnerability Database (NVD) publishes over a thousand new\nvulnerabilities monthly, with a projected 25 percent increase in 2024,\nhighlighting the crucial need for rapid vulnerability identification to\nmitigate cybersecurity attacks and save costs and resources. In this work, we\npropose using large language models (LLMs) to learn vulnerability evaluation\nfrom historical assessments of medical device vulnerabilities in a single\nmanufacturer's portfolio. We highlight the effectiveness and challenges of\nusing LLMs for automatic vulnerability evaluation and introduce a method to\nenrich historical data with cybersecurity ontologies, enabling the system to\nunderstand new vulnerabilities without retraining the LLM. Our LLM system\nintegrates with the in-house application - Cybersecurity Management System\n(CSMS) - to help Siemens Healthineers (SHS) product cybersecurity experts\nefficiently assess the vulnerabilities in our products. Also, we present\nguidelines for efficient integration of LLMs into the cybersecurity tool.", "published": "2025-02-21 20:59:15", "link": "http://arxiv.org/abs/2502.15932v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Optimizing Pre-Training Data Mixtures with Mixtures of Data Expert\n  Models", "abstract": "We propose a method to optimize language model pre-training data mixtures\nthrough efficient approximation of the cross-entropy loss corresponding to each\ncandidate mixture via a Mixture of Data Experts (MDE). We use this\napproximation as a source of additional features in a regression model, trained\nfrom observations of model loss for a small number of mixtures.\n  Experiments with Transformer decoder-only language models in the range of 70M\nto 1B parameters on the SlimPajama dataset show that our method achieves\nsignificantly better performance than approaches that train regression models\nusing only the mixture rates as input features. Combining this improved\noptimization method with an objective that takes into account cross-entropy on\nend task data leads to superior performance on few-shot downstream evaluations.\n  We also provide theoretical insights on why aggregation of data expert\npredictions can provide good approximations to model losses for data mixtures.", "published": "2025-02-21 21:27:48", "link": "http://arxiv.org/abs/2502.15950v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MMRAG: Multi-Mode Retrieval-Augmented Generation with Large Language\n  Models for Biomedical In-Context Learning", "abstract": "Objective: To optimize in-context learning in biomedical natural language\nprocessing by improving example selection. Methods: We introduce a novel\nmulti-mode retrieval-augmented generation (MMRAG) framework, which integrates\nfour retrieval strategies: (1) Random Mode, selecting examples arbitrarily; (2)\nTop Mode, retrieving the most relevant examples based on similarity; (3)\nDiversity Mode, ensuring variation in selected examples; and (4) Class Mode,\nselecting category-representative examples. This study evaluates MMRAG on three\ncore biomedical NLP tasks: Named Entity Recognition (NER), Relation Extraction\n(RE), and Text Classification (TC). The datasets used include BC2GM for gene\nand protein mention recognition (NER), DDI for drug-drug interaction extraction\n(RE), GIT for general biomedical information extraction (RE), and HealthAdvice\nfor health-related text classification (TC). The framework is tested with two\nlarge language models (Llama2-7B, Llama3-8B) and three retrievers (Contriever,\nMedCPT, BGE-Large) to assess performance across different retrieval strategies.\nResults: The results from the Random mode indicate that providing more examples\nin the prompt improves the model's generation performance. Meanwhile, Top mode\nand Diversity mode significantly outperform Random mode on the RE (DDI) task,\nachieving an F1 score of 0.9669, a 26.4% improvement. Among the three\nretrievers tested, Contriever outperformed the other two in a greater number of\nexperiments. Additionally, Llama 2 and Llama 3 demonstrated varying\ncapabilities across different tasks, with Llama 3 showing a clear advantage in\nhandling NER tasks. Conclusion: MMRAG effectively enhances biomedical\nin-context learning by refining example selection, mitigating data scarcity\nissues, and demonstrating superior adaptability for NLP-driven healthcare\napplications.", "published": "2025-02-21 21:36:48", "link": "http://arxiv.org/abs/2502.15954v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "R$^3$Mem: Bridging Memory Retention and Retrieval via Reversible\n  Compression", "abstract": "Memory plays a key role in enhancing LLMs' performance when deployed to\nreal-world applications. Existing solutions face trade-offs: explicit memory\ndesigns based on external storage require complex management and incur storage\noverhead, while implicit memory designs that store information via parameters\nstruggle with reliable retrieval. In this paper, we propose R$^3$Mem, a memory\nnetwork that optimizes both information Retention and Retrieval through\nReversible context compression. Specifically, R$^3$Mem employs virtual memory\ntokens to compress and encode infinitely long histories, further enhanced by a\nhierarchical compression strategy that refines information from document- to\nentity-level for improved assimilation across granularities. For retrieval,\nR$^3$Mem employs a reversible architecture, reconstructing raw data by invoking\nthe model backward with compressed information. Implemented via\nparameter-efficient fine-tuning, it can integrate seamlessly with any\nTransformer-based model. Experiments demonstrate that our memory design\nachieves state-of-the-art performance in long-context language modeling and\nretrieval-augmented generation tasks. It also significantly outperforms\nconventional memory modules in long-horizon interaction tasks like\nconversational agents, showcasing its potential for next-generation retrieval\nsystems.", "published": "2025-02-21 21:39:00", "link": "http://arxiv.org/abs/2502.15957v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sparsity May Be All You Need: Sparse Random Parameter Adaptation", "abstract": "Full fine-tuning of large language models for alignment and task adaptation\nhas become prohibitively expensive as models have grown in size.\nParameter-Efficient Fine-Tuning (PEFT) methods aim at significantly reducing\nthe computational and memory resources needed for fine-tuning these models by\nonly training on a small number of parameters instead of all model parameters.\nCurrently, the most popular PEFT method is the Low-Rank Adaptation (LoRA),\nwhich freezes the parameters of the model to be fine-tuned and introduces a\nsmall set of trainable parameters in the form of low-rank matrices. We propose\nsimply reducing the number of trainable parameters by randomly selecting a\nsmall proportion of the model parameters to train on. In this paper, we compare\nthe efficiency and performance of our proposed approach with PEFT methods,\nincluding LoRA, as well as full parameter fine-tuning.", "published": "2025-02-21 22:23:16", "link": "http://arxiv.org/abs/2502.15975v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Med-gte-hybrid: A contextual embedding transformer model for extracting\n  actionable information from clinical texts", "abstract": "We introduce a novel contextual embedding model med-gte-hybrid that was\nderived from the gte-large sentence transformer to extract information from\nunstructured clinical narratives. Our model tuning strategy for med-gte-hybrid\ncombines contrastive learning and a denoising autoencoder. To evaluate the\nperformance of med-gte-hybrid, we investigate several clinical prediction tasks\nin large patient cohorts extracted from the MIMIC-IV dataset, including Chronic\nKidney Disease (CKD) patient prognosis, estimated glomerular filtration rate\n(eGFR) prediction, and patient mortality prediction. Furthermore, we\ndemonstrate that the med-gte-hybrid model improves patient stratification,\nclustering, and text retrieval, thus outperforms current state-of-the-art\nmodels on the Massive Text Embedding Benchmark (MTEB). While some of our\nevaluations focus on CKD, our hybrid tuning of sentence transformers could be\ntransferred to other medical domains and has the potential to improve clinical\ndecision-making and personalised treatment pathways in various healthcare\napplications.", "published": "2025-02-21 23:17:31", "link": "http://arxiv.org/abs/2502.15996v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Eeyore: Realistic Depression Simulation via Supervised and Preference\n  Optimization", "abstract": "Large Language Models (LLMs) have been previously explored for mental\nhealthcare training and therapy client simulation, but they still fall short in\nauthentically capturing diverse client traits and psychological conditions. We\nintroduce \\textbf{Eeyore}, an 8B model optimized for realistic depression\nsimulation through a structured alignment framework, incorporating expert input\nat every stage. First, we systematically curate real-world depression-related\nconversations, extracting depressive traits to guide data filtering and\npsychological profile construction, and use this dataset to instruction-tune\nEeyore for profile adherence. Next, to further enhance realism, Eeyore\nundergoes iterative preference optimization -- first leveraging model-generated\npreferences and then calibrating with a small set of expert-annotated\npreferences. Throughout the entire pipeline, we actively collaborate with\ndomain experts, developing interactive interfaces to validate trait extraction\nand iteratively refine structured psychological profiles for clinically\nmeaningful role-play customization. Despite its smaller model size, the Eeyore\ndepression simulation outperforms GPT-4o with SOTA prompting strategies, both\nin linguistic authenticity and profile adherence.", "published": "2025-02-21 20:29:44", "link": "http://arxiv.org/abs/2503.00018v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "CoT-ICL Lab: A Petri Dish for Studying Chain-of-Thought Learning from\n  In-Context Demonstrations", "abstract": "We introduce CoT-ICL Lab, a framework and methodology to generate synthetic\ntokenized datasets and systematically study chain-of-thought (CoT) in-context\nlearning (ICL) in language models. CoT-ICL Lab allows fine grained control over\nthe complexity of in-context examples by decoupling (1) the causal structure\ninvolved in chain token generation from (2) the underlying token processing\nfunctions. We train decoder-only transformers (up to 700M parameters) on these\ndatasets and show that CoT accelerates the accuracy transition to higher values\nacross model sizes. In particular, we find that model depth is crucial for\nleveraging CoT with limited in-context examples, while more examples help\nshallow models match deeper model performance. Additionally, limiting the\ndiversity of token processing functions throughout training improves causal\nstructure learning via ICL. We also interpret these transitions by analyzing\ntransformer embeddings and attention maps. Overall, CoT-ICL Lab serves as a\nsimple yet powerful testbed for theoretical and empirical insights into ICL and\nCoT in language models.", "published": "2025-02-21 01:24:54", "link": "http://arxiv.org/abs/2502.15132v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PairBench: A Systematic Framework for Selecting Reliable Judge VLMs", "abstract": "As large vision language models (VLMs) are increasingly used as automated\nevaluators, understanding their ability to effectively compare data pairs as\ninstructed in the prompt becomes essential. To address this, we present\nPairBench, a low-cost framework that systematically evaluates VLMs as\ncustomizable similarity tools across various modalities and scenarios. Through\nPairBench, we introduce four metrics that represent key desiderata of\nsimilarity scores: alignment with human annotations, consistency for data pairs\nirrespective of their order, smoothness of similarity distributions, and\ncontrollability through prompting. Our analysis demonstrates that no model,\nwhether closed- or open-source, is superior on all metrics; the optimal choice\ndepends on an auto evaluator's desired behavior (e.g., a smooth vs. a sharp\njudge), highlighting risks of widespread adoption of VLMs as evaluators without\nthorough assessment. For instance, the majority of VLMs struggle with\nmaintaining symmetric similarity scores regardless of order. Additionally, our\nresults show that the performance of VLMs on the metrics in PairBench closely\ncorrelates with popular benchmarks, showcasing its predictive power in ranking\nmodels.", "published": "2025-02-21 04:53:11", "link": "http://arxiv.org/abs/2502.15210v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "The Evolving Landscape of LLM- and VLM-Integrated Reinforcement Learning", "abstract": "Reinforcement learning (RL) has shown impressive results in sequential\ndecision-making tasks. Meanwhile, Large Language Models (LLMs) and\nVision-Language Models (VLMs) have emerged, exhibiting impressive capabilities\nin multimodal understanding and reasoning. These advances have led to a surge\nof research integrating LLMs and VLMs into RL. In this survey, we review\nrepresentative works in which LLMs and VLMs are used to overcome key challenges\nin RL, such as lack of prior knowledge, long-horizon planning, and reward\ndesign. We present a taxonomy that categorizes these LLM/VLM-assisted RL\napproaches into three roles: agent, planner, and reward. We conclude by\nexploring open problems, including grounding, bias mitigation, improved\nrepresentations, and action advice. By consolidating existing research and\nidentifying future directions, this survey establishes a framework for\nintegrating LLMs and VLMs into RL, advancing approaches that unify natural\nlanguage and visual understanding with sequential decision-making.", "published": "2025-02-21 05:01:30", "link": "http://arxiv.org/abs/2502.15214v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ESPnet-SpeechLM: An Open Speech Language Model Toolkit", "abstract": "We present ESPnet-SpeechLM, an open toolkit designed to democratize the\ndevelopment of speech language models (SpeechLMs) and voice-driven agentic\napplications. The toolkit standardizes speech processing tasks by framing them\nas universal sequential modeling problems, encompassing a cohesive workflow of\ndata preprocessing, pre-training, inference, and task evaluation. With\nESPnet-SpeechLM, users can easily define task templates and configure key\nsettings, enabling seamless and streamlined SpeechLM development. The toolkit\nensures flexibility, efficiency, and scalability by offering highly\nconfigurable modules for every stage of the workflow. To illustrate its\ncapabilities, we provide multiple use cases demonstrating how competitive\nSpeechLMs can be constructed with ESPnet-SpeechLM, including a 1.7B-parameter\nmodel pre-trained on both text and speech tasks, across diverse benchmarks. The\ntoolkit and its recipes are fully transparent and reproducible at:\nhttps://github.com/espnet/espnet/tree/speechlm.", "published": "2025-02-21 05:21:58", "link": "http://arxiv.org/abs/2502.15218v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A BERT Based Hybrid Recommendation System For Academic Collaboration", "abstract": "Universities serve as a hub for academic collaboration, promoting the\nexchange of diverse ideas and perspectives among students and faculty through\ninterdisciplinary dialogue. However, as universities expand in size,\nconventional networking approaches via student chapters, class groups, and\nfaculty committees become cumbersome. To address this challenge, an\nacademia-specific profile recommendation system is proposed to connect\nlike-minded stakeholders within any university community. This study evaluates\nthree techniques: Term Frequency-Inverse Document Frequency (TF-IDF),\nBidirectional Encoder Representations from Transformers (BERT), and a hybrid\napproach to generate effective recommendations. Due to the unlabelled nature of\nthe dataset, Affinity Propagation cluster-based relabelling is performed to\nunderstand the grouping of similar profiles. The hybrid model demonstrated\nsuperior performance, evidenced by its similarity score, Silhouette score,\nDavies-Bouldin index, and Normalized Discounted Cumulative Gain (NDCG),\nachieving an optimal balance between diversity and relevance in\nrecommendations. Furthermore, the optimal model has been implemented as a\nmobile application, which dynamically suggests relevant profiles based on\nusers' skills and collaboration interests, incorporating contextual\nunderstanding. The potential impact of this application is significant, as it\npromises to enhance networking opportunities within large academic institutions\nthrough the deployment of intelligent recommendation systems.", "published": "2025-02-21 05:35:08", "link": "http://arxiv.org/abs/2502.15223v1", "categories": ["cs.IR", "cs.CL", "cs.SI"], "primary_category": "cs.IR"}
{"title": "Understand User Opinions of Large Language Models via LLM-Powered\n  In-the-Moment User Experience Interviews", "abstract": "Which large language model (LLM) is better? Every evaluation tells a story,\nbut what do users really think about current LLMs? This paper presents CLUE, an\nLLM-powered interviewer that conducts in-the-moment user experience interviews,\nright after users interacted with LLMs, and automatically gathers insights\nabout user opinions from massive interview logs. We conduct a study with\nthousands of users to understand user opinions on mainstream LLMs, recruiting\nusers to first chat with a target LLM and then interviewed by CLUE. Our\nexperiments demonstrate that CLUE captures interesting user opinions, for\nexample, the bipolar views on the displayed reasoning process of DeepSeek-R1\nand demands for information freshness and multi-modality. Our collected\nchat-and-interview logs will be released.", "published": "2025-02-21 05:42:22", "link": "http://arxiv.org/abs/2502.15226v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Retrieval-Augmented Speech Recognition Approach for Domain Challenges", "abstract": "Speech recognition systems often face challenges due to domain mismatch,\nparticularly in real-world applications where domain-specific data is\nunavailable because of data accessibility and confidentiality constraints.\nInspired by Retrieval-Augmented Generation (RAG) techniques for large language\nmodels (LLMs), this paper introduces a LLM-based retrieval-augmented speech\nrecognition method that incorporates domain-specific textual data at the\ninference stage to enhance recognition performance. Rather than relying on\ndomain-specific textual data during the training phase, our model is trained to\nlearn how to utilize textual information provided in prompts for LLM decoder to\nimprove speech recognition performance. Benefiting from the advantages of the\nRAG retrieval mechanism, our approach efficiently accesses locally available\ndomain-specific documents, ensuring a convenient and effective process for\nsolving domain mismatch problems. Experiments conducted on the CSJ database\ndemonstrate that the proposed method significantly improves speech recognition\naccuracy and achieves state-of-the-art results on the CSJ dataset, even without\nrelying on the full training data.", "published": "2025-02-21 07:47:50", "link": "http://arxiv.org/abs/2502.15264v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention", "abstract": "For the efficient inference of Large Language Models (LLMs), the effective\ncompression of key-value (KV) cache is essential. Three main types of KV cache\ncompression techniques, namely sparsity, channel compression, and quantization,\nhave been identified. This study presents SVDq, a Singular Value Decomposition\n(SVD) - based mixed precision quantization method for K cache. Initially, K\ncache is transformed into latent channels using SVD basis representations.\nSince the values in latent channels decay rapidly and become negligible after\nonly a few latent channels, our method then incorporates importance-aware\nquantization and compression for latent channels. This enables the effective\nallocation of higher precision to more significant channels. Theoretically, we\nprove that SVDq results in quantization errors (x0.1 or even lower) that are\nmuch lower than those of per-channel key quantization in the original space.\nOur findings based on RULER and LongBench benchmarks demonstrate that SVDq can\nachieve an equivalent key cache precision as low as 1.25-bit. When combined\nwith key sparsity, it can reach a key compression ratio of up to 410x for\nattention computation, all while maintaining comparable model performance.\nNotably, our method is nearly lossless for LongBench datasets. This indicates\nthat SVDq enables high-precision low-bit quantization, providing a more\nefficient solution for KV cache compression in LLMs.", "published": "2025-02-21 08:55:21", "link": "http://arxiv.org/abs/2502.15304v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T50"], "primary_category": "cs.LG"}
{"title": "AttentionEngine: A Versatile Framework for Efficient Attention\n  Mechanisms on Diverse Hardware Platforms", "abstract": "Transformers and large language models (LLMs) have revolutionized machine\nlearning, with attention mechanisms at the core of their success. As the\nlandscape of attention variants expands, so too do the challenges of optimizing\ntheir performance, particularly across different hardware platforms. Current\noptimization strategies are often narrowly focused, requiring extensive manual\nintervention to accommodate changes in model configurations or hardware\nenvironments. In this paper, we introduce AttentionEngine, a comprehensive\nframework designed to streamline the optimization of attention mechanisms\nacross heterogeneous hardware backends. By decomposing attention computation\ninto modular operations with customizable components, AttentionEngine enables\nflexible adaptation to diverse algorithmic requirements. The framework further\nautomates kernel optimization through a combination of programmable templates\nand a robust cross-platform scheduling strategy. Empirical results reveal\nperformance gains of up to 10x on configurations beyond the reach of existing\nmethods. AttentionEngine offers a scalable, efficient foundation for developing\nand deploying attention mechanisms with minimal manual tuning. Our code has\nbeen open-sourced and is available at\nhttps://github.com/microsoft/AttentionEngine.", "published": "2025-02-21 10:06:41", "link": "http://arxiv.org/abs/2502.15349v1", "categories": ["cs.CL", "cs.LG", "cs.PF"], "primary_category": "cs.CL"}
{"title": "Identifying Features that Shape Perceived Consciousness in Large\n  Language Model-based AI: A Quantitative Study of Human Responses", "abstract": "This study quantitively examines which features of AI-generated text lead\nhumans to perceive subjective consciousness in large language model (LLM)-based\nAI systems. Drawing on 99 passages from conversations with Claude 3 Opus and\nfocusing on eight features -- metacognitive self-reflection, logical reasoning,\nempathy, emotionality, knowledge, fluency, unexpectedness, and subjective\nexpressiveness -- we conducted a survey with 123 participants. Using regression\nand clustering analyses, we investigated how these features influence\nparticipants' perceptions of AI consciousness. The results reveal that\nmetacognitive self-reflection and the AI's expression of its own emotions\nsignificantly increased perceived consciousness, while a heavy emphasis on\nknowledge reduced it. Participants clustered into seven subgroups, each showing\ndistinct feature-weighting patterns. Additionally, higher prior knowledge of\nLLMs and more frequent usage of LLM-based chatbots were associated with greater\noverall likelihood assessments of AI consciousness. This study underscores the\nmultidimensional and individualized nature of perceived AI consciousness and\nprovides a foundation for better understanding the psychosocial implications of\nhuman-AI interaction.", "published": "2025-02-21 10:27:28", "link": "http://arxiv.org/abs/2502.15365v2", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "I.2.7; K.4"], "primary_category": "cs.HC"}
{"title": "Chitrarth: Bridging Vision and Language for a Billion People", "abstract": "Recent multimodal foundation models are primarily trained on English or high\nresource European language data, which hinders their applicability to other\nmedium and low-resource languages. To address this limitation, we introduce\nChitrarth (Chitra: Image; Artha: Meaning), an inclusive Vision-Language Model\n(VLM), specifically targeting the rich linguistic diversity and visual\nreasoning across 10 prominent Indian languages. Our model effectively\nintegrates a state-of-the-art (SOTA) multilingual Large Language Model (LLM)\nwith a vision module, primarily trained on multilingual image-text data.\nFurthermore, we also introduce BharatBench, a comprehensive framework for\nevaluating VLMs across various Indian languages, ultimately contributing to\nmore diverse and effective AI systems. Our model achieves SOTA results for\nbenchmarks across low resource languages while retaining its efficiency in\nEnglish. Through our research, we aim to set new benchmarks in\nmultilingual-multimodal capabilities, offering substantial improvements over\nexisting models and establishing a foundation to facilitate future advancements\nin this arena.", "published": "2025-02-21 11:38:40", "link": "http://arxiv.org/abs/2502.15392v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Beyond Translation: LLM-Based Data Generation for Multilingual\n  Fact-Checking", "abstract": "Robust automatic fact-checking systems have the potential to combat online\nmisinformation at scale. However, most existing research primarily focuses on\nEnglish. In this paper, we introduce MultiSynFact, the first large-scale\nmultilingual fact-checking dataset containing 2.2M claim-source pairs designed\nto support Spanish, German, English, and other low-resource languages. Our\ndataset generation pipeline leverages Large Language Models (LLMs), integrating\nexternal knowledge from Wikipedia and incorporating rigorous claim validation\nsteps to ensure data quality. We evaluate the effectiveness of MultiSynFact\nacross multiple models and experimental settings. Additionally, we open-source\na user-friendly framework to facilitate further research in multilingual\nfact-checking and dataset generation.", "published": "2025-02-21 12:38:26", "link": "http://arxiv.org/abs/2502.15419v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Evaluating Multimodal Generative AI with Korean Educational Standards", "abstract": "This paper presents the Korean National Educational Test Benchmark (KoNET), a\nnew benchmark designed to evaluate Multimodal Generative AI Systems using\nKorean national educational tests. KoNET comprises four exams: the Korean\nElementary General Educational Development Test (KoEGED), Middle (KoMGED), High\n(KoHGED), and College Scholastic Ability Test (KoCSAT). These exams are\nrenowned for their rigorous standards and diverse questions, facilitating a\ncomprehensive analysis of AI performance across different educational levels.\nBy focusing on Korean, KoNET provides insights into model performance in\nless-explored languages. We assess a range of models - open-source,\nopen-access, and closed APIs - by examining difficulties, subject diversity,\nand human error rates. The code and dataset builder will be made fully\nopen-sourced at https://github.com/naver-ai/KoNET.", "published": "2025-02-21 12:46:40", "link": "http://arxiv.org/abs/2502.15422v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Single-pass Detection of Jailbreaking Input in Large Language Models", "abstract": "Defending aligned Large Language Models (LLMs) against jailbreaking attacks\nis a challenging problem, with existing approaches requiring multiple requests\nor even queries to auxiliary LLMs, making them computationally heavy. Instead,\nwe focus on detecting jailbreaking input in a single forward pass. Our method,\ncalled Single Pass Detection SPD, leverages the information carried by the\nlogits to predict whether the output sentence will be harmful. This allows us\nto defend in just one forward pass. SPD can not only detect attacks effectively\non open-source models, but also minimizes the misclassification of harmless\ninputs. Furthermore, we show that SPD remains effective even without complete\nlogit access in GPT-3.5 and GPT-4. We believe that our proposed method offers a\npromising approach to efficiently safeguard LLMs against adversarial attacks.", "published": "2025-02-21 13:04:13", "link": "http://arxiv.org/abs/2502.15435v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Fed-SB: A Silver Bullet for Extreme Communication Efficiency and\n  Performance in (Private) Federated LoRA Fine-Tuning", "abstract": "Low-Rank Adaptation (LoRA) has become ubiquitous for efficiently fine-tuning\nfoundation models. However, federated fine-tuning using LoRA is challenging due\nto suboptimal updates arising from traditional federated averaging of\nindividual adapters. Existing solutions either incur prohibitively high\ncommunication cost that scales linearly with the number of clients or suffer\nfrom performance degradation due to limited expressivity. We introduce\nFederated Silver Bullet (Fed-SB), a novel approach for federated fine-tuning of\nLLMs using LoRA-SB, a recently proposed low-rank adaptation method. LoRA-SB\noptimally aligns the optimization trajectory with the ideal low-rank full\nfine-tuning projection by learning a small square matrix (R) between adapters B\nand A, keeping other components fixed. Direct averaging of R guarantees exact\nupdates, substantially reducing communication cost, which remains independent\nof the number of clients, and enables scalability. Fed-SB achieves\nstate-of-the-art performance across commonsense reasoning, arithmetic\nreasoning, and language inference tasks while reducing communication costs by\nup to 230x. In private settings, Fed-SB further improves performance by (1)\nreducing trainable parameters, thereby lowering the noise required for\ndifferential privacy and (2) avoiding noise amplification introduced by other\nmethods. Overall, Fed-SB establishes a new Pareto frontier in the tradeoff\nbetween communication and performance, offering an efficient and scalable\nsolution for both private and non-private federated fine-tuning. Our code is\npublicly available at https://github.com/CERT-Lab/fed-sb.", "published": "2025-02-21 13:05:19", "link": "http://arxiv.org/abs/2502.15436v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC"], "primary_category": "cs.LG"}
{"title": "Activation Steering in Neural Theorem Provers", "abstract": "Large Language Models (LLMs) have shown promise in proving formal theorems\nusing proof assistants like Lean. However, current state of the art language\nmodels struggles to predict next step in proofs leading practitioners to use\ndifferent sampling techniques to improve LLMs capabilities. We observe that the\nLLM is capable of predicting the correct tactic; however, it faces challenges\nin ranking it appropriately within the set of candidate tactics, affecting the\noverall selection process. To overcome this hurdle, we use activation steering\nto guide LLMs responses to improve the generations at the time of inference.\nOur results suggest that activation steering offers a promising lightweight\nalternative to specialized fine-tuning for enhancing theorem proving\ncapabilities in LLMs, particularly valuable in resource-constrained\nenvironments.", "published": "2025-02-21 15:04:48", "link": "http://arxiv.org/abs/2502.15507v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SOTOPIA-\u03a9: Dynamic Strategy Injection Learning and Social\n  Instruction Following Evaluation for Social Agents", "abstract": "Despite the abundance of prior social strategies possessed by humans, there\nremains a paucity of research dedicated to their transfer and integration into\nsocial agents. Our proposed SOTOPIA-{\\Omega} framework aims to address and\nbridge this gap, with a particular focus on enhancing the social capabilities\nof language agents. This framework dynamically injects multi-step reasoning\nstrategies inspired by negotiation theory and two simple direct strategies into\nexpert agents, thereby automating the construction of a high-quality social\ndialogue training corpus. Additionally, we introduce the concept of Social\nInstruction Following (S-IF) and propose two new S-IF evaluation metrics that\ncomplement social capability. We demonstrate that several 7B models trained on\nhigh-quality corpus not only significantly surpass the expert agent (GPT-4) in\nachieving social goals but also enhance S-IF performance. Analysis and variant\nexperiments validate the advantages of dynamic construction, which can\nespecially break the agent's prolonged deadlock.", "published": "2025-02-21 15:40:37", "link": "http://arxiv.org/abs/2502.15538v2", "categories": ["cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Bridging vision language model (VLM) evaluation gaps with a framework\n  for scalable and cost-effective benchmark generation", "abstract": "Reliable evaluation of AI models is critical for scientific progress and\npractical application. While existing VLM benchmarks provide general insights\ninto model capabilities, their heterogeneous designs and limited focus on a few\nimaging domains pose significant challenges for both cross-domain performance\ncomparison and targeted domain-specific evaluation. To address this, we propose\nthree key contributions: (1) a framework for the resource-efficient creation of\ndomain-specific VLM benchmarks enabled by task augmentation for creating\nmultiple diverse tasks from a single existing task, (2) the release of new VLM\nbenchmarks for seven domains, created according to the same homogeneous\nprotocol and including 162,946 thoroughly human-validated answers, and (3) an\nextensive benchmarking of 22 state-of-the-art VLMs on a total of 37,171 tasks,\nrevealing performance variances across domains and tasks, thereby supporting\nthe need for tailored VLM benchmarks. Adoption of our methodology will pave the\nway for the resource-efficient domain-specific selection of models and guide\nfuture research efforts toward addressing core open questions.", "published": "2025-02-21 16:24:10", "link": "http://arxiv.org/abs/2502.15563v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Do Multilingual LLMs Think In English?", "abstract": "Large language models (LLMs) have multilingual capabilities and can solve\ntasks across various languages. However, we show that current LLMs make key\ndecisions in a representation space closest to English, regardless of their\ninput and output languages. Exploring the internal representations with a logit\nlens for sentences in French, German, Dutch, and Mandarin, we show that the LLM\nfirst emits representations close to English for semantically-loaded words\nbefore translating them into the target language. We further show that\nactivation steering in these LLMs is more effective when the steering vectors\nare computed in English rather than in the language of the inputs and outputs.\nThis suggests that multilingual LLMs perform key reasoning steps in a\nrepresentation that is heavily shaped by English in a way that is not\ntransparent to system users.", "published": "2025-02-21 17:19:23", "link": "http://arxiv.org/abs/2502.15603v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Robustness of Transformers against Context Hijacking for Linear\n  Classification", "abstract": "Transformer-based Large Language Models (LLMs) have demonstrated powerful\nin-context learning capabilities. However, their predictions can be disrupted\nby factually correct context, a phenomenon known as context hijacking,\nrevealing a significant robustness issue. To understand this phenomenon\ntheoretically, we explore an in-context linear classification problem based on\nrecent advances in linear transformers. In our setup, context tokens are\ndesigned as factually correct query-answer pairs, where the queries are similar\nto the final query but have opposite labels. Then, we develop a general\ntheoretical analysis on the robustness of the linear transformers, which is\nformulated as a function of the model depth, training context lengths, and\nnumber of hijacking context tokens. A key finding is that a well-trained deeper\ntransformer can achieve higher robustness, which aligns with empirical\nobservations. We show that this improvement arises because deeper layers enable\nmore fine-grained optimization steps, effectively mitigating interference from\ncontext hijacking. This is also well supported by our numerical experiments.\nOur findings provide theoretical insights into the benefits of deeper\narchitectures and contribute to enhancing the understanding of transformer\narchitectures.", "published": "2025-02-21 17:31:00", "link": "http://arxiv.org/abs/2502.15609v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Probe Pruning: Accelerating LLMs through Dynamic Pruning via\n  Model-Probing", "abstract": "We introduce Probe Pruning (PP), a novel framework for online, dynamic,\nstructured pruning of Large Language Models (LLMs) applied in a batch-wise\nmanner. PP leverages the insight that not all samples and tokens contribute\nequally to the model's output, and probing a small portion of each batch\neffectively identifies crucial weights, enabling tailored dynamic pruning for\ndifferent batches. It comprises three main stages: probing, history-informed\npruning, and full inference. In the probing stage, PP selects a small yet\ncrucial set of hidden states, based on residual importance, to run a few model\nlayers ahead. During the history-informed pruning stage, PP strategically\nintegrates the probing states with historical states. Subsequently, it\nstructurally prunes weights based on the integrated states and the PP\nimportance score, a metric developed specifically to assess the importance of\neach weight channel in maintaining performance. In the final stage, full\ninference is conducted on the remaining weights. A major advantage of PP is its\ncompatibility with existing models, as it operates without requiring additional\nneural network modules or fine-tuning. Comprehensive evaluations of PP on\nLLaMA-2/3 and OPT models reveal that even minimal probing-using just 1.5% of\nFLOPs-can substantially enhance the efficiency of structured pruning of LLMs.\nFor instance, when evaluated on LLaMA-2-7B with WikiText2, PP achieves a 2.56\ntimes lower ratio of performance degradation per unit of runtime reduction\ncompared to the state-of-the-art method at a 40% pruning ratio. Our code is\navailable at https://github.com/Qi-Le1/Probe_Pruning.", "published": "2025-02-21 17:41:21", "link": "http://arxiv.org/abs/2502.15618v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Steering into New Embedding Spaces: Analyzing Cross-Lingual Alignment\n  Induced by Model Interventions in Multilingual Language Models", "abstract": "Aligned representations across languages is a desired property in\nmultilingual large language models (mLLMs), as alignment can improve\nperformance in cross-lingual tasks. Typically alignment requires fine-tuning a\nmodel, which is computationally expensive, and sizable language data, which\noften may not be available. A data-efficient alternative to fine-tuning is\nmodel interventions -- a method for manipulating model activations to steer\ngeneration into the desired direction. We analyze the effect of a popular\nintervention (finding experts) on the alignment of cross-lingual\nrepresentations in mLLMs. We identify the neurons to manipulate for a given\nlanguage and introspect the embedding space of mLLMs pre- and\npost-manipulation. We show that modifying the mLLM's activations changes its\nembedding space such that cross-lingual alignment is enhanced. Further, we show\nthat the changes to the embedding space translate into improved downstream\nperformance on retrieval tasks, with up to 2x improvements in top-1 accuracy on\ncross-lingual retrieval.", "published": "2025-02-21 18:09:54", "link": "http://arxiv.org/abs/2502.15639v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Almost AI, Almost Human: The Challenge of Detecting AI-Polished Writing", "abstract": "The growing use of large language models (LLMs) for text generation has led\nto widespread concerns about AI-generated content detection. However, an\noverlooked challenge is AI-polished text, where human-written content undergoes\nsubtle refinements using AI tools. This raises a critical question: should\nminimally polished text be classified as AI-generated? Misclassification can\nlead to false plagiarism accusations and misleading claims about AI prevalence\nin online content. In this study, we systematically evaluate eleven\nstate-of-the-art AI-text detectors using our AI-Polished-Text Evaluation\n(APT-Eval) dataset, which contains $11.7K$ samples refined at varying\nAI-involvement levels. Our findings reveal that detectors frequently\nmisclassify even minimally polished text as AI-generated, struggle to\ndifferentiate between degrees of AI involvement, and exhibit biases against\nolder and smaller models. These limitations highlight the urgent need for more\nnuanced detection methodologies.", "published": "2025-02-21 18:45:37", "link": "http://arxiv.org/abs/2502.15666v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FLEKE: Federated Locate-then-Edit Knowledge Editing", "abstract": "Locate-then-Edit Knowledge Editing (LEKE) is a key technique for updating\nlarge language models (LLMs) without full retraining. However, existing methods\nassume a single-user setting and become inefficient in real-world multi-client\nscenarios, where decentralized organizations (e.g., hospitals, financial\ninstitutions) independently update overlapping knowledge, leading to redundant\nmediator knowledge vector (MKV) computations and privacy concerns. To address\nthese challenges, we introduce Federated Locate-then-Edit Knowledge Editing\n(FLEKE), a novel task that enables multiple clients to collaboratively perform\nLEKE while preserving privacy and reducing computational overhead. To achieve\nthis, we propose FedEdit, a two-stage framework that optimizes MKV selection\nand reuse. In the first stage, clients locally apply LEKE and upload the\ncomputed MKVs. In the second stage, rather than relying solely on server-based\nMKV sharing, FLEKE allows clients retrieve relevant MKVs based on cosine\nsimilarity, enabling knowledge re-edit and minimizing redundant computations.\nExperimental results on two benchmark datasets demonstrate that FedEdit retains\nover 96% of the performance of non-federated LEKE while significantly\noutperforming a FedAvg-based baseline by approximately twofold. Besides, we\nfind that MEMIT performs more consistently than PMET in the FLEKE task with our\nFedEdit framework. Our code is available at https://github.com/zongkaiz/FLEKE.", "published": "2025-02-21 18:58:06", "link": "http://arxiv.org/abs/2502.15677v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Domain-Specific Retrieval-Augmented Generation: Synthetic Data\n  Generation and Evaluation using Reasoning Models", "abstract": "Retrieval-Augmented Generation (RAG) systems face significant performance\ngaps when applied to technical domains requiring precise information extraction\nfrom complex documents. Current evaluation methodologies relying on\ndocument-level metrics inadequately capture token-resolution retrieval accuracy\nthat is critical for domain-related documents. We propose a framework combining\ngranular evaluation metrics with synthetic data generation to optimize\ndomain-specific RAG performance. First, we introduce token-aware metrics\nPrecision $\\Omega$ and Intersection-over-Union (IoU) that quantify context\npreservation versus information density trade-offs inherent in technical texts.\nSecond, we develop a reasoning model-driven pipeline using instruction-tuned\nLLMs (DeepSeek-R1, DeepSeek-R1 distilled variants, and Phi-4) to generate\ncontext-anchored QA pairs with discontinuous reference spans across three\nspecialized corpora: SEC 10-K filings (finance), biomedical abstracts (PubMed),\nand APT threat reports (cybersecurity).\n  Our empirical analysis reveals critical insights: smaller chunks (less than\n10 tokens) improve precision by 31-42% (IoU = 0.071 vs. baseline 0.053) at\nrecall costs (-18%), while domain-specific embedding strategies yield 22%\nvariance in optimal chunk sizing (5-20 tokens). The\nDeepSeek-R1-Distill-Qwen-32B model demonstrates superior concept alignment\n(+14% mean IoU over alternatives), though no configuration universally\ndominates. Financial texts favor larger chunks for risk factor coverage (Recall\n= 0.81 at size = 20), whereas cybersecurity content benefits from atomic\nsegmentation, Precision $\\Omega = 0.28$ at size = 5.\n  Our code is available on\nhttps://github.com/aryan-jadon/Synthetic-Data-Generation-and-Evaluation-using-Reasoning-Model", "published": "2025-02-21 06:38:57", "link": "http://arxiv.org/abs/2502.15854v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "PPC-GPT: Federated Task-Specific Compression of Large Language Models\n  via Pruning and Chain-of-Thought Distillation", "abstract": "Compressing Large Language Models (LLMs) into task-specific Small Language\nModels (SLMs) encounters two significant challenges: safeguarding\ndomain-specific knowledge privacy and managing limited resources. To tackle\nthese challenges, we propose PPC-GPT, a innovative privacy-preserving federated\nframework specifically designed for compressing LLMs into task-specific SLMs\nvia pruning and Chain-of-Thought (COT) distillation. PPC-GPT works on a\nserver-client federated architecture, where the client sends differentially\nprivate (DP) perturbed task-specific data to the server's LLM. The LLM then\ngenerates synthetic data along with their corresponding rationales. This\nsynthetic data is subsequently used for both LLM pruning and retraining\nprocesses. Additionally, we harness COT knowledge distillation, leveraging the\nsynthetic data to further improve the retraining of structurally-pruned SLMs.\nOur experimental results demonstrate the effectiveness of PPC-GPT across\nvarious text generation tasks. By compressing LLMs into task-specific SLMs,\nPPC-GPT not only achieves competitive performance but also prioritizes data\nprivacy protection.", "published": "2025-02-21 07:32:49", "link": "http://arxiv.org/abs/2502.15857v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Synthetic vs. Gold: The Role of LLM-Generated Labels and Data in\n  Cyberbullying Detection", "abstract": "Cyberbullying (CB) presents a pressing threat, especially to children,\nunderscoring the urgent need for robust detection systems to ensure online\nsafety. However, progress in developing such systems is hindered by the\nscarcity of large, labeled datasets that are specifically tailored for\nspecialized tasks and the target age groups. Creating these datasets relies\nheavily on human annotation, which not only strains resources but also raises\nsignificant ethical and legal concerns due to annotators' exposure to harmful\ncontent, notwithstanding the acquisition of this type of data from vulnerable\npopulations such as children. In this paper, we address these challenges by\nleveraging Large Language Models (LLMs) to generate synthetic data and labels.\nOur experiments demonstrate that synthetic data enables BERT-based CB\nclassifiers to achieve performance close to that of those trained on fully\nauthentic datasets (75.8% vs. 81.5% accuracy). Additionally, LLMs can\neffectively label authentic yet unlabeled data, allowing BERT classifiers to\nattain a comparable performance level (79.1% vs. 81.5% accuracy). These results\nhighlight the potential of LLMs as a scalable, ethical, and cost-effective\nsolution for generating data for CB detection.", "published": "2025-02-21 10:17:29", "link": "http://arxiv.org/abs/2502.15860v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Position: Standard Benchmarks Fail -- LLM Agents Present Overlooked\n  Risks for Financial Applications", "abstract": "Current financial LLM agent benchmarks are inadequate. They prioritize task\nperformance while ignoring fundamental safety risks. Threats like\nhallucinations, temporal misalignment, and adversarial vulnerabilities pose\nsystemic risks in high-stakes financial environments, yet existing evaluation\nframeworks fail to capture these risks. We take a firm position: traditional\nbenchmarks are insufficient to ensure the reliability of LLM agents in finance.\nTo address this, we analyze existing financial LLM agent benchmarks, finding\nsafety gaps and introducing ten risk-aware evaluation metrics. Through an\nempirical evaluation of both API-based and open-weight LLM agents, we reveal\nhidden vulnerabilities that remain undetected by conventional assessments. To\nmove the field forward, we propose the Safety-Aware Evaluation Agent (SAEA),\ngrounded in a three-level evaluation framework that assesses agents at the\nmodel level (intrinsic capabilities), workflow level (multi-step process\nreliability), and system level (integration robustness). Our findings highlight\nthe urgent need to redefine LLM agent evaluation standards by shifting the\nfocus from raw performance to safety, robustness, and real world resilience.", "published": "2025-02-21 12:56:15", "link": "http://arxiv.org/abs/2502.15865v1", "categories": ["q-fin.GN", "cs.AI", "cs.CL"], "primary_category": "q-fin.GN"}
{"title": "A Comprehensive Survey on the Trustworthiness of Large Language Models\n  in Healthcare", "abstract": "The application of large language models (LLMs) in healthcare has the\npotential to revolutionize clinical decision-making, medical research, and\npatient care. As LLMs are increasingly integrated into healthcare systems,\nseveral critical challenges must be addressed to ensure their reliable and\nethical deployment. These challenges include truthfulness, where models\ngenerate misleading information; privacy, with risks of unintentional data\nretention; robustness, requiring defenses against adversarial attacks;\nfairness, addressing biases in clinical outcomes; explainability, ensuring\ntransparent decision-making; and safety, mitigating risks of misinformation and\nmedical errors. Recently, researchers have begun developing benchmarks and\nevaluation frameworks to systematically assess the trustworthiness of LLMs.\nHowever, the trustworthiness of LLMs in healthcare remains underexplored,\nlacking a systematic review that provides a comprehensive understanding and\nfuture insights into this area. This survey bridges this gap by providing a\ncomprehensive overview of the recent research of existing methodologies and\nsolutions aimed at mitigating the above risks in healthcare. By focusing on key\ntrustworthiness dimensions including truthfulness, privacy and safety,\nrobustness, fairness and bias, and explainability, we present a thorough\nanalysis of how these issues impact the reliability and ethical use of LLMs in\nhealthcare. This paper highlights ongoing efforts and offers insights into\nfuture research directions to ensure the safe and trustworthy deployment of\nLLMs in healthcare.", "published": "2025-02-21 18:43:06", "link": "http://arxiv.org/abs/2502.15871v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "MutaGReP: Execution-Free Repository-Grounded Plan Search for Code-Use", "abstract": "When a human requests an LLM to complete a coding task using functionality\nfrom a large code repository, how do we provide context from the repo to the\nLLM? One approach is to add the entire repo to the LLM's context window.\nHowever, most tasks involve only fraction of symbols from a repo, longer\ncontexts are detrimental to the LLM's reasoning abilities, and context windows\nare not unlimited. Alternatively, we could emulate the human ability to\nnavigate a large repo, pick out the right functionality, and form a plan to\nsolve the task. We propose MutaGReP (Mutation-guided Grounded Repository Plan\nSearch), an approach to search for plans that decompose a user request into\nnatural language steps grounded in the codebase. MutaGReP performs neural tree\nsearch in plan space, exploring by mutating plans and using a symbol retriever\nfor grounding. On the challenging LongCodeArena benchmark, our plans use less\nthan 5% of the 128K context window for GPT-4o but rival the coding performance\nof GPT-4o with a context window filled with the repo. Plans produced by\nMutaGReP allow Qwen 2.5 Coder 32B and 72B to match the performance of GPT-4o\nwith full repo context and enable progress on the hardest LongCodeArena tasks.\nProject page: zaidkhan.me/MutaGReP", "published": "2025-02-21 18:58:17", "link": "http://arxiv.org/abs/2502.15872v1", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Directional Gradient Projection for Robust Fine-Tuning of Foundation\n  Models", "abstract": "Robust fine-tuning aims to adapt large foundation models to downstream tasks\nwhile preserving their robustness to distribution shifts. Existing methods\nprimarily focus on constraining and projecting current model towards the\npre-trained initialization based on the magnitudes between fine-tuned and\npre-trained weights, which often require extensive hyper-parameter tuning and\ncan sometimes result in underfitting. In this work, we propose Directional\nGradient Projection (DiGraP), a novel layer-wise trainable method that\nincorporates directional information from gradients to bridge regularization\nand multi-objective optimization. Besides demonstrating our method on image\nclassification, as another contribution we generalize this area to the\nmulti-modal evaluation settings for robust fine-tuning. Specifically, we first\nbridge the uni-modal and multi-modal gap by performing analysis on Image\nClassification reformulated Visual Question Answering (VQA) benchmarks and\nfurther categorize ten out-of-distribution (OOD) VQA datasets by distribution\nshift types and degree (i.e. near versus far OOD). Experimental results show\nthat DiGraP consistently outperforms existing baselines across Image\nClassfication and VQA tasks with discriminative and generative backbones,\nimproving both in-distribution (ID) generalization and OOD robustness.", "published": "2025-02-21 19:31:55", "link": "http://arxiv.org/abs/2502.15895v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "IPAD: Inverse Prompt for AI Detection -- A Robust and Explainable\n  LLM-Generated Text Detector", "abstract": "Large Language Models (LLMs) have attained human-level fluency in text\ngeneration, which complicates the distinguishing between human-written and\nLLM-generated texts. This increases the risk of misuse and highlights the need\nfor reliable detectors. Yet, existing detectors exhibit poor robustness on\nout-of-distribution (OOD) data and attacked data, which is critical for\nreal-world scenarios. Also, they struggle to provide explainable evidence to\nsupport their decisions, thus undermining the reliability. In light of these\nchallenges, we propose IPAD (Inverse Prompt for AI Detection), a novel\nframework consisting of a Prompt Inverter that identifies predicted prompts\nthat could have generated the input text, and a Distinguisher that examines how\nwell the input texts align with the predicted prompts. We develop and examine\ntwo versions of Distinguishers. Empirical evaluations demonstrate that both\nDistinguishers perform significantly better than the baseline methods, with\nversion2 outperforming baselines by 9.73% on in-distribution data (F1-score)\nand 12.65% on OOD data (AUROC). Furthermore, a user study is conducted to\nillustrate that IPAD enhances the AI detection trustworthiness by allowing\nusers to directly examine the decision-making evidence, which provides\ninterpretable support for its state-of-the-art detection results.", "published": "2025-02-21 19:41:32", "link": "http://arxiv.org/abs/2502.15902v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Mind the Gap! Static and Interactive Evaluations of Large Audio Models", "abstract": "As AI chatbots become ubiquitous, voice interaction presents a compelling way\nto enable rapid, high-bandwidth communication for both semantic and social\nsignals. This has driven research into Large Audio Models (LAMs) to power\nvoice-native experiences. However, aligning LAM development with user goals\nrequires a clear understanding of user needs and preferences to establish\nreliable progress metrics. This study addresses these challenges by introducing\nan interactive approach to evaluate LAMs and collecting 7,500 LAM interactions\nfrom 484 participants. Through topic modeling of user queries, we identify\nprimary use cases for audio interfaces. We then analyze user preference\nrankings and qualitative feedback to determine which models best align with\nuser needs. Finally, we evaluate how static benchmarks predict interactive\nperformance - our analysis reveals no individual benchmark strongly correlates\nwith interactive results ($\\tau \\leq 0.33$ for all benchmarks). While combining\nmultiple coarse-grained features yields modest predictive power ($R^2$=$0.30$),\nonly two out of twenty datasets on spoken question answering and age prediction\nshow significantly positive correlations. This suggests a clear need to develop\nLAM evaluations that better correlate with user preferences.", "published": "2025-02-21 20:29:02", "link": "http://arxiv.org/abs/2502.15919v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Straight to Zero: Why Linearly Decaying the Learning Rate to Zero Works\n  Best for LLMs", "abstract": "LLMs are commonly trained with a learning rate (LR) warmup, followed by\ncosine decay to 10% of the maximum (10x decay). In a large-scale empirical\nstudy, we show that under an optimal peak LR, a simple linear decay-to-zero\n(D2Z) schedule consistently outperforms other schedules when training at\ncompute-optimal dataset sizes. D2Z is superior across a range of model sizes,\nbatch sizes, datasets, and vocabularies. Benefits increase as dataset size\nincreases. Leveraging a novel interpretation of AdamW as an exponential moving\naverage of weight updates, we show how linear D2Z optimally balances the\ndemands of early training (moving away from initial conditions) and late\ntraining (averaging over more updates in order to mitigate gradient noise). In\nexperiments, a 610M-parameter model trained for 80 tokens-per-parameter (TPP)\nusing D2Z achieves lower loss than when trained for 200 TPP using 10x decay,\ncorresponding to an astonishing 60% compute savings. Models such as Llama2-7B,\ntrained for 286 TPP with 10x decay, could likely have saved a majority of\ncompute by training with D2Z.", "published": "2025-02-21 21:08:24", "link": "http://arxiv.org/abs/2502.15938v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Minions: Cost-efficient Collaboration Between On-device and Cloud\n  Language Models", "abstract": "We investigate an emerging setup in which a small, on-device language model\n(LM) with access to local data communicates with a frontier, cloud-hosted LM to\nsolve real-world tasks involving financial, medical, and scientific reasoning\nover long documents. Can a local-remote collaboration reduce cloud inference\ncosts while preserving quality? First, we consider a naive collaboration\nprotocol where the local and remote models simply chat back and forth. Because\nonly the local model reads the full context, this protocol achieves a 30.4x\nreduction in remote costs, but recovers only 87% of the performance of the\nfrontier model. We identify two key limitations of this protocol: the local\nmodel struggles to (1) follow the remote model's multi-step instructions and\n(2) reason over long contexts. Motivated by these observations, we study an\nextension of this protocol, coined MinionS, in which the remote model\ndecomposes the task into easier subtasks over shorter chunks of the document,\nthat are executed locally in parallel. MinionS reduces costs by 5.7x on average\nwhile recovering 97.9% of the performance of the remote model alone. Our\nanalysis reveals several key design choices that influence the trade-off\nbetween cost and performance in local-remote systems.", "published": "2025-02-21 21:54:40", "link": "http://arxiv.org/abs/2502.15964v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC"], "primary_category": "cs.LG"}
{"title": "Forgotten Polygons: Multimodal Large Language Models are Shape-Blind", "abstract": "Despite strong performance on vision-language tasks, Multimodal Large\nLanguage Models (MLLMs) struggle with mathematical problem-solving, with both\nopen-source and state-of-the-art models falling short of human performance on\nvisual-math benchmarks. To systematically examine visual-mathematical reasoning\nin MLLMs, we (1) evaluate their understanding of geometric primitives, (2) test\nmulti-step reasoning, and (3) explore a potential solution to improve visual\nreasoning capabilities. Our findings reveal fundamental shortcomings in shape\nrecognition, with top models achieving under 50% accuracy in identifying\nregular polygons. We analyze these failures through the lens of dual-process\ntheory and show that MLLMs rely on System 1 (intuitive, memorized associations)\nrather than System 2 (deliberate reasoning). Consequently, MLLMs fail to count\nthe sides of both familiar and novel shapes, suggesting they have neither\nlearned the concept of sides nor effectively process visual inputs. Finally, we\npropose Visually Cued Chain-of-Thought (VC-CoT) prompting, which enhances\nmulti-step mathematical reasoning by explicitly referencing visual annotations\nin diagrams, boosting GPT-4o's accuracy on an irregular polygon side-counting\ntask from 7% to 93%. Our findings suggest that System 2 reasoning in MLLMs\nremains an open problem, and visually-guided prompting is essential for\nsuccessfully engaging visual reasoning. Code available at:\nhttps://github.com/rsinghlab/Shape-Blind.", "published": "2025-02-21 22:04:09", "link": "http://arxiv.org/abs/2502.15969v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Automated Query-Product Relevance Labeling using Large Language Models\n  for E-commerce Search", "abstract": "Accurate query-product relevance labeling is indispensable to generate ground\ntruth dataset for search ranking in e-commerce. Traditional approaches for\nannotating query-product pairs rely on human-based labeling services, which is\nexpensive, time-consuming and prone to errors. In this work, we explore the\napplication of Large Language Models (LLMs) to automate query-product relevance\nlabeling for large-scale e-commerce search. We use several publicly available\nand proprietary LLMs for this task, and conducted experiments on two\nopen-source datasets and an in-house e-commerce search dataset. Using prompt\nengineering techniques such as Chain-of-Thought (CoT) prompting, In-context\nLearning (ICL), and Retrieval Augmented Generation (RAG) with Maximum Marginal\nRelevance (MMR), we show that LLM's performance has the potential to approach\nhuman-level accuracy on this task in a fraction of the time and cost required\nby human-labelers, thereby suggesting that our approach is more efficient than\nthe conventional methods. We have generated query-product relevance labels\nusing LLMs at scale, and are using them for evaluating improvements to our\nsearch algorithms. Our work demonstrates the potential of LLMs to improve\nquery-product relevance thus enhancing e-commerce search user experience. More\nimportantly, this scalable alternative to human-annotation has significant\nimplications for information retrieval domains including search and\nrecommendation systems, where relevance scoring is crucial for optimizing the\nranking of products and content to improve customer engagement and other\nconversion metrics.", "published": "2025-02-21 22:59:36", "link": "http://arxiv.org/abs/2502.15990v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Improving Value-based Process Verifier via Structural Prior Injection", "abstract": "In the Large Language Model(LLM) reasoning scenario, people often estimate\nstate value via Monte Carlo sampling. Though Monte Carlo estimation is an\nelegant method with less inductive bias, noise and errors are inevitably\nintroduced due to the limited sampling. To handle the problem, we inject the\nstructural prior into the value representation and transfer the scalar value\ninto the expectation of a pre-defined categorical distribution, representing\nthe noise and errors from a distribution perspective. Specifically, by treating\nthe result of Monte Carlo sampling as a single sample from the prior\nground-truth Binomial distribution, we quantify the sampling error as the\nmismatch between posterior estimated distribution and ground-truth\ndistribution, which is thus optimized via distribution selection optimization.\nWe test the performance of value-based process verifiers on Best-of-N task and\nBeam search task. Compared with the scalar value representation, we show that\nreasonable structural prior injection induced by different objective functions\nor optimization methods can improve the performance of value-based process\nverifiers for about 1$\\sim$2 points at little-to-no cost. We also show that\nunder different structural prior, the verifiers' performances vary greatly\ndespite having the same optimal solution, indicating the importance of\nreasonable structural prior injection.", "published": "2025-02-21 07:57:59", "link": "http://arxiv.org/abs/2502.17498v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "TurboFuzzLLM: Turbocharging Mutation-based Fuzzing for Effectively\n  Jailbreaking Large Language Models in Practice", "abstract": "Jailbreaking large-language models (LLMs) involves testing their robustness\nagainst adversarial prompts and evaluating their ability to withstand prompt\nattacks that could elicit unauthorized or malicious responses. In this paper,\nwe present TurboFuzzLLM, a mutation-based fuzzing technique for efficiently\nfinding a collection of effective jailbreaking templates that, when combined\nwith harmful questions, can lead a target LLM to produce harmful responses\nthrough black-box access via user prompts. We describe the limitations of\ndirectly applying existing template-based attacking techniques in practice, and\npresent functional and efficiency-focused upgrades we added to mutation-based\nfuzzing to generate effective jailbreaking templates automatically.\nTurboFuzzLLM achieves $\\geq$ 95\\% attack success rates (ASR) on public datasets\nfor leading LLMs (including GPT-4o \\& GPT-4 Turbo), shows impressive\ngeneralizability to unseen harmful questions, and helps in improving model\ndefenses to prompt attacks.", "published": "2025-02-21 21:10:12", "link": "http://arxiv.org/abs/2502.18504v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Comprehensive Analysis of Transparency and Accessibility of ChatGPT,\n  DeepSeek, And other SoTA Large Language Models", "abstract": "Despite increasing discussions on open-source Artificial Intelligence (AI),\nexisting research lacks a discussion on the transparency and accessibility of\nstate-of-the-art (SoTA) Large Language Models (LLMs). The Open Source\nInitiative (OSI) has recently released its first formal definition of\nopen-source software. This definition, when combined with standard dictionary\ndefinitions and the sparse published literature, provide an initial framework\nto support broader accessibility to AI models such as LLMs, but more work is\nessential to capture the unique dynamics of openness in AI. In addition,\nconcerns about open-washing, where models claim openness but lack full\ntransparency, has been raised, which limits the reproducibility, bias\nmitigation, and domain adaptation of these models. In this context, our study\ncritically analyzes SoTA LLMs from the last five years, including ChatGPT,\nDeepSeek, LLaMA, and others, to assess their adherence to transparency\nstandards and the implications of partial openness. Specifically, we examine\ntransparency and accessibility from two perspectives: open-source vs.\nopen-weight models. Our findings reveal that while some models are labeled as\nopen-source, this does not necessarily mean they are fully open-sourced. Even\nin the best cases, open-source models often do not report model training data,\nand code as well as key metrics, such as weight accessibility, and carbon\nemissions. To the best of our knowledge, this is the first study that\nsystematically examines the transparency and accessibility of over 100\ndifferent SoTA LLMs through the dual lens of open-source and open-weight\nmodels. The findings open avenues for further research and call for responsible\nand sustainable AI practices to ensure greater transparency, accountability,\nand ethical deployment of these models.(DeepSeek transparency, ChatGPT\naccessibility, open source, DeepSeek open source)", "published": "2025-02-21 23:53:13", "link": "http://arxiv.org/abs/2502.18505v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "CSTRL: Context-Driven Sequential Transfer Learning for Abstractive\n  Radiology Report Summarization", "abstract": "A radiology report comprises several sections, including the Findings and\nImpression of the diagnosis. Automatically generating the Impression from the\nFindings is crucial for reducing radiologists' workload and improving\ndiagnostic accuracy. Pretrained models that excel in common abstractive\nsummarization problems encounter challenges when applied to specialized medical\ndomains largely due to the complex terminology and the necessity for accurate\nclinical context. Such tasks in medical domains demand extracting core\ninformation, avoiding context shifts, and maintaining proper flow. Misuse of\nmedical terms can lead to drastic clinical errors. To address these issues, we\nintroduce a sequential transfer learning that ensures key content extraction\nand coherent summarization. Sequential transfer learning often faces challenges\nlike initial parameter decay and knowledge loss, which we resolve with the\nFisher matrix regularization. Using MIMIC-CXR and Open-I datasets, our model,\nCSTRL-Context-driven Sequential TRansfer Learning-achieved state-of-the-art\nperformance, showing 56.2% improvement in BLEU-1, 40.5% in BLEU-2, 84.3% in\nBLEU-3, 28.9% in ROUGE-1, 41.0% in ROUGE-2 and 26.5% in ROGUE-3 score over\nbenchmark studies. We also analyze factual consistency scores while preserving\nthe medical context. Our code is publicly available at TBA.", "published": "2025-02-21 08:32:11", "link": "http://arxiv.org/abs/2503.05750v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "On Synthesizing Data for Context Attribution in Question Answering", "abstract": "Question Answering (QA) accounts for a significant portion of LLM usage \"in\nthe wild\". However, LLMs sometimes produce false or misleading responses, also\nknown as \"hallucinations\". Therefore, grounding the generated answers in\ncontextually provided information -- i.e., providing evidence for the generated\ntext -- is paramount for LLMs' trustworthiness. Providing this information is\nthe task of context attribution. In this paper, we systematically study\nLLM-based approaches for this task, namely we investigate (i) zero-shot\ninference, (ii) LLM ensembling, and (iii) fine-tuning of small LMs on synthetic\ndata generated by larger LLMs. Our key contribution is SynQA: a novel\ngenerative strategy for synthesizing context attribution data. Given selected\ncontext sentences, an LLM generates QA pairs that are supported by these\nsentences. This leverages LLMs' natural strengths in text generation while\nensuring clear attribution paths in the synthetic training data. We show that\nthe attribution data synthesized via SynQA is highly effective for fine-tuning\nsmall LMs for context attribution in different QA tasks and domains. Finally,\nwith a user study, we validate the usefulness of small LMs (fine-tuned on\nsynthetic data from SynQA) in context attribution for QA.", "published": "2025-02-21 09:43:18", "link": "http://arxiv.org/abs/2504.05317v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "LightThinker: Thinking Step-by-Step Compression", "abstract": "Large language models (LLMs) have shown remarkable performance in complex\nreasoning tasks, but their efficiency is hindered by the substantial memory and\ncomputational costs associated with generating lengthy tokens. In this paper,\nwe propose LightThinker, a novel method that enables LLMs to dynamically\ncompress intermediate thoughts during reasoning. Inspired by human cognitive\nprocesses, LightThinker compresses verbose thought steps into compact\nrepresentations and discards the original reasoning chains, thereby\nsignificantly reducing the number of tokens stored in the context window. This\nis achieved by training the model on when and how to perform compression\nthrough data construction, mapping hidden states to condensed gist tokens, and\ncreating specialized attention masks. Additionally, we introduce the Dependency\n(Dep) metric to quantify the degree of compression by measuring the reliance on\nhistorical tokens during generation. Extensive experiments on four datasets and\ntwo models show that LightThinker reduces peak memory usage and inference time,\nwhile maintaining competitive accuracy. Our work provides a new direction for\nimproving the efficiency of LLMs in complex reasoning tasks without sacrificing\nperformance. Code will be released at https://github.com/zjunlp/LightThinker.", "published": "2025-02-21 16:57:22", "link": "http://arxiv.org/abs/2502.15589v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Protein Large Language Models: A Comprehensive Survey", "abstract": "Protein-specific large language models (Protein LLMs) are revolutionizing\nprotein science by enabling more efficient protein structure prediction,\nfunction annotation, and design. While existing surveys focus on specific\naspects or applications, this work provides the first comprehensive overview of\nProtein LLMs, covering their architectures, training datasets, evaluation\nmetrics, and diverse applications. Through a systematic analysis of over 100\narticles, we propose a structured taxonomy of state-of-the-art Protein LLMs,\nanalyze how they leverage large-scale protein sequence data for improved\naccuracy, and explore their potential in advancing protein engineering and\nbiomedical research. Additionally, we discuss key challenges and future\ndirections, positioning Protein LLMs as essential tools for scientific\ndiscovery in protein science. Resources are maintained at\nhttps://github.com/Yijia-Xiao/Protein-LLM-Survey.", "published": "2025-02-21 19:22:10", "link": "http://arxiv.org/abs/2502.17504v2", "categories": ["q-bio.BM", "cs.AI", "cs.CE", "cs.CL", "cs.LG"], "primary_category": "q-bio.BM"}
{"title": "Improving Streaming Speech Recognition With Time-Shifted Contextual\n  Attention And Dynamic Right Context Masking", "abstract": "Chunk-based inference stands out as a popular approach in developing\nreal-time streaming speech recognition, valued for its simplicity and\nefficiency. However, because it restricts the model's focus to only the history\nand current chunk context, it may result in performance degradation in\nscenarios that demand consideration of future context. Addressing this, we\npropose a novel approach featuring Time-Shifted Contextual Attention (TSCA) and\nDynamic Right Context (DRC) masking. Our method shows a relative word error\nrate reduction of 10 to 13.9% on the Librispeech dataset with the inclusion of\nin-context future information provided by TSCA. Moreover, we present a\nstreaming automatic speech recognition pipeline that facilitates the\nintegration of TSCA with minimal user-perceived latency, while also enabling\nbatch processing capability, making it practical for various applications.", "published": "2025-02-21 02:38:28", "link": "http://arxiv.org/abs/2502.15158v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Enhancing Speech Large Language Models with Prompt-Aware Mixture of\n  Audio Encoders", "abstract": "Connecting audio encoders with large language models (LLMs) allows the LLM to\nperform various audio understanding tasks, such as automatic speech recognition\n(ASR) and audio captioning (AC). Most research focuses on training an adapter\nlayer to generate a unified audio feature for the LLM. However, different tasks\nmay require distinct features that emphasize either semantic or acoustic\naspects, making task-specific audio features more desirable. In this paper, we\npropose Prompt-aware Mixture (PaM) to enhance the Speech LLM that uses multiple\naudio encoders. Our approach involves using different experts to extract\ndifferent features based on the prompt that indicates different tasks.\nExperiments demonstrate that with PaM, only one Speech LLM surpasses the best\nperformances achieved by all single-encoder Speech LLMs on ASR, Speaker Number\nVerification, and AC tasks. PaM also outperforms other feature fusion\nbaselines, such as concatenation and averaging.", "published": "2025-02-21 03:16:23", "link": "http://arxiv.org/abs/2502.15178v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Advancing User-Voice Interaction: Exploring Emotion-Aware Voice\n  Assistants Through a Role-Swapping Approach", "abstract": "As voice assistants (VAs) become increasingly integrated into daily life, the\nneed for emotion-aware systems that can recognize and respond appropriately to\nuser emotions has grown. While significant progress has been made in speech\nemotion recognition (SER) and sentiment analysis, effectively addressing user\nemotions-particularly negative ones-remains a challenge. This study explores\nhuman emotional response strategies in VA interactions using a role-swapping\napproach, where participants regulate AI emotions rather than receiving\npre-programmed responses. Through speech feature analysis and natural language\nprocessing (NLP), we examined acoustic and linguistic patterns across various\nemotional scenarios. Results show that participants favor neutral or positive\nemotional responses when engaging with negative emotional cues, highlighting a\nnatural tendency toward emotional regulation and de-escalation. Key acoustic\nindicators such as root mean square (RMS), zero-crossing rate (ZCR), and jitter\nwere identified as sensitive to emotional states, while sentiment polarity and\nlexical diversity (TTR) distinguished between positive and negative responses.\nThese findings provide valuable insights for developing adaptive, context-aware\nVAs capable of delivering empathetic, culturally sensitive, and user-aligned\nresponses. By understanding how humans naturally regulate emotions in AI\ninteractions, this research contributes to the design of more intuitive and\nemotionally intelligent voice assistants, enhancing user trust and engagement\nin human-AI interactions.", "published": "2025-02-21 10:33:44", "link": "http://arxiv.org/abs/2502.15367v1", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Audio signal interpolation using optimal transportation of spectrograms", "abstract": "We present a novel approach for generating an artificial audio signal that\ninterpolates between given source and target sounds. Our approach relies on the\ncomputation of Wasserstein barycenters of the source and target spectrograms,\nfollowed by phase reconstruction and inversion. In contrast with previous\nworks, our new method considers the spectrograms globally and does not operate\non a temporal frame-to-frame basis. An other contribution is to endow the\ntransportation cost matrix with a specific structure that prohibits remote\ndisplacements of energy along the time axis, and for which optimal transport is\nmade possible by leveraging the unbalanced transport framework. The proposed\ncost matrix makes sense from the audio perspective and also allows to reduce\nthe computation load. Results with synthetic musical notes and real\nenvironmental sounds illustrate the potential of our novel approach.", "published": "2025-02-21 12:57:50", "link": "http://arxiv.org/abs/2502.15430v1", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "KAD: No More FAD! An Effective and Efficient Evaluation Metric for Audio\n  Generation", "abstract": "Although being widely adopted for evaluating generated audio signals, the\nFr\\'echet Audio Distance (FAD) suffers from significant limitations, including\nreliance on Gaussian assumptions, sensitivity to sample size, and high\ncomputational complexity. As an alternative, we introduce the Kernel Audio\nDistance (KAD), a novel, distribution-free, unbiased, and computationally\nefficient metric based on Maximum Mean Discrepancy (MMD). Through analysis and\nempirical validation, we demonstrate KAD's advantages: (1) faster convergence\nwith smaller sample sizes, enabling reliable evaluation with limited data; (2)\nlower computational cost, with scalable GPU acceleration; and (3) stronger\nalignment with human perceptual judgments. By leveraging advanced embeddings\nand characteristic kernels, KAD captures nuanced differences between real and\ngenerated audio. Open-sourced in the kadtk toolkit, KAD provides an efficient,\nreliable, and perceptually aligned benchmark for evaluating generative audio\nmodels.", "published": "2025-02-21 17:19:15", "link": "http://arxiv.org/abs/2502.15602v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Benchmarking machine learning for bowel sound pattern classification\n  from tabular features to pretrained models", "abstract": "The development of electronic stethoscopes and wearable recording sensors\nopened the door to the automated analysis of bowel sound (BS) signals. This\nenables a data-driven analysis of bowel sound patterns, their interrelations,\nand their correlation to different pathologies. This work leverages a BS\ndataset collected from 16 healthy subjects that was annotated according to four\nestablished BS patterns. This dataset is used to evaluate the performance of\nmachine learning models to detect and/or classify BS patterns. The selection of\nconsidered models covers models using tabular features, convolutional neural\nnetworks based on spectrograms and models pre-trained on large audio datasets.\nThe results highlight the clear superiority of pre-trained models, particularly\nin detecting classes with few samples, achieving an AUC of 0.89 in\ndistinguishing BS from non-BS using a HuBERT model and an AUC of 0.89 in\ndifferentiating bowel sound patterns using a Wav2Vec 2.0 model. These results\npave the way for an improved understanding of bowel sounds in general and\nfuture machine-learning-driven diagnostic applications for gastrointestinal\nexaminations", "published": "2025-02-21 17:22:48", "link": "http://arxiv.org/abs/2502.15607v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP", "cs.SD (Primary) cs.LG, eess.AS, eess.SP (Secondary)"], "primary_category": "cs.SD"}
{"title": "Deriving Representative Structure from Music Corpora", "abstract": "Western music is an innately hierarchical system of interacting levels of\nstructure, from fine-grained melody to high-level form. In order to analyze\nmusic compositions holistically and at multiple granularities, we propose a\nunified, hierarchical meta-representation of musical structure called the\nstructural temporal graph (STG). For a single piece, the STG is a data\nstructure that defines a hierarchy of progressively finer structural musical\nfeatures and the temporal relationships between them. We use the STG to enable\na novel approach for deriving a representative structural summary of a music\ncorpus, which we formalize as a dually NP-hard combinatorial optimization\nproblem extending the Generalized Median Graph problem. Our approach first\napplies simulated annealing to develop a measure of structural distance between\ntwo music pieces rooted in graph isomorphism. Our approach then combines the\nformal guarantees of SMT solvers with nested simulated annealing over\nstructural distances to produce a structurally sound, representative centroid\nSTG for an entire corpus of STGs from individual pieces. To evaluate our\napproach, we conduct experiments verifying that structural distance accurately\ndifferentiates between music pieces, and that derived centroids accurately\nstructurally characterize their corpora.", "published": "2025-02-21 02:32:29", "link": "http://arxiv.org/abs/2502.15849v2", "categories": ["cs.SD", "cs.AI", "cs.LO", "eess.AS", "G.1.6; I.2.4; J.5; G.2.2"], "primary_category": "cs.SD"}
{"title": "Offload Rethinking by Cloud Assistance for Efficient Environmental Sound\n  Recognition on LPWANs", "abstract": "Learning-based environmental sound recognition has emerged as a crucial\nmethod for ultra-low-power environmental monitoring in biological research and\ncity-scale sensing systems. These systems usually operate under limited\nresources and are often powered by harvested energy in remote areas. Recent\nefforts in on-device sound recognition suffer from low accuracy due to resource\nconstraints, whereas cloud offloading strategies are hindered by high\ncommunication costs. In this work, we introduce ORCA, a novel\nresource-efficient cloud-assisted environmental sound recognition system on\nbatteryless devices operating over the Low-Power Wide-Area Networks (LPWANs),\ntargeting wide-area audio sensing applications. We propose a cloud assistance\nstrategy that remedies the low accuracy of on-device inference while minimizing\nthe communication costs for cloud offloading. By leveraging a\nself-attention-based cloud sub-spectral feature selection method to facilitate\nefficient on-device inference, ORCA resolves three key challenges for\nresource-constrained cloud offloading over LPWANs: 1) high communication costs\nand low data rates, 2) dynamic wireless channel conditions, and 3) unreliable\noffloading. We implement ORCA on an energy-harvesting batteryless\nmicrocontroller and evaluate it in a real world urban sound testbed. Our\nresults show that ORCA outperforms state-of-the-art methods by up to $80\n\\times$ in energy savings and $220 \\times$ in latency reduction while\nmaintaining comparable accuracy.", "published": "2025-02-21 08:23:32", "link": "http://arxiv.org/abs/2502.15285v3", "categories": ["cs.SD", "cs.AI", "cs.DC", "cs.NI", "eess.AS"], "primary_category": "cs.SD"}
