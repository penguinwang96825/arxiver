{"title": "Transfer Learning in Multilingual Neural Machine Translation with\n  Dynamic Vocabulary", "abstract": "We propose a method to transfer knowledge across neural machine translation\n(NMT) models by means of a shared dynamic vocabulary. Our approach allows to\nextend an initial model for a given language pair to cover new languages by\nadapting its vocabulary as long as new data become available (i.e., introducing\nnew vocabulary items if they are not included in the initial model). The\nparameter transfer mechanism is evaluated in two scenarios: i) to adapt a\ntrained single language NMT system to work with a new language pair and ii) to\ncontinuously add new language pairs to grow to a multilingual NMT system. In\nboth the scenarios our goal is to improve the translation performance, while\nminimizing the training convergence time. Preliminary experiments spanning five\nlanguages with different training data sizes (i.e., 5k and 50k parallel\nsentences) show a significant performance gain ranging from +3.85 up to +13.63\nBLEU in different language directions. Moreover, when compared with training an\nNMT model from scratch, our transfer-learning approach allows us to reach\nhigher performance after training up to 4% of the total training steps.", "published": "2018-11-03 00:38:45", "link": "http://arxiv.org/abs/1811.01137v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying and Controlling Important Neurons in Neural Machine\n  Translation", "abstract": "Neural machine translation (NMT) models learn representations containing\nsubstantial linguistic information. However, it is not clear if such\ninformation is fully distributed or if some of it can be attributed to\nindividual neurons. We develop unsupervised methods for discovering important\nneurons in NMT models. Our methods rely on the intuition that different models\nlearn similar properties, and do not require any costly external supervision.\nWe show experimentally that translation quality depends on the discovered\nneurons, and find that many of them capture common linguistic phenomena.\nFinally, we show how to control NMT translations in predictable ways, by\nmodifying activations of individual neurons.", "published": "2018-11-03 04:22:52", "link": "http://arxiv.org/abs/1811.01157v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Wizard of Wikipedia: Knowledge-Powered Conversational agents", "abstract": "In open-domain dialogue intelligent agents should exhibit the use of\nknowledge, however there are few convincing demonstrations of this to date. The\nmost popular sequence to sequence models typically \"generate and hope\" generic\nutterances that can be memorized in the weights of the model when mapping from\ninput utterance(s) to output, rather than employing recalled knowledge as\ncontext. Use of knowledge has so far proved difficult, in part because of the\nlack of a supervised learning benchmark task which exhibits knowledgeable open\ndialogue with clear grounding. To that end we collect and release a large\ndataset with conversations directly grounded with knowledge retrieved from\nWikipedia. We then design architectures capable of retrieving knowledge,\nreading and conditioning on it, and finally generating natural responses. Our\nbest performing dialogue models are able to conduct knowledgeable discussions\non open-domain topics as evaluated by automatic metrics and human evaluations,\nwhile our new benchmark allows for measuring further improvements in this\nimportant research direction.", "published": "2018-11-03 16:11:29", "link": "http://arxiv.org/abs/1811.01241v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Challenges in detecting evolutionary forces in language change using\n  diachronic corpora", "abstract": "Newberry et al. (Detecting evolutionary forces in language change, Nature\n551, 2017) tackle an important but difficult problem in linguistics, the\ntesting of selective theories of language change against a null model of drift.\nHaving applied a test from population genetics (the Frequency Increment Test)\nto a number of relevant examples, they suggest stochasticity has a previously\nunder-appreciated role in language evolution. We replicate their results and\nfind that while the overall observation holds, results produced by this\napproach on individual time series can be sensitive to how the corpus is\norganized into temporal segments (binning). Furthermore, we use a large set of\nsimulations in conjunction with binning to systematically explore the range of\napplicability of the Frequency Increment Test. We conclude that care should be\nexercised with interpreting results of tests like the Frequency Increment Test\non individual series, given the researcher degrees of freedom available when\napplying the test to corpus data, and fundamental differences between genetic\nand linguistic data. Our findings have implications for selection testing and\ntemporal binning in general, as well as demonstrating the usefulness of\nsimulations for evaluating methods newly introduced to the field.", "published": "2018-11-03 20:02:17", "link": "http://arxiv.org/abs/1811.01275v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Identification of Study Descriptors in Toxicology Research:\n  An Experimental Study", "abstract": "Identifying and extracting data elements such as study descriptors in\npublication full texts is a critical yet manual and labor-intensive step\nrequired in a number of tasks. In this paper we address the question of\nidentifying data elements in an unsupervised manner. Specifically, provided a\nset of criteria describing specific study parameters, such as species, route of\nadministration, and dosing regimen, we develop an unsupervised approach to\nidentify text segments (sentences) relevant to the criteria. A binary\nclassifier trained to identify publications that met the criteria performs\nbetter when trained on the candidate sentences than when trained on sentences\nrandomly picked from the text, supporting the intuition that our method is able\nto accurately identify study descriptors.", "published": "2018-11-03 09:29:36", "link": "http://arxiv.org/abs/1811.01183v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Learning Contextual Hierarchical Structure of Medical Concepts with\n  Poincair\u00e9 Embeddings to Clarify Phenotypes", "abstract": "Biomedical association studies are increasingly done using clinical concepts,\nand in particular diagnostic codes from clinical data repositories as\nphenotypes. Clinical concepts can be represented in a meaningful, vector space\nusing word embedding models. These embeddings allow for comparison between\nclinical concepts or for straightforward input to machine learning models.\nUsing traditional approaches, good representations require high dimensionality,\nmaking downstream tasks such as visualization more difficult. We applied\nPoincar\\'e embeddings in a 2-dimensional hyperbolic space to a large-scale\nadministrative claims database and show performance comparable to\n100-dimensional embeddings in a euclidean space. We then examine disease\nrelationships under different disease contexts to better understand potential\nphenotypes.", "published": "2018-11-03 22:47:59", "link": "http://arxiv.org/abs/1811.01294v1", "categories": ["q-bio.QM", "cs.CL"], "primary_category": "q-bio.QM"}
{"title": "Content preserving text generation with attribute controls", "abstract": "In this work, we address the problem of modifying textual attributes of\nsentences. Given an input sentence and a set of attribute labels, we attempt to\ngenerate sentences that are compatible with the conditioning information. To\nensure that the model generates content compatible sentences, we introduce a\nreconstruction loss which interpolates between auto-encoding and\nback-translation loss components. We propose an adversarial loss to enforce\ngenerated samples to be attribute compatible and realistic. Through\nquantitative, qualitative and human evaluations we demonstrate that our model\nis capable of generating fluent sentences that better reflect the conditioning\ninformation compared to prior methods. We further demonstrate that the model is\ncapable of simultaneously controlling multiple attributes.", "published": "2018-11-03 00:29:41", "link": "http://arxiv.org/abs/1811.01135v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Margin-based Parallel Corpus Mining with Multilingual Sentence\n  Embeddings", "abstract": "Machine translation is highly sensitive to the size and quality of the\ntraining data, which has led to an increasing interest in collecting and\nfiltering large parallel corpora. In this paper, we propose a new method for\nthis task based on multilingual sentence embeddings. In contrast to previous\napproaches, which rely on nearest neighbor retrieval with a hard threshold over\ncosine similarity, our proposed method accounts for the scale inconsistencies\nof this measure, considering the margin between a given sentence pair and its\nclosest candidates instead. Our experiments show large improvements over\nexisting methods. We outperform the best published results on the BUCC mining\ntask and the UN reconstruction task by more than 10 F1 and 30 precision points,\nrespectively. Filtering the English-German ParaCrawl corpus with our approach,\nwe obtain 31.2 BLEU points on newstest2014, an improvement of more than one\npoint over the best official filtered version.", "published": "2018-11-03 00:34:05", "link": "http://arxiv.org/abs/1811.01136v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Relation Mention Extraction from Noisy Data with Hierarchical\n  Reinforcement Learning", "abstract": "In this paper we address a task of relation mention extraction from noisy\ndata: extracting representative phrases for a particular relation from noisy\nsentences that are collected via distant supervision. Despite its significance\nand value in many downstream applications, this task is less studied on noisy\ndata. The major challenges exists in 1) the lack of annotation on mention\nphrases, and more severely, 2) handling noisy sentences which do not express a\nrelation at all. To address the two challenges, we formulate the task as a\nsemi-Markov decision process and propose a novel hierarchical reinforcement\nlearning model. Our model consists of a top-level sentence selector to remove\nnoisy sentences, a low-level mention extractor to extract relation mentions,\nand a reward estimator to provide signals to guide data denoising and mention\nextraction without explicit annotations. Experimental results show that our\nmodel is effective to extract relation mentions from noisy data.", "published": "2018-11-03 15:50:27", "link": "http://arxiv.org/abs/1811.01237v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SimplerVoice: A Key Message & Visual Description Generator System for\n  Illiteracy", "abstract": "We introduce SimplerVoice: a key message and visual description generator\nsystem to help low-literate adults navigate the information-dense world with\nconfidence, on their own. SimplerVoice can automatically generate sensible\nsentences describing an unknown object, extract semantic meanings of the object\nusage in the form of a query string, then, represent the string as multiple\ntypes of visual guidance (pictures, pictographs, etc.). We demonstrate\nSimplerVoice system in a case study of generating grocery products' manuals\nthrough a mobile application. To evaluate, we conducted a user study on\nSimplerVoice's generated description in comparison to the information\ninterpreted by users from other methods: the original product package and\nsearch engines' top result, in which SimplerVoice achieved the highest\nperformance score: 4.82 on 5-point mean opinion score scale. Our result shows\nthat SimplerVoice is able to provide low-literate end-users with simple yet\ninformative components to help them understand how to use the grocery products,\nand that the system may potentially provide benefits in other real-world use\ncases", "published": "2018-11-03 23:43:38", "link": "http://arxiv.org/abs/1811.01299v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "DAPPER: Scaling Dynamic Author Persona Topic Model to Billion Word\n  Corpora", "abstract": "Extracting common narratives from multi-author dynamic text corpora requires\ncomplex models, such as the Dynamic Author Persona (DAP) topic model. However,\nsuch models are complex and can struggle to scale to large corpora, often\nbecause of challenging non-conjugate terms. To overcome such challenges, in\nthis paper we adapt new ideas in approximate inference to the DAP model,\nresulting in the DAP Performed Exceedingly Rapidly (DAPPER) topic model.\nSpecifically, we develop Conjugate-Computation Variational Inference (CVI)\nbased variational Expectation-Maximization (EM) for learning the model,\nyielding fast, closed form updates for each document, replacing iterative\noptimization in earlier work. Our results show significant improvements in\nmodel fit and training time without needing to compromise the model's temporal\nstructure or the application of Regularized Variation Inference (RVI). We\ndemonstrate the scalability and effectiveness of the DAPPER model by extracting\nhealth journeys from the CaringBridge corpus --- a collection of 9 million\njournals written by 200,000 authors during health crises.", "published": "2018-11-03 21:27:56", "link": "http://arxiv.org/abs/1811.01931v1", "categories": ["stat.ML", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "stat.ML"}
{"title": "A Robust Target Linearly Constrained Minimum Variance Beamformer With\n  Spatial Cues Preservation for Binaural Hearing Aids", "abstract": "In this paper, a binaural beamforming algorithm for hearing aid applications\nis introduced.The beamforming algorithm is designed to be robust to some error\nin the estimate of the target speaker direction. The algorithm has two main\ncomponents: a robust target linearly constrained minimum variance (TLCMV)\nalgorithm based on imposing two constraints around the estimated direction of\nthe target signal, and a post-processor to help with the preservation of\nbinaural cues. The robust TLCMV provides a good level of noise reduction and\nlow level of target distortion under realistic conditions. The post-processor\nenhances the beamformer abilities to preserve the binaural cues for both\ndiffuse-like background noise and directional interferers (competing speakers),\nwhile keeping a good level of noise reduction. The introduced algorithm does\nnot require knowledge or estimation of the directional interferers' directions\nnor the second-order statistics of noise-only components. The introduced\nalgorithm requires an estimate of the target speaker direction, but it is\ndesigned to be robust to some deviation from the estimated direction. Compared\nwith recently proposed state-of-the-art methods, comprehensive evaluations are\nperformed under complex realistic acoustic scenarios generated in both anechoic\nand mildly reverberant environments, considering a mismatch between estimated\nand true sources direction of arrival. Mismatch between the anechoic\npropagation models used for the design of the beamformers and the mildly\nreverberant propagation models used to generate the simulated directional\nsignals is also considered. The results illustrate the robustness of the\nproposed algorithm to such mismatches.", "published": "2018-11-03 00:17:22", "link": "http://arxiv.org/abs/1811.01133v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multitask learning for frame-level instrument recognition", "abstract": "For many music analysis problems, we need to know the presence of instruments\nfor each time frame in a multi-instrument musical piece. However, such a\nframe-level instrument recognition task remains difficult, mainly due to the\nlack of labeled datasets. To address this issue, we present in this paper a\nlarge-scale dataset that contains synthetic polyphonic music with frame-level\npitch and instrument labels. Moreover, we propose a simple yet novel network\narchitecture to jointly predict the pitch and instrument for each frame. With\nthis multitask learning method, the pitch information can be leveraged to\npredict the instruments, and also the other way around. And, by using the\nso-called pianoroll representation of music as the main target output of the\nmodel, our model also predicts the instruments that play each individual note\nevent. We validate the effectiveness of the proposed method for framelevel\ninstrument recognition by comparing it with its singletask ablated versions and\nthree state-of-the-art methods. We also demonstrate the result of the proposed\nmethod for multipitch streaming with real-world music. For reproducibility, we\nwill share the code to crawl the data and to implement the proposed model at:\nhttps://github.com/biboamy/ instrument-streaming.", "published": "2018-11-03 02:34:52", "link": "http://arxiv.org/abs/1811.01143v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Time-Frequency Audio Features for Speech-Music Classification", "abstract": "Distinct striation patterns are observed in the spectrograms of speech and\nmusic. This motivated us to propose three novel time-frequency features for\nspeech-music classification. These features are extracted in two stages. First,\na preset number of prominent spectral peak locations are identified from the\nspectra of each frame. These important peak locations obtained from each frame\nare used to form Spectral peak sequences (SPS) for an audio interval. In second\nstage, these SPS are treated as time series data of frequency locations. The\nproposed features are extracted as periodicity, average frequency and\nstatistical attributes of these spectral peak sequences. Speech-music\ncategorization is performed by learning binary classifiers on these features.\nWe have experimented with Gaussian mixture models, support vector machine and\nrandom forest classifiers. Our proposal is validated on four datasets and\nbenchmarked against three baseline approaches. Experimental results establish\nthe validity of our proposal.", "published": "2018-11-03 14:07:41", "link": "http://arxiv.org/abs/1811.01222v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deep Ad-hoc Beamforming", "abstract": "Far-field speech processing is an important and challenging problem. In this\npaper, we propose \\textit{deep ad-hoc beamforming}, a deep-learning-based\nmultichannel speech enhancement framework based on ad-hoc microphone arrays, to\naddress the problem. It contains three novel components. First, it combines\n\\textit{ad-hoc microphone arrays} with deep-learning-based multichannel speech\nenhancement, which reduces the probability of the occurrence of far-field\nacoustic environments significantly. Second, it groups the microphones around\nthe speech source to a local microphone array by a supervised channel selection\nframework based on deep neural networks. Third, it develops a simple time\nsynchronization framework to synchronize the channels that have different time\ndelay. Besides the above novelties and advantages, the proposed model is also\ntrained in a single-channel fashion, so that it can easily employ new\ndevelopment of speech processing techniques. Its test stage is also flexible in\nincorporating any number of microphones without retraining or modifying the\nframework. We have developed many implementations of the proposed framework and\nconducted an extensive experiment in scenarios where the locations of the\nspeech sources are far-field, random, and blind to the microphones. Results on\nspeech enhancement tasks show that our method outperforms its counterpart that\nworks with linear microphone arrays by a considerable margin in both diffuse\nnoise reverberant environments and point source noise reverberant environments.\nWe have also tested the framework with different handcrafted features. Results\nshow that although designing good features lead to high performance, they do\nnot affect the conclusion on the effectiveness of the proposed framework.", "published": "2018-11-03 15:31:24", "link": "http://arxiv.org/abs/1811.01233v7", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-View Networks For Multi-Channel Audio Classification", "abstract": "In this paper we introduce the idea of multi-view networks for sound\nclassification with multiple sensors. We show how one can build a multi-channel\nsound recognition model trained on a fixed number of channels, and deploy it to\nscenarios with arbitrary (and potentially dynamically changing) number of input\nchannels and not observe degradation in performance. We demonstrate that at\ninference time you can safely provide this model all available channels as it\ncan ignore noisy information and leverage new information better than standard\nbaseline approaches. The model is evaluated in both an anechoic environment and\nin rooms generated by a room acoustics simulator. We demonstrate that this\nmodel can generalize to unseen numbers of channels as well as unseen room\ngeometries.", "published": "2018-11-03 17:22:07", "link": "http://arxiv.org/abs/1811.01251v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Nonparallel Emotional Speech Conversion", "abstract": "We propose a nonparallel data-driven emotional speech conversion method. It\nenables the transfer of emotion-related characteristics of a speech signal\nwhile preserving the speaker's identity and linguistic content. Most existing\napproaches require parallel data and time alignment, which is not available in\nmost real applications. We achieve nonparallel training based on an\nunsupervised style transfer technique, which learns a translation model between\ntwo distributions instead of a deterministic one-to-one mapping between paired\nexamples. The conversion model consists of an encoder and a decoder for each\nemotion domain. We assume that the speech signal can be decomposed into an\nemotion-invariant content code and an emotion-related style code in latent\nspace. Emotion conversion is performed by extracting and recombining the\ncontent code of the source speech and the style code of the target emotion. We\ntested our method on a nonparallel corpora with four emotions. Both subjective\nand objective evaluations show the effectiveness of our approach.", "published": "2018-11-03 08:28:04", "link": "http://arxiv.org/abs/1811.01174v3", "categories": ["cs.LG", "eess.AS", "stat.ML", "68T50", "I.2.7"], "primary_category": "cs.LG"}
