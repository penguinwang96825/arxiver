{"title": "#P is Sandwiched by One and Two #2DNF Calls: Is Subtraction Stronger Than We Thought?", "abstract": "The canonical class in the realm of counting complexity is #P. It is well\nknown that the problem of counting the models of a propositional formula in\ndisjunctive normal form (#DNF) is complete for #P under Turing reductions. On\nthe other hand, #DNF $\\in$ spanL and spanL $\\not\\subseteq$ #P unless NL = NP.\nHence, the class of functions logspace-reducible to #DNF is a strict subset of\n#P under plausible complexity-theoretic assumptions. By contrast, we show that\ntwo calls to a (restricted) #2DNF oracle suffice to capture gapP, namely, that\nthe logspace many-one closure of the subtraction between the results of two\n#2DNF calls is gapP. Because #P $\\not\\subseteq$ gapP, #P is strictly contained\nbetween one and two #2DNF oracle calls.\n  Surprisingly, the propositional formulas needed in both calls are linear-time\ncomputable, and the reduction preserves interesting structural as well as\nsymmetry properties, leading to algorithmic applications. We show that a single\nsubtraction suffices to compensate for the absence of negation while still\ncapturing gapP, i.e., our results carry over to the monotone fragments of #2SAT\nand #2DNF. Since our reduction is linear-time, it preserves sparsity and, as a\nconsequence we obtain a sparsification lemma for both #2SAT and #2DNF. This has\nonly been known for kSAT with k $\\geq$ 3 and respective counting versions. We\nfurther show that both #2DNF calls can be combined into a single call if we\nallow a little postprocessing (computable by AC0- or TC0-circuits).\nConsequently, we derive refined versions of Toda's Theorem: PH $\\subseteq$\n[#MON2SAT]$^{log}_{TC0}$ = [#MON2DNF]$^{log}_{TC0}$ and PH $\\subseteq$\n[#IMPL2SAT]$^{log}_{AC0}$. Our route to these results is via structure-aware\nreductions that preserve parameters like treewidth up to an additive overhead.\nThe absence of multiplicative overhead indeed yields parameterized SETH-tight\nlower bounds.", "published": "2025-06-07 08:53:38", "link": "http://arxiv.org/abs/2506.06716v1", "categories": ["cs.CC", "cs.DM", "cs.DS", "cs.LO", "math.CO", "68Q17, 68Q15, 68Q25, 68Q27, 03B05, 03D10, 03D15, 05C85, 05C62,\n  05-08, 94C15", "G.2.1; G.2.2; F.4.1; F.1.3; F.1.1"], "primary_category": "cs.CC"}
{"title": "OneSug: The Unified End-to-End Generative Framework for E-commerce Query Suggestion", "abstract": "Query suggestion plays a crucial role in enhancing user experience in\ne-commerce search systems by providing relevant query recommendations that\nalign with users' initial input. This module helps users navigate towards\npersonalized preference needs and reduces typing effort, thereby improving\nsearch experience. Traditional query suggestion modules usually adopt\nmulti-stage cascading architectures, for making a well trade-off between system\nresponse time and business conversion. But they often suffer from\ninefficiencies and suboptimal performance due to inconsistent optimization\nobjectives across stages. To address these, we propose OneSug, the first\nend-to-end generative framework for e-commerce query suggestion. OneSug\nincorporates a prefix2query representation enhancement module to enrich\nprefixes using semantically and interactively related queries to bridge content\nand business characteristics, an encoder-decoder generative model that unifies\nthe query suggestion process, and a reward-weighted ranking strategy with\nbehavior-level weights to capture fine-grained user preferences. Extensive\nevaluations on large-scale industry datasets demonstrate OneSug's ability for\neffective and efficient query suggestion. Furthermore, OneSug has been\nsuccessfully deployed for the entire traffic on the e-commerce search engine in\nKuaishou platform for over 1 month, with statistically significant improvements\nin user top click position (-9.33%), CTR (+2.01%), Order (+2.04%), and Revenue\n(+1.69%) over the online multi-stage strategy, showing great potential in\ne-commercial conversion.", "published": "2025-06-07 20:24:05", "link": "http://arxiv.org/abs/2506.06913v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "The State-of-the-Art in Lifelog Retrieval: A Review of Progress at the ACM Lifelog Search Challenge Workshop 2022-24", "abstract": "The ACM Lifelog Search Challenge (LSC) is a venue that welcomes and compares\nsystems that support the exploration of lifelog data, and in particular the\nretrieval of specific information, through an interactive competition format.\nThis paper reviews the recent advances in interactive lifelog retrieval as\ndemonstrated at the ACM LSC from 2022 to 2024. Through a detailed comparative\nanalysis, we highlight key improvements across three main retrieval tasks:\nknown-item search, question answering, and ad-hoc search. Our analysis\nidentifies trends such as the widespread adoption of embedding-based retrieval\nmethods (e.g., CLIP, BLIP), increased integration of large language models\n(LLMs) for conversational retrieval, and continued innovation in multimodal and\ncollaborative search interfaces. We further discuss how specific retrieval\ntechniques and user interface (UI) designs have impacted system performance,\nemphasizing the importance of balancing retrieval complexity with usability.\nOur findings indicate that embedding-driven approaches combined with LLMs show\npromise for lifelog retrieval systems. Likewise, improving UI design can\nenhance usability and efficiency. Additionally, we recommend reconsidering\nmulti-instance system evaluations within the expert track to better manage\nvariability in user familiarity and configuration effectiveness.", "published": "2025-06-07 10:19:37", "link": "http://arxiv.org/abs/2506.06743v1", "categories": ["cs.MM", "cs.IR"], "primary_category": "cs.MM"}
{"title": "Dynamic and Parametric Retrieval-Augmented Generation", "abstract": "Retrieval-Augmented Generation (RAG) has become a foundational paradigm for\nequipping large language models (LLMs) with external knowledge, playing a\ncritical role in information retrieval and knowledge-intensive applications.\nHowever, conventional RAG systems typically adopt a static\nretrieve-then-generate pipeline and rely on in-context knowledge injection,\nwhich can be suboptimal for complex tasks that require multihop reasoning,\nadaptive information access, and deeper integration of external knowledge.\nMotivated by these limitations, the research community has moved beyond static\nretrieval and in-context knowledge injection. Among the emerging directions,\nthis tutorial delves into two rapidly growing and complementary research areas\non RAG: Dynamic RAG and Parametric RAG. Dynamic RAG adaptively determines when\nand what to retrieve during the LLM's generation process, enabling real-time\nadaptation to the LLM's evolving information needs. Parametric RAG rethinks how\nretrieved knowledge should be injected into LLMs, transitioning from\ninput-level to parameter-level knowledge injection for enhanced efficiency and\neffectiveness. This tutorial offers a comprehensive overview of recent advances\nin these emerging research areas. It also shares theoretical foundations and\npractical insights to support and inspire further research in RAG.", "published": "2025-06-07 07:59:33", "link": "http://arxiv.org/abs/2506.06704v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Cross-Entropy Games for Language Models: From Implicit Knowledge to General Capability Measures", "abstract": "Large Language Models (LLMs) define probability measures on text. By\nconsidering the implicit knowledge question of what it means for an LLM to know\nsuch a measure and what it entails algorithmically, we are naturally led to\nformulate a series of tasks that go beyond generative sampling, involving forms\nof summarization, counterfactual thinking, anomaly detection, originality\nsearch, reverse prompting, debating, creative solving, etc. These tasks can be\nformulated as games based on LLM measures, which we call Cross-Entropy (Xent)\nGames. Xent Games can be single-player or multi-player. They involve\ncross-entropy scores and cross-entropy constraints, and can be expressed as\nsimple computational graphs and programs. We show the Xent Game space is large\nenough to contain a wealth of interesting examples, while being constructible\nfrom basic game-theoretic consistency axioms. We then discuss how the Xent Game\nspace can be used to measure the abilities of LLMs. This leads to the\nconstruction of Xent Game measures: finite families of Xent Games that can be\nused as capability benchmarks, built from a given scope, by extracting a\ncovering measure. To address the unbounded scope problem associated with the\nchallenge of measuring general abilities, we propose to explore the space of\nXent Games in a coherent fashion, using ideas inspired by evolutionary\ndynamics.", "published": "2025-06-07 15:25:10", "link": "http://arxiv.org/abs/2506.06832v1", "categories": ["cs.AI", "cs.CL", "cs.GT", "cs.IT", "cs.NE", "math.IT"], "primary_category": "cs.AI"}
{"title": "Polarized Element-pair Code Based FFMA over a Gaussian Multiple-access Channel", "abstract": "This paper presents polarized element-pair (EP) codes for\npolarization-adjusted finite-field multiple-access (PA-FFMA) systems. The core\ninnovation of FFMA systems lies in their unique processing order that exchanges\nthe conventional sequence of channel coding and multiplexing operations,\neffectively solving the multiuser finite-blocklength (FBL) problem while\nenhancing error performance. In this architecture, EPs serve as virtual\nresources for user separation, where different EP codes provide distinct error\nperformance characteristics. The proposed polarized EP code differs from\nclassical polar codes in one aspect that it is specifically designed for\nGaussian multiple access channel (GMAC) environments rather than single-user\nGaussian channels. We derive the channel capacity for this polarized EP code\nbased FFMA system, then develop an optimal power allocation scheme to maximize\nmultiuser channel capacity. The code construction employs the Marto Loco method\nfor selecting the polarized index set. For decoding, we introduce two\nspecialized algorithms. A successive cancellation list (SCL) decoder for the\nbalanced information-parity section scenarios, and a top $L$ bifurcated minimum\ndistance (Top$L$-BMD) decoder for small payload cases while maintaining\ncomparable error performance. Simulations show that, for $15$ users, our system\nachieves a $1.25$ dB coding gain compared to the state-of-the-art polar random\nspreading systems.", "published": "2025-06-07 13:36:41", "link": "http://arxiv.org/abs/2506.06796v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "MIMO Pinching-Antenna-Aided SWIPT", "abstract": "Pinching-antenna systems (PASS) have recently emerged as a promising\ntechnology for improving wireless communications by establishing or\nstrengthening reliable line-of-sight (LoS) links by adjusting the positions of\npinching antennas (PAs). Motivated by these benefits, we propose a novel\nPASS-aided multi-input multi-output (MIMO) system for simultaneous wireless\ninformation and power transfer (SWIPT), where the PASS are equipped with\nmultiple waveguides to provide information transmission and wireless power\ntransfer (WPT) for several multiple antenna information decoding receivers\n(IDRs), and energy harvesting receivers (EHRs), respectively. Based on the\nsystem, we consider maximizing the sum-rate of all IDRs while guaranteeing the\nminimum harvested energy of each EHR by jointly optimizing the pinching\nbeamforming and the PA positions. To solve this highly non-convex problem, we\niteratively optimize the pinching beamforming based on a weighted minimum\nmean-squared-error (WMMSE) method and update the PA positions with a\nGauss-Seidel-based approach in an alternating optimization (AO) framework.\nNumerical results verify the significant superiority of the PASS compared with\nconventional designs.", "published": "2025-06-07 10:51:04", "link": "http://arxiv.org/abs/2506.06754v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Statistical Limits for Finite-Rank Tensor Estimation", "abstract": "This paper provides a unified framework for analyzing tensor estimation\nproblems that allow for nonlinear observations, heteroskedastic noise, and\ncovariate information. We study a general class of high-dimensional models\nwhere each observation depends on the interactions among a finite number of\nunknown parameters. Our main results provide asymptotically exact formulas for\nthe mutual information (equivalently, the free energy) as well as the minimum\nmean-squared error in the Bayes-optimal setting. We then apply this framework\nto derive sharp characterizations of statistical thresholds for two novel\nscenarios: (1) tensor estimation in heteroskedastic noise that is independent\nbut not identically distributed, and (2) higher-order assignment problems,\nwhere the goal is to recover an unknown permutation from tensor-valued\nobservations.", "published": "2025-06-07 10:41:54", "link": "http://arxiv.org/abs/2506.06749v1", "categories": ["cs.IT", "math.IT", "math.ST", "stat.TH"], "primary_category": "cs.IT"}
{"title": "Quantum accessible information and classical entropy inequalities", "abstract": "Computing accessible information for an ensemble of quantum states is a basic\nproblem in quantum information theory. The optimality criterion recently\nobtained in [7], when applied to specific ensembles of states, leads to\nnontrivial tight lower bounds for the Shannon entropy that are discrete\nrelatives of the famous log-Sobolev inequality. In this light, the hypothesis\nof globally information-optimal measurement for an ensemble of equiangular\nequiprobable states (quantum pyramids) put forward and numerically\nsubstantiated in [2] is reconsidered and the corresponding tight entropy\ninequalities are proposed. We prove these inequalities in the cases of state\nensembles corresponding to acute or flat pyramids, thus providing the proof of\nthe hypothesis concerning the globally information-optimal observable.", "published": "2025-06-07 07:50:49", "link": "http://arxiv.org/abs/2506.06700v1", "categories": ["quant-ph", "cs.IT", "math-ph", "math.IT", "math.MP"], "primary_category": "quant-ph"}
{"title": "Skewness of von Neumann entropy over Bures-Hall random states", "abstract": "We study the degree of entanglement, as measured by von Neumann entropy, of\nbipartite systems over the Bures-Hall ensemble. Closed-form expressions of the\nfirst two cumulants of von Neumann entropy over the ensemble have been recently\nderived in the literature. In this paper, we focus on its skewness by\ncalculating the third cumulant that describes the degree of asymmetry of the\ndistribution. The main result is an exact closed-form formula of the third\ncumulant, which leads to a more accurate approximation to the distribution of\nvon Neumann entropy. The key to obtaining the result lies on finding a dozen of\nnew summation identities in simplifying a large number of finite summations\ninvolving polygamma functions.", "published": "2025-06-07 04:55:57", "link": "http://arxiv.org/abs/2506.06663v1", "categories": ["math-ph", "cs.IT", "math.IT", "math.MP", "quant-ph"], "primary_category": "math-ph"}
{"title": "AI-Generated Compromises for Coalition Formation", "abstract": "The challenge of finding compromises between agent proposals is fundamental\nto AI subfields such as argumentation, mediation, and negotiation. Building on\nthis tradition, Elkind et al. (2021) introduced a process for coalition\nformation that seeks majority-supported proposals preferable to the status quo,\nusing a metric space where each agent has an ideal point. A crucial step in\nthis process involves identifying compromise proposals around which agent\ncoalitions can unite. How to effectively find such compromise proposals remains\nan open question. We address this gap by formalizing a model that incorporates\nagent bounded rationality and uncertainty, and by developing AI methods to\ngenerate compromise proposals. We focus on the domain of collaborative document\nwriting, such as the democratic drafting of a community constitution. Our\napproach uses natural language processing techniques and large language models\nto induce a semantic metric space over text. Based on this space, we design\nalgorithms to suggest compromise points likely to receive broad support. To\nevaluate our methods, we simulate coalition formation processes and show that\nAI can facilitate large-scale democratic text editing, a domain where\ntraditional tools are limited.", "published": "2025-06-07 15:28:27", "link": "http://arxiv.org/abs/2506.06837v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "On the randomized SVD in infinite dimensions", "abstract": "Randomized methods, such as the randomized SVD (singular value decomposition)\nand Nystr\\\"om approximation, are an effective way to compute low-rank\napproximations of large matrices. Motivated by applications to operator\nlearning, Boull\\'e and Townsend (FoCM, 2023) recently proposed an\ninfinite-dimensional extension of the randomized SVD for a Hilbert--Schmidt\noperator $A$ that invokes randomness through a Gaussian process with a\ncovariance operator $K$. While the non-isotropy introduced by $K$ allows one to\nincorporate prior information on $A$, an unfortunate choice may lead to\nunfavorable performance and large constants in the error bounds. In this work,\nwe introduce a novel infinite-dimensional extension of the randomized SVD that\ndoes not require such a choice and enjoys error bounds that match those for the\nfinite-dimensional case. Moreover, it reflects the common practice of using the\nrandomized SVD with isotropic random vectors, also when approximating\ndiscretized operators. In fact, the theoretical results of this work show how\nthe usual randomized SVD applied to a discretization of $A$ approaches our\ninfinite-dimensional extension as the discretization gets refined, both in\nterms of error bounds and the Wasserstein distance. We also present and analyze\na novel extension of the Nystr\\\"om approximation for self-adjoint positive\nsemi-definite trace class operators.", "published": "2025-06-07 18:11:22", "link": "http://arxiv.org/abs/2506.06882v1", "categories": ["math.NA", "cs.NA", "65F55, 65N80"], "primary_category": "math.NA"}
{"title": "Estimation of sparse polynomial approximation error to continuous function", "abstract": "The sparse polynomial approximation of continuous functions has emerged as a\nprominent area of interest in function approximation theory in recent years. A\nkey challenge within this domain is the accurate estimation of approximation\nerrors. This paper focuses on continuous functions, characterizing their\nsampled values as a combination of the values of their best approximation\npolynomials within a finite-dimensional polynomial space and the associated\nremainder terms. Consequently, the sampled values of a function can be\ninterpreted as noisy samples of the values of its best approximation\npolynomial, with the noise equivalent to the remainder term's values at those\npoints. By selecting a uniformly bounded orthonormal polynomial system as the\nbasis for this finite-dimensional space, it becomes feasible to formulate noise\nconstraint inequalities and l1-minimization problems or their weighted\nl1-minimization variants. This paper provides estimations for the approximation\nerror of the sparse polynomial derived from the l1-minimization method,\ncharacterizing the error in terms of the quasi-norm of the sampled function or\nits best uniform approximation polynomial, the sparsity, and the best\napproximation error. The analysis reveals that if the sampled function is a\nsparse polynomial from a finite-dimensional space, it can be reconstructed\nexactly. Moreover, it is observed that the smoother the sampled function, the\nfewer degrees of the sparse polynomial are required to attain a given\napproximation accuracy. The paper also extends this analysis to estimate the\nL2-norm approximation error for the sparse polynomial obtained via the weighted\nl1-minimization method, noting that in this context, the orthonormal polynomial\nsystem does not need to be uniformly bounded for the conclusions to hold.", "published": "2025-06-07 18:00:40", "link": "http://arxiv.org/abs/2506.06880v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "A structure-preserving, second-order-in-time scheme for the von Neumann equation with power nonlinearity", "abstract": "In this paper we propose a structure-preserving, linearly implicit,\nsecond-order-in-time scheme for the numerical solution of the von Neumann\nequation with power nonlinearity (also known as the Alber equation). Fourth\norder finite differences are used for the spatial discretization. We highlight\nthe importance of the correct initialization of the method in achieving the\nexpected order of convergence in space and time. As illustrative examples, we\ninvestigate the bifurcation from Landau damping to modulation instability. In\nthat context, amplification factors in the fully developed modulation\ninstability for this nonlinear equation are computed for the first time.", "published": "2025-06-07 17:59:06", "link": "http://arxiv.org/abs/2506.06879v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Fourth- and higher-order finite element methods for the incompressible Navier-Stokes equations with Dirichlet boundary conditions", "abstract": "Inspired by the unconstrained pressure Poisson equation (PPE) formulation\n[Liu, Liu, \\& Pego, Comm. Pure Appl. Math. 60 (2007): 1443-1487], we previously\nproposed the generic projection and unconstrained PPE (GePUP) formulation\n[Zhang, J. Sci. Comput. 67 (2016): 1134-1180] for numerically solving the\nincompressible Navier-Stokes equations (INSE) with no-slip boundary conditions.\nIn GePUP, the main evolutionary variable does not have to be solenoidal with\nits divergence controlled by a heat equation. This work presents high-order\nfinite-element solvers for the INSE under the framework of method-of-lines.\nContinuous Lagrange finite elements of equal order are utilized for the\nvelocity and pressure finite element spaces to discretize the weak form of\nGePUP in space, while high-order implicit-explicit Runge-Kutta methods are then\nemployed to treat the stiff diffusion term implicitly and the other terms\nexplicitly. Due to the implicit treatment of the diffusion term, the time step\nsize is only restricted by convection. The solver is efficient in that\nadvancing the solution at each time step only involves solving a sequence of\nlinear systems either on the velocity or on the pressure with geometric\nmultigrid methods. Furthermore, the solver is enhanced with adaptive mesh\nrefinement so that the multiple length scales and time scales in flows at\nmoderate or high Reynolds numbers can be efficiently resolved. Numerical tests\nwith various Reynolds numbers are performed for the single-vortex test, the\nlid-driven cavity, and the flow past a cylinder/sphere, demonstrating the\nhigh-order accuracy of GePUP-FEM both in time and in space and its capability\nof accurately and efficiently capturing the right physics. Moreover, our solver\noffers the flexibility in choosing velocity and pressure finite element spaces\nand is free of the standard inf-sup condition.", "published": "2025-06-07 17:04:45", "link": "http://arxiv.org/abs/2506.06863v1", "categories": ["math.NA", "cs.NA", "physics.comp-ph"], "primary_category": "math.NA"}
{"title": "Fully discrete finite element approximation for the projection method to solve the Chemotaxis-Fluid System", "abstract": "In this paper, we investigate a chemotaxis-fluid interaction model governed\nby the incompressible Navier-Stokes equations coupled with the classical\nKeller-Segel chemotaxis system. To numerically solve this coupled system, we\ndevelop a pressure-correction projection finite element method based on a\nprojection framework. The proposed scheme employs a backward Euler method for\ntemporal discretization and a mixed finite element method for spatial\ndiscretization. Nonlinear terms are treated semi-implicitly to enhance\ncomputational stability and efficiency. We further establish rigorous error\nestimates for the fully discrete scheme, demonstrating the convergence of the\nnumerical method. A series of numerical experiments are conducted to validate\nthe stability, accuracy, and effectiveness of the proposed method. The results\nconfirm the scheme's capability to capture the essential dynamical behaviors\nand characteristic features of the chemotaxis-fluid system.", "published": "2025-06-07 13:19:21", "link": "http://arxiv.org/abs/2506.06792v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "A robust finite element method for linearized magnetohydrodynamics on general domains", "abstract": "We propose a new finite element method for linearized Magnetohydrodynamics.\nThe main novelty is that the proposed scheme is able to handle also non-convex\ndomains and less regular solutions. The method is proved to be pressure robust\nand quasi-robust with respect to both fluid and magnetic Reynolds numbers.", "published": "2025-06-07 06:52:05", "link": "http://arxiv.org/abs/2506.06685v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Explaining Risks: Axiomatic Risk Attributions for Financial Models", "abstract": "In recent years, machine learning models have achieved great success at the\nexpense of highly complex black-box structures. By using axiomatic attribution\nmethods, we can fairly allocate the contributions of each feature, thus\nallowing us to interpret the model predictions. In high-risk sectors such as\nfinance, risk is just as important as mean predictions. Throughout this work,\nwe address the following risk attribution problem: how to fairly allocate the\nrisk given a model with data? We demonstrate with analysis and empirical\nexamples that risk can be well allocated by extending the Shapley value\nframework.", "published": "2025-06-07 04:15:27", "link": "http://arxiv.org/abs/2506.06653v1", "categories": ["q-fin.CP", "cs.LG", "stat.ML"], "primary_category": "q-fin.CP"}
{"title": "Scalable Gaussian Processes with Latent Kronecker Structure", "abstract": "Applying Gaussian processes (GPs) to very large datasets remains a challenge\ndue to limited computational scalability. Matrix structures, such as the\nKronecker product, can accelerate operations significantly, but their\napplication commonly entails approximations or unrealistic assumptions. In\nparticular, the most common path to creating a Kronecker-structured kernel\nmatrix is by evaluating a product kernel on gridded inputs that can be\nexpressed as a Cartesian product. However, this structure is lost if any\nobservation is missing, breaking the Cartesian product structure, which\nfrequently occurs in real-world data such as time series. To address this\nlimitation, we propose leveraging latent Kronecker structure, by expressing the\nkernel matrix of observed values as the projection of a latent Kronecker\nproduct. In combination with iterative linear system solvers and pathwise\nconditioning, our method facilitates inference of exact GPs while requiring\nsubstantially fewer computational resources than standard iterative methods. We\ndemonstrate that our method outperforms state-of-the-art sparse and variational\nGPs on real-world datasets with up to five million examples, including\nrobotics, automated machine learning, and climate applications.", "published": "2025-06-07 18:47:36", "link": "http://arxiv.org/abs/2506.06895v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Log-Sum-Exponential Estimator for Off-Policy Evaluation and Learning", "abstract": "Off-policy learning and evaluation leverage logged bandit feedback datasets,\nwhich contain context, action, propensity score, and feedback for each data\npoint. These scenarios face significant challenges due to high variance and\npoor performance with low-quality propensity scores and heavy-tailed reward\ndistributions. We address these issues by introducing a novel estimator based\non the log-sum-exponential (LSE) operator, which outperforms traditional\ninverse propensity score estimators. Our LSE estimator demonstrates variance\nreduction and robustness under heavy-tailed conditions. For off-policy\nevaluation, we derive upper bounds on the estimator's bias and variance. In the\noff-policy learning scenario, we establish bounds on the regret -- the\nperformance gap between our LSE estimator and the optimal policy -- assuming\nbounded $(1+\\epsilon)$-th moment of weighted reward. Notably, we achieve a\nconvergence rate of $O(n^{-\\epsilon/(1+ \\epsilon)})$ for the regret bounds,\nwhere $\\epsilon \\in [0,1]$ and $n$ is the size of logged bandit feedback\ndataset. Theoretical analysis is complemented by comprehensive empirical\nevaluations in both off-policy learning and evaluation scenarios, confirming\nthe practical advantages of our approach. The code for our estimator is\navailable at the following link:\nhttps://github.com/armin-behnamnia/lse-offpolicy-learning.", "published": "2025-06-07 17:37:10", "link": "http://arxiv.org/abs/2506.06873v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Curvature Enhanced Data Augmentation for Regression", "abstract": "Deep learning models with a large number of parameters, often referred to as\nover-parameterized models, have achieved exceptional performance across various\ntasks. Despite concerns about overfitting, these models frequently generalize\nwell to unseen data, thanks to effective regularization techniques, with data\naugmentation being among the most widely used. While data augmentation has\nshown great success in classification tasks using label-preserving\ntransformations, its application in regression problems has received less\nattention. Recently, a novel \\emph{manifold learning} approach for generating\nsynthetic data was proposed, utilizing a first-order approximation of the data\nmanifold. Building on this foundation, we present a theoretical framework and\npractical tools for approximating and sampling general data manifolds.\nFurthermore, we introduce the Curvature-Enhanced Manifold Sampling (CEMS)\nmethod for regression tasks. CEMS leverages a second-order representation of\nthe data manifold to enable efficient sampling and reconstruction of new data\npoints. Extensive evaluations across multiple datasets and comparisons with\nstate-of-the-art methods demonstrate that CEMS delivers superior performance in\nboth in-distribution and out-of-distribution scenarios, while introducing only\nminimal computational overhead. Code is available at\nhttps://github.com/azencot-group/CEMS.", "published": "2025-06-07 16:18:37", "link": "http://arxiv.org/abs/2506.06853v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Linear Discriminant Analysis with Gradient Optimization on Covariance Inverse", "abstract": "Linear discriminant analysis (LDA) is a fundamental method in statistical\npattern recognition and classification, achieving Bayes optimality under\nGaussian assumptions. However, it is well-known that classical LDA may struggle\nin high-dimensional settings due to instability in covariance estimation. In\nthis work, we propose LDA with gradient optimization (LDA-GO), a new approach\nthat directly optimizes the inverse covariance matrix via gradient descent. The\nalgorithm parametrizes the inverse covariance matrix through Cholesky\nfactorization, incorporates a low-rank extension to reduce computational\ncomplexity, and considers a multiple-initialization strategy, including\nidentity initialization and warm-starting from the classical LDA estimates. The\neffectiveness of LDA-GO is demonstrated through extensive multivariate\nsimulations and real-data experiments.", "published": "2025-06-07 15:50:43", "link": "http://arxiv.org/abs/2506.06845v1", "categories": ["stat.CO", "stat.ML"], "primary_category": "stat.CO"}
{"title": "A Statistical Framework for Model Selection in LSTM Networks", "abstract": "Long Short-Term Memory (LSTM) neural network models have become the\ncornerstone for sequential data modeling in numerous applications, ranging from\nnatural language processing to time series forecasting. Despite their success,\nthe problem of model selection, including hyperparameter tuning, architecture\nspecification, and regularization choice remains largely heuristic and\ncomputationally expensive. In this paper, we propose a unified statistical\nframework for systematic model selection in LSTM networks. Our framework\nextends classical model selection ideas, such as information criteria and\nshrinkage estimation, to sequential neural networks. We define penalized\nlikelihoods adapted to temporal structures, propose a generalized threshold\napproach for hidden state dynamics, and provide efficient estimation strategies\nusing variational Bayes and approximate marginal likelihood methods. Several\nbiomedical data centric examples demonstrate the flexibility and improved\nperformance of the proposed framework.", "published": "2025-06-07 15:44:27", "link": "http://arxiv.org/abs/2506.06840v1", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.AP", "62M10, 92B20, 62P10, 62P99"], "primary_category": "stat.ML"}
{"title": "The Currents of Conflict: Decomposing Conflict Trends with Gaussian Processes", "abstract": "I present a novel approach to estimating the temporal and spatial patterns of\nviolent conflict. I show how we can use highly temporally and spatially\ndisaggregated data on conflict events in tandem with Gaussian processes to\nestimate temporospatial conflict trends. These trends can be studied to gain\ninsight into conflict traps, diffusion and tempo-spatial conflict exposure in\ngeneral; they can also be used to control for such phenomenons given other\nestimation tasks; lastly, the approach allow us to extrapolate the estimated\ntempo-spatial conflict patterns into future temporal units, thus facilitating\npowerful, stat-of-the-art, conflict forecasts. Importantly, these results are\nachieved via a relatively parsimonious framework using only one data source:\npast conflict patterns.", "published": "2025-06-07 15:16:28", "link": "http://arxiv.org/abs/2506.06828v1", "categories": ["stat.ML", "cs.LG", "stat.AP"], "primary_category": "stat.ML"}
{"title": "Continuous Semi-Implicit Models", "abstract": "Semi-implicit distributions have shown great promise in variational inference\nand generative modeling. Hierarchical semi-implicit models, which stack\nmultiple semi-implicit layers, enhance the expressiveness of semi-implicit\ndistributions and can be used to accelerate diffusion models given pretrained\nscore networks. However, their sequential training often suffers from slow\nconvergence. In this paper, we introduce CoSIM, a continuous semi-implicit\nmodel that extends hierarchical semi-implicit models into a continuous\nframework. By incorporating a continuous transition kernel, CoSIM enables\nefficient, simulation-free training. Furthermore, we show that CoSIM achieves\nconsistency with a carefully designed transition kernel, offering a novel\napproach for multistep distillation of generative models at the distributional\nlevel. Extensive experiments on image generation demonstrate that CoSIM\nperforms on par or better than existing diffusion model acceleration methods,\nachieving superior performance on FD-DINOv2.", "published": "2025-06-07 12:36:56", "link": "http://arxiv.org/abs/2506.06778v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "A Framework for Controllable Multi-objective Learning with Annealed Stein Variational Hypernetworks", "abstract": "Pareto Set Learning (PSL) is popular as an efficient approach to obtaining\nthe complete optimal solution in Multi-objective Learning (MOL). A set of\noptimal solutions approximates the Pareto set, and its mapping is a set of\ndense points in the Pareto front in objective space. However, some current\nmethods face a challenge: how to make the Pareto solution is diverse while\nmaximizing the hypervolume value. In this paper, we propose a novel method to\naddress this challenge, which employs Stein Variational Gradient Descent (SVGD)\nto approximate the entire Pareto set. SVGD pushes a set of particles towards\nthe Pareto set by applying a form of functional gradient descent, which helps\nto converge and diversify optimal solutions. Additionally, we employ diverse\ngradient direction strategies to thoroughly investigate a unified framework for\nSVGD in multi-objective optimization and adapt this framework with an annealing\nschedule to promote stability. We introduce our method, SVH-MOL, and validate\nits effectiveness through extensive experiments on multi-objective problems and\nmulti-task learning, demonstrating its superior performance.", "published": "2025-06-07 08:50:42", "link": "http://arxiv.org/abs/2506.06715v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Through the Gaps: Uncovering Tactical Line-Breaking Passes with Clustering", "abstract": "Line-breaking passes (LBPs) are crucial tactical actions in football,\nallowing teams to penetrate defensive lines and access high-value spaces. In\nthis study, we present an unsupervised, clustering-based framework for\ndetecting and analysing LBPs using synchronised event and tracking data from\nelite matches. Our approach models opponent team shape through vertical spatial\nsegmentation and identifies passes that disrupt defensive lines within open\nplay. Beyond detection, we introduce several tactical metrics, including the\nspace build-up ratio (SBR) and two chain-based variants, LBPCh$^1$ and\nLBPCh$^2$, which quantify the effectiveness of LBPs in generating immediate or\nsustained attacking threats. We evaluate these metrics across teams and players\nin the 2022 FIFA World Cup, revealing stylistic differences in vertical\nprogression and structural disruption. The proposed methodology is explainable,\nscalable, and directly applicable to modern performance analysis and scouting\nworkflows.", "published": "2025-06-07 05:08:24", "link": "http://arxiv.org/abs/2506.06666v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Rescaled Influence Functions: Accurate Data Attribution in High Dimension", "abstract": "How does the training data affect a model's behavior? This is the question we\nseek to answer with data attribution. The leading practical approaches to data\nattribution are based on influence functions (IF). IFs utilize a first-order\nTaylor approximation to efficiently predict the effect of removing a set of\nsamples from the training set without retraining the model, and are used in a\nwide variety of machine learning applications. However, especially in the\nhigh-dimensional regime (# params $\\geq \\Omega($# samples$)$), they are often\nimprecise and tend to underestimate the effect of sample removals, even for\nsimple models such as logistic regression. We present rescaled influence\nfunctions (RIF), a new tool for data attribution which can be used as a drop-in\nreplacement for influence functions, with little computational overhead but\nsignificant improvement in accuracy. We compare IF and RIF on a range of\nreal-world datasets, showing that RIFs offer significantly better predictions\nin practice, and present a theoretical analysis explaining this improvement.\nFinally, we present a simple class of data poisoning attacks that would fool\nIF-based detections but would be detected by RIF.", "published": "2025-06-07 04:19:21", "link": "http://arxiv.org/abs/2506.06656v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "SAFER: A Calibrated Risk-Aware Multimodal Recommendation Model for Dynamic Treatment Regimes", "abstract": "Dynamic treatment regimes (DTRs) are critical to precision medicine,\noptimizing long-term outcomes through personalized, real-time decision-making\nin evolving clinical contexts, but require careful supervision for unsafe\ntreatment risks. Existing efforts rely primarily on clinician-prescribed gold\nstandards despite the absence of a known optimal strategy, and predominantly\nusing structured EHR data without extracting valuable insights from clinical\nnotes, limiting their reliability for treatment recommendations. In this work,\nwe introduce SAFER, a calibrated risk-aware tabular-language recommendation\nframework for DTR that integrates both structured EHR and clinical notes,\nenabling them to learn from each other, and addresses inherent label\nuncertainty by assuming ambiguous optimal treatment solution for deceased\npatients. Moreover, SAFER employs conformal prediction to provide statistical\nguarantees, ensuring safe treatment recommendations while filtering out\nuncertain predictions. Experiments on two publicly available sepsis datasets\ndemonstrate that SAFER outperforms state-of-the-art baselines across multiple\nrecommendation metrics and counterfactual mortality rate, while offering robust\nformal assurances. These findings underscore SAFER potential as a trustworthy\nand theoretically grounded solution for high-stakes DTR applications.", "published": "2025-06-07 04:05:43", "link": "http://arxiv.org/abs/2506.06649v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Spark Transformer: Reactivating Sparsity in FFN and Attention", "abstract": "The discovery of the lazy neuron phenomenon in trained Transformers, where\nthe vast majority of neurons in their feed-forward networks (FFN) are inactive\nfor each token, has spurred tremendous interests in activation sparsity for\nenhancing large model efficiency. While notable progress has been made in\ntranslating such sparsity to wall-time benefits, modern Transformers have moved\naway from the ReLU activation function crucial to this phenomenon. Existing\nefforts on re-introducing activation sparsity often degrade model quality,\nincrease parameter count, complicate or slow down training. Sparse attention,\nthe application of sparse activation to the attention mechanism, often faces\nsimilar challenges.\n  This paper introduces the Spark Transformer, a novel architecture that\nachieves a high level of activation sparsity in both FFN and the attention\nmechanism while maintaining model quality, parameter count, and standard\ntraining procedures. Our method realizes sparsity via top-k masking for\nexplicit control over sparsity level. Crucially, we introduce statistical\ntop-k, a hardware-accelerator-friendly, linear-time approximate algorithm that\navoids costly sorting and mitigates significant training slowdown from standard\ntop-$k$ operators. Furthermore, Spark Transformer reallocates existing FFN\nparameters and attention key embeddings to form a low-cost predictor for\nidentifying activated entries. This design not only mitigates quality loss from\nenforced sparsity, but also enhances wall-time benefit. Pretrained with the\nGemma-2 recipe, Spark Transformer demonstrates competitive performance on\nstandard benchmarks while exhibiting significant sparsity: only 8% of FFN\nneurons are activated, and each token attends to a maximum of 256 tokens. This\nsparsity translates to a 2.5x reduction in FLOPs, leading to decoding wall-time\nspeedups of up to 1.79x on CPU and 1.40x on GPU.", "published": "2025-06-07 03:51:13", "link": "http://arxiv.org/abs/2506.06644v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Robust Learnability of Sample-Compressible Distributions under Noisy or Adversarial Perturbations", "abstract": "Learning distribution families over $\\mathbb{R}^d$ is a fundamental problem\nin unsupervised learning and statistics. A central question in this setting is\nwhether a given family of distributions possesses sufficient structure to be\n(at least) information-theoretically learnable and, if so, to characterize its\nsample complexity. In 2018, Ashtiani et al. reframed \\emph{sample\ncompressibility}, originally due to Littlestone and Warmuth (1986), as a\nstructural property of distribution classes, proving that it guarantees\nPAC-learnability. This discovery subsequently enabled a series of recent\nadvancements in deriving nearly tight sample complexity bounds for various\nhigh-dimensional open problems. It has been further conjectured that the\nconverse also holds: every learnable class admits a tight sample compression\nscheme.\n  In this work, we establish that sample compressible families remain learnable\neven from perturbed samples, subject to a set of necessary and sufficient\nconditions. We analyze two models of data perturbation: (i) an additive\nindependent noise model, and (ii) an adversarial corruption model, where an\nadversary manipulates a limited subset of the samples unknown to the learner.\nOur results are general and rely on as minimal assumptions as possible. We\ndevelop a perturbation-quantization framework that interfaces naturally with\nthe compression scheme and leads to sample complexity bounds that scale\ngracefully with the noise level and corruption budget. As concrete\napplications, we establish new sample complexity bounds for learning finite\nmixtures of high-dimensional uniform distributions under both noise and\nadversarial perturbations, as well as for learning Gaussian mixture models from\nadversarially corrupted samples, resolving two open problems in the literature.", "published": "2025-06-07 01:11:50", "link": "http://arxiv.org/abs/2506.06613v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Direct Prediction Set Minimization via Bilevel Conformal Classifier Training", "abstract": "Conformal prediction (CP) is a promising uncertainty quantification framework\nwhich works as a wrapper around a black-box classifier to construct prediction\nsets (i.e., subset of candidate classes) with provable guarantees. However,\nstandard calibration methods for CP tend to produce large prediction sets which\nmakes them less useful in practice. This paper considers the problem of\nintegrating conformal principles into the training process of deep classifiers\nto directly minimize the size of prediction sets. We formulate conformal\ntraining as a bilevel optimization problem and propose the {\\em Direct\nPrediction Set Minimization (DPSM)} algorithm to solve it. The key insight\nbehind DPSM is to minimize a measure of the prediction set size (upper level)\nthat is conditioned on the learned quantile of conformity scores (lower level).\nWe analyze that DPSM has a learning bound of $O(1/\\sqrt{n})$ (with $n$ training\nsamples), while prior conformal training methods based on stochastic\napproximation for the quantile has a bound of $\\Omega(1/s)$ (with batch size\n$s$ and typically $s \\ll \\sqrt{n}$). Experiments on various benchmark datasets\nand deep models show that DPSM significantly outperforms the best prior\nconformal training baseline with $20.46\\%\\downarrow$ in the prediction set size\nand validates our theory.", "published": "2025-06-07 00:19:00", "link": "http://arxiv.org/abs/2506.06599v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Automatic Speech Recognition of African American English: Lexical and Contextual Effects", "abstract": "Automatic Speech Recognition (ASR) models often struggle with the phonetic,\nphonological, and morphosyntactic features found in African American English\n(AAE). This study focuses on two key AAE variables: Consonant Cluster Reduction\n(CCR) and ING-reduction. It examines whether the presence of CCR and\nING-reduction increases ASR misrecognition. Subsequently, it investigates\nwhether end-to-end ASR systems without an external Language Model (LM) are more\ninfluenced by lexical neighborhood effect and less by contextual predictability\ncompared to systems with an LM. The Corpus of Regional African American\nLanguage (CORAAL) was transcribed using wav2vec 2.0 with and without an LM. CCR\nand ING-reduction were detected using the Montreal Forced Aligner (MFA) with\npronunciation expansion. The analysis reveals a small but significant effect of\nCCR and ING on Word Error Rate (WER) and indicates a stronger presence of\nlexical neighborhood effect in ASR systems without LMs.", "published": "2025-06-07 18:30:59", "link": "http://arxiv.org/abs/2506.06888v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multimodal Spatial Language Maps for Robot Navigation and Manipulation", "abstract": "Grounding language to a navigating agent's observations can leverage\npretrained multimodal foundation models to match perceptions to object or event\ndescriptions. However, previous approaches remain disconnected from environment\nmapping, lack the spatial precision of geometric maps, or neglect additional\nmodality information beyond vision. To address this, we propose multimodal\nspatial language maps as a spatial map representation that fuses pretrained\nmultimodal features with a 3D reconstruction of the environment. We build these\nmaps autonomously using standard exploration. We present two instances of our\nmaps, which are visual-language maps (VLMaps) and their extension to\naudio-visual-language maps (AVLMaps) obtained by adding audio information. When\ncombined with large language models (LLMs), VLMaps can (i) translate natural\nlanguage commands into open-vocabulary spatial goals (e.g., \"in between the\nsofa and TV\") directly localized in the map, and (ii) be shared across\ndifferent robot embodiments to generate tailored obstacle maps on demand.\nBuilding upon the capabilities above, AVLMaps extend VLMaps by introducing a\nunified 3D spatial representation integrating audio, visual, and language cues\nthrough the fusion of features from pretrained multimodal foundation models.\nThis enables robots to ground multimodal goal queries (e.g., text, images, or\naudio snippets) to spatial locations for navigation. Additionally, the\nincorporation of diverse sensory inputs significantly enhances goal\ndisambiguation in ambiguous environments. Experiments in simulation and\nreal-world settings demonstrate that our multimodal spatial language maps\nenable zero-shot spatial and multimodal goal navigation and improve recall by\n50% in ambiguous scenarios. These capabilities extend to mobile robots and\ntabletop manipulators, supporting navigation and interaction guided by visual,\naudio, and spatial cues.", "published": "2025-06-07 17:02:13", "link": "http://arxiv.org/abs/2506.06862v1", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.RO"}
{"title": "Rhythm Features for Speaker Identification", "abstract": "While deep learning models have demonstrated robust performance in speaker\nrecognition tasks, they primarily rely on low-level audio features learned\nempirically from spectrograms or raw waveforms. However, prior work has\nindicated that idiosyncratic speaking styles heavily influence the temporal\nstructure of linguistic units in speech signals (rhythm). This makes rhythm a\nstrong yet largely overlooked candidate for a speech identity feature. In this\npaper, we test this hypothesis by applying deep learning methods to perform\ntext-independent speaker identification from rhythm features. Our findings\nsupport the usefulness of rhythmic information for speaker recognition tasks\nbut also suggest that high intra-subject variability in ad-hoc speech can\ndegrade its effectiveness.", "published": "2025-06-07 15:25:52", "link": "http://arxiv.org/abs/2506.06834v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Beyond Classification: Towards Speech Emotion Reasoning with Multitask AudioLLMs", "abstract": "Audio Large Language Models (AudioLLMs) have achieved strong results in\nsemantic tasks like speech recognition and translation, but remain limited in\nmodeling paralinguistic cues such as emotion. Existing approaches often treat\nemotion understanding as a classification problem, offering little insight into\nthe underlying rationale behind predictions. In this work, we explore emotion\nreasoning, a strategy that leverages the generative capabilities of AudioLLMs\nto enhance emotion recognition by producing semantically aligned,\nevidence-grounded explanations. To support this in multitask AudioLLMs, we\nintroduce a unified framework combining reasoning-augmented data supervision,\ndual-encoder architecture, and task-alternating training. This approach enables\nAudioLLMs to effectively learn different tasks while incorporating emotional\nreasoning. Experiments on IEMOCAP and MELD show that our approach not only\nimproves emotion prediction accuracy but also enhances the coherence and\nevidential grounding of the generated responses.", "published": "2025-06-07 14:52:58", "link": "http://arxiv.org/abs/2506.06820v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "SynHate: Detecting Hate Speech in Synthetic Deepfake Audio", "abstract": "The rise of deepfake audio and hate speech, powered by advanced\ntext-to-speech, threatens online safety. We present SynHate, the first\nmultilingual dataset for detecting hate speech in synthetic audio, spanning 37\nlanguages. SynHate uses a novel four-class scheme: Real-normal, Real-hate,\nFake-normal, and Fake-hate. Built from MuTox and ADIMA datasets, it captures\ndiverse hate speech patterns globally and in India. We evaluate five leading\nself-supervised models (Whisper-small/medium, XLS-R, AST, mHuBERT), finding\nnotable performance differences by language, with Whisper-small performing best\noverall. Cross-dataset generalization remains a challenge. By releasing SynHate\nand baseline code, we aim to advance robust, culturally sensitive, and\nmultilingual solutions against synthetic hate speech. The dataset is available\nat https://www.iab-rubric.org/resources.", "published": "2025-06-07 11:46:39", "link": "http://arxiv.org/abs/2506.06772v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Can Quantized Audio Language Models Perform Zero-Shot Spoofing Detection?", "abstract": "Quantization is essential for deploying large audio language models (LALMs)\nefficiently in resource-constrained environments. However, its impact on\ncomplex tasks, such as zero-shot audio spoofing detection, remains\nunderexplored. This study evaluates the zero-shot capabilities of five LALMs,\nGAMA, LTU-AS, MERaLiON, Qwen-Audio, and SALMONN, across three distinct\ndatasets: ASVspoof2019, In-the-Wild, and WaveFake, and investigates their\nrobustness to quantization (FP32, FP16, INT8). Despite high initial spoof\ndetection accuracy, our analysis demonstrates severe predictive biases toward\nspoof classification across all models, rendering their practical performance\nequivalent to random classification. Interestingly, quantization to FP16\nprecision resulted in negligible performance degradation compared to FP32,\neffectively halving memory and computational requirements without materially\nimpacting accuracy. However, INT8 quantization intensified model biases,\nsignificantly degrading balanced accuracy. These findings highlight critical\narchitectural limitations and emphasize FP16 quantization as an optimal\ntrade-off, providing guidelines for practical deployment and future model\nrefinement.", "published": "2025-06-07 10:56:33", "link": "http://arxiv.org/abs/2506.06756v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Neural Spectral Band Generation for Audio Coding", "abstract": "Audio bandwidth extension is the task of reconstructing missing high\nfrequency components of bandwidth-limited audio signals, where bandwidth\nlimitation is a common issue for audio signals due to several reasons,\nincluding channel capacity and data constraints. While conventional spectral\nband replication is a well-established parametric approach to audio bandwidth\nextension, the SBR usually entails coarse feature extraction and reconstruction\ntechniques, which leads to limitations when processing various types of audio\nsignals. In parallel, numerous deep neural network-based audio bandwidth\nextension methods have been proposed. These DNN-based methods are usually\nreferred to as blind BWE, as these methods do not rely on prior information\nextracted from original signals, and only utilize given low frequency band\nsignals to estimate missing high frequency components. In order to replace\nconventional SBR with DNNs, simply adopting existing DNN-based methodologies\nresults in suboptimal performance due to the blindness of these methods. My\nproposed research suggests a new approach to parametric non-blind bandwidth\nextension, as DNN-based side information extraction and DNN-based bandwidth\nextension are performed only at the front and end of the audio coding pipeline.", "published": "2025-06-07 09:35:08", "link": "http://arxiv.org/abs/2506.06732v1", "categories": ["eess.AS", "cs.AI", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Exploring Length Generalization For Transformer-based Speech Enhancement", "abstract": "Transformer network architecture has proven effective in speech enhancement.\nHowever, as its core module, self-attention suffers from quadratic complexity,\nmaking it infeasible for training on long speech utterances. In practical\nscenarios, speech enhancement models are often required to perform on noisy\nspeech at run-time that is substantially longer than the training utterances.\nIt remains a challenge how a Transformer-based speech enhancement model can\ngeneralize to long speech utterances. In this paper, extensive empirical\nstudies are conducted to explore the model's length generalization ability. In\nparticular, we conduct speech enhancement experiments on four training\nobjectives and evaluate with five metrics. Our studies establish that\npositional encoding is an effective instrument to dampen the effect of\nutterance length on speech enhancement. We first explore several existing\npositional encoding methods, and the results show that relative positional\nencoding methods exhibit a better length generalization property than absolute\npositional encoding methods. Additionally, we also explore a simpler and more\neffective positional encoding scheme, i.e. LearnLin, that uses only one\ntrainable parameter for each attention head to scale the real relative position\nbetween time frames, which learns the different preferences on short- or\nlong-term dependencies of these heads. The results demonstrate that our\nproposal exhibits excellent length generalization ability with comparable or\nsuperior performance than other state-of-the-art positional encoding\nstrategies.", "published": "2025-06-07 07:45:22", "link": "http://arxiv.org/abs/2506.06697v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A Fast and Lightweight Model for Causal Audio-Visual Speech Separation", "abstract": "Audio-visual speech separation (AVSS) aims to extract a target speech signal\nfrom a mixed signal by leveraging both auditory and visual (lip movement) cues.\nHowever, most existing AVSS methods exhibit complex architectures and rely on\nfuture context, operating offline, which renders them unsuitable for real-time\napplications. Inspired by the pipeline of RTFSNet, we propose a novel streaming\nAVSS model, named Swift-Net, which enhances the causal processing capabilities\nrequired for real-time applications. Swift-Net adopts a lightweight visual\nfeature extraction module and an efficient fusion module for audio-visual\nintegration. Additionally, Swift-Net employs Grouped SRUs to integrate\nhistorical information across different feature spaces, thereby improving the\nutilization efficiency of historical information. We further propose a causal\ntransformation template to facilitate the conversion of non-causal AVSS models\ninto causal counterparts. Experiments on three standard benchmark datasets\n(LRS2, LRS3, and VoxCeleb2) demonstrated that under causal conditions, our\nproposed Swift-Net exhibited outstanding performance, highlighting the\npotential of this method for processing speech in complex environments.", "published": "2025-06-07 07:04:41", "link": "http://arxiv.org/abs/2506.06689v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Accurate analysis of the pitch pulse-based magnitude/phase structure of natural vowels and assessment of three lightweight time/frequency voicing restoration methods", "abstract": "Whispered speech is produced when the vocal folds are not used, either\nintentionally, or due to a temporary or permanent voice condition. The\nessential difference between natural speech and whispered speech is that\nperiodic signal components that exist in certain regions of the former, called\nvoiced regions, as a consequence of the vibration of the vocal folds, are\nmissing in the latter. The restoration of natural speech from whispered speech\nrequires delicate signal processing procedures that are especially useful if\nthey can be implemented on low-resourced portable devices, in real-time, and\non-the-fly, taking advantage of the established source-filter paradigm of voice\nproduction and related models. This paper addresses two challenges that are\nintertwined and are key in informing and making viable this envisioned\ntechnological realization. The first challenge involves characterizing and\nmodeling the evolution of the harmonic phase/magnitude structure of a sequence\nof individual pitch periods in a voiced region of natural speech comprising\nsustained or co-articulated vowels. This paper proposes a novel algorithm\nsegmenting individual pitch pulses, which is then used to obtain illustrative\nresults highlighting important differences between sustained and co-articulated\nvowels, and suggesting practical synthetic voicing approaches. The second\nchallenge involves model-based synthetic voicing. Three implementation\nalternatives are described that differ in their signal reconstruction\napproaches: frequency-domain, combined frequency and time-domain, and\nphysiologically-inspired separate filtering of glottal excitation pulses\nindividually generated. The three alternatives are compared objectively using\nillustrative examples, and subjectively using the results of listening tests\ninvolving synthetic voicing of sustained and co-articulated vowels in word\ncontext.", "published": "2025-06-07 06:00:36", "link": "http://arxiv.org/abs/2506.06675v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CAtCh: Cognitive Assessment through Cookie Thief", "abstract": "Several machine learning algorithms have been developed for the prediction of\nAlzheimer's disease and related dementia (ADRD) from spontaneous speech.\nHowever, none of these algorithms have been translated for the prediction of\nbroader cognitive impairment (CI), which in some cases is a precursor and risk\nfactor of ADRD. In this paper, we evaluated several speech-based open-source\nmethods originally proposed for the prediction of ADRD, as well as methods from\nmultimodal sentiment analysis for the task of predicting CI from patient audio\nrecordings. Results demonstrated that multimodal methods outperformed unimodal\nones for CI prediction, and that acoustics-based approaches performed better\nthan linguistics-based ones. Specifically, interpretable acoustic features\nrelating to affect and prosody were found to significantly outperform\nBERT-based linguistic features and interpretable linguistic features,\nrespectively. All the code developed for this study is available at\nhttps://github.com/JTColonel/catch.", "published": "2025-06-07 00:41:34", "link": "http://arxiv.org/abs/2506.06603v1", "categories": ["cs.LG", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "WiFi Pathologies Detection using LLMs", "abstract": "In this paper, we fine-tune encoder-only and decoder-only large language\nmodels (LLMs) to detect pathologies in IEEE 802.11 networks, commonly known as\nWiFi. Our approach involves manually crafting prompts followed by fine-tuning.\nEvaluations show that the sequential model achieves high detection accuracy\nusing labeled data, while the causal model performs equally well for unlabeled\ndata.", "published": "2025-06-07 22:48:48", "link": "http://arxiv.org/abs/2506.06943v1", "categories": ["eess.SP", "cs.NI"], "primary_category": "eess.SP"}
{"title": "Conditional Denoising Diffusion for ISAC Enhanced Channel Estimation in Cell-Free 6G", "abstract": "Cell-free Integrated Sensing and Communication (ISAC) aims to revolutionize\n6th Generation (6G) networks. By combining distributed access points with ISAC\ncapabilities, it boosts spectral efficiency, situational awareness, and\ncommunication reliability. Channel estimation is a critical step in cell-free\nISAC systems to ensure reliable communication, but its performance is usually\nlimited by challenges such as pilot contamination and noisy channel estimates.\nThis paper presents a novel framework leveraging sensing information as a key\ninput within a Conditional Denoising Diffusion Model (CDDM). In this framework,\nwe integrate CDDM with a Multimodal Transformer (MMT) to enhance channel\nestimation in ISAC-enabled cell-free systems. The MMT encoder effectively\ncaptures inter-modal relationships between sensing and location data, enabling\nthe CDDM to iteratively denoise and refine channel estimates. Simulation\nresults demonstrate that the proposed approach achieves significant performance\ngains. As compared with Least Squares (LS) and Minimum Mean Squared Error\n(MMSE) estimators, the proposed model achieves normalized mean squared error\n(NMSE) improvements of 8 dB and 9 dB, respectively. Moreover, we achieve a\n27.8% NMSE improvement compared to the traditional denoising diffusion model\n(TDDM), which does not incorporate sensing channel information. Additionally,\nthe model exhibits higher robustness against pilot contamination and maintains\nhigh accuracy under challenging conditions, such as low signal-to-noise ratios\n(SNRs). According to the simulation results, the model performs well for users\nnear sensing targets by leveraging the correlation between sensing and\ncommunication channels.", "published": "2025-06-07 22:45:21", "link": "http://arxiv.org/abs/2506.06942v1", "categories": ["eess.SP", "cs.LG"], "primary_category": "eess.SP"}
{"title": "Towards AI-Native Fronthaul: Neural Compression for NextG Cloud RAN", "abstract": "The rapid growth of data traffic and the emerging AI-native wireless\narchitectures in NextG cellular systems place new demands on the fronthaul\nlinks of Cloud Radio Access Networks (C-RAN). In this paper, we investigate\nneural compression techniques for the Common Public Radio Interface (CPRI),\naiming to reduce the fronthaul bandwidth while preserving signal quality. We\nintroduce two deep learning-based compression algorithms designed to optimize\nthe transformation of wireless signals into bit sequences for CPRI\ntransmission. The first algorithm utilizes a non-linear transformation coupled\nwith scalar/vector quantization based on a learned codebook. The second\nalgorithm generates a latent vector transformed into a variable-length output\nbit sequence via arithmetic encoding, guided by the predicted probability\ndistribution of each latent element. Novel techniques such as a shared weight\nmodel for storage-limited devices and a successive refinement model for\nmanaging multiple CPRI links with varying Quality of Service (QoS) are\nproposed. Extensive simulation results demonstrate notable Error Vector\nMagnitude (EVM) gains with improved rate-distortion performance for both\nalgorithms compared to traditional methods. The proposed solutions are robust\nto variations in channel conditions, modulation formats, and noise levels,\nhighlighting their potential for enabling efficient and scalable fronthaul in\nNextG AI-native networks as well as aligning with the current 3GPP research\ndirections.", "published": "2025-06-07 21:25:47", "link": "http://arxiv.org/abs/2506.06925v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "SPC to 3D: Novel View Synthesis from Binary SPC via I2I translation", "abstract": "Single Photon Avalanche Diodes (SPADs) represent a cutting-edge imaging\ntechnology, capable of detecting individual photons with remarkable timing\nprecision. Building on this sensitivity, Single Photon Cameras (SPCs) enable\nimage capture at exceptionally high speeds under both low and high\nillumination. Enabling 3D reconstruction and radiance field recovery from such\nSPC data holds significant promise. However, the binary nature of SPC images\nleads to severe information loss, particularly in texture and color, making\ntraditional 3D synthesis techniques ineffective. To address this challenge, we\npropose a modular two-stage framework that converts binary SPC images into\nhigh-quality colorized novel views. The first stage performs image-to-image\n(I2I) translation using generative models such as Pix2PixHD, converting binary\nSPC inputs into plausible RGB representations. The second stage employs 3D\nscene reconstruction techniques like Neural Radiance Fields (NeRF) or Gaussian\nSplatting (3DGS) to generate novel views. We validate our two-stage pipeline\n(Pix2PixHD + Nerf/3DGS) through extensive qualitative and quantitative\nexperiments, demonstrating significant improvements in perceptual quality and\ngeometric consistency over the alternative baseline.", "published": "2025-06-07 18:33:21", "link": "http://arxiv.org/abs/2506.06890v1", "categories": ["eess.IV", "cs.CV", "eess.SP"], "primary_category": "eess.IV"}
{"title": "Energy-efficient Deep Reinforcement Learning-based Network Function Disaggregation in Hybrid Non-terrestrial Open Radio Access Networks", "abstract": "This paper explores the integration of Open Radio Access Network (O-RAN)\nprinciples with non-terrestrial networks (NTN) and investigates the\noptimization of the functional split between Centralized Units (CU) and\nDistributed Units (DU) to improve energy efficiency in dynamic network\nenvironments. Given the inherent constraints of NTN platforms, such as Low\nEarth Orbit (LEO) satellites and high-altitude platform stations (HAPS), we\npropose a reinforcement learning-based framework utilizing Deep Q-Network (DQN)\nto intelligently determine the optimal RAN functional split. The proposed\napproach dynamically adapts to real-time fluctuations in traffic demand,\nnetwork conditions, and power limitations, ensuring efficient resource\nallocation and enhanced system performance.The numerical results demonstrate\nthat the proposed policy effectively adapts to network traffic flow by\nselecting an efficient network disaggregation strategy and corresponding\nfunctional split option based on data rate and latency requirements.", "published": "2025-06-07 17:54:14", "link": "http://arxiv.org/abs/2506.06876v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Deep Inertial Pose: A deep learning approach for human pose estimation", "abstract": "Inertial-based Motion capture system has been attracting growing attention\ndue to its wearability and unsconstrained use. However, accurate human joint\nestimation demands several complex and expertise demanding steps, which leads\nto expensive software such as the state-of-the-art MVN Awinda from Xsens\nTechnologies. This work aims to study the use of Neural Networks to abstract\nthe complex biomechanical models and analytical mathematics required for pose\nestimation. Thus, it presents a comparison of different Neural Network\narchitectures and methodologies to understand how accurately these methods can\nestimate human pose, using both low cost(MPU9250) and high end (Mtw Awinda)\nMagnetic, Angular Rate, and Gravity (MARG) sensors. The most efficient method\nwas the Hybrid LSTM-Madgwick detached, which achieved an Quaternion Angle\ndistance error of 7.96, using Mtw Awinda data. Also, an ablation study was\nconducted to study the impact of data augmentation, output representation,\nwindow size, loss function and magnetometer data on the pose estimation error.\nThis work indicates that Neural Networks can be trained to estimate human pose,\nwith results comparable to the state-of-the-art fusion filters.", "published": "2025-06-07 16:12:49", "link": "http://arxiv.org/abs/2506.06850v1", "categories": ["cs.CV", "eess.SP"], "primary_category": "cs.CV"}
{"title": "Cell-Free Massive MIMO under a Non-Linear Power Amplifier Consumption Model", "abstract": "Existing works on Cell-Free Massive MIMO primarily focus on optimising system\nthroughput and energy efficiency under high-traffic scenarios with only a\nlimited focus on variable user demand as required by higher network layers.\nAdditionally, existing works only minimise the transmitted power instead of the\nconsumed power at the power amplifier. This work introduces a\npenalty-method-based approach to minimise the amplifier's power consumption\nwhile scaling much better with network size than current solutions and\npromoting sparsity in the power allocated to each access point. Furthermore, we\ndemonstrate substantial reductions in power consumption (up to 24%) by\nconsidering the non-linear power consumption.", "published": "2025-06-07 13:51:51", "link": "http://arxiv.org/abs/2506.06799v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "A Novel Spreading-Factor-Index-Aided LoRa Scheme: Design and Performance Analysis", "abstract": "LoRa is a widely recognized modulation technology in the field of low power\nwide area networks (LPWANs). However, the data rate of LoRa is too low to\nsatisfy the requirements in the context of modern Internet of Things (IoT)\napplications. To address this issue, we propose a novel high-data-rate LoRa\nscheme based on the spreading factor index (SFI). In the proposed SFI-LoRa\nscheme, the starting frequency bin (SFB) of chirp signals is used to transmit\ninformation bits, while the combinations of spreading factors (SFs) are\nexploited as a set of indices to convey additional information bits. Moreover,\ntheoretical expressions for the symbol error rate (SER) and throughput of the\nproposed SFI-LoRa scheme are derived over additive white Gaussian noise (AWGN)\nand Rayleigh fading channels. Simulation results not only verify the accuracy\nof the theoretical analysis, but also demonstrate that the proposed SFI-LoRa\nscheme improves both the bit error rate (BER) and throughput performance\ncompared to existing high-data-rate LoRa schemes. Therefore, the proposed\nSFI-LoRa scheme is a potential solution for applications requiring a high data\nrate in the LPWAN domain.", "published": "2025-06-07 11:01:33", "link": "http://arxiv.org/abs/2506.06758v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "IQFM A Wireless Foundational Model for I/Q Streams in AI-Native 6G", "abstract": "Foundational models have shown remarkable potential in natural language\nprocessing and computer vision, yet remain in their infancy in wireless\ncommunications. While a few efforts have explored image-based modalities such\nas channel state information (CSI) and frequency spectrograms, foundational\nmodels that operate directly on raw IQ data remain largely unexplored. This\npaper presents, IQFM, the first I/Q signal foundational model for wireless\ncommunications. IQFM supporting diverse tasks: modulation classification,\nangle-of-arrival (AoA), beam prediction, and RF fingerprinting, without heavy\npreprocessing or handcrafted features. We also introduce a task-aware\naugmentation strategy that categorizes transformations into core augmentations,\nsuch as cyclic time shifting, and task-specific augmentations. This strategy\nforms the basis for structured, task-dependent representation learning within a\ncontrastive self-supervised learning (SSL) framework. Using this strategy, the\nlightweight encoder, pre-trained via SSL on over-the-air multi-antenna IQ data,\nachieves up to 99.67% and 65.45% accuracy on modulation and AoA classification,\nrespectively, using only one labeled sample per class, outperforming supervised\nbaselines by up to 7x and 145x. The model also generalizes to\nout-of-distribution tasks; when adapted to new tasks using only 500 samples per\nclass and minimal parameter updates via LoRA, the same frozen encoder achieves\n94.15% on beam prediction (vs. 89.53% supervised), 50.00% on RML2016a\nmodulation classification (vs. 49.30%), and 96.05% on RF fingerprinting (vs.\n96.64%). These results demonstrate the potential of raw IQ-based foundational\nmodels as efficient, reusable encoders for multi-task learning in AI-native 6G\nsystems.", "published": "2025-06-07 09:01:38", "link": "http://arxiv.org/abs/2506.06718v1", "categories": ["eess.SP", "cs.LG"], "primary_category": "eess.SP"}
{"title": "Design and Implementation of a RISC-V SoC with Custom DSP Accelerators for Edge Computing", "abstract": "This paper presents a comprehensive analysis of the RISC-V instruction set\narchitecture, focusing on its modular design, implementation challenges, and\nperformance characteristics. We examine the RV32I base instruction set with\nextensions for multiplication (M) and atomic operations (A). Through\ncycle-accurate simulation of a pipelined implementation, we evaluate\nperformance metrics including CPI (cycles per instruction) and power\nefficiency. Our results demonstrate RISC-V's advantages in embedded systems and\nits scalability for custom accelerators. Comparative analysis shows a 17%\nreduction in power consumption compared to ARM Cortex-M0 implementations in\nsimilar process nodes. The open-standard nature of RISC-V provides significant\nflexibility for domain-specific optimizations.", "published": "2025-06-07 07:17:40", "link": "http://arxiv.org/abs/2506.06693v1", "categories": ["cs.AR", "cs.AI", "eess.SP", "C.1.3, B.5.2, I.5.1 C.1.3, B.5.2, I.5.1 C.1.3, B.5.2, I.5.1"], "primary_category": "cs.AR"}
{"title": "Non-Intrusive Load Monitoring Based on Image Load Signatures and Continual Learning", "abstract": "Non-Intrusive Load Monitoring (NILM) identifies the operating status and\nenergy consumption of each electrical device in the circuit by analyzing the\nelectrical signals at the bus, which is of great significance for smart power\nmanagement. However, the complex and changeable load combinations and\napplication environments lead to the challenges of poor feature robustness and\ninsufficient model generalization of traditional NILM methods. To this end,\nthis paper proposes a new non-intrusive load monitoring method that integrates\n\"image load signature\" and continual learning. This method converts\nmulti-dimensional power signals such as current, voltage, and power factor into\nvisual image load feature signatures, and combines deep convolutional neural\nnetworks to realize the identification and classification of multiple devices;\nat the same time, self-supervised pre-training is introduced to improve feature\ngeneralization, and continual online learning strategies are used to overcome\nmodel forgetting to adapt to the emergence of new loads. This paper conducts a\nlarge number of experiments on high-sampling rate load datasets, and compares a\nvariety of existing methods and model variants. The results show that the\nproposed method has achieved significant improvements in recognition accuracy.", "published": "2025-06-07 03:13:15", "link": "http://arxiv.org/abs/2506.06637v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.SP"], "primary_category": "cs.LG"}
{"title": "Toward High Accuracy DME for Alternative Aircraft Positioning: SFOL Pulse Transmission in High-Power DME", "abstract": "The Stretched-FrOnt-Leg (SFOL) pulse is an advanced distance measuring\nequipment (DME) pulse that offers superior ranging accuracy compared to\nconventional Gaussian pulses. Successful SFOL pulse transmission has been\nrecently demonstrated from a commercial Gaussian pulse-based DME in low-power\nmode utilizing digital predistortion (DPD) techniques for power amplifiers.\nThese adjustments were achieved through software modifications, enabling SFOL\nintegration without replacing existing DME infrastructure. However, the SFOL\npulse is designed to optimize ranging capabilities by leveraging the effective\nradiated power (ERP) and pulse shape parameters permitted within DME\nspecifications. Consequently, it operates with narrow margins against these\nspecifications, potentially leading to non-compliance when transmitted in\nhigh-power mode. This paper introduces strategies to enable a Gaussian\npulse-based DME to transmit the SFOL pulse while adhering to DME specifications\nin high-power mode. The proposed strategies involve use of a variant of the\nSFOL pulse and DPD techniques utilizing truncated singular value decomposition,\ntailored for high-power DME operations. Test results, conducted on a testbed\nutilizing a commercial Gaussian pulse-based DME, demonstrate the effectiveness\nof these strategies, ensuring compliance with DME specifications in high-power\nmode with minimal performance loss. This study enables cost-effective\nintegration of high-accuracy SFOL pulses into existing high-power DME systems,\nenhancing aircraft positioning precision while ensuring compliance with\nindustry standards.", "published": "2025-06-07 01:16:12", "link": "http://arxiv.org/abs/2506.06614v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "A Framework for Controllable Multi-objective Learning with Annealed Stein Variational Hypernetworks", "abstract": "Pareto Set Learning (PSL) is popular as an efficient approach to obtaining\nthe complete optimal solution in Multi-objective Learning (MOL). A set of\noptimal solutions approximates the Pareto set, and its mapping is a set of\ndense points in the Pareto front in objective space. However, some current\nmethods face a challenge: how to make the Pareto solution is diverse while\nmaximizing the hypervolume value. In this paper, we propose a novel method to\naddress this challenge, which employs Stein Variational Gradient Descent (SVGD)\nto approximate the entire Pareto set. SVGD pushes a set of particles towards\nthe Pareto set by applying a form of functional gradient descent, which helps\nto converge and diversify optimal solutions. Additionally, we employ diverse\ngradient direction strategies to thoroughly investigate a unified framework for\nSVGD in multi-objective optimization and adapt this framework with an annealing\nschedule to promote stability. We introduce our method, SVH-MOL, and validate\nits effectiveness through extensive experiments on multi-objective problems and\nmulti-task learning, demonstrating its superior performance.", "published": "2025-06-07 08:50:42", "link": "http://arxiv.org/abs/2506.06715v2", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Graph-based RAG Enhancement via Global Query Disambiguation and Dependency-Aware Reranking", "abstract": "Contemporary graph-based retrieval-augmented generation (RAG) methods\ntypically begin by extracting entities from user queries and then leverage\npre-constructed knowledge graphs to retrieve related relationships and\nmetadata. However, this pipeline's exclusive reliance on entity-level\nextraction can lead to the misinterpretation or omission of latent yet critical\ninformation and relations. As a result, retrieved content may be irrelevant or\ncontradictory, and essential knowledge may be excluded, exacerbating\nhallucination risks and degrading the fidelity of generated responses. To\naddress these limitations, we introduce PankRAG, a framework that combines a\nglobally aware, hierarchical query-resolution strategy with a novel\ndependency-aware reranking mechanism. PankRAG first constructs a multi-level\nresolution path that captures both parallel and sequential interdependencies\nwithin a query, guiding large language models (LLMs) through structured\nreasoning. It then applies its dependency-aware reranker to exploit the\ndependency structure among resolved sub-questions, enriching and validating\nretrieval results for subsequent sub-questions. Empirical evaluations\ndemonstrate that PankRAG consistently outperforms state-of-the-art approaches\nacross multiple benchmarks, underscoring its robustness and generalizability.", "published": "2025-06-07 07:17:14", "link": "http://arxiv.org/abs/2506.11106v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
