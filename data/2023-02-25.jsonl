{"title": "Robust language-based mental health assessments in time and space\n  through social media", "abstract": "Compared to physical health, population mental health measurement in the U.S.\nis very coarse-grained. Currently, in the largest population surveys, such as\nthose carried out by the Centers for Disease Control or Gallup, mental health\nis only broadly captured through \"mentally unhealthy days\" or \"sadness\", and\nlimited to relatively infrequent state or metropolitan estimates. Through the\nlarge scale analysis of social media data, robust estimation of population\nmental health is feasible at much higher resolutions, up to weekly estimates\nfor counties. In the present work, we validate a pipeline that uses a sample of\n1.2 billion Tweets from 2 million geo-located users to estimate mental health\nchanges for the two leading mental health conditions, depression and anxiety.\nWe find moderate to large associations between the language-based mental health\nassessments and survey scores from Gallup for multiple levels of granularity,\ndown to the county-week (fixed effects $\\beta = .25$ to $1.58$; $p<.001$).\nLanguage-based assessment allows for the cost-effective and scalable monitoring\nof population mental health at weekly time scales. Such spatially fine-grained\ntime series are well suited to monitor effects of societal events and policies\nas well as enable quasi-experimental study designs in population health and\nother disciplines. Beyond mental health in the U.S., this method generalizes to\na broad set of psychological outcomes and allows for community measurement in\nunder-resourced settings where no traditional survey measures - but social\nmedia data - are available.", "published": "2023-02-25 01:39:13", "link": "http://arxiv.org/abs/2302.12952v1", "categories": ["cs.CL", "J.4; I.2.7"], "primary_category": "cs.CL"}
{"title": "Choice Fusion as Knowledge for Zero-Shot Dialogue State Tracking", "abstract": "With the demanding need for deploying dialogue systems in new domains with\nless cost, zero-shot dialogue state tracking (DST), which tracks user's\nrequirements in task-oriented dialogues without training on desired domains,\ndraws attention increasingly. Although prior works have leveraged\nquestion-answering (QA) data to reduce the need for in-domain training in DST,\nthey fail to explicitly model knowledge transfer and fusion for tracking\ndialogue states. To address this issue, we propose CoFunDST, which is trained\non domain-agnostic QA datasets and directly uses candidate choices of\nslot-values as knowledge for zero-shot dialogue-state generation, based on a T5\npre-trained language model. Specifically, CoFunDST selects highly-relevant\nchoices to the reference context and fuses them to initialize the decoder to\nconstrain the model outputs. Our experimental results show that our proposed\nmodel achieves outperformed joint goal accuracy compared to existing zero-shot\nDST approaches in most domains on the MultiWOZ 2.1. Extensive analyses\ndemonstrate the effectiveness of our proposed approach for improving zero-shot\nDST learning from QA.", "published": "2023-02-25 07:32:04", "link": "http://arxiv.org/abs/2302.13013v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HADES: Homologous Automated Document Exploration and Summarization", "abstract": "This paper introduces HADES, a novel tool for automatic comparative documents\nwith similar structures. HADES is designed to streamline the work of\nprofessionals dealing with large volumes of documents, such as policy\ndocuments, legal acts, and scientific papers. The tool employs a multi-step\npipeline that begins with processing PDF documents using topic modeling,\nsummarization, and analysis of the most important words for each topic. The\nprocess concludes with an interactive web app with visualizations that\nfacilitate the comparison of the documents. HADES has the potential to\nsignificantly improve the productivity of professionals dealing with high\nvolumes of documents, reducing the time and effort required to complete tasks\nrelated to comparative document analysis. Our package is publically available\non GitHub.", "published": "2023-02-25 15:16:10", "link": "http://arxiv.org/abs/2302.13099v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topic-Selective Graph Network for Topic-Focused Summarization", "abstract": "Due to the success of the pre-trained language model (PLM), existing\nPLM-based summarization models show their powerful generative capability.\nHowever, these models are trained on general-purpose summarization datasets,\nleading to generated summaries failing to satisfy the needs of different\nreaders. To generate summaries with topics, many efforts have been made on\ntopic-focused summarization. However, these works generate a summary only\nguided by a prompt comprising topic words. Despite their success, these methods\nstill ignore the disturbance of sentences with non-relevant topics and only\nconduct cross-interaction between tokens by attention module. To address this\nissue, we propose a topic-arc recognition objective and topic-selective graph\nnetwork. First, the topic-arc recognition objective is used to model training,\nwhich endows the capability to discriminate topics for the model. Moreover, the\ntopic-selective graph network can conduct topic-guided cross-interaction on\nsentences based on the results of topic-arc recognition. In the experiments, we\nconduct extensive evaluations on NEWTS and COVIDET datasets. Results show that\nour methods achieve state-of-the-art performance.", "published": "2023-02-25 15:56:06", "link": "http://arxiv.org/abs/2302.13106v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MetaAID 2.0: An Extensible Framework for Developing Metaverse\n  Applications via Human-controllable Pre-trained Models", "abstract": "Pre-trained models (PM) have achieved promising results in content\ngeneration. However, the space for human creativity and imagination is endless,\nand it is still unclear whether the existing models can meet the needs.\nModel-generated content faces uncontrollable responsibility and potential\nunethical problems. This paper presents the MetaAID 2.0 framework, dedicated to\nhuman-controllable PM information flow. Through the PM information flow, humans\ncan autonomously control their creativity. Through the Universal Resource\nIdentifier extension (URI-extension), the responsibility of the model outputs\ncan be controlled. Our framework includes modules for handling multimodal data\nand supporting transformation and generation. The URI-extension consists of\nURI, detailed description, and URI embeddings, and supports fuzzy retrieval of\nmodel outputs. Based on this framework, we conduct experiments on PM\ninformation flow and URI embeddings, and the results demonstrate the good\nperformance of our system.", "published": "2023-02-25 21:42:31", "link": "http://arxiv.org/abs/2302.13173v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dependency Dialogue Acts -- Annotation Scheme and Case Study", "abstract": "In this paper, we introduce Dependency Dialogue Acts (DDA), a novel framework\nfor capturing the structure of speaker-intentions in multi-party dialogues. DDA\ncombines and adapts features from existing dialogue annotation frameworks, and\nemphasizes the multi-relational response structure of dialogues in addition to\nthe dialogue acts and rhetorical relations. It represents the functional,\ndiscourse, and response structure in multi-party multi-threaded conversations.\nA few key features distinguish DDA from existing dialogue annotation frameworks\nsuch as SWBD-DAMSL and the ISO 24617-2 standard. First, DDA prioritizes the\nrelational structure of the dialogue units and the dialog context, annotating\nboth dialog acts and rhetorical relations as response relations to particular\nutterances. Second, DDA embraces overloading in dialogues, encouraging\nannotators to specify multiple response relations and dialog acts for each\ndialog unit. Lastly, DDA places an emphasis on adequately capturing how a\nspeaker is using the full dialog context to plan and organize their speech.\nWith these features, DDA is highly expressive and recall-oriented with regard\nto conversation dynamics between multiple speakers. In what follows, we present\nthe DDA annotation framework and case studies annotating DDA structures in\nmulti-party, multi-threaded conversations.", "published": "2023-02-25 00:41:46", "link": "http://arxiv.org/abs/2302.12944v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Locale Encoding For Scalable Multilingual Keyword Spotting Models", "abstract": "A Multilingual Keyword Spotting (KWS) system detects spokenkeywords over\nmultiple locales. Conventional monolingual KWSapproaches do not scale well to\nmultilingual scenarios because ofhigh development/maintenance costs and lack of\nresource sharing.To overcome this limit, we propose two locale-conditioned\nuniversalmodels with locale feature concatenation and feature-wise\nlinearmodulation (FiLM). We compare these models with two baselinemethods:\nlocale-specific monolingual KWS, and a single universalmodel trained over all\ndata. Experiments over 10 localized languagedatasets show that\nlocale-conditioned models substantially improveaccuracy over baseline methods\nacross all locales in different noiseconditions.FiLMperformed the best,\nimproving on average FRRby 61% (relative) compared to monolingual KWS models of\nsimilarsizes.", "published": "2023-02-25 02:20:59", "link": "http://arxiv.org/abs/2302.12961v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SynGen: A Syntactic Plug-and-play Module for Generative Aspect-based\n  Sentiment Analysis", "abstract": "Aspect-based Sentiment Analysis (ABSA) is a sentiment analysis task at\nfine-grained level. Recently, generative frameworks have attracted increasing\nattention in ABSA due to their ability to unify subtasks and their continuity\nto upstream pre-training tasks. However, these generative models suffer from\nthe neighboring dependency problem that induces neighboring words to get higher\nattention. In this paper, we propose SynGen, a plug-and-play syntactic\ninformation aware module. As a plug-in module, our SynGen can be easily applied\nto any generative framework backbones. The key insight of our module is to add\nsyntactic inductive bias to attention assignment and thus direct attention to\nthe correct target words. To the best of our knowledge, we are the first one to\nintroduce syntactic information to generative ABSA frameworks. Our module\ndesign is based on two main principles: (1) maintaining the structural\nintegrity of backbone PLMs and (2) disentangling the added syntactic\ninformation and original semantic information. Empirical results on four\npopular ABSA datasets demonstrate that SynGen enhanced model achieves a\ncomparable performance to the state-of-the-art model with relaxed labeling\nspecification and less training consumption.", "published": "2023-02-25 09:10:03", "link": "http://arxiv.org/abs/2302.13032v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Human-in-the-Loop Schema Induction", "abstract": "Schema induction builds a graph representation explaining how events unfold\nin a scenario. Existing approaches have been based on information retrieval\n(IR) and information extraction(IE), often with limited human curation. We\ndemonstrate a human-in-the-loop schema induction system powered by GPT-3. We\nfirst describe the different modules of our system, including prompting to\ngenerate schematic elements, manual edit of those elements, and conversion of\nthose into a schema graph. By qualitatively comparing our system to previous\nones, we show that our system not only transfers to new domains more easily\nthan previous approaches, but also reduces efforts of human curation thanks to\nour interactive interface.", "published": "2023-02-25 10:20:02", "link": "http://arxiv.org/abs/2302.13048v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Sequential Query Encoding For Complex Query Answering on Knowledge\n  Graphs", "abstract": "Complex Query Answering (CQA) is an important and fundamental task for\nknowledge graph (KG) reasoning. Query encoding (QE) is proposed as a fast and\nrobust solution to CQA. In the encoding process, most existing QE methods first\nparse the logical query into an executable computational direct-acyclic graph\n(DAG), then use neural networks to parameterize the operators, and finally,\nrecursively execute these neuralized operators. However, the\nparameterization-and-execution paradigm may be potentially over-complicated, as\nit can be structurally simplified by a single neural network encoder.\nMeanwhile, sequence encoders, like LSTM and Transformer, proved to be effective\nfor encoding semantic graphs in related tasks. Motivated by this, we propose\nsequential query encoding (SQE) as an alternative to encode queries for CQA.\nInstead of parameterizing and executing the computational graph, SQE first uses\na search-based algorithm to linearize the computational graph to a sequence of\ntokens and then uses a sequence encoder to compute its vector representation.\nThen this vector representation is used as a query embedding to retrieve\nanswers from the embedding space according to similarity scores. Despite its\nsimplicity, SQE demonstrates state-of-the-art neural query encoding performance\non FB15k, FB15k-237, and NELL on an extended benchmark including twenty-nine\ntypes of in-distribution queries. Further experiment shows that SQE also\ndemonstrates comparable knowledge inference capability on out-of-distribution\nqueries, whose query types are not observed during the training process.", "published": "2023-02-25 16:33:53", "link": "http://arxiv.org/abs/2302.13114v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Toward Fairness in Text Generation via Mutual Information Minimization\n  based on Importance Sampling", "abstract": "Pretrained language models (PLMs), such as GPT2, have achieved remarkable\nempirical performance in text generation tasks. However, pretrained on\nlarge-scale natural language corpora, the generated text from PLMs may exhibit\nsocial bias against disadvantaged demographic groups. To improve the fairness\nof PLMs in text generation, we propose to minimize the mutual information\nbetween the semantics in the generated text sentences and their demographic\npolarity, i.e., the demographic group to which the sentence is referring. In\nthis way, the mentioning of a demographic group (e.g., male or female) is\nencouraged to be independent from how it is described in the generated text,\nthus effectively alleviating the social bias. Moreover, we propose to\nefficiently estimate the upper bound of the above mutual information via\nimportance sampling, leveraging a natural language corpus. We also propose a\ndistillation mechanism that preserves the language modeling ability of the PLMs\nafter debiasing. Empirical results on real-world benchmarks demonstrate that\nthe proposed method yields superior performance in term of both fairness and\nlanguage modeling ability.", "published": "2023-02-25 18:29:02", "link": "http://arxiv.org/abs/2302.13136v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Prompt-based Learning for Text Readability Assessment", "abstract": "We propose the novel adaptation of a pre-trained seq2seq model for\nreadability assessment. We prove that a seq2seq model - T5 or BART - can be\nadapted to discern which text is more difficult from two given texts\n(pairwise). As an exploratory study to prompt-learn a neural network for text\nreadability in a text-to-text manner, we report useful tips for future work in\nseq2seq training and ranking-based approach to readability assessment.\nSpecifically, we test nine input-output formats/prefixes and show that they can\nsignificantly influence the final model performance.\n  Also, we argue that the combination of text-to-text training and pairwise\nranking setup 1) enables leveraging multiple parallel text simplification data\nfor teaching readability and 2) trains a neural model for the general concept\nof readability (therefore, better cross-domain generalization). At last, we\nreport a 99.6% pairwise classification accuracy on Newsela and a 98.7% for\nOneStopEnglish, through a joint training approach.", "published": "2023-02-25 18:39:59", "link": "http://arxiv.org/abs/2302.13139v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Jointly Optimizing Translations and Speech Timing to Improve Isochrony\n  in Automatic Dubbing", "abstract": "Automatic dubbing (AD) is the task of translating the original speech in a\nvideo into target language speech. The new target language speech should\nsatisfy isochrony; that is, the new speech should be time aligned with the\noriginal video, including mouth movements, pauses, hand gestures, etc. In this\npaper, we propose training a model that directly optimizes both the translation\nas well as the speech duration of the generated translations. We show that this\nsystem generates speech that better matches the timing of the original speech,\ncompared to prior work, while simplifying the system architecture.", "published": "2023-02-25 04:23:25", "link": "http://arxiv.org/abs/2302.12979v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "AugGPT: Leveraging ChatGPT for Text Data Augmentation", "abstract": "Text data augmentation is an effective strategy for overcoming the challenge\nof limited sample sizes in many natural language processing (NLP) tasks. This\nchallenge is especially prominent in the few-shot learning scenario, where the\ndata in the target domain is generally much scarcer and of lowered quality. A\nnatural and widely-used strategy to mitigate such challenges is to perform data\naugmentation to better capture the data invariance and increase the sample\nsize. However, current text data augmentation methods either can't ensure the\ncorrect labeling of the generated data (lacking faithfulness) or can't ensure\nsufficient diversity in the generated data (lacking compactness), or both.\nInspired by the recent success of large language models, especially the\ndevelopment of ChatGPT, which demonstrated improved language comprehension\nabilities, in this work, we propose a text data augmentation approach based on\nChatGPT (named AugGPT). AugGPT rephrases each sentence in the training samples\ninto multiple conceptually similar but semantically different samples. The\naugmented samples can then be used in downstream model training. Experiment\nresults on few-shot learning text classification tasks show the superior\nperformance of the proposed AugGPT approach over state-of-the-art text data\naugmentation methods in terms of testing accuracy and distribution of the\naugmented samples.", "published": "2023-02-25 06:58:16", "link": "http://arxiv.org/abs/2302.13007v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Abstractive Text Summarization using Attentive GRU based Encoder-Decoder", "abstract": "In todays era huge volume of information exists everywhere. Therefore, it is\nvery crucial to evaluate that information and extract useful, and often\nsummarized, information out of it so that it may be used for relevant purposes.\nThis extraction can be achieved through a crucial technique of artificial\nintelligence, namely, machine learning. Indeed automatic text summarization has\nemerged as an important application of machine learning in text processing. In\nthis paper, an english text summarizer has been built with GRU-based encoder\nand decoder. Bahdanau attention mechanism has been added to overcome the\nproblem of handling long sequences in the input text. A news-summary dataset\nhas been used to train the model. The output is observed to outperform\ncompetitive models in the literature. The generated summary can be used as a\nnewspaper headline.", "published": "2023-02-25 16:45:46", "link": "http://arxiv.org/abs/2302.13117v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "STACC: Code Comment Classification using SentenceTransformers", "abstract": "Code comments are a key resource for information about software artefacts.\nDepending on the use case, only some types of comments are useful. Thus,\nautomatic approaches to classify these comments have been proposed. In this\nwork, we address this need by proposing, STACC, a set of\nSentenceTransformers-based binary classifiers. These lightweight classifiers\nare trained and tested on the NLBSE Code Comment Classification tool\ncompetition dataset, and surpass the baseline by a significant margin,\nachieving an average F1 score of 0.74 against the baseline of 0.31, which is an\nimprovement of 139%. A replication package, as well as the models themselves,\nare publicly available.", "published": "2023-02-25 20:24:58", "link": "http://arxiv.org/abs/2302.13149v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Cross-modal Contrastive Learning for Multimodal Fake News Detection", "abstract": "Automatic detection of multimodal fake news has gained a widespread attention\nrecently. Many existing approaches seek to fuse unimodal features to produce\nmultimodal news representations. However, the potential of powerful cross-modal\ncontrastive learning methods for fake news detection has not been well\nexploited. Besides, how to aggregate features from different modalities to\nboost the performance of the decision-making process is still an open question.\nTo address that, we propose COOLANT, a cross-modal contrastive learning\nframework for multimodal fake news detection, aiming to achieve more accurate\nimage-text alignment. To further improve the alignment precision, we leverage\nan auxiliary task to soften the loss term of negative samples during the\ncontrast process. A cross-modal fusion module is developed to learn the\ncross-modality correlations. An attention mechanism with an attention guidance\nmodule is implemented to help effectively and interpretably aggregate the\naligned unimodal representations and the cross-modality correlations. Finally,\nwe evaluate the COOLANT and conduct a comparative study on two widely used\ndatasets, Twitter and Weibo. The experimental results demonstrate that our\nCOOLANT outperforms previous approaches by a large margin and achieves new\nstate-of-the-art results on the two datasets.", "published": "2023-02-25 10:12:34", "link": "http://arxiv.org/abs/2302.14057v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Named Entity Recognition Based Automatic Generation of Research\n  Highlights", "abstract": "A scientific paper is traditionally prefaced by an abstract that summarizes\nthe paper. Recently, research highlights that focus on the main findings of the\npaper have emerged as a complementary summary in addition to an abstract.\nHowever, highlights are not yet as common as abstracts, and are absent in many\npapers. In this paper, we aim to automatically generate research highlights\nusing different sections of a research paper as input. We investigate whether\nthe use of named entity recognition on the input improves the quality of the\ngenerated highlights. In particular, we have used two deep learning-based\nmodels: the first is a pointer-generator network, and the second augments the\nfirst model with coverage mechanism. We then augment each of the above models\nwith named entity recognition features. The proposed method can be used to\nproduce highlights for papers with missing highlights. Our experiments show\nthat adding named entity information improves the performance of the deep\nlearning-based summarizers in terms of ROUGE, METEOR and BERTScore measures.", "published": "2023-02-25 16:33:03", "link": "http://arxiv.org/abs/2303.12795v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Analysis of Abstractive Text Summarization Using Pre-trained Models", "abstract": "People nowadays use search engines like Google, Yahoo, and Bing to find\ninformation on the Internet. Due to explosion in data, it is helpful for users\nif they are provided relevant summaries of the search results rather than just\nlinks to webpages. Text summarization has become a vital approach to help\nconsumers swiftly grasp vast amounts of information.In this paper, different\npre-trained models for text summarization are evaluated on different datasets.\nSpecifically, we have used three different pre-trained models, namely,\ngoogle/pegasus-cnn-dailymail, T5-base, facebook/bart-large-cnn. We have\nconsidered three different datasets, namely, CNN-dailymail, SAMSum and BillSum\nto get the output from the above three models. The pre-trained models are\ncompared over these different datasets, each of 2000 examples, through ROUGH\nand BLEU metrics.", "published": "2023-02-25 16:44:37", "link": "http://arxiv.org/abs/2303.12796v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On pitfalls (and advantages) of sophisticated large language models", "abstract": "Natural language processing based on large language models (LLMs) is a\nbooming field of AI research. After neural networks have proven to outperform\nhumans in games and practical domains based on pattern recognition, we might\nstand now at a road junction where artificial entities might eventually enter\nthe realm of human communication. However, this comes with serious risks. Due\nto the inherent limitations regarding the reliability of neural networks,\noverreliance on LLMs can have disruptive consequences. Since it will be\nincreasingly difficult to distinguish between human-written and\nmachine-generated text, one is confronted with new ethical challenges. This\nbegins with the no longer undoubtedly verifiable human authorship and continues\nwith various types of fraud, such as a new form of plagiarism. This also\nconcerns the violation of privacy rights, the possibility of circulating\ncounterfeits of humans, and, last but not least, it makes a massive spread of\nmisinformation possible.", "published": "2023-02-25 11:14:39", "link": "http://arxiv.org/abs/2303.17511v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Speaker Recognition in Realistic Scenario Using Multimodal Data", "abstract": "In recent years, an association is established between faces and voices of\ncelebrities leveraging large scale audio-visual information from YouTube. The\navailability of large scale audio-visual datasets is instrumental in developing\nspeaker recognition methods based on standard Convolutional Neural Networks.\nThus, the aim of this paper is to leverage large scale audio-visual information\nto improve speaker recognition task. To achieve this task, we proposed a\ntwo-branch network to learn joint representations of faces and voices in a\nmultimodal system. Afterwards, features are extracted from the two-branch\nnetwork to train a classifier for speaker recognition. We evaluated our\nproposed framework on a large scale audio-visual dataset named VoxCeleb$1$. Our\nresults show that addition of facial information improved the performance of\nspeaker recognition. Moreover, our results indicate that there is an overlap\nbetween face and voice.", "published": "2023-02-25 09:11:09", "link": "http://arxiv.org/abs/2302.13033v1", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Time-Variance Aware Real-Time Speech Enhancement", "abstract": "Time-variant factors often occur in real-world full-duplex communication\napplications. Some of them are caused by the complex environment such as\nnon-stationary environmental noises and varying acoustic path while some are\ncaused by the communication system such as the dynamic delay between the\nfar-end and near-end signals. Current end-to-end deep neural network (DNN)\nbased methods usually model the time-variant components implicitly and can\nhardly handle the unpredictable time-variance in real-time speech enhancement.\nTo explicitly capture the time-variant components, we propose a dynamic kernel\ngeneration (DKG) module that can be introduced as a learnable plug-in to a\nDNN-based end-to-end pipeline. Specifically, the DKG module generates a\nconvolutional kernel regarding to each input audio frame, so that the DNN model\nis able to dynamically adjust its weights according to the input signal during\ninference. Experimental results verify that DKG module improves the performance\nof the model under time-variant scenarios, in the joint acoustic echo\ncancellation (AEC) and deep noise suppression (DNS) tasks.", "published": "2023-02-25 11:37:35", "link": "http://arxiv.org/abs/2302.13063v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
