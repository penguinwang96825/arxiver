{"title": "He Thinks He Knows Better than the Doctors: BERT for Event Factuality\n  Fails on Pragmatics", "abstract": "We investigate how well BERT performs on predicting factuality in several\nexisting English datasets, encompassing various linguistic constructions.\nAlthough BERT obtains a strong performance on most datasets, it does so by\nexploiting common surface patterns that correlate with certain factuality\nlabels, and it fails on instances where pragmatic reasoning is necessary.\nContrary to what the high performance suggests, we are still far from having a\nrobust system for factuality prediction.", "published": "2021-07-02 02:47:25", "link": "http://arxiv.org/abs/2107.00807v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ClueReader: Heterogeneous Graph Attention Network for Multi-hop Machine\n  Reading Comprehension", "abstract": "Multi-hop machine reading comprehension is a challenging task in natural\nlanguage processing as it requires more reasoning ability across multiple\ndocuments. Spectral models based on graph convolutional networks have shown\ngood inferring abilities and lead to competitive results. However, the analysis\nand reasoning of some are inconsistent with those of humans. Inspired by the\nconcept of grandmother cells in cognitive neuroscience, we propose a\nheterogeneous graph attention network model named ClueReader to imitate the\ngrandmother cell concept. The model is designed to assemble the semantic\nfeatures in multi-level representations and automatically concentrate or\nalleviate information for reasoning through the attention mechanism. The name\nClueReader is a metaphor for the pattern of the model: it regards the subjects\nof queries as the starting points of clues, takes the reasoning entities as\nbridge points, considers the latent candidate entities as grandmother cells,\nand the clues end up in candidate entities. The proposed model enables the\nvisualization of the reasoning graph, making it possible to analyze the\nimportance of edges connecting entities and the selectivity in the mention and\ncandidate nodes, which is easier to comprehend empirically. Evaluations on the\nopen-domain multi-hop reading dataset WikiHop and drug-drug interaction dataset\nMedHop proved the validity of ClueReader and showed the feasibility of its\napplication of the model in the molecular biology domain.", "published": "2021-07-02 05:29:39", "link": "http://arxiv.org/abs/2107.00841v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learned Token Pruning for Transformers", "abstract": "Deploying transformer models in practice is challenging due to their\ninference cost, which scales quadratically with input sequence length. To\naddress this, we present a novel Learned Token Pruning (LTP) method which\nadaptively removes unimportant tokens as an input sequence passes through\ntransformer layers. In particular, LTP prunes tokens with an attention score\nbelow a threshold value which is learned for each layer during training. Our\nthreshold-based method allows the length of the pruned sequence to vary\nadaptively based on the input sequence, and avoids algorithmically expensive\noperations such as top-k token selection. We extensively test the performance\nof LTP on GLUE tasks and show that our method outperforms the prior\nstate-of-the-art token pruning methods by up to ~2.5% higher accuracy with the\nsame amount of FLOPs. In particular, LTP achieves up to 2.1x FLOPs reduction\nwith less than 1% accuracy drop, which results in up to 1.9x and 2.0x\nthroughput improvement on Intel Haswell CPUs and NVIDIA V100 GPUs,\nrespectively. Furthermore, we demonstrate that LTP is more robust than prior\nmethods to variations on input sentence lengths. Our code has been developed in\nPyTorch and has been open-sourced.", "published": "2021-07-02 09:00:13", "link": "http://arxiv.org/abs/2107.00910v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Concept Identification of Directly and Indirectly Related Mentions\n  Referring to Groups of Persons", "abstract": "Unsupervised concept identification through clustering, i.e., identification\nof semantically related words and phrases, is a common approach to identify\ncontextual primitives employed in various use cases, e.g., text dimension\nreduction, i.e., replace words with the concepts to reduce the vocabulary size,\nsummarization, and named entity resolution. We demonstrate the first results of\nan unsupervised approach for the identification of groups of persons as actors\nextracted from a set of related articles. Specifically, the approach clusters\nmentions of groups of persons that act as non-named entity actors in the texts,\ne.g., \"migrant families\" = \"asylum-seekers.\" Compared to our baseline, the\napproach keeps the mentions of the geopolitical entities separated, e.g., \"Iran\nleaders\" != \"European leaders,\" and clusters (in)directly related mentions with\ndiverse wording, e.g., \"American officials\" = \"Trump Administration.\"", "published": "2021-07-02 10:38:43", "link": "http://arxiv.org/abs/2107.00955v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DRIFT: A Toolkit for Diachronic Analysis of Scientific Literature", "abstract": "In this work, we present to the NLP community, and to the wider research\ncommunity as a whole, an application for the diachronic analysis of research\ncorpora. We open source an easy-to-use tool coined: DRIFT, which allows\nresearchers to track research trends and development over the years. The\nanalysis methods are collated from well-cited research works, with a few of our\nown methods added for good measure. Succinctly put, some of the analysis\nmethods are: keyword extraction, word clouds, predicting\ndeclining/stagnant/growing trends using Productivity, tracking bi-grams using\nAcceleration plots, finding the Semantic Drift of words, tracking trends using\nsimilarity, etc. To demonstrate the utility and efficacy of our tool, we\nperform a case study on the cs.CL corpus of the arXiv repository and draw\ninferences from the analysis methods. The toolkit and the associated code are\navailable here: https://github.com/rajaswa/DRIFT.", "published": "2021-07-02 17:33:25", "link": "http://arxiv.org/abs/2107.01198v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is GPT-3 Text Indistinguishable from Human Text? Scarecrow: A Framework\n  for Scrutinizing Machine Text", "abstract": "Modern neural language models can produce remarkably fluent and grammatical\ntext. So much, in fact, that recent work by Clark et al. (2021) has reported\nthat conventional crowdsourcing can no longer reliably distinguish between\nmachine-authored (GPT-3) and human-authored writing. As errors in machine\ngenerations become ever subtler and harder to spot, it poses a new challenge to\nthe research community for robust machine text evaluation. We propose a new\nframework called Scarecrow for scrutinizing machine text via crowd annotation.\nTo support the broad range of real machine errors that can be identified by\nlaypeople, the ten error categories of Scarecrow -- such as redundancy,\ncommonsense errors, and incoherence -- are identified through several rounds of\ncrowd annotation experiments without a predefined ontology. We then use\nScarecrow to collect over 41k error spans in human-written and\nmachine-generated paragraphs of English language news text. We isolate factors\nfor detailed analysis, including parameter count, training data, and various\ndecoding-time configurations. Our approach successfully quantifies measurable\ngaps between human authored text and generations from models of several sizes,\nincluding fourteen configurations of GPT-3. In addition, our analysis unveils\nnew insights, with detailed rationales provided by laypeople, e.g., that the\ncommonsense capabilities have been improving with larger models while math\ncapabilities have not, and that the choices of simple decoding hyperparameters\ncan make remarkable differences on the perceived quality of machine text. We\nrelease our training material, annotation toolkit and dataset at\nhttps://yao-dou.github.io/scarecrow/.", "published": "2021-07-02 22:37:03", "link": "http://arxiv.org/abs/2107.01294v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformer-F: A Transformer network with effective methods for learning\n  universal sentence representation", "abstract": "The Transformer model is widely used in natural language processing for\nsentence representation. However, the previous Transformer-based models focus\non function words that have limited meaning in most cases and could merely\nextract high-level semantic abstraction features. In this paper, two approaches\nare introduced to improve the performance of Transformers. We calculated the\nattention score by multiplying the part-of-speech weight vector with the\ncorrelation coefficient, which helps extract the words with more practical\nmeaning. The weight vector is obtained by the input text sequence based on the\nimportance of the part-of-speech. Furthermore, we fuse the features of each\nlayer to make the sentence representation results more comprehensive and\naccurate. In experiments, we demonstrate the effectiveness of our model\nTransformer-F on three standard text classification datasets. Experimental\nresults show that our proposed model significantly boosts the performance of\ntext classification as compared to the baseline model. Specifically, we obtain\na 5.28% relative improvement over the vanilla Transformer on the simple tasks.", "published": "2021-07-02 03:20:11", "link": "http://arxiv.org/abs/2107.00653v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Data Centric Domain Adaptation for Historical Text with OCR Errors", "abstract": "We propose new methods for in-domain and cross-domain Named Entity\nRecognition (NER) on historical data for Dutch and French. For the cross-domain\ncase, we address domain shift by integrating unsupervised in-domain data via\ncontextualized string embeddings; and OCR errors by injecting synthetic OCR\nerrors into the source domain and address data centric domain adaptation. We\npropose a general approach to imitate OCR errors in arbitrary input data. Our\ncross-domain as well as our in-domain results outperform several strong\nbaselines and establish state-of-the-art results. We publish preprocessed\nversions of the French and Dutch Europeana NER corpora.", "published": "2021-07-02 09:37:15", "link": "http://arxiv.org/abs/2107.00927v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Misinformation Detection on YouTube Using Video Captions", "abstract": "Millions of people use platforms such as YouTube, Facebook, Twitter, and\nother mass media. Due to the accessibility of these platforms, they are often\nused to establish a narrative, conduct propaganda, and disseminate\nmisinformation. This work proposes an approach that uses state-of-the-art NLP\ntechniques to extract features from video captions (subtitles). To evaluate our\napproach, we utilize a publicly accessible and labeled dataset for classifying\nvideos as misinformation or not. The motivation behind exploring video captions\nstems from our analysis of videos metadata. Attributes such as the number of\nviews, likes, dislikes, and comments are ineffective as videos are hard to\ndifferentiate using this information. Using caption dataset, the proposed\nmodels can classify videos among three classes (Misinformation, Debunking\nMisinformation, and Neutral) with 0.85 to 0.90 F1-score. To emphasize the\nrelevance of the misinformation class, we re-formulate our classification\nproblem as a two-class classification - Misinformation vs. others (Debunking\nMisinformation and Neutral). In our experiments, the proposed models can\nclassify videos with 0.92 to 0.95 F1-score and 0.78 to 0.90 AUC ROC.", "published": "2021-07-02 10:02:36", "link": "http://arxiv.org/abs/2107.00941v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "R2D2: Recursive Transformer based on Differentiable Tree for\n  Interpretable Hierarchical Language Modeling", "abstract": "Human language understanding operates at multiple levels of granularity\n(e.g., words, phrases, and sentences) with increasing levels of abstraction\nthat can be hierarchically combined. However, existing deep models with stacked\nlayers do not explicitly model any sort of hierarchical process. This paper\nproposes a recursive Transformer model based on differentiable CKY style binary\ntrees to emulate the composition process. We extend the bidirectional language\nmodel pre-training objective to this architecture, attempting to predict each\nword given its left and right abstraction nodes. To scale up our approach, we\nalso introduce an efficient pruned tree induction algorithm to enable encoding\nin just a linear number of composition steps. Experimental results on language\nmodeling and unsupervised parsing show the effectiveness of our approach.", "published": "2021-07-02 11:00:46", "link": "http://arxiv.org/abs/2107.00967v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Spoken Utterance Classification", "abstract": "An intelligent virtual assistant (IVA) enables effortless conversations in\ncall routing through spoken utterance classification (SUC) which is a special\nform of spoken language understanding (SLU). Building a SUC system requires a\nlarge amount of supervised in-domain data that is not always available. In this\npaper, we introduce an unsupervised spoken utterance classification approach\n(USUC) that does not require any in-domain data except for the intent labels\nand a few para-phrases per intent. USUC is consisting of a KNN classifier (K=1)\nand a complex embedding model trained on a large amount of unsupervised\ncustomer service corpus. Among all embedding models, we demonstrate that Elmo\nworks best for USUC. However, an Elmo model is too slow to be used at run-time\nfor call routing. To resolve this issue, first, we compute the uni- and bi-gram\nembedding vectors offline and we build a lookup table of n-grams and their\ncorresponding embedding vector. Then we use this table to compute sentence\nembedding vectors at run-time, along with back-off techniques for unseen\nn-grams. Experiments show that USUC outperforms the traditional utterance\nclassification methods by reducing the classification error rate from 32.9% to\n27.0% without requiring supervised data. Moreover, our lookup and back-off\ntechnique increases the processing speed from 16 utterances per second to 118\nutterances per second.", "published": "2021-07-02 13:22:15", "link": "http://arxiv.org/abs/2107.01068v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Ethics Sheets for AI Tasks", "abstract": "Several high-profile events, such as the mass testing of emotion recognition\nsystems on vulnerable sub-populations and using question answering systems to\nmake moral judgments, have highlighted how technology will often lead to more\nadverse outcomes for those that are already marginalized. At issue here are not\njust individual systems and datasets, but also the AI tasks themselves. In this\nposition paper, I make a case for thinking about ethical considerations not\njust at the level of individual models and datasets, but also at the level of\nAI tasks. I will present a new form of such an effort, Ethics Sheets for AI\nTasks, dedicated to fleshing out the assumptions and ethical considerations\nhidden in how a task is commonly framed and in the choices we make regarding\nthe data, method, and evaluation. I will also present a template for ethics\nsheets with 50 ethical considerations, using the task of emotion recognition as\na running example. Ethics sheets are a mechanism to engage with and document\nethical considerations before building datasets and systems. Similar to survey\narticles, a small number of ethics sheets can serve numerous researchers and\ndevelopers.", "published": "2021-07-02 16:45:40", "link": "http://arxiv.org/abs/2107.01183v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Language Identification of Hindi-English tweets using code-mixed BERT", "abstract": "Language identification of social media text has been an interesting problem\nof study in recent years. Social media messages are predominantly in code mixed\nin non-English speaking states. Prior knowledge by pre-training contextual\nembeddings have shown state of the art results for a range of downstream tasks.\nRecently, models such as BERT have shown that using a large amount of unlabeled\ndata, the pretrained language models are even more beneficial for learning\ncommon language representations. Extensive experiments exploiting transfer\nlearning and fine-tuning BERT models to identify language on Twitter are\npresented in this paper. The work utilizes a data collection of\nHindi-English-Urdu codemixed text for language pre-training and Hindi-English\ncodemixed for subsequent word-level language classification. The results show\nthat the representations pre-trained over codemixed data produce better results\nby their monolingual counterpart.", "published": "2021-07-02 17:51:36", "link": "http://arxiv.org/abs/2107.01202v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How to make qubits speak", "abstract": "This is a story about making quantum computers speak, and doing so in a\nquantum-native, compositional and meaning-aware manner. Recently we did\nquestion-answering with an actual quantum computer. We explain what we did,\nstress that this was all done in terms of pictures, and provide many pointers\nto the related literature. In fact, besides natural language, many other things\ncan be implemented in a quantum-native, compositional and meaning-aware manner,\nand we provide the reader with some indications of that broader pictorial\nlandscape, including our account on the notion of compositionality. We also\nprovide some guidance for the actual execution, so that the reader can give it\na go as well.", "published": "2021-07-02 13:34:38", "link": "http://arxiv.org/abs/2107.06776v1", "categories": ["quant-ph", "cs.CL"], "primary_category": "quant-ph"}
{"title": "Case Relation Transformer: A Crossmodal Language Generation Model for\n  Fetching Instructions", "abstract": "There have been many studies in robotics to improve the communication skills\nof domestic service robots. Most studies, however, have not fully benefited\nfrom recent advances in deep neural networks because the training datasets are\nnot large enough. In this paper, our aim is to augment the datasets based on a\ncrossmodal language generation model. We propose the Case Relation Transformer\n(CRT), which generates a fetching instruction sentence from an image, such as\n\"Move the blue flip-flop to the lower left box.\" Unlike existing methods, the\nCRT uses the Transformer to integrate the visual features and geometry features\nof objects in the image. The CRT can handle the objects because of the Case\nRelation Block. We conducted comparison experiments and a human evaluation. The\nexperimental results show the CRT outperforms baseline methods.", "published": "2021-07-02 01:40:33", "link": "http://arxiv.org/abs/2107.00789v1", "categories": ["cs.RO", "cs.CL", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Target-dependent UNITER: A Transformer-Based Multimodal Language\n  Comprehension Model for Domestic Service Robots", "abstract": "Currently, domestic service robots have an insufficient ability to interact\nnaturally through language. This is because understanding human instructions is\ncomplicated by various ambiguities and missing information. In existing\nmethods, the referring expressions that specify the relationships between\nobjects are insufficiently modeled. In this paper, we propose Target-dependent\nUNITER, which learns the relationship between the target object and other\nobjects directly by focusing on the relevant regions within an image, rather\nthan the whole image. Our method is an extension of the UNITER-based\nTransformer that can be pretrained on general-purpose datasets. We extend the\nUNITER approach by introducing a new architecture for handling the target\ncandidates. Our model is validated on two standard datasets, and the results\nshow that Target-dependent UNITER outperforms the baseline method in terms of\nclassification accuracy.", "published": "2021-07-02 03:11:02", "link": "http://arxiv.org/abs/2107.00811v1", "categories": ["cs.RO", "cs.CL", "cs.CV"], "primary_category": "cs.RO"}
{"title": "SocialAI: Benchmarking Socio-Cognitive Abilities in Deep Reinforcement\n  Learning Agents", "abstract": "Building embodied autonomous agents capable of participating in social\ninteractions with humans is one of the main challenges in AI. Within the Deep\nReinforcement Learning (DRL) field, this objective motivated multiple works on\nembodied language use. However, current approaches focus on language as a\ncommunication tool in very simplified and non-diverse social situations: the\n\"naturalness\" of language is reduced to the concept of high vocabulary size and\nvariability. In this paper, we argue that aiming towards human-level AI\nrequires a broader set of key social skills: 1) language use in complex and\nvariable social contexts; 2) beyond language, complex embodied communication in\nmultimodal settings within constantly evolving social worlds. We explain how\nconcepts from cognitive sciences could help AI to draw a roadmap towards\nhuman-like intelligence, with a focus on its social dimensions. As a first\nstep, we propose to expand current research to a broader set of core social\nskills. To do this, we present SocialAI, a benchmark to assess the acquisition\nof social skills of DRL agents using multiple grid-world environments featuring\nother (scripted) social agents. We then study the limits of a recent SOTA DRL\napproach when tested on SocialAI and discuss important next steps towards\nproficient social agents. Videos and code are available at\nhttps://sites.google.com/view/socialai.", "published": "2021-07-02 10:39:18", "link": "http://arxiv.org/abs/2107.00956v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "DUKweb: Diachronic word representations from the UK Web Archive corpus", "abstract": "Lexical semantic change (detecting shifts in the meaning and usage of words)\nis an important task for social and cultural studies as well as for Natural\nLanguage Processing applications. Diachronic word embeddings (time-sensitive\nvector representations of words that preserve their meaning) have become the\nstandard resource for this task. However, given the significant computational\nresources needed for their generation, very few resources exist that make\ndiachronic word embeddings available to the scientific community.\n  In this paper we present DUKweb, a set of large-scale resources designed for\nthe diachronic analysis of contemporary English. DUKweb was created from the\nJISC UK Web Domain Dataset (1996-2013), a very large archive which collects\nresources from the Internet Archive that were hosted on domains ending in\n`.uk'. DUKweb consists of a series word co-occurrence matrices and two types of\nword embeddings for each year in the JISC UK Web Domain dataset. We show the\nreuse potential of DUKweb and its quality standards via a case study on word\nmeaning change detection.", "published": "2021-07-02 13:32:33", "link": "http://arxiv.org/abs/2107.01076v2", "categories": ["cs.CL", "cs.DL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Relaxed Attention: A Simple Method to Boost Performance of End-to-End\n  Automatic Speech Recognition", "abstract": "Recently, attention-based encoder-decoder (AED) models have shown high\nperformance for end-to-end automatic speech recognition (ASR) across several\ntasks. Addressing overconfidence in such models, in this paper we introduce the\nconcept of relaxed attention, which is a simple gradual injection of a uniform\ndistribution to the encoder-decoder attention weights during training that is\neasily implemented with two lines of code. We investigate the effect of relaxed\nattention across different AED model architectures and two prominent ASR tasks,\nWall Street Journal (WSJ) and Librispeech. We found that transformers trained\nwith relaxed attention outperform the standard baseline models consistently\nduring decoding with external language models. On WSJ, we set a new benchmark\nfor transformer-based end-to-end speech recognition with a word error rate of\n3.65%, outperforming state of the art (4.20%) by 13.1% relative, while\nintroducing only a single hyperparameter.", "published": "2021-07-02 21:01:17", "link": "http://arxiv.org/abs/2107.01275v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Supervised Contrastive Learning for Accented Speech Recognition", "abstract": "Neural network based speech recognition systems suffer from performance\ndegradation due to accented speech, especially unfamiliar accents. In this\npaper, we study the supervised contrastive learning framework for accented\nspeech recognition. To build different views (similar \"positive\" data samples)\nfor contrastive learning, three data augmentation techniques including noise\ninjection, spectrogram augmentation and TTS-same-sentence generation are\nfurther investigated. From the experiments on the Common Voice dataset, we have\nshown that contrastive learning helps to build data-augmentation invariant and\npronunciation invariant representations, which significantly outperforms\ntraditional joint training methods in both zero-shot and full-shot settings.\nExperiments show that contrastive learning can improve accuracy by 3.66%\n(zero-shot) and 3.78% (full-shot) on average, comparing to the joint training\nmethod.", "published": "2021-07-02 09:23:33", "link": "http://arxiv.org/abs/2107.00921v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CrowdSpeech and VoxDIY: Benchmark Datasets for Crowdsourced Audio\n  Transcription", "abstract": "Domain-specific data is the crux of the successful transfer of machine\nlearning systems from benchmarks to real life. In simple problems such as image\nclassification, crowdsourcing has become one of the standard tools for cheap\nand time-efficient data collection: thanks in large part to advances in\nresearch on aggregation methods. However, the applicability of crowdsourcing to\nmore complex tasks (e.g., speech recognition) remains limited due to the lack\nof principled aggregation methods for these modalities. The main obstacle\ntowards designing aggregation methods for more advanced applications is the\nabsence of training data, and in this work, we focus on bridging this gap in\nspeech recognition. For this, we collect and release CrowdSpeech -- the first\npublicly available large-scale dataset of crowdsourced audio transcriptions.\nEvaluation of existing and novel aggregation methods on our data shows room for\nimprovement, suggesting that our work may entail the design of better\nalgorithms. At a higher level, we also contribute to the more general challenge\nof developing the methodology for reliable data collection via crowdsourcing.\nIn that, we design a principled pipeline for constructing datasets of\ncrowdsourced audio transcriptions in any novel domain. We show its\napplicability on an under-resourced language by constructing VoxDIY -- a\ncounterpart of CrowdSpeech for the Russian language. We also release the code\nthat allows a full replication of our data collection pipeline and share\nvarious insights on best practices of data collection via crowdsourcing.", "published": "2021-07-02 14:05:28", "link": "http://arxiv.org/abs/2107.01091v2", "categories": ["cs.SD", "cs.HC", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-user VoiceFilter-Lite via Attentive Speaker Embedding", "abstract": "In this paper, we propose a solution to allow speaker conditioned speech\nmodels, such as VoiceFilter-Lite, to support an arbitrary number of enrolled\nusers in a single pass. This is achieved by using an attention mechanism on\nmultiple speaker embeddings to compute a single attentive embedding, which is\nthen used as a side input to the model. We implemented multi-user\nVoiceFilter-Lite and evaluated it for three tasks: (1) a streaming automatic\nspeech recognition (ASR) task; (2) a text-independent speaker verification\ntask; and (3) a personalized keyphrase detection task, where ASR has to detect\nkeyphrases from multiple enrolled users in a noisy environment. Our experiments\nshow that, with up to four enrolled users, multi-user VoiceFilter-Lite is able\nto significantly reduce speech recognition and speaker verification errors when\nthere is overlapping speech, without affecting performance under other acoustic\nconditions. This attentive speaker embedding approach can also be easily\napplied to other speaker-conditioned models such as personal VAD and\npersonalized ASR.", "published": "2021-07-02 17:45:37", "link": "http://arxiv.org/abs/2107.01201v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Dual Causal/Non-Causal Self-Attention for Streaming End-to-End Speech\n  Recognition", "abstract": "Attention-based end-to-end automatic speech recognition (ASR) systems have\nrecently demonstrated state-of-the-art results for numerous tasks. However, the\napplication of self-attention and attention-based encoder-decoder models\nremains challenging for streaming ASR, where each word must be recognized\nshortly after it was spoken. In this work, we present the dual\ncausal/non-causal self-attention (DCN) architecture, which in contrast to\nrestricted self-attention prevents the overall context to grow beyond the\nlook-ahead of a single layer when used in a deep architecture. DCN is compared\nto chunk-based and restricted self-attention using streaming transformer and\nconformer architectures, showing improved ASR performance over restricted\nself-attention and competitive ASR results compared to chunk-based\nself-attention, while providing the advantage of frame-synchronous processing.\nCombined with triggered attention, the proposed streaming end-to-end ASR\nsystems obtained state-of-the-art results on the LibriSpeech, HKUST, and\nSwitchboard ASR tasks.", "published": "2021-07-02 20:56:13", "link": "http://arxiv.org/abs/2107.01269v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
