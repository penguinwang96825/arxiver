{"title": "Multi-Perspective Context Aggregation for Semi-supervised Cloze-style\n  Reading Comprehension", "abstract": "Cloze-style reading comprehension has been a popular task for measuring the\nprogress of natural language understanding in recent years. In this paper, we\ndesign a novel multi-perspective framework, which can be seen as the joint\ntraining of heterogeneous experts and aggregate context information from\ndifferent perspectives. Each perspective is modeled by a simple aggregation\nmodule. The outputs of multiple aggregation modules are fed into a one-timestep\npointer network to get the final answer. At the same time, to tackle the\nproblem of insufficient labeled data, we propose an efficient sampling\nmechanism to automatically generate more training examples by matching the\ndistribution of candidates between labeled and unlabeled data. We conduct our\nexperiments on a recently released cloze-test dataset CLOTH (Xie et al., 2017),\nwhich consists of nearly 100k questions designed by professional teachers.\nResults show that our method achieves new state-of-the-art performance over\nprevious strong baselines.", "published": "2018-08-20 02:36:55", "link": "http://arxiv.org/abs/1808.06289v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question Generation from SQL Queries Improves Neural Semantic Parsing", "abstract": "We study how to learn a semantic parser of state-of-the-art accuracy with\nless supervised training data. We conduct our study on WikiSQL, the largest\nhand-annotated semantic parsing dataset to date. First, we demonstrate that\nquestion generation is an effective method that empowers us to learn a\nstate-of-the-art neural network based semantic parser with thirty percent of\nthe supervised training data. Second, we show that applying question generation\nto the full supervised training data further improves the state-of-the-art\nmodel. In addition, we observe that there is a logarithmic relationship between\nthe accuracy of a semantic parser and the amount of training data.", "published": "2018-08-20 04:39:58", "link": "http://arxiv.org/abs/1808.06304v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Post-Processing of Word Representations via Variance Normalization and\n  Dynamic Embedding", "abstract": "Although embedded vector representations of words offer impressive\nperformance on many natural language processing (NLP) applications, the\ninformation of ordered input sequences is lost to some extent if only\ncontext-based samples are used in the training. For further performance\nimprovement, two new post-processing techniques, called post-processing via\nvariance normalization (PVN) and post-processing via dynamic embedding (PDE),\nare proposed in this work. The PVN method normalizes the variance of principal\ncomponents of word vectors while the PDE method learns orthogonal latent\nvariables from ordered input sequences. The PVN and the PDE methods can be\nintegrated to achieve better performance. We apply these post-processing\ntechniques to two popular word embedding methods (i.e., word2vec and GloVe) to\nyield their post-processed representations. Extensive experiments are conducted\nto demonstrate the effectiveness of the proposed post-processing techniques.", "published": "2018-08-20 04:51:33", "link": "http://arxiv.org/abs/1808.06305v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive Document Retrieval for Deep Question Answering", "abstract": "State-of-the-art systems in deep question answering proceed as follows: (1)\nan initial document retrieval selects relevant documents, which (2) are then\nprocessed by a neural network in order to extract the final answer. Yet the\nexact interplay between both components is poorly understood, especially\nconcerning the number of candidate documents that should be retrieved. We show\nthat choosing a static number of documents -- as used in prior research --\nsuffers from a noise-information trade-off and yields suboptimal results. As a\nremedy, we propose an adaptive document retrieval model. This learns the\noptimal candidate number for document retrieval, conditional on the size of the\ncorpus and the query. We report extensive experimental results showing that our\nadaptive approach outperforms state-of-the-art methods on multiple benchmark\ndatasets, as well as in the context of corpora with variable sizes.", "published": "2018-08-20 15:53:32", "link": "http://arxiv.org/abs/1808.06528v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Watset: Local-Global Graph Clustering with Applications in Sense and\n  Frame Induction", "abstract": "We present a detailed theoretical and computational analysis of the Watset\nmeta-algorithm for fuzzy graph clustering, which has been found to be widely\napplicable in a variety of domains. This algorithm creates an intermediate\nrepresentation of the input graph that reflects the \"ambiguity\" of its nodes.\nThen, it uses hard clustering to discover clusters in this \"disambiguated\"\nintermediate graph. After outlining the approach and analyzing its\ncomputational complexity, we demonstrate that Watset shows competitive results\nin three applications: unsupervised synset induction from a synonymy graph,\nunsupervised semantic frame induction from dependency triples, and unsupervised\nsemantic class induction from a distributional thesaurus. Our algorithm is\ngeneric and can be also applied to other networks of linguistic data.", "published": "2018-08-20 21:06:01", "link": "http://arxiv.org/abs/1808.06696v4", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Goal-oriented Dialogue Policy Learning from Failures", "abstract": "Reinforcement learning methods have been used for learning dialogue policies.\nHowever, learning an effective dialogue policy frequently requires\nprohibitively many conversations. This is partly because of the sparse rewards\nin dialogues, and the very few successful dialogues in early learning phase.\nHindsight experience replay (HER) enables learning from failures, but the\nvanilla HER is inapplicable to dialogue learning due to the implicit goals. In\nthis work, we develop two complex HER methods providing different trade-offs\nbetween complexity and performance, and, for the first time, enabled HER-based\ndialogue policy learning. Experiments using a realistic user simulator show\nthat our HER methods perform better than existing experience replay methods (as\napplied to deep Q-networks) in learning rate.", "published": "2018-08-20 15:04:30", "link": "http://arxiv.org/abs/1808.06497v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "State-of-the-art Chinese Word Segmentation with Bi-LSTMs", "abstract": "A wide variety of neural-network architectures have been proposed for the\ntask of Chinese word segmentation.\n  Surprisingly, we find that a bidirectional LSTM model, when combined with\nstandard deep learning techniques and best practices, can achieve better\naccuracy on many of the popular datasets as compared to models based on more\ncomplex neural-network architectures.\n  Furthermore, our error analysis shows that out-of-vocabulary words remain\nchallenging for neural-network models, and many of the remaining errors are\nunlikely to be fixed through architecture changes.\n  Instead, more effort should be made on exploring resources for further\nimprovement.", "published": "2018-08-20 15:19:38", "link": "http://arxiv.org/abs/1808.06511v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Detecting cognitive impairments by agreeing on interpretations of\n  linguistic features", "abstract": "Linguistic features have shown promising applications for detecting various\ncognitive impairments. To improve detection accuracies, increasing the amount\nof data or the number of linguistic features have been two applicable\napproaches. However, acquiring additional clinical data can be expensive, and\nhand-crafting features is burdensome. In this paper, we take a third approach,\nproposing Consensus Networks (CNs), a framework to classify after reaching\nagreements between modalities. We divide linguistic features into\nnon-overlapping subsets according to their modalities, and let neural networks\nlearn low-dimensional representations that agree with each other. These\nrepresentations are passed into a classifier network. All neural networks are\noptimized iteratively.\n  In this paper, we also present two methods that improve the performance of\nCNs. We then present ablation studies to illustrate the effectiveness of\nmodality division. To understand further what happens in CNs, we visualize the\nrepresentations during training. Overall, using all of the 413 linguistic\nfeatures, our models significantly outperform traditional classifiers, which\nare used by the state-of-the-art papers.", "published": "2018-08-20 17:05:46", "link": "http://arxiv.org/abs/1808.06570v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multimodal speech synthesis architecture for unsupervised speaker\n  adaptation", "abstract": "This paper proposes a new architecture for speaker adaptation of\nmulti-speaker neural-network speech synthesis systems, in which an unseen\nspeaker's voice can be built using a relatively small amount of speech data\nwithout transcriptions. This is sometimes called \"unsupervised speaker\nadaptation\". More specifically, we concatenate the layers to the audio inputs\nwhen performing unsupervised speaker adaptation while we concatenate them to\nthe text inputs when synthesizing speech from text. Two new training schemes\nfor the new architecture are also proposed in this paper. These training\nschemes are not limited to speech synthesis, other applications are suggested.\nExperimental results show that the proposed model not only enables adaptation\nto unseen speakers using untranscribed speech but it also improves the\nperformance of multi-speaker modeling and speaker adaptation using transcribed\naudio files.", "published": "2018-08-20 02:36:19", "link": "http://arxiv.org/abs/1808.06288v1", "categories": ["eess.AS", "cs.CL", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Adversarial Removal of Demographic Attributes from Text Data", "abstract": "Recent advances in Representation Learning and Adversarial Training seem to\nsucceed in removing unwanted features from the learned representation. We show\nthat demographic information of authors is encoded in -- and can be recovered\nfrom -- the intermediate representations learned by text-based neural\nclassifiers. The implication is that decisions of classifiers trained on\ntextual data are not agnostic to -- and likely condition on -- demographic\nattributes. When attempting to remove such demographic information using\nadversarial training, we find that while the adversarial component achieves\nchance-level development-set accuracy during training, a post-hoc classifier,\ntrained on the encoded sentences from the first part, still manages to reach\nsubstantially higher classification accuracies on the same data. This behavior\nis consistent across several tasks, demographic properties and datasets. We\nexplore several techniques to improve the effectiveness of the adversarial\ncomponent. Our main conclusion is a cautionary one: do not rely on the\nadversarial training to achieve invariant representation to sensitive features.", "published": "2018-08-20 18:20:01", "link": "http://arxiv.org/abs/1808.06640v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Deep Residual Network for Sound Source Localization in the Time Domain", "abstract": "This study presents a system for sound source localization in time domain\nusing a deep residual neural network. Data from the linear 8 channel microphone\narray with 3 cm spacing is used by the network for direction estimation. We\npropose to use the deep residual network for sound source localization\nconsidering the localization task as a classification task. This study\ndescribes the gathered dataset and developed architecture of the neural\nnetwork. We will show the training process and its result in this study. The\ndeveloped system was tested on validation part of the dataset and on new data\ncapture in real time. The accuracy classification of 30 m sec sound frames is\n99.2%. The standard deviation of sound source localization is 4{\\deg}. The\nproposed method of sound source localization was tested inside of speech\nrecognition pipeline. Its usage decreased word error rate by 1.14% in\ncomparison with similar speech recognition pipeline using GCC-PHAT sound source\nlocalization.", "published": "2018-08-20 12:54:51", "link": "http://arxiv.org/abs/1808.06429v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "R-CRNN: Region-based Convolutional Recurrent Neural Network for Audio\n  Event Detection", "abstract": "This paper proposes a Region-based Convolutional Recurrent Neural Network\n(R-CRNN) for audio event detection (AED). The proposed network is inspired by\nFaster-RCNN, a well known region-based convolutional network framework for\nvisual object detection. Different from the original Faster-RCNN, a recurrent\nlayer is added on top of the convolutional network to capture the long-term\ntemporal context from the extracted high level features. While most of the\nprevious works on AED generate predictions at frame level first, and then use\npost-processing to predict the onset/offset timestamps of events from a\nprobability sequence; the proposed method generates predictions at event level\ndirectly and can be trained end-to-end with a multitask loss, which optimizes\nthe classification and localization of audio events simultaneously. The\nproposed method is tested on DCASE 2017 Challenge dataset. To the best of our\nknowledge, R-CRNN is the best performing single-model method among all methods\nwithout using ensembles both on development and evaluation sets. Compared to\nthe other region-based network for AED (R-FCN) with an event-based error rate\n(ER) of 0.18 on the development set, our method reduced the ER to half.", "published": "2018-08-20 18:01:52", "link": "http://arxiv.org/abs/1808.06627v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A simple model for detection of rare sound events", "abstract": "We propose a simple recurrent model for detecting rare sound events, when the\ntime boundaries of events are available for training. Our model optimizes the\ncombination of an utterance-level loss, which classifies whether an event\noccurs in an utterance, and a frame-level loss, which classifies whether each\nframe corresponds to the event when it does occur. The two losses make use of a\nshared vectorial representation the event, and are connected by an attention\nmechanism. We demonstrate our model on Task 2 of the DCASE 2017 challenge, and\nachieve competitive performance.", "published": "2018-08-20 19:59:09", "link": "http://arxiv.org/abs/1808.06676v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Fast Spectrogram Inversion using Multi-head Convolutional Neural\n  Networks", "abstract": "We propose the multi-head convolutional neural network (MCNN) architecture\nfor waveform synthesis from spectrograms. Nonlinear interpolation in MCNN is\nemployed with transposed convolution layers in parallel heads. MCNN achieves\nmore than an order of magnitude higher compute intensity than commonly-used\niterative algorithms like Griffin-Lim, yielding efficient utilization for\nmodern multi-core processors, and very fast (more than 300x real-time) waveform\nsynthesis. For training of MCNN, we use a large-scale speech recognition\ndataset and losses defined on waveforms that are related to perceptual audio\nquality. We demonstrate that MCNN constitutes a very promising approach for\nhigh-quality speech synthesis, without any iterative algorithms or\nautoregression in computations.", "published": "2018-08-20 23:19:48", "link": "http://arxiv.org/abs/1808.06719v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
