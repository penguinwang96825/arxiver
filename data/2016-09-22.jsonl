{"title": "Generating Abstractive Summaries from Meeting Transcripts", "abstract": "Summaries of meetings are very important as they convey the essential content\nof discussions in a concise form. Generally, it is time consuming to read and\nunderstand the whole documents. Therefore, summaries play an important role as\nthe readers are interested in only the important context of discussions. In\nthis work, we address the task of meeting document summarization. Automatic\nsummarization systems on meeting conversations developed so far have been\nprimarily extractive, resulting in unacceptable summaries that are hard to\nread. The extracted utterances contain disfluencies that affect the quality of\nthe extractive summaries. To make summaries much more readable, we propose an\napproach to generating abstractive summaries by fusing important content from\nseveral utterances. We first separate meeting transcripts into various topic\nsegments, and then identify the important utterances in each segment using a\nsupervised learning approach. The important utterances are then combined\ntogether to generate a one-sentence summary. In the text generation step, the\ndependency parses of the utterances in each segment are combined together to\ncreate a directed graph. The most informative and well-formed sub-graph\nobtained by integer linear programming (ILP) is selected to generate a\none-sentence summary for each topic segment. The ILP formulation reduces\ndisfluencies by leveraging grammatical relations that are more prominent in\nnon-conversational style of text, and therefore generates summaries that is\ncomparable to human-written abstractive summaries. Experimental results show\nthat our method can generate more informative summaries than the baselines. In\naddition, readability assessments by human judges as well as log-likelihood\nestimates obtained from the dependency parser show that our generated summaries\nare significantly readable and well-formed.", "published": "2016-09-22 15:50:50", "link": "http://arxiv.org/abs/1609.07033v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-document abstractive summarization using ILP based multi-sentence\n  compression", "abstract": "Abstractive summarization is an ideal form of summarization since it can\nsynthesize information from multiple documents to create concise informative\nsummaries. In this work, we aim at developing an abstractive summarizer. First,\nour proposed approach identifies the most important document in the\nmulti-document set. The sentences in the most important document are aligned to\nsentences in other documents to generate clusters of similar sentences. Second,\nwe generate K-shortest paths from the sentences in each cluster using a\nword-graph structure. Finally, we select sentences from the set of shortest\npaths generated from all the clusters employing a novel integer linear\nprogramming (ILP) model with the objective of maximizing information content\nand readability of the final summary. Our ILP model represents the shortest\npaths as binary variables and considers the length of the path, information\nscore and linguistic quality score in the objective function. Experimental\nresults on the DUC 2004 and 2005 multi-document summarization datasets show\nthat our proposed approach outperforms all the baselines and state-of-the-art\nextractive summarizers as measured by the ROUGE scores. Our method also\noutperforms a recent abstractive summarization technique. In manual evaluation,\nour approach also achieves promising results on informativeness and\nreadability.", "published": "2016-09-22 15:51:43", "link": "http://arxiv.org/abs/1609.07034v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Abstractive Meeting Summarization UsingDependency Graph Fusion", "abstract": "Automatic summarization techniques on meeting conversations developed so far\nhave been primarily extractive, resulting in poor summaries. To improve this,\nwe propose an approach to generate abstractive summaries by fusing important\ncontent from several utterances. Any meeting is generally comprised of several\ndiscussion topic segments. For each topic segment within a meeting\nconversation, we aim to generate a one sentence summary from the most important\nutterances using an integer linear programming-based sentence fusion approach.\nExperimental results show that our method can generate more informative\nsummaries than the baselines.", "published": "2016-09-22 15:53:04", "link": "http://arxiv.org/abs/1609.07035v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Tagging with Deep Residual Networks", "abstract": "We propose a novel semantic tagging task, sem-tagging, tailored for the\npurpose of multilingual semantic parsing, and present the first tagger using\ndeep residual networks (ResNets). Our tagger uses both word and character\nrepresentations and includes a novel residual bypass architecture. We evaluate\nthe tagset both intrinsically on the new task of semantic tagging, as well as\non Part-of-Speech (POS) tagging. Our system, consisting of a ResNet and an\nauxiliary loss function predicting our semantic tags, significantly outperforms\nprior results on English Universal Dependencies POS tagging (95.71% accuracy on\nUD v1.2 and 95.67% accuracy on UD v1.3).", "published": "2016-09-22 16:34:00", "link": "http://arxiv.org/abs/1609.07053v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Representation via Joint Learning of Sequential Text and\n  Knowledge Graphs", "abstract": "Textual information is considered as significant supplement to knowledge\nrepresentation learning (KRL). There are two main challenges for constructing\nknowledge representations from plain texts: (1) How to take full advantages of\nsequential contexts of entities in plain texts for KRL. (2) How to dynamically\nselect those informative sentences of the corresponding entities for KRL. In\nthis paper, we propose the Sequential Text-embodied Knowledge Representation\nLearning to build knowledge representations from multiple sentences. Given each\nreference sentence of an entity, we first utilize recurrent neural network with\npooling or long short-term memory network to encode the semantic information of\nthe sentence with respect to the entity. Then we further design an attention\nmodel to measure the informativeness of each sentence, and build text-based\nrepresentations of entities. We evaluate our method on two tasks, including\ntriple classification and link prediction. Experimental results demonstrate\nthat our method outperforms other baselines on both tasks, which indicates that\nour method is capable of selecting informative sentences and encoding the\ntextual information well into knowledge representations.", "published": "2016-09-22 17:16:43", "link": "http://arxiv.org/abs/1609.07075v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Image-embodied Knowledge Representation Learning", "abstract": "Entity images could provide significant visual information for knowledge\nrepresentation learning. Most conventional methods learn knowledge\nrepresentations merely from structured triples, ignoring rich visual\ninformation extracted from entity images. In this paper, we propose a novel\nImage-embodied Knowledge Representation Learning model (IKRL), where knowledge\nrepresentations are learned with both triple facts and images. More\nspecifically, we first construct representations for all images of an entity\nwith a neural image encoder. These image representations are then integrated\ninto an aggregated image-based representation via an attention-based method. We\nevaluate our IKRL models on knowledge graph completion and triple\nclassification. Experimental results demonstrate that our models outperform all\nbaselines on both tasks, which indicates the significance of visual information\nfor knowledge representations and the capability of our models in learning\nknowledge representations with images.", "published": "2016-09-22 15:37:45", "link": "http://arxiv.org/abs/1609.07028v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Nonparametric Bayesian Topic Modelling with the Hierarchical Pitman-Yor\n  Processes", "abstract": "The Dirichlet process and its extension, the Pitman-Yor process, are\nstochastic processes that take probability distributions as a parameter. These\nprocesses can be stacked up to form a hierarchical nonparametric Bayesian\nmodel. In this article, we present efficient methods for the use of these\nprocesses in this hierarchical context, and apply them to latent variable\nmodels for text analytics. In particular, we propose a general framework for\ndesigning these Bayesian models, which are called topic models in the computer\nscience community. We then propose a specific nonparametric Bayesian topic\nmodel for modelling text from social media. We focus on tweets (posts on\nTwitter) in this article due to their ease of access. We find that our\nnonparametric model performs better than existing parametric models in both\ngoodness of fit and real world applications.", "published": "2016-09-22 00:10:16", "link": "http://arxiv.org/abs/1609.06783v1", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Twitter-Network Topic Model: A Full Bayesian Treatment for Social\n  Network and Text Modeling", "abstract": "Twitter data is extremely noisy -- each tweet is short, unstructured and with\ninformal language, a challenge for current topic modeling. On the other hand,\ntweets are accompanied by extra information such as authorship, hashtags and\nthe user-follower network. Exploiting this additional information, we propose\nthe Twitter-Network (TN) topic model to jointly model the text and the social\nnetwork in a full Bayesian nonparametric way. The TN topic model employs the\nhierarchical Poisson-Dirichlet processes (PDP) for text modeling and a Gaussian\nprocess random function model for social network modeling. We show that the TN\ntopic model significantly outperforms several existing nonparametric models due\nto its flexibility. Moreover, the TN topic model enables additional informative\ninference such as authors' interests, hashtag analysis, as well as leading to\nfurther applications such as author recommendation, automatic topic labeling\nand hashtag suggestion. Note our general inference framework can readily be\napplied to other topic models with embedded PDP nodes.", "published": "2016-09-22 01:08:31", "link": "http://arxiv.org/abs/1609.06791v1", "categories": ["cs.CL", "cs.IR", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Conversational Recommendation System with Unsupervised Learning", "abstract": "We will demonstrate a conversational products recommendation agent. This\nsystem shows how we combine research in personalized recommendation systems\nwith research in dialogue systems to build a virtual sales agent. Based on new\ndeep learning technologies we developed, the virtual agent is capable of\nlearning how to interact with users, how to answer user questions, what is the\nnext question to ask, and what to recommend when chatting with a human user.\n  Normally a descent conversational agent for a particular domain requires tens\nof thousands of hand labeled conversational data or hand written rules. This is\na major barrier when launching a conversation agent for a new domain. We will\nexplore and demonstrate the effectiveness of the learning solution even when\nthere is no hand written rules or hand labeled training data.", "published": "2016-09-22 05:46:49", "link": "http://arxiv.org/abs/1610.01546v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
