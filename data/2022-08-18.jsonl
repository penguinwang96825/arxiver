{"title": "MulZDG: Multilingual Code-Switching Framework for Zero-shot Dialogue\n  Generation", "abstract": "Building dialogue generation systems in a zero-shot scenario remains a huge\nchallenge, since the typical zero-shot approaches in dialogue generation rely\nheavily on large-scale pre-trained language generation models such as GPT-3 and\nT5. The research on zero-shot dialogue generation without cumbersome language\nmodels is limited due to lacking corresponding parallel dialogue corpora. In\nthis paper, we propose a simple but effective Multilingual learning framework\nfor Zero-shot Dialogue Generation (dubbed as MulZDG) that can effectively\ntransfer knowledge from an English corpus with large-scale training samples to\na non-English corpus with zero samples. Besides, MulZDG can be viewed as a\nmultilingual data augmentation method to improve the performance of the\nresource-rich language. First, we construct multilingual code-switching\ndialogue datasets via translation utterances randomly selected from monolingual\nEnglish datasets. Then we employ MulZDG to train a unified multilingual\ndialogue model based on the code-switching datasets. The MulZDG can conduct\nimplicit semantic alignment between different languages. Experiments on\nDailyDialog and DSTC7 datasets demonstrate that MulZDG not only achieve\ncompetitive performance under zero-shot case compared to training with\nsufficient examples but also greatly improve the performance of the source\nlanguage.", "published": "2022-08-18 04:28:20", "link": "http://arxiv.org/abs/2208.08629v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Two-Phase Paradigm for Joint Entity-Relation Extraction", "abstract": "An exhaustive study has been conducted to investigate span-based models for\nthe joint entity and relation extraction task. However, these models sample a\nlarge number of negative entities and negative relations during the model\ntraining, which are essential but result in grossly imbalanced data\ndistributions and in turn cause suboptimal model performance. In order to\naddress the above issues, we propose a two-phase paradigm for the span-based\njoint entity and relation extraction, which involves classifying the entities\nand relations in the first phase, and predicting the types of these entities\nand relations in the second phase. The two-phase paradigm enables our model to\nsignificantly reduce the data distribution gap, including the gap between\nnegative entities and other entities, as well as the gap between negative\nrelations and other relations. In addition, we make the first attempt at\ncombining entity type and entity distance as global features, which has proven\neffective, especially for the relation extraction. Experimental results on\nseveral datasets demonstrate that the spanbased joint extraction model\naugmented with the two-phase paradigm and the global features consistently\noutperforms previous state-of-the-art span-based models for the joint\nextraction task, establishing a new standard benchmark. Qualitative and\nquantitative analyses further validate the effectiveness the proposed paradigm\nand the global features.", "published": "2022-08-18 06:40:25", "link": "http://arxiv.org/abs/2208.08659v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Open Information Extraction from Rule-based Model to Large\n  Language Model", "abstract": "Open Information Extraction (OpenIE) represents a crucial NLP task aimed at\nderiving structured information from unstructured text, unrestricted by\nrelation type or domain. This survey paper provides an overview of OpenIE\ntechnologies spanning from 2007 to 2024, emphasizing a chronological\nperspective absent in prior surveys. It examines the evolution of task settings\nin OpenIE to align with the advances in recent technologies. The paper\ncategorizes OpenIE approaches into rule-based, neural, and pre-trained large\nlanguage models, discussing each within a chronological framework.\nAdditionally, it highlights prevalent datasets and evaluation metrics currently\nin use. Building on this extensive review, the paper outlines potential future\ndirections in terms of datasets, information sources, output formats,\nmethodologies, and evaluation metrics.", "published": "2022-08-18 08:03:45", "link": "http://arxiv.org/abs/2208.08690v7", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding Interpersonal Conflict Types and their Impact on\n  Perception Classification", "abstract": "Studies on interpersonal conflict have a long history and contain many\nsuggestions for conflict typology. We use this as the basis of a novel\nannotation scheme and release a new dataset of situations and conflict aspect\nannotations. We then build a classifier to predict whether someone will\nperceive the actions of one individual as right or wrong in a given situation.\nOur analyses include conflict aspects, but also generated clusters, which are\nhuman validated, and show differences in conflict content based on the\nrelationship of participants to the author. Our findings have important\nimplications for understanding conflict and social norms.", "published": "2022-08-18 10:39:35", "link": "http://arxiv.org/abs/2208.08758v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Sentiment and Common Sense for Zero-shot Stance Detection", "abstract": "The stance detection task aims to classify the stance toward given documents\nand topics. Since the topics can be implicit in documents and unseen in\ntraining data for zero-shot settings, we propose to boost the transferability\nof the stance detection model by using sentiment and commonsense knowledge,\nwhich are seldom considered in previous studies. Our model includes a graph\nautoencoder module to obtain commonsense knowledge and a stance detection\nmodule with sentiment and commonsense. Experimental results show that our model\noutperforms the state-of-the-art methods on the zero-shot and few-shot\nbenchmark dataset--VAST. Meanwhile, ablation studies prove the significance of\neach module in our model. Analysis of the relations between sentiment, common\nsense, and stance indicates the effectiveness of sentiment and common sense.", "published": "2022-08-18 12:27:24", "link": "http://arxiv.org/abs/2208.08797v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Brand Celebrity Matching Model Based on Natural Language Processing", "abstract": "Celebrity Endorsement is one of the most significant strategies in brand\ncommunication. Nowadays, more and more companies try to build a vivid\ncharacteristic for themselves. Therefore, their brand identity communications\nshould accord with some characteristics as humans and regulations. However, the\nprevious works mostly stop by assumptions, instead of proposing a specific way\nto perform matching between brands and celebrities. In this paper, we propose a\nbrand celebrity matching model (BCM) based on Natural Language Processing (NLP)\ntechniques. Given a brand and a celebrity, we firstly obtain some descriptive\ndocuments of them from the Internet, then summarize these documents, and\nfinally calculate a matching degree between the brand and the celebrity to\ndetermine whether they are matched. According to the experimental result, our\nproposed model outperforms the best baselines with a 0.362 F1 score and 6.3% of\naccuracy, which indicates the effectiveness and application value of our model\nin the real-world scene. What's more, to our best knowledge, the proposed BCM\nmodel is the first work on using NLP to solve endorsement issues, so it can\nprovide some novel research ideas and methodologies for the following works.", "published": "2022-08-18 15:07:14", "link": "http://arxiv.org/abs/2208.08887v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ered: Enhanced Text Representations with Entities and Descriptions", "abstract": "External knowledge,e.g., entities and entity descriptions, can help humans\nunderstand texts. Many works have been explored to include external knowledge\nin the pre-trained models. These methods, generally, design pre-training tasks\nand implicitly introduce knowledge by updating model weights, alternatively,\nuse it straightforwardly together with the original text. Though effective,\nthere are some limitations. On the one hand, it is implicit and only model\nweights are paid attention to, the pre-trained entity embeddings are ignored.\nOn the other hand, entity descriptions may be lengthy, and inputting into the\nmodel together with the original text may distract the model's attention. This\npaper aims to explicitly include both entities and entity descriptions in the\nfine-tuning stage. First, the pre-trained entity embeddings are fused with the\noriginal text representation and updated by the backbone model layer by layer.\nSecond, descriptions are represented by the knowledge module outside the\nbackbone model, and each knowledge layer is selectively connected to one\nbackbone layer for fusing. Third, two knowledge-related auxiliary tasks, i.e.,\nentity/description enhancement and entity enhancement/pollution task, are\ndesigned to smooth the semantic gaps among evolved representations. We\nconducted experiments on four knowledge-oriented tasks and two common tasks,\nand the results achieved new state-of-the-art on several datasets. Besides, we\nconduct an ablation study to show that each module in our method is necessary.\nThe code is available at https://github.com/lshowway/Ered.", "published": "2022-08-18 16:51:16", "link": "http://arxiv.org/abs/2208.08954v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Kind Introduction to Lexical and Grammatical Aspect, with a Survey of\n  Computational Approaches", "abstract": "Aspectual meaning refers to how the internal temporal structure of situations\nis presented. This includes whether a situation is described as a state or as\nan event, whether the situation is finished or ongoing, and whether it is\nviewed as a whole or with a focus on a particular phase. This survey gives an\noverview of computational approaches to modeling lexical and grammatical aspect\nalong with intuitive explanations of the necessary linguistic concepts and\nterminology. In particular, we describe the concepts of stativity, telicity,\nhabituality, perfective and imperfective, as well as influential inventories of\neventuality and situation types. We argue that because aspect is a crucial\ncomponent of semantics, especially when it comes to reporting the temporal\nstructure of situations in a precise way, future NLP approaches need to be able\nto handle and evaluate it systematically in order to achieve human-level\nlanguage understanding.", "published": "2022-08-18 18:22:42", "link": "http://arxiv.org/abs/2208.09012v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mere Contrastive Learning for Cross-Domain Sentiment Analysis", "abstract": "Cross-domain sentiment analysis aims to predict the sentiment of texts in the\ntarget domain using the model trained on the source domain to cope with the\nscarcity of labeled data. Previous studies are mostly cross-entropy-based\nmethods for the task, which suffer from instability and poor generalization. In\nthis paper, we explore contrastive learning on the cross-domain sentiment\nanalysis task. We propose a modified contrastive objective with in-batch\nnegative samples so that the sentence representations from the same class will\nbe pushed close while those from the different classes become further apart in\nthe latent space. Experiments on two widely used datasets show that our model\ncan achieve state-of-the-art performance in both cross-domain and multi-domain\nsentiment analysis tasks. Meanwhile, visualizations demonstrate the\neffectiveness of transferring knowledge learned in the source domain to the\ntarget domain and the adversarial test verifies the robustness of our model.", "published": "2022-08-18 07:25:55", "link": "http://arxiv.org/abs/2208.08678v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Active PETs: Active Data Annotation Prioritisation for Few-Shot Claim\n  Verification with Pattern Exploiting Training", "abstract": "To mitigate the impact of the scarcity of labelled data on fact-checking\nsystems, we focus on few-shot claim verification. Despite recent work on\nfew-shot classification by proposing advanced language models, there is a\ndearth of research in data annotation prioritisation that improves the\nselection of the few shots to be labelled for optimal model performance. We\npropose Active PETs, a novel weighted approach that utilises an ensemble of\nPattern Exploiting Training (PET) models based on various language models, to\nactively select unlabelled data as candidates for annotation. Using Active PETs\nfor few-shot data selection shows consistent improvement over the baseline\nmethods, on two technical fact-checking datasets and using six different\npretrained language models. We show further improvement with Active PETs-o,\nwhich further integrates an oversampling strategy. Our approach enables\neffective selection of instances to be labelled where unlabelled data is\nabundant but resources for labelling are limited, leading to consistently\nimproved few-shot claim verification performance. Our code is available.", "published": "2022-08-18 10:11:36", "link": "http://arxiv.org/abs/2208.08749v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring and Exploiting Multi-Granularity Representations for Machine\n  Reading Comprehension", "abstract": "Recently, the attention-enhanced multi-layer encoder, such as Transformer,\nhas been extensively studied in Machine Reading Comprehension (MRC). To predict\nthe answer, it is common practice to employ a predictor to draw information\nonly from the final encoder layer which generates the coarse-grained\nrepresentations of the source sequences, i.e., passage and question. The\nanalysis shows that the representation of source sequence becomes more\ncoarse-grained from finegrained as the encoding layer increases. It is\ngenerally believed that with the growing number of layers in deep neural\nnetworks, the encoding process will gather relevant information for each\nlocation increasingly, resulting in more coarse-grained representations, which\nadds the likelihood of similarity to other locations (referring to\nhomogeneity). Such phenomenon will mislead the model to make wrong judgement\nand degrade the performance. In this paper, we argue that it would be better if\nthe predictor could exploit representations of different granularity from the\nencoder, providing different views of the source sequences, such that the\nexpressive power of the model could be fully utilized. To this end, we propose\na novel approach called Adaptive Bidirectional Attention-Capsule Network\n(ABA-Net), which adaptively exploits the source representations of different\nlevels to the predictor. Furthermore, due to the better representations are at\nthe core for boosting MRC performance, the capsule network and self-attention\nmodule are carefully designed as the building blocks of our encoders, which\nprovides the capability to explore the local and global representations,\nrespectively. Experimental results on three benchmark datasets, i.e., SQuAD\n1.0, SQuAD 2.0 and COQA, demonstrate the effectiveness of our approach. In\nparticular, we set the new state-of-the-art performance on the SQuAD 1.0\ndataset", "published": "2022-08-18 10:14:32", "link": "http://arxiv.org/abs/2208.08750v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CASE: Aligning Coarse-to-Fine Cognition and Affection for Empathetic\n  Response Generation", "abstract": "Empathetic conversation is psychologically supposed to be the result of\nconscious alignment and interaction between the cognition and affection of\nempathy. However, existing empathetic dialogue models usually consider only the\naffective aspect or treat cognition and affection in isolation, which limits\nthe capability of empathetic response generation. In this work, we propose the\nCASE model for empathetic dialogue generation. It first builds upon a\ncommonsense cognition graph and an emotional concept graph and then aligns the\nuser's cognition and affection at both the coarse-grained and fine-grained\nlevels. Through automatic and manual evaluation, we demonstrate that CASE\noutperforms state-of-the-art baselines of empathetic dialogues and can generate\nmore empathetic and informative responses.", "published": "2022-08-18 14:28:38", "link": "http://arxiv.org/abs/2208.08845v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Treeformer: Dense Gradient Trees for Efficient Attention Computation", "abstract": "Standard inference and training with transformer based architectures scale\nquadratically with input sequence length. This is prohibitively large for a\nvariety of applications especially in web-page translation, query-answering\netc. Consequently, several approaches have been developed recently to speedup\nattention computation by enforcing different attention structures such as\nsparsity, low-rank, approximating attention using kernels. In this work, we\nview attention computation as that of nearest neighbor retrieval, and use\ndecision tree based hierarchical navigation to reduce the retrieval cost per\nquery token from linear in sequence length to nearly logarithmic. Based on such\nhierarchical navigation, we design Treeformer which can use one of two\nefficient attention layers -- TF-Attention and TC-Attention. TF-Attention\ncomputes the attention in a fine-grained style, while TC-Attention is a coarse\nattention layer which also ensures that the gradients are \"dense\". To optimize\nsuch challenging discrete layers, we propose a two-level bootstrapped training\nmethod. Using extensive experiments on standard NLP benchmarks, especially for\nlong-sequences, we demonstrate that our Treeformer architecture can be almost\nas accurate as baseline Transformer while using 30x lesser FLOPs in the\nattention layer. Compared to Linformer, the accuracy can be as much as 12%\nhigher while using similar FLOPs in the attention layer.", "published": "2022-08-18 18:31:40", "link": "http://arxiv.org/abs/2208.09015v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GSRFormer: Grounded Situation Recognition Transformer with Alternate\n  Semantic Attention Refinement", "abstract": "Grounded Situation Recognition (GSR) aims to generate structured semantic\nsummaries of images for \"human-like\" event understanding. Specifically, GSR\ntask not only detects the salient activity verb (e.g. buying), but also\npredicts all corresponding semantic roles (e.g. agent and goods). Inspired by\nobject detection and image captioning tasks, existing methods typically employ\na two-stage framework: 1) detect the activity verb, and then 2) predict\nsemantic roles based on the detected verb. Obviously, this illogical framework\nconstitutes a huge obstacle to semantic understanding. First, pre-detecting\nverbs solely without semantic roles inevitably fails to distinguish many\nsimilar daily activities (e.g., offering and giving, buying and selling).\nSecond, predicting semantic roles in a closed auto-regressive manner can hardly\nexploit the semantic relations among the verb and roles. To this end, in this\npaper we propose a novel two-stage framework that focuses on utilizing such\nbidirectional relations within verbs and roles. In the first stage, instead of\npre-detecting the verb, we postpone the detection step and assume a pseudo\nlabel, where an intermediate representation for each corresponding semantic\nrole is learned from images. In the second stage, we exploit transformer layers\nto unearth the potential semantic relations within both verbs and semantic\nroles. With the help of a set of support images, an alternate learning scheme\nis designed to simultaneously optimize the results: update the verb using nouns\ncorresponding to the image, and update nouns using verbs from support images.\nExtensive experimental results on challenging SWiG benchmarks show that our\nrenovated framework outperforms other state-of-the-art methods under various\nmetrics.", "published": "2022-08-18 17:13:59", "link": "http://arxiv.org/abs/2208.08965v4", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "VAuLT: Augmenting the Vision-and-Language Transformer for Sentiment\n  Classification on Social Media", "abstract": "We propose the Vision-and-Augmented-Language Transformer (VAuLT). VAuLT is an\nextension of the popular Vision-and-Language Transformer (ViLT), and improves\nperformance on vision-and-language (VL) tasks that involve more complex text\ninputs than image captions while having minimal impact on training and\ninference efficiency. ViLT, importantly, enables efficient training and\ninference in VL tasks, achieved by encoding images using a linear projection of\npatches instead of an object detector. However, it is pretrained on captioning\ndatasets, where the language input is simple, literal, and descriptive,\ntherefore lacking linguistic diversity. So, when working with multimedia data\nin the wild, such as multimodal social media data, there is a notable shift\nfrom captioning language data, as well as diversity of tasks. We indeed find\nevidence that the language capacity of ViLT is lacking. The key insight and\nnovelty of VAuLT is to propagate the output representations of a large language\nmodel (LM) like BERT to the language input of ViLT. We show that joint training\nof the LM and ViLT can yield relative improvements up to 20% over ViLT and\nachieve state-of-the-art or comparable performance on VL tasks involving richer\nlanguage inputs and affective constructs, such as for Target-Oriented Sentiment\nClassification in TWITTER-2015 and TWITTER-2017, and Sentiment Classification\nin MVSA-Single and MVSA-Multiple. Our code is available at\nhttps://github.com/gchochla/VAuLT.", "published": "2022-08-18 18:51:13", "link": "http://arxiv.org/abs/2208.09021v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Using Large Language Models to Simulate Multiple Humans and Replicate\n  Human Subject Studies", "abstract": "We introduce a new type of test, called a Turing Experiment (TE), for\nevaluating to what extent a given language model, such as GPT models, can\nsimulate different aspects of human behavior. A TE can also reveal consistent\ndistortions in a language model's simulation of a specific human behavior.\nUnlike the Turing Test, which involves simulating a single arbitrary\nindividual, a TE requires simulating a representative sample of participants in\nhuman subject research. We carry out TEs that attempt to replicate\nwell-established findings from prior studies. We design a methodology for\nsimulating TEs and illustrate its use to compare how well different language\nmodels are able to reproduce classic economic, psycholinguistic, and social\npsychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock\nExperiment, and Wisdom of Crowds. In the first three TEs, the existing findings\nwere replicated using recent models, while the last TE reveals a\n\"hyper-accuracy distortion\" present in some language models (including ChatGPT\nand GPT-4), which could affect downstream applications in education and the\narts.", "published": "2022-08-18 17:54:49", "link": "http://arxiv.org/abs/2208.10264v5", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The DialPort tools", "abstract": "The DialPort project http://dialport.org/, funded by the National Science\nFoundation (NSF), covers a group of tools and services that aim at fulfilling\nthe needs of the dialog research community. Over the course of six years,\nseveral offerings have been created, including the DialPort Portal and\nDialCrowd. This paper describes these contributions, which will be demoed at\nSIGDIAL, including implementation, prior studies, corresponding discoveries,\nand the locations at which the tools will remain freely available to the\ncommunity going forward.", "published": "2022-08-18 19:22:36", "link": "http://arxiv.org/abs/2208.10918v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Musika! Fast Infinite Waveform Music Generation", "abstract": "Fast and user-controllable music generation could enable novel ways of\ncomposing or performing music. However, state-of-the-art music generation\nsystems require large amounts of data and computational resources for training,\nand are slow at inference. This makes them impractical for real-time\ninteractive use. In this work, we introduce Musika, a music generation system\nthat can be trained on hundreds of hours of music using a single consumer GPU,\nand that allows for much faster than real-time generation of music of arbitrary\nlength on a consumer CPU. We achieve this by first learning a compact\ninvertible representation of spectrogram magnitudes and phases with adversarial\nautoencoders, then training a Generative Adversarial Network (GAN) on this\nrepresentation for a particular music domain. A latent coordinate system\nenables generating arbitrarily long sequences of excerpts in parallel, while a\nglobal context vector allows the music to remain stylistically coherent through\ntime. We perform quantitative evaluations to assess the quality of the\ngenerated samples and showcase options for user control in piano and techno\nmusic generation. We release the source code and pretrained autoencoder weights\nat github.com/marcoppasini/musika, such that a GAN can be trained on a new\nmusic domain with a single GPU in a matter of hours.", "published": "2022-08-18 08:31:15", "link": "http://arxiv.org/abs/2208.08706v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech Representation Disentanglement with Adversarial Mutual\n  Information Learning for One-shot Voice Conversion", "abstract": "One-shot voice conversion (VC) with only a single target speaker's speech for\nreference has become a hot research topic. Existing works generally disentangle\ntimbre, while information about pitch, rhythm and content is still mixed\ntogether. To perform one-shot VC effectively with further disentangling these\nspeech components, we employ random resampling for pitch and content encoder\nand use the variational contrastive log-ratio upper bound of mutual information\nand gradient reversal layer based adversarial mutual information learning to\nensure the different parts of the latent space containing only the desired\ndisentangled representation during training. Experiments on the VCTK dataset\nshow the model achieves state-of-the-art performance for one-shot VC in terms\nof naturalness and intellgibility. In addition, we can transfer characteristics\nof one-shot VC on timbre, pitch and rhythm separately by speech representation\ndisentanglement. Our code, pre-trained models and demo are available at\nhttps://im1eon.github.io/IS2022-SRDVC/.", "published": "2022-08-18 10:36:27", "link": "http://arxiv.org/abs/2208.08757v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deploying Enhanced Speech Feature Decreased Audio Complaints at SVT Play\n  VOD Service", "abstract": "At Public Service Broadcaster SVT in Sweden, background music and sounds in\nprograms have for many years been one of the most common complaints from the\nviewers. The most sensitive group are people with hearing disabilities, but\nmany others also find background sounds annoying. To address this problem SVT\nhas added Enhanced Speech, a feature with lower background noise, to a number\nof TV programs in VOD service SVT Play. As a result, when the number of\nprograms with the Enhanced Speech feature increased, the level of audio\ncomplaints to customer service decreased. The Enhanced Speech feature got the\nrating 8.3/10 in a survey with 86 participants. The rating for possible future\nusage was 9.0/10. In this article we describe this feature's design and\ndevelopment process, as well as its technical specification, limitations and\nfuture development opportunities.", "published": "2022-08-18 17:05:00", "link": "http://arxiv.org/abs/2208.08960v1", "categories": ["cs.SD", "cs.MM", "eess.AS", "q-bio.NC"], "primary_category": "cs.SD"}
{"title": "Representation Learning for the Automatic Indexing of Sound Effects\n  Libraries", "abstract": "Labeling and maintaining a commercial sound effects library is a\ntime-consuming task exacerbated by databases that continually grow in size and\nundergo taxonomy updates. Moreover, sound search and taxonomy creation are\ncomplicated by non-uniform metadata, an unrelenting problem even with the\nintroduction of a new industry standard, the Universal Category System. To\naddress these problems and overcome dataset-dependent limitations that inhibit\nthe successful training of deep learning models, we pursue representation\nlearning to train generalized embeddings that can be used for a wide variety of\nsound effects libraries and are a taxonomy-agnostic representation of sound. We\nshow that a task-specific but dataset-independent representation can\nsuccessfully address data issues such as class imbalance, inconsistent class\nlabels, and insufficient dataset size, outperforming established\nrepresentations such as OpenL3. Detailed experimental results show the impact\nof metric learning approaches and different cross-dataset training methods on\nrepresentational effectiveness.", "published": "2022-08-18 23:46:13", "link": "http://arxiv.org/abs/2208.09096v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
