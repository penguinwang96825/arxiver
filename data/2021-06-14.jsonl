{"title": "Toward Diverse Precondition Generation", "abstract": "Language understanding must identify the logical connections between events\nin a discourse, but core events are often unstated due to their commonsense\nnature. This paper fills in these missing events by generating precondition\nevents. Precondition generation can be framed as a sequence-to-sequence\nproblem: given a target event, generate a possible precondition. However, in\nmost real-world scenarios, an event can have several preconditions, requiring\ndiverse generation -- a challenge for standard seq2seq approaches. We propose\nDiP, a Diverse Precondition generation system that can generate unique and\ndiverse preconditions. DiP uses a generative process with three components --\nan event sampler, a candidate generator, and a post-processor. The event\nsampler provides control codes (precondition triggers) which the candidate\ngenerator uses to focus its generation. Unlike other conditional generation\nsystems, DiP automatically generates control codes without training on diverse\nexamples. Analysis against baselines reveals that DiP improves the diversity of\npreconditions significantly while also generating more preconditions.", "published": "2021-06-14 00:33:29", "link": "http://arxiv.org/abs/2106.07117v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Why Can You Lay Off Heads? Investigating How BERT Heads Transfer", "abstract": "The huge size of the widely used BERT family models has led to recent efforts\nabout model distillation. The main goal of distillation is to create a\ntask-agnostic pre-trained model that can be fine-tuned on downstream tasks\nwithout fine-tuning its full-sized version. Despite the progress of\ndistillation, to what degree and for what reason a task-agnostic model can be\ncreated from distillation has not been well studied. Also, the mechanisms\nbehind transfer learning of those BERT models are not well investigated either.\nTherefore, this work focuses on analyzing the acceptable deduction when\ndistillation for guiding the future distillation procedure. Specifically, we\nfirst inspect the prunability of the Transformer heads in RoBERTa and ALBERT\nusing their head importance estimation proposed by Michel et al. (2019), and\nthen check the coherence of the important heads between the pre-trained task\nand downstream tasks. Hence, the acceptable deduction of performance on the\npre-trained task when distilling a model can be derived from the results, and\nwe further compare the behavior of the pruned model before and after\nfine-tuning. Our studies provide guidance for future directions about BERT\nfamily model distillation.", "published": "2021-06-14 02:27:47", "link": "http://arxiv.org/abs/2106.07137v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Mutual Information Maximization Approach for the Spurious Solution\n  Problem in Weakly Supervised Question Answering", "abstract": "Weakly supervised question answering usually has only the final answers as\nsupervision signals while the correct solutions to derive the answers are not\nprovided. This setting gives rise to the spurious solution problem: there may\nexist many spurious solutions that coincidentally derive the correct answer,\nbut training on such solutions can hurt model performance (e.g., producing\nwrong solutions or answers). For example, for discrete reasoning tasks as on\nDROP, there may exist many equations to derive a numeric answer, and typically\nonly one of them is correct. Previous learning methods mostly filter out\nspurious solutions with heuristics or using model confidence, but do not\nexplicitly exploit the semantic correlations between a question and its\nsolution. In this paper, to alleviate the spurious solution problem, we propose\nto explicitly exploit such semantic correlations by maximizing the mutual\ninformation between question-answer pairs and predicted solutions. Extensive\nexperiments on four question answering datasets show that our method\nsignificantly outperforms previous learning methods in terms of task\nperformance and is more effective in training models to produce correct\nsolutions.", "published": "2021-06-14 05:47:41", "link": "http://arxiv.org/abs/2106.07174v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Document Sketching: Generating Drafts from Analogous Texts", "abstract": "The advent of large pre-trained language models has made it possible to make\nhigh-quality predictions on how to add or change a sentence in a document.\nHowever, the high branching factor inherent to text generation impedes the\nability of even the strongest language models to offer useful editing\nsuggestions at a more global or document level. We introduce a new task,\ndocument sketching, which involves generating entire draft documents for the\nwriter to review and revise. These drafts are built from sets of documents that\noverlap in form - sharing large segments of potentially reusable text - while\ndiverging in content. To support this task, we introduce a Wikipedia-based\ndataset of analogous documents and investigate the application of weakly\nsupervised methods, including use of a transformer-based mixture of experts,\ntogether with reinforcement learning. We report experiments using automated and\nhuman evaluation methods and discuss relative merits of these models.", "published": "2021-06-14 06:46:06", "link": "http://arxiv.org/abs/2106.07192v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Straight to the Gradient: Learning to Use Novel Tokens for Neural Text\n  Generation", "abstract": "Advanced large-scale neural language models have led to significant success\nin many language generation tasks. However, the most commonly used training\nobjective, Maximum Likelihood Estimation (MLE), has been shown problematic,\nwhere the trained model prefers using dull and repetitive phrases. In this\nwork, we introduce ScaleGrad, a modification straight to the gradient of the\nloss function, to remedy the degeneration issue of the standard MLE objective.\nBy directly maneuvering the gradient information, ScaleGrad makes the model\nlearn to use novel tokens. Empirical results show the effectiveness of our\nmethod not only in open-ended generation, but also in directed generation\ntasks. With the simplicity in architecture, our method can serve as a general\ntraining objective that is applicable to most of the neural text generation\ntasks.", "published": "2021-06-14 07:46:30", "link": "http://arxiv.org/abs/2106.07207v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mitigating Biases in Toxic Language Detection through Invariant\n  Rationalization", "abstract": "Automatic detection of toxic language plays an essential role in protecting\nsocial media users, especially minority groups, from verbal abuse. However,\nbiases toward some attributes, including gender, race, and dialect, exist in\nmost training datasets for toxicity detection. The biases make the learned\nmodels unfair and can even exacerbate the marginalization of people.\nConsidering that current debiasing methods for general natural language\nunderstanding tasks cannot effectively mitigate the biases in the toxicity\ndetectors, we propose to use invariant rationalization (InvRat), a\ngame-theoretic framework consisting of a rationale generator and a predictor,\nto rule out the spurious correlation of certain syntactic patterns (e.g.,\nidentity mentions, dialect) to toxicity labels. We empirically show that our\nmethod yields lower false positive rate in both lexical and dialectal\nattributes than previous debiasing methods.", "published": "2021-06-14 08:49:52", "link": "http://arxiv.org/abs/2106.07240v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contemporary Amharic Corpus: Automatically Morpho-Syntactically Tagged\n  Amharic Corpus", "abstract": "We introduced the contemporary Amharic corpus, which is automatically tagged\nfor morpho-syntactic information. Texts are collected from 25,199 documents\nfrom different domains and about 24 million orthographic words are tokenized.\nSince it is partly a web corpus, we made some automatic spelling error\ncorrection. We have also modified the existing morphological analyzer,\nHornMorpho, to use it for the automatic tagging.", "published": "2021-06-14 08:49:52", "link": "http://arxiv.org/abs/2106.07241v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Determinantal Beam Search", "abstract": "Beam search is a go-to strategy for decoding neural sequence models. The\nalgorithm can naturally be viewed as a subset optimization problem, albeit one\nwhere the corresponding set function does not reflect interactions between\ncandidates. Empirically, this leads to sets often exhibiting high overlap,\ne.g., strings may differ by only a single word. Yet in use-cases that call for\nmultiple solutions, a diverse or representative set is often desired. To\naddress this issue, we propose a reformulation of beam search, which we call\ndeterminantal beam search. Determinantal beam search has a natural relationship\nto determinantal point processes (DPPs), models over sets that inherently\nencode intra-set interactions. By posing iterations in beam search as a series\nof subdeterminant maximization problems, we can turn the algorithm into a\ndiverse subset selection process. In a case study, we use the string\nsubsequence kernel to explicitly encourage n-gram coverage in text generated\nfrom a sequence model. We observe that our algorithm offers competitive\nperformance against other diverse set generation strategies in the context of\nlanguage generation, while providing a more general approach to optimizing for\ndiversity.", "published": "2021-06-14 13:01:46", "link": "http://arxiv.org/abs/2106.07400v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Profanity and Hate Speech in Social Media with Semantic\n  Subspaces", "abstract": "Hate speech and profanity detection suffer from data sparsity, especially for\nlanguages other than English, due to the subjective nature of the tasks and the\nresulting annotation incompatibility of existing corpora. In this study, we\nidentify profane subspaces in word and sentence representations and explore\ntheir generalization capability on a variety of similar and distant target\ntasks in a zero-shot setting. This is done monolingually (German) and\ncross-lingually to closely-related (English), distantly-related (French) and\nnon-related (Arabic) tasks. We observe that, on both similar and distant target\ntasks and across all languages, the subspace-based representations transfer\nmore effectively than standard BERT representations in the zero-shot setting,\nwith improvements between F1 +10.9 and F1 +42.9 over the baselines across all\ntested monolingual and cross-lingual scenarios.", "published": "2021-06-14 15:34:37", "link": "http://arxiv.org/abs/2106.07505v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Biomedical Entity Linking with Contrastive Context Matching", "abstract": "We introduce BioCoM, a contrastive learning framework for biomedical entity\nlinking that uses only two resources: a small-sized dictionary and a large\nnumber of raw biomedical articles. Specifically, we build the training\ninstances from raw PubMed articles by dictionary matching and use them to train\na context-aware entity linking model with contrastive learning. We predict the\nnormalized biomedical entity at inference time through a nearest-neighbor\nsearch. Results found that BioCoM substantially outperforms state-of-the-art\nmodels, especially in low-resource settings, by effectively using the context\nof the entities.", "published": "2021-06-14 16:43:33", "link": "http://arxiv.org/abs/2106.07583v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing the Use of Prosody in Constituency Parsing of Imperfect\n  Transcripts", "abstract": "This work explores constituency parsing on automatically recognized\ntranscripts of conversational speech. The neural parser is based on a sentence\nencoder that leverages word vectors contextualized with prosodic features,\njointly learning prosodic feature extraction with parsing. We assess the\nutility of the prosody in parsing on imperfect transcripts, i.e. transcripts\nwith automatic speech recognition (ASR) errors, by applying the parser in an\nN-best reranking framework. In experiments on Switchboard, we obtain 13-15% of\nthe oracle N-best gain relative to parsing the 1-best ASR output, with\ninsignificant impact on word recognition error rate. Prosody provides a\nsignificant part of the gain, and analyses suggest that it leads to more\ngrammatical utterances via recovering function words.", "published": "2021-06-14 23:05:59", "link": "http://arxiv.org/abs/2106.07794v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Launching into clinical space with medspaCy: a new clinical text\n  processing toolkit in Python", "abstract": "Despite impressive success of machine learning algorithms in clinical natural\nlanguage processing (cNLP), rule-based approaches still have a prominent role.\nIn this paper, we introduce medspaCy, an extensible, open-source cNLP library\nbased on spaCy framework that allows flexible integration of rule-based and\nmachine learning-based algorithms adapted to clinical text. MedspaCy includes a\nvariety of components that meet common cNLP needs such as context analysis and\nmapping to standard terminologies. By utilizing spaCy's clear and easy-to-use\nconventions, medspaCy enables development of custom pipelines that integrate\neasily with other spaCy-based modules. Our toolkit includes several core\ncomponents and facilitates rapid development of pipelines for clinical text.", "published": "2021-06-14 23:19:13", "link": "http://arxiv.org/abs/2106.07799v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GPT3-to-plan: Extracting plans from text using GPT-3", "abstract": "Operations in many essential industries including finance and banking are\noften characterized by the need to perform repetitive sequential tasks. Despite\ntheir criticality to the business, workflows are rarely fully automated or even\nformally specified, though there may exist a number of natural language\ndocuments describing these procedures for the employees of the company. Plan\nextraction methods provide us with the possibility of extracting structure\nplans from such natural language descriptions of the plans/workflows, which\ncould then be leveraged by an automated system. In this paper, we investigate\nthe utility of generalized language models in performing such extractions\ndirectly from such texts. Such models have already been shown to be quite\neffective in multiple translation tasks, and our initial results seem to point\nto their effectiveness also in the context of plan extractions. Particularly,\nwe show that GPT-3 is able to generate plan extraction results that are\ncomparable to many of the current state of the art plan extraction methods.", "published": "2021-06-14 01:45:47", "link": "http://arxiv.org/abs/2106.07131v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pre-Trained Models: Past, Present and Future", "abstract": "Large-scale pre-trained models (PTMs) such as BERT and GPT have recently\nachieved great success and become a milestone in the field of artificial\nintelligence (AI). Owing to sophisticated pre-training objectives and huge\nmodel parameters, large-scale PTMs can effectively capture knowledge from\nmassive labeled and unlabeled data. By storing knowledge into huge parameters\nand fine-tuning on specific tasks, the rich knowledge implicitly encoded in\nhuge parameters can benefit a variety of downstream tasks, which has been\nextensively demonstrated via experimental verification and empirical analysis.\nIt is now the consensus of the AI community to adopt PTMs as backbone for\ndownstream tasks rather than learning models from scratch. In this paper, we\ntake a deep look into the history of pre-training, especially its special\nrelation with transfer learning and self-supervised learning, to reveal the\ncrucial position of PTMs in the AI development spectrum. Further, we\ncomprehensively review the latest breakthroughs of PTMs. These breakthroughs\nare driven by the surge of computational power and the increasing availability\nof data, towards four important directions: designing effective architectures,\nutilizing rich contexts, improving computational efficiency, and conducting\ninterpretation and theoretical analysis. Finally, we discuss a series of open\nproblems and research directions of PTMs, and hope our view can inspire and\nadvance the future study of PTMs.", "published": "2021-06-14 02:40:32", "link": "http://arxiv.org/abs/2106.07139v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "SAS: Self-Augmentation Strategy for Language Model Pre-training", "abstract": "The core of self-supervised learning for pre-training language models\nincludes pre-training task design as well as appropriate data augmentation.\nMost data augmentations in language model pre-training are context-independent.\nA seminal contextualized augmentation was recently proposed in ELECTRA and\nachieved state-of-the-art performance by introducing an auxiliary generation\nnetwork (generator) to produce contextualized data augmentation for the\ntraining of a main discrimination network (discriminator). This design,\nhowever, introduces extra computation cost of the generator and a need to\nadjust the relative capability between the generator and the discriminator. In\nthis paper, we propose a self-augmentation strategy (SAS) where a single\nnetwork is utilized for both regular pre-training and contextualized data\naugmentation for the training in later epochs. Essentially, this strategy\neliminates a separate generator and uses the single network to jointly conduct\ntwo pre-training tasks with MLM (Masked Language Modeling) and RTD (Replaced\nToken Detection) heads. It avoids the challenge to search for an appropriate\nsize of the generator, which is critical to the performance as evidenced in\nELECTRA and its subsequent variant models. In addition, SAS is a general\nstrategy that can be seamlessly combined with many new techniques emerging\nrecently or in the future, such as the disentangled attention mechanism from\nDeBERTa. Our experiments show that SAS is able to outperform ELECTRA and other\nstate-of-the-art models in the GLUE tasks with similar or less computation\ncost.", "published": "2021-06-14 05:57:46", "link": "http://arxiv.org/abs/2106.07176v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "English to Bangla Machine Translation Using Recurrent Neural Network", "abstract": "The applications of recurrent neural networks in machine translation are\nincreasing in natural language processing. Besides other languages, Bangla\nlanguage contains a large amount of vocabulary. Improvement of English to\nBangla machine translation would be a significant contribution to Bangla\nLanguage processing. This paper describes an architecture of English to Bangla\nmachine translation system. The system has been implemented with the\nencoder-decoder recurrent neural network. The model uses a knowledge-based\ncontext vector for the mapping of English and Bangla words. Performances of the\nmodel based on activation functions are measured here. The best performance is\nachieved for the linear activation function in encoder layer and the tanh\nactivation function in decoder layer. From the execution of GRU and LSTM layer,\nGRU performed better than LSTM. The attention layers are enacted with softmax\nand sigmoid activation function. The approach of the model outperforms the\nprevious state-of-the-art systems in terms of cross-entropy loss metrics. The\nreader can easily find out the structure of the machine translation of English\nto Bangla and the efficient activation functions from the paper.", "published": "2021-06-14 08:26:50", "link": "http://arxiv.org/abs/2106.07225v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Probing Pre-Trained Language Models for Disease Knowledge", "abstract": "Pre-trained language models such as ClinicalBERT have achieved impressive\nresults on tasks such as medical Natural Language Inference. At first glance,\nthis may suggest that these models are able to perform medical reasoning tasks,\nsuch as mapping symptoms to diseases. However, we find that standard benchmarks\nsuch as MedNLI contain relatively few examples that require such forms of\nreasoning. To better understand the medical reasoning capabilities of existing\nlanguage models, in this paper we introduce DisKnE, a new benchmark for Disease\nKnowledge Evaluation. To construct this benchmark, we annotated each positive\nMedNLI example with the types of medical reasoning that are needed. We then\ncreated negative examples by corrupting these positive examples in an\nadversarial way. Furthermore, we define training-test splits per disease,\nensuring that no knowledge about test diseases can be learned from the training\ndata, and we canonicalize the formulation of the hypotheses to avoid the\npresence of artefacts. This leads to a number of binary classification\nproblems, one for each type of reasoning and each disease. When analysing\npre-trained models for the clinical/biomedical domain on the proposed\nbenchmark, we find that their performance drops considerably.", "published": "2021-06-14 10:31:25", "link": "http://arxiv.org/abs/2106.07285v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Constraining Linear-chain CRFs to Regular Languages", "abstract": "A major challenge in structured prediction is to represent the\ninterdependencies within output structures. When outputs are structured as\nsequences, linear-chain conditional random fields (CRFs) are a widely used\nmodel class which can learn \\textit{local} dependencies in the output. However,\nthe CRF's Markov assumption makes it impossible for CRFs to represent\ndistributions with \\textit{nonlocal} dependencies, and standard CRFs are unable\nto respect nonlocal constraints of the data (such as global arity constraints\non output labels). We present a generalization of CRFs that can enforce a broad\nclass of constraints, including nonlocal ones, by specifying the space of\npossible output structures as a regular language $\\mathcal{L}$. The resulting\nregular-constrained CRF (RegCCRF) has the same formal properties as a standard\nCRF, but assigns zero probability to all label sequences not in $\\mathcal{L}$.\nNotably, RegCCRFs can incorporate their constraints during training, while\nrelated models only enforce constraints during decoding. We prove that\nconstrained training is never worse than constrained decoding, and show\nempirically that it can be substantially better in practice. Additionally, we\ndemonstrate a practical benefit on downstream tasks by incorporating a RegCCRF\ninto a deep neural model for semantic role labeling, exceeding state-of-the-art\nresults on a standard dataset.", "published": "2021-06-14 11:23:59", "link": "http://arxiv.org/abs/2106.07306v6", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Model Explainability in Deep Learning Based Natural Language Processing", "abstract": "Machine learning (ML) model explainability has received growing attention,\nespecially in the area related to model risk and regulations. In this paper, we\nreviewed and compared some popular ML model explainability methodologies,\nespecially those related to Natural Language Processing (NLP) models. We then\napplied one of the NLP explainability methods Layer-wise Relevance Propagation\n(LRP) to a NLP classification model. We used the LRP method to derive a\nrelevance score for each word in an instance, which is a local explainability.\nThe relevance scores are then aggregated together to achieve global variable\nimportance of the model. Through the case study, we also demonstrated how to\napply the local explainability method to false positive and false negative\ninstances to discover the weakness of a NLP model. These analysis can help us\nto understand NLP models better and reduce the risk due to the black-box nature\nof NLP models. We also identified some common issues due to the special natures\nof NLP models and discussed how explainability analysis can act as a control to\ndetect these issues after the model has been trained.", "published": "2021-06-14 13:23:20", "link": "http://arxiv.org/abs/2106.07410v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Grammar Equations", "abstract": "Diagrammatically speaking, grammatical calculi such as pregroups provide\nwires between words in order to elucidate their interactions, and this enables\none to verify grammatical correctness of phrases and sentences. In this paper\nwe also provide wirings within words. This will enable us to identify\ngrammatical constructs that we expect to be either equal or closely related.\nHence, our work paves the way for a new theory of grammar, that provides novel\n`grammatical truths'. We give a nogo-theorem for the fact that our wirings for\nwords make no sense for preordered monoids, the form which grammatical calculi\nusually take. Instead, they require diagrams -- or equivalently, (free)\nmonoidal categories.", "published": "2021-06-14 15:16:09", "link": "http://arxiv.org/abs/2106.07485v1", "categories": ["cs.CL", "math.CT"], "primary_category": "cs.CL"}
{"title": "An Empirical Survey of Data Augmentation for Limited Data Learning in\n  NLP", "abstract": "NLP has achieved great progress in the past decade through the use of neural\nmodels and large labeled datasets. The dependence on abundant data prevents NLP\nmodels from being applied to low-resource settings or novel tasks where\nsignificant time, money, or expertise is required to label massive amounts of\ntextual data. Recently, data augmentation methods have been explored as a means\nof improving data efficiency in NLP. To date, there has been no systematic\nempirical overview of data augmentation for NLP in the limited labeled data\nsetting, making it difficult to understand which methods work in which\nsettings. In this paper, we provide an empirical survey of recent progress on\ndata augmentation for NLP in the limited labeled data setting, summarizing the\nlandscape of methods (including token-level augmentations, sentence-level\naugmentations, adversarial augmentations, and hidden-space augmentations) and\ncarrying out experiments on 11 datasets covering topics/news classification,\ninference tasks, paraphrasing tasks, and single-sentence tasks. Based on the\nresults, we draw several conclusions to help practitioners choose appropriate\naugmentations in different settings and discuss the current challenges and\nfuture directions for limited data learning in NLP.", "published": "2021-06-14 15:27:22", "link": "http://arxiv.org/abs/2106.07499v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating Various Tokenizers for Arabic Text Classification", "abstract": "The first step in any NLP pipeline is to split the text into individual\ntokens. The most obvious and straightforward approach is to use words as\ntokens. However, given a large text corpus, representing all the words is not\nefficient in terms of vocabulary size. In the literature, many tokenization\nalgorithms have emerged to tackle this problem by creating subwords which in\nturn limits the vocabulary size in a given text corpus. Most tokenization\ntechniques are language-agnostic i.e they don't incorporate the linguistic\nfeatures of a given language. Not to mention the difficulty of evaluating such\ntechniques in practice. In this paper, we introduce three new tokenization\nalgorithms for Arabic and compare them to three other baselines using\nunsupervised evaluations. In addition to that, we compare all the six\nalgorithms by evaluating them on three supervised classification tasks which\nare sentiment analysis, news classification and poetry classification using six\npublicly available datasets. Our experiments show that none of the tokenization\ntechnique is the best choice overall and that the performance of a given\ntokenization algorithm depends on the size of the dataset, type of the task,\nand the amount of morphology that exists in the dataset. However, some\ntokenization techniques are better overall as compared to others on various\ntext classification tasks.", "published": "2021-06-14 16:05:58", "link": "http://arxiv.org/abs/2106.07540v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Paraphrase Detection with the Adversarial Paraphrasing Task", "abstract": "If two sentences have the same meaning, it should follow that they are\nequivalent in their inferential properties, i.e., each sentence should\ntextually entail the other. However, many paraphrase datasets currently in\nwidespread use rely on a sense of paraphrase based on word overlap and syntax.\nCan we teach them instead to identify paraphrases in a way that draws on the\ninferential properties of the sentences, and is not over-reliant on lexical and\nsyntactic similarities of a sentence pair? We apply the adversarial paradigm to\nthis question, and introduce a new adversarial method of dataset creation for\nparaphrase identification: the Adversarial Paraphrasing Task (APT), which asks\nparticipants to generate semantically equivalent (in the sense of mutually\nimplicative) but lexically and syntactically disparate paraphrases. These\nsentence pairs can then be used both to test paraphrase identification models\n(which get barely random accuracy) and then improve their performance. To\naccelerate dataset generation, we explore automation of APT using T5, and show\nthat the resulting dataset also improves accuracy. We discuss implications for\nparaphrase detection and release our dataset in the hope of making paraphrase\ndetection models better able to detect sentence-level meaning equivalence.", "published": "2021-06-14 18:15:20", "link": "http://arxiv.org/abs/2106.07691v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficient (Soft) Q-Learning for Text Generation with Limited Good Data", "abstract": "Maximum likelihood estimation (MLE) is the predominant algorithm for training\ntext generation models. This paradigm relies on direct supervision examples,\nwhich is not applicable to many emerging applications, such as generating\nadversarial attacks or generating prompts to control language models.\nReinforcement learning (RL) on the other hand offers a more flexible solution\nby allowing users to plug in arbitrary task metrics as reward. Yet previous RL\nalgorithms for text generation, such as policy gradient (on-policy RL) and\nQ-learning (off-policy RL), are often notoriously inefficient or unstable to\ntrain due to the large sequence space and the sparse reward received only at\nthe end of sequences. In this paper, we introduce a new RL formulation for text\ngeneration from the soft Q-learning (SQL) perspective. It enables us to draw\nfrom the latest RL advances, such as path consistency learning, to combine the\nbest of on-/off-policy updates, and learn effectively from sparse reward. We\napply the approach to a wide range of novel text generation tasks, including\nlearning from noisy/negative examples, adversarial attacks, and prompt\ngeneration. Experiments show our approach consistently outperforms both\ntask-specialized algorithms and the previous RL methods.", "published": "2021-06-14 18:48:40", "link": "http://arxiv.org/abs/2106.07704v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can BERT Dig It? -- Named Entity Recognition for Information Retrieval\n  in the Archaeology Domain", "abstract": "The amount of archaeological literature is growing rapidly. Until recently,\nthese data were only accessible through metadata search. We implemented a text\nretrieval engine for a large archaeological text collection ($\\sim 658$ Million\nwords). In archaeological IR, domain-specific entities such as locations, time\nperiods, and artefacts, play a central role. This motivated the development of\na named entity recognition (NER) model to annotate the full collection with\narchaeological named entities. In this paper, we present ArcheoBERTje, a BERT\nmodel pre-trained on Dutch archaeological texts. We compare the model's quality\nand output on a Named Entity Recognition task to a generic multilingual model\nand a generic Dutch model. We also investigate ensemble methods for combining\nmultiple BERT models, and combining the best BERT model with a domain thesaurus\nusing Conditional Random Fields (CRF). We find that ArcheoBERTje outperforms\nboth the multilingual and Dutch model significantly with a smaller standard\ndeviation between runs, reaching an average F1 score of 0.735. The model also\noutperforms ensemble methods combining the three models. Combining ArcheoBERTje\npredictions and explicit domain knowledge from the thesaurus did not increase\nthe F1 score. We quantitatively and qualitatively analyse the differences\nbetween the vocabulary and output of the BERT models on the full collection and\nprovide some valuable insights in the effect of fine-tuning for specific\ndomains. Our results indicate that for a highly specific text domain such as\narchaeology, further pre-training on domain-specific data increases the model's\nquality on NER by a much larger margin than shown for other domains in the\nliterature, and that domain-specific pre-training makes the addition of domain\nknowledge from a thesaurus unnecessary.", "published": "2021-06-14 20:26:19", "link": "http://arxiv.org/abs/2106.07742v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Kaizen: Continuously improving teacher using Exponential Moving Average\n  for semi-supervised speech recognition", "abstract": "In this paper, we introduce the Kaizen framework that uses a continuously\nimproving teacher to generate pseudo-labels for semi-supervised speech\nrecognition (ASR). The proposed approach uses a teacher model which is updated\nas the exponential moving average (EMA) of the student model parameters. We\ndemonstrate that it is critical for EMA to be accumulated with full-precision\nfloating point. The Kaizen framework can be seen as a continuous version of the\niterative pseudo-labeling approach for semi-supervised training. It is\napplicable for different training criteria, and in this paper we demonstrate\nits effectiveness for frame-level hybrid hidden Markov model-deep neural\nnetwork (HMM-DNN) systems as well as sequence-level Connectionist Temporal\nClassification (CTC) based models.\n  For large scale real-world unsupervised public videos in UK English and\nItalian languages the proposed approach i) shows more than 10% relative word\nerror rate (WER) reduction over standard teacher-student training; ii) using\njust 10 hours of supervised data and a large amount of unsupervised data closes\nthe gap to the upper-bound supervised ASR system that uses 650h or 2700h\nrespectively.", "published": "2021-06-14 21:15:36", "link": "http://arxiv.org/abs/2106.07759v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Toward a Knowledge Discovery Framework for Data Science Job Market in\n  the United States", "abstract": "The growth of the data science field requires better tools to understand such\na fast-paced growing domain. Moreover, individuals from different backgrounds\nbecame interested in following a career as data scientists. Therefore,\nproviding a quantitative guide for individuals and organizations to understand\nthe skills required in the job market would be crucial. This paper introduces a\nframework to analyze the job market for data science-related jobs within the US\nwhile providing an interface to access insights in this market. The proposed\nframework includes three sub-modules allowing continuous data collection,\ninformation extraction, and a web-based dashboard visualization to investigate\nthe spatial and temporal distribution of data science-related jobs and skills.\nThe result of this work shows important skills for the main branches of data\nscience jobs and attempts to provide a skill-based definition of these data\nscience branches. The current version of this application is deployed on the\nweb and allows individuals and institutes to investigate skills required for\ndata science positions through the industry lens.", "published": "2021-06-14 21:23:15", "link": "http://arxiv.org/abs/2106.11077v2", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "End-to-end Neural Diarization: From Transformer to Conformer", "abstract": "We propose a new end-to-end neural diarization (EEND) system that is based on\nConformer, a recently proposed neural architecture that combines convolutional\nmappings and Transformer to model both local and global dependencies in speech.\nWe first show that data augmentation and convolutional subsampling layers\nenhance the original self-attentive EEND in the Transformer-based EEND, and\nthen Conformer gives an additional gain over the Transformer-based EEND.\nHowever, we notice that the Conformer-based EEND does not generalize as well\nfrom simulated to real conversation data as the Transformer-based model. This\nleads us to quantify the mismatch between simulated data and real speaker\nbehavior in terms of temporal statistics reflecting turn-taking between\nspeakers, and investigate its correlation with diarization error. By mixing\nsimulated and real data in EEND training, we mitigate the mismatch further,\nwith Conformer-based EEND achieving 24% error reduction over the baseline\nSA-EEND system, and 10% improvement over the best augmented Transformer-based\nsystem, on two-speaker CALLHOME data.", "published": "2021-06-14 05:21:08", "link": "http://arxiv.org/abs/2106.07167v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Is Einstein more agreeable and less neurotic than Hitler? A\n  computational exploration of the emotional and personality profiles of\n  historical persons", "abstract": "Recent progress in distributed semantic models (DSM) offers new ways to\nestimate personality traits of both fictive and real people. In this\nexploratory study we applied an extended version of the algorithm developed in\nJacobs (2019) to compute the likeability scores, emotional figure profiles and\nBIG5 personality traits for 100 historical persons from the arts, politics or\nscience domains whose names are rather unique (e.g., Einstein, Kahlo, Picasso).\nWe compared the results produced by static (word2vec) and dynamic (BERT)\nlanguage model representations in four studies. The results show both the\npotential and limitations of such DSM-based computations of personality\nprofiles and point ways to further develop this approach to become a useful\ntool in data science, psychology or computational and neurocognitive poetics\n(Jacobs, 2015).", "published": "2021-06-14 08:45:49", "link": "http://arxiv.org/abs/2106.07237v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unified Interpretation of Softmax Cross-Entropy and Negative Sampling:\n  With Case Study for Knowledge Graph Embedding", "abstract": "In knowledge graph embedding, the theoretical relationship between the\nsoftmax cross-entropy and negative sampling loss functions has not been\ninvestigated. This makes it difficult to fairly compare the results of the two\ndifferent loss functions. We attempted to solve this problem by using the\nBregman divergence to provide a unified interpretation of the softmax\ncross-entropy and negative sampling loss functions. Under this interpretation,\nwe can derive theoretical findings for fair comparison. Experimental results on\nthe FB15k-237 and WN18RR datasets show that the theoretical findings are valid\nin practical settings.", "published": "2021-06-14 09:07:02", "link": "http://arxiv.org/abs/2106.07250v4", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Cascaded Span Extraction and Response Generation for Document-Grounded\n  Dialog", "abstract": "This paper summarizes our entries to both subtasks of the first DialDoc\nshared task which focuses on the agent response prediction task in\ngoal-oriented document-grounded dialogs. The task is split into two subtasks:\npredicting a span in a document that grounds an agent turn and generating an\nagent response based on a dialog and grounding document. In the first subtask,\nwe restrict the set of valid spans to the ones defined in the dataset, use a\nbiaffine classifier to model spans, and finally use an ensemble of different\nmodels. For the second subtask, we use a cascaded model which grounds the\nresponse prediction on the predicted span instead of the full document. With\nthese approaches, we obtain significant improvements in both subtasks compared\nto the baseline.", "published": "2021-06-14 10:04:53", "link": "http://arxiv.org/abs/2106.07275v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HuBERT: Self-Supervised Speech Representation Learning by Masked\n  Prediction of Hidden Units", "abstract": "Self-supervised approaches for speech representation learning are challenged\nby three unique problems: (1) there are multiple sound units in each input\nutterance, (2) there is no lexicon of input sound units during the pre-training\nphase, and (3) sound units have variable lengths with no explicit segmentation.\nTo deal with these three problems, we propose the Hidden-Unit BERT (HuBERT)\napproach for self-supervised speech representation learning, which utilizes an\noffline clustering step to provide aligned target labels for a BERT-like\nprediction loss. A key ingredient of our approach is applying the prediction\nloss over the masked regions only, which forces the model to learn a combined\nacoustic and language model over the continuous inputs. HuBERT relies primarily\non the consistency of the unsupervised clustering step rather than the\nintrinsic quality of the assigned cluster labels. Starting with a simple\nk-means teacher of 100 clusters, and using two iterations of clustering, the\nHuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0\nperformance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with\n10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model,\nHuBERT shows up to 19% and 13% relative WER reduction on the more challenging\ndev-other and test-other evaluation subsets.", "published": "2021-06-14 14:14:28", "link": "http://arxiv.org/abs/2106.07447v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Using heterogeneity in semi-supervised transcription hypotheses to\n  improve code-switched speech recognition", "abstract": "Modeling code-switched speech is an important problem in automatic speech\nrecognition (ASR). Labeled code-switched data are rare, so monolingual data are\noften used to model code-switched speech. These monolingual data may be more\nclosely matched to one of the languages in the code-switch pair. We show that\nsuch asymmetry can bias prediction toward the better-matched language and\ndegrade overall model performance. To address this issue, we propose a\nsemi-supervised approach for code-switched ASR. We consider the case of\nEnglish-Mandarin code-switching, and the problem of using monolingual data to\nbuild bilingual \"transcription models'' for annotation of unlabeled\ncode-switched data. We first build multiple transcription models so that their\nindividual predictions are variously biased toward either English or Mandarin.\nWe then combine these biased transcriptions using confidence-based selection.\nThis strategy generates a superior transcript for semi-supervised training, and\nobtains a 19% relative improvement compared to a semi-supervised system that\nrelies on a transcription model built with only the best-matched monolingual\ndata.", "published": "2021-06-14 18:39:18", "link": "http://arxiv.org/abs/2106.07699v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Overcoming Domain Mismatch in Low Resource Sequence-to-Sequence ASR\n  Models using Hybrid Generated Pseudotranscripts", "abstract": "Sequence-to-sequence (seq2seq) models are competitive with hybrid models for\nautomatic speech recognition (ASR) tasks when large amounts of training data\nare available. However, data sparsity and domain adaptation are more\nproblematic for seq2seq models than their hybrid counterparts. We examine\ncorpora of five languages from the IARPA MATERIAL program where the transcribed\ndata is conversational telephone speech (CTS) and evaluation data is broadcast\nnews (BN). We show that there is a sizable initial gap in such a data condition\nbetween hybrid and seq2seq models, and the hybrid model is able to further\nimprove through the use of additional language model (LM) data. We use an\nadditional set of untranscribed data primarily in the BN domain for\nsemisupervised training. In semisupervised training, a seed model trained on\ntranscribed data generates hypothesized transcripts for unlabeled\ndomain-matched data for further training. By using a hybrid model with an\nexpanded language model for pseudotranscription, we are able to improve our\nseq2seq model from an average word error rate (WER) of 66.7% across all five\nlanguages to 29.0% WER. While this puts the seq2seq model at a competitive\noperating point, hybrid models are still able to use additional LM data to\nmaintain an advantage.", "published": "2021-06-14 19:25:57", "link": "http://arxiv.org/abs/2106.07716v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Targeted Data Acquisition for Evolving Negotiation Agents", "abstract": "Successful negotiators must learn how to balance optimizing for self-interest\nand cooperation. Yet current artificial negotiation agents often heavily depend\non the quality of the static datasets they were trained on, limiting their\ncapacity to fashion an adaptive response balancing self-interest and\ncooperation. For this reason, we find that these agents can achieve either high\nutility or cooperation, but not both. To address this, we introduce a targeted\ndata acquisition framework where we guide the exploration of a reinforcement\nlearning agent using annotations from an expert oracle. The guided exploration\nincentivizes the learning agent to go beyond its static dataset and develop new\nnegotiation strategies. We show that this enables our agents to obtain\nhigher-reward and more Pareto-optimal solutions when negotiating with both\nsimulated and human partners compared to standard supervised learning and\nreinforcement learning methods. This trend additionally holds when comparing\nagents using our targeted data acquisition framework to variants of agents\ntrained with a mix of supervised learning and reinforcement learning, or to\nagents using tailored reward functions that explicitly optimize for utility and\nPareto-optimality.", "published": "2021-06-14 19:45:59", "link": "http://arxiv.org/abs/2106.07728v2", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI"}
{"title": "CoDERT: Distilling Encoder Representations with Co-learning for\n  Transducer-based Speech Recognition", "abstract": "We propose a simple yet effective method to compress an RNN-Transducer\n(RNN-T) through the well-known knowledge distillation paradigm. We show that\nthe transducer's encoder outputs naturally have a high entropy and contain rich\ninformation about acoustically similar word-piece confusions. This rich\ninformation is suppressed when combined with the lower entropy decoder outputs\nto produce the joint network logits. Consequently, we introduce an auxiliary\nloss to distill the encoder logits from a teacher transducer's encoder, and\nexplore training strategies where this encoder distillation works effectively.\nWe find that tandem training of teacher and student encoders with an inplace\nencoder distillation outperforms the use of a pre-trained and static teacher\ntransducer. We also report an interesting phenomenon we refer to as implicit\ndistillation, that occurs when the teacher and student encoders share the same\ndecoder. Our experiments show 5.37-8.4% relative word error rate reductions\n(WERR) on in-house test sets, and 5.05-6.18% relative WERRs on LibriSpeech test\nsets.", "published": "2021-06-14 20:03:57", "link": "http://arxiv.org/abs/2106.07734v1", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Selective Listening by Synchronizing Speech with Lips", "abstract": "A speaker extraction algorithm seeks to extract the speech of a target\nspeaker from a multi-talker speech mixture when given a cue that represents the\ntarget speaker, such as a pre-enrolled speech utterance, or an accompanying\nvideo track. Visual cues are particularly useful when a pre-enrolled speech is\nnot available. In this work, we don't rely on the target speaker's pre-enrolled\nspeech, but rather use the target speaker's face track as the speaker cue, that\nis referred to as the auxiliary reference, to form an attractor towards the\ntarget speaker. We advocate that the temporal synchronization between the\nspeech and its accompanying lip movements is a direct and dominant audio-visual\ncue. Therefore, we propose a self-supervised pre-training strategy, to exploit\nthe speech-lip synchronization cue for target speaker extraction, which allows\nus to leverage abundant unlabeled in-domain data. We transfer the knowledge\nfrom the pre-trained model to the attractor encoder of the speaker extraction\nnetwork. We show that the proposed speaker extraction network outperforms\nvarious competitive baselines in terms of signal quality, perceptual quality,\nand intelligibility, achieving state-of-the-art performance.", "published": "2021-06-14 04:06:01", "link": "http://arxiv.org/abs/2106.07150v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Speech Disorder Classification Using Extended Factorized Hierarchical\n  Variational Auto-encoders", "abstract": "Objective speech disorder classification for speakers with communication\ndifficulty is desirable for diagnosis and administering therapy. With the\ncurrent state of speech technology, it is evident to propose neural networks\nfor this application. But neural network model training is hampered by a lack\nof labeled disordered speech data. In this research, we apply an extended\nversion of Factorized Hierarchical Variational Auto-encoders (FHVAE) for\nrepresentation learning on disordered speech. The FHVAE model extracts both\ncontent-related and sequence-related latent variables from speech data, and we\nutilize the extracted variables to explore how disorder type information is\nrepresented in the latent variables. For better classification performance, the\nlatent variables are aggregated at the word and sentence level. We show that an\nextension of the FHVAE model succeeds in the better disentanglement of the\ncontent-related and sequence-related related representations, but both\nrepresentations are still required for best results on disorder type\nclassification.", "published": "2021-06-14 12:29:27", "link": "http://arxiv.org/abs/2106.07337v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Dual-Path Filter Network: Speaker-Aware Modeling for Speech Separation", "abstract": "Speech separation has been extensively studied to deal with the cocktail\nparty problem in recent years. All related approaches can be divided into two\ncategories: time-frequency domain methods and time domain methods. In addition,\nsome methods try to generate speaker vectors to support source separation. In\nthis study, we propose a new model called dual-path filter network (DPFN). Our\nmodel focuses on the post-processing of speech separation to improve speech\nseparation performance. DPFN is composed of two parts: the speaker module and\nthe separation module. First, the speaker module infers the identities of the\nspeakers. Then, the separation module uses the speakers' information to extract\nthe voices of individual speakers from the mixture. DPFN constructed based on\nDPRNN-TasNet is not only superior to DPRNN-TasNet, but also avoids the problem\nof permutation-invariant training (PIT).", "published": "2021-06-14 16:37:15", "link": "http://arxiv.org/abs/2106.07579v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Few-shot learning of new sound classes for target sound extraction", "abstract": "Target sound extraction consists of extracting the sound of a target acoustic\nevent (AE) class from a mixture of AE sounds. It can be realized using a neural\nnetwork that extracts the target sound conditioned on a 1-hot vector that\nrepresents the desired AE class. With this approach, embedding vectors\nassociated with the AE classes are directly optimized for the extraction of\nsound classes seen during training. However, it is not easy to extend this\nframework to new AE classes, i.e. unseen during training. Recently, speech,\nmusic, or AE sound extraction based on enrollment audio of the desired sound\noffers the potential of extracting any target sound in a mixture given only a\nshort audio signal of a similar sound. In this work, we propose combining\n1-hot- and enrollment-based target sound extraction, allowing optimal\nperformance for seen AE classes and simple extension to new classes. In\nexperiments with synthesized sound mixtures generated with the Freesound\nDataset (FSD) datasets, we demonstrate the benefit of the combined framework\nfor both seen and new AE classes. Besides, we also propose adapting the\nembedding vectors obtained from a few enrollment audio samples (few-shot) to\nfurther improve performance on new classes.", "published": "2021-06-14 03:13:57", "link": "http://arxiv.org/abs/2106.07144v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multiple scattering ambisonics: three-dimensional sound field estimation\n  using interacting spheres", "abstract": "Rigid spherical microphone arrays (RSMAs) have been widely used in ambisonics\nsound field recording. While it is desired to combine the information captured\nby a grid of densely arranged RSMAs for expanding the area of accurate\nreconstruction, or sweet-spots, this is not trivial due to inter-array\ninterference. Here we propose multiple scattering ambisonics, a method for\nthree-dimensional ambisonics sound field recording using multiple acoustically\ninteracting RSMAs. Numerical experiments demonstrate the sweet-spot expansion\nrealized by the proposed method. The proposed method can be used with existing\nRSMAs as building blocks and opens possibilities including higher\ndegrees-of-freedom spatial audio.", "published": "2021-06-14 04:32:36", "link": "http://arxiv.org/abs/2106.07157v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "F-T-LSTM based Complex Network for Joint Acoustic Echo Cancellation and\n  Speech Enhancement", "abstract": "With the increasing demand for audio communication and online conference,\nensuring the robustness of Acoustic Echo Cancellation (AEC) under the\ncomplicated acoustic scenario including noise, reverberation and nonlinear\ndistortion has become a top issue. Although there have been some traditional\nmethods that consider nonlinear distortion, they are still inefficient for echo\nsuppression and the performance will be attenuated when noise is present. In\nthis paper, we present a real-time AEC approach using complex neural network to\nbetter modeling the important phase information and frequency-time-LSTMs\n(F-T-LSTM), which scan both frequency and time axis, for better temporal\nmodeling. Moreover, we utilize modified SI-SNR as cost function to make the\nmodel to have better echo cancellation and noise suppression (NS) performance.\nWith only 1.4M parameters, the proposed approach outperforms the AEC-challenge\nbaseline by 0.27 in terms of Mean Opinion Score (MOS).", "published": "2021-06-14 16:33:36", "link": "http://arxiv.org/abs/2106.07577v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FastICARL: Fast Incremental Classifier and Representation Learning with\n  Efficient Budget Allocation in Audio Sensing Applications", "abstract": "Various incremental learning (IL) approaches have been proposed to help deep\nlearning models learn new tasks/classes continuously without forgetting what\nwas learned previously (i.e., avoid catastrophic forgetting). With the growing\nnumber of deployed audio sensing applications that need to dynamically\nincorporate new tasks and changing input distribution from users, the ability\nof IL on-device becomes essential for both efficiency and user privacy.\n  However, prior works suffer from high computational costs and storage demands\nwhich hinders the deployment of IL on-device. In this work, to overcome these\nlimitations, we develop an end-to-end and on-device IL framework, FastICARL,\nthat incorporates an exemplar-based IL and quantization in the context of\naudio-based applications. We first employ k-nearest-neighbor to reduce the\nlatency of IL. Then, we jointly utilize a quantization technique to decrease\nthe storage requirements of IL. We implement FastICARL on two types of mobile\ndevices and demonstrate that FastICARL remarkably decreases the IL time up to\n78-92% and the storage requirements by 2-4 times without sacrificing its\nperformance. FastICARL enables complete on-device IL, ensuring user privacy as\nthe user data does not need to leave the device.", "published": "2021-06-14 09:42:58", "link": "http://arxiv.org/abs/2106.07268v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio Attacks and Defenses against AED Systems -- A Practical Study", "abstract": "In this paper, we evaluate deep learning-enabled AED systems against evasion\nattacks based on adversarial examples. We test the robustness of multiple\nsecurity critical AED tasks, implemented as CNNs classifiers, as well as\nexisting third-party Nest devices, manufactured by Google, which run their own\nblack-box deep learning models. Our adversarial examples use audio\nperturbations made of white and background noises. Such disturbances are easy\nto create, to perform and to reproduce, and can be accessible to a large number\nof potential attackers, even non-technically savvy ones.\n  We show that an adversary can focus on audio adversarial inputs to cause AED\nsystems to misclassify, achieving high success rates, even when we use small\nlevels of a given type of noisy disturbance. For instance, on the case of the\ngunshot sound class, we achieve nearly 100% success rate when employing as\nlittle as 0.05 white noise level. Similarly to what has been previously done by\nworks focusing on adversarial examples from the image domain as well as on the\nspeech recognition domain. We then, seek to improve classifiers' robustness\nthrough countermeasures. We employ adversarial training and audio denoising. We\nshow that these countermeasures, when applied to audio input, can be\nsuccessful, either in isolation or in combination, generating relevant\nincreases of nearly fifty percent in the performance of the classifiers when\nthese are under attack.", "published": "2021-06-14 13:42:49", "link": "http://arxiv.org/abs/2106.07428v4", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CRASH: Raw Audio Score-based Generative Modeling for Controllable\n  High-resolution Drum Sound Synthesis", "abstract": "In this paper, we propose a novel score-base generative model for\nunconditional raw audio synthesis. Our proposal builds upon the latest\ndevelopments on diffusion process modeling with stochastic differential\nequations, which already demonstrated promising results on image generation. We\nmotivate novel heuristics for the choice of the diffusion processes better\nsuited for audio generation, and consider the use of a conditional U-Net to\napproximate the score function. While previous approaches on diffusion models\non audio were mainly designed as speech vocoders in medium resolution, our\nmethod termed CRASH (Controllable Raw Audio Synthesis with High-resolution)\nallows us to generate short percussive sounds in 44.1kHz in a controllable way.\nThrough extensive experiments, we showcase on a drum sound generation task the\nnumerous sampling schemes offered by our method (unconditional generation,\ndeterministic generation, inpainting, interpolation, variations,\nclass-conditional sampling) and propose the class-mixing sampling, a novel way\nto generate \"hybrid\" sounds. Our proposed method closes the gap with GAN-based\nmethods on raw audio, while offering more flexible generation capabilities with\nlighter and easier-to-train models.", "published": "2021-06-14 13:48:03", "link": "http://arxiv.org/abs/2106.07431v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Non Gaussian Denoising Diffusion Models", "abstract": "Generative diffusion processes are an emerging and effective tool for image\nand speech generation. In the existing methods, the underline noise\ndistribution of the diffusion process is Gaussian noise. However, fitting\ndistributions with more degrees of freedom, could help the performance of such\ngenerative models. In this work, we investigate other types of noise\ndistribution for the diffusion process. Specifically, we show that noise from\nGamma distribution provides improved results for image and speech generation.\nMoreover, we show that using a mixture of Gaussian noise variables in the\ndiffusion process improves the performance over a diffusion process that is\nbased on a single distribution. Our approach preserves the ability to\nefficiently sample state in the training diffusion process while using Gamma\nnoise and a mixture of noise.", "published": "2021-06-14 16:42:43", "link": "http://arxiv.org/abs/2106.07582v1", "categories": ["cs.LG", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Learning Audio-Visual Dereverberation", "abstract": "Reverberation not only degrades the quality of speech for human perception,\nbut also severely impacts the accuracy of automatic speech recognition. Prior\nwork attempts to remove reverberation based on the audio modality only. Our\nidea is to learn to dereverberate speech from audio-visual observations. The\nvisual environment surrounding a human speaker reveals important cues about the\nroom geometry, materials, and speaker location, all of which influence the\nprecise reverberation effects. We introduce Visually-Informed Dereverberation\nof Audio (VIDA), an end-to-end approach that learns to remove reverberation\nbased on both the observed monaural sound and visual scene. In support of this\nnew task, we develop a large-scale dataset SoundSpaces-Speech that uses\nrealistic acoustic renderings of speech in real-world 3D scans of homes\noffering a variety of room acoustics. Demonstrating our approach on both\nsimulated and real imagery for speech enhancement, speech recognition, and\nspeaker identification, we show it achieves state-of-the-art performance and\nsubstantially improves over audio-only methods.", "published": "2021-06-14 20:01:24", "link": "http://arxiv.org/abs/2106.07732v2", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Tracing Back Music Emotion Predictions to Sound Sources and Intuitive\n  Perceptual Qualities", "abstract": "Music emotion recognition is an important task in MIR (Music Information\nRetrieval) research. Owing to factors like the subjective nature of the task\nand the variation of emotional cues between musical genres, there are still\nsignificant challenges in developing reliable and generalizable models. One\nimportant step towards better models would be to understand what a model is\nactually learning from the data and how the prediction for a particular input\nis made. In previous work, we have shown how to derive explanations of model\npredictions in terms of spectrogram image segments that connect to the\nhigh-level emotion prediction via a layer of easily interpretable perceptual\nfeatures. However, that scheme lacks intuitive musical comprehensibility at the\nspectrogram level. In the present work, we bridge this gap by merging audioLIME\n-- a source-separation based explainer -- with mid-level perceptual features,\nthus forming an intuitive connection chain between the input audio and the\noutput emotion predictions. We demonstrate the usefulness of this method by\napplying it to debug a biased emotion prediction model.", "published": "2021-06-14 22:49:19", "link": "http://arxiv.org/abs/2106.07787v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SynthASR: Unlocking Synthetic Data for Speech Recognition", "abstract": "End-to-end (E2E) automatic speech recognition (ASR) models have recently\ndemonstrated superior performance over the traditional hybrid ASR models.\nTraining an E2E ASR model requires a large amount of data which is not only\nexpensive but may also raise dependency on production data. At the same time,\nsynthetic speech generated by the state-of-the-art text-to-speech (TTS) engines\nhas advanced to near-human naturalness. In this work, we propose to utilize\nsynthetic speech for ASR training (SynthASR) in applications where data is\nsparse or hard to get for ASR model training. In addition, we apply continual\nlearning with a novel multi-stage training strategy to address catastrophic\nforgetting, achieved by a mix of weighted multi-style training, data\naugmentation, encoder freezing, and parameter regularization. In our\nexperiments conducted on in-house datasets for a new application of recognizing\nmedication names, training ASR RNN-T models with synthetic audio via the\nproposed multi-stage training improved the recognition performance on new\napplication by more than 65% relative, without degradation on existing general\napplications. Our observations show that SynthASR holds great promise in\ntraining the state-of-the-art large-scale E2E ASR models for new applications\nwhile reducing the costs and dependency on production data.", "published": "2021-06-14 23:26:44", "link": "http://arxiv.org/abs/2106.07803v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Automatic Analysis of the Emotional Content of Speech in Daylong\n  Child-Centered Recordings from a Neonatal Intensive Care Unit", "abstract": "Researchers have recently started to study how the emotional speech heard by\nyoung infants can affect their developmental outcomes. As a part of this\nresearch, hundreds of hours of daylong recordings from preterm infants' audio\nenvironments were collected from two hospitals in Finland and Estonia in the\ncontext of so-called APPLE study. In order to analyze the emotional content of\nspeech in such a massive dataset, an automatic speech emotion recognition (SER)\nsystem is required. However, there are no emotion labels or existing indomain\nSER systems to be used for this purpose. In this paper, we introduce this\ninitially unannotated large-scale real-world audio dataset and describe the\ndevelopment of a functional SER system for the Finnish subset of the data. We\nexplore the effectiveness of alternative state-of-the-art techniques to deploy\na SER system to a new domain, comparing cross-corpus generalization, WGAN-based\ndomain adaptation, and active learning in the task. As a result, we show that\nthe best-performing models are able to achieve a classification performance of\n73.4% unweighted average recall (UAR) and 73.2% UAR for a binary classification\nfor valence and arousal, respectively. The results also show that active\nlearning achieves the most consistent performance compared to the two\nalternatives.", "published": "2021-06-14 11:17:52", "link": "http://arxiv.org/abs/2106.09539v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A learned conditional prior for the VAE acoustic space of a TTS system", "abstract": "Many factors influence speech yielding different renditions of a given\nsentence. Generative models, such as variational autoencoders (VAEs), capture\nthis variability and allow multiple renditions of the same sentence via\nsampling. The degree of prosodic variability depends heavily on the prior that\nis used when sampling. In this paper, we propose a novel method to compute an\ninformative prior for the VAE latent space of a neural text-to-speech (TTS)\nsystem. By doing so, we aim to sample with more prosodic variability, while\ngaining controllability over the latent space's structure.\n  By using as prior the posterior distribution of a secondary VAE, which we\ncondition on a speaker vector, we can sample from the primary VAE taking\nexplicitly the conditioning into account and resulting in samples from a\nspecific region of the latent space for each condition (i.e. speaker). A formal\npreference test demonstrates significant preference of the proposed approach\nover standard Conditional VAE. We also provide visualisations of the latent\nspace where well-separated condition-specific clusters appear, as well as\nablation studies to better understand the behaviour of the system.", "published": "2021-06-14 15:36:16", "link": "http://arxiv.org/abs/2106.10229v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Novel mapping for visual to auditory sensory substitution", "abstract": "visual information can be converted into audio stream via sensory\nsubstitution devices in order to give visually impaired people the chance of\nperception of their surrounding easily and simultaneous to performing everyday\ntasks. In this study, visual environmental features namely, coordinate, type of\nobjects and their size are assigned to audio features related to music tones\nsuch as frequency, time duration and note permutations. Results demonstrated\nthat this new method has more training time efficiency in comparison with our\nprevious method named VBTones which sinusoidal tones were applied. Moreover,\nresults in blind object recognition for real objects was achieved 88.05 on\naverage.", "published": "2021-06-14 14:14:50", "link": "http://arxiv.org/abs/2106.07448v1", "categories": ["cs.SD", "cs.AI", "cs.CV", "eess.AS", "eess.IV"], "primary_category": "cs.SD"}
