{"title": "A Deep Architecture for Semantic Parsing", "abstract": "Many successful approaches to semantic parsing build on top of the syntactic\nanalysis of text, and make use of distributional representations or statistical\nmodels to match parses to ontology-specific queries. This paper presents a\nnovel deep learning architecture which provides a semantic parsing system\nthrough the union of two neural models of language semantics. It allows for the\ngeneration of ontology-specific queries from natural language statements and\nquestions without the need for parsing, which makes it especially suitable to\ngrammatically malformed or syntactically atypical text, such as tweets, as well\nas permitting the development of semantic parsers for resource-poor languages.", "published": "2014-04-29 10:10:13", "link": "http://arxiv.org/abs/1404.7296v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Concise comparative summaries (CCS) of large text corpora with a human\n  experiment", "abstract": "In this paper we propose a general framework for topic-specific summarization\nof large text corpora and illustrate how it can be used for the analysis of\nnews databases. Our framework, concise comparative summarization (CCS), is\nbuilt on sparse classification methods. CCS is a lightweight and flexible tool\nthat offers a compromise between simple word frequency based methods currently\nin wide use and more heavyweight, model-intensive methods such as latent\nDirichlet allocation (LDA). We argue that sparse methods have much to offer for\ntext analysis and hope CCS opens the door for a new branch of research in this\nimportant field. For a particular topic of interest (e.g., China or energy),\nCSS automatically labels documents as being either on- or off-topic (usually\nvia keyword search), and then uses sparse classification methods to predict\nthese labels with the high-dimensional counts of all the other words and\nphrases in the documents. The resulting small set of phrases found as\npredictive are then harvested as the summary. To validate our tool, we, using\nnews articles from the New York Times international section, designed and\nconducted a human survey to compare the different summarizers with human\nunderstanding. We demonstrate our approach with two case studies, a media\nanalysis of the framing of \"Egypt\" in the New York Times throughout the Arab\nSpring and an informal comparison of the New York Times' and Wall Street\nJournal's coverage of \"energy.\" Overall, we find that the Lasso with $L^2$\nnormalization can be effectively and usefully used to summarize large corpora,\nregardless of document size.", "published": "2014-04-29 13:53:38", "link": "http://arxiv.org/abs/1404.7362v1", "categories": ["cs.CL", "stat.AP"], "primary_category": "cs.CL"}
