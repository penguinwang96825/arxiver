{"title": "Seq2Seq and Joint Learning Based Unix Command Line Prediction System", "abstract": "Despite being an open-source operating system pioneered in the early 90s,\nUNIX based platforms have not been able to garner an overwhelming reception\nfrom amateur end users. One of the rationales for under popularity of UNIX\nbased systems is the steep learning curve corresponding to them due to\nextensive use of command line interface instead of usual interactive graphical\nuser interface. In past years, the majority of insights used to explore the\nconcern are eminently centered around the notion of utilizing chronic log\nhistory of the user to make the prediction of successive command. The\napproaches directed at anatomization of this notion are predominantly in\naccordance with Probabilistic inference models. The techniques employed in\npast, however, have not been competent enough to address the predicament as\nlegitimately as anticipated. Instead of deploying usual mechanism of\nrecommendation systems, we have employed a simple yet novel approach of Seq2seq\nmodel by leveraging continuous representations of self-curated exhaustive\nKnowledge Base (KB) to enhance the embedding employed in the model. This work\ndescribes an assistive, adaptive and dynamic way of enhancing UNIX command line\nprediction systems. Experimental methods state that our model has achieved\naccuracy surpassing mixture of other techniques and adaptive command line\ninterface mechanism as acclaimed in the past.", "published": "2020-06-20 11:57:01", "link": "http://arxiv.org/abs/2006.11558v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SIGMORPHON 2020 Shared Task 0: Typologically Diverse Morphological\n  Inflection", "abstract": "A broad goal in natural language processing (NLP) is to develop a system that\nhas the capacity to process any natural language. Most systems, however, are\ndeveloped using data from just one language such as English. The SIGMORPHON\n2020 shared task on morphological reinflection aims to investigate systems'\nability to generalize across typologically distinct languages, many of which\nare low resource. Systems were developed using data from 45 languages and just\n5 language families, fine-tuned with data from an additional 45 languages and\n10 language families (13 in total), and evaluated on all 90 languages. A total\nof 22 systems (19 neural) from 10 teams were submitted to the task. All four\nwinning systems were neural (two monolingual transformers and two massively\nmultilingual RNN-based models with gated attention). Most teams demonstrate\nutility of data hallucination and augmentation, ensembles, and multilingual\ntraining for low-resource languages. Non-neural learners and manually designed\ngrammars showed competitive and even superior performance on some languages\n(such as Ingrian, Tajik, Tagalog, Zarma, Lingala), especially with very limited\ndata. Some language families (Afro-Asiatic, Niger-Congo, Turkic) were\nrelatively easy for most systems and achieved over 90% mean accuracy while\nothers were more challenging.", "published": "2020-06-20 13:24:14", "link": "http://arxiv.org/abs/2006.11572v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AraDIC: Arabic Document Classification using Image-Based Character\n  Embeddings and Class-Balanced Loss", "abstract": "Classical and some deep learning techniques for Arabic text classification\noften depend on complex morphological analysis, word segmentation, and\nhand-crafted feature engineering. These could be eliminated by using\ncharacter-level features. We propose a novel end-to-end Arabic document\nclassification framework, Arabic document image-based classifier (AraDIC),\ninspired by the work on image-based character embeddings. AraDIC consists of an\nimage-based character encoder and a classifier. They are trained in an\nend-to-end fashion using the class balanced loss to deal with the long-tailed\ndata distribution problem. To evaluate the effectiveness of AraDIC, we created\nand published two datasets, the Arabic Wikipedia title (AWT) dataset and the\nArabic poetry (AraP) dataset. To the best of our knowledge, this is the first\nimage-based character embedding framework addressing the problem of Arabic text\nclassification. We also present the first deep learning-based text classifier\nwidely evaluated on modern standard Arabic, colloquial Arabic and classical\nArabic. AraDIC shows performance improvement over classical and deep learning\nbaselines by 12.29% and 23.05% for the micro and macro F-score, respectively.", "published": "2020-06-20 14:25:06", "link": "http://arxiv.org/abs/2006.11586v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Studying Attention Models in Sentiment Attitude Extraction Task", "abstract": "In the sentiment attitude extraction task, the aim is to identify\n<<attitudes>> -- sentiment relations between entities mentioned in text. In\nthis paper, we provide a study on attention-based context encoders in the\nsentiment attitude extraction task. For this task, we adapt attentive context\nencoders of two types: (i) feature-based; (ii) self-based. Our experiments with\na corpus of Russian analytical texts RuSentRel illustrate that the models\ntrained with attentive encoders outperform ones that were trained without them\nand achieve 1.5-5.9% increase by F1. We also provide the analysis of attention\nweight distributions in dependence on the term type.", "published": "2020-06-20 16:09:24", "link": "http://arxiv.org/abs/2006.11605v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Importance of Category Labels in Grammar Induction with\n  Child-directed Utterances", "abstract": "Recent progress in grammar induction has shown that grammar induction is\npossible without explicit assumptions of language-specific knowledge. However,\nevaluation of induced grammars usually has ignored phrasal labels, an essential\npart of a grammar. Experiments in this work using a labeled evaluation metric,\nRH, show that linguistically motivated predictions about grammar sparsity and\nuse of categories can only be revealed through labeled evaluation. Furthermore,\ndepth-bounding as an implementation of human memory constraints in grammar\ninducers is still effective with labeled evaluation on multilingual transcribed\nchild-directed utterances.", "published": "2020-06-20 20:21:17", "link": "http://arxiv.org/abs/2006.11646v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Query Safety at Pinterest", "abstract": "Query recommendations in search engines is a double edged sword, with\nundeniable benefits but potential of harm. Identifying unsafe queries is\nnecessary to protect users from inappropriate query suggestions. However,\nidentifying these is non-trivial because of the linguistic diversity resulting\nfrom large vocabularies, social-group-specific slang and typos, and because the\ninappropriateness of a term depends on the context. Here we formulate the\nproblem as query-set expansion, where we are given a small and potentially\nbiased seed set and the aim is to identify a diverse set of semantically\nrelated queries. We present PinSets, a system for query-set expansion, which\napplies a simple yet powerful mechanism to search user sessions, expanding a\ntiny seed set into thousands of related queries at nearly perfect precision,\ndeep into the tail, along with explanations that are easy to interpret. PinSets\nowes its high quality expansion to using a hybrid of textual and behavioral\ntechniques (i.e., treating queries both as compositional and as black boxes).\nExperiments show that, for the domain of drugs-related queries, PinSets expands\n20 seed queries into 15,670 positive training examples at over 99\\% precision.\nThe generated expansions have diverse vocabulary and correctly handles words\nwith ambiguous safety. PinSets decreased unsafe query suggestions at Pinterest\nby 90\\%.", "published": "2020-06-20 07:35:22", "link": "http://arxiv.org/abs/2006.11511v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Sarcasm Detection in Tweets with BERT and GloVe Embeddings", "abstract": "Sarcasm is a form of communication in whichthe person states opposite of what\nhe actually means. It is ambiguous in nature. In this paper, we propose using\nmachine learning techniques with BERT and GloVe embeddings to detect sarcasm in\ntweets. The dataset is preprocessed before extracting the embeddings. The\nproposed model also uses the context in which the user is reacting to along\nwith his actual response.", "published": "2020-06-20 07:36:06", "link": "http://arxiv.org/abs/2006.11512v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "IQA: Interactive Query Construction in Semantic Question Answering\n  Systems", "abstract": "Semantic Question Answering (SQA) systems automatically interpret user\nquestions expressed in a natural language in terms of semantic queries. This\nprocess involves uncertainty, such that the resulting queries do not always\naccurately match the user intent, especially for more complex and less common\nquestions. In this article, we aim to empower users in guiding SQA systems\ntowards the intended semantic queries through interaction. We introduce IQA -\nan interaction scheme for SQA pipelines. This scheme facilitates seamless\nintegration of user feedback in the question answering process and relies on\nOption Gain - a novel metric that enables efficient and intuitive user\ninteraction. Our evaluation shows that using the proposed scheme, even a small\nnumber of user interactions can lead to significant improvements in the\nperformance of SQA systems.", "published": "2020-06-20 10:02:20", "link": "http://arxiv.org/abs/2006.11534v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Named Entity Extraction with Finite State Transducers", "abstract": "We describe a named entity tagging system that requires minimal linguistic\nknowledge and can be applied to more target languages without substantial\nchanges. The system is based on the ideas of the Brill's tagger which makes it\nreally simple. Using supervised machine learning, we construct a series of\nautomatons (or transducers) in order to tag a given text. The final model is\ncomposed entirely of automatons and it requires a lineal time for tagging. It\nwas tested with the Spanish data set provided in the CoNLL-$2002$ attaining an\noverall $F_{\\beta = 1}$ measure of $60\\%.$ Also, we present an algorithm for\nthe construction of the final transducer used to encode all the learned\ncontextual rules.", "published": "2020-06-20 11:09:04", "link": "http://arxiv.org/abs/2006.11548v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning aligned embeddings for semi-supervised word translation using\n  Maximum Mean Discrepancy", "abstract": "Word translation is an integral part of language translation. In machine\ntranslation, each language is considered a domain with its own word embedding.\nThe alignment between word embeddings allows linking semantically equivalent\nwords in multilingual contexts. Moreover, it offers a way to infer\ncross-lingual meaning for words without a direct translation. Current methods\nfor word embedding alignment are either supervised, i.e. they require known\nword pairs, or learn a cross-domain transformation on fixed embeddings in an\nunsupervised way. Here we propose an end-to-end approach for word embedding\nalignment that does not require known word pairs. Our method, termed Word\nAlignment through MMD (WAM), learns embeddings that are aligned during sentence\ntranslation training using a localized Maximum Mean Discrepancy (MMD)\nconstraint between the embeddings. We show that our method not only\nout-performs unsupervised methods, but also supervised methods that train on\nknown word translations.", "published": "2020-06-20 13:57:55", "link": "http://arxiv.org/abs/2006.11578v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Defense against Adversarial Attacks in NLP via Dirichlet Neighborhood\n  Ensemble", "abstract": "Despite neural networks have achieved prominent performance on many natural\nlanguage processing (NLP) tasks, they are vulnerable to adversarial examples.\nIn this paper, we propose Dirichlet Neighborhood Ensemble (DNE), a randomized\nsmoothing method for training a robust model to defense substitution-based\nattacks. During training, DNE forms virtual sentences by sampling embedding\nvectors for each word in an input sentence from a convex hull spanned by the\nword and its synonyms, and it augments them with the training data. In such a\nway, the model is robust to adversarial attacks while maintaining the\nperformance on the original clean data. DNE is agnostic to the network\narchitectures and scales to large models for NLP applications. We demonstrate\nthrough extensive experimentation that our method consistently outperforms\nrecently proposed defense methods by a significant margin across different\nnetwork architectures and multiple data sets.", "published": "2020-06-20 18:01:16", "link": "http://arxiv.org/abs/2006.11627v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MDR Cluster-Debias: A Nonlinear WordEmbedding Debiasing Pipeline", "abstract": "Existing methods for debiasing word embeddings often do so only\nsuperficially, in that words that are stereotypically associated with, e.g., a\nparticular gender in the original embedding space can still be clustered\ntogether in the debiased space. However, there has yet to be a study that\nexplores why this residual clustering exists, and how it might be addressed.\nThe present work fills this gap. We identify two potential reasons for which\nresidual bias exists and develop a new pipeline, MDR Cluster-Debias, to\nmitigate this bias. We explore the strengths and weaknesses of our method,\nfinding that it significantly outperforms other existing debiasing approaches\non a variety of upstream bias tests but achieves limited improvement on\ndecreasing gender bias in a downstream task. This indicates that word\nembeddings encode gender bias in still other ways, not necessarily captured by\nupstream tests.", "published": "2020-06-20 20:03:07", "link": "http://arxiv.org/abs/2006.11642v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Towards a self-organizing pre-symbolic neural model representing\n  sensorimotor primitives", "abstract": "The acquisition of symbolic and linguistic representations of sensorimotor\nbehavior is a cognitive process performed by an agent when it is executing\nand/or observing own and others' actions. According to Piaget's theory of\ncognitive development, these representations develop during the sensorimotor\nstage and the pre-operational stage. We propose a model that relates the\nconceptualization of the higher-level information from visual stimuli to the\ndevelopment of ventral/dorsal visual streams. This model employs neural network\narchitecture incorporating a predictive sensory module based on an RNNPB\n(Recurrent Neural Network with Parametric Biases) and a horizontal product\nmodel. We exemplify this model through a robot passively observing an object to\nlearn its features and movements. During the learning process of observing\nsensorimotor primitives, i.e. observing a set of trajectories of arm movements\nand its oriented object features, the pre-symbolic representation is\nself-organized in the parametric units. These representational units act as\nbifurcation parameters, guiding the robot to recognize and predict various\nlearned sensorimotor primitives. The pre-symbolic representation also accounts\nfor the learning of sensorimotor primitives in a latent learning context.", "published": "2020-06-20 01:58:28", "link": "http://arxiv.org/abs/2006.11465v2", "categories": ["cs.NE", "cs.AI", "cs.CL"], "primary_category": "cs.NE"}
{"title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech\n  Representations", "abstract": "We show for the first time that learning powerful representations from speech\naudio alone followed by fine-tuning on transcribed speech can outperform the\nbest semi-supervised methods while being conceptually simpler. wav2vec 2.0\nmasks the speech input in the latent space and solves a contrastive task\ndefined over a quantization of the latent representations which are jointly\nlearned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER\non the clean/other test sets. When lowering the amount of labeled data to one\nhour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour\nsubset while using 100 times less labeled data. Using just ten minutes of\nlabeled data and pre-training on 53k hours of unlabeled data still achieves\n4.8/8.2 WER. This demonstrates the feasibility of speech recognition with\nlimited amounts of labeled data.", "published": "2020-06-20 02:35:02", "link": "http://arxiv.org/abs/2006.11477v3", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Memory Transformer", "abstract": "Transformer-based models have achieved state-of-the-art results in many\nnatural language processing tasks. The self-attention architecture allows\ntransformer to combine information from all elements of a sequence into\ncontext-aware representations. However, information about the context is stored\nmostly in the same element-wise representations. This might limit the\nprocessing of properties related to the sequence as a whole more difficult.\nAdding trainable memory to selectively store local as well as global\nrepresentations of a sequence is a promising direction to improve the\nTransformer model. Memory-augmented neural networks (MANNs) extend traditional\nneural architectures with general-purpose memory for representations. MANNs\nhave demonstrated the capability to learn simple algorithms like Copy or\nReverse and can be successfully trained via backpropagation on diverse tasks\nfrom question answering to language modeling outperforming RNNs and LSTMs of\ncomparable complexity. In this work, we propose and study few extensions of the\nTransformer baseline (1) by adding memory tokens to store non-local\nrepresentations, (2) creating memory bottleneck for the global information, (3)\ncontrolling memory update with dedicated layer. We evaluate these memory\naugmented Transformers and demonstrate that presence of memory positively\ncorrelates with the model performance for machine translation and language\nmodelling tasks. Augmentation of pre-trained masked language model with memory\ntokens shows mixed results for tasks from GLUE benchmark. Visualization of\nattention patterns over the memory suggest that it improves the model's ability\nto process a global context.", "published": "2020-06-20 09:06:27", "link": "http://arxiv.org/abs/2006.11527v2", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Speaker conditioned acoustic-to-articulatory inversion using x-vectors", "abstract": "Speech production involves the movement of various articulators, including\ntongue, jaw, and lips. Estimating the movement of the articulators from the\nacoustics of speech is known as acoustic-to-articulatory inversion (AAI).\nRecently, it has been shown that instead of training AAI in a speaker specific\nmanner, pooling the acoustic-articulatory data from multiple speakers is\nbeneficial. Further, additional conditioning with speaker specific information\nby one-hot encoding at the input of AAI along with acoustic features benefits\nthe AAI performance in a closed-set speaker train and test condition. In this\nwork, we carry out an experimental study on the benefit of using x-vectors for\nproviding speaker specific information to condition AAI. Experiments with 30\nspeakers have shown that the AAI performance benefits from the use of x-vectors\nin a closed set seen speaker condition. Further, x-vectors also generalizes\nwell for unseen speaker evaluation.", "published": "2020-06-20 10:08:06", "link": "http://arxiv.org/abs/2006.11536v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Speaker Independent and Multilingual/Mixlingual Speech-Driven Talking\n  Head Generation Using Phonetic Posteriorgrams", "abstract": "Generating 3D speech-driven talking head has received more and more attention\nin recent years. Recent approaches mainly have following limitations: 1) most\nspeaker-independent methods need handcrafted features that are time-consuming\nto design or unreliable; 2) there is no convincing method to support\nmultilingual or mixlingual speech as input. In this work, we propose a novel\napproach using phonetic posteriorgrams (PPG). In this way, our method doesn't\nneed hand-crafted features and is more robust to noise compared to recent\napproaches. Furthermore, our method can support multilingual speech as input by\nbuilding a universal phoneme space. As far as we know, our model is the first\nto support multilingual/mixlingual speech as input with convincing results.\nObjective and subjective experiments have shown that our model can generate\nhigh quality animations given speech from unseen languages or speakers and be\nrobust to noise.", "published": "2020-06-20 16:32:43", "link": "http://arxiv.org/abs/2006.11610v1", "categories": ["eess.AS", "cs.LG", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Embodied Self-supervised Learning by Coordinated Sampling and Training", "abstract": "Self-supervised learning can significantly improve the performance of\ndownstream tasks, however, the dimensions of learned representations normally\nlack explicit physical meanings. In this work, we propose a novel\nself-supervised approach to solve inverse problems by employing the\ncorresponding physical forward process so that the learned representations can\nhave explicit physical meanings. The proposed approach works in an\nanalysis-by-synthesis manner to learn an inference network by iteratively\nsampling and training. At the sampling step, given observed data, the inference\nnetwork is used to approximate the intractable posterior, from which we sample\ninput parameters and feed them to a physical process to generate data in the\nobservational space; At the training step, the same network is optimized with\nthe sampled paired data. We prove the feasibility of the proposed method by\ntackling the acoustic-to-articulatory inversion problem to infer articulatory\ninformation from speech. Given an articulatory synthesizer, an inference model\ncan be trained completely from scratch with random initialization. Our\nexperiments demonstrate that the proposed method can converge steadily and the\nnetwork learns to control the articulatory synthesizer to speak like a human.\nWe also demonstrate that trained models can generalize well to unseen speakers\nor even new languages, and performance can be further improved through\nself-adaptation.", "published": "2020-06-20 14:05:47", "link": "http://arxiv.org/abs/2006.13350v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
