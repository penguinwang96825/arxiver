{"title": "MasonPerplexity at Multimodal Hate Speech Event Detection 2024: Hate\n  Speech and Target Detection Using Transformer Ensembles", "abstract": "The automatic identification of offensive language such as hate speech is\nimportant to keep discussions civil in online communities. Identifying hate\nspeech in multimodal content is a particularly challenging task because\noffensiveness can be manifested in either words or images or a juxtaposition of\nthe two. This paper presents the MasonPerplexity submission for the Shared Task\non Multimodal Hate Speech Event Detection at CASE 2024 at EACL 2024. The task\nis divided into two sub-tasks: sub-task A focuses on the identification of hate\nspeech and sub-task B focuses on the identification of targets in text-embedded\nimages during political events. We use an XLM-roBERTa-large model for sub-task\nA and an ensemble approach combining XLM-roBERTa-base, BERTweet-large, and\nBERT-base for sub-task B. Our approach obtained 0.8347 F1-score in sub-task A\nand 0.6741 F1-score in sub-task B ranking 3rd on both sub-tasks.", "published": "2024-02-03 00:23:36", "link": "http://arxiv.org/abs/2402.01967v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MasonPerplexity at ClimateActivism 2024: Integrating Advanced Ensemble\n  Techniques and Data Augmentation for Climate Activism Stance and Hate Event\n  Identification", "abstract": "The task of identifying public opinions on social media, particularly\nregarding climate activism and the detection of hate events, has emerged as a\ncritical area of research in our rapidly changing world. With a growing number\nof people voicing either to support or oppose to climate-related issues -\nunderstanding these diverse viewpoints has become increasingly vital. Our team,\nMasonPerplexity, participates in a significant research initiative focused on\nthis subject. We extensively test various models and methods, discovering that\nour most effective results are achieved through ensemble modeling, enhanced by\ndata augmentation techniques like back-translation. In the specific components\nof this research task, our team achieved notable positions, ranking 5th, 1st,\nand 6th in the respective sub-tasks, thereby illustrating the effectiveness of\nour approach in this important field of study.", "published": "2024-02-03 01:06:33", "link": "http://arxiv.org/abs/2402.01976v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SOCIALITE-LLAMA: An Instruction-Tuned Model for Social Scientific Tasks", "abstract": "Social science NLP tasks, such as emotion or humor detection, are required to\ncapture the semantics along with the implicit pragmatics from text, often with\nlimited amounts of training data. Instruction tuning has been shown to improve\nthe many capabilities of large language models (LLMs) such as commonsense\nreasoning, reading comprehension, and computer programming. However, little is\nknown about the effectiveness of instruction tuning on the social domain where\nimplicit pragmatic cues are often needed to be captured. We explore the use of\ninstruction tuning for social science NLP tasks and introduce Socialite-Llama\n-- an open-source, instruction-tuned Llama. On a suite of 20 social science\ntasks, Socialite-Llama improves upon the performance of Llama as well as\nmatches or improves upon the performance of a state-of-the-art, multi-task\nfinetuned model on a majority of them. Further, Socialite-Llama also leads to\nimprovement on 5 out of 6 related social tasks as compared to Llama, suggesting\ninstruction tuning can lead to generalized social understanding. All resources\nincluding our code, model and dataset can be found through\nbit.ly/socialitellama.", "published": "2024-02-03 01:33:16", "link": "http://arxiv.org/abs/2402.01980v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Panacea: Pareto Alignment via Preference Adaptation for LLMs", "abstract": "Current methods for large language model alignment typically use scalar human\npreference labels. However, this convention tends to oversimplify the\nmulti-dimensional and heterogeneous nature of human preferences, leading to\nreduced expressivity and even misalignment. This paper presents Panacea, an\ninnovative approach that reframes alignment as a multi-dimensional preference\noptimization problem. Panacea trains a single model capable of adapting online\nand Pareto-optimally to diverse sets of preferences without the need for\nfurther tuning. A major challenge here is using a low-dimensional preference\nvector to guide the model's behavior, despite it being governed by an\noverwhelmingly large number of parameters. To address this, Panacea is designed\nto use singular value decomposition (SVD)-based low-rank adaptation, which\nallows the preference vector to be simply injected online as singular values.\nTheoretically, we prove that Panacea recovers the entire Pareto front with\ncommon loss aggregation methods under mild conditions. Moreover, our\nexperiments demonstrate, for the first time, the feasibility of aligning a\nsingle LLM to represent an exponentially vast spectrum of human preferences\nthrough various optimization methods. Our work marks a step forward in\neffectively and efficiently aligning models to diverse and intricate human\npreferences in a controllable and Pareto-optimal manner.", "published": "2024-02-03 05:01:04", "link": "http://arxiv.org/abs/2402.02030v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Content Planning for Navigating Trade-offs in\n  Knowledge-Grounded Dialogue", "abstract": "Knowledge-grounded dialogue generation is a challenging task because it\nrequires satisfying two fundamental yet often competing constraints: being\nresponsive in a manner that is specific to what the conversation partner has\nsaid while also being attributable to an underlying source document. In this\nwork, we bring this trade-off between these two objectives (specificity and\nattribution) to light and ask the question: Can explicit content planning\nbefore the response generation help the model to address this challenge? To\nanswer this question, we design a framework called PLEDGE, which allows us to\nexperiment with various plan variables explored in prior work, supporting both\nmetric-agnostic and metric-aware approaches. While content planning shows\npromise, our results on whether it can actually help to navigate this trade-off\nare mixed -- planning mechanisms that are metric-aware (use automatic metrics\nduring training) are better at automatic evaluations but underperform in human\njudgment compared to metric-agnostic mechanisms. We discuss how this may be\ncaused by over-fitting to automatic metrics and the need for future work to\nbetter calibrate these metrics towards human judgment. We hope the observations\nfrom our analysis will inform future work that aims to apply content planning\nin this context.", "published": "2024-02-03 08:16:39", "link": "http://arxiv.org/abs/2402.02077v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Robustness of Task-oriented Dialogue Systems for\n  Colloquial German Varieties", "abstract": "Mainstream cross-lingual task-oriented dialogue (ToD) systems leverage the\ntransfer learning paradigm by training a joint model for intent recognition and\nslot-filling in English and applying it, zero-shot, to other languages. We\naddress a gap in prior research, which often overlooked the transfer to\nlower-resource colloquial varieties due to limited test data. Inspired by prior\nwork on English varieties, we craft and manually evaluate perturbation rules\nthat transform German sentences into colloquial forms and use them to\nsynthesize test sets in four ToD datasets. Our perturbation rules cover 18\ndistinct language phenomena, enabling us to explore the impact of each\nperturbation on slot and intent performance. Using these new datasets, we\nconduct an experimental evaluation across six different transformers. Here, we\ndemonstrate that when applied to colloquial varieties, ToD systems maintain\ntheir intent recognition performance, losing 6% (4.62 percentage points) in\naccuracy on average. However, they exhibit a significant drop in slot\ndetection, with a decrease of 31% (21 percentage points) in slot F1 score. Our\nfindings are further supported by a transfer experiment from Standard American\nEnglish to synthetic Urban African American Vernacular English.", "published": "2024-02-03 08:16:43", "link": "http://arxiv.org/abs/2402.02078v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Translation Errors Significantly Impact Low-Resource Languages in\n  Cross-Lingual Learning", "abstract": "Popular benchmarks (e.g., XNLI) used to evaluate cross-lingual language\nunderstanding consist of parallel versions of English evaluation sets in\nmultiple target languages created with the help of professional translators.\nWhen creating such parallel data, it is critical to ensure high-quality\ntranslations for all target languages for an accurate characterization of\ncross-lingual transfer. In this work, we find that translation inconsistencies\ndo exist and interestingly they disproportionally impact low-resource languages\nin XNLI. To identify such inconsistencies, we propose measuring the gap in\nperformance between zero-shot evaluations on the human-translated and\nmachine-translated target text across multiple target languages; relatively\nlarge gaps are indicative of translation errors. We also corroborate that\ntranslation errors exist for two target languages, namely Hindi and Urdu, by\ndoing a manual reannotation of human-translated test instances in these two\nlanguages and finding poor agreement with the original English labels these\ninstances were supposed to inherit.", "published": "2024-02-03 08:22:51", "link": "http://arxiv.org/abs/2402.02080v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GliDe with a CaPE: A Low-Hassle Method to Accelerate Speculative\n  Decoding", "abstract": "Speculative decoding is a relatively new decoding framework that leverages\nsmall and efficient draft models to reduce the latency of LLMs. In this study,\nwe introduce GliDe and CaPE, two low-hassle modifications to vanilla\nspeculative decoding to further improve the decoding speed of a frozen LLM.\nSpecifically, GliDe is a modified draft model architecture that reuses the\ncached keys and values from the target LLM, while CaPE is a proposal expansion\nmethod that uses the draft model's confidence scores to help select additional\ncandidate tokens for verification. Extensive experiments on different\nbenchmarks demonstrate that our proposed GliDe draft model significantly\nreduces the expected decoding latency. Additional evaluation using walltime\nreveals that GliDe can accelerate Vicuna models up to 2.17x and further extend\nthe improvement to 2.61x with CaPE. We will release our code, data, and the\ntrained draft models.", "published": "2024-02-03 08:44:11", "link": "http://arxiv.org/abs/2402.02082v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting the Markov Property for Machine Translation", "abstract": "In this paper, we re-examine the Markov property in the context of neural\nmachine translation. We design a Markov Autoregressive Transformer~(MAT) and\nundertake a comprehensive assessment of its performance across four WMT\nbenchmarks. Our findings indicate that MAT with an order larger than 4 can\ngenerate translations with quality on par with that of conventional\nautoregressive transformers. In addition, counter-intuitively, we also find\nthat the advantages of utilizing a higher-order MAT do not specifically\ncontribute to the translation of longer sentences.", "published": "2024-02-03 08:50:50", "link": "http://arxiv.org/abs/2402.02084v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-shot Sentiment Analysis in Low-Resource Languages Using a\n  Multilingual Sentiment Lexicon", "abstract": "Improving multilingual language models capabilities in low-resource languages\nis generally difficult due to the scarcity of large-scale data in those\nlanguages. In this paper, we relax the reliance on texts in low-resource\nlanguages by using multilingual lexicons in pretraining to enhance multilingual\ncapabilities. Specifically, we focus on zero-shot sentiment analysis tasks\nacross 34 languages, including 6 high/medium-resource languages, 25\nlow-resource languages, and 3 code-switching datasets. We demonstrate that\npretraining using multilingual lexicons, without using any sentence-level\nsentiment data, achieves superior zero-shot performance compared to models\nfine-tuned on English sentiment datasets, and large language models like\nGPT--3.5, BLOOMZ, and XGLM. These findings are observable for unseen\nlow-resource languages to code-mixed scenarios involving high-resource\nlanguages.", "published": "2024-02-03 10:41:05", "link": "http://arxiv.org/abs/2402.02113v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GITA: Graph to Visual and Textual Integration for Vision-Language Graph\n  Reasoning", "abstract": "Large Language Models (LLMs) are increasingly used for various tasks with\ngraph structures. Though LLMs can process graph information in a textual\nformat, they overlook the rich vision modality, which is an intuitive way for\nhumans to comprehend structural information and conduct general graph\nreasoning. The potential benefits and capabilities of representing graph\nstructures as visual images (i.e., $\\textit{visual graph}$) are still\nunexplored. To fill the gap, we innovatively propose an end-to-end framework,\ncalled $\\textbf{G}$raph to v$\\textbf{I}$sual and $\\textbf{T}$extual\nIntegr$\\textbf{A}$tion (GITA), which firstly incorporates visual graphs into\ngeneral graph reasoning. Besides, we establish $\\textbf{G}$raph-based\n$\\textbf{V}$ision-$\\textbf{L}$anguage $\\textbf{Q}$uestion $\\textbf{A}$nswering\n(GVLQA) dataset from existing graph data, which is the first vision-language\ndataset for general graph reasoning purposes. Extensive experiments on the\nGVLQA dataset and five real-world datasets show that GITA outperforms\nmainstream LLMs in terms of general graph reasoning capabilities. Moreover, We\nhighlight the effectiveness of the layout augmentation on visual graphs and\npretraining on the GVLQA dataset.", "published": "2024-02-03 12:19:47", "link": "http://arxiv.org/abs/2402.02130v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing Critical Learning Dynamics of PLMs for Hate Speech Detection", "abstract": "Despite the widespread adoption, there is a lack of research into how various\ncritical aspects of pretrained language models (PLMs) affect their performance\nin hate speech detection. Through five research questions, our findings and\nrecommendations lay the groundwork for empirically investigating different\naspects of PLMs' use in hate speech detection. We deep dive into comparing\ndifferent pretrained models, evaluating their seed robustness, finetuning\nsettings, and the impact of pretraining data collection time. Our analysis\nreveals early peaks for downstream tasks during pretraining, the limited\nbenefit of employing a more recent pretraining corpus, and the significance of\nspecific layers during finetuning. We further call into question the use of\ndomain-specific models and highlight the need for dynamic datasets for\nbenchmarking hate speech detection.", "published": "2024-02-03 13:23:51", "link": "http://arxiv.org/abs/2402.02144v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Sentiment Polarity Reduction in News Presentation through\n  Contextual Perturbation and Large Language Models", "abstract": "In today's media landscape, where news outlets play a pivotal role in shaping\npublic opinion, it is imperative to address the issue of sentiment manipulation\nwithin news text. News writers often inject their own biases and emotional\nlanguage, which can distort the objectivity of reporting. This paper introduces\na novel approach to tackle this problem by reducing the polarity of latent\nsentiments in news content. Drawing inspiration from adversarial attack-based\nsentence perturbation techniques and a prompt based method using ChatGPT, we\nemploy transformation constraints to modify sentences while preserving their\ncore semantics. Using three perturbation methods: replacement, insertion, and\ndeletion coupled with a context-aware masked language model, we aim to maximize\nthe desired sentiment score for targeted news aspects through a beam search\nalgorithm. Our experiments and human evaluations demonstrate the effectiveness\nof these two models in achieving reduced sentiment polarity with minimal\nmodifications while maintaining textual similarity, fluency, and grammatical\ncorrectness. Comparative analysis confirms the competitive performance of the\nadversarial attack based perturbation methods and prompt-based methods,\noffering a promising solution to foster more objective news reporting and\ncombat emotional language bias in the media.", "published": "2024-02-03 13:27:32", "link": "http://arxiv.org/abs/2402.02145v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey to Recent Progress Towards Understanding In-Context Learning", "abstract": "In-Context Learning (ICL) empowers Large Language Models (LLMs) with the\nability to learn from a few examples provided in the prompt, enabling\ndownstream generalization without the requirement for gradient updates. Despite\nencouragingly empirical success, the underlying mechanism of ICL remains\nunclear. Existing research remains ambiguous with various viewpoints, utilizing\nintuition-driven and ad-hoc technical solutions to interpret ICL. In this\npaper, we leverage a data generation perspective to reinterpret recent efforts\nfrom a systematic angle, demonstrating the potential broader usage of these\npopular technical solutions. For a conceptual definition, we rigorously adopt\nthe terms of skill recognition and skill learning. Skill recognition selects\none learned data generation function previously seen during pre-training while\nskill learning can learn new data generation functions from in-context data.\nFurthermore, we provide insights into the strengths and weaknesses of both\nabilities, emphasizing their commonalities through the perspective of data\ngeneration. This analysis suggests potential directions for future research.", "published": "2024-02-03 17:13:03", "link": "http://arxiv.org/abs/2402.02212v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How well do LLMs cite relevant medical references? An evaluation\n  framework and analyses", "abstract": "Large language models (LLMs) are currently being used to answer medical\nquestions across a variety of clinical domains. Recent top-performing\ncommercial LLMs, in particular, are also capable of citing sources to support\ntheir responses. In this paper, we ask: do the sources that LLMs generate\nactually support the claims that they make? To answer this, we propose three\ncontributions. First, as expert medical annotations are an expensive and\ntime-consuming bottleneck for scalable evaluation, we demonstrate that GPT-4 is\nhighly accurate in validating source relevance, agreeing 88% of the time with a\npanel of medical doctors. Second, we develop an end-to-end, automated pipeline\ncalled \\textit{SourceCheckup} and use it to evaluate five top-performing LLMs\non a dataset of 1200 generated questions, totaling over 40K pairs of statements\nand sources. Interestingly, we find that between ~50% to 90% of LLM responses\nare not fully supported by the sources they provide. We also evaluate GPT-4\nwith retrieval augmented generation (RAG) and find that, even still, around\n30\\% of individual statements are unsupported, while nearly half of its\nresponses are not fully supported. Third, we open-source our curated dataset of\nmedical questions and expert annotations for future evaluations. Given the\nrapid pace of LLM development and the potential harms of incorrect or outdated\nmedical information, it is crucial to also understand and quantify their\ncapability to produce relevant, trustworthy medical references.", "published": "2024-02-03 03:44:57", "link": "http://arxiv.org/abs/2402.02008v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EffiBench: Benchmarking the Efficiency of Automatically Generated Code", "abstract": "Code generation models have increasingly become integral to aiding software\ndevelopment. Although current research has thoroughly examined the correctness\nof the code produced by code generation models, a vital aspect that plays a\npivotal role in green computing and sustainability efforts has often been\nneglected. This paper presents EffiBench, a benchmark with 1,000\nefficiency-critical coding problems to assess the efficiency of code generated\nby code generation models. EffiBench contains a diverse set of LeetCode coding\nproblems. Each problem is paired with an executable human-written canonical\nsolution, which obtains the SOTA efficiency on the LeetCode solution\nleaderboard. With EffiBench, we empirically examine the ability of 42 large\nlanguage models (35 open-source and 7 closed-source) to generate efficient\ncode. Our evaluation results demonstrate that the efficiency of the code\ngenerated by LLMs is generally worse than the efficiency of human-written\ncanonical solutions. For example, GPT-4 generated code has an average\n\\textbf{3.12} times execution time that of the human-written canonical\nsolutions. In the most extreme cases, the execution time and total memory usage\nof GPT-4 generated code are \\textbf{13.89} and \\textbf{43.92} times that of the\ncanonical solutions. The source code of EffiBench is released on\nhttps://github.com/huangd1999/EffiBench. We also provide the LeaderBoard at\nhttps://huggingface.co/spaces/EffiBench/effibench-leaderboard.", "published": "2024-02-03 05:24:39", "link": "http://arxiv.org/abs/2402.02037v5", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Break the Sequential Dependency of LLM Inference Using Lookahead\n  Decoding", "abstract": "Autoregressive decoding of large language models (LLMs) is memory bandwidth\nbounded, resulting in high latency and significant wastes of the parallel\nprocessing power of modern accelerators. Existing methods for accelerating LLM\ndecoding often require a draft model (e.g., speculative decoding), which is\nnontrivial to obtain and unable to generalize. In this paper, we introduce\nLookahead decoding, an exact, parallel decoding algorithm that accelerates LLM\ndecoding without needing auxiliary models or data stores. It allows trading\nper-step log(FLOPs) to reduce the number of total decoding steps, is more\nparallelizable on single or multiple modern accelerators, and is compatible\nwith concurrent memory-efficient attention (e.g., FlashAttention). Our\nimplementation of Lookahead decoding can speed up autoregressive decoding by up\nto 1.8x on MT-bench and 4x with strong scaling on multiple GPUs in code\ncompletion tasks. Our code is avialable at\nhttps://github.com/hao-ai-lab/LookaheadDecoding", "published": "2024-02-03 06:37:50", "link": "http://arxiv.org/abs/2402.02057v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Are Large Language Models Good Prompt Optimizers?", "abstract": "LLM-based Automatic Prompt Optimization, which typically utilizes LLMs as\nPrompt Optimizers to self-reflect and refine prompts, has shown promising\nperformance in recent studies. Despite the success, the underlying mechanism of\nthis approach remains unexplored, and the true effectiveness of LLMs as Prompt\nOptimizers requires further validation. In this work, we conducted a\ncomprehensive study to uncover the actual mechanism of LLM-based Prompt\nOptimization. Our findings reveal that the LLM optimizers struggle to identify\nthe true causes of errors during reflection, tending to be biased by their own\nprior knowledge rather than genuinely reflecting on the errors. Furthermore,\neven when the reflection is semantically valid, the LLM optimizers often fail\nto generate appropriate prompts for the target models with a single prompt\nrefinement step, partly due to the unpredictable behaviors of the target\nmodels. Based on the observations, we introduce a new \"Automatic Behavior\nOptimization\" paradigm, which directly optimizes the target model's behavior in\na more controllable manner. We hope our study can inspire new directions for\nautomatic prompt optimization development.", "published": "2024-02-03 09:48:54", "link": "http://arxiv.org/abs/2402.02101v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do Moral Judgment and Reasoning Capability of LLMs Change with Language?\n  A Study using the Multilingual Defining Issues Test", "abstract": "This paper explores the moral judgment and moral reasoning abilities\nexhibited by Large Language Models (LLMs) across languages through the Defining\nIssues Test. It is a well known fact that moral judgment depends on the\nlanguage in which the question is asked. We extend the work of beyond English,\nto 5 new languages (Chinese, Hindi, Russian, Spanish and Swahili), and probe\nthree LLMs -- ChatGPT, GPT-4 and Llama2Chat-70B -- that shows substantial\nmultilingual text processing and generation abilities. Our study shows that the\nmoral reasoning ability for all models, as indicated by the post-conventional\nscore, is substantially inferior for Hindi and Swahili, compared to Spanish,\nRussian, Chinese and English, while there is no clear trend for the performance\nof the latter four languages. The moral judgments too vary considerably by the\nlanguage.", "published": "2024-02-03 12:52:36", "link": "http://arxiv.org/abs/2402.02135v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Complex Question Answering over Knowledge Graphs through\n  Evidence Pattern Retrieval", "abstract": "Information retrieval (IR) methods for KGQA consist of two stages: subgraph\nextraction and answer reasoning. We argue current subgraph extraction methods\nunderestimate the importance of structural dependencies among evidence facts.\nWe propose Evidence Pattern Retrieval (EPR) to explicitly model the structural\ndependencies during subgraph extraction. We implement EPR by indexing the\natomic adjacency pattern of resource pairs. Given a question, we perform dense\nretrieval to obtain atomic patterns formed by resource pairs. We then enumerate\ntheir combinations to construct candidate evidence patterns. These evidence\npatterns are scored using a neural model, and the best one is selected to\nextract a subgraph for downstream answer reasoning. Experimental results\ndemonstrate that the EPR-based approach has significantly improved the F1\nscores of IR-KGQA methods by over 10 points on ComplexWebQuestions and achieves\ncompetitive performance on WebQuestionsSP.", "published": "2024-02-03 14:54:13", "link": "http://arxiv.org/abs/2402.02175v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Language Writ Large: LLMs, ChatGPT, Grounding, Meaning and Understanding", "abstract": "Apart from what (little) OpenAI may be concealing from us, we all know\n(roughly) how ChatGPT works (its huge text database, its statistics, its vector\nrepresentations, and their huge number of parameters, its next-word training,\nand so on). But none of us can say (hand on heart) that we are not surprised by\nwhat ChatGPT has proved to be able to do with these resources. This has even\ndriven some of us to conclude that ChatGPT actually understands. It is not true\nthat it understands. But it is also not true that we understand how it can do\nwhat it can do. I will suggest some hunches about benign biases: convergent\nconstraints that emerge at LLM scale that may be helping ChatGPT do so much\nbetter than we would have expected. These biases are inherent in the nature of\nlanguage itself, at LLM scale, and they are closely linked to what it is that\nChatGPT lacks, which is direct sensorimotor grounding to connect its words to\ntheir referents and its propositions to their meanings. These convergent biases\nare related to (1) the parasitism of indirect verbal grounding on direct\nsensorimotor grounding, (2) the circularity of verbal definition, (3) the\nmirroring of language production and comprehension, (4) iconicity in\npropositions at LLM scale, (5) computational counterparts of human categorical\nperception in category learning by neural nets, and perhaps also (6) a\nconjecture by Chomsky about the laws of thought. The exposition will be in the\nform of a dialogue with ChatGPT-4.", "published": "2024-02-03 19:19:34", "link": "http://arxiv.org/abs/2402.02243v2", "categories": ["cs.CL", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Beyond the Limits: A Survey of Techniques to Extend the Context Length\n  in Large Language Models", "abstract": "Recently, large language models (LLMs) have shown remarkable capabilities\nincluding understanding context, engaging in logical reasoning, and generating\nresponses. However, this is achieved at the expense of stringent computational\nand memory requirements, hindering their ability to effectively support long\ninput sequences. This survey provides an inclusive review of the recent\ntechniques and methods devised to extend the sequence length in LLMs, thereby\nenhancing their capacity for long-context understanding. In particular, we\nreview and categorize a wide range of techniques including architectural\nmodifications, such as modified positional encoding and altered attention\nmechanisms, which are designed to enhance the processing of longer sequences\nwhile avoiding a proportional increase in computational requirements. The\ndiverse methodologies investigated in this study can be leveraged across\ndifferent phases of LLMs, i.e., training, fine-tuning and inference. This\nenables LLMs to efficiently process extended sequences. The limitations of the\ncurrent methodologies is discussed in the last section along with the\nsuggestions for future research directions, underscoring the importance of\nsequence length in the continued advancement of LLMs.", "published": "2024-02-03 19:20:02", "link": "http://arxiv.org/abs/2402.02244v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Frequency Explains the Inverse Correlation of Large Language Models'\n  Size, Training Data Amount, and Surprisal's Fit to Reading Times", "abstract": "Recent studies have shown that as Transformer-based language models become\nlarger and are trained on very large amounts of data, the fit of their\nsurprisal estimates to naturalistic human reading times degrades. The current\nwork presents a series of analyses showing that word frequency is a key\nexplanatory factor underlying these two trends. First, residual errors from\nfour language model families on four corpora show that the inverse correlation\nbetween model size and fit to reading times is the strongest on the subset of\nleast frequent words, which is driven by excessively accurate predictions of\nlarger model variants. Additionally, training dynamics reveal that during later\ntraining steps, all model variants learn to predict rare words and that larger\nmodel variants do so more accurately, which explains the detrimental effect of\nboth training data amount and model size on fit to reading times. Finally, a\nfeature attribution analysis demonstrates that larger model variants are able\nto accurately predict rare words based on both an effectively longer context\nwindow size as well as stronger local associations compared to smaller model\nvariants. Taken together, these results indicate that Transformer-based\nlanguage models' surprisal estimates diverge from human-like expectations due\nto the superhumanly complex associations they learn for predicting rare words.", "published": "2024-02-03 20:22:54", "link": "http://arxiv.org/abs/2402.02255v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Data Quality Matters: Suicide Intention Detection on Social Media Posts\n  Using RoBERTa-CNN", "abstract": "Suicide remains a pressing global health concern, necessitating innovative\napproaches for early detection and intervention. This paper focuses on\nidentifying suicidal intentions in posts from the SuicideWatch subreddit by\nproposing a novel deep-learning approach that utilizes the state-of-the-art\nRoBERTa-CNN model. The robustly Optimized BERT Pretraining Approach (RoBERTa)\nexcels at capturing textual nuances and forming semantic relationships within\nthe text. The remaining Convolutional Neural Network (CNN) head enhances\nRoBERTa's capacity to discern critical patterns from extensive datasets. To\nevaluate RoBERTa-CNN, we conducted experiments on the Suicide and Depression\nDetection dataset, yielding promising results. For instance, RoBERTa-CNN\nachieves a mean accuracy of 98% with a standard deviation (STD) of 0.0009.\nAdditionally, we found that data quality significantly impacts the training of\na robust model. To improve data quality, we removed noise from the text data\nwhile preserving its contextual content through either manually cleaning or\nutilizing the OpenAI API.", "published": "2024-02-03 20:58:09", "link": "http://arxiv.org/abs/2402.02262v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SemPool: Simple, robust, and interpretable KG pooling for enhancing\n  language models", "abstract": "Knowledge Graph (KG) powered question answering (QA) performs complex\nreasoning over language semantics as well as knowledge facts. Graph Neural\nNetworks (GNNs) learn to aggregate information from the underlying KG, which is\ncombined with Language Models (LMs) for effective reasoning with the given\nquestion. However, GNN-based methods for QA rely on the graph information of\nthe candidate answer nodes, which limits their effectiveness in more\nchallenging settings where critical answer information is not included in the\nKG. We propose a simple graph pooling approach that learns useful semantics of\nthe KG that can aid the LM's reasoning and that its effectiveness is robust\nunder graph perturbations. Our method, termed SemPool, represents KG facts with\npre-trained LMs, learns to aggregate their semantic information, and fuses it\nat different layers of the LM. Our experimental results show that SemPool\noutperforms state-of-the-art GNN-based methods by 2.27% accuracy points on\naverage when answer information is missing from the KG. In addition, SemPool\noffers interpretability on what type of graph information is fused at different\nLM layers.", "published": "2024-02-03 23:03:51", "link": "http://arxiv.org/abs/2402.02289v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Predicting positive transfer for improved low-resource speech\n  recognition using acoustic pseudo-tokens", "abstract": "While massively multilingual speech models like wav2vec 2.0 XLSR-128 can be\ndirectly fine-tuned for automatic speech recognition (ASR), downstream\nperformance can still be relatively poor on languages that are\nunder-represented in the pre-training data. Continued pre-training on 70-200\nhours of untranscribed speech in these languages can help -- but what about\nlanguages without that much recorded data? For such cases, we show that\nsupplementing the target language with data from a similar, higher-resource\n'donor' language can help. For example, continued pre-training on only 10 hours\nof low-resource Punjabi supplemented with 60 hours of donor Hindi is almost as\ngood as continued pretraining on 70 hours of Punjabi. By contrast, sourcing\ndata from less similar donors like Bengali does not improve ASR performance. To\ninform donor language selection, we propose a novel similarity metric based on\nthe sequence distribution of induced acoustic units: the Acoustic Token\nDistribution Similarity (ATDS). Across a set of typologically different target\nlanguages (Punjabi, Galician, Iban, Setswana), we show that the ATDS between\nthe target language and its candidate donors precisely predicts target language\nASR performance.", "published": "2024-02-03 23:54:03", "link": "http://arxiv.org/abs/2402.02302v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "A Closer Look at the Limitations of Instruction Tuning", "abstract": "Instruction Tuning (IT), the process of training large language models (LLMs)\nusing instruction-response pairs, has emerged as the predominant method for\ntransforming base pre-trained LLMs into open-domain conversational agents.\nWhile IT has achieved notable success and widespread adoption, its limitations\nand shortcomings remain underexplored. In this paper, through rigorous\nexperiments and an in-depth analysis of the changes LLMs undergo through IT, we\nreveal various limitations of IT. In particular, we show that (1) IT fails to\nenhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning\nresponse initiation and style tokens, and full-parameter fine-tuning leads to\nknowledge degradation. (2) Copying response patterns from IT datasets derived\nfrom knowledgeable sources leads to a decline in response quality. (3)\nFull-parameter fine-tuning increases hallucination by inaccurately borrowing\ntokens from conceptually similar instances in the IT dataset for generating\nresponses. (4) Popular methods to improve IT do not lead to performance\nimprovements over a simple LoRA fine-tuned model. Our findings reveal that\nresponses generated solely from pre-trained knowledge consistently outperform\nresponses by models that learn any form of new knowledge from IT on open-source\ndatasets. We hope the insights and challenges revealed in this paper inspire\nfuture work in related directions.", "published": "2024-02-03 04:45:25", "link": "http://arxiv.org/abs/2402.05119v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DE$^3$-BERT: Distance-Enhanced Early Exiting for BERT based on\n  Prototypical Networks", "abstract": "Early exiting has demonstrated its effectiveness in accelerating the\ninference of pre-trained language models like BERT by dynamically adjusting the\nnumber of layers executed. However, most existing early exiting methods only\nconsider local information from an individual test sample to determine their\nexiting indicators, failing to leverage the global information offered by\nsample population. This leads to suboptimal estimation of prediction\ncorrectness, resulting in erroneous exiting decisions. To bridge the gap, we\nexplore the necessity of effectively combining both local and global\ninformation to ensure reliable early exiting during inference. Purposefully, we\nleverage prototypical networks to learn class prototypes and devise a distance\nmetric between samples and class prototypes. This enables us to utilize global\ninformation for estimating the correctness of early predictions. On this basis,\nwe propose a novel Distance-Enhanced Early Exiting framework for BERT\n(DE$^3$-BERT). DE$^3$-BERT implements a hybrid exiting strategy that\nsupplements classic entropy-based local information with distance-based global\ninformation to enhance the estimation of prediction correctness for more\nreliable early exiting decisions. Extensive experiments on the GLUE benchmark\ndemonstrate that DE$^3$-BERT consistently outperforms state-of-the-art models\nunder different speed-up ratios with minimal storage or computational overhead,\nyielding a better trade-off between model performance and inference efficiency.\nAdditionally, an in-depth analysis further validates the generality and\ninterpretability of our method.", "published": "2024-02-03 15:51:17", "link": "http://arxiv.org/abs/2402.05948v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Improving Large-Scale k-Nearest Neighbor Text Categorization with Label\n  Autoencoders", "abstract": "In this paper, we introduce a multi-label lazy learning approach to deal with\nautomatic semantic indexing in large document collections in the presence of\ncomplex and structured label vocabularies with high inter-label correlation.\nThe proposed method is an evolution of the traditional k-Nearest Neighbors\nalgorithm which uses a large autoencoder trained to map the large label space\nto a reduced size latent space and to regenerate the predicted labels from this\nlatent space. We have evaluated our proposal in a large portion of the MEDLINE\nbiomedical document collection which uses the Medical Subject Headings (MeSH)\nthesaurus as a controlled vocabulary. In our experiments we propose and\nevaluate several document representation approaches and different label\nautoencoder configurations.", "published": "2024-02-03 00:11:29", "link": "http://arxiv.org/abs/2402.01963v1", "categories": ["cs.LG", "cs.CL", "cs.IR", "68T50, 68T07", "I.2.6; I.2.7; H.3.1"], "primary_category": "cs.LG"}
{"title": "Self-Debiasing Large Language Models: Zero-Shot Recognition and\n  Reduction of Stereotypes", "abstract": "Large language models (LLMs) have shown remarkable advances in language\ngeneration and understanding but are also prone to exhibiting harmful social\nbiases. While recognition of these behaviors has generated an abundance of bias\nmitigation techniques, most require modifications to the training data, model\nparameters, or decoding strategy, which may be infeasible without access to a\ntrainable model. In this work, we leverage the zero-shot capabilities of LLMs\nto reduce stereotyping in a technique we introduce as zero-shot self-debiasing.\nWith two approaches, self-debiasing via explanation and self-debiasing via\nreprompting, we show that self-debiasing can significantly reduce the degree of\nstereotyping across nine different social groups while relying only on the LLM\nitself and a simple prompt, with explanations correctly identifying invalid\nassumptions and reprompting delivering the greatest reductions in bias. We hope\nthis work opens inquiry into other zero-shot techniques for bias mitigation.", "published": "2024-02-03 01:40:11", "link": "http://arxiv.org/abs/2402.01981v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AnthroScore: A Computational Linguistic Measure of Anthropomorphism", "abstract": "Anthropomorphism, or the attribution of human-like characteristics to\nnon-human entities, has shaped conversations about the impacts and\npossibilities of technology. We present AnthroScore, an automatic metric of\nimplicit anthropomorphism in language. We use a masked language model to\nquantify how non-human entities are implicitly framed as human by the\nsurrounding context. We show that AnthroScore corresponds with human judgments\nof anthropomorphism and dimensions of anthropomorphism described in social\nscience literature. Motivated by concerns of misleading anthropomorphism in\ncomputer science discourse, we use AnthroScore to analyze 15 years of research\npapers and downstream news articles. In research papers, we find that\nanthropomorphism has steadily increased over time, and that papers related to\nlanguage models have the most anthropomorphism. Within ACL papers, temporal\nincreases in anthropomorphism are correlated with key neural advancements.\nBuilding upon concerns of scientific misinformation in mass media, we identify\nhigher levels of anthropomorphism in news headlines compared to the research\npapers they cite. Since AnthroScore is lexicon-free, it can be directly applied\nto a wide range of text sources.", "published": "2024-02-03 06:36:11", "link": "http://arxiv.org/abs/2402.02056v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Analyzing the Evaluation of Cross-Lingual Knowledge Transfer in\n  Multilingual Language Models", "abstract": "Recent advances in training multilingual language models on large datasets\nseem to have shown promising results in knowledge transfer across languages and\nachieve high performance on downstream tasks. However, we question to what\nextent the current evaluation benchmarks and setups accurately measure\nzero-shot cross-lingual knowledge transfer. In this work, we challenge the\nassumption that high zero-shot performance on target tasks reflects high\ncross-lingual ability by introducing more challenging setups involving\ninstances with multiple languages. Through extensive experiments and analysis,\nwe show that the observed high performance of multilingual models can be\nlargely attributed to factors not requiring the transfer of actual linguistic\nknowledge, such as task- and surface-level knowledge. More specifically, we\nobserve what has been transferred across languages is mostly data artifacts and\nbiases, especially for low-resource languages. Our findings highlight the\noverlooked drawbacks of existing cross-lingual test data and evaluation setups,\ncalling for a more nuanced understanding of the cross-lingual capabilities of\nmultilingual models.", "published": "2024-02-03 09:41:52", "link": "http://arxiv.org/abs/2402.02099v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State\n  Tracking", "abstract": "In-context learning with Large Language Models (LLMs) has emerged as a\npromising avenue of research in Dialog State Tracking (DST). However, the\nbest-performing in-context learning methods involve retrieving and adding\nsimilar examples to the prompt, requiring access to labeled training data.\nProcuring such training data for a wide range of domains and applications is\ntime-consuming, expensive, and, at times, infeasible. While zero-shot learning\nrequires no training data, it significantly lags behind the few-shot setup.\nThus, `\\textit{Can we efficiently generate synthetic data for any dialogue\nschema to enable few-shot prompting?}' Addressing this question, we propose\n\\method, a data generation framework tailored for DST, utilizing LLMs. Our\napproach only requires the dialogue schema and a few hand-crafted dialogue\ntemplates to synthesize natural, coherent, and free-flowing dialogues with DST\nannotations. Few-shot learning using data from {\\method} results in $4-5%$\nimprovement in Joint Goal Accuracy over the zero-shot baseline on MultiWOZ 2.1\nand 2.4. Remarkably, our few-shot learning approach recovers nearly $98%$ of\nthe performance compared to the few-shot setup using human-annotated training\ndata. Our synthetic data and code can be accessed at\nhttps://github.com/apple/ml-synthdst", "published": "2024-02-03 22:49:00", "link": "http://arxiv.org/abs/2402.02285v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "More Agents Is All You Need", "abstract": "We find that, simply via a sampling-and-voting method, the performance of\nlarge language models (LLMs) scales with the number of agents instantiated.\nAlso, this method, termed as Agent Forest, is orthogonal to existing\ncomplicated methods to further enhance LLMs, while the degree of enhancement is\ncorrelated to the task difficulty. We conduct comprehensive experiments on a\nwide range of LLM benchmarks to verify the presence of our finding, and to\nstudy the properties that can facilitate its occurrence. Our code is publicly\navailable at: https://github.com/MoreAgentsIsAllYouNeed/AgentForest", "published": "2024-02-03 05:55:24", "link": "http://arxiv.org/abs/2402.05120v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sentiment analysis in non-fixed length audios using a Fully\n  Convolutional Neural Network", "abstract": "In this work, a sentiment analysis method that is capable of accepting audio\nof any length, without being fixed a priori, is proposed. Mel spectrogram and\nMel Frequency Cepstral Coefficients are used as audio description methods and a\nFully Convolutional Neural Network architecture is proposed as a classifier.\nThe results have been validated using three well known datasets: EMODB,\nRAVDESS, and TESS. The results obtained were promising, outperforming the\nstate-of-the-art methods. Also, thanks to the fact that the proposed method\nadmits audios of any size, it allows a sentiment analysis to be made in near\nreal time, which is very interesting for a wide range of fields such as call\ncenters, medical consultations, or financial brokers.", "published": "2024-02-03 15:26:28", "link": "http://arxiv.org/abs/2402.02184v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
