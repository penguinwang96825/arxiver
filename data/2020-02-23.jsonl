{"title": "Fill in the BLANC: Human-free quality estimation of document summaries", "abstract": "We present BLANC, a new approach to the automatic estimation of document\nsummary quality. Our goal is to measure the functional performance of a summary\nwith an objective, reproducible, and fully automated method. Our approach\nachieves this by measuring the performance boost gained by a pre-trained\nlanguage model with access to a document summary while carrying out its\nlanguage understanding task on the document's text. We present evidence that\nBLANC scores have as good correlation with human evaluations as do the ROUGE\nfamily of summary quality measurements. And unlike ROUGE, the BLANC method does\nnot require human-written reference summaries, allowing for fully human-free\nsummary quality estimation.", "published": "2020-02-23 06:21:43", "link": "http://arxiv.org/abs/2002.09836v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automata for Hyperlanguages", "abstract": "Hyperproperties lift conventional trace properties from a set of execution\ntraces to a set of sets of execution traces. Hyperproperties have been shown to\nbe a powerful formalism for expressing and reasoning about information-flow\nsecurity policies and important properties of cyber-physical systems such as\nsensitivity and robustness, as well as consistency conditions in distributed\ncomputing such as linearizability. Although there is an extensive body of work\non automata-based representation of trace properties, we currently lack such\ncharacterization for hyperproperties. We introduce hyperautomata for em\nhyperlanguages, which are languages over sets of words. Essentially,\nhyperautomata allow running multiple quantified words over an automaton. We\npropose a specific type of hyperautomata called nondeterministic finite\nhyperautomata (NFH), which accept regular hyperlanguages. We demonstrate the\nability of regular hyperlanguages to express hyperproperties for finite traces.\nWe then explore the fundamental properties of NFH and show their closure under\nthe Boolean operations. We show that while nonemptiness is undecidable in\ngeneral, it is decidable for several fragments of NFH. We further show the\ndecidability of the membership problem for finite sets and regular languages\nfor NFH, as well as the containment problem for several fragments of NFH.\nFinally, we introduce learning algorithms based on Angluin's L-star algorithm\nfor the fragments NFH in which the quantification is either strictly universal\nor strictly existential.", "published": "2020-02-23 09:52:20", "link": "http://arxiv.org/abs/2002.09877v1", "categories": ["cs.FL", "cs.CL"], "primary_category": "cs.FL"}
{"title": "A Nepali Rule Based Stemmer and its performance on different NLP\n  applications", "abstract": "Stemming is an integral part of Natural Language Processing (NLP). It's a\npreprocessing step in almost every NLP application. Arguably, the most\nimportant usage of stemming is in Information Retrieval (IR). While there are\nlots of work done on stemming in languages like English, Nepali stemming has\nonly a few works. This study focuses on creating a Rule Based stemmer for\nNepali text. Specifically, it is an affix stripping system that identifies two\ndifferent class of suffixes in Nepali grammar and strips them separately. Only\na single negativity prefix (Na) is identified and stripped. This study focuses\non a number of techniques like exception word identification, morphological\nnormalization and word transformation to increase stemming performance. The\nstemmer is tested intrinsically using Paice's method and extrinsically on a\nbasic tf-idf based IR system and an elementary news topic classifier using\nMultinomial Naive Bayes Classifier. The difference in performance of these\nsystems with and without using the stemmer is analysed.", "published": "2020-02-23 13:33:04", "link": "http://arxiv.org/abs/2002.09901v1", "categories": ["cs.CL", "cs.IR", "I.7.2"], "primary_category": "cs.CL"}
{"title": "Do Multi-Hop Question Answering Systems Know How to Answer the\n  Single-Hop Sub-Questions?", "abstract": "Multi-hop question answering (QA) requires a model to retrieve and integrate\ninformation from different parts of a long text to answer a question. Humans\nanswer this kind of complex questions via a divide-and-conquer approach. In\nthis paper, we investigate whether top-performing models for multi-hop\nquestions understand the underlying sub-questions like humans. We adopt a\nneural decomposition model to generate sub-questions for a multi-hop complex\nquestion, followed by extracting the corresponding sub-answers. We show that\nmultiple state-of-the-art multi-hop QA models fail to correctly answer a large\nportion of sub-questions, although their corresponding multi-hop questions are\ncorrectly answered. This indicates that these models manage to answer the\nmulti-hop questions using some partial clues, instead of truly understanding\nthe reasoning paths. We also propose a new model which significantly improves\nthe performance on answering the sub-questions. Our work takes a step forward\ntowards building a more explainable multi-hop QA system.", "published": "2020-02-23 15:16:43", "link": "http://arxiv.org/abs/2002.09919v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unique Chinese Linguistic Phenomena", "abstract": "Linguistics holds unique characteristics of generality, stability, and\nnationality, which will affect the formulation of extraction strategies and\nshould be incorporated into the relation extraction. Chinese open relation\nextraction is not well-established, because of the complexity of Chinese\nlinguistics makes it harder to operate, and the methods for English are not\ncompatible with that for Chinese. The diversities between Chinese and English\nlinguistics are mainly reflected in morphology and syntax.", "published": "2020-02-23 12:13:48", "link": "http://arxiv.org/abs/2004.00499v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sketching Transformed Matrices with Applications to Natural Language\n  Processing", "abstract": "Suppose we are given a large matrix $A=(a_{i,j})$ that cannot be stored in\nmemory but is in a disk or is presented in a data stream. However, we need to\ncompute a matrix decomposition of the entry-wisely transformed matrix,\n$f(A):=(f(a_{i,j}))$ for some function $f$. Is it possible to do it in a space\nefficient way? Many machine learning applications indeed need to deal with such\nlarge transformed matrices, for example word embedding method in NLP needs to\nwork with the pointwise mutual information (PMI) matrix, while the entrywise\ntransformation makes it difficult to apply known linear algebraic tools.\nExisting approaches for this problem either need to store the whole matrix and\nperform the entry-wise transformation afterwards, which is space consuming or\ninfeasible, or need to redesign the learning method, which is application\nspecific and requires substantial remodeling.\n  In this paper, we first propose a space-efficient sketching algorithm for\ncomputing the product of a given small matrix with the transformed matrix. It\nworks for a general family of transformations with provable small error bounds\nand thus can be used as a primitive in downstream learning tasks. We then apply\nthis primitive to a concrete application: low-rank approximation. We show that\nour approach obtains small error and is efficient in both space and time. We\ncomplement our theoretical results with experiments on synthetic and real data.", "published": "2020-02-23 03:07:31", "link": "http://arxiv.org/abs/2002.09812v1", "categories": ["cs.DS", "cs.CL", "cs.LG"], "primary_category": "cs.DS"}
{"title": "Data Augmentation for Personal Knowledge Base Population", "abstract": "Cold start knowledge base population (KBP) is the problem of populating a\nknowledge base from unstructured documents. While artificial neural networks\nhave led to significant improvements in the different tasks that are part of\nKBP, the overall F1 of the end-to-end system remains quite low. This problem is\nmore acute in personal knowledge bases, which present additional challenges\nwith regard to data protection, fairness and privacy. In this work, we present\na system that uses rule based annotators and a graph neural network for missing\nlink prediction, to populate a more complete, fair and diverse knowledge base\nfrom the TACRED dataset.", "published": "2020-02-23 07:39:55", "link": "http://arxiv.org/abs/2002.10943v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "ScopeIt: Scoping Task Relevant Sentences in Documents", "abstract": "Intelligent assistants like Cortana, Siri, Alexa, and Google Assistant are\ntrained to parse information when the conversation is synchronous and short;\nhowever, for email-based conversational agents, the communication is\nasynchronous, and often contains information irrelevant to the assistant. This\nmakes it harder for the system to accurately detect intents, extract entities\nrelevant to those intents and thereby perform the desired action. We present a\nneural model for scoping relevant information for the agent from a large query.\nWe show that when used as a preprocessing step, the model improves performance\nof both intent detection and entity extraction tasks. We demonstrate the\nmodel's impact on Scheduler (Cortana is the persona of the agent, while\nScheduler is the name of the service. We use them interchangeably in the\ncontext of this paper.) - a virtual conversational meeting scheduling assistant\nthat interacts asynchronously with users through email. The model helps the\nentity extraction and intent detection tasks requisite by Scheduler achieve an\naverage gain of 35% in precision without any drop in recall. Additionally, we\ndemonstrate that the same approach can be used for component level analysis in\nlarge documents, such as signature block identification.", "published": "2020-02-23 02:33:10", "link": "http://arxiv.org/abs/2003.04988v2", "categories": ["cs.CL", "cs.LG", "stat.ML", "I.2.7; I.7.5"], "primary_category": "cs.CL"}
{"title": "Deep Multimodal Image-Text Embeddings for Automatic Cross-Media\n  Retrieval", "abstract": "This paper considers the task of matching images and sentences by learning a\nvisual-textual embedding space for cross-modal retrieval. Finding such a space\nis a challenging task since the features and representations of text and image\nare not comparable. In this work, we introduce an end-to-end deep multimodal\nconvolutional-recurrent network for learning both vision and language\nrepresentations simultaneously to infer image-text similarity. The model learns\nwhich pairs are a match (positive) and which ones are a mismatch (negative)\nusing a hinge-based triplet ranking. To learn about the joint representations,\nwe leverage our newly extracted collection of tweets from Twitter. The main\ncharacteristic of our dataset is that the images and tweets are not\nstandardized the same as the benchmarks. Furthermore, there can be a higher\nsemantic correlation between the pictures and tweets contrary to benchmarks in\nwhich the descriptions are well-organized. Experimental results on MS-COCO\nbenchmark dataset show that our model outperforms certain methods presented\npreviously and has competitive performance compared to the state-of-the-art.\nThe code and dataset have been made available publicly.", "published": "2020-02-23 23:58:04", "link": "http://arxiv.org/abs/2002.10016v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.CV", "cs.LG", "E.0; H.3.3; I.2.0; I.2.6; I.2.7; I.2.10; I.5.0; I.4.0; I.4.10; I.7.0"], "primary_category": "cs.IR"}
{"title": "A Multi-view CNN-based Acoustic Classification System for Automatic\n  Animal Species Identification", "abstract": "Automatic identification of animal species by their vocalization is an\nimportant and challenging task. Although many kinds of audio monitoring system\nhave been proposed in the literature, they suffer from several disadvantages\nsuch as non-trivial feature selection, accuracy degradation because of\nenvironmental noise or intensive local computation. In this paper, we propose a\ndeep learning based acoustic classification framework for Wireless Acoustic\nSensor Network (WASN). The proposed framework is based on cloud architecture\nwhich relaxes the computational burden on the wireless sensor node. To improve\nthe recognition accuracy, we design a multi-view Convolution Neural Network\n(CNN) to extract the short-, middle-, and long-term dependencies in parallel.\nThe evaluation on two real datasets shows that the proposed architecture can\nachieve high accuracy and outperforms traditional classification systems\nsignificantly when the environmental noise dominate the audio signal (low SNR).\nMoreover, we implement and deploy the proposed system on a testbed and analyse\nthe system performance in real-world environments. Both simulation and\nreal-world evaluation demonstrate the accuracy and robustness of the proposed\nacoustic classification system in distinguishing species of animals.", "published": "2020-02-23 03:51:08", "link": "http://arxiv.org/abs/2002.09821v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DIHARD II is Still Hard: Experimental Results and Discussions from the\n  DKU-LENOVO Team", "abstract": "In this paper, we present the submitted system for the second DIHARD Speech\nDiarization Challenge from the DKULENOVO team. Our diarization system includes\nmultiple modules, namely voice activity detection (VAD), segmentation, speaker\nembedding extraction, similarity scoring, clustering, resegmentation and\noverlap detection. For each module, we explore different techniques to enhance\nperformance. Our final submission employs the ResNet-LSTM based VAD, the Deep\nResNet based speaker embedding, the LSTM based similarity scoring and spectral\nclustering. Variational Bayes (VB) diarization is applied in the resegmentation\nstage and overlap detection also brings slight improvement. Our proposed system\nachieves 18.84% DER in Track1 and 27.90% DER in Track2. Although our systems\nhave reduced the DERs by 27.5% and 31.7% relatively against the official\nbaselines, we believe that the diarization task is still very difficult.", "published": "2020-02-23 11:50:32", "link": "http://arxiv.org/abs/2002.12761v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
