{"title": "Low Rank Fusion based Transformers for Multimodal Sequences", "abstract": "Our senses individually work in a coordinated fashion to express our\nemotional intentions. In this work, we experiment with modeling\nmodality-specific sensory signals to attend to our latent multimodal emotional\nintentions and vice versa expressed via low-rank multimodal fusion and\nmultimodal transformers. The low-rank factorization of multimodal fusion\namongst the modalities helps represent approximate multiplicative latent signal\ninteractions. Motivated by the work of~\\cite{tsai2019MULT} and~\\cite{Liu_2018},\nwe present our transformer-based cross-fusion architecture without any\nover-parameterization of the model. The low-rank fusion helps represent the\nlatent signal interactions while the modality-specific attention helps focus on\nrelevant parts of the signal. We present two methods for the Multimodal\nSentiment and Emotion Recognition results on CMU-MOSEI, CMU-MOSI, and IEMOCAP\ndatasets and show that our models have lesser parameters, train faster and\nperform comparably to many larger fusion-based architectures.", "published": "2020-07-04 08:05:40", "link": "http://arxiv.org/abs/2007.02038v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Birds of a Feather Flock Together: Satirical News Detection via Language\n  Model Differentiation", "abstract": "Satirical news is regularly shared in modern social media because it is\nentertaining with smartly embedded humor. However, it can be harmful to society\nbecause it can sometimes be mistaken as factual news, due to its deceptive\ncharacter. We found that in satirical news, the lexical and pragmatical\nattributes of the context are the key factors in amusing the readers. In this\nwork, we propose a method that differentiates the satirical news and true news.\nIt takes advantage of satirical writing evidence by leveraging the difference\nbetween the prediction loss of two language models, one trained on true news\nand the other on satirical news, when given a new news article. We compute\nseveral statistical metrics of language model prediction loss as features,\nwhich are then used to conduct downstream classification. The proposed method\nis computationally effective because the language models capture the language\nusage differences between satirical news documents and traditional news\ndocuments, and are sensitive when applied to documents outside their domains.", "published": "2020-07-04 18:46:36", "link": "http://arxiv.org/abs/2007.02164v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pynsett: A programmable relation extractor", "abstract": "This paper proposes a programmable relation extraction method for the English\nlanguage by parsing texts into semantic graphs. A person can define rules in\nplain English that act as matching patterns onto the graph representation.\nThese rules are designed to capture the semantic content of the documents,\nallowing for flexibility and ad-hoc entities. Relation extraction is a complex\ntask that typically requires sizable training corpora. The method proposed here\nis ideal for extracting specialized ontologies in a limited collection of\ndocuments.", "published": "2020-07-04 14:03:48", "link": "http://arxiv.org/abs/2007.02100v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Coronavirus Knowledge Graph: A Case Study", "abstract": "The emergence of the novel COVID-19 pandemic has had a significant impact on\nglobal healthcare and the economy over the past few months. The virus's rapid\nwidespread has led to a proliferation in biomedical research addressing the\npandemic and its related topics. One of the essential Knowledge Discovery tools\nthat could help the biomedical research community understand and eventually\nfind a cure for COVID-19 are Knowledge Graphs. The CORD-19 dataset is a\ncollection of publicly available full-text research articles that have been\nrecently published on COVID-19 and coronavirus topics. Here, we use several\nMachine Learning, Deep Learning, and Knowledge Graph construction and mining\ntechniques to formalize and extract insights from the PubMed dataset and the\nCORD-19 dataset to identify COVID-19 related experts and bio-entities. Besides,\nwe suggest possible techniques to predict related diseases, drug candidates,\ngene, gene mutations, and related compounds as part of a systematic effort to\napply Knowledge Discovery methods to help biomedical researchers tackle the\npandemic.", "published": "2020-07-04 03:55:31", "link": "http://arxiv.org/abs/2007.10287v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Robust Prediction of Punctuation and Truecasing for Medical ASR", "abstract": "Automatic speech recognition (ASR) systems in the medical domain that focus\non transcribing clinical dictations and doctor-patient conversations often pose\nmany challenges due to the complexity of the domain. ASR output typically\nundergoes automatic punctuation to enable users to speak naturally, without\nhaving to vocalise awkward and explicit punctuation commands, such as \"period\",\n\"add comma\" or \"exclamation point\", while truecasing enhances user readability\nand improves the performance of downstream NLP tasks. This paper proposes a\nconditional joint modeling framework for prediction of punctuation and\ntruecasing using pretrained masked language models such as BERT, BioBERT and\nRoBERTa. We also present techniques for domain and task specific adaptation by\nfine-tuning masked language models with medical domain data. Finally, we\nimprove the robustness of the model against common errors made in ASR by\nperforming data augmentation. Experiments performed on dictation and\nconversational style corpora show that our proposed model achieves ~5% absolute\nimprovement on ground truth text and ~10% improvement on ASR outputs over\nbaseline models under F1 metric.", "published": "2020-07-04 07:15:13", "link": "http://arxiv.org/abs/2007.02025v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Text Data Augmentation: Towards better detection of spear-phishing\n  emails", "abstract": "Text data augmentation, i.e., the creation of new textual data from an\nexisting text, is challenging. Indeed, augmentation transformations should take\ninto account language complexity while being relevant to the target Natural\nLanguage Processing (NLP) task (e.g., Machine Translation, Text\nClassification). Initially motivated by an application of Business Email\nCompromise (BEC) detection, we propose a corpus and task agnostic augmentation\nframework used as a service to augment English texts within our company. Our\nproposal combines different methods, utilizing BERT language model, multi-step\nback-translation and heuristics. We show that our augmentation framework\nimproves performances on several text classification tasks using publicly\navailable models and corpora as well as on a BEC detection task. We also\nprovide a comprehensive argumentation about the limitations of our augmentation\nframework.", "published": "2020-07-04 07:45:04", "link": "http://arxiv.org/abs/2007.02033v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis on Social Media Content", "abstract": "Nowadays, people from all around the world use social media sites to share\ninformation. Twitter for example is a platform in which users send, read posts\nknown as tweets and interact with different communities. Users share their\ndaily lives, post their opinions on everything such as brands and places.\nCompanies can benefit from this massive platform by collecting data related to\nopinions on them. The aim of this paper is to present a model that can perform\nsentiment analysis of real data collected from Twitter. Data in Twitter is\nhighly unstructured which makes it difficult to analyze. However, our proposed\nmodel is different from prior work in this field because it combined the use of\nsupervised and unsupervised machine learning algorithms. The process of\nperforming sentiment analysis as follows: Tweet extracted directly from Twitter\nAPI, then cleaning and discovery of data performed. After that the data were\nfed into several models for the purpose of training. Each tweet extracted\nclassified based on its sentiment whether it is a positive, negative or\nneutral. Data were collected on two subjects McDonalds and KFC to show which\nrestaurant has more popularity. Different machine learning algorithms were\nused. The result from these models were tested using various testing metrics\nlike cross validation and f-score. Moreover, our model demonstrates strong\nperformance on mining texts extracted directly from Twitter.", "published": "2020-07-04 17:03:30", "link": "http://arxiv.org/abs/2007.02144v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deep Graph Random Process for Relational-Thinking-Based Speech\n  Recognition", "abstract": "Lying at the core of human intelligence, relational thinking is characterized\nby initially relying on innumerable unconscious percepts pertaining to\nrelations between new sensory signals and prior knowledge, consequently\nbecoming a recognizable concept or object through coupling and transformation\nof these percepts. Such mental processes are difficult to model in real-world\nproblems such as in conversational automatic speech recognition (ASR), as the\npercepts (if they are modelled as graphs indicating relationships among\nutterances) are supposed to be innumerable and not directly observable. In this\npaper, we present a Bayesian nonparametric deep learning method called deep\ngraph random process (DGP) that can generate an infinite number of\nprobabilistic graphs representing percepts. We further provide a closed-form\nsolution for coupling and transformation of these percept graphs for acoustic\nmodeling. Our approach is able to successfully infer relations among utterances\nwithout using any relational data during training. Experimental evaluations on\nASR tasks including CHiME-2 and CHiME-5 demonstrate the effectiveness and\nbenefits of our method.", "published": "2020-07-04 15:27:57", "link": "http://arxiv.org/abs/2007.02126v2", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
