{"title": "Attentive Convolutional Neural Network based Speech Emotion Recognition:\n  A Study on the Impact of Input Features, Signal Length, and Acted Speech", "abstract": "Speech emotion recognition is an important and challenging task in the realm\nof human-computer interaction. Prior work proposed a variety of models and\nfeature sets for training a system. In this work, we conduct extensive\nexperiments using an attentive convolutional neural network with multi-view\nlearning objective function. We compare system performance using different\nlengths of the input signal, different types of acoustic features and different\ntypes of emotion speech (improvised/scripted). Our experimental results on the\nInteractive Emotional Motion Capture (IEMOCAP) database reveal that the\nrecognition performance strongly depends on the type of speech data independent\nof the choice of input features. Furthermore, we achieved state-of-the-art\nresults on the improvised speech data of IEMOCAP.", "published": "2017-06-02 10:12:52", "link": "http://arxiv.org/abs/1706.00612v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prosodic Event Recognition using Convolutional Neural Networks with\n  Context Information", "abstract": "This paper demonstrates the potential of convolutional neural networks (CNN)\nfor detecting and classifying prosodic events on words, specifically pitch\naccents and phrase boundary tones, from frame-based acoustic features. Typical\napproaches use not only feature representations of the word in question but\nalso its surrounding context. We show that adding position features indicating\nthe current word benefits the CNN. In addition, this paper discusses the\ngeneralization from a speaker-dependent modelling approach to a\nspeaker-independent setup. The proposed method is simple and efficient and\nyields strong results not only in speaker-dependent but also\nspeaker-independent cases.", "published": "2017-06-02 16:20:19", "link": "http://arxiv.org/abs/1706.00741v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Modeling of Topics, Citations, and Topical Authority in Academic\n  Corpora", "abstract": "Much of scientific progress stems from previously published findings, but\nsearching through the vast sea of scientific publications is difficult. We\noften rely on metrics of scholarly authority to find the prominent authors but\nthese authority indices do not differentiate authority based on research\ntopics. We present Latent Topical-Authority Indexing (LTAI) for jointly\nmodeling the topics, citations, and topical authority in a corpus of academic\npapers. Compared to previous models, LTAI differs in two main aspects. First,\nit explicitly models the generative process of the citations, rather than\ntreating the citations as given. Second, it models each author's influence on\ncitations of a paper based on the topics of the cited papers, as well as the\nciting papers. We fit LTAI to four academic corpora: CORA, Arxiv Physics, PNAS,\nand Citeseer. We compare the performance of LTAI against various baselines,\nstarting with the latent Dirichlet allocation, to the more advanced models\nincluding author-link topic model and dynamic author citation topic model. The\nresults show that LTAI achieves improved accuracy over other similar models\nwhen predicting words, citations and authors of publications.", "published": "2017-06-02 08:52:47", "link": "http://arxiv.org/abs/1706.00593v1", "categories": ["cs.CL", "cs.DL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Yeah, Right, Uh-Huh: A Deep Learning Backchannel Predictor", "abstract": "Using supporting backchannel (BC) cues can make human-computer interaction\nmore social. BCs provide a feedback from the listener to the speaker indicating\nto the speaker that he is still listened to. BCs can be expressed in different\nways, depending on the modality of the interaction, for example as gestures or\nacoustic cues. In this work, we only considered acoustic cues. We are proposing\nan approach towards detecting BC opportunities based on acoustic input features\nlike power and pitch. While other works in the field rely on the use of a\nhand-written rule set or specialized features, we made use of artificial neural\nnetworks. They are capable of deriving higher order features from input\nfeatures themselves. In our setup, we first used a fully connected feed-forward\nnetwork to establish an updated baseline in comparison to our previously\nproposed setup. We also extended this setup by the use of Long Short-Term\nMemory (LSTM) networks which have shown to outperform feed-forward based setups\non various tasks. Our best system achieved an F1-Score of 0.37 using power and\npitch features. Adding linguistic information using word2vec, the score\nincreased to 0.39.", "published": "2017-06-02 17:05:26", "link": "http://arxiv.org/abs/1706.01340v1", "categories": ["cs.CL", "cs.CV", "cs.HC", "cs.LG", "cs.SD"], "primary_category": "cs.CL"}
