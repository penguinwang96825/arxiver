{"title": "Word Sense Disambiguation with LSTM: Do We Really Need 100 Billion\n  Words?", "abstract": "Recently, Yuan et al. (2016) have shown the effectiveness of using Long\nShort-Term Memory (LSTM) for performing Word Sense Disambiguation (WSD). Their\nproposed technique outperformed the previous state-of-the-art with several\nbenchmarks, but neither the training data nor the source code was released.\nThis paper presents the results of a reproduction study of this technique using\nonly openly available datasets (GigaWord, SemCore, OMSTI) and software\n(TensorFlow). From them, it emerged that state-of-the-art results can be\nobtained with much less data than hinted by Yuan et al. All code and trained\nmodels are made freely available.", "published": "2017-12-09 10:47:19", "link": "http://arxiv.org/abs/1712.03376v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aspect Extraction and Sentiment Classification of Mobile Apps using\n  App-Store Reviews", "abstract": "Understanding of customer sentiment can be useful for product development. On\ntop of that if the priorities for the development order can be known, then\ndevelopment procedure become simpler. This work has tried to address this issue\nin the mobile app domain. Along with aspect and opinion extraction this work\nhas also categorized the extracted aspects ac-cording to their importance. This\ncan help developers to focus their time and energy at the right place.", "published": "2017-12-09 20:06:52", "link": "http://arxiv.org/abs/1712.03430v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modulating and attending the source image during encoding improves\n  Multimodal Translation", "abstract": "We propose a new and fully end-to-end approach for multimodal translation\nwhere the source text encoder modulates the entire visual input processing\nusing conditional batch normalization, in order to compute the most informative\nimage features for our task. Additionally, we propose a new attention mechanism\nderived from this original idea, where the attention model for the visual input\nis conditioned on the source text encoder representations. In the paper, we\ndetail our models as well as the image analysis pipeline. Finally, we report\nexperimental results. They are, as far as we know, the new state of the art on\nthree different test sets.", "published": "2017-12-09 23:17:22", "link": "http://arxiv.org/abs/1712.03449v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Implementation of the Room Simulator for Training Deep Neural\n  Network Acoustic Models", "abstract": "In this paper, we describe how to efficiently implement an acoustic room\nsimulator to generate large-scale simulated data for training deep neural\nnetworks. Even though Google Room Simulator in [1] was shown to be quite\neffective in reducing the Word Error Rates (WERs) for far-field applications by\ngenerating simulated far-field training sets, it requires a very large number\nof Fast Fourier Transforms (FFTs) of large size. Room Simulator in [1] used\napproximately 80 percent of Central Processing Unit (CPU) usage in our CPU +\nGraphics Processing Unit (GPU) training architecture [2]. In this work, we\nimplement an efficient OverLap Addition (OLA) based filtering using the\nopen-source FFTW3 library. Further, we investigate the effects of the Room\nImpulse Response (RIR) lengths. Experimentally, we conclude that we can cut the\ntail portions of RIRs whose power is less than 20 dB below the maximum power\nwithout sacrificing the speech recognition accuracy. However, we observe that\ncutting RIR tail more than this threshold harms the speech recognition accuracy\nfor rerecorded test sets. Using these approaches, we were able to reduce CPU\nusage for the room simulator portion down to 9.69 percent in CPU/GPU training\narchitecture. Profiling result shows that we obtain 22.4 times speed-up on a\nsingle machine and 37.3 times speed up on Google's distributed training\ninfrastructure.", "published": "2017-12-09 20:58:39", "link": "http://arxiv.org/abs/1712.03439v2", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Music Generation by Deep Learning - Challenges and Directions", "abstract": "In addition to traditional tasks such as prediction, classification and\ntranslation, deep learning is receiving growing attention as an approach for\nmusic generation, as witnessed by recent research groups such as Magenta at\nGoogle and CTRL (Creator Technology Research Lab) at Spotify. The motivation is\nin using the capacity of deep learning architectures and training techniques to\nautomatically learn musical styles from arbitrary musical corpora and then to\ngenerate samples from the estimated distribution. However, a direct application\nof deep learning to generate content rapidly reaches limits as the generated\ncontent tends to mimic the training set without exhibiting true creativity.\nMoreover, deep learning architectures do not offer direct ways for controlling\ngeneration (e.g., imposing some tonality or other arbitrary constraints).\nFurthermore, deep learning architectures alone are autistic automata which\ngenerate music autonomously without human user interaction, far from the\nobjective of interactively assisting musicians to compose and refine music.\nIssues such as: control, structure, creativity and interactivity are the focus\nof our analysis. In this paper, we select some limitations of a direct\napplication of deep learning to music generation, analyze why the issues are\nnot fulfilled and how to address them by possible approaches. Various examples\nof recent systems are cited as examples of promising directions.", "published": "2017-12-09 14:57:47", "link": "http://arxiv.org/abs/1712.04371v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
