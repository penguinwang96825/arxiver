{"title": "Deep RNNs Encode Soft Hierarchical Syntax", "abstract": "We present a set of experiments to demonstrate that deep recurrent neural\nnetworks (RNNs) learn internal representations that capture soft hierarchical\nnotions of syntax from highly varied supervision. We consider four syntax tasks\nat different depths of the parse tree; for each word, we predict its part of\nspeech as well as the first (parent), second (grandparent) and third level\n(great-grandparent) constituent labels that appear above it. These predictions\nare made from representations produced at different depths in networks that are\npretrained with one of four objectives: dependency parsing, semantic role\nlabeling, machine translation, or language modeling. In every case, we find a\ncorrespondence between network depth and syntactic depth, suggesting that a\nsoft syntactic hierarchy emerges. This effect is robust across all conditions,\nindicating that the models encode significant amounts of syntax even in the\nabsence of an explicit syntactic training supervision.", "published": "2018-05-11 01:34:52", "link": "http://arxiv.org/abs/1805.04218v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation for Bilingually Scarce Scenarios: A Deep\n  Multi-task Learning Approach", "abstract": "Neural machine translation requires large amounts of parallel training text\nto learn a reasonable-quality translation model. This is particularly\ninconvenient for language pairs for which enough parallel text is not\navailable. In this paper, we use monolingual linguistic resources in the source\nside to address this challenging problem based on a multi-task learning\napproach. More specifically, we scaffold the machine translation task on\nauxiliary tasks including semantic parsing, syntactic parsing, and named-entity\nrecognition. This effectively injects semantic and/or syntactic knowledge into\nthe translation model, which would otherwise require a large amount of training\nbitext. We empirically evaluate and show the effectiveness of our multi-task\nlearning approach on three translation tasks: English-to-French,\nEnglish-to-Farsi, and English-to-Vietnamese.", "published": "2018-05-11 03:36:32", "link": "http://arxiv.org/abs/1805.04237v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Open Information Extraction", "abstract": "Conventional Open Information Extraction (Open IE) systems are usually built\non hand-crafted patterns from other NLP tools such as syntactic parsing, yet\nthey face problems of error propagation. In this paper, we propose a neural\nOpen IE approach with an encoder-decoder framework. Distinct from existing\nmethods, the neural Open IE approach learns highly confident arguments and\nrelation tuples bootstrapped from a state-of-the-art Open IE system. An\nempirical study on a large benchmark dataset shows that the neural Open IE\nsystem significantly outperforms several baselines, while maintaining\ncomparable computational efficiency.", "published": "2018-05-11 08:17:49", "link": "http://arxiv.org/abs/1805.04270v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The risk of sub-optimal use of Open Source NLP Software: UKB is\n  inadvertently state-of-the-art in knowledge-based WSD", "abstract": "UKB is an open source collection of programs for performing, among other\ntasks, knowledge-based Word Sense Disambiguation (WSD). Since it was released\nin 2009 it has been often used out-of-the-box in sub-optimal settings. We show\nthat nine years later it is the state-of-the-art on knowledge-based WSD. This\ncase shows the pitfalls of releasing open source NLP software without optimal\ndefault settings and precise instructions for reproducibility.", "published": "2018-05-11 08:46:15", "link": "http://arxiv.org/abs/1805.04277v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bootstrapping Multilingual Intent Models via Machine Translation for\n  Dialog Automation", "abstract": "With the resurgence of chat-based dialog systems in consumer and enterprise\napplications, there has been much success in developing data-driven and\nrule-based natural language models to understand human intent. Since these\nmodels require large amounts of data and in-domain knowledge, expanding an\nequivalent service into new markets is disrupted by language barriers that\ninhibit dialog automation.\n  This paper presents a user study to evaluate the utility of out-of-the-box\nmachine translation technology to (1) rapidly bootstrap multilingual spoken\ndialog systems and (2) enable existing human analysts to understand foreign\nlanguage utterances. We additionally evaluate the utility of machine\ntranslation in human assisted environments, where a portion of the traffic is\nprocessed by analysts. In English->Spanish experiments, we observe a high\npotential for dialog automation, as well as the potential for human analysts to\nprocess foreign language utterances with high accuracy.", "published": "2018-05-11 15:42:27", "link": "http://arxiv.org/abs/1805.04453v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems", "abstract": "Automatic machine learning systems can inadvertently accentuate and\nperpetuate inappropriate human biases. Past work on examining inappropriate\nbiases has largely focused on just individual systems. Further, there is no\nbenchmark dataset for examining inappropriate biases in systems. Here for the\nfirst time, we present the Equity Evaluation Corpus (EEC), which consists of\n8,640 English sentences carefully chosen to tease out biases towards certain\nraces and genders. We use the dataset to examine 219 automatic sentiment\nanalysis systems that took part in a recent shared task, SemEval-2018 Task 1\n'Affect in Tweets'. We find that several of the systems show statistically\nsignificant bias; that is, they consistently provide slightly higher sentiment\nintensity predictions for one race or one gender. We make the EEC freely\navailable.", "published": "2018-05-11 17:57:40", "link": "http://arxiv.org/abs/1805.04508v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentiment Composition of Words with Opposing Polarities", "abstract": "In this paper, we explore sentiment composition in phrases that have at least\none positive and at least one negative word---phrases like 'happy accident' and\n'best winter break'. We compiled a dataset of such opposing polarity phrases\nand manually annotated them with real-valued scores of sentiment association.\nUsing this dataset, we analyze the linguistic patterns present in opposing\npolarity phrases. Finally, we apply several unsupervised and supervised\ntechniques of sentiment composition to determine their efficacy on this\ndataset. Our best system, which incorporates information from the phrase's\nconstituents, their parts of speech, their sentiment association scores, and\ntheir embedding vectors, obtains an accuracy of over 80% on the opposing\npolarity phrases.", "published": "2018-05-11 18:16:54", "link": "http://arxiv.org/abs/1805.04542v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NRC-Canada at SMM4H Shared Task: Classifying Tweets Mentioning Adverse\n  Drug Reactions and Medication Intake", "abstract": "Our team, NRC-Canada, participated in two shared tasks at the AMIA-2017\nWorkshop on Social Media Mining for Health Applications (SMM4H): Task 1 -\nclassification of tweets mentioning adverse drug reactions, and Task 2 -\nclassification of tweets describing personal medication intake. For both tasks,\nwe trained Support Vector Machine classifiers using a variety of surface-form,\nsentiment, and domain-specific features. With nine teams participating in each\ntask, our submissions ranked first on Task 1 and third on Task 2. Handling\nconsiderable class imbalance proved crucial for Task 1. We applied an\nunder-sampling technique to reduce class imbalance (from about 1:10 to 1:2).\nStandard n-gram features, n-grams generalized over domain terms, as well as\ngeneral-domain and domain-specific word embeddings had a substantial impact on\nthe overall performance in both tasks. On the other hand, including sentiment\nlexicon features did not result in any improvement.", "published": "2018-05-11 19:01:49", "link": "http://arxiv.org/abs/1805.04558v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Factor Graph Models for Cross-lingual Morphological Tagging", "abstract": "Morphological analysis involves predicting the syntactic traits of a word\n(e.g. {POS: Noun, Case: Acc, Gender: Fem}). Previous work in morphological\ntagging improves performance for low-resource languages (LRLs) through\ncross-lingual training with a high-resource language (HRL) from the same\nfamily, but is limited by the strict, often false, assumption that tag sets\nexactly overlap between the HRL and LRL. In this paper we propose a method for\ncross-lingual morphological tagging that aims to improve information sharing\nbetween languages by relaxing this assumption. The proposed model uses\nfactorial conditional random fields with neural network potentials, making it\npossible to (1) utilize the expressive power of neural network representations\nto smooth over superficial differences in the surface forms, (2) model pairwise\nand transitive relationships between tags, and (3) accurately generate tag sets\nthat are unseen or rare in the training data. Experiments on four languages\nfrom the Universal Dependencies Treebank demonstrate superior tagging\naccuracies over existing cross-lingual approaches.", "published": "2018-05-11 19:27:07", "link": "http://arxiv.org/abs/1805.04570v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain Adapted Word Embeddings for Improved Sentiment Classification", "abstract": "Generic word embeddings are trained on large-scale generic corpora; Domain\nSpecific (DS) word embeddings are trained only on data from a domain of\ninterest. This paper proposes a method to combine the breadth of generic\nembeddings with the specificity of domain specific embeddings. The resulting\nembeddings, called Domain Adapted (DA) word embeddings, are formed by aligning\ncorresponding word vectors using Canonical Correlation Analysis (CCA) or the\nrelated nonlinear Kernel CCA. Evaluation results on sentiment classification\ntasks show that the DA embeddings substantially outperform both generic and DS\nembeddings when used as input features to standard or state-of-the-art sentence\nencoding algorithms for classification.", "published": "2018-05-11 19:58:59", "link": "http://arxiv.org/abs/1805.04576v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Statistical and Semantic Models for Multi-Document Summarization", "abstract": "We report a series of experiments with different semantic models on top of\nvarious statistical models for extractive text summarization. Though\nstatistical models may better capture word co-occurrences and distribution\naround the text, they fail to detect the context and the sense of sentences\n/words as a whole. Semantic models help us gain better insight into the context\nof sentences. We show that how tuning weights between different models can help\nus achieve significant results on various benchmarks. Learning pre-trained\nvectors used in semantic models further, on given corpus, can give addition\nspike in performance. Using weighing techniques in between different\nstatistical models too further refines our result. For Statistical models, we\nhave used TF/IDF, TextRAnk, Jaccard/Cosine Similarities. For Semantic Models,\nwe have used WordNet-based Model and proposed two models based on Glove Vectors\nand Facebook's InferSent. We tested our approach on DUC 2004 dataset,\ngenerating 100-word summaries. We have discussed the system, algorithms,\nanalysis and also proposed and tested possible improvements. ROUGE scores were\nused to compare to other summarizers.", "published": "2018-05-11 20:15:52", "link": "http://arxiv.org/abs/1805.04579v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Double Embeddings and CNN-based Sequence Labeling for Aspect Extraction", "abstract": "One key task of fine-grained sentiment analysis of product reviews is to\nextract product aspects or features that users have expressed opinions on. This\npaper focuses on supervised aspect extraction using deep learning. Unlike other\nhighly sophisticated supervised deep learning models, this paper proposes a\nnovel and yet simple CNN model employing two types of pre-trained embeddings\nfor aspect extraction: general-purpose embeddings and domain-specific\nembeddings. Without using any additional supervision, this model achieves\nsurprisingly good results, outperforming state-of-the-art sophisticated\nexisting methods. To our knowledge, this paper is the first to report such\ndouble embeddings based CNN model for aspect extraction and achieve very good\nresults.", "published": "2018-05-11 21:53:23", "link": "http://arxiv.org/abs/1805.04601v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Confidence Modeling for Neural Semantic Parsing", "abstract": "In this work we focus on confidence modeling for neural semantic parsers\nwhich are built upon sequence-to-sequence models. We outline three major causes\nof uncertainty, and design various metrics to quantify these factors. These\nmetrics are then used to estimate confidence scores that indicate whether model\npredictions are likely to be correct. Beyond confidence estimation, we identify\nwhich parts of the input contribute to uncertain predictions allowing users to\ninterpret their model, and verify or refine its input. Experimental results\nshow that our confidence model significantly outperforms a widely used method\nthat relies on posterior probability, and improves the quality of\ninterpretation compared to simply relying on attention scores.", "published": "2018-05-11 22:09:37", "link": "http://arxiv.org/abs/1805.04604v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TutorialBank: A Manually-Collected Corpus for Prerequisite Chains,\n  Survey Extraction and Resource Recommendation", "abstract": "The field of Natural Language Processing (NLP) is growing rapidly, with new\nresearch published daily along with an abundance of tutorials, codebases and\nother online resources. In order to learn this dynamic field or stay up-to-date\non the latest research, students as well as educators and researchers must\nconstantly sift through multiple sources to find valuable, relevant\ninformation. To address this situation, we introduce TutorialBank, a new,\npublicly available dataset which aims to facilitate NLP education and research.\nWe have manually collected and categorized over 6,300 resources on NLP as well\nas the related fields of Artificial Intelligence (AI), Machine Learning (ML)\nand Information Retrieval (IR). Our dataset is notably the largest\nmanually-picked corpus of resources intended for NLP education which does not\ninclude only academic papers. Additionally, we have created both a search\nengine and a command-line tool for the resources and have annotated the corpus\nto include lists of research topics, relevant resources for each topic,\nprerequisite relations among topics, relevant sub-parts of individual\nresources, among other annotations. We are releasing the dataset and present\nseveral avenues for further research.", "published": "2018-05-11 23:13:34", "link": "http://arxiv.org/abs/1805.04617v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Behavior Analysis of NLI Models: Uncovering the Influence of Three\n  Factors on Robustness", "abstract": "Natural Language Inference is a challenging task that has received\nsubstantial attention, and state-of-the-art models now achieve impressive test\nset performance in the form of accuracy scores. Here, we go beyond this single\nevaluation metric to examine robustness to semantically-valid alterations to\nthe input data. We identify three factors - insensitivity, polarity and unseen\npairs - and compare their impact on three SNLI models under a variety of\nconditions. Our results demonstrate a number of strengths and weaknesses in the\nmodels' ability to generalise to new in-domain instances. In particular, while\nstrong performance is possible on unseen hypernyms, unseen antonyms are more\nchallenging for all the models. More generally, the models suffer from an\ninsensitivity to certain small but semantically significant alterations, and\nare also often influenced by simple statistical correlations between words and\ntraining labels. Overall, we show that evaluations of NLI models can benefit\nfrom studying the influence of factors intrinsic to the models or found in the\ndataset used.", "published": "2018-05-11 00:43:59", "link": "http://arxiv.org/abs/1805.04212v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "State Gradients for RNN Memory Analysis", "abstract": "We present a framework for analyzing what the state in RNNs remembers from\nits input embeddings. Our approach is inspired by backpropagation, in the sense\nthat we compute the gradients of the states with respect to the input\nembeddings. The gradient matrix is decomposed with Singular Value Decomposition\nto analyze which directions in the embedding space are best transferred to the\nhidden state space, characterized by the largest singular values. We apply our\napproach to LSTM language models and investigate to what extent and for how\nlong certain classes of words are remembered on average for a certain corpus.\nAdditionally, the extent to which a specific property or relationship is\nremembered by the RNN can be tracked by comparing a vector characterizing that\nproperty with the direction(s) in embedding space that are best preserved in\nhidden state space.", "published": "2018-05-11 07:51:28", "link": "http://arxiv.org/abs/1805.04264v2", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Decision problems for Clark-congruential languages", "abstract": "A common question when studying a class of context-free grammars is whether\nequivalence is decidable within this class. We answer this question positively\nfor the class of Clark-congruential grammars, which are of interest to\ngrammatical inference. We also consider the problem of checking whether a given\nCFG is Clark-congruential, and show that it is decidable given that the CFG is\na DCFG.", "published": "2018-05-11 13:50:26", "link": "http://arxiv.org/abs/1805.04402v2", "categories": ["cs.CL", "cs.FL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Document Retrieval using Regularized Wasserstein Distance", "abstract": "Many information retrieval algorithms rely on the notion of a good distance\nthat allows to efficiently compare objects of different nature. Recently, a new\npromising metric called Word Mover's Distance was proposed to measure the\ndivergence between text passages. In this paper, we demonstrate that this\nmetric can be extended to incorporate term-weighting schemes and provide more\naccurate and computationally efficient matching between documents using\nentropic regularization. We evaluate the benefits of both extensions in the\ntask of cross-lingual document retrieval (CLDR). Our experimental results on\neight CLDR problems suggest that the proposed methods achieve remarkable\nimprovements in terms of Mean Reciprocal Rank compared to several baselines.", "published": "2018-05-11 15:01:00", "link": "http://arxiv.org/abs/1805.04437v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "iLCM - A Virtual Research Infrastructure for Large-Scale Qualitative\n  Data", "abstract": "The iLCM project pursues the development of an integrated research\nenvironment for the analysis of structured and unstructured data in a \"Software\nas a Service\" architecture (SaaS). The research environment addresses\nrequirements for the quantitative evaluation of large amounts of qualitative\ndata with text mining methods as well as requirements for the reproducibility\nof data-driven research designs in the social sciences. For this, the iLCM\nresearch environment comprises two central components. First, the Leipzig\nCorpus Miner (LCM), a decentralized SaaS application for the analysis of large\namounts of news texts developed in a previous Digital Humanities project.\nSecond, the text mining tools implemented in the LCM are extended by an \"Open\nResearch Computing\" (ORC) environment for executable script documents,\nso-called \"notebooks\". This novel integration allows to combine generic,\nhigh-performance methods to process large amounts of unstructured text data and\nwith individual program scripts to address specific research requirements in\ncomputational social science and digital humanities.", "published": "2018-05-11 10:24:11", "link": "http://arxiv.org/abs/1805.11404v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Reciprocal Attention Fusion for Visual Question Answering", "abstract": "Existing attention mechanisms either attend to local image grid or object\nlevel features for Visual Question Answering (VQA). Motivated by the\nobservation that questions can relate to both object instances and their parts,\nwe propose a novel attention mechanism that jointly considers reciprocal\nrelationships between the two levels of visual details. The bottom-up attention\nthus generated is further coalesced with the top-down information to only focus\non the scene elements that are most relevant to a given question. Our design\nhierarchically fuses multi-modal information i.e., language, object- and\ngird-level features, through an efficient tensor decomposition scheme. The\nproposed model improves the state-of-the-art single model performances from\n67.9% to 68.2% on VQAv1 and from 65.7% to 67.4% on VQAv2, demonstrating a\nsignificant boost.", "published": "2018-05-11 06:13:56", "link": "http://arxiv.org/abs/1805.04247v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Textual Membership Queries", "abstract": "Human labeling of data can be very time-consuming and expensive, yet, in many\ncases it is critical for the success of the learning process. In order to\nminimize human labeling efforts, we propose a novel active learning solution\nthat does not rely on existing sources of unlabeled data. It uses a small\namount of labeled data as the core set for the synthesis of useful membership\nqueries (MQs) - unlabeled instances generated by an algorithm for human\nlabeling. Our solution uses modification operators, functions that modify\ninstances to some extent. We apply the operators on a small set of instances\n(core set), creating a set of new membership queries. Using this framework, we\nlook at the instance space as a search space and apply search algorithms in\norder to generate new examples highly relevant to the learner. We implement\nthis framework in the textual domain and test it on several text classification\ntasks and show improved classifier performance as more MQs are labeled and\nincorporated into the training set. To the best of our knowledge, this is the\nfirst work on membership queries in the textual domain.", "published": "2018-05-11 22:40:59", "link": "http://arxiv.org/abs/1805.04609v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
