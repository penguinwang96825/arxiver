{"title": "Vector Projection Network for Few-shot Slot Tagging in Natural Language\n  Understanding", "abstract": "Few-shot slot tagging becomes appealing for rapid domain transfer and\nadaptation, motivated by the tremendous development of conversational dialogue\nsystems. In this paper, we propose a vector projection network for few-shot\nslot tagging, which exploits projections of contextual word embeddings on each\ntarget label vector as word-label similarities. Essentially, this approach is\nequivalent to a normalized linear model with an adaptive bias. The contrastive\nexperiment demonstrates that our proposed vector projection based similarity\nmetric can significantly surpass other variants. Specifically, in the five-shot\nsetting on benchmarks SNIPS and NER, our method outperforms the strongest\nfew-shot learning baseline by $6.30$ and $13.79$ points on F$_1$ score,\nrespectively. Our code will be released at\nhttps://github.com/sz128/few_shot_slot_tagging_and_NER.", "published": "2020-09-21 01:52:32", "link": "http://arxiv.org/abs/2009.09568v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Robustness and Generality of NLP Models Using Disentangled\n  Representations", "abstract": "Supervised neural networks, which first map an input $x$ to a single\nrepresentation $z$, and then map $z$ to the output label $y$, have achieved\nremarkable success in a wide range of natural language processing (NLP) tasks.\nDespite their success, neural models lack for both robustness and generality:\nsmall perturbations to inputs can result in absolutely different outputs; the\nperformance of a model trained on one domain drops drastically when tested on\nanother domain.\n  In this paper, we present methods to improve robustness and generality of NLP\nmodels from the standpoint of disentangled representation learning. Instead of\nmapping $x$ to a single representation $z$, the proposed strategy maps $x$ to a\nset of representations $\\{z_1,z_2,...,z_K\\}$ while forcing them to be\ndisentangled. These representations are then mapped to different logits $l$s,\nthe ensemble of which is used to make the final prediction $y$. We propose\ndifferent methods to incorporate this idea into currently widely-used models,\nincluding adding an $L$2 regularizer on $z$s or adding Total Correlation (TC)\nunder the framework of variational information bottleneck (VIB). We show that\nmodels trained with the proposed criteria provide better robustness and domain\nadaptation ability in a wide range of supervised learning tasks.", "published": "2020-09-21 02:48:46", "link": "http://arxiv.org/abs/2009.09587v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing the Severity of Health States based on Social Media Posts", "abstract": "The unprecedented growth of Internet users has resulted in an abundance of\nunstructured information on social media including health forums, where\npatients request health-related information or opinions from other users.\nPrevious studies have shown that online peer support has limited effectiveness\nwithout expert intervention. Therefore, a system capable of assessing the\nseverity of health state from the patients' social media posts can help health\nprofessionals (HP) in prioritizing the user's post. In this study, we inspect\nthe efficacy of different aspects of Natural Language Understanding (NLU) to\nidentify the severity of the user's health state in relation to two\nperspectives(tasks) (a) Medical Condition (i.e., Recover, Exist, Deteriorate,\nOther) and (b) Medication (i.e., Effective, Ineffective, Serious Adverse\nEffect, Other) in online health communities. We propose a multiview learning\nframework that models both the textual content as well as\ncontextual-information to assess the severity of the user's health state.\nSpecifically, our model utilizes the NLU views such as sentiment, emotions,\npersonality, and use of figurative language to extract the contextual\ninformation. The diverse NLU views demonstrate its effectiveness on both the\ntasks and as well as on the individual disease to assess a user's health.", "published": "2020-09-21 03:45:14", "link": "http://arxiv.org/abs/2009.09600v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modality-Transferable Emotion Embeddings for Low-Resource Multimodal\n  Emotion Recognition", "abstract": "Despite the recent achievements made in the multi-modal emotion recognition\ntask, two problems still exist and have not been well investigated: 1) the\nrelationship between different emotion categories are not utilized, which leads\nto sub-optimal performance; and 2) current models fail to cope well with\nlow-resource emotions, especially for unseen emotions. In this paper, we\npropose a modality-transferable model with emotion embeddings to tackle the\naforementioned issues. We use pre-trained word embeddings to represent emotion\ncategories for textual data. Then, two mapping functions are learned to\ntransfer these embeddings into visual and acoustic spaces. For each modality,\nthe model calculates the representation distance between the input sequence and\ntarget emotions and makes predictions based on the distances. By doing so, our\nmodel can directly adapt to the unseen emotions in any modality since we have\ntheir pre-trained embeddings and modality mapping functions. Experiments show\nthat our model achieves state-of-the-art performance on most of the emotion\ncategories. In addition, our model also outperforms existing baselines in the\nzero-shot and few-shot scenarios for unseen emotions.", "published": "2020-09-21 06:10:39", "link": "http://arxiv.org/abs/2009.09629v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Alleviating the Inequality of Attention Heads for Neural Machine\n  Translation", "abstract": "Recent studies show that the attention heads in Transformer are not equal. We\nrelate this phenomenon to the imbalance training of multi-head attention and\nthe model dependence on specific heads. To tackle this problem, we propose a\nsimple masking method: HeadMask, in two specific ways. Experiments show that\ntranslation improvements are achieved on multiple language pairs. Subsequent\nempirical analyses also support our assumption and confirm the effectiveness of\nthe method.", "published": "2020-09-21 08:14:30", "link": "http://arxiv.org/abs/2009.09672v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Profile Consistency Identification for Open-domain Dialogue Agents", "abstract": "Maintaining a consistent attribute profile is crucial for dialogue agents to\nnaturally converse with humans. Existing studies on improving attribute\nconsistency mainly explored how to incorporate attribute information in the\nresponses, but few efforts have been made to identify the consistency relations\nbetween response and attribute profile. To facilitate the study of profile\nconsistency identification, we create a large-scale human-annotated dataset\nwith over 110K single-turn conversations and their key-value attribute\nprofiles. Explicit relation between response and profile is manually labeled.\nWe also propose a key-value structure information enriched BERT model to\nidentify the profile consistency, and it gained improvements over strong\nbaselines. Further evaluations on downstream tasks demonstrate that the profile\nconsistency identification model is conducive for improving dialogue\nconsistency.", "published": "2020-09-21 08:38:23", "link": "http://arxiv.org/abs/2009.09680v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Bridging for Empathetic Dialogue Generation", "abstract": "Lack of external knowledge makes empathetic dialogue systems difficult to\nperceive implicit emotions and learn emotional interactions from limited\ndialogue history. To address the above problems, we propose to leverage\nexternal knowledge, including commonsense knowledge and emotional lexical\nknowledge, to explicitly understand and express emotions in empathetic dialogue\ngeneration. We first enrich the dialogue history by jointly interacting with\nexternal knowledge and construct an emotional context graph. Then we learn\nemotional context representations from the knowledge-enriched emotional context\ngraph and distill emotional signals, which are the prerequisites to predicate\nemotions expressed in responses. Finally, to generate the empathetic response,\nwe propose an emotional cross-attention mechanism to learn the emotional\ndependencies from the emotional context graph. Extensive experiments conducted\non a benchmark dataset verify the effectiveness of the proposed method. In\naddition, we find the performance of our method can be further improved by\nintegrating with a pre-trained model that works orthogonally.", "published": "2020-09-21 09:21:52", "link": "http://arxiv.org/abs/2009.09708v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multitask Pointer Network for Multi-Representational Parsing", "abstract": "We propose a transition-based approach that, by training a single model, can\nefficiently parse any input sentence with both constituent and dependency\ntrees, supporting both continuous/projective and discontinuous/non-projective\nsyntactic structures. To that end, we develop a Pointer Network architecture\nwith two separate task-specific decoders and a common encoder, and follow a\nmultitask learning strategy to jointly train them. The resulting quadratic\nsystem, not only becomes the first parser that can jointly produce both\nunrestricted constituent and dependency trees from a single model, but also\nproves that both syntactic formalisms can benefit from each other during\ntraining, achieving state-of-the-art accuracies in several widely-used\nbenchmarks such as the continuous English and Chinese Penn Treebanks, as well\nas the discontinuous German NEGRA and TIGER datasets.", "published": "2020-09-21 10:04:07", "link": "http://arxiv.org/abs/2009.09730v2", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Adjusting for Confounders with Text: Challenges and an Empirical\n  Evaluation Framework for Causal Inference", "abstract": "Causal inference studies using textual social media data can provide\nactionable insights on human behavior. Making accurate causal inferences with\ntext requires controlling for confounding which could otherwise impart bias.\nRecently, many different methods for adjusting for confounders have been\nproposed, and we show that these existing methods disagree with one another on\ntwo datasets inspired by previous social media studies. Evaluating causal\nmethods is challenging, as ground truth counterfactuals are almost never\navailable. Presently, no empirical evaluation framework for causal methods\nusing text exists, and as such, practitioners must select their methods without\nguidance. We contribute the first such framework, which consists of five tasks\ndrawn from real world studies. Our framework enables the evaluation of any\ncasual inference method using text. Across 648 experiments and two datasets, we\nevaluate every commonly used causal inference method and identify their\nstrengths and weaknesses to inform social media researchers seeking to use such\nmethods, and guide future improvements. We make all tasks, data, and models\npublic to inform applications and encourage additional research.", "published": "2020-09-21 15:38:45", "link": "http://arxiv.org/abs/2009.09961v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UCD-CS at W-NUT 2020 Shared Task-3: A Text to Text Approach for COVID-19\n  Event Extraction on Social Media", "abstract": "In this paper, we describe our approach in the shared task: COVID-19 event\nextraction from Twitter. The objective of this task is to extract answers from\nCOVID-related tweets to a set of predefined slot-filling questions. Our\napproach treats the event extraction task as a question answering task by\nleveraging the transformer-based T5 text-to-text model.\n  According to the official evaluation scores returned, namely F1, our\nsubmitted run achieves competitive performance compared to other participating\nruns (Top 3). However, we argue that this evaluation may underestimate the\nactual performance of runs based on text-generation. Although some such runs\nmay answer the slot questions well, they may not be an exact string match for\nthe gold standard answers. To measure the extent of this underestimation, we\nadopt a simple exact-answer transformation method aiming at converting the\nwell-answered predictions to exactly-matched predictions. The results show that\nafter this transformation our run overall reaches the same level of performance\nas the best participating run and state-of-the-art F1 scores in three of five\nCOVID-related events. Our code is publicly available to aid reproducibility", "published": "2020-09-21 17:39:00", "link": "http://arxiv.org/abs/2009.10047v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Latin BERT: A Contextual Language Model for Classical Philology", "abstract": "We present Latin BERT, a contextual language model for the Latin language,\ntrained on 642.7 million words from a variety of sources spanning the Classical\nera to the 21st century. In a series of case studies, we illustrate the\naffordances of this language-specific model both for work in natural language\nprocessing for Latin and in using computational methods for traditional\nscholarship: we show that Latin BERT achieves a new state of the art for\npart-of-speech tagging on all three Universal Dependency datasets for Latin and\ncan be used for predicting missing text (including critical emendations); we\ncreate a new dataset for assessing word sense disambiguation for Latin and\ndemonstrate that Latin BERT outperforms static word embeddings; and we show\nthat it can be used for semantically-informed search by querying contextual\nnearest neighbors. We publicly release trained models to help drive future work\nin this space.", "published": "2020-09-21 17:47:44", "link": "http://arxiv.org/abs/2009.10053v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Composed Variational Natural Language Generation for Few-shot Intents", "abstract": "In this paper, we focus on generating training examples for few-shot intents\nin the realistic imbalanced scenario. To build connections between existing\nmany-shot intents and few-shot intents, we consider an intent as a combination\nof a domain and an action, and propose a composed variational natural language\ngenerator (CLANG), a transformer-based conditional variational autoencoder.\nCLANG utilizes two latent variables to represent the utterances corresponding\nto two different independent parts (domain and action) in the intent, and the\nlatent variables are composed together to generate natural examples.\nAdditionally, to improve the generator learning, we adopt the contrastive\nregularization loss that contrasts the in-class with the out-of-class utterance\ngeneration given the intent. To evaluate the quality of the generated\nutterances, experiments are conducted on the generalized few-shot intent\ndetection task. Empirical results show that our proposed model achieves\nstate-of-the-art performances on two real-world intent detection datasets.", "published": "2020-09-21 17:48:43", "link": "http://arxiv.org/abs/2009.10056v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"When they say weed causes depression, but it's your fav\n  antidepressant\": Knowledge-aware Attention Framework for Relationship\n  Extraction", "abstract": "With the increasing legalization of medical and recreational use of cannabis,\nmore research is needed to understand the association between depression and\nconsumer behavior related to cannabis consumption. Big social media data has\npotential to provide deeper insights about these associations to public health\nanalysts. In this interdisciplinary study, we demonstrate the value of\nincorporating domain-specific knowledge in the learning process to identify the\nrelationships between cannabis use and depression. We develop an end-to-end\nknowledge infused deep learning framework (Gated-K-BERT) that leverages the\npre-trained BERT language representation model and domain-specific declarative\nknowledge source (Drug Abuse Ontology (DAO)) to jointly extract entities and\ntheir relationship using gated fusion sharing mechanism. Our model is further\ntailored to provide more focus to the entities mention in the sentence through\nentity-position aware attention layer, where ontology is used to locate the\ntarget entities position. Experimental results show that inclusion of the\nknowledge-aware attentive representation in association with BERT can extract\nthe cannabis-depression relationship with better coverage in comparison to the\nstate-of-the-art relation extractor.", "published": "2020-09-21 19:54:42", "link": "http://arxiv.org/abs/2009.10155v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Persian Dependency Treebank Made Universal", "abstract": "We describe an automatic method for converting the Persian Dependency\nTreebank (Rasooli et al, 2013) to Universal Dependencies. This treebank\ncontains 29107 sentences. Our experiments along with manual linguistic analysis\nshow that our data is more compatible with Universal Dependencies than the\nUppsala Persian Universal Dependency Treebank (Seraji et al., 2016), and is\nlarger in size and more diverse in vocabulary. Our data brings in a labeled\nattachment F-score of 85.2 in supervised parsing. Our delexicalized\nPersian-to-English parser transfer experiments show that a parsing model\ntrained on our data is ~2% absolutely more accurate than that of Seraji et al.\n(2016) in terms of labeled attachment score.", "published": "2020-09-21 22:34:13", "link": "http://arxiv.org/abs/2009.10205v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generative Imagination Elevates Machine Translation", "abstract": "There are common semantics shared across text and images. Given a sentence in\na source language, whether depicting the visual scene helps translation into a\ntarget language? Existing multimodal neural machine translation methods (MNMT)\nrequire triplets of bilingual sentence - image for training and tuples of\nsource sentence - image for inference. In this paper, we propose ImagiT, a\nnovel machine translation method via visual imagination. ImagiT first learns to\ngenerate visual representation from the source sentence, and then utilizes both\nsource sentence and the \"imagined representation\" to produce a target\ntranslation. Unlike previous methods, it only needs the source sentence at the\ninference time. Experiments demonstrate that ImagiT benefits from visual\nimagination and significantly outperforms the text-only neural machine\ntranslation baselines. Further analysis reveals that the imagination process in\nImagiT helps fill in missing information when performing the degradation\nstrategy.", "published": "2020-09-21 07:44:04", "link": "http://arxiv.org/abs/2009.09654v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Accent Estimation of Japanese Words from Their Surfaces and\n  Romanizations for Building Large Vocabulary Accent Dictionaries", "abstract": "In Japanese text-to-speech (TTS), it is necessary to add accent information\nto the input sentence. However, there are a limited number of publicly\navailable accent dictionaries, and those dictionaries e.g. UniDic, do not\ncontain many compound words, proper nouns, etc., which are required in a\npractical TTS system. In order to build a large scale accent dictionary that\ncontains those words, the authors developed an accent estimation technique that\npredicts the accent of a word from its limited information, namely the surface\n(e.g. kanji) and the yomi (simplified phonetic information). It is\nexperimentally shown that the technique can estimate accents with high\naccuracies, especially for some categories of words. The authors applied this\ntechnique to an existing large vocabulary Japanese dictionary NEologd, and\nobtained a large vocabulary Japanese accent dictionary. Many cases have been\nobserved in which the use of this dictionary yields more appropriate phonetic\ninformation than UniDic.", "published": "2020-09-21 08:38:21", "link": "http://arxiv.org/abs/2009.09679v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "\"Listen, Understand and Translate\": Triple Supervision Decouples\n  End-to-end Speech-to-text Translation", "abstract": "An end-to-end speech-to-text translation (ST) takes audio in a source\nlanguage and outputs the text in a target language. Existing methods are\nlimited by the amount of parallel corpus. Can we build a system to fully\nutilize signals in a parallel ST corpus? We are inspired by human understanding\nsystem which is composed of auditory perception and cognitive processing. In\nthis paper, we propose Listen-Understand-Translate, (LUT), a unified framework\nwith triple supervision signals to decouple the end-to-end speech-to-text\ntranslation task. LUT is able to guide the acoustic encoder to extract as much\ninformation from the auditory input. In addition, LUT utilizes a pre-trained\nBERT model to enforce the upper encoder to produce as much semantic information\nas possible, without extra data. We perform experiments on a diverse set of\nspeech translation benchmarks, including Librispeech English-French, IWSLT\nEnglish-German and TED English-Chinese. Our results demonstrate LUT achieves\nthe state-of-the-art performance, outperforming previous methods. The code is\navailable at https://github.com/dqqcasia/st.", "published": "2020-09-21 09:19:07", "link": "http://arxiv.org/abs/2009.09704v3", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Consecutive Decoding for Speech-to-text Translation", "abstract": "Speech-to-text translation (ST), which directly translates the source\nlanguage speech to the target language text, has attracted intensive attention\nrecently. However, the combination of speech recognition and machine\ntranslation in a single model poses a heavy burden on the direct cross-modal\ncross-lingual mapping. To reduce the learning difficulty, we propose\nCOnSecutive Transcription and Translation (COSTT), an integral approach for\nspeech-to-text translation. The key idea is to generate source transcript and\ntarget translation text with a single decoder. It benefits the model training\nso that additional large parallel text corpus can be fully exploited to enhance\nthe speech translation training. Our method is verified on three mainstream\ndatasets, including Augmented LibriSpeech English-French dataset, IWSLT2018\nEnglish-German dataset, and TED English-Chinese dataset. Experiments show that\nour proposed COSTT outperforms or on par with the previous state-of-the-art\nmethods on the three datasets. We have released our code at\n\\url{https://github.com/dqqcasia/st}.", "published": "2020-09-21 10:10:45", "link": "http://arxiv.org/abs/2009.09737v4", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Rethinking Supervised Learning and Reinforcement Learning in\n  Task-Oriented Dialogue Systems", "abstract": "Dialogue policy learning for task-oriented dialogue systems has enjoyed great\nprogress recently mostly through employing reinforcement learning methods.\nHowever, these approaches have become very sophisticated. It is time to\nre-evaluate it. Are we really making progress developing dialogue agents only\nbased on reinforcement learning? We demonstrate how (1)~traditional supervised\nlearning together with (2)~a simulator-free adversarial learning method can be\nused to achieve performance comparable to state-of-the-art RL-based methods.\nFirst, we introduce a simple dialogue action decoder to predict the appropriate\nactions. Then, the traditional multi-label classification solution for dialogue\npolicy learning is extended by adding dense layers to improve the dialogue\nagent performance. Finally, we employ the Gumbel-Softmax estimator to\nalternatively train the dialogue agent and the dialogue reward model without\nusing reinforcement learning. Based on our extensive experimentation, we can\nconclude the proposed methods can achieve more stable and higher performance\nwith fewer efforts, such as the domain knowledge required to design a user\nsimulator and the intractable parameter tuning in reinforcement learning. Our\nmain goal is not to beat reinforcement learning with supervised learning, but\nto demonstrate the value of rethinking the role of reinforcement learning and\nsupervised learning in optimizing task-oriented dialogue systems.", "published": "2020-09-21 12:04:18", "link": "http://arxiv.org/abs/2009.09781v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Content Planning for Neural Story Generation with Aristotelian Rescoring", "abstract": "Long-form narrative text generated from large language models manages a\nfluent impersonation of human writing, but only at the local sentence level,\nand lacks structure or global cohesion. We posit that many of the problems of\nstory generation can be addressed via high-quality content planning, and\npresent a system that focuses on how to learn good plot structures to guide\nstory generation. We utilize a plot-generation language model along with an\nensemble of rescoring models that each implement an aspect of good\nstory-writing as detailed in Aristotle's Poetics. We find that stories written\nwith our more principled plot-structure are both more relevant to a given\nprompt and higher quality than baselines that do not content plan, or that plan\nin an unprincipled way.", "published": "2020-09-21 13:41:32", "link": "http://arxiv.org/abs/2009.09870v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WESSA at SemEval-2020 Task 9: Code-Mixed Sentiment Analysis using\n  Transformers", "abstract": "In this paper, we describe our system submitted for SemEval 2020 Task 9,\nSentiment Analysis for Code-Mixed Social Media Text alongside other\nexperiments. Our best performing system is a Transfer Learning-based model that\nfine-tunes \"XLM-RoBERTa\", a transformer-based multilingual masked language\nmodel, on monolingual English and Spanish data and Spanish-English code-mixed\ndata. Our system outperforms the official task baseline by achieving a 70.1%\naverage F1-Score on the official leaderboard using the test set. For later\nsubmissions, our system manages to achieve a 75.9% average F1-Score on the test\nset using CodaLab username \"ahmed0sultan\".", "published": "2020-09-21 13:59:24", "link": "http://arxiv.org/abs/2009.09879v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Weakly Supervised Learning of Nuanced Frames for Analyzing Polarization\n  in News Media", "abstract": "In this paper we suggest a minimally-supervised approach for identifying\nnuanced frames in news article coverage of politically divisive topics. We\nsuggest to break the broad policy frames suggested by Boydstun et al., 2014\ninto fine-grained subframes which can capture differences in political ideology\nin a better way. We evaluate the suggested subframes and their embedding,\nlearned using minimal supervision, over three topics, namely, immigration,\ngun-control and abortion. We demonstrate the ability of the subframes to\ncapture ideological differences and analyze political discourse in news media.", "published": "2020-09-21 04:29:54", "link": "http://arxiv.org/abs/2009.09609v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Visual-Semantic Embedding Model Informed by Structured Knowledge", "abstract": "We propose a novel approach to improve a visual-semantic embedding model by\nincorporating concept representations captured from an external structured\nknowledge base. We investigate its performance on image classification under\nboth standard and zero-shot settings. We propose two novel evaluation\nframeworks to analyse classification errors with respect to the class hierarchy\nindicated by the knowledge base. The approach is tested using the ILSVRC 2012\nimage dataset and a WordNet knowledge base. With respect to both standard and\nzero-shot image classification, our approach shows superior performance\ncompared with the original approach, which uses word embeddings.", "published": "2020-09-21 17:04:32", "link": "http://arxiv.org/abs/2009.10026v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving\n  Out-of-Domain Robustness", "abstract": "Models that perform well on a training domain often fail to generalize to\nout-of-domain (OOD) examples. Data augmentation is a common method used to\nprevent overfitting and improve OOD generalization. However, in natural\nlanguage, it is difficult to generate new examples that stay on the underlying\ndata manifold. We introduce SSMBA, a data augmentation method for generating\nsynthetic training examples by using a pair of corruption and reconstruction\nfunctions to move randomly on a data manifold. We investigate the use of SSMBA\nin the natural language domain, leveraging the manifold assumption to\nreconstruct corrupted text with masked language models. In experiments on\nrobustness benchmarks across 3 tasks and 9 datasets, SSMBA consistently\noutperforms existing data augmentation methods and baseline models on both\nin-domain and OOD data, achieving gains of 0.8% accuracy on OOD Amazon reviews,\n1.8% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English.", "published": "2020-09-21 22:02:33", "link": "http://arxiv.org/abs/2009.10195v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "DiffWave: A Versatile Diffusion Model for Audio Synthesis", "abstract": "In this work, we propose DiffWave, a versatile diffusion probabilistic model\nfor conditional and unconditional waveform generation. The model is\nnon-autoregressive, and converts the white noise signal into structured\nwaveform through a Markov chain with a constant number of steps at synthesis.\nIt is efficiently trained by optimizing a variant of variational bound on the\ndata likelihood. DiffWave produces high-fidelity audios in different waveform\ngeneration tasks, including neural vocoding conditioned on mel spectrogram,\nclass-conditional generation, and unconditional generation. We demonstrate that\nDiffWave matches a strong WaveNet vocoder in terms of speech quality (MOS: 4.44\nversus 4.43), while synthesizing orders of magnitude faster. In particular, it\nsignificantly outperforms autoregressive and GAN-based waveform models in the\nchallenging unconditional generation task in terms of audio quality and sample\ndiversity from various automatic and human evaluations.", "published": "2020-09-21 11:20:38", "link": "http://arxiv.org/abs/2009.09761v3", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Light Convolutional Neural Network with Feature Genuinization for\n  Detection of Synthetic Speech Attacks", "abstract": "Modern text-to-speech (TTS) and voice conversion (VC) systems produce natural\nsounding speech that questions the security of automatic speaker verification\n(ASV). This makes detection of such synthetic speech very important to\nsafeguard ASV systems from unauthorized access. Most of the existing spoofing\ncountermeasures perform well when the nature of the attacks is made known to\nthe system during training. However, their performance degrades in face of\nunseen nature of attacks. In comparison to the synthetic speech created by a\nwide range of TTS and VC methods, genuine speech has a more consistent\ndistribution. We believe that the difference between the distribution of\nsynthetic and genuine speech is an important discriminative feature between the\ntwo classes. In this regard, we propose a novel method referred to as feature\ngenuinization that learns a transformer with convolutional neural network (CNN)\nusing the characteristics of only genuine speech. We then use this\ngenuinization transformer with a light CNN classifier. The ASVspoof 2019\nlogical access corpus is used to evaluate the proposed method. The studies show\nthat the proposed feature genuinization based LCNN system outperforms other\nstate-of-the-art spoofing countermeasures, depicting its effectiveness for\ndetection of synthetic speech attacks.", "published": "2020-09-21 06:38:19", "link": "http://arxiv.org/abs/2009.09637v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "DcaseNet: An integrated pretrained deep neural network for detecting and\n  classifying acoustic scenes and events", "abstract": "Although acoustic scenes and events include many related tasks, their\ncombined detection and classification have been scarcely investigated. We\npropose three architectures of deep neural networks that are integrated to\nsimultaneously perform acoustic scene classification, audio tagging, and sound\nevent detection. The first two architectures are inspired by human cognitive\nprocesses. The first architecture resembles the short-term perception for scene\nclassification of adults, who can detect various sound events that are then\nused to identify the acoustic scene. The second architecture resembles the\nlong-term learning of babies, being also the concept underlying self-supervised\nlearning. Babies first observe the effects of abstract notions such as gravity\nand then learn specific tasks using such perceptions. The third architecture\nadds a few layers to the second one that solely perform a single task before\nits corresponding output layer. We aim to build an integrated system that can\nserve as a pretrained model to perform the three abovementioned tasks.\nExperiments on three datasets demonstrate that the proposed architecture,\ncalled DcaseNet, can be either directly used for any of the tasks while\nproviding suitable results or fine-tuned to improve the performance of one\ntask. The code and pretrained DcaseNet weights are available at\nhttps://github.com/Jungjee/DcaseNet.", "published": "2020-09-21 07:04:23", "link": "http://arxiv.org/abs/2009.09642v4", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "End-to-End Bengali Speech Recognition", "abstract": "Bengali is a prominent language of the Indian subcontinent. However, while\nmany state-of-the-art acoustic models exist for prominent languages spoken in\nthe region, research and resources for Bengali are few and far between. In this\nwork, we apply CTC based CNN-RNN networks, a prominent deep learning based\nend-to-end automatic speech recognition technique, to the Bengali ASR task. We\nalso propose and evaluate the applicability and efficacy of small 7x3 and 3x3\nconvolution kernels which are prominently used in the computer vision domain\nprimarily because of their FLOPs and parameter efficient nature. We propose two\nCNN blocks, 2-layer Block A and 4-layer Block B, with the first layer\ncomprising of 7x3 kernel and the subsequent layers comprising solely of 3x3\nkernels. Using the publicly available Large Bengali ASR Training data set, we\nbenchmark and evaluate the performance of seven deep neural network\nconfigurations of varying complexities and depth on the Bengali ASR task. Our\nbest model, with Block B, has a WER of 13.67, having an absolute reduction of\n1.39% over comparable model with larger convolution kernels of size 41x11 and\n21x11.", "published": "2020-09-21 05:08:07", "link": "http://arxiv.org/abs/2009.09615v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Detecting Sound Events Using Convolutional Macaron Net With Pseudo\n  Strong Labels", "abstract": "In this paper, we propose addressing the lack of strongly labeled data by\nusing pseudo strongly labeled data approximated using Convolutive Nonnegative\nMatrix Factorization. Using this set of data, we then train a novel\narchitecture called the Convolutional Macaron Net (CMN), which combines\nConvolutional Neural Network (CNN) with MN, in a semi-supervised manner.\nInstead of training only a single model or using the Mean-teacher approach, we\ntrain two different CMNs synchronously using a curriculum consistency cost and\na curriculum interpolated consistency cost. In the inference stage, one of the\nmodels will provide the frame-level prediction while the other model will\nprovide the clip-level prediction. Our system outperforms the baseline system\nof the Detection and Classification of Acoustic Scenes and Events (DCASE) 2020\nChallenge Task 4 by a margin of over 10% based on our proposed framework. By\ncomparing with the top submission of the DCASE 2019 challenge, our system\naccuracy is also higher by 1.8%. On the other hand, as compared to the top\nsubmission of DCASE 2020, our accuracy is also marginally higher by 0.3%, even\nwith fewer Transformer encoding layers. Our system remains robust on unseen\nYouTube evaluation dataset and has a winning margin of 0.6% and 6.3% against\nthe top submission of DCASE 2019 and the baseline system.", "published": "2020-09-21 06:23:37", "link": "http://arxiv.org/abs/2009.09632v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Deep Learning Based Analysis-Synthesis Framework For Unison Singing", "abstract": "Unison singing is the name given to an ensemble of singers simultaneously\nsinging the same melody and lyrics. While each individual singer in a unison\nsings the same principle melody, there are slight timing and pitch deviations\nbetween the singers, which, along with the ensemble of timbres, give the\nlistener a perceived sense of \"unison\". In this paper, we present a study of\nunison singing in the context of choirs; utilising some recently proposed\ndeep-learning based methodologies, we analyse the fundamental frequency (F0)\ndistribution of the individual singers in recordings of unison mixtures. Based\non the analysis, we propose a system for synthesising a unison signal from an a\ncappella input and a single voice prototype representative of a unison mixture.\nWe use subjective listening tests to evaluate perceptual factors of our\nproposed system for synthesis, including quality, adherence to the melody as\nwell the degree of perceived unison.", "published": "2020-09-21 13:48:01", "link": "http://arxiv.org/abs/2009.09875v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "End-to-End Speaker-Dependent Voice Activity Detection", "abstract": "Voice activity detection (VAD) is an essential pre-processing step for tasks\nsuch as automatic speech recognition (ASR) and speaker recognition. A basic\ngoal is to remove silent segments within an audio, while a more general VAD\nsystem could remove all the irrelevant segments such as noise and even unwanted\nspeech from non-target speakers. We define the task, which only detects the\nspeech from the target speaker, as speaker-dependent voice activity detection\n(SDVAD). This task is quite common in real applications and usually implemented\nby performing speaker verification (SV) on audio segments extracted from VAD.\nIn this paper, we propose an end-to-end neural network based approach to\naddress this problem, which explicitly takes the speaker identity into the\nmodeling process. Moreover, inference can be performed in an online fashion,\nwhich leads to low system latency. Experiments are carried out on a\nconversational telephone dataset generated from the Switchboard corpus. Results\nshow that our proposed online approach achieves significantly better\nperformance than the usual VAD/SV system in terms of both frame accuracy and\nF-score. We also used our previously proposed segment-level metric for a more\ncomprehensive analysis.", "published": "2020-09-21 14:26:12", "link": "http://arxiv.org/abs/2009.09906v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Open-set Short Utterance Forensic Speaker Verification using\n  Teacher-Student Network with Explicit Inductive Bias", "abstract": "In forensic applications, it is very common that only small naturalistic\ndatasets consisting of short utterances in complex or unknown acoustic\nenvironments are available. In this study, we propose a pipeline solution to\nimprove speaker verification on a small actual forensic field dataset. By\nleveraging large-scale out-of-domain datasets, a knowledge distillation based\nobjective function is proposed for teacher-student learning, which is applied\nfor short utterance forensic speaker verification. The objective function\ncollectively considers speaker classification loss, Kullback-Leibler\ndivergence, and similarity of embeddings. In order to advance the trained deep\nspeaker embedding network to be robust for a small target dataset, we introduce\na novel strategy to fine-tune the pre-trained student model towards a forensic\ntarget domain by utilizing the model as a finetuning start point and a\nreference in regularization. The proposed approaches are evaluated on the\n1st48-UTD forensic corpus, a newly established naturalistic dataset of actual\nhomicide investigations consisting of short utterances recorded in uncontrolled\nconditions. We show that the proposed objective function can efficiently\nimprove the performance of teacher-student learning on short utterances and\nthat our fine-tuning strategy outperforms the commonly used weight decay method\nby providing an explicit inductive bias towards the pre-trained model.", "published": "2020-09-21 00:58:40", "link": "http://arxiv.org/abs/2009.09556v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Correlating Subword Articulation with Lip Shapes for Embedding Aware\n  Audio-Visual Speech Enhancement", "abstract": "In this paper, we propose a visual embedding approach to improving embedding\naware speech enhancement (EASE) by synchronizing visual lip frames at the phone\nand place of articulation levels. We first extract visual embedding from lip\nframes using a pre-trained phone or articulation place recognizer for\nvisual-only EASE (VEASE). Next, we extract audio-visual embedding from noisy\nspeech and lip videos in an information intersection manner, utilizing a\ncomplementarity of audio and visual features for multi-modal EASE (MEASE).\nExperiments on the TCD-TIMIT corpus corrupted by simulated additive noises show\nthat our proposed subword based VEASE approach is more effective than\nconventional embedding at the word level. Moreover, visual embedding at the\narticulation place level, leveraging upon a high correlation between place of\narticulation and lip shapes, shows an even better performance than that at the\nphone level. Finally the proposed MEASE framework, incorporating both audio and\nvisual embedding, yields significantly better speech quality and\nintelligibility than those obtained with the best visual-only and audio-only\nEASE systems.", "published": "2020-09-21 01:26:19", "link": "http://arxiv.org/abs/2009.09561v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Using Inaudible Audio and Voice Assistants to Transmit Sensitive Data\n  over Telephony", "abstract": "New security and privacy concerns arise due to the growing popularity of\nvoice assistant (VA) deployments in home and enterprise networks. A number of\npast research results have demonstrated how malicious actors can use hidden\ncommands to get VAs to perform certain operations even when a person may be in\ntheir vicinity. However, such work has not explored how compromised computers\nthat are close to VAs can leverage the phone channel to exfiltrate data with\nthe help of VAs. After characterizing the communication channel that is set up\nby commanding a VA to make a call to a phone number, we demonstrate how malware\ncan encode data into audio and send it via the phone channel. Such an attack,\nwhich can be crafted remotely, at scale and at low cost, can be used to bypass\nnetwork defenses that may be deployed against leakage of sensitive data. We use\nDual-Tone Multi-Frequency tones to encode arbitrary binary data into audio that\ncan be played over computer speakers and sent through a VA mediated phone\nchannel to a remote system. We show that modest amounts of data can be\ntransmitted with high accuracy with a short phone call lasting a few minutes.\nThis can be done while making the audio nearly inaudible for most people by\nmodulating it with a carrier with frequencies that are near the higher end of\nthe human hearing range. Several factors influence the data transfer rate,\nincluding the distance between the computer and the VA, the ambient noise that\nmay be present and the frequency of modulating carrier. With the help of a\nprototype built by us, we experimentally assess the impact of these factors on\ndata transfer rates and transmission accuracy. Our results show that voice\nassistants in the vicinity of computers can pose new threats to data stored on\nsuch computers. These threats are not addressed by traditional host and network\ndefenses. We briefly discuss possible mitigation ways.", "published": "2020-09-21 22:16:31", "link": "http://arxiv.org/abs/2009.10200v1", "categories": ["cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
