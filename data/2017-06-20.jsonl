{"title": "Improving text classification with vectors of reduced precision", "abstract": "This paper presents the analysis of the impact of a floating-point number\nprecision reduction on the quality of text classification. The precision\nreduction of the vectors representing the data (e.g. TF-IDF representation in\nour case) allows for a decrease of computing time and memory footprint on\ndedicated hardware platforms. The impact of precision reduction on the\nclassification quality was performed on 5 corpora, using 4 different\nclassifiers. Also, dimensionality reduction was taken into account. Results\nindicate that the precision reduction improves classification accuracy for most\ncases (up to 25% of error reduction). In general, the reduction from 64 to 4\nbits gives the best scores and ensures that the results will not be worse than\nwith the full floating-point representation.", "published": "2017-06-20 11:13:06", "link": "http://arxiv.org/abs/1706.06363v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "THUMT: An Open Source Toolkit for Neural Machine Translation", "abstract": "This paper introduces THUMT, an open-source toolkit for neural machine\ntranslation (NMT) developed by the Natural Language Processing Group at\nTsinghua University. THUMT implements the standard attention-based\nencoder-decoder framework on top of Theano and supports three training\ncriteria: maximum likelihood estimation, minimum risk training, and\nsemi-supervised training. It features a visualization tool for displaying the\nrelevance between hidden states in neural networks and contextual words, which\nhelps to analyze the internal workings of NMT. Experiments on Chinese-English\ndatasets show that THUMT using minimum risk training significantly outperforms\nGroundHog, a state-of-the-art toolkit for NMT.", "published": "2017-06-20 13:29:16", "link": "http://arxiv.org/abs/1706.06415v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effective Spoken Language Labeling with Deep Recurrent Neural Networks", "abstract": "Understanding spoken language is a highly complex problem, which can be\ndecomposed into several simpler tasks. In this paper, we focus on Spoken\nLanguage Understanding (SLU), the module of spoken dialog systems responsible\nfor extracting a semantic interpretation from the user utterance. The task is\ntreated as a labeling problem. In the past, SLU has been performed with a wide\nvariety of probabilistic models. The rise of neural networks, in the last\ncouple of years, has opened new interesting research directions in this domain.\nRecurrent Neural Networks (RNNs) in particular are able not only to represent\nseveral pieces of information as embeddings but also, thanks to their recurrent\narchitecture, to encode as embeddings relatively long contexts. Such long\ncontexts are in general out of reach for models previously used for SLU. In\nthis paper we propose novel RNNs architectures for SLU which outperform\nprevious ones. Starting from a published idea as base block, we design new deep\nRNNs achieving state-of-the-art results on two widely used corpora for SLU:\nATIS (Air Traveling Information System), in English, and MEDIA (Hotel\ninformation and reservation in France), in French.", "published": "2017-06-20 09:44:52", "link": "http://arxiv.org/abs/1706.06896v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Neural Ad-hoc Ranking with Kernel Pooling", "abstract": "This paper proposes K-NRM, a kernel based neural model for document ranking.\nGiven a query and a set of documents, K-NRM uses a translation matrix that\nmodels word-level similarities via word embeddings, a new kernel-pooling\ntechnique that uses kernels to extract multi-level soft match features, and a\nlearning-to-rank layer that combines those features into the final ranking\nscore. The whole model is trained end-to-end. The ranking layer learns desired\nfeature patterns from the pairwise ranking loss. The kernels transfer the\nfeature patterns into soft-match targets at each similarity level and enforce\nthem on the translation matrix. The word embeddings are tuned accordingly so\nthat they can produce the desired soft matches. Experiments on a commercial\nsearch engine's query log demonstrate the improvements of K-NRM over prior\nfeature-based and neural-based states-of-the-art, and explain the source of\nK-NRM's advantage: Its kernel-guided embedding encodes a similarity metric\ntailored for matching query words to document words, and provides effective\nmulti-level soft matches.", "published": "2017-06-20 18:19:54", "link": "http://arxiv.org/abs/1706.06613v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Graph-based Neural Multi-Document Summarization", "abstract": "We propose a neural multi-document summarization (MDS) system that\nincorporates sentence relation graphs. We employ a Graph Convolutional Network\n(GCN) on the relation graphs, with sentence embeddings obtained from Recurrent\nNeural Networks as input node features. Through multiple layer-wise\npropagation, the GCN generates high-level hidden sentence features for salience\nestimation. We then use a greedy heuristic to extract salient sentences while\navoiding redundancy. In our experiments on DUC 2004, we consider three types of\nsentence relation graphs and demonstrate the advantage of combining sentence\nrelations in graphs with the representation power of deep neural networks. Our\nmodel improves upon traditional graph-based extractive approaches and the\nvanilla GRU sequence model with no graph, and it achieves competitive results\nagainst other state-of-the-art multi-document summarization systems.", "published": "2017-06-20 22:12:14", "link": "http://arxiv.org/abs/1706.06681v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Grounded Language Learning in a Simulated 3D World", "abstract": "We are increasingly surrounded by artificially intelligent technology that\ntakes decisions and executes actions on our behalf. This creates a pressing\nneed for general means to communicate with, instruct and guide artificial\nagents, with human language the most compelling means for such communication.\nTo achieve this in a scalable fashion, agents must be able to relate language\nto the world and to actions; that is, their understanding of language must be\ngrounded and embodied. However, learning grounded language is a notoriously\nchallenging problem in artificial intelligence research. Here we present an\nagent that learns to interpret language in a simulated 3D environment where it\nis rewarded for the successful execution of written instructions. Trained via a\ncombination of reinforcement and unsupervised learning, and beginning with\nminimal prior knowledge, the agent learns to relate linguistic symbols to\nemergent perceptual representations of its physical surroundings and to\npertinent sequences of actions. The agent's comprehension of language extends\nbeyond its prior experience, enabling it to apply familiar language to\nunfamiliar situations and to interpret entirely novel instructions. Moreover,\nthe speed with which this agent learns new words increases as its semantic\nknowledge grows. This facility for generalising and bootstrapping semantic\nknowledge indicates the potential of the present approach for reconciling\nambiguous natural language with the complexity of the physical world.", "published": "2017-06-20 17:09:29", "link": "http://arxiv.org/abs/1706.06551v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
