{"title": "Multilingual Constituency Parsing with Self-Attention and Pre-Training", "abstract": "We show that constituency parsing benefits from unsupervised pre-training\nacross a variety of languages and a range of pre-training conditions. We first\ncompare the benefits of no pre-training, fastText, ELMo, and BERT for English\nand find that BERT outperforms ELMo, in large part due to increased model\ncapacity, whereas ELMo in turn outperforms the non-contextual fastText\nembeddings. We also find that pre-training is beneficial across all 11\nlanguages tested; however, large model sizes (more than 100 million parameters)\nmake it computationally expensive to train separate models for each language.\nTo address this shortcoming, we show that joint multilingual pre-training and\nfine-tuning allows sharing all but a small number of parameters between ten\nlanguages in the final model. The 10x reduction in model size compared to\nfine-tuning one model per language causes only a 3.2% relative error increase\nin aggregate. We further explore the idea of joint fine-tuning and show that it\ngives low-resource languages a way to benefit from the larger datasets of other\nlanguages. Finally, we demonstrate new state-of-the-art results for 11\nlanguages, including English (95.8 F1) and Chinese (91.8 F1).", "published": "2018-12-31 11:01:02", "link": "http://arxiv.org/abs/1812.11760v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Advancing Acoustic-to-Word CTC Model with Attention and Mixed-Units", "abstract": "The acoustic-to-word model based on the Connectionist Temporal Classification\n(CTC) criterion is a natural end-to-end (E2E) system directly targeting word as\noutput unit. Two issues exist in the system: first, the current output of the\nCTC model relies on the current input and does not account for context weighted\ninputs. This is the hard alignment issue. Second, the word-based CTC model\nsuffers from the out-of-vocabulary (OOV) issue. This means it can model only\nfrequently occurring words while tagging the remaining words as OOV. Hence,\nsuch a model is limited in its capacity in recognizing only a fixed set of\nfrequent words. In this study, we propose addressing these problems using a\ncombination of attention mechanism and mixed-units. In particular, we introduce\nAttention CTC, Self-Attention CTC, Hybrid CTC, and Mixed-unit CTC.\n  First, we blend attention modeling capabilities directly into the CTC network\nusing Attention CTC and Self-Attention CTC. Second, to alleviate the OOV issue,\nwe present Hybrid CTC which uses a word and letter CTC with shared hidden\nlayers. The Hybrid CTC consults the letter CTC when the word CTC emits an OOV.\nThen, we propose a much better solution by training a Mixed-unit CTC which\ndecomposes all the OOV words into sequences of frequent words and multi-letter\nunits. Evaluated on a 3400 hours Microsoft Cortana voice assistant task, our\nfinal acoustic-to-word solution using attention and mixed-units achieves a\nrelative reduction in word error rate (WER) over the vanilla word CTC by\n12.09\\%. Such an E2E model without using any language model (LM) or complex\ndecoder also outperforms a traditional context-dependent (CD) phoneme CTC with\nstrong LM and decoder by 6.79% relative.", "published": "2018-12-31 18:10:42", "link": "http://arxiv.org/abs/1812.11928v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Types, Tokens, and Hapaxes: A New Heap's Law", "abstract": "Heap's Law states that in a large enough text corpus, the number of types as\na function of tokens grows as $N=KM^\\beta$ for some free parameters $K,\\beta$.\nMuch has been written about how this result and various generalizations can be\nderived from Zipf's Law. Here we derive from first principles a completely\nnovel expression of the type-token curve and prove its superior accuracy on\nreal text. This expression naturally generalizes to equally accurate estimates\nfor counting hapaxes and higher $n$-legomena.", "published": "2018-12-31 15:00:38", "link": "http://arxiv.org/abs/1901.00521v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-language Citation Recommendation via Hierarchical Representation\n  Learning on Heterogeneous Graph", "abstract": "While the volume of scholarly publications has increased at a frenetic pace,\naccessing and consuming the useful candidate papers, in very large digital\nlibraries, is becoming an essential and challenging task for scholars.\nUnfortunately, because of language barrier, some scientists (especially the\njunior ones or graduate students who do not master other languages) cannot\nefficiently locate the publications hosted in a foreign language repository. In\nthis study, we propose a novel solution, cross-language citation recommendation\nvia Hierarchical Representation Learning on Heterogeneous Graph (HRLHG), to\naddress this new problem. HRLHG can learn a representation function by mapping\nthe publications, from multilingual repositories, to a low-dimensional joint\nembedding space from various kinds of vertexes and relations on a heterogeneous\ngraph. By leveraging both global (task specific) plus local (task independent)\ninformation as well as a novel supervised hierarchical random walk algorithm,\nthe proposed method can optimize the publication representations by maximizing\nthe likelihood of locating the important cross-language neighborhoods on the\ngraph. Experiment results show that the proposed method can not only outperform\nstate-of-the-art baseline models, but also improve the interpretability of the\nrepresentation model for cross-language citation recommendation task.", "published": "2018-12-31 07:09:59", "link": "http://arxiv.org/abs/1812.11709v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Entity Synonym Discovery via Multipiece Bilateral Context Matching", "abstract": "Being able to automatically discover synonymous entities in an open-world\nsetting benefits various tasks such as entity disambiguation or knowledge graph\ncanonicalization. Existing works either only utilize entity features, or rely\non structured annotations from a single piece of context where the entity is\nmentioned. To leverage diverse contexts where entities are mentioned, in this\npaper, we generalize the distributional hypothesis to a multi-context setting\nand propose a synonym discovery framework that detects entity synonyms from\nfree-text corpora with considerations on effectiveness and robustness. As one\nof the key components in synonym discovery, we introduce a neural network model\nSYNONYMNET to determine whether or not two given entities are synonym with each\nother. Instead of using entities features, SYNONYMNET makes use of multiple\npieces of contexts in which the entity is mentioned, and compares the\ncontext-level similarity via a bilateral matching schema. Experimental results\ndemonstrate that the proposed model is able to detect synonym sets that are not\nobserved during training on both generic and domain-specific datasets:\nWiki+Freebase, PubMed+UMLS, and MedBook+MKG, with up to 4.16% improvement in\nterms of Area Under the Curve and 3.19% in terms of Mean Average Precision\ncompared to the best baseline method.", "published": "2018-12-31 22:05:05", "link": "http://arxiv.org/abs/1901.00056v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The meaning of \"most\" for visual question answering models", "abstract": "The correct interpretation of quantifier statements in the context of a\nvisual scene requires non-trivial inference mechanisms. For the example of\n\"most\", we discuss two strategies which rely on fundamentally different\ncognitive concepts. Our aim is to identify what strategy deep learning models\nfor visual question answering learn when trained on such questions. To this\nend, we carefully design data to replicate experiments from psycholinguistics\nwhere the same question was investigated for humans. Focusing on the FiLM\nvisual question answering model, our experiments indicate that a form of\napproximate number system emerges whose performance declines with more\ndifficult scenes as predicted by Weber's law. Moreover, we identify confounding\nfactors, like spatial arrangement of the scene, which impede the effectiveness\nof this system.", "published": "2018-12-31 09:41:04", "link": "http://arxiv.org/abs/1812.11737v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Unary and Binary Classification Approaches and their Implications for\n  Authorship Verification", "abstract": "Retrieving indexed documents, not by their topical content but their writing\nstyle opens the door for a number of applications in information retrieval\n(IR). One application is to retrieve textual content of a certain author X,\nwhere the queried IR system is provided beforehand with a set of reference\ntexts of X. Authorship verification (AV), which is a research subject in the\nfield of digital text forensics, is suitable for this purpose. The task of AV\nis to determine if two documents (i.e. an indexed and a reference document)\nhave been written by the same author X. Even though AV represents a unary\nclassification problem, a number of existing approaches consider it as a binary\nclassification task. However, the underlying classification model of an AV\nmethod has a number of serious implications regarding its prerequisites,\nevaluability, and applicability. In our comprehensive literature review, we\nobserved several misunderstandings regarding the differentiation of unary and\nbinary AV approaches that require consideration. The objective of this paper\nis, therefore, to clarify these by proposing clear criteria and new properties\nthat aim to improve the characterization of existing and future AV approaches.\nGiven both, we investigate the applicability of eleven existing unary and\nbinary AV methods as well as four generic unary classification algorithms on\ntwo self-compiled corpora. Furthermore, we highlight an important issue\nconcerning the evaluation of AV methods based on fixed decision criterions,\nwhich has not been paid attention in previous AV studies.", "published": "2018-12-31 16:04:16", "link": "http://arxiv.org/abs/1901.00399v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "Sentence-Level Sentiment Analysis of Financial News Using Distributed\n  Text Representations and Multi-Instance Learning", "abstract": "Researchers and financial professionals require robust computerized tools\nthat allow users to rapidly operationalize and assess the semantic textual\ncontent in financial news. However, existing methods commonly work at the\ndocument-level while deeper insights into the actual structure and the\nsentiment of individual sentences remain blurred. As a result, investors are\nrequired to apply the utmost attention and detailed, domain-specific knowledge\nin order to assess the information on a fine-grained basis. To facilitate this\nmanual process, this paper proposes the use of distributed text representations\nand multi-instance learning to transfer information from the document-level to\nthe sentence-level. Compared to alternative approaches, this method features\nsuperior predictive performance while preserving context and interpretability.\nOur analysis of a manually-labeled dataset yields a predictive accuracy of up\nto 69.90%, exceeding the performance of alternative approaches by at least 3.80\npercentage points. Accordingly, this study not only benefits investors with\nregard to their financial decision-making, but also helps companies to\ncommunicate their messages as intended.", "published": "2018-12-31 16:30:21", "link": "http://arxiv.org/abs/1901.00400v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "Pull out all the stops: Textual analysis via punctuation sequences", "abstract": "Whether enjoying the lucid prose of a favorite author or slogging through\nsome other writer's cumbersome, heavy-set prattle (full of parentheses, em\ndashes, compound adjectives, and Oxford commas), readers will notice stylistic\nsignatures not only in word choice and grammar, but also in punctuation itself.\nIndeed, visual sequences of punctuation from different authors produce\nmarvelously different (and visually striking) sequences. Punctuation is a\nlargely overlooked stylistic feature in \"stylometry\", the quantitative analysis\nof written text. In this paper, we examine punctuation sequences in a corpus of\nliterary documents and ask the following questions: Are the properties of such\nsequences a distinctive feature of different authors? Is it possible to\ndistinguish literary genres based on their punctuation sequences? Do the\npunctuation styles of authors evolve over time? Are we on to something\ninteresting in trying to do stylometry without words, or are we full of sound\nand fury (signifying nothing)?", "published": "2018-12-31 18:48:20", "link": "http://arxiv.org/abs/1901.00519v2", "categories": ["cs.CL", "cs.LG", "physics.soc-ph"], "primary_category": "cs.CL"}
