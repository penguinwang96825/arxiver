{"title": "Inroads to a Structured Data Natural Language Bijection and the role of\n  LLM annotation", "abstract": "This work finds limited evidence supporting the theory that using multiple\ntasks with sequence-to-sequence transformer language models can improve\nperformance on some metrics. In particular, the multi-task generalist t5-small\noutperforms the specialist t5-small with a $F_1$ of $0.771$ up from $0.692$,\nwhich may point to underlying cross-task knowledge generalization. This further\nsuggests that even with the same network, \"re-using\" the same data in a\ndifferent way may lead to higher performance in some metrics. However, the\ninverse task alone is likely only an optimization strategy, since it does not\nyield a significant general improvement at the model sizes explored in this\nwork. Also, adding $\\approx 4500$ LLM annotated records (interlaced with the\n$12800$ WebNLG training records) does not substantially change automatic metric\nperformance compared to the same t5-small model without the synthetic data.\nThis may be due to a learning capacity bottleneck on account of model size, and\ndecreases observed may be due to distributional differences in the corpora.\nFuture research using larger models or human evaluation is required to more\nfully explain the mechanisms contributing to performance on these tasks.", "published": "2024-01-14 03:16:49", "link": "http://arxiv.org/abs/2401.07190v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Domain Adaptation through Extended-Text Reading Comprehension", "abstract": "To enhance the domain-specific capabilities of large language models,\ncontinued pre-training on a domain-specific corpus is a prevalent method.\nRecent work demonstrates that adapting models using reading comprehension data\nformatted by regex-based patterns can significantly improve performance on\ndomain-specific tasks. However, regex-based patterns are incapable of parsing\nraw corpora using domain-specific knowledge. Furthermore, the question and\nanswer pairs are extracted directly from the corpus in predefined formats\noffers limited context. To address this limitation, we improve reading\ncomprehension via LLM and clustering. LLM focuses on leveraging domain\nknowledge within the corpus to refine comprehension stage, while clustering\nsupplies relevant knowledge by extending the context to enrich reading stage.\nAdditionally, our method incorporates parameter-efficient fine-tuning to\nimprove the efficiency of domain adaptation. In comparison to AdaptLLM, our\nmethod achieves an improvement exceeding 5% in domain-specific tasks. Our code\nwill available at https://github.com/microsoft/LMOps.", "published": "2024-01-14 13:11:31", "link": "http://arxiv.org/abs/2401.07284v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CANDLE: Iterative Conceptualization and Instantiation Distillation from\n  Large Language Models for Commonsense Reasoning", "abstract": "The sequential process of conceptualization and instantiation is essential to\ngeneralizable commonsense reasoning as it allows the application of existing\nknowledge to unfamiliar scenarios. However, existing works tend to undervalue\nthe step of instantiation and heavily rely on pre-built concept taxonomies and\nhuman annotations to collect both types of knowledge, resulting in a lack of\ninstantiated knowledge to complete reasoning, high cost, and limited\nscalability. To tackle these challenges, we introduce CANDLE, a distillation\nframework that iteratively performs contextualized conceptualization and\ninstantiation over commonsense knowledge bases by instructing large language\nmodels to generate both types of knowledge with critic filtering. By applying\nCANDLE to ATOMIC, we construct a comprehensive knowledge base comprising six\nmillion conceptualizations and instantiated commonsense knowledge triples. Both\ntypes of knowledge are firmly rooted in the original ATOMIC dataset, and\nintrinsic evaluations demonstrate their exceptional quality and diversity.\nEmpirical results indicate that distilling CANDLE on student models provides\nbenefits across four downstream tasks. Our code, data, and models are publicly\navailable at https://github.com/HKUST-KnowComp/CANDLE.", "published": "2024-01-14 13:24:30", "link": "http://arxiv.org/abs/2401.07286v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Harnessing Large Language Models Over Transformer Models for Detecting\n  Bengali Depressive Social Media Text: A Comprehensive Study", "abstract": "In an era where the silent struggle of underdiagnosed depression pervades\nglobally, our research delves into the crucial link between mental health and\nsocial media. This work focuses on early detection of depression, particularly\nin extroverted social media users, using LLMs such as GPT 3.5, GPT 4 and our\nproposed GPT 3.5 fine-tuned model DepGPT, as well as advanced Deep learning\nmodels(LSTM, Bi-LSTM, GRU, BiGRU) and Transformer models(BERT, BanglaBERT,\nSahajBERT, BanglaBERT-Base). The study categorized Reddit and X datasets into\n\"Depressive\" and \"Non-Depressive\" segments, translated into Bengali by native\nspeakers with expertise in mental health, resulting in the creation of the\nBengali Social Media Depressive Dataset (BSMDD). Our work provides full\narchitecture details for each model and a methodical way to assess their\nperformance in Bengali depressive text categorization using zero-shot and\nfew-shot learning techniques. Our work demonstrates the superiority of\nSahajBERT and Bi-LSTM with FastText embeddings in their respective domains also\ntackles explainability issues with transformer models and emphasizes the\neffectiveness of LLMs, especially DepGPT, demonstrating flexibility and\ncompetence in a range of learning contexts. According to the experiment\nresults, the proposed model, DepGPT, outperformed not only Alpaca Lora 7B in\nzero-shot and few-shot scenarios but also every other model, achieving a\nnear-perfect accuracy of 0.9796 and an F1-score of 0.9804, high recall, and\nexceptional precision. Although competitive, GPT-3.5 Turbo and Alpaca Lora 7B\nshow relatively poorer effectiveness in zero-shot and few-shot situations. The\nwork emphasizes the effectiveness and flexibility of LLMs in a variety of\nlinguistic circumstances, providing insightful information about the complex\nfield of depression detection models.", "published": "2024-01-14 15:15:58", "link": "http://arxiv.org/abs/2401.07310v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Afterlives of Shakespeare and Company in Online Social Readership", "abstract": "The growth of social reading platforms such as Goodreads and LibraryThing\nenables us to analyze reading activity at very large scale and in remarkable\ndetail. But twenty-first century systems give us a perspective only on\ncontemporary readers. Meanwhile, the digitization of the lending library\nrecords of Shakespeare and Company provides a window into the reading activity\nof an earlier, smaller community in interwar Paris. In this article, we explore\nthe extent to which we can make comparisons between the Shakespeare and Company\nand Goodreads communities. By quantifying similarities and differences, we can\nidentify patterns in how works have risen or fallen in popularity across these\ndatasets. We can also measure differences in how works are received by\nmeasuring similarities and differences in co-reading patterns. Finally, by\nexamining the complete networks of co-readership, we can observe changes in the\noverall structures of literary reception.", "published": "2024-01-14 18:15:06", "link": "http://arxiv.org/abs/2401.07340v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PersonalityChat: Conversation Distillation for Personalized Dialog\n  Modeling with Facts and Traits", "abstract": "The new wave of Large Language Models (LLM) has offered an efficient tool to\ncurate sizeable conversational datasets. So far studies have mainly focused on\ntask-oriented or generic open-domain dialogs, and have not fully explored the\nability of LLMs in following complicated prompts. In this work, we focus on\npersonalization, and employ LLMs to curate a dataset which is difficult and\ncostly to crowd-source: PersonalityChat is a synthetic conversational dataset\nbased upon the popular PersonaChat dataset, but conditioned on both personas\nand (Big-5) personality traits. Evaluating models fine-tuned on this dataset,\nwe show that the personality trait labels can be used for trait-based\npersonalization of generative dialogue models. We also perform a head-to-head\ncomparison between PersonalityChat and PersonaChat, and show that training on\nthe distilled dataset results in more fluent and coherent dialog agents in the\nsmall-model regime.", "published": "2024-01-14 20:35:33", "link": "http://arxiv.org/abs/2401.07363v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Active Learning for NLP with Large Language Models", "abstract": "Human annotation of training samples is expensive, laborious, and sometimes\nchallenging, especially for Natural Language Processing (NLP) tasks. To reduce\nthe labeling cost and enhance the sample efficiency, Active Learning (AL)\ntechnique can be used to label as few samples as possible to reach a reasonable\nor similar results. To reduce even more costs and with the significant advances\nof Large Language Models (LLMs), LLMs can be a good candidate to annotate\nsamples. This work investigates the accuracy and cost of using LLMs (GPT-3.5\nand GPT-4) to label samples on 3 different datasets. A consistency-based\nstrategy is proposed to select samples that are potentially incorrectly labeled\nso that human annotations can be used for those samples in AL settings, and we\ncall it mixed annotation strategy. Then we test performance of AL under two\ndifferent settings: (1) using human annotations only; (2) using the proposed\nmixed annotation strategy. The accuracy of AL models under 3 AL query\nstrategies are reported on 3 text classification datasets, i.e., AG's News,\nTREC-6, and Rotten Tomatoes. On AG's News and Rotten Tomatoes, the models\ntrained with the mixed annotation strategy achieves similar or better results\ncompared to that with human annotations. The method reveals great potentials of\nLLMs as annotators in terms of accuracy and cost efficiency in active learning\nsettings.", "published": "2024-01-14 21:00:52", "link": "http://arxiv.org/abs/2401.07367v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distilling Event Sequence Knowledge From Large Language Models", "abstract": "Event sequence models have been found to be highly effective in the analysis\nand prediction of events. Building such models requires availability of\nabundant high-quality event sequence data. In certain applications, however,\nclean structured event sequences are not available, and automated sequence\nextraction results in data that is too noisy and incomplete. In this work, we\nexplore the use of Large Language Models (LLMs) to generate event sequences\nthat can effectively be used for probabilistic event model construction. This\ncan be viewed as a mechanism of distilling event sequence knowledge from LLMs.\nOur approach relies on a Knowledge Graph (KG) of event concepts with partial\ncausal relations to guide the generative language model for causal event\nsequence generation. We show that our approach can generate high-quality event\nsequences, filling a knowledge gap in the input KG. Furthermore, we explore how\nthe generated sequences can be leveraged to discover useful and more complex\nstructured knowledge from pattern mining and probabilistic event models. We\nrelease our sequence generation code and evaluation framework, as well as\ncorpus of event sequence data.", "published": "2024-01-14 09:34:42", "link": "http://arxiv.org/abs/2401.07237v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Small Language Model Can Self-correct", "abstract": "Generative Language Models (LMs) such as ChatGPT have exhibited remarkable\nperformance across various downstream tasks. Nevertheless, one of their most\nprominent drawbacks is generating inaccurate or false information with a\nconfident tone. Previous studies have devised sophisticated pipelines and\nprompts to induce large LMs to exhibit the capability for self-correction.\nHowever, large LMs are explicitly prompted to verify and modify its answers\nseparately rather than completing all steps spontaneously like humans.\nMoreover, these complex prompts are extremely challenging for small LMs to\nfollow. In this paper, we introduce the \\underline{I}ntrinsic\n\\underline{S}elf-\\underline{C}orrection (ISC) in generative language models,\naiming to correct the initial output of LMs in a self-triggered manner, even\nfor those small LMs with 6 billion parameters. Specifically, we devise a\npipeline for constructing self-correction data and propose Partial Answer\nMasking (PAM), aiming to endow the model with the capability for intrinsic\nself-correction through fine-tuning. We conduct experiments using LMs with\nparameters sizes ranging from 6 billion to 13 billion in two tasks, including\ncommonsense reasoning and factual knowledge reasoning. Our experiments\ndemonstrate that the outputs generated using ISC outperform those generated\nwithout self-correction. We believe that the output quality of even small LMs\ncan be further improved by empowering them with the ability to intrinsic\nself-correct.", "published": "2024-01-14 14:29:07", "link": "http://arxiv.org/abs/2401.07301v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Small LLMs Are Weak Tool Learners: A Multi-LLM Agent", "abstract": "Large Language Model (LLM) agents significantly extend the capabilities of\nstandalone LLMs, empowering them to interact with external tools (e.g., APIs,\nfunctions) and complete various tasks in a self-directed fashion. The challenge\nof tool use demands that LLMs not only understand user queries and generate\nanswers accurately but also excel in task planning, tool invocation, and result\nsummarization. While traditional works focus on training a single LLM with all\nthese capabilities, performance limitations become apparent, particularly with\nsmaller models. To overcome these challenges, we propose a novel approach that\ndecomposes the aforementioned capabilities into a planner, caller, and\nsummarizer. Each component is implemented by a single LLM that focuses on a\nspecific capability and collaborates with others to accomplish the task. This\nmodular framework facilitates individual updates and the potential use of\nsmaller LLMs for building each capability. To effectively train this framework,\nwe introduce a two-stage training paradigm. First, we fine-tune a backbone LLM\non the entire dataset without discriminating sub-tasks, providing the model\nwith a comprehensive understanding of the task. Second, the fine-tuned LLM is\nused to instantiate the planner, caller, and summarizer respectively, which are\ncontinually fine-tuned on respective sub-tasks. Evaluation across various\ntool-use benchmarks illustrates that our proposed multi-LLM framework surpasses\nthe traditional single-LLM approach, highlighting its efficacy and advantages\nin tool learning.", "published": "2024-01-14 16:17:07", "link": "http://arxiv.org/abs/2401.07324v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Beyond Sparse Rewards: Enhancing Reinforcement Learning with Language\n  Model Critique in Text Generation", "abstract": "Reinforcement learning (RL) can align language models with non-differentiable\nreward signals, such as human preferences. However, a major challenge arises\nfrom the sparsity of these reward signals - typically, there is only a single\nreward for an entire output. This sparsity of rewards can lead to inefficient\nand unstable learning. To address this challenge, our paper introduces an novel\nframework that utilizes the critique capability of Large Language Models (LLMs)\nto produce intermediate-step rewards during RL training. Our method involves\ncoupling a policy model with a critic language model, which is responsible for\nproviding comprehensive feedback of each part of the output. This feedback is\nthen translated into token or span-level rewards that can be used to guide the\nRL training process. We investigate this approach under two different settings:\none where the policy model is smaller and is paired with a more powerful critic\nmodel, and another where a single language model fulfills both roles. We assess\nour approach on three text generation tasks: sentiment control, language model\ndetoxification, and summarization. Experimental results show that incorporating\nartificial intrinsic rewards significantly improve both sample efficiency and\nthe overall performance of the policy model, supported by both automatic and\nhuman evaluation.", "published": "2024-01-14 22:05:11", "link": "http://arxiv.org/abs/2401.07382v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Forecasting GDP in Europe with Textual Data", "abstract": "We evaluate the informational content of news-based sentiment indicators for\nforecasting Gross Domestic Product (GDP) and other macroeconomic variables of\nthe five major European economies. Our data set includes over 27 million\narticles for 26 major newspapers in 5 different languages. The evidence\nindicates that these sentiment indicators are significant predictors to\nforecast macroeconomic variables and their predictive content is robust to\ncontrolling for other indicators available to forecasters in real-time.", "published": "2024-01-14 00:33:30", "link": "http://arxiv.org/abs/2401.07179v1", "categories": ["cs.CE", "cs.AI", "cs.CL", "91B62, 91B84, 91B86"], "primary_category": "cs.CE"}
{"title": "ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided\n  Sequence Reordering", "abstract": "The language model (LM) approach based on acoustic and linguistic prompts,\nsuch as VALL-E, has achieved remarkable progress in the field of zero-shot\naudio generation. However, existing methods still have some limitations: 1)\nrepetitions, transpositions, and omissions in the output synthesized speech due\nto limited alignment constraints between audio and phoneme tokens; 2)\nchallenges of fine-grained control over the synthesized speech with\nautoregressive (AR) language model; 3) infinite silence generation due to the\nnature of AR-based decoding, especially under the greedy strategy. To alleviate\nthese issues, we propose ELLA-V, a simple but efficient LM-based zero-shot\ntext-to-speech (TTS) framework, which enables fine-grained control over\nsynthesized audio at the phoneme level. The key to ELLA-V is interleaving\nsequences of acoustic and phoneme tokens, where phoneme tokens appear ahead of\nthe corresponding acoustic tokens. The experimental findings reveal that our\nmodel outperforms VALL-E in terms of accuracy and delivers more stable results\nusing both greedy and sampling-based decoding strategies. The code of ELLA-V\nwill be open-sourced after cleanups. Audio samples are available at\nhttps://ereboas.github.io/ELLAV/.", "published": "2024-01-14 17:43:55", "link": "http://arxiv.org/abs/2401.07333v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Promptformer: Prompted Conformer Transducer for ASR", "abstract": "Context cues carry information which can improve multi-turn interactions in\nautomatic speech recognition (ASR) systems. In this paper, we introduce a novel\nmechanism inspired by hyper-prompting to fuse textual context with acoustic\nrepresentations in the attention mechanism. Results on a test set with\nmulti-turn interactions show that our method achieves 5.9% relative word error\nrate reduction (rWERR) over a strong baseline. We show that our method does not\ndegrade in the absence of context and leads to improvements even if the model\nis trained without context. We further show that leveraging a pre-trained\nsentence-piece model for context embedding generation can outperform an\nexternal BERT model.", "published": "2024-01-14 20:14:35", "link": "http://arxiv.org/abs/2401.07360v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Who Said What? An Automated Approach to Analyzing Speech in Preschool\n  Classrooms", "abstract": "Young children spend substantial portions of their waking hours in noisy\npreschool classrooms. In these environments, children's vocal interactions with\nteachers are critical contributors to their language outcomes, but manually\ntranscribing these interactions is prohibitive. Using audio from child- and\nteacher-worn recorders, we propose an automated framework that uses open source\nsoftware both to classify speakers (ALICE) and to transcribe their utterances\n(Whisper). We compare results from our framework to those from a human expert\nfor 110 minutes of classroom recordings, including 85 minutes from child-word\nmicrophones (n=4 children) and 25 minutes from teacher-worn microphones (n=2\nteachers). The overall proportion of agreement, that is, the proportion of\ncorrectly classified teacher and child utterances, was .76, with an\nerror-corrected kappa of .50 and a weighted F1 of .76. The word error rate for\nboth teacher and child transcriptions was .15, meaning that 15% of words would\nneed to be deleted, added, or changed to equate the Whisper and expert\ntranscriptions. Moreover, speech features such as the mean length of utterances\nin words, the proportion of teacher and child utterances that were questions,\nand the proportion of utterances that were responded to within 2.5 seconds were\nsimilar when calculated separately from expert and automated transcriptions.\nThe results suggest substantial progress in analyzing classroom speech that may\nsupport children's language development. Future research using natural language\nprocessing is under way to improve speaker classification and to analyze\nresults from the application of the automated framework to a larger dataset\ncontaining classroom recordings from 13 children and 3 teachers observed on 17\noccasions over one year.", "published": "2024-01-14 18:27:37", "link": "http://arxiv.org/abs/2401.07342v3", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Construction and Evaluation of Mandarin Multimodal Emotional Speech\n  Database", "abstract": "A multi-modal emotional speech Mandarin database including articulatory\nkinematics, acoustics, glottal and facial micro-expressions is designed and\nestablished, which is described in detail from the aspects of corpus design,\nsubject selection, recording details and data processing. Where signals are\nlabeled with discrete emotion labels (neutral, happy, pleasant, indifferent,\nangry, sad, grief) and dimensional emotion labels (pleasure, arousal,\ndominance). In this paper, the validity of dimension annotation is verified by\nstatistical analysis of dimension annotation data. The SCL-90 scale data of\nannotators are verified and combined with PAD annotation data for analysis, so\nas to explore the internal relationship between the outlier phenomenon in\nannotation and the psychological state of annotators. In order to verify the\nspeech quality and emotion discrimination of the database, this paper uses 3\nbasic models of SVM, CNN and DNN to calculate the recognition rate of these\nseven emotions. The results show that the average recognition rate of seven\nemotions is about 82% when using acoustic data alone. When using glottal data\nalone, the average recognition rate is about 72%. Using kinematics data alone,\nthe average recognition rate also reaches 55.7%. Therefore, the database is of\nhigh quality and can be used as an important source for speech analysis\nresearch, especially for the task of multimodal emotional speech analysis.", "published": "2024-01-14 17:56:36", "link": "http://arxiv.org/abs/2401.07336v1", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
