{"title": "\"where is this relationship going?\": Understanding Relationship\n  Trajectories in Narrative Text", "abstract": "We examine a new commonsense reasoning task: given a narrative describing a\nsocial interaction that centers on two protagonists, systems make inferences\nabout the underlying relationship trajectory. Specifically, we propose two\nevaluation tasks: Relationship Outlook Prediction MCQ and Resolution Prediction\nMCQ. In Relationship Outlook Prediction, a system maps an interaction to a\nrelationship outlook that captures how the interaction is expected to change\nthe relationship. In Resolution Prediction, a system attributes a given\nrelationship outlook to a particular resolution that explains the outcome.\nThese two tasks parallel two real-life questions that people frequently ponder\nupon as they navigate different social situations: \"where is this relationship\ngoing?\" and \"how did we end up here?\". To facilitate the investigation of human\nsocial relationships through these two tasks, we construct a new dataset,\nSocial Narrative Tree, which consists of 1250 stories documenting a variety of\ndaily social interactions. The narratives encode a multitude of social elements\nthat interweave to give rise to rich commonsense knowledge of how relationships\nevolve with respect to social interactions. We establish baseline performances\nusing language models and the accuracies are significantly lower than human\nperformance. The results demonstrate that models need to look beyond syntactic\nand semantic signals to comprehend complex human relationships.", "published": "2020-10-29 02:07:05", "link": "http://arxiv.org/abs/2010.15313v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combining Self-Training and Self-Supervised Learning for Unsupervised\n  Disfluency Detection", "abstract": "Most existing approaches to disfluency detection heavily rely on\nhuman-annotated corpora, which is expensive to obtain in practice. There have\nbeen several proposals to alleviate this issue with, for instance,\nself-supervised learning techniques, but they still require human-annotated\ncorpora. In this work, we explore the unsupervised learning paradigm which can\npotentially work with unlabeled text corpora that are cheaper and easier to\nobtain. Our model builds upon the recent work on Noisy Student Training, a\nsemi-supervised learning approach that extends the idea of self-training.\nExperimental results on the commonly used English Switchboard test set show\nthat our approach achieves competitive performance compared to the previous\nstate-of-the-art supervised systems using contextualized word embeddings (e.g.\nBERT and ELECTRA).", "published": "2020-10-29 05:29:26", "link": "http://arxiv.org/abs/2010.15360v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conversation Graph: Data Augmentation, Training and Evaluation for\n  Non-Deterministic Dialogue Management", "abstract": "Task-oriented dialogue systems typically rely on large amounts of\nhigh-quality training data or require complex handcrafted rules. However,\nexisting datasets are often limited in size considering the complexity of the\ndialogues. Additionally, conventional training signal inference is not suitable\nfor non-deterministic agent behaviour, i.e. considering multiple actions as\nvalid in identical dialogue states. We propose the Conversation Graph\n(ConvGraph), a graph-based representation of dialogues that can be exploited\nfor data augmentation, multi-reference training and evaluation of\nnon-deterministic agents. ConvGraph generates novel dialogue paths to augment\ndata volume and diversity. Intrinsic and extrinsic evaluation across three\ndatasets shows that data augmentation and/or multi-reference training with\nConvGraph can improve dialogue success rates by up to 6.4%.", "published": "2020-10-29 08:23:24", "link": "http://arxiv.org/abs/2010.15411v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tilde at WMT 2020: News Task Systems", "abstract": "This paper describes Tilde's submission to the WMT2020 shared task on news\ntranslation for both directions of the English-Polish language pair in both the\nconstrained and the unconstrained tracks. We follow our submissions from the\nprevious years and build our baseline systems to be morphologically motivated\nsub-word unit-based Transformer base models that we train using the Marian\nmachine translation toolkit. Additionally, we experiment with different\nparallel and monolingual data selection schemes, as well as sampled\nback-translation. Our final models are ensembles of Transformer base and\nTransformer big models that feature right-to-left re-ranking.", "published": "2020-10-29 08:59:37", "link": "http://arxiv.org/abs/2010.15423v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Memory Attentive Fusion: External Language Model Integration for\n  Transformer-based Sequence-to-Sequence Model", "abstract": "This paper presents a novel fusion method for integrating an external\nlanguage model (LM) into the Transformer based sequence-to-sequence (seq2seq)\nmodel. While paired data are basically required to train the seq2seq model, the\nexternal LM can be trained with only unpaired data. Thus, it is important to\nleverage memorized knowledge in the external LM for building the seq2seq model,\nsince it is hard to prepare a large amount of paired data. However, the\nexisting fusion methods assume that the LM is integrated with recurrent neural\nnetwork-based seq2seq models instead of the Transformer. Therefore, this paper\nproposes a fusion method that can explicitly utilize network structures in the\nTransformer. The proposed method, called {\\bf memory attentive fusion},\nleverages the Transformer-style attention mechanism that repeats source-target\nattention in a multi-hop manner for reading the memorized knowledge in the LM.\nOur experiments on two text-style conversion tasks demonstrate that the\nproposed method performs better than conventional fusion methods.", "published": "2020-10-29 09:16:23", "link": "http://arxiv.org/abs/2010.15437v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Named Entity Recognition for Social Media Texts with Semantic\n  Augmentation", "abstract": "Existing approaches for named entity recognition suffer from data sparsity\nproblems when conducted on short and informal texts, especially user-generated\nsocial media content. Semantic augmentation is a potential way to alleviate\nthis problem. Given that rich semantic information is implicitly preserved in\npre-trained word embeddings, they are potential ideal resources for semantic\naugmentation. In this paper, we propose a neural-based approach to NER for\nsocial media texts where both local (from running text) and augmented semantics\nare taken into account. In particular, we obtain the augmented semantic\ninformation from a large-scale corpus, and propose an attentive semantic\naugmentation module and a gate module to encode and aggregate such information,\nrespectively. Extensive experiments are performed on three benchmark datasets\ncollected from English and Chinese social media platforms, where the results\ndemonstrate the superiority of our approach to previous studies across all\nthree datasets.", "published": "2020-10-29 10:06:46", "link": "http://arxiv.org/abs/2010.15458v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Named Entity Recognition with Attentive Ensemble of Syntactic\n  Information", "abstract": "Named entity recognition (NER) is highly sensitive to sentential syntactic\nand semantic properties where entities may be extracted according to how they\nare used and placed in the running text. To model such properties, one could\nrely on existing resources to providing helpful knowledge to the NER task; some\nexisting studies proved the effectiveness of doing so, and yet are limited in\nappropriately leveraging the knowledge such as distinguishing the important\nones for particular context. In this paper, we improve NER by leveraging\ndifferent types of syntactic information through attentive ensemble, which\nfunctionalizes by the proposed key-value memory networks, syntax attention, and\nthe gate mechanism for encoding, weighting and aggregating such syntactic\ninformation, respectively. Experimental results on six English and Chinese\nbenchmark datasets suggest the effectiveness of the proposed model and show\nthat it outperforms previous studies on all experiment datasets.", "published": "2020-10-29 10:25:17", "link": "http://arxiv.org/abs/2010.15466v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unbabel's Participation in the WMT20 Metrics Shared Task", "abstract": "We present the contribution of the Unbabel team to the WMT 2020 Shared Task\non Metrics. We intend to participate on the segment-level, document-level and\nsystem-level tracks on all language pairs, as well as the 'QE as a Metric'\ntrack. Accordingly, we illustrate results of our models in these tracks with\nreference to test sets from the previous year. Our submissions build upon the\nrecently proposed COMET framework: We train several estimator models to regress\non different human-generated quality scores and a novel ranking model trained\non relative ranks obtained from Direct Assessments. We also propose a simple\ntechnique for converting segment-level predictions into a document-level score.\nOverall, our systems achieve strong results for all language pairs on previous\ntest sets and in many cases set a new state-of-the-art.", "published": "2020-10-29 12:59:44", "link": "http://arxiv.org/abs/2010.15535v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "May I Ask Who's Calling? Named Entity Recognition on Call Center\n  Transcripts for Privacy Law Compliance", "abstract": "We investigate using Named Entity Recognition on a new type of user-generated\ntext: a call center conversation. These conversations combine problems from\nspontaneous speech with problems novel to conversational Automated Speech\nRecognition, including incorrect recognition, alongside other common problems\nfrom noisy user-generated text. Using our own corpus with new annotations,\ntraining custom contextual string embeddings, and applying a BiLSTM-CRF, we\nmatch state-of-the-art results on our novel task.", "published": "2020-10-29 13:53:42", "link": "http://arxiv.org/abs/2010.15598v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextual BERT: Conditioning the Language Model Using a Global State", "abstract": "BERT is a popular language model whose main pre-training task is to fill in\nthe blank, i.e., predicting a word that was masked out of a sentence, based on\nthe remaining words. In some applications, however, having an additional\ncontext can help the model make the right prediction, e.g., by taking the\ndomain or the time of writing into account. This motivates us to advance the\nBERT architecture by adding a global state for conditioning on a fixed-sized\ncontext. We present our two novel approaches and apply them to an industry\nuse-case, where we complete fashion outfits with missing articles, conditioned\non a specific customer. An experimental comparison to other methods from the\nliterature shows that our methods improve personalization significantly.", "published": "2020-10-29 17:25:20", "link": "http://arxiv.org/abs/2010.15778v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning as Abduction: Trainable Natural Logic Theorem Prover for\n  Natural Language Inference", "abstract": "Tackling Natural Language Inference with a logic-based method is becoming\nless and less common. While this might have been counterintuitive several\ndecades ago, nowadays it seems pretty obvious. The main reasons for such a\nconception are that (a) logic-based methods are usually brittle when it comes\nto processing wide-coverage texts, and (b) instead of automatically learning\nfrom data, they require much of manual effort for development. We make a step\ntowards to overcome such shortcomings by modeling learning from data as\nabduction: reversing a theorem-proving procedure to abduce semantic relations\nthat serve as the best explanation for the gold label of an inference problem.\nIn other words, instead of proving sentence-level inference relations with the\nhelp of lexical relations, the lexical relations are proved taking into account\nthe sentence-level inference relations. We implement the learning method in a\ntableau theorem prover for natural language and show that it improves the\nperformance of the theorem prover on the SICK dataset by 1.4% while still\nmaintaining high precision (>94%). The obtained results are competitive with\nthe state of the art among logic-based systems.", "published": "2020-10-29 19:49:17", "link": "http://arxiv.org/abs/2010.15909v2", "categories": ["cs.CL", "03B65, 68T50", "F.4.1; I.2.3; K.3.2; I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "Multiple Sclerosis Severity Classification From Clinical Text", "abstract": "Multiple Sclerosis (MS) is a chronic, inflammatory and degenerative\nneurological disease, which is monitored by a specialist using the Expanded\nDisability Status Scale (EDSS) and recorded in unstructured text in the form of\na neurology consult note. An EDSS measurement contains an overall \"EDSS\" score\nand several functional subscores. Typically, expert knowledge is required to\ninterpret consult notes and generate these scores. Previous approaches used\nlimited context length Word2Vec embeddings and keyword searches to predict\nscores given a consult note, but often failed when scores were not explicitly\nstated. In this work, we present MS-BERT, the first publicly available\ntransformer model trained on real clinical data other than MIMIC. Next, we\npresent MSBC, a classifier that applies MS-BERT to generate embeddings and\npredict EDSS and functional subscores. Lastly, we explore combining MSBC with\nother models through the use of Snorkel to generate scores for unlabelled\nconsult notes. MSBC achieves state-of-the-art performance on all metrics and\nprediction tasks and outperforms the models generated from the Snorkel\nensemble. We improve Macro-F1 by 0.12 (to 0.88) for predicting EDSS and on\naverage by 0.29 (to 0.63) for predicting functional subscores over previous\nWord2Vec CNN and rule-based approaches.", "published": "2020-10-29 02:15:23", "link": "http://arxiv.org/abs/2010.15316v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Explainable Automated Coding of Clinical Notes using Hierarchical\n  Label-wise Attention Networks and Label Embedding Initialisation", "abstract": "Diagnostic or procedural coding of clinical notes aims to derive a coded\nsummary of disease-related information about patients. Such coding is usually\ndone manually in hospitals but could potentially be automated to improve the\nefficiency and accuracy of medical coding. Recent studies on deep learning for\nautomated medical coding achieved promising performances. However, the\nexplainability of these models is usually poor, preventing them to be used\nconfidently in supporting clinical practice. Another limitation is that these\nmodels mostly assume independence among labels, ignoring the complex\ncorrelation among medical codes which can potentially be exploited to improve\nthe performance. We propose a Hierarchical Label-wise Attention Network (HLAN),\nwhich aimed to interpret the model by quantifying importance (as attention\nweights) of words and sentences related to each of the labels. Secondly, we\npropose to enhance the major deep learning models with a label embedding (LE)\ninitialisation approach, which learns a dense, continuous vector representation\nand then injects the representation into the final layers and the label-wise\nattention layers in the models. We evaluated the methods using three settings\non the MIMIC-III discharge summaries: full codes, top-50 codes, and the UK NHS\nCOVID-19 shielding codes. Experiments were conducted to compare HLAN and LE\ninitialisation to the state-of-the-art neural network based methods. HLAN\nachieved the best Micro-level AUC and $F_1$ on the top-50 code prediction and\ncomparable results on the NHS COVID-19 shielding code prediction to other\nmodels. By highlighting the most salient words and sentences for each label,\nHLAN showed more meaningful and comprehensive model interpretation compared to\nits downgraded baselines and the CNN-based models. LE initialisation\nconsistently boosted most deep learning models for automated medical coding.", "published": "2020-10-29 16:21:26", "link": "http://arxiv.org/abs/2010.15728v4", "categories": ["cs.CL", "cs.LG", "68T07 (Primary), 68T50 (Secondary)", "I.2.7; J.3"], "primary_category": "cs.CL"}
{"title": "Retrieve, Program, Repeat: Complex Knowledge Base Question Answering via\n  Alternate Meta-learning", "abstract": "A compelling approach to complex question answering is to convert the\nquestion to a sequence of actions, which can then be executed on the knowledge\nbase to yield the answer, aka the programmer-interpreter approach. Use similar\ntraining questions to the test question, meta-learning enables the programmer\nto adapt to unseen questions to tackle potential distributional biases quickly.\nHowever, this comes at the cost of manually labeling similar questions to learn\na retrieval model, which is tedious and expensive. In this paper, we present a\nnovel method that automatically learns a retrieval model alternately with the\nprogrammer from weak supervision, i.e., the system's performance with respect\nto the produced answers. To the best of our knowledge, this is the first\nattempt to train the retrieval model with the programmer jointly. Our system\nleads to state-of-the-art performance on a large-scale task for complex\nquestion answering over knowledge bases. We have released our code at\nhttps://github.com/DevinJake/MARL.", "published": "2020-10-29 18:28:16", "link": "http://arxiv.org/abs/2010.15875v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Few-Shot Complex Knowledge Base Question Answering via Meta\n  Reinforcement Learning", "abstract": "Complex question-answering (CQA) involves answering complex natural-language\nquestions on a knowledge base (KB). However, the conventional neural program\ninduction (NPI) approach exhibits uneven performance when the questions have\ndifferent types, harboring inherently different characteristics, e.g.,\ndifficulty level. This paper proposes a meta-reinforcement learning approach to\nprogram induction in CQA to tackle the potential distributional bias in\nquestions. Our method quickly and effectively adapts the meta-learned\nprogrammer to new questions based on the most similar questions retrieved from\nthe training data. The meta-learned policy is then used to learn a good\nprogramming policy, utilizing the trial trajectories and their rewards for\nsimilar questions in the support set. Our method achieves state-of-the-art\nperformance on the CQA dataset (Saha et al., 2018) while using only five trial\ntrajectories for the top-5 retrieved questions in each support set, and\nmetatraining on tasks constructed from only 1% of the training set. We have\nreleased our code at https://github.com/DevinJake/MRL-CQA.", "published": "2020-10-29 18:34:55", "link": "http://arxiv.org/abs/2010.15877v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Less is More: Data-Efficient Complex Question Answering over Knowledge\n  Bases", "abstract": "Question answering is an effective method for obtaining information from\nknowledge bases (KB). In this paper, we propose the Neural-Symbolic Complex\nQuestion Answering (NS-CQA) model, a data-efficient reinforcement learning\nframework for complex question answering by using only a modest number of\ntraining samples. Our framework consists of a neural generator and a symbolic\nexecutor that, respectively, transforms a natural-language question into a\nsequence of primitive actions, and executes them over the knowledge base to\ncompute the answer. We carefully formulate a set of primitive symbolic actions\nthat allows us to not only simplify our neural network design but also\naccelerate model convergence. To reduce search space, we employ the copy and\nmasking mechanisms in our encoder-decoder architecture to drastically reduce\nthe decoder output vocabulary and improve model generalizability. We equip our\nmodel with a memory buffer that stores high-reward promising programs. Besides,\nwe propose an adaptive reward function. By comparing the generated trial with\nthe trials stored in the memory buffer, we derive the curriculum-guided reward\nbonus, i.e., the proximity and the novelty. To mitigate the sparse reward\nproblem, we combine the adaptive reward and the reward bonus, reshaping the\nsparse reward into dense feedback. Also, we encourage the model to generate new\ntrials to avoid imitating the spurious trials while making the model remember\nthe past high-reward trials to improve data efficiency. Our NS-CQA model is\nevaluated on two datasets: CQA, a recent large-scale complex question answering\ndataset, and WebQuestionsSP, a multi-hop question answering dataset. On both\ndatasets, our model outperforms the state-of-the-art models. Notably, on CQA,\nNS-CQA performs well on questions with higher complexity, while only using\napproximately 1% of the total training samples.", "published": "2020-10-29 18:42:44", "link": "http://arxiv.org/abs/2010.15881v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Systolic Computing on GPUs for Productive Performance", "abstract": "We propose a language and compiler to productively build high-performance\n{\\it software systolic arrays} that run on GPUs. Based on a rigorous\nmathematical foundation (uniform recurrence equations and space-time\ntransform), our language has a high abstraction level and covers a wide range\nof applications. A programmer {\\it specifies} a projection of a dataflow\ncompute onto a linear systolic array, while leaving the detailed implementation\nof the projection to a compiler; the compiler implements the specified\nprojection and maps the linear systolic array to the SIMD execution units and\nvector registers of GPUs. In this way, both productivity and performance are\nachieved in the same time. This approach neatly combines loop transformations,\ndata shuffling, and vector register allocation into a single framework.\nMeanwhile, many other optimizations can be applied as well; the compiler\ncomposes the optimizations together to generate efficient code.\n  We implemented the approach on Intel GPUs. This is the first system that\nallows productive construction of systolic arrays on GPUs. We allow multiple\nprojections, arbitrary projection directions and linear schedules, which can\nexpress most, if not all, systolic arrays in practice. Experiments with 1- and\n2-D convolution on an Intel GEN9.5 GPU have demonstrated the generality of the\napproach, and its productivity in expressing various systolic designs for\nfinding the best candidate. Although our systolic arrays are purely software\nrunning on generic SIMD hardware, compared with the GPU's specialized, hardware\nsamplers that perform the same convolutions, some of our best designs are up to\n59\\% faster. Overall, this approach holds promise for productive\nhigh-performance computing on GPUs.", "published": "2020-10-29 18:49:54", "link": "http://arxiv.org/abs/2010.15884v1", "categories": ["cs.PL", "cs.CL"], "primary_category": "cs.PL"}
{"title": "RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark", "abstract": "In this paper, we introduce an advanced Russian general language\nunderstanding evaluation benchmark -- RussianGLUE. Recent advances in the field\nof universal language models and transformers require the development of a\nmethodology for their broad diagnostics and testing for general intellectual\nskills - detection of natural language inference, commonsense reasoning,\nability to perform simple logical operations regardless of text subject or\nlexicon. For the first time, a benchmark of nine tasks, collected and organized\nanalogically to the SuperGLUE methodology, was developed from scratch for the\nRussian language. We provide baselines, human level evaluation, an open-source\nframework for evaluating models\n(https://github.com/RussianNLP/RussianSuperGLUE), and an overall leaderboard of\ntransformer models for the Russian language. Besides, we present the first\nresults of comparing multilingual models in the adapted diagnostic test set and\noffer the first steps to further expanding or assessing state-of-the-art models\nindependently of language.", "published": "2020-10-29 20:31:39", "link": "http://arxiv.org/abs/2010.15925v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RuREBus: a Case Study of Joint Named Entity Recognition and Relation\n  Extraction from e-Government Domain", "abstract": "We show-case an application of information extraction methods, such as named\nentity recognition (NER) and relation extraction (RE) to a novel corpus,\nconsisting of documents, issued by a state agency. The main challenges of this\ncorpus are: 1) the annotation scheme differs greatly from the one used for the\ngeneral domain corpora, and 2) the documents are written in a language other\nthan English. Unlike expectations, the state-of-the-art transformer-based\nmodels show modest performance for both tasks, either when approached\nsequentially, or in an end-to-end fashion. Our experiments have demonstrated\nthat fine-tuning on a large unlabeled corpora does not automatically yield\nsignificant improvement and thus we may conclude that more sophisticated\nstrategies of leveraging unlabelled texts are demanded. In this paper, we\ndescribe the whole developed pipeline, starting from text annotation, baseline\ndevelopment, and designing a shared task in hopes of improving the baseline.\nEventually, we realize that the current NER and RE technologies are far from\nbeing mature and do not overcome so far challenges like ours.", "published": "2020-10-29 20:56:15", "link": "http://arxiv.org/abs/2010.15939v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically\n  Generated Prompts", "abstract": "The remarkable success of pretrained language models has motivated the study\nof what kinds of knowledge these models learn during pretraining. Reformulating\ntasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach\nfor gauging such knowledge, however, its usage is limited by the manual effort\nand guesswork required to write suitable prompts. To address this, we develop\nAutoPrompt, an automated method to create prompts for a diverse set of tasks,\nbased on a gradient-guided search. Using AutoPrompt, we show that masked\nlanguage models (MLMs) have an inherent capability to perform sentiment\nanalysis and natural language inference without additional parameters or\nfinetuning, sometimes achieving performance on par with recent state-of-the-art\nsupervised models. We also show that our prompts elicit more accurate factual\nknowledge from MLMs than the manually created prompts on the LAMA benchmark,\nand that MLMs can be used as relation extractors more effectively than\nsupervised relation extraction models. These results demonstrate that\nautomatically generated prompts are a viable parameter-free alternative to\nexisting probing methods, and as pretrained LMs become more sophisticated and\ncapable, potentially a replacement for finetuning.", "published": "2020-10-29 22:54:00", "link": "http://arxiv.org/abs/2010.15980v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Uncovering Latent Biases in Text: Method and Application to Peer Review", "abstract": "Quantifying systematic disparities in numerical quantities such as employment\nrates and wages between population subgroups provides compelling evidence for\nthe existence of societal biases. However, biases in the text written for\nmembers of different subgroups (such as in recommendation letters for male and\nnon-male candidates), though widely reported anecdotally, remain challenging to\nquantify. In this work, we introduce a novel framework to quantify bias in text\ncaused by the visibility of subgroup membership indicators. We develop a\nnonparametric estimation and inference procedure to estimate this bias. We then\nformalize an identification strategy to causally link the estimated bias to the\nvisibility of subgroup membership indicators, provided observations from time\nperiods both before and after an identity-hiding policy change. We identify an\napplication wherein \"ground truth\" bias can be inferred to evaluate our\nframework, instead of relying on synthetic or secondary data. Specifically, we\napply our framework to quantify biases in the text of peer reviews from a\nreputed machine learning conference before and after the conference adopted a\ndouble-blind reviewing policy. We show evidence of biases in the review ratings\nthat serves as \"ground truth\", and show that our proposed framework accurately\ndetects these biases from the review text without having access to the review\nratings.", "published": "2020-10-29 01:24:19", "link": "http://arxiv.org/abs/2010.15300v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Stabilizing Label Assignment for Speech Separation by Self-supervised\n  Pre-training", "abstract": "Speech separation has been well developed, with the very successful\npermutation invariant training (PIT) approach, although the frequent label\nassignment switching happening during PIT training remains to be a problem when\nbetter convergence speed and achievable performance are desired. In this paper,\nwe propose to perform self-supervised pre-training to stabilize the label\nassignment in training the speech separation model. Experiments over several\ntypes of self-supervised approaches, several typical speech separation models\nand two different datasets showed that very good improvements are achievable if\na proper self-supervised approach is chosen.", "published": "2020-10-29 06:07:01", "link": "http://arxiv.org/abs/2010.15366v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Semi-Supervised Speech Recognition via Graph-based Temporal\n  Classification", "abstract": "Semi-supervised learning has demonstrated promising results in automatic\nspeech recognition (ASR) by self-training using a seed ASR model with\npseudo-labels generated for unlabeled data. The effectiveness of this approach\nlargely relies on the pseudo-label accuracy, for which typically only the\n1-best ASR hypothesis is used. However, alternative ASR hypotheses of an N-best\nlist can provide more accurate labels for an unlabeled speech utterance and\nalso reflect uncertainties of the seed ASR model. In this paper, we propose a\ngeneralized form of the connectionist temporal classification (CTC) objective\nthat accepts a graph representation of the training labels. The newly proposed\ngraph-based temporal classification (GTC) objective is applied for\nself-training with WFST-based supervision, which is generated from an N-best\nlist of pseudo-labels. In this setup, GTC is used to learn not only a temporal\nalignment, similarly to CTC, but also a label alignment to obtain the optimal\npseudo-label sequence from the weighted graph. Results show that this approach\ncan effectively exploit an N-best list of pseudo-labels with associated scores,\nconsiderably outperforming standard pseudo-labeling, with ASR results\napproaching an oracle experiment in which the best hypotheses of the N-best\nlists are selected manually.", "published": "2020-10-29 14:56:56", "link": "http://arxiv.org/abs/2010.15653v2", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "How Many Pages? Paper Length Prediction from the Metadata", "abstract": "Being able to predict the length of a scientific paper may be helpful in\nnumerous situations. This work defines the paper length prediction task as a\nregression problem and reports several experimental results using popular\nmachine learning models. We also create a huge dataset of publication metadata\nand the respective lengths in number of pages. The dataset will be freely\navailable and is intended to foster research in this domain. As future work, we\nwould like to explore more advanced regressors based on neural networks and big\npretrained language models.", "published": "2020-10-29 20:28:24", "link": "http://arxiv.org/abs/2010.15924v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "T-vectors: Weakly Supervised Speaker Identification Using Hierarchical\n  Transformer Model", "abstract": "Identifying multiple speakers without knowing where a speaker's voice is in a\nrecording is a challenging task. This paper proposes a hierarchical network\nwith transformer encoders and memory mechanism to address this problem. The\nproposed model contains a frame-level encoder and segment-level encoder, both\nof them make use of the transformer encoder block. The multi-head attention\nmechanism in the transformer structure could better capture different speaker\nproperties when the input utterance contains multiple speakers. The memory\nmechanism used in the frame-level encoders can build a recurrent connection\nthat better capture long-term speaker features. The experiments are conducted\non artificial datasets based on the Switchboard Cellular part1 (SWBC) and\nVoxceleb1 datasets. In different data construction scenarios (Concat and\nOverlap), the proposed model shows better performance comparaing with four\nstrong baselines, reaching 13.3% and 10.5% relative improvement compared with\nH-vectors and S-vectors. The use of memory mechanism could reach 10.6% and 7.7%\nrelative improvement compared with not using memory mechanism.", "published": "2020-10-29 09:38:17", "link": "http://arxiv.org/abs/2010.16071v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Robust Raw Waveform Speech Recognition Using Relevance Weighted\n  Representations", "abstract": "Speech recognition in noisy and channel distorted scenarios is often\nchallenging as the current acoustic modeling schemes are not adaptive to the\nchanges in the signal distribution in the presence of noise. In this work, we\ndevelop a novel acoustic modeling framework for noise robust speech recognition\nbased on relevance weighting mechanism. The relevance weighting is achieved\nusing a sub-network approach that performs feature selection. A relevance\nsub-network is applied on the output of first layer of a convolutional network\nmodel operating on raw speech signals while a second relevance sub-network is\napplied on the second convolutional layer output. The relevance weights for the\nfirst layer correspond to an acoustic filterbank selection while the relevance\nweights in the second layer perform modulation filter selection. The model is\ntrained for a speech recognition task on noisy and reverberant speech. The\nspeech recognition experiments on multiple datasets (Aurora-4, CHiME-3, VOiCES)\nreveal that the incorporation of relevance weighting in the neural network\narchitecture improves the speech recognition word error rates significantly\n(average relative improvements of 10% over the baseline systems)", "published": "2020-10-29 19:32:50", "link": "http://arxiv.org/abs/2011.00721v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Interpretable Representation Learning for Speech and Audio Signals Based\n  on Relevance Weighting", "abstract": "The learning of interpretable representations from raw data presents\nsignificant challenges for time series data like speech. In this work, we\npropose a relevance weighting scheme that allows the interpretation of the\nspeech representations during the forward propagation of the model itself. The\nrelevance weighting is achieved using a sub-network approach that performs the\ntask of feature selection. A relevance sub-network, applied on the output of\nfirst layer of a convolutional neural network model operating on raw speech\nsignals, acts as an acoustic filterbank (FB) layer with relevance weighting. A\nsimilar relevance sub-network applied on the second convolutional layer\nperforms modulation filterbank learning with relevance weighting. The full\nacoustic model consisting of relevance sub-networks, convolutional layers and\nfeed-forward layers is trained for a speech recognition task on noisy and\nreverberant speech in the Aurora-4, CHiME-3 and VOiCES datasets. The proposed\nrepresentation learning framework is also applied for the task of sound\nclassification in the UrbanSound8K dataset. A detailed analysis of the\nrelevance weights learned by the model reveals that the relevance weights\ncapture information regarding the underlying speech/audio content. In addition,\nspeech recognition and sound classification experiments reveal that the\nincorporation of relevance weighting in the neural network architecture\nimproves the performance significantly.", "published": "2020-10-29 20:36:25", "link": "http://arxiv.org/abs/2011.02136v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ACCDOA: Activity-Coupled Cartesian Direction of Arrival Representation\n  for Sound Event Localization and Detection", "abstract": "Neural-network (NN)-based methods show high performance in sound event\nlocalization and detection (SELD). Conventional NN-based methods use two\nbranches for a sound event detection (SED) target and a direction-of-arrival\n(DOA) target. The two-branch representation with a single network has to decide\nhow to balance the two objectives during optimization. Using two networks\ndedicated to each task increases system complexity and network size. To address\nthese problems, we propose an activity-coupled Cartesian DOA (ACCDOA)\nrepresentation, which assigns a sound event activity to the length of a\ncorresponding Cartesian DOA vector. The ACCDOA representation enables us to\nsolve a SELD task with a single target and has two advantages: avoiding the\nnecessity of balancing the objectives and model size increase. In experimental\nevaluations with the DCASE 2020 Task 3 dataset, the ACCDOA representation\noutperformed the two-branch representation in SELD metrics with a smaller\nnetwork size. The ACCDOA-based SELD system also performed better than\nstate-of-the-art SELD systems in terms of localization and location-dependent\ndetection.", "published": "2020-10-29 01:47:07", "link": "http://arxiv.org/abs/2010.15306v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DeviceTTS: A Small-Footprint, Fast, Stable Network for On-Device\n  Text-to-Speech", "abstract": "With the number of smart devices increasing, the demand for on-device\ntext-to-speech (TTS) increases rapidly. In recent years, many prominent\nEnd-to-End TTS methods have been proposed, and have greatly improved the\nquality of synthesized speech. However, to ensure the qualified speech, most\nTTS systems depend on large and complex neural network models, and it's hard to\ndeploy these TTS systems on-device. In this paper, a small-footprint, fast,\nstable network for on-device TTS is proposed, named as DeviceTTS. DeviceTTS\nmakes use of a duration predictor as a bridge between encoder and decoder so as\nto avoid the problem of words skipping and repeating in Tacotron. As we all\nknow, model size is a key factor for on-device TTS. For DeviceTTS, Deep\nFeedforward Sequential Memory Network (DFSMN) is used as the basic component.\nMoreover, to speed up inference, mix-resolution decoder is proposed for balance\nthe inference speed and speech quality. Experiences are done with WORLD and\nLPCNet vocoder. Finally, with only 1.4 million model parameters and 0.099\nGFLOPS, DeviceTTS achieves comparable performance with Tacotron and FastSpeech.\nAs far as we know, the DeviceTTS can meet the needs of most of the devices in\npractical application.", "published": "2020-10-29 02:04:09", "link": "http://arxiv.org/abs/2010.15311v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The IQIYI System for Voice Conversion Challenge 2020", "abstract": "This paper presents the IQIYI voice conversion system (T24) for Voice\nConversion 2020. In the competition, each target speaker has 70 sentences. We\nhave built an end-to-end voice conversion system based on PPG. First, the ASR\nacoustic model calculates the BN feature, which represents the content-related\ninformation in the speech. Then the Mel feature is calculated through an\nimproved prosody tacotron model. Finally, the Mel spectrum is converted to wav\nthrough an improved LPCNet. The evaluation results show that this system can\nachieve better voice conversion effects. In the case of using 16k rather than\n24k sampling rate audio, the conversion result is relatively good in\nnaturalness and similarity. Among them, our best results are in the similarity\nevaluation of the Task 2, the 2nd in the ASV-based objective evaluation and the\n5th in the subjective evaluation.", "published": "2020-10-29 02:18:16", "link": "http://arxiv.org/abs/2010.15317v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning Audio Embeddings with User Listening Data for Content-based\n  Music Recommendation", "abstract": "Personalized recommendation on new track releases has always been a\nchallenging problem in the music industry. To combat this problem, we first\nexplore user listening history and demographics to construct a user embedding\nrepresenting the user's music preference. With the user embedding and audio\ndata from user's liked and disliked tracks, an audio embedding can be obtained\nfor each track using metric learning with Siamese networks. For a new track, we\ncan decide the best group of users to recommend by computing the similarity\nbetween the track's audio embedding and different user embeddings,\nrespectively. The proposed system yields state-of-the-art performance on\ncontent-based music recommendation tested with millions of users and tracks.\nAlso, we extract audio embeddings as features for music genre classification\ntasks. The results show the generalization ability of our audio embeddings.", "published": "2020-10-29 06:59:21", "link": "http://arxiv.org/abs/2010.15389v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Playing a Part: Speaker Verification at the Movies", "abstract": "The goal of this work is to investigate the performance of popular speaker\nrecognition models on speech segments from movies, where often actors\nintentionally disguise their voice to play a character. We make the following\nthree contributions: (i) We collect a novel, challenging speaker recognition\ndataset called VoxMovies, with speech for 856 identities from almost 4000 movie\nclips. VoxMovies contains utterances with varying emotion, accents and\nbackground noise, and therefore comprises an entirely different domain to the\ninterview-style, emotionally calm utterances in current speaker recognition\ndatasets such as VoxCeleb; (ii) We provide a number of domain adaptation\nevaluation sets, and benchmark the performance of state-of-the-art speaker\nrecognition models on these evaluation pairs. We demonstrate that both speaker\nverification and identification performance drops steeply on this new data,\nshowing the challenge in transferring models across domains; and finally (iii)\nWe show that simple domain adaptation paradigms improve performance, but there\nis still large room for improvement.", "published": "2020-10-29 16:01:48", "link": "http://arxiv.org/abs/2010.15716v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The ins and outs of speaker recognition: lessons from VoxSRC 2020", "abstract": "The VoxCeleb Speaker Recognition Challenge (VoxSRC) at Interspeech 2020\noffers a challenging evaluation for speaker recognition systems, which includes\ncelebrities playing different parts in movies. The goal of this work is robust\nspeaker recognition of utterances recorded in these challenging environments.\nWe utilise variants of the popular ResNet architecture for speaker recognition\nand perform extensive experiments using a range of loss functions and training\nparameters. To this end, we optimise an efficient training framework that\nallows powerful models to be trained with limited time and resources. Our\ntrained models demonstrate improvements over most existing works with lighter\nmodels and a simple pipeline. The paper shares the lessons learned from our\nparticipation in the challenge.", "published": "2020-10-29 17:45:15", "link": "http://arxiv.org/abs/2010.15809v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Progressive Voice Trigger Detection: Accuracy vs Latency", "abstract": "We present an architecture for voice trigger detection for virtual\nassistants. The main idea in this work is to exploit information in words that\nimmediately follow the trigger phrase. We first demonstrate that by including\nmore audio context after a detected trigger phrase, we can indeed get a more\naccurate decision. However, waiting to listen to more audio each time incurs a\nlatency increase. Progressive Voice Trigger Detection allows us to trade-off\nlatency and accuracy by accepting clear trigger candidates quickly, but waiting\nfor more context to decide whether to accept more marginal examples. Using a\ntwo-stage architecture, we show that by delaying the decision for just 3% of\ndetected true triggers in the test set, we are able to obtain a relative\nimprovement of 66% in false rejection rate, while incurring only a negligible\nincrease in latency.", "published": "2020-10-29 09:43:04", "link": "http://arxiv.org/abs/2010.15446v2", "categories": ["eess.AS", "cs.HC", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "FullSubNet: A Full-Band and Sub-Band Fusion Model for Real-Time\n  Single-Channel Speech Enhancement", "abstract": "This paper proposes a full-band and sub-band fusion model, named as\nFullSubNet, for single-channel real-time speech enhancement. Full-band and\nsub-band refer to the models that input full-band and sub-band noisy spectral\nfeature, output full-band and sub-band speech target, respectively. The\nsub-band model processes each frequency independently. Its input consists of\none frequency and several context frequencies. The output is the prediction of\nthe clean speech target for the corresponding frequency. These two types of\nmodels have distinct characteristics. The full-band model can capture the\nglobal spectral context and the long-distance cross-band dependencies. However,\nit lacks the ability to modeling signal stationarity and attending the local\nspectral pattern. The sub-band model is just the opposite. In our proposed\nFullSubNet, we connect a pure full-band model and a pure sub-band model\nsequentially and use practical joint training to integrate these two types of\nmodels' advantages. We conducted experiments on the DNS challenge (INTERSPEECH\n2020) dataset to evaluate the proposed method. Experimental results show that\nfull-band and sub-band information are complementary, and the FullSubNet can\neffectively integrate them. Besides, the performance of the FullSubNet also\nexceeds that of the top-ranked methods in the DNS Challenge (INTERSPEECH 2020).", "published": "2020-10-29 12:01:38", "link": "http://arxiv.org/abs/2010.15508v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "UNetGAN: A Robust Speech Enhancement Approach in Time Domain for\n  Extremely Low Signal-to-noise Ratio Condition", "abstract": "Speech enhancement at extremely low signal-to-noise ratio (SNR) condition is\na very challenging problem and rarely investigated in previous works. This\npaper proposes a robust speech enhancement approach (UNetGAN) based on U-Net\nand generative adversarial learning to deal with this problem. This approach\nconsists of a generator network and a discriminator network, which operate\ndirectly in the time domain. The generator network adopts a U-Net like\nstructure and employs dilated convolution in the bottleneck of it. We evaluate\nthe performance of the UNetGAN at low SNR conditions (up to -20dB) on the\npublic benchmark. The result demonstrates that it significantly improves the\nspeech quality and substantially outperforms the representative deep learning\nmodels, including SEGAN, cGAN fo SE, Bidirectional LSTM using phase-sensitive\nspectrum approximation cost function (PSA-BLSTM) and Wave-U-Net regarding\nShort-Time Objective Intelligibility (STOI) and Perceptual evaluation of speech\nquality (PESQ).", "published": "2020-10-29 12:33:22", "link": "http://arxiv.org/abs/2010.15521v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "GANs & Reels: Creating Irish Music using a Generative Adversarial\n  Network", "abstract": "In this paper we present a method for algorithmic melody generation using a\ngenerative adversarial network without recurrent components. Music generation\nhas been successfully done using recurrent neural networks, where the model\nlearns sequence information that can help create authentic sounding melodies.\nHere, we use DC-GAN architecture with dilated convolutions and towers to\ncapture sequential information as spatial image information, and learn\nlong-range dependencies in fixed-length melody forms such as Irish traditional\nreel.", "published": "2020-10-29 17:16:22", "link": "http://arxiv.org/abs/2010.15772v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Acoustic Correlates of the Voice Qualifiers: A Survey", "abstract": "Our voices are as distinctive as our faces and fingerprints. There is a\nspectrum of non-disjoint traits that make our voices unique and identifiable,\nsuch as the fundamental frequency, the intensity, and most interestingly the\nquality of the speech. Voice quality refers to the characteristic features of\nan individual's voice. Previous research has from time-to-time proven the\nubiquity of voice quality in making different paralinguistic inferences. These\ninferences range from identifying personality traits, to health conditions and\nbeyond. In this manuscript, we first map the paralinguistic voice qualifiers to\ntheir acoustic correlates in the light of the previous research and literature.\nWe also determine the openSMILE correlates one could possibly use to measure\nthose correlates. In the second part, we give a set of example paralinguistic\ninferences that can be made using different acoustic and perceptual voice\nquality features.", "published": "2020-10-29 18:14:54", "link": "http://arxiv.org/abs/2010.15869v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Latent Space Oddity: Exploring Latent Spaces to Design Guitar Timbres", "abstract": "We introduce a novel convolutional network architecture with an interpretable\nlatent space for modeling guitar amplifiers. Leveraging domain knowledge of\npopular amplifiers spanning a range of styles, the proposed system intuitively\ncombines or subtracts characteristics of different amplifiers, allowing\nmusicians to design entirely new guitar timbres.", "published": "2020-10-29 23:19:58", "link": "http://arxiv.org/abs/2010.15989v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Interpreting glottal flow dynamics for detecting COVID-19 from voice", "abstract": "In the pathogenesis of COVID-19, impairment of respiratory functions is often\none of the key symptoms. Studies show that in these cases, voice production is\nalso adversely affected -- vocal fold oscillations are asynchronous,\nasymmetrical and more restricted during phonation. This paper proposes a method\nthat analyzes the differential dynamics of the glottal flow waveform (GFW)\nduring voice production to identify features in them that are most significant\nfor the detection of COVID-19 from voice. Since it is hard to measure this\ndirectly in COVID-19 patients, we infer it from recorded speech signals and\ncompare it to the GFW computed from physical model of phonation. For normal\nvoices, the difference between the two should be minimal, since physical models\nare constructed to explain phonation under assumptions of normalcy. Greater\ndifferences implicate anomalies in the bio-physical factors that contribute to\nthe correctness of the physical model, revealing their significance indirectly.\nOur proposed method uses a CNN-based 2-step attention model that locates\nanomalies in time-feature space in the difference of the two GFWs, allowing us\nto infer their potential as discriminative features for classification. The\nviability of this method is demonstrated using a clinically curated dataset of\nCOVID-19 positive and negative subjects.", "published": "2020-10-29 13:16:57", "link": "http://arxiv.org/abs/2010.16318v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
