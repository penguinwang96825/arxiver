{"title": "High Quality Real-Time Structured Debate Generation", "abstract": "Automatically generating debates is a challenging task that requires an\nunderstanding of arguments and how to negate or support them. In this work we\ndefine debate trees and paths for generating debates while enforcing a high\nlevel structure and grammar. We leverage a large corpus of tree-structured\ndebates that have metadata associated with each argument. We develop a\nframework for generating plausible debates which is agnostic to the sentence\nembedding model. Our results demonstrate the ability to generate debates in\nreal-time on complex topics at a quality that is close to humans, as evaluated\nby the style, content, and strategy metrics used for judging competitive human\ndebates. In the spirit of reproducible research we make our data, models, and\ncode publicly available.", "published": "2020-12-01 01:39:38", "link": "http://arxiv.org/abs/2012.00209v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BAN-ABSA: An Aspect-Based Sentiment Analysis dataset for Bengali and\n  it's baseline evaluation", "abstract": "Due to the breathtaking growth of social media or newspaper user comments,\nonline product reviews comments, sentiment analysis (SA) has captured\nsubstantial interest from the researchers. With the fast increase of domain, SA\nwork aims not only to predict the sentiment of a sentence or document but also\nto give the necessary detail on different aspects of the sentence or document\n(i.e. aspect-based sentiment analysis). A considerable number of datasets for\nSA and aspect-based sentiment analysis (ABSA) have been made available for\nEnglish and other well-known European languages. In this paper, we present a\nmanually annotated Bengali dataset of high quality, BAN-ABSA, which is\nannotated with aspect and its associated sentiment by 3 native Bengali\nspeakers. The dataset consists of 2,619 positive, 4,721 negative and 1,669\nneutral data samples from 9,009 unique comments gathered from some famous\nBengali news portals. In addition, we conducted a baseline evaluation with a\nfocus on deep learning model, achieved an accuracy of 78.75% for aspect term\nextraction and accuracy of 71.08% for sentiment classification. Experiments on\nthe BAN-ABSA dataset show that the CNN model is better in terms of accuracy\nthough Bi-LSTM significantly outperforms CNN model in terms of average\nF1-score.", "published": "2020-12-01 06:09:44", "link": "http://arxiv.org/abs/2012.00288v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "An Enhanced Knowledge Injection Model for Commonsense Generation", "abstract": "Commonsense generation aims at generating plausible everyday scenario\ndescription based on a set of provided concepts. Digging the relationship of\nconcepts from scratch is non-trivial, therefore, we retrieve prototypes from\nexternal knowledge to assist the understanding of the scenario for better\ndescription generation. We integrate two additional modules, namely position\nindicator and scaling module, into the pretrained encoder-decoder model for\nprototype modeling to enhance the knowledge injection procedure. We conduct\nexperiment on CommonGen benchmark, and experimental results show that our\nmethod significantly improves the performance on all the metrics.", "published": "2020-12-01 09:51:23", "link": "http://arxiv.org/abs/2012.00366v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CPM: A Large-scale Generative Chinese Pre-trained Language Model", "abstract": "Pre-trained Language Models (PLMs) have proven to be beneficial for various\ndownstream NLP tasks. Recently, GPT-3, with 175 billion parameters and 570GB\ntraining data, drew a lot of attention due to the capacity of few-shot (even\nzero-shot) learning. However, applying GPT-3 to address Chinese NLP tasks is\nstill challenging, as the training corpus of GPT-3 is primarily English, and\nthe parameters are not publicly available. In this technical report, we release\nthe Chinese Pre-trained Language Model (CPM) with generative pre-training on\nlarge-scale Chinese training data. To the best of our knowledge, CPM, with 2.6\nbillion parameters and 100GB Chinese training data, is the largest Chinese\npre-trained language model, which could facilitate several downstream Chinese\nNLP tasks, such as conversation, essay generation, cloze test, and language\nunderstanding. Extensive experiments demonstrate that CPM achieves strong\nperformance on many NLP tasks in the settings of few-shot (even zero-shot)\nlearning. The code and parameters are available at\nhttps://github.com/TsinghuaAI/CPM-Generate.", "published": "2020-12-01 11:32:56", "link": "http://arxiv.org/abs/2012.00413v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Denoising Pre-Training and Data Augmentation Strategies for Enhanced RDF\n  Verbalization with Transformers", "abstract": "The task of verbalization of RDF triples has known a growth in popularity due\nto the rising ubiquity of Knowledge Bases (KBs). The formalism of RDF triples\nis a simple and efficient way to store facts at a large scale. However, its\nabstract representation makes it difficult for humans to interpret. For this\npurpose, the WebNLG challenge aims at promoting automated RDF-to-text\ngeneration. We propose to leverage pre-trainings from augmented data with the\nTransformer model using a data augmentation strategy. Our experiment results\nshow a minimum relative increases of 3.73%, 126.05% and 88.16% in BLEU score\nfor seen categories, unseen entities and unseen categories respectively over\nthe standard training.", "published": "2020-12-01 15:25:47", "link": "http://arxiv.org/abs/2012.00571v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Intrinsic analysis for dual word embedding space models", "abstract": "Recent word embeddings techniques represent words in a continuous vector\nspace, moving away from the atomic and sparse representations of the past. Each\nsuch technique can further create multiple varieties of embeddings based on\ndifferent settings of hyper-parameters like embedding dimension size, context\nwindow size and training method. One additional variety appears when we\nespecially consider the Dual embedding space techniques which generate not one\nbut two-word embeddings as output. This gives rise to an interesting question -\n\"is there one or a combination of the two word embeddings variety, which works\nbetter for a specific task?\". This paper tries to answer this question by\nconsidering all of these variations. Herein, we compare two classical embedding\nmethods belonging to two different methodologies - Word2Vec from window-based\nand Glove from count-based. For an extensive evaluation after considering all\nvariations, a total of 84 different models were compared against semantic,\nassociation and analogy evaluations tasks which are made up of 9 open-source\nlinguistics datasets. The final Word2vec reports showcase the preference of\nnon-default model for 2 out of 3 tasks. In case of Glove, non-default models\noutperform in all 3 evaluation tasks.", "published": "2020-12-01 18:40:42", "link": "http://arxiv.org/abs/2012.00728v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Statistical patterns of word frequency suggesting the probabilistic\n  nature of human languages", "abstract": "Traditional linguistic theories have largely regard language as a formal\nsystem composed of rigid rules. However, their failures in processing real\nlanguage, the recent successes in statistical natural language processing, and\nthe findings of many psychological experiments have suggested that language may\nbe more a probabilistic system than a formal system, and thus cannot be\nfaithfully modeled with the either/or rules of formal linguistic theory. The\npresent study, based on authentic language data, confirmed that those important\nlinguistic issues, such as linguistic universal, diachronic drift, and language\nvariations can be translated into probability and frequency patterns in parole.\nThese findings suggest that human language may well be probabilistic systems by\nnature, and that statistical may well make inherent properties of human\nlanguages.", "published": "2020-12-01 00:48:27", "link": "http://arxiv.org/abs/2012.00187v1", "categories": ["cs.CL", "physics.comp-ph"], "primary_category": "cs.CL"}
{"title": "Modifying Memories in Transformer Models", "abstract": "Large Transformer models have achieved impressive performance in many natural\nlanguage tasks. In particular, Transformer based language models have been\nshown to have great capabilities in encoding factual knowledge in their vast\namount of parameters. While the tasks of improving the memorization and\ngeneralization of Transformers have been widely studied, it is not well known\nhow to make transformers forget specific old facts and memorize new ones. In\nthis paper, we propose a new task of \\emph{explicitly modifying specific\nfactual knowledge in Transformer models while ensuring the model performance\ndoes not degrade on the unmodified facts}. This task is useful in many\nscenarios, such as updating stale knowledge, protecting privacy, and\neliminating unintended biases stored in the models. We benchmarked several\napproaches that provide natural baseline performances on this task. This leads\nto the discovery of key components of a Transformer model that are especially\neffective for knowledge modifications. The work also provides insights into the\nrole that different training phases (such as pretraining and fine-tuning) play\ntowards memorization and knowledge modification.", "published": "2020-12-01 09:39:13", "link": "http://arxiv.org/abs/2012.00363v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ClimaText: A Dataset for Climate Change Topic Detection", "abstract": "Climate change communication in the mass media and other textual sources may\naffect and shape public perception. Extracting climate change information from\nthese sources is an important task, e.g., for filtering content and\ne-discovery, sentiment analysis, automatic summarization, question-answering,\nand fact-checking. However, automating this process is a challenge, as climate\nchange is a complex, fast-moving, and often ambiguous topic with scarce\nresources for popular text-based AI tasks. In this paper, we introduce\n\\textsc{ClimaText}, a dataset for sentence-based climate change topic\ndetection, which we make publicly available. We explore different approaches to\nidentify the climate change topic in various text sources. We find that popular\nkeyword-based models are not adequate for such a complex and evolving task.\nContext-based algorithms like BERT \\cite{devlin2018bert} can detect, in\naddition to many trivial cases, a variety of complex and implicit topic\npatterns. Nevertheless, our analysis reveals a great potential for improvement\nin several directions, such as, e.g., capturing the discussion on indirect\neffects of climate change. Hence, we hope this work can serve as a good\nstarting point for further research on this topic.", "published": "2020-12-01 13:42:37", "link": "http://arxiv.org/abs/2012.00483v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Neural language models for text classification in evidence-based\n  medicine", "abstract": "The COVID-19 has brought about a significant challenge to the whole of\nhumanity, but with a special burden upon the medical community. Clinicians must\nkeep updated continuously about symptoms, diagnoses, and effectiveness of\nemergent treatments under a never-ending flood of scientific literature. In\nthis context, the role of evidence-based medicine (EBM) for curating the most\nsubstantial evidence to support public health and clinical practice turns\nessential but is being challenged as never before due to the high volume of\nresearch articles published and pre-prints posted daily. Artificial\nIntelligence can have a crucial role in this situation. In this article, we\nreport the results of an applied research project to classify scientific\narticles to support Epistemonikos, one of the most active foundations worldwide\nconducting EBM. We test several methods, and the best one, based on the XLNet\nneural language model, improves the current approach by 93\\% on average\nF1-score, saving valuable time from physicians who volunteer to curate COVID-19\nresearch articles manually.", "published": "2020-12-01 15:53:44", "link": "http://arxiv.org/abs/2012.00584v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims", "abstract": "We introduce CLIMATE-FEVER, a new publicly available dataset for verification\nof climate change-related claims. By providing a dataset for the research\ncommunity, we aim to facilitate and encourage work on improving algorithms for\nretrieving evidential support for climate-specific claims, addressing the\nunderlying language understanding challenges, and ultimately help alleviate the\nimpact of misinformation on climate change. We adapt the methodology of FEVER\n[1], the largest dataset of artificially designed claims, to real-life claims\ncollected from the Internet. While during this process, we could rely on the\nexpertise of renowned climate scientists, it turned out to be no easy task. We\ndiscuss the surprising, subtle complexity of modeling real-world\nclimate-related claims within the \\textsc{fever} framework, which we believe\nprovides a valuable challenge for general natural language understanding. We\nhope that our work will mark the beginning of a new exciting long-term joint\neffort by the climate science and AI community.", "published": "2020-12-01 16:32:54", "link": "http://arxiv.org/abs/2012.00614v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatically Identifying Language Family from Acoustic Examples in Low\n  Resource Scenarios", "abstract": "Existing multilingual speech NLP works focus on a relatively small subset of\nlanguages, and thus current linguistic understanding of languages predominantly\nstems from classical approaches. In this work, we propose a method to analyze\nlanguage similarity using deep learning. Namely, we train a model on the\nWilderness dataset and investigate how its latent space compares with classical\nlanguage family findings. Our approach provides a new direction for\ncross-lingual data augmentation in any speech-based NLP task.", "published": "2020-12-01 22:44:42", "link": "http://arxiv.org/abs/2012.00876v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Evaluating Explanations: How much do explanations from the teacher aid\n  students?", "abstract": "While many methods purport to explain predictions by highlighting salient\nfeatures, what aims these explanations serve and how they ought to be evaluated\noften go unstated. In this work, we introduce a framework to quantify the value\nof explanations via the accuracy gains that they confer on a student model\ntrained to simulate a teacher model. Crucially, the explanations are available\nto the student during training, but are not available at test time. Compared to\nprior proposals, our approach is less easily gamed, enabling principled,\nautomatic, model-agnostic evaluation of attributions. Using our framework, we\ncompare numerous attribution methods for text classification and question\nanswering, and observe quantitative differences that are consistent (to a\nmoderate to high degree) across different student model architectures and\nlearning strategies.", "published": "2020-12-01 23:40:21", "link": "http://arxiv.org/abs/2012.00893v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Federated Marginal Personalization for ASR Rescoring", "abstract": "We introduce federated marginal personalization (FMP), a novel method for\ncontinuously updating personalized neural network language models (NNLMs) on\nprivate devices using federated learning (FL). Instead of fine-tuning the\nparameters of NNLMs on personal data, FMP regularly estimates global and\npersonalized marginal distributions of words, and adjusts the probabilities\nfrom NNLMs by an adaptation factor that is specific to each word. Our presented\napproach can overcome the limitations of federated fine-tuning and efficiently\nlearn personalized NNLMs on devices. We study the application of FMP on\nsecond-pass ASR rescoring tasks. Experiments on two speech evaluation datasets\nshow modest word error rate (WER) reductions. We also demonstrate that FMP\ncould offer reasonable privacy with only a negligible cost in speech\nrecognition accuracy.", "published": "2020-12-01 23:54:41", "link": "http://arxiv.org/abs/2012.00898v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Towards Label-Agnostic Emotion Embeddings", "abstract": "Research in emotion analysis is scattered across different label formats\n(e.g., polarity types, basic emotion categories, and affective dimensions),\nlinguistic levels (word vs. sentence vs. discourse), and, of course, (few\nwell-resourced but much more under-resourced) natural languages and text genres\n(e.g., product reviews, tweets, news). The resulting heterogeneity makes data\nand software developed under these conflicting constraints hard to compare and\nchallenging to integrate. To resolve this unsatisfactory state of affairs we\nhere propose a training scheme that learns a shared latent representation of\nemotion independent from different label formats, natural languages, and even\ndisparate model architectures. Experiments on a wide range of datasets indicate\nthat this approach yields the desired interoperability without penalizing\nprediction quality. Code and data are archived under DOI\n10.5281/zenodo.5466068.", "published": "2020-12-01 00:54:13", "link": "http://arxiv.org/abs/2012.00190v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Introducing Inter-Relatedness between Wikipedia Articles in Explicit\n  Semantic Analysis", "abstract": "Explicit Semantic Analysis (ESA) is a technique used to represent a piece of\ntext as a vector in the space of concepts, such as Articles found in Wikipedia.\nWe propose a methodology to incorporate knowledge of Inter-relatedness between\nWikipedia Articles to the vectors obtained from ESA using a technique called\nRetrofitting to improve the performance of subsequent tasks that use ESA to\nform vector embeddings. Especially we use an undirected Graph to represent this\nknowledge with nodes as Articles and edges as inter relations between two\nArticles. Here, we also emphasize how the ESA step could be seen as a\npredominantly bottom-up approach using a corpus to come up with vector\nrepresentations and the incorporation of top-down knowledge which is the\nrelations between Articles to further improve it. We test our hypothesis on\nseveral smaller subsets of the Wikipedia corpus and show that our proposed\nmethodology leads to decent improvements in performance measures including\nSpearman's Rank correlation coefficient in most cases.", "published": "2020-12-01 10:55:07", "link": "http://arxiv.org/abs/2012.00398v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Deep Ad-hoc Beamforming Based on Speaker Extraction for Target-Dependent\n  Speech Separation", "abstract": "Recently, the research on ad-hoc microphone arrays with deep learning has\ndrawn much attention, especially in speech enhancement and separation. Because\nan ad-hoc microphone array may cover such a large area that multiple speakers\nmay locate far apart and talk independently, target-dependent speech\nseparation, which aims to extract a target speaker from a mixed speech, is\nimportant for extracting and tracing a specific speaker in the ad-hoc array.\nHowever, this technique has not been explored yet. In this paper, we propose\ndeep ad-hoc beamforming based on speaker extraction, which is to our knowledge\nthe first work for target-dependent speech separation based on ad-hoc\nmicrophone arrays and deep learning. The algorithm contains three components.\nFirst, we propose a supervised channel selection framework based on speaker\nextraction, where the estimated utterance-level SNRs of the target speech are\nused as the basis for the channel selection. Second, we apply the selected\nchannels to a deep learning based MVDR algorithm, where a single-channel\nspeaker extraction algorithm is applied to each selected channel for estimating\nthe mask of the target speech. We conducted an extensive experiment on a\nWSJ0-adhoc corpus. Experimental results demonstrate the effectiveness of the\nproposed method.", "published": "2020-12-01 11:06:36", "link": "http://arxiv.org/abs/2012.00403v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Just Ask: Learning to Answer Questions from Millions of Narrated Videos", "abstract": "Recent methods for visual question answering rely on large-scale annotated\ndatasets. Manual annotation of questions and answers for videos, however, is\ntedious, expensive and prevents scalability. In this work, we propose to avoid\nmanual annotation and generate a large-scale training dataset for video\nquestion answering making use of automatic cross-modal supervision. We leverage\na question generation transformer trained on text data and use it to generate\nquestion-answer pairs from transcribed video narrations. Given narrated videos,\nwe then automatically generate the HowToVQA69M dataset with 69M\nvideo-question-answer triplets. To handle the open vocabulary of diverse\nanswers in this dataset, we propose a training procedure based on a contrastive\nloss between a video-question multi-modal transformer and an answer\ntransformer. We introduce the zero-shot VideoQA task and show excellent\nresults, in particular for rare answers. Furthermore, we demonstrate our method\nto significantly outperform the state of the art on MSRVTT-QA, MSVD-QA,\nActivityNet-QA and How2QA. Finally, for a detailed evaluation we introduce\niVQA, a new VideoQA dataset with reduced language biases and high-quality\nredundant manual annotations. Our code, datasets and trained models are\navailable at https://antoyang.github.io/just-ask.html.", "published": "2020-12-01 12:59:20", "link": "http://arxiv.org/abs/2012.00451v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A Unified Deep Speaker Embedding Framework for Mixed-Bandwidth Speech\n  Data", "abstract": "This paper proposes a unified deep speaker embedding framework for modeling\nspeech data with different sampling rates. Considering the narrowband\nspectrogram as a sub-image of the wideband spectrogram, we tackle the joint\nmodeling problem of the mixed-bandwidth data in an image classification manner.\nFrom this perspective, we elaborate several mixed-bandwidth joint training\nstrategies under different training and test data scenarios. The proposed\nsystems are able to flexibly handle the mixed-bandwidth speech data in a single\nspeaker embedding model without any additional downsampling, upsampling,\nbandwidth extension, or padding operations. We conduct extensive experimental\nstudies on the VoxCeleb1 dataset. Furthermore, the effectiveness of the\nproposed approach is validated by the SITW and NIST SRE 2016 datasets.", "published": "2020-12-01 13:45:38", "link": "http://arxiv.org/abs/2012.00486v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SeMantic AnsweR Type prediction task (SMART) at ISWC 2020 Semantic Web\n  Challenge", "abstract": "Each year the International Semantic Web Conference accepts a set of Semantic\nWeb Challenges to establish competitions that will advance the state of the art\nsolutions in any given problem domain. The SeMantic AnsweR Type prediction task\n(SMART) was part of ISWC 2020 challenges. Question type and answer type\nprediction can play a key role in knowledge base question answering systems\nproviding insights that are helpful to generate correct queries or rank the\nanswer candidates. More concretely, given a question in natural language, the\ntask of SMART challenge is, to predict the answer type using a target ontology\n(e.g., DBpedia or Wikidata).", "published": "2020-12-01 15:02:11", "link": "http://arxiv.org/abs/2012.00555v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Extracting Synonyms from Bilingual Dictionaries", "abstract": "We present our progress in developing a novel algorithm to extract synonyms\nfrom bilingual dictionaries. Identification and usage of synonyms play a\nsignificant role in improving the performance of information access\napplications. The idea is to construct a translation graph from translation\npairs, then to extract and consolidate cyclic paths to form bilingual sets of\nsynonyms. The initial evaluation of this algorithm illustrates promising\nresults in extracting Arabic-English bilingual synonyms. In the evaluation, we\nfirst converted the synsets in the Arabic WordNet into translation pairs (i.e.,\nlosing word-sense memberships). Next, we applied our algorithm to rebuild these\nsynsets. We compared the original and extracted synsets obtaining an F-Measure\nof 82.3% and 82.1% for Arabic and English synsets extraction, respectively.", "published": "2020-12-01 16:09:22", "link": "http://arxiv.org/abs/2012.00600v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Meta-Embeddings for Natural Language Inference and Semantic Similarity\n  tasks", "abstract": "Word Representations form the core component for almost all advanced Natural\nLanguage Processing (NLP) applications such as text mining, question-answering,\nand text summarization, etc. Over the last two decades, immense research is\nconducted to come up with one single model to solve all major NLP tasks. The\nmajor problem currently is that there are a plethora of choices for different\nNLP tasks. Thus for NLP practitioners, the task of choosing the right model to\nbe used itself becomes a challenge. Thus combining multiple pre-trained word\nembeddings and forming meta embeddings has become a viable approach to improve\ntackle NLP tasks. Meta embedding learning is a process of producing a single\nword embedding from a given set of pre-trained input word embeddings. In this\npaper, we propose to use Meta Embedding derived from few State-of-the-Art\n(SOTA) models to efficiently tackle mainstream NLP tasks like classification,\nsemantic relatedness, and text similarity. We have compared both ensemble and\ndynamic variants to identify an efficient approach. The results obtained show\nthat even the best State-of-the-Art models can be bettered. Thus showing us\nthat meta-embeddings can be used for several NLP tasks by harnessing the power\nof several individual representations.", "published": "2020-12-01 16:58:01", "link": "http://arxiv.org/abs/2012.00633v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mutual Information Constraints for Monte-Carlo Objectives", "abstract": "A common failure mode of density models trained as variational autoencoders\nis to model the data without relying on their latent variables, rendering these\nvariables useless. Two contributing factors, the underspecification of the\nmodel and the looseness of the variational lower bound, have been studied\nseparately in the literature. We weave these two strands of research together,\nspecifically the tighter bounds of Monte-Carlo objectives and constraints on\nthe mutual information between the observable and the latent variables.\nEstimating the mutual information as the average Kullback-Leibler divergence\nbetween the easily available variational posterior $q(z|x)$ and the prior does\nnot work with Monte-Carlo objectives because $q(z|x)$ is no longer a direct\napproximation to the model's true posterior $p(z|x)$. Hence, we construct\nestimators of the Kullback-Leibler divergence of the true posterior from the\nprior by recycling samples used in the objective, with which we train models of\ncontinuous and discrete latents at much improved rate-distortion and no\nposterior collapse. While alleviated, the tradeoff between modelling the data\nand using the latents still remains, and we urge for evaluating inference\nmethods across a range of mutual information values.", "published": "2020-12-01 18:14:08", "link": "http://arxiv.org/abs/2012.00708v2", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "StructFormer: Joint Unsupervised Induction of Dependency and\n  Constituency Structure from Masked Language Modeling", "abstract": "There are two major classes of natural language grammar -- the dependency\ngrammar that models one-to-one correspondences between words and the\nconstituency grammar that models the assembly of one or several corresponded\nwords. While previous unsupervised parsing methods mostly focus on only\ninducing one class of grammars, we introduce a novel model, StructFormer, that\ncan simultaneously induce dependency and constituency structure. To achieve\nthis, we propose a new parsing framework that can jointly generate a\nconstituency tree and dependency graph. Then we integrate the induced\ndependency relations into the transformer, in a differentiable manner, through\na novel dependency-constrained self-attention mechanism. Experimental results\nshow that our model can achieve strong results on unsupervised constituency\nparsing, unsupervised dependency parsing, and masked language modeling at the\nsame time.", "published": "2020-12-01 21:54:51", "link": "http://arxiv.org/abs/2012.00857v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-artform performance using networked interfaces: Last Man to Die's\n  Vital LMTD", "abstract": "In 2009 the cross artform group, Last Man to Die, presented a series of\nperformances using new interfaces and networked performance to integrate the\nthree artforms of its members (actor, Hanna Cormick, visual artist, Benjamin\nForster and percussionist, Charles Martin). This paper explains our artistic\nmotivations and design for a computer vision surface and networked heartbeat\nsensor as well as the experience of mounting our first major work, Vital LMTD.", "published": "2020-12-01 04:09:39", "link": "http://arxiv.org/abs/2012.00249v1", "categories": ["cs.HC", "cs.SD", "eess.AS", "H.5.5"], "primary_category": "cs.HC"}
{"title": "Strike on Stage: a percussion and media performance", "abstract": "This paper describes Strike on Stage, an interface and corresponding\naudio-visual performance work developed and performed in 2010 by percussionists\nand media artists Chi-Hsia Lai and Charles Martin. The concept of Strike on\nStage is to integrate computer visuals and sound into an improvised percussion\nperformance. A large projection surface is positioned directly behind the\nperformers, while a computer vision system tracks their movements. The setup\nallows computer visualisation and sonification to be directly responsive and\nunified with the performers' gestures.", "published": "2020-12-01 04:10:24", "link": "http://arxiv.org/abs/2012.00250v1", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Performing with a Mobile Computer System for Vibraphone", "abstract": "This paper describes the development of an Apple iPhone based mobile computer\nsystem for vibraphone and its use in a series of the author's performance\nprojects in 2011 and 2012. This artistic research was motivated by a desire to\ndevelop an alternative to laptop computers for the author's existing percussion\nand computer performance practice. The aims were to develop a light, compact\nand flexible system using mobile devices that would allow computer music to\ninfiltrate solo and ensemble performance situations where it is difficult to\nuse a laptop computer. The project began with a system that brought computer\nelements to Nordlig Vinter, a suite of percussion duos, using an iPhone, RjDj,\nPure Data and a home-made pickup system. This process was documented with video\nrecordings and analysed using ethnographic methods. The mobile computer music\nsetup proved to be elegant and convenient in performance situations with very\nlittle time and space to set up, as well as in performance classes and\nworkshops. The simple mobile system encouraged experimentation and the\nplatforms used enabled sharing with a wider audience.", "published": "2020-12-01 04:57:25", "link": "http://arxiv.org/abs/2012.00265v1", "categories": ["cs.SD", "cs.HC", "eess.AS", "H.5.5"], "primary_category": "cs.SD"}
{"title": "Tracking Ensemble Performance on Touch-Screens with Gesture\n  Classification and Transition Matrices", "abstract": "We present and evaluate a novel interface for tracking ensemble performances\non touch-screens. The system uses a Random Forest classifier to extract\ntouch-screen gestures and transition matrix statistics. It analyses the\nresulting gesture-state sequences across an ensemble of performers. A series of\nspecially designed iPad apps respond to this real-time analysis of free-form\ngestural performances with calculated modifications to their musical\ninterfaces. We describe our system and evaluate it through cross-validation and\nprofiling as well as concert experience.", "published": "2020-12-01 06:36:26", "link": "http://arxiv.org/abs/2012.00296v1", "categories": ["cs.HC", "cs.SD", "eess.AS", "H.5.5; H.5.3"], "primary_category": "cs.HC"}
{"title": "NHSS: A Speech and Singing Parallel Database", "abstract": "We present a database of parallel recordings of speech and singing, collected\nand released by the Human Language Technology (HLT) laboratory at the National\nUniversity of Singapore (NUS), that is called NUS-HLT Speak-Sing (NHSS)\ndatabase. We release this database to the public to support research\nactivities, that include, but not limited to comparative studies of acoustic\nattributes of speech and singing signals, cooperative synthesis of speech and\nsinging voices, and speech-to-singing conversion. This database consists of\nrecordings of sung vocals of English pop songs, the spoken counterpart of\nlyrics of the songs read by the singers in their natural reading manner, and\nmanually prepared utterance-level and word-level annotations. The audio\nrecordings in the NHSS database correspond to 100 songs sung and spoken by 10\nsingers, resulting in a total of 7 hours of audio data. There are 5 male and 5\nfemale singers, singing and reading the lyrics of 10 songs each. In this paper,\nwe discuss the design methodology of the database, analyse the similarities and\ndissimilarities in characteristics of speech and singing voices, and provide\nsome strategies to address relationships between these characteristics for\nconverting one to another. We develop benchmark systems, which can be used as\nreference for speech-to-singing alignment, spectral mapping, and conversion\nusing the NHSS database.", "published": "2020-12-01 08:44:37", "link": "http://arxiv.org/abs/2012.00337v2", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MusicTM-Dataset for Joint Representation Learning among Sheet Music,\n  Lyrics, and Musical Audio", "abstract": "This work present a music dataset named MusicTM-Dataset, which is utilized in\nimproving the representation learning ability of different types of cross-modal\nretrieval (CMR). Little large music dataset including three modalities is\navailable for learning representations for CMR. To collect a music dataset, we\nexpand the original musical notation to synthesize audio and generated\nsheet-music image, and build musical notation based sheet-music image, audio\nclip and syllable-denotation text as fine-grained alignment, such that the\nMusicTM-Dataset can be exploited to receive shared representation for\nmultimodal data points. The MusicTM-Dataset presents 3 kinds of modalities,\nwhich consists of the image of sheet-music, the text of lyrics and synthesized\naudio, their representations are extracted by some advanced models. In this\npaper, we introduce the background of music dataset and express the process of\nour data collection. Based on our dataset, we achieve some basic methods for\nCMR tasks. The MusicTM-Dataset are accessible in https:\n//github.com/dddzeng/MusicTM-Dataset.", "published": "2020-12-01 06:18:53", "link": "http://arxiv.org/abs/2012.00290v2", "categories": ["cs.SD", "cs.DB", "cs.IR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
