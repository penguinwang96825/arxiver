{"title": "On Modular Training of Neural Acoustics-to-Word Model for LVCSR", "abstract": "End-to-end (E2E) automatic speech recognition (ASR) systems directly map\nacoustics to words using a unified model. Previous works mostly focus on E2E\ntraining a single model which integrates acoustic and language model into a\nwhole. Although E2E training benefits from sequence modeling and simplified\ndecoding pipelines, large amount of transcribed acoustic data is usually\nrequired, and traditional acoustic and language modelling techniques cannot be\nutilized. In this paper, a novel modular training framework of E2E ASR is\nproposed to separately train neural acoustic and language models during\ntraining stage, while still performing end-to-end inference in decoding stage.\nHere, an acoustics-to-phoneme model (A2P) and a phoneme-to-word model (P2W) are\ntrained using acoustic data and text data respectively. A phone synchronous\ndecoding (PSD) module is inserted between A2P and P2W to reduce sequence\nlengths without precision loss. Finally, modules are integrated into an\nacousticsto-word model (A2W) and jointly optimized using acoustic data to\nretain the advantage of sequence modeling. Experiments on a 300- hour\nSwitchboard task show significant improvement over the direct A2W model. The\nefficiency in both training and decoding also benefits from the proposed\nmethod.", "published": "2018-03-03 02:08:46", "link": "http://arxiv.org/abs/1803.01090v1", "categories": ["cs.CL", "68T10", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Tag-Enhanced Tree-Structured Neural Networks for Implicit Discourse\n  Relation Classification", "abstract": "Identifying implicit discourse relations between text spans is a challenging\ntask because it requires understanding the meaning of the text. To tackle this\ntask, recent studies have tried several deep learning methods but few of them\nexploited the syntactic information. In this work, we explore the idea of\nincorporating syntactic parse tree into neural networks. Specifically, we\nemploy the Tree-LSTM model and Tree-GRU model, which are based on the tree\nstructure, to encode the arguments in a relation. Moreover, we further leverage\nthe constituent tags to control the semantic composition process in these\ntree-structured neural networks. Experimental results show that our method\nachieves state-of-the-art performance on PDTB corpus.", "published": "2018-03-03 13:57:37", "link": "http://arxiv.org/abs/1803.01165v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding and Improving Multi-Sense Word Embeddings via Extended\n  Robust Principal Component Analysis", "abstract": "Unsupervised learned representations of polysemous words generate a large of\npseudo multi senses since unsupervised methods are overly sensitive to\ncontextual variations. In this paper, we address the pseudo multi-sense\ndetection for word embeddings by dimensionality reduction of sense pairs. We\npropose a novel principal analysis method, termed Ex-RPCA, designed to detect\nboth pseudo multi senses and real multi senses. With Ex-RPCA, we empirically\nshow that pseudo multi senses are generated systematically in unsupervised\nmethod. Moreover, the multi-sense word embeddings can by improved by a simple\nlinear transformation based on Ex-RPCA. Our improved word embedding outperform\nthe original one by 5.6 points on Stanford contextual word similarity (SCWS)\ndataset. We hope our simple yet effective approach will help the linguistic\nanalysis of multi-sense word embeddings in the future.", "published": "2018-03-03 22:39:02", "link": "http://arxiv.org/abs/1803.01255v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SpeechPy - A Library for Speech Processing and Recognition", "abstract": "SpeechPy is an open source Python package that contains speech preprocessing\ntechniques, speech features, and important post-processing operations. It\nprovides most frequent used speech features including MFCCs and filterbank\nenergies alongside with the log-energy of filter-banks. The aim of the package\nis to provide researchers with a simple tool for speech feature extraction and\nprocessing purposes in applications such as Automatic Speech Recognition and\nSpeaker Verification.", "published": "2018-03-03 02:30:55", "link": "http://arxiv.org/abs/1803.01094v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio-only Bird Species Automated Identification Method with Limited\n  Training Data Based on Multi-Channel Deep Convolutional Neural Networks", "abstract": "Based on the transfer learning, we design a bird species identification model\nthat uses the VGG-16 model (pretrained on ImageNet) for feature extraction,\nthen a classifier consisting of two fully-connected hidden layers and a Softmax\nlayer is attached. We compare the performance of the proposed model with the\noriginal VGG16 model. The results show that the former has higher train\nefficiency, but lower mean average precisions(MAP). To improve the MAP of the\nproposed model, we investigate the result fusion mode to form multi-channel\nidentification model, the best MAP reaches 0.9998. The number of model\nparameters is 13110, which is only 0.0082% of the VGG16 model. Also, the size\ndemand of sample is decreased.", "published": "2018-03-03 05:12:06", "link": "http://arxiv.org/abs/1803.01107v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Ensemble Framework of Voice-Based Emotion Recognition System for\n  Films and TV Programs", "abstract": "Employing voice-based emotion recognition function in artificial intelligence\n(AI) product will improve the user experience. Most of researches that have\nbeen done only focus on the speech collected under controlled conditions. The\nscenarios evaluated in these research were well controlled. The conventional\napproach may fail when background noise or nonspeech filler exist. In this\npaper, we propose an ensemble framework combining several aspects of features\nfrom audio. The framework incorporates gender and speaker information relying\non multi-task learning. Therefore it is able to dig and capture emotional\ninformation as much as possible. This framework is evaluated on multimodal\nemotion challenge (MEC) 2017 corpus which is close to real world. The proposed\nframework outperformed the best baseline system by 29.5% (relative\nimprovement).", "published": "2018-03-03 08:17:12", "link": "http://arxiv.org/abs/1803.01122v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Enhancement of Noisy Speech exploiting a Gaussian Modeling based\n  Threshold and a PDF Dependent Thresholding Function", "abstract": "This paper presents a speech enhancement method, where an adaptive threshold\nis statistically determined based on Gaussian modeling of Teager energy (TE)\noperated perceptual wavelet packet (PWP) coefficients of noisy speech. In order\nto obtain an enhanced speech, the threshold thus derived is applied upon the\nPWP coefficients by employing a Gaussian pdf dependent custom thresholding\nfunction, which is designed based on a combination of modified hard and\nsemisoft thresholding functions. The effectiveness of the proposed method is\nevaluated for car and multi-talker babble noise corrupted speech signals\nthrough performing extensive simulations using the NOIZEUS database. The\nproposed method is found to outperform some of the state-of-the-art speech\nenhancement methods not only at at high but also at low levels of SNRs in the\nsense of standard objective measures and subjective evaluations including\nformal listening tests.", "published": "2018-03-03 05:55:59", "link": "http://arxiv.org/abs/1803.01841v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Enhancement Based on Non-stationary Noise-driven Geometric\n  Spectral Subtraction and Phase Spectrum Compensation", "abstract": "In this paper, a speech enhancement method based on noise compensation\nperformed on short time magnitude as well phase spectra is presented. Unlike\nthe conventional geometric approach (GA) to spectral subtraction (SS), here the\nnoise estimate to be subtracted from the noisy speech spectrum is proposed to\nbe determined by exploiting the low frequency regions of current frame of noisy\nspeech rather than depending only on the initial silence frames. This approach\ngives the capability of tracking non-stationary noise thus resulting in a\nnon-stationary noise-driven geometric approach of spectral subtraction for\nspeech enhancement. The noise compensated magnitude spectrum from the GA step\nis then recombined with unchanged phase of noisy speech spectrum and used in\nphase compensation to obtain an enhanced complex spectrum, which is used to\nproduce an enhanced speech frame. Extensive simulations are carried out using\nspeech files available in the NOIZEUS database shows that the proposed method\nconsistently outperforms some of the recent methods of speech enhancement when\nemployed on the noisy speeches corrupted by street or babble noise at different\nlevels of SNR in terms of objective measures, spectrogram analysis and formal\nsubjective listening tests.", "published": "2018-03-03 05:57:11", "link": "http://arxiv.org/abs/1803.02870v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
