{"title": "The Helsinki Neural Machine Translation System", "abstract": "We introduce the Helsinki Neural Machine Translation system (HNMT) and how it\nis applied in the news translation task at WMT 2017, where it ranked first in\nboth the human and automatic evaluations for English--Finnish. We discuss the\nsuccess of English--Finnish translations and the overall advantage of NMT over\na strong SMT baseline. We also discuss our submissions for English--Latvian,\nEnglish--Chinese and Chinese--English.", "published": "2017-08-20 09:24:46", "link": "http://arxiv.org/abs/1708.05942v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation with Extended Context", "abstract": "We investigate the use of extended context in attention-based neural machine\ntranslation. We base our experiments on translated movie subtitles and discuss\nthe effect of increasing the segments beyond single translation units. We study\nthe use of extended source language context as well as bilingual context\nextensions. The models learn to distinguish between information from different\nsegments and are surprisingly robust with respect to translation quality. In\nthis pilot study, we observe interesting cross-sentential attention patterns\nthat improve textual coherence in translation at least in some selected cases.", "published": "2017-08-20 09:31:49", "link": "http://arxiv.org/abs/1708.05943v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An End-to-End Trainable Neural Network Model with Belief Tracking for\n  Task-Oriented Dialog", "abstract": "We present a novel end-to-end trainable neural network model for\ntask-oriented dialog systems. The model is able to track dialog state, issue\nAPI calls to knowledge base (KB), and incorporate structured KB query results\ninto system responses to successfully complete task-oriented dialogs. The\nproposed model produces well-structured system responses by jointly learning\nbelief tracking and KB result processing conditioning on the dialog history. We\nevaluate the model in a restaurant search domain using a dataset that is\nconverted from the second Dialog State Tracking Challenge (DSTC2) corpus.\nExperiment results show that the proposed model can robustly track dialog state\ngiven the dialog history. Moreover, our model demonstrates promising results in\nproducing appropriate system responses, outperforming prior end-to-end\ntrainable neural network models using per-response accuracy evaluation metrics.", "published": "2017-08-20 12:00:16", "link": "http://arxiv.org/abs/1708.05956v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Expanding Abbreviations in a Strongly Inflected Language: Are\n  Morphosyntactic Tags Sufficient?", "abstract": "In this paper, the problem of recovery of morphological information lost in\nabbreviated forms is addressed with a focus on highly inflected languages.\nEvidence is presented that the correct inflected form of an expanded\nabbreviation can in many cases be deduced solely from the morphosyntactic tags\nof the context. The prediction model is a deep bidirectional LSTM network with\ntag embedding. The training and evaluation data are gathered by finding the\nwords which could have been abbreviated and using their corresponding\nmorphosyntactic tags as the labels, while the tags of the context words are\nused as the input features for classification. The network is trained on over\n10 million words from the Polish Sejm Corpus and achieves 74.2% prediction\naccuracy on a smaller, but more general National Corpus of Polish. The analysis\nof errors suggests that performance in this task may improve if some prior\nknowledge about the abbreviated word is incorporated into the model.", "published": "2017-08-20 16:29:54", "link": "http://arxiv.org/abs/1708.05992v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Paraphrase for Question Answering", "abstract": "Question answering (QA) systems are sensitive to the many different ways\nnatural language expresses the same information need. In this paper we turn to\nparaphrases as a means of capturing this knowledge and present a general\nframework which learns felicitous paraphrases for various QA tasks. Our method\nis trained end-to-end using question-answer pairs as a supervision signal. A\nquestion and its paraphrases serve as input to a neural scoring model which\nassigns higher weights to linguistic expressions most likely to yield correct\nanswers. We evaluate our approach on QA over Freebase and answer sentence\nselection. Experimental results on three datasets show that our framework\nconsistently improves performance, achieving competitive results despite the\nuse of simple QA models.", "published": "2017-08-20 21:19:01", "link": "http://arxiv.org/abs/1708.06022v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Portuguese Word Embeddings: Evaluating on Word Analogies and Natural\n  Language Tasks", "abstract": "Word embeddings have been found to provide meaningful representations for\nwords in an efficient way; therefore, they have become common in Natural\nLanguage Processing sys- tems. In this paper, we evaluated different word\nembedding models trained on a large Portuguese corpus, including both Brazilian\nand European variants. We trained 31 word embedding models using FastText,\nGloVe, Wang2Vec and Word2Vec. We evaluated them intrinsically on syntactic and\nsemantic analogies and extrinsically on POS tagging and sentence semantic\nsimilarity tasks. The obtained results suggest that word analogies are not\nappropriate for word embedding evaluation; task-specific evaluations appear to\nbe a better option.", "published": "2017-08-20 21:21:37", "link": "http://arxiv.org/abs/1708.06025v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Batch Noise Contrastive Estimation Approach for Training Large\n  Vocabulary Language Models", "abstract": "Training large vocabulary Neural Network Language Models (NNLMs) is a\ndifficult task due to the explicit requirement of the output layer\nnormalization, which typically involves the evaluation of the full softmax\nfunction over the complete vocabulary. This paper proposes a Batch Noise\nContrastive Estimation (B-NCE) approach to alleviate this problem. This is\nachieved by reducing the vocabulary, at each time step, to the target words in\nthe batch and then replacing the softmax by the noise contrastive estimation\napproach, where these words play the role of targets and noise samples at the\nsame time. In doing so, the proposed approach can be fully formulated and\nimplemented using optimal dense matrix operations. Applying B-NCE to train\ndifferent NNLMs on the Large Text Compression Benchmark (LTCB) and the One\nBillion Word Benchmark (OBWB) shows a significant reduction of the training\ntime with no noticeable degradation of the models performance. This paper also\npresents a new baseline comparative study of different standard NNLMs on the\nlarge OBWB on a single Titan-X GPU.", "published": "2017-08-20 17:48:35", "link": "http://arxiv.org/abs/1708.05997v2", "categories": ["cs.CL", "cs.AI", "97K50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Neural Networks Compression for Language Modeling", "abstract": "In this paper, we consider several compression techniques for the language\nmodeling problem based on recurrent neural networks (RNNs). It is known that\nconventional RNNs, e.g, LSTM-based networks in language modeling, are\ncharacterized with either high space complexity or substantial inference time.\nThis problem is especially crucial for mobile applications, in which the\nconstant interaction with the remote server is inappropriate. By using the Penn\nTreebank (PTB) dataset we compare pruning, quantization, low-rank\nfactorization, tensor train decomposition for LSTM networks in terms of model\nsize and suitability for fast inference.", "published": "2017-08-20 13:37:06", "link": "http://arxiv.org/abs/1708.05963v1", "categories": ["stat.ML", "cs.CL", "cs.LG", "cs.NE", "62M45, 68T50", "I.2.7, I.2.6, I.5.1, I.5.4"], "primary_category": "stat.ML"}
{"title": "Efficient Online Inference for Infinite Evolutionary Cluster models with\n  Applications to Latent Social Event Discovery", "abstract": "The Recurrent Chinese Restaurant Process (RCRP) is a powerful statistical\nmethod for modeling evolving clusters in large scale social media data. With\nthe RCRP, one can allow both the number of clusters and the cluster parameters\nin a model to change over time. However, application of the RCRP has largely\nbeen limited due to the non-conjugacy between the cluster evolutionary priors\nand the Multinomial likelihood. This non-conjugacy makes inference di cult and\nrestricts the scalability of models which use the RCRP, leading to the RCRP\nbeing applied only in simple problems, such as those that can be approximated\nby a single Gaussian emission. In this paper, we provide a novel solution for\nthe non-conjugacy issues for the RCRP and an example of how to leverage our\nsolution for one speci c problem - the social event discovery problem. By\nutilizing Sequential Monte Carlo methods in inference, our approach can be\nmassively paralleled and is highly scalable, to the extent it can work on tens\nof millions of documents. We are able to generate high quality topical and\nlocation distributions of the clusters that can be directly interpreted as real\nsocial events, and our experimental results suggest that the approaches\nproposed achieve much better predictive performance than techniques reported in\nprior work. We also demonstrate how the techniques we develop can be used in a\nmuch more general ways toward similar problems.", "published": "2017-08-20 18:17:27", "link": "http://arxiv.org/abs/1708.06000v1", "categories": ["cs.AI", "cs.CL", "cs.SI"], "primary_category": "cs.AI"}
