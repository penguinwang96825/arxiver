{"title": "StyleDubber: Towards Multi-Scale Style Learning for Movie Dubbing", "abstract": "Given a script, the challenge in Movie Dubbing (Visual Voice Cloning, V2C) is\nto generate speech that aligns well with the video in both time and emotion,\nbased on the tone of a reference audio track. Existing state-of-the-art V2C\nmodels break the phonemes in the script according to the divisions between\nvideo frames, which solves the temporal alignment problem but leads to\nincomplete phoneme pronunciation and poor identity stability. To address this\nproblem, we propose StyleDubber, which switches dubbing learning from the frame\nlevel to phoneme level. It contains three main components: (1) A multimodal\nstyle adaptor operating at the phoneme level to learn pronunciation style from\nthe reference audio, and generate intermediate representations informed by the\nfacial emotion presented in the video; (2) An utterance-level style learning\nmodule, which guides both the mel-spectrogram decoding and the refining\nprocesses from the intermediate embeddings to improve the overall style\nexpression; And (3) a phoneme-guided lip aligner to maintain lip sync.\nExtensive experiments on two of the primary benchmarks, V2C and Grid,\ndemonstrate the favorable performance of the proposed method as compared to the\ncurrent stateof-the-art. The code will be made available at\nhttps://github.com/GalaxyCong/StyleDubber.", "published": "2024-02-20 01:28:34", "link": "http://arxiv.org/abs/2402.12636v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simpson's Paradox and the Accuracy-Fluency Tradeoff in Translation", "abstract": "A good translation should be faithful to the source and should respect the\nnorms of the target language. We address a theoretical puzzle about the\nrelationship between these objectives. On one hand, intuition and some prior\nwork suggest that accuracy and fluency should trade off against each other, and\nthat capturing every detail of the source can only be achieved at the cost of\nfluency. On the other hand, quality assessment researchers often suggest that\naccuracy and fluency are highly correlated and difficult for human raters to\ndistinguish (Callison-Burch et al., 2007). We show that the tension between\nthese views is an instance of Simpson's paradox, and that accuracy and fluency\nare positively correlated at the level of the corpus but trade off at the level\nof individual source segments. We further suggest that the relationship between\naccuracy and fluency is best evaluated at the segment (or sentence) level, and\nthat the trade off between these dimensions has implications both for assessing\ntranslation quality and developing improved MT systems.", "published": "2024-02-20 03:37:16", "link": "http://arxiv.org/abs/2402.12690v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tree-Planted Transformers: Unidirectional Transformer Language Models\n  with Implicit Syntactic Supervision", "abstract": "Syntactic Language Models (SLMs) can be trained efficiently to reach\nrelatively high performance; however, they have trouble with inference\nefficiency due to the explicit generation of syntactic structures. In this\npaper, we propose a new method dubbed tree-planting: instead of explicitly\ngenerating syntactic structures, we \"plant\" trees into attention weights of\nunidirectional Transformer LMs to implicitly reflect syntactic structures of\nnatural language. Specifically, unidirectional Transformer LMs trained with\ntree-planting will be called Tree-Planted Transformers (TPT), which inherit the\ntraining efficiency from SLMs without changing the inference efficiency of\ntheir underlying Transformer LMs. Targeted syntactic evaluations on the\nSyntaxGym benchmark demonstrated that TPTs, despite the lack of explicit\ngeneration of syntactic structures, significantly outperformed not only vanilla\nTransformer LMs but also various SLMs that generate hundreds of syntactic\nstructures in parallel. This result suggests that TPTs can learn human-like\nsyntactic knowledge as data-efficiently as SLMs while maintaining the modeling\nspace of Transformer LMs unchanged.", "published": "2024-02-20 03:37:24", "link": "http://arxiv.org/abs/2402.12691v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FormulaReasoning: A Dataset for Formula-Based Numerical Reasoning", "abstract": "The application of formulas is a fundamental ability of humans when\naddressing numerical reasoning problems. However, existing numerical reasoning\ndatasets seldom indicate explicitly the formulas employed during the reasoning\nsteps. To bridge this gap, we construct a dataset for formula-based numerical\nreasoning called FormulaReasoning, which consists of 5,420 reasoning-based\nquestions. We employ it to conduct evaluations of LLMs with size ranging from\n7B to over 100B parameters utilizing zero-shot and few-shot chain-of-thought\nmethods, and we further explore using retrieval-augmented LLMs provided with an\nexternal formula database associated with our dataset. We also experiment with\nsupervised methods where we divide the reasoning process into formula\ngeneration, parameter extraction, and numerical calculation, and perform data\naugmentation. Our empirical findings underscore the significant potential for\nimprovement in existing models when applied to our challenging, formula-driven\nFormulaReasoning.", "published": "2024-02-20 03:39:49", "link": "http://arxiv.org/abs/2402.12692v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are LLMs Rational Investors? A Study on Detecting and Reducing the\n  Financial Bias in LLMs", "abstract": "Large Language Models (LLMs) are increasingly adopted in financial analysis\nfor interpreting complex market data and trends. However, their use is\nchallenged by intrinsic biases (e.g., risk-preference bias) and a superficial\nunderstanding of market intricacies, necessitating a thorough assessment of\ntheir financial insight. To address these issues, we introduce Financial Bias\nIndicators (FBI), a framework with components like Bias Unveiler, Bias\nDetective, Bias Tracker, and Bias Antidote to identify, detect, analyze, and\neliminate irrational biases in LLMs. By combining behavioral finance principles\nwith bias examination, we evaluate 23 leading LLMs and propose a de-biasing\nmethod based on financial causal knowledge. Results show varying degrees of\nfinancial irrationality among models, influenced by their design and training.\nModels trained specifically on financial datasets may exhibit more\nirrationality, and even larger financial language models (FinLLMs) can show\nmore bias than smaller, general models. We utilize four prompt-based methods\nincorporating causal debiasing, effectively reducing financial biases in these\nmodels. This work enhances the understanding of LLMs' bias in financial\napplications, laying the foundation for developing more reliable and rational\nfinancial analysis tools.", "published": "2024-02-20 04:26:08", "link": "http://arxiv.org/abs/2402.12713v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Acknowledgment of Emotional States: Generating Validating Responses for\n  Empathetic Dialogue", "abstract": "In the realm of human-AI dialogue, the facilitation of empathetic responses\nis important. Validation is one of the key communication techniques in\npsychology, which entails recognizing, understanding, and acknowledging others'\nemotional states, thoughts, and actions. This study introduces the first\nframework designed to engender empathetic dialogue with validating responses.\nOur approach incorporates a tripartite module system: 1) validation timing\ndetection, 2) users' emotional state identification, and 3) validating response\ngeneration. Utilizing Japanese EmpatheticDialogues dataset - a textual-based\ndialogue dataset consisting of 8 emotional categories from Plutchik's wheel of\nemotions - the Task Adaptive Pre-Training (TAPT) BERT-based model outperforms\nboth random baseline and the ChatGPT performance, in term of F1-score, in all\nmodules. Further validation of our model's efficacy is confirmed in its\napplication to the TUT Emotional Storytelling Corpus (TESC), a speech-based\ndialogue dataset, by surpassing both random baseline and the ChatGPT. This\nconsistent performance across both textual and speech-based dialogues\nunderscores the effectiveness of our framework in fostering empathetic human-AI\ncommunication.", "published": "2024-02-20 07:20:03", "link": "http://arxiv.org/abs/2402.12770v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Few-shot clinical entity recognition in English, French and Spanish:\n  masked language models outperform generative model prompting", "abstract": "Large language models (LLMs) have become the preferred solution for many\nnatural language processing tasks. In low-resource environments such as\nspecialized domains, their few-shot capabilities are expected to deliver high\nperformance. Named Entity Recognition (NER) is a critical task in information\nextraction that is not covered in recent LLM benchmarks. There is a need for\nbetter understanding the performance of LLMs for NER in a variety of settings\nincluding languages other than English. This study aims to evaluate generative\nLLMs, employed through prompt engineering, for few-shot clinical NER. %from the\nperspective of F1 performance and environmental impact. We compare 13\nauto-regressive models using prompting and 16 masked models using fine-tuning\non 14 NER datasets covering English, French and Spanish. While prompt-based\nauto-regressive models achieve competitive F1 for general NER, they are\noutperformed within the clinical domain by lighter biLSTM-CRF taggers based on\nmasked models. Additionally, masked models exhibit lower environmental impact\ncompared to auto-regressive models. Findings are consistent across the three\nlanguages studied, which suggests that LLM prompting is not yet suited for NER\nproduction in the clinical domain.", "published": "2024-02-20 08:20:49", "link": "http://arxiv.org/abs/2402.12801v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SymBa: Symbolic Backward Chaining for Structured Natural Language\n  Reasoning", "abstract": "To improve the performance and explainability of LLM-based natural language\nreasoning, structured reasoning can be applied to generate explicitly\nstructured proofs. Among different methods for structured reasoning, we\nspecifically focus on backward chaining, where the proof goal is recursively\ndecomposed to subgoals by searching and applying rules. We argue that current\nLLM-based backward chaining systems (e.g. Least-to-most prompting and LAMBADA)\nare incomplete, as they omit crucial algorithmic components identified from the\nclassic backward chaining algorithm in computational logic (SLD Resolution). To\nthis end, we propose a novel backward chaining system, SymBa (Symbolic Backward\nChaining), which integrates a symbolic solver and an LLM. In SymBa, the solver\ncontrols the proof process, and the LLM is only called when the solver requires\nnew information to complete the proof. Empowered by completeness, SymBa\nachieves a significant improvement in seven deductive, relational, and\narithmetic reasoning benchmarks compared to the baselines.", "published": "2024-02-20 08:27:05", "link": "http://arxiv.org/abs/2402.12806v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ArabicMMLU: Assessing Massive Multitask Language Understanding in Arabic", "abstract": "The focus of language model evaluation has transitioned towards reasoning and\nknowledge-intensive tasks, driven by advancements in pretraining large models.\nWhile state-of-the-art models are partially trained on large Arabic texts,\nevaluating their performance in Arabic remains challenging due to the limited\navailability of relevant datasets. To bridge this gap, we present\n\\datasetname{}, the first multi-task language understanding benchmark for the\nArabic language, sourced from school exams across diverse educational levels in\ndifferent countries spanning North Africa, the Levant, and the Gulf regions.\nOur data comprises 40 tasks and 14,575 multiple-choice questions in Modern\nStandard Arabic (MSA) and is carefully constructed by collaborating with native\nspeakers in the region. Our comprehensive evaluations of 35 models reveal\nsubstantial room for improvement, particularly among the best open-source\nmodels. Notably, BLOOMZ, mT0, LLaMA2, and Falcon struggle to achieve a score of\n50%, while even the top-performing Arabic-centric model only achieves a score\nof 62.3%.", "published": "2024-02-20 09:07:41", "link": "http://arxiv.org/abs/2402.12840v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MoELoRA: Contrastive Learning Guided Mixture of Experts on\n  Parameter-Efficient Fine-Tuning for Large Language Models", "abstract": "Fine-tuning is often necessary to enhance the adaptability of Large Language\nModels (LLM) to downstream tasks. Nonetheless, the process of updating billions\nof parameters demands significant computational resources and training time,\nwhich poses a substantial obstacle to the widespread application of large-scale\nmodels in various scenarios. To address this issue, Parameter-Efficient\nFine-Tuning (PEFT) has emerged as a prominent paradigm in recent research.\nHowever, current PEFT approaches that employ a limited set of global parameters\n(such as LoRA, which adds low-rank approximation matrices to all weights) face\nchallenges in flexibly combining different computational modules in downstream\ntasks. In this work, we introduce a novel PEFT method: MoELoRA. We consider\nLoRA as Mixture of Experts (MoE), and to mitigate the random routing phenomenon\nobserved in MoE, we propose the utilization of contrastive learning to\nencourage experts to learn distinct features. We conducted experiments on 11\ntasks in math reasoning and common-sense reasoning benchmarks. With the same\nnumber of parameters, our approach outperforms LoRA significantly. In math\nreasoning, MoELoRA achieved an average performance that was 4.2% higher than\nLoRA, and demonstrated competitive performance compared to the 175B GPT-3.5 on\nseveral benchmarks.", "published": "2024-02-20 09:30:48", "link": "http://arxiv.org/abs/2402.12851v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Handling Ambiguity in Emotion: From Out-of-Domain Detection to\n  Distribution Estimation", "abstract": "The subjective perception of emotion leads to inconsistent labels from human\nannotators. Typically, utterances lacking majority-agreed labels are excluded\nwhen training an emotion classifier, which cause problems when encountering\nambiguous emotional expressions during testing. This paper investigates three\nmethods to handle ambiguous emotion. First, we show that incorporating\nutterances without majority-agreed labels as an additional class in the\nclassifier reduces the classification performance of the other emotion classes.\nThen, we propose detecting utterances with ambiguous emotions as out-of-domain\nsamples by quantifying the uncertainty in emotion classification using\nevidential deep learning. This approach retains the classification accuracy\nwhile effectively detects ambiguous emotion expressions. Furthermore, to obtain\nfine-grained distinctions among ambiguous emotions, we propose representing\nemotion as a distribution instead of a single class label. The task is thus\nre-framed from classification to distribution estimation where every individual\nannotation is taken into account, not just the majority opinion. The evidential\nuncertainty measure is extended to quantify the uncertainty in emotion\ndistribution estimation. Experimental results on the IEMOCAP and CREMA-D\ndatasets demonstrate the superior capability of the proposed method in terms of\nmajority class prediction, emotion distribution estimation, and uncertainty\nestimation.", "published": "2024-02-20 09:53:38", "link": "http://arxiv.org/abs/2402.12862v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based\n  Question Answering with Domain Hybrid Data", "abstract": "Augmenting Large Language Models (LLMs) for Question Answering (QA) with\ndomain specific data has attracted wide attention. However, domain data often\nexists in a hybrid format, including text and semi-structured tables, posing\nchallenges for the seamless integration of information. Table-to-Text\nGeneration is a promising solution by facilitating the transformation of hybrid\ndata into a uniformly text-formatted corpus. Although this technique has been\nwidely studied by the NLP community, there is currently no comparative analysis\non how corpora generated by different table-to-text methods affect the\nperformance of QA systems. In this paper, we address this research gap in two\nsteps. First, we innovatively integrate table-to-text generation into the\nframework of enhancing LLM-based QA systems with domain hybrid data. Then, we\nutilize this framework in real-world industrial data to conduct extensive\nexperiments on two types of QA systems (DSFT and RAG frameworks) with four\nrepresentative methods: Markdown format, Template serialization, TPLM-based\nmethod, and LLM-based method. Based on the experimental results, we draw some\nempirical findings and explore the underlying reasons behind the success of\nsome methods. We hope the findings of this work will provide a valuable\nreference for the academic and industrial communities in developing robust QA\nsystems.", "published": "2024-02-20 10:00:58", "link": "http://arxiv.org/abs/2402.12869v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Autism Detection in Speech -- A Survey", "abstract": "There has been a range of studies of how autism is displayed in voice,\nspeech, and language. We analyse studies from the biomedical, as well as the\npsychological domain, but also from the NLP domain in order to find linguistic,\nprosodic and acoustic cues that could indicate autism. Our survey looks at all\nthree domains. We define autism and which comorbidities might influence the\ncorrect detection of the disorder. We especially look at observations such as\nverbal and semantic fluency, prosodic features, but also disfluencies and\nspeaking rate. We also show word-based approaches and describe machine learning\nand transformer-based approaches both on the audio data as well as the\ntranscripts. Lastly, we conclude, while there already is a lot of research,\nfemale patients seem to be severely under-researched. Also, most NLP research\nfocuses on traditional machine learning methods instead of transformers which\ncould be beneficial in this context. Additionally, we were unable to find\nresearch combining both features from audio and transcripts.", "published": "2024-02-20 10:18:18", "link": "http://arxiv.org/abs/2402.12880v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TEXT2AFFORD: Probing Object Affordance Prediction abilities of Language\n  Models solely from Text", "abstract": "We investigate the knowledge of object affordances in pre-trained language\nmodels (LMs) and pre-trained Vision-Language models (VLMs). A growing body of\nliterature shows that PTLMs fail inconsistently and non-intuitively,\ndemonstrating a lack of reasoning and grounding. To take a first step toward\nquantifying the effect of grounding (or lack thereof), we curate a novel and\ncomprehensive dataset of object affordances -- Text2Afford, characterized by 15\naffordance classes. Unlike affordance datasets collected in vision and language\ndomains, we annotate in-the-wild sentences with objects and affordances.\nExperimental results reveal that PTLMs exhibit limited reasoning abilities when\nit comes to uncommon object affordances. We also observe that pre-trained VLMs\ndo not necessarily capture object affordances effectively. Through few-shot\nfine-tuning, we demonstrate improvement in affordance knowledge in PTLMs and\nVLMs. Our research contributes a novel dataset for language grounding tasks,\nand presents insights into LM capabilities, advancing the understanding of\nobject affordances. Codes and data are available at\nhttps://github.com/sayantan11995/Affordance", "published": "2024-02-20 10:23:00", "link": "http://arxiv.org/abs/2402.12881v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OPDAI at SemEval-2024 Task 6: Small LLMs can Accelerate Hallucination\n  Detection with Weakly Supervised Data", "abstract": "This paper mainly describes a unified system for hallucination detection of\nLLMs, which wins the second prize in the model-agnostic track of the\nSemEval-2024 Task 6, and also achieves considerable results in the model-aware\ntrack. This task aims to detect hallucination with LLMs for three different\ntext-generation tasks without labeled training data. We utilize prompt\nengineering and few-shot learning to verify the performance of different LLMs\non the validation data. Then we select the LLMs with better performance to\ngenerate high-quality weakly supervised training data, which not only satisfies\nthe consistency of different LLMs, but also satisfies the consistency of the\noptimal LLM with different sampling parameters. Furthermore, we finetune\ndifferent LLMs by using the constructed training data, and finding that a\nrelatively small LLM can achieve a competitive level of performance in\nhallucination detection, when compared to the large LLMs and the prompt-based\napproaches using GPT-4.", "published": "2024-02-20 11:01:39", "link": "http://arxiv.org/abs/2402.12913v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Normalized Orthography for Tunisian Arabic", "abstract": "Tunisian Arabic (ISO 693-3: aeb) isa distinct variety native to Tunisia,\nderived from Arabic and enriched by various historical influences. This\nresearch introduces the \"Normalized Orthography for Tunisian Arabic\" (NOTA), an\nadaptation of CODA* guidelines for transcribing Tunisian Arabic using Arabic\nscript. The aim is to enhance language resource development by ensuring\nuser-friendliness and consistency. The updated standard addresses challenges in\naccurately representing Tunisian phonology and morphology, correcting issues\nfrom transcriptions based on Modern Standard Arabic.", "published": "2024-02-20 11:52:29", "link": "http://arxiv.org/abs/2402.12940v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GumbelSoft: Diversified Language Model Watermarking via the\n  GumbelMax-trick", "abstract": "Large language models (LLMs) excellently generate human-like text, but also\nraise concerns about misuse in fake news and academic dishonesty.\nDecoding-based watermark, particularly the GumbelMax-trick-based watermark(GM\nwatermark), is a standout solution for safeguarding machine-generated texts due\nto its notable detectability. However, GM watermark encounters a major\nchallenge with generation diversity, always yielding identical outputs for the\nsame prompt, negatively impacting generation diversity and user experience. To\novercome this limitation, we propose a new type of GM watermark, the\nLogits-Addition watermark, and its three variants, specifically designed to\nenhance diversity. Among these, the GumbelSoft watermark (a softmax variant of\nthe Logits-Addition watermark) demonstrates superior performance in high\ndiversity settings, with its AUROC score outperforming those of the two\nalternative variants by 0.1 to 0.3 and surpassing other decoding-based\nwatermarking methods by a minimum of 0.1.", "published": "2024-02-20 12:05:47", "link": "http://arxiv.org/abs/2402.12948v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Phonotactic Complexity across Dialects", "abstract": "Received wisdom in linguistic typology holds that if the structure of a\nlanguage becomes more complex in one dimension, it will simplify in another,\nbuilding on the assumption that all languages are equally complex (Joseph and\nNewmeyer, 2012). We study this claim on a micro-level, using a\ntightly-controlled sample of Dutch dialects (across 366 collection sites) and\nMin dialects (across 60 sites), which enables a more fair comparison across\nvarieties. Even at the dialect level, we find empirical evidence for a tradeoff\nbetween word length and a computational measure of phonotactic complexity from\na LSTM-based phone-level language model-a result previously documented only at\nthe language level. A generalized additive model (GAM) shows that dialects with\nlow phonotactic complexity concentrate around the capital regions, which we\nhypothesize to correspond to prior hypotheses that language varieties of\ngreater or more diverse populations show reduced phonotactic complexity. We\nalso experiment with incorporating the auxiliary task of predicting syllable\nconstituency, but do not find an increase in the negative correlation observed.", "published": "2024-02-20 13:25:39", "link": "http://arxiv.org/abs/2402.12998v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Code Needs Comments: Enhancing Code LLMs with Comment Augmentation", "abstract": "The programming skill is one crucial ability for Large Language Models\n(LLMs), necessitating a deep understanding of programming languages (PLs) and\ntheir correlation with natural languages (NLs). We examine the impact of\npre-training data on code-focused LLMs' performance by assessing the comment\ndensity as a measure of PL-NL alignment. Given the scarcity of code-comment\naligned data in pre-training corpora, we introduce a novel data augmentation\nmethod that generates comments for existing code, coupled with a data filtering\nstrategy that filters out code data poorly correlated with natural language. We\nconducted experiments on three code-focused LLMs and observed consistent\nimprovements in performance on two widely-used programming skill benchmarks.\nNotably, the model trained on the augmented data outperformed both the model\nused for generating comments and the model further trained on the data without\naugmentation.", "published": "2024-02-20 13:56:38", "link": "http://arxiv.org/abs/2402.13013v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding the effects of language-specific class imbalance in\n  multilingual fine-tuning", "abstract": "We study the effect of one type of imbalance often present in real-life\nmultilingual classification datasets: an uneven distribution of labels across\nlanguages. We show evidence that fine-tuning a transformer-based Large Language\nModel (LLM) on a dataset with this imbalance leads to worse performance, a more\npronounced separation of languages in the latent space, and the promotion of\nuninformative features. We modify the traditional class weighing approach to\nimbalance by calculating class weights separately for each language and show\nthat this helps mitigate those detrimental effects. These results create\nawareness of the negative effects of language-specific class imbalance in\nmultilingual fine-tuning and the way in which the model learns to rely on the\nseparation of languages to perform the task.", "published": "2024-02-20 13:59:12", "link": "http://arxiv.org/abs/2402.13016v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SiLLM: Large Language Models for Simultaneous Machine Translation", "abstract": "Simultaneous Machine Translation (SiMT) generates translations while reading\nthe source sentence, necessitating a policy to determine the optimal timing for\nreading and generating words. Despite the remarkable performance achieved by\nLarge Language Models (LLM) across various NLP tasks, existing SiMT methods\npredominantly focus on conventional transformers, employing a single model to\nconcurrently determine the policy and generate the translations. However, given\nthe complexity of SiMT, it is challenging to effectively address both tasks\nwith a single model. Therefore, there is a need to decouple the SiMT task into\npolicy-decision and translation sub-tasks. We propose SiLLM, which delegates\nthe two sub-tasks to separate agents, thereby incorporating LLM into SiMT. The\npolicy-decision agent is managed by a conventional SiMT model, responsible for\ndetermining the translation policy. The translation agent, leveraging the\ncapabilities of LLM, generates translation using the partial source sentence.\nThe two agents collaborate to accomplish SiMT. To facilitate the application of\ntoken-level policies determined by conventional SiMT models to LLM, we propose\na word-level policy adapted for LLM. Experiments on two datasets demonstrate\nthat, with a small amount of data for fine-tuning LLM, SiLLM attains\nstate-of-the-art performance.", "published": "2024-02-20 14:23:34", "link": "http://arxiv.org/abs/2402.13036v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effective and Efficient Conversation Retrieval for Dialogue State\n  Tracking with Implicit Text Summaries", "abstract": "Few-shot dialogue state tracking (DST) with Large Language Models (LLM)\nrelies on an effective and efficient conversation retriever to find similar\nin-context examples for prompt learning. Previous works use raw dialogue\ncontext as search keys and queries, and a retriever is fine-tuned with\nannotated dialogues to achieve superior performance. However, the approach is\nless suited for scaling to new domains or new annotation languages, where\nfine-tuning data is unavailable. To address this problem, we handle the task of\nconversation retrieval based on text summaries of the conversations. A\nLLM-based conversation summarizer is adopted for query and key generation,\nwhich enables effective maximum inner product search. To avoid the extra\ninference cost brought by LLM-based conversation summarization, we further\ndistill a light-weight conversation encoder which produces query embeddings\nwithout decoding summaries for test conversations. We validate our retrieval\napproach on MultiWOZ datasets with GPT-Neo-2.7B and LLaMA-7B/30B. The\nexperimental results show a significant improvement over relevant baselines in\nreal few-shot DST settings.", "published": "2024-02-20 14:31:17", "link": "http://arxiv.org/abs/2402.13043v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stable Knowledge Editing in Large Language Models", "abstract": "Efficient knowledge editing of large language models is crucial for replacing\nobsolete information or incorporating specialized knowledge on a large scale.\nHowever, previous methods implicitly assume that knowledge is localized and\nisolated within the model, an assumption that oversimplifies the interconnected\nnature of model knowledge. The premise of localization results in an incomplete\nknowledge editing, whereas an isolated assumption may impair both other\nknowledge and general abilities. It introduces instability to the performance\nof the knowledge editing method. To transcend these assumptions, we introduce\nStableKE, a method adopts a novel perspective based on knowledge augmentation\nrather than knowledge localization. To overcome the expense of human labeling,\nStableKE integrates two automated knowledge augmentation strategies: Semantic\nParaphrase Enhancement strategy, which diversifies knowledge descriptions to\nfacilitate the teaching of new information to the model, and Contextual\nDescription Enrichment strategy, expanding the surrounding knowledge to prevent\nthe forgetting of related information. StableKE surpasses other knowledge\nediting methods, demonstrating stability both edited knowledge and multi-hop\nknowledge, while also preserving unrelated knowledge and general abilities.\nMoreover, StableKE can edit knowledge on ChatGPT.", "published": "2024-02-20 14:36:23", "link": "http://arxiv.org/abs/2402.13048v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for\n  Language Models", "abstract": "We introduce Generalized Instruction Tuning (called GLAN), a general and\nscalable method for instruction tuning of Large Language Models (LLMs). Unlike\nprior work that relies on seed examples or existing datasets to construct\ninstruction tuning data, GLAN exclusively utilizes a pre-curated taxonomy of\nhuman knowledge and capabilities as input and generates large-scale synthetic\ninstruction data across all disciplines. Specifically, inspired by the\nsystematic structure in human education system, we build the taxonomy by\ndecomposing human knowledge and capabilities to various fields, sub-fields and\nultimately, distinct disciplines semi-automatically, facilitated by LLMs.\nSubsequently, we generate a comprehensive list of subjects for every discipline\nand proceed to design a syllabus tailored to each subject, again utilizing\nLLMs. With the fine-grained key concepts detailed in every class session of the\nsyllabus, we are able to generate diverse instructions with a broad coverage\nacross the entire spectrum of human knowledge and skills. Extensive experiments\non large language models (e.g., Mistral) demonstrate that GLAN excels in\nmultiple dimensions from mathematical reasoning, coding, academic exams,\nlogical reasoning to general instruction following without using task-specific\ntraining data of these tasks. In addition, GLAN allows for easy customization\nand new fields or skills can be added by simply incorporating a new node into\nour taxonomy.", "published": "2024-02-20 15:00:35", "link": "http://arxiv.org/abs/2402.13064v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When Only Time Will Tell: Interpreting How Transformers Process Local\n  Ambiguities Through the Lens of Restart-Incrementality", "abstract": "Incremental models that process sentences one token at a time will sometimes\nencounter points where more than one interpretation is possible. Causal models\nare forced to output one interpretation and continue, whereas models that can\nrevise may edit their previous output as the ambiguity is resolved. In this\nwork, we look at how restart-incremental Transformers build and update internal\nstates, in an effort to shed light on what processes cause revisions not viable\nin autoregressive models. We propose an interpretable way to analyse the\nincremental states, showing that their sequential structure encodes information\non the garden path effect and its resolution. Our method brings insights on\nvarious bidirectional encoders for contextualised meaning representation and\ndependency parsing, contributing to show their advantage over causal models\nwhen it comes to revisions.", "published": "2024-02-20 16:09:49", "link": "http://arxiv.org/abs/2402.13113v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Knowledge Distillation of Large Language Models", "abstract": "In the era of Large Language Models (LLMs), Knowledge Distillation (KD)\nemerges as a pivotal methodology for transferring advanced capabilities from\nleading proprietary LLMs, such as GPT-4, to their open-source counterparts like\nLLaMA and Mistral. Additionally, as open-source LLMs flourish, KD plays a\ncrucial role in both compressing these models, and facilitating their\nself-improvement by employing themselves as teachers. This paper presents a\ncomprehensive survey of KD's role within the realm of LLM, highlighting its\ncritical function in imparting advanced knowledge to smaller models and its\nutility in model compression and self-improvement. Our survey is meticulously\nstructured around three foundational pillars: \\textit{algorithm},\n\\textit{skill}, and \\textit{verticalization} -- providing a comprehensive\nexamination of KD mechanisms, the enhancement of specific cognitive abilities,\nand their practical implications across diverse fields. Crucially, the survey\nnavigates the intricate interplay between data augmentation (DA) and KD,\nillustrating how DA emerges as a powerful paradigm within the KD framework to\nbolster LLMs' performance. By leveraging DA to generate context-rich,\nskill-specific training data, KD transcends traditional boundaries, enabling\nopen-source models to approximate the contextual adeptness, ethical alignment,\nand deep semantic insights characteristic of their proprietary counterparts.\nThis work aims to provide an insightful guide for researchers and\npractitioners, offering a detailed overview of current methodologies in KD and\nproposing future research directions. Importantly, we firmly advocate for\ncompliance with the legal terms that regulate the use of LLMs, ensuring ethical\nand lawful application of KD of LLMs. An associated Github repository is\navailable at https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs.", "published": "2024-02-20 16:17:37", "link": "http://arxiv.org/abs/2402.13116v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic\n  Textual Similarity", "abstract": "While BERT produces high-quality sentence embeddings, its pre-training\ncomputational cost is a significant drawback. In contrast, ELECTRA provides a\ncost-effective pre-training objective and downstream task performance\nimprovements, but worse sentence embeddings. The community tacitly stopped\nutilizing ELECTRA's sentence embeddings for semantic textual similarity (STS).\nWe notice a significant drop in performance for the ELECTRA discriminator's\nlast layer in comparison to prior layers. We explore this drop and propose a\nway to repair the embeddings using a novel truncated model fine-tuning (TMFT)\nmethod. TMFT improves the Spearman correlation coefficient by over $8$ points\nwhile increasing parameter efficiency on the STS Benchmark. We extend our\nanalysis to various model sizes, languages, and two other tasks. Further, we\ndiscover the surprising efficacy of ELECTRA's generator model, which performs\non par with BERT, using significantly fewer parameters and a substantially\nsmaller embedding size. Finally, we observe boosts by combining TMFT with word\nsimilarity or domain adaptive pre-training.", "published": "2024-02-20 16:43:20", "link": "http://arxiv.org/abs/2402.13130v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Hidden Space of Transformer Language Adapters", "abstract": "We analyze the operation of transformer language adapters, which are small\nmodules trained on top of a frozen language model to adapt its predictions to\nnew target languages. We show that adapted predictions mostly evolve in the\nsource language the model was trained on, while the target language becomes\npronounced only in the very last layers of the model. Moreover, the adaptation\nprocess is gradual and distributed across layers, where it is possible to skip\nsmall groups of adapters without decreasing adaptation performance. Last, we\nshow that adapters operate on top of the model's frozen representation space\nwhile largely preserving its structure, rather than on an 'isolated' subspace.\nOur findings provide a deeper view into the adaptation process of language\nmodels to new languages, showcasing the constraints imposed on it by the\nunderlying model and introduces practical implications to enhance its\nefficiency.", "published": "2024-02-20 16:53:26", "link": "http://arxiv.org/abs/2402.13137v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What if LLMs Have Different World Views: Simulating Alien Civilizations\n  with LLM-based Agents", "abstract": "This study introduces \"CosmoAgent,\" an innovative artificial intelligence\nsystem that utilizes Large Language Models (LLMs) to simulate complex\ninteractions between human and extraterrestrial civilizations. This paper\nintroduces a mathematical model for quantifying the levels of civilization\ndevelopment and further employs a state transition matrix approach to evaluate\ntheir trajectories. Through this methodology, our study quantitatively analyzes\nthe growth trajectories of civilizations, providing insights into future\ndecision-making at critical points of growth and saturation. Furthermore, this\npaper acknowledges the vast diversity of potential living conditions across the\nuniverse, which could foster unique cosmologies, ethical codes, and worldviews\namong different civilizations. Recognizing the Earth-centric bias inherent in\ncurrent LLM designs, we propose the novel concept of using LLM agents with\ndiverse ethical paradigms and simulating interactions between entities with\ndistinct moral principles. This innovative research not only introduces a novel\nmethod for comprehending potential inter-civilizational dynamics but also holds\npractical value in enabling entities with divergent value systems to\nstrategize, prevent conflicts, and engage in games under conditions of\nasymmetric information. The accompanying code is available at\nhttps://github.com/MingyuJ666/Simulating-Alien-Civilizations-with-LLM-based-Agents.", "published": "2024-02-20 17:49:46", "link": "http://arxiv.org/abs/2402.13184v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question Calibration and Multi-Hop Modeling for Temporal Question\n  Answering", "abstract": "Many models that leverage knowledge graphs (KGs) have recently demonstrated\nremarkable success in question answering (QA) tasks. In the real world, many\nfacts contained in KGs are time-constrained thus temporal KGQA has received\nincreasing attention. Despite the fruitful efforts of previous models in\ntemporal KGQA, they still have several limitations. (I) They adopt pre-trained\nlanguage models (PLMs) to obtain question representations, while PLMs tend to\nfocus on entity information and ignore entity transfer caused by temporal\nconstraints, and finally fail to learn specific temporal representations of\nentities. (II) They neither emphasize the graph structure between entities nor\nexplicitly model the multi-hop relationship in the graph, which will make it\ndifficult to solve complex multi-hop question answering. To alleviate this\nproblem, we propose a novel Question Calibration and Multi-Hop Modeling\n(QC-MHM) approach. Specifically, We first calibrate the question representation\nby fusing the question and the time-constrained concepts in KG. Then, we\nconstruct the GNN layer to complete multi-hop message passing. Finally, the\nquestion representation is combined with the embedding output by the GNN to\ngenerate the final prediction. Empirical results verify that the proposed model\nachieves better performance than the state-of-the-art models in the benchmark\ndataset. Notably, the Hits@1 and Hits@10 results of QC-MHM on the CronQuestions\ndataset's complex questions are absolutely improved by 5.1% and 1.2% compared\nto the best-performing baseline. Moreover, QC-MHM can generate interpretable\nand trustworthy predictions.", "published": "2024-02-20 17:56:24", "link": "http://arxiv.org/abs/2402.13188v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models be Good Emotional Supporter? Mitigating\n  Preference Bias on Emotional Support Conversation", "abstract": "Emotional Support Conversation (ESC) is a task aimed at alleviating\nindividuals' emotional distress through daily conversation. Given its inherent\ncomplexity and non-intuitive nature, ESConv dataset incorporates support\nstrategies to facilitate the generation of appropriate responses. Recently,\ndespite the remarkable conversational ability of large language models (LLMs),\nprevious studies have suggested that they often struggle with providing useful\nemotional support. Hence, this work initially analyzes the results of LLMs on\nESConv, revealing challenges in selecting the correct strategy and a notable\npreference for a specific strategy. Motivated by these, we explore the impact\nof the inherent preference in LLMs on providing emotional support, and\nconsequently, we observe that exhibiting high preference for specific\nstrategies hinders effective emotional support, aggravating its robustness in\npredicting the appropriate strategy. Moreover, we conduct a methodological\nstudy to offer insights into the necessary approaches for LLMs to serve as\nproficient emotional supporters. Our findings emphasize that (1) low preference\nfor specific strategies hinders the progress of emotional support, (2) external\nassistance helps reduce preference bias, and (3) existing LLMs alone cannot\nbecome good emotional supporters. These insights suggest promising avenues for\nfuture research to enhance the emotional intelligence of LLMs.", "published": "2024-02-20 18:21:32", "link": "http://arxiv.org/abs/2402.13211v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RoCode: A Dataset for Measuring Code Intelligence from Problem\n  Definitions in Romanian", "abstract": "Recently, large language models (LLMs) have become increasingly powerful and\nhave become capable of solving a plethora of tasks through proper instructions\nin natural language. However, the vast majority of testing suites assume that\nthe instructions are written in English, the de facto prompting language. Code\nintelligence and problem solving still remain a difficult task, even for the\nmost advanced LLMs. Currently, there are no datasets to measure the\ngeneralization power for code-generation models in a language other than\nEnglish. In this work, we present RoCode, a competitive programming dataset,\nconsisting of 2,642 problems written in Romanian, 11k solutions in C, C++ and\nPython and comprehensive testing suites for each problem. The purpose of RoCode\nis to provide a benchmark for evaluating the code intelligence of language\nmodels trained on Romanian / multilingual text as well as a fine-tuning set for\npretrained Romanian models. Through our results and review of related works, we\nargue for the need to develop code models for languages other than English.", "published": "2024-02-20 18:32:47", "link": "http://arxiv.org/abs/2402.13222v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BiMediX: Bilingual Medical Mixture of Experts LLM", "abstract": "In this paper, we introduce BiMediX, the first bilingual medical mixture of\nexperts LLM designed for seamless interaction in both English and Arabic. Our\nmodel facilitates a wide range of medical interactions in English and Arabic,\nincluding multi-turn chats to inquire about additional details such as patient\nsymptoms and medical history, multiple-choice question answering, and\nopen-ended question answering. We propose a semi-automated English-to-Arabic\ntranslation pipeline with human refinement to ensure high-quality translations.\nWe also introduce a comprehensive evaluation benchmark for Arabic medical LLMs.\nFurthermore, we introduce BiMed1.3M, an extensive Arabic-English bilingual\ninstruction set covering 1.3 Million diverse medical interactions, resulting in\nover 632 million healthcare specialized tokens for instruction tuning. Our\nBiMed1.3M dataset includes 250k synthesized multi-turn doctor-patient chats and\nmaintains a 1:2 Arabic-to-English ratio. Our model outperforms state-of-the-art\nMed42 and Meditron by average absolute gains of 2.5% and 4.1%, respectively,\ncomputed across multiple medical evaluation benchmarks in English, while\noperating at 8-times faster inference. Moreover, our BiMediX outperforms the\ngeneric Arabic-English bilingual LLM, Jais-30B, by average absolute gains of\n10% on our Arabic medical benchmark and 15% on bilingual evaluations across\nmultiple datasets. Our project page with source code and trained model is\navailable at https://github.com/mbzuai-oryx/BiMediX .", "published": "2024-02-20 18:59:26", "link": "http://arxiv.org/abs/2402.13253v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Modern Supervised Word Sense Disambiguation Models by Semantic\n  Lexical Resources", "abstract": "Supervised models for Word Sense Disambiguation (WSD) currently yield to\nstate-of-the-art results in the most popular benchmarks. Despite the recent\nintroduction of Word Embeddings and Recurrent Neural Networks to design\npowerful context-related features, the interest in improving WSD models using\nSemantic Lexical Resources (SLRs) is mostly restricted to knowledge-based\napproaches. In this paper, we enhance \"modern\" supervised WSD models exploiting\ntwo popular SLRs: WordNet and WordNet Domains. We propose an effective way to\nintroduce semantic features into the classifiers, and we consider using the SLR\nstructure to augment the training data. We study the effect of different types\nof semantic features, investigating their interaction with local contexts\nencoded by means of mixtures of Word Embeddings or Recurrent Neural Networks,\nand we extend the proposed model into a novel multi-layer architecture for WSD.\nA detailed experimental comparison in the recent Unified Evaluation Framework\n(Raganato et al., 2017) shows that the proposed approach leads to supervised\nmodels that compare favourably with the state-of-the art.", "published": "2024-02-20 13:47:51", "link": "http://arxiv.org/abs/2402.13302v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhanced Hallucination Detection in Neural Machine Translation through\n  Simple Detector Aggregation", "abstract": "Hallucinated translations pose significant threats and safety concerns when\nit comes to the practical deployment of machine translation systems. Previous\nresearch works have identified that detectors exhibit complementary performance\ndifferent detectors excel at detecting different types of hallucinations. In\nthis paper, we propose to address the limitations of individual detectors by\ncombining them and introducing a straightforward method for aggregating\nmultiple detectors. Our results demonstrate the efficacy of our aggregated\ndetector, providing a promising step towards evermore reliable machine\ntranslation systems.", "published": "2024-02-20 19:19:47", "link": "http://arxiv.org/abs/2402.13331v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PIRB: A Comprehensive Benchmark of Polish Dense and Hybrid Text\n  Retrieval Methods", "abstract": "We present Polish Information Retrieval Benchmark (PIRB), a comprehensive\nevaluation framework encompassing 41 text information retrieval tasks for\nPolish. The benchmark incorporates existing datasets as well as 10 new,\npreviously unpublished datasets covering diverse topics such as medicine, law,\nbusiness, physics, and linguistics. We conduct an extensive evaluation of over\n20 dense and sparse retrieval models, including the baseline models trained by\nus as well as other available Polish and multilingual methods. Finally, we\nintroduce a three-step process for training highly effective language-specific\nretrievers, consisting of knowledge distillation, supervised fine-tuning, and\nbuilding sparse-dense hybrid retrievers using a lightweight rescoring model. In\norder to validate our approach, we train new text encoders for Polish and\ncompare their results with previously evaluated methods. Our dense models\noutperform the best solutions available to date, and the use of hybrid methods\nfurther improves their performance.", "published": "2024-02-20 19:53:36", "link": "http://arxiv.org/abs/2402.13350v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EvoGrad: A Dynamic Take on the Winograd Schema Challenge with Human\n  Adversaries", "abstract": "While Large Language Models (LLMs) excel at the Winograd Schema Challenge\n(WSC), a coreference resolution task testing common-sense reasoning through\npronoun disambiguation, they struggle with instances that feature minor\nalterations or rewording. To address this, we introduce EvoGrad, an open-source\nplatform that harnesses a human-in-the-loop approach to create a dynamic\ndataset tailored to such altered WSC instances. Leveraging ChatGPT's\ncapabilities, we expand our task instances from 182 to 3,691, setting a new\nbenchmark for diverse common-sense reasoning datasets. Additionally, we\nintroduce the error depth metric, assessing model stability in dynamic tasks.\nOur results emphasize the challenge posed by EvoGrad: Even the best performing\nLLM, GPT-3.5, achieves an accuracy of 65.0% with an average error depth of 7.2,\na stark contrast to human performance of 92. 8% accuracy without perturbation\nerrors. This highlights ongoing model limitations and the value of dynamic\ndatasets in uncovering them.", "published": "2024-02-20 20:53:24", "link": "http://arxiv.org/abs/2402.13372v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reliable LLM-based User Simulator for Task-Oriented Dialogue Systems", "abstract": "In the realm of dialogue systems, user simulation techniques have emerged as\na game-changer, redefining the evaluation and enhancement of task-oriented\ndialogue (TOD) systems. These methods are crucial for replicating real user\ninteractions, enabling applications like synthetic data augmentation, error\ndetection, and robust evaluation. However, existing approaches often rely on\nrigid rule-based methods or on annotated data. This paper introduces DAUS, a\nDomain-Aware User Simulator. Leveraging large language models, we fine-tune\nDAUS on real examples of task-oriented dialogues. Results on two relevant\nbenchmarks showcase significant improvements in terms of user goal fulfillment.\nNotably, we have observed that fine-tuning enhances the simulator's coherence\nwith user goals, effectively mitigating hallucinations -- a major source of\ninconsistencies in simulator responses.", "published": "2024-02-20 20:57:47", "link": "http://arxiv.org/abs/2402.13374v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Unified Taxonomy-Guided Instruction Tuning Framework for Entity Set\n  Expansion and Taxonomy Expansion", "abstract": "Entity set expansion, taxonomy expansion, and seed-guided taxonomy\nconstruction are three representative tasks that can be applied to\nautomatically populate an existing taxonomy with emerging concepts. Previous\nstudies view them as three separate tasks. Therefore, their proposed techniques\nusually work for one specific task only, lacking generalizability and a\nholistic perspective. In this paper, we aim at a unified solution to the three\ntasks. To be specific, we identify two common skills needed for entity set\nexpansion, taxonomy expansion, and seed-guided taxonomy construction: finding\n\"siblings\" and finding \"parents\". We propose a taxonomy-guided instruction\ntuning framework to teach a large language model to generate siblings and\nparents for query entities, where the joint pre-training process facilitates\nthe mutual enhancement of the two skills. Extensive experiments on multiple\nbenchmark datasets demonstrate the efficacy of our proposed TaxoInstruct\nframework, which outperforms task-specific baselines across all three tasks.", "published": "2024-02-20 22:19:56", "link": "http://arxiv.org/abs/2402.13405v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Healthcare Copilot: Eliciting the Power of General LLMs for Medical\n  Consultation", "abstract": "The copilot framework, which aims to enhance and tailor large language models\n(LLMs) for specific complex tasks without requiring fine-tuning, is gaining\nincreasing attention from the community. In this paper, we introduce the\nconstruction of a Healthcare Copilot designed for medical consultation. The\nproposed Healthcare Copilot comprises three main components: 1) the Dialogue\ncomponent, responsible for effective and safe patient interactions; 2) the\nMemory component, storing both current conversation data and historical patient\ninformation; and 3) the Processing component, summarizing the entire dialogue\nand generating reports. To evaluate the proposed Healthcare Copilot, we\nimplement an auto-evaluation scheme using ChatGPT for two roles: as a virtual\npatient engaging in dialogue with the copilot, and as an evaluator to assess\nthe quality of the dialogue. Extensive results demonstrate that the proposed\nHealthcare Copilot significantly enhances the capabilities of general LLMs for\nmedical consultations in terms of inquiry capability, conversational fluency,\nresponse accuracy, and safety. Furthermore, we conduct ablation studies to\nhighlight the contribution of each individual module in the Healthcare Copilot.\nCode will be made publicly available on GitHub.", "published": "2024-02-20 22:26:35", "link": "http://arxiv.org/abs/2402.13408v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structure Guided Prompt: Instructing Large Language Model in Multi-Step\n  Reasoning by Exploring Graph Structure of the Text", "abstract": "Although Large Language Models (LLMs) excel at addressing straightforward\nreasoning tasks, they frequently struggle with difficulties when confronted by\nmore complex multi-step reasoning due to a range of factors. Firstly, natural\nlanguage often encompasses complex relationships among entities, making it\nchallenging to maintain a clear reasoning chain over longer spans. Secondly,\nthe abundance of linguistic diversity means that the same entities and\nrelationships can be expressed using different terminologies and structures,\ncomplicating the task of identifying and establishing connections between\nmultiple pieces of information. Graphs provide an effective solution to\nrepresent data rich in relational information and capture long-term\ndependencies among entities. To harness the potential of graphs, our paper\nintroduces Structure Guided Prompt, an innovative three-stage task-agnostic\nprompting framework designed to improve the multi-step reasoning capabilities\nof LLMs in a zero-shot setting. This framework explicitly converts unstructured\ntext into a graph via LLMs and instructs them to navigate this graph using\ntask-specific strategies to formulate responses. By effectively organizing\ninformation and guiding navigation, it enables LLMs to provide more accurate\nand context-aware responses. Our experiments show that this framework\nsignificantly enhances the reasoning capabilities of LLMs, enabling them to\nexcel in a broader spectrum of natural language scenarios.", "published": "2024-02-20 22:56:23", "link": "http://arxiv.org/abs/2402.13415v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explaining Relationships Among Research Papers", "abstract": "Due to the rapid pace of research publications, keeping up to date with all\nthe latest related papers is very time-consuming, even with daily feed tools.\nThere is a need for automatically generated, short, customized literature\nreviews of sets of papers to help researchers decide what to read. While\nseveral works in the last decade have addressed the task of explaining a single\nresearch paper, usually in the context of another paper citing it, the\nrelationship among multiple papers has been ignored; prior works have focused\non generating a single citation sentence in isolation, without addressing the\nexpository and transition sentences needed to connect multiple papers in a\ncoherent story. In this work, we explore a feature-based, LLM-prompting\napproach to generate richer citation texts, as well as generating multiple\ncitations at once to capture the complex relationships among research papers.\nWe perform an expert evaluation to investigate the impact of our proposed\nfeatures on the quality of the generated paragraphs and find a strong\ncorrelation between human preference and integrative writing style, suggesting\nthat humans prefer high-level, abstract citations, with transition sentences\nbetween them to provide an overall story.", "published": "2024-02-20 23:38:39", "link": "http://arxiv.org/abs/2402.13426v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reflect-RL: Two-Player Online RL Fine-Tuning for LMs", "abstract": "As language models (LMs) demonstrate their capabilities in various fields,\ntheir application to tasks requiring multi-round interactions has become\nincreasingly popular. These tasks usually have complex dynamics, so supervised\nfine-tuning (SFT) on a limited offline dataset does not yield good performance.\nHowever, only a few works attempted to directly train the LMs within\ninteractive decision-making environments. We aim to create an effective\napproach to fine-tune LMs with online reinforcement learning (RL) in these\nenvironments. We propose Reflect-RL, a two-player system to fine-tune an LM\nusing SFT and online RL, where a frozen reflection model (player) assists the\npolicy model (player). To generate data for the warm-up SFT stage, we use\nnegative example generation to enhance the error-correction ability of the\nreflection model. Furthermore, we designed single-prompt action enumeration and\napplied curriculum learning to allow the policy model to learn more\nefficiently. Empirically, we verify that Reflect-RL outperforms SFT and online\nRL without reflection. Testing results indicate GPT-2 XL 1.56B fine-tuned with\nReflect-RL outperforms larger open-source LMs, such as Mistral 7B. The\nbenchmarks, dataset, and code involved in this work are publicly available:\nhttps://github.com/zhourunlong/Reflect-RL.", "published": "2024-02-20 01:04:21", "link": "http://arxiv.org/abs/2402.12621v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation", "abstract": "Standard benchmarks of bias and fairness in large language models (LLMs)\nmeasure the association between social attributes implied in user prompts and\nshort LLM responses. In the commonly studied domain of gender-occupation bias,\nwe test whether these benchmarks are robust to lengthening the LLM responses as\na measure of Realistic Use and Tangible Effects (i.e., RUTEd evaluations). From\nthe current literature, we adapt three standard bias metrics (neutrality, skew,\nand stereotype), and we develop analogous RUTEd evaluations from three contexts\nof real-world use: children's bedtime stories, user personas, and English\nlanguage learning exercises. We find that standard bias metrics have no\nsignificant correlation with the more realistic bias metrics. For example,\nselecting the least biased model based on the standard \"trick tests\" coincides\nwith selecting the least biased model as measured in more realistic use no more\nthan random chance. We suggest that there is not yet evidence to justify\nstandard benchmarks as reliable proxies of real-world biases, and we encourage\nfurther development of context-specific RUTEd evaluations.", "published": "2024-02-20 01:49:15", "link": "http://arxiv.org/abs/2402.12649v2", "categories": ["cs.CL", "stat.AP"], "primary_category": "cs.CL"}
{"title": "Me LLaMA: Foundation Large Language Models for Medical Applications", "abstract": "Recent advancements in large language models (LLMs) like ChatGPT and LLaMA\nshow promise in medical applications, yet challenges remain in medical language\ncomprehension. This study presents Me-LLaMA, a new medical LLM family based on\nopen-source LLaMA models, optimized for medical text analysis and diagnosis by\nleveraging large-scale, domain-specific datasets. The Me-LLaMA family,\nincluding foundation models Me-LLaMA 13/70B and their chat-enhanced versions,\nwas developed through continued pre-training and instruction tuning with 129B\ntokens and 214K samples from biomedical and clinical sources. Training the 70B\nmodels required over 100,000 A100 GPU hours. Me-LLaMA's performance was\nevaluated across six medical text analysis tasks using 12 benchmark datasets\nand complex clinical case diagnosis, with automatic and human evaluations.\nResults indicate Me-LLaMA outperforms LLaMA and other open-source medical LLMs\nin zero-shot and supervised settings. Task-specific tuning further boosts\nperformance, surpassing ChatGPT on 7 of 8 datasets and GPT-4 on 5 of 8. For\ncomplex clinical cases, Me-LLaMA achieves performance comparable to ChatGPT and\nGPT-4. This work underscores the importance of domain-specific data in\ndeveloping medical LLMs and addresses the high computational costs involved in\ntraining, highlighting a balance between pre-training and fine-tuning\nstrategies. Me-LLaMA models are now accessible under user agreements, providing\na valuable resource for advancing medical AI.", "published": "2024-02-20 06:37:31", "link": "http://arxiv.org/abs/2402.12749v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Understanding and Mitigating the Threat of Vec2Text to Dense Retrieval\n  Systems", "abstract": "The emergence of Vec2Text -- a method for text embedding inversion -- has\nraised serious privacy concerns for dense retrieval systems which use text\nembeddings, such as those offered by OpenAI and Cohere. This threat comes from\nthe ability for a malicious attacker with access to embeddings to reconstruct\nthe original text. In this paper, we investigate various factors related to\nembedding models that may impact text recoverability via Vec2Text. We explore\nfactors such as distance metrics, pooling functions, bottleneck pre-training,\ntraining with noise addition, embedding quantization, and embedding dimensions,\nwhich were not considered in the original Vec2Text paper. Through a\ncomprehensive analysis of these factors, our objective is to gain a deeper\nunderstanding of the key elements that affect the trade-offs between the text\nrecoverability and retrieval effectiveness of dense retrieval systems, offering\ninsights for practitioners designing privacy-aware dense retrieval systems. We\nalso propose a simple embedding transformation fix that guarantees equal\nranking effectiveness while mitigating the recoverability risk. Overall, this\nstudy reveals that Vec2Text could pose a threat to current dense retrieval\nsystems, but there are some effective methods to patch such systems.", "published": "2024-02-20 07:49:30", "link": "http://arxiv.org/abs/2402.12784v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Advancing Large Language Models to Capture Varied Speaking Styles and\n  Respond Properly in Spoken Conversations", "abstract": "In spoken dialogue, even if two current turns are the same sentence, their\nresponses might still differ when they are spoken in different styles. The\nspoken styles, containing paralinguistic and prosodic information, mark the\nmost significant difference between text and speech modality. When using\ntext-only LLMs to model spoken dialogue, text-only LLMs cannot give different\nresponses based on the speaking style of the current turn. In this paper, we\nfocus on enabling LLMs to listen to the speaking styles and respond properly.\nOur goal is to teach the LLM that \"even if the sentences are identical if they\nare spoken in different styles, their corresponding responses might be\ndifferent\". Since there is no suitable dataset for achieving this goal, we\ncollect a speech-to-speech dataset, StyleTalk, with the following desired\ncharacteristics: when two current speeches have the same content but are spoken\nin different styles, their responses will be different. To teach LLMs to\nunderstand and respond properly to the speaking styles, we propose the\nSpoken-LLM framework that can model the linguistic content and the speaking\nstyles. We train Spoken-LLM using the StyleTalk dataset and devise a two-stage\ntraining pipeline to help the Spoken-LLM better learn the speaking styles.\nBased on extensive experiments, we show that Spoken-LLM outperforms text-only\nbaselines and prior speech LLMs methods.", "published": "2024-02-20 07:51:43", "link": "http://arxiv.org/abs/2402.12786v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Identifying Factual Inconsistencies in Summaries: Grounding LLM\n  Inference via Task Taxonomy", "abstract": "Factual inconsistencies pose a significant hurdle for the faithful\nsummarization by generative models. While a major direction to enhance\ninconsistency detection is to derive stronger Natural Language Inference (NLI)\nmodels, we propose an orthogonal aspect that underscores the importance of\nincorporating task-specific taxonomy into the inference. To this end, we\nconsolidate key error types of inconsistent facts in summaries, and incorporate\nthem to facilitate both the zero-shot and supervised paradigms of LLMs.\nExtensive experiments on ten datasets of five distinct domains suggest that,\nzero-shot LLM inference could benefit from the explicit solution space depicted\nby the error type taxonomy, and achieves state-of-the-art performance overall,\nsurpassing specialized non-LLM baselines, as well as recent LLM baselines. We\nfurther distill models that fuse the taxonomy into parameters through our\ndesigned prompt completions and supervised training strategies, efficiently\nsubstituting state-of-the-art zero-shot inference with much larger LLMs.", "published": "2024-02-20 08:41:23", "link": "http://arxiv.org/abs/2402.12821v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PANDA: Preference Adaptation for Enhancing Domain-Specific Abilities of\n  LLMs", "abstract": "While Large language models (LLMs) have demonstrated considerable\ncapabilities across various natural language tasks, they often fall short of\nthe performance achieved by domain-specific state-of-the-art models. One\npotential approach to enhance domain-specific capabilities of LLMs involves\nfine-tuning them using corresponding datasets. However, this method can be both\nresource and time-intensive, and not applicable to closed-source commercial\nLLMs. In this paper, we propose Preference Adaptation for Enhancing\nDomain-specific Abilities of LLMs (PANDA), a method designed to augment the\ndomain-specific capabilities of LLMs by leveraging insights from the response\npreference of expert models without requiring fine-tuning. Our experimental\nresults reveal that PANDA significantly enhances the domain-specific ability of\nLLMs on text classification and interactive decision tasks. Moreover, LLM with\nPANDA even outperforms the expert model that being learned on 4 tasks of\nScienceWorld. This finding highlights the potential of exploring tuning-free\napproaches to achieve weak-to-strong generalization.", "published": "2024-02-20 09:02:55", "link": "http://arxiv.org/abs/2402.12835v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ICON: Improving Inter-Report Consistency in Radiology Report Generation\n  via Lesion-aware Mixup Augmentation", "abstract": "Previous research on radiology report generation has made significant\nprogress in terms of increasing the clinical accuracy of generated reports. In\nthis paper, we emphasize another crucial quality that it should possess, i.e.,\ninter-report consistency, which refers to the capability of generating\nconsistent reports for semantically equivalent radiographs. This quality is\neven of greater significance than the overall report accuracy in terms of\nensuring the system's credibility, as a system prone to providing conflicting\nresults would severely erode users' trust. Regrettably, existing approaches\nstruggle to maintain inter-report consistency, exhibiting biases towards common\npatterns and susceptibility to lesion variants. To address this issue, we\npropose ICON, which improves the inter-report consistency of radiology report\ngeneration. Aiming to enhance the system's ability to capture similarities in\nsemantically equivalent lesions, our approach first involves extracting lesions\nfrom input images and examining their characteristics. Then, we introduce a\nlesion-aware mixup technique to ensure that the representations of the\nsemantically equivalent lesions align with the same attributes, achieved\nthrough a linear combination during the training phase. Extensive experiments\non three publicly available chest X-ray datasets verify the effectiveness of\nour approach, both in terms of improving the consistency and accuracy of the\ngenerated reports.", "published": "2024-02-20 09:13:15", "link": "http://arxiv.org/abs/2402.12844v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "More Discriminative Sentence Embeddings via Semantic Graph Smoothing", "abstract": "This paper explores an empirical approach to learn more discriminantive\nsentence representations in an unsupervised fashion. Leveraging semantic graph\nsmoothing, we enhance sentence embeddings obtained from pretrained models to\nimprove results for the text clustering and classification tasks. Our method,\nvalidated on eight benchmarks, demonstrates consistent improvements, showcasing\nthe potential of semantic graph smoothing in improving sentence embeddings for\nthe supervised and unsupervised document categorization tasks.", "published": "2024-02-20 10:34:19", "link": "http://arxiv.org/abs/2402.12890v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language Model-based Human-Agent Collaboration for Complex Task\n  Solving", "abstract": "In recent developments within the research community, the integration of\nLarge Language Models (LLMs) in creating fully autonomous agents has garnered\nsignificant interest. Despite this, LLM-based agents frequently demonstrate\nnotable shortcomings in adjusting to dynamic environments and fully grasping\nhuman needs. In this work, we introduce the problem of LLM-based human-agent\ncollaboration for complex task-solving, exploring their synergistic potential.\nIn addition, we propose a Reinforcement Learning-based Human-Agent\nCollaboration method, ReHAC. This approach includes a policy model designed to\ndetermine the most opportune stages for human intervention within the\ntask-solving process. We construct a human-agent collaboration dataset to train\nthis policy model in an offline reinforcement learning environment. Our\nvalidation tests confirm the model's effectiveness. The results demonstrate\nthat the synergistic efforts of humans and LLM-based agents significantly\nimprove performance in complex tasks, primarily through well-planned, limited\nhuman intervention. Datasets and code are available at:\nhttps://github.com/XueyangFeng/ReHAC.", "published": "2024-02-20 11:03:36", "link": "http://arxiv.org/abs/2402.12914v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Prompt Stealing Attacks Against Large Language Models", "abstract": "The increasing reliance on large language models (LLMs) such as ChatGPT in\nvarious fields emphasizes the importance of ``prompt engineering,'' a\ntechnology to improve the quality of model outputs. With companies investing\nsignificantly in expert prompt engineers and educational resources rising to\nmeet market demand, designing high-quality prompts has become an intriguing\nchallenge. In this paper, we propose a novel attack against LLMs, named prompt\nstealing attacks. Our proposed prompt stealing attack aims to steal these\nwell-designed prompts based on the generated answers. The prompt stealing\nattack contains two primary modules: the parameter extractor and the prompt\nreconstruction. The goal of the parameter extractor is to figure out the\nproperties of the original prompts. We first observe that most prompts fall\ninto one of three categories: direct prompt, role-based prompt, and in-context\nprompt. Our parameter extractor first tries to distinguish the type of prompts\nbased on the generated answers. Then, it can further predict which role or how\nmany contexts are used based on the types of prompts. Following the parameter\nextractor, the prompt reconstructor can be used to reconstruct the original\nprompts based on the generated answers and the extracted features. The final\ngoal of the prompt reconstructor is to generate the reversed prompts, which are\nsimilar to the original prompts. Our experimental results show the remarkable\nperformance of our proposed attacks. Our proposed attacks add a new dimension\nto the study of prompt engineering and call for more attention to the security\nissues on LLMs.", "published": "2024-02-20 12:25:26", "link": "http://arxiv.org/abs/2402.12959v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Gl\u00f3rIA -- A Generative and Open Large Language Model for Portuguese", "abstract": "Significant strides have been made in natural language tasks, largely\nattributed to the emergence of powerful large language models (LLMs). These\nmodels, pre-trained on extensive and diverse corpora, have become increasingly\ncapable of comprehending the intricacies of language. Despite the abundance of\nLLMs for many high-resource languages, the availability of such models remains\nlimited for European Portuguese. We introduce Gl\\'orIA, a robust European\nPortuguese decoder LLM. To pre-train Gl\\'orIA, we assembled a comprehensive\nPT-PT text corpus comprising 35 billion tokens from various sources. We present\nour pre-training methodology, followed by an assessment of the model's\neffectiveness on multiple downstream tasks. Additionally, to evaluate our\nmodels' language modeling capabilities, we introduce CALAME-PT (Context-Aware\nLAnguage Modeling Evaluation for Portuguese), the first Portuguese zero-shot\nlanguage-modeling benchmark. Evaluation shows that Gl\\'orIA significantly\noutperforms existing open PT decoder models in language modeling and that it\ncan generate sound, knowledge-rich, and coherent PT-PT text. The model also\nexhibits strong potential for various downstream tasks.", "published": "2024-02-20 12:36:40", "link": "http://arxiv.org/abs/2402.12969v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Impact of Demonstrations on Multilingual In-Context Learning: A\n  Multidimensional Analysis", "abstract": "In-context learning is a popular inference strategy where large language\nmodels solve a task using only a few labeled demonstrations without needing any\nparameter updates. Although there have been extensive studies on English\nin-context learning, multilingual in-context learning remains under-explored,\nand we lack an in-depth understanding of the role of demonstrations in this\ncontext. To address this gap, we conduct a multidimensional analysis of\nmultilingual in-context learning, experimenting with 5 models from different\nmodel families, 9 datasets covering classification and generation tasks, and 56\ntypologically diverse languages. Our results reveal that the effectiveness of\ndemonstrations varies significantly across models, tasks, and languages. We\nalso find that strong instruction-following models including Llama 2-Chat,\nGPT-3.5, and GPT-4 are largely insensitive to the quality of demonstrations.\nInstead, a carefully crafted template often eliminates the benefits of\ndemonstrations for some tasks and languages altogether. These findings show\nthat the importance of demonstrations might be overestimated. Our work\nhighlights the need for granular evaluation across multiple axes towards a\nbetter understanding of in-context learning.", "published": "2024-02-20 12:53:31", "link": "http://arxiv.org/abs/2402.12976v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can GNN be Good Adapter for LLMs?", "abstract": "Recently, large language models (LLMs) have demonstrated superior\ncapabilities in understanding and zero-shot learning on textual data, promising\nsignificant advances for many text-related domains. In the graph domain,\nvarious real-world scenarios also involve textual data, where tasks and node\nfeatures can be described by text. These text-attributed graphs (TAGs) have\nbroad applications in social media, recommendation systems, etc. Thus, this\npaper explores how to utilize LLMs to model TAGs. Previous methods for TAG\nmodeling are based on million-scale LMs. When scaled up to billion-scale LLMs,\nthey face huge challenges in computational costs. Additionally, they also\nignore the zero-shot inference capabilities of LLMs. Therefore, we propose\nGraphAdapter, which uses a graph neural network (GNN) as an efficient adapter\nin collaboration with LLMs to tackle TAGs. In terms of efficiency, the GNN\nadapter introduces only a few trainable parameters and can be trained with low\ncomputation costs. The entire framework is trained using auto-regression on\nnode text (next token prediction). Once trained, GraphAdapter can be seamlessly\nfine-tuned with task-specific prompts for various downstream tasks. Through\nextensive experiments across multiple real-world TAGs, GraphAdapter based on\nLlama 2 gains an average improvement of approximately 5\\% in terms of node\nclassification. Furthermore, GraphAdapter can also adapt to other language\nmodels, including RoBERTa, GPT-2. The promising results demonstrate that GNNs\ncan serve as effective adapters for LLMs in TAG modeling.", "published": "2024-02-20 13:13:13", "link": "http://arxiv.org/abs/2402.12984v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Trustworthy Reranking: A Simple yet Effective Abstention\n  Mechanism", "abstract": "Neural Information Retrieval (NIR) has significantly improved upon\nheuristic-based Information Retrieval (IR) systems. Yet, failures remain\nfrequent, the models used often being unable to retrieve documents relevant to\nthe user's query. We address this challenge by proposing a lightweight\nabstention mechanism tailored for real-world constraints, with particular\nemphasis placed on the reranking phase. We introduce a protocol for evaluating\nabstention strategies in black-box scenarios (typically encountered when\nrelying on API services), demonstrating their efficacy, and propose a simple\nyet effective data-driven mechanism. We provide open-source code for experiment\nreplication and abstention implementation, fostering wider adoption and\napplication in diverse contexts.", "published": "2024-02-20 13:25:16", "link": "http://arxiv.org/abs/2402.12997v5", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Investigating the Impact of Model Instability on Explanations and\n  Uncertainty", "abstract": "Explainable AI methods facilitate the understanding of model behaviour, yet,\nsmall, imperceptible perturbations to inputs can vastly distort explanations.\nAs these explanations are typically evaluated holistically, before model\ndeployment, it is difficult to assess when a particular explanation is\ntrustworthy. Some studies have tried to create confidence estimators for\nexplanations, but none have investigated an existing link between uncertainty\nand explanation quality. We artificially simulate epistemic uncertainty in text\ninput by introducing noise at inference time. In this large-scale empirical\nstudy, we insert different levels of noise perturbations and measure the effect\non the output of pre-trained language models and different uncertainty metrics.\nRealistic perturbations have minimal effect on performance and explanations,\nyet masking has a drastic effect. We find that high uncertainty doesn't\nnecessarily imply low explanation plausibility; the correlation between the two\nmetrics can be moderately positive when noise is exposed during the training\nprocess. This suggests that noise-augmented models may be better at identifying\nsalient tokens when uncertain. Furthermore, when predictive and epistemic\nuncertainty measures are over-confident, the robustness of a saliency map to\nperturbation can indicate model stability issues. Integrated Gradients shows\nthe overall greatest robustness to perturbation, while still showing\nmodel-specific patterns in performance; however, this phenomenon is limited to\nsmaller Transformer-based language models.", "published": "2024-02-20 13:41:21", "link": "http://arxiv.org/abs/2402.13006v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SoMeLVLM: A Large Vision Language Model for Social Media Processing", "abstract": "The growth of social media, characterized by its multimodal nature, has led\nto the emergence of diverse phenomena and challenges, which calls for an\neffective approach to uniformly solve automated tasks. The powerful Large\nVision Language Models make it possible to handle a variety of tasks\nsimultaneously, but even with carefully designed prompting methods, the general\ndomain models often fall short in aligning with the unique speaking style and\ncontext of social media tasks. In this paper, we introduce a Large Vision\nLanguage Model for Social Media Processing (SoMeLVLM), which is a cognitive\nframework equipped with five key capabilities including knowledge &\ncomprehension, application, analysis, evaluation, and creation. SoMeLVLM is\ndesigned to understand and generate realistic social media behavior. We have\ndeveloped a 654k multimodal social media instruction-tuning dataset to support\nour cognitive framework and fine-tune our model. Our experiments demonstrate\nthat SoMeLVLM achieves state-of-the-art performance in multiple social media\ntasks. Further analysis shows its significant advantages over baselines in\nterms of cognitive abilities.", "published": "2024-02-20 14:02:45", "link": "http://arxiv.org/abs/2402.13022v1", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "CFEVER: A Chinese Fact Extraction and VERification Dataset", "abstract": "We present CFEVER, a Chinese dataset designed for Fact Extraction and\nVERification. CFEVER comprises 30,012 manually created claims based on content\nin Chinese Wikipedia. Each claim in CFEVER is labeled as \"Supports\", \"Refutes\",\nor \"Not Enough Info\" to depict its degree of factualness. Similar to the FEVER\ndataset, claims in the \"Supports\" and \"Refutes\" categories are also annotated\nwith corresponding evidence sentences sourced from single or multiple pages in\nChinese Wikipedia. Our labeled dataset holds a Fleiss' kappa value of 0.7934\nfor five-way inter-annotator agreement. In addition, through the experiments\nwith the state-of-the-art approaches developed on the FEVER dataset and a\nsimple baseline for CFEVER, we demonstrate that our dataset is a new rigorous\nbenchmark for factual extraction and verification, which can be further used\nfor developing automated systems to alleviate human fact-checking efforts.\nCFEVER is available at https://ikmlab.github.io/CFEVER.", "published": "2024-02-20 14:08:24", "link": "http://arxiv.org/abs/2402.13025v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Heterogeneous Graph Reasoning for Fact Checking over Texts and Tables", "abstract": "Fact checking aims to predict claim veracity by reasoning over multiple\nevidence pieces. It usually involves evidence retrieval and veracity reasoning.\nIn this paper, we focus on the latter, reasoning over unstructured text and\nstructured table information. Previous works have primarily relied on\nfine-tuning pretrained language models or training homogeneous-graph-based\nmodels. Despite their effectiveness, we argue that they fail to explore the\nrich semantic information underlying the evidence with different structures. To\naddress this, we propose a novel word-level Heterogeneous-graph-based model for\nFact Checking over unstructured and structured information, namely HeterFC. Our\napproach leverages a heterogeneous evidence graph, with words as nodes and\nthoughtfully designed edges representing different evidence properties. We\nperform information propagation via a relational graph neural network,\nfacilitating interactions between claims and evidence. An attention-based\nmethod is utilized to integrate information, combined with a language model for\ngenerating predictions. We introduce a multitask loss function to account for\npotential inaccuracies in evidence retrieval. Comprehensive experiments on the\nlarge fact checking dataset FEVEROUS demonstrate the effectiveness of HeterFC.\nCode will be released at: https://github.com/Deno-V/HeterFC.", "published": "2024-02-20 14:10:40", "link": "http://arxiv.org/abs/2402.13028v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning to Check: Unleashing Potentials for Self-Correction in Large\n  Language Models", "abstract": "Self-correction has achieved impressive results in enhancing the style and\nsecurity of the generated output from large language models (LLMs). However,\nrecent studies suggest that self-correction might be limited or even\ncounterproductive in reasoning tasks due to LLMs' difficulties in identifying\nlogical mistakes.\n  In this paper, we aim to enhance the self-checking capabilities of LLMs by\nconstructing training data for checking tasks. Specifically, we apply the Chain\nof Thought (CoT) methodology to self-checking tasks, utilizing fine-grained\nstep-level analyses and explanations to assess the correctness of reasoning\npaths. We propose a specialized checking format called \"Step CoT Check\".\nFollowing this format, we construct a checking-correction dataset that includes\ndetailed step-by-step analysis and checking. Then we fine-tune LLMs to enhance\ntheir error detection and correction abilities.\n  Our experiments demonstrate that fine-tuning with the \"Step CoT Check\" format\nsignificantly improves the self-checking and self-correction abilities of LLMs\nacross multiple benchmarks. This approach outperforms other formats, especially\nin locating the incorrect position, with greater benefits observed in larger\nmodels.\n  For reproducibility, all the datasets and code are provided in\nhttps://github.com/bammt/Learn-to-check.", "published": "2024-02-20 14:23:23", "link": "http://arxiv.org/abs/2402.13035v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Identifying Semantic Induction Heads to Understand In-Context Learning", "abstract": "Although large language models (LLMs) have demonstrated remarkable\nperformance, the lack of transparency in their inference logic raises concerns\nabout their trustworthiness. To gain a better understanding of LLMs, we conduct\na detailed analysis of the operations of attention heads and aim to better\nunderstand the in-context learning of LLMs. Specifically, we investigate\nwhether attention heads encode two types of relationships between tokens\npresent in natural languages: the syntactic dependency parsed from sentences\nand the relation within knowledge graphs. We find that certain attention heads\nexhibit a pattern where, when attending to head tokens, they recall tail tokens\nand increase the output logits of those tail tokens. More crucially, the\nformulation of such semantic induction heads has a close correlation with the\nemergence of the in-context learning ability of language models. The study of\nsemantic attention heads advances our understanding of the intricate operations\nof attention heads in transformers, and further provides new insights into the\nin-context learning of LLMs.", "published": "2024-02-20 14:43:39", "link": "http://arxiv.org/abs/2402.13055v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Event-level Knowledge Editing", "abstract": "Knowledge editing aims at updating knowledge of large language models (LLMs)\nto prevent them from becoming outdated. Existing work edits LLMs at the level\nof factual knowledge triplets. However, natural knowledge updates in the real\nworld come from the occurrences of new events rather than direct changes in\nfactual triplets. In this paper, we propose a new task setting: event-level\nknowledge editing, which directly edits new events into LLMs and improves over\nconventional triplet-level editing on (1) Efficiency. A single event edit leads\nto updates in multiple entailed knowledge triplets. (2) Completeness. Beyond\nupdating factual knowledge, event-level editing also requires considering the\nevent influences and updating LLMs' knowledge about future trends. We construct\na high-quality event-level editing benchmark ELKEN, consisting of 1,515 event\nedits, 6,449 questions about factual knowledge, and 10,150 questions about\nfuture tendencies. We systematically evaluate the performance of various\nknowledge editing methods and LLMs on this benchmark. We find that ELKEN poses\nsignificant challenges to existing knowledge editing approaches. Our codes and\ndataset are publicly released to facilitate further research.", "published": "2024-02-20 15:36:41", "link": "http://arxiv.org/abs/2402.13093v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Digital Comprehensibility Assessment of Simplified Texts among Persons\n  with Intellectual Disabilities", "abstract": "Text simplification refers to the process of increasing the comprehensibility\nof texts. Automatic text simplification models are most commonly evaluated by\nexperts or crowdworkers instead of the primary target groups of simplified\ntexts, such as persons with intellectual disabilities. We conducted an\nevaluation study of text comprehensibility including participants with and\nwithout intellectual disabilities reading unsimplified, automatically and\nmanually simplified German texts on a tablet computer. We explored four\ndifferent approaches to measuring comprehensibility: multiple-choice\ncomprehension questions, perceived difficulty ratings, response time, and\nreading speed. The results revealed significant variations in these\nmeasurements, depending on the reader group and whether the text had undergone\nautomatic or manual simplification. For the target group of persons with\nintellectual disabilities, comprehension questions emerged as the most reliable\nmeasure, while analyzing reading speed provided valuable insights into\nparticipants' reading behavior.", "published": "2024-02-20 15:37:08", "link": "http://arxiv.org/abs/2402.13094v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "ELAD: Explanation-Guided Large Language Models Active Distillation", "abstract": "The deployment and application of Large Language Models (LLMs) is hindered by\ntheir memory inefficiency, computational demands, and the high costs of API\ninferences. Traditional distillation methods, which transfer the capabilities\nof LLMs to smaller models, often fail to determine whether the knowledge has\nbeen sufficiently transferred, potentially resulting in high costs or\nincomplete distillation. In this paper, we propose an Explanation-Guided LLMs\nActive Distillation (ELAD) framework that employs an active learning strategy\nto optimize the balance between annotation costs and model performance. To\nimprove efficient sample selection, we introduce an explanation-guided sample\nselection method that identifies samples challenging its reasoning by\nexploiting uncertainties in explanation steps. Additionally, we present a\ncustomized LLM-annotated explanation revision technique where the teacher model\ndetects and corrects flaws in the student model's reasoning. Our experiments\nacross various reasoning datasets demonstrate that our framework significantly\nenhances the efficiency of LLM knowledge distillation.", "published": "2024-02-20 15:47:59", "link": "http://arxiv.org/abs/2402.13098v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the\n  Generalizability of Large Language Models", "abstract": "The advancement of large language models (LLMs) has enhanced the ability to\ngeneralize across a wide range of unseen natural language processing (NLP)\ntasks through instruction-following. Yet, their effectiveness often diminishes\nin low-resource languages like Chinese, exacerbated by biased evaluations from\ndata leakage, casting doubt on their true generalizability to new linguistic\nterritories. In response, we introduce the Chinese Instruction-Following\nBenchmark (CIF-Bench), designed to evaluate the zero-shot generalizability of\nLLMs to the Chinese language. CIF-Bench comprises 150 tasks and 15,000\ninput-output pairs, developed by native speakers to test complex reasoning and\nChinese cultural nuances across 20 categories. To mitigate data contamination,\nwe release only half of the dataset publicly, with the remainder kept private,\nand introduce diversified instructions to minimize score variance, totaling\n45,000 data instances. Our evaluation of 28 selected LLMs reveals a noticeable\nperformance gap, with the best model scoring only 52.9%, highlighting the\nlimitations of LLMs in less familiar language and task contexts. This work not\nonly uncovers the current limitations of LLMs in handling Chinese language\ntasks but also sets a new standard for future LLM generalizability research,\npushing towards the development of more adaptable, culturally informed, and\nlinguistically diverse models.", "published": "2024-02-20 16:02:12", "link": "http://arxiv.org/abs/2402.13109v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TreeEval: Benchmark-Free Evaluation of Large Language Models through\n  Tree Planning", "abstract": "Recently, numerous new benchmarks have been established to evaluate the\nperformance of large language models (LLMs) via either computing a holistic\nscore or employing another LLM as a judge. However, these approaches suffer\nfrom data leakage due to the open access of the benchmark and inflexible\nevaluation process. To address this issue, we introduce $\\textbf{TreeEval}$, a\nbenchmark-free evaluation method for LLMs that let a high-performance LLM host\nan irreproducible evaluation session and essentially avoids the data leakage.\nMoreover, this LLM performs as an examiner to raise up a series of questions\nunder a topic with a tree planing strategy, which considers the current\nevaluation status to decide the next question generation and ensures the\ncompleteness and efficiency of the evaluation process. We evaluate $6$ models\nof different parameter sizes, including $7$B, $13$B, and $33$B, and ultimately\nachieved the highest correlation coefficient with AlpacaEval2.0 using only\naround $45$ questions. We also conduct more analysis to show the robustness and\nreliability of TreeEval. Our code can be accessed via the provided\nhttps://github.com/Ashura5/TreeEval.", "published": "2024-02-20 16:38:33", "link": "http://arxiv.org/abs/2402.13125v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CMDAG: A Chinese Metaphor Dataset with Annotated Grounds as CoT for\n  Boosting Metaphor Generation", "abstract": "Metaphor is a prominent linguistic device in human language and literature,\nas they add color, imagery, and emphasis to enhance effective communication.\nThis paper introduces a large-scale high quality annotated Chinese Metaphor\nCorpus, which comprises around 28K sentences drawn from a diverse range of\nChinese literary sources, such as poems, prose, song lyrics, etc. To ensure the\naccuracy and consistency of our annotations, we introduce a comprehensive set\nof guidelines. These guidelines address the facets of metaphor annotation,\nincluding identifying tenors, vehicles, and grounds to handling the\ncomplexities of similes, personifications, juxtapositions, and hyperboles.\nBreaking tradition, our approach to metaphor generation emphasizes grounds and\ntheir distinct features rather than the conventional combination of tenors and\nvehicles. By integrating \"ground\" as a CoT (Chain of Thoughts) input, we are\nable to generate metaphors that resonate more with real-world intuition. We\ntest generative models such as Belle, Baichuan, and Chinese-alpaca-33B using\nour annotated corpus. These models are able to generate creative and fluent\nmetaphor sentences more frequently induced by selected samples from our\ndataset, demonstrating the value of our corpus for Chinese metaphor research.\nThe code is available in\nhttps://github.com/JasonShao55/Chinese_Metaphor_Explanation.", "published": "2024-02-20 17:00:41", "link": "http://arxiv.org/abs/2402.13145v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech\n  Technologies", "abstract": "More than 7,000 known languages are spoken around the world. However, due to\nthe lack of annotated resources, only a small fraction of them are currently\ncovered by speech technologies. Albeit self-supervised speech representations,\nrecent massive speech corpora collections, as well as the organization of\nchallenges, have alleviated this inequality, most studies are mainly\nbenchmarked on English. This situation is aggravated when tasks involving both\nacoustic and visual speech modalities are addressed. In order to promote\nresearch on low-resource languages for audio-visual speech technologies, we\npresent AnnoTheia, a semi-automatic annotation toolkit that detects when a\nperson speaks on the scene and the corresponding transcription. In addition, to\nshow the complete process of preparing AnnoTheia for a language of interest, we\nalso describe the adaptation of a pre-trained model for active speaker\ndetection to Spanish, using a database not initially conceived for this type of\ntask. The AnnoTheia toolkit, tutorials, and pre-trained models are available on\nGitHub.", "published": "2024-02-20 17:07:08", "link": "http://arxiv.org/abs/2402.13152v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Benchmarking Retrieval-Augmented Generation for Medicine", "abstract": "While large language models (LLMs) have achieved state-of-the-art performance\non a wide range of medical question answering (QA) tasks, they still face\nchallenges with hallucinations and outdated knowledge. Retrieval-augmented\ngeneration (RAG) is a promising solution and has been widely adopted. However,\na RAG system can involve multiple flexible components, and there is a lack of\nbest practices regarding the optimal RAG setting for various medical purposes.\nTo systematically evaluate such systems, we propose the Medical Information\nRetrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind\nbenchmark including 7,663 questions from five medical QA datasets. Using\nMIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt\ntokens on 41 combinations of different corpora, retrievers, and backbone LLMs\nthrough the MedRAG toolkit introduced in this work. Overall, MedRAG improves\nthe accuracy of six different LLMs by up to 18% over chain-of-thought\nprompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4-level. Our\nresults show that the combination of various medical corpora and retrievers\nachieves the best performance. In addition, we discovered a log-linear scaling\nproperty and the \"lost-in-the-middle\" effects in medical RAG. We believe our\ncomprehensive evaluations can serve as practical guidelines for implementing\nRAG systems for medicine.", "published": "2024-02-20 17:44:06", "link": "http://arxiv.org/abs/2402.13178v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How do Hyenas deal with Human Speech? Speech Recognition and Translation\n  with ConfHyena", "abstract": "The attention mechanism, a cornerstone of state-of-the-art neural models,\nfaces computational hurdles in processing long sequences due to its quadratic\ncomplexity. Consequently, research efforts in the last few years focused on\nfinding more efficient alternatives. Among them, Hyena (Poli et al., 2023)\nstands out for achieving competitive results in both language modeling and\nimage classification, while offering sub-quadratic memory and computational\ncomplexity. Building on these promising results, we propose ConfHyena, a\nConformer whose encoder self-attentions are replaced with an adaptation of\nHyena for speech processing, where the long input sequences cause high\ncomputational costs. Through experiments in automatic speech recognition (for\nEnglish) and translation (from English into 8 target languages), we show that\nour best ConfHyena model significantly reduces the training time by 27%, at the\ncost of minimal quality degradation (~1%), which, in most cases, is not\nstatistically significant.", "published": "2024-02-20 18:19:08", "link": "http://arxiv.org/abs/2402.13208v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on\n  Deceptive Prompts", "abstract": "The remarkable advancements in Multimodal Large Language Models (MLLMs) have\nnot rendered them immune to challenges, particularly in the context of handling\ndeceptive information in prompts, thus producing hallucinated responses under\nsuch conditions. To quantitatively assess this vulnerability, we present\nMAD-Bench, a carefully curated benchmark that contains 1000 test samples\ndivided into 5 categories, such as non-existent objects, count of objects, and\nspatial relationship. We provide a comprehensive analysis of popular MLLMs,\nranging from GPT-4v, Reka, Gemini-Pro, to open-sourced models, such as\nLLaVA-NeXT and MiniCPM-Llama3. Empirically, we observe significant performance\ngaps between GPT-4o and other models; and previous robust instruction-tuned\nmodels are not effective on this new benchmark. While GPT-4o achieves 82.82%\naccuracy on MAD-Bench, the accuracy of any other model in our experiments\nranges from 9% to 50%. We further propose a remedy that adds an additional\nparagraph to the deceptive prompts to encourage models to think twice before\nanswering the question. Surprisingly, this simple method can even double the\naccuracy; however, the absolute numbers are still too low to be satisfactory.\nWe hope MAD-Bench can serve as a valuable benchmark to stimulate further\nresearch to enhance model resilience against deceptive prompts.", "published": "2024-02-20 18:31:27", "link": "http://arxiv.org/abs/2402.13220v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "AgentMD: Empowering Language Agents for Risk Prediction with Large-Scale\n  Clinical Tool Learning", "abstract": "Clinical calculators play a vital role in healthcare by offering accurate\nevidence-based predictions for various purposes such as prognosis.\nNevertheless, their widespread utilization is frequently hindered by usability\nchallenges, poor dissemination, and restricted functionality. Augmenting large\nlanguage models with extensive collections of clinical calculators presents an\nopportunity to overcome these obstacles and improve workflow efficiency, but\nthe scalability of the manual curation process poses a significant challenge.\nIn response, we introduce AgentMD, a novel language agent capable of curating\nand applying clinical calculators across various clinical contexts. Using the\npublished literature, AgentMD has automatically curated a collection of 2,164\ndiverse clinical calculators with executable functions and structured\ndocumentation, collectively named RiskCalcs. Manual evaluations show that\nRiskCalcs tools achieve an accuracy of over 80% on three quality metrics. At\ninference time, AgentMD can automatically select and apply the relevant\nRiskCalcs tools given any patient description. On the newly established RiskQA\nbenchmark, AgentMD significantly outperforms chain-of-thought prompting with\nGPT-4 (87.7% vs. 40.9% in accuracy). Additionally, we also applied AgentMD to\nreal-world clinical notes for analyzing both population-level and risk-level\npatient characteristics. In summary, our study illustrates the utility of\nlanguage agents augmented with clinical calculators for healthcare analytics\nand patient care.", "published": "2024-02-20 18:37:19", "link": "http://arxiv.org/abs/2402.13225v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Investigating Cultural Alignment of Large Language Models", "abstract": "The intricate relationship between language and culture has long been a\nsubject of exploration within the realm of linguistic anthropology. Large\nLanguage Models (LLMs), promoted as repositories of collective human knowledge,\nraise a pivotal question: do these models genuinely encapsulate the diverse\nknowledge adopted by different cultures? Our study reveals that these models\ndemonstrate greater cultural alignment along two dimensions -- firstly, when\nprompted with the dominant language of a specific culture, and secondly, when\npretrained with a refined mixture of languages employed by that culture. We\nquantify cultural alignment by simulating sociological surveys, comparing model\nresponses to those of actual survey participants as references. Specifically,\nwe replicate a survey conducted in various regions of Egypt and the United\nStates through prompting LLMs with different pretraining data mixtures in both\nArabic and English with the personas of the real respondents and the survey\nquestions. Further analysis reveals that misalignment becomes more pronounced\nfor underrepresented personas and for culturally sensitive topics, such as\nthose probing social values. Finally, we introduce Anthropological Prompting, a\nnovel method leveraging anthropological reasoning to enhance cultural\nalignment. Our study emphasizes the necessity for a more balanced multilingual\npretraining dataset to better represent the diversity of human experience and\nthe plurality of different cultures with many implications on the topic of\ncross-lingual transfer.", "published": "2024-02-20 18:47:28", "link": "http://arxiv.org/abs/2402.13231v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Unlocking Insights: Semantic Search in Jupyter Notebooks", "abstract": "Semantic search, a process aimed at delivering highly relevant search results\nby comprehending the searcher's intent and the contextual meaning of terms\nwithin a searchable dataspace, plays a pivotal role in information retrieval.\nIn this paper, we investigate the application of large language models to\nenhance semantic search capabilities, specifically tailored for the domain of\nJupyter Notebooks. Our objective is to retrieve generated outputs, such as\nfigures or tables, associated functions and methods, and other pertinent\ninformation.\n  We demonstrate a semantic search framework that achieves a comprehensive\nsemantic understanding of the entire notebook's contents, enabling it to\neffectively handle various types of user queries. Key components of this\nframework include:\n  1). A data preprocessor is designed to handle diverse types of cells within\nJupyter Notebooks, encompassing both markdown and code cells. 2). An innovative\nmethodology is devised to address token size limitations that arise with\ncode-type cells. We implement a finer-grained approach to data input,\ntransitioning from the cell level to the function level, effectively resolving\nthese issues.", "published": "2024-02-20 18:49:41", "link": "http://arxiv.org/abs/2402.13234v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue\n  Summarization", "abstract": "Single document news summarization has seen substantial progress on\nfaithfulness in recent years, driven by research on the evaluation of factual\nconsistency, or hallucinations. We ask whether these advances carry over to\nother text summarization domains. We propose a new evaluation benchmark on\ntopic-focused dialogue summarization, generated by LLMs of varying sizes. We\nprovide binary sentence-level human annotations of the factual consistency of\nthese summaries along with detailed explanations of factually inconsistent\nsentences. Our analysis shows that existing LLMs hallucinate significant\namounts of factual errors in the dialogue domain, regardless of the model's\nsize. On the other hand, when LLMs, including GPT-4, serve as binary factual\nevaluators, they perform poorly and can be outperformed by prevailing\nstate-of-the-art specialized factuality evaluation metrics. Finally, we\nconducted an analysis of hallucination types with a curated error taxonomy. We\nfind that there are diverse errors and error distributions in model-generated\nsummaries and that non-LLM based metrics can capture all error types better\nthan LLM-based evaluators.", "published": "2024-02-20 18:58:49", "link": "http://arxiv.org/abs/2402.13249v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Simple but Effective Approach to Improve Structured Language Model\n  Output for Information Extraction", "abstract": "Large language models (LLMs) have demonstrated impressive abilities in\ngenerating unstructured natural language according to instructions. However,\ntheir performance can be inconsistent when tasked with producing text that\nadheres to specific structured formats, which is crucial in applications like\nnamed entity recognition (NER) or relation extraction (RE). To address this\nissue, this paper introduces an efficient method, G&O, to enhance their\nstructured text generation capabilities. It breaks the generation into a\ntwo-step pipeline: initially, LLMs generate answers in natural language as\nintermediate responses. Subsequently, LLMs are asked to organize the output\ninto the desired structure, using the intermediate responses as context. G&O\neffectively separates the generation of content from the structuring process,\nreducing the pressure of completing two orthogonal tasks simultaneously. Tested\non zero-shot NER and RE, the results indicate a significant improvement in LLM\nperformance with minimal additional efforts. This straightforward and adaptable\nprompting technique can also be combined with other strategies, like\nself-consistency, to further elevate LLM capabilities in various structured\ntext generation tasks.", "published": "2024-02-20 20:42:02", "link": "http://arxiv.org/abs/2402.13364v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Harnessing Large Language Models as Post-hoc Correctors", "abstract": "As Machine Learning (ML) models grow in size and demand higher-quality\ntraining data, the expenses associated with re-training and fine-tuning these\nmodels are escalating rapidly. Inspired by recent impressive achievements of\nLarge Language Models (LLMs) in different fields, this paper delves into the\nquestion: can LLMs efficiently improve an ML's performance at a minimal cost?\nWe show that, through our proposed training-free framework LlmCorr, an LLM can\nwork as a post-hoc corrector to propose corrections for the predictions of an\narbitrary ML model. In particular, we form a contextual knowledge database by\nincorporating the dataset's label information and the ML model's predictions on\nthe validation dataset. Leveraging the in-context learning capability of LLMs,\nwe ask the LLM to summarise the instances in which the ML model makes mistakes\nand the correlation between primary predictions and true labels. Following\nthis, the LLM can transfer its acquired knowledge to suggest corrections for\nthe ML model's predictions. Our experimental results on text analysis and the\nchallenging molecular predictions show that \\model improves the performance of\na number of models by up to 39%.", "published": "2024-02-20 22:50:41", "link": "http://arxiv.org/abs/2402.13414v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CHATATC: Large Language Model-Driven Conversational Agents for\n  Supporting Strategic Air Traffic Flow Management", "abstract": "Generative artificial intelligence (AI) and large language models (LLMs) have\ngained rapid popularity through publicly available tools such as ChatGPT. The\nadoption of LLMs for personal and professional use is fueled by the natural\ninteractions between human users and computer applications such as ChatGPT,\nalong with powerful summarization and text generation capabilities. Given the\nwidespread use of such generative AI tools, in this work we investigate how\nthese tools can be deployed in a non-safety critical, strategic traffic flow\nmanagement setting. Specifically, we train an LLM, CHATATC, based on a large\nhistorical data set of Ground Delay Program (GDP) issuances, spanning 2000-2023\nand consisting of over 80,000 GDP implementations, revisions, and\ncancellations. We test the query and response capabilities of CHATATC,\ndocumenting successes (e.g., providing correct GDP rates, durations, and\nreason) and shortcomings (e.g,. superlative questions). We also detail the\ndesign of a graphical user interface for future users to interact and\ncollaborate with the CHATATC conversational agent.", "published": "2024-02-20 01:59:11", "link": "http://arxiv.org/abs/2402.14850v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NL2Formula: Generating Spreadsheet Formulas from Natural Language\n  Queries", "abstract": "Writing formulas on spreadsheets, such as Microsoft Excel and Google Sheets,\nis a widespread practice among users performing data analysis. However,\ncrafting formulas on spreadsheets remains a tedious and error-prone task for\nmany end-users, particularly when dealing with complex operations. To alleviate\nthe burden associated with writing spreadsheet formulas, this paper introduces\na novel benchmark task called NL2Formula, with the aim to generate executable\nformulas that are grounded on a spreadsheet table, given a Natural Language\n(NL) query as input. To accomplish this, we construct a comprehensive dataset\nconsisting of 70,799 paired NL queries and corresponding spreadsheet formulas,\ncovering 21,670 tables and 37 types of formula functions. We realize the\nNL2Formula task by providing a sequence-to-sequence baseline implementation\ncalled fCoder. Experimental results validate the effectiveness of fCoder,\ndemonstrating its superior performance compared to the baseline models.\nFurthermore, we also compare fCoder with an initial GPT-3.5 model (i.e.,\ntext-davinci-003). Lastly, through in-depth error analysis, we identify\npotential challenges in the NL2Formula task and advocate for further\ninvestigation.", "published": "2024-02-20 05:58:05", "link": "http://arxiv.org/abs/2402.14853v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Dual-Prompting for Interpretable Mental Health Language Models", "abstract": "Despite the increasing demand for AI-based mental health monitoring tools,\ntheir practical utility for clinicians is limited by the lack of\ninterpretability.The CLPsych 2024 Shared Task (Chim et al., 2024) aims to\nenhance the interpretability of Large Language Models (LLMs), particularly in\nmental health analysis, by providing evidence of suicidality through linguistic\ncontent. We propose a dual-prompting approach: (i) Knowledge-aware evidence\nextraction by leveraging the expert identity and a suicide dictionary with a\nmental health-specific LLM; and (ii) Evidence summarization by employing an\nLLM-based consistency evaluator. Comprehensive experiments demonstrate the\neffectiveness of combining domain-specific information, revealing performance\nimprovements and the approach's potential to aid clinicians in assessing mental\nstate progression.", "published": "2024-02-20 06:18:02", "link": "http://arxiv.org/abs/2402.14854v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An LLM Maturity Model for Reliable and Transparent Text-to-Query", "abstract": "Recognizing the imperative to address the reliability and transparency issues\nof Large Language Models (LLM), this work proposes an LLM maturity model\ntailored for text-to-query applications. This maturity model seeks to fill the\nexisting void in evaluating LLMs in such applications by incorporating\ndimensions beyond mere correctness or accuracy. Moreover, this work introduces\na real-world use case from the law enforcement domain and showcases QueryIQ, an\nLLM-powered, domain-specific text-to-query assistant to expedite user workflows\nand reveal hidden relationship in data.", "published": "2024-02-20 06:20:09", "link": "http://arxiv.org/abs/2402.14855v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Comparing Inferential Strategies of Humans and Large Language Models in\n  Deductive Reasoning", "abstract": "Deductive reasoning plays a pivotal role in the formulation of sound and\ncohesive arguments. It allows individuals to draw conclusions that logically\nfollow, given the truth value of the information provided. Recent progress in\nthe domain of large language models (LLMs) has showcased their capability in\nexecuting deductive reasoning tasks. Nonetheless, a significant portion of\nresearch primarily assesses the accuracy of LLMs in solving such tasks, often\noverlooking a deeper analysis of their reasoning behavior. In this study, we\ndraw upon principles from cognitive psychology to examine inferential\nstrategies employed by LLMs, through a detailed evaluation of their responses\nto propositional logic problems. Our findings indicate that LLMs display\nreasoning patterns akin to those observed in humans, including strategies like\n$\\textit{supposition following}$ or $\\textit{chain construction}$. Moreover,\nour research demonstrates that the architecture and scale of the model\nsignificantly affect its preferred method of reasoning, with more advanced\nmodels tending to adopt strategies more frequently than less sophisticated\nones. Importantly, we assert that a model's accuracy, that is the correctness\nof its final conclusion, does not necessarily reflect the validity of its\nreasoning process. This distinction underscores the necessity for more nuanced\nevaluation procedures in the field.", "published": "2024-02-20 12:58:14", "link": "http://arxiv.org/abs/2402.14856v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ChatEL: Entity Linking with Chatbots", "abstract": "Entity Linking (EL) is an essential and challenging task in natural language\nprocessing that seeks to link some text representing an entity within a\ndocument or sentence with its corresponding entry in a dictionary or knowledge\nbase. Most existing approaches focus on creating elaborate contextual models\nthat look for clues the words surrounding the entity-text to help solve the\nlinking problem. Although these fine-tuned language models tend to work, they\ncan be unwieldy, difficult to train, and do not transfer well to other domains.\nFortunately, Large Language Models (LLMs) like GPT provide a highly-advanced\nsolution to the problems inherent in EL models, but simply naive prompts to\nLLMs do not work well. In the present work, we define ChatEL, which is a\nthree-step framework to prompt LLMs to return accurate results. Overall the\nChatEL framework improves the average F1 performance across 10 datasets by more\nthan 2%. Finally, a thorough error analysis shows many instances with the\nground truth labels were actually incorrect, and the labels predicted by ChatEL\nwere actually correct. This indicates that the quantitative results presented\nin this paper may be a conservative estimate of the actual performance. All\ndata and code are available as an open-source package on GitHub at\nhttps://github.com/yifding/In_Context_EL.", "published": "2024-02-20 20:52:57", "link": "http://arxiv.org/abs/2402.14858v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "OWSM-CTC: An Open Encoder-Only Speech Foundation Model for Speech\n  Recognition, Translation, and Language Identification", "abstract": "There has been an increasing interest in large speech models that can perform\nmultiple tasks in a single model. Such models usually adopt an encoder-decoder\nor decoder-only architecture due to their popularity and good performance in\nmany domains. However, autoregressive models can be slower during inference\ncompared to non-autoregressive models and also have potential risks of\nhallucination. Though prior studies observed promising results of\nnon-autoregressive models for certain tasks at small scales, it remains unclear\nif they can be scaled to speech-to-text generation in diverse languages and\ntasks. Inspired by the Open Whisper-style Speech Model (OWSM) project, we\npropose OWSM-CTC, a novel encoder-only speech foundation model based on\nConnectionist Temporal Classification (CTC). It is trained on 180k hours of\npublic audio data for multilingual automatic speech recognition (ASR), speech\ntranslation (ST), and language identification (LID). Compared to\nencoder-decoder OWSM, our OWSM-CTC achieves competitive results on ASR and up\nto 24% relative improvement on ST, while it is more robust and 3 to 4 times\nfaster for inference. OWSM-CTC also improves the long-form ASR result with 20x\nspeed-up. We will publicly release our code, pre-trained model, and training\nlogs to promote open science in speech foundation models.", "published": "2024-02-20 02:04:38", "link": "http://arxiv.org/abs/2402.12654v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "FinBen: A Holistic Financial Benchmark for Large Language Models", "abstract": "LLMs have transformed NLP and shown promise in various fields, yet their\npotential in finance is underexplored due to a lack of comprehensive evaluation\nbenchmarks, the rapid development of LLMs, and the complexity of financial\ntasks. In this paper, we introduce FinBen, the first extensive open-source\nevaluation benchmark, including 36 datasets spanning 24 financial tasks,\ncovering seven critical aspects: information extraction (IE), textual analysis,\nquestion answering (QA), text generation, risk management, forecasting, and\ndecision-making. FinBen offers several key innovations: a broader range of\ntasks and datasets, the first evaluation of stock trading, novel agent and\nRetrieval-Augmented Generation (RAG) evaluation, and three novel open-source\nevaluation datasets for text summarization, question answering, and stock\ntrading. Our evaluation of 15 representative LLMs, including GPT-4, ChatGPT,\nand the latest Gemini, reveals several key findings: While LLMs excel in IE and\ntextual analysis, they struggle with advanced reasoning and complex tasks like\ntext generation and forecasting. GPT-4 excels in IE and stock trading, while\nGemini is better at text generation and forecasting. Instruction-tuned LLMs\nimprove textual analysis but offer limited benefits for complex tasks such as\nQA. FinBen has been used to host the first financial LLMs shared task at the\nFinNLP-AgentScen workshop during IJCAI-2024, attracting 12 teams. Their novel\nsolutions outperformed GPT-4, showcasing FinBen's potential to drive innovation\nin financial LLMs. All datasets, results, and codes are released for the\nresearch community: https://github.com/The-FinAI/PIXIU.", "published": "2024-02-20 02:16:16", "link": "http://arxiv.org/abs/2402.12659v2", "categories": ["cs.CL", "cs.AI", "cs.CE"], "primary_category": "cs.CL"}
{"title": "SoftQE: Learned Representations of Queries Expanded by LLMs", "abstract": "We investigate the integration of Large Language Models (LLMs) into query\nencoders to improve dense retrieval without increasing latency and cost, by\ncircumventing the dependency on LLMs at inference time. SoftQE incorporates\nknowledge from LLMs by mapping embeddings of input queries to those of the\nLLM-expanded queries. While improvements over various strong baselines on\nin-domain MS-MARCO metrics are marginal, SoftQE improves performance by 2.83\nabsolute percentage points on average on five out-of-domain BEIR tasks.", "published": "2024-02-20 02:23:15", "link": "http://arxiv.org/abs/2402.12663v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "UMBCLU at SemEval-2024 Task 1A and 1C: Semantic Textual Relatedness with\n  and without machine translation", "abstract": "The aim of SemEval-2024 Task 1, \"Semantic Textual Relatedness for African and\nAsian Languages\" is to develop models for identifying semantic textual\nrelatedness (STR) between two sentences using multiple languages (14 African\nand Asian languages) and settings (supervised, unsupervised, and\ncross-lingual). Large language models (LLMs) have shown impressive performance\non several natural language understanding tasks such as multilingual machine\ntranslation (MMT), semantic similarity (STS), and encoding sentence embeddings.\nUsing a combination of LLMs that perform well on these tasks, we developed two\nSTR models, $\\textit{TranSem}$ and $\\textit{FineSem}$, for the supervised and\ncross-lingual settings. We explore the effectiveness of several training\nmethods and the usefulness of machine translation. We find that direct\nfine-tuning on the task is comparable to using sentence embeddings and\ntranslating to English leads to better performance for some languages. In the\nsupervised setting, our model performance is better than the official baseline\nfor 3 languages with the remaining 4 performing on par. In the cross-lingual\nsetting, our model performance is better than the baseline for 3 languages\n(leading to $1^{st}$ place for Africaans and $2^{nd}$ place for Indonesian), is\non par for 2 languages and performs poorly on the remaining 7 languages. Our\ncode is publicly available at https://github.com/dipta007/SemEval24-Task8.", "published": "2024-02-20 05:46:29", "link": "http://arxiv.org/abs/2402.12730v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models be Used to Provide Psychological Counselling?\n  An Analysis of GPT-4-Generated Responses Using Role-play Dialogues", "abstract": "Mental health care poses an increasingly serious challenge to modern\nsocieties. In this context, there has been a surge in research that utilizes\ninformation technologies to address mental health problems, including those\naiming to develop counseling dialogue systems. However, there is a need for\nmore evaluations of the performance of counseling dialogue systems that use\nlarge language models. For this study, we collected counseling dialogue data\nvia role-playing scenarios involving expert counselors, and the utterances were\nannotated with the intentions of the counselors. To determine the feasibility\nof a dialogue system in real-world counseling scenarios, third-party counselors\nevaluated the appropriateness of responses from human counselors and those\ngenerated by GPT-4 in identical contexts in role-play dialogue data. Analysis\nof the evaluation results showed that the responses generated by GPT-4 were\ncompetitive with those of human counselors.", "published": "2024-02-20 06:05:36", "link": "http://arxiv.org/abs/2402.12738v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Model Composition for Multimodal Large Language Models", "abstract": "Recent developments in Multimodal Large Language Models (MLLMs) have shown\nrapid progress, moving towards the goal of creating versatile MLLMs that\nunderstand inputs from various modalities. However, existing methods typically\nrely on joint training with paired multimodal instruction data, which is\nresource-intensive and challenging to extend to new modalities. In this paper,\nwe propose a new paradigm through the model composition of existing MLLMs to\ncreate a new model that retains the modal understanding capabilities of each\noriginal model. Our basic implementation, NaiveMC, demonstrates the\neffectiveness of this paradigm by reusing modality encoders and merging LLM\nparameters. Furthermore, we introduce DAMC to address parameter interference\nand mismatch issues during the merging process, thereby enhancing the model\nperformance. To facilitate research in this area, we propose MCUB, a benchmark\nfor assessing ability of MLLMs to understand inputs from diverse modalities.\nExperiments on this benchmark and four other multimodal understanding tasks\nshow significant improvements over baselines, proving that model composition\ncan create a versatile model capable of processing inputs from multiple\nmodalities.", "published": "2024-02-20 06:38:10", "link": "http://arxiv.org/abs/2402.12750v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "On Sensitivity of Learning with Limited Labelled Data to the Effects of\n  Randomness: Impact of Interactions and Systematic Choices", "abstract": "While learning with limited labelled data can improve performance when the\nlabels are lacking, it is also sensitive to the effects of uncontrolled\nrandomness introduced by so-called randomness factors (e.g., varying order of\ndata). We propose a method to systematically investigate the effects of\nrandomness factors while taking the interactions between them into\nconsideration. To measure the true effects of an individual randomness factor,\nour method mitigates the effects of other factors and observes how the\nperformance varies across multiple runs. Applying our method to multiple\nrandomness factors across in-context learning and fine-tuning approaches on 7\nrepresentative text classification tasks and meta-learning on 3 tasks, we show\nthat: 1) disregarding interactions between randomness factors in existing works\ncaused inconsistent findings due to incorrect attribution of the effects of\nrandomness factors, such as disproving the consistent sensitivity of in-context\nlearning to sample order even with random sample selection; and 2) besides\nmutual interactions, the effects of randomness factors, especially sample\norder, are also dependent on more systematic choices unexplored in existing\nworks, such as number of classes, samples per class or choice of prompt format.", "published": "2024-02-20 08:38:19", "link": "http://arxiv.org/abs/2402.12817v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Comparing Specialised Small and General Large Language Models on Text\n  Classification: 100 Labelled Samples to Achieve Break-Even Performance", "abstract": "When solving NLP tasks with limited labelled data, researchers can either use\na general large language model without further update, or use a small number of\nlabelled examples to tune a specialised smaller model. In this work, we address\nthe research gap of how many labelled samples are required for the specialised\nsmall models to outperform general large models, while taking the performance\nvariance into consideration. By observing the behaviour of fine-tuning,\ninstruction-tuning, prompting and in-context learning on 7 language models, we\nidentify such performance break-even points across 8 representative text\nclassification tasks of varying characteristics. We show that the specialised\nmodels often need only few samples (on average $10 - 1000$) to be on par or\nbetter than the general ones. At the same time, the number of required labels\nstrongly depends on the dataset or task characteristics, with this number being\nsignificantly lower on multi-class datasets (up to $100$) than on binary\ndatasets (up to $5000$). When performance variance is taken into consideration,\nthe number of required labels increases on average by $100 - 200\\%$ and even up\nto $1500\\%$ in specific cases.", "published": "2024-02-20 08:38:24", "link": "http://arxiv.org/abs/2402.12819v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PromptKD: Distilling Student-Friendly Knowledge for Generative Language\n  Models via Prompt Tuning", "abstract": "Recent advancements in large language models (LLMs) have raised concerns\nabout inference costs, increasing the need for research into model compression.\nWhile knowledge distillation (KD) is a prominent method for this, research on\nKD for generative language models like LLMs is relatively sparse, and the\napproach of distilling student-friendly knowledge, which has shown promising\nperformance in KD for classification models, remains unexplored in generative\nlanguage models. To explore this approach, we propose PromptKD, a simple yet\neffective method that utilizes prompt tuning - for the first time in KD - to\nenable generative language models to transfer student-friendly knowledge.\nUnlike previous works in classification that require fine-tuning the entire\nteacher model for extracting student-friendly knowledge, PromptKD achieves\nsimilar effects by adding a small number of prompt tokens and tuning only the\nprompt with student guidance. Extensive experiments on instruction-following\ndatasets show that PromptKD achieves state-of-the-art performance while adding\nonly 0.0007% of the teacher's parameters as prompts. Further analysis suggests\nthat distilling student-friendly knowledge alleviates exposure bias effectively\nthroughout the entire training process, leading to performance enhancements.", "published": "2024-02-20 09:10:08", "link": "http://arxiv.org/abs/2402.12842v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Instruction-tuned Language Models are Better Knowledge Learners", "abstract": "In order for large language model (LLM)-based assistants to effectively adapt\nto evolving information needs, it must be possible to update their factual\nknowledge through continued training on new data. The standard recipe for doing\nso involves continued pre-training on new documents followed by\ninstruction-tuning on question-answer (QA) pairs. However, we find that LLMs\ntrained with this recipe struggle to answer questions, even though the\nperplexity of documents is minimized. We found that QA pairs are generally\nstraightforward, while documents are more complex, weaving many factual\nstatements together in an intricate manner. Therefore, we hypothesize that it\nis beneficial to expose LLMs to QA pairs before continued pre-training on\ndocuments so that the process of encoding knowledge from complex documents\ntakes into account how this knowledge is accessed through questions. Based on\nthis, we propose pre-instruction-tuning (PIT), a method that instruction-tunes\non questions prior to training on documents. This contrasts with standard\ninstruction-tuning, which learns how to extract knowledge after training on\ndocuments. Extensive experiments and ablation studies demonstrate that\npre-instruction-tuning significantly enhances the ability of LLMs to absorb\nknowledge from new documents, outperforming standard instruction-tuning by\n17.8%.", "published": "2024-02-20 09:20:32", "link": "http://arxiv.org/abs/2402.12847v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Backward Lens: Projecting Language Model Gradients into the Vocabulary\n  Space", "abstract": "Understanding how Transformer-based Language Models (LMs) learn and recall\ninformation is a key goal of the deep learning community. Recent\ninterpretability methods project weights and hidden states obtained from the\nforward pass to the models' vocabularies, helping to uncover how information\nflows within LMs. In this work, we extend this methodology to LMs' backward\npass and gradients. We first prove that a gradient matrix can be cast as a\nlow-rank linear combination of its forward and backward passes' inputs. We then\ndevelop methods to project these gradients into vocabulary items and explore\nthe mechanics of how new information is stored in the LMs' neurons.", "published": "2024-02-20 09:57:08", "link": "http://arxiv.org/abs/2402.12865v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box\n  Identification", "abstract": "Large Language Model (LLM) services and models often come with legal rules on\nwho can use them and how they must use them. Assessing the compliance of the\nreleased LLMs is crucial, as these rules protect the interests of the LLM\ncontributor and prevent misuse. In this context, we describe the novel\nfingerprinting problem of Black-box Identity Verification (BBIV). The goal is\nto determine whether a third-party application uses a certain LLM through its\nchat function. We propose a method called Targeted Random Adversarial Prompt\n(TRAP) that identifies the specific LLM in use. We repurpose adversarial\nsuffixes, originally proposed for jailbreaking, to get a pre-defined answer\nfrom the target LLM, while other models give random answers. TRAP detects the\ntarget LLMs with over 95% true positive rate at under 0.2% false positive rate\neven after a single interaction. TRAP remains effective even if the LLM has\nminor changes that do not significantly alter the original function.", "published": "2024-02-20 13:20:39", "link": "http://arxiv.org/abs/2402.12991v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Towards an empirical understanding of MoE design choices", "abstract": "In this study, we systematically evaluate the impact of common design choices\nin Mixture of Experts (MoEs) on validation performance, uncovering distinct\ninfluences at token and sequence levels. We also present empirical evidence\nshowing comparable performance between a learned router and a frozen, randomly\ninitialized router, suggesting that learned routing may not be essential. Our\nstudy further reveals that Sequence-level routing can result in topic-specific\nweak expert specialization, in contrast to syntax specialization observed with\nToken-level routing.", "published": "2024-02-20 15:31:44", "link": "http://arxiv.org/abs/2402.13089v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Soft Self-Consistency Improves Language Model Agents", "abstract": "Generations from large language models (LLMs) can be improved by sampling and\nscoring multiple solutions to select a final answer. Current \"sample and\nselect\" methods such as self-consistency (SC) rely on majority voting to score\nanswers. However, when tasks have many distinct and valid answers, selection by\nvoting requires a large number of samples. This makes SC prohibitively\nexpensive for interactive tasks that involve generating multiple actions\n(answers) sequentially. After establishing that majority voting fails to\nprovide consistent gains on such tasks, we demonstrate how to increase success\nrates by softening the scoring criterion. We introduce Soft Self-Consistency\n(SOFT-SC), which replaces SC's discontinuous scoring with a continuous score\ncomputed from model likelihoods, allowing for selection even when actions are\nsparsely distributed. SOFT-SC improves both performance and efficiency on\nlong-horizon interactive tasks, requiring half as many samples as SC for\ncomparable or better performance. For a fixed number of samples, SOFT-SC leads\nto a 1.3% increase over SC in absolute success rate on writing bash programs, a\n6.6% increase on online shopping (WebShop), and a 4.7% increase for an\ninteractive household game (ALFWorld). Finally, we show that SOFT-SC can be\napplied to both open-source and black-box models.", "published": "2024-02-20 18:22:38", "link": "http://arxiv.org/abs/2402.13212v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Probabilities of Chat LLMs Are Miscalibrated but Still Predict\n  Correctness on Multiple-Choice Q&A", "abstract": "We study 15 large language models (LLMs) fine-tuned for chat and find that\ntheir maximum softmax probabilities (MSPs) are consistently miscalibrated on\nmultiple-choice Q&A. However, those MSPs might still encode useful uncertainty\ninformation. Specifically, we hypothesized that wrong answers would be\nassociated with smaller MSPs compared to correct answers. Via rigorous\nstatistical testing, we show that this hypothesis holds for models which\nperform well on the underlying Q&A task. We also find a strong direction\ncorrelation between Q&A accuracy and MSP correctness prediction, while finding\nno correlation between Q&A accuracy and calibration error. This suggests that\nwithin the current fine-tuning paradigm, we can expect correctness prediction\nbut not calibration to improve as LLM capabilities progress. To demonstrate the\nutility of correctness prediction, we show that when models have the option to\nabstain, performance can be improved by selectively abstaining based on the MSP\nof the initial model response, using only a small amount of labeled data to\nchoose the MSP threshold.", "published": "2024-02-20 18:24:47", "link": "http://arxiv.org/abs/2402.13213v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive", "abstract": "Direct Preference Optimisation (DPO) is effective at significantly improving\nthe performance of large language models (LLMs) on downstream tasks such as\nreasoning, summarisation, and alignment. Using pairs of preferred and\ndispreferred data, DPO models the relative probability of picking one response\nover another. In this work, first we show theoretically that the standard DPO\nloss can lead to a reduction of the model's likelihood of the preferred\nexamples, as long as the relative probability between the preferred and\ndispreferred classes increases. We then show empirically that this phenomenon\noccurs when fine-tuning LLMs on common datasets, especially datasets in which\nthe edit distance between pairs of completions is low. Using these insights, we\ndesign DPO-Positive (DPOP), a new loss function and training procedure which\navoids this failure mode. Surprisingly, we find that DPOP outperforms DPO and\nother fine-tuning procedures across a wide variety of datasets and downstream\ntasks, including datasets with high edit distances between completions.\nFurthermore, we find that the DPOP-tuned model outperforms the DPO-tuned model\n(all else equal) on benchmarks independent of the fine-tuning data, such as\nMT-Bench. Finally, using DPOP, we create and open-source Smaug-34B and\nSmaug-72B, with the latter becoming the first open-source LLM to surpass an\naverage accuracy of 80% on the HuggingFace Open LLM Leaderboard.", "published": "2024-02-20 18:42:34", "link": "http://arxiv.org/abs/2402.13228v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CounterCurate: Enhancing Physical and Semantic Visio-Linguistic\n  Compositional Reasoning via Counterfactual Examples", "abstract": "We propose CounterCurate, a framework to comprehensively improve the\nvisio-linguistic compositional reasoning capability for both contrastive and\ngenerative multimodal models. In particular, we identify two critical\nunder-explored problems: the neglect of the physically grounded reasoning\n(counting and position understanding) and the potential of using highly capable\ntext and image generation models for semantic counterfactual fine-tuning. Our\nwork pioneers an approach that addresses these gaps. We first spotlight the\nnear-chance performance of multimodal models like CLIP and LLaVA in physically\ngrounded compositional reasoning. We then apply simple data augmentation using\ngrounded image generation model GLIGEN to generate fine-tuning data, resulting\nin significant performance improvements: +33% and +37% for CLIP and LLaVA,\nrespectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we\nexploit the capabilities of high-performing text generation and image\ngeneration models, specifically GPT-4V and DALLE-3, to curate challenging\nsemantic counterfactuals, thereby further enhancing compositional reasoning\ncapabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms\nGPT-4V. To facilitate future research, we release our code, dataset, benchmark,\nand checkpoints at https://countercurate.github.io.", "published": "2024-02-20 18:59:55", "link": "http://arxiv.org/abs/2402.13254v4", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "DrBenchmark: A Large Language Understanding Evaluation Benchmark for\n  French Biomedical Domain", "abstract": "The biomedical domain has sparked a significant interest in the field of\nNatural Language Processing (NLP), which has seen substantial advancements with\npre-trained language models (PLMs). However, comparing these models has proven\nchallenging due to variations in evaluation protocols across different models.\nA fair solution is to aggregate diverse downstream tasks into a benchmark,\nallowing for the assessment of intrinsic PLMs qualities from various\nperspectives. Although still limited to few languages, this initiative has been\nundertaken in the biomedical field, notably English and Chinese. This\nlimitation hampers the evaluation of the latest French biomedical models, as\nthey are either assessed on a minimal number of tasks with non-standardized\nprotocols or evaluated using general downstream tasks. To bridge this research\ngap and account for the unique sensitivities of French, we present the\nfirst-ever publicly available French biomedical language understanding\nbenchmark called DrBenchmark. It encompasses 20 diversified tasks, including\nnamed-entity recognition, part-of-speech tagging, question-answering, semantic\ntextual similarity, and classification. We evaluate 8 state-of-the-art\npre-trained masked language models (MLMs) on general and biomedical-specific\ndata, as well as English specific MLMs to assess their cross-lingual\ncapabilities. Our experiments reveal that no single model excels across all\ntasks, while generalist models are sometimes still competitive.", "published": "2024-02-20 23:54:02", "link": "http://arxiv.org/abs/2402.13432v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "$R^3$: \"This is My SQL, Are You With Me?\" A Consensus-Based Multi-Agent\n  System for Text-to-SQL Tasks", "abstract": "Large Language Models (LLMs) have demonstrated strong performance on various\ntasks. To unleash their power on the Text-to-SQL task, we propose $R^3$\n(Review-Rebuttal-Revision), a consensus-based multi-agent system for\nText-to-SQL tasks. $R^3$ outperforms the existing single LLM Text-to-SQL\nsystems as well as the multi-agent Text-to-SQL systems by $1.3\\%$ to $8.1\\%$ on\nSpider and Bird. Surprisingly, we find that for Llama-3-8B, $R^3$ outperforms\nchain-of-thought prompting by over 20\\%, even outperforming GPT-3.5 on the\ndevelopment set of Spider.", "published": "2024-02-20 03:57:55", "link": "http://arxiv.org/abs/2402.14851v2", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "HumanEval on Latest GPT Models -- 2024", "abstract": "In 2023, we are using the latest models of GPT-4 to advance program\nsynthesis. The large language models have significantly improved the\nstate-of-the-art for this purpose. To make these advancements more accessible,\nwe have created a repository that connects these models to Huamn Eval. This\ndataset was initally developed to be used with a language model called CODEGEN\non natural and programming language data. The utility of these trained models\nis showcased by demonstrating their competitive performance in zero-shot Python\ncode generation on HumanEval tasks compared to previous state-of-the-art\nsolutions. Additionally, this gives way to developing more multi-step paradigm\nsynthesis. This benchmark features 160 diverse problem sets factorized into\nmultistep prompts that our analysis shows significantly improves program\nsynthesis over single-turn inputs. All code is open source at\nhttps://github.com/daniel442li/gpt-human-eval .", "published": "2024-02-20 04:17:21", "link": "http://arxiv.org/abs/2402.14852v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Is the System Message Really Important to Jailbreaks in Large Language\n  Models?", "abstract": "The rapid evolution of Large Language Models (LLMs) has rendered them\nindispensable in modern society. While security measures are typically to align\nLLMs with human values prior to release, recent studies have unveiled a\nconcerning phenomenon named \"Jailbreak\". This term refers to the unexpected and\npotentially harmful responses generated by LLMs when prompted with malicious\nquestions. Most existing research focus on generating jailbreak prompts but\nsystem message configurations vary significantly in experiments. In this paper,\nwe aim to answer a question: Is the system message really important for\njailbreaks in LLMs? We conduct experiments in mainstream LLMs to generate\njailbreak prompts with varying system messages: short, long, and none. We\ndiscover that different system messages have distinct resistances to\njailbreaks. Therefore, we explore the transferability of jailbreaks across LLMs\nwith different system messages. Furthermore, we propose the System Messages\nEvolutionary Algorithm (SMEA) to generate system messages that are more\nresistant to jailbreak prompts, even with minor changes. Through SMEA, we get a\nrobust system messages population with little change in the length of system\nmessages. Our research not only bolsters LLMs security but also raises the bar\nfor jailbreaks, fostering advancements in this field of study.", "published": "2024-02-20 17:39:40", "link": "http://arxiv.org/abs/2402.14857v2", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "PRECISE Framework: GPT-based Text For Improved Readability, Reliability,\n  and Understandability of Radiology Reports For Patient-Centered Care", "abstract": "This study introduces and evaluates the PRECISE framework, utilizing OpenAI's\nGPT-4 to enhance patient engagement by providing clearer and more accessible\nchest X-ray reports at a sixth-grade reading level. The framework was tested on\n500 reports, demonstrating significant improvements in readability,\nreliability, and understandability. Statistical analyses confirmed the\neffectiveness of the PRECISE approach, highlighting its potential to foster\npatient-centric care delivery in healthcare decision-making.", "published": "2024-02-20 04:26:31", "link": "http://arxiv.org/abs/2403.00788v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Thermometer: Towards Universal Calibration for Large Language Models", "abstract": "We consider the issue of calibration in large language models (LLM). Recent\nstudies have found that common interventions such as instruction tuning often\nresult in poorly calibrated LLMs. Although calibration is well-explored in\ntraditional applications, calibrating LLMs is uniquely challenging. These\nchallenges stem as much from the severe computational requirements of LLMs as\nfrom their versatility, which allows them to be applied to diverse tasks.\nAddressing these challenges, we propose THERMOMETER, a calibration approach\ntailored to LLMs. THERMOMETER learns an auxiliary model, given data from\nmultiple tasks, for calibrating a LLM. It is computationally efficient,\npreserves the accuracy of the LLM, and produces better-calibrated responses for\nnew tasks. Extensive empirical evaluations across various benchmarks\ndemonstrate the effectiveness of the proposed method.", "published": "2024-02-20 04:13:48", "link": "http://arxiv.org/abs/2403.08819v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Exploring the Frontier of Vision-Language Models: A Survey of Current\n  Methodologies and Future Directions", "abstract": "The advent of Large Language Models (LLMs) has significantly reshaped the\ntrajectory of the AI revolution. Nevertheless, these LLMs exhibit a notable\nlimitation, as they are primarily adept at processing textual information. To\naddress this constraint, researchers have endeavored to integrate visual\ncapabilities with LLMs, resulting in the emergence of Vision-Language Models\n(VLMs). These advanced models are instrumental in tackling more intricate tasks\nsuch as image captioning and visual question answering. In our comprehensive\nsurvey paper, we delve into the key advancements within the realm of VLMs. Our\nclassification organizes VLMs into three distinct categories: models dedicated\nto vision-language understanding, models that process multimodal inputs to\ngenerate unimodal (textual) outputs and models that both accept and produce\nmultimodal inputs and outputs.This classification is based on their respective\ncapabilities and functionalities in processing and generating various\nmodalities of data.We meticulously dissect each model, offering an extensive\nanalysis of its foundational architecture, training data sources, as well as\nits strengths and limitations wherever possible, providing readers with a\ncomprehensive understanding of its essential components. We also analyzed the\nperformance of VLMs in various benchmark datasets. By doing so, we aim to offer\na nuanced understanding of the diverse landscape of VLMs. Additionally, we\nunderscore potential avenues for future research in this dynamic domain,\nanticipating further breakthroughs and advancements.", "published": "2024-02-20 18:57:34", "link": "http://arxiv.org/abs/2404.07214v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Generative AI Security: Challenges and Countermeasures", "abstract": "Generative AI's expanding footprint across numerous industries has led to\nboth excitement and increased scrutiny. This paper delves into the unique\nsecurity challenges posed by Generative AI, and outlines potential research\ndirections for managing these risks.", "published": "2024-02-20 00:51:05", "link": "http://arxiv.org/abs/2402.12617v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Modality-Aware Integration with Large Language Models for\n  Knowledge-based Visual Question Answering", "abstract": "Knowledge-based visual question answering (KVQA) has been extensively studied\nto answer visual questions with external knowledge, e.g., knowledge graphs\n(KGs). While several attempts have been proposed to leverage large language\nmodels (LLMs) as an implicit knowledge source, it remains challenging since\nLLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g.,\nimages, KGs and LLMs, cannot be readily aligned for complex scenarios. To\ntackle these, we present a novel modality-aware integration with LLMs for KVQA\n(MAIL). It carefully leverages multimodal knowledge for both image\nunderstanding and knowledge reasoning. Specifically, (i) we propose a two-stage\nprompting strategy with LLMs to densely embody the image into a scene graph\nwith detailed visual features; (ii) We construct a coupled concept graph by\nlinking the mentioned entities with external facts. (iii) A tailored\npseudo-siamese graph medium fusion is designed for sufficient multimodal\nfusion. We utilize the shared mentioned entities in two graphs as mediums to\nbridge a tight inter-modal exchange, while maximally preserving insightful\nintra-modal learning by constraining the fusion within mediums. Extensive\nexperiments on two benchmark datasets show the superiority of MAIL with 24x\nless resources.", "published": "2024-02-20 05:32:24", "link": "http://arxiv.org/abs/2402.12728v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Text-Guided Molecule Generation with Diffusion Language Model", "abstract": "Text-guided molecule generation is a task where molecules are generated to\nmatch specific textual descriptions. Recently, most existing SMILES-based\nmolecule generation methods rely on an autoregressive architecture. In this\nwork, we propose the Text-Guided Molecule Generation with Diffusion Language\nModel (TGM-DLM), a novel approach that leverages diffusion models to address\nthe limitations of autoregressive methods. TGM-DLM updates token embeddings\nwithin the SMILES string collectively and iteratively, using a two-phase\ndiffusion generation process. The first phase optimizes embeddings from random\nnoise, guided by the text description, while the second phase corrects invalid\nSMILES strings to form valid molecular representations. We demonstrate that\nTGM-DLM outperforms MolT5-Base, an autoregressive model, without the need for\nadditional data resources. Our findings underscore the remarkable effectiveness\nof TGM-DLM in generating coherent and precise molecules with specific\nproperties, opening new avenues in drug discovery and related scientific\ndomains. Code will be released at: https://github.com/Deno-V/tgm-dlm.", "published": "2024-02-20 14:29:02", "link": "http://arxiv.org/abs/2402.13040v1", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.CL", "q-bio.BM"], "primary_category": "cs.LG"}
{"title": "Guiding the underwater acoustic target recognition with interpretable\n  contrastive learning", "abstract": "Recognizing underwater targets from acoustic signals is a challenging task\nowing to the intricate ocean environments and variable underwater channels.\nWhile deep learning-based systems have become the mainstream approach for\nunderwater acoustic target recognition, they have faced criticism for their\nlack of interpretability and weak generalization performance in practical\napplications. In this work, we apply the class activation mapping (CAM) to\ngenerate visual explanations for the predictions of a spectrogram-based\nrecognition system. CAM can help to understand the behavior of recognition\nmodels by highlighting the regions of the input features that contribute the\nmost to the prediction. Our explorations reveal that recognition models tend to\nfocus on the low-frequency line spectrum and high-frequency periodic modulation\ninformation of underwater signals. Based on the observation, we propose an\ninterpretable contrastive learning (ICL) strategy that employs two encoders to\nlearn from acoustic features with different emphases (line spectrum and\nmodulation information). By imposing constraints between encoders, the proposed\nstrategy can enhance the generalization performance of the recognition system.\nOur experiments demonstrate that the proposed contrastive learning approach can\nimprove the recognition accuracy and bring significant improvements across\nvarious underwater databases.", "published": "2024-02-20 02:14:45", "link": "http://arxiv.org/abs/2402.12658v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Plugin Speech Enhancement: A Universal Speech Enhancement Framework\n  Inspired by Dynamic Neural Network", "abstract": "The expectation to deploy a universal neural network for speech enhancement,\nwith the aim of improving noise robustness across diverse speech processing\ntasks, faces challenges due to the existing lack of awareness within static\nspeech enhancement frameworks regarding the expected speech in downstream\nmodules. These limitations impede the effectiveness of static speech\nenhancement approaches in achieving optimal performance for a range of speech\nprocessing tasks, thereby challenging the notion of universal applicability.\nThe fundamental issue in achieving universal speech enhancement lies in\neffectively informing the speech enhancement module about the features of\ndownstream modules. In this study, we present a novel weighting prediction\napproach, which explicitly learns the task relationships from downstream\ntraining information to address the core challenge of universal speech\nenhancement. We found the role of deciding whether to employ data augmentation\ntechniques as crucial downstream training information. This decision\nsignificantly impacts the expected speech and the performance of the speech\nenhancement module. Moreover, we introduce a novel speech enhancement network,\nthe Plugin Speech Enhancement (Plugin-SE). The Plugin-SE is a dynamic neural\nnetwork that includes the speech enhancement module, gate module, and weight\nprediction module. Experimental results demonstrate that the proposed Plugin-SE\napproach is competitive or superior to other joint training methods across\nvarious downstream tasks.", "published": "2024-02-20 06:24:38", "link": "http://arxiv.org/abs/2402.12746v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "EMO-SUPERB: An In-depth Look at Speech Emotion Recognition", "abstract": "Speech emotion recognition (SER) is a pivotal technology for human-computer\ninteraction systems. However, 80.77% of SER papers yield results that cannot be\nreproduced. We develop EMO-SUPERB, short for EMOtion Speech Universal\nPERformance Benchmark, which aims to enhance open-source initiatives for SER.\nEMO-SUPERB includes a user-friendly codebase to leverage 15 state-of-the-art\nspeech self-supervised learning models (SSLMs) for exhaustive evaluation across\nsix open-source SER datasets. EMO-SUPERB streamlines result sharing via an\nonline leaderboard, fostering collaboration within a community-driven benchmark\nand thereby enhancing the development of SER. On average, 2.58% of annotations\nare annotated using natural language. SER relies on classification models and\nis unable to process natural languages, leading to the discarding of these\nvaluable annotations. We prompt ChatGPT to mimic annotators, comprehend natural\nlanguage annotations, and subsequently re-label the data. By utilizing labels\ngenerated by ChatGPT, we consistently achieve an average relative gain of 3.08%\nacross all settings.", "published": "2024-02-20 14:00:53", "link": "http://arxiv.org/abs/2402.13018v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Codec-SUPERB: An In-Depth Analysis of Sound Codec Models", "abstract": "The sound codec's dual roles in minimizing data transmission latency and\nserving as tokenizers underscore its critical importance. Recent years have\nwitnessed significant developments in codec models. The ideal sound codec\nshould preserve content, paralinguistics, speakers, and audio information.\nHowever, the question of which codec achieves optimal sound information\npreservation remains unanswered, as in different papers, models are evaluated\non their selected experimental settings. This study introduces Codec-SUPERB, an\nacronym for Codec sound processing Universal PERformance Benchmark. It is an\necosystem designed to assess codec models across representative sound\napplications and signal-level metrics rooted in sound domain\nknowledge.Codec-SUPERB simplifies result sharing through an online leaderboard,\npromoting collaboration within a community-driven benchmark database, thereby\nstimulating new development cycles for codecs. Furthermore, we undertake an\nin-depth analysis to offer insights into codec models from both application and\nsignal perspectives, diverging from previous codec papers mainly concentrating\non signal-level comparisons. Finally, we will release codes, the leaderboard,\nand data to accelerate progress within the community.", "published": "2024-02-20 15:13:38", "link": "http://arxiv.org/abs/2402.13071v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards audio language modeling -- an overview", "abstract": "Neural audio codecs are initially introduced to compress audio data into\ncompact codes to reduce transmission latency. Researchers recently discovered\nthe potential of codecs as suitable tokenizers for converting continuous audio\ninto discrete codes, which can be employed to develop audio language models\n(LMs). Numerous high-performance neural audio codecs and codec-based LMs have\nbeen developed. The paper aims to provide a thorough and systematic overview of\nthe neural audio codec models and codec-based LMs.", "published": "2024-02-20 18:50:25", "link": "http://arxiv.org/abs/2402.13236v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SingVisio: Visual Analytics of Diffusion Model for Singing Voice\n  Conversion", "abstract": "In this study, we present SingVisio, an interactive visual analysis system\nthat aims to explain the diffusion model used in singing voice conversion.\nSingVisio provides a visual display of the generation process in diffusion\nmodels, showcasing the step-by-step denoising of the noisy spectrum and its\ntransformation into a clean spectrum that captures the desired singer's timbre.\nThe system also facilitates side-by-side comparisons of different conditions,\nsuch as source content, melody, and target timbre, highlighting the impact of\nthese conditions on the diffusion generation process and resulting conversions.\nThrough comparative and comprehensive evaluations, SingVisio demonstrates its\neffectiveness in terms of system design, functionality, explainability, and\nuser-friendliness. It offers users of various backgrounds valuable learning\nexperiences and insights into the diffusion model for singing voice conversion.", "published": "2024-02-20 02:16:24", "link": "http://arxiv.org/abs/2402.12660v2", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Breaking Down Power Barriers in On-Device Streaming ASR: Insights and\n  Solutions", "abstract": "Power consumption plays a crucial role in on-device streaming speech\nrecognition, significantly influencing the user experience. This study explores\nhow the configuration of weight parameters in speech recognition models affects\ntheir overall energy efficiency. We found that the influence of these\nparameters on power consumption varies depending on factors such as invocation\nfrequency and memory allocation. Leveraging these insights, we propose design\nprinciples that enhance on-device speech recognition models by reducing power\nconsumption with minimal impact on accuracy. Our approach, which adjusts model\ncomponents based on their specific energy sensitivities, achieves up to 47%\nlower energy usage while preserving comparable model accuracy and improving\nreal-time performance compared to leading methods.", "published": "2024-02-20 15:22:25", "link": "http://arxiv.org/abs/2402.13076v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "HiRIS: an Airborne Sonar Sensor with a 1024 Channel Microphone Array for\n  In-Air Acoustic Imaging", "abstract": "Airborne 3D imaging using ultrasound is a promising sensing modality for\nrobotic applications in harsh environments. Over the last decade, several\nhigh-performance systems have been proposed in the literature. Most of these\nsensors use a reduced aperture microphone array, leading to artifacts in the\nresulting acoustic images. This paper presents a novel in-air ultrasound sensor\nthat incorporates 1024 microphones, in a 32-by- 32 uniform rectangular array,\nin combination with a distributed embedded hardware design to perform the data\nacquisition. Using a broadband Minimum Variance Distortionless Response (MVDR)\nbeamformer with Forward-Backward Spatial Smoothing (FB-SS), the sensor is able\nto create both 2D and 3D ultrasound images of the full-frontal hemisphere with\nhigh angular accuracy with up to 70dB main lobe to side lobe ratio. This paper\ndescribes both the hardware infrastructure needed to obtain such highly\ndetailed acoustical images, as well as the signal processing chain needed to\nconvert the raw acoustic data into said images. Utilizing this novel\nhigh-resolution ultrasound imaging sensor, we wish to investigate the limits of\nboth passive and active airborne ultrasound sensing by utilizing this virtually\nartifact-free imaging modality.", "published": "2024-02-20 16:03:02", "link": "http://arxiv.org/abs/2402.13110v1", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Structure-informed Positional Encoding for Music Generation", "abstract": "Music generated by deep learning methods often suffers from a lack of\ncoherence and long-term organization. Yet, multi-scale hierarchical structure\nis a distinctive feature of music signals. To leverage this information, we\npropose a structure-informed positional encoding framework for music generation\nwith Transformers. We design three variants in terms of absolute, relative and\nnon-stationary positional information. We comprehensively test them on two\nsymbolic music generation tasks: next-timestep prediction and accompaniment\ngeneration. As a comparison, we choose multiple baselines from the literature\nand demonstrate the merits of our methods using several musically-motivated\nevaluation metrics. In particular, our methods improve the melodic and\nstructural consistency of the generated pieces.", "published": "2024-02-20 13:41:35", "link": "http://arxiv.org/abs/2402.13301v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "WhaleNet: a Novel Deep Learning Architecture for Marine Mammals\n  Vocalizations on Watkins Marine Mammal Sound Database", "abstract": "Marine mammal communication is a complex field, hindered by the diversity of\nvocalizations and environmental factors. The Watkins Marine Mammal Sound\nDatabase (WMMD) constitutes a comprehensive labeled dataset employed in machine\nlearning applications. Nevertheless, the methodologies for data preparation,\npreprocessing, and classification documented in the literature exhibit\nconsiderable variability and are typically not applied to the dataset in its\nentirety. This study initially undertakes a concise review of the\nstate-of-the-art benchmarks pertaining to the dataset, with a particular focus\non clarifying data preparation and preprocessing techniques. Subsequently, we\nexplore the utilization of the Wavelet Scattering Transform (WST) and Mel\nspectrogram as preprocessing mechanisms for feature extraction. In this paper,\nwe introduce \\textbf{WhaleNet} (Wavelet Highly Adaptive Learning Ensemble\nNetwork), a sophisticated deep ensemble architecture for the classification of\nmarine mammal vocalizations, leveraging both WST and Mel spectrogram for\nenhanced feature discrimination. By integrating the insights derived from WST\nand Mel representations, we achieved an improvement in classification accuracy\nby $8-10\\%$ over existing architectures, corresponding to a classification\naccuracy of $97.61\\%$.", "published": "2024-02-20 11:36:23", "link": "http://arxiv.org/abs/2402.17775v2", "categories": ["eess.SP", "cs.AI", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
