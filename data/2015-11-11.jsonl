{"title": "Larger-Context Language Modelling", "abstract": "In this work, we propose a novel method to incorporate corpus-level discourse\ninformation into language modelling. We call this larger-context language\nmodel. We introduce a late fusion approach to a recurrent language model based\non long short-term memory units (LSTM), which helps the LSTM unit keep\nintra-sentence dependencies and inter-sentence dependencies separate from each\nother. Through the evaluation on three corpora (IMDB, BBC, and PennTree Bank),\nwe demon- strate that the proposed model improves perplexity significantly. In\nthe experi- ments, we evaluate the proposed approach while varying the number\nof context sentences and observe that the proposed late fusion is superior to\nthe usual way of incorporating additional inputs to the LSTM. By analyzing the\ntrained larger- context language model, we discover that content words,\nincluding nouns, adjec- tives and verbs, benefit most from an increasing number\nof context sentences. This analysis suggests that larger-context language model\nimproves the unconditional language model by capturing the theme of a document\nbetter and more easily.", "published": "2015-11-11 23:24:29", "link": "http://arxiv.org/abs/1511.03729v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generative Concatenative Nets Jointly Learn to Write and Classify\n  Reviews", "abstract": "A recommender system's basic task is to estimate how users will respond to\nunseen items. This is typically modeled in terms of how a user might rate a\nproduct, but here we aim to extend such approaches to model how a user would\nwrite about the product. To do so, we design a character-level Recurrent Neural\nNetwork (RNN) that generates personalized product reviews. The network\nconvincingly learns styles and opinions of nearly 1000 distinct authors, using\na large corpus of reviews from BeerAdvocate.com. It also tailors reviews to\ndescribe specific items, categories, and star ratings. Using a simple input\nreplication strategy, the Generative Concatenative Network (GCN) preserves the\nsignal of static auxiliary inputs across wide sequence intervals. Without any\nadditional training, the generative model can classify reviews, identifying the\nauthor of the review, the product category, and the sentiment (rating), with\nremarkable accuracy. Our evaluation shows the GCN captures complex dynamics in\ntext, such as the effect of negation, misspellings, slang, and large\nvocabularies gracefully absent any machinery explicitly dedicated to the\npurpose.", "published": "2015-11-11 21:16:59", "link": "http://arxiv.org/abs/1511.03683v5", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hierarchical Latent Semantic Mapping for Automated Topic Generation", "abstract": "Much of information sits in an unprecedented amount of text data. Managing\nallocation of these large scale text data is an important problem for many\nareas. Topic modeling performs well in this problem. The traditional generative\nmodels (PLSA,LDA) are the state-of-the-art approaches in topic modeling and\nmost recent research on topic generation has been focusing on improving or\nextending these models. However, results of traditional generative models are\nsensitive to the number of topics K, which must be specified manually. The\nproblem of generating topics from corpus resembles community detection in\nnetworks. Many effective algorithms can automatically detect communities from\nnetworks without a manually specified number of the communities. Inspired by\nthese algorithms, in this paper, we propose a novel method named Hierarchical\nLatent Semantic Mapping (HLSM), which automatically generates topics from\ncorpus. HLSM calculates the association between each pair of words in the\nlatent topic space, then constructs a unipartite network of words with this\nassociation and hierarchically generates topics from this network. We apply\nHLSM to several document collections and the experimental comparisons against\nseveral state-of-the-art approaches demonstrate the promising performance.", "published": "2015-11-11 15:58:30", "link": "http://arxiv.org/abs/1511.03546v4", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Deep Multimodal Semantic Embeddings for Speech and Images", "abstract": "In this paper, we present a model which takes as input a corpus of images\nwith relevant spoken captions and finds a correspondence between the two\nmodalities. We employ a pair of convolutional neural networks to model visual\nobjects and speech signals at the word level, and tie the networks together\nwith an embedding and alignment model which learns a joint semantic space over\nboth modalities. We evaluate our model using image search and annotation tasks\non the Flickr8k dataset, which we augmented by collecting a corpus of 40,000\nspoken captions using Amazon Mechanical Turk.", "published": "2015-11-11 21:30:10", "link": "http://arxiv.org/abs/1511.03690v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
