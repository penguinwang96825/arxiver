{"title": "Naturalization of Text by the Insertion of Pauses and Filler Words", "abstract": "In this article, we introduce a set of methods to naturalize text based on\nnatural human speech. Voice-based interactions provide a natural way of\ninterfacing with electronic systems and are seeing a widespread adaptation of\nlate. These computerized voices can be naturalized to some degree by inserting\npauses and filler words at appropriate positions. The first proposed text\ntransformation method uses the frequency of bigrams in the training data to\nmake appropriate insertions in the input sentence. It uses a probability\ndistribution to choose the insertions from a set of all possible insertions.\nThis method is fast and can be included before a Text-To-Speech module. The\nsecond method uses a Recurrent Neural Network to predict the next word to be\ninserted. It confirms the insertions given by the bigram method. Additionally,\nthe degree of naturalization can be controlled in both these methods. On the\nconduction of a blind survey, we conclude that the output of these text\ntransformation methods is comparable to natural speech.", "published": "2020-11-07 06:56:54", "link": "http://arxiv.org/abs/2011.03713v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Know What You Don't Need: Single-Shot Meta-Pruning for Attention Heads", "abstract": "Deep pre-trained Transformer models have achieved state-of-the-art results\nover a variety of natural language processing (NLP) tasks. By learning rich\nlanguage knowledge with millions of parameters, these models are usually\noverparameterized and significantly increase the computational overhead in\napplications. It is intuitive to address this issue by model compression. In\nthis work, we propose a method, called Single-Shot Meta-Pruning, to compress\ndeep pre-trained Transformers before fine-tuning. Specifically, we focus on\npruning unnecessary attention heads adaptively for different downstream tasks.\nTo measure the informativeness of attention heads, we train our Single-Shot\nMeta-Pruner (SMP) with a meta-learning paradigm aiming to maintain the\ndistribution of text representations after pruning. Compared with existing\ncompression methods for pre-trained models, our method can reduce the overhead\nof both fine-tuning and inference. Experimental results show that our pruner\ncan selectively prune 50% of attention heads with little impact on the\nperformance on downstream tasks and even provide better text representations.\nThe source code will be released in the future.", "published": "2020-11-07 12:58:37", "link": "http://arxiv.org/abs/2011.03770v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking the Value of Transformer Components", "abstract": "Transformer becomes the state-of-the-art translation model, while it is not\nwell studied how each intermediate component contributes to the model\nperformance, which poses significant challenges for designing optimal\narchitectures. In this work, we bridge this gap by evaluating the impact of\nindividual component (sub-layer) in trained Transformer models from different\nperspectives. Experimental results across language pairs, training strategies,\nand model capacities show that certain components are consistently more\nimportant than the others. We also report a number of interesting findings that\nmight help humans better analyze, understand and improve Transformer models.\nBased on these observations, we further propose a new training strategy that\ncan improves translation performance by distinguishing the unimportant\ncomponents in training.", "published": "2020-11-07 16:31:45", "link": "http://arxiv.org/abs/2011.03803v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Acoustics Based Intent Recognition Using Discovered Phonetic Units for\n  Low Resource Languages", "abstract": "With recent advancements in language technologies, humans are now speaking to\ndevices. Increasing the reach of spoken language technologies requires building\nsystems in local languages. A major bottleneck here are the underlying\ndata-intensive parts that make up such systems, including automatic speech\nrecognition (ASR) systems that require large amounts of labelled data. With the\naim of aiding development of spoken dialog systems in low resourced languages,\nwe propose a novel acoustics based intent recognition system that uses\ndiscovered phonetic units for intent classification. The system is made up of\ntwo blocks - the first block is a universal phone recognition system that\ngenerates a transcript of discovered phonetic units for the input audio, and\nthe second block performs intent classification from the generated phonetic\ntranscripts. We propose a CNN+LSTM based architecture and present results for\ntwo languages families - Indic languages and Romance languages, for two\ndifferent intent recognition tasks. We also perform multilingual training of\nour intent classifier and show improved cross-lingual transfer and zero-shot\nperformance on an unknown language within the same language family.", "published": "2020-11-07 00:35:31", "link": "http://arxiv.org/abs/2011.03646v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NLP-CIC @ DIACR-Ita: POS and Neighbor Based Distributional Models for\n  Lexical Semantic Change in Diachronic Italian Corpora", "abstract": "We present our systems and findings on unsupervised lexical semantic change\nfor the Italian language in the DIACR-Ita shared-task at EVALITA 2020. The task\nis to determine whether a target word has evolved its meaning with time, only\nrelying on raw-text from two time-specific datasets. We propose two models\nrepresenting the target words across the periods to predict the changing words\nusing threshold and voting schemes. Our first model solely relies on\npart-of-speech usage and an ensemble of distance measures. The second model\nuses word embedding representation to extract the neighbor's relative distances\nacross spaces and propose \"the average of absolute differences\" to estimate\nlexical semantic change. Our models achieved competent results, ranking third\nin the DIACR-Ita competition. Furthermore, we experiment with the k_neighbor\nparameter of our second model to compare the impact of using \"the average of\nabsolute differences\" versus the cosine distance used in Hamilton et al.\n(2016).", "published": "2020-11-07 11:27:18", "link": "http://arxiv.org/abs/2011.03755v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "NLP-CIC @ PRELEARN: Mastering prerequisites relations, from handcrafted\n  features to embeddings", "abstract": "We present our systems and findings for the prerequisite relation learning\ntask (PRELEARN) at EVALITA 2020. The task aims to classify whether a pair of\nconcepts hold a prerequisite relation or not. We model the problem using\nhandcrafted features and embedding representations for in-domain and\ncross-domain scenarios. Our submissions ranked first place in both scenarios\nwith average F1 score of 0.887 and 0.690 respectively across domains on the\ntest sets. We made our code is freely available.", "published": "2020-11-07 12:13:09", "link": "http://arxiv.org/abs/2011.03760v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "AlphaMWE: Construction of Multilingual Parallel Corpora with MWE\n  Annotations", "abstract": "In this work, we present the construction of multilingual parallel corpora\nwith annotation of multiword expressions (MWEs). MWEs include verbal MWEs\n(vMWEs) defined in the PARSEME shared task that have a verb as the head of the\nstudied terms. The annotated vMWEs are also bilingually and multilingually\naligned manually. The languages covered include English, Chinese, Polish, and\nGerman. Our original English corpus is taken from the PARSEME shared task in\n2018. We performed machine translation of this source corpus followed by human\npost editing and annotation of target MWEs. Strict quality control was applied\nfor error limitation, i.e., each MT output sentence received first manual post\nediting and annotation plus second manual quality rechecking. One of our\nfindings during corpora preparation is that accurate translation of MWEs\npresents challenges to MT systems. To facilitate further MT research, we\npresent a categorisation of the error types encountered by MT systems in\nperforming MWE related translation. To acquire a broader view of MT issues, we\nselected four popular state-of-the-art MT models for comparisons namely:\nMicrosoft Bing Translator, GoogleMT, Baidu Fanyi and DeepL MT. Because of the\nnoise removal, translation post editing and MWE annotation by human\nprofessionals, we believe our AlphaMWE dataset will be an asset for\ncross-lingual and multilingual research, such as MT and information extraction.\nOur multilingual corpora are available as open access at\ngithub.com/poethan/AlphaMWE.", "published": "2020-11-07 14:28:54", "link": "http://arxiv.org/abs/2011.03783v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PairRE: Knowledge Graph Embeddings via Paired Relation Vectors", "abstract": "Distance based knowledge graph embedding methods show promising results on\nlink prediction task, on which two topics have been widely studied: one is the\nability to handle complex relations, such as N-to-1, 1-to-N and N-to-N, the\nother is to encode various relation patterns, such as symmetry/antisymmetry.\nHowever, the existing methods fail to solve these two problems at the same\ntime, which leads to unsatisfactory results. To mitigate this problem, we\npropose PairRE, a model with paired vectors for each relation representation.\nThe paired vectors enable an adaptive adjustment of the margin in loss function\nto fit for complex relations. Besides, PairRE is capable of encoding three\nimportant relation patterns, symmetry/antisymmetry, inverse and composition.\nGiven simple constraints on relation representations, PairRE can encode\nsubrelation further. Experiments on link prediction benchmarks demonstrate the\nproposed key capabilities of PairRE. Moreover, We set a new state-of-the-art on\ntwo knowledge graph datasets of the challenging Open Graph Benchmark.", "published": "2020-11-07 16:09:03", "link": "http://arxiv.org/abs/2011.03798v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge-driven Data Construction for Zero-shot Evaluation in\n  Commonsense Question Answering", "abstract": "Recent developments in pre-trained neural language modeling have led to leaps\nin accuracy on commonsense question-answering benchmarks. However, there is\nincreasing concern that models overfit to specific tasks, without learning to\nutilize external knowledge or perform general semantic reasoning. In contrast,\nzero-shot evaluations have shown promise as a more robust measure of a model's\ngeneral reasoning abilities. In this paper, we propose a novel neuro-symbolic\nframework for zero-shot question answering across commonsense tasks. Guided by\na set of hypotheses, the framework studies how to transform various\npre-existing knowledge resources into a form that is most effective for\npre-training models. We vary the set of language models, training regimes,\nknowledge sources, and data generation strategies, and measure their impact\nacross tasks. Extending on prior work, we devise and compare four constrained\ndistractor-sampling strategies. We provide empirical results across five\ncommonsense question-answering tasks with data generated from five external\nknowledge resources. We show that, while an individual knowledge graph is\nbetter suited for specific tasks, a global knowledge graph brings consistent\ngains across different tasks. In addition, both preserving the structure of the\ntask as well as generating fair and informative questions help language models\nlearn more effectively.", "published": "2020-11-07 22:52:21", "link": "http://arxiv.org/abs/2011.03863v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Explainable Automated Fact-Checking: A Survey", "abstract": "A number of exciting advances have been made in automated fact-checking\nthanks to increasingly larger datasets and more powerful systems, leading to\nimprovements in the complexity of claims which can be accurately fact-checked.\nHowever, despite these advances, there are still desirable functionalities\nmissing from the fact-checking pipeline. In this survey, we focus on the\nexplanation functionality -- that is fact-checking systems providing reasons\nfor their predictions. We summarize existing methods for explaining the\npredictions of fact-checking systems and we explore trends in this topic.\nFurther, we consider what makes for good explanations in this specific domain\nthrough a comparative analysis of existing fact-checking explanations against\nsome desirable properties. Finally, we propose further research directions for\ngenerating fact-checking explanations, and describe how these may lead to\nimprovements in the research area.", "published": "2020-11-07 23:56:02", "link": "http://arxiv.org/abs/2011.03870v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Template Controllable keywords-to-text Generation", "abstract": "This paper proposes a novel neural model for the understudied task of\ngenerating text from keywords. The model takes as input a set of un-ordered\nkeywords, and part-of-speech (POS) based template instructions. This makes it\nideal for surface realization in any NLG setup. The framework is based on the\nencode-attend-decode paradigm, where keywords and templates are encoded first,\nand the decoder judiciously attends over the contexts derived from the encoded\nkeywords and templates to generate the sentences. Training exploits weak\nsupervision, as the model trains on a large amount of labeled data with\nkeywords and POS based templates prepared through completely automatic means.\nQualitative and quantitative performance analyses on publicly available\ntest-data in various domains reveal our system's superiority over baselines,\nbuilt using state-of-the-art neural machine translation and controllable\ntransfer techniques. Our approach is indifferent to the order of input\nkeywords.", "published": "2020-11-07 08:05:58", "link": "http://arxiv.org/abs/2011.03722v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.AI"}
{"title": "Sim-to-Real Transfer for Vision-and-Language Navigation", "abstract": "We study the challenging problem of releasing a robot in a previously unseen\nenvironment, and having it follow unconstrained natural language navigation\ninstructions. Recent work on the task of Vision-and-Language Navigation (VLN)\nhas achieved significant progress in simulation. To assess the implications of\nthis work for robotics, we transfer a VLN agent trained in simulation to a\nphysical robot. To bridge the gap between the high-level discrete action space\nlearned by the VLN agent, and the robot's low-level continuous action space, we\npropose a subgoal model to identify nearby waypoints, and use domain\nrandomization to mitigate visual domain differences. For accurate sim and real\ncomparisons in parallel environments, we annotate a 325m2 office space with\n1.3km of navigation instructions, and create a digitized replica in simulation.\nWe find that sim-to-real transfer to an environment not seen in training is\nsuccessful if an occupancy map and navigation graph can be collected and\nannotated in advance (success rate of 46.8% vs. 55.9% in sim), but much more\nchallenging in the hardest setting with no prior mapping at all (success rate\nof 22.5%).", "published": "2020-11-07 16:49:04", "link": "http://arxiv.org/abs/2011.03807v1", "categories": ["cs.CV", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Learning to Model and Ignore Dataset Bias with Mixed Capacity Ensembles", "abstract": "Many datasets have been shown to contain incidental correlations created by\nidiosyncrasies in the data collection process. For example, sentence entailment\ndatasets can have spurious word-class correlations if nearly all contradiction\nsentences contain the word \"not\", and image recognition datasets can have\ntell-tale object-background correlations if dogs are always indoors. In this\npaper, we propose a method that can automatically detect and ignore these kinds\nof dataset-specific patterns, which we call dataset biases. Our method trains a\nlower capacity model in an ensemble with a higher capacity model. During\ntraining, the lower capacity model learns to capture relatively shallow\ncorrelations, which we hypothesize are likely to reflect dataset bias. This\nfrees the higher capacity model to focus on patterns that should generalize\nbetter. We ensure the models learn non-overlapping approaches by introducing a\nnovel method to make them conditionally independent. Importantly, our approach\ndoes not require the bias to be known in advance. We evaluate performance on\nsynthetic datasets, and four datasets built to penalize models that exploit\nknown biases on textual entailment, visual question answering, and image\nrecognition tasks. We show improvement in all settings, including a 10 point\ngain on the visual question answering dataset.", "published": "2020-11-07 22:20:03", "link": "http://arxiv.org/abs/2011.03856v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Non-local convolutional neural networks (nlcnn) for speaker recognition", "abstract": "Speaker recognition is the process of identifying a speaker based on the\nvoice. The technology has attracted more attention with the recent increase in\npopularity of smart voice assistants, such as Amazon Alexa. In the past few\nyears, various convolutional neural network (CNN) based speaker recognition\nalgorithms have been proposed and achieved satisfactory performance. However,\nconvolutional operations are building blocks that typically perform on a local\nneighborhood at a time and thus miss to capture global, long-range interactions\nat the feature level which are critical for understanding the pattern in a\nspeaker's voice. In this work, we propose to apply Non-local Convolutional\nNeural Networks (NLCNN) to improve the capability of capturing long-range\ndependencies at the feature level, therefore improving speaker recognition\nperformance. Specifically, we introduce non-local blocks where the output\nresponse of a position is computed as a weighted sum of the input features at\nall positions. Combining non-local blocks with pre-defined CNN networks, we\ninvestigate the effectiveness of NLCNN models. Without extensive tuning, the\nproposed NLCNN models outperform state-of-the-art speaker recognition\nalgorithms on the public Voxceleb dataset. What's more, we investigate\ndifferent types of non-local operations applied to the frequency-time domain,\ntime domain, frequency domain and frame-level respectively. Among them, time\ndomain is the most effective one for speaker recognition applications.", "published": "2020-11-07 04:02:17", "link": "http://arxiv.org/abs/2011.03682v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Detection and Evaluation of human and machine generated speech in\n  spoofing attacks on automatic speaker verification systems", "abstract": "Automatic speaker verification (ASV) systems utilize the biometric\ninformation in human speech to verify the speaker's identity. The techniques\nused for performing speaker verification are often vulnerable to malicious\nattacks that attempt to induce the ASV system to return wrong results, allowing\nan impostor to bypass the system and gain access. Attackers use a multitude of\nspoofing techniques for this, such as voice conversion, audio replay, speech\nsynthesis, etc. In recent years, easily available tools to generate deepfaked\naudio have increased the potential threat to ASV systems. In this paper, we\ncompare the potential of human impersonation (voice disguise) based attacks\nwith attacks based on machine-generated speech, on black-box and white-box ASV\nsystems. We also study countermeasures by using features that capture the\nunique aspects of human speech production, under the hypothesis that machines\ncannot emulate many of the fine-level intricacies of the human speech\nproduction mechanism. We show that fundamental frequency sequence-related\nentropy, spectral envelope, and aperiodic parameters are promising candidates\nfor robust detection of deepfaked speech generated by unknown methods.", "published": "2020-11-07 04:42:27", "link": "http://arxiv.org/abs/2011.03689v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ESPnet-se: end-to-end speech enhancement and separation toolkit designed\n  for asr integration", "abstract": "We present ESPnet-SE, which is designed for the quick development of speech\nenhancement and speech separation systems in a single framework, along with the\noptional downstream speech recognition module. ESPnet-SE is a new project which\nintegrates rich automatic speech recognition related models, resources and\nsystems to support and validate the proposed front-end implementation (i.e.\nspeech enhancement and separation).It is capable of processing both\nsingle-channel and multi-channel data, with various functionalities including\ndereverberation, denoising and source separation. We provide all-in-one recipes\nincluding data pre-processing, feature extraction, training and evaluation\npipelines for a wide range of benchmark datasets. This paper describes the\ndesign of the toolkit, several important functionalities, especially the speech\nrecognition integration, which differentiates ESPnet-SE from other open source\ntoolkits, and experimental results with major benchmark datasets.", "published": "2020-11-07 06:14:18", "link": "http://arxiv.org/abs/2011.03706v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Enhancement by postfiltering for speech and audio coding in ad-hoc\n  sensor networks", "abstract": "Enhancement algorithms for wireless acoustics sensor networks~(WASNs) are\nindispensable with the increasing availability and usage of connected devices\nwith microphones. Conventional spatial filtering approaches for enhancement in\nWASNs approximate quantization noise with an additive Gaussian distribution,\nwhich limits performance due to the non-linear nature of quantization noise at\nlower bitrates. In this work, we propose a postfilter for enhancement based on\nBayesian statistics to obtain a multidevice signal estimate, which explicitly\nmodels the quantization noise. Our experiments using PSNR, PESQ and MUSHRA\nscores demonstrate that the proposed postfilter can be used to enhance signal\nquality in ad-hoc sensor networks.", "published": "2020-11-07 17:06:18", "link": "http://arxiv.org/abs/2011.03810v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Dual Application of Speech Enhancement for Automatic Speech Recognition", "abstract": "In this work, we exploit speech enhancement for improving a recurrent neural\nnetwork transducer (RNN-T) based ASR system. We employ a dense convolutional\nrecurrent network (DCRN) for complex spectral mapping based speech enhancement,\nand find it helpful for ASR in two ways: a data augmentation technique, and a\npreprocessing frontend. In using it for ASR data augmentation, we exploit a KL\ndivergence based consistency loss that is computed between the ASR outputs of\noriginal and enhanced utterances. In using speech enhancement as an effective\nASR frontend, we propose a three-step training scheme based on model\npretraining and feature selection. We evaluate our proposed techniques on a\nchallenging social media English video dataset, and achieve an average relative\nimprovement of 11.2% with speech enhancement based data augmentation, 8.3% with\nenhancement based preprocessing, and 13.4% when combining both.", "published": "2020-11-07 19:56:55", "link": "http://arxiv.org/abs/2011.03840v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
