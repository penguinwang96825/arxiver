{"title": "Vietnamese Capitalization and Punctuation Recovery Models", "abstract": "Despite the rise of recent performant methods in Automatic Speech Recognition\n(ASR), such methods do not ensure proper casing and punctuation for their\noutputs. This problem has a significant impact on the comprehension of both\nNatural Language Processing (NLP) algorithms and human to process.\nCapitalization and punctuation restoration is imperative in pre-processing\npipelines for raw textual inputs. For low resource languages like Vietnamese,\npublic datasets for this task are scarce. In this paper, we contribute a public\ndataset for capitalization and punctuation recovery for Vietnamese; and propose\na joint model for both tasks named JointCapPunc. Experimental results on the\nVietnamese dataset show the effectiveness of our joint model compare to single\nmodel and previous joint learning model. We publicly release our dataset and\nthe implementation of our model at\nhttps://github.com/anhtunguyen98/JointCapPunc", "published": "2022-07-04 10:45:24", "link": "http://arxiv.org/abs/2207.01312v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BoAT v2 -- A Web-Based Dependency Annotation Tool with Focus on\n  Agglutinative Languages", "abstract": "The value of quality treebanks is steadily increasing due to the crucial role\nthey play in the development of natural language processing tools. The creation\nof such treebanks is enormously labor-intensive and time-consuming. Especially\nwhen the size of treebanks is considered, tools that support the annotation\nprocess are essential. Various annotation tools have been proposed, however,\nthey are often not suitable for agglutinative languages such as Turkish. BoAT\nv1 was developed for annotating dependency relations and was subsequently used\nto create the manually annotated BOUN Treebank (UD_Turkish-BOUN). In this work,\nwe report on the design and implementation of a dependency annotation tool BoAT\nv2 based on the experiences gained from the use of BoAT v1, which revealed\nseveral opportunities for improvement. BoAT v2 is a multi-user and web-based\ndependency annotation tool that is designed with a focus on the annotator user\nexperience to yield valid annotations. The main objectives of the tool are to:\n(1) support creating valid and consistent annotations with increased speed, (2)\nsignificantly improve the user experience of the annotator, (3) support\ncollaboration among annotators, and (4) provide an open-source and easily\ndeployable web-based annotation tool with a flexible application programming\ninterface (API) to benefit the scientific community. This paper discusses the\nrequirements elicitation, design, and implementation of BoAT v2 along with\nexamples.", "published": "2022-07-04 11:10:11", "link": "http://arxiv.org/abs/2207.01327v2", "categories": ["cs.CL", "I.2.7; J.m; E.m"], "primary_category": "cs.CL"}
{"title": "VEM$^2$L: A Plug-and-play Framework for Fusing Text and Structure\n  Knowledge on Sparse Knowledge Graph Completion", "abstract": "Knowledge Graph Completion (KGC) aims to reason over known facts and infer\nmissing links but achieves weak performances on those sparse Knowledge Graphs\n(KGs). Recent works introduce text information as auxiliary features or apply\ngraph densification to alleviate this challenge, but suffer from problems of\nineffectively incorporating structure features and injecting noisy triples. In\nthis paper, we solve the sparse KGC from these two motivations simultaneously\nand handle their respective drawbacks further, and propose a plug-and-play\nunified framework VEM$^2$L over sparse KGs. The basic idea of VEM$^2$L is to\nmotivate a text-based KGC model and a structure-based KGC model to learn with\neach other to fuse respective knowledge into unity. To exploit text and\nstructure features together in depth, we partition knowledge within models into\ntwo nonoverlapping parts: expressiveness ability on the training set and\ngeneralization ability upon unobserved queries. For the former, we motivate\nthese two text-based and structure-based models to learn from each other on the\ntraining sets. And for the generalization ability, we propose a novel knowledge\nfusion strategy derived by the Variational EM (VEM) algorithm, during which we\nalso apply a graph densification operation to alleviate the sparse graph\nproblem further. Our graph densification is derived by VEM algorithm. Due to\nthe convergence of EM algorithm, we guarantee the increase of likelihood\nfunction theoretically with less being impacted by noisy injected triples\nheavily. By combining these two fusion methods and graph densification, we\npropose the VEM$^2$L framework finally. Both detailed theoretical evidence, as\nwell as qualitative experiments, demonstrates the effectiveness of our proposed\nframework.", "published": "2022-07-04 15:50:21", "link": "http://arxiv.org/abs/2207.01528v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Cascade Model for Argument Mining in Japanese Political Discussions:\n  the QA Lab-PoliInfo-3 Case Study", "abstract": "The rVRAIN team tackled the Budget Argument Mining (BAM) task, consisting of\na combination of classification and information retrieval sub-tasks. For the\nargument classification (AC), the team achieved its best performing results\nwith a five-class BERT-based cascade model complemented with some handcrafted\nrules. The rules were used to determine if the expression was monetary or not.\nThen, each monetary expression was classified as a premise or as a conclusion\nin the first level of the cascade model. Finally, each premise was classified\ninto the three premise classes, and each conclusion into the two conclusion\nclasses. For the information retrieval (i.e., relation ID detection or RID),\nour best results were achieved by a combination of a BERT-based binary\nclassifier, and the cosine similarity of pairs consisting of the monetary\nexpression and budget dense embeddings.", "published": "2022-07-04 18:49:18", "link": "http://arxiv.org/abs/2207.01672v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Location reference recognition from texts: A survey and comparison", "abstract": "A vast amount of location information exists in unstructured texts, such as\nsocial media posts, news stories, scientific articles, web pages, travel blogs,\nand historical archives. Geoparsing refers to the process of recognizing\nlocation references from texts and identifying their geospatial\nrepresentations. While geoparsing can benefit many domains, a summary of the\nspecific applications is still missing. Further, there lacks a comprehensive\nreview and comparison of existing approaches for location reference\nrecognition, which is the first and a core step of geoparsing. To fill these\nresearch gaps, this review first summarizes seven typical application domains\nof geoparsing: geographic information retrieval, disaster management, disease\nsurveillance, traffic management, spatial humanities, tourism management, and\ncrime management. We then review existing approaches for location reference\nrecognition by categorizing these approaches into four groups based on their\nunderlying functional principle: rule-based, gazetteer matching-based,\nstatistical learning-based, and hybrid approaches. Next, we thoroughly evaluate\nthe correctness and computational efficiency of the 27 most widely used\napproaches for location reference recognition based on 26 public datasets with\ndifferent types of texts (e.g., social media posts and news stories) containing\n39,736 location references across the world. Results from this thorough\nevaluation can help inform future methodological developments for location\nreference recognition, and can help guide the selection of proper approaches\nbased on application needs.", "published": "2022-07-04 19:25:15", "link": "http://arxiv.org/abs/2207.01683v1", "categories": ["cs.CL", "Natural language processing", "H.3.3; I.2.7"], "primary_category": "cs.CL"}
{"title": "Probing via Prompting", "abstract": "Probing is a popular method to discern what linguistic information is\ncontained in the representations of pre-trained language models. However, the\nmechanism of selecting the probe model has recently been subject to intense\ndebate, as it is not clear if the probes are merely extracting information or\nmodeling the linguistic property themselves. To address this challenge, this\npaper introduces a novel model-free approach to probing, by formulating probing\nas a prompting task. We conduct experiments on five probing tasks and show that\nour approach is comparable or better at extracting information than diagnostic\nprobes while learning much less on its own. We further combine the probing via\nprompting approach with attention head pruning to analyze where the model\nstores the linguistic information in its architecture. We then examine the\nusefulness of a specific linguistic property for pre-training by removing the\nheads that are essential to that property and evaluating the resulting model's\nperformance on language modeling.", "published": "2022-07-04 22:14:40", "link": "http://arxiv.org/abs/2207.01736v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Customized Text Sanitization Mechanism with Differential Privacy", "abstract": "As privacy issues are receiving increasing attention within the Natural\nLanguage Processing (NLP) community, numerous methods have been proposed to\nsanitize texts subject to differential privacy. However, the state-of-the-art\ntext sanitization mechanisms based on metric local differential privacy (MLDP)\ndo not apply to non-metric semantic similarity measures and cannot achieve good\ntrade-offs between privacy and utility. To address the above limitations, we\npropose a novel Customized Text (CusText) sanitization mechanism based on the\noriginal $\\epsilon$-differential privacy (DP) definition, which is compatible\nwith any similarity measure. Furthermore, CusText assigns each input token a\ncustomized output set of tokens to provide more advanced privacy protection at\nthe token level. Extensive experiments on several benchmark datasets show that\nCusText achieves a better trade-off between privacy and utility than existing\nmechanisms. The code is available at https://github.com/sai4july/CusText.", "published": "2022-07-04 04:37:42", "link": "http://arxiv.org/abs/2207.01193v2", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Attributed Abnormality Graph Embedding for Clinically Accurate X-Ray\n  Report Generation", "abstract": "Automatic generation of medical reports from X-ray images can assist\nradiologists to perform the time-consuming and yet important reporting task.\nYet, achieving clinically accurate generated reports remains challenging.\nModeling the underlying abnormalities using the knowledge graph approach has\nbeen found promising in enhancing the clinical accuracy. In this paper, we\nintroduce a novel fined-grained knowledge graph structure called an attributed\nabnormality graph (ATAG). The ATAG consists of interconnected abnormality nodes\nand attribute nodes, allowing it to better capture the abnormality details. In\ncontrast to the existing methods where the abnormality graph was constructed\nmanually, we propose a methodology to automatically construct the fine-grained\ngraph structure based on annotations, medical reports in X-ray datasets, and\nthe RadLex radiology lexicon. We then learn the ATAG embedding using a deep\nmodel with an encoder-decoder architecture for the report generation. In\nparticular, graph attention networks are explored to encode the relationships\namong the abnormalities and their attributes. A gating mechanism is adopted and\nintegrated with various decoders for the generation. We carry out extensive\nexperiments based on the benchmark datasets, and show that the proposed\nATAG-based deep model outperforms the SOTA methods by a large margin and can\nimprove the clinical accuracy of the generated reports.", "published": "2022-07-04 05:32:00", "link": "http://arxiv.org/abs/2207.01208v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Understanding Performance of Long-Document Ranking Models through\n  Comprehensive Evaluation and Leaderboarding", "abstract": "We evaluated 20+ Transformer models for ranking of long documents (including\nrecent LongP models trained with FlashAttention) and compared them with a\nsimple FirstP baseline, which applies the same model to the truncated input (at\nmost 512 tokens). We used MS MARCO Documents v1 as a primary training set and\nevaluated both the zero-shot transferred and fine-tuned models.\n  On MS MARCO, TREC DLs, and Robust04 no long-document model outperformed\nFirstP by more than 5% in NDCG and MRR (when averaged over all test sets). We\nconjectured this was not due to models' inability to process long context, but\ndue to a positional bias of relevant passages, whose distribution was skewed\ntowards the beginning of documents. We found direct evidence of this bias in\nsome test sets, which motivated us to create MS MARCO FarRelevant (based on MS\nMARCO Passages) where the relevant passages were not present among the first\n512 tokens.\n  Unlike standard collections where we saw both little benefit from\nincorporating longer contexts and limited variability in model performance\n(within a few %), experiments on MS MARCO FarRelevant uncovered dramatic\ndifferences among models. The FirstP models performed roughly at the\nrandom-baseline level in both zero-shot and fine-tuning scenarios. Simple\naggregation models including MaxP and PARADE Attention had good zero-shot\naccuracy, but benefited little from fine-tuning. Most other models had poor\nzero-shot performance (sometimes at a random baseline level), but outstripped\nMaxP by as much as 13-28% after fine-tuning. Thus, the positional bias not only\ndiminishes benefits of processing longer document contexts, but also leads to\nmodel overfitting to positional bias and performing poorly in a zero-shot\nsetting when the distribution of relevant passages changes substantially. We\nmake our software and data available.", "published": "2022-07-04 08:54:43", "link": "http://arxiv.org/abs/2207.01262v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Discourse-Aware Graph Networks for Textual Logical Reasoning", "abstract": "Textual logical reasoning, especially question-answering (QA) tasks with\nlogical reasoning, requires awareness of particular logical structures. The\npassage-level logical relations represent entailment or contradiction between\npropositional units (e.g., a concluding sentence). However, such structures are\nunexplored as current QA systems focus on entity-based relations. In this work,\nwe propose logic structural-constraint modeling to solve the logical reasoning\nQA and introduce discourse-aware graph networks (DAGNs). The networks first\nconstruct logic graphs leveraging in-line discourse connectives and generic\nlogic theories, then learn logic representations by end-to-end evolving the\nlogic relations with an edge-reasoning mechanism and updating the graph\nfeatures. This pipeline is applied to a general encoder, whose fundamental\nfeatures are joined with the high-level logic features for answer prediction.\nExperiments on three textual logical reasoning datasets demonstrate the\nreasonability of the logical structures built in DAGNs and the effectiveness of\nthe learned logic features. Moreover, zero-shot transfer results show the\nfeatures' generality to unseen logical texts.", "published": "2022-07-04 14:38:49", "link": "http://arxiv.org/abs/2207.01450v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unify and Conquer: How Phonetic Feature Representation Affects Polyglot\n  Text-To-Speech (TTS)", "abstract": "An essential design decision for multilingual Neural Text-To-Speech (NTTS)\nsystems is how to represent input linguistic features within the model. Looking\nat the wide variety of approaches in the literature, two main paradigms emerge,\nunified and separate representations. The former uses a shared set of phonetic\ntokens across languages, whereas the latter uses unique phonetic tokens for\neach language. In this paper, we conduct a comprehensive study comparing\nmultilingual NTTS systems models trained with both representations. Our results\nreveal that the unified approach consistently achieves better cross-lingual\nsynthesis with respect to both naturalness and accent. Separate representations\ntend to have an order of magnitude more tokens than unified ones, which may\naffect model capacity. For this reason, we carry out an ablation study to\nunderstand the interaction of the representation type with the size of the\ntoken embedding. We find that the difference between the two paradigms only\nemerges above a certain threshold embedding size. This study provides strong\nevidence that unified representations should be the preferred paradigm when\nbuilding multilingual NTTS systems.", "published": "2022-07-04 16:14:57", "link": "http://arxiv.org/abs/2207.01547v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "BERT, can HE predict contrastive focus? Predicting and controlling\n  prominence in neural TTS using a language model", "abstract": "Several recent studies have tested the use of transformer language model\nrepresentations to infer prosodic features for text-to-speech synthesis (TTS).\nWhile these studies have explored prosody in general, in this work, we look\nspecifically at the prediction of contrastive focus on personal pronouns. This\nis a particularly challenging task as it often requires semantic, discursive\nand/or pragmatic knowledge to predict correctly. We collect a corpus of\nutterances containing contrastive focus and we evaluate the accuracy of a BERT\nmodel, finetuned to predict quantized acoustic prominence features, on these\nsamples. We also investigate how past utterances can provide relevant\ninformation for this prediction. Furthermore, we evaluate the controllability\nof pronoun prominence in a TTS model conditioned on acoustic prominence\nfeatures.", "published": "2022-07-04 20:43:41", "link": "http://arxiv.org/abs/2207.01718v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multilingual Disinformation Detection for Digital Advertising", "abstract": "In today's world, the presence of online disinformation and propaganda is\nmore widespread than ever. Independent publishers are funded mostly via digital\nadvertising, which is unfortunately also the case for those publishing\ndisinformation content. The question of how to remove such publishers from\nadvertising inventory has long been ignored, despite the negative impact on the\nopen internet. In this work, we make the first step towards quickly detecting\nand red-flagging websites that potentially manipulate the public with\ndisinformation. We build a machine learning model based on multilingual text\nembeddings that first determines whether the page mentions a topic of interest,\nthen estimates the likelihood of the content being malicious, creating a\nshortlist of publishers that will be reviewed by human experts. Our system\nempowers internal teams to proactively, rather than defensively, blacklist\nunsafe content, thus protecting the reputation of the advertisement provider.", "published": "2022-07-04 10:29:20", "link": "http://arxiv.org/abs/2207.10649v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "WebShop: Towards Scalable Real-World Web Interaction with Grounded\n  Language Agents", "abstract": "Existing benchmarks for grounding language in interactive environments either\nlack real-world linguistic elements, or prove difficult to scale up due to\nsubstantial human involvement in the collection of data or feedback signals. To\nbridge this gap, we develop WebShop -- a simulated e-commerce website\nenvironment with $1.18$ million real-world products and $12,087$ crowd-sourced\ntext instructions. Given a text instruction specifying a product requirement,\nan agent needs to navigate multiple types of webpages and issue diverse actions\nto find, customize, and purchase an item. WebShop provides several challenges\nfor language grounding including understanding compositional instructions,\nquery (re-)formulation, comprehending and acting on noisy text in webpages, and\nperforming strategic exploration. We collect over $1,600$ human demonstrations\nfor the task, and train and evaluate a diverse range of agents using\nreinforcement learning, imitation learning, and pre-trained image and language\nmodels. Our best model achieves a task success rate of $29\\%$, which\noutperforms rule-based heuristics ($9.6\\%$) but is far lower than human expert\nperformance ($59\\%$). We also analyze agent and human trajectories and ablate\nvarious model components to provide insights for developing future agents with\nstronger language understanding and decision making abilities. Finally, we show\nthat agents trained on WebShop exhibit non-trivial sim-to-real transfer when\nevaluated on amazon.com and ebay.com, indicating the potential value of WebShop\nin developing practical web-based agents that can operate in the wild.", "published": "2022-07-04 05:30:22", "link": "http://arxiv.org/abs/2207.01206v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Using contextual sentence analysis models to recognize ESG concepts", "abstract": "This paper summarizes the joint participation of the Trading Central Labs and\nthe L3i laboratory of the University of La Rochelle on both sub-tasks of the\nShared Task FinSim-4 evaluation campaign. The first sub-task aims to enrich the\n'Fortia ESG taxonomy' with new lexicon entries while the second one aims to\nclassify sentences to either 'sustainable' or 'unsustainable' with respect to\nESG (Environment, Social and Governance) related factors. For the first\nsub-task, we proposed a model based on pre-trained Sentence-BERT models to\nproject sentences and concepts in a common space in order to better represent\nESG concepts. The official task results show that our system yields a\nsignificant performance improvement compared to the baseline and outperforms\nall other submissions on the first sub-task. For the second sub-task, we\ncombine the RoBERTa model with a feed-forward multi-layer perceptron in order\nto extract the context of sentences and classify them. Our model achieved high\naccuracy scores (over 92%) and was ranked among the top 5 systems.", "published": "2022-07-04 13:33:21", "link": "http://arxiv.org/abs/2207.01402v1", "categories": ["cs.CL", "cs.LG", "q-fin.GN"], "primary_category": "cs.CL"}
{"title": "Comparing Feature Importance and Rule Extraction for Interpretability on\n  Text Data", "abstract": "Complex machine learning algorithms are used more and more often in critical\ntasks involving text data, leading to the development of interpretability\nmethods. Among local methods, two families have emerged: those computing\nimportance scores for each feature and those extracting simple logical rules.\nIn this paper we show that using different methods can lead to unexpectedly\ndifferent explanations, even when applied to simple models for which we would\nexpect qualitative coincidence. To quantify this effect, we propose a new\napproach to compare explanations produced by different methods.", "published": "2022-07-04 13:54:55", "link": "http://arxiv.org/abs/2207.01420v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Dynamic Contrastive Distillation for Image-Text Retrieval", "abstract": "Although the vision-and-language pretraining (VLP) equipped cross-modal\nimage-text retrieval (ITR) has achieved remarkable progress in the past two\nyears, it suffers from a major drawback: the ever-increasing size of VLP models\nrestricts its deployment to real-world search scenarios (where the high latency\nis unacceptable). To alleviate this problem, we present a novel plug-in dynamic\ncontrastive distillation (DCD) framework to compress the large VLP models for\nthe ITR task. Technically, we face the following two challenges: 1) the typical\nuni-modal metric learning approach is difficult to directly apply to the\ncross-modal tasks, due to the limited GPU memory to optimize too many negative\nsamples during handling cross-modal fusion features. 2) it is inefficient to\nstatic optimize the student network from different hard samples, which have\ndifferent effects on distillation learning and student network optimization. We\ntry to overcome these challenges from two points. First, to achieve multi-modal\ncontrastive learning, and balance the training costs and effects, we propose to\nuse a teacher network to estimate the difficult samples for students, making\nthe students absorb the powerful knowledge from pre-trained teachers, and\nmaster the knowledge from hard samples. Second, to dynamic learn from hard\nsample pairs, we propose dynamic distillation to dynamically learn samples of\ndifferent difficulties, from the perspective of better balancing the difficulty\nof knowledge and students' self-learning ability. We successfully apply our\nproposed DCD strategy to two state-of-the-art vision-language pretrained\nmodels, i.e. ViLT and METER. Extensive experiments on MS-COCO and Flickr30K\nbenchmarks show the effectiveness and efficiency of our DCD framework.\nEncouragingly, we can speed up the inference at least 129$\\times$ compared to\nthe existing ITR models.", "published": "2022-07-04 14:08:59", "link": "http://arxiv.org/abs/2207.01426v1", "categories": ["cs.MM", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.MM"}
{"title": "GlowVC: Mel-spectrogram space disentangling model for\n  language-independent text-free voice conversion", "abstract": "In this paper, we propose GlowVC: a multilingual multi-speaker flow-based\nmodel for language-independent text-free voice conversion. We build on\nGlow-TTS, which provides an architecture that enables use of linguistic\nfeatures during training without the necessity of using them for VC inference.\nWe consider two versions of our model: GlowVC-conditional and GlowVC-explicit.\nGlowVC-conditional models the distribution of mel-spectrograms with\nspeaker-conditioned flow and disentangles the mel-spectrogram space into\ncontent- and pitch-relevant dimensions, while GlowVC-explicit models the\nexplicit distribution with unconditioned flow and disentangles said space into\ncontent-, pitch- and speaker-relevant dimensions. We evaluate our models in\nterms of intelligibility, speaker similarity and naturalness for intra- and\ncross-lingual conversion in seen and unseen languages. GlowVC models greatly\noutperform AutoVC baseline in terms of intelligibility, while achieving just as\nhigh speaker similarity in intra-lingual VC, and slightly worse in the\ncross-lingual setting. Moreover, we demonstrate that GlowVC-explicit surpasses\nboth GlowVC-conditional and AutoVC in terms of naturalness.", "published": "2022-07-04 14:42:44", "link": "http://arxiv.org/abs/2207.01454v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Mix and Match: An Empirical Study on Training Corpus Composition for\n  Polyglot Text-To-Speech (TTS)", "abstract": "Training multilingual Neural Text-To-Speech (NTTS) models using only\nmonolingual corpora has emerged as a popular way for building voice cloning\nbased Polyglot NTTS systems. In order to train these models, it is essential to\nunderstand how the composition of the training corpora affects the quality of\nmultilingual speech synthesis. In this context, it is common to hear questions\nsuch as \"Would including more Spanish data help my Italian synthesis, given the\ncloseness of both languages?\". Unfortunately, we found existing literature on\nthe topic lacking in completeness in this regard. In the present work, we\nconduct an extensive ablation study aimed at understanding how various factors\nof the training corpora, such as language family affiliation, gender\ncomposition, and the number of speakers, contribute to the quality of Polyglot\nsynthesis. Our findings include the observation that female speaker data are\npreferred in most scenarios, and that it is not always beneficial to have more\nspeakers from the target language variant in the training corpus. The findings\nherein are informative for the process of data procurement and corpora\nbuilding.", "published": "2022-07-04 15:23:06", "link": "http://arxiv.org/abs/2207.01507v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Disentangled Action Recognition with Knowledge Bases", "abstract": "Action in video usually involves the interaction of human with objects.\nAction labels are typically composed of various combinations of verbs and\nnouns, but we may not have training data for all possible combinations. In this\npaper, we aim to improve the generalization ability of the compositional action\nrecognition model to novel verbs or novel nouns that are unseen during training\ntime, by leveraging the power of knowledge graphs. Previous work utilizes\nverb-noun compositional action nodes in the knowledge graph, making it\ninefficient to scale since the number of compositional action nodes grows\nquadratically with respect to the number of verbs and nouns. To address this\nissue, we propose our approach: Disentangled Action Recognition with\nKnowledge-bases (DARK), which leverages the inherent compositionality of\nactions. DARK trains a factorized model by first extracting disentangled\nfeature representations for verbs and nouns, and then predicting classification\nweights using relations in external knowledge graphs. The type constraint\nbetween verb and noun is extracted from external knowledge bases and finally\napplied when composing actions. DARK has better scalability in the number of\nobjects and verbs, and achieves state-of-the-art performance on the Charades\ndataset. We further propose a new benchmark split based on the Epic-kitchen\ndataset which is an order of magnitude bigger in the numbers of classes and\nsamples, and benchmark various models on this benchmark.", "published": "2022-07-04 20:19:13", "link": "http://arxiv.org/abs/2207.01708v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Building a Relation Extraction Baseline for Gene-Disease Associations: A\n  Reproducibility Study", "abstract": "Reproducibility is an important task in scientific research. It is crucial\nfor researchers to compare newly developed systems with the state-of-the-art to\nassess whether they made a breakthrough. However previous works may not be\nimmediately reproducible, for example due to the lack of source code. In this\nwork we reproduce DEXTER, a system to automatically extract Gene-Disease\nAssociations (GDAs) from biomedical abstracts. The goal is to provide a\nbenchmark for future works regarding Relation Extraction (RE), enabling\nresearchers to test and compare their results.", "published": "2022-07-04 08:19:43", "link": "http://arxiv.org/abs/2207.06226v1", "categories": ["cs.CL", "cs.IR", "q-bio.QM"], "primary_category": "cs.CL"}
{"title": "Adversarial Multi-Task Deep Learning for Noise-Robust Voice Activity\n  Detection with Low Algorithmic Delay", "abstract": "Voice Activity Detection (VAD) is an important pre-processing step in a wide\nvariety of speech processing systems. VAD should in a practical application be\nable to detect speech in both noisy and noise-free environments, while not\nintroducing significant latency. In this work we propose using an adversarial\nmulti-task learning method when training a supervised VAD. The method has been\napplied to the state-of-the-art VAD Waveform-based Voice Activity Detection.\nAdditionally the performance of the VADis investigated under different\nalgorithmic delays, which is an important factor in latency. Introducing\nadversarial multi-task learning to the model is observed to increase\nperformance in terms of Area Under Curve (AUC), particularly in noisy\nenvironments, while the performance is not degraded at higher SNR levels. The\nadversarial multi-task learning is only applied in the training phase and thus\nintroduces no additional cost in testing. Furthermore the correlation between\nperformance and algorithmic delays is investigated, and it is observed that the\nVAD performance degradation is only moderate when lowering the algorithmic\ndelay from 398 ms to 23 ms.", "published": "2022-07-04 19:43:09", "link": "http://arxiv.org/abs/2207.01691v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "DEFORMER: Coupling Deformed Localized Patterns with Global Context for\n  Robust End-to-end Speech Recognition", "abstract": "Convolutional neural networks (CNN) have improved speech recognition\nperformance greatly by exploiting localized time-frequency patterns. But these\npatterns are assumed to appear in symmetric and rigid kernels by the\nconventional CNN operation. It motivates the question: What about asymmetric\nkernels? In this study, we illustrate adaptive views can discover local\nfeatures which couple better with attention than fixed views of the input. We\nreplace depthwise CNNs in the Conformer architecture with a deformable\ncounterpart, dubbed this \"Deformer\". By analyzing our best-performing model, we\nvisualize both local receptive fields and global attention maps learned by the\nDeformer and show increased feature associations on the utterance level. The\nstatistical analysis of learned kernel offsets provides an insight into the\nchange of information in features with the network depth. Finally, replacing\nonly half of the layers in the encoder, the Deformer improves +5.6% relative\nWER without a LM and +6.4% relative WER with a LM over the Conformer baseline\non the WSJ eval92 set.", "published": "2022-07-04 21:49:05", "link": "http://arxiv.org/abs/2207.01732v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Cross-speaker Emotion Transfer Based On Prosody Compensation for\n  End-to-End Speech Synthesis", "abstract": "Cross-speaker emotion transfer speech synthesis aims to synthesize emotional\nspeech for a target speaker by transferring the emotion from reference speech\nrecorded by another (source) speaker. In this task, extracting\nspeaker-independent emotion embedding from reference speech plays an important\nrole. However, the emotional information conveyed by such emotion embedding\ntends to be weakened in the process to squeeze out the source speaker's timbre\ninformation. In response to this problem, a prosody compensation module (PCM)\nis proposed in this paper to compensate for the emotional information loss.\nSpecifically, the PCM tries to obtain speaker-independent emotional information\nfrom the intermediate feature of a pre-trained ASR model. To this end, a\nprosody compensation encoder with global context (GC) blocks is introduced to\nobtain global emotional information from the ASR model's intermediate feature.\nExperiments demonstrate that the proposed PCM can effectively compensate the\nemotion embedding for the emotional information loss, and meanwhile maintain\nthe timbre of the target speaker. Comparisons with state-of-the-art models show\nthat our proposed method presents obvious superiority on the cross-speaker\nemotion transfer task.", "published": "2022-07-04 04:56:10", "link": "http://arxiv.org/abs/2207.01198v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TMGAN-PLC: Audio Packet Loss Concealment using Temporal Memory\n  Generative Adversarial Network", "abstract": "Real-time communications in packet-switched networks have become widely used\nin daily communication, while they inevitably suffer from network delays and\ndata losses in constrained real-time conditions. To solve these problems, audio\npacket loss concealment (PLC) algorithms have been developed to mitigate voice\ntransmission failures by reconstructing the lost information. Limited by the\ntransmission latency and device memory, it is still intractable for PLC to\naccomplish high-quality voice reconstruction using a relatively small packet\nbuffer. In this paper, we propose a temporal memory generative adversarial\nnetwork for audio PLC, dubbed TMGAN-PLC, which is comprised of a novel\nnested-UNet generator and the time-domain/frequency-domain discriminators.\nSpecifically, a combination of the nested-UNet and temporal feature-wise linear\nmodulation is elaborately devised in the generator to finely adjust the\nintra-frame information and establish inter-frame temporal dependencies. To\ncomplement the missing speech content caused by longer loss bursts, we employ\nmulti-stage gated vector quantizers to capture the correct content and\nreconstruct the near-real smooth audio. Extensive experiments on the PLC\nChallenge dataset demonstrate that the proposed method yields promising\nperformance in terms of speech quality, intelligibility, and PLCMOS.", "published": "2022-07-04 08:27:19", "link": "http://arxiv.org/abs/2207.01255v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Minimizing Sequential Confusion Error in Speech Command Recognition", "abstract": "Speech command recognition (SCR) has been commonly used on resource\nconstrained devices to achieve hands-free user experience. However, in real\napplications, confusion among commands with similar pronunciations often\nhappens due to the limited capacity of small models deployed on edge devices,\nwhich drastically affects the user experience. In this paper, inspired by the\nadvances of discriminative training in speech recognition, we propose a novel\nminimize sequential confusion error (MSCE) training criterion particularly for\nSCR, aiming to alleviate the command confusion problem. Specifically, we aim to\nimprove the ability of discriminating the target command from other commands on\nthe basis of MCE discriminative criteria. We define the likelihood of different\ncommands through connectionist temporal classification (CTC). During training,\nwe propose several strategies to use prior knowledge creating a confusing\nsequence set for similar-sounding command instead of creating the whole\nnon-target command set, which can better save the training resources and\neffectively reduce command confusion errors. Specifically, we design and\ncompare three different strategies for confusing set construction. By using our\nproposed method, we can relatively reduce the False Reject Rate~(FRR) by 33.7%\nat 0.01 False Alarm Rate~(FAR) and confusion errors by 18.28% on our collected\nspeech command set.", "published": "2022-07-04 08:50:32", "link": "http://arxiv.org/abs/2207.01261v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CaTT-KWS: A Multi-stage Customized Keyword Spotting Framework based on\n  Cascaded Transducer-Transformer", "abstract": "Customized keyword spotting (KWS) has great potential to be deployed on edge\ndevices to achieve hands-free user experience. However, in real applications,\nfalse alarm (FA) would be a serious problem for spotting dozens or even\nhundreds of keywords, which drastically affects user experience. To solve this\nproblem, in this paper, we leverage the recent advances in transducer and\ntransformer based acoustic models and propose a new multi-stage customized KWS\nframework named Cascaded Transducer-Transformer KWS (CaTT-KWS), which includes\na transducer based keyword detector, a frame-level phone predictor based force\nalignment module and a transformer based decoder. Specifically, the streaming\ntransducer module is used to spot keyword candidates in audio stream. Then\nforce alignment is implemented using the phone posteriors predicted by the\nphone predictor to finish the first stage keyword verification and refine the\ntime boundaries of keyword. Finally, the transformer decoder further verifies\nthe triggered keyword. Our proposed CaTT-KWS framework reduces FA rate\neffectively without obviously hurting keyword recognition accuracy.\nSpecifically, we can get impressively 0.13 FA per hour on a challenging\ndataset, with over 90% relative reduction on FA comparing to the transducer\nbased detection model, while keyword recognition accuracy only drops less than\n2%.", "published": "2022-07-04 09:01:30", "link": "http://arxiv.org/abs/2207.01267v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Stochastic Restoration of Heavily Compressed Musical Audio using\n  Generative Adversarial Networks", "abstract": "Lossy audio codecs compress (and decompress) digital audio streams by\nremoving information that tends to be inaudible in human perception. Under high\ncompression rates, such codecs may introduce a variety of impairments in the\naudio signal. Many works have tackled the problem of audio enhancement and\ncompression artifact removal using deep learning techniques. However, only a\nfew works tackle the restoration of heavily compressed audio signals in the\nmusical domain. In such a scenario, there is no unique solution for the\nrestoration of the original signal. Therefore, in this study, we test a\nstochastic generator of a Generative Adversarial Network (GAN) architecture for\nthis task. Such a stochastic generator, conditioned on highly compressed\nmusical audio signals, could one day generate outputs indistinguishable from\nhigh-quality releases. Therefore, the present study may yield insights into\nmore efficient musical data storage and transmission. We train stochastic and\ndeterministic generators on MP3-compressed audio signals with 16, 32, and 64\nkbit/s. We perform an extensive evaluation of the different experiments\nutilizing objective metrics and listening tests. We find that the models can\nimprove the quality of the audio signals over the MP3 versions for 16 and 32\nkbit/s and that the stochastic generators are capable of generating outputs\nthat are closer to the original signals than those of the deterministic\ngenerators.", "published": "2022-07-04 18:33:26", "link": "http://arxiv.org/abs/2207.01667v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-Modal Multi-Correlation Learning for Audio-Visual Speech\n  Separation", "abstract": "In this paper we propose a multi-modal multi-correlation learning framework\ntargeting at the task of audio-visual speech separation. Although previous\nefforts have been extensively put on combining audio and visual modalities,\nmost of them solely adopt a straightforward concatenation of audio and visual\nfeatures. To exploit the real useful information behind these two modalities,\nwe define two key correlations which are: (1) identity correlation (between\ntimbre and facial attributes); (2) phonetic correlation (between phoneme and\nlip motion). These two correlations together comprise the complete information,\nwhich shows a certain superiority in separating target speaker's voice\nespecially in some hard cases, such as the same gender or similar content. For\nimplementation, contrastive learning or adversarial training approach is\napplied to maximize these two correlations. Both of them work well, while\nadversarial training shows its advantage by avoiding some limitations of\ncontrastive learning. Compared with previous research, our solution\ndemonstrates clear improvement on experimental metrics without additional\ncomplexity. Further analysis reveals the validity of the proposed architecture\nand its good potential for future extension.", "published": "2022-07-04 04:53:39", "link": "http://arxiv.org/abs/2207.01197v1", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Semi-blind source separation using convolutive transfer function for\n  nonlinear acoustic echo cancellation", "abstract": "The recently proposed semi-blind source separation (SBSS) method for\nnonlinear acoustic echo cancellation (NAEC) outperforms adaptive NAEC in\nattenuating the nonlinear acoustic echo. However, the multiplicative transfer\nfunction (MTF) approximation makes it unsuitable for real-time applications\nespecially in highly reverberant environments, and the natural gradient makes\nit hard to balance well between fast convergence speed and stability. In this\npaper, we propose two more effective SBSS methods based on\nauxiliary-function-based independent vector analysis (AuxIVA) and independent\nlow-rank matrix analysis (ILRMA). The convolutive transfer function (CTF)\napproximation is used instead of MTF so that a long impulse response can be\nmodeled with a short latency. The optimization schemes used in AuxIVA and ILRMA\nare carefully regularized according to the constrained demixing matrix of NAEC.\nExperimental results validate significantly better echo cancellation\nperformance of the proposed methods.", "published": "2022-07-04 16:24:37", "link": "http://arxiv.org/abs/2207.01556v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "An adaptive music generation architecture for games based on the deep\n  learning Transformer mode", "abstract": "This paper presents an architecture for generating music for video games\nbased on the Transformer deep learning model. Our motivation is to be able to\ncustomize the generation according to the taste of the player, who can select a\ncorpus of training examples, corresponding to his preferred musical style. The\nsystem generates various musical layers, following the standard layering\nstrategy currently used by composers designing video game music. To adapt the\nmusic generated to the game play and to the player(s) situation, we are using\nan arousal-valence model of emotions, in order to control the selection of\nmusical layers. We discuss current limitations and prospects for the future,\nsuch as collaborative and interactive control of the musical components.", "published": "2022-07-04 19:53:43", "link": "http://arxiv.org/abs/2207.01698v2", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS", "OOA65, 91A18, 68T07", "J.5"], "primary_category": "cs.SD"}
