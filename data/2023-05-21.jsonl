{"title": "F-PABEE: Flexible-patience-based Early Exiting for Single-label and\n  Multi-label text Classification Tasks", "abstract": "Computational complexity and overthinking problems have become the\nbottlenecks for pre-training language models (PLMs) with millions or even\ntrillions of parameters. A Flexible-Patience-Based Early Exiting method\n(F-PABEE) has been proposed to alleviate the problems mentioned above for\nsingle-label classification (SLC) and multi-label classification (MLC) tasks.\nF-PABEE makes predictions at the classifier and will exit early if predicted\ndistributions of cross-layer are consecutively similar. It is more flexible\nthan the previous state-of-the-art (SOTA) early exiting method PABEE because it\ncan simultaneously adjust the similarity score thresholds and the patience\nparameters. Extensive experiments show that: (1) F-PABEE makes a better\nspeedup-accuracy balance than existing early exiting strategies on both SLC and\nMLC tasks. (2) F-PABEE achieves faster inference and better performances on\ndifferent PLMs such as BERT and ALBERT. (3) F-PABEE-JSKD performs best for\nF-PABEE with different similarity measures.", "published": "2023-05-21 12:17:27", "link": "http://arxiv.org/abs/2305.11916v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OntoType: Ontology-Guided and Pre-Trained Language Model Assisted\n  Fine-Grained Entity Typing", "abstract": "Fine-grained entity typing (FET), which assigns entities in text with\ncontext-sensitive, fine-grained semantic types, is a basic but important task\nfor knowledge extraction from unstructured text. FET has been studied\nextensively in natural language processing and typically relies on\nhuman-annotated corpora for training, which is costly and difficult to scale.\nRecent studies explore the utilization of pre-trained language models (PLMs) as\na knowledge base to generate rich and context-aware weak supervision for FET.\nHowever, a PLM still requires direction and guidance to serve as a knowledge\nbase as they often generate a mixture of rough and fine-grained types, or\ntokens unsuitable for typing. In this study, we vision that an ontology\nprovides a semantics-rich, hierarchical structure, which will help select the\nbest results generated by multiple PLM models and head words. Specifically, we\npropose a novel annotation-free, ontology-guided FET method, OntoType, which\nfollows a type ontological structure, from coarse to fine, ensembles multiple\nPLM prompting results to generate a set of type candidates, and refines its\ntype resolution, under the local context with a natural language inference\nmodel. Our experiments on the Ontonotes, FIGER, and NYT datasets using their\nassociated ontological structures demonstrate that our method outperforms the\nstate-of-the-art zero-shot fine-grained entity typing methods as well as a\ntypical LLM method, ChatGPT. Our error analysis shows that refinement of the\nexisting ontology structures will further improve fine-grained entity typing.", "published": "2023-05-21 00:32:37", "link": "http://arxiv.org/abs/2305.12307v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Task-agnostic Distillation of Encoder-Decoder Language Models", "abstract": "Finetuning pretrained language models (LMs) have enabled appealing\nperformance on a diverse array of tasks. The intriguing task-agnostic property\nhas driven a shifted focus from task-specific to task-agnostic distillation of\nLMs. While task-agnostic, compute-efficient, performance-preserved LMs can be\nyielded by task-agnostic distillation, previous studies mainly sit in\ndistillation of either encoder-only LMs (e.g., BERT) or decoder-only ones\n(e.g., GPT) yet largely neglect that distillation of encoder-decoder LMs (e.g.,\nT5) can posit very distinguished behaviors. Frustratingly, we discover that\nexisting task-agnostic distillation methods can fail to handle the distillation\nof encoder-decoder LMs. To the demand, we explore a few paths and uncover a\npath named as MiniEnD that successfully tackles the distillation of\nencoder-decoder LMs in a task-agnostic fashion. We examine MiniEnD on language\nunderstanding and abstractive summarization. The results showcase that MiniEnD\nis generally effective and is competitive compared to other alternatives. We\nfurther scale MiniEnD up to distillation of 3B encoder-decoder language models\nwith interpolated distillation. The results imply the opportunities and\nchallenges in distilling large language models (e.g., LLaMA).", "published": "2023-05-21 03:35:45", "link": "http://arxiv.org/abs/2305.12330v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Machine Translation by Projecting Text into the Same\n  Phonetic-Orthographic Space Using a Common Encoding", "abstract": "The use of subword embedding has proved to be a major innovation in Neural\nMachine Translation (NMT). It helps NMT to learn better context vectors for Low\nResource Languages (LRLs) so as to predict the target words by better modelling\nthe morphologies of the two languages and also the morphosyntax transfer. Even\nso, their performance for translation in Indian language to Indian language\nscenario is still not as good as for resource-rich languages. One reason for\nthis is the relative morphological richness of Indian languages, while another\nis that most of them fall into the extremely low resource or zero-shot\ncategories. Since most major Indian languages use Indic or Brahmi origin\nscripts, the text written in them is highly phonetic in nature and phonetically\nsimilar in terms of abstract letters and their arrangements. We use these\ncharacteristics of Indian languages and their scripts to propose an approach\nbased on common multilingual Latin-based encodings (WX notation) that take\nadvantage of language similarity while addressing the morphological complexity\nissue in NMT. These multilingual Latin-based encodings in NMT, together with\nByte Pair Embedding (BPE) allow us to better exploit their phonetic and\northographic as well as lexical similarities to improve the translation quality\nby projecting different but similar languages on the same orthographic-phonetic\ncharacter space. We verify the proposed approach by demonstrating experiments\non similar language pairs (Gujarati-Hindi, Marathi-Hindi, Nepali-Hindi,\nMaithili-Hindi, Punjabi-Hindi, and Urdu-Hindi) under low resource conditions.\nThe proposed approach shows an improvement in a majority of cases, in one case\nas much as ~10 BLEU points compared to baseline techniques for similar language\npairs. We also get up to ~1 BLEU points improvement on distant and zero-shot\nlanguage pairs.", "published": "2023-05-21 06:46:33", "link": "http://arxiv.org/abs/2305.12371v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SHINE: Syntax-augmented Hierarchical Interactive Encoder for Zero-shot\n  Cross-lingual Information Extraction", "abstract": "Zero-shot cross-lingual information extraction(IE) aims at constructing an IE\nmodel for some low-resource target languages, given annotations exclusively in\nsome rich-resource languages. Recent studies based on language-universal\nfeatures have shown their effectiveness and are attracting increasing\nattention. However, prior work has neither explored the potential of\nestablishing interactions between language-universal features and contextual\nrepresentations nor incorporated features that can effectively model\nconstituent span attributes and relationships between multiple spans. In this\nstudy, a syntax-augmented hierarchical interactive encoder (SHINE) is proposed\nto transfer cross-lingual IE knowledge. The proposed encoder is capable of\ninteractively capturing complementary information between features and\ncontextual information, to derive language-agnostic representations for various\nIE tasks. Concretely, a multi-level interaction network is designed to\nhierarchically interact the complementary information to strengthen domain\nadaptability. Besides, in addition to the well-studied syntax features of\npart-of-speech and dependency relation, a new syntax feature of constituency\nstructure is introduced to model the constituent span information which is\ncrucial for IE. Experiments across seven languages on three IE tasks and four\nbenchmarks verify the effectiveness and generalization ability of the proposed\nmethod.", "published": "2023-05-21 08:02:06", "link": "http://arxiv.org/abs/2305.12389v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PiVe: Prompting with Iterative Verification Improving Graph-based\n  Generative Capability of LLMs", "abstract": "Large language models (LLMs) have shown great abilities of solving various\nnatural language tasks in different domains. Due to the training objective of\nLLMs and their pre-training data, LLMs are not very well equipped for tasks\ninvolving structured data generation. We propose a framework, Prompting with\nIterative Verification (PiVe), to improve graph-based generative capability of\nLLMs. We show how a small language model could be trained to act as a verifier\nmodule for the output of an LLM~(i.e., ChatGPT, GPT-4), and to iteratively\nimprove its performance via fine-grained corrective instructions. We also show\nhow the verifier module could apply iterative corrections offline for a more\ncost-effective solution to the text-to-graph generation task. Experiments on\nthree graph-based datasets show consistent improvement gained via PiVe.\nAdditionally, we create GenWiki-HIQ and highlight that the verifier module can\nbe used as a data augmentation tool to help improve the quality of\nautomatically generated parallel text-graph datasets.", "published": "2023-05-21 08:11:24", "link": "http://arxiv.org/abs/2305.12392v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pruning Pre-trained Language Models with Principled Importance and\n  Self-regularization", "abstract": "Iterative pruning is one of the most effective compression methods for\npre-trained language models. We discovered that finding the optimal pruning\ndecision is an equality-constrained 0-1 Integer Linear Programming problem. The\nsolution to this optimization problem leads to a principled importance\ncriterion which we use to rank parameters during iterative model pruning. To\nmitigate the poor generalization at high sparsity levels, we propose a\nself-regularization scheme where model prediction is regularized by the latest\ncheckpoint with increasing sparsity throughout pruning. Our experiments on\nnatural language understanding, question-answering, named entity recognition,\nand data-to-text generation with various Transformer-based PLMs show the\neffectiveness of the approach at various sparsity levels.", "published": "2023-05-21 08:15:12", "link": "http://arxiv.org/abs/2305.12394v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WOT-Class: Weakly Supervised Open-world Text Classification", "abstract": "State-of-the-art weakly supervised text classification methods, while\nsignificantly reduced the required human supervision, still requires the\nsupervision to cover all the classes of interest. This is never easy to meet in\npractice when human explore new, large corpora without complete pictures. In\nthis paper, we work on a novel yet important problem of weakly supervised\nopen-world text classification, where supervision is only needed for a few\nexamples from a few known classes and the machine should handle both known and\nunknown classes in test time. General open-world classification has been\nstudied mostly using image classification; however, existing methods typically\nassume the availability of sufficient known-class supervision and strong\nunknown-class prior knowledge (e.g., the number and/or data distribution). We\npropose a novel framework WOT-Class that lifts those strong assumptions.\nSpecifically, it follows an iterative process of (a) clustering text to new\nclasses, (b) mining and ranking indicative words for each class, and (c)\nmerging redundant classes by using the overlapped indicative words as a bridge.\nExtensive experiments on 7 popular text classification datasets demonstrate\nthat WOT-Class outperforms strong baselines consistently with a large margin,\nattaining 23.33% greater average absolute macro-F1 over existing approaches\nacross all datasets. Such competent accuracy illuminates the practical\npotential of further reducing human effort for text classification.", "published": "2023-05-21 08:51:24", "link": "http://arxiv.org/abs/2305.12401v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EM Pre-training for Multi-party Dialogue Response Generation", "abstract": "Dialogue response generation requires an agent to generate a response\naccording to the current dialogue history, in terms of which two-party\ndialogues have been well studied, but leaving a great gap for multi-party\ndialogues at the same time. Different from two-party dialogues where each\nresponse is a direct reply to its previous utterance, the addressee of a\nresponse utterance should be specified before it is generated in the\nmulti-party scenario. Thanks to the huge amount of two-party conversational\ndata, various pre-trained language models for two-party dialogue response\ngeneration have been proposed. However, due to the lack of annotated addressee\nlabels in multi-party dialogue datasets, it is hard to use them to pre-train a\nresponse generation model for multi-party dialogues. To tackle this obstacle,\nwe propose an Expectation-Maximization (EM) approach that iteratively performs\nthe expectation steps to generate addressee labels, and the maximization steps\nto optimize a response generation model. Theoretical analyses and extensive\nexperiments have justified the feasibility and effectiveness of our proposed\nmethod.", "published": "2023-05-21 09:22:41", "link": "http://arxiv.org/abs/2305.12412v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Pilot Study on Dialogue-Level Dependency Parsing for Chinese", "abstract": "Dialogue-level dependency parsing has received insufficient attention,\nespecially for Chinese. To this end, we draw on ideas from syntactic dependency\nand rhetorical structure theory (RST), developing a high-quality\nhuman-annotated corpus, which contains 850 dialogues and 199,803 dependencies.\nConsidering that such tasks suffer from high annotation costs, we investigate\nzero-shot and few-shot scenarios. Based on an existing syntactic treebank, we\nadopt a signal-based method to transform seen syntactic dependencies into\nunseen ones between elementary discourse units (EDUs), where the signals are\ndetected by masked language modeling. Besides, we apply single-view and\nmulti-view data selection to access reliable pseudo-labeled instances.\nExperimental results show the effectiveness of these baselines. Moreover, we\ndiscuss several crucial points about our dataset and approach.", "published": "2023-05-21 12:20:13", "link": "http://arxiv.org/abs/2305.12441v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Infor-Coef: Information Bottleneck-based Dynamic Token Downsampling for\n  Compact and Efficient language model", "abstract": "The prevalence of Transformer-based pre-trained language models (PLMs) has\nled to their wide adoption for various natural language processing tasks.\nHowever, their excessive overhead leads to large latency and computational\ncosts. The statically compression methods allocate fixed computation to\ndifferent samples, resulting in redundant computation. The dynamic token\npruning method selectively shortens the sequences but are unable to change the\nmodel size and hardly achieve the speedups as static pruning. In this paper, we\npropose a model accelaration approaches for large language models that\nincorporates dynamic token downsampling and static pruning, optimized by the\ninformation bottleneck loss. Our model, Infor-Coef, achieves an 18x FLOPs\nspeedup with an accuracy degradation of less than 8\\% compared to BERT. This\nwork provides a promising approach to compress and accelerate transformer-based\nmodels for NLP tasks.", "published": "2023-05-21 13:30:56", "link": "http://arxiv.org/abs/2305.12458v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is Translation Helpful? An Empirical Analysis of Cross-Lingual Transfer\n  in Low-Resource Dialog Generation", "abstract": "Cross-lingual transfer is important for developing high-quality chatbots in\nmultiple languages due to the strongly imbalanced distribution of language\nresources. A typical approach is to leverage off-the-shelf machine translation\n(MT) systems to utilize either the training corpus or developed models from\nhigh-resource languages. In this work, we investigate whether it is helpful to\nutilize MT at all in this task. To do so, we simulate a low-resource scenario\nassuming access to limited Chinese dialog data in the movie domain and large\namounts of English dialog data from multiple domains. Experiments show that\nleveraging English dialog corpora can indeed improve the naturalness, relevance\nand cross-domain transferability in Chinese. However, directly using English\ndialog corpora in its original form, surprisingly, is better than using its\ntranslated version. As the topics and wording habits in daily conversations are\nstrongly culture-dependent, MT can reinforce the bias from high-resource\nlanguages, yielding unnatural generations in the target language. Considering\nthe cost of translating large amounts of text and the strong effects of the\ntranslation quality, we suggest future research should rather focus on\nutilizing the original English data for cross-lingual transfer in dialog\ngeneration. We perform extensive human evaluations and ablation studies. The\nanalysis results, together with the collected dataset, are presented to draw\nattention towards this area and benefit future research.", "published": "2023-05-21 15:07:04", "link": "http://arxiv.org/abs/2305.12480v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VAKTA-SETU: A Speech-to-Speech Machine Translation Service in Select\n  Indic Languages", "abstract": "In this work, we present our deployment-ready Speech-to-Speech Machine\nTranslation (SSMT) system for English-Hindi, English-Marathi, and Hindi-Marathi\nlanguage pairs. We develop the SSMT system by cascading Automatic Speech\nRecognition (ASR), Disfluency Correction (DC), Machine Translation (MT), and\nText-to-Speech Synthesis (TTS) models. We discuss the challenges faced during\nthe research and development stage and the scalable deployment of the SSMT\nsystem as a publicly accessible web service. On the MT part of the pipeline\ntoo, we create a Text-to-Text Machine Translation (TTMT) service in all six\ntranslation directions involving English, Hindi, and Marathi. To mitigate data\nscarcity, we develop a LaBSE-based corpus filtering tool to select high-quality\nparallel sentences from a noisy pseudo-parallel corpus for training the TTMT\nsystem. All the data used for training the SSMT and TTMT systems and the best\nmodels are being made publicly available. Users of our system are (a) Govt. of\nIndia in the context of its new education policy (NEP), (b) tourists who\ncriss-cross the multilingual landscape of India, (c) Indian Judiciary where a\nleading cause of the pendency of cases (to the order of 10 million as on date)\nis the translation of case papers, (d) farmers who need weather and price\ninformation and so on. We also share the feedback received from various\nstakeholders when our SSMT and TTMT systems were demonstrated in large public\nevents.", "published": "2023-05-21 17:23:54", "link": "http://arxiv.org/abs/2305.12518v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Simplification of Medical Texts", "abstract": "Automated text simplification aims to produce simple versions of complex\ntexts. This task is especially useful in the medical domain, where the latest\nmedical findings are typically communicated via complex and technical articles.\nThis creates barriers for laypeople seeking access to up-to-date medical\nfindings, consequently impeding progress on health literacy. Most existing work\non medical text simplification has focused on monolingual settings, with the\nresult that such evidence would be available only in just one language (most\noften, English). This work addresses this limitation via multilingual\nsimplification, i.e., directly simplifying complex texts into simplified texts\nin multiple languages. We introduce MultiCochrane, the first sentence-aligned\nmultilingual text simplification dataset for the medical domain in four\nlanguages: English, Spanish, French, and Farsi. We evaluate fine-tuned and\nzero-shot models across these languages, with extensive human assessments and\nanalyses. Although models can now generate viable simplified texts, we identify\noutstanding challenges that this dataset might be used to address.", "published": "2023-05-21 18:25:07", "link": "http://arxiv.org/abs/2305.12532v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding the Effect of Data Augmentation on Knowledge Distillation", "abstract": "Knowledge distillation (KD) requires sufficient data to transfer knowledge\nfrom large-scale teacher models to small-scale student models. Therefore, data\naugmentation has been widely used to mitigate the shortage of data under\nspecific scenarios. Classic data augmentation techniques, such as synonym\nreplacement and k-nearest-neighbors, are initially designed for fine-tuning. To\navoid severe semantic shifts and preserve task-specific labels, those methods\nprefer to change only a small proportion of tokens (e.g., changing 10% tokens\nis generally the best option for fine-tuning). However, such data augmentation\nmethods are sub-optimal for knowledge distillation since the teacher model\ncould provide label distributions and is more tolerant to semantic shifts. We\nfirst observe that KD prefers as much data as possible, which is different from\nfine-tuning that too much data will not gain more performance. Since changing\nmore tokens leads to more semantic shifts, we use the proportion of changed\ntokens to reflect semantic shift degrees. Then we find that KD prefers\naugmented data with a larger semantic shift degree (e.g., changing 30% tokens\nis generally the best option for KD) than fine-tuning (changing 10% tokens).\nBesides, our findings show that smaller datasets prefer larger degrees until\nthe out-of-distribution problem occurs (e.g., datasets with less than 10k\ninputs may prefer the 50% degree, and datasets with more than 100k inputs may\nprefer the 10% degree). Our work sheds light on the preference difference in\ndata augmentation between fine-tuning and knowledge distillation and encourages\nthe community to explore KD-specific data augmentation methods.", "published": "2023-05-21 21:02:55", "link": "http://arxiv.org/abs/2305.12565v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Model-Generated Pretraining Signals Improves Zero-Shot Generalization of\n  Text-to-Text Transformers", "abstract": "This paper explores the effectiveness of model-generated signals in improving\nzero-shot generalization of text-to-text Transformers such as T5. We study\nvarious designs to pretrain T5 using an auxiliary model to construct more\nchallenging token replacements for the main model to denoise. Key aspects under\nstudy include the decoding target, the location of the RTD head, and the\nmasking pattern. Based on these studies, we develop a new model, METRO-T0,\nwhich is pretrained using the redesigned ELECTRA-Style pretraining strategies\nand then prompt-finetuned on a mixture of NLP tasks. METRO-T0 outperforms all\nsimilar-sized baselines on prompted NLP benchmarks, such as T0 Eval and MMLU,\nand rivals the state-of-the-art T0-11B model with only 8% of its parameters.\nOur analysis on model's neural activation and parameter sensitivity reveals\nthat the effectiveness of METRO-T0 stems from more balanced contribution of\nparameters and better utilization of their capacity. The code and model\ncheckpoints are available at https://github.com/gonglinyuan/metro_t0.", "published": "2023-05-21 21:06:23", "link": "http://arxiv.org/abs/2305.12567v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Few-shot Classification with Instruction-Finetuned Language\n  Models", "abstract": "A particularly successful class of approaches for few-shot learning combines\nlanguage models with prompts -- hand-crafted task descriptions that complement\ndata samples. However, designing prompts by hand for each task commonly\nrequires domain knowledge and substantial guesswork. We observe, in the context\nof classification tasks, that instruction finetuned language models exhibit\nremarkable prompt robustness, and we subsequently propose a simple method to\neliminate the need for handcrafted prompts, named AuT-Few. This approach\nconsists of (i) a prompt retrieval module that selects suitable task\ninstructions from the instruction-tuning knowledge base, and (ii) the\ngeneration of two distinct, semantically meaningful, class descriptions and a\nselection mechanism via cross-validation. Over $12$ datasets, spanning $8$\nclassification tasks, we show that AuT-Few outperforms current state-of-the-art\nfew-shot learning methods. Moreover, AuT-Few is the best ranking method across\ndatasets on the RAFT few-shot benchmark. Notably, these results are achieved\nwithout task-specific handcrafted prompts on unseen tasks.", "published": "2023-05-21 21:50:27", "link": "http://arxiv.org/abs/2305.12576v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Framework for Bidirectional Decoding: Case Study in Morphological\n  Inflection", "abstract": "Transformer-based encoder-decoder models that generate outputs in a\nleft-to-right fashion have become standard for sequence-to-sequence tasks. In\nthis paper, we propose a framework for decoding that produces sequences from\nthe \"outside-in\": at each step, the model chooses to generate a token on the\nleft, on the right, or join the left and right sequences. We argue that this is\nmore principled than prior bidirectional decoders. Our proposal supports a\nvariety of model architectures and includes several training methods, such as a\ndynamic programming algorithm that marginalizes out the latent ordering\nvariable. Our model sets state-of-the-art (SOTA) on the 2022 and 2023 shared\ntasks, beating the next best systems by over 4.7 and 2.7 points in average\naccuracy respectively. The model performs particularly well on long sequences,\ncan implicitly learn the split point of words composed of stem and affix, and\nperforms better relative to the baseline on datasets that have fewer unique\nlemmas (but more examples per lemma).", "published": "2023-05-21 22:08:31", "link": "http://arxiv.org/abs/2305.12580v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Few-shot Text-to-SQL Capabilities of Large Language Models: A\n  Study on Prompt Design Strategies", "abstract": "In-context learning (ICL) has emerged as a new approach to various natural\nlanguage processing tasks, utilizing large language models (LLMs) to make\npredictions based on context that has been supplemented with a few examples or\ntask-specific instructions. In this paper, we aim to extend this method to\nquestion answering tasks that utilize structured knowledge sources, and improve\nText-to-SQL systems by exploring various prompt design strategies for employing\nLLMs. We conduct a systematic investigation into different demonstration\nselection methods and optimal instruction formats for prompting LLMs in the\nText-to-SQL task. Our approach involves leveraging the syntactic structure of\nan example's SQL query to retrieve demonstrations, and we demonstrate that\npursuing both diversity and similarity in demonstration selection leads to\nenhanced performance. Furthermore, we show that LLMs benefit from\ndatabase-related knowledge augmentations. Our most effective strategy\noutperforms the state-of-the-art system by 2.5 points (Execution Accuracy) and\nthe best fine-tuned system by 5.1 points on the Spider dataset. These results\nhighlight the effectiveness of our approach in adapting LLMs to the Text-to-SQL\ntask, and we present an analysis of the factors contributing to the success of\nour strategy.", "published": "2023-05-21 22:44:25", "link": "http://arxiv.org/abs/2305.12586v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling User Satisfaction Dynamics in Dialogue via Hawkes Process", "abstract": "Dialogue systems have received increasing attention while automatically\nevaluating their performance remains challenging. User satisfaction estimation\n(USE) has been proposed as an alternative. It assumes that the performance of a\ndialogue system can be measured by user satisfaction and uses an estimator to\nsimulate users. The effectiveness of USE depends heavily on the estimator.\nExisting estimators independently predict user satisfaction at each turn and\nignore satisfaction dynamics across turns within a dialogue. In order to fully\nsimulate users, it is crucial to take satisfaction dynamics into account. To\nfill this gap, we propose a new estimator ASAP (sAtisfaction eStimation via\nHAwkes Process) that treats user satisfaction across turns as an event sequence\nand employs a Hawkes process to effectively model the dynamics in this\nsequence. Experimental results on four benchmark dialogue datasets demonstrate\nthat ASAP can substantially outperform state-of-the-art baseline estimators.", "published": "2023-05-21 23:04:14", "link": "http://arxiv.org/abs/2305.12594v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Your Explanations Reliable? Investigating the Stability of LIME in\n  Explaining Text Classifiers by Marrying XAI and Adversarial Attack", "abstract": "LIME has emerged as one of the most commonly referenced tools in explainable\nAI (XAI) frameworks that is integrated into critical machine learning\napplications--e.g., healthcare and finance. However, its stability remains\nlittle explored, especially in the context of text data, due to the unique\ntext-space constraints. To address these challenges, in this paper, we first\nevaluate the inherent instability of LIME on text data to establish a baseline,\nand then propose a novel algorithm XAIFooler to perturb text inputs and\nmanipulate explanations that casts investigation on the stability of LIME as a\ntext perturbation optimization problem. XAIFooler conforms to the constraints\nto preserve text semantics and original prediction with small perturbations,\nand introduces Rank-biased Overlap (RBO) as a key part to guide the\noptimization of XAIFooler that satisfies all the requirements for explanation\nsimilarity measure. Extensive experiments on real-world text datasets\ndemonstrate that XAIFooler significantly outperforms all baselines by large\nmargins in its ability to manipulate LIME's explanations with high semantic\npreservability.", "published": "2023-05-21 05:06:46", "link": "http://arxiv.org/abs/2305.12351v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Evaluating Open-QA Evaluation", "abstract": "This study focuses on the evaluation of the Open Question Answering (Open-QA)\ntask, which can directly estimate the factuality of large language models\n(LLMs). Current automatic evaluation methods have shown limitations, indicating\nthat human evaluation still remains the most reliable approach. We introduce a\nnew task, Evaluating QA Evaluation (QA-Eval) and the corresponding dataset\nEVOUNA, designed to assess the accuracy of AI-generated answers in relation to\nstandard answers within Open-QA. Our evaluation of these methods utilizes\nhuman-annotated results to measure their performance. Specifically, the work\ninvestigates methods that show high correlation with human evaluations, deeming\nthem more reliable. We also discuss the pitfalls of current methods and methods\nto improve LLM-based evaluators. We believe this new QA-Eval task and\ncorresponding dataset EVOUNA will facilitate the development of more effective\nautomatic evaluation tools and prove valuable for future research in this area.\nAll resources are available at \\url{https://github.com/wangcunxiang/QA-Eval}\nand it is under the Apache-2.0 License.", "published": "2023-05-21 10:40:55", "link": "http://arxiv.org/abs/2305.12421v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BiasAsker: Measuring the Bias in Conversational AI System", "abstract": "Powered by advanced Artificial Intelligence (AI) techniques, conversational\nAI systems, such as ChatGPT and digital assistants like Siri, have been widely\ndeployed in daily life. However, such systems may still produce content\ncontaining biases and stereotypes, causing potential social problems. Due to\nthe data-driven, black-box nature of modern AI techniques, comprehensively\nidentifying and measuring biases in conversational systems remains a\nchallenging task. Particularly, it is hard to generate inputs that can\ncomprehensively trigger potential bias due to the lack of data containing both\nsocial groups as well as biased properties. In addition, modern conversational\nsystems can produce diverse responses (e.g., chatting and explanation), which\nmakes existing bias detection methods simply based on the sentiment and the\ntoxicity hardly being adopted. In this paper, we propose BiasAsker, an\nautomated framework to identify and measure social bias in conversational AI\nsystems. To obtain social groups and biased properties, we construct a\ncomprehensive social bias dataset, containing a total of 841 groups and 8,110\nbiased properties. Given the dataset, BiasAsker automatically generates\nquestions and adopts a novel method based on existence measurement to identify\ntwo types of biases (i.e., absolute bias and related bias) in conversational\nsystems. Extensive experiments on 8 commercial systems and 2 famous research\nmodels, such as ChatGPT and GPT-3, show that 32.83% of the questions generated\nby BiasAsker can trigger biased behaviors in these widely deployed\nconversational systems. All the code, data, and experimental results have been\nreleased to facilitate future research.", "published": "2023-05-21 11:25:59", "link": "http://arxiv.org/abs/2305.12434v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Communication Efficient Federated Learning for Multilingual Neural\n  Machine Translation with Adapter", "abstract": "Federated Multilingual Neural Machine Translation (Fed-MNMT) has emerged as a\npromising paradigm for institutions with limited language resources. This\napproach allows multiple institutions to act as clients and train a unified\nmodel through model synchronization, rather than collecting sensitive data for\ncentralized training. This significantly reduces the cost of corpus collection\nand preserves data privacy. However, as pre-trained language models (PLMs)\ncontinue to increase in size, the communication cost for transmitting\nparameters during synchronization has become a training speed bottleneck. In\nthis paper, we propose a communication-efficient Fed-MNMT framework that\naddresses this issue by keeping PLMs frozen and only transferring lightweight\nadapter modules between clients. Since different language pairs exhibit\nsubstantial discrepancies in data distributions, adapter parameters of clients\nmay conflict with each other. To tackle this, we explore various clustering\nstrategies to group parameters for integration and mitigate the negative\neffects of conflicting parameters. Experimental results demonstrate that our\nframework reduces communication cost by over 98% while achieving similar or\neven better performance compared to competitive baselines. Further analysis\nreveals that clustering strategies effectively solve the problem of linguistic\ndiscrepancy and pruning adapter modules further improves communication\nefficiency.", "published": "2023-05-21 12:48:38", "link": "http://arxiv.org/abs/2305.12449v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Teaching the Pre-trained Model to Generate Simple Texts for Text\n  Simplification", "abstract": "Randomly masking text spans in ordinary texts in the pre-training stage\nhardly allows models to acquire the ability to generate simple texts. It can\nhurt the performance of pre-trained models on text simplification tasks. In\nthis paper, we propose a new continued pre-training strategy to teach the\npre-trained model to generate simple texts. We continue pre-training BART, a\nrepresentative model, to obtain SimpleBART. It consistently and significantly\nimproves the results on lexical simplification, sentence simplification, and\ndocument-level simplification tasks over BART. At the end, we compare\nSimpleBART with several representative large language models (LLMs).", "published": "2023-05-21 14:03:49", "link": "http://arxiv.org/abs/2305.12463v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Self-supervised Predictive Coding Models Encode Speaker and Phonetic\n  Information in Orthogonal Subspaces", "abstract": "Self-supervised speech representations are known to encode both speaker and\nphonetic information, but how they are distributed in the high-dimensional\nspace remains largely unexplored. We hypothesize that they are encoded in\northogonal subspaces, a property that lends itself to simple disentanglement.\nApplying principal component analysis to representations of two predictive\ncoding models, we identify two subspaces that capture speaker and phonetic\nvariances, and confirm that they are nearly orthogonal. Based on this property,\nwe propose a new speaker normalization method which collapses the subspace that\nencodes speaker information, without requiring transcriptions. Probing\nexperiments show that our method effectively eliminates speaker information and\noutperforms a previous baseline in phone discrimination tasks. Moreover, the\napproach generalizes and can be used to remove information of unseen speakers.", "published": "2023-05-21 14:03:54", "link": "http://arxiv.org/abs/2305.12464v3", "categories": ["cs.CL", "cs.SD"], "primary_category": "cs.CL"}
{"title": "Evaluating the Performance of Large Language Models on GAOKAO Benchmark", "abstract": "Large Language Models(LLMs) have demonstrated remarkable performance across\nvarious natural language processing tasks; however, how to comprehensively and\naccurately assess their performance becomes an urgent issue to be addressed.\nThis paper introduces GAOKAO-Bench, an intuitive benchmark that employs\nquestions from the Chinese GAOKAO examination as test samples, including both\nsubjective and objective questions. To align with human examination methods, we\ndesign a method based on zero-shot settings to evaluate the performance of\nLLMs. With human evaluation, we obtain the converted total score of LLMs,\nincluding GPT-4, ChatGPT and ERNIE-Bot.Our findings reveal that LLMs have\nachieved competitive scores in Chinese GAOKAO examination, while they exhibit\nsignificant performance disparities across various subjects. We also use LLMs\nto grade the subjective questions, and find that model scores achieve a\nmoderate level of consistency with human scores. In conclusion, this research\ncontributes a robust evaluation benchmark for future large language models and\noffers valuable insights into the advantages and limitations of such models.", "published": "2023-05-21 14:39:28", "link": "http://arxiv.org/abs/2305.12474v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GPT-3.5, GPT-4, or BARD? Evaluating LLMs Reasoning Ability in Zero-Shot\n  Setting and Performance Boosting Through Prompts", "abstract": "Large Language Models (LLMs) have exhibited remarkable performance on various\nNatural Language Processing (NLP) tasks. However, there is a current hot debate\nregarding their reasoning capacity. In this paper, we examine the performance\nof GPT-3.5, GPT-4, and BARD models, by performing a thorough technical\nevaluation on different reasoning tasks across eleven distinct datasets. Our\npaper provides empirical evidence showcasing the superior performance of\nChatGPT-4 in comparison to both ChatGPT-3.5 and BARD in zero-shot setting\nthroughout almost all evaluated tasks. While the superiority of GPT-4 compared\nto GPT-3.5 might be explained by its larger size and NLP efficiency, this was\nnot evident for BARD. We also demonstrate that the three models show limited\nproficiency in Inductive, Mathematical, and Multi-hop Reasoning Tasks. To\nbolster our findings, we present a detailed and comprehensive analysis of the\nresults from these three models. Furthermore, we propose a set of engineered\nprompts that enhances the zero-shot setting performance of all three models.", "published": "2023-05-21 14:45:17", "link": "http://arxiv.org/abs/2305.12477v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Confidence-based Partial Label Learning Model for Crowd-Annotated\n  Named Entity Recognition", "abstract": "Existing models for named entity recognition (NER) are mainly based on\nlarge-scale labeled datasets, which always obtain using crowdsourcing. However,\nit is hard to obtain a unified and correct label via majority voting from\nmultiple annotators for NER due to the large labeling space and complexity of\nthis task. To address this problem, we aim to utilize the original\nmulti-annotator labels directly. Particularly, we propose a Confidence-based\nPartial Label Learning (CPLL) method to integrate the prior confidence (given\nby annotators) and posterior confidences (learned by models) for\ncrowd-annotated NER. This model learns a token- and content-dependent\nconfidence via an Expectation-Maximization (EM) algorithm by minimizing\nempirical risk. The true posterior estimator and confidence estimator perform\niteratively to update the true posterior and confidence respectively. We\nconduct extensive experimental results on both real-world and synthetic\ndatasets, which show that our model can improve performance effectively\ncompared with strong baselines.", "published": "2023-05-21 15:31:23", "link": "http://arxiv.org/abs/2305.12485v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Deeper (Autoregressive) Approach to Non-Convergent Discourse Parsing", "abstract": "Online social platforms provide a bustling arena for information-sharing and\nfor multi-party discussions. Various frameworks for dialogic discourse parsing\nwere developed and used for the processing of discussions and for predicting\nthe productivity of a dialogue. However, most of these frameworks are not\nsuitable for the analysis of contentious discussions that are commonplace in\nmany online platforms. A novel multi-label scheme for contentious dialog\nparsing was recently introduced by Zakharov et al. (2021). While the schema is\nwell developed, the computational approach they provide is both naive and\ninefficient, as a different model (architecture) using a different\nrepresentation of the input, is trained for each of the 31 tags in the\nannotation scheme. Moreover, all their models assume full knowledge of label\ncollocations and context, which is unlikely in any realistic setting. In this\nwork, we present a unified model for Non-Convergent Discourse Parsing that does\nnot require any additional input other than the previous dialog utterances. We\nfine-tuned a RoBERTa backbone, combining embeddings of the utterance, the\ncontext and the labels through GRN layers and an asymmetric loss function.\nOverall, our model achieves results comparable with SOTA, without using label\ncollocation and without training a unique architecture/model for each label.", "published": "2023-05-21 17:04:21", "link": "http://arxiv.org/abs/2305.12510v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "TheoremQA: A Theorem-driven Question Answering dataset", "abstract": "The recent LLMs like GPT-4 and PaLM-2 have made tremendous progress in\nsolving fundamental math problems like GSM8K by achieving over 90% accuracy.\nHowever, their capabilities to solve more challenging math problems which\nrequire domain-specific knowledge (i.e. theorem) have yet to be investigated.\nIn this paper, we introduce TheoremQA, the first theorem-driven\nquestion-answering dataset designed to evaluate AI models' capabilities to\napply theorems to solve challenging science problems. TheoremQA is curated by\ndomain experts containing 800 high-quality questions covering 350 theorems\n(e.g. Taylor's theorem, Lagrange's theorem, Huffman coding, Quantum Theorem,\nElasticity Theorem, etc) from Math, Physics, EE&CS, and Finance. We evaluate a\nwide spectrum of 16 large language and code models with different prompting\nstrategies like Chain-of-Thoughts and Program-of-Thoughts. We found that\nGPT-4's capabilities to solve these problems are unparalleled, achieving an\naccuracy of 51% with Program-of-Thoughts Prompting. All the existing\nopen-sourced models are below 15%, barely surpassing the random-guess baseline.\nGiven the diversity and broad coverage of TheoremQA, we believe it can be used\nas a better benchmark to evaluate LLMs' capabilities to solve challenging\nscience problems. The data and code are released in\nhttps://github.com/wenhuchen/TheoremQA.", "published": "2023-05-21 17:51:35", "link": "http://arxiv.org/abs/2305.12524v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ToxBuster: In-game Chat Toxicity Buster with BERT", "abstract": "Detecting toxicity in online spaces is challenging and an ever more pressing\nproblem given the increase in social media and gaming consumption. We introduce\nToxBuster, a simple and scalable model trained on a relatively large dataset of\n194k lines of game chat from Rainbow Six Siege and For Honor, carefully\nannotated for different kinds of toxicity. Compared to the existing\nstate-of-the-art, ToxBuster achieves 82.95% (+7) in precision and 83.56% (+57)\nin recall. This improvement is obtained by leveraging past chat history and\nmetadata. We also study the implication towards real-time and post-game\nmoderation as well as the model transferability from one game to another.", "published": "2023-05-21 18:53:26", "link": "http://arxiv.org/abs/2305.12542v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Has It All Been Solved? Open NLP Research Questions Not Solved by Large\n  Language Models", "abstract": "Recent progress in large language models (LLMs) has enabled the deployment of\nmany generative NLP applications. At the same time, it has also led to a\nmisleading public discourse that ``it's all been solved.'' Not surprisingly,\nthis has, in turn, made many NLP researchers -- especially those at the\nbeginning of their careers -- worry about what NLP research area they should\nfocus on. Has it all been solved, or what remaining questions can we work on\nregardless of LLMs? To address this question, this paper compiles NLP research\ndirections rich for exploration. We identify fourteen different research areas\nencompassing 45 research directions that require new research and are not\ndirectly solvable by LLMs. While we identify many research areas, many others\nexist; we do not cover areas currently addressed by LLMs, but where LLMs lag\nbehind in performance or those focused on LLM development. We welcome\nsuggestions for other research directions to include:\nhttps://bit.ly/nlp-era-llm", "published": "2023-05-21 19:06:30", "link": "http://arxiv.org/abs/2305.12544v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Symbolic Framework for Evaluating Mathematical Reasoning and\n  Generalisation with Transformers", "abstract": "This paper proposes a methodology for generating and perturbing detailed\nderivations of equations at scale, aided by a symbolic engine, to evaluate the\ngeneralisability of Transformers to out-of-distribution mathematical reasoning\nproblems. Instantiating the framework in the context of sequence classification\ntasks, we compare the capabilities of GPT-4, GPT-3.5, and a canon of fine-tuned\nBERT models, exploring the relationship between specific operators and\ngeneralisation failure via the perturbation of reasoning aspects such as\nsymmetry and variable surface forms. Surprisingly, our empirical evaluation\nreveals that the average in-distribution performance of fine-tuned models\nsurpasses GPT-3.5, and rivals GPT-4. However, perturbations to input reasoning\ncan reduce their performance by up to 80 F1 points. Overall, the results\nsuggest that the in-distribution performance of smaller open-source models may\npotentially rival GPT by incorporating appropriately structured derivation\ndependencies during training, and highlight a shared weakness between BERT and\nGPT involving a relative inability to decode indirect references to\nmathematical entities. We release the full codebase, constructed datasets, and\nfine-tuned models to encourage future progress in the field.", "published": "2023-05-21 20:40:37", "link": "http://arxiv.org/abs/2305.12563v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Abstract Meaning Representation-Based Logic-Driven Data Augmentation for\n  Logical Reasoning", "abstract": "Combining large language models with logical reasoning enhances their\ncapacity to address problems in a robust and reliable manner. Nevertheless, the\nintricate nature of logical reasoning poses challenges when gathering reliable\ndata from the web to build comprehensive training datasets, subsequently\naffecting performance on downstream tasks. To address this, we introduce a\nnovel logic-driven data augmentation approach, AMR-LDA. AMR-LDA converts the\noriginal text into an Abstract Meaning Representation (AMR) graph, a structured\nsemantic representation that encapsulates the logical structure of the\nsentence, upon which operations are performed to generate logically modified\nAMR graphs. The modified AMR graphs are subsequently converted back into text\nto create augmented data. Notably, our methodology is architecture-agnostic and\nenhances both generative large language models, such as GPT-3.5 and GPT-4,\nthrough prompt augmentation, and discriminative large language models through\ncontrastive learning with logic-driven data augmentation. Empirical evidence\nunderscores the efficacy of our proposed method with improvement in performance\nacross seven downstream tasks, such as reading comprehension requiring logical\nreasoning, textual entailment, and natural language inference. Furthermore, our\nmethod leads on the ReClor leaderboard at\nhttps://eval.ai/web/challenges/challenge-page/503/leaderboard/1347. The source\ncode and data are publicly available at\nhttps://github.com/Strong-AI-Lab/Logical-Equivalence-driven-AMR-Data-Augmentation-for-Representation-Learning.", "published": "2023-05-21 23:16:26", "link": "http://arxiv.org/abs/2305.12599v6", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Limitations of Simulating Active Learning", "abstract": "Active learning (AL) is a human-and-model-in-the-loop paradigm that\niteratively selects informative unlabeled data for human annotation, aiming to\nimprove over random sampling. However, performing AL experiments with human\nannotations on-the-fly is a laborious and expensive process, thus unrealistic\nfor academic research. An easy fix to this impediment is to simulate AL, by\ntreating an already labeled and publicly available dataset as the pool of\nunlabeled data. In this position paper, we first survey recent literature and\nhighlight the challenges across all different steps within the AL loop. We\nfurther unveil neglected caveats in the experimental setup that can\nsignificantly affect the quality of AL research. We continue with an\nexploration of how the simulation setting can govern empirical findings,\narguing that it might be one of the answers behind the ever posed question\n``why do active learning algorithms sometimes fail to outperform random\nsampling?''. We argue that evaluating AL algorithms on available labeled\ndatasets might provide a lower bound as to their effectiveness in real data. We\nbelieve it is essential to collectively shape the best practices for AL\nresearch, particularly as engineering advancements in LLMs push the research\nfocus towards data-driven approaches (e.g., data efficiency, alignment,\nfairness). In light of this, we have developed guidelines for future work. Our\naim is to draw attention to these limitations within the community, in the hope\nof finding ways to address them.", "published": "2023-05-21 22:52:13", "link": "http://arxiv.org/abs/2305.13342v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Measuring Intersectional Biases in Historical Documents", "abstract": "Data-driven analyses of biases in historical texts can help illuminate the\norigin and development of biases prevailing in modern society.\n  However, digitised historical documents pose a challenge for NLP\npractitioners as these corpora suffer from errors introduced by optical\ncharacter recognition (OCR) and are written in an archaic language. In this\npaper, we investigate the continuities and transformations of bias in\nhistorical newspapers published in the Caribbean during the colonial era (18th\nto 19th centuries). Our analyses are performed along the axes of gender, race,\nand their intersection. We examine these biases by conducting a temporal study\nin which we measure the development of lexical associations using\ndistributional semantics models and word embeddings. Further, we evaluate the\neffectiveness of techniques designed to process OCR-generated data and assess\ntheir stability when trained on and applied to the noisy historical newspapers.\nWe find that there is a trade-off between the stability of the word embeddings\nand their compatibility with the historical dataset. We provide evidence that\ngender and racial biases are interdependent, and their intersection triggers\ndistinct effects. These findings align with the theory of intersectionality,\nwhich stresses that biases affecting people with multiple marginalised\nidentities compound to more than the sum of their constituents.", "published": "2023-05-21 07:10:31", "link": "http://arxiv.org/abs/2305.12376v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Continually Improving Extractive QA via Human Feedback", "abstract": "We study continually improving an extractive question answering (QA) system\nvia human user feedback. We design and deploy an iterative approach, where\ninformation-seeking users ask questions, receive model-predicted answers, and\nprovide feedback. We conduct experiments involving thousands of user\ninteractions under diverse setups to broaden the understanding of learning from\nfeedback over time. Our experiments show effective improvement from user\nfeedback of extractive QA models over time across different data regimes,\nincluding significant potential for domain adaptation.", "published": "2023-05-21 14:35:32", "link": "http://arxiv.org/abs/2305.12473v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Model Analysis & Evaluation for Ambiguous Question Answering", "abstract": "Ambiguous questions are a challenge for Question Answering models, as they\nrequire answers that cover multiple interpretations of the original query. To\nthis end, these models are required to generate long-form answers that often\ncombine conflicting pieces of information. Although recent advances in the\nfield have shown strong capabilities in generating fluent responses, certain\nresearch questions remain unanswered. Does model/data scaling improve the\nanswers' quality? Do automated metrics align with human judgment? To what\nextent do these models ground their answers in evidence? In this study, we aim\nto thoroughly investigate these aspects, and provide valuable insights into the\nlimitations of the current approaches. To aid in reproducibility and further\nextension of our work, we open-source our code at\nhttps://github.com/din0s/ambig_lfqa.", "published": "2023-05-21 15:20:20", "link": "http://arxiv.org/abs/2305.12483v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Augmenting Autotelic Agents with Large Language Models", "abstract": "Humans learn to master open-ended repertoires of skills by imagining and\npracticing their own goals. This autotelic learning process, literally the\npursuit of self-generated (auto) goals (telos), becomes more and more\nopen-ended as the goals become more diverse, abstract and creative. The\nresulting exploration of the space of possible skills is supported by an\ninter-individual exploration: goal representations are culturally evolved and\ntransmitted across individuals, in particular using language. Current\nartificial agents mostly rely on predefined goal representations corresponding\nto goal spaces that are either bounded (e.g. list of instructions), or\nunbounded (e.g. the space of possible visual inputs) but are rarely endowed\nwith the ability to reshape their goal representations, to form new\nabstractions or to imagine creative goals. In this paper, we introduce a\nlanguage model augmented autotelic agent (LMA3) that leverages a pretrained\nlanguage model (LM) to support the representation, generation and learning of\ndiverse, abstract, human-relevant goals. The LM is used as an imperfect model\nof human cultural transmission; an attempt to capture aspects of humans'\ncommon-sense, intuitive physics and overall interests. Specifically, it\nsupports three key components of the autotelic architecture: 1)~a relabeler\nthat describes the goals achieved in the agent's trajectories, 2)~a goal\ngenerator that suggests new high-level goals along with their decomposition\ninto subgoals the agent already masters, and 3)~reward functions for each of\nthese goals. Without relying on any hand-coded goal representations, reward\nfunctions or curriculum, we show that LMA3 agents learn to master a large\ndiversity of skills in a task-agnostic text-based environment.", "published": "2023-05-21 15:42:41", "link": "http://arxiv.org/abs/2305.12487v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Contextualized End-to-End Speech Recognition with Contextual Phrase\n  Prediction Network", "abstract": "Contextual information plays a crucial role in speech recognition\ntechnologies and incorporating it into the end-to-end speech recognition models\nhas drawn immense interest recently. However, previous deep bias methods lacked\nexplicit supervision for bias tasks. In this study, we introduce a contextual\nphrase prediction network for an attention-based deep bias method. This network\npredicts context phrases in utterances using contextual embeddings and\ncalculates bias loss to assist in the training of the contextualized model. Our\nmethod achieved a significant word error rate (WER) reduction across various\nend-to-end speech recognition models. Experiments on the LibriSpeech corpus\nshow that our proposed model obtains a 12.1% relative WER improvement over the\nbaseline model, and the WER of the context phrases decreases relatively by\n40.5%. Moreover, by applying a context phrase filtering strategy, we also\neffectively eliminate the WER degradation when using a larger biasing list.", "published": "2023-05-21 16:08:04", "link": "http://arxiv.org/abs/2305.12493v5", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Fair Without Leveling Down: A New Intersectional Fairness Definition", "abstract": "In this work, we consider the problem of intersectional group fairness in the\nclassification setting, where the objective is to learn discrimination-free\nmodels in the presence of several intersecting sensitive groups. First, we\nillustrate various shortcomings of existing fairness measures commonly used to\ncapture intersectional fairness. Then, we propose a new definition called the\n$\\alpha$-Intersectional Fairness, which combines the absolute and the relative\nperformance across sensitive groups and can be seen as a generalization of the\nnotion of differential fairness. We highlight several desirable properties of\nthe proposed definition and analyze its relation to other fairness measures.\nFinally, we benchmark multiple popular in-processing fair machine learning\napproaches using our new fairness definition and show that they do not achieve\nany improvement over a simple baseline. Our results reveal that the increase in\nfairness measured by previous definitions hides a \"leveling down\" effect, i.e.,\ndegrading the best performance over groups rather than improving the worst one.", "published": "2023-05-21 16:15:12", "link": "http://arxiv.org/abs/2305.12495v2", "categories": ["cs.LG", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Exploring How Generative Adversarial Networks Learn Phonological\n  Representations", "abstract": "This paper explores how Generative Adversarial Networks (GANs) learn\nrepresentations of phonological phenomena. We analyze how GANs encode\ncontrastive and non-contrastive nasality in French and English vowels by\napplying the ciwGAN architecture (Begus 2021a). Begus claims that ciwGAN\nencodes linguistically meaningful representations with categorical variables in\nits latent space and manipulating the latent variables shows an almost one to\none corresponding control of the phonological features in ciwGAN's generated\noutputs. However, our results show an interactive effect of latent variables on\nthe features in the generated outputs, which suggests the learned\nrepresentations in neural networks are different from the phonological\nrepresentations proposed by linguists. On the other hand, ciwGAN is able to\ndistinguish contrastive and noncontrastive features in English and French by\nencoding them differently. Comparing the performance of GANs learning from\ndifferent languages results in a better understanding of what language specific\nfeatures contribute to developing language specific phonological\nrepresentations. We also discuss the role of training data frequencies in\nphonological feature learning.", "published": "2023-05-21 16:37:21", "link": "http://arxiv.org/abs/2305.12501v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Description-Based Text Similarity", "abstract": "Identifying texts with a given semantics is central for many information\nseeking scenarios. Similarity search over vector embeddings appear to be\ncentral to this ability, yet the similarity reflected in current text\nembeddings is corpus-driven, and is inconsistent and sub-optimal for many use\ncases. What, then, is a good notion of similarity for effective retrieval of\ntext?\n  We identify the need to search for texts based on abstract descriptions of\ntheir content, and the corresponding notion of \\emph{description based\nsimilarity}. We demonstrate the inadequacy of current text embeddings and\npropose an alternative model that significantly improves when used in standard\nnearest neighbor search. The model is trained using positive and negative pairs\nsourced through prompting a LLM, demonstrating how data from LLMs can be used\nfor creating new capabilities not immediately possible using the original\nmodel.", "published": "2023-05-21 17:14:31", "link": "http://arxiv.org/abs/2305.12517v5", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DPIC: Decoupling Prompt and Intrinsic Characteristics for LLM Generated\n  Text Detection", "abstract": "Large language models (LLMs) have the potential to generate texts that pose\nrisks of misuse, such as plagiarism, planting fake reviews on e-commerce\nplatforms, or creating inflammatory false tweets. Consequently, detecting\nwhether a text is generated by LLMs has become increasingly important. Existing\nhigh-quality detection methods usually require access to the interior of the\nmodel to extract the intrinsic characteristics. However, since we do not have\naccess to the interior of the black-box model, we must resort to surrogate\nmodels, which impacts detection quality. In order to achieve high-quality\ndetection of black-box models, we would like to extract deep intrinsic\ncharacteristics of the black-box model generated texts. We view the generation\nprocess as a coupled process of prompt and intrinsic characteristics of the\ngenerative model. Based on this insight, we propose to decouple prompt and\nintrinsic characteristics (DPIC) for LLM-generated text detection method.\nSpecifically, given a candidate text, DPIC employs an auxiliary LLM to\nreconstruct the prompt corresponding to the candidate text, then uses the\nprompt to regenerate text by the auxiliary LLM, which makes the candidate text\nand the regenerated text align with their prompts, respectively. Then, the\nsimilarity between the candidate text and the regenerated text is used as a\ndetection feature, thus eliminating the prompt in the detection process, which\nallows the detector to focus on the intrinsic characteristics of the generative\nmodel. Compared to the baselines, DPIC has achieved an average improvement of\n6.76\\% and 2.91\\% in detecting texts from different domains generated by GPT4\nand Claude3, respectively.", "published": "2023-05-21 17:26:16", "link": "http://arxiv.org/abs/2305.12519v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Explaining How Transformers Use Context to Build Predictions", "abstract": "Language Generation Models produce words based on the previous context.\nAlthough existing methods offer input attributions as explanations for a\nmodel's prediction, it is still unclear how prior words affect the model's\ndecision throughout the layers. In this work, we leverage recent advances in\nexplainability of the Transformer and present a procedure to analyze models for\nlanguage generation. Using contrastive examples, we compare the alignment of\nour explanations with evidence of the linguistic phenomena, and show that our\nmethod consistently aligns better than gradient-based and perturbation-based\nbaselines. Then, we investigate the role of MLPs inside the Transformer and\nshow that they learn features that help the model predict words that are\ngrammatically acceptable. Lastly, we apply our method to Neural Machine\nTranslation models, and demonstrate that they generate human-like source-target\nalignments for building predictions.", "published": "2023-05-21 18:29:10", "link": "http://arxiv.org/abs/2305.12535v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Wav2SQL: Direct Generalizable Speech-To-SQL Parsing", "abstract": "Speech-to-SQL (S2SQL) aims to convert spoken questions into SQL queries given\nrelational databases, which has been traditionally implemented in a cascaded\nmanner while facing the following challenges: 1) model training is faced with\nthe major issue of data scarcity, where limited parallel data is available; and\n2) the systems should be robust enough to handle diverse out-of-domain speech\nsamples that differ from the source data. In this work, we propose the first\ndirect speech-to-SQL parsing model Wav2SQL which avoids error compounding\nacross cascaded systems. Specifically, 1) to accelerate speech-driven SQL\nparsing research in the community, we release a large-scale and multi-speaker\ndataset MASpider; 2) leveraging the recent progress in the large-scale\npre-training, we show that it alleviates the data scarcity issue and allow for\ndirect speech-to-SQL parsing; and 3) we include the speech re-programming and\ngradient reversal classifier techniques to reduce acoustic variance and learned\nstyle-agnostic representation, improving generalization to unseen out-of-domain\ncustom data. Experimental results demonstrate that Wav2SQL avoids error\ncompounding and achieves state-of-the-art results by up to 2.5\\% accuracy\nimprovement over the baseline.", "published": "2023-05-21 19:26:46", "link": "http://arxiv.org/abs/2305.12552v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ChatGPT Is More Likely to Be Perceived as Male Than Female", "abstract": "We investigate how people perceive ChatGPT, and, in particular, how they\nassign human-like attributes such as gender to the chatbot. Across five\npre-registered studies (N = 1,552), we find that people are more likely to\nperceive ChatGPT to be male than female. Specifically, people perceive male\ngender identity (1) following demonstrations of ChatGPT's core abilities (e.g.,\nproviding information or summarizing text), (2) in the absence of such\ndemonstrations, and (3) across different methods of eliciting perceived gender\n(using various scales and asking to name ChatGPT). Moreover, we find that this\nseemingly default perception of ChatGPT as male can reverse when ChatGPT's\nfeminine-coded abilities are highlighted (e.g., providing emotional support for\na user).", "published": "2023-05-21 20:57:12", "link": "http://arxiv.org/abs/2305.12564v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.HC"}
{"title": "Hystoc: Obtaining word confidences for fusion of end-to-end ASR systems", "abstract": "End-to-end (e2e) systems have recently gained wide popularity in automatic\nspeech recognition. However, these systems do generally not provide\nwell-calibrated word-level confidences. In this paper, we propose Hystoc, a\nsimple method for obtaining word-level confidences from hypothesis-level\nscores. Hystoc is an iterative alignment procedure which turns hypotheses from\nan n-best output of the ASR system into a confusion network. Eventually,\nword-level confidences are obtained as posterior probabilities in the\nindividual bins of the confusion network. We show that Hystoc provides\nconfidences that correlate well with the accuracy of the ASR hypothesis.\nFurthermore, we show that utilizing Hystoc in fusion of multiple e2e ASR\nsystems increases the gains from the fusion by up to 1\\,\\% WER absolute on\nSpanish RTVE2020 dataset. Finally, we experiment with using Hystoc for direct\nfusion of n-best outputs from multiple systems, but we only achieve minor gains\nwhen fusing very similar systems.", "published": "2023-05-21 22:06:14", "link": "http://arxiv.org/abs/2305.12579v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Comparison of Multilingual Self-Supervised and Weakly-Supervised Speech\n  Pre-Training for Adaptation to Unseen Languages", "abstract": "Recent models such as XLS-R and Whisper have made multilingual speech\ntechnologies more accessible by pre-training on audio from around 100 spoken\nlanguages each. However, there are thousands of spoken languages worldwide, and\nadapting to new languages is an important problem. In this work, we aim to\nunderstand which model adapts better to languages unseen during pre-training.\nWe fine-tune both models on 13 unseen languages and 18 seen languages. Our\nresults show that the number of hours seen per language and language family\nduring pre-training is predictive of how the models compare, despite the\nsignificant differences in the pre-training methods.", "published": "2023-05-21 23:53:12", "link": "http://arxiv.org/abs/2305.12606v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Gene Set Summarization using Large Language Models", "abstract": "Molecular biologists frequently interpret gene lists derived from\nhigh-throughput experiments and computational analysis. This is typically done\nas a statistical enrichment analysis that measures the over- or\nunder-representation of biological function terms associated with genes or\ntheir properties, based on curated assertions from a knowledge base (KB) such\nas the Gene Ontology (GO). Interpreting gene lists can also be framed as a\ntextual summarization task, enabling the use of Large Language Models (LLMs),\npotentially utilizing scientific texts directly and avoiding reliance on a KB.\n  We developed SPINDOCTOR (Structured Prompt Interpolation of Natural Language\nDescriptions of Controlled Terms for Ontology Reporting), a method that uses\nGPT models to perform gene set function summarization as a complement to\nstandard enrichment analysis. This method can use different sources of gene\nfunctional information: (1) structured text derived from curated ontological KB\nannotations, (2) ontology-free narrative gene summaries, or (3) direct model\nretrieval.\n  We demonstrate that these methods are able to generate plausible and\nbiologically valid summary GO term lists for gene sets. However, GPT-based\napproaches are unable to deliver reliable scores or p-values and often return\nterms that are not statistically significant. Crucially, these methods were\nrarely able to recapitulate the most precise and informative term from standard\nenrichment, likely due to an inability to generalize and reason using an\nontology. Results are highly nondeterministic, with minor variations in prompt\nresulting in radically different term lists. Our results show that at this\npoint, LLM-based methods are unsuitable as a replacement for standard term\nenrichment analysis and that manual curation of ontological assertions remains\nnecessary.", "published": "2023-05-21 02:06:33", "link": "http://arxiv.org/abs/2305.13338v3", "categories": ["q-bio.GN", "cs.AI", "cs.CL", "q-bio.QM"], "primary_category": "q-bio.GN"}
{"title": "i-Code V2: An Autoregressive Generation Framework over Vision, Language,\n  and Speech Data", "abstract": "The convergence of text, visual, and audio data is a key step towards\nhuman-like artificial intelligence, however the current Vision-Language-Speech\nlandscape is dominated by encoder-only models which lack generative abilities.\nWe propose closing this gap with i-Code V2, the first model capable of\ngenerating natural language from any combination of Vision, Language, and\nSpeech data. i-Code V2 is an integrative system that leverages state-of-the-art\nsingle-modality encoders, combining their outputs with a new modality-fusing\nencoder in order to flexibly project combinations of modalities into a shared\nrepresentational space. Next, language tokens are generated from these\nrepresentations via an autoregressive decoder. The whole framework is\npretrained end-to-end on a large collection of dual- and single-modality\ndatasets using a novel text completion objective that can be generalized across\narbitrary combinations of modalities. i-Code V2 matches or outperforms\nstate-of-the-art single- and dual-modality baselines on 7 multimodal tasks,\ndemonstrating the power of generative multimodal pretraining across a diversity\nof tasks and signals.", "published": "2023-05-21 01:25:44", "link": "http://arxiv.org/abs/2305.12311v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multi-Head State Space Model for Speech Recognition", "abstract": "State space models (SSMs) have recently shown promising results on\nsmall-scale sequence and language modelling tasks, rivalling and outperforming\nmany attention-based approaches. In this paper, we propose a multi-head state\nspace (MH-SSM) architecture equipped with special gating mechanisms, where\nparallel heads are taught to learn local and global temporal dynamics on\nsequence data. As a drop-in replacement for multi-head attention in transformer\nencoders, this new model significantly outperforms the transformer transducer\non the LibriSpeech speech recognition corpus. Furthermore, we augment the\ntransformer block with MH-SSMs layers, referred to as the Stateformer,\nachieving state-of-the-art performance on the LibriSpeech task, with word error\nrates of 1.76\\%/4.37\\% on the development and 1.91\\%/4.36\\% on the test sets\nwithout using an external language model.", "published": "2023-05-21 16:28:57", "link": "http://arxiv.org/abs/2305.12498v2", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DCCRN-KWS: an audio bias based model for noise robust small-footprint\n  keyword spotting", "abstract": "Real-world complex acoustic environments especially the ones with a low\nsignal-to-noise ratio (SNR) will bring tremendous challenges to a keyword\nspotting (KWS) system. Inspired by the recent advances of neural speech\nenhancement and context bias in speech recognition, we propose a robust audio\ncontext bias based DCCRN-KWS model to address this challenge. We form the whole\narchitecture as a multi-task learning framework for both denosing and keyword\nspotting, where the DCCRN encoder is connected with the KWS model. Helped with\nthe denoising task, we further introduce an audio context bias module to\nleverage the real keyword samples and bias the network to better iscriminate\nkeywords in noisy conditions. Feature merge and complex context linear modules\nare also introduced to strength such discrimination and to effectively leverage\ncontextual information respectively. Experiments on the internal challenging\ndataset and the HIMIYA public dataset show that our DCCRN-KWS system is\nsuperior in performance, while ablation study demonstrates the good design of\nthe whole model.", "published": "2023-05-21 03:36:56", "link": "http://arxiv.org/abs/2305.12331v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DualVC: Dual-mode Voice Conversion using Intra-model Knowledge\n  Distillation and Hybrid Predictive Coding", "abstract": "Voice conversion is an increasingly popular technology, and the growing\nnumber of real-time applications requires models with streaming conversion\ncapabilities. Unlike typical (non-streaming) voice conversion, which can\nleverage the entire utterance as full context, streaming voice conversion faces\nsignificant challenges due to the missing future information, resulting in\ndegraded intelligibility, speaker similarity, and sound quality. To address\nthis challenge, we propose DualVC, a dual-mode neural voice conversion approach\nthat supports both streaming and non-streaming modes using jointly trained\nseparate network parameters. Furthermore, we propose intra-model knowledge\ndistillation and hybrid predictive coding (HPC) to enhance the performance of\nstreaming conversion. Additionally, we incorporate data augmentation to train a\nnoise-robust autoregressive decoder, improving the model's performance on\nlong-form speech conversion. Experimental results demonstrate that the proposed\nmodel outperforms the baseline models in the context of streaming voice\nconversion, while maintaining comparable performance to the non-streaming\ntopline system that leverages the complete context, albeit with a latency of\nonly 252.8 ms.", "published": "2023-05-21 10:45:48", "link": "http://arxiv.org/abs/2305.12425v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Laughter Synthesis using Pseudo Phonetic Tokens with a Large-scale\n  In-the-wild Laughter Corpus", "abstract": "We present a large-scale in-the-wild Japanese laughter corpus and a laughter\nsynthesis method. Previous work on laughter synthesis lacks not only data but\nalso proper ways to represent laughter. To solve these problems, we first\npropose an in-the-wild corpus comprising $3.5$ hours of laughter, which is to\nour best knowledge the largest laughter corpus designed for laughter synthesis.\nWe then propose pseudo phonetic tokens (PPTs) to represent laughter by a\nsequence of discrete tokens, which are obtained by training a clustering model\non features extracted from laughter by a pretrained self-supervised model.\nLaughter can then be synthesized by feeding PPTs into a text-to-speech system.\nWe further show PPTs can be used to train a language model for unconditional\nlaughter generation. Results of comprehensive subjective and objective\nevaluations demonstrate that the proposed method significantly outperforms a\nbaseline method, and can generate natural laughter unconditionally.", "published": "2023-05-21 12:25:25", "link": "http://arxiv.org/abs/2305.12442v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "JNV Corpus: A Corpus of Japanese Nonverbal Vocalizations with Diverse\n  Phrases and Emotions", "abstract": "We present JNV (Japanese Nonverbal Vocalizations) corpus, a corpus of\nJapanese nonverbal vocalizations (NVs) with diverse phrases and emotions.\nExisting Japanese NV corpora lack phrase or emotion diversity, which makes it\ndifficult to analyze NVs and support downstream tasks like emotion recognition.\nWe first propose a corpus-design method that contains two phases: (1)\ncollecting NVs phrases based on crowd-sourcing; (2) recording NVs by\nstimulating speakers with emotional scenarios. We then collect $420$ audio\nclips from $4$ speakers that cover $6$ emotions based on the proposed method.\nResults of comprehensive objective and subjective experiments demonstrate that\nthe collected NVs have high emotion recognizability and authenticity that are\ncomparable to previous corpora of English NVs. Additionally, we analyze the\ndistributions of vowel types in Japanese NVs. To our best knowledge, JNV is\ncurrently the largest Japanese NVs corpus in terms of phrase and emotion\ndiversities.", "published": "2023-05-21 12:32:03", "link": "http://arxiv.org/abs/2305.12445v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Semantic VAD: Low-Latency Voice Activity Detection for Speech\n  Interaction", "abstract": "For speech interaction, voice activity detection (VAD) is often used as a\nfront-end. However, traditional VAD algorithms usually need to wait for a\ncontinuous tail silence to reach a preset maximum duration before segmentation,\nresulting in a large latency that affects user experience. In this paper, we\npropose a novel semantic VAD for low-latency segmentation. Different from\nexisting methods, a frame-level punctuation prediction task is added to the\nsemantic VAD, and the artificial endpoint is included in the classification\ncategory in addition to the often-used speech presence and absence. To enhance\nthe semantic information of the model, we also incorporate an automatic speech\nrecognition (ASR) related semantic loss. Evaluations on an internal dataset\nshow that the proposed method can reduce the average latency by 53.3% without\nsignificant deterioration of character error rate in the back-end ASR compared\nto the traditional VAD approach.", "published": "2023-05-21 13:02:53", "link": "http://arxiv.org/abs/2305.12450v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CASA-ASR: Context-Aware Speaker-Attributed ASR", "abstract": "Recently, speaker-attributed automatic speech recognition (SA-ASR) has\nattracted a wide attention, which aims at answering the question ``who spoke\nwhat''. Different from modular systems, end-to-end (E2E) SA-ASR minimizes the\nspeaker-dependent recognition errors directly and shows a promising\napplicability. In this paper, we propose a context-aware SA-ASR (CASA-ASR)\nmodel by enhancing the contextual modeling ability of E2E SA-ASR. Specifically,\nin CASA-ASR, a contextual text encoder is involved to aggregate the semantic\ninformation of the whole utterance, and a context-dependent scorer is employed\nto model the speaker discriminability by contrasting with speakers in the\ncontext. In addition, a two-pass decoding strategy is further proposed to fully\nleverage the contextual modeling ability resulting in a better recognition\nperformance. Experimental results on AliMeeting corpus show that the proposed\nCASA-ASR model outperforms the original E2E SA-ASR system with a relative\nimprovement of 11.76% in terms of speaker-dependent character error rate.", "published": "2023-05-21 13:32:19", "link": "http://arxiv.org/abs/2305.12459v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Study of GANs for Noisy Speech Simulation from Clean Speech", "abstract": "The performance of speech processing models trained on clean speech drops\nsignificantly in noisy conditions. Training with noisy datasets alleviates the\nproblem, but procuring such datasets is not always feasible. Noisy speech\nsimulation models that generate noisy speech from clean speech help remedy this\nissue. In our work, we study the ability of Generative Adversarial Networks\n(GANs) to simulate a variety of noises. Noise from the\nUltra-High-Frequency/Very-High-Frequency (UHF/VHF), additive stationary and\nnon-stationary, and codec distortion categories are studied. We propose four\nGANs, including the non-parallel translators, SpeechAttentionGAN, SimuGAN, and\nMaskCycleGAN-Augment, and the parallel translator, Speech2Speech-Augment. We\nachieved improvements of 55.8%, 28.9%, and 22.8% in terms of Multi-Scale\nSpectral Loss (MSSL) as compared to the baseline for the RATS, TIMIT-Cabin, and\nTIMIT-Helicopter datasets, respectively, after training on small datasets of\nabout 3 minutes.", "published": "2023-05-21 13:43:14", "link": "http://arxiv.org/abs/2305.12460v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards robust paralinguistic assessment for real-world mobile health\n  (mHealth) monitoring: an initial study of reverberation effects on speech", "abstract": "Speech is promising as an objective, convenient tool to monitor health\nremotely over time using mobile devices. Numerous paralinguistic features have\nbeen demonstrated to contain salient information related to an individual's\nhealth. However, mobile device specification and acoustic environments vary\nwidely, risking the reliability of the extracted features. In an initial step\ntowards quantifying these effects, we report the variability of 13 exemplar\nparalinguistic features commonly reported in the speech-health literature and\nextracted from the speech of 42 healthy volunteers recorded consecutively in\nrooms with low and high reverberation with one budget and two higher-end\nsmartphones and a condenser microphone. Our results show reverberation has a\nclear effect on several features, in particular voice quality markers. They\npoint to new research directions investigating how best to record and process\nin-the-wild speech for reliable longitudinal health state assessment.", "published": "2023-05-21 17:07:30", "link": "http://arxiv.org/abs/2305.12514v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Robust Family-Infant Audio Analysis Based on Unsupervised\n  Pretraining of Wav2vec 2.0 on Large-Scale Unlabeled Family Audio", "abstract": "To perform automatic family audio analysis, past studies have collected\nrecordings using phone, video, or audio-only recording devices like LENA,\ninvestigated supervised learning methods, and used or fine-tuned\ngeneral-purpose embeddings learned from large pretrained models. In this study,\nwe advance the audio component of a new infant wearable multi-modal device\ncalled LittleBeats (LB) by learning family audio representation via wav2vec 2.0\n(W2V2) pertaining. We show given a limited number of labeled LB home\nrecordings, W2V2 pretrained using 1k-hour of unlabeled home recordings\noutperforms oracle W2V2 pretrained on 960-hour unlabeled LibriSpeech in terms\nof parent/infant speaker diarization (SD) and vocalization classifications (VC)\nat home. Extra relevant external unlabeled and labeled data further benefit\nW2V2 pretraining and fine-tuning. With SpecAug and environmental speech\ncorruptions, we obtain 12% relative gain on SD and moderate boost on VC. Code\nand model weights are available.", "published": "2023-05-21 18:00:16", "link": "http://arxiv.org/abs/2305.12530v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On the Efficacy and Noise-Robustness of Jointly Learned Speech Emotion\n  and Automatic Speech Recognition", "abstract": "New-age conversational agent systems perform both speech emotion recognition\n(SER) and automatic speech recognition (ASR) using two separate and often\nindependent approaches for real-world application in noisy environments. In\nthis paper, we investigate a joint ASR-SER multitask learning approach in a\nlow-resource setting and show that improvements are observed not only in SER,\nbut also in ASR. We also investigate the robustness of such jointly trained\nmodels to the presence of background noise, babble, and music. Experimental\nresults on the IEMOCAP dataset show that joint learning can improve ASR word\nerror rate (WER) and SER classification accuracy by 10.7% and 2.3% respectively\nin clean scenarios. In noisy scenarios, results on data augmented with MUSAN\nshow that the joint approach outperforms the independent ASR and SER approaches\nacross many noisy conditions. Overall, the joint ASR-SER approach yielded more\nnoise-resistant models than the independent ASR and SER approaches.", "published": "2023-05-21 18:52:21", "link": "http://arxiv.org/abs/2305.12540v2", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
