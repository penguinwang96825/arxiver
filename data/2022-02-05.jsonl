{"title": "Semantic Similarity Computing Model Based on Multi Model Fine-Grained\n  Nonlinear Fusion", "abstract": "Natural language processing (NLP) task has achieved excellent performance in\nmany fields, including semantic understanding, automatic summarization, image\nrecognition and so on. However, most of the neural network models for NLP\nextract the text in a fine-grained way, which is not conducive to grasp the\nmeaning of the text from a global perspective. To alleviate the problem, the\ncombination of the traditional statistical method and deep learning model as\nwell as a novel model based on multi model nonlinear fusion are proposed in\nthis paper. The model uses the Jaccard coefficient based on part of speech,\nTerm Frequency-Inverse Document Frequency (TF-IDF) and word2vec-CNN algorithm\nto measure the similarity of sentences respectively. According to the\ncalculation accuracy of each model, the normalized weight coefficient is\nobtained and the calculation results are compared. The weighted vector is input\ninto the fully connected neural network to give the final classification\nresults. As a result, the statistical sentence similarity evaluation algorithm\nreduces the granularity of feature extraction, so it can grasp the sentence\nfeatures globally. Experimental results show that the matching of sentence\nsimilarity calculation method based on multi model nonlinear fusion is 84%, and\nthe F1 value of the model is 75%.", "published": "2022-02-05 03:12:37", "link": "http://arxiv.org/abs/2202.02476v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A simple language-agnostic yet very strong baseline system for hate\n  speech and offensive content identification", "abstract": "For automatically identifying hate speech and offensive content in tweets, a\nsystem based on a classical supervised algorithm only fed with character\nn-grams, and thus completely language-agnostic, is proposed by the SATLab team.\nAfter its optimization in terms of the feature weighting and the classifier\nparameters, it reached, in the multilingual HASOC 2021 challenge, a medium\nperformance level in English, the language for which it is easy to develop deep\nlearning approaches relying on many external linguistic resources, but a far\nbetter level for the two less resourced language, Hindi and Marathi. It ends\neven first when performances are averaged over the three tasks in these\nlanguages, outperforming many deep learning approaches. These performances\nsuggest that it is an interesting reference level to evaluate the benefits of\nusing more complex approaches such as deep learning or taking into account\ncomplementary resources.", "published": "2022-02-05 08:09:09", "link": "http://arxiv.org/abs/2202.02511v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Automated Sarcasm Detection on Twitter", "abstract": "Automatic sarcasm detection is a growing field in computer science. Short\ntext messages are increasingly used for communication, especially over social\nmedia platforms such as Twitter. Due to insufficient or missing context,\nunidentified sarcasm in these messages can invert the meaning of a statement,\nleading to confusion and communication failures. This paper covers a variety of\ncurrent methods used for sarcasm detection, including detection by context,\nposting history and machine learning models. Additionally, a shift towards deep\nlearning methods is observable, likely due to the benefit of using a model with\ninduced instead of discrete features combined with the innovation of\ntransformers.", "published": "2022-02-05 08:38:38", "link": "http://arxiv.org/abs/2202.02516v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LEAPMood: Light and Efficient Architecture to Predict Mood with Genetic\n  Algorithm driven Hyperparameter Tuning", "abstract": "Accurate and automatic detection of mood serves as a building block for use\ncases like user profiling which in turn power applications such as advertising,\nrecommendation systems, and many more. One primary source indicative of an\nindividual's mood is textual data. While there has been extensive research on\nemotion recognition, the field of mood prediction has been barely explored. In\naddition, very little work is done in the area of on-device inferencing, which\nis highly important from the user privacy point of view. In this paper, we\npropose for the first time, an on-device deep learning approach for mood\nprediction from textual data, LEAPMood. We use a novel on-device\ndeployment-focused objective function for hyperparameter tuning based on the\nGenetic Algorithm (GA) and optimize the parameters concerning both performance\nand size. LEAPMood consists of Emotion Recognition in Conversion (ERC) as the\nfirst building block followed by mood prediction using K-means clustering. We\nshow that using a combination of character embedding, phonetic hashing, and\nattention along with Conditional Random Fields (CRF), results in a performance\nclosely comparable to that of the current State-Of-the-Art with a significant\nreduction in model size (> 90%) for the task of ERC. We achieve a Micro F1\nscore of 62.05% with a memory footprint of a mere 1.67MB on the DailyDialog\ndataset. Furthermore, we curate a dataset for the task of mood prediction\nachieving a Macro F1-score of 72.12% with LEAPMood.", "published": "2022-02-05 09:44:37", "link": "http://arxiv.org/abs/2202.02522v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aspect-based Sentiment Analysis through EDU-level Attentions", "abstract": "A sentence may express sentiments on multiple aspects. When these aspects are\nassociated with different sentiment polarities, a model's accuracy is often\nadversely affected. We observe that multiple aspects in such hard sentences are\nmostly expressed through multiple clauses, or formally known as elementary\ndiscourse units (EDUs), and one EDU tends to express a single aspect with\nunitary sentiment towards that aspect. In this paper, we propose to consider\nEDU boundaries in sentence modeling, with attentions at both word and EDU\nlevels. Specifically, we highlight sentiment-bearing words in EDU through\nword-level sparse attention. Then at EDU level, we force the model to attend to\nthe right EDU for the right aspect, by using EDU-level sparse attention and\northogonal regularization. Experiments on three benchmark datasets show that\nour simple EDU-Attention model outperforms state-of-the-art baselines. Because\nEDU can be automatically segmented with high accuracy, our model can be applied\nto sentences directly without the need of manual EDU boundary annotation.", "published": "2022-02-05 11:26:40", "link": "http://arxiv.org/abs/2202.02535v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LST: Lexicon-Guided Self-Training for Few-Shot Text Classification", "abstract": "Self-training provides an effective means of using an extremely small amount\nof labeled data to create pseudo-labels for unlabeled data. Many\nstate-of-the-art self-training approaches hinge on different regularization\nmethods to prevent overfitting and improve generalization. Yet they still rely\nheavily on predictions initially trained with the limited labeled data as\npseudo-labels and are likely to put overconfident label belief on erroneous\nclasses depending on the first prediction. To tackle this issue in text\nclassification, we introduce LST, a simple self-training method that uses a\nlexicon to guide the pseudo-labeling mechanism in a linguistically-enriched\nmanner. We consistently refine the lexicon by predicting confidence of the\nunseen data to teach pseudo-labels better in the training iterations. We\ndemonstrate that this simple yet well-crafted lexical knowledge achieves\n1.0-2.0% better performance on 30 labeled samples per class for five benchmark\ndatasets than the current state-of-the-art approaches.", "published": "2022-02-05 14:33:12", "link": "http://arxiv.org/abs/2202.02566v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Hate Speech and Offensive Content Detection using Modified\n  Cross-entropy Loss", "abstract": "The number of increased social media users has led to a lot of people\nmisusing these platforms to spread offensive content and use hate speech.\nManual tracking the vast amount of posts is impractical so it is necessary to\ndevise automated methods to identify them quickly. Large language models are\ntrained on a lot of data and they also make use of contextual embeddings. We\nfine-tune the large language models to help in our task. The data is also quite\nunbalanced; so we used a modified cross-entropy loss to tackle the issue. We\nobserved that using a model which is fine-tuned in hindi corpora performs\nbetter. Our team (HNLP) achieved the macro F1-scores of 0.808, 0.639 in English\nSubtask A and English Subtask B respectively. For Hindi Subtask A, Hindi\nSubtask B our team achieved macro F1-scores of 0.737, 0.443 respectively in\nHASOC 2021.", "published": "2022-02-05 20:31:40", "link": "http://arxiv.org/abs/2202.02635v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Classification on Sentence Embeddings for Legal Assistance", "abstract": "Legal proceedings take plenty of time and also cost a lot. The lawyers have\nto do a lot of work in order to identify the different sections of prior cases\nand statutes. The paper tries to solve the first tasks in AILA2021 (Artificial\nIntelligence for Legal Assistance) that will be held in FIRE2021 (Forum for\nInformation Retrieval Evaluation). The task is to semantically segment the\ndocument into different assigned one of the 7 predefined labels or \"rhetorical\nroles.\" The paper uses BERT to obtain the sentence embeddings from a sentence,\nand then a linear classifier is used to output the final prediction. The\nexperiments show that when more weightage is assigned to the class with the\nhighest frequency, the results are better than those when more weightage is\ngiven to the class with a lower frequency. In task 1, the team legalNLP\nobtained a F1 score of 0.22.", "published": "2022-02-05 20:57:05", "link": "http://arxiv.org/abs/2202.02639v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RerrFact: Reduced Evidence Retrieval Representations for Scientific\n  Claim Verification", "abstract": "Exponential growth in digital information outlets and the race to publish has\nmade scientific misinformation more prevalent than ever. However, the task to\nfact-verify a given scientific claim is not straightforward even for\nresearchers. Scientific claim verification requires in-depth knowledge and\ngreat labor from domain experts to substantiate supporting and refuting\nevidence from credible scientific sources. The SciFact dataset and\ncorresponding task provide a benchmarking leaderboard to the community to\ndevelop automatic scientific claim verification systems via extracting and\nassimilating relevant evidence rationales from source abstracts. In this work,\nwe propose a modular approach that sequentially carries out binary\nclassification for every prediction subtask as in the SciFact leaderboard. Our\nsimple classifier-based approach uses reduced abstract representations to\nretrieve relevant abstracts. These are further used to train the relevant\nrationale-selection model. Finally, we carry out two-step stance predictions\nthat first differentiate non-relevant rationales and then identify supporting\nor refuting rationales for a given claim. Experimentally, our system RerrFact\nwith no fine-tuning, simple design, and a fraction of model parameters fairs\ncompetitively on the leaderboard against large-scale, modular, and joint\nmodeling approaches. We make our codebase available at\nhttps://github.com/ashishrana160796/RerrFact.", "published": "2022-02-05 21:52:45", "link": "http://arxiv.org/abs/2202.02646v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive Fine-Tuning of Transformer-Based Language Models for Named\n  Entity Recognition", "abstract": "The current standard approach for fine-tuning transformer-based language\nmodels includes a fixed number of training epochs and a linear learning rate\nschedule. In order to obtain a near-optimal model for the given downstream\ntask, a search in optimization hyperparameter space is usually required. In\nparticular, the number of training epochs needs to be adjusted to the dataset\nsize. In this paper, we introduce adaptive fine-tuning, which is an alternative\napproach that uses early stopping and a custom learning rate schedule to\ndynamically adjust the number of training epochs to the dataset size. For the\nexample use case of named entity recognition, we show that our approach not\nonly makes hyperparameter search with respect to the number of training epochs\nredundant, but also leads to improved results in terms of performance,\nstability and efficiency. This holds true especially for small datasets, where\nwe outperform the state-of-the-art fine-tuning method by a large margin.", "published": "2022-02-05 19:20:03", "link": "http://arxiv.org/abs/2202.02617v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Ethics, Rules of Engagement, and AI: Neural Narrative Mapping Using\n  Large Transformer Language Models", "abstract": "The problem of determining if a military unit has correctly understood an\norder and is properly executing on it is one that has bedeviled military\nplanners throughout history. The advent of advanced language models such as\nOpenAI's GPT-series offers new possibilities for addressing this problem. This\npaper presents a mechanism to harness the narrative output of large language\nmodels and produce diagrams or \"maps\" of the relationships that are latent in\nthe weights of such models as the GPT-3. The resulting \"Neural Narrative Maps\"\n(NNMs), are intended to provide insight into the organization of information,\nopinion, and belief in the model, which in turn provide means to understand\nintent and response in the context of physical distance. This paper discusses\nthe problem of mapping information spaces in general, and then presents a\nconcrete implementation of this concept in the context of OpenAI's GPT-3\nlanguage model for determining if a subordinate is following a commander's\nintent in a high-risk situation. The subordinate's locations within the NNM\nallow a novel capability to evaluate the intent of the subordinate with respect\nto the commander. We show that is is possible not only to determine if they are\nnearby in narrative space, but also how they are oriented, and what\n\"trajectory\" they are on. Our results show that our method is able to produce\nhigh-quality maps, and demonstrate new ways of evaluating intent more\ngenerally.", "published": "2022-02-05 22:08:21", "link": "http://arxiv.org/abs/2202.02647v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Optimization of a Real-Time Wavelet-Based Algorithm for Improving Speech\n  Intelligibility", "abstract": "The optimization of a wavelet-based algorithm to improve speech\nintelligibility along with the full data set and results are reported. The\ndiscrete-time speech signal is split into frequency sub-bands via a multi-level\ndiscrete wavelet transform. Various gains are applied to the sub-band signals\nbefore they are recombined to form a modified version of the speech. The\nsub-band gains are adjusted while keeping the overall signal energy unchanged,\nand the speech intelligibility under various background interference and\nsimulated hearing loss conditions is enhanced and evaluated objectively and\nquantitatively using Google Speech-to-Text transcription. A universal set of\nsub-band gains can work over a range of noise-to-signal ratios up to 4.8 dB.\nFor noise-free speech, overall intelligibility is improved, and the Google\ntranscription accuracy is increased by 16.9 percentage points on average and\n86.7 maximum by reallocating the spectral energy toward the mid-frequency\nsub-bands. For speech already corrupted by noise, improving intelligibility is\nchallenging but still realizable with an increased transcription accuracy of\n9.5 percentage points on average and 71.4 maximum. The proposed algorithm is\nimplementable for real-time speech processing and comparatively simpler than\nprevious algorithms. Potential applications include speech enhancement, hearing\naids, machine listening, and a better understanding of speech intelligibility.", "published": "2022-02-05 13:03:57", "link": "http://arxiv.org/abs/2202.02545v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Probabilistic Models in Text Classification via Active\n  Learning", "abstract": "Social scientists often classify text documents to use the resulting labels\nas an outcome or a predictor in empirical research. Automated text\nclassification has become a standard tool, since it requires less human coding.\nHowever, scholars still need many human-labeled documents to train automated\nclassifiers. To reduce labeling costs, we propose a new algorithm for text\nclassification that combines a probabilistic model with active learning. The\nprobabilistic model uses both labeled and unlabeled data, and active learning\nconcentrates labeling efforts on difficult documents to classify. Our\nvalidation study shows that the classification performance of our algorithm is\ncomparable to state-of-the-art methods at a fraction of the computational cost.\nMoreover, we replicate two recently published articles and reach the same\nsubstantive conclusions with only a small proportion of the original labeled\ndata used in those studies. We provide activeText, an open-source software to\nimplement our method.", "published": "2022-02-05 20:09:26", "link": "http://arxiv.org/abs/2202.02629v2", "categories": ["cs.CL", "stat.AP", "stat.ME"], "primary_category": "cs.CL"}
{"title": "A Neural Beam Filter for Real-time Multi-channel Speech Enhancement", "abstract": "Most deep learning-based multi-channel speech enhancement methods focus on\ndesigning a set of beamforming coefficients to directly filter the low\nsignal-to-noise ratio signals received by microphones, which hinders the\nperformance of these approaches. To handle these problems, this paper designs a\ncausal neural beam filter that fully exploits the spatial-spectral information\nin the beam domain. Specifically, multiple beams are designed to steer towards\nall directions using a parameterized super-directive beamformer in the first\nstage. After that, the neural spatial filter is learned by simultaneously\nmodeling the spatial and spectral discriminability of the speech and the\ninterference, so as to extract the desired speech coarsely in the second stage.\nFinally, to further suppress the interference components especially at low\nfrequencies, a residual estimation module is adopted to refine the output of\nthe second stage. Experimental results demonstrate that the proposed approach\noutperforms many state-of-the-art multi-channel methods on the generated\nmulti-channel speech dataset based on the DNS-Challenge dataset.", "published": "2022-02-05 07:00:46", "link": "http://arxiv.org/abs/2202.02500v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SEED: Sound Event Early Detection via Evidential Uncertainty", "abstract": "Sound Event Early Detection (SEED) is an essential task in recognizing the\nacoustic environments and soundscapes. However, most of the existing methods\nfocus on the offline sound event detection, which suffers from the\nover-confidence issue of early-stage event detection and usually yield\nunreliable results. To solve the problem, we propose a novel Polyphonic\nEvidential Neural Network (PENet) to model the evidential uncertainty of the\nclass probability with Beta distribution. Specifically, we use a Beta\ndistribution to model the distribution of class probabilities, and the\nevidential uncertainty enriches uncertainty representation with evidence\ninformation, which plays a central role in reliable prediction. To further\nimprove the event detection performance, we design the backtrack inference\nmethod that utilizes both the forward and backward audio features of an ongoing\nevent. Experiments on the DESED database show that the proposed method can\nsimultaneously improve 13.0\\% and 3.8\\% in time delay and detection F1 score\ncompared to the state-of-the-art methods.", "published": "2022-02-05 00:10:00", "link": "http://arxiv.org/abs/2202.02441v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Privacy-preserving Speech Emotion Recognition through Semi-Supervised\n  Federated Learning", "abstract": "Speech Emotion Recognition (SER) refers to the recognition of human emotions\nfrom natural speech. If done accurately, it can offer a number of benefits in\nbuilding human-centered context-aware intelligent systems. Existing SER\napproaches are largely centralized, without considering users' privacy.\nFederated Learning (FL) is a distributed machine learning paradigm dealing with\ndecentralization of privacy-sensitive personal data. In this paper, we present\na privacy-preserving and data-efficient SER approach by utilizing the concept\nof FL. To the best of our knowledge, this is the first federated SER approach,\nwhich utilizes self-training learning in conjunction with federated learning to\nexploit both labeled and unlabeled on-device data. Our experimental evaluations\non the IEMOCAP dataset shows that our federated approach can learn\ngeneralizable SER models even under low availability of data labels and highly\nnon-i.i.d. distributions. We show that our approach with as few as 10% labeled\ndata, on average, can improve the recognition rate by 8.67% compared to the\nfully-supervised federated counterparts.", "published": "2022-02-05 18:30:23", "link": "http://arxiv.org/abs/2202.02611v1", "categories": ["cs.LG", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Speech Analysis for Automatic Mania Assessment in Bipolar Disorder", "abstract": "Bipolar disorder is a mental disorder that causes periods of manic and\ndepressive episodes. In this work, we classify recordings from Bipolar Disorder\ncorpus that contain 7 different tasks, into hypomania, mania, and remission\nclasses using only speech features. We perform our experiments on splitted\ntasks from the interviews. Best results achieved on the model trained with 6th\nand 7th tasks together gives 0.53 UAR (unweighted average recall) result which\nis higher than the baseline results of the corpus.", "published": "2022-02-05 14:30:37", "link": "http://arxiv.org/abs/2202.06766v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
