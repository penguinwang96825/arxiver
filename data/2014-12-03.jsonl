{"title": "Mary Astell's words in A Serious Proposal to the Ladies (part I), a\n  lexicographic inquiry with NooJ", "abstract": "In the following article we elected to study with NooJ the lexis of a 17 th\ncentury text, Mary Astell's seminal essay, A Serious Proposal to the Ladies,\npart I, published in 1694. We first focused on the semantics to see how Astell\nbuilds her vindication of the female sex, which words she uses to sensitise\nwomen to their alienated condition and promote their education. Then we studied\nthe morphology of the lexemes (which is different from contemporary English)\nused by the author, thanks to the NooJ tools we have devised for this purpose.\nNooJ has great functionalities for lexicographic work. Its commands and graphs\nprove to be most efficient in the spotting of archaic words or variants in\nspelling. Introduction In our previous articles, we have studied the\nsingularities of 17 th century English within the framework of a diachronic\nanalysis thanks to syntactical and morphological graphs and thanks to the\ndictionaries we have compiled from a corpus that may be expanded overtime. Our\nearly work was based on a limited corpus of English travel literature to Greece\nin the 17 th century. This article deals with a late seventeenth century text\nwritten by a woman philosopher and essayist, Mary Astell (1666--1731),\nconsidered as one of the first English feminists. Astell wrote her essay at a\ntime in English history when women were \"the weaker vessel\" and their main\nbusiness in life was to charm and please men by their looks and submissiveness.\nIn this essay we will see how NooJ can help us analyse Astell's rhetoric (what\npoint of view does she adopt, does she speak in her own name, in the name of\nall women, what is her representation of men and women and their relationships\nin the text, what are the goals of education?). Then we will turn our attention\nto the morphology of words in the text and use NooJ commands and graphs to\ncarry out a lexicographic inquiry into Astell's lexemes.", "published": "2014-12-03 07:16:04", "link": "http://arxiv.org/abs/1412.1215v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A perspective on the advancement of natural language processing tasks\n  via topological analysis of complex networks", "abstract": "Comment on \"Approaching human language with complex networks\" by Cong and Liu\n(Physics of Life Reviews, Volume 11, Issue 4, December 2014, Pages 598-618).", "published": "2014-12-03 14:37:36", "link": "http://arxiv.org/abs/1412.1342v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-Dependent Fine-Grained Entity Type Tagging", "abstract": "Entity type tagging is the task of assigning category labels to each mention\nof an entity in a document. While standard systems focus on a small set of\ntypes, recent work (Ling and Weld, 2012) suggests that using a large\nfine-grained label set can lead to dramatic improvements in downstream tasks.\nIn the absence of labeled training data, existing fine-grained tagging systems\nobtain examples automatically, using resolved entities and their types\nextracted from a knowledge base. However, since the appropriate type often\ndepends on context (e.g. Washington could be tagged either as city or\ngovernment), this procedure can result in spurious labels, leading to poorer\ngeneralization. We propose the task of context-dependent fine type tagging,\nwhere the set of acceptable labels for a mention is restricted to only those\ndeducible from the local context (e.g. sentence or document). We introduce new\nresources for this task: 12,017 mentions annotated with their context-dependent\nfine types, and we provide baseline experimental results on this data.", "published": "2014-12-03 23:26:33", "link": "http://arxiv.org/abs/1412.1820v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Skip-gram Language Modeling Using Sparse Non-negative Matrix Probability\n  Estimation", "abstract": "We present a novel family of language model (LM) estimation techniques named\nSparse Non-negative Matrix (SNM) estimation. A first set of experiments\nempirically evaluating it on the One Billion Word Benchmark shows that SNM\n$n$-gram LMs perform almost as well as the well-established Kneser-Ney (KN)\nmodels. When using skip-gram features the models are able to match the\nstate-of-the-art recurrent neural network (RNN) LMs; combining the two modeling\ntechniques yields the best known result on the benchmark. The computational\nadvantages of SNM over both maximum entropy and RNN LM estimation are probably\nits main strength, promising an approach that has the same flexibility in\ncombining arbitrary features effectively and yet should scale to very large\namounts of data as gracefully as $n$-gram LMs do.", "published": "2014-12-03 19:42:12", "link": "http://arxiv.org/abs/1412.1454v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
