{"title": "Using CollGram to Compare Formulaic Language in Human and Neural Machine\n  Translation", "abstract": "A comparison of formulaic sequences in human and neural machine translation\nof quality newspaper articles shows that neural machine translations contain\nless lower-frequency, but strongly-associated formulaic sequences, and more\nhigh-frequency formulaic sequences. These differences were statistically\nsignificant and the effect sizes were almost always medium or large. These\nobservations can be related to the differences between second language learners\nof various levels and between translated and untranslated texts. The comparison\nbetween the neural machine translation systems indicates that some systems\nproduce more formulaic sequences of both types than other systems.", "published": "2021-07-08 06:30:35", "link": "http://arxiv.org/abs/2107.03625v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HinGE: A Dataset for Generation and Evaluation of Code-Mixed Hinglish\n  Text", "abstract": "Text generation is a highly active area of research in the computational\nlinguistic community. The evaluation of the generated text is a challenging\ntask and multiple theories and metrics have been proposed over the years.\nUnfortunately, text generation and evaluation are relatively understudied due\nto the scarcity of high-quality resources in code-mixed languages where the\nwords and phrases from multiple languages are mixed in a single utterance of\ntext and speech. To address this challenge, we present a corpus (HinGE) for a\nwidely popular code-mixed language Hinglish (code-mixing of Hindi and English\nlanguages). HinGE has Hinglish sentences generated by humans as well as two\nrule-based algorithms corresponding to the parallel Hindi-English sentences. In\naddition, we demonstrate the inefficacy of widely-used evaluation metrics on\nthe code-mixed data. The HinGE dataset will facilitate the progress of natural\nlanguage generation research in code-mixed languages.", "published": "2021-07-08 11:11:37", "link": "http://arxiv.org/abs/2107.03760v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COMBO: a new module for EUD parsing", "abstract": "We introduce the COMBO-based approach for EUD parsing and its implementation,\nwhich took part in the IWPT 2021 EUD shared task. The goal of this task is to\nparse raw texts in 17 languages into Enhanced Universal Dependencies (EUD). The\nproposed approach uses COMBO to predict UD trees and EUD graphs. These\nstructures are then merged into the final EUD graphs. Some EUD edge labels are\nextended with case information using a single language-independent expansion\nrule. In the official evaluation, the solution ranked fourth, achieving an\naverage ELAS of 83.79%. The source code is available at\nhttps://gitlab.clarin-pl.eu/syntactic-tools/combo.", "published": "2021-07-08 12:34:25", "link": "http://arxiv.org/abs/2107.03809v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Vector Space Morphology with Linear Discriminative Learning", "abstract": "This paper presents three case studies of modeling aspects of lexical\nprocessing with Linear Discriminative Learning (LDL), the computational engine\nof the Discriminative Lexicon model (Baayen et al., 2019). With numeric\nrepresentations of word forms and meanings, LDL learns to map one vector space\nonto the other, without being informed about any morphological structure or\ninflectional classes. The modeling results demonstrated that LDL not only\nperforms well for understanding and producing morphologically complex words,\nbut also generates quantitative measures that are predictive for human\nbehavioral data. LDL models are straightforward to implement with the JudiLing\npackage (Luo et al., 2021). Worked examples are provided for three modeling\nchallenges: producing and understanding Korean verb inflection, predicting\nprimed Dutch lexical decision latencies, and predicting the acoustic duration\nof Mandarin words.", "published": "2021-07-08 16:22:50", "link": "http://arxiv.org/abs/2107.03950v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CANDLE: Decomposing Conditional and Conjunctive Queries for\n  Task-Oriented Dialogue Systems", "abstract": "Domain-specific dialogue systems generally determine user intents by relying\non sentence level classifiers that mainly focus on single action sentences.\nSuch classifiers are not designed to effectively handle complex queries\ncomposed of conditional and sequential clauses that represent multiple actions.\nWe attempt to decompose such queries into smaller single action subqueries that\nare reasonable for intent classifiers to understand in a dialogue pipeline. We\nrelease, CANDLE(Conditional & AND type Expressions), a dataset consisting of\n4282 utterances manually tagged with conditional and sequential labels, and\ndemonstrates this decomposition by training two baseline taggers.", "published": "2021-07-08 15:07:11", "link": "http://arxiv.org/abs/2107.03884v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Systematic Survey of Text Worlds as Embodied Natural Language\n  Environments", "abstract": "Text Worlds are virtual environments for embodied agents that, unlike 2D or\n3D environments, are rendered exclusively using textual descriptions. These\nenvironments offer an alternative to higher-fidelity 3D environments due to\ntheir low barrier to entry, providing the ability to study semantics,\ncompositional inference, and other high-level tasks with rich high-level action\nspaces while controlling for perceptual input. This systematic survey outlines\nrecent developments in tooling, environments, and agent modeling for Text\nWorlds, while examining recent trends in knowledge graphs, common sense\nreasoning, transfer learning of Text World performance to higher-fidelity\nenvironments, as well as near-term development targets that, once achieved,\nmake Text Worlds an attractive general research paradigm for natural language\nprocessing.", "published": "2021-07-08 22:15:16", "link": "http://arxiv.org/abs/2107.04132v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fuzzy-Rough Nearest Neighbour Approaches for Emotion Detection in Tweets", "abstract": "Social media are an essential source of meaningful data that can be used in\ndifferent tasks such as sentiment analysis and emotion recognition. Mostly,\nthese tasks are solved with deep learning methods. Due to the fuzzy nature of\ntextual data, we consider using classification methods based on fuzzy rough\nsets. Specifically, we develop an approach for the SemEval-2018 emotion\ndetection task, based on the fuzzy rough nearest neighbour (FRNN) classifier\nenhanced with ordered weighted average (OWA) operators. We use tuned ensembles\nof FRNN--OWA models based on different text embedding methods. Our results are\ncompetitive with the best SemEval solutions based on more complicated deep\nlearning methods.", "published": "2021-07-08 12:52:47", "link": "http://arxiv.org/abs/2107.05392v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Parameter Selection: Why We Should Pay More Attention to It", "abstract": "The importance of parameter selection in supervised learning is well known.\nHowever, due to the many parameter combinations, an incomplete or an\ninsufficient procedure is often applied. This situation may cause misleading or\nconfusing conclusions. In this opinion paper, through an intriguing example we\npoint out that the seriousness goes beyond what is generally recognized. In the\ntopic of multi-label classification for medical code prediction, one\ninfluential paper conducted a proper parameter selection on a set, but when\nmoving to a subset of frequently occurring labels, the authors used the same\nparameters without a separate tuning. The set of frequent labels became a\npopular benchmark in subsequent studies, which kept pushing the state of the\nart. However, we discovered that most of the results in these studies cannot\nsurpass the approach in the original paper if a parameter tuning had been\nconducted at the time. Thus it is unclear how much progress the subsequent\ndevelopments have actually brought. The lesson clearly indicates that without\nenough attention on parameter selection, the research progress in our field can\nbe uncertain or even illusive.", "published": "2021-07-08 12:55:34", "link": "http://arxiv.org/abs/2107.05393v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Nearest neighbour approaches for Emotion Detection in Tweets", "abstract": "Emotion detection is an important task that can be applied to social media\ndata to discover new knowledge. While the use of deep learning methods for this\ntask has been prevalent, they are black-box models, making their decisions hard\nto interpret for a human operator. Therefore, in this paper, we propose an\napproach using weighted $k$ Nearest Neighbours (kNN), a simple, easy to\nimplement, and explainable machine learning model. These qualities can help to\nenhance results' reliability and guide error analysis. In particular, we apply\nthe weighted kNN model to the shared emotion detection task in tweets from\nSemEval-2018. Tweets are represented using different text embedding methods and\nemotion lexicon vocabulary scores, and classification is done by an ensemble of\nweighted kNN models. Our best approaches obtain results competitive with\nstate-of-the-art solutions and open up a promising alternative path to neural\nnetwork methods.", "published": "2021-07-08 13:00:06", "link": "http://arxiv.org/abs/2107.05394v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multilingual Speech Evaluation: Case Studies on English, Malay and Tamil", "abstract": "Speech evaluation is an essential component in computer-assisted language\nlearning (CALL). While speech evaluation on English has been popular, automatic\nspeech scoring on low resource languages remains challenging. Work in this area\nhas focused on monolingual specific designs and handcrafted features stemming\nfrom resource-rich languages like English. Such approaches are often difficult\nto generalize to other languages, especially if we also want to consider\nsuprasegmental qualities such as rhythm. In this work, we examine three\ndifferent languages that possess distinct rhythm patterns: English\n(stress-timed), Malay (syllable-timed), and Tamil (mora-timed). We exploit\nrobust feature representations inspired by music processing and vector\nrepresentation learning. Empirical validations show consistent gains for all\nthree languages when predicting pronunciation, rhythm and intonation\nperformance.", "published": "2021-07-08 08:36:51", "link": "http://arxiv.org/abs/2107.03675v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Review of Bangla Natural Language Processing Tasks and the Utility of\n  Transformer Models", "abstract": "Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.", "published": "2021-07-08 13:49:46", "link": "http://arxiv.org/abs/2107.03844v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Privacy Concerns in Chatbot Interactions: When to Trust and When to\n  Worry", "abstract": "Through advances in their conversational abilities, chatbots have started to\nrequest and process an increasing variety of sensitive personal information.\nThe accurate disclosure of sensitive information is essential where it is used\nto provide advice and support to users in the healthcare and finance sectors.\nIn this study, we explore users' concerns regarding factors associated with the\nuse of sensitive data by chatbot providers. We surveyed a representative sample\nof 491 British citizens. Our results show that the user concerns focus on\ndeleting personal information and concerns about their data's inappropriate\nuse. We also identified that individuals were concerned about losing control\nover their data after a conversation with conversational agents. We found no\neffect from a user's gender or education but did find an effect from the user's\nage, with those over 45 being more concerned than those under 45. We also\nconsidered the factors that engender trust in a chatbot. Our respondents'\nprimary focus was on the chatbot's technical elements, with factors such as the\nresponse quality being identified as the most critical factor. We again found\nno effect from the user's gender or education level; however, when we\nconsidered some social factors (e.g. avatars or perceived 'friendliness'), we\nfound those under 45 years old rated these as more important than those over\n45. The paper concludes with a discussion of these results within the context\nof designing inclusive, digital systems that support a wide range of users.", "published": "2021-07-08 16:31:58", "link": "http://arxiv.org/abs/2107.03959v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.CY"}
{"title": "Inspiration through Observation: Demonstrating the Influence of\n  Automatically Generated Text on Creative Writing", "abstract": "Getting machines to generate text perceived as creative is a long-pursued\ngoal. A growing body of research directs this goal towards augmenting the\ncreative writing abilities of human authors. In this paper, we pursue this\nobjective by analyzing how observing examples of automatically generated text\ninfluences writing. In particular, we examine a task referred to as sentence\ninfilling, which involves transforming a list of words into a complete\nsentence. We emphasize \"storiability\" as a desirable feature of the resulting\nsentences, where \"storiable\" sentences are those that suggest a story a reader\nwould be curious to hear about. Both humans and an automated system (based on a\nneural language model) performed this sentence infilling task. In one setting,\npeople wrote sentences on their own; in a different setting, people observed\nthe sentences produced by the model while writing their own sentences. Readers\nthen assigned storiability preferences to the resulting sentences in a\nsubsequent evaluation. We find that human-authored sentences were judged as\nmore storiable when authors observed the generated examples, and that\nstoriability increased as authors derived more semantic content from the\nexamples. This result gives evidence of an \"inspiration through observation\"\nparadigm for human-computer collaborative writing, through which human writing\ncan be enhanced by text generation models without directly copying their\noutput.", "published": "2021-07-08 17:53:22", "link": "http://arxiv.org/abs/2107.04007v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Improved Language Identification Through Cross-Lingual Self-Supervised\n  Learning", "abstract": "Language identification greatly impacts the success of downstream tasks such\nas automatic speech recognition. Recently, self-supervised speech\nrepresentations learned by wav2vec 2.0 have been shown to be very effective for\na range of speech tasks. We extend previous self-supervised work on language\nidentification by experimenting with pre-trained models which were learned on\nreal-world unconstrained speech in multiple languages and not just on English.\nWe show that models pre-trained on many languages perform better and enable\nlanguage identification systems that require very little labeled data to\nperform well. Results on a 26 languages setup show that with only 10 minutes of\nlabeled data per language, a cross-lingually pre-trained model can achieve over\n89.2% accuracy.", "published": "2021-07-08 19:37:06", "link": "http://arxiv.org/abs/2107.04082v4", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Heavily Augmented Sound Event Detection utilizing Weak Predictions", "abstract": "The performances of Sound Event Detection (SED) systems are greatly limited\nby the difficulty in generating large strongly labeled dataset. In this work,\nwe used two main approaches to overcome the lack of strongly labeled data.\nFirst, we applied heavy data augmentation on input features. Data augmentation\nmethods used include not only conventional methods used in speech/audio domains\nbut also our proposed method named FilterAugment. Second, we propose two\nmethods to utilize weak predictions to enhance weakly supervised SED\nperformance. As a result, we obtained the best PSDS1 of 0.4336 and best PSDS2\nof 0.8161 on the DESED real validation dataset. This work is submitted to DCASE\n2021 Task4 and is ranked on the 3rd place. Code availa-ble:\nhttps://github.com/frednam93/FilterAugSED.", "published": "2021-07-08 07:32:12", "link": "http://arxiv.org/abs/2107.03649v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Expressive Voice Conversion: A Joint Framework for Speaker Identity and\n  Emotional Style Transfer", "abstract": "Traditional voice conversion(VC) has been focused on speaker identity\nconversion for speech with a neutral expression. We note that emotional\nexpression plays an essential role in daily communication, and the emotional\nstyle of speech can be speaker-dependent. In this paper, we study the technique\nto jointly convert the speaker identity and speaker-dependent emotional style,\nthat is called expressive voice conversion. We propose a StarGAN-based\nframework to learn a many-to-many mapping across different speakers, that takes\ninto account speaker-dependent emotional style without the need for parallel\ndata. To achieve this, we condition the generator on emotional style encoding\nderived from a pre-trained speech emotion recognition(SER) model. The\nexperiments validate the effectiveness of our proposed framework in both\nobjective and subjective evaluations. To our best knowledge, this is the first\nstudy on expressive voice conversion.", "published": "2021-07-08 10:48:04", "link": "http://arxiv.org/abs/2107.03748v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Comparing Supervised Models And Learned Speech Representations For\n  Classifying Intelligibility Of Disordered Speech On Selected Phrases", "abstract": "Automatic classification of disordered speech can provide an objective tool\nfor identifying the presence and severity of speech impairment. Classification\napproaches can also help identify hard-to-recognize speech samples to teach ASR\nsystems about the variable manifestations of impaired speech. Here, we develop\nand compare different deep learning techniques to classify the intelligibility\nof disordered speech on selected phrases. We collected samples from a diverse\nset of 661 speakers with a variety of self-reported disorders speaking 29 words\nor phrases, which were rated by speech-language pathologists for their overall\nintelligibility using a five-point Likert scale. We then evaluated classifiers\ndeveloped using 3 approaches: (1) a convolutional neural network (CNN) trained\nfor the task, (2) classifiers trained on non-semantic speech representations\nfrom CNNs that used an unsupervised objective [1], and (3) classifiers trained\non the acoustic (encoder) embeddings from an ASR system trained on typical\nspeech [2]. We found that the ASR encoder's embeddings considerably outperform\nthe other two on detecting and classifying disordered speech. Further analysis\nshows that the ASR embeddings cluster speech by the spoken phrase, while the\nnon-semantic embeddings cluster speech by speaker. Also, longer phrases are\nmore indicative of intelligibility deficits than single words.", "published": "2021-07-08 17:24:25", "link": "http://arxiv.org/abs/2107.03985v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Machine Learning for Stuttering Identification: Review, Challenges and\n  Future Directions", "abstract": "Stuttering is a speech disorder during which the flow of speech is\ninterrupted by involuntary pauses and repetition of sounds. Stuttering\nidentification is an interesting interdisciplinary domain research problem\nwhich involves pathology, psychology, acoustics, and signal processing that\nmakes it hard and complicated to detect. Recent developments in machine and\ndeep learning have dramatically revolutionized speech domain, however minimal\nattention has been given to stuttering identification. This work fills the gap\nby trying to bring researchers together from interdisciplinary fields. In this\npaper, we review comprehensively acoustic features, statistical and deep\nlearning based stuttering/disfluency classification methods. We also present\nseveral challenges and possible future directions.", "published": "2021-07-08 18:15:20", "link": "http://arxiv.org/abs/2107.04057v5", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Calliope -- A Polyphonic Music Transformer", "abstract": "The polyphonic nature of music makes the application of deep learning to\nmusic modelling a challenging task. On the other hand, the Transformer\narchitecture seems to be a good fit for this kind of data. In this work, we\npresent Calliope, a novel autoencoder model based on Transformers for the\nefficient modelling of multi-track sequences of polyphonic music. The\nexperiments show that our model is able to improve the state of the art on\nmusical sequence reconstruction and generation, with remarkably good results\nespecially on long sequences.", "published": "2021-07-08 08:18:57", "link": "http://arxiv.org/abs/2107.05546v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
