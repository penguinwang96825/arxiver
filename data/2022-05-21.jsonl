{"title": "Named Entity Linking with Entity Representation by Multiple Embeddings", "abstract": "We propose a simple and practical method for named entity linking (NEL),\nbased on entity representation by multiple embeddings. To explore this method,\nand to review its dependency on parameters, we measure its performance on\nNamesakes, a highly challenging dataset of ambiguously named entities. Our\nobservations suggest that the minimal number of mentions required to create a\nknowledge base (KB) entity is very important for NEL performance. The number of\nembeddings is less important and can be kept small, within as few as 10 or\nless. We show that our representations of KB entities can be adjusted using\nonly KB data, and the adjustment can improve NEL performance. We also compare\nNEL performance of embeddings obtained from tuning language model on diverse\nnews texts as opposed to tuning on more uniform texts from public datasets\nXSum, CNN / Daily Mail. We found that tuning on diverse news provides better\nembeddings.", "published": "2022-05-21 03:31:25", "link": "http://arxiv.org/abs/2205.10498v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pre-training Data Quality and Quantity for a Low-Resource Language: New\n  Corpus and BERT Models for Maltese", "abstract": "Multilingual language models such as mBERT have seen impressive cross-lingual\ntransfer to a variety of languages, but many languages remain excluded from\nthese models. In this paper, we analyse the effect of pre-training with\nmonolingual data for a low-resource language that is not included in mBERT --\nMaltese -- with a range of pre-training set ups. We conduct evaluations with\nthe newly pre-trained models on three morphosyntactic tasks -- dependency\nparsing, part-of-speech tagging, and named-entity recognition -- and one\nsemantic classification task -- sentiment analysis. We also present a newly\ncreated corpus for Maltese, and determine the effect that the pre-training data\nsize and domain have on the downstream performance. Our results show that using\na mixture of pre-training domains is often superior to using Wikipedia text\nonly. We also find that a fraction of this corpus is enough to make significant\nleaps in performance over Wikipedia-trained models. We pre-train and compare\ntwo models on the new corpus: a monolingual BERT model trained from scratch\n(BERTu), and a further pre-trained multilingual BERT (mBERTu). The models\nachieve state-of-the-art performance on these tasks, despite the new corpus\nbeing considerably smaller than typically used corpora for high-resourced\nlanguages. On average, BERTu outperforms or performs competitively with mBERTu,\nand the largest gains are observed for higher-level tasks.", "published": "2022-05-21 06:44:59", "link": "http://arxiv.org/abs/2205.10517v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CORAL: Contextual Response Retrievability Loss Function for Training\n  Dialog Generation Models", "abstract": "In the field of Natural Language Processing, there are many tasks that can be\ntackled effectively using the cross-entropy (CE) loss function. However, the\ntask of dialog generation poses unique challenges for CE loss. This is because\nCE loss assumes that, for any given input, the only possible output is the one\navailable as the ground truth in the training dataset. But, in dialog\ngeneration, there can be multiple valid responses (for a given context) that\nnot only have different surface forms but can also be semantically different.\nFurthermore, CE loss computation for the dialog generation task does not take\nthe input context into consideration and, hence, it grades the response\nirrespective of the context. To grade the generated response for qualities like\nrelevance, engagingness, etc., the loss function should depend on both the\ncontext and the generated response. To address these limitations, this paper\nproposes CORAL, a novel loss function based on a reinforcement learning (RL)\nview of the dialog generation task with a reward function that estimates human\npreference for generated responses while considering both the context and the\nresponse. Furthermore, to overcome challenges such as high sample complexity of\nRL training and a large action space, we propose a mix-policy training\nalgorithm. Notably, using CORAL we can train dialog generation models without\nassuming the ground-truth as the only correct response. Extensive comparisons\non benchmark datasets demonstrate that CORAL based models outperform strong\nstate-of-the-art baseline models of different sizes.", "published": "2022-05-21 10:36:22", "link": "http://arxiv.org/abs/2205.10558v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context Matters for Image Descriptions for Accessibility: Challenges for\n  Referenceless Evaluation Metrics", "abstract": "Few images on the Web receive alt-text descriptions that would make them\naccessible to blind and low vision (BLV) users. Image-based NLG systems have\nprogressed to the point where they can begin to address this persistent\nsocietal problem, but these systems will not be fully successful unless we\nevaluate them on metrics that guide their development correctly. Here, we argue\nagainst current referenceless metrics -- those that don't rely on\nhuman-generated ground-truth descriptions -- on the grounds that they do not\nalign with the needs of BLV users. The fundamental shortcoming of these metrics\nis that they do not take context into account, whereas contextual information\nis highly valued by BLV users. To substantiate these claims, we present a study\nwith BLV participants who rated descriptions along a variety of dimensions. An\nin-depth analysis reveals that the lack of context-awareness makes current\nreferenceless metrics inadequate for advancing image accessibility. As a\nproof-of-concept, we provide a contextual version of the referenceless metric\nCLIPScore which begins to address the disconnect to the BLV data. An accessible\nHTML version of this paper is available at\nhttps://elisakreiss.github.io/contextual-description-evaluation/paper/reflessmetrics.html", "published": "2022-05-21 17:35:26", "link": "http://arxiv.org/abs/2205.10646v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Pre-trained Language Models and their Evaluation for Arabic\n  Natural Language Understanding", "abstract": "There is a growing body of work in recent years to develop pre-trained\nlanguage models (PLMs) for the Arabic language. This work concerns addressing\ntwo major problems in existing Arabic PLMs which constraint progress of the\nArabic NLU and NLG fields.First, existing Arabic PLMs are not well-explored and\ntheir pre-trainig can be improved significantly using a more methodical\napproach. Second, there is a lack of systematic and reproducible evaluation of\nthese models in the literature. In this work, we revisit both the pre-training\nand evaluation of Arabic PLMs. In terms of pre-training, we explore improving\nArabic LMs from three perspectives: quality of the pre-training data, size of\nthe model, and incorporating character-level information. As a result, we\nrelease three new Arabic BERT-style models ( JABER, Char-JABER, and SABER), and\ntwo T5-style models (AT5S and AT5B). In terms of evaluation, we conduct a\ncomprehensive empirical study to systematically evaluate the performance of\nexisting state-of-the-art models on ALUE that is a leaderboard-powered\nbenchmark for Arabic NLU tasks, and on a subset of the ARGEN benchmark for\nArabic NLG tasks. We show that our models significantly outperform existing\nArabic PLMs and achieve a new state-of-the-art performance on discriminative\nand generative Arabic NLU and NLG tasks. Our models and source code to\nreproduce of results will be made available shortly.", "published": "2022-05-21 22:38:19", "link": "http://arxiv.org/abs/2205.10687v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Life after BERT: What do Other Muppets Understand about Language?", "abstract": "Existing pre-trained transformer analysis works usually focus only on one or\ntwo model families at a time, overlooking the variability of the architecture\nand pre-training objectives. In our work, we utilize the oLMpics benchmark and\npsycholinguistic probing datasets for a diverse set of 29 models including T5,\nBART, and ALBERT. Additionally, we adapt the oLMpics zero-shot setup for\nautoregressive models and evaluate GPT networks of different sizes. Our\nfindings show that none of these models can resolve compositional questions in\na zero-shot fashion, suggesting that this skill is not learnable using existing\npre-training objectives. Furthermore, we find that global model decisions such\nas architecture, directionality, size of the dataset, and pre-training\nobjective are not predictive of a model's linguistic capabilities.", "published": "2022-05-21 23:57:17", "link": "http://arxiv.org/abs/2205.10696v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DEER: Descriptive Knowledge Graph for Explaining Entity Relationships", "abstract": "We propose DEER (Descriptive Knowledge Graph for Explaining Entity\nRelationships) - an open and informative form of modeling entity relationships.\nIn DEER, relationships between entities are represented by free-text relation\ndescriptions. For instance, the relationship between entities of machine\nlearning and algorithm can be represented as ``Machine learning explores the\nstudy and construction of algorithms that can learn from and make predictions\non data.'' To construct DEER, we propose a self-supervised learning method to\nextract relation descriptions with the analysis of dependency patterns and\ngenerate relation descriptions with a transformer-based relation description\nsynthesizing model, where no human labeling is required. Experiments\ndemonstrate that our system can extract and generate high-quality relation\ndescriptions for explaining entity relationships. The results suggest that we\ncan build an open and informative knowledge graph without human annotation.", "published": "2022-05-21 01:16:04", "link": "http://arxiv.org/abs/2205.10479v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Long Tailed Document-Level Relation Extraction via Easy\n  Relation Augmentation and Contrastive Learning", "abstract": "Towards real-world information extraction scenario, research of relation\nextraction is advancing to document-level relation extraction(DocRE). Existing\napproaches for DocRE aim to extract relation by encoding various information\nsources in the long context by novel model architectures. However, the inherent\nlong-tailed distribution problem of DocRE is overlooked by prior work. We argue\nthat mitigating the long-tailed distribution problem is crucial for DocRE in\nthe real-world scenario. Motivated by the long-tailed distribution problem, we\npropose an Easy Relation Augmentation(ERA) method for improving DocRE by\nenhancing the performance of tailed relations. In addition, we further propose\na novel contrastive learning framework based on our ERA, i.e., ERACL, which can\nfurther improve the model performance on tailed relations and achieve\ncompetitive overall DocRE performance compared to the state-of-arts.", "published": "2022-05-21 06:15:11", "link": "http://arxiv.org/abs/2205.10511v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Sign Language Phoneme Clustering using HamNoSys Notation", "abstract": "Traditionally, sign language resources have been collected in controlled\nsettings for specific tasks involving supervised sign classification or\nlinguistic studies accompanied by specific annotation type. To date, very few\nwho explored signing videos found online on social media platforms as well as\nthe use of unsupervised methods applied to such resources. Due to the fact that\nthe field is striving to achieve acceptable model performance on the data that\ndiffers from that seen during training calls for more diversity in sign\nlanguage data, stepping away from the data obtained in controlled laboratory\nsettings. Moreover, since the sign language data collection and annotation\ncarries large overheads, it is desirable to accelerate the annotation process.\nConsidering the aforementioned tendencies, this paper takes the side of\nharvesting online data in a pursuit for automatically generating and annotating\nsign language corpora through phoneme clustering.", "published": "2022-05-21 10:49:45", "link": "http://arxiv.org/abs/2205.10560v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "HLATR: Enhance Multi-stage Text Retrieval with Hybrid List Aware\n  Transformer Reranking", "abstract": "Deep pre-trained language models (e,g. BERT) are effective at large-scale\ntext retrieval task. Existing text retrieval systems with state-of-the-art\nperformance usually adopt a retrieve-then-reranking architecture due to the\nhigh computational cost of pre-trained language models and the large corpus\nsize. Under such a multi-stage architecture, previous studies mainly focused on\noptimizing single stage of the framework thus improving the overall retrieval\nperformance. However, how to directly couple multi-stage features for\noptimization has not been well studied. In this paper, we design Hybrid List\nAware Transformer Reranking (HLATR) as a subsequent reranking module to\nincorporate both retrieval and reranking stage features. HLATR is lightweight\nand can be easily parallelized with existing text retrieval systems so that the\nreranking process can be performed in a single yet efficient processing.\nEmpirical experiments on two large-scale text retrieval datasets show that\nHLATR can efficiently improve the ranking performance of existing multi-stage\ntext retrieval methods.", "published": "2022-05-21 11:38:33", "link": "http://arxiv.org/abs/2205.10569v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Non-Autoregressive Neural Machine Translation: A Call for Clarity", "abstract": "Non-autoregressive approaches aim to improve the inference speed of\ntranslation models by only requiring a single forward pass to generate the\noutput sequence instead of iteratively producing each predicted token.\nConsequently, their translation quality still tends to be inferior to their\nautoregressive counterparts due to several issues involving output token\ninterdependence. In this work, we take a step back and revisit several\ntechniques that have been proposed for improving non-autoregressive translation\nmodels and compare their combined translation quality and speed implications\nunder third-party testing environments. We provide novel insights for\nestablishing strong baselines using length prediction or CTC-based architecture\nvariants and contribute standardized BLEU, chrF++, and TER scores using\nsacreBLEU on four translation tasks, which crucially have been missing as\ninconsistencies in the use of tokenized BLEU lead to deviations of up to 1.7\nBLEU points. Our open-sourced code is integrated into fairseq for\nreproducibility.", "published": "2022-05-21 12:15:22", "link": "http://arxiv.org/abs/2205.10577v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Calibration of Natural Language Understanding Models with Venn--ABERS\n  Predictors", "abstract": "Transformers, currently the state-of-the-art in natural language\nunderstanding (NLU) tasks, are prone to generate uncalibrated predictions or\nextreme probabilities, making the process of taking different decisions based\non their output relatively difficult. In this paper we propose to build several\ninductive Venn--ABERS predictors (IVAP), which are guaranteed to be well\ncalibrated under minimal assumptions, based on a selection of pre-trained\ntransformers. We test their performance over a set of diverse NLU tasks and\nshow that they are capable of producing well-calibrated probabilistic\npredictions that are uniformly spread over the [0,1] interval -- all while\nretaining the original model's predictive accuracy.", "published": "2022-05-21 13:09:01", "link": "http://arxiv.org/abs/2205.10586v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Few-Shot Natural Language Inference Generation with PDD: Prompt and\n  Dynamic Demonstration", "abstract": "Natural Language Inference Generation task is to generate a text hypothesis\ngiven a text premise and a logical relation between the two. This task can be\nused in data augmentation and controllable text generation in practice. In this\npaper, we propose language models with prompt and dynamic demonstration\n(LM-PDD) to tackle this problem in few-shot settings. Our framework outperforms\nstandard fine-tuned models with low resource, achieving an average 8% absolute\nimprovement on SNLI and MNLI datasets, and the results on 13 natural language\nclassification tasks also show that our dynamic demonstration method has good\ngeneralizability.", "published": "2022-05-21 13:25:22", "link": "http://arxiv.org/abs/2205.10593v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language\n  Models", "abstract": "Chain-of-thought prompting has demonstrated remarkable performance on various\nnatural language reasoning tasks. However, it tends to perform poorly on tasks\nwhich requires solving problems harder than the exemplars shown in the prompts.\nTo overcome this challenge of easy-to-hard generalization, we propose a novel\nprompting strategy, least-to-most prompting. The key idea in this strategy is\nto break down a complex problem into a series of simpler subproblems and then\nsolve them in sequence. Solving each subproblem is facilitated by the answers\nto previously solved subproblems. Our experimental results on tasks related to\nsymbolic manipulation, compositional generalization, and math reasoning reveal\nthat least-to-most prompting is capable of generalizing to more difficult\nproblems than those seen in the prompts. A notable finding is that when the\nGPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve\nthe compositional generalization benchmark SCAN in any split (including length\nsplit) with an accuracy of at least 99% using just 14 exemplars, compared to\nonly 16% accuracy with chain-of-thought prompting. This is particularly\nnoteworthy because neural-symbolic models in the literature that specialize in\nsolving SCAN are trained on the entire training set containing over 15,000\nexamples. We have included prompts for all the tasks in the Appendix.", "published": "2022-05-21 15:34:53", "link": "http://arxiv.org/abs/2205.10625v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "An Empirical Investigation of Commonsense Self-Supervision with\n  Knowledge Graphs", "abstract": "Self-supervision based on the information extracted from large knowledge\ngraphs has been shown to improve the generalization of language models, in\nzero-shot evaluation on various downstream language reasoning tasks. Since\nthese improvements are reported in aggregate, however, little is known about\n(i) how to select the appropriate knowledge for solid performance across tasks,\n(ii) how to combine this knowledge with neural language models, and (iii) how\nthese pairings affect granular task performance. In this paper, we study the\neffect of knowledge sampling strategies and sizes that can be used to generate\nsynthetic data for adapting language models. We study the effect of different\nsynthetic datasets on language models with various architectures and sizes. The\nresulting models are evaluated against four task properties: domain overlap,\nanswer similarity, vocabulary overlap, and answer length. Our experiments show\nthat encoder-decoder models benefit from more data to learn from, whereas\nsampling strategies that balance across different aspects yield best\nperformance. Most of the improvement occurs on questions with short answers and\ndissimilar answer candidates, which corresponds to the characteristics of the\ndata used for pre-training.", "published": "2022-05-21 19:49:04", "link": "http://arxiv.org/abs/2205.10661v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Online Coreference Resolution for Dialogue Processing: Improving\n  Mention-Linking on Real-Time Conversations", "abstract": "This paper suggests a direction of coreference resolution for online decoding\non actively generated input such as dialogue, where the model accepts an\nutterance and its past context, then finds mentions in the current utterance as\nwell as their referents, upon each dialogue turn. A baseline and four\nincremental-updated models adapted from the mention-linking paradigm are\nproposed for this new setting, which address different aspects including the\nsingletons, speaker-grounded encoding and cross-turn mention contextualization.\nOur approach is assessed on three datasets: Friends, OntoNotes, and BOLT.\nResults show that each aspect brings out steady improvement, and our best\nmodels outperform the baseline by over 10%, presenting an effective system for\nthis setting. Further analysis highlights the task characteristics, such as the\nsignificance of addressing the mention recall.", "published": "2022-05-21 20:40:32", "link": "http://arxiv.org/abs/2205.10670v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Retrieval-Augmented Multilingual Keyphrase Generation with\n  Retriever-Generator Iterative Training", "abstract": "Keyphrase generation is the task of automatically predicting keyphrases given\na piece of long text. Despite its recent flourishing, keyphrase generation on\nnon-English languages haven't been vastly investigated. In this paper, we call\nattention to a new setting named multilingual keyphrase generation and we\ncontribute two new datasets, EcommerceMKP and AcademicMKP, covering six\nlanguages. Technically, we propose a retrieval-augmented method for\nmultilingual keyphrase generation to mitigate the data shortage problem in\nnon-English languages. The retrieval-augmented model leverages keyphrase\nannotations in English datasets to facilitate generating keyphrases in\nlow-resource languages. Given a non-English passage, a cross-lingual dense\npassage retrieval module finds relevant English passages. Then the associated\nEnglish keyphrases serve as external knowledge for keyphrase generation in the\ncurrent language. Moreover, we develop a retriever-generator iterative training\nalgorithm to mine pseudo parallel passage pairs to strengthen the cross-lingual\npassage retriever. Comprehensive experiments and ablations show that the\nproposed approach outperforms all baselines.", "published": "2022-05-21 00:45:21", "link": "http://arxiv.org/abs/2205.10471v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DeepStruct: Pretraining of Language Models for Structure Prediction", "abstract": "We introduce a method for improving the structural understanding abilities of\nlanguage models. Unlike previous approaches that finetune the models with\ntask-specific augmentation, we pretrain language models on a collection of\ntask-agnostic corpora to generate structures from text. Our structure\npretraining enables zero-shot transfer of the learned knowledge that models\nhave about the structure tasks. We study the performance of this approach on 28\ndatasets, spanning 10 structure prediction tasks including open information\nextraction, joint entity and relation extraction, named entity recognition,\nrelation classification, semantic role labeling, event extraction, coreference\nresolution, factual probe, intent detection, and dialogue state tracking. We\nfurther enhance the pretraining with the task-specific training sets. We show\nthat a 10B parameter language model transfers non-trivially to most tasks and\nobtains state-of-the-art performance on 21 of 28 datasets that we evaluate.", "published": "2022-05-21 00:58:22", "link": "http://arxiv.org/abs/2205.10475v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Study on Transformer Configuration and Training Objective", "abstract": "Transformer-based models have delivered impressive results on many tasks,\nparticularly vision and language tasks. In many model training situations,\nconventional configurations are typically adopted. For example, we often set\nthe base model with hidden dimensions (i.e. model width) to be 768 and the\nnumber of transformer layers (i.e. model depth) to be 12. In this paper, we\nrevisit these conventional configurations. Through theoretical analysis and\nexperimental evaluation, we show that the masked autoencoder is effective in\nalleviating the over-smoothing issue in deep transformer training. Based on\nthis finding, we propose Bamboo, an idea of using deeper and narrower\ntransformer configurations, for masked autoencoder training. On ImageNet, with\nsuch a simple change in configuration, re-designed model achieves 87.1% top-1\naccuracy and outperforms SoTA models like MAE and BEiT. On language tasks,\nre-designed model outperforms BERT with default setting by 1.1 points on\naverage, on GLUE datasets.", "published": "2022-05-21 05:17:11", "link": "http://arxiv.org/abs/2205.10505v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Self-Supervised Speech Representation Learning: A Review", "abstract": "Although supervised deep learning has revolutionized speech and audio\nprocessing, it has necessitated the building of specialist models for\nindividual tasks and application scenarios. It is likewise difficult to apply\nthis to dialects and languages for which only limited labeled data is\navailable. Self-supervised representation learning methods promise a single\nuniversal model that would benefit a wide variety of tasks and domains. Such\nmethods have shown success in natural language processing and computer vision\ndomains, achieving new levels of performance while reducing the number of\nlabels required for many downstream scenarios. Speech representation learning\nis experiencing similar progress in three main categories: generative,\ncontrastive, and predictive methods. Other approaches rely on multi-modal data\nfor pre-training, mixing text or visual data streams with speech. Although\nself-supervised speech representation is still a nascent research area, it is\nclosely related to acoustic word embedding and learning with zero lexical\nresources, both of which have seen active research for many years. This review\npresents approaches for self-supervised speech representation learning and\ntheir connection to other research areas. Since many current methods focus\nsolely on automatic speech recognition as a downstream task, we review recent\nefforts on benchmarking learned representations to extend the application\nbeyond speech recognition.", "published": "2022-05-21 16:52:57", "link": "http://arxiv.org/abs/2205.10643v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
