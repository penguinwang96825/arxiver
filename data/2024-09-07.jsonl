{"title": "Semi-analytical pricing of options written on SOFR futures", "abstract": "In this paper, we propose a semi-analytical approach to pricing options on\nSOFR futures where the underlying SOFR follows a time-dependent CEV model. By\ndefinition, these options change their type at the beginning of the reference\nperiod: before this time, this is an American option written on a SOFR forward\nprice as an underlying, and after this point, this is an arithmetic Asian\noption with an American style exercise written on the daily SOFR rates. We\ndevelop a new version of the GIT method and solve both problems\nsemi-analytically, obtaining the option price, the exercise boundary, and the\noption Greeks. This work is intended to address the concern that the transfer\nfrom LIBOR to SOFR has resulted in a situation in which the options of the key\nmoney market (i.e., futures on the reference rate) are options without any\npricing model available. Therefore, the trading in options on 3M SOFR futures\ncurrently ends before their reference quarter starts, to eliminate the final\nmetamorphosis into exotic options.", "published": "2024-09-07 20:13:44", "link": "http://arxiv.org/abs/2409.04903v2", "categories": ["q-fin.CP", "q-fin.MF", "q-fin.PR"], "primary_category": "q-fin.CP"}
{"title": "Constrained Multi-Layer Contrastive Learning for Implicit Discourse\n  Relationship Recognition", "abstract": "Previous approaches to the task of implicit discourse relation recognition\n(IDRR) generally view it as a classification task. Even with pre-trained\nlanguage models, like BERT and RoBERTa, IDRR still relies on complicated neural\nnetworks with multiple intermediate layers to proper capture the interaction\nbetween two discourse units. As a result, the outputs of these intermediate\nlayers may have different capability in discriminating instances of different\nclasses. To this end, we propose to adapt a supervised contrastive learning\n(CL) method, label- and instance-centered CL, to enhance representation\nlearning. Moreover, we propose a novel constrained multi-layer CL approach to\nproperly impose a constraint that the contrastive loss of higher layers should\nbe smaller than that of lower layers. Experimental results on PDTB 2.0 and PDTB\n3.0 show that our approach can significantly improve the performance on both\nmulti-class classification and binary classification.", "published": "2024-09-07 17:55:41", "link": "http://arxiv.org/abs/2409.13716v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding\n  Models", "abstract": "Many use cases require retrieving smaller portions of text, and dense\nvector-based retrieval systems often perform better with shorter text segments,\nas the semantics are less likely to be over-compressed in the embeddings.\nConsequently, practitioners often split text documents into smaller chunks and\nencode them separately. However, chunk embeddings created in this way can lose\ncontextual information from surrounding chunks, resulting in sub-optimal\nrepresentations. In this paper, we introduce a novel method called late\nchunking, which leverages long context embedding models to first embed all\ntokens of the long text, with chunking applied after the transformer model and\njust before mean pooling - hence the term late in its naming. The resulting\nchunk embeddings capture the full contextual information, leading to superior\nresults across various retrieval tasks. The method is generic enough to be\napplied to a wide range of long-context embedding models and works without\nadditional training. To further increase the effectiveness of late chunking, we\npropose a dedicated fine-tuning approach for embedding models.", "published": "2024-09-07 03:54:46", "link": "http://arxiv.org/abs/2409.04701v2", "categories": ["cs.CL", "cs.IR", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Untie the Knots: An Efficient Data Augmentation Strategy for\n  Long-Context Pre-Training in Language Models", "abstract": "Large language models (LLM) have prioritized expanding the context window\nfrom which models can incorporate more information. However, training models to\nhandle long contexts presents significant challenges. These include the\nscarcity of high-quality natural long-context data, the potential for\nperformance degradation on short-context tasks, and the reduced training\nefficiency associated with attention mechanisms. In this paper, we introduce\nUntie the Knots (\\textbf{UtK}), a novel data augmentation strategy employed\nduring the continue pre-training phase, designed to efficiently enable LLMs to\ngain long-context capabilities without the need to modify the existing data\nmixture. In particular, we chunk the documents, shuffle the chunks, and create\na complex and knotted structure of long texts; LLMs are then trained to untie\nthese knots and identify relevant segments within seemingly chaotic token\nsequences. This approach greatly improves the model's performance by accurately\nattending to relevant information in long context and the training efficiency\nis also largely increased. We conduct extensive experiments on models with 7B\nand 72B parameters, trained on 20 billion tokens, demonstrating that UtK\nachieves 75\\% and 84.5\\% accurracy on RULER at 128K context length,\nsignificantly outperforming other long context strategies. The trained models\nwill open-source for further research.", "published": "2024-09-07 09:28:55", "link": "http://arxiv.org/abs/2409.04774v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LoCa: Logit Calibration for Knowledge Distillation", "abstract": "Knowledge Distillation (KD), aiming to train a better student model by\nmimicking the teacher model, plays an important role in model compression. One\ntypical way is to align the output logits. However, we find a common issue\nnamed mis-instruction, that the student would be misled when the predictions\nbased on teacher logits do not follow the labels. Meanwhile, there is other\nuseful dark knowledge in the logits such as the class discriminability, which\nis vital for distillation. In this paper, we propose a simple yet effective\nLogit Calibration (LoCa) method, which calibrates the logits from the teacher\nmodel based on the ground-truth labels. The key insight is to correct the\nprediction (to address the mis-instruction issue) and maintain useful dark\nknowledge simultaneously. Our proposed LoCa does not require any additional\nparameters. Empirical results on image classification and text generation tasks\ndemonstrate that LoCa can effectively improve the performance of baselines.", "published": "2024-09-07 09:38:36", "link": "http://arxiv.org/abs/2409.04778v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring Straightforward Conversational Red-Teaming", "abstract": "Large language models (LLMs) are increasingly used in business dialogue\nsystems but they pose security and ethical risks. Multi-turn conversations,\nwhere context influences the model's behavior, can be exploited to produce\nundesired responses. In this paper, we examine the effectiveness of utilizing\noff-the-shelf LLMs in straightforward red-teaming approaches, where an attacker\nLLM aims to elicit undesired output from a target LLM, comparing both\nsingle-turn and conversational red-teaming tactics. Our experiments offer\ninsights into various usage strategies that significantly affect their\nperformance as red teamers. They suggest that off-the-shelf models can act as\neffective red teamers and even adjust their attack strategy based on past\nattempts, although their effectiveness decreases with greater alignment.", "published": "2024-09-07 13:28:01", "link": "http://arxiv.org/abs/2409.04822v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Achieving Peak Performance for Large Language Models: A Systematic\n  Review", "abstract": "In recent years, large language models (LLMs) have achieved remarkable\nsuccess in natural language processing (NLP). LLMs require an extreme amount of\nparameters to attain high performance. As models grow into the\ntrillion-parameter range, computational and memory costs increase\nsignificantly. This makes it difficult for many researchers to access the\nresources needed to train or apply these models. Optimizing LLM performance\ninvolves two main approaches: fine-tuning pre-trained models for specific tasks\nto achieve state-of-the-art performance, and reducing costs or improving\ntraining time while maintaining similar performance. This paper presents a\nsystematic literature review (SLR) following the Preferred Reporting Items for\nSystematic Reviews and Meta-Analyses (PRISMA) statement. We reviewed 65\npublications out of 983 from 2017 to December 2023, retrieved from 5 databases.\nThe study presents methods to optimize and accelerate LLMs while achieving\ncutting-edge results without sacrificing accuracy. We begin with an overview of\nthe development of language modeling, followed by a detailed explanation of\ncommonly used frameworks and libraries, and a taxonomy for improving and\nspeeding up LLMs based on three classes: LLM training, LLM inference, and\nsystem serving. We then delve into recent optimization and acceleration\nstrategies such as training optimization, hardware optimization, scalability\nand reliability, accompanied by the taxonomy and categorization of these\nstrategies. Finally, we provide an in-depth comparison of each class and\nstrategy, with two case studies on optimizing model training and enhancing\ninference efficiency. These case studies showcase practical approaches to\naddress LLM resource limitations while maintaining performance.", "published": "2024-09-07 13:57:41", "link": "http://arxiv.org/abs/2409.04833v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Just ASR + LLM? A Study on Speech Large Language Models' Ability to\n  Identify and Understand Speaker in Spoken Dialogue", "abstract": "In recent years, we have observed a rapid advancement in speech language\nmodels (SpeechLLMs), catching up with humans' listening and reasoning\nabilities. SpeechLLMs have demonstrated impressive spoken dialog\nquestion-answering (SQA) performance in benchmarks like Gaokao, the English\nlistening test of the college entrance exam in China, which seemingly requires\nunderstanding both the spoken content and voice characteristics of speakers in\na conversation. However, after carefully examining Gaokao's questions, we find\nthe correct answers to many questions can be inferred from the conversation\ntranscript alone, i.e.\\ without speaker segmentation and identification. Our\nevaluation of state-of-the-art models Qwen-Audio and WavLLM on both Gaokao and\nour proposed \"What Do You Like?\" dataset shows a significantly higher accuracy\nin these context-based questions than in identity-critical questions, which can\nonly be answered reliably with correct speaker identification. The results and\nanalysis suggest that when solving SQA, the current SpeechLLMs exhibit limited\nspeaker awareness from the audio and behave similarly to an LLM reasoning from\nthe conversation transcription without sound. We propose that tasks focused on\nidentity-critical questions could offer a more accurate evaluation framework of\nSpeechLLMs in SQA.", "published": "2024-09-07 22:54:47", "link": "http://arxiv.org/abs/2409.04927v3", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Maximizing Relation Extraction Potential: A Data-Centric Study to Unveil\n  Challenges and Opportunities", "abstract": "Relation extraction is a Natural Language Processing task that aims to\nextract relationships from textual data. It is a critical step for information\nextraction. Due to its wide-scale applicability, research in relation\nextraction has rapidly scaled to using highly advanced neural networks. Despite\ntheir computational superiority, modern relation extractors fail to handle\ncomplicated extraction scenarios. However, a comprehensive performance analysis\nof the state-of-the-art extractors that compile these challenges has been\nmissing from the literature, and this paper aims to bridge this gap. The goal\nhas been to investigate the possible data-centric characteristics that impede\nneural relation extraction. Based on extensive experiments conducted using 15\nstate-of-the-art relation extraction algorithms ranging from recurrent\narchitectures to large language models and seven large-scale datasets, this\nresearch suggests that modern relation extractors are not robust to complex\ndata and relation characteristics. It emphasizes pivotal issues, such as\ncontextual ambiguity, correlating relations, long-tail data, and fine-grained\nrelation distributions. In addition, it sets a marker for future directions to\nalleviate these issues, thereby proving to be a critical resource for novice\nand advanced researchers. Efficient handling of the challenges described can\nhave significant implications for the field of information extraction, which is\na critical part of popular systems such as search engines and chatbots. Data\nand relevant code can be found at\n\\url{https://aaig.ece.ufl.edu/projects/relation-extraction}.", "published": "2024-09-07 23:40:47", "link": "http://arxiv.org/abs/2409.04934v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Good Idea or Not, Representation of LLM Could Tell", "abstract": "In the ever-expanding landscape of academic research, the proliferation of\nideas presents a significant challenge for researchers: discerning valuable\nideas from the less impactful ones. The ability to efficiently evaluate the\npotential of these ideas is crucial for the advancement of science and paper\nreview. In this work, we focus on idea assessment, which aims to leverage the\nknowledge of large language models to assess the merit of scientific ideas.\nFirst, we investigate existing text evaluation research and define the problem\nof quantitative evaluation of ideas. Second, we curate and release a benchmark\ndataset from nearly four thousand manuscript papers with full texts,\nmeticulously designed to train and evaluate the performance of different\napproaches to this task. Third, we establish a framework for quantifying the\nvalue of ideas by employing representations in a specific layer of large\nlanguage models. Experimental results show that the scores predicted by our\nmethod are relatively consistent with those of humans. Our findings suggest\nthat the representations of large language models hold more potential in\nquantifying the value of ideas than their generative outputs, demonstrating a\npromising avenue for automating the idea assessment process.", "published": "2024-09-07 02:07:22", "link": "http://arxiv.org/abs/2409.13712v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "QueryBuilder: Human-in-the-Loop Query Development for Information\n  Retrieval", "abstract": "Frequently, users of an Information Retrieval (IR) system start with an\noverarching information need (a.k.a., an analytic task) and proceed to define\nfiner-grained queries covering various important aspects (i.e., sub-topics) of\nthat analytic task. We present a novel, interactive system called\n$\\textit{QueryBuilder}$, which allows a novice, English-speaking user to create\nqueries with a small amount of effort, through efficient exploration of an\nEnglish development corpus in order to rapidly develop cross-lingual\ninformation retrieval queries corresponding to the user's information needs.\nQueryBuilder performs near real-time retrieval of documents based on\nuser-entered search terms; the user looks through the retrieved documents and\nmarks sentences as relevant to the information needed. The marked sentences are\nused by the system as additional information in query formation and refinement:\nquery terms (and, optionally, event features, which capture event $'triggers'$\n(indicator terms) and agent/patient roles) are appropriately weighted, and a\nneural-based system, which better captures textual meaning, retrieves other\nrelevant content. The process of retrieval and marking is repeated as many\ntimes as desired, giving rise to increasingly refined queries in each\niteration. The final product is a fine-grained query used in Cross-Lingual\nInformation Retrieval (CLIR). Our experiments using analytic tasks and requests\nfrom the IARPA BETTER IR datasets show that with a small amount of effort (at\nmost 10 minutes per sub-topic), novice users can form $\\textit{useful}$\nfine-grained queries including in languages they don't understand. QueryBuilder\nalso provides beneficial capabilities to the traditional corpus exploration and\nquery formation process. A demonstration video is released at\nhttps://vimeo.com/734795835", "published": "2024-09-07 00:46:58", "link": "http://arxiv.org/abs/2409.04667v2", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Selective Self-Rehearsal: A Fine-Tuning Approach to Improve\n  Generalization in Large Language Models", "abstract": "Fine-tuning Large Language Models (LLMs) on specific datasets is a common\npractice to improve performance on target tasks. However, this performance gain\noften leads to overfitting, where the model becomes too specialized in either\nthe task or the characteristics of the training data, resulting in a loss of\ngeneralization. This paper introduces Selective Self-Rehearsal (SSR), a\nfine-tuning approach that achieves performance comparable to the standard\nsupervised fine-tuning (SFT) while improving generalization. SSR leverages the\nfact that there can be multiple valid responses to a query. By utilizing the\nmodel's correct responses, SSR reduces model specialization during the\nfine-tuning stage. SSR first identifies the correct model responses from the\ntraining set by deploying an appropriate LLM as a judge. Then, it fine-tunes\nthe model using the correct model responses and the gold response for the\nremaining samples. The effectiveness of SSR is demonstrated through experiments\non the task of identifying unanswerable queries across various datasets. The\nresults show that standard SFT can lead to an average performance drop of up to\n$16.7\\%$ on multiple benchmarks, such as MMLU and TruthfulQA. In contrast, SSR\nresults in close to $2\\%$ drop on average, indicating better generalization\ncapabilities compared to standard SFT.", "published": "2024-09-07 10:21:03", "link": "http://arxiv.org/abs/2409.04787v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Phrase-Level Adversarial Training for Mitigating Bias in Neural\n  Network-based Automatic Essay Scoring", "abstract": "Automatic Essay Scoring (AES) is widely used to evaluate candidates for\neducational purposes. However, due to the lack of representative data, most\nexisting AES systems are not robust, and their scoring predictions are biased\ntowards the most represented data samples. In this study, we propose a\nmodel-agnostic phrase-level method to generate an adversarial essay set to\naddress the biases and robustness of AES models. Specifically, we construct an\nattack test set comprising samples from the original test set and adversarially\ngenerated samples using our proposed method. To evaluate the effectiveness of\nthe attack strategy and data augmentation, we conducted a comprehensive\nanalysis utilizing various neural network scoring models. Experimental results\nshow that the proposed approach significantly improves AES model performance in\nthe presence of adversarial examples and scenarios without such attacks.", "published": "2024-09-07 11:22:35", "link": "http://arxiv.org/abs/2409.04795v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Property Neurons in Self-Supervised Speech Transformers", "abstract": "There have been many studies on analyzing self-supervised speech\nTransformers, in particular, with layer-wise analysis. It is, however,\ndesirable to have an approach that can pinpoint exactly a subset of neurons\nthat is responsible for a particular property of speech, being amenable to\nmodel pruning and model editing. In this work, we identify a set of property\nneurons in the feedforward layers of Transformers to study how speech-related\nproperties, such as phones, gender, and pitch, are stored. When removing\nneurons of a particular property (a simple form of model editing), the\nrespective downstream performance significantly degrades, showing the\nimportance of the property neurons. We apply this approach to pruning the\nfeedforward layers in Transformers, where most of the model parameters are. We\nshow that protecting property neurons during pruning is significantly more\neffective than norm-based pruning. The code for identifying property neurons is\navailable at https://github.com/nervjack2/PropertyNeurons.", "published": "2024-09-07 05:59:19", "link": "http://arxiv.org/abs/2409.05910v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "TracrBench: Generating Interpretability Testbeds with Large Language\n  Models", "abstract": "Achieving a mechanistic understanding of transformer-based language models is\nan open challenge, especially due to their large number of parameters.\nMoreover, the lack of ground truth mappings between model weights and their\nfunctional roles hinders the effective evaluation of interpretability methods,\nimpeding overall progress. Tracr, a method for generating compiled transformers\nwith inherent ground truth mappings in RASP, has been proposed to address this\nissue. However, manually creating a large number of models needed for verifying\ninterpretability methods is labour-intensive and time-consuming. In this work,\nwe present a novel approach for generating interpretability test beds using\nlarge language models (LLMs) and introduce TracrBench, a novel dataset\nconsisting of 121 manually written and LLM-generated, human-validated RASP\nprograms and their corresponding transformer weights. During this process, we\nevaluate the ability of frontier LLMs to autonomously generate RASP programs\nand find that this task poses significant challenges. GPT-4-turbo, with a\n20-shot prompt and best-of-5 sampling, correctly implements only 57 out of 101\ntest programs, necessitating the manual implementation of the remaining\nprograms. With its 121 samples, TracrBench aims to serve as a valuable testbed\nfor evaluating and comparing interpretability methods.", "published": "2024-09-07 10:02:51", "link": "http://arxiv.org/abs/2409.13714v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Introducing MeMo: A Multimodal Dataset for Memory Modelling in\n  Multiparty Conversations", "abstract": "Conversational memory is the process by which humans encode, retain and\nretrieve verbal, non-verbal and contextual information from a conversation.\nSince human memory is selective, differing recollections of the same events can\nlead to misunderstandings and misalignments within a group. Yet, conversational\nfacilitation systems, aimed at advancing the quality of group interactions,\nusually focus on tracking users' states within an individual session, ignoring\nwhat remains in each participant's memory after the interaction. Understanding\nconversational memory can be used as a source of information on the long-term\ndevelopment of social connections within a group. This paper introduces the\nMeMo corpus, the first conversational dataset annotated with participants'\nmemory retention reports, aimed at facilitating computational modelling of\nhuman conversational memory. The MeMo corpus includes 31 hours of small-group\ndiscussions on Covid-19, repeated 3 times over the term of 2 weeks. It\nintegrates validated behavioural and perceptual measures, audio, video, and\nmultimodal annotations, offering a valuable resource for studying and modelling\nconversational memory and group dynamics. By introducing the MeMo corpus,\nanalysing its validity, and demonstrating its usefulness for future research,\nthis paper aims to pave the way for future research in conversational memory\nmodelling for intelligent system development.", "published": "2024-09-07 16:09:36", "link": "http://arxiv.org/abs/2409.13715v2", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DiVA-DocRE: A Discriminative and Voice-Aware Paradigm for Document-Level\n  Relation Extraction", "abstract": "The remarkable capabilities of Large Language Models (LLMs) in text\ncomprehension and generation have revolutionized Information Extraction (IE).\nOne such advancement is in Document-level Relation Triplet Extraction (DocRTE),\na critical task in information systems that aims to extract entities and their\nsemantic relationships from documents. However, existing methods are primarily\ndesigned for Sentence level Relation Triplet Extraction (SentRTE), which\ntypically handles a limited set of relations and triplet facts within a single\nsentence. Additionally, some approaches treat relations as candidate choices\nintegrated into prompt templates, resulting in inefficient processing and\nsuboptimal performance when determining the relation elements in triplets. To\naddress these limitations, we introduce a Discriminative and Voice Aware\nParadigm DiVA. DiVA involves only two steps: performing document-level relation\nextraction (DocRE) and then identifying the subject object entities based on\nthe relation. No additional processing is required simply input the document to\ndirectly obtain the triplets. This streamlined process more accurately reflects\nreal-world scenarios for triplet extraction. Our innovation lies in\ntransforming DocRE into a discriminative task, where the model pays attention\nto each relation and to the often overlooked issue of active vs. passive voice\nwithin the triplet. Our experiments on the Re-DocRED and DocRED datasets\ndemonstrate state-of-the-art results for the DocRTE task.", "published": "2024-09-07 18:47:38", "link": "http://arxiv.org/abs/2409.13717v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "MILE: A Mutation Testing Framework of In-Context Learning Systems", "abstract": "In-context Learning (ICL) has achieved notable success in the applications of\nlarge language models (LLMs). By adding only a few input-output pairs that\ndemonstrate a new task, the LLM can efficiently learn the task during inference\nwithout modifying the model parameters. Such mysterious ability of LLMs has\nattracted great research interests in understanding, formatting, and improving\nthe in-context demonstrations, while still suffering from drawbacks like\nblack-box mechanisms and sensitivity against the selection of examples. In this\nwork, inspired by the foundations of adopting testing techniques in machine\nlearning (ML) systems, we propose a mutation testing framework designed to\ncharacterize the quality and effectiveness of test data for ICL systems. First,\nwe propose several mutation operators specialized for ICL demonstrations, as\nwell as corresponding mutation scores for ICL test sets. With comprehensive\nexperiments, we showcase the effectiveness of our framework in evaluating the\nreliability and quality of ICL test suites. Our code is available at\nhttps://github.com/weizeming/MILE.", "published": "2024-09-07 13:51:42", "link": "http://arxiv.org/abs/2409.04831v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Sentiment Informed Sentence BERT-Ensemble Algorithm for Depression\n  Detection", "abstract": "The World Health Organisation (WHO) revealed approximately 280 million people\nin the world suffer from depression. Yet, existing studies on early-stage\ndepression detection using machine learning (ML) techniques are limited. Prior\nstudies have applied a single stand-alone algorithm, which is unable to deal\nwith data complexities, prone to overfitting, and limited in generalization. To\nthis end, our paper examined the performance of several ML algorithms for\nearly-stage depression detection using two benchmark social media datasets (D1\nand D2). More specifically, we incorporated sentiment indicators to improve our\nmodel performance. Our experimental results showed that sentence bidirectional\nencoder representations from transformers (SBERT) numerical vectors fitted into\nthe stacking ensemble model achieved comparable F1 scores of 69% in the dataset\n(D1) and 76% in the dataset (D2). Our findings suggest that utilizing sentiment\nindicators as an additional feature for depression detection yields an improved\nmodel performance, and thus, we recommend the development of a depressive term\ncorpus for future work.", "published": "2024-09-07 07:47:55", "link": "http://arxiv.org/abs/2409.13713v1", "categories": ["cs.CL", "cs.LG", "math.ST", "stat.AP", "stat.TH", "H.3.3"], "primary_category": "cs.CL"}
{"title": "Mel-RoFormer for Vocal Separation and Vocal Melody Transcription", "abstract": "Developing a versatile deep neural network to model music audio is crucial in\nMIR. This task is challenging due to the intricate spectral variations inherent\nin music signals, which convey melody, harmonics, and timbres of diverse\ninstruments. In this paper, we introduce Mel-RoFormer, a spectrogram-based\nmodel featuring two key designs: a novel Mel-band Projection module at the\nfront-end to enhance the model's capability to capture informative features\nacross multiple frequency bands, and interleaved RoPE Transformers to\nexplicitly model the frequency and time dimensions as two separate sequences.\nWe apply Mel-RoFormer to tackle two essential MIR tasks: vocal separation and\nvocal melody transcription, aimed at isolating singing voices from audio\nmixtures and transcribing their lead melodies, respectively. Despite their\nshared focus on singing signals, these tasks possess distinct optimization\nobjectives. Instead of training a unified model, we adopt a two-step approach.\nInitially, we train a vocal separation model, which subsequently serves as a\nfoundation model for fine-tuning for vocal melody transcription. Through\nextensive experiments conducted on benchmark datasets, we showcase that our\nmodels achieve state-of-the-art performance in both vocal separation and melody\ntranscription tasks, underscoring the efficacy and versatility of Mel-RoFormer\nin modeling complex music audio signals.", "published": "2024-09-07 03:55:02", "link": "http://arxiv.org/abs/2409.04702v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "PB-LRDWWS System for the SLT 2024 Low-Resource Dysarthria Wake-Up Word\n  Spotting Challenge", "abstract": "For the SLT 2024 Low-Resource Dysarthria Wake-Up Word Spotting (LRDWWS)\nChallenge, we introduce the PB-LRDWWS system. This system combines a dysarthric\nspeech content feature extractor for prototype construction with a\nprototype-based classification method. The feature extractor is a fine-tuned\nHuBERT model obtained through a three-stage fine-tuning process using\ncross-entropy loss. This fine-tuned HuBERT extracts features from the target\ndysarthric speaker's enrollment speech to build prototypes. Classification is\nachieved by calculating the cosine similarity between the HuBERT features of\nthe target dysarthric speaker's evaluation speech and prototypes. Despite its\nsimplicity, our method demonstrates effectiveness through experimental results.\nOur system achieves second place in the final Test-B of the LRDWWS Challenge.", "published": "2024-09-07 11:39:15", "link": "http://arxiv.org/abs/2409.04799v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Cross-attention Inspired Selective State Space Models for Target Sound\n  Extraction", "abstract": "The Transformer model, particularly its cross-attention module, is widely\nused for feature fusion in target sound extraction which extracts the signal of\ninterest based on given clues. Despite its effectiveness, this approach suffers\nfrom low computational efficiency. Recent advancements in state space models,\nnotably the latest work Mamba, have shown comparable performance to\nTransformer-based methods while significantly reducing computational complexity\nin various tasks. However, Mamba's applicability in target sound extraction is\nlimited due to its inability to capture dependencies between different\nsequences as the cross-attention does. In this paper, we propose CrossMamba for\ntarget sound extraction, which leverages the hidden attention mechanism of\nMamba to compute dependencies between the given clues and the audio mixture.\nThe calculation of Mamba can be divided to the query, key and value. We utilize\nthe clue to generate the query and the audio mixture to derive the key and\nvalue, adhering to the principle of the cross-attention mechanism in\nTransformers. Experimental results from two representative target sound\nextraction methods validate the efficacy of the proposed CrossMamba.", "published": "2024-09-07 12:01:08", "link": "http://arxiv.org/abs/2409.04803v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Leveraging Sound Source Trajectories for Universal Sound Separation", "abstract": "Existing methods utilizing spatial information for sound source separation\nrequire prior knowledge of the direction of arrival (DOA) of the source or\nutilize estimated but imprecise localization results, which impairs the\nseparation performance, especially when the sound sources are moving. In fact,\nsound source localization and separation are interconnected problems, that is,\nsound source localization facilitates sound separation while sound separation\ncontributes to refined source localization. This paper proposes a method\nutilizing the mutual facilitation mechanism between sound source localization\nand separation for moving sources. The proposed method comprises three stages.\nThe first stage is initial tracking, which tracks each sound source from the\naudio mixture based on the source signal envelope estimation. These tracking\nresults may lack sufficient accuracy. The second stage involves mutual\nfacilitation: Sound separation is conducted using preliminary sound source\ntracking results. Subsequently, sound source tracking is performed on the\nseparated signals, thereby refining the tracking precision. The refined\ntrajectories further improve separation performance. This mutual facilitation\nprocess can be iterated multiple times. In the third stage, a neural beamformer\nestimates precise single-channel separation results based on the refined\ntracking trajectories and multi-channel separation outputs. Simulation\nexperiments conducted under reverberant conditions and with moving sound\nsources demonstrate that the proposed method can achieve more accurate\nseparation based on refined tracking results.", "published": "2024-09-07 14:48:11", "link": "http://arxiv.org/abs/2409.04843v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Flow-TSVAD: Target-Speaker Voice Activity Detection via Latent Flow\n  Matching", "abstract": "Speaker diarization is typically considered a discriminative task, using\ndiscriminative approaches to produce fixed diarization results. In this paper,\nwe explore the use of neural network-based generative methods for speaker\ndiarization for the first time. We implement a Flow-Matching (FM) based\ngenerative algorithm within the sequence-to-sequence target speaker voice\nactivity detection (Seq2Seq-TSVAD) diarization system. Our experiments reveal\nthat applying the generative method directly to the original binary label\nsequence space of the TS-VAD output is ineffective. To address this issue, we\npropose mapping the binary label sequence into a dense latent space before\napplying the generative algorithm and our proposed Flow-TSVAD method\noutperforms the Seq2Seq-TSVAD system. Additionally, we observe that the FM\nalgorithm converges rapidly during the inference stage, requiring only two\ninference steps to achieve promising results. As a generative model, Flow-TSVAD\nallows for sampling different diarization results by running the model multiple\ntimes. Moreover, ensembling results from various sampling instances further\nenhances diarization performance.", "published": "2024-09-07 15:38:28", "link": "http://arxiv.org/abs/2409.04859v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
