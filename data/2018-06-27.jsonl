{"title": "Generalized chart constraints for efficient PCFG and TAG parsing", "abstract": "Chart constraints, which specify at which string positions a constituent may\nbegin or end, have been shown to speed up chart parsers for PCFGs. We\ngeneralize chart constraints to more expressive grammar formalisms and describe\na neural tagger which predicts chart constraints at very high precision. Our\nconstraints accelerate both PCFG and TAG parsing, and combine effectively with\nother pruning techniques (coarse-to-fine and supertagging) for an overall\nspeedup of two orders of magnitude, while improving accuracy.", "published": "2018-06-27 19:36:05", "link": "http://arxiv.org/abs/1806.10654v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "hep-th", "abstract": "We apply techniques in natural language processing, computational\nlinguistics, and machine-learning to investigate papers in hep-th and four\nrelated sections of the arXiv: hep-ph, hep-lat, gr-qc, and math-ph. All of the\ntitles of papers in each of these sections, from the inception of the arXiv\nuntil the end of 2017, are extracted and treated as a corpus which we use to\ntrain the neural network Word2Vec. A comparative study of common n-grams,\nlinear syntactical identities, word cloud and word similarities is carried out.\nWe find notable scientific and sociological differences between the fields. In\nconjunction with support vector machines, we also show that the syntactic\nstructure of the titles in different sub-fields of high energy and mathematical\nphysics are sufficiently different that a neural network can perform a binary\nclassification of formal versus phenomenological sections with 87.1% accuracy,\nand can perform a finer five-fold classification across all sections with 65.1%\naccuracy.", "published": "2018-06-27 11:44:35", "link": "http://arxiv.org/abs/1807.00735v1", "categories": ["cs.CL", "hep-th"], "primary_category": "cs.CL"}
{"title": "Unsupervised and Efficient Vocabulary Expansion for Recurrent Neural\n  Network Language Models in ASR", "abstract": "In automatic speech recognition (ASR) systems, recurrent neural network\nlanguage models (RNNLM) are used to rescore a word lattice or N-best hypotheses\nlist. Due to the expensive training, the RNNLM's vocabulary set accommodates\nonly small shortlist of most frequent words. This leads to suboptimal\nperformance if an input speech contains many out-of-shortlist (OOS) words. An\neffective solution is to increase the shortlist size and retrain the entire\nnetwork which is highly inefficient. Therefore, we propose an efficient method\nto expand the shortlist set of a pretrained RNNLM without incurring expensive\nretraining and using additional training data. Our method exploits the\nstructure of RNNLM which can be decoupled into three parts: input projection\nlayer, middle layers, and output projection layer. Specifically, our method\nexpands the word embedding matrices in projection layers and keeps the middle\nlayers unchanged. In this approach, the functionality of the pretrained RNNLM\nwill be correctly maintained as long as OOS words are properly modeled in two\nembedding spaces. We propose to model the OOS words by borrowing linguistic\nknowledge from appropriate in-shortlist words. Additionally, we propose to\ngenerate the list of OOS words to expand vocabulary in unsupervised manner by\nautomatically extracting them from ASR output.", "published": "2018-06-27 05:50:05", "link": "http://arxiv.org/abs/1806.10306v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Learning Visually-Grounded Semantics from Contrastive Adversarial\n  Samples", "abstract": "We study the problem of grounding distributional representations of texts on\nthe visual domain, namely visual-semantic embeddings (VSE for short). Begin\nwith an insightful adversarial attack on VSE embeddings, we show the limitation\nof current frameworks and image-text datasets (e.g., MS-COCO) both\nquantitatively and qualitatively. The large gap between the number of possible\nconstitutions of real-world semantics and the size of parallel data, to a large\nextent, restricts the model to establish the link between textual semantics and\nvisual concepts. We alleviate this problem by augmenting the MS-COCO image\ncaptioning datasets with textual contrastive adversarial samples. These samples\nare synthesized using linguistic rules and the WordNet knowledge base. The\nconstruction procedure is both syntax- and semantics-aware. The samples enforce\nthe model to ground learned embeddings to concrete concepts within the image.\nThis simple but powerful technique brings a noticeable improvement over the\nbaselines on a diverse set of downstream tasks, in addition to defending\nknown-type adversarial attacks. We release the codes at\nhttps://github.com/ExplorerFreda/VSE-C.", "published": "2018-06-27 08:58:57", "link": "http://arxiv.org/abs/1806.10348v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation for Query Construction and Composition", "abstract": "Research on question answering with knowledge base has recently seen an\nincreasing use of deep architectures. In this extended abstract, we study the\napplication of the neural machine translation paradigm for question parsing. We\nemploy a sequence-to-sequence model to learn graph patterns in the SPARQL graph\nquery language and their compositions. Instead of inducing the programs through\nquestion-answer pairs, we expect a semi-supervised approach, where alignments\nbetween questions and queries are built through templates. We argue that the\ncoverage of language utterances can be expanded using late notable works in\nnatural language generation.", "published": "2018-06-27 13:40:49", "link": "http://arxiv.org/abs/1806.10478v2", "categories": ["cs.CL", "cs.AI", "cs.DB", "68T99", "I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "Independent Deeply Learned Matrix Analysis for Multichannel Audio Source\n  Separation", "abstract": "In this paper, we address a multichannel audio source separation task and\npropose a new efficient method called independent deeply learned matrix\nanalysis (IDLMA). IDLMA estimates the demixing matrix in a blind manner and\nupdates the time-frequency structures of each source using a pretrained deep\nneural network (DNN). Also, we introduce a complex Student's t-distribution as\na generalized source generative model including both complex Gaussian and\nCauchy distributions. Experiments are conducted using music signals with a\ntraining dataset, and the results show the validity of the proposed method in\nterms of separation accuracy and computational cost.", "published": "2018-06-27 05:52:40", "link": "http://arxiv.org/abs/1806.10307v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Denoising with Deep Feature Losses", "abstract": "We present an end-to-end deep learning approach to denoising speech signals\nby processing the raw waveform directly. Given input audio containing speech\ncorrupted by an additive background signal, the system aims to produce a\nprocessed signal that contains only the speech content. Recent approaches have\nshown promising results using various deep network architectures. In this\npaper, we propose to train a fully-convolutional context aggregation network\nusing a deep feature loss. That loss is based on comparing the internal feature\nactivations in a different network, trained for acoustic environment detection\nand domestic audio tagging. Our approach outperforms the state-of-the-art in\nobjective speech quality metrics and in large-scale perceptual experiments with\nhuman listeners. It also outperforms an identical network trained using\ntraditional regression losses. The advantage of the new approach is\nparticularly pronounced for the hardest data with the most intrusive background\nnoise, for which denoising is most needed and most challenging.", "published": "2018-06-27 15:08:23", "link": "http://arxiv.org/abs/1806.10522v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Modeling Majorness as a Perceptual Property in Music from Listener\n  Ratings", "abstract": "For the tasks of automatic music emotion recognition, genre recognition,\nmusic recommendation it is helpful to be able to extract mode from any section\nof a musical piece as a perceived amount of major or minor mode (majorness)\ninside that section, perceived as a whole (one or several melodies and any\nharmony present). In this paper we take a data-driven approach (modeling\ndirectly from data without giving an explicit definition or explicitly\nprogramming an algorithm) towards modeling this property. We collect\nannotations from musicians and show that majorness can be understood by\nmusicians in an intuitive way. We model this property from the data using deep\nlearning.", "published": "2018-06-27 17:05:48", "link": "http://arxiv.org/abs/1806.10570v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
