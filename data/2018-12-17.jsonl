{"title": "Siamese Networks for Semantic Pattern Similarity", "abstract": "Semantic Pattern Similarity is an interesting, though not often encountered\nNLP task where two sentences are compared not by their specific meaning, but by\ntheir more abstract semantic pattern (e.g., preposition or frame). We utilize\nSiamese Networks to model this task, and show its usefulness in determining SQL\npatterns for unseen questions in a database-backed question answering scenario.\nOur approach achieves high accuracy and contains a built-in proxy for\nconfidence, which can be used to keep precision arbitrarily high.", "published": "2018-12-17 03:52:11", "link": "http://arxiv.org/abs/1812.06604v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fully Convolutional Speech Recognition", "abstract": "Current state-of-the-art speech recognition systems build on recurrent neural\nnetworks for acoustic and/or language modeling, and rely on feature extraction\npipelines to extract mel-filterbanks or cepstral coefficients. In this paper we\npresent an alternative approach based solely on convolutional neural networks,\nleveraging recent advances in acoustic models from the raw waveform and\nlanguage modeling. This fully convolutional approach is trained end-to-end to\npredict characters from the raw waveform, removing the feature extraction step\naltogether. An external convolutional language model is used to decode words.\nOn Wall Street Journal, our model matches the current state-of-the-art. On\nLibrispeech, we report state-of-the-art performance among end-to-end models,\nincluding Deep Speech 2 trained with 12 times more acoustic data and\nsignificantly more linguistic data.", "published": "2018-12-17 16:07:12", "link": "http://arxiv.org/abs/1812.06864v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-task learning to improve natural language understanding", "abstract": "Recently advancements in sequence-to-sequence neural network architectures\nhave led to an improved natural language understanding. When building a neural\nnetwork-based Natural Language Understanding component, one main challenge is\nto collect enough training data. The generation of a synthetic dataset is an\ninexpensive and quick way to collect data. Since this data often has less\nvariety than real natural language, neural networks often have problems to\ngeneralize to unseen utterances during testing. In this work, we address this\nchallenge by using multi-task learning. We train out-of-domain real data\nalongside in-domain synthetic data to improve natural language understanding.\nWe evaluate this approach in the domain of airline travel information with two\nsynthetic datasets. As out-of-domain real data, we test two datasets based on\nthe subtitles of movies and series. By using an attention-based encoder-decoder\nmodel, we were able to improve the F1-score over strong baselines from 80.76 %\nto 84.98 % in the smaller synthetic dataset.", "published": "2018-12-17 16:32:05", "link": "http://arxiv.org/abs/1812.06876v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Feature Fusion Effects of Tensor Product Representation on\n  (De)Compositional Network for Caption Generation for Images", "abstract": "Progress in image captioning is gradually getting complex as researchers try\nto generalized the model and define the representation between visual features\nand natural language processing. This work tried to define such kind of\nrelationship in the form of representation called Tensor Product Representation\n(TPR) which generalized the scheme of language modeling and structuring the\nlinguistic attributes (related to grammar and parts of speech of language)\nwhich will provide a much better structure and grammatically correct sentence.\nTPR enables better and unique representation and structuring of the feature\nspace and will enable better sentence composition from these representations. A\nlarge part of the different ways of defining and improving these TPR are\ndiscussed and their performance with respect to the traditional procedures and\nfeature representations are evaluated for image captioning application. The new\nmodels achieved considerable improvement than the corresponding previous\narchitectures.", "published": "2018-12-17 06:24:03", "link": "http://arxiv.org/abs/1812.06624v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Hateminers : Detecting Hate speech against Women", "abstract": "With the online proliferation of hate speech, there is an urgent need for\nsystems that can detect such harmful content. In this paper, We present the\nmachine learning models developed for the Automatic Misogyny Identification\n(AMI) shared task at EVALITA 2018. We generate three types of features:\nSentence Embeddings, TF-IDF Vectors, and BOW Vectors to represent each tweet.\nThese features are then concatenated and fed into the machine learning models.\nOur model came First for the English Subtask A and Fifth for the English\nSubtask B. We release our winning model for public use and it's available at\nhttps://github.com/punyajoy/Hateminers-EVALITA.", "published": "2018-12-17 11:20:15", "link": "http://arxiv.org/abs/1812.06700v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "From FiLM to Video: Multi-turn Question Answering with Multi-modal\n  Context", "abstract": "Understanding audio-visual content and the ability to have an informative\nconversation about it have both been challenging areas for intelligent systems.\nThe Audio Visual Scene-aware Dialog (AVSD) challenge, organized as a track of\nthe Dialog System Technology Challenge 7 (DSTC7), proposes a combined task,\nwhere a system has to answer questions pertaining to a video given a dialogue\nwith previous question-answer pairs and the video itself. We propose for this\ntask a hierarchical encoder-decoder model which computes a multi-modal\nembedding of the dialogue context. It first embeds the dialogue history using\ntwo LSTMs. We extract video and audio frames at regular intervals and compute\nsemantic features using pre-trained I3D and VGGish models, respectively. Before\nsummarizing both modalities into fixed-length vectors using LSTMs, we use FiLM\nblocks to condition them on the embeddings of the current question, which\nallows us to reduce the dimensionality considerably. Finally, we use an LSTM\ndecoder that we train with scheduled sampling and evaluate using beam search.\nCompared to the modality-fusing baseline model released by the AVSD challenge\norganizers, our model achieves a relative improvements of more than 16%,\nscoring 0.36 BLEU-4 and more than 33%, scoring 0.997 CIDEr.", "published": "2018-12-17 19:41:37", "link": "http://arxiv.org/abs/1812.07023v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Learning Private Neural Language Modeling with Attentive Aggregation", "abstract": "Mobile keyboard suggestion is typically regarded as a word-level language\nmodeling problem. Centralized machine learning technique requires massive user\ndata collected to train on, which may impose privacy concerns for sensitive\npersonal typing data of users. Federated learning (FL) provides a promising\napproach to learning private language modeling for intelligent personalized\nkeyboard suggestion by training models in distributed clients rather than\ntraining in a central server. To obtain a global model for prediction, existing\nFL algorithms simply average the client models and ignore the importance of\neach client during model aggregation. Furthermore, there is no optimization for\nlearning a well-generalized global model on the central server. To solve these\nproblems, we propose a novel model aggregation with the attention mechanism\nconsidering the contribution of clients models to the global model, together\nwith an optimization technique during server aggregation. Our proposed\nattentive aggregation method minimizes the weighted distance between the server\nmodel and client models through iterative parameters updating while attends the\ndistance between the server model and client models. Through experiments on two\npopular language modeling datasets and a social media dataset, our proposed\nmethod outperforms its counterparts in terms of perplexity and communication\ncost in most settings of comparison.", "published": "2018-12-17 23:51:58", "link": "http://arxiv.org/abs/1812.07108v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Conditional BERT Contextual Augmentation", "abstract": "We propose a novel data augmentation method for labeled sentences called\nconditional BERT contextual augmentation. Data augmentation methods are often\napplied to prevent overfitting and improve generalization of deep neural\nnetwork models. Recently proposed contextual augmentation augments labeled\nsentences by randomly replacing words with more varied substitutions predicted\nby language model. BERT demonstrates that a deep bidirectional language model\nis more powerful than either an unidirectional language model or the shallow\nconcatenation of a forward and backward model. We retrofit BERT to conditional\nBERT by introducing a new conditional masked language model\\footnote{The term\n\"conditional masked language model\" appeared once in original BERT paper, which\nindicates context-conditional, is equivalent to term \"masked language model\".\nIn our paper, \"conditional masked language model\" indicates we apply extra\nlabel-conditional constraint to the \"masked language model\".} task. The well\ntrained conditional BERT can be applied to enhance contextual augmentation.\nExperiments on six various different text classification tasks show that our\nmethod can be easily applied to both convolutional or recurrent neural networks\nclassifier to obtain obvious improvement.", "published": "2018-12-17 11:26:42", "link": "http://arxiv.org/abs/1812.06705v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TechKG: A Large-Scale Chinese Technology-Oriented Knowledge Graph", "abstract": "Knowledge graph is a kind of valuable knowledge base which would benefit lots\nof AI-related applications. Up to now, lots of large-scale knowledge graphs\nhave been built. However, most of them are non-Chinese and designed for general\npurpose. In this work, we introduce TechKG, a large scale Chinese knowledge\ngraph that is technology-oriented. It is built automatically from massive\ntechnical papers that are published in Chinese academic journals of different\nresearch domains. Some carefully designed heuristic rules are used to extract\nhigh quality entities and relations. Totally, it comprises of over 260 million\ntriplets that are built upon more than 52 million entities which come from 38\nresearch domains. Our preliminary ex-periments indicate that TechKG has high\nadaptability and can be used as a dataset for many diverse AI-related\napplications. We released TechKG at: http://www.techkg.cn.", "published": "2018-12-17 11:59:53", "link": "http://arxiv.org/abs/1812.06722v1", "categories": ["cs.AI", "cs.CL", "cs.DL"], "primary_category": "cs.AI"}
{"title": "A Tutorial on Deep Latent Variable Models of Natural Language", "abstract": "There has been much recent, exciting work on combining the complementary\nstrengths of latent variable models and deep learning. Latent variable modeling\nmakes it easy to explicitly specify model constraints through conditional\nindependence properties, while deep learning makes it possible to parameterize\nthese conditional likelihoods with powerful function approximators. While these\n\"deep latent variable\" models provide a rich, flexible framework for modeling\nmany real-world phenomena, difficulties exist: deep parameterizations of\nconditional likelihoods usually make posterior inference intractable, and\nlatent variable objectives often complicate backpropagation by introducing\npoints of non-differentiability. This tutorial explores these issues in depth\nthrough the lens of variational inference.", "published": "2018-12-17 15:26:29", "link": "http://arxiv.org/abs/1812.06834v3", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Persian Vowel recognition with MFCC and ANN on PCVC speech dataset", "abstract": "In this paper a new method for recognition of consonant-vowel phonemes\ncombination on a new Persian speech dataset titled as PCVC (Persian\nConsonant-Vowel Combination) is proposed which is used to recognize Persian\nphonemes. In PCVC dataset, there are 20 sets of audio samples from 10 speakers\nwhich are combinations of 23 consonant and 6 vowel phonemes of Persian\nlanguage. In each sample, there is a combination of one vowel and one\nconsonant. First, the consonant phoneme is pronounced and just after it, the\nvowel phoneme is pronounced. Each sound sample is a frame of 2 seconds of\naudio. In every 2 seconds, there is an average of 0.5 second speech and the\nrest is silence. In this paper, the proposed method is the implementations of\nthe MFCC (Mel Frequency Cepstrum Coefficients) on every partitioned sound\nsample. Then, every train sample of MFCC vector is given to a multilayer\nperceptron feed-forward ANN (Artificial Neural Network) for training process.\nAt the end, the test samples are examined on ANN model for phoneme recognition.\nAfter training and testing process, the results are presented in recognition of\nvowels. Then, the average percent of recognition for vowel phonemes are\ncomputed.", "published": "2018-12-17 18:50:36", "link": "http://arxiv.org/abs/1812.06953v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Analogy Search Engine: Finding Analogies in Cross-Domain Research Papers", "abstract": "In recent years, with the rapid proliferation of research publications in the\nfield of Artificial Intelligence, it is becoming increasingly difficult for\nresearchers to effectively keep up with all the latest research in one's own\ndomains. However, history has shown that scientific breakthroughs often come\nfrom collaborations of researchers from different domains. Traditional search\nalgorithms like Lexical search, which look for literal matches or synonyms and\nvariants of the query words, are not effective for discovering cross-domain\nresearch papers and meeting the needs of researchers in this age of information\noverflow. In this paper, we developed and tested an innovative semantic search\nengine, Analogy Search Engine (ASE), for 2000 AI research paper abstracts\nacross domains like Language Technologies, Robotics, Machine Learning,\nComputational Biology, Human Computer Interactions, etc. ASE combines recent\ntheories and methods from Computational Analogy and Natural Language Processing\nto go beyond keyword-based lexical search and discover the deeper analogical\nrelationships among research paper abstracts. We experimentally show that ASE\nis capable of finding more interesting and useful research papers than baseline\nelasticsearch. Furthermore, we believe that the methods used in ASE go beyond\nacademic paper and will benefit many other document search tasks.", "published": "2018-12-17 16:15:13", "link": "http://arxiv.org/abs/1812.06974v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "A multi-layered energy consumption model for smart wireless acoustic\n  sensor networks", "abstract": "Smart sensing is expected to become a pervasive technology in smart cities\nand environments of the near future. These services are improving their\ncapabilities due to integrated devices shrinking in size while maintaining\ntheir computational power, which can run diverse Machine Learning algorithms\nand achieve high performance in various data-processing tasks. One attractive\nsensor modality to be used for smart sensing are acoustic sensors, which can\nconvey highly informative data while keeping a moderate energy consumption.\nUnfortunately, the energy budget of current wireless sensor networks is usually\nnot enough to support the requirements of standard microphones. Therefore,\nenergy efficiency needs to be increased at all layers --- sensing, signal\nprocessing and communication --- in order to bring wireless smart acoustic\nsensors into the market. To help to attain this goal, this paper introduces\nWASN-EM: an energy consumption model for wireless acoustic sensors networks\n(WASN), whose aim is to aid in the development of novel techniques to increase\nthe energy-efficient of smart wireless acoustic sensors. This model provides a\nfirst step of exploration prior to custom design of a smart wireless acoustic\nsensor, and also can be used to compare the energy consumption of different\nprotocols.", "published": "2018-12-17 09:59:24", "link": "http://arxiv.org/abs/1812.06672v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Circular Statistics-based low complexity DOA estimation for hearing aid\n  application", "abstract": "The proposed Circular statistics-based Inter-Microphone Phase difference\nestimation Localizer (CIMPL) method is tailored toward binaural hearing aid\nsystems with microphone arrays in each unit. The method utilizes the circular\nstatistics (circular mean and circular variance) of inter-microphone phase\ndifference (IPD) across different microphone pairs. These IPDs are firstly\nmapped to time delays through a variance-weighted linear fit, then mapped to\nazimuth direction-of-arrival (DoA) and lastly information of different\nmicrophone pairs is combined. The variance is carried through the different\ntransformations and acts as a reliability index of the estimated angle. Both\nthe resulting angle and variance are fed into a wrapped Kalman filter, which\nprovides a smoothed estimate of the DoA. The proposed method improves the\naccuracy of the tracked angle of a single moving source compared with the\nbenchmark method provided by the LOCATA challenge, and it runs approximately 75\ntimes faster.", "published": "2018-12-17 11:09:30", "link": "http://arxiv.org/abs/1812.06697v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Voiceprint recognition of Parkinson patients based on deep learning", "abstract": "More than 90% of the Parkinson Disease (PD) patients suffer from vocal\ndisorders. Speech impairment is already indicator of PD. This study focuses on\nPD diagnosis through voiceprint features. In this paper, a method based on Deep\nNeural Network (DNN) recognition and classification combined with Mini-Batch\nGradient Descent (MBGD) is proposed to distinguish PD patients from healthy\npeople using voiceprint features. In order to exact the voiceprint features\nfrom patients, Weighted Mel Frequency Cepstrum Coefficients (WMFCC) is applied.\nThe proposed method is tested on experimental data obtained by the voice\nrecordings of three sustained vowels /a/, /o/ and /u/ from participants (48 PD\nand 20 healthy people). The results show that the proposed method achieves a\nhigh accuracy of diagnosis of PD patients from healthy people, than the\nconventional methods like Support Vector Machine (SVM) and other mentioned in\nthis paper. The accuracy achieved is 89.5%. WMFCC approach can solve the\nproblem that the high-order cepstrum coefficients are small and the features\ncomponent's representation ability to the audio is weak. MBGD reduces the\ncomputational loads of the loss function, and increases the training speed of\nthe system. DNN classifier enhances the classification ability of voiceprint\nfeatures. Therefore, the above approaches can provide a solid solution for the\nquick auxiliary diagnosis of PD in early stage.", "published": "2018-12-17 04:24:46", "link": "http://arxiv.org/abs/1812.06613v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning to Generate Music with BachProp", "abstract": "As deep learning advances, algorithms of music composition increase in\nperformance. However, most of the successful models are designed for specific\nmusical structures. Here, we present BachProp, an algorithmic composer that can\ngenerate music scores in many styles given sufficient training data. To adapt\nBachProp to a broad range of musical styles, we propose a novel representation\nof music and train a deep network to predict the note transition probabilities\nof a given music corpus. In this paper, new music scores generated by BachProp\nare compared with the original corpora as well as with different network\narchitectures and other related models. We show that BachProp captures\nimportant features of the original datasets better than other models and invite\nthe reader to a qualitative comparison on a large collection of generated\nsongs.", "published": "2018-12-17 09:37:34", "link": "http://arxiv.org/abs/1812.06669v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Quaternion Convolutional Neural Networks for Detection and Localization\n  of 3D Sound Events", "abstract": "Learning from data in the quaternion domain enables us to exploit internal\ndependencies of 4D signals and treating them as a single entity. One of the\nmodels that perfectly suits with quaternion-valued data processing is\nrepresented by 3D acoustic signals in their spherical harmonics decomposition.\nIn this paper, we address the problem of localizing and detecting sound events\nin the spatial sound field by using quaternion-valued data processing. In\nparticular, we consider the spherical harmonic components of the signals\ncaptured by a first-order ambisonic microphone and process them by using a\nquaternion convolutional neural network. Experimental results show that the\nproposed approach exploits the correlated nature of the ambisonic signals, thus\nimproving accuracy results in 3D sound event detection and localization.", "published": "2018-12-17 14:49:25", "link": "http://arxiv.org/abs/1812.06811v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Instrument-Independent Dastgah Recognition of Iranian Classical Music\n  Using AzarNet", "abstract": "In this paper, AzarNet, a deep neural network (DNN), is proposed to\nrecognizing seven different Dastgahs of Iranian classical music in Maryam\nIranian classical music (MICM) dataset. Over the last years, there has been\nremarkable interest in employing feature learning and DNNs which lead to\ndecreasing the required engineering effort. DNNs have shown better performance\nin many classification tasks such as audio signal classification compares to\nshallow processing architectures. Despite image data, audio data need some\npreprocessing steps to extract spectra and temporal features. Some\ntransformations like Short-Time Fourier Transform (STFT) have been used in the\nstate of art researches to transform audio signals from time-domain to\ntime-frequency domain to extract both temporal and spectra features. In this\nresearch, the STFT output results which are extracted features are given to\nAzarNet for learning and classification processes. It is worth noting that, the\nmentioned dataset contains music tracks composed with two instruments (violin\nand straw). The overall f1 score of AzarNet on test set, for average of all\nseven classes was 86.21% which is the best result ever reported in Dastgah\nclassification according to our best knowledge.", "published": "2018-12-17 19:33:27", "link": "http://arxiv.org/abs/1812.07017v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Recognition Of Persian Phonemes Using PPNet", "abstract": "In this paper, a novel approach is proposed for the recognition of Persian\nphonemes in the Persian Consonant-Vowel Combination (PCVC) speech dataset.\nNowadays, deep neural networks play a crucial role in classification tasks.\nHowever, the best results in speech recognition are not yet as perfect as human\nrecognition rate. Deep learning techniques show outstanding performance over\nmany other classification tasks like image classification, document\nclassification, etc. Furthermore, the performance is sometimes better than a\nhuman. The reason why automatic speech recognition (ASR) systems are not as\nqualified as the human speech recognition system, mostly depends on features of\ndata which is fed to deep neural networks. Methods: In this research, firstly,\nthe sound samples are cut for the exact extraction of phoneme sounds in 50ms\nsamples. Then, phonemes are divided into 30 groups, containing 23 consonants, 6\nvowels, and a silence phoneme. Results: The short-time Fourier transform (STFT)\nis conducted on them, and the results are given to PPNet (A new deep\nconvolutional neural network architecture) classifier and a total average of\n75.87% accuracy is reached which is the best result ever compared to other\nalgorithms on separated Persian phonemes (Like in PCVC speech dataset).\nConclusion: This method can be used not only for recognizing mono-phonemes but\nalso it can be adopted as an input to the selection of the best words in speech\ntranscription.", "published": "2018-12-17 19:20:29", "link": "http://arxiv.org/abs/1812.08600v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
