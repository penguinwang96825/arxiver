{"title": "EcoVerse: An Annotated Twitter Dataset for Eco-Relevance Classification,\n  Environmental Impact Analysis, and Stance Detection", "abstract": "Anthropogenic ecological crisis constitutes a significant challenge that all\nwithin the academy must urgently face, including the Natural Language\nProcessing (NLP) community. While recent years have seen increasing work\nrevolving around climate-centric discourse, crucial environmental and\necological topics outside of climate change remain largely unaddressed, despite\ntheir prominent importance. Mainstream NLP tasks, such as sentiment analysis,\ndominate the scene, but there remains an untouched space in the literature\ninvolving the analysis of environmental impacts of certain events and\npractices. To address this gap, this paper presents EcoVerse, an annotated\nEnglish Twitter dataset of 3,023 tweets spanning a wide spectrum of\nenvironmental topics. We propose a three-level annotation scheme designed for\nEco-Relevance Classification, Stance Detection, and introducing an original\napproach for Environmental Impact Analysis. We detail the data collection,\nfiltering, and labeling process that led to the creation of the dataset.\nRemarkable Inter-Annotator Agreement indicates that the annotation scheme\nproduces consistent annotations of high quality. Subsequent classification\nexperiments using BERT-based models, including ClimateBERT, are presented.\nThese yield encouraging results, while also indicating room for a model\nspecifically tailored for environmental texts. The dataset is made freely\navailable to stimulate further research.", "published": "2024-04-08 01:21:11", "link": "http://arxiv.org/abs/2404.05133v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Supervised Gradual Machine Learning for Aspect Category Detection", "abstract": "Aspect Category Detection (ACD) aims to identify implicit and explicit\naspects in a given review sentence. The state-of-the-art approaches for ACD use\nDeep Neural Networks (DNNs) to address the problem as a multi-label\nclassification task. However, learning category-specific representations\nheavily rely on the amount of labeled examples, which may not readily available\nin real-world scenarios. In this paper, we propose a novel approach to tackle\nthe ACD task by combining DNNs with Gradual Machine Learning (GML) in a\nsupervised setting. we aim to leverage the strength of DNN in semantic relation\nmodeling, which can facilitate effective knowledge transfer between labeled and\nunlabeled instances during the gradual inference of GML. To achieve this, we\nfirst analyze the learned latent space of the DNN to model the relations, i.e.,\nsimilar or opposite, between instances. We then represent these relations as\nbinary features in a factor graph to efficiently convey knowledge. Finally, we\nconduct a comparative study of our proposed solution on real benchmark datasets\nand demonstrate that the GML approach, in collaboration with DNNs for feature\nextraction, consistently outperforms pure DNN solutions.", "published": "2024-04-08 07:21:46", "link": "http://arxiv.org/abs/2404.05245v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interpreting Themes from Educational Stories", "abstract": "Reading comprehension continues to be a crucial research focus in the NLP\ncommunity. Recent advances in Machine Reading Comprehension (MRC) have mostly\ncentered on literal comprehension, referring to the surface-level understanding\nof content. In this work, we focus on the next level - interpretive\ncomprehension, with a particular emphasis on inferring the themes of a\nnarrative text. We introduce the first dataset specifically designed for\ninterpretive comprehension of educational narratives, providing corresponding\nwell-edited theme texts. The dataset spans a variety of genres and cultural\norigins and includes human-annotated theme keywords with varying levels of\ngranularity. We further formulate NLP tasks under different abstractions of\ninterpretive comprehension toward the main idea of a story. After conducting\nextensive experiments with state-of-the-art methods, we found the task to be\nboth challenging and significant for NLP research. The dataset and source code\nhave been made publicly available to the research community at\nhttps://github.com/RiTUAL-UH/EduStory.", "published": "2024-04-08 07:26:27", "link": "http://arxiv.org/abs/2404.05250v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Task Learning for Features Extraction in Financial Annual Reports", "abstract": "For assessing various performance indicators of companies, the focus is\nshifting from strictly financial (quantitative) publicly disclosed information\nto qualitative (textual) information. This textual data can provide valuable\nweak signals, for example through stylistic features, which can complement the\nquantitative data on financial performance or on Environmental, Social and\nGovernance (ESG) criteria. In this work, we use various multi-task learning\nmethods for financial text classification with the focus on financial\nsentiment, objectivity, forward-looking sentence prediction and ESG-content\ndetection. We propose different methods to combine the information extracted\nfrom training jointly on different tasks; our best-performing method highlights\nthe positive effect of explicitly adding auxiliary task predictions as features\nfor the final target task during the multi-task training. Next, we use these\nclassifiers to extract textual features from annual reports of FTSE350\ncompanies and investigate the link between ESG quantitative scores and these\nfeatures.", "published": "2024-04-08 08:13:40", "link": "http://arxiv.org/abs/2404.05281v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PORTULAN ExtraGLUE Datasets and Models: Kick-starting a Benchmark for\n  the Neural Processing of Portuguese", "abstract": "Leveraging research on the neural modelling of Portuguese, we contribute a\ncollection of datasets for an array of language processing tasks and a\ncorresponding collection of fine-tuned neural language models on these\ndownstream tasks. To align with mainstream benchmarks in the literature,\noriginally developed in English, and to kick start their Portuguese\ncounterparts, the datasets were machine-translated from English with a\nstate-of-the-art translation engine. The resulting PORTULAN ExtraGLUE benchmark\nis a basis for research on Portuguese whose improvement can be pursued in\nfuture work. Similarly, the respective fine-tuned neural language models,\ndeveloped with a low-rank adaptation approach, are made available as baselines\nthat can stimulate future work on the neural processing of Portuguese. All\ndatasets and models have been developed and are made available for two variants\nof Portuguese: European and Brazilian.", "published": "2024-04-08 09:22:41", "link": "http://arxiv.org/abs/2404.05333v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NLP Progress in Indigenous Latin American Languages", "abstract": "The paper focuses on the marginalization of indigenous language communities\nin the face of rapid technological advancements. We highlight the cultural\nrichness of these languages and the risk they face of being overlooked in the\nrealm of Natural Language Processing (NLP). We aim to bridge the gap between\nthese communities and researchers, emphasizing the need for inclusive\ntechnological advancements that respect indigenous community perspectives. We\nshow the NLP progress of indigenous Latin American languages and the survey\nthat covers the status of indigenous languages in Latin America, their\nrepresentation in NLP, and the challenges and innovations required for their\npreservation and development. The paper contributes to the current literature\nin understanding the need and progress of NLP for indigenous communities of\nLatin America, specifically low-resource and indigenous communities in general.", "published": "2024-04-08 10:04:55", "link": "http://arxiv.org/abs/2404.05365v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Know When To Stop: A Study of Semantic Drift in Text Generation", "abstract": "In this work, we explicitly show that modern LLMs tend to generate correct\nfacts first, then \"drift away\" and generate incorrect facts later: this was\noccasionally observed but never properly measured. We develop a semantic drift\nscore that measures the degree of separation between correct and incorrect\nfacts in generated texts and confirm our hypothesis when generating\nWikipedia-style biographies. This correct-then-incorrect generation pattern\nsuggests that factual accuracy can be improved by knowing when to stop\ngeneration. Therefore, we explore the trade-off between information quantity\nand factual accuracy for several early stopping methods and manage to improve\nfactuality by a large margin. We further show that reranking with semantic\nsimilarity can further improve these results, both compared to the baseline and\nwhen combined with early stopping. Finally, we try calling external API to\nbring the model back to the right generation path, but do not get positive\nresults. Overall, our methods generalize and can be applied to any long-form\ntext generation to produce more reliable information, by balancing trade-offs\nbetween factual accuracy, information quantity and computational cost.", "published": "2024-04-08 11:25:30", "link": "http://arxiv.org/abs/2404.05411v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Models on a Diet: Cost-Efficient Development of Encoders for\n  Closely-Related Languages via Additional Pretraining", "abstract": "The world of language models is going through turbulent times, better and\never larger models are coming out at an unprecedented speed. However, we argue\nthat, especially for the scientific community, encoder models of up to 1\nbillion parameters are still very much needed, their primary usage being in\nenriching large collections of data with metadata necessary for downstream\nresearch. We investigate the best way to ensure the existence of such encoder\nmodels on the set of very closely related languages - Croatian, Serbian,\nBosnian and Montenegrin, by setting up a diverse benchmark for these languages,\nand comparing the trained-from-scratch models with the new models constructed\nvia additional pretraining of existing multilingual models. We show that\ncomparable performance to dedicated from-scratch models can be obtained by\nadditionally pretraining available multilingual models even with a limited\namount of computation. We also show that neighboring languages, in our case\nSlovenian, can be included in the additional pretraining with little to no loss\nin the performance of the final model.", "published": "2024-04-08 11:55:44", "link": "http://arxiv.org/abs/2404.05428v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XL$^2$Bench: A Benchmark for Extremely Long Context Understanding with\n  Long-range Dependencies", "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse tasks but are constrained by their small context window sizes. Various\nefforts have been proposed to expand the context window to accommodate even up\nto 200K input tokens. Meanwhile, building high-quality benchmarks with much\nlonger text lengths and more demanding tasks to provide comprehensive\nevaluations is of immense practical interest to facilitate long context\nunderstanding research of LLMs. However, prior benchmarks create datasets that\nostensibly cater to long-text comprehension by expanding the input of\ntraditional tasks, which falls short to exhibit the unique characteristics of\nlong-text understanding, including long dependency tasks and longer text length\ncompatible with modern LLMs' context window size. In this paper, we introduce a\nbenchmark for extremely long context understanding with long-range\ndependencies, XL$^2$Bench, which includes three scenarios: Fiction Reading,\nPaper Reading, and Law Reading, and four tasks of increasing complexity: Memory\nRetrieval, Detailed Understanding, Overall Understanding, and Open-ended\nGeneration, covering 27 subtasks in English and Chinese. It has an average\nlength of 100K+ words (English) and 200K+ characters (Chinese). Evaluating six\nleading LLMs on XL$^2$Bench, we find that their performance significantly lags\nbehind human levels. Moreover, the observed decline in performance across both\nthe original and enhanced datasets underscores the efficacy of our approach to\nmitigating data contamination.", "published": "2024-04-08 12:29:07", "link": "http://arxiv.org/abs/2404.05446v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RoT: Enhancing Large Language Models with Reflection on Search Trees", "abstract": "Large language models (LLMs) have demonstrated impressive capability in\nreasoning and planning when integrated with tree-search-based prompting\nmethods. However, since these methods ignore the previous search experiences,\nthey often make the same mistakes in the search process. To address this issue,\nwe introduce Reflection on search Trees (RoT), an LLM reflection framework\ndesigned to improve the performance of tree-search-based prompting methods. It\nuses a strong LLM to summarize guidelines from previous tree search experiences\nto enhance the ability of a weak LLM. The guidelines are instructions about\nsolving this task through tree search which can prevent the weak LLMs from\nmaking similar mistakes in the past search process. In addition, we proposed a\nnovel state selection method, which identifies the critical information from\nhistorical search processes to help RoT generate more specific and meaningful\nguidelines. In our extensive experiments, we find that RoT significantly\nimproves the performance of LLMs in reasoning or planning tasks with various\ntree-search-based prompting methods (e.g., BFS and MCTS). Non-tree-search-based\nprompting methods such as Chain-of-Thought (CoT) can also benefit from RoT\nguidelines since RoT can provide task-specific knowledge collected from the\nsearch experience.", "published": "2024-04-08 12:31:23", "link": "http://arxiv.org/abs/2404.05449v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chinese Sequence Labeling with Semi-Supervised Boundary-Aware Language\n  Model Pre-training", "abstract": "Chinese sequence labeling tasks are heavily reliant on accurate word boundary\ndemarcation. Although current pre-trained language models (PLMs) have achieved\nsubstantial gains on these tasks, they rarely explicitly incorporate boundary\ninformation into the modeling process. An exception to this is BABERT, which\nincorporates unsupervised statistical boundary information into Chinese BERT's\npre-training objectives. Building upon this approach, we input supervised\nhigh-quality boundary information to enhance BABERT's learning, developing a\nsemi-supervised boundary-aware PLM. To assess PLMs' ability to encode\nboundaries, we introduce a novel ``Boundary Information Metric'' that is both\nsimple and effective. This metric allows comparison of different PLMs without\ntask-specific fine-tuning. Experimental results on Chinese sequence labeling\ndatasets demonstrate that the improved BABERT variant outperforms the vanilla\nversion, not only on these tasks but also more broadly across a range of\nChinese natural language understanding tasks. Additionally, our proposed metric\noffers a convenient and accurate means of evaluating PLMs' boundary awareness.", "published": "2024-04-08 14:32:52", "link": "http://arxiv.org/abs/2404.05560v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Software-Related Information Extraction via Single-Choice\n  Question Answering with Large Language Models", "abstract": "This paper describes our participation in the Shared Task on Software\nMentions Disambiguation (SOMD), with a focus on improving relation extraction\nin scholarly texts through generative Large Language Models (LLMs) using\nsingle-choice question-answering. The methodology prioritises the use of\nin-context learning capabilities of GLMs to extract software-related entities\nand their descriptive attributes, such as distributive information. Our\napproach uses Retrieval-Augmented Generation (RAG) techniques and GLMs for\nNamed Entity Recognition (NER) and Attributive NER to identify relationships\nbetween extracted software entities, providing a structured solution for\nanalysing software citations in academic literature. The paper provides a\ndetailed description of our approach, demonstrating how using GLMs in a\nsingle-choice QA paradigm can greatly enhance IE methodologies. Our\nparticipation in the SOMD shared task highlights the importance of precise\nsoftware citation practices and showcases our system's ability to overcome the\nchallenges of disambiguating and extracting relationships between software\nmentions. This sets the groundwork for future research and development in this\nfield.", "published": "2024-04-08 15:00:36", "link": "http://arxiv.org/abs/2404.05587v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "MedExpQA: Multilingual Benchmarking of Large Language Models for Medical\n  Question Answering", "abstract": "Large Language Models (LLMs) have the potential of facilitating the\ndevelopment of Artificial Intelligence technology to assist medical experts for\ninteractive decision support, which has been demonstrated by their competitive\nperformances in Medical QA. However, while impressive, the required quality bar\nfor medical applications remains far from being achieved. Currently, LLMs\nremain challenged by outdated knowledge and by their tendency to generate\nhallucinated content. Furthermore, most benchmarks to assess medical knowledge\nlack reference gold explanations which means that it is not possible to\nevaluate the reasoning of LLMs predictions. Finally, the situation is\nparticularly grim if we consider benchmarking LLMs for languages other than\nEnglish which remains, as far as we know, a totally neglected topic. In order\nto address these shortcomings, in this paper we present MedExpQA, the first\nmultilingual benchmark based on medical exams to evaluate LLMs in Medical\nQuestion Answering. To the best of our knowledge, MedExpQA includes for the\nfirst time reference gold explanations written by medical doctors which can be\nleveraged to establish various gold-based upper-bounds for comparison with LLMs\nperformance. Comprehensive multilingual experimentation using both the gold\nreference explanations and Retrieval Augmented Generation (RAG) approaches show\nthat performance of LLMs still has large room for improvement, especially for\nlanguages other than English. Furthermore, and despite using state-of-the-art\nRAG methods, our results also demonstrate the difficulty of obtaining and\nintegrating readily available medical knowledge that may positively impact\nresults on downstream evaluations for Medical Question Answering. So far the\nbenchmark is available in four languages, but we hope that this work may\nencourage further development to other languages.", "published": "2024-04-08 15:03:57", "link": "http://arxiv.org/abs/2404.05590v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fighting crime with Transformers: Empirical analysis of address parsing\n  methods in payment data", "abstract": "In the financial industry, identifying the location of parties involved in\npayments is a major challenge in the context of various regulatory\nrequirements. For this purpose address parsing entails extracting fields such\nas street, postal code, or country from free text message attributes. While\npayment processing platforms are updating their standards with more structured\nformats such as SWIFT with ISO 20022, address parsing remains essential for a\nconsiderable volume of messages. With the emergence of Transformers and\nGenerative Large Language Models (LLM), we explore the performance of\nstate-of-the-art solutions given the constraint of processing a vast amount of\ndaily data. This paper also aims to show the need for training robust models\ncapable of dealing with real-world noisy transactional data. Our results\nsuggest that a well fine-tuned Transformer model using early-stopping\nsignificantly outperforms other approaches. Nevertheless, generative LLMs\ndemonstrate strong zero-shot performance and warrant further investigations.", "published": "2024-04-08 16:04:26", "link": "http://arxiv.org/abs/2404.05632v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Mathematical Reasoning Beyond Accuracy", "abstract": "The leaderboard of Large Language Models (LLMs) in mathematical tasks has\nbeen continuously updated. However, the majority of evaluations focus solely on\nthe final results, neglecting the quality of the intermediate steps. This\noversight can mask underlying problems, such as logical errors or unnecessary\nsteps in the reasoning process. To measure reasoning beyond final-answer\naccuracy, we introduce ReasonEval, a new methodology for evaluating the quality\nof reasoning steps. ReasonEval employs validity and redundancy to characterize\nthe reasoning quality, as well as accompanying LLMs to assess them\nautomatically. We explore different design options for the LLM-based evaluators\nand empirically demonstrate that ReasonEval, when instantiated with base models\npossessing strong mathematical knowledge and trained with high-quality labeled\ndata, consistently outperforms baseline methods in the meta-evaluation\ndatasets. We also highlight the strong generalization capabilities of\nReasonEval. By utilizing ReasonEval to evaluate LLMs specialized in math, we\nfind that an increase in final-answer accuracy does not necessarily guarantee\nan improvement in the overall quality of the reasoning steps for challenging\nmathematical problems. Additionally, we observe that ReasonEval can play a\nsignificant role in data selection. We open-source the best-performing model,\nmeta-evaluation script, and all evaluation results to facilitate future\nresearch.", "published": "2024-04-08 17:18:04", "link": "http://arxiv.org/abs/2404.05692v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\u00daFAL LatinPipe at EvaLatin 2024: Morphosyntactic Analysis of Latin", "abstract": "We present LatinPipe, the winning submission to the EvaLatin 2024 Dependency\nParsing shared task. Our system consists of a fine-tuned concatenation of base\nand large pre-trained LMs, with a dot-product attention head for parsing and\nsoftmax classification heads for morphology to jointly learn both dependency\nparsing and morphological analysis. It is trained by sampling from seven\npublicly available Latin corpora, utilizing additional harmonization of\nannotations to achieve a more unified annotation style. Before fine-tuning, we\ntrain the system for a few initial epochs with frozen weights. We also add\nadditional local relative contextualization by stacking the BiLSTM layers on\ntop of the Transformer(s). Finally, we ensemble output probability\ndistributions from seven randomly instantiated networks for the final\nsubmission. The code is available at\nhttps://github.com/ufal/evalatin2024-latinpipe.", "published": "2024-04-08 20:05:25", "link": "http://arxiv.org/abs/2404.05839v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GeniL: A Multilingual Dataset on Generalizing Language", "abstract": "Generative language models are transforming our digital ecosystem, but they\noften inherit societal biases, for instance stereotypes associating certain\nattributes with specific identity groups. While whether and how these biases\nare mitigated may depend on the specific use cases, being able to effectively\ndetect instances of stereotype perpetuation is a crucial first step. Current\nmethods to assess presence of stereotypes in generated language rely on simple\ntemplate or co-occurrence based measures, without accounting for the variety of\nsentential contexts they manifest in. We argue that understanding the\nsentential context is crucial for detecting instances of generalization. We\ndistinguish two types of generalizations: (1) language that merely mentions the\npresence of a generalization (\"people think the French are very rude\"), and (2)\nlanguage that reinforces such a generalization (\"as French they must be rude\"),\nfrom non-generalizing context (\"My French friends think I am rude\"). For\nmeaningful stereotype evaluations, we need to reliably distinguish such\ninstances of generalizations. We introduce the new task of detecting\ngeneralization in language, and build GeniL, a multilingual dataset of over 50K\nsentences from 9 languages (English, Arabic, Bengali, Spanish, French, Hindi,\nIndonesian, Malay, and Portuguese) annotated for instances of generalizations.\nWe demonstrate that the likelihood of a co-occurrence being an instance of\ngeneralization is usually low, and varies across different languages, identity\ngroups, and attributes. We build classifiers to detect generalization in\nlanguage with an overall PR-AUC of 58.7, with varying degrees of performance\nacross languages. Our research provides data and tools to enable a nuanced\nunderstanding of stereotype perpetuation, a crucial step towards more inclusive\nand responsible language technologies.", "published": "2024-04-08 20:58:06", "link": "http://arxiv.org/abs/2404.05866v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Eraser: Jailbreaking Defense in Large Language Models via Unlearning\n  Harmful Knowledge", "abstract": "Jailbreaking attacks can enable Large Language Models (LLMs) to bypass the\nsafeguard and generate harmful content. Existing jailbreaking defense methods\nhave failed to address the fundamental issue that harmful knowledge resides\nwithin the model, leading to potential jailbreak risks for LLMs. In this paper,\nwe propose a novel defense method called Eraser, which mainly includes three\ngoals: unlearning harmful knowledge, retaining general knowledge, and\nmaintaining safety alignment. The intuition is that if an LLM forgets the\nspecific knowledge required to answer a harmful question, it will no longer\nhave the ability to answer harmful questions. The training of Erase does not\nactually require the model's own harmful knowledge, and it can benefit from\nunlearning general answers related to harmful queries, which means it does not\nneed assistance from the red team. The experimental results show that Eraser\ncan significantly reduce the jailbreaking success rate for various attacks\nwithout compromising the general capabilities of the model. Our codes are\navailable at https://github.com/ZeroNLP/Eraser.", "published": "2024-04-08 21:26:22", "link": "http://arxiv.org/abs/2404.05880v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Hallucinations Leaderboard -- An Open Effort to Measure\n  Hallucinations in Large Language Models", "abstract": "Large Language Models (LLMs) have transformed the Natural Language Processing\n(NLP) landscape with their remarkable ability to understand and generate\nhuman-like text. However, these models are prone to ``hallucinations'' --\noutputs that do not align with factual reality or the input context. This paper\nintroduces the Hallucinations Leaderboard, an open initiative to quantitatively\nmeasure and compare the tendency of each model to produce hallucinations. The\nleaderboard uses a comprehensive set of benchmarks focusing on different\naspects of hallucinations, such as factuality and faithfulness, across various\ntasks, including question-answering, summarisation, and reading comprehension.\nOur analysis provides insights into the performance of different models,\nguiding researchers and practitioners in choosing the most reliable models for\ntheir applications.", "published": "2024-04-08 23:16:22", "link": "http://arxiv.org/abs/2404.05904v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EFSA: Towards Event-Level Financial Sentiment Analysis", "abstract": "In this paper, we extend financial sentiment analysis~(FSA) to event-level\nsince events usually serve as the subject of the sentiment in financial text.\nThough extracting events from the financial text may be conducive to accurate\nsentiment predictions, it has specialized challenges due to the lengthy and\ndiscontinuity of events in a financial text. To this end, we reconceptualize\nthe event extraction as a classification task by designing a categorization\ncomprising coarse-grained and fine-grained event categories. Under this\nsetting, we formulate the \\textbf{E}vent-Level \\textbf{F}inancial\n\\textbf{S}entiment \\textbf{A}nalysis~(\\textbf{EFSA} for short) task that\noutputs quintuples consisting of (company, industry, coarse-grained event,\nfine-grained event, sentiment) from financial text. A large-scale Chinese\ndataset containing $12,160$ news articles and $13,725$ quintuples is publicized\nas a brand new testbed for our task. A four-hop Chain-of-Thought LLM-based\napproach is devised for this task. Systematically investigations are conducted\non our dataset, and the empirical results demonstrate the benchmarking scores\nof existing methods and our proposed method can reach the current\nstate-of-the-art. Our dataset and framework implementation are available at\nhttps://anonymous.4open.science/r/EFSA-645E", "published": "2024-04-08 07:36:26", "link": "http://arxiv.org/abs/2404.08681v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistic Changes in Spontaneous Speech for Detecting Parkinsons\n  Disease Using Large Language Models", "abstract": "Parkinsons disease is the second most prevalent neurodegenerative disorder\nwith over ten million active cases worldwide and one million new diagnoses per\nyear. Detecting and subsequently diagnosing the disease is challenging because\nof symptom heterogeneity with respect to complexity, as well as the type and\ntiming of phenotypic manifestations. Typically, language impairment can present\nin the prodromal phase and precede motor symptoms suggesting that a\nlinguistic-based approach could serve as a diagnostic method for incipient\nParkinsons disease. Additionally, improved linguistic models may enhance other\napproaches through ensemble techniques. The field of large language models is\nadvancing rapidly, presenting the opportunity to explore the use of these new\nmodels for detecting Parkinsons disease and to improve on current linguistic\napproaches with high-dimensional representations of linguistics. We evaluate\nthe application of state-of-the-art large language models to detect Parkinsons\ndisease automatically from spontaneous speech with up to 73% accuracy.", "published": "2024-04-08 03:00:10", "link": "http://arxiv.org/abs/2404.05160v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step\n  Reasoning with Large Language Models", "abstract": "Generating accurate step-by-step reasoning is essential for Large Language\nModels (LLMs) to address complex problems and enhance robustness and\ninterpretability. Despite the flux of research on developing advanced reasoning\napproaches, systematically analyzing the diverse LLMs and reasoning strategies\nin generating reasoning chains remains a significant challenge. The\ndifficulties stem from the lack of two key elements: (1) an automatic method\nfor evaluating the generated reasoning chains on different tasks, and (2) a\nunified formalism and implementation of the diverse reasoning approaches for\nsystematic comparison. This paper aims to close the gap: (1) We introduce\nAutoRace for fully automated reasoning chain evaluation. Existing metrics rely\non expensive human annotations or pre-defined LLM prompts not adaptable to\ndifferent tasks. In contrast, AutoRace automatically creates detailed\nevaluation criteria tailored for each task, and uses GPT-4 for accurate\nevaluation following the criteria. (2) We develop LLM Reasoners, a library for\nstandardized modular implementation of existing and new reasoning algorithms,\nunder a unified formulation of the search, reward, and world model components.\nWith the new evaluation and library, (3) we conduct extensive study of\ndifferent reasoning approaches (e.g., CoT, ToT, RAP). The analysis reveals\ninteresting findings about different factors contributing to reasoning,\nincluding the reward-guidance, breadth-vs-depth in search, world model, and\nprompt formats, etc.", "published": "2024-04-08 06:35:09", "link": "http://arxiv.org/abs/2404.05221v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LayoutLLM: Layout Instruction Tuning with Large Language Models for\n  Document Understanding", "abstract": "Recently, leveraging large language models (LLMs) or multimodal large\nlanguage models (MLLMs) for document understanding has been proven very\npromising. However, previous works that employ LLMs/MLLMs for document\nunderstanding have not fully explored and utilized the document layout\ninformation, which is vital for precise document understanding. In this paper,\nwe propose LayoutLLM, an LLM/MLLM based method for document understanding. The\ncore of LayoutLLM is a layout instruction tuning strategy, which is specially\ndesigned to enhance the comprehension and utilization of document layouts. The\nproposed layout instruction tuning strategy consists of two components:\nLayout-aware Pre-training and Layout-aware Supervised Fine-tuning. To capture\nthe characteristics of document layout in Layout-aware Pre-training, three\ngroups of pre-training tasks, corresponding to document-level, region-level and\nsegment-level information, are introduced. Furthermore, a novel module called\nlayout chain-of-thought (LayoutCoT) is devised to enable LayoutLLM to focus on\nregions relevant to the question and generate accurate answers. LayoutCoT is\neffective for boosting the performance of document understanding. Meanwhile, it\nbrings a certain degree of interpretability, which could facilitate manual\ninspection and correction. Experiments on standard benchmarks show that the\nproposed LayoutLLM significantly outperforms existing methods that adopt\nopen-source 7B LLMs/MLLMs for document understanding. The training data of the\nLayoutLLM is publicly available at\nhttps://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/DocumentUnderstanding/LayoutLLM", "published": "2024-04-08 06:40:28", "link": "http://arxiv.org/abs/2404.05225v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Product Description and QA Assisted Self-Supervised Opinion\n  Summarization", "abstract": "In e-commerce, opinion summarization is the process of summarizing the\nconsensus opinions found in product reviews. However, the potential of\nadditional sources such as product description and question-answers (QA) has\nbeen considered less often. Moreover, the absence of any supervised training\ndata makes this task challenging. To address this, we propose a novel synthetic\ndataset creation (SDC) strategy that leverages information from reviews as well\nas additional sources for selecting one of the reviews as a pseudo-summary to\nenable supervised training. Our Multi-Encoder Decoder framework for Opinion\nSummarization (MEDOS) employs a separate encoder for each source, enabling\neffective selection of information while generating the summary. For\nevaluation, due to the unavailability of test sets with additional sources, we\nextend the Amazon, Oposum+, and Flipkart test sets and leverage ChatGPT to\nannotate summaries. Experiments across nine test sets demonstrate that the\ncombination of our SDC approach and MEDOS model achieves on average a 14.5%\nimprovement in ROUGE-1 F1 over the SOTA. Moreover, comparative analysis\nunderlines the significance of incorporating additional sources for generating\nmore informative summaries. Human evaluations further indicate that MEDOS\nscores relatively higher in coherence and fluency with 0.41 and 0.5 (-1 to 1)\nrespectively, compared to existing models. To the best of our knowledge, we are\nthe first to generate opinion summaries leveraging additional sources in a\nself-supervised setting.", "published": "2024-04-08 07:15:06", "link": "http://arxiv.org/abs/2404.05243v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Objectively Benchmarking Social Intelligence for Language Agents\n  at Action Level", "abstract": "Prominent large language models have exhibited human-level performance in\nmany domains, even enabling the derived agents to simulate human and social\ninteractions. While practical works have substantiated the practicability of\ngrounding language agents in sandbox simulation or embodied simulators, current\nsocial intelligence benchmarks either stay at the language level or use\nsubjective metrics. In pursuit of a more realistic and objective evaluation, we\nintroduce the Social Tasks in Sandbox Simulation (STSS) benchmark, which\nassesses language agents \\textbf{objectively} at the \\textbf{action level} by\nscrutinizing the goal achievements within the multi-agent simulation.\nAdditionally, we sample conversation scenarios to build a language-level\nbenchmark to provide an economically prudent preliminary evaluation and align\nwith prevailing benchmarks. To gauge the significance of agent architecture, we\nimplement a target-driven planning (TDP) module as an adjunct to the existing\nagent. Our evaluative findings highlight that the STSS benchmark is challenging\nfor state-of-the-art language agents. Furthermore, it effectively discriminates\nbetween distinct language agents, suggesting its usefulness as a benchmark for\nevaluating both language models and agent architectures.", "published": "2024-04-08 09:25:32", "link": "http://arxiv.org/abs/2404.05337v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SafetyPrompts: a Systematic Review of Open Datasets for Evaluating and\n  Improving Large Language Model Safety", "abstract": "The last two years have seen a rapid growth in concerns around the safety of\nlarge language models (LLMs). Researchers and practitioners have met these\nconcerns by creating an abundance of datasets for evaluating and improving LLM\nsafety. However, much of this work has happened in parallel, and with very\ndifferent goals in mind, ranging from the mitigation of near-term risks around\nbias and toxic content generation to the assessment of longer-term catastrophic\nrisk potential. This makes it difficult for researchers and practitioners to\nfind the most relevant datasets for their use case, and to identify gaps in\ndataset coverage that future work may fill. To remedy these issues, we conduct\na first systematic review of open datasets for evaluating and improving LLM\nsafety. We review 144 datasets, which we identified through an iterative and\ncommunity-driven process over the course of several months. We highlight\npatterns and trends, such as a trend towards fully synthetic datasets, as well\nas gaps in dataset coverage, such as a clear lack of non-English and\nnaturalistic datasets. We also examine how LLM safety datasets are used in\npractice -- in LLM release publications and popular LLM benchmarks -- finding\nthat current evaluation practices are highly idiosyncratic and make use of only\na small fraction of available datasets. Our contributions are based on\nSafetyPrompts.com, a living catalogue of open datasets for LLM safety, which we\nplan to update continuously as the field of LLM safety develops.", "published": "2024-04-08 10:57:25", "link": "http://arxiv.org/abs/2404.05399v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PerkwE_COQA: Enhanced Persian Conversational Question Answering by\n  combining contextual keyword extraction with Large Language Models", "abstract": "Smart cities need the involvement of their residents to enhance quality of\nlife. Conversational query-answering is an emerging approach for user\nengagement. There is an increasing demand of an advanced conversational\nquestion-answering that goes beyond classic systems. Existing approaches have\nshown that LLMs offer promising capabilities for CQA, but may struggle to\ncapture the nuances of conversational contexts. The new approach involves\nunderstanding the content and engaging in a multi-step conversation with the\nuser to fulfill their needs. This paper presents a novel method to elevate the\nperformance of Persian Conversational question-answering (CQA) systems. It\ncombines the strengths of Large Language Models (LLMs) with contextual keyword\nextraction. Our method extracts keywords specific to the conversational flow,\nproviding the LLM with additional context to understand the user's intent and\ngenerate more relevant and coherent responses. We evaluated the effectiveness\nof this combined approach through various metrics, demonstrating significant\nimprovements in CQA performance compared to an LLM-only baseline. The proposed\nmethod effectively handles implicit questions, delivers contextually relevant\nanswers, and tackles complex questions that rely heavily on conversational\ncontext. The findings indicate that our method outperformed the evaluation\nbenchmarks up to 8% higher than existing methods and the LLM-only baseline.", "published": "2024-04-08 11:14:58", "link": "http://arxiv.org/abs/2404.05406v2", "categories": ["cs.CL", "cs.AI", "68T50, 68T07", "J.3; I.2.7; I.2.1"], "primary_category": "cs.CL"}
{"title": "Relation Extraction Using Large Language Models: A Case Study on\n  Acupuncture Point Locations", "abstract": "In acupuncture therapy, the accurate location of acupoints is essential for\nits effectiveness. The advanced language understanding capabilities of large\nlanguage models (LLMs) like Generative Pre-trained Transformers (GPT) present a\nsignificant opportunity for extracting relations related to acupoint locations\nfrom textual knowledge sources. This study aims to compare the performance of\nGPT with traditional deep learning models (Long Short-Term Memory (LSTM) and\nBidirectional Encoder Representations from Transformers for Biomedical Text\nMining (BioBERT)) in extracting acupoint-related location relations and assess\nthe impact of pretraining and fine-tuning on GPT's performance. We utilized the\nWorld Health Organization Standard Acupuncture Point Locations in the Western\nPacific Region (WHO Standard) as our corpus, which consists of descriptions of\n361 acupoints. Five types of relations ('direction_of,' 'distance_of,'\n'part_of,' 'near_acupoint,' and 'located_near') (n= 3,174) between acupoints\nwere annotated. Five models were compared: BioBERT, LSTM, pre-trained GPT-3.5,\nfine-tuned GPT-3.5, as well as pre-trained GPT-4. Performance metrics included\nmicro-average exact match precision, recall, and F1 scores. Our results\ndemonstrate that fine-tuned GPT-3.5 consistently outperformed other models in\nF1 scores across all relation types. Overall, it achieved the highest\nmicro-average F1 score of 0.92. This study underscores the effectiveness of\nLLMs like GPT in extracting relations related to acupoint locations, with\nimplications for accurately modeling acupuncture knowledge and promoting\nstandard implementation in acupuncture training and practice. The findings also\ncontribute to advancing informatics applications in traditional and\ncomplementary medicine, showcasing the potential of LLMs in natural language\nprocessing.", "published": "2024-04-08 11:33:00", "link": "http://arxiv.org/abs/2404.05415v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PetKaz at SemEval-2024 Task 8: Can Linguistics Capture the Specifics of\n  LLM-generated Text?", "abstract": "In this paper, we present our submission to the SemEval-2024 Task 8\n\"Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text\nDetection\", focusing on the detection of machine-generated texts (MGTs) in\nEnglish. Specifically, our approach relies on combining embeddings from the\nRoBERTa-base with diversity features and uses a resampled training set. We\nscore 12th from 124 in the ranking for Subtask A (monolingual track), and our\nresults show that our approach is generalizable across unseen models and\ndomains, achieving an accuracy of 0.91.", "published": "2024-04-08 13:05:02", "link": "http://arxiv.org/abs/2404.05483v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "PetKaz at SemEval-2024 Task 3: Advancing Emotion Classification with an\n  LLM for Emotion-Cause Pair Extraction in Conversations", "abstract": "In this paper, we present our submission to the SemEval-2023 Task~3 \"The\nCompetition of Multimodal Emotion Cause Analysis in Conversations\", focusing on\nextracting emotion-cause pairs from dialogs. Specifically, our approach relies\non combining fine-tuned GPT-3.5 for emotion classification and a BiLSTM-based\nneural network to detect causes. We score 2nd in the ranking for Subtask 1,\ndemonstrating the effectiveness of our approach through one of the highest\nweighted-average proportional F1 scores recorded at 0.264.", "published": "2024-04-08 13:25:03", "link": "http://arxiv.org/abs/2404.05502v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "AnchorAL: Computationally Efficient Active Learning for Large and\n  Imbalanced Datasets", "abstract": "Active learning for imbalanced classification tasks is challenging as the\nminority classes naturally occur rarely. Gathering a large pool of unlabelled\ndata is thus essential to capture minority instances. Standard pool-based\nactive learning is computationally expensive on large pools and often reaches\nlow accuracy by overfitting the initial decision boundary, thus failing to\nexplore the input space and find minority instances. To address these issues we\npropose AnchorAL. At each iteration, AnchorAL chooses class-specific instances\nfrom the labelled set, or anchors, and retrieves the most similar unlabelled\ninstances from the pool. This resulting subpool is then used for active\nlearning. Using a small, fixed-sized subpool AnchorAL allows scaling any active\nlearning strategy to large pools. By dynamically selecting different anchors at\neach iteration it promotes class balance and prevents overfitting the initial\ndecision boundary, thus promoting the discovery of new clusters of minority\ninstances. In experiments across different classification tasks, active\nlearning strategies, and model architectures AnchorAL is (i) faster, often\nreducing runtime from hours to minutes, (ii) trains more performant models,\n(iii) and returns more balanced datasets than competing methods.", "published": "2024-04-08 15:53:46", "link": "http://arxiv.org/abs/2404.05623v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LTNER: Large Language Model Tagging for Named Entity Recognition with\n  Contextualized Entity Marking", "abstract": "The use of LLMs for natural language processing has become a popular trend in\nthe past two years, driven by their formidable capacity for context\ncomprehension and learning, which has inspired a wave of research from\nacademics and industry professionals. However, for certain NLP tasks, such as\nNER, the performance of LLMs still falls short when compared to supervised\nlearning methods. In our research, we developed a NER processing framework\ncalled LTNER that incorporates a revolutionary Contextualized Entity Marking\nGen Method. By leveraging the cost-effective GPT-3.5 coupled with context\nlearning that does not require additional training, we significantly improved\nthe accuracy of LLMs in handling NER tasks. The F1 score on the CoNLL03 dataset\nincreased from the initial 85.9% to 91.9%, approaching the performance of\nsupervised fine-tuning. This outcome has led to a deeper understanding of the\npotential of LLMs.", "published": "2024-04-08 15:54:02", "link": "http://arxiv.org/abs/2404.05624v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language-Independent Representations Improve Zero-Shot Summarization", "abstract": "Finetuning pretrained models on downstream generation tasks often leads to\ncatastrophic forgetting in zero-shot conditions. In this work, we focus on\nsummarization and tackle the problem through the lens of language-independent\nrepresentations. After training on monolingual summarization, we perform\nzero-shot transfer to new languages or language pairs. We first show naively\nfinetuned models are highly language-specific in both output behavior and\ninternal representations, resulting in poor zero-shot performance. Next, we\npropose query-key (QK) finetuning to decouple task-specific knowledge from the\npretrained language generation abilities. Then, after showing downsides of the\nstandard adversarial language classifier, we propose a balanced variant that\nmore directly enforces language-agnostic representations. Moreover, our\nqualitative analyses show removing source language identity correlates to\nzero-shot summarization performance. Our code is openly available.", "published": "2024-04-08 17:56:43", "link": "http://arxiv.org/abs/2404.05720v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Softmax Attention with Constant Cost per Token", "abstract": "We propose a simple modification to the conventional attention mechanism\napplied by Transformers: Instead of quantifying pairwise query-key similarity\nwith scaled dot-products, we quantify it with the logarithms of scaled\ndot-products of exponentials. Our modification linearizes attention with\nexponential kernel feature maps, whose corresponding feature function is\ninfinite dimensional. We show that our modification is expressible as a\ncomposition of log-sums of exponentials, with a latent space of constant size,\nenabling application with constant time and space complexity per token. We\nimplement our modification, verify that it works in practice, and conclude that\nit is a promising alternative to conventional attention.", "published": "2024-04-08 20:14:10", "link": "http://arxiv.org/abs/2404.05843v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence", "abstract": "We present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon\nthe RWKV (RWKV-4) architecture. Our architectural design advancements include\nmulti-headed matrix-valued states and a dynamic recurrence mechanism that\nimprove expressivity while maintaining the inference efficiency characteristics\nof RNNs. We introduce a new multilingual corpus with 1.12 trillion tokens and a\nfast tokenizer based on greedy matching for enhanced multilinguality. We\ntrained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two\nFinch models with 1.6 and 3.1 billion parameters and find that they achieve\ncompetitive performance across a wide variety of benchmarks. We release all our\nmodels on HuggingFace under the Apache 2.0 license. Models at:\nhttps://huggingface.co/RWKV Training code at: https://github.com/RWKV/RWKV-LM\nInference code at: https://github.com/RWKV/ChatRWKV Time-parallel training code\nat: https://github.com/RWKV/RWKV-infctx-trainer", "published": "2024-04-08 22:20:59", "link": "http://arxiv.org/abs/2404.05892v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WILBUR: Adaptive In-Context Learning for Robust and Accurate Web Agents", "abstract": "In the realm of web agent research, achieving both generalization and\naccuracy remains a challenging problem. Due to high variance in website\nstructure, existing approaches often fail. Moreover, existing fine-tuning and\nin-context learning techniques fail to generalize across multiple websites. We\nintroduce Wilbur, an approach that uses a differentiable ranking model and a\nnovel instruction synthesis technique to optimally populate a black-box large\nlanguage model's prompt with task demonstrations from previous runs. To\nmaximize end-to-end success rates, we also propose an intelligent backtracking\nmechanism that learns and recovers from its mistakes. Finally, we show that our\nranking model can be trained on data from a generative auto-curriculum which\nsamples representative goals from an LLM, runs the agent, and automatically\nevaluates it, with no manual annotation. Wilbur achieves state-of-the-art\nresults on the WebVoyager benchmark, beating text-only models by 8% overall,\nand up to 36% on certain websites. On the same benchmark, Wilbur is within 5%\nof a strong multi-modal model despite only receiving textual inputs, and\nfurther analysis reveals a substantial number of failures are due to\nengineering challenges of operating the web.", "published": "2024-04-08 23:10:47", "link": "http://arxiv.org/abs/2404.05902v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Text clustering applied to data augmentation in legal contexts", "abstract": "Data analysis and machine learning are of preeminent importance in the legal\ndomain, especially in tasks like clustering and text classification. In this\nstudy, we harnessed the power of natural language processing tools to enhance\ndatasets meticulously curated by experts. This process significantly improved\nthe classification workflow for legal texts using machine learning techniques.\nWe considered the Sustainable Development Goals (SDGs) data from the United\nNations 2030 Agenda as a practical case study. Data augmentation\nclustering-based strategy led to remarkable enhancements in the accuracy and\nsensitivity metrics of classification models. For certain SDGs within the 2030\nAgenda, we observed performance gains of over 15%. In some cases, the example\nbase expanded by a noteworthy factor of 5. When dealing with unclassified legal\ntexts, data augmentation strategies centered around clustering prove to be\nhighly effective. They provide a valuable means to expand the existing\nknowledge base without the need for labor-intensive manual classification\nefforts.", "published": "2024-04-08 16:18:33", "link": "http://arxiv.org/abs/2404.08683v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Sequence-to-Sequence Modeling with Attention by Leveraging Deep\n  Learning Architectures for Enhanced Contextual Understanding in Abstractive\n  Text Summarization", "abstract": "Automatic text summarization (TS) plays a pivotal role in condensing large\nvolumes of information into concise, coherent summaries, facilitating efficient\ninformation retrieval and comprehension. This paper presents a novel framework\nfor abstractive TS of single documents, which integrates three dominant\naspects: structural, semantic, and neural-based approaches. The proposed\nframework merges machine learning and knowledge-based techniques to achieve a\nunified methodology. The framework consists of three main phases:\npre-processing, machine learning, and post-processing. In the pre-processing\nphase, a knowledge-based Word Sense Disambiguation (WSD) technique is employed\nto generalize ambiguous words, enhancing content generalization. Semantic\ncontent generalization is then performed to address out-of-vocabulary (OOV) or\nrare words, ensuring comprehensive coverage of the input document.\nSubsequently, the generalized text is transformed into a continuous vector\nspace using neural language processing techniques. A deep sequence-to-sequence\n(seq2seq) model with an attention mechanism is employed to predict a\ngeneralized summary based on the vector representation. In the post-processing\nphase, heuristic algorithms and text similarity metrics are utilized to refine\nthe generated summary further. Concepts from the generalized summary are\nmatched with specific entities, enhancing coherence and readability.\nExperimental evaluations conducted on prominent datasets, including Gigaword,\nDuc 2004, and CNN/DailyMail, demonstrate the effectiveness of the proposed\nframework. Results indicate significant improvements in handling rare and OOV\nwords, outperforming existing state-of-the-art deep learning techniques. The\nproposed framework presents a comprehensive and unified approach towards\nabstractive TS, combining the strengths of structure, semantics, and\nneural-based methodologies.", "published": "2024-04-08 18:33:59", "link": "http://arxiv.org/abs/2404.08685v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Plug and Play with Prompts: A Prompt Tuning Approach for Controlling\n  Text Generation", "abstract": "Transformer-based Large Language Models (LLMs) have shown exceptional\nlanguage generation capabilities in response to text-based prompts. However,\ncontrolling the direction of generation via textual prompts has been\nchallenging, especially with smaller models. In this work, we explore the use\nof Prompt Tuning to achieve controlled language generation. Generated text is\nsteered using prompt embeddings, which are trained using a small language\nmodel, used as a discriminator. Moreover, we demonstrate that these prompt\nembeddings can be trained with a very small dataset, with as low as a few\nhundred training examples. Our method thus offers a data and parameter\nefficient solution towards controlling language model outputs. We carry out\nextensive evaluation on four datasets: SST-5 and Yelp (sentiment analysis),\nGYAFC (formality) and JIGSAW (toxic language). Finally, we demonstrate the\nefficacy of our method towards mitigating harmful, toxic, and biased text\ngenerated by language models.", "published": "2024-04-08 01:54:28", "link": "http://arxiv.org/abs/2404.05143v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Clinical Efficiency through LLM: Discharge Note Generation for\n  Cardiac Patients", "abstract": "Medical documentation, including discharge notes, is crucial for ensuring\npatient care quality, continuity, and effective medical communication. However,\nthe manual creation of these documents is not only time-consuming but also\nprone to inconsistencies and potential errors. The automation of this\ndocumentation process using artificial intelligence (AI) represents a promising\narea of innovation in healthcare. This study directly addresses the\ninefficiencies and inaccuracies in creating discharge notes manually,\nparticularly for cardiac patients, by employing AI techniques, specifically\nlarge language model (LLM). Utilizing a substantial dataset from a cardiology\ncenter, encompassing wide-ranging medical records and physician assessments,\nour research evaluates the capability of LLM to enhance the documentation\nprocess. Among the various models assessed, Mistral-7B distinguished itself by\naccurately generating discharge notes that significantly improve both\ndocumentation efficiency and the continuity of care for patients. These notes\nunderwent rigorous qualitative evaluation by medical expert, receiving high\nmarks for their clinical relevance, completeness, readability, and contribution\nto informed decision-making and care planning. Coupled with quantitative\nanalyses, these results confirm Mistral-7B's efficacy in distilling complex\nmedical information into concise, coherent summaries. Overall, our findings\nilluminate the considerable promise of specialized LLM, such as Mistral-7B, in\nrefining healthcare documentation workflows and advancing patient care. This\nstudy lays the groundwork for further integrating advanced AI technologies in\nhealthcare, demonstrating their potential to revolutionize patient\ndocumentation and support better care outcomes.", "published": "2024-04-08 01:55:28", "link": "http://arxiv.org/abs/2404.05144v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semantic Stealth: Adversarial Text Attacks on NLP Using Several Methods", "abstract": "In various real-world applications such as machine translation, sentiment\nanalysis, and question answering, a pivotal role is played by NLP models,\nfacilitating efficient communication and decision-making processes in domains\nranging from healthcare to finance. However, a significant challenge is posed\nto the robustness of these natural language processing models by text\nadversarial attacks. These attacks involve the deliberate manipulation of input\ntext to mislead the predictions of the model while maintaining human\ninterpretability. Despite the remarkable performance achieved by\nstate-of-the-art models like BERT in various natural language processing tasks,\nthey are found to remain vulnerable to adversarial perturbations in the input\ntext. In addressing the vulnerability of text classifiers to adversarial\nattacks, three distinct attack mechanisms are explored in this paper using the\nvictim model BERT: BERT-on-BERT attack, PWWS attack, and Fraud Bargain's Attack\n(FBA). Leveraging the IMDB, AG News, and SST2 datasets, a thorough comparative\nanalysis is conducted to assess the effectiveness of these attacks on the BERT\nclassifier model. It is revealed by the analysis that PWWS emerges as the most\npotent adversary, consistently outperforming other methods across multiple\nevaluation scenarios, thereby emphasizing its efficacy in generating\nadversarial examples for text classification. Through comprehensive\nexperimentation, the performance of these attacks is assessed and the findings\nindicate that the PWWS attack outperforms others, demonstrating lower runtime,\nhigher accuracy, and favorable semantic similarity scores. The key insight of\nthis paper lies in the assessment of the relative performances of three\nprevalent state-of-the-art attack mechanisms.", "published": "2024-04-08 02:55:01", "link": "http://arxiv.org/abs/2404.05159v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DLoRA: Distributed Parameter-Efficient Fine-Tuning Solution for Large\n  Language Model", "abstract": "To enhance the performance of large language models (LLM) on downstream\ntasks, one solution is to fine-tune certain LLM parameters and make it better\nalign with the characteristics of the training dataset. This process is\ncommonly known as parameter-efficient fine-tuning (PEFT). Due to the scale of\nLLM, PEFT operations are usually executed in the public environment (e.g.,\ncloud server). This necessitates the sharing of sensitive user data across\npublic environments, thereby raising potential privacy concerns. To tackle\nthese challenges, we propose a distributed PEFT framework called DLoRA. DLoRA\nenables scalable PEFT operations to be performed collaboratively between the\ncloud and user devices. Coupled with the proposed Kill and Revive algorithm,\nthe evaluation results demonstrate that DLoRA can significantly reduce the\ncomputation and communication workload over the user devices while achieving\nsuperior accuracy and privacy protection.", "published": "2024-04-08 04:14:02", "link": "http://arxiv.org/abs/2404.05182v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC"], "primary_category": "cs.LG"}
{"title": "Have You Merged My Model? On The Robustness of Large Language Model IP\n  Protection Methods Against Model Merging", "abstract": "Model merging is a promising lightweight model empowerment technique that\ndoes not rely on expensive computing devices (e.g., GPUs) or require the\ncollection of specific training data. Instead, it involves editing different\nupstream model parameters to absorb their downstream task capabilities.\nHowever, uncertified model merging can infringe upon the Intellectual Property\n(IP) rights of the original upstream models. In this paper, we conduct the\nfirst study on the robustness of IP protection methods under model merging\nscenarios. Specifically, we investigate two state-of-the-art IP protection\ntechniques: Quantization Watermarking and Instructional Fingerprint, along with\nvarious advanced model merging technologies, such as Task Arithmetic,\nTIES-MERGING, and so on. Experimental results indicate that current Large\nLanguage Model (LLM) watermarking techniques cannot survive in the merged\nmodels, whereas model fingerprinting techniques can. Our research aims to\nhighlight that model merging should be an indispensable consideration in the\nrobustness assessment of model IP protection techniques, thereby promoting the\nhealthy development of the open-source LLM community. Our code is available at\nhttps://github.com/ThuCCSLab/MergeGuard.", "published": "2024-04-08 04:30:33", "link": "http://arxiv.org/abs/2404.05188v2", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Bidirectional Long-Range Parser for Sequential Data Understanding", "abstract": "The transformer is a powerful data modelling framework responsible for\nremarkable performance on a wide range of tasks. However, they are limited in\nterms of scalability as it is suboptimal and inefficient to process\nlong-sequence data. To this purpose we introduce BLRP (Bidirectional Long-Range\nParser), a novel and versatile attention mechanism designed to increase\nperformance and efficiency on long-sequence tasks. It leverages short and long\nrange heuristics in the form of a local sliding window approach combined with a\nglobal bidirectional latent space synthesis technique. We show the benefits and\nversatility of our approach on vision and language domains by demonstrating\ncompetitive results against state-of-the-art methods on the Long-Range-Arena\nand CIFAR benchmarks together with ablations demonstrating the computational\nefficiency.", "published": "2024-04-08 05:45:03", "link": "http://arxiv.org/abs/2404.05210v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws", "abstract": "Scaling laws describe the relationship between the size of language models\nand their capabilities. Unlike prior studies that evaluate a model's capability\nvia loss or benchmarks, we estimate the number of knowledge bits a model\nstores. We focus on factual knowledge represented as tuples, such as (USA,\ncapital, Washington D.C.) from a Wikipedia page. Through multiple controlled\ndatasets, we establish that language models can and only can store 2 bits of\nknowledge per parameter, even when quantized to int8, and such knowledge can be\nflexibly extracted for downstream applications. Consequently, a 7B model can\nstore 14B bits of knowledge, surpassing the English Wikipedia and textbooks\ncombined based on our estimation.\n  More broadly, we present 12 results on how (1) training duration, (2) model\narchitecture, (3) quantization, (4) sparsity constraints such as MoE, and (5)\ndata signal-to-noise ratio affect a model's knowledge storage capacity. Notable\ninsights include:\n  * The GPT-2 architecture, with rotary embedding, matches or even surpasses\nLLaMA/Mistral architectures in knowledge storage, particularly over shorter\ntraining durations. This arises because LLaMA/Mistral uses GatedMLP, which is\nless stable and harder to train.\n  * Prepending training data with domain names (e.g., wikipedia.org)\nsignificantly increases a model's knowledge capacity. Language models can\nautonomously identify and prioritize domains rich in knowledge, optimizing\ntheir storage capacity.", "published": "2024-04-08 11:11:31", "link": "http://arxiv.org/abs/2404.05405v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Synergy of Large Language Model and Model Driven Engineering for\n  Automated Development of Centralized Vehicular Systems", "abstract": "We present a prototype of a tool leveraging the synergy of model driven\nengineering (MDE) and Large Language Models (LLM) for the purpose of software\ndevelopment process automation in the automotive industry. In this approach,\nthe user-provided input is free form textual requirements, which are first\ntranslated to Ecore model instance representation using an LLM, which is\nafterwards checked for consistency using Object Constraint Language (OCL)\nrules. After successful consistency check, the model instance is fed as input\nto another LLM for the purpose of code generation. The generated code is\nevaluated in a simulated environment using CARLA simulator connected to an\nexample centralized vehicle architecture, in an emergency brake scenario.", "published": "2024-04-08 13:28:11", "link": "http://arxiv.org/abs/2404.05508v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "D.2.1; D.2.2; D.2.4; I.2.7; I.2.2; I.7.0"], "primary_category": "cs.SE"}
{"title": "Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data", "abstract": "Reinforcement Learning from Human Feedback (RLHF) is a popular method for\naligning Language Models (LM) with human values and preferences. RLHF requires\na large number of preference pairs as training data, which are often used in\nboth the Supervised Fine-Tuning and Reward Model training and therefore\npublicly available datasets are commonly used. In this work, we study to what\nextent a malicious actor can manipulate the LMs generations by poisoning the\npreferences, i.e., injecting poisonous preference pairs into these datasets and\nthe RLHF training process. We propose strategies to build poisonous preference\npairs and test their performance by poisoning two widely used preference\ndatasets. Our results show that preference poisoning is highly effective:\ninjecting a small amount of poisonous data (1-5\\% of the original dataset), we\ncan effectively manipulate the LM to generate a target entity in a target\nsentiment (positive or negative). The findings from our experiments also shed\nlight on strategies to defend against the preference poisoning attack.", "published": "2024-04-08 13:59:02", "link": "http://arxiv.org/abs/2404.05530v2", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "OPSD: an Offensive Persian Social media Dataset and its baseline\n  evaluations", "abstract": "The proliferation of hate speech and offensive comments on social media has\nbecome increasingly prevalent due to user activities. Such comments can have\ndetrimental effects on individuals' psychological well-being and social\nbehavior. While numerous datasets in the English language exist in this domain,\nfew equivalent resources are available for Persian language. To address this\ngap, this paper introduces two offensive datasets. The first dataset comprises\nannotations provided by domain experts, while the second consists of a large\ncollection of unlabeled data obtained through web crawling for unsupervised\nlearning purposes. To ensure the quality of the former dataset, a meticulous\nthree-stage labeling process was conducted, and kappa measures were computed to\nassess inter-annotator agreement. Furthermore, experiments were performed on\nthe dataset using state-of-the-art language models, both with and without\nemploying masked language modeling techniques, as well as machine learning\nalgorithms, in order to establish the baselines for the dataset using\ncontemporary cutting-edge approaches. The obtained F1-scores for the\nthree-class and two-class versions of the dataset were 76.9% and 89.9% for\nXLM-RoBERTa, respectively.", "published": "2024-04-08 14:08:56", "link": "http://arxiv.org/abs/2404.05540v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating Interventional Reasoning Capabilities of Large Language\n  Models", "abstract": "Numerous decision-making tasks require estimating causal effects under\ninterventions on different parts of a system. As practitioners consider using\nlarge language models (LLMs) to automate decisions, studying their causal\nreasoning capabilities becomes crucial. A recent line of work evaluates LLMs\nability to retrieve commonsense causal facts, but these evaluations do not\nsufficiently assess how LLMs reason about interventions. Motivated by the role\nthat interventions play in causal inference, in this paper, we conduct\nempirical analyses to evaluate whether LLMs can accurately update their\nknowledge of a data-generating process in response to an intervention. We\ncreate benchmarks that span diverse causal graphs (e.g., confounding,\nmediation) and variable types, and enable a study of intervention-based\nreasoning. These benchmarks allow us to isolate the ability of LLMs to\naccurately predict changes resulting from their ability to memorize facts or\nfind other shortcuts. We evaluate six LLMs on the benchmarks, finding that GPT\nmodels show promising accuracy at predicting the intervention effects.", "published": "2024-04-08 14:15:56", "link": "http://arxiv.org/abs/2404.05545v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ME"], "primary_category": "cs.LG"}
{"title": "Dense Training, Sparse Inference: Rethinking Training of\n  Mixture-of-Experts Language Models", "abstract": "Mixture-of-Experts (MoE) language models can reduce computational costs by\n2-4$\\times$ compared to dense models without sacrificing performance, making\nthem more efficient in computation-bounded scenarios. However, MoE models\ngenerally require 2-4$\\times$ times more parameters to achieve comparable\nperformance to a dense model, which incurs larger GPU memory requirements and\nmakes MoE models less efficient in I/O-bounded scenarios like autoregressive\ngeneration. In this work, we propose a hybrid dense training and sparse\ninference framework for MoE models (DS-MoE) which achieves strong computation\nand parameter efficiency by employing dense computation across all experts\nduring training and sparse computation during inference. Our experiments on\ntraining LLMs demonstrate that our DS-MoE models are more parameter-efficient\nthan standard sparse MoEs and are on par with dense models in terms of total\nparameter size and performance while being computationally cheaper (activating\n30-40% of the model's parameters). Performance tests using vLLM show that our\nDS-MoE-6B model runs up to $1.86\\times$ faster than similar dense models like\nMistral-7B, and between $1.50\\times$ and $1.71\\times$ faster than comparable\nMoEs, such as DeepSeekMoE-16B and Qwen1.5-MoE-A2.7B.", "published": "2024-04-08 14:39:49", "link": "http://arxiv.org/abs/2404.05567v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "360$^\\circ$REA: Towards A Reusable Experience Accumulation with\n  360\u00b0 Assessment for Multi-Agent System", "abstract": "Large language model agents have demonstrated remarkable advancements across\nvarious complex tasks. Recent works focus on optimizing the agent team or\nemploying self-reflection to iteratively solve complex tasks. Since these\nagents are all based on the same LLM, only conducting self-evaluation or\nremoving underperforming agents does not substantively enhance the capability\nof the agents. We argue that a comprehensive evaluation and accumulating\nexperience from evaluation feedback is an effective approach to improving\nsystem performance. In this paper, we propose Reusable Experience Accumulation\nwith 360$^\\circ$ Assessment (360$^\\circ$REA), a hierarchical multi-agent\nframework inspired by corporate organizational practices. The framework employs\na novel 360$^\\circ$ performance assessment method for multi-perspective\nperformance evaluation with fine-grained assessment. To enhance the capability\nof agents in addressing complex tasks, we introduce dual-level experience pool\nfor agents to accumulate experience through fine-grained assessment. Extensive\nexperiments on complex task datasets demonstrate the effectiveness of\n360$^\\circ$REA.", "published": "2024-04-08 14:43:13", "link": "http://arxiv.org/abs/2404.05569v3", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI"}
{"title": "SpeechAlign: Aligning Speech Generation to Human Preferences", "abstract": "Speech language models have significantly advanced in generating realistic\nspeech, with neural codec language models standing out. However, the\nintegration of human feedback to align speech outputs to human preferences is\noften neglected. This paper addresses this gap by first analyzing the\ndistribution gap in codec language models, highlighting how it leads to\ndiscrepancies between the training and inference phases, which negatively\naffects performance. Then we explore leveraging learning from human feedback to\nbridge the distribution gap. We introduce SpeechAlign, an iterative\nself-improvement strategy that aligns speech language models to human\npreferences. SpeechAlign involves constructing a preference codec dataset\ncontrasting golden codec tokens against synthetic tokens, followed by\npreference optimization to improve the codec language model. This cycle of\nimprovement is carried out iteratively to steadily convert weak models to\nstrong ones. Through both subjective and objective evaluations, we show that\nSpeechAlign can bridge the distribution gap and facilitating continuous\nself-improvement of the speech language model. Moreover, SpeechAlign exhibits\nrobust generalization capabilities and works for smaller models. Code and\nmodels will be available at https://github.com/0nutation/SpeechGPT.", "published": "2024-04-08 15:21:17", "link": "http://arxiv.org/abs/2404.05600v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "How to Evaluate Entity Resolution Systems: An Entity-Centric Framework\n  with Application to Inventor Name Disambiguation", "abstract": "Entity resolution (record linkage, microclustering) systems are notoriously\ndifficult to evaluate. Looking for a needle in a haystack, traditional\nevaluation methods use sophisticated, application-specific sampling schemes to\nfind matching pairs of records among an immense number of non-matches. We\npropose an alternative that facilitates the creation of representative,\nreusable benchmark data sets without necessitating complex sampling schemes.\nThese benchmark data sets can then be used for model training and a variety of\nevaluation tasks. Specifically, we propose an entity-centric data labeling\nmethodology that integrates with a unified framework for monitoring summary\nstatistics, estimating key performance metrics such as cluster and pairwise\nprecision and recall, and analyzing root causes for errors. We validate the\nframework in an application to inventor name disambiguation and through\nsimulation studies. Software: https://github.com/OlivierBinette/er-evaluation/", "published": "2024-04-08 15:53:29", "link": "http://arxiv.org/abs/2404.05622v1", "categories": ["cs.CL", "cs.LG", "stat.ME"], "primary_category": "cs.CL"}
{"title": "Causality Extraction from Nuclear Licensee Event Reports Using a Hybrid\n  Framework", "abstract": "Industry-wide nuclear power plant operating experience is a critical source\nof raw data for performing parameter estimations in reliability and risk\nmodels. Much operating experience information pertains to failure events and is\nstored as reports containing unstructured data, such as narratives. Event\nreports are essential for understanding how failures are initiated and\npropagated, including the numerous causal relations involved. Causal relation\nextraction using deep learning represents a significant frontier in the field\nof natural language processing (NLP), and is crucial since it enables the\ninterpretation of intricate narratives and connections contained within vast\namounts of written information. This paper proposed a hybrid framework for\ncausality detection and extraction from nuclear licensee event reports. The\nmain contributions include: (1) we compiled an LER corpus with 20,129 text\nsamples for causality analysis, (2) developed an interactive tool for labeling\ncause effect pairs, (3) built a deep-learning-based approach for causal\nrelation detection, and (4) developed a knowledge based cause-effect extraction\napproach.", "published": "2024-04-08 16:39:34", "link": "http://arxiv.org/abs/2404.05656v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "VietMed: A Dataset and Benchmark for Automatic Speech Recognition of\n  Vietnamese in the Medical Domain", "abstract": "Due to privacy restrictions, there's a shortage of publicly available speech\nrecognition datasets in the medical domain. In this work, we present VietMed -\na Vietnamese speech recognition dataset in the medical domain comprising 16h of\nlabeled medical speech, 1000h of unlabeled medical speech and 1200h of\nunlabeled general-domain speech. To our best knowledge, VietMed is by far the\nworld's largest public medical speech recognition dataset in 7 aspects: total\nduration, number of speakers, diseases, recording conditions, speaker roles,\nunique medical terms and accents. VietMed is also by far the largest public\nVietnamese speech dataset in terms of total duration. Additionally, we are the\nfirst to present a medical ASR dataset covering all ICD-10 disease groups and\nall accents within a country. Moreover, we release the first public large-scale\npre-trained models for Vietnamese ASR, w2v2-Viet and XLSR-53-Viet, along with\nthe first public large-scale fine-tuned models for medical ASR. Even without\nany medical data in unsupervised pre-training, our best pre-trained model\nXLSR-53-Viet generalizes very well to the medical domain by outperforming\nstate-of-the-art XLSR-53, from 51.8% to 29.6% WER on test set (a relative\nreduction of more than 40%). All code, data and models are made publicly\navailable: https://github.com/leduckhai/MultiMed/tree/master/VietMed.", "published": "2024-04-08 16:43:52", "link": "http://arxiv.org/abs/2404.05659v3", "categories": ["cs.CL", "cs.AI", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Comprehensive Study on German Language Models for Clinical and\n  Biomedical Text Understanding", "abstract": "Recent advances in natural language processing (NLP) can be largely\nattributed to the advent of pre-trained language models such as BERT and\nRoBERTa. While these models demonstrate remarkable performance on general\ndatasets, they can struggle in specialized domains such as medicine, where\nunique domain-specific terminologies, domain-specific abbreviations, and\nvarying document structures are common. This paper explores strategies for\nadapting these models to domain-specific requirements, primarily through\ncontinuous pre-training on domain-specific data. We pre-trained several German\nmedical language models on 2.4B tokens derived from translated public English\nmedical data and 3B tokens of German clinical data. The resulting models were\nevaluated on various German downstream tasks, including named entity\nrecognition (NER), multi-label classification, and extractive question\nanswering. Our results suggest that models augmented by clinical and\ntranslation-based pre-training typically outperform general domain models in\nmedical contexts. We conclude that continuous pre-training has demonstrated the\nability to match or even exceed the performance of clinical models trained from\nscratch. Furthermore, pre-training on clinical data or leveraging translated\ntexts have proven to be reliable methods for domain adaptation in medical NLP\ntasks.", "published": "2024-04-08 17:24:04", "link": "http://arxiv.org/abs/2404.05694v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs", "abstract": "Recent advancements in multimodal large language models (MLLMs) have been\nnoteworthy, yet, these general-domain MLLMs often fall short in their ability\nto comprehend and interact effectively with user interface (UI) screens. In\nthis paper, we present Ferret-UI, a new MLLM tailored for enhanced\nunderstanding of mobile UI screens, equipped with referring, grounding, and\nreasoning capabilities. Given that UI screens typically exhibit a more\nelongated aspect ratio and contain smaller objects of interest (e.g., icons,\ntexts) than natural images, we incorporate \"any resolution\" on top of Ferret to\nmagnify details and leverage enhanced visual features. Specifically, each\nscreen is divided into 2 sub-images based on the original aspect ratio (i.e.,\nhorizontal division for portrait screens and vertical division for landscape\nscreens). Both sub-images are encoded separately before being sent to LLMs. We\nmeticulously gather training samples from an extensive range of elementary UI\ntasks, such as icon recognition, find text, and widget listing. These samples\nare formatted for instruction-following with region annotations to facilitate\nprecise referring and grounding. To augment the model's reasoning ability, we\nfurther compile a dataset for advanced tasks, including detailed description,\nperception/interaction conversations, and function inference. After training on\nthe curated datasets, Ferret-UI exhibits outstanding comprehension of UI\nscreens and the capability to execute open-ended instructions. For model\nevaluation, we establish a comprehensive benchmark encompassing all the\naforementioned tasks. Ferret-UI excels not only beyond most open-source UI\nMLLMs, but also surpasses GPT-4V on all the elementary UI tasks.", "published": "2024-04-08 17:55:44", "link": "http://arxiv.org/abs/2404.05719v1", "categories": ["cs.CV", "cs.CL", "cs.HC"], "primary_category": "cs.CV"}
{"title": "A Survey on Responsible Generative AI: What to Generate and What Not", "abstract": "In recent years, generative AI (GenAI), like large language models and\ntext-to-image models, has received significant attention across various\ndomains. However, ensuring the responsible generation of content by these\nmodels is crucial for their real-world applicability. This raises an\ninteresting question: What should responsible GenAI generate, and what should\nit not? To answer the question, this paper investigates the practical\nresponsible requirements of both textual and visual generative models,\noutlining five key considerations: generating truthful content, avoiding toxic\ncontent, refusing harmful instruction, leaking no training data-related\ncontent, and ensuring generated content identifiable. Specifically, we review\nrecent advancements and challenges in addressing these requirements. Besides,\nwe discuss and emphasize the importance of responsible GenAI across healthcare,\neducation, finance, and artificial general intelligence domains. Through a\nunified perspective on both textual and visual generative models, this paper\naims to provide insights into practical safety-related issues and further\nbenefit the community in building responsible GenAI.", "published": "2024-04-08 17:53:21", "link": "http://arxiv.org/abs/2404.05783v2", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.CY"}
{"title": "SambaLingo: Teaching Large Language Models New Languages", "abstract": "Despite the widespread availability of LLMs, there remains a substantial gap\nin their capabilities and availability across diverse languages. One approach\nto address these issues has been to take an existing pre-trained LLM and\ncontinue to train it on new languages. While prior works have experimented with\nlanguage adaptation, many questions around best practices and methodology have\nnot been covered. In this paper, we present a comprehensive investigation into\nthe adaptation of LLMs to new languages. Our study covers the key components in\nthis process, including vocabulary extension, direct preference optimization\nand the data scarcity problem for human alignment in low-resource languages. We\nscale these experiments across 9 languages and 2 parameter scales (7B and 70B).\nWe compare our models against Llama 2, Aya-101, XGLM, BLOOM and existing\nlanguage experts, outperforming all prior published baselines. Additionally,\nall evaluation code and checkpoints are made public to facilitate future\nresearch.", "published": "2024-04-08 19:48:36", "link": "http://arxiv.org/abs/2404.05829v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Negative Preference Optimization: From Catastrophic Collapse to\n  Effective Unlearning", "abstract": "Large Language Models (LLMs) often memorize sensitive, private, or\ncopyrighted data during pre-training. LLM unlearning aims to eliminate the\ninfluence of undesirable data from the pre-trained model while preserving the\nmodel's utilities on other tasks. Several practical methods have recently been\nproposed for LLM unlearning, mostly based on gradient ascent (GA) on the loss\nof undesirable data. However, on certain unlearning tasks, these methods either\nfail to effectively unlearn the target data or suffer from catastrophic\ncollapse -- a drastic degradation of the model's utilities.\n  In this paper, we propose Negative Preference Optimization (NPO), a simple\nalignment-inspired method that could efficiently and effectively unlearn a\ntarget dataset. We theoretically show that the progression toward catastrophic\ncollapse by minimizing the NPO loss is exponentially slower than GA. Through\nexperiments on synthetic data and the benchmark TOFU dataset, we demonstrate\nthat NPO-based methods achieve a better balance between unlearning the\nundesirable data and maintaining the model's utilities. We also observe that\nNPO-based methods generate more sensible outputs than GA-based methods, whose\noutputs are often gibberish. Remarkably, on TOFU, NPO-based methods are the\nfirst to achieve reasonable unlearning results in forgetting 50% (or more) of\nthe training data, whereas existing methods already struggle with forgetting\n10% of training data.", "published": "2024-04-08 21:05:42", "link": "http://arxiv.org/abs/2404.05868v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "CodecLM: Aligning Language Models with Tailored Synthetic Data", "abstract": "Instruction tuning has emerged as the key in aligning large language models\n(LLMs) with specific task instructions, thereby mitigating the discrepancy\nbetween the next-token prediction objective and users' actual goals. To reduce\nthe labor and time cost to collect or annotate data by humans, researchers\nstart to explore the use of LLMs to generate instruction-aligned synthetic\ndata. Recent works focus on generating diverse instructions and applying LLM to\nincrease instruction complexity, often neglecting downstream use cases. It\nremains unclear how to tailor high-quality data to elicit better\ninstruction-following abilities in different target instruction distributions\nand LLMs. To this end, we introduce CodecLM, a general framework for adaptively\ngenerating high-quality synthetic data for LLM alignment with different\ndownstream instruction distributions and LLMs. Drawing on the Encode-Decode\nprinciples, we use LLMs as codecs to guide the data generation process. We\nfirst encode seed instructions into metadata, which are concise keywords\ngenerated on-the-fly to capture the target instruction distribution, and then\ndecode metadata to create tailored instructions. We also introduce Self-Rubrics\nand Contrastive Filtering during decoding to tailor data-efficient samples.\nExtensive experiments on four open-domain instruction following benchmarks\nvalidate the effectiveness of CodecLM over the current state-of-the-arts.", "published": "2024-04-08 21:15:36", "link": "http://arxiv.org/abs/2404.05875v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Use of a Structured Knowledge Base Enhances Metadata Curation by Large\n  Language Models", "abstract": "Metadata play a crucial role in ensuring the findability, accessibility,\ninteroperability, and reusability of datasets. This paper investigates the\npotential of large language models (LLMs), specifically GPT-4, to improve\nadherence to metadata standards. We conducted experiments on 200 random data\nrecords describing human samples relating to lung cancer from the NCBI\nBioSample repository, evaluating GPT-4's ability to suggest edits for adherence\nto metadata standards. We computed the adherence accuracy of field name-field\nvalue pairs through a peer review process, and we observed a marginal average\nimprovement in adherence to the standard data dictionary from 79% to 80%\n(p<0.5). We then prompted GPT-4 with domain information in the form of the\ntextual descriptions of CEDAR templates and recorded a significant improvement\nto 97% from 79% (p<0.01). These results indicate that, while LLMs may not be\nable to correct legacy metadata to ensure satisfactory adherence to standards\nwhen unaided, they do show promise for use in automated metadata curation when\nintegrated with a structured knowledge base", "published": "2024-04-08 22:29:53", "link": "http://arxiv.org/abs/2404.05893v5", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Automating Research Synthesis with Domain-Specific Large Language Model\n  Fine-Tuning", "abstract": "This research pioneers the use of fine-tuned Large Language Models (LLMs) to\nautomate Systematic Literature Reviews (SLRs), presenting a significant and\nnovel contribution in integrating AI to enhance academic research\nmethodologies. Our study employed the latest fine-tuning methodologies together\nwith open-sourced LLMs, and demonstrated a practical and efficient approach to\nautomating the final execution stages of an SLR process that involves knowledge\nsynthesis. The results maintained high fidelity in factual accuracy in LLM\nresponses, and were validated through the replication of an existing\nPRISMA-conforming SLR. Our research proposed solutions for mitigating LLM\nhallucination and proposed mechanisms for tracking LLM responses to their\nsources of information, thus demonstrating how this approach can meet the\nrigorous demands of scholarly research. The findings ultimately confirmed the\npotential of fine-tuned LLMs in streamlining various labor-intensive processes\nof conducting literature reviews. Given the potential of this approach and its\napplicability across all research domains, this foundational study also\nadvocated for updating PRISMA reporting guidelines to incorporate AI-driven\nprocesses, ensuring methodological transparency and reliability in future SLRs.\nThis study broadens the appeal of AI-enhanced tools across various academic and\nresearch fields, setting a new standard for conducting comprehensive and\naccurate literature reviews with more efficiency in the face of ever-increasing\nvolumes of academic studies.", "published": "2024-04-08 00:08:29", "link": "http://arxiv.org/abs/2404.08680v1", "categories": ["cs.CL", "cs.DL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Is English the New Programming Language? How About Pseudo-code\n  Engineering?", "abstract": "Background: The integration of artificial intelligence (AI) into daily life,\nparticularly through chatbots utilizing natural language processing (NLP),\npresents both revolutionary potential and unique challenges. This intended to\ninvestigate how different input forms impact ChatGPT, a leading language model\nby OpenAI, performance in understanding and executing complex, multi-intention\ntasks. Design: Employing a case study methodology supplemented by discourse\nanalysis, the research analyzes ChatGPT's responses to inputs varying from\nnatural language to pseudo-code engineering. The study specifically examines\nthe model's proficiency across four categories: understanding of intentions,\ninterpretability, completeness, and creativity. Setting and Participants: As a\ntheoretical exploration of AI interaction, this study focuses on the analysis\nof structured and unstructured inputs processed by ChatGPT, without direct\nhuman participants. Data collection and analysis: The research utilizes\nsynthetic case scenarios, including the organization of a \"weekly meal plan\"\nand a \"shopping list,\" to assess ChatGPT's response to prompts in both natural\nlanguage and pseudo-code engineering. The analysis is grounded in the\nidentification of patterns, contradictions, and unique response elements across\ndifferent input formats. Results: Findings reveal that pseudo-code engineering\ninputs significantly enhance the clarity and determinism of ChatGPT's\nresponses, reducing ambiguity inherent in natural language. Enhanced natural\nlanguage, structured through prompt engineering techniques, similarly improves\nthe model's interpretability and creativity. Conclusions: The study underscores\nthe potential of pseudo-code engineering in refining human-AI interaction and\nachieving more deterministic, concise, and direct outcomes, advocating for its\nbroader application across disciplines requiring precise AI responses.", "published": "2024-04-08 16:28:52", "link": "http://arxiv.org/abs/2404.08684v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.CY", "J.4; K.3; I.2"], "primary_category": "cs.CL"}
{"title": "Xiwu: A Basis Flexible and Learnable LLM for High Energy Physics", "abstract": "Large Language Models (LLMs) are undergoing a period of rapid updates and\nchanges, with state-of-the-art (SOTA) model frequently being replaced. When\napplying LLMs to a specific scientific field, it's challenging to acquire\nunique domain knowledge while keeping the model itself advanced. To address\nthis challenge, a sophisticated large language model system named as Xiwu has\nbeen developed, allowing you switch between the most advanced foundation models\nand quickly teach the model domain knowledge. In this work, we will report on\nthe best practices for applying LLMs in the field of high-energy physics (HEP),\nincluding: a seed fission technology is proposed and some data collection and\ncleaning tools are developed to quickly obtain domain AI-Ready dataset; a\njust-in-time learning system is implemented based on the vector store\ntechnology; an on-the-fly fine-tuning system has been developed to facilitate\nrapid training under a specified foundation model. The results show that Xiwu\ncan smoothly switch between foundation models such as LLaMA, Vicuna, ChatGLM\nand Grok-1. The trained Xiwu model is significantly outperformed the benchmark\nmodel on the HEP knowledge question-and-answering and code generation. This\nstrategy significantly enhances the potential for growth of our model's\nperformance, with the hope of surpassing GPT-4 as it evolves with the\ndevelopment of open-source models. This work provides a customized LLM for the\nfield of HEP, while also offering references for applying LLM to other fields,\nthe corresponding codes are available on Github.", "published": "2024-04-08 07:37:31", "link": "http://arxiv.org/abs/2404.08001v1", "categories": ["hep-ph", "cs.AI", "cs.CL", "cs.LG", "hep-ex", "physics.comp-ph", "I.2.7"], "primary_category": "hep-ph"}
{"title": "Efficient High-Performance Bark-Scale Neural Network for Residual Echo\n  and Noise Suppression", "abstract": "In recent years, the introduction of neural networks (NNs) into the field of\nspeech enhancement has brought significant improvements. However, many of the\nproposed methods are quite demanding in terms of computational complexity and\nmemory footprint. For the application in dedicated communication devices, such\nas speakerphones, hands-free car systems, or smartphones, efficiency plays a\nmajor role along with performance. In this context, we present an efficient,\nhigh-performance hybrid joint acoustic echo control and noise suppression\nsystem, whereby our main contribution is the postfilter NN, performing both\nnoise and residual echo suppression. The preservation of nearend speech is\nimproved by a Bark-scale auditory filterbank for the NN postfilter. The\nproposed hybrid method is benchmarked with state-of-the-art methods and its\neffectiveness is demonstrated on the ICASSP 2023 AEC Challenge blind test set.\nWe demonstrate that it offers high-quality nearend speech preservation during\nboth double-talk and nearend speech conditions. At the same time, it is capable\nof efficient removal of echo leaks, achieving a comparable performance to\nalready small state-of-the-art models such as the end-to-end DeepVQE-S, while\nrequiring only around 10 % of its computational complexity. This makes it\neasily realtime implementable on a speakerphone device.", "published": "2024-04-08 08:47:53", "link": "http://arxiv.org/abs/2404.11621v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "SoundingActions: Learning How Actions Sound from Narrated Egocentric\n  Videos", "abstract": "We propose a novel self-supervised embedding to learn how actions sound from\nnarrated in-the-wild egocentric videos. Whereas existing methods rely on\ncurated data with known audio-visual correspondence, our multimodal\ncontrastive-consensus coding (MC3) embedding reinforces the associations\nbetween audio, language, and vision when all modality pairs agree, while\ndiminishing those associations when any one pair does not. We show our approach\ncan successfully discover how the long tail of human actions sound from\negocentric video, outperforming an array of recent multimodal embedding\ntechniques on two datasets (Ego4D and EPIC-Sounds) and multiple cross-modal\ntasks.", "published": "2024-04-08 05:19:28", "link": "http://arxiv.org/abs/2404.05206v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
