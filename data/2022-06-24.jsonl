{"title": "Do You Know My Emotion? Emotion-Aware Strategy Recognition towards a\n  Persuasive Dialogue System", "abstract": "Persuasive strategy recognition task requires the system to recognize the\nadopted strategy of the persuader according to the conversation. However,\nprevious methods mainly focus on the contextual information, little is known\nabout incorporating the psychological feedback, i.e. emotion of the persuadee,\nto predict the strategy. In this paper, we propose a Cross-channel Feedback\nmemOry Network (CFO-Net) to leverage the emotional feedback to iteratively\nmeasure the potential benefits of strategies and incorporate them into the\ncontextual-aware dialogue information. Specifically, CFO-Net designs a feedback\nmemory module, including strategy pool and feedback pool, to obtain\nemotion-aware strategy representation. The strategy pool aims to store\nhistorical strategies and the feedback pool is to obtain updated strategy\nweight based on feedback emotional information. Furthermore, a cross-channel\nfusion predictor is developed to make a mutual interaction between the\nemotion-aware strategy representation and the contextual-aware dialogue\ninformation for strategy recognition. Experimental results on\n\\textsc{PersuasionForGood} confirm that the proposed model CFO-Net is effective\nto improve the performance on M-F1 from 61.74 to 65.41.", "published": "2022-06-24 06:24:46", "link": "http://arxiv.org/abs/2206.12101v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MVP: Multi-task Supervised Pre-training for Natural Language Generation", "abstract": "Pre-trained language models (PLMs) have achieved remarkable success in\nnatural language generation (NLG) tasks. Up to now, most NLG-oriented PLMs are\npre-trained in an unsupervised manner using the large-scale general corpus. In\nthe meanwhile, an increasing number of models pre-trained with labeled data\n(i.e. \"supervised pre-training\") showcase superior performance compared to\nunsupervised pre-trained models. Motivated by the success of supervised\npre-training, we propose Multi-task superVised Pre-training (MVP) for natural\nlanguage generation. We collect a large-scale natural language generation\ncorpus, MVPCorpus, from $77$ datasets over $11$ diverse NLG tasks. Then we\nunify these examples into a general text-to-text format to pre-train the text\ngeneration model MVP in a supervised manner. For each task, we further\npre-train specific soft prompts to stimulate the model's capacity to perform a\nspecific task. Our MVP model can be seen as a practice that utilizes recent\ninstruction tuning on relatively small PLMs. Extensive experiments have\ndemonstrated the effectiveness and generality of our MVP model in a number of\nNLG tasks, which achieves state-of-the-art performance on $13$ out of $17$\ndatasets, outperforming BART by $9.3\\%$ and Flan-T5 by $5.8\\%$.", "published": "2022-06-24 07:49:47", "link": "http://arxiv.org/abs/2206.12131v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text and author-level political inference using heterogeneous knowledge\n  representations", "abstract": "The inference of politically-charged information from text data is a popular\nresearch topic in Natural Language Processing (NLP) at both text- and\nauthor-level. In recent years, studies of this kind have been implemented with\nthe aid of representations from transformers such as BERT. Despite considerable\nsuccess, however, we may ask whether results may be improved even further by\ncombining transformed-based models with additional knowledge representations.\nTo shed light on this issue, the present work describes a series of experiments\nto compare alternative model configurations for political inference from text\nin both English and Portuguese languages. Results suggest that certain text\nrepresentations - in particular, the combined use of BERT pre-trained language\nmodels with a syntactic dependency model - may outperform the alternatives\nacross multiple experimental settings, making a potentially strong case for\nfurther research in the use of heterogeneous text representations in these and\npossibly other NLP tasks.", "published": "2022-06-24 13:45:36", "link": "http://arxiv.org/abs/2206.12293v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "QAGAN: Adversarial Approach To Learning Domain Invariant Language\n  Features", "abstract": "Training models that are robust to data domain shift has gained an increasing\ninterest both in academia and industry. Question-Answering language models,\nbeing one of the typical problem in Natural Language Processing (NLP) research,\nhas received much success with the advent of large transformer models. However,\nexisting approaches mostly work under the assumption that data is drawn from\nsame distribution during training and testing which is unrealistic and\nnon-scalable in the wild.\n  In this paper, we explore adversarial training approach towards learning\ndomain-invariant features so that language models can generalize well to\nout-of-domain datasets. We also inspect various other ways to boost our model\nperformance including data augmentation by paraphrasing sentences, conditioning\nend of answer span prediction on the start word, and carefully designed\nannealing function. Our initial results show that in combination with these\nmethods, we are able to achieve $15.2\\%$ improvement in EM score and $5.6\\%$\nboost in F1 score on out-of-domain validation dataset over the baseline. We\nalso dissect our model outputs and visualize the model hidden-states by\nprojecting them onto a lower-dimensional space, and discover that our specific\nadversarial training approach indeed encourages the model to learn domain\ninvariant embedding and bring them closer in the multi-dimensional space.", "published": "2022-06-24 17:42:18", "link": "http://arxiv.org/abs/2206.12388v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The syntax-lexicon tradeoff in writing", "abstract": "As speakers turn their thoughts into sentences, they maintain a balance\nbetween the complexity of words and syntax. However, it is unclear whether this\nsyntax-lexicon tradeoff is unique to the spoken language production that is\nunder the pressure of rapid online processing. Alternatively, it is possible\nthat the tradeoff is a basic property of language irrespective of the modality\nof production. This work evaluates the relationship between the complexity of\nwords and syntactic rules in the written language of neurotypical individuals\non three different topics. We found that similar to speaking, constructing\nsentences in writing involves a tradeoff between the complexity of the lexical\nand syntactic items. We also show that the reduced online processing demands\nduring writing allows for retrieving more complex words at the cost of\nincorporating simpler syntax. This work further highlights the role of\naccessibility of the elements of a sentence as the driving force in the\nemergence of the syntax-lexicon tradeoff.", "published": "2022-06-24 19:57:12", "link": "http://arxiv.org/abs/2206.12485v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DetIE: Multilingual Open Information Extraction Inspired by Object\n  Detection", "abstract": "State of the art neural methods for open information extraction (OpenIE)\nusually extract triplets (or tuples) iteratively in an autoregressive or\npredicate-based manner in order not to produce duplicates. In this work, we\npropose a different approach to the problem that can be equally or more\nsuccessful. Namely, we present a novel single-pass method for OpenIE inspired\nby object detection algorithms from computer vision. We use an order-agnostic\nloss based on bipartite matching that forces unique predictions and a\nTransformer-based encoder-only architecture for sequence labeling. The proposed\napproach is faster and shows superior or similar performance in comparison with\nstate of the art models on standard benchmarks in terms of both quality metrics\nand inference time. Our model sets the new state of the art performance of\n67.7% F1 on CaRB evaluated as OIE2016 while being 3.35x faster at inference\nthan previous state of the art. We also evaluate the multilingual version of\nour model in the zero-shot setting for two languages and introduce a strategy\nfor generating synthetic multilingual data to fine-tune the model for each\nspecific language. In this setting, we show performance improvement 15% on\nmultilingual Re-OIE2016, reaching 75% F1 for both Portuguese and Spanish\nlanguages. Code and models are available at\nhttps://github.com/sberbank-ai/DetIE.", "published": "2022-06-24 23:47:00", "link": "http://arxiv.org/abs/2206.12514v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A multi-model-based deep learning framework for short text multiclass\n  classification with the imbalanced and extremely small data set", "abstract": "Text classification plays an important role in many practical applications.\nIn the real world, there are extremely small datasets. Most existing methods\nadopt pre-trained neural network models to handle this kind of dataset.\nHowever, these methods are either difficult to deploy on mobile devices because\nof their large output size or cannot fully extract the deep semantic\ninformation between phrases and clauses. This paper proposes a multimodel-based\ndeep learning framework for short-text multiclass classification with an\nimbalanced and extremely small data set. Our framework mainly includes five\nlayers: The encoder layer uses DISTILBERT to obtain context-sensitive dynamic\nword vectors that are difficult to represent in traditional feature engineering\nmethods. Since the transformer part of this layer is distilled, our framework\nis compressed. Then, we use the next two layers to extract deep semantic\ninformation. The output of the encoder layer is sent to a bidirectional LSTM\nnetwork, and the feature matrix is extracted hierarchically through the LSTM at\nthe word and sentence level to obtain the fine-grained semantic representation.\nAfter that, the max-pooling layer converts the feature matrix into a\nlower-dimensional matrix, preserving only the obvious features. Finally, the\nfeature matrix is taken as the input of a fully connected softmax layer, which\ncontains a function that can convert the predicted linear vector into the\noutput value as the probability of the text in each classification. Extensive\nexperiments on two public benchmarks demonstrate the effectiveness of our\nproposed approach on an extremely small data set. It retains the\nstate-of-the-art baseline performance in terms of precision, recall, accuracy,\nand F1 score, and through the model size, training time, and convergence epoch,\nwe can conclude that our method can be deployed faster and lighter on mobile\ndevices.", "published": "2022-06-24 00:51:02", "link": "http://arxiv.org/abs/2206.12027v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DialogID: A Dialogic Instruction Dataset for Improving Teaching\n  Effectiveness in Online Environments", "abstract": "Online dialogic instructions are a set of pedagogical instructions used in\nreal-world online educational contexts to motivate students, help understand\nlearning materials, and build effective study habits. In spite of the\npopularity and advantages of online learning, the education technology and\neducational data mining communities still suffer from the lack of large-scale,\nhigh-quality, and well-annotated teaching instruction datasets to study\ncomputational approaches to automatically detect online dialogic instructions\nand further improve the online teaching effectiveness. Therefore, in this\npaper, we present a dataset of online dialogic instruction detection,\n\\textsc{DialogID}, which contains 30,431 effective dialogic instructions. These\nteaching instructions are well annotated into 8 categories. Furthermore, we\nutilize the prevalent pre-trained language models (PLMs) and propose a simple\nyet effective adversarial training learning paradigm to improve the quality and\ngeneralization of dialogic instruction detection. Extensive experiments\ndemonstrate that our approach outperforms a wide range of baseline methods. The\ndata and our code are available for research purposes from:\nhttps://github.com/ai4ed/DialogID.", "published": "2022-06-24 02:07:12", "link": "http://arxiv.org/abs/2206.12034v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SC-Ques: A Sentence Completion Question Dataset for English as a Second\n  Language Learners", "abstract": "Sentence completion (SC) questions present a sentence with one or more blanks\nthat need to be filled in, three to five possible words or phrases as options.\nSC questions are widely used for students learning English as a Second Language\n(ESL). In this paper, we present a large-scale SC dataset, \\textsc{SC-Ques},\nwhich is made up of 289,148 ESL SC questions from real-world standardized\nEnglish examinations. Furthermore, we build a comprehensive benchmark of\nautomatically solving the SC questions by training the large-scale pre-trained\nlanguage models on the proposed \\textsc{SC-Ques} dataset. We conduct detailed\nanalysis of the baseline models performance, limitations and trade-offs. The\ndata and our code are available for research purposes from:\n\\url{https://github.com/ai4ed/SC-Ques}.", "published": "2022-06-24 02:17:13", "link": "http://arxiv.org/abs/2206.12036v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Classifying Unstructured Clinical Notes via Automatic Weak Supervision", "abstract": "Healthcare providers usually record detailed notes of the clinical care\ndelivered to each patient for clinical, research, and billing purposes. Due to\nthe unstructured nature of these narratives, providers employ dedicated staff\nto assign diagnostic codes to patients' diagnoses using the International\nClassification of Diseases (ICD) coding system. This manual process is not only\ntime-consuming but also costly and error-prone. Prior work demonstrated\npotential utility of Machine Learning (ML) methodology in automating this\nprocess, but it has relied on large quantities of manually labeled data to\ntrain the models. Additionally, diagnostic coding systems evolve with time,\nwhich makes traditional supervised learning strategies unable to generalize\nbeyond local applications. In this work, we introduce a general\nweakly-supervised text classification framework that learns from class-label\ndescriptions only, without the need to use any human-labeled documents. It\nleverages the linguistic domain knowledge stored within pre-trained language\nmodels and the data programming framework to assign code labels to individual\ntexts. We demonstrate the efficacy and flexibility of our method by comparing\nit to state-of-the-art weak text classifiers across four real-world text\nclassification datasets, in addition to assigning ICD codes to medical notes in\nthe publicly available MIMIC-III database.", "published": "2022-06-24 05:55:49", "link": "http://arxiv.org/abs/2206.12088v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unified BERT for Few-shot Natural Language Understanding", "abstract": "Even as pre-trained language models share a semantic encoder, natural\nlanguage understanding suffers from a diversity of output schemas. In this\npaper, we propose UBERT, a unified bidirectional language understanding model\nbased on BERT framework, which can universally model the training objects of\ndifferent NLU tasks through a biaffine network. Specifically, UBERT encodes\nprior knowledge from various aspects, uniformly constructing learning\nrepresentations across multiple NLU tasks, which is conducive to enhancing the\nability to capture common semantic understanding. By using the biaffine to\nmodel scores pair of the start and end position of the original text, various\nclassification and extraction structures can be converted into a universal,\nspan-decoding approach. Experiments show that UBERT wins the first price in the\n2022 AIWIN - World Artificial Intelligence Innovation Competition, Chinese\ninsurance few-shot multi-task track, and realizes the unification of extensive\ninformation extraction and linguistic reasoning tasks.", "published": "2022-06-24 06:10:53", "link": "http://arxiv.org/abs/2206.12094v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Capture Salient Historical Information: A Fast and Accurate\n  Non-Autoregressive Model for Multi-turn Spoken Language Understanding", "abstract": "Spoken Language Understanding (SLU), a core component of the task-oriented\ndialogue system, expects a shorter inference facing the impatience of human\nusers. Existing work increases inference speed by designing non-autoregressive\nmodels for single-turn SLU tasks but fails to apply to multi-turn SLU in\nconfronting the dialogue history. The intuitive idea is to concatenate all\nhistorical utterances and utilize the non-autoregressive models directly.\nHowever, this approach seriously misses the salient historical information and\nsuffers from the uncoordinated-slot problems. To overcome those shortcomings,\nwe propose a novel model for multi-turn SLU named Salient History Attention\nwith Layer-Refined Transformer (SHA-LRT), which composes of an SHA module, a\nLayer-Refined Mechanism (LRM), and a Slot Label Generation (SLG) task. SHA\ncaptures salient historical information for the current dialogue from both\nhistorical utterances and results via a well-designed history-attention\nmechanism. LRM predicts preliminary SLU results from Transformer's middle\nstates and utilizes them to guide the final prediction, and SLG obtains the\nsequential dependency information for the non-autoregressive encoder.\nExperiments on public datasets indicate that our model significantly improves\nmulti-turn SLU performance (17.5% on Overall) with accelerating (nearly 15\ntimes) the inference process over the state-of-the-art baseline as well as\neffective on the single-turn SLU tasks.", "published": "2022-06-24 10:45:32", "link": "http://arxiv.org/abs/2206.12209v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Robustness of Explanation Methods for NLP Models", "abstract": "Explanation methods have emerged as an important tool to highlight the\nfeatures responsible for the predictions of neural networks. There is mounting\nevidence that many explanation methods are rather unreliable and susceptible to\nmalicious manipulations. In this paper, we particularly aim to understand the\nrobustness of explanation methods in the context of text modality. We provide\ninitial insights and results towards devising a successful adversarial attack\nagainst text explanations. To our knowledge, this is the first attempt to\nevaluate the adversarial robustness of an explanation method. Our experiments\nshow the explanation method can be largely disturbed for up to 86% of the\ntested samples with small changes in the input sentence and its semantics.", "published": "2022-06-24 13:34:07", "link": "http://arxiv.org/abs/2206.12284v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Rhetorical Structure Theory-based descriptions of observed\n  behaviour", "abstract": "In a previous paper, we have proposed a set of concepts, axiom schemata and\nalgorithms that can be used by agents to learn to describe their behaviour,\ngoals, capabilities, and environment. The current paper proposes a new set of\nconcepts, axiom schemata and algorithms that allow the agent to learn new\ndescriptions of an observed behaviour (e.g., perplexing actions), of its actor\n(e.g., undesired propositions or actions), and of its environment (e.g.,\nincompatible propositions). Each learned description (e.g., a certain action\nprevents another action from being performed in the future) is represented by a\nrelationship between entities (either propositions or actions) and is learned\nby the agent, just by observation, using domain-independent axiom schemata and\nor learning algorithms. The relations used by agents to represent the\ndescriptions they learn were inspired on the Theory of Rhetorical Structure\n(RST). The main contribution of the paper is the relation family Although,\ninspired on the RST relation Concession. The accurate definition of the\nrelations of the family Although involves a set of deontic concepts whose\ndefinition and corresponding algorithms are presented. The relations of the\nfamily Although, once extracted from the agent's observations, express surprise\nat the observed behaviour and, in certain circumstances, present a\njustification for it.\n  The paper shows results of the presented proposals in a demonstration\nscenario, using implemented software.", "published": "2022-06-24 13:47:20", "link": "http://arxiv.org/abs/2206.12294v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "End-to-End Text-to-Speech Based on Latent Representation of Speaking\n  Styles Using Spontaneous Dialogue", "abstract": "The recent text-to-speech (TTS) has achieved quality comparable to that of\nhumans; however, its application in spoken dialogue has not been widely\nstudied. This study aims to realize a TTS that closely resembles human\ndialogue. First, we record and transcribe actual spontaneous dialogues. Then,\nthe proposed dialogue TTS is trained in two stages: first stage, variational\nautoencoder (VAE)-VITS or Gaussian mixture variational autoencoder (GMVAE)-VITS\nis trained, which introduces an utterance-level latent variable into\nvariational inference with adversarial learning for end-to-end text-to-speech\n(VITS), a recently proposed end-to-end TTS model. A style encoder that extracts\na latent speaking style representation from speech is trained jointly with TTS.\nIn the second stage, a style predictor is trained to predict the speaking style\nto be synthesized from dialogue history. During inference, by passing the\nspeaking style representation predicted by the style predictor to\nVAE/GMVAE-VITS, speech can be synthesized in a style appropriate to the context\nof the dialogue. Subjective evaluation results demonstrate that the proposed\nmethod outperforms the original VITS in terms of dialogue-level naturalness.", "published": "2022-06-24 02:32:12", "link": "http://arxiv.org/abs/2206.12040v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exact Prosody Cloning in Zero-Shot Multispeaker Text-to-Speech", "abstract": "The cloning of a speaker's voice using an untranscribed reference sample is\none of the great advances of modern neural text-to-speech (TTS) methods.\nApproaches for mimicking the prosody of a transcribed reference audio have also\nbeen proposed recently. In this work, we bring these two tasks together for the\nfirst time through utterance level normalization in conjunction with an\nutterance level speaker embedding. We further introduce a lightweight aligner\nfor extracting fine-grained prosodic features, that can be finetuned on\nindividual samples within seconds. We show that it is possible to clone the\nvoice of a speaker as well as the prosody of a spoken reference independently\nwithout any degradation in quality and high similarity to both original voice\nand prosody, as our objective evaluation and human study show. All of our code\nand trained models are available, alongside static and interactive demos.", "published": "2022-06-24 11:54:59", "link": "http://arxiv.org/abs/2206.12229v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Using BERT Embeddings to Model Word Importance in Conversational\n  Transcripts for Deaf and Hard of Hearing Users", "abstract": "Deaf and hard of hearing individuals regularly rely on captioning while\nwatching live TV. Live TV captioning is evaluated by regulatory agencies using\nvarious caption evaluation metrics. However, caption evaluation metrics are\noften not informed by preferences of DHH users or how meaningful the captions\nare. There is a need to construct caption evaluation metrics that take the\nrelative importance of words in a transcript into account. We conducted\ncorrelation analysis between two types of word embeddings and human-annotated\nlabeled word-importance scores in existing corpus. We found that normalized\ncontextualized word embeddings generated using BERT correlated better with\nmanually annotated importance scores than word2vec-based word embeddings. We\nmake available a pairing of word embeddings and their human-annotated\nimportance scores. We also provide proof-of-concept utility by training word\nimportance models, achieving an F1-score of 0.57 in the 6-class word importance\nclassification task.", "published": "2022-06-24 16:35:57", "link": "http://arxiv.org/abs/2206.12368v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "OPERA: Harmonizing Task-Oriented Dialogs and Information Seeking\n  Experience", "abstract": "Existing studies in conversational AI mostly treat task-oriented dialog (TOD)\nand question answering (QA) as separate tasks. Towards the goal of constructing\na conversational agent that can complete user tasks and support information\nseeking, it is important to build a system that handles both TOD and QA with\naccess to various external knowledge. In this work, we propose a new task,\nOpen-Book TOD (OB-TOD), which combines TOD with QA task and expand external\nknowledge sources to include both explicit knowledge sources (e.g., the Web)\nand implicit knowledge sources (e.g., pre-trained language models). We create a\nnew dataset OB-MultiWOZ, where we enrich TOD sessions with QA-like information\nseeking experience grounded on external knowledge. We propose a unified model\nOPERA (Open-book End-to-end Task-oriented Dialog) which can appropriately\naccess explicit and implicit external knowledge to tackle the defined task.\nExperimental results demonstrate OPERA's superior performance compared to\nclosed-book baselines and illustrate the value of both knowledge types.", "published": "2022-06-24 18:21:26", "link": "http://arxiv.org/abs/2206.12449v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Burst2Vec: An Adversarial Multi-Task Approach for Predicting Emotion,\n  Age, and Origin from Vocal Bursts", "abstract": "We present Burst2Vec, our multi-task learning approach to predict emotion,\nage, and origin (i.e., native country/language) from vocal bursts. Burst2Vec\nutilises pre-trained speech representations to capture acoustic information\nfrom raw waveforms and incorporates the concept of model debiasing via\nadversarial training. Our models achieve a relative 30 % performance gain over\nbaselines using pre-extracted features and score the highest amongst all\nparticipants in the ICML ExVo 2022 Multi-Task Challenge.", "published": "2022-06-24 18:57:41", "link": "http://arxiv.org/abs/2206.12469v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Competence-based Multimodal Curriculum Learning for Medical Report\n  Generation", "abstract": "Medical report generation task, which targets to produce long and coherent\ndescriptions of medical images, has attracted growing research interests\nrecently. Different from the general image captioning tasks, medical report\ngeneration is more challenging for data-driven neural models. This is mainly\ndue to 1) the serious data bias and 2) the limited medical data. To alleviate\nthe data bias and make best use of available data, we propose a\nCompetence-based Multimodal Curriculum Learning framework (CMCL). Specifically,\nCMCL simulates the learning process of radiologists and optimizes the model in\na step by step manner. Firstly, CMCL estimates the difficulty of each training\ninstance and evaluates the competence of current model; Secondly, CMCL selects\nthe most suitable batch of training instances considering current model\ncompetence. By iterating above two steps, CMCL can gradually improve the\nmodel's performance. The experiments on the public IU-Xray and MIMIC-CXR\ndatasets show that CMCL can be incorporated into existing models to improve\ntheir performance.", "published": "2022-06-24 08:16:01", "link": "http://arxiv.org/abs/2206.14579v3", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Confidence Score Based Conformer Speaker Adaptation for Speech\n  Recognition", "abstract": "A key challenge for automatic speech recognition (ASR) systems is to model\nthe speaker level variability. In this paper, compact speaker dependent\nlearning hidden unit contributions (LHUC) are used to facilitate both speaker\nadaptive training (SAT) and test time unsupervised speaker adaptation for\nstate-of-the-art Conformer based end-to-end ASR systems. The sensitivity during\nadaptation to supervision error rate is reduced using confidence score based\nselection of the more \"trustworthy\" subset of speaker specific data. A\nconfidence estimation module is used to smooth the over-confident Conformer\ndecoder output probabilities before serving as confidence scores. The increased\ndata sparsity due to speaker level data selection is addressed using Bayesian\nestimation of LHUC parameters. Experiments on the 300-hour Switchboard corpus\nsuggest that the proposed LHUC-SAT Conformer with confidence score based test\ntime unsupervised adaptation outperformed the baseline speaker independent and\ni-vector adapted Conformer systems by up to 1.0%, 1.0%, and 1.2% absolute\n(9.0%, 7.9%, and 8.9% relative) word error rate (WER) reductions on the NIST\nHub5'00, RT02, and RT03 evaluation sets respectively. Consistent performance\nimprovements were retained after external Transformer and LSTM language models\nwere used for rescoring.", "published": "2022-06-24 02:48:00", "link": "http://arxiv.org/abs/2206.12045v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Data Augmentation and Squeeze-and-Excitation Network on Multiple\n  Dimension for Sound Event Localization and Detection in Real Scenes", "abstract": "Performance of sound event localization and detection (SELD) in real scenes\nis limited by small size of SELD dataset, due to difficulty in obtaining\nsufficient amount of realistic multi-channel audio data recordings with\naccurate label. We used two main strategies to solve problems arising from the\nsmall real SELD dataset. First, we applied various data augmentation methods on\nall data dimensions: channel, frequency and time. We also propose original data\naugmentation method named Moderate Mixup in order to simulate situations where\nnoise floor or interfering events exist. Second, we applied\nSqueeze-and-Excitation block on channel and frequency dimensions to efficiently\nextract feature characteristics. Result of our trained models on the STARSS22\ntest dataset achieved the best ER, F1, LE, and LR of 0.53, 49.8%, 16.0deg., and\n56.2% respectively.", "published": "2022-06-24 03:31:37", "link": "http://arxiv.org/abs/2206.12059v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SANE-TTS: Stable And Natural End-to-End Multilingual Text-to-Speech", "abstract": "In this paper, we present SANE-TTS, a stable and natural end-to-end\nmultilingual TTS model. By the difficulty of obtaining multilingual corpus for\ngiven speaker, training multilingual TTS model with monolingual corpora is\nunavoidable. We introduce speaker regularization loss that improves speech\nnaturalness during cross-lingual synthesis as well as domain adversarial\ntraining, which is applied in other multilingual TTS models. Furthermore, by\nadding speaker regularization loss, replacing speaker embedding with zero\nvector in duration predictor stabilizes cross-lingual inference. With this\nreplacement, our model generates speeches with moderate rhythm regardless of\nsource speaker in cross-lingual synthesis. In MOS evaluation, SANE-TTS achieves\nnaturalness score above 3.80 both in cross-lingual and intralingual synthesis,\nwhere the ground truth score is 3.99. Also, SANE-TTS maintains speaker\nsimilarity close to that of ground truth even in cross-lingual inference. Audio\nsamples are available on our web page.", "published": "2022-06-24 07:53:05", "link": "http://arxiv.org/abs/2206.12132v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Iterative Sound Source Localization for Unknown Number of Sources", "abstract": "Sound source localization aims to seek the direction of arrival (DOA) of all\nsound sources from the observed multi-channel audio. For the practical problem\nof unknown number of sources, existing localization algorithms attempt to\npredict a likelihood-based coding (i.e., spatial spectrum) and employ a\npre-determined threshold to detect the source number and corresponding DOA\nvalue. However, these threshold-based algorithms are not stable since they are\nlimited by the careful choice of threshold. To address this problem, we propose\nan iterative sound source localization approach called ISSL, which can\niteratively extract each source's DOA without threshold until the termination\ncriterion is met. Unlike threshold-based algorithms, ISSL designs an active\nsource detector network based on binary classifier to accept residual spatial\nspectrum and decide whether to stop the iteration. By doing so, our ISSL can\ndeal with an arbitrary number of sources, even more than the number of sources\nseen during the training stage. The experimental results show that our ISSL\nachieves significant performance improvements in both DOA estimation and source\nnumber detection compared with the existing threshold-based algorithms.", "published": "2022-06-24 13:19:44", "link": "http://arxiv.org/abs/2206.12273v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Open-source objective-oriented framework for head-related transfer\n  function", "abstract": "Throughout last 30 years, numerous head-related transfer function (HRTF)\nmodels have been developed and there are more to come. This paper describes a\nframework based on objective-oriented programming paradigm, in which each HRTF\nrepresentation method can be implemented as a separate class. Its modular\nstructure allows the source code to be conveniently shared between researchers,\nwhile common interface provides easy access to data regardless of the internal\nstructure of the classes. The paper discusses difficulties of designing the\nframework, maintaining the balance between its flexibility and finding common\nfeatures of every possible directivity representation. Exemplary use cases are\nincluded and explained. Adoption of the framework will enhance possibilities of\naccuracy comparison between various HRTF models, thus improving the evaluation\nof current and future representation methods. The framework, developed in the\nform of a MATLAB toolbox, is designed to handle not only HRTFs but also other\ntypes of spatial data, such as e.g. sound source directivity, microphone\ndirectivity, etc.", "published": "2022-06-24 13:31:59", "link": "http://arxiv.org/abs/2206.12283v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Quality Assessment through MOS using Non-Matching References", "abstract": "Human judgments obtained through Mean Opinion Scores (MOS) are the most\nreliable way to assess the quality of speech signals. However, several recent\nattempts to automatically estimate MOS using deep learning approaches lack\nrobustness and generalization capabilities, limiting their use in real-world\napplications. In this work, we present a novel framework, NORESQA-MOS, for\nestimating the MOS of a speech signal. Unlike prior works, our approach uses\nnon-matching references as a form of conditioning to ground the MOS estimation\nby neural networks. We show that NORESQA-MOS provides better generalization and\nmore robust MOS estimation than previous state-of-the-art methods such as\nDNSMOS and NISQA, even though we use a smaller training set. Moreover, we also\nshow that our generic framework can be combined with other learning methods\nsuch as self-supervised learning and can further supplement the benefits from\nthese methods.", "published": "2022-06-24 13:34:53", "link": "http://arxiv.org/abs/2206.12285v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SAQAM: Spatial Audio Quality Assessment Metric", "abstract": "Audio quality assessment is critical for assessing the perceptual realism of\nsounds. However, the time and expense of obtaining ''gold standard'' human\njudgments limit the availability of such data. For AR&VR, good perceived sound\nquality and localizability of sources are among the key elements to ensure\ncomplete immersion of the user. Our work introduces SAQAM which uses a\nmulti-task learning framework to assess listening quality (LQ) and\nspatialization quality (SQ) between any given pair of binaural signals without\nusing any subjective data. We model LQ by training on a simulated dataset of\ntriplet human judgments, and SQ by utilizing activation-level distances from\nnetworks trained for direction of arrival (DOA) estimation. We show that SAQAM\ncorrelates well with human responses across four diverse datasets. Since it is\na deep network, the metric is differentiable, making it suitable as a loss\nfunction for other tasks. For example, simply replacing an existing loss with\nour metric yields improvement in a speech-enhancement network.", "published": "2022-06-24 13:47:52", "link": "http://arxiv.org/abs/2206.12297v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Predicting within and across language phoneme recognition performance of\n  self-supervised learning speech pre-trained models", "abstract": "In this work, we analyzed and compared speech representations extracted from\ndifferent frozen self-supervised learning (SSL) speech pre-trained models on\ntheir ability to capture articulatory features (AF) information and their\nsubsequent prediction of phone recognition performance for within and across\nlanguage scenarios. Specifically, we compared CPC, wav2vec 2.0, and HuBert.\nFirst, frame-level AF probing tasks were implemented. Subsequently, phone-level\nend-to-end ASR systems for phoneme recognition tasks were implemented, and the\nperformance on the frame-level AF probing task and the phone accuracy were\ncorrelated. Compared to the conventional speech representation MFCC, all SSL\npre-trained speech representations captured more AF information, and achieved\nbetter phoneme recognition performance within and across languages, with HuBert\nperforming best. The frame-level AF probing task is a good predictor of phoneme\nrecognition performance, showing the importance of capturing AF information in\nthe speech representations. Compared with MFCC, in the within-language\nscenario, the performance of these SSL speech pre-trained models on AF probing\ntasks achieved a maximum relative increase of 34.4%, and it resulted in the\nlowest PER of 10.2%. In the cross-language scenario, the maximum relative\nincrease of 26.7% also resulted in the lowest PER of 23.0%.", "published": "2022-06-24 20:55:46", "link": "http://arxiv.org/abs/2206.12489v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "BYOL-S: Learning Self-supervised Speech Representations by Bootstrapping", "abstract": "Methods for extracting audio and speech features have been studied since\npioneering work on spectrum analysis decades ago. Recent efforts are guided by\nthe ambition to develop general-purpose audio representations. For example,\ndeep neural networks can extract optimal embeddings if they are trained on\nlarge audio datasets. This work extends existing methods based on\nself-supervised learning by bootstrapping, proposes various encoder\narchitectures, and explores the effects of using different pre-training\ndatasets. Lastly, we present a novel training framework to come up with a\nhybrid audio representation, which combines handcrafted and data-driven learned\naudio features. All the proposed representations were evaluated within the HEAR\nNeurIPS 2021 challenge for auditory scene classification and timestamp\ndetection tasks. Our results indicate that the hybrid model with a\nconvolutional transformer as the encoder yields superior performance in most\nHEAR challenge tasks.", "published": "2022-06-24 02:26:40", "link": "http://arxiv.org/abs/2206.12038v4", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deformable CNN and Imbalance-Aware Feature Learning for Singing\n  Technique Classification", "abstract": "Singing techniques are used for expressive vocal performances by employing\ntemporal fluctuations of the timbre, the pitch, and other components of the\nvoice. Their classification is a challenging task, because of mainly two\nfactors: 1) the fluctuations in singing techniques have a wide variety and are\naffected by many factors and 2) existing datasets are imbalanced. To deal with\nthese problems, we developed a novel audio feature learning method based on\ndeformable convolution with decoupled training of the feature extractor and the\nclassifier using a class-weighted loss function. The experimental results show\nthe following: 1) the deformable convolution improves the classification\nresults, particularly when it is applied to the last two convolutional layers,\nand 2) both re-training the classifier and weighting the cross-entropy loss\nfunction by a smoothed inverse frequency enhance the classification\nperformance.", "published": "2022-06-24 11:56:51", "link": "http://arxiv.org/abs/2206.12230v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Analyzing the impact of SARS-CoV-2 variants on respiratory sound signals", "abstract": "The COVID-19 outbreak resulted in multiple waves of infections that have been\nassociated with different SARS-CoV-2 variants. Studies have reported\ndifferential impact of the variants on respiratory health of patients. We\nexplore whether acoustic signals, collected from COVID-19 subjects, show\ncomputationally distinguishable acoustic patterns suggesting a possibility to\npredict the underlying virus variant. We analyze the Coswara dataset which is\ncollected from three subject pools, namely, i) healthy, ii) COVID-19 subjects\nrecorded during the delta variant dominant period, and iii) data from COVID-19\nsubjects recorded during the omicron surge. Our findings suggest that multiple\nsound categories, such as cough, breathing, and speech, indicate significant\nacoustic feature differences when comparing COVID-19 subjects with omicron and\ndelta variants. The classification areas-under-the-curve are significantly\nabove chance for differentiating subjects infected by omicron from those\ninfected by delta. Using a score fusion from multiple sound categories, we\nobtained an area-under-the-curve of 89% and 52.4% sensitivity at 95%\nspecificity. Additionally, a hierarchical three class approach was used to\nclassify the acoustic data into healthy and COVID-19 positive, and further\nCOVID-19 subjects into delta and omicron variants providing high level of\n3-class classification accuracy. These results suggest new ways for designing\nsound based COVID-19 diagnosis approaches.", "published": "2022-06-24 14:10:31", "link": "http://arxiv.org/abs/2206.12309v1", "categories": ["eess.AS", "cs.LG", "eess.SP"], "primary_category": "eess.AS"}
{"title": "PoCaP Corpus: A Multimodal Dataset for Smart Operating Room Speech\n  Assistant using Interventional Radiology Workflow Analysis", "abstract": "This paper presents a new multimodal interventional radiology dataset, called\nPoCaP (Port Catheter Placement) Corpus. This corpus consists of speech and\naudio signals in German, X-ray images, and system commands collected from 31\nPoCaP interventions by six surgeons with average duration of 81.4 $\\pm$ 41.0\nminutes. The corpus aims to provide a resource for developing a smart speech\nassistant in operating rooms. In particular, it may be used to develop a speech\ncontrolled system that enables surgeons to control the operation parameters\nsuch as C-arm movements and table positions. In order to record the dataset, we\nacquired consent by the institutional review board and workers council in the\nUniversity Hospital Erlangen and by the patients for data privacy. We describe\nthe recording set-up, data structure, workflow and preprocessing steps, and\nreport the first PoCaP Corpus speech recognition analysis results with 11.52\n$\\%$ word error rate using pretrained models. The findings suggest that the\ndata has the potential to build a robust command recognition system and will\nallow the development of a novel intervention support systems using speech and\nimage processing in the medical domain.", "published": "2022-06-24 14:39:11", "link": "http://arxiv.org/abs/2206.12320v1", "categories": ["cs.SD", "cs.AI", "eess.AS", "00b20"], "primary_category": "cs.SD"}
{"title": "How to hide your voice: Noise-cancelling bird photography blind", "abstract": "Getting close to birds is a great challenge in wildlife photography. Bird\nphotography blinds may be the most effective and least intrusive way if\nproperly designed. However, the acoustic design of the blinds has been\noverlooked so far. Herein, we present noise-cancelling blinds which allow\nphotographing birds at close range. Firstly, we conduct a questionnaire in the\neco-tourism centre located in Yunnan, China. Thus, we determine the birders'\nexpectations of the indoor sound environment. We then identify diverse\nvariables to examine the impact of architectural and acoustic decisions on\nnoise propagation. Finally, we examine the acoustic performance of the blinds\nby considering the birds' hearing threshold. The numerical simulations are\nperformed in the acoustics module of Comsol MultiPhysics. Our study\ndemonstrated that photography blinds require a strong and thorough acoustic\ndesign for both human and bird well-being.", "published": "2022-06-24 15:22:48", "link": "http://arxiv.org/abs/2206.12340v2", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "An Intensity and Phase Stacked Analysis of Phase-OTDR System using Deep\n  Transfer Learning and Recurrent Neural Networks", "abstract": "Distributed acoustic sensors (DAS) are effective apparatus which are widely\nused in many application areas for recording signals of various events with\nvery high spatial resolution along the optical fiber. To detect and recognize\nthe recorded events properly, advanced signal processing algorithms with high\ncomputational demands are crucial. Convolutional neural networks are highly\ncapable tools for extracting spatial information and very suitable for event\nrecognition applications in DAS. Long-short term memory (LSTM) is an effective\ninstrument for processing sequential data. In this study, we proposed a\nmulti-input multi-output, two stage feature extraction methodology that\ncombines the capabilities of these neural network architectures with transfer\nlearning to classify vibrations applied to an optical fiber by a piezo\ntransducer. First, we extracted the differential amplitude and phase\ninformation from the Phase-OTDR recordings and stored them in a\ntemporal-spatial data matrix. Then, we used a state-of-the-art pre-trained CNN\nwithout dense layers as a feature extractor in the first stage. In the second\nstage, we used LSTMs to further analyze the features extracted by the CNN.\nFinally, we used a dense layer to classify the extracted features. To observe\nthe effect of the utilized CNN architecture, we tested our model with five\nstate-of-the art pre-trained models (VGG-16, ResNet-50, DenseNet-121, MobileNet\nand Inception-v3). The results show that using the VGG-16 architecture in our\nframework manages to obtain 100% classification accuracy in 50 trainings and\ngot the best results on our Phase-OTDR dataset. Outcomes of this study indicate\nthat the pre-trained CNNs combined with LSTM are very suitable for the analysis\nof differential amplitude and phase information, represented in a temporal\nspatial data matrix which is promising for event recognition operations in DAS\napplications.", "published": "2022-06-24 19:56:01", "link": "http://arxiv.org/abs/2206.12484v2", "categories": ["cs.LG", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Multitask vocal burst modeling with ResNets and pre-trained\n  paralinguistic Conformers", "abstract": "This technical report presents the modeling approaches used in our submission\nto the ICML Expressive Vocalizations Workshop & Competition multitask track\n(ExVo-MultiTask). We first applied image classification models of various sizes\non mel-spectrogram representations of the vocal bursts, as is standard in sound\nevent detection literature. Results from these models show an increase of\n21.24% over the baseline system with respect to the harmonic mean of the task\nmetrics, and comprise our team's main submission to the MultiTask track. We\nthen sought to characterize the headroom in the MultiTask track by applying a\nlarge pre-trained Conformer model that previously achieved state-of-the-art\nresults on paralinguistic tasks like speech emotion recognition and mask\ndetection. We additionally investigated the relationship between the sub-tasks\nof emotional expression, country of origin, and age prediction, and discovered\nthat the best performing models are trained as single-task models, questioning\nwhether the problem truly benefits from a multitask setting.", "published": "2022-06-24 21:42:16", "link": "http://arxiv.org/abs/2206.12494v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Domain Generalization with Relaxed Instance Frequency-wise Normalization\n  for Multi-device Acoustic Scene Classification", "abstract": "While using two-dimensional convolutional neural networks (2D-CNNs) in image\nprocessing, it is possible to manipulate domain information using channel\nstatistics, and instance normalization has been a promising way to get\ndomain-invariant features. However, unlike image processing, we analyze that\ndomain-relevant information in an audio feature is dominant in frequency\nstatistics rather than channel statistics. Motivated by our analysis, we\nintroduce Relaxed Instance Frequency-wise Normalization (RFN): a plug-and-play,\nexplicit normalization module along the frequency axis which can eliminate\ninstance-specific domain discrepancy in an audio feature while relaxing\nundesirable loss of useful discriminative information. Empirically, simply\nadding RFN to networks shows clear margins compared to previous domain\ngeneralization approaches on acoustic scene classification and yields improved\nrobustness for multiple audio devices. Especially, the proposed RFN won the\nDCASE2021 challenge TASK1A, low-complexity acoustic scene classification with\nmultiple devices, with a clear margin, and RFN is an extended work of our\ntechnical report.", "published": "2022-06-24 23:45:50", "link": "http://arxiv.org/abs/2206.12513v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
