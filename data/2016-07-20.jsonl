{"title": "An Adaptation of Topic Modeling to Sentences", "abstract": "Advances in topic modeling have yielded effective methods for characterizing\nthe latent semantics of textual data. However, applying standard topic modeling\napproaches to sentence-level tasks introduces a number of challenges. In this\npaper, we adapt the approach of latent-Dirichlet allocation to include an\nadditional layer for incorporating information about the sentence boundaries in\ndocuments. We show that the addition of this minimal information of document\nstructure improves the perplexity results of a trained model.", "published": "2016-07-20 04:22:50", "link": "http://arxiv.org/abs/1607.05818v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incremental Learning for Fully Unsupervised Word Segmentation Using\n  Penalized Likelihood and Model Selection", "abstract": "We present a novel incremental learning approach for unsupervised word\nsegmentation that combines features from probabilistic modeling and model\nselection. This includes super-additive penalties for addressing the cognitive\nburden imposed by long word formation, and new model selection criteria based\non higher-order generative assumptions. Our approach is fully unsupervised; it\nrelies on a small number of parameters that permits flexible modeling and a\nmechanism that automatically learns parameters from the data. Through\nexperimentation, we show that this intricate design has led to top-tier\nperformance in both phonemic and orthographic word segmentation.", "published": "2016-07-20 04:38:01", "link": "http://arxiv.org/abs/1607.05822v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Contextual Conversation Learning with Labeled Question-Answering\n  Pairs", "abstract": "Neural conversational models tend to produce generic or safe responses in\ndifferent contexts, e.g., reply \\textit{\"Of course\"} to narrative statements or\n\\textit{\"I don't know\"} to questions. In this paper, we propose an end-to-end\napproach to avoid such problem in neural generative models. Additional memory\nmechanisms have been introduced to standard sequence-to-sequence (seq2seq)\nmodels, so that context can be considered while generating sentences. Three\nseq2seq models, which memorize a fix-sized contextual vector from hidden input,\nhidden input/output and a gated contextual attention structure respectively,\nhave been trained and tested on a dataset of labeled question-answering pairs\nin Chinese. The model with contextual attention outperforms others including\nthe state-of-the-art seq2seq models on perplexity test. The novel contextual\nmodel generates diverse and robust responses, and is able to carry out\nconversations on a wide range of topics appropriately.", "published": "2016-07-20 03:25:31", "link": "http://arxiv.org/abs/1607.05809v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Robust Natural Language Processing - Combining Reasoning, Cognitive\n  Semantics and Construction Grammar for Spatial Language", "abstract": "We present a system for generating and understanding of dynamic and static\nspatial relations in robotic interaction setups. Robots describe an environment\nof moving blocks using English phrases that include spatial relations such as\n\"across\" and \"in front of\". We evaluate the system in robot-robot interactions\nand show that the system can robustly deal with visual perception errors,\nlanguage omissions and ungrammatical utterances.", "published": "2016-07-20 14:15:24", "link": "http://arxiv.org/abs/1607.05968v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Compositional Sequence Labeling Models for Error Detection in Learner\n  Writing", "abstract": "In this paper, we present the first experiments using neural network models\nfor the task of error detection in learner writing. We perform a systematic\ncomparison of alternative compositional architectures and propose a framework\nfor error detection based on bidirectional LSTMs. Experiments on the CoNLL-14\nshared task dataset show the model is able to outperform other participants on\ndetecting errors in learner writing. Finally, the model is integrated with a\npublicly deployed self-assessment system, leading to performance comparable to\nhuman annotators.", "published": "2016-07-20 23:26:33", "link": "http://arxiv.org/abs/1607.06153v1", "categories": ["cs.CL", "cs.NE", "I.5.1; I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "Constructing a Natural Language Inference Dataset using Generative\n  Neural Networks", "abstract": "Natural Language Inference is an important task for Natural Language\nUnderstanding. It is concerned with classifying the logical relation between\ntwo sentences. In this paper, we propose several text generative neural\nnetworks for generating text hypothesis, which allows construction of new\nNatural Language Inference datasets. To evaluate the models, we propose a new\nmetric -- the accuracy of the classifier trained on the generated dataset. The\naccuracy obtained by our best generative model is only 2.7% lower than the\naccuracy of the classifier trained on the original, human crafted dataset.\nFurthermore, the best generated dataset combined with the original dataset\nachieves the highest accuracy. The best model learns a mapping embedding for\neach training example. By comparing various metrics we show that datasets that\nobtain higher ROUGE or METEOR scores do not necessarily yield higher\nclassification accuracies. We also provide analysis of what are the\ncharacteristics of a good dataset including the distinguishability of the\ngenerated datasets from the original one.", "published": "2016-07-20 16:59:21", "link": "http://arxiv.org/abs/1607.06025v2", "categories": ["cs.AI", "cs.CL", "cs.NE"], "primary_category": "cs.AI"}
