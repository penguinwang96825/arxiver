{"title": "Tracing a Loose Wordhood for Chinese Input Method Engine", "abstract": "Chinese input methods are used to convert pinyin sequence or other Latin\nencoding systems into Chinese character sentences. For more effective\npinyin-to-character conversion, typical Input Method Engines (IMEs) rely on a\npredefined vocabulary that demands manually maintenance on schedule. For the\npurpose of removing the inconvenient vocabulary setting, this work focuses on\nautomatic wordhood acquisition by fully considering that Chinese inputting is a\nfree human-computer interaction procedure. Instead of strictly defining words,\na loose word likelihood is introduced for measuring how likely a character\nsequence can be a user-recognized word with respect to using IME. Then an\nonline algorithm is proposed to adjust the word likelihood or generate new\nwords by comparing user true choice for inputting and the algorithm prediction.\nThe experimental results show that the proposed solution can agilely adapt to\ndiverse typings and demonstrate performance approaching highly-optimized IME\nwith fixed vocabulary.", "published": "2017-12-12 08:03:17", "link": "http://arxiv.org/abs/1712.04158v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Zero Resource Speech Challenge 2017", "abstract": "We describe a new challenge aimed at discovering subword and word units from\nraw speech. This challenge is the followup to the Zero Resource Speech\nChallenge 2015. It aims at constructing systems that generalize across\nlanguages and adapt to new speakers. The design features and evaluation metrics\nof the challenge are presented and the results of seventeen models are\ndiscussed.", "published": "2017-12-12 14:36:15", "link": "http://arxiv.org/abs/1712.04313v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Novel Document Generation Process for Topic Detection based on\n  Hierarchical Latent Tree Models", "abstract": "We propose a novel document generation process based on hierarchical latent\ntree models (HLTMs) learned from data. An HLTM has a layer of observed word\nvariables at the bottom and multiple layers of latent variables on top. For\neach document, we first sample values for the latent variables layer by layer\nvia logic sampling, then draw relative frequencies for the words conditioned on\nthe values of the latent variables, and finally generate words for the document\nusing the relative word frequencies. The motivation for the work is to take\nword counts into consideration with HLTMs. In comparison with LDA-based\nhierarchical document generation processes, the new process achieves\ndrastically better model fit with much fewer parameters. It also yields more\nmeaningful topics and topic hierarchies. It is the new state-of-the-art for the\nhierarchical topic detection.", "published": "2017-12-12 04:07:10", "link": "http://arxiv.org/abs/1712.04116v3", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generating and Estimating Nonverbal Alphabets for Situated and\n  Multimodal Communications", "abstract": "In this paper, we discuss the formalized approach for generating and\nestimating symbols (and alphabets), which can be communicated by the wide range\nof non-verbal means based on specific user requirements (medium, priorities,\ntype of information that needs to be conveyed). The short characterization of\nbasic terms and parameters of such symbols (and alphabets) with approaches to\ngenerate them are given. Then the framework, experimental setup, and some\nmachine learning methods to estimate usefulness and effectiveness of the\nnonverbal alphabets and systems are presented. The previous results demonstrate\nthat usage of multimodal data sources (like wearable accelerometer, heart\nmonitor, muscle movements sensors, braincomputer interface) along with machine\nlearning approaches can provide the deeper understanding of the usefulness and\neffectiveness of such alphabets and systems for nonverbal and situated\ncommunication. The symbols (and alphabets) generated and estimated by such\nmethods may be useful in various applications: from synthetic languages and\nconstructed scripts to multimodal nonverbal and situated interaction between\npeople and artificial intelligence systems through Human-Computer Interfaces,\nsuch as mouse gestures, touchpads, body gestures, eyetracking cameras,\nwearables, and brain-computing interfaces, especially in applications for\nelderly care and people with disabilities.", "published": "2017-12-12 14:38:34", "link": "http://arxiv.org/abs/1712.04314v1", "categories": ["cs.HC", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.HC"}
{"title": "Learning Spontaneity to Improve Emotion Recognition In Speech", "abstract": "We investigate the effect and usefulness of spontaneity (i.e. whether a given\nspeech is spontaneous or not) in speech in the context of emotion recognition.\nWe hypothesize that emotional content in speech is interrelated with its\nspontaneity, and use spontaneity classification as an auxiliary task to the\nproblem of emotion recognition. We propose two supervised learning settings\nthat utilize spontaneity to improve speech emotion recognition: a hierarchical\nmodel that performs spontaneity detection before performing emotion\nrecognition, and a multitask learning model that jointly learns to recognize\nboth spontaneity and emotion. Through various experiments on the well known\nIEMOCAP database, we show that by using spontaneity detection as an additional\ntask, significant improvement can be achieved over emotion recognition systems\nthat are unaware of spontaneity. We achieve state-of-the-art emotion\nrecognition accuracy (4-class, 69.1%) on the IEMOCAP database outperforming\nseveral relevant and competitive baselines.", "published": "2017-12-12 14:30:27", "link": "http://arxiv.org/abs/1712.04753v3", "categories": ["eess.AS", "cs.CL", "cs.HC", "cs.SD"], "primary_category": "eess.AS"}
{"title": "auDeep: Unsupervised Learning of Representations from Audio with Deep\n  Recurrent Neural Networks", "abstract": "auDeep is a Python toolkit for deep unsupervised representation learning from\nacoustic data. It is based on a recurrent sequence to sequence autoencoder\napproach which can learn representations of time series data by taking into\naccount their temporal dynamics. We provide an extensive command line interface\nin addition to a Python API for users and developers, both of which are\ncomprehensively documented and publicly available at\nhttps://github.com/auDeep/auDeep. Experimental results indicate that auDeep\nfeatures are competitive with state-of-the art audio classification.", "published": "2017-12-12 16:43:33", "link": "http://arxiv.org/abs/1712.04382v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Classification vs. Regression in Supervised Learning for Single Channel\n  Speaker Count Estimation", "abstract": "The task of estimating the maximum number of concurrent speakers from single\nchannel mixtures is important for various audio-based applications, such as\nblind source separation, speaker diarisation, audio surveillance or auditory\nscene classification. Building upon powerful machine learning methodology, we\ndevelop a Deep Neural Network (DNN) that estimates a speaker count. While DNNs\nefficiently map input representations to output targets, it remains unclear how\nto best handle the network output to infer integer source count estimates, as a\ndiscrete count estimate can either be tackled as a regression or a\nclassification problem. In this paper, we investigate this important design\ndecision and also address complementary parameter choices such as the input\nrepresentation. We evaluate a state-of-the-art DNN audio model based on a\nBi-directional Long Short-Term Memory network architecture for speaker count\nestimations. Through experimental evaluations aimed at identifying the best\noverall strategy for the task and show results for five seconds speech segments\nin mixtures of up to ten speakers.", "published": "2017-12-12 22:32:55", "link": "http://arxiv.org/abs/1712.04555v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-Speaker Localization Using Convolutional Neural Network Trained\n  with Noise", "abstract": "The problem of multi-speaker localization is formulated as a multi-class\nmulti-label classification problem, which is solved using a convolutional\nneural network (CNN) based source localization method. Utilizing the common\nassumption of disjoint speaker activities, we propose a novel method to train\nthe CNN using synthesized noise signals. The proposed localization method is\nevaluated for two speakers and compared to a well-known steered response power\nmethod.", "published": "2017-12-12 13:17:30", "link": "http://arxiv.org/abs/1712.04276v1", "categories": ["cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
