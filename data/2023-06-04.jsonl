{"title": "A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean\n  Language Models", "abstract": "Polyglot is a pioneering project aimed at enhancing the non-English language\nperformance of multilingual language models. Despite the availability of\nvarious multilingual models such as mBERT (Devlin et al., 2019), XGLM (Lin et\nal., 2022), and BLOOM (Scao et al., 2022), researchers and developers often\nresort to building monolingual models in their respective languages due to the\ndissatisfaction with the current multilingual models non-English language\ncapabilities. Addressing this gap, we seek to develop advanced multilingual\nlanguage models that offer improved performance in non-English languages. In\nthis paper, we introduce the Polyglot Korean models, which represent a specific\nfocus rather than being multilingual in nature. In collaboration with TUNiB,\nour team collected 1.2TB of Korean data meticulously curated for our research\njourney. We made a deliberate decision to prioritize the development of Korean\nmodels before venturing into multilingual models. This choice was motivated by\nmultiple factors: firstly, the Korean models facilitated performance\ncomparisons with existing multilingual models; and finally, they catered to the\nspecific needs of Korean companies and researchers. This paper presents our\nwork in developing the Polyglot Korean models, which propose some steps towards\naddressing the non-English language performance gap in multilingual language\nmodels.", "published": "2023-06-04 04:04:04", "link": "http://arxiv.org/abs/2306.02254v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing Physical Reasoning with Counter-Commonsense Context", "abstract": "In this study, we create a CConS (Counter-commonsense Contextual Size\ncomparison) dataset to investigate how physical commonsense affects the\ncontextualized size comparison task; the proposed dataset consists of both\ncontexts that fit physical commonsense and those that do not. This dataset\ntests the ability of language models to predict the size relationship between\nobjects under various contexts generated from our curated noun list and\ntemplates. We measure the ability of several masked language models and\ngenerative models. The results show that while large language models can use\nprepositions such as ``in'' and ``into'' in the provided context to infer size\nrelationships, they fail to use verbs and thus make incorrect judgments led by\ntheir prior physical commonsense.", "published": "2023-06-04 04:24:43", "link": "http://arxiv.org/abs/2306.02258v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OWQ: Outlier-Aware Weight Quantization for Efficient Fine-Tuning and\n  Inference of Large Language Models", "abstract": "Large language models (LLMs) with hundreds of billions of parameters require\npowerful server-grade GPUs for inference, limiting their practical deployment.\nTo address this challenge, we introduce the outlier-aware weight quantization\n(OWQ) method, which aims to minimize LLM's footprint through low-precision\nrepresentation. OWQ prioritizes a small subset of structured weights sensitive\nto quantization, storing them in high-precision, while applying highly tuned\nquantization to the remaining dense weights. This sensitivity-aware\nmixed-precision scheme reduces the quantization error notably, and extensive\nexperiments demonstrate that 3.1-bit models using OWQ perform comparably to\n4-bit models optimized by OPTQ. Furthermore, OWQ incorporates a\nparameter-efficient fine-tuning for task-specific adaptation, called weak\ncolumn tuning (WCT), enabling accurate task-specific LLM adaptation with\nminimal memory overhead in the optimized format. OWQ represents a notable\nadvancement in the flexibility, efficiency, and practicality of LLM\noptimization literature. The source code is available at\nhttps://github.com/xvyaward/owq", "published": "2023-06-04 06:33:13", "link": "http://arxiv.org/abs/2306.02272v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does Character-level Information Always Improve DRS-based Semantic\n  Parsing?", "abstract": "Even in the era of massive language models, it has been suggested that\ncharacter-level representations improve the performance of neural models. The\nstate-of-the-art neural semantic parser for Discourse Representation Structures\nuses character-level representations, improving performance in the four\nlanguages (i.e., English, German, Dutch, and Italian) in the Parallel Meaning\nBank dataset. However, how and why character-level information improves the\nparser's performance remains unclear. This study provides an in-depth analysis\nof performance changes by order of character sequences. In the experiments, we\ncompare F1-scores by shuffling the order and randomizing character sequences\nafter testing the performance of character-level information. Our results\nindicate that incorporating character-level information does not improve the\nperformance in English and German. In addition, we find that the parser is not\nsensitive to correct character order in Dutch. Nevertheless, performance\nimprovements are observed when using character-level information.", "published": "2023-06-04 08:54:32", "link": "http://arxiv.org/abs/2306.02302v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Long Text Generation Challenge", "abstract": "We propose a shared task of human-like long text generation, LTG Challenge,\nthat asks models to output a consistent human-like long text (a Harry Potter\ngeneric audience fanfic in English), given a prompt of about 1000 tokens. We\nsuggest a novel statistical metric of the text structuredness, GloVe\nAutocorrelations Power/ Exponential Law Mean Absolute Percentage Error Ratio\n(GAPELMAPER) and a human evaluation protocol. We hope that LTG can open new\navenues for researchers to investigate sampling approaches, prompting\nstrategies, autoregressive and non-autoregressive text generation architectures\nand break the barrier to generate consistent long (40K+ token) texts.", "published": "2023-06-04 11:52:36", "link": "http://arxiv.org/abs/2306.02334v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Leverage Points in Modality Shifts: Comparing Language-only and\n  Multimodal Word Representations", "abstract": "Multimodal embeddings aim to enrich the semantic information in neural\nrepresentations of language compared to text-only models. While different\nembeddings exhibit different applicability and performance on downstream tasks,\nlittle is known about the systematic representation differences attributed to\nthe visual modality. Our paper compares word embeddings from three\nvision-and-language models (CLIP, OpenCLIP and Multilingual CLIP) and three\ntext-only models, with static (FastText) as well as contextual representations\n(multilingual BERT; XLM-RoBERTa). This is the first large-scale study of the\neffect of visual grounding on language representations, including 46 semantic\nparameters. We identify meaning properties and relations that characterize\nwords whose embeddings are most affected by the inclusion of visual modality in\nthe training data; that is, points where visual grounding turns out most\nimportant. We find that the effect of visual modality correlates most with\ndenotational semantic properties related to concreteness, but is also detected\nfor several specific semantic classes, as well as for valence, a\nsentiment-related connotational property of linguistic expressions.", "published": "2023-06-04 12:53:12", "link": "http://arxiv.org/abs/2306.02348v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Are you telling me to put glasses on the dog?'' Content-Grounded\n  Annotation of Instruction Clarification Requests in the CoDraw Dataset", "abstract": "Instruction Clarification Requests are a mechanism to solve communication\nproblems, which is very functional in instruction-following interactions.\nRecent work has argued that the CoDraw dataset is a valuable source of\nnaturally occurring iCRs. Beyond identifying when iCRs should be made, dialogue\nmodels should also be able to generate them with suitable form and content. In\nthis work, we introduce CoDraw-iCR (v2), extending the existing iCR identifiers\nwith fine-grained information grounded in the underlying dialogue game items\nand possible actions. Our annotation can serve to model and evaluate repair\ncapabilities of dialogue agents.", "published": "2023-06-04 15:23:16", "link": "http://arxiv.org/abs/2306.02377v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Information-Theoretic Analysis of Self-supervised Discrete\n  Representations of Speech", "abstract": "Self-supervised representation learning for speech often involves a\nquantization step that transforms the acoustic input into discrete units.\nHowever, it remains unclear how to characterize the relationship between these\ndiscrete units and abstract phonetic categories such as phonemes. In this\npaper, we develop an information-theoretic framework whereby we represent each\nphonetic category as a distribution over discrete units. We then apply our\nframework to two different self-supervised models (namely wav2vec 2.0 and XLSR)\nand use American English speech as a case study. Our study demonstrates that\nthe entropy of phonetic distributions reflects the variability of the\nunderlying speech sounds, with phonetically similar sounds exhibiting similar\ndistributions. While our study confirms the lack of direct, one-to-one\ncorrespondence, we find an intriguing, indirect relationship between phonetic\ncategories and discrete units.", "published": "2023-06-04 16:52:11", "link": "http://arxiv.org/abs/2306.02405v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating and Improving Tool-Augmented Computation-Intensive Math\n  Reasoning", "abstract": "Chain-of-thought prompting~(CoT) and tool augmentation have been validated in\nrecent work as effective practices for improving large language models~(LLMs)\nto perform step-by-step reasoning on complex math-related tasks. However, most\nexisting math reasoning datasets may be not able to fully evaluate and analyze\nthe ability of LLMs in manipulating tools and performing reasoning, as they may\nonly require very few invocations of tools or miss annotations for evaluating\nintermediate reasoning steps. To address the issue, we construct \\textbf{CARP},\na new Chinese dataset consisting of 4,886 computation-intensive algebra\nproblems with formulated annotations on intermediate steps. In CARP, we test\nfour LLMs with CoT prompting, and find that they are all prone to make mistakes\nat the early steps of the solution, leading to wrong answers. Based on this\nfinding, we propose a new approach that can deliberate the reasoning steps with\ntool interfaces, namely \\textbf{DELI}. In DELI, we first initialize a\nstep-by-step solution based on retrieved exemplars, then iterate two\ndeliberation procedures that check and refine the intermediate steps of the\ngenerated solution, from the perspectives of tool manipulation and natural\nlanguage reasoning, until obtaining converged solutions or reaching the maximum\nturn. Experimental results on CARP and six other datasets show that the\nproposed DELI mostly outperforms competitive baselines, and can further boost\nthe performance of existing CoT methods. Our data and code are available in\n\\url{https://github.com/RUCAIBox/CARP}.", "published": "2023-06-04 17:02:59", "link": "http://arxiv.org/abs/2306.02408v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Cross-Cultural Pragmatic Inference with Codenames Duet", "abstract": "Pragmatic reference enables efficient interpersonal communication. Prior work\nuses simple reference games to test models of pragmatic reasoning, often with\nunidentified speakers and listeners. In practice, however, speakers'\nsociocultural background shapes their pragmatic assumptions. For example,\nreaders of this paper assume NLP refers to \"Natural Language Processing,\" and\nnot \"Neuro-linguistic Programming.\" This work introduces the Cultural Codes\ndataset, which operationalizes sociocultural pragmatic inference in a simple\nword reference game.\n  Cultural Codes is based on the multi-turn collaborative two-player game,\nCodenames Duet. Our dataset consists of 794 games with 7,703 turns, distributed\nacross 153 unique players. Alongside gameplay, we collect information about\nplayers' personalities, values, and demographics. Utilizing theories of\ncommunication and pragmatics, we predict each player's actions via joint\nmodeling of their sociocultural priors and the game context. Our experiments\nshow that accounting for background characteristics significantly improves\nmodel performance for tasks related to both clue giving and guessing,\nindicating that sociocultural priors play a vital role in gameplay decisions.", "published": "2023-06-04 20:47:07", "link": "http://arxiv.org/abs/2306.02475v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RadLing: Towards Efficient Radiology Report Understanding", "abstract": "Most natural language tasks in the radiology domain use language models\npre-trained on biomedical corpus. There are few pretrained language models\ntrained specifically for radiology, and fewer still that have been trained in a\nlow data setting and gone on to produce comparable results in fine-tuning\ntasks. We present RadLing, a continuously pretrained language model using\nElectra-small (Clark et al., 2020) architecture, trained using over 500K\nradiology reports, that can compete with state-of-the-art results for fine\ntuning tasks in radiology domain. Our main contribution in this paper is\nknowledge-aware masking which is a taxonomic knowledge-assisted pretraining\ntask that dynamically masks tokens to inject knowledge during pretraining. In\naddition, we also introduce an knowledge base-aided vocabulary extension to\nadapt the general tokenization vocabulary to radiology domain.", "published": "2023-06-04 21:53:04", "link": "http://arxiv.org/abs/2306.02492v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extract and Attend: Improving Entity Translation in Neural Machine\n  Translation", "abstract": "While Neural Machine Translation(NMT) has achieved great progress in recent\nyears, it still suffers from inaccurate translation of entities (e.g.,\nperson/organization name, location), due to the lack of entity training\ninstances. When we humans encounter an unknown entity during translation, we\nusually first look up in a dictionary and then organize the entity translation\ntogether with the translations of other parts to form a smooth target sentence.\nInspired by this translation process, we propose an Extract-and-Attend approach\nto enhance entity translation in NMT, where the translation candidates of\nsource entities are first extracted from a dictionary and then attended to by\nthe NMT model to generate the target sentence. Specifically, the translation\ncandidates are extracted by first detecting the entities in a source sentence\nand then translating the entities through looking up in a dictionary. Then, the\nextracted candidates are added as a prefix of the decoder input to be attended\nto by the decoder when generating the target sentence through self-attention.\nExperiments conducted on En-Zh and En-Ru demonstrate that the proposed method\nis effective on improving both the translation accuracy of entities and the\noverall translation quality, with up to 35% reduction on entity error rate and\n0.85 gain on BLEU and 13.8 gain on COMET.", "published": "2023-06-04 03:05:25", "link": "http://arxiv.org/abs/2306.02242v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sen2Pro: A Probabilistic Perspective to Sentence Embedding from\n  Pre-trained Language Model", "abstract": "Sentence embedding is one of the most fundamental tasks in Natural Language\nProcessing and plays an important role in various tasks. The recent\nbreakthrough in sentence embedding is achieved by pre-trained language models\n(PLMs). Despite its success, an embedded vector (Sen2Vec) representing a point\nestimate does not naturally express uncertainty in a taskagnostic way. This\npaper thereby proposes an efficient framework on probabilistic sentence\nembedding (Sen2Pro) from PLMs, and it represents a sentence as a probability\ndensity distribution in an embedding space to reflect both model uncertainty\nand data uncertainty (i.e., many-to-one nature) in the sentence representation.\nThe proposed framework performs in a plug-and-play way without retraining PLMs\nanymore, and it is easy to implement and generally applied on top of any PLM.\nThe superiority of Sen2Pro over Sen2Vec has been theoretically verified and\npractically illustrated on different NLP tasks.", "published": "2023-06-04 03:26:43", "link": "http://arxiv.org/abs/2306.02247v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Model Augmented Narrative Driven Recommendations", "abstract": "Narrative-driven recommendation (NDR) presents an information access problem\nwhere users solicit recommendations with verbose descriptions of their\npreferences and context, for example, travelers soliciting recommendations for\npoints of interest while describing their likes/dislikes and travel\ncircumstances. These requests are increasingly important with the rise of\nnatural language-based conversational interfaces for search and recommendation\nsystems. However, NDR lacks abundant training data for models, and current\nplatforms commonly do not support these requests. Fortunately, classical\nuser-item interaction datasets contain rich textual data, e.g., reviews, which\noften describe user preferences and context - this may be used to bootstrap\ntraining for NDR models. In this work, we explore using large language models\n(LLMs) for data augmentation to train NDR models. We use LLMs for authoring\nsynthetic narrative queries from user-item interactions with few-shot prompting\nand train retrieval models for NDR on synthetic queries and user-item\ninteraction data. Our experiments demonstrate that this is an effective\nstrategy for training small-parameter retrieval models that outperform other\nretrieval and LLM baselines for narrative-driven recommendation.", "published": "2023-06-04 03:46:45", "link": "http://arxiv.org/abs/2306.02250v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Exploring and Verbalizing Academic Ideas by Concept Co-occurrence", "abstract": "Researchers usually come up with new ideas only after thoroughly\ncomprehending vast quantities of literature. The difficulty of this procedure\nis exacerbated by the fact that the number of academic publications is growing\nexponentially. In this study, we devise a framework based on concept\nco-occurrence for academic idea inspiration, which has been integrated into a\nresearch assistant system. From our perspective, the fusion of two concepts\nthat co-occur in an academic paper can be regarded as an important way of the\nemergence of a new idea. We construct evolving concept graphs according to the\nco-occurrence relationship of concepts from 20 disciplines or topics. Then we\ndesign a temporal link prediction method based on masked language model to\nexplore potential connections between different concepts. To verbalize the\nnewly discovered connections, we also utilize the pretrained language model to\ngenerate a description of an idea based on a new data structure called\nco-occurrence citation quintuple. We evaluate our proposed system using both\nautomatic metrics and human assessment. The results demonstrate that our system\nhas broad prospects and can assist researchers in expediting the process of\ndiscovering new ideas.", "published": "2023-06-04 07:01:30", "link": "http://arxiv.org/abs/2306.02282v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Mathematical Abstraction for Balancing the Trade-off Between\n  Creativity and Reality in Large Language Models", "abstract": "Large Language Models have become popular for their remarkable capabilities\nin human-oriented tasks and traditional natural language processing tasks. Its\nefficient functioning is attributed to the attention mechanism in the\nTransformer architecture, enabling it to concentrate on particular aspects of\nthe input.\n  LLMs are increasingly being used in domains such as generating prose, poetry\nor art, which require the model to be creative (e.g. Adobe firefly). LLMs\npossess advanced language generation abilities that enable them to generate\ndistinctive and captivating content. This utilization of LLMs in generating\nnarratives shows their flexibility and potential for use in domains that extend\nbeyond conventional natural language processing duties.\n  In different contexts, we may expect the LLM to generate factually correct\nanswers, that match reality; e.g., question-answering systems or online\nassistants. In such situations, being correct is critical to LLMs being trusted\nin practice. The Bing Chatbot provides its users with the flexibility to select\none of the three output modes: creative, balanced, and precise. Each mode\nemphasizes creativity and factual accuracy differently.\n  In this work, we provide a mathematical abstraction to describe creativity\nand reality based on certain losses. A model trained on these losses balances\nthe trade-off between the creativity and reality of the model.", "published": "2023-06-04 08:12:34", "link": "http://arxiv.org/abs/2306.02295v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring the Impact of Model Scaling on Parameter-Efficient Tuning", "abstract": "Parameter-efficient tuning (PET) methods can effectively drive extremely\nlarge pre-trained language models (PLMs) by training only minimal parameters.\nDifferent PET methods utilize different manually designed tunable modules. In\nsmall PLMs, there are usually noticeable performance differences among PET\nmethods. Nevertheless, as the model scale increases, the performance\ndifferences become marginal. Hence, we hypothesize that model scaling mitigates\nthe impact of design differences on PET methods. To investigate this\nhypothesis, we introduce a more flexible PET method called Arbitrary PET (APET)\nmethod. The APET method is compatible with a tunable module, which consists of\nany number of parameters distributed in arbitrary positions. Then, we utilize\nit and conduct experiments on 11 NLP tasks across 3 representative PLMs. Our\ninvestigations reveal that model scaling (1) mitigates the effects of the\npositions of tunable parameters on performance, and (2) enables tuning methods\nto achieve performance comparable to full-parameter fine-tuning by optimizing\nfewer tunable parameters. Intriguingly, we also observe that tuning methods\noptimize the similar number of tunable parameters to exceed random guess\nperformance on different tasks. We collectively discuss this phenomenon and the\ntwo aforementioned findings from an optimization perspective to understand the\nunderlying mechanisms. These conclusions enhance our understanding of the\nimpact of model scaling on PET and assist in designing more effective and\nefficient PET methods for PLMs of different scales. The source code can be\nobtained from this GitHub repository:\n\\url{https://github.com/yushengsu-thu/PET_Scaling}.", "published": "2023-06-04 10:10:54", "link": "http://arxiv.org/abs/2306.02320v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Modular Transformers: Compressing Transformers into Modularized Layers\n  for Flexible Efficient Inference", "abstract": "Pre-trained Transformer models like T5 and BART have advanced the state of\nthe art on a wide range of text generation tasks. Compressing these models into\nsmaller ones has become critically important for practical use. Common neural\nnetwork compression techniques such as knowledge distillation or quantization\nare limited to static compression where the compression ratio is fixed. In this\npaper, we introduce Modular Transformers, a modularized encoder-decoder\nframework for flexible sequence-to-sequence model compression. Modular\nTransformers train modularized layers that have the same function of two or\nmore consecutive layers in the original model via module replacing and\nknowledge distillation. After training, the modularized layers can be flexibly\nassembled into sequence-to-sequence models that meet different\nperformance-efficiency trade-offs. Experimental results show that after a\nsingle training phase, by simply varying the assembling strategy, Modular\nTransformers can achieve flexible compression ratios from 1.1x to 6x with\nlittle to moderate relative performance drop.", "published": "2023-06-04 15:26:28", "link": "http://arxiv.org/abs/2306.02379v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Taught by the Internet, Exploring Bias in OpenAIs GPT3", "abstract": "This research delves into the current literature on bias in Natural Language\nProcessing Models and the techniques proposed to mitigate the problem of bias,\nincluding why it is important to tackle bias in the first place. Additionally,\nthese techniques are further analysed in the light of newly developed models\nthat tower in size over past editions. To achieve those aims, the authors of\nthis paper conducted their research on GPT3 by OpenAI, the largest NLP model\navailable to consumers today. With 175 billion parameters in contrast to BERTs\n340 million, GPT3 is the perfect model to test the common pitfalls of NLP\nmodels. Tests were conducted through the development of an Applicant Tracking\nSystem using GPT3. For the sake of feasibility and time constraints, the tests\nprimarily focused on gender bias, rather than all or multiple types of bias.\nFinally, current mitigation techniques are considered and tested to measure\ntheir degree of functionality.", "published": "2023-06-04 18:21:44", "link": "http://arxiv.org/abs/2306.02428v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adaptive and Personalized Exercise Generation for Online Language\n  Learning", "abstract": "Adaptive learning aims to provide customized educational activities (e.g.,\nexercises) to address individual learning needs. However, manual construction\nand delivery of such activities is a laborious process. Thus, in this paper, we\nstudy a novel task of adaptive and personalized exercise generation for online\nlanguage learning. To this end, we combine a knowledge tracing model that\nestimates each student's evolving knowledge states from their learning history\nand a controlled text generation model that generates exercise sentences based\non the student's current estimated knowledge state and instructor requirements\nof desired properties (e.g., domain knowledge and difficulty). We train and\nevaluate our model on real-world learner interaction data from Duolingo and\ndemonstrate that LMs guided by student states can generate superior exercises.\nThen, we discuss the potential use of our model in educational applications\nusing various simulations. These simulations show that our model can adapt to\nstudents' individual abilities and can facilitate their learning efficiency by\npersonalizing learning sequences.", "published": "2023-06-04 20:18:40", "link": "http://arxiv.org/abs/2306.02457v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "End-to-End Joint Target and Non-Target Speakers ASR", "abstract": "This paper proposes a novel automatic speech recognition (ASR) system that\ncan transcribe individual speaker's speech while identifying whether they are\ntarget or non-target speakers from multi-talker overlapped speech.\nTarget-speaker ASR systems are a promising way to only transcribe a target\nspeaker's speech by enrolling the target speaker's information. However, in\nconversational ASR applications, transcribing both the target speaker's speech\nand non-target speakers' ones is often required to understand interactive\ninformation. To naturally consider both target and non-target speakers in a\nsingle ASR model, our idea is to extend autoregressive modeling-based\nmulti-talker ASR systems to utilize the enrollment speech of the target\nspeaker. Our proposed ASR is performed by recursively generating both textual\ntokens and tokens that represent target or non-target speakers. Our experiments\ndemonstrate the effectiveness of our proposed method.", "published": "2023-06-04 06:38:15", "link": "http://arxiv.org/abs/2306.02273v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Exposing Bias in Online Communities through Large-Scale Language Models", "abstract": "Progress in natural language generation research has been shaped by the\never-growing size of language models. While large language models pre-trained\non web data can generate human-sounding text, they also reproduce social biases\nand contribute to the propagation of harmful stereotypes. This work utilises\nthe flaw of bias in language models to explore the biases of six different\nonline communities. In order to get an insight into the communities'\nviewpoints, we fine-tune GPT-Neo 1.3B with six social media datasets. The bias\nof the resulting models is evaluated by prompting the models with different\ndemographics and comparing the sentiment and toxicity values of these\ngenerations. Together, these methods reveal that bias differs in type and\nintensity for the various models. This work not only affirms how easily bias is\nabsorbed from training data but also presents a scalable method to identify and\ncompare the bias of different datasets or communities. Additionally, the\nexamples generated for this work demonstrate the limitations of using automated\nsentiment and toxicity classifiers in bias research.", "published": "2023-06-04 08:09:26", "link": "http://arxiv.org/abs/2306.02294v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference\n  in Low Resource Settings", "abstract": "Adaptive inference is a simple method for reducing inference costs. The\nmethod works by maintaining multiple classifiers of different capacities, and\nallocating resources to each test instance according to its difficulty. In this\nwork, we compare the two main approaches for adaptive inference, Early-Exit and\nMulti-Model, when training data is limited. First, we observe that for models\nwith the same architecture and size, individual Multi-Model classifiers\noutperform their Early-Exit counterparts by an average of 2.3%. We show that\nthis gap is caused by Early-Exit classifiers sharing model parameters during\ntraining, resulting in conflicting gradient updates of model weights. We find\nthat despite this gap, Early-Exit still provides a better speed-accuracy\ntrade-off due to the overhead of the Multi-Model approach. To address these\nissues, we propose SWEET (Separating Weights in Early Exit Transformers), an\nEarly-Exit fine-tuning method that assigns each classifier its own set of\nunique model weights, not updated by other classifiers. We compare SWEET's\nspeed-accuracy curve to standard Early-Exit and Multi-Model baselines and find\nthat it outperforms both methods at fast speeds while maintaining comparable\nscores to Early-Exit at slow speeds. Moreover, SWEET individual classifiers\noutperform Early-Exit ones by 1.1% on average. SWEET enjoys the benefits of\nboth methods, paving the way for further reduction of inference costs in NLP.", "published": "2023-06-04 09:16:39", "link": "http://arxiv.org/abs/2306.02307v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SpellMapper: A non-autoregressive neural spellchecker for ASR\n  customization with candidate retrieval based on n-gram mappings", "abstract": "Contextual spelling correction models are an alternative to shallow fusion to\nimprove automatic speech recognition (ASR) quality given user vocabulary. To\ndeal with large user vocabularies, most of these models include candidate\nretrieval mechanisms, usually based on minimum edit distance between fragments\nof ASR hypothesis and user phrases. However, the edit-distance approach is\nslow, non-trainable, and may have low recall as it relies only on common\nletters. We propose: 1) a novel algorithm for candidate retrieval, based on\nmisspelled n-gram mappings, which gives up to 90% recall with just the top 10\ncandidates on Spoken Wikipedia; 2) a non-autoregressive neural model based on\nBERT architecture, where the initial transcript and ten candidates are combined\ninto one input. The experiments on Spoken Wikipedia show 21.4% word error rate\nimprovement compared to a baseline ASR system.", "published": "2023-06-04 10:00:12", "link": "http://arxiv.org/abs/2306.02317v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "bgGLUE: A Bulgarian General Language Understanding Evaluation Benchmark", "abstract": "We present bgGLUE(Bulgarian General Language Understanding Evaluation), a\nbenchmark for evaluating language models on Natural Language Understanding\n(NLU) tasks in Bulgarian. Our benchmark includes NLU tasks targeting a variety\nof NLP problems (e.g., natural language inference, fact-checking, named entity\nrecognition, sentiment analysis, question answering, etc.) and machine learning\ntasks (sequence labeling, document-level classification, and regression). We\nrun the first systematic evaluation of pre-trained language models for\nBulgarian, comparing and contrasting results across the nine tasks in the\nbenchmark. The evaluation results show strong performance on sequence labeling\ntasks, but there is a lot of room for improvement for tasks that require more\ncomplex reasoning. We make bgGLUE publicly available together with the\nfine-tuning and the evaluation code, as well as a public leaderboard at\nhttps://bgglue.github.io/, and we hope that it will enable further advancements\nin developing NLU models for Bulgarian.", "published": "2023-06-04 12:54:00", "link": "http://arxiv.org/abs/2306.02349v2", "categories": ["cs.CL", "cs.IR", "cs.LG", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Evolution of Efficient Symbolic Communication Codes", "abstract": "The paper explores how the human natural language structure can be seen as a\nproduct of evolution of inter-personal communication code, targeting\nmaximisation of such culture-agnostic and cross-lingual metrics such as\nanti-entropy, compression factor and cross-split F1 score. The exploration is\ndone as part of a larger unsupervised language learning effort, the attempt is\nmade to perform meta-learning in a space of hyper-parameters maximising F1\nscore based on the \"ground truth\" language structure, by means of maximising\nthe metrics mentioned above. The paper presents preliminary results of\ncross-lingual word-level segmentation tokenisation study for Russian, Chinese\nand English as well as subword segmentation or morphological parsing study for\nEnglish. It is found that language structure form the word-level segmentation\nor tokenisation can be found as driven by all of these metrics, anti-entropy\nbeing more relevant to English and Russian while compression factor more\nspecific for Chinese. The study for subword segmentation or morphological\nparsing on English lexicon has revealed straight connection between the\ncompression been found to be associated with compression factor, while,\nsurprising, the same connection with anti-entropy has turned to be the inverse.", "published": "2023-06-04 15:33:16", "link": "http://arxiv.org/abs/2306.02383v2", "categories": ["cs.CL", "cs.IT", "cs.LG", "math.IT"], "primary_category": "cs.CL"}
{"title": "Commonsense Knowledge Transfer for Pre-trained Language Models", "abstract": "Despite serving as the foundation models for a wide range of NLP benchmarks,\npre-trained language models have shown limited capabilities of acquiring\nimplicit commonsense knowledge from self-supervision alone, compared to\nlearning linguistic and factual knowledge that appear more explicitly in the\nsurface patterns in text. In this work, we introduce commonsense knowledge\ntransfer, a framework to transfer the commonsense knowledge stored in a neural\ncommonsense knowledge model to a general-purpose pre-trained language model. It\nfirst exploits general texts to form queries for extracting commonsense\nknowledge from the neural commonsense knowledge model and then refines the\nlanguage model with two self-supervised objectives: commonsense mask infilling\nand commonsense relation prediction, which align human language with the\nunderlying commonsense knowledge. Empirical results show that our approach\nconsistently improves the model's performance on downstream tasks that require\ncommonsense reasoning. Moreover, we find that the improvement is more\nsignificant in the few-shot setting. This suggests that our approach helps\nlanguage models better transfer to downstream tasks without extensive\nsupervision by injecting commonsense knowledge into their parameters.", "published": "2023-06-04 15:44:51", "link": "http://arxiv.org/abs/2306.02388v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fine-Tuning Language Models with Advantage-Induced Policy Alignment", "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as a reliable\napproach to aligning large language models (LLMs) to human preferences. Among\nthe plethora of RLHF techniques, proximal policy optimization (PPO) is of the\nmost widely used methods. Despite its popularity, however, PPO may suffer from\nmode collapse, instability, and poor sample efficiency. We show that these\nissues can be alleviated by a novel algorithm that we refer to as\nAdvantage-Induced Policy Alignment (APA), which leverages a squared error loss\nfunction based on the estimated advantages. We demonstrate empirically that APA\nconsistently outperforms PPO in language tasks by a large margin, when a\nseparate reward model is employed as the evaluator. In addition, compared with\nPPO, APA offers a more stable form of control over the deviation from the\nmodel's initial policy, ensuring that the model improves its performance\nwithout collapsing to deterministic output. In addition to empirical results,\nwe also provide a theoretical justification supporting the design of our loss\nfunction.", "published": "2023-06-04 01:59:40", "link": "http://arxiv.org/abs/2306.02231v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "primary_category": "cs.CL"}
{"title": "Influence of Lossy Speech Codecs on Hearing-aid, Binaural Sound Source\n  Localisation using DNNs", "abstract": "Hearing aids are typically equipped with multiple microphones to exploit\nspatial information for source localisation and speech enhancement. Especially\nfor hearing aids, a good source localisation is important: it not only guides\nsource separation methods but can also be used to enhance spatial cues,\nincreasing user-awareness of important events in their surroundings. We use a\nstate-of-the-art deep neural network (DNN) to perform binaural\ndirection-of-arrival (DoA) estimation, where the DNN uses information from all\nmicrophones at both ears. However, hearing aids have limited bandwidth to\nexchange this data. Bluetooth low-energy (BLE) is emerging as an attractive\noption to facilitate such data exchange, with the LC3plus codec offering\nseveral bitrate and latency trade-off possibilities. In this paper, we\ninvestigate the effect of such lossy codecs on localisation accuracy.\nSpecifically, we consider two conditions: processing at one ear vs processing\nat a central point, which influences the number of channels that need to be\nencoded. Performance is benchmarked against a baseline that allows full\naudio-exchange - yielding valuable insights into the usage of DNNs under lossy\nencoding. We also extend the Pyroomacoustics library to include hearing-device\nand head-related transfer functions (HD-HRTFs) to suitably train the networks.\nThis can also benefit other researchers in the field.", "published": "2023-06-04 12:34:46", "link": "http://arxiv.org/abs/2306.02344v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "SingNet: A Real-time Singing Voice Beat and Downbeat Tracking System", "abstract": "Singing voice beat and downbeat tracking posses several applications in\nautomatic music production, analysis and manipulation. Among them, some require\nreal-time processing, such as live performance processing and\nauto-accompaniment for singing inputs. This task is challenging owing to the\nnon-trivial rhythmic and harmonic patterns in singing signals. For real-time\nprocessing, it introduces further constraints such as inaccessibility to future\ndata and the impossibility to correct the previous results that are\ninconsistent with the latter ones. In this paper, we introduce the first system\nthat tracks the beats and downbeats of singing voices in real-time.\nSpecifically, we propose a novel dynamic particle filtering approach that\nincorporates offline historical data to correct the online inference by using a\nvariable number of particles. We evaluate the performance on two datasets:\nGTZAN with the separated vocal tracks, and an in-house dataset with the\noriginal vocal stems. Experimental result demonstrates that our proposed\napproach outperforms the baseline by 3-5%.", "published": "2023-06-04 15:09:26", "link": "http://arxiv.org/abs/2306.02372v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "End-To-End Deep Learning-based Adaptation Control for Linear Acoustic\n  Echo Cancellation", "abstract": "The attenuation of acoustic loudspeaker echoes remains to be one of the open\nchallenges to achieve pleasant full-duplex hands free speech communication. In\nmany modern signal enhancement interfaces, this problem is addressed by a\nlinear acoustic echo canceler which subtracts a loudspeaker echo estimate from\nthe recorded microphone signal. To obtain precise echo estimates, the\nparameters of the echo canceler, i.e., the filter coefficients, need to be\nestimated quickly and precisely from the observed loudspeaker and microphone\nsignals. For this a sophisticated adaptation control is required to deal with\nhigh-power double-talk and rapidly track time-varying acoustic environments\nwhich are often faced with portable devices. In this paper, we address this\nproblem by end-to-end deep learning. In particular, we suggest to infer the\nstep-size for a least mean squares frequency-domain adaptive filter update by a\nDeep Neural Network (DNN). Two different step-size inference approaches are\ninvestigated. On the one hand broadband approaches, which use a single DNN to\njointly infer step-sizes for all frequency bands, and on the other hand\nnarrowband methods, which exploit individual DNNs per frequency band. The\ndiscussion of benefits and disadvantages of both approaches leads to a novel\nhybrid approach which shows improved echo cancellation while requiring only\nsmall DNN architectures. Furthermore, we investigate the effect of different\nloss functions, signal feature vectors, and DNN output layer architectures on\nthe echo cancellation performance from which we obtain valuable insights into\nthe general design and functionality of DNN-based adaptation control\nalgorithms.", "published": "2023-06-04 19:44:43", "link": "http://arxiv.org/abs/2306.02450v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Effects of Tonal Coarticulation and Prosodic Positions on Tonal Contours\n  of Low Rising Tones: In the Case of Xiamen Dialect", "abstract": "Few studies have worked on the effects of tonal coarticulation and prosodic\npositions on the low rising tone in Xiamen Dialect. This study addressed such\nan issue. To do so, a new method, the Tonal Contour Analysis in Tonal Triangle,\nwas proposed to measure the subtle curvature of the tonal contour. Findings are\nas follows: (1) The low rising tone in Xiamen Dialect has a tendency towards\nthe falling-rising tone, which is significantly affected by the tonal\ncoarticulation and prosodic positions. (2) The low rising tone presents as a\nfalling-rising tone when preceded by a tone with a high offset, and as a low\nrising tone when preceded by a tone that ends up low. (3) The curvature of the\nlow rising tone is greatest in the sentence-initial position, and is positively\ncorrelated to its own duration.", "published": "2023-06-04 03:49:36", "link": "http://arxiv.org/abs/2306.02251v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
