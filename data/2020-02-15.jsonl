{"title": "Deeper Task-Specificity Improves Joint Entity and Relation Extraction", "abstract": "Multi-task learning (MTL) is an effective method for learning related tasks,\nbut designing MTL models necessitates deciding which and how many parameters\nshould be task-specific, as opposed to shared between tasks. We investigate\nthis issue for the problem of jointly learning named entity recognition (NER)\nand relation extraction (RE) and propose a novel neural architecture that\nallows for deeper task-specificity than does prior work. In particular, we\nintroduce additional task-specific bidirectional RNN layers for both the NER\nand RE tasks and tune the number of shared and task-specific layers separately\nfor different datasets. We achieve state-of-the-art (SOTA) results for both\ntasks on the ADE dataset; on the CoNLL04 dataset, we achieve SOTA results on\nthe NER task and competitive results on the RE task while using an order of\nmagnitude fewer trainable parameters than the current SOTA architecture. An\nablation study confirms the importance of the additional task-specific layers\nfor achieving these results. Our work suggests that previous solutions to joint\nNER and RE undervalue task-specificity and demonstrates the importance of\ncorrectly balancing the number of shared and task-specific parameters for MTL\napproaches in general.", "published": "2020-02-15 18:34:52", "link": "http://arxiv.org/abs/2002.06424v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-Tuning Pretrained Language Models: Weight Initializations, Data\n  Orders, and Early Stopping", "abstract": "Fine-tuning pretrained contextual word embedding models to supervised\ndownstream tasks has become commonplace in natural language processing. This\nprocess, however, is often brittle: even with the same hyperparameter values,\ndistinct random seeds can lead to substantially different results. To better\nunderstand this phenomenon, we experiment with four datasets from the GLUE\nbenchmark, fine-tuning BERT hundreds of times on each while varying only the\nrandom seeds. We find substantial performance increases compared to previously\nreported results, and we quantify how the performance of the best-found model\nvaries as a function of the number of fine-tuning trials. Further, we examine\ntwo factors influenced by the choice of random seed: weight initialization and\ntraining data order. We find that both contribute comparably to the variance of\nout-of-sample performance, and that some weight initializations perform well\nacross all tasks explored. On small datasets, we observe that many fine-tuning\ntrials diverge part of the way through training, and we offer best practices\nfor practitioners to stop training less promising runs early. We publicly\nrelease all of our experimental data, including training and validation scores\nfor 2,100 trials, to encourage further analysis of training dynamics during\nfine-tuning.", "published": "2020-02-15 02:40:10", "link": "http://arxiv.org/abs/2002.06305v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Open Knowledge Enrichment for Long-tail Entities", "abstract": "Knowledge bases (KBs) have gradually become a valuable asset for many AI\napplications. While many current KBs are quite large, they are widely\nacknowledged as incomplete, especially lacking facts of long-tail entities,\ne.g., less famous persons. Existing approaches enrich KBs mainly on completing\nmissing links or filling missing values. However, they only tackle a part of\nthe enrichment problem and lack specific considerations regarding long-tail\nentities. In this paper, we propose a full-fledged approach to knowledge\nenrichment, which predicts missing properties and infers true facts of\nlong-tail entities from the open Web. Prior knowledge from popular entities is\nleveraged to improve every enrichment step. Our experiments on the synthetic\nand real-world datasets and comparison with related work demonstrate the\nfeasibility and superiority of the approach.", "published": "2020-02-15 15:25:44", "link": "http://arxiv.org/abs/2002.06397v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Supervised Phrase-boundary Embeddings", "abstract": "We propose a new word embedding model, called SPhrase, that incorporates\nsupervised phrase information. Our method modifies traditional word embeddings\nby ensuring that all target words in a phrase have exactly the same context. We\ndemonstrate that including this information within a context window produces\nsuperior embeddings for both intrinsic evaluation tasks and downstream\nextrinsic tasks.", "published": "2020-02-15 21:05:07", "link": "http://arxiv.org/abs/2002.06450v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Undersensitivity in Neural Reading Comprehension", "abstract": "Current reading comprehension models generalise well to in-distribution test\nsets, yet perform poorly on adversarially selected inputs. Most prior work on\nadversarial inputs studies oversensitivity: semantically invariant text\nperturbations that cause a model's prediction to change when it should not. In\nthis work we focus on the complementary problem: excessive prediction\nundersensitivity, where input text is meaningfully changed but the model's\nprediction does not, even though it should. We formulate a noisy adversarial\nattack which searches among semantic variations of the question for which a\nmodel erroneously predicts the same answer, and with even higher probability.\nDespite comprising unanswerable questions, both SQuAD2.0 and NewsQA models are\nvulnerable to this attack. This indicates that although accurate, models tend\nto rely on spurious patterns and do not fully consider the information\nspecified in a question. We experiment with data augmentation and adversarial\ntraining as defences, and find that both substantially decrease vulnerability\nto attacks on held out data, as well as held out attack spaces. Addressing\nundersensitivity also improves results on AddSent and AddOneSent, and models\nfurthermore generalise better when facing train/evaluation distribution\nmismatch: they are less prone to overly rely on predictive cues present only in\nthe training set, and outperform a conventional model by as much as 10.9% F1.", "published": "2020-02-15 19:03:36", "link": "http://arxiv.org/abs/2003.04808v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Fake News Detection with Different Models", "abstract": "This is a paper for exploring various different models aiming at developing\nfake news detection models and we had used certain machine learning algorithms\nand we had used pretrained algorithms such as TFIDF and CV and W2V as features\nfor processing textual data.", "published": "2020-02-15 06:15:17", "link": "http://arxiv.org/abs/2003.04978v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "UniVL: A Unified Video and Language Pre-Training Model for Multimodal\n  Understanding and Generation", "abstract": "With the recent success of the pre-training technique for NLP and\nimage-linguistic tasks, some video-linguistic pre-training works are gradually\ndeveloped to improve video-text related downstream tasks. However, most of the\nexisting multimodal models are pre-trained for understanding tasks, leading to\na pretrain-finetune discrepancy for generation tasks. This paper proposes\nUniVL: a Unified Video and Language pre-training model for both multimodal\nunderstanding and generation. It comprises four components, including two\nsingle-modal encoders, a cross encoder, and a decoder with the Transformer\nbackbone. Five objectives, including video-text joint, conditioned masked\nlanguage model (CMLM), conditioned masked frame model (CMFM), video-text\nalignment, and language reconstruction, are designed to train each of the\ncomponents. We further develop two pre-training strategies, stage by stage\npre-training (StagedP) and enhanced video representation (EnhancedV), to make\nthe training process of the UniVL more effective. The pre-train is carried out\non a sizeable instructional video dataset HowTo100M. Experimental results\ndemonstrate that the UniVL can learn strong video-text representation and\nachieves state-of-the-art results on five downstream tasks.", "published": "2020-02-15 10:03:25", "link": "http://arxiv.org/abs/2002.06353v3", "categories": ["cs.CV", "cs.CL", "cs.LG", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Small energy masking for improved neural network training for end-to-end\n  speech recognition", "abstract": "In this paper, we present a Small Energy Masking (SEM) algorithm, which masks\ninputs having values below a certain threshold. More specifically, a\ntime-frequency bin is masked if the filterbank energy in this bin is less than\na certain energy threshold. A uniform distribution is employed to randomly\ngenerate the ratio of this energy threshold to the peak filterbank energy of\neach utterance in decibels. The unmasked feature elements are scaled so that\nthe total sum of the feature values remain the same through this masking\nprocedure. This very simple algorithm shows relatively 11.2 % and 13.5 % Word\nError Rate (WER) improvements on the standard LibriSpeech test-clean and\ntest-other sets over the baseline end-to-end speech recognition system.\nAdditionally, compared to the input dropout algorithm, SEM algorithm shows\nrelatively 7.7 % and 11.6 % improvements on the same LibriSpeech test-clean and\ntest-other sets. With a modified shallow-fusion technique with a Transformer\nLM, we obtained a 2.62 % WER on the LibriSpeech test-clean set and a 7.87 % WER\non the LibriSpeech test-other set.", "published": "2020-02-15 03:36:46", "link": "http://arxiv.org/abs/2002.06312v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Many-to-Many Voice Conversion using Conditional Cycle-Consistent\n  Adversarial Networks", "abstract": "Voice conversion (VC) refers to transforming the speaker characteristics of\nan utterance without altering its linguistic contents. Many works on voice\nconversion require to have parallel training data that is highly expensive to\nacquire. Recently, the cycle-consistent adversarial network (CycleGAN), which\ndoes not require parallel training data, has been applied to voice conversion,\nshowing the state-of-the-art performance. The CycleGAN based voice conversion,\nhowever, can be used only for a pair of speakers, i.e., one-to-one voice\nconversion between two speakers. In this paper, we extend the CycleGAN by\nconditioning the network on speakers. As a result, the proposed method can\nperform many-to-many voice conversion among multiple speakers using a single\ngenerative adversarial network (GAN). Compared to building multiple CycleGANs\nfor each pair of speakers, the proposed method reduces the computational and\nspatial cost significantly without compromising the sound quality of the\nconverted voice. Experimental results using the VCC2018 corpus confirm the\nefficiency of the proposed method.", "published": "2020-02-15 06:03:36", "link": "http://arxiv.org/abs/2002.06328v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
