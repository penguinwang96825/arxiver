{"title": "The COVID That Wasn't: Counterfactual Journalism Using GPT", "abstract": "In this paper, we explore the use of large language models to assess human\ninterpretations of real world events. To do so, we use a language model trained\nprior to 2020 to artificially generate news articles concerning COVID-19 given\nthe headlines of actual articles written during the pandemic. We then compare\nstylistic qualities of our artificially generated corpus with a news corpus, in\nthis case 5,082 articles produced by CBC News between January 23 and May 5,\n2020. We find our artificially generated articles exhibits a considerably more\nnegative attitude towards COVID and a significantly lower reliance on\ngeopolitical framing. Our methods and results hold importance for researchers\nseeking to simulate large scale cultural processes via recent breakthroughs in\ntext generation.", "published": "2022-10-13 00:50:24", "link": "http://arxiv.org/abs/2210.06644v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge-grounded Dialog State Tracking", "abstract": "Knowledge (including structured knowledge such as schema and ontology, and\nunstructured knowledge such as web corpus) is a critical part of dialog\nunderstanding, especially for unseen tasks and domains. Traditionally, such\ndomain-specific knowledge is encoded implicitly into model parameters for the\nexecution of downstream tasks, which makes training inefficient. In addition,\nsuch models are not easily transferable to new tasks with different schemas. In\nthis work, we propose to perform dialog state tracking grounded on knowledge\nencoded externally. We query relevant knowledge of various forms based on the\ndialog context where such information can ground the prediction of dialog\nstates. We demonstrate superior performance of our proposed method over strong\nbaselines, especially in the few-shot learning setting.", "published": "2022-10-13 01:34:08", "link": "http://arxiv.org/abs/2210.06656v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Categorizing Semantic Representations for Neural Machine Translation", "abstract": "Modern neural machine translation (NMT) models have achieved competitive\nperformance in standard benchmarks. However, they have recently been shown to\nsuffer limitation in compositional generalization, failing to effectively learn\nthe translation of atoms (e.g., words) and their semantic composition (e.g.,\nmodification) from seen compounds (e.g., phrases), and thus suffering from\nsignificantly weakened translation performance on unseen compounds during\ninference. We address this issue by introducing categorization to the source\ncontextualized representations. The main idea is to enhance generalization by\nreducing sparsity and overfitting, which is achieved by finding prototypes of\ntoken representations over the training set and integrating their embeddings\ninto the source encoding. Experiments on a dedicated MT dataset (i.e.,\nCoGnition) show that our method reduces compositional generalization error\nrates by 24\\% error reduction. In addition, our conceptually simple method\ngives consistently better results than the Transformer baseline on a range of\ngeneral MT datasets.", "published": "2022-10-13 04:07:08", "link": "http://arxiv.org/abs/2210.06709v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models are few(1)-shot Table Reasoners", "abstract": "Recent literature has shown that large language models (LLMs) are generally\nexcellent few-shot reasoners to solve text reasoning tasks. However, the\ncapability of LLMs on table reasoning tasks is yet to be explored. In this\npaper, we aim at understanding how well LLMs can perform table-related tasks\nwith few-shot in-context learning. Specifically, we evaluated LLMs on popular\ntable QA and fact verification datasets like WikiTableQuestion, FetaQA,\nTabFact, and FEVEROUS and found that LLMs are competent at complex reasoning\nover table structures, though these models are not pre-trained on any table\ncorpus. When combined with `chain of thoughts' prompting, LLMs can achieve very\nstrong performance with only a 1-shot demonstration, even on par with some SoTA\nmodels. We show that LLMs are even more competent at generating comprehensive\nlong-form answers on FetaQA than tuned T5-large. We further manually studied\nthe reasoning chains elicited from LLMs and found that these reasoning chains\nare highly consistent with the underlying semantic form. We believe that LLMs\ncan serve as a simple yet generic baseline for future research. The code and\ndata are released in https://github.com/wenhuchen/TableCoT.", "published": "2022-10-13 04:08:24", "link": "http://arxiv.org/abs/2210.06710v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Low-resource Neural Machine Translation with Cross-modal Alignment", "abstract": "How to achieve neural machine translation with limited parallel data?\nExisting techniques often rely on large-scale monolingual corpora, which is\nimpractical for some low-resource languages. In this paper, we turn to connect\nseveral low-resource languages to a particular high-resource one by additional\nvisual modality. Specifically, we propose a cross-modal contrastive learning\nmethod to learn a shared space for all languages, where both a coarse-grained\nsentence-level objective and a fine-grained token-level one are introduced.\nExperimental results and further analysis show that our method can effectively\nlearn the cross-modal and cross-lingual alignment with a small amount of\nimage-text pairs and achieves significant improvements over the text-only\nbaseline under both zero-shot and few-shot scenarios.", "published": "2022-10-13 04:15:43", "link": "http://arxiv.org/abs/2210.06716v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing Out-of-Domain Language Model Performance from Few Examples", "abstract": "While pretrained language models have exhibited impressive generalization\ncapabilities, they still behave unpredictably under certain domain shifts. In\nparticular, a model may learn a reasoning process on in-domain training data\nthat does not hold for out-of-domain test data. We address the task of\npredicting out-of-domain (OOD) performance in a few-shot fashion: given a few\ntarget-domain examples and a set of models with similar training performance,\ncan we understand how these models will perform on OOD test data? We benchmark\nthe performance on this task when looking at model accuracy on the few-shot\nexamples, then investigate how to incorporate analysis of the models' behavior\nusing feature attributions to better tackle this problem. Specifically, we\nexplore a set of \"factors\" designed to reveal model agreement with certain\npathological heuristics that may indicate worse generalization capabilities. On\ntextual entailment, paraphrase recognition, and a synthetic classification\ntask, we show that attribution-based factors can help rank relative model OOD\nperformance. However, accuracy on a few-shot test set is a surprisingly strong\nbaseline, particularly when the system designer does not have in-depth prior\nknowledge about the domain shift.", "published": "2022-10-13 04:45:26", "link": "http://arxiv.org/abs/2210.06725v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explanations from Large Language Models Make Small Reasoners Better", "abstract": "Integrating free-text explanations to in-context learning of large language\nmodels (LLM) is shown to elicit strong reasoning capabilities along with\nreasonable explanations. In this paper, we consider the problem of leveraging\nthe explanations generated by LLM to improve the training of small reasoners,\nwhich are more favorable in real-production deployment due to their low cost.\nWe systematically explore three explanation generation approaches from LLM and\nutilize a multi-task learning framework to facilitate small models to acquire\nstrong reasoning power together with explanation generation capabilities.\nExperiments on multiple reasoning tasks show that our method can consistently\nand significantly outperform finetuning baselines across different settings,\nand even perform better than finetuning/prompting a 60x larger GPT-3 (175B)\nmodel by up to 9.5% in accuracy. As a side benefit, human evaluation further\nshows that our method can generate high-quality explanations to justify its\npredictions, moving towards the goal of explainable AI.", "published": "2022-10-13 04:50:02", "link": "http://arxiv.org/abs/2210.06726v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Shortcomings of Question Answering Based Factuality Frameworks for Error\n  Localization", "abstract": "Despite recent progress in abstractive summarization, models often generate\nsummaries with factual errors. Numerous approaches to detect these errors have\nbeen proposed, the most popular of which are question answering (QA)-based\nfactuality metrics. These have been shown to work well at predicting\nsummary-level factuality and have potential to localize errors within\nsummaries, but this latter capability has not been systematically evaluated in\npast research. In this paper, we conduct the first such analysis and find that,\ncontrary to our expectations, QA-based frameworks fail to correctly identify\nerror spans in generated summaries and are outperformed by trivial exact match\nbaselines. Our analysis reveals a major reason for such poor localization:\nquestions generated by the QG module often inherit errors from non-factual\nsummaries which are then propagated further into downstream modules. Moreover,\neven human-in-the-loop question generation cannot easily offset these problems.\nOur experiments conclusively show that there exist fundamental issues with\nlocalization using the QA framework which cannot be fixed solely by stronger QA\nand QG models.", "published": "2022-10-13 05:23:38", "link": "http://arxiv.org/abs/2210.06748v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Closed-book Question Generation via Contrastive Learning", "abstract": "Question Generation (QG) is a fundamental NLP task for many downstream\napplications. Recent studies on open-book QG, where supportive answer-context\npairs are provided to models, have achieved promising progress. However,\ngenerating natural questions under a more practical closed-book setting that\nlacks these supporting documents still remains a challenge. In this work, we\npropose a new QG model for this closed-book setting that is designed to better\nunderstand the semantics of long-form abstractive answers and store more\ninformation in its parameters through contrastive learning and an answer\nreconstruction module. Through experiments, we validate the proposed QG model\non both public datasets and a new WikiCQA dataset. Empirical results show that\nthe proposed QG model outperforms baselines in both automatic evaluation and\nhuman evaluation. In addition, we show how to leverage the proposed model to\nimprove existing question-answering systems. These results further indicate the\neffectiveness of our QG model for enhancing closed-book question-answering\ntasks.", "published": "2022-10-13 06:45:46", "link": "http://arxiv.org/abs/2210.06781v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking Long-tail Generalization with Likelihood Splits", "abstract": "In order to reliably process natural language, NLP systems must generalize to\nthe long tail of rare utterances. We propose a method to create challenging\nbenchmarks that require generalizing to the tail of the distribution by\nre-splitting existing datasets. We create 'Likelihood Splits' where examples\nthat are assigned lower likelihood by a pre-trained language model (LM) are\nplaced in the test set, and more likely examples are in the training set. This\nsimple approach can be customized to construct meaningful train-test splits for\na wide range of tasks. Likelihood Splits surface more challenges than random\nsplits: relative error rates of state-of-the-art models increase by 59% for\nsemantic parsing on Spider, 93% for natural language inference on SNLI, and 33%\nfor yes/no question answering on BoolQ, on our splits compared with the\ncorresponding random splits. Moreover, Likelihood Splits create fairer\nbenchmarks than adversarial filtering; when the LM used to create the splits is\nalso employed as the task model, our splits do not unfairly penalize the LM.", "published": "2022-10-13 07:27:14", "link": "http://arxiv.org/abs/2210.06799v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Study on Finding Spans", "abstract": "We present an empirical study on methods for span finding, the selection of\nconsecutive tokens in text for some downstream tasks. We focus on approaches\nthat can be employed in training end-to-end information extraction systems, and\nfind there is no definitive solution without considering task properties, and\nprovide our observations to help with future design choices: 1) a tagging\napproach often yields higher precision while span enumeration and boundary\nprediction provide higher recall; 2) span type information can benefit a\nboundary prediction approach; 3) additional contextualization does not help\nspan finding in most cases.", "published": "2022-10-13 08:15:48", "link": "http://arxiv.org/abs/2210.06824v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking Annotation: Can Language Learners Contribute?", "abstract": "Researchers have traditionally recruited native speakers to provide\nannotations for widely used benchmark datasets. However, there are languages\nfor which recruiting native speakers can be difficult, and it would help to\nfind learners of those languages to annotate the data. In this paper, we\ninvestigate whether language learners can contribute annotations to benchmark\ndatasets. In a carefully controlled annotation experiment, we recruit 36\nlanguage learners, provide two types of additional resources (dictionaries and\nmachine-translated sentences), and perform mini-tests to measure their language\nproficiency. We target three languages, English, Korean, and Indonesian, and\nthe four NLP tasks of sentiment analysis, natural language inference, named\nentity recognition, and machine reading comprehension. We find that language\nlearners, especially those with intermediate or advanced levels of language\nproficiency, are able to provide fairly accurate labels with the help of\nadditional resources. Moreover, we show that data annotation improves learners'\nlanguage proficiency in terms of vocabulary and grammar. One implication of our\nfindings is that broadening the annotation task to include language learners\ncan open up the opportunity to build benchmark datasets for languages for which\nit is difficult to recruit native speakers.", "published": "2022-10-13 08:22:25", "link": "http://arxiv.org/abs/2210.06828v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Algorithms for Weighted Pushdown Automata", "abstract": "Weighted pushdown automata (WPDAs) are at the core of many natural language\nprocessing tasks, like syntax-based statistical machine translation and\ntransition-based dependency parsing. As most existing dynamic programming\nalgorithms are designed for context-free grammars (CFGs), algorithms for PDAs\noften resort to a PDA-to-CFG conversion. In this paper, we develop novel\nalgorithms that operate directly on WPDAs. Our algorithms are inspired by\nLang's algorithm, but use a more general definition of pushdown automaton and\neither reduce the space requirements by a factor of $|\\Gamma|$ (the size of the\nstack alphabet) or reduce the runtime by a factor of more than $|Q|$ (the\nnumber of states). When run on the same class of PDAs as Lang's algorithm, our\nalgorithm is both more space-efficient by a factor of $|\\Gamma|$ and more\ntime-efficient by a factor of $|Q| \\cdot |\\Gamma|$.", "published": "2022-10-13 10:21:31", "link": "http://arxiv.org/abs/2210.06884v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Evaluation of the Plausibility and Faithfulness of Sentiment\n  Analysis Explanations", "abstract": "Current Explainable AI (ExAI) methods, especially in the NLP field, are\nconducted on various datasets by employing different metrics to evaluate\nseveral aspects. The lack of a common evaluation framework is hindering the\nprogress tracking of such methods and their wider adoption. In this work,\ninspired by offline information retrieval, we propose different metrics and\ntechniques to evaluate the explainability of SA models from two angles. First,\nwe evaluate the strength of the extracted \"rationales\" in faithfully explaining\nthe predicted outcome. Second, we measure the agreement between ExAI methods\nand human judgment on a homegrown dataset1 to reflect on the rationales\nplausibility. Our conducted experiments comprise four dimensions: (1) the\nunderlying architectures of SA models, (2) the approach followed by the ExAI\nmethod, (3) the reasoning difficulty, and (4) the homogeneity of the\nground-truth rationales. We empirically demonstrate that anchors explanations\nare more aligned with the human judgment and can be more confident in\nextracting supporting rationales. As can be foreseen, the reasoning complexity\nof sentiment is shown to thwart ExAI methods from extracting supporting\nevidence. Moreover, a remarkable discrepancy is discerned between the results\nof different explainability methods on the various architectures suggesting the\nneed for consolidation to observe enhanced performance. Predominantly,\ntransformers are shown to exhibit better explainability than convolutional and\nrecurrent architectures. Our work paves the way towards designing more\ninterpretable NLP models and enabling a common evaluation ground for their\nrelative strengths and robustness.", "published": "2022-10-13 11:29:17", "link": "http://arxiv.org/abs/2210.06916v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automotive Multilingual Fault Diagnosis", "abstract": "Automated fault diagnosis can facilitate diagnostics assistance, speedier\ntroubleshooting, and better-organised logistics. Currently, AI-based\nprognostics and health management in the automotive industry ignore the textual\ndescriptions of the experienced problems or symptoms. With this study, however,\nwe show that a multilingual pre-trained Transformer can effectively classify\nthe textual claims from a large company with vehicle fleets, despite the task's\nchallenging nature due to the 38 languages and 1,357 classes involved. Overall,\nwe report an accuracy of more than 80% for high-frequency classes and above 60%\nfor above-low-frequency classes, bringing novel evidence that multilingual\nclassification can benefit automotive troubleshooting management.", "published": "2022-10-13 11:33:10", "link": "http://arxiv.org/abs/2210.06918v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentence Ambiguity, Grammaticality and Complexity Probes", "abstract": "It is unclear whether, how and where large pre-trained language models\ncapture subtle linguistic traits like ambiguity, grammaticality and sentence\ncomplexity. We present results of automatic classification of these traits and\ncompare their viability and patterns across representation types. We\ndemonstrate that template-based datasets with surface-level artifacts should\nnot be used for probing, careful comparisons with baselines should be done and\nthat t-SNE plots should not be used to determine the presence of a feature\namong dense vectors representations. We also show how features might be highly\nlocalized in the layers for these models and get lost in the upper layers.", "published": "2022-10-13 11:57:14", "link": "http://arxiv.org/abs/2210.06928v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tone prediction and orthographic conversion for Basaa", "abstract": "In this paper, we present a seq2seq approach for transliterating missionary\nBasaa orthographies into the official orthography. Our model uses pre-trained\nBasaa missionary and official orthography corpora using BERT. Since Basaa is a\nlow-resource language, we have decided to use the mT5 model for our project.\nBefore training our model, we pre-processed our corpora by eliminating\none-to-one correspondences between spellings and unifying characters variably\ncontaining either one to two characters into single-character form. Our best\nmT5 model achieved a CER equal to 12.6747 and a WER equal to 40.1012.", "published": "2022-10-13 12:58:39", "link": "http://arxiv.org/abs/2210.06986v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ComSearch: Equation Searching with Combinatorial Strategy for Solving\n  Math Word Problems with Weak Supervision", "abstract": "Previous studies have introduced a weakly-supervised paradigm for solving\nmath word problems requiring only the answer value annotation. While these\nmethods search for correct value equation candidates as pseudo labels, they\nsearch among a narrow sub-space of the enormous equation space. To address this\nproblem, we propose a novel search algorithm with combinatorial strategy\n\\textbf{ComSearch}, which can compress the search space by excluding\nmathematically equivalent equations. The compression allows the searching\nalgorithm to enumerate all possible equations and obtain high-quality data. We\ninvestigate the noise in the pseudo labels that hold wrong mathematical logic,\nwhich we refer to as the \\textit{false-matching} problem, and propose a ranking\nmodel to denoise the pseudo labels. Our approach holds a flexible framework to\nutilize two existing supervised math word problem solvers to train pseudo\nlabels, and both achieve state-of-the-art performance in the weak supervision\ntask.", "published": "2022-10-13 13:25:22", "link": "http://arxiv.org/abs/2210.07017v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CROP: Zero-shot Cross-lingual Named Entity Recognition with Multilingual\n  Labeled Sequence Translation", "abstract": "Named entity recognition (NER) suffers from the scarcity of annotated\ntraining data, especially for low-resource languages without labeled data.\nCross-lingual NER has been proposed to alleviate this issue by transferring\nknowledge from high-resource languages to low-resource languages via aligned\ncross-lingual representations or machine translation results. However, the\nperformance of cross-lingual NER methods is severely affected by the\nunsatisfactory quality of translation or label projection. To address these\nproblems, we propose a Cross-lingual Entity Projection framework (CROP) to\nenable zero-shot cross-lingual NER with the help of a multilingual labeled\nsequence translation model. Specifically, the target sequence is first\ntranslated into the source language and then tagged by a source NER model. We\nfurther adopt a labeled sequence translation model to project the tagged\nsequence back to the target language and label the target raw sentence.\nUltimately, the whole pipeline is integrated into an end-to-end model by the\nway of self-training. Experimental results on two benchmarks demonstrate that\nour method substantially outperforms the previous strong baseline by a large\nmargin of +3~7 F1 scores and achieves state-of-the-art performance.", "published": "2022-10-13 13:32:36", "link": "http://arxiv.org/abs/2210.07022v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompt-based Connective Prediction Method for Fine-grained Implicit\n  Discourse Relation Recognition", "abstract": "Due to the absence of connectives, implicit discourse relation recognition\n(IDRR) is still a challenging and crucial task in discourse analysis. Most of\nthe current work adopted multi-task learning to aid IDRR through explicit\ndiscourse relation recognition (EDRR) or utilized dependencies between\ndiscourse relation labels to constrain model predictions. But these methods\nstill performed poorly on fine-grained IDRR and even utterly misidentified on\nmost of the few-shot discourse relation classes. To address these problems, we\npropose a novel Prompt-based Connective Prediction (PCP) method for IDRR. Our\nmethod instructs large-scale pre-trained models to use knowledge relevant to\ndiscourse relation and utilizes the strong correlation between connectives and\ndiscourse relation to help the model recognize implicit discourse relations.\nExperimental results show that our method surpasses the current\nstate-of-the-art model and achieves significant improvements on those\nfine-grained few-shot discourse relation. Moreover, our approach is able to be\ntransferred to EDRR and obtain acceptable results. Our code is released in\nhttps://github.com/zh-i9/PCP-for-IDRR.", "published": "2022-10-13 13:47:13", "link": "http://arxiv.org/abs/2210.07032v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Open-World Lottery Ticket Hypothesis for OOD Intent Classification", "abstract": "Most existing methods of Out-of-Domain (OOD) intent classification rely on\nextensive auxiliary OOD corpora or specific training paradigms. However, they\nare underdeveloped in the underlying principle that the models should have\ndifferentiated confidence in In- and Out-of-domain intent. In this work, we\nshed light on the fundamental cause of model overconfidence on OOD and\ndemonstrate that calibrated subnetworks can be uncovered by pruning the\noverparameterized model. Calibrated confidence provided by the subnetwork can\nbetter distinguish In- and Out-of-domain, which can be a benefit for almost all\npost hoc methods. In addition to bringing fundamental insights, we also extend\nthe Lottery Ticket Hypothesis to open-world scenarios. We conduct extensive\nexperiments on four real-world datasets to demonstrate our approach can\nestablish consistent improvements compared with a suite of competitive\nbaselines.", "published": "2022-10-13 14:58:35", "link": "http://arxiv.org/abs/2210.07071v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Query Expansion Using Contextual Clue Sampling with Language Models", "abstract": "Query expansion is an effective approach for mitigating vocabulary mismatch\nbetween queries and documents in information retrieval. One recent line of\nresearch uses language models to generate query-related contexts for expansion.\nAlong this line, we argue that expansion terms from these contexts should\nbalance two key aspects: diversity and relevance. The obvious way to increase\ndiversity is to sample multiple contexts from the language model. However, this\ncomes at the cost of relevance, because there is a well-known tendency of\nmodels to hallucinate incorrect or irrelevant contexts. To balance these two\nconsiderations, we propose a combination of an effective filtering strategy and\nfusion of the retrieved documents based on the generation probability of each\ncontext. Our lexical matching based approach achieves a similar top-5/top-20\nretrieval accuracy and higher top-100 accuracy compared with the\nwell-established dense retrieval model DPR, while reducing the index size by\nmore than 96%. For end-to-end QA, the reader model also benefits from our\nmethod and achieves the highest Exact-Match score against several competitive\nbaselines.", "published": "2022-10-13 15:18:04", "link": "http://arxiv.org/abs/2210.07093v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incorporating Context into Subword Vocabularies", "abstract": "Most current popular subword tokenizers are trained based on word frequency\nstatistics over a corpus, without considering information about co-occurrence\nor context. Nevertheless, the resulting vocabularies are used in language\nmodels' highly contextualized settings. We present SaGe, a tokenizer that\ntailors subwords for their downstream use by baking in the contextualized\nsignal at the vocabulary creation phase. We show that SaGe does a better job\nthan current widespread tokenizers in keeping token contexts cohesive, while\nnot incurring a large price in terms of encoding efficiency or domain\nrobustness. SaGe improves performance on English GLUE classification tasks as\nwell as on NER, and on Inference and NER in Turkish, demonstrating its\nrobustness to language properties such as morphological exponence and\nagglutination.", "published": "2022-10-13 15:22:59", "link": "http://arxiv.org/abs/2210.07095v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "You Can Have Your Data and Balance It Too: Towards Balanced and\n  Efficient Multilingual Models", "abstract": "Multilingual models have been widely used for cross-lingual transfer to\nlow-resource languages. However, the performance on these languages is hindered\nby their underrepresentation in the pretraining data. To alleviate this\nproblem, we propose a novel multilingual training technique based on\nteacher-student knowledge distillation. In this setting, we utilize monolingual\nteacher models optimized for their language. We use those teachers along with\nbalanced (sub-sampled) data to distill the teachers' knowledge into a single\nmultilingual student. Our method outperforms standard training methods in\nlow-resource languages and retrains performance on high-resource languages\nwhile using the same amount of data. If applied widely, our approach can\nincrease the representation of low-resource languages in NLP systems.", "published": "2022-10-13 16:19:36", "link": "http://arxiv.org/abs/2210.07135v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ezCoref: Towards Unifying Annotation Guidelines for Coreference\n  Resolution", "abstract": "Large-scale, high-quality corpora are critical for advancing research in\ncoreference resolution. However, existing datasets vary in their definition of\ncoreferences and have been collected via complex and lengthy guidelines that\nare curated for linguistic experts. These concerns have sparked a growing\ninterest among researchers to curate a unified set of guidelines suitable for\nannotators with various backgrounds. In this work, we develop a\ncrowdsourcing-friendly coreference annotation methodology, ezCoref, consisting\nof an annotation tool and an interactive tutorial. We use ezCoref to\nre-annotate 240 passages from seven existing English coreference datasets\n(spanning fiction, news, and multiple other domains) while teaching annotators\nonly cases that are treated similarly across these datasets. Surprisingly, we\nfind that reasonable quality annotations were already achievable (>90%\nagreement between the crowd and expert annotations) even without extensive\ntraining. On carefully analyzing the remaining disagreements, we identify the\npresence of linguistic cases that our annotators unanimously agree upon but\nlack unified treatments (e.g., generic pronouns, appositives) in existing\ndatasets. We propose the research community should revisit these phenomena when\ncurating future unified annotation guidelines.", "published": "2022-10-13 17:09:59", "link": "http://arxiv.org/abs/2210.07188v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards a Unified Multi-Dimensional Evaluator for Text Generation", "abstract": "Multi-dimensional evaluation is the dominant paradigm for human evaluation in\nNatural Language Generation (NLG), i.e., evaluating the generated text from\nmultiple explainable dimensions, such as coherence and fluency. However,\nautomatic evaluation in NLG is still dominated by similarity-based metrics, and\nwe lack a reliable framework for a more comprehensive evaluation of advanced\nmodels. In this paper, we propose a unified multi-dimensional evaluator UniEval\nfor NLG. We re-frame NLG evaluation as a Boolean Question Answering (QA) task,\nand by guiding the model with different questions, we can use one evaluator to\nevaluate from multiple dimensions. Furthermore, thanks to the unified Boolean\nQA format, we are able to introduce an intermediate learning phase that enables\nUniEval to incorporate external knowledge from multiple related tasks and gain\nfurther improvement. Experiments on three typical NLG tasks show that UniEval\ncorrelates substantially better with human judgments than existing metrics.\nSpecifically, compared to the top-performing unified evaluators, UniEval\nachieves a 23% higher correlation on text summarization, and over 43% on\ndialogue response generation. Also, UniEval demonstrates a strong zero-shot\nlearning ability for unseen evaluation dimensions and tasks. Source code, data\nand all pre-trained evaluators are available on our GitHub repository\n(https://github.com/maszhongming/UniEval).", "published": "2022-10-13 17:17:03", "link": "http://arxiv.org/abs/2210.07197v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Searching for Better Database Queries in the Outputs of Semantic Parsers", "abstract": "The task of generating a database query from a question in natural language\nsuffers from ambiguity and insufficiently precise description of the goal. The\nproblem is amplified when the system needs to generalize to databases unseen at\ntraining. In this paper, we consider the case when, at the test time, the\nsystem has access to an external criterion that evaluates the generated\nqueries. The criterion can vary from checking that a query executes without\nerrors to verifying the query on a set of tests. In this setting, we augment\nneural autoregressive models with a search algorithm that looks for a query\nsatisfying the criterion. We apply our approach to the state-of-the-art\nsemantic parsers and report that it allows us to find many queries passing all\nthe tests on different datasets.", "published": "2022-10-13 17:20:45", "link": "http://arxiv.org/abs/2210.07201v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Best Practices in the Creation and Use of Emotion Lexicons", "abstract": "Words play a central role in how we express ourselves. Lexicons of\nword-emotion associations are widely used in research and real-world\napplications for sentiment analysis, tracking emotions associated with products\nand policies, studying health disorders, tracking emotional arcs of stories,\nand so on. However, inappropriate and incorrect use of these lexicons can lead\nto not just sub-optimal results, but also inferences that are directly harmful\nto people. This paper brings together ideas from Affective Computing and AI\nEthics to present, some of the practical and ethical considerations involved in\nthe creation and use of emotion lexicons -- best practices. The goal is to\nprovide a comprehensive set of relevant considerations, so that readers\n(especially those new to work with emotions) can find relevant information in\none place. We hope this work will facilitate more thoughtfulness when one is\ndeciding on what emotions to work on, how to create an emotion lexicon, how to\nuse an emotion lexicon, how to draw meaningful inferences, and how to judge\nsuccess.", "published": "2022-10-13 17:31:09", "link": "http://arxiv.org/abs/2210.07206v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Saliency Map Verbalization: Comparing Feature Importance Representations\n  from Model-free and Instruction-based Methods", "abstract": "Saliency maps can explain a neural model's predictions by identifying\nimportant input features. They are difficult to interpret for laypeople,\nespecially for instances with many features. In order to make them more\naccessible, we formalize the underexplored task of translating saliency maps\ninto natural language and compare methods that address two key challenges of\nthis approach -- what and how to verbalize. In both automatic and human\nevaluation setups, using token-level attributions from text classification\ntasks, we compare two novel methods (search-based and instruction-based\nverbalizations) against conventional feature importance representations\n(heatmap visualizations and extractive rationales), measuring simulatability,\nfaithfulness, helpfulness and ease of understanding. Instructing GPT-3.5 to\ngenerate saliency map verbalizations yields plausible explanations which\ninclude associations, abstractive summarization and commonsense reasoning,\nachieving by far the highest human ratings, but they are not faithfully\ncapturing numeric information and are inconsistent in their interpretation of\nthe task. In comparison, our search-based, model-free verbalization approach\nefficiently completes templated verbalizations, is faithful by design, but\nfalls short in helpfulness and simulatability. Our results suggest that\nsaliency map verbalization makes feature attribution explanations more\ncomprehensible and less cognitively challenging to humans than conventional\nrepresentations.", "published": "2022-10-13 17:48:15", "link": "http://arxiv.org/abs/2210.07222v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SODAPOP: Open-Ended Discovery of Social Biases in Social Commonsense\n  Reasoning Models", "abstract": "A common limitation of diagnostic tests for detecting social biases in NLP\nmodels is that they may only detect stereotypic associations that are\npre-specified by the designer of the test. Since enumerating all possible\nproblematic associations is infeasible, it is likely these tests fail to detect\nbiases that are present in a model but not pre-specified by the designer. To\naddress this limitation, we propose SODAPOP (SOcial bias Discovery from Answers\nabout PeOPle) in social commonsense question-answering. Our pipeline generates\nmodified instances from the Social IQa dataset (Sap et al., 2019) by (1)\nsubstituting names associated with different demographic groups, and (2)\ngenerating many distractor answers from a masked language model. By using a\nsocial commonsense model to score the generated distractors, we are able to\nuncover the model's stereotypic associations between demographic groups and an\nopen set of words. We also test SODAPOP on debiased models and show the\nlimitations of multiple state-of-the-art debiasing algorithms.", "published": "2022-10-13 18:04:48", "link": "http://arxiv.org/abs/2210.07269v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Task Learning for Joint Semantic Role and Proto-Role Labeling", "abstract": "We put forward an end-to-end multi-step machine learning model which jointly\nlabels semantic roles and the proto-roles of Dowty (1991), given a sentence and\nthe predicates therein. Our best architecture first learns argument spans\nfollowed by learning the argument's syntactic heads. This information is shared\nwith the next steps for predicting the semantic roles and proto-roles. We also\nexperiment with transfer learning from argument and head prediction to role and\nproto-role labeling. We compare using static and contextual embeddings for\nwords, arguments, and sentences. Unlike previous work, our model does not\nrequire pre-training or fine-tuning on additional tasks, beyond using\noff-the-shelf (static or contextual) embeddings and supervision. It also does\nnot require argument spans, their semantic roles, and/or their gold syntactic\nheads as additional input, because it learns to predict all these during\ntraining. Our multi-task learning model raises the state-of-the-art predictions\nfor most proto-roles.", "published": "2022-10-13 18:05:51", "link": "http://arxiv.org/abs/2210.07270v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Demographic Factors Improve Text Classification? Revisiting\n  Demographic Adaptation in the Age of Transformers", "abstract": "Demographic factors (e.g., gender or age) shape our language. Previous work\nshowed that incorporating demographic factors can consistently improve\nperformance for various NLP tasks with traditional NLP models. In this work, we\ninvestigate whether these previous findings still hold with state-of-the-art\npretrained Transformer-based language models (PLMs). We use three common\nspecialization methods proven effective for incorporating external knowledge\ninto pretrained Transformers (e.g., domain-specific or geographic knowledge).\nWe adapt the language representations for the demographic dimensions of gender\nand age, using continuous language modeling and dynamic multi-task learning for\nadaptation, where we couple language modeling objectives with the prediction of\ndemographic classes. Our results, when employing a multilingual PLM, show\nsubstantial gains in task performance across four languages (English, German,\nFrench, and Danish), which is consistent with the results of previous work.\nHowever, controlling for confounding factors - primarily domain and language\nproficiency of Transformer-based PLMs - shows that downstream performance gains\nfrom our demographic adaptation do not actually stem from demographic\nknowledge. Our results indicate that demographic specialization of PLMs, while\nholding promise for positive societal impact, still represents an unsolved\nproblem for (modern) NLP.", "published": "2022-10-13 21:16:27", "link": "http://arxiv.org/abs/2210.07362v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is It Worth the (Environmental) Cost? Limited Evidence for Temporal\n  Adaptation via Continuous Training", "abstract": "Language is constantly changing and evolving, leaving language models to\nbecome quickly outdated. Consequently, we should continuously update our models\nwith new data to expose them to new events and facts. However, that requires\nadditional computing, which means new carbon emissions. Do any measurable\nbenefits justify this cost? This paper looks for empirical evidence to support\ncontinuous training. We reproduce existing benchmarks and extend them to\ninclude additional time periods, models, and tasks. Our results show that the\ndownstream task performance of temporally adapted English models for social\nmedia data do not improve over time. Pretrained models without temporal\nadaptation are actually significantly more effective and efficient. However, we\nalso note a lack of suitable temporal benchmarks. Our findings invite a\ncritical reflection on when and how to temporally adapt language models,\naccounting for sustainability.", "published": "2022-10-13 21:18:55", "link": "http://arxiv.org/abs/2210.07365v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "M2D2: A Massively Multi-domain Language Modeling Dataset", "abstract": "We present M2D2, a fine-grained, massively multi-domain corpus for studying\ndomain adaptation in language models (LMs). M2D2 consists of 8.5B tokens and\nspans 145 domains extracted from Wikipedia and Semantic Scholar. Using\nontologies derived from Wikipedia and ArXiv categories, we organize the domains\nin each data source into 22 groups. This two-level hierarchy enables the study\nof relationships between domains and their effects on in- and out-of-domain\nperformance after adaptation. We also present a number of insights into the\nnature of effective domain adaptation in LMs, as examples of the new types of\nstudies M2D2 enables. To improve in-domain performance, we show the benefits of\nadapting the LM along a domain hierarchy; adapting to smaller amounts of\nfine-grained domain-specific data can lead to larger in-domain performance\ngains than larger amounts of weakly relevant data. We further demonstrate a\ntrade-off between in-domain specialization and out-of-domain generalization\nwithin and across ontologies, as well as a strong correlation between\nout-of-domain performance and lexical overlap between domains.", "published": "2022-10-13 21:34:52", "link": "http://arxiv.org/abs/2210.07370v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mind the Labels: Describing Relations in Knowledge Graphs With\n  Pretrained Models", "abstract": "Pretrained language models (PLMs) for data-to-text (D2T) generation can use\nhuman-readable data labels such as column headings, keys, or relation names to\ngeneralize to out-of-domain examples. However, the models are well-known in\nproducing semantically inaccurate outputs if these labels are ambiguous or\nincomplete, which is often the case in D2T datasets. In this paper, we expose\nthis issue on the task of descibing a relation between two entities. For our\nexperiments, we collect a novel dataset for verbalizing a diverse set of 1,522\nunique relations from three large-scale knowledge graphs (Wikidata, DBPedia,\nYAGO). We find that although PLMs for D2T generation expectedly fail on unclear\ncases, models trained with a large variety of relation labels are surprisingly\nrobust in verbalizing novel, unseen relations. We argue that using data with a\ndiverse set of clear and meaningful labels is key to training D2T generation\nsystems capable of generalizing to novel domains.", "published": "2022-10-13 21:38:52", "link": "http://arxiv.org/abs/2210.07373v3", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Frustratingly Easy Sentiment Analysis of Text Streams: Generating\n  High-Quality Emotion Arcs Using Emotion Lexicons", "abstract": "Automatically generated emotion arcs -- that capture how an individual or a\npopulation feels over time -- are widely used in industry and research.\nHowever, there is little work on evaluating the generated arcs. This is in part\ndue to the difficulty of establishing the true (gold) emotion arc. Our work,\nfor the first time, systematically and quantitatively evaluates automatically\ngenerated emotion arcs. We also compare two common ways of generating emotion\narcs: Machine-Learning (ML) models and Lexicon-Only (LexO) methods. Using a\nnumber of diverse datasets, we systematically study the relationship between\nthe quality of an emotion lexicon and the quality of the emotion arc that can\nbe generated with it. We also study the relationship between the quality of an\ninstance-level emotion detection system (say from an ML model) and the quality\nof emotion arcs that can be generated with it. We show that despite being\nmarkedly poor at instance level, LexO methods are highly accurate at generating\nemotion arcs by aggregating information from hundreds of instances. This has\nwide-spread implications for commercial development, as well as research in\npsychology, public health, digital humanities, etc. that values simple\ninterpretable methods and disprefers the need for domain-specific training\ndata, programming expertise, and high-carbon-footprint models.", "published": "2022-10-13 21:50:54", "link": "http://arxiv.org/abs/2210.07381v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Early Discovery of Disappearing Entities in Microblogs", "abstract": "We make decisions by reacting to changes in the real world, in particular,\nthe emergence and disappearance of impermanent entities such as events,\nrestaurants, and services. Because we want to avoid missing out on\nopportunities or making fruitless actions after they have disappeared, it is\nimportant to know when entities disappear as early as possible. We thus tackle\nthe task of detecting disappearing entities from microblogs, whose posts\nmention various entities, in a timely manner. The major challenge is detecting\nuncertain contexts of disappearing entities from noisy microblog posts. To\ncollect these disappearing contexts, we design time-sensitive distant\nsupervision, which utilizes entities from the knowledge base and time-series\nposts, for this task to build large-scale Twitter datasets\\footnote{We will\nrelease the datasets (tweet IDs) used in the experiments to promote\nreproducibility.} for English and Japanese. To ensure robust detection in noisy\nenvironments, we refine pretrained word embeddings of the detection model on\nmicroblog streams of the target day. Experimental results on the Twitter\ndatasets confirmed the effectiveness of the collected labeled data and refined\nword embeddings; more than 70\\% of the detected disappearing entities in\nWikipedia are discovered earlier than the update on Wikipedia, and the average\nlead-time is over one month.", "published": "2022-10-13 22:52:56", "link": "http://arxiv.org/abs/2210.07404v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LSG Attention: Extrapolation of pretrained Transformers to long\n  sequences", "abstract": "Transformer models achieve state-of-the-art performance on a wide range of\nNLP tasks. They however suffer from a prohibitive limitation due to the\nself-attention mechanism, inducing $O(n^2)$ complexity with regard to sequence\nlength. To answer this limitation we introduce the LSG architecture which\nrelies on Local, Sparse and Global attention. We show that LSG attention is\nfast, efficient and competitive in classification and summarization tasks on\nlong documents. Interestingly, it can also be used to adapt existing pretrained\nmodels to efficiently extrapolate to longer sequences with no additional\ntraining. Along with the introduction of the LSG attention mechanism, we\npropose tools to train new models and adapt existing ones based on this\nmechanism.", "published": "2022-10-13 13:10:41", "link": "http://arxiv.org/abs/2210.15497v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Equi-Tuning: Group Equivariant Fine-Tuning of Pretrained Models", "abstract": "We introduce equi-tuning, a novel fine-tuning method that transforms\n(potentially non-equivariant) pretrained models into group equivariant models\nwhile incurring minimum $L_2$ loss between the feature representations of the\npretrained and the equivariant models. Large pretrained models can be\nequi-tuned for different groups to satisfy the needs of various downstream\ntasks. Equi-tuned models benefit from both group equivariance as an inductive\nbias and semantic priors from pretrained models. We provide applications of\nequi-tuning on three different tasks: image classification, compositional\ngeneralization in language, and fairness in natural language generation (NLG).\nWe also provide a novel group-theoretic definition for fairness in NLG. The\neffectiveness of this definition is shown by testing it against a standard\nempirical method of fairness in NLG. We provide experimental results for\nequi-tuning using a variety of pretrained models: Alexnet, Resnet, VGG, and\nDensenet for image classification; RNNs, GRUs, and LSTMs for compositional\ngeneralization; and GPT2 for fairness in NLG. We test these models on benchmark\ndatasets across all considered tasks to show the generality and effectiveness\nof the proposed method.", "published": "2022-10-13 08:45:23", "link": "http://arxiv.org/abs/2210.06475v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SubeventWriter: Iterative Sub-event Sequence Generation with Coherence\n  Controller", "abstract": "In this paper, we propose a new task of sub-event generation for an unseen\nprocess to evaluate the understanding of the coherence of sub-event actions and\nobjects. To solve the problem, we design SubeventWriter, a sub-event sequence\ngeneration framework with a coherence controller. Given an unseen process, the\nframework can iteratively construct the sub-event sequence by generating one\nsub-event at each iteration. We also design a very effective coherence\ncontroller to decode more coherent sub-events. As our extensive experiments and\nanalysis indicate, SubeventWriter can generate more reliable and meaningful\nsub-event sequences for unseen processes.", "published": "2022-10-13 03:19:01", "link": "http://arxiv.org/abs/2210.06694v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Jointly Reinforced User Simulator and Task-oriented Dialog System with\n  Simplified Generative Architecture", "abstract": "Recently, there has been progress in supervised funetuning pretrained GPT-2\nto build end-to-end task-oriented dialog (TOD) systems. However, online\nreinforcement learning of a GPT-2 based dialog system (DS), together with a\nend-to-end user simulator (US), has not ever been explored. Moreover, a\ndrawback with existing GPT-2 based TOD systems is that they mostly employ the\nwhole dialog history as input, which brings inefficiencies in memory and\ncompute. In this paper, we first propose Simplified Generative Architectures\n(SGA) for DS and US respectively, both based on GPT-2 but using shortened\nhistory. Then, we successfully develop Jointly Reinforced US and DS, called\nSGA-JRUD. Our DS with the proposed SGA, when only supervised trained, achieves\nstate-of-the-art performance on MultiWOZ2.1 and is more compute-efficient in\nboth training and generation. Extensive experiments on MultiWOZ2.1 further show\nthe superiority of SGA-JRUD in both offline and online evaluations.", "published": "2022-10-13 03:57:17", "link": "http://arxiv.org/abs/2210.06706v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LIME: Weakly-Supervised Text Classification Without Seeds", "abstract": "In weakly-supervised text classification, only label names act as sources of\nsupervision. Predominant approaches to weakly-supervised text classification\nutilize a two-phase framework, where test samples are first assigned\npseudo-labels and are then used to train a neural text classifier. In most\nprevious work, the pseudo-labeling step is dependent on obtaining seed words\nthat best capture the relevance of each class label. We present LIME, a\nframework for weakly-supervised text classification that entirely replaces the\nbrittle seed-word generation process with entailment-based\npseudo-classification. We find that combining weakly-supervised classification\nand textual entailment mitigates shortcomings of both, resulting in a more\nstreamlined and effective classification pipeline. With just an off-the-shelf\ntextual entailment model, LIME outperforms recent baselines in\nweakly-supervised text classification and achieves state-of-the-art in 4\nbenchmarks. We open source our code at https://github.com/seongminp/LIME.", "published": "2022-10-13 04:28:28", "link": "http://arxiv.org/abs/2210.06720v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Why self-attention is Natural for Sequence-to-Sequence Problems? A\n  Perspective from Symmetries", "abstract": "In this paper, we show that structures similar to self-attention are natural\nto learn many sequence-to-sequence problems from the perspective of symmetry.\nInspired by language processing applications, we study the orthogonal\nequivariance of seq2seq functions with knowledge, which are functions taking\ntwo inputs -- an input sequence and a ``knowledge'' -- and outputting another\nsequence. The knowledge consists of a set of vectors in the same embedding\nspace as the input sequence, containing the information of the language used to\nprocess the input sequence. We show that orthogonal equivariance in the\nembedding space is natural for seq2seq functions with knowledge, and under such\nequivariance the function must take the form close to the self-attention. This\nshows that network structures similar to self-attention are the right\nstructures to represent the target function of many seq2seq problems. The\nrepresentation can be further refined if a ``finite information principle'' is\nconsidered, or a permutation equivariance holds for the elements of the input\nsequence.", "published": "2022-10-13 05:10:48", "link": "http://arxiv.org/abs/2210.06741v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Mitigating Unintended Memorization in Language Models via Alternating\n  Teaching", "abstract": "Recent research has shown that language models have a tendency to memorize\nrare or unique sequences in the training corpora which can thus leak sensitive\nattributes of user data. We employ a teacher-student framework and propose a\nnovel approach called alternating teaching to mitigate unintended memorization\nin sequential modeling. In our method, multiple teachers are trained on\ndisjoint training sets whose privacy one wishes to protect, and teachers'\npredictions supervise the training of a student model in an alternating manner\nat each time step. Experiments on LibriSpeech datasets show that the proposed\nmethod achieves superior privacy-preserving results than other counterparts. In\ncomparison with no prevention for unintended memorization, the overall utility\nloss is small when training records are sufficient.", "published": "2022-10-13 06:26:41", "link": "http://arxiv.org/abs/2210.06772v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Re3: Generating Longer Stories With Recursive Reprompting and Revision", "abstract": "We consider the problem of automatically generating longer stories of over\ntwo thousand words. Compared to prior work on shorter stories, long-range plot\ncoherence and relevance are more central challenges here. We propose the\nRecursive Reprompting and Revision framework (Re3) to address these challenges\nby (a) prompting a general-purpose language model to construct a structured\noverarching plan, and (b) generating story passages by repeatedly injecting\ncontextual information from both the plan and current story state into a\nlanguage model prompt. We then revise by (c) reranking different continuations\nfor plot coherence and premise relevance, and finally (d) editing the best\ncontinuation for factual consistency. Compared to similar-length stories\ngenerated directly from the same base model, human evaluators judged\nsubstantially more of Re3's stories as having a coherent overarching plot (by\n14% absolute increase), and relevant to the given initial premise (by 20%).", "published": "2022-10-13 06:29:57", "link": "http://arxiv.org/abs/2210.06774v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Ensemble Creation via Anchored Regularization for Unsupervised Aspect\n  Extraction", "abstract": "Aspect Based Sentiment Analysis is the most granular form of sentiment\nanalysis that can be performed on the documents / sentences. Besides delivering\nthe most insights at a finer grain, it also poses equally daunting challenges.\nOne of them being the shortage of labelled data. To bring in value right out of\nthe box for the text data being generated at a very fast pace in today's world,\nunsupervised aspect-based sentiment analysis allows us to generate insights\nwithout investing time or money in generating labels. From topic modelling\napproaches to recent deep learning-based aspect extraction models, this domain\nhas seen a lot of development. One of the models that we improve upon is ABAE\nthat reconstructs the sentences as a linear combination of aspect terms present\nin it, In this research we explore how we can use information from another\nunsupervised model to regularize ABAE, leading to better performance. We\ncontrast it with baseline rule based ensemble and show that the ensemble\nmethods work better than the individual models and the regularization based\nensemble performs better than the rule-based one.", "published": "2022-10-13 08:23:56", "link": "http://arxiv.org/abs/2210.06829v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CS-Insights: A System for Analyzing Computer Science Research", "abstract": "This paper presents CS-Insights, an interactive web application to analyze\ncomputer science publications from DBLP through multiple perspectives. The\ndedicated interfaces allow its users to identify trends in research activity,\nproductivity, accessibility, author's productivity, venues' statistics, topics\nof interest, and the impact of computer science research on other fields.\nCS-Insightsis publicly available, and its modular architecture can be easily\nadapted to domains other than computer science.", "published": "2022-10-13 10:03:52", "link": "http://arxiv.org/abs/2210.06878v2", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "On the Explainability of Natural Language Processing Deep Models", "abstract": "While there has been a recent explosion of work on ExplainableAI ExAI on deep\nmodels that operate on imagery and tabular data, textual datasets present new\nchallenges to the ExAI community. Such challenges can be attributed to the lack\nof input structure in textual data, the use of word embeddings that add to the\nopacity of the models and the difficulty of the visualization of the inner\nworkings of deep models when they are trained on textual data.\n  Lately, methods have been developed to address the aforementioned challenges\nand present satisfactory explanations on Natural Language Processing (NLP)\nmodels. However, such methods are yet to be studied in a comprehensive\nframework where common challenges are properly stated and rigorous evaluation\npractices and metrics are proposed. Motivated to democratize ExAI methods in\nthe NLP field, we present in this work a survey that studies model-agnostic as\nwell as model-specific explainability methods on NLP models. Such methods can\neither develop inherently interpretable NLP models or operate on pre-trained\nmodels in a post-hoc manner. We make this distinction and we further decompose\nthe methods into three categories according to what they explain: (1) word\nembeddings (input-level), (2) inner workings of NLP models (processing-level)\nand (3) models' decisions (output-level). We also detail the different\nevaluation approaches interpretability methods in the NLP field. Finally, we\npresent a case-study on the well-known neural machine translation in an\nappendix and we propose promising future research directions for ExAI in the\nNLP field.", "published": "2022-10-13 11:59:39", "link": "http://arxiv.org/abs/2210.06929v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Differential Bias: On the Perceptibility of Stance Imbalance in\n  Argumentation", "abstract": "Most research on natural language processing treats bias as an absolute\nconcept: Based on a (probably complex) algorithmic analysis, a sentence, an\narticle, or a text is classified as biased or not. Given the fact that for\nhumans the question of whether a text is biased can be difficult to answer or\nis answered contradictory, we ask whether an \"absolute bias classification\" is\na promising goal at all. We see the problem not in the complexity of\ninterpreting language phenomena but in the diversity of sociocultural\nbackgrounds of the readers, which cannot be handled uniformly: To decide\nwhether a text has crossed the proverbial line between non-biased and biased is\nsubjective. By asking \"Is text X more [less, equally] biased than text Y?\" we\npropose to analyze a simpler problem, which, by its construction, is rather\nindependent of standpoints, views, or sociocultural aspects. In such a model,\nbias becomes a preference relation that induces a partial ordering from least\nbiased to most biased texts without requiring a decision on where to draw the\nline. A prerequisite for this kind of bias model is the ability of humans to\nperceive relative bias differences in the first place. In our research, we\nselected a specific type of bias in argumentation, the stance bias, and\ndesigned a crowdsourcing study showing that differences in stance bias are\nperceptible when (light) support is provided through training or visual aid.", "published": "2022-10-13 12:48:07", "link": "http://arxiv.org/abs/2210.06970v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "DICTDIS: Dictionary Constrained Disambiguation for Improved NMT", "abstract": "Domain-specific neural machine translation (NMT) systems (e.g., in\neducational applications) are socially significant with the potential to help\nmake information accessible to a diverse set of users in multilingual\nsocieties. It is desirable that such NMT systems be lexically constrained and\ndraw from domain-specific dictionaries. Dictionaries could present multiple\ncandidate translations for a source word/phrase due to the polysemous nature of\nwords. The onus is then on the NMT model to choose the contextually most\nappropriate candidate. Prior work has largely ignored this problem and focused\non the single candidate constraint setting wherein the target word or phrase is\nreplaced by a single constraint. In this work we present DictDis, a lexically\nconstrained NMT system that disambiguates between multiple candidate\ntranslations derived from dictionaries. We achieve this by augmenting training\ndata with multiple dictionary candidates to actively encourage disambiguation\nduring training by implicitly aligning multiple candidate constraints. We\ndemonstrate the utility of DictDis via extensive experiments on English-Hindi\nand English-German sentences in a variety of domains including regulatory,\nfinance, engineering. We also present comparisons on standard benchmark test\ndatasets. In comparison with existing approaches for lexically constrained and\nunconstrained NMT, we demonstrate superior performance with respect to\nconstraint copy and disambiguation related measures on all domains while also\nobtaining improved fluency of up to 2-3 BLEU points on some domains.", "published": "2022-10-13 13:04:16", "link": "http://arxiv.org/abs/2210.06996v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Spontaneous Emerging Preference in Two-tower Language Model", "abstract": "The ever-growing size of the foundation language model has brought\nsignificant performance gains in various types of downstream tasks. With the\nexistence of side-effects brought about by the large size of the foundation\nlanguage model such as deployment cost, availability issues, and environmental\ncost, there is some interest in exploring other possible directions, such as a\ndivide-and-conquer scheme. In this paper, we are asking a basic question: are\nlanguage processes naturally dividable? We study this problem with a simple\ntwo-tower language model setting, where two language models with identical\nconfigurations are trained side-by-side cooperatively. With this setting, we\ndiscover the spontaneous emerging preference phenomenon, where some of the\ntokens are consistently better predicted by one tower while others by another\ntower. This phenomenon is qualitatively stable, regardless of model\nconfiguration and type, suggesting this as an intrinsic property of natural\nlanguage. This study suggests that interesting properties of natural language\nare still waiting to be discovered, which may aid the future development of\nnatural language processing techniques.", "published": "2022-10-13 13:55:19", "link": "http://arxiv.org/abs/2210.07041v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Scaling Back-Translation with Domain Text Generation for Sign Language\n  Gloss Translation", "abstract": "Sign language gloss translation aims to translate the sign glosses into\nspoken language texts, which is challenging due to the scarcity of labeled\ngloss-text parallel data. Back translation (BT), which generates\npseudo-parallel data by translating in-domain spoken language texts into sign\nglosses, has been applied to alleviate the data scarcity problem. However, the\nlack of large-scale high-quality domain spoken language text data limits the\neffect of BT. In this paper, to overcome the limitation, we propose a Prompt\nbased domain text Generation (PGEN) approach to produce the large-scale\nin-domain spoken language text data. Specifically, PGEN randomly concatenates\nsentences from the original in-domain spoken language text data as prompts to\ninduce a pre-trained language model (i.e., GPT-2) to generate spoken language\ntexts in a similar style. Experimental results on three benchmarks of sign\nlanguage gloss translation in varied languages demonstrate that BT with spoken\nlanguage texts generated by PGEN significantly outperforms the compared\nmethods. In addition, as the scale of spoken language texts generated by PGEN\nincreases, the BT technique can achieve further improvements, demonstrating the\neffectiveness of our approach. We release the code and data for facilitating\nfuture research in this field.", "published": "2022-10-13 14:25:08", "link": "http://arxiv.org/abs/2210.07054v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dungeons and Dragons as a Dialog Challenge for Artificial Intelligence", "abstract": "AI researchers have posited Dungeons and Dragons (D&D) as a challenge problem\nto test systems on various language-related capabilities. In this paper, we\nframe D&D specifically as a dialogue system challenge, where the tasks are to\nboth generate the next conversational turn in the game and predict the state of\nthe game given the dialogue history. We create a gameplay dataset consisting of\nnearly 900 games, with a total of 7,000 players, 800,000 dialogue turns,\n500,000 dice rolls, and 58 million words. We automatically annotate the data\nwith partial state information about the game play. We train a large language\nmodel (LM) to generate the next game turn, conditioning it on different\ninformation. The LM can respond as a particular character or as the player who\nruns the game--i.e., the Dungeon Master (DM). It is trained to produce dialogue\nthat is either in-character (roleplaying in the fictional world) or\nout-of-character (discussing rules or strategy). We perform a human evaluation\nto determine what factors make the generated output plausible and interesting.\nWe further perform an automatic evaluation to determine how well the model can\npredict the game state given the history and examine how well tracking the game\nstate improves its ability to produce plausible conversational output.", "published": "2022-10-13 15:43:39", "link": "http://arxiv.org/abs/2210.07109v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Multi-dimensional Evaluation of Tokenizer-free Multilingual Pretrained\n  Models", "abstract": "Recent work on tokenizer-free multilingual pretrained models show promising\nresults in improving cross-lingual transfer and reducing engineering overhead\n(Clark et al., 2022; Xue et al., 2022). However, these works mainly focus on\nreporting accuracy on a limited set of tasks and data settings, placing less\nemphasis on other important factors when tuning and deploying the models in\npractice, such as memory usage, inference speed, and fine-tuning data\nrobustness. We attempt to fill this gap by performing a comprehensive empirical\ncomparison of multilingual tokenizer-free and subword-based models considering\nthese various dimensions. Surprisingly, we find that subword-based models might\nstill be the most practical choice in many settings, achieving better\nperformance for lower inference latency and memory usage. Based on these\nresults, we encourage future work in tokenizer-free methods to consider these\nfactors when designing and evaluating new models.", "published": "2022-10-13 15:47:09", "link": "http://arxiv.org/abs/2210.07111v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language Models of Code are Few-Shot Commonsense Learners", "abstract": "We address the general task of structured commonsense reasoning: given a\nnatural language input, the goal is to generate a graph such as an event -- or\na reasoning-graph. To employ large language models (LMs) for this task,\nexisting approaches ``serialize'' the output graph as a flat list of nodes and\nedges. Although feasible, these serialized graphs strongly deviate from the\nnatural language corpora that LMs were pre-trained on, hindering LMs from\ngenerating them correctly. In this paper, we show that when we instead frame\nstructured commonsense reasoning tasks as code generation tasks, pre-trained\nLMs of code are better structured commonsense reasoners than LMs of natural\nlanguage, even when the downstream task does not involve source code at all. We\ndemonstrate our approach across three diverse structured commonsense reasoning\ntasks. In all these natural language tasks, we show that using our approach, a\ncode generation LM (CODEX) outperforms natural-LMs that are fine-tuned on the\ntarget task (e.g., T5) and other strong LMs such as GPT-3 in the few-shot\nsetting.", "published": "2022-10-13 16:09:36", "link": "http://arxiv.org/abs/2210.07128v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Counterfactual Multihop QA: A Cause-Effect Approach for Reducing\n  Disconnected Reasoning", "abstract": "Multi-hop QA requires reasoning over multiple supporting facts to answer the\nquestion. However, the existing QA models always rely on shortcuts, e.g.,\nproviding the true answer by only one fact, rather than multi-hop reasoning,\nwhich is referred as $\\textit{disconnected reasoning}$ problem. To alleviate\nthis issue, we propose a novel counterfactual multihop QA, a causal-effect\napproach that enables to reduce the disconnected reasoning. It builds upon\nexplicitly modeling of causality: 1) the direct causal effects of disconnected\nreasoning and 2) the causal effect of true multi-hop reasoning from the total\ncausal effect. With the causal graph, a counterfactual inference is proposed to\ndisentangle the disconnected reasoning from the total causal effect, which\nprovides us a new perspective and technology to learn a QA model that exploits\nthe true multi-hop reasoning instead of shortcuts. Extensive experiments have\nconducted on the benchmark HotpotQA dataset, which demonstrate that the\nproposed method can achieve notable improvement on reducing disconnected\nreasoning. For example, our method achieves 5.8% higher points of its Supp$_s$\nscore on HotpotQA through true multihop reasoning. The code is available at\nsupplementary material.", "published": "2022-10-13 16:21:53", "link": "http://arxiv.org/abs/2210.07138v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "SQuAT: Sharpness- and Quantization-Aware Training for BERT", "abstract": "Quantization is an effective technique to reduce memory footprint, inference\nlatency, and power consumption of deep learning models. However, existing\nquantization methods suffer from accuracy degradation compared to\nfull-precision (FP) models due to the errors introduced by coarse gradient\nestimation through non-differentiable quantization layers. The existence of\nsharp local minima in the loss landscapes of overparameterized models (e.g.,\nTransformers) tends to aggravate such performance penalty in low-bit (2, 4\nbits) settings. In this work, we propose sharpness- and quantization-aware\ntraining (SQuAT), which would encourage the model to converge to flatter minima\nwhile performing quantization-aware training. Our proposed method alternates\ntraining between sharpness objective and step-size objective, which could\npotentially let the model learn the most suitable parameter update magnitude to\nreach convergence near-flat minima. Extensive experiments show that our method\ncan consistently outperform state-of-the-art quantized BERT models under 2, 3,\nand 4-bit settings on GLUE benchmarks by 1%, and can sometimes even outperform\nfull precision (32-bit) models. Our experiments on empirical measurement of\nsharpness also suggest that our method would lead to flatter minima compared to\nother quantization methods.", "published": "2022-10-13 16:52:19", "link": "http://arxiv.org/abs/2210.07171v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "On the Utility of Self-supervised Models for Prosody-related Tasks", "abstract": "Self-Supervised Learning (SSL) from speech data has produced models that have\nachieved remarkable performance in many tasks, and that are known to implicitly\nrepresent many aspects of information latently present in speech signals.\nHowever, relatively little is known about the suitability of such models for\nprosody-related tasks or the extent to which they encode prosodic information.\nWe present a new evaluation framework, SUPERB-prosody, consisting of three\nprosody-related downstream tasks and two pseudo tasks. We find that 13 of the\n15 SSL models outperformed the baseline on all the prosody-related tasks. We\nalso show good performance on two pseudo tasks: prosody reconstruction and\nfuture prosody prediction. We further analyze the layerwise contributions of\nthe SSL models. Overall we conclude that SSL speech models are highly effective\nfor prosody-related tasks.", "published": "2022-10-13 17:06:30", "link": "http://arxiv.org/abs/2210.07185v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Language Model Decoding as Likelihood-Utility Alignment", "abstract": "A critical component of a successful language generation pipeline is the\ndecoding algorithm. However, the general principles that should guide the\nchoice of a decoding algorithm remain unclear. Previous works only compare\ndecoding algorithms in narrow scenarios, and their findings do not generalize\nacross tasks. We argue that the misalignment between the model's likelihood and\nthe task-specific notion of utility is the key factor to understanding the\neffectiveness of decoding algorithms. To structure the discussion, we introduce\na taxonomy of misalignment mitigation strategies (MMSs), providing a unifying\nview of decoding as a tool for alignment. The MMS taxonomy groups decoding\nalgorithms based on their implicit assumptions about likelihood--utility\nmisalignment, yielding general statements about their applicability across\ntasks. Specifically, by analyzing the correlation between the likelihood and\nthe utility of predictions across a diverse set of tasks, we provide empirical\nevidence supporting the proposed taxonomy and a set of principles to structure\nreasoning when choosing a decoding algorithm. Crucially, our analysis is the\nfirst to relate likelihood-based decoding algorithms with algorithms that rely\non external information, such as value-guided methods and prompting, and covers\nthe most diverse set of tasks to date. Code, data, and models are available at\nhttps://github.com/epfl-dlab/understanding-decoding.", "published": "2022-10-13 17:55:51", "link": "http://arxiv.org/abs/2210.07228v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mass-Editing Memory in a Transformer", "abstract": "Recent work has shown exciting promise in updating large language models with\nnew memories, so as to replace obsolete information or add specialized\nknowledge. However, this line of work is predominantly limited to updating\nsingle associations. We develop MEMIT, a method for directly updating a\nlanguage model with many memories, demonstrating experimentally that it can\nscale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B),\nexceeding prior work by orders of magnitude. Our code and data are at\nhttps://memit.baulab.info.", "published": "2022-10-13 17:55:53", "link": "http://arxiv.org/abs/2210.07229v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Joint Reasoning on Hybrid-knowledge sources for Task-Oriented Dialog", "abstract": "Traditional systems designed for task oriented dialog utilize knowledge\npresent only in structured knowledge sources to generate responses. However,\nrelevant information required to generate responses may also reside in\nunstructured sources, such as documents. Recent state of the art models such as\nHyKnow and SeKnow aimed at overcoming these challenges make limiting\nassumptions about the knowledge sources. For instance, these systems assume\nthat certain types of information, such as a phone number, is always present in\na structured knowledge base (KB) while information about aspects such as\nentrance ticket prices, would always be available in documents.\n  In this paper, we create a modified version of the MutliWOZ-based dataset\nprepared by SeKnow to demonstrate how current methods have significant\ndegradation in performance when strict assumptions about the source of\ninformation are removed. Then, in line with recent work exploiting pre-trained\nlanguage models, we fine-tune a BART based model using prompts for the tasks of\nquerying knowledge sources, as well as, for response generation, without making\nassumptions about the information present in each knowledge source. Through a\nseries of experiments, we demonstrate that our model is robust to perturbations\nto knowledge modality (source of information), and that it can fuse information\nfrom structured as well as unstructured knowledge to generate responses.", "published": "2022-10-13 18:49:59", "link": "http://arxiv.org/abs/2210.07295v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bootstrapping Multilingual Semantic Parsers using Large Language Models", "abstract": "Despite cross-lingual generalization demonstrated by pre-trained multilingual\nmodels, the translate-train paradigm of transferring English datasets across\nmultiple languages remains to be a key mechanism for training task-specific\nmultilingual models. However, for many low-resource languages, the availability\nof a reliable translation service entails significant amounts of costly\nhuman-annotated translation pairs. Further, translation services may continue\nto be brittle due to domain mismatch between task-specific input text and\ngeneral-purpose text used for training translation models. For multilingual\nsemantic parsing, we demonstrate the effectiveness and flexibility offered by\nlarge language models (LLMs) for translating English datasets into several\nlanguages via few-shot prompting. Through extensive comparisons on two public\ndatasets, MTOP and MASSIVE, spanning 50 languages and several domains, we show\nthat our method of translating data using LLMs outperforms a strong\ntranslate-train baseline on 41 out of 50 languages. We study the key design\nchoices that enable more effective multilingual data translation via prompted\nLLMs.", "published": "2022-10-13 19:34:14", "link": "http://arxiv.org/abs/2210.07313v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Behavior Cloned Transformers are Neurosymbolic Reasoners", "abstract": "In this work, we explore techniques for augmenting interactive agents with\ninformation from symbolic modules, much like humans use tools like calculators\nand GPS systems to assist with arithmetic and navigation. We test our agent's\nabilities in text games -- challenging benchmarks for evaluating the multi-step\nreasoning abilities of game agents in grounded, language-based environments.\nOur experimental study indicates that injecting the actions from these symbolic\nmodules into the action space of a behavior cloned transformer agent increases\nperformance on four text game benchmarks that test arithmetic, navigation,\nsorting, and common sense reasoning by an average of 22%, allowing an agent to\nreach the highest possible performance on unseen games. This action injection\ntechnique is easily extended to new agents, environments, and symbolic modules.", "published": "2022-10-13 21:54:33", "link": "http://arxiv.org/abs/2210.07382v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Noise Audits Improve Moral Foundation Classification", "abstract": "Morality plays an important role in culture, identity, and emotion. Recent\nadvances in natural language processing have shown that it is possible to\nclassify moral values expressed in text at scale. Morality classification\nrelies on human annotators to label the moral expressions in text, which\nprovides training data to achieve state-of-the-art performance. However, these\nannotations are inherently subjective and some of the instances are hard to\nclassify, resulting in noisy annotations due to error or lack of agreement. The\npresence of noise in training data harms the classifier's ability to accurately\nrecognize moral foundations from text. We propose two metrics to audit the\nnoise of annotations. The first metric is entropy of instance labels, which is\na proxy measure of annotator disagreement about how the instance should be\nlabeled. The second metric is the silhouette coefficient of a label assigned by\nan annotator to an instance. This metric leverages the idea that instances with\nthe same label should have similar latent representations, and deviations from\ncollective judgments are indicative of errors. Our experiments on three widely\nused moral foundations datasets show that removing noisy annotations based on\nthe proposed metrics improves classification performance.", "published": "2022-10-13 23:37:47", "link": "http://arxiv.org/abs/2210.07415v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Cross-domain Variational Capsules for Information Extraction", "abstract": "In this paper, we present a characteristic extraction algorithm and the\nMulti-domain Image Characteristics Dataset of characteristic-tagged images to\nsimulate the way a human brain classifies cross-domain information and\ngenerates insight. The intent was to identify prominent characteristics in data\nand use this identification mechanism to auto-generate insight from data in\nother unseen domains. An information extraction algorithm is proposed which is\na combination of Variational Autoencoders (VAEs) and Capsule Networks. Capsule\nNetworks are used to decompose images into their individual features and VAEs\nare used to explore variations on these decomposed features. Thus, making the\nmodel robust in recognizing characteristics from variations of the data. A\nnoteworthy point is that the algorithm uses efficient hierarchical decoding of\ndata which helps in richer output interpretation. Noticing a dearth in the\nnumber of datasets that contain visible characteristics in images belonging to\nvarious domains, the Multi-domain Image Characteristics Dataset was created and\nmade publicly available. It consists of thousands of images across three\ndomains. This dataset was created with the intent of introducing a new\nbenchmark for fine-grained characteristic recognition tasks in the future.", "published": "2022-10-13 20:04:36", "link": "http://arxiv.org/abs/2210.09053v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Few-shot Relational Reasoning via Connection Subgraph Pretraining", "abstract": "Few-shot knowledge graph (KG) completion task aims to perform inductive\nreasoning over the KG: given only a few support triplets of a new relation\n$\\bowtie$ (e.g., (chop,$\\bowtie$,kitchen), (read,$\\bowtie$,library), the goal\nis to predict the query triplets of the same unseen relation $\\bowtie$, e.g.,\n(sleep,$\\bowtie$,?). Current approaches cast the problem in a meta-learning\nframework, where the model needs to be first jointly trained over many training\nfew-shot tasks, each being defined by its own relation, so that\nlearning/prediction on the target few-shot task can be effective. However, in\nreal-world KGs, curating many training tasks is a challenging ad hoc process.\nHere we propose Connection Subgraph Reasoner (CSR), which can make predictions\nfor the target few-shot task directly without the need for pre-training on the\nhuman curated set of training tasks. The key to CSR is that we explicitly model\na shared connection subgraph between support and query triplets, as inspired by\nthe principle of eliminative induction. To adapt to specific KG, we design a\ncorresponding self-supervised pretraining scheme with the objective of\nreconstructing automatically sampled connection subgraphs. Our pretrained model\ncan then be directly applied to target few-shot tasks on without the need for\ntraining few-shot tasks. Extensive experiments on real KGs, including NELL,\nFB15K-237, and ConceptNet, demonstrate the effectiveness of our framework: we\nshow that even a learning-free implementation of CSR can already perform\ncompetitively to existing methods on target few-shot tasks; with pretraining,\nCSR can achieve significant gains of up to 52% on the more challenging\ninductive few-shot tasks where the entities are also unseen during\n(pre)training.", "published": "2022-10-13 04:35:14", "link": "http://arxiv.org/abs/2210.06722v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SDW-ASL: A Dynamic System to Generate Large Scale Dataset for Continuous\n  American Sign Language", "abstract": "Despite tremendous progress in natural language processing using deep\nlearning techniques in recent years, sign language production and comprehension\nhas advanced very little. One critical barrier is the lack of largescale\ndatasets available to the public due to the unbearable cost of labeled data\ngeneration. Efforts to provide public data for American Sign Language (ASL)\ncomprehension have yielded two datasets, comprising more than thousand video\nclips. These datasets are large enough to enable a meaningful start to deep\nlearning research on sign languages but are far too small to lead to any\nsolution that can be practically deployed. So far, there is still no suitable\ndataset for ASL production. We proposed a system that can generate large scale\nASL datasets for continuous ASL. It is suitable for general ASL processing and\nis particularly useful for ASL production. The continuous ASL dataset contains\nEnglish labeled human articulations in condensed body pose data formats. To\nbetter serve the research community, we are releasing the first version of our\nASL dataset, which contains 30k sentences, 416k words, a vocabulary of 18k\nwords, in a total of 104 hours. This is the largest continuous sign language\ndataset published to date in terms of video duration. We also describe a system\nthat can evolve and expand the dataset to incorporate better data processing\ntechniques and more contents when available. It is our hope that the release of\nthis ASL dataset and the sustainable dataset generation system to the public\nwill propel better deep-learning research in ASL natural language processing.", "published": "2022-10-13 07:08:00", "link": "http://arxiv.org/abs/2210.06791v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Overview of BioASQ 2022: The tenth BioASQ challenge on Large-Scale\n  Biomedical Semantic Indexing and Question Answering", "abstract": "This paper presents an overview of the tenth edition of the BioASQ challenge\nin the context of the Conference and Labs of the Evaluation Forum (CLEF) 2022.\nBioASQ is an ongoing series of challenges that promotes advances in the domain\nof large-scale biomedical semantic indexing and question answering. In this\nedition, the challenge was composed of the three established tasks a, b, and\nSynergy, and a new task named DisTEMIST for automatic semantic annotation and\ngrounding of diseases from clinical content in Spanish, a key concept for\nsemantic indexing and search engines of literature and clinical records. This\nyear, BioASQ received more than 170 distinct systems from 38 teams in total for\nthe four different tasks of the challenge. As in previous years, the majority\nof the competing systems outperformed the strong baselines, indicating the\ncontinuous advancement of the state-of-the-art in this domain.", "published": "2022-10-13 09:04:18", "link": "http://arxiv.org/abs/2210.06852v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Multilingual Zero Resource Speech Recognition Base on Self-Supervise\n  Pre-Trained Acoustic Models", "abstract": "Labeled audio data is insufficient to build satisfying speech recognition\nsystems for most of the languages in the world. There have been some\nzero-resource methods trying to perform phoneme or word-level speech\nrecognition without labeled audio data of the target language, but the error\nrate of these methods is usually too high to be applied in real-world\nscenarios. Recently, the representation ability of self-supervise pre-trained\nmodels has been found to be extremely beneficial in zero-resource phoneme\nrecognition. As far as we are concerned, this paper is the first attempt to\nextend the use of pre-trained models into word-level zero-resource speech\nrecognition. This is done by fine-tuning the pre-trained models on IPA phoneme\ntranscriptions and decoding with a language model trained on extra texts.\nExperiments on Wav2vec 2.0 and HuBERT models show that this method can achieve\nless than 20% word error rate on some languages, and the average error rate on\n8 languages is 33.77%.", "published": "2022-10-13 12:11:18", "link": "http://arxiv.org/abs/2210.06936v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Anonymizing Speech with Generative Adversarial Networks to Preserve\n  Speaker Privacy", "abstract": "In order to protect the privacy of speech data, speaker anonymization aims\nfor hiding the identity of a speaker by changing the voice in speech\nrecordings. This typically comes with a privacy-utility trade-off between\nprotection of individuals and usability of the data for downstream\napplications. One of the challenges in this context is to create non-existent\nvoices that sound as natural as possible.\n  In this work, we propose to tackle this issue by generating speaker\nembeddings using a generative adversarial network with Wasserstein distance as\ncost function. By incorporating these artificial embeddings into a\nspeech-to-text-to-speech pipeline, we outperform previous approaches in terms\nof privacy and utility. According to standard objective metrics and human\nevaluation, our approach generates intelligible and content-preserving yet\nprivacy-protecting versions of the original recordings.", "published": "2022-10-13 13:12:42", "link": "http://arxiv.org/abs/2210.07002v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CLASP: Few-Shot Cross-Lingual Data Augmentation for Semantic Parsing", "abstract": "A bottleneck to developing Semantic Parsing (SP) models is the need for a\nlarge volume of human-labeled training data. Given the complexity and cost of\nhuman annotation for SP, labeled data is often scarce, particularly in\nmultilingual settings. Large Language Models (LLMs) excel at SP given only a\nfew examples, however LLMs are unsuitable for runtime systems which require low\nlatency. In this work, we propose CLASP, a simple method to improve\nlow-resource SP for moderate-sized models: we generate synthetic data from\nAlexaTM 20B to augment the training set for a model 40x smaller (500M\nparameters). We evaluate on two datasets in low-resource settings: English\nPIZZA, containing either 348 or 16 real examples, and mTOP cross-lingual\nzero-shot, where training data is available only in English, and the model must\ngeneralize to four new languages. On both datasets, we show significant\nimprovements over strong baseline methods.", "published": "2022-10-13 15:01:03", "link": "http://arxiv.org/abs/2210.07074v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Challenges in Explanation Quality Evaluation", "abstract": "While much research focused on producing explanations, it is still unclear\nhow the produced explanations' quality can be evaluated in a meaningful way.\nToday's predominant approach is to quantify explanations using proxy scores\nwhich compare explanations to (human-annotated) gold explanations. This\napproach assumes that explanations which reach higher proxy scores will also\nprovide a greater benefit to human users. In this paper, we present problems of\nthis approach. Concretely, we (i) formulate desired characteristics of\nexplanation quality, (ii) describe how current evaluation practices violate\nthem, and (iii) support our argumentation with initial evidence from a\ncrowdsourcing case study in which we investigate the explanation quality of\nstate-of-the-art explainable question answering systems. We find that proxy\nscores correlate poorly with human quality ratings and, additionally, become\nless expressive the more often they are used (i.e. following Goodhart's law).\nFinally, we propose guidelines to enable a meaningful evaluation of\nexplanations to drive the development of systems that provide tangible benefits\nto human users.", "published": "2022-10-13 16:06:59", "link": "http://arxiv.org/abs/2210.07126v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "MAPL: Parameter-Efficient Adaptation of Unimodal Pre-Trained Models for\n  Vision-Language Few-Shot Prompting", "abstract": "Large pre-trained models have proved to be remarkable zero- and\n(prompt-based) few-shot learners in unimodal vision and language tasks. We\npropose MAPL, a simple and parameter-efficient method that reuses frozen\npre-trained unimodal models and leverages their strong generalization\ncapabilities in multimodal vision-language (VL) settings. MAPL learns a\nlightweight mapping between the representation spaces of unimodal models using\naligned image-text data, and can generalize to unseen VL tasks from just a few\nin-context examples. The small number of trainable parameters makes MAPL\neffective at low-data and in-domain learning. Moreover, MAPL's modularity\nenables easy extension to other pre-trained models. Extensive experiments on\nseveral visual question answering and image captioning benchmarks show that\nMAPL achieves superior or competitive performance compared to similar methods\nwhile training orders of magnitude fewer parameters. MAPL can be trained in\njust a few hours using modest computational resources and public datasets. We\nrelease our code and pre-trained model weights at\nhttps://github.com/mair-lab/mapl.", "published": "2022-10-13 17:02:23", "link": "http://arxiv.org/abs/2210.07179v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "On Compressing Sequences for Self-Supervised Speech Models", "abstract": "Compressing self-supervised models has become increasingly necessary, as\nself-supervised models become larger. While previous approaches have primarily\nfocused on compressing the model size, shortening sequences is also effective\nin reducing the computational cost. In this work, we study fixed-length and\nvariable-length subsampling along the time axis in self-supervised learning. We\nexplore how individual downstream tasks are sensitive to input frame rates.\nSubsampling while training self-supervised models not only improves the overall\nperformance on downstream tasks under certain frame rates, but also brings\nsignificant speed-up in inference. Variable-length subsampling performs\nparticularly well under low frame rates. In addition, if we have access to\nphonetic boundaries, we find no degradation in performance for an average frame\nrate as low as 10 Hz.", "published": "2022-10-13 17:10:02", "link": "http://arxiv.org/abs/2210.07189v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Towards Trustworthy Automatic Diagnosis Systems by Emulating Doctors'\n  Reasoning with Deep Reinforcement Learning", "abstract": "The automation of the medical evidence acquisition and diagnosis process has\nrecently attracted increasing attention in order to reduce the workload of\ndoctors and democratize access to medical care. However, most works proposed in\nthe machine learning literature focus solely on improving the prediction\naccuracy of a patient's pathology. We argue that this objective is insufficient\nto ensure doctors' acceptability of such systems. In their initial interaction\nwith patients, doctors do not only focus on identifying the pathology a patient\nis suffering from; they instead generate a differential diagnosis (in the form\nof a short list of plausible diseases) because the medical evidence collected\nfrom patients is often insufficient to establish a final diagnosis. Moreover,\ndoctors explicitly explore severe pathologies before potentially ruling them\nout from the differential, especially in acute care settings. Finally, for\ndoctors to trust a system's recommendations, they need to understand how the\ngathered evidences led to the predicted diseases. In particular, interactions\nbetween a system and a patient need to emulate the reasoning of doctors. We\ntherefore propose to model the evidence acquisition and automatic diagnosis\ntasks using a deep reinforcement learning framework that considers three\nessential aspects of a doctor's reasoning, namely generating a differential\ndiagnosis using an exploration-confirmation approach while prioritizing severe\npathologies. We propose metrics for evaluating interaction quality based on\nthese three aspects. We show that our approach performs better than existing\nmodels while maintaining competitive pathology prediction accuracy.", "published": "2022-10-13 17:17:17", "link": "http://arxiv.org/abs/2210.07198v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MTEB: Massive Text Embedding Benchmark", "abstract": "Text embeddings are commonly evaluated on a small set of datasets from a\nsingle task not covering their possible applications to other tasks. It is\nunclear whether state-of-the-art embeddings on semantic textual similarity\n(STS) can be equally well applied to other tasks like clustering or reranking.\nThis makes progress in the field difficult to track, as various models are\nconstantly being proposed without proper evaluation. To solve this problem, we\nintroduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding\ntasks covering a total of 58 datasets and 112 languages. Through the\nbenchmarking of 33 models on MTEB, we establish the most comprehensive\nbenchmark of text embeddings to date. We find that no particular text embedding\nmethod dominates across all tasks. This suggests that the field has yet to\nconverge on a universal text embedding method and scale it up sufficiently to\nprovide state-of-the-art results on all embedding tasks. MTEB comes with\nopen-source code and a public leaderboard at\nhttps://github.com/embeddings-benchmark/mteb.", "published": "2022-10-13 19:42:08", "link": "http://arxiv.org/abs/2210.07316v3", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Machine Generated Text: A Comprehensive Survey of Threat Models and\n  Detection Methods", "abstract": "Machine generated text is increasingly difficult to distinguish from human\nauthored text. Powerful open-source models are freely available, and\nuser-friendly tools that democratize access to generative models are\nproliferating. ChatGPT, which was released shortly after the first edition of\nthis survey, epitomizes these trends. The great potential of state-of-the-art\nnatural language generation (NLG) systems is tempered by the multitude of\navenues for abuse. Detection of machine generated text is a key countermeasure\nfor reducing abuse of NLG models, with significant technical challenges and\nnumerous open problems. We provide a survey that includes both 1) an extensive\nanalysis of threat models posed by contemporary NLG systems, and 2) the most\ncomplete review of machine generated text detection methods to date. This\nsurvey places machine generated text within its cybersecurity and social\ncontext, and provides strong guidance for future work addressing the most\ncritical threat models, and ensuring detection systems themselves demonstrate\ntrustworthiness through fairness, robustness, and accountability.", "published": "2022-10-13 19:46:14", "link": "http://arxiv.org/abs/2210.07321v4", "categories": ["cs.CL", "cs.CR", "cs.CY", "cs.LG", "I.2.7; K.4.2"], "primary_category": "cs.CL"}
{"title": "Experiments on Turkish ASR with Self-Supervised Speech Representation\n  Learning", "abstract": "While the Turkish language is listed among low-resource languages, literature\non Turkish automatic speech recognition (ASR) is relatively old. In this\nreport, we present our findings on Turkish ASR with speech representation\nlearning using HUBERT. We investigate pre-training HUBERT for Turkish with\nlarge-scale data curated from online resources. We pre-train our model using\n6,500 hours of speech data from YouTube. The results show that the models are\nnot ready for commercial use since they are not robust against disturbances\nthat typically occur in real-world settings such as variations in accents,\nslang, background noise and interference. We analyze typical errors and the\nlimitations of the models for use in commercial settings.", "published": "2022-10-13 19:46:39", "link": "http://arxiv.org/abs/2210.07323v3", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Predicting Fine-Tuning Performance with Probing", "abstract": "Large NLP models have recently shown impressive performance in language\nunderstanding tasks, typically evaluated by their fine-tuned performance.\nAlternatively, probing has received increasing attention as being a lightweight\nmethod for interpreting the intrinsic mechanisms of large NLP models. In\nprobing, post-hoc classifiers are trained on \"out-of-domain\" datasets that\ndiagnose specific abilities. While probing the language models has led to\ninsightful findings, they appear disjointed from the development of models.\nThis paper explores the utility of probing deep NLP models to extract a proxy\nsignal widely used in model development -- the fine-tuning performance. We find\nthat it is possible to use the accuracies of only three probing tests to\npredict the fine-tuning performance with errors $40\\%$ - $80\\%$ smaller than\nbaselines. We further discuss possible avenues where probing can empower the\ndevelopment of deep NLP models.", "published": "2022-10-13 20:58:14", "link": "http://arxiv.org/abs/2210.07352v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "JOIST: A Joint Speech and Text Streaming Model For ASR", "abstract": "We present JOIST, an algorithm to train a streaming, cascaded, encoder\nend-to-end (E2E) model with both speech-text paired inputs, and text-only\nunpaired inputs. Unlike previous works, we explore joint training with both\nmodalities, rather than pre-training and fine-tuning. In addition, we explore\nJOIST using a streaming E2E model with an order of magnitude more data, which\nare also novelties compared to previous works. Through a series of ablation\nstudies, we explore different types of text modeling, including how to model\nthe length of the text sequence and the appropriate text sub-word unit\nrepresentation. We find that best text representation for JOIST improves WER\nacross a variety of search and rare-word test sets by 4-14% relative, compared\nto a model not trained with text. In addition, we quantitatively show that\nJOIST maintains streaming capabilities, which is important for good user-level\nexperience.", "published": "2022-10-13 20:59:22", "link": "http://arxiv.org/abs/2210.07353v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Codes, Patterns and Shapes of Contemporary Online Antisemitism and\n  Conspiracy Narratives -- an Annotation Guide and Labeled German-Language\n  Dataset in the Context of COVID-19", "abstract": "Over the course of the COVID-19 pandemic, existing conspiracy theories were\nrefreshed and new ones were created, often interwoven with antisemitic\nnarratives, stereotypes and codes. The sheer volume of antisemitic and\nconspiracy theory content on the Internet makes data-driven algorithmic\napproaches essential for anti-discrimination organizations and researchers\nalike. However, the manifestation and dissemination of these two interrelated\nphenomena is still quite under-researched in scholarly empirical research of\nlarge text corpora. Algorithmic approaches for the detection and classification\nof specific contents usually require labeled datasets, annotated based on\nconceptually sound guidelines. While there is a growing number of datasets for\nthe more general phenomenon of hate speech, the development of corpora and\nannotation guidelines for antisemitic and conspiracy content is still in its\ninfancy, especially for languages other than English. We contribute to closing\nthis gap by developing an annotation guide for antisemitic and conspiracy\ntheory online content in the context of the COVID-19 pandemic. We provide\nworking definitions, including specific forms of antisemitism such as encoded\nand post-Holocaust antisemitism. We use these to annotate a German-language\ndataset consisting of ~3,700 Telegram messages sent between 03/2020 and\n12/2021.", "published": "2022-10-13 10:32:39", "link": "http://arxiv.org/abs/2210.07934v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Real-Time Automated Answer Scoring", "abstract": "In recent years, the role of big data analytics has exponentially grown and\nis now slowly making its way into the education industry. Several attempts are\nbeing made in this sphere in order to improve the quality of education being\nprovided to students and while many collaborations have been carried out\nbefore, automated scoring of answers has been explored to a rather limited\nextent. One of the biggest hurdles to choosing constructed-response assessments\nover multiple-choice assessments is the effort and large cost that comes with\ntheir evaluation and this is precisely the issue that this project aims to\nsolve. The aim is to accept raw-input from the student in the form of their\nanswer, preprocess the answer, and automatically score the answer. In addition,\nwe have made this a real-time system that captures \"snapshots\" of the writer's\nprogress with respect to the answer, allowing us to unearth trends with respect\nto the way a student thinks, and how the student has arrived at their final\nanswer.", "published": "2022-10-13 20:19:59", "link": "http://arxiv.org/abs/2210.09004v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Towards End-to-End Open Conversational Machine Reading", "abstract": "In open-retrieval conversational machine reading (OR-CMR) task, machines are\nrequired to do multi-turn question answering given dialogue history and a\ntextual knowledge base. Existing works generally utilize two independent\nmodules to approach this problem's two successive sub-tasks: first with a\nhard-label decision making and second with a question generation aided by\nvarious entailment reasoning methods. Such usual cascaded modeling is\nvulnerable to error propagation and prevents the two sub-tasks from being\nconsistently optimized. In this work, we instead model OR-CMR as a unified\ntext-to-text task in a fully end-to-end style. Experiments on the ShARC and\nOR-ShARC dataset show the effectiveness of our proposed end-to-end framework on\nboth sub-tasks by a large margin, achieving new state-of-the-art results.\nFurther ablation studies support that our framework can generalize to different\nbackbone models.", "published": "2022-10-13 15:50:44", "link": "http://arxiv.org/abs/2210.07113v2", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Analysis Method for Metric-Level Switching in Beat Tracking", "abstract": "For expressive music, the tempo may change over time, posing challenges to\ntracking the beats by an automatic model. The model may first tap to the\ncorrect tempo, but then may fail to adapt to a tempo change, or switch between\nseveral incorrect but perceptually plausible ones (e.g., half- or\ndouble-tempo). Existing evaluation metrics for beat tracking do not reflect\nsuch behaviors, as they typically assume a fixed relationship between the\nreference beats and estimated beats. In this paper, we propose a new\nperformance analysis method, called annotation coverage ratio (ACR), that\naccounts for a variety of possible metric-level switching behaviors of beat\ntrackers. The idea is to derive sequences of modified reference beats of all\nmetrical levels for every two consecutive reference beats, and compare every\nsequence of modified reference beats to the subsequences of estimated beats. We\nshow via experiments on three datasets of different genres the usefulness of\nACR when utilized alongside existing metrics, and discuss the new insights to\nbe gained.", "published": "2022-10-13 08:01:35", "link": "http://arxiv.org/abs/2210.06817v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deepfake Detection System for the ADD Challenge Track 3.2 Based on Score\n  Fusion", "abstract": "This paper describes the deepfake audio detection system submitted to the\nAudio Deep Synthesis Detection (ADD) Challenge Track 3.2 and gives an analysis\nof score fusion. The proposed system is a score-level fusion of several light\nconvolutional neural network (LCNN) based models. Various front-ends are used\nas input features, including low-frequency short-time Fourier transform and\nConstant Q transform. Due to the complex noise and rich synthesis algorithms,\nit is difficult to obtain the desired performance using the training set\ndirectly. Online data augmentation methods effectively improve the robustness\nof fake audio detection systems. In particular, the reasons for the poor\nimprovement of score fusion are explored through visualization of the score\ndistributions and comparison with score distribution on another dataset. The\noverfitting of the model to the training set leads to extreme values of the\nscores and low correlation of the score distributions, which makes score fusion\ndifficult. Fusion with partially fake audio detection system improves system\nperformance further. The submission on track 3.2 obtained the weighted equal\nerror rate (WEER) of 11.04\\%, which is one of the best performing systems in\nthe challenge.", "published": "2022-10-13 08:04:29", "link": "http://arxiv.org/abs/2210.06818v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Sparse in Space and Time: Audio-visual Synchronisation with Trainable\n  Selectors", "abstract": "The objective of this paper is audio-visual synchronisation of general videos\n'in the wild'. For such videos, the events that may be harnessed for\nsynchronisation cues may be spatially small and may occur only infrequently\nduring a many seconds-long video clip, i.e. the synchronisation signal is\n'sparse in space and time'. This contrasts with the case of synchronising\nvideos of talking heads, where audio-visual correspondence is dense in both\ntime and space.\n  We make four contributions: (i) in order to handle longer temporal sequences\nrequired for sparse synchronisation signals, we design a multi-modal\ntransformer model that employs 'selectors' to distil the long audio and visual\nstreams into small sequences that are then used to predict the temporal offset\nbetween streams. (ii) We identify artefacts that can arise from the compression\ncodecs used for audio and video and can be used by audio-visual models in\ntraining to artificially solve the synchronisation task. (iii) We curate a\ndataset with only sparse in time and space synchronisation signals; and (iv)\nthe effectiveness of the proposed model is shown on both dense and sparse\ndatasets quantitatively and qualitatively.\n  Project page: v-iashin.github.io/SparseSync", "published": "2022-10-13 14:25:37", "link": "http://arxiv.org/abs/2210.07055v1", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
