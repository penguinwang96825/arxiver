{"title": "Neuromodulation Gated Transformer", "abstract": "We introduce a novel architecture, the Neuromodulation Gated Transformer\n(NGT), which is a simple implementation of neuromodulation in transformers via\na multiplicative effect. We compare it to baselines and show that it results in\nthe best average performance on the SuperGLUE benchmark validation sets.", "published": "2023-05-05 01:23:22", "link": "http://arxiv.org/abs/2305.03232v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VicunaNER: Zero/Few-shot Named Entity Recognition using Vicuna", "abstract": "Large Language Models (LLMs, e.g., ChatGPT) have shown impressive zero- and\nfew-shot capabilities in Named Entity Recognition (NER). However, these models\ncan only be accessed via online APIs, which may cause data leak and\nnon-reproducible problems. In this paper, we propose VicunaNER, a zero/few-shot\nNER framework based on the newly released open-source LLM -- Vicuna. VicunaNER\nis a two-phase framework, where each phase leverages multi-turn dialogues with\nVicuna to recognize entities from texts. We name the second phase as\nRe-Recognition, which recognizes those entities not recognized in the first\nphase (a.k.a. Recognition). Moreover, we set entity correctness check dialogues\nin each phase to filter out wrong entities. We evaluate VicunaNER's zero-shot\ncapacity on 10 datasets crossing 5 domains and few-shot capacity on Few-NERD.\nExperimental results demonstrate that VicunaNER achieves superior performance\nin both shot settings. Additionally, we conduct comprehensive investigations on\nVicuna from multiple perspectives.", "published": "2023-05-05 02:46:22", "link": "http://arxiv.org/abs/2305.03253v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stylized Data-to-Text Generation: A Case Study in the E-Commerce Domain", "abstract": "Existing data-to-text generation efforts mainly focus on generating a\ncoherent text from non-linguistic input data, such as tables and\nattribute-value pairs, but overlook that different application scenarios may\nrequire texts of different styles. Inspired by this, we define a new task,\nnamely stylized data-to-text generation, whose aim is to generate coherent text\nfor the given non-linguistic data according to a specific style. This task is\nnon-trivial, due to three challenges: the logic of the generated text,\nunstructured style reference, and biased training samples. To address these\nchallenges, we propose a novel stylized data-to-text generation model, named\nStyleD2T, comprising three components: logic planning-enhanced data embedding,\nmask-based style embedding, and unbiased stylized text generation. In the first\ncomponent, we introduce a graph-guided logic planner for attribute organization\nto ensure the logic of generated text. In the second component, we devise\nfeature-level mask-based style embedding to extract the essential style signal\nfrom the given unstructured style reference. In the last one, pseudo triplet\naugmentation is utilized to achieve unbiased text generation, and a\nmulti-condition based confidence assignment function is designed to ensure the\nquality of pseudo samples. Extensive experiments on a newly collected dataset\nfrom Taobao have been conducted, and the results show the superiority of our\nmodel over existing methods.", "published": "2023-05-05 03:02:41", "link": "http://arxiv.org/abs/2305.03256v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework", "abstract": "As large language models (LLMs) have become the norm in NLP, demonstrating\ngood performance in generation and reasoning tasks, one of its most fatal\ndisadvantages is the lack of factual correctness. Generating unfactual texts\nnot only leads to lower performances but also degrades the trust and validity\nof their applications. Chain-of-Thought (CoT) prompting improves trust and\nmodel performance on complex reasoning tasks by generating interpretable\nreasoning chains, but still suffers from factuality concerns in\nknowledge-intensive tasks. In this paper, we propose the Verify-and-Edit\nframework for CoT prompting, which seeks to increase prediction factuality by\npost-editing reasoning chains according to external knowledge. Building on top\nof GPT-3, our framework lead to accuracy improvements in multiple open-domain\nquestion-answering tasks.", "published": "2023-05-05 03:49:14", "link": "http://arxiv.org/abs/2305.03268v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Expository Text Generation: Imitate, Retrieve, Paraphrase", "abstract": "Expository documents are vital resources for conveying complex information to\nreaders. Despite their usefulness, writing expository text by hand is a\nchallenging process that requires careful content planning, obtaining facts\nfrom multiple sources, and the ability to clearly synthesize these facts. To\nease these burdens, we propose the task of expository text generation, which\nseeks to automatically generate an accurate and stylistically consistent\nexpository text for a topic by intelligently searching a knowledge source. We\nsolve our task by developing IRP, a framework that overcomes the limitations of\nretrieval-augmented models and iteratively performs content planning, fact\nretrieval, and rephrasing. Through experiments on three diverse,\nnewly-collected datasets, we show that IRP produces factual and organized\nexpository texts that accurately inform readers.", "published": "2023-05-05 04:26:29", "link": "http://arxiv.org/abs/2305.03276v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TransESC: Smoothing Emotional Support Conversation via Turn-Level State\n  Transition", "abstract": "Emotion Support Conversation (ESC) is an emerging and challenging task with\nthe goal of reducing the emotional distress of people. Previous attempts fail\nto maintain smooth transitions between utterances in ESC because they ignore to\ngrasp the fine-grained transition information at each dialogue turn. To solve\nthis problem, we propose to take into account turn-level state\n\\textbf{Trans}itions of \\textbf{ESC} (\\textbf{TransESC}) from three\nperspectives, including semantics transition, strategy transition and emotion\ntransition, to drive the conversation in a smooth and natural way.\nSpecifically, we construct the state transition graph with a two-step way,\nnamed transit-then-interact, to grasp such three types of turn-level transition\ninformation. Finally, they are injected into the transition-aware decoder to\ngenerate more engaging responses. Both automatic and human evaluations on the\nbenchmark dataset demonstrate the superiority of TransESC to generate more\nsmooth and effective supportive responses. Our source code is available at\n\\url{https://github.com/circle-hit/TransESC}.", "published": "2023-05-05 05:50:26", "link": "http://arxiv.org/abs/2305.03296v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM-RM at SemEval-2023 Task 2: Multilingual Complex NER using\n  XLM-RoBERTa", "abstract": "Named Entity Recognition(NER) is a task of recognizing entities at a token\nlevel in a sentence. This paper focuses on solving NER tasks in a multilingual\nsetting for complex named entities. Our team, LLM-RM participated in the\nrecently organized SemEval 2023 task, Task 2: MultiCoNER II,Multilingual\nComplex Named Entity Recognition. We approach the problem by leveraging\ncross-lingual representation provided by fine-tuning XLM-Roberta base model on\ndatasets of all of the 12 languages provided -- Bangla, Chinese, English,\nFarsi, French, German, Hindi, Italian, Portuguese, Spanish, Swedish and\nUkrainian", "published": "2023-05-05 06:05:45", "link": "http://arxiv.org/abs/2305.03300v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Block the Label and Noise: An N-Gram Masked Speller for Chinese Spell\n  Checking", "abstract": "Recently, Chinese Spell Checking(CSC), a task to detect erroneous characters\nin a sentence and correct them, has attracted extensive interest because of its\nwide applications in various NLP tasks. Most of the existing methods have\nutilized BERT to extract semantic information for CSC task. However, these\nmethods directly take sentences with only a few errors as inputs, where the\ncorrect characters may leak answers to the model and dampen its ability to\ncapture distant context; while the erroneous characters may disturb the\nsemantic encoding process and result in poor representations. Based on such\nobservations, this paper proposes an n-gram masking layer that masks current\nand/or surrounding tokens to avoid label leakage and error disturbance.\nMoreover, considering that the mask strategy may ignore multi-modal information\nindicated by errors, a novel dot-product gating mechanism is proposed to\nintegrate the phonological and morphological information with semantic\nrepresentation. Extensive experiments on SIGHAN datasets have demonstrated that\nthe pluggable n-gram masking mechanism can improve the performance of prevalent\nCSC models and the proposed methods in this paper outperform multiple powerful\nstate-of-the-art models.", "published": "2023-05-05 06:43:56", "link": "http://arxiv.org/abs/2305.03314v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HiPool: Modeling Long Documents Using Graph Neural Networks", "abstract": "Encoding long sequences in Natural Language Processing (NLP) is a challenging\nproblem. Though recent pretraining language models achieve satisfying\nperformances in many NLP tasks, they are still restricted by a pre-defined\nmaximum length, making them challenging to be extended to longer sequences. So\nsome recent works utilize hierarchies to model long sequences. However, most of\nthem apply sequential models for upper hierarchies, suffering from long\ndependency issues. In this paper, we alleviate these issues through a\ngraph-based method. We first chunk the sequence with a fixed length to model\nthe sentence-level information. We then leverage graphs to model intra- and\ncross-sentence correlations with a new attention mechanism. Additionally, due\nto limited standard benchmarks for long document classification (LDC), we\npropose a new challenging benchmark, totaling six datasets with up to 53k\nsamples and 4034 average tokens' length. Evaluation shows our model surpasses\ncompetitive baselines by 2.6% in F1 score, and 4.8% on the longest sequence\ndataset. Our method is shown to outperform hierarchical sequential models with\nbetter performance and scalability, especially for longer sequences.", "published": "2023-05-05 06:58:24", "link": "http://arxiv.org/abs/2305.03319v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Online Gesture Recognition using Transformer and Natural Language\n  Processing", "abstract": "The Transformer architecture is shown to provide a powerful machine\ntransduction framework for online handwritten gestures corresponding to glyph\nstrokes of natural language sentences. The attention mechanism is successfully\nused to create latent representations of an end-to-end encoder-decoder model,\nsolving multi-level segmentation while also learning some language features and\nsyntax rules. The additional use of a large decoding space with some learned\nByte-Pair-Encoding (BPE) is shown to provide robustness to ablated inputs and\nsyntax rules. The encoder stack was directly fed with spatio-temporal data\ntokens potentially forming an infinitely large input vocabulary, an approach\nthat finds applications beyond that of this work. Encoder transfer learning\ncapabilities is also demonstrated on several languages resulting in faster\noptimisation and shared parameters. A new supervised dataset of online\nhandwriting gestures suitable for generic handwriting recognition tasks was\nused to successfully train a small transformer model to an average normalised\nLevenshtein accuracy of 96% on English or German sentences and 94% in French.", "published": "2023-05-05 10:17:22", "link": "http://arxiv.org/abs/2305.03407v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using ChatGPT for Entity Matching", "abstract": "Entity Matching is the task of deciding if two entity descriptions refer to\nthe same real-world entity. State-of-the-art entity matching methods often rely\non fine-tuning Transformer models such as BERT or RoBERTa. Two major drawbacks\nof using these models for entity matching are that (i) the models require\nsignificant amounts of fine-tuning data for reaching a good performance and\n(ii) the fine-tuned models are not robust concerning out-of-distribution\nentities. In this paper, we investigate using ChatGPT for entity matching as a\nmore robust, training data-efficient alternative to traditional Transformer\nmodels. We perform experiments along three dimensions: (i) general prompt\ndesign, (ii) in-context learning, and (iii) provision of higher-level matching\nknowledge. We show that ChatGPT is competitive with a fine-tuned RoBERTa model,\nreaching a zero-shot performance of 82.35% F1 on a challenging matching task on\nwhich RoBERTa requires 2000 training examples for reaching a similar\nperformance. Adding in-context demonstrations to the prompts further improves\nthe F1 by up to 7.85% when using similarity-based example selection. Always\nusing the same set of 10 handpicked demonstrations leads to an improvement of\n4.92% over the zero-shot performance. Finally, we show that ChatGPT can also be\nguided by adding higher-level matching knowledge in the form of rules to the\nprompts. Providing matching rules leads to similar performance gains as\nproviding in-context demonstrations.", "published": "2023-05-05 10:39:32", "link": "http://arxiv.org/abs/2305.03423v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simulating H.P. Lovecraft horror literature with the ChatGPT large\n  language model", "abstract": "In this paper, we present a novel approach to simulating H.P. Lovecraft's\nhorror literature using the ChatGPT large language model, specifically the\nGPT-4 architecture. Our study aims to generate text that emulates Lovecraft's\nunique writing style and themes, while also examining the effectiveness of\nprompt engineering techniques in guiding the model's output. To achieve this,\nwe curated a prompt containing several specialized literature references and\nemployed advanced prompt engineering methods. We conducted an empirical\nevaluation of the generated text by administering a survey to a sample of\nundergraduate students. Utilizing statistical hypothesis testing, we assessed\nthe students ability to distinguish between genuine Lovecraft works and those\ngenerated by our model. Our findings demonstrate that the participants were\nunable to reliably differentiate between the two, indicating the effectiveness\nof the GPT-4 model and our prompt engineering techniques in emulating\nLovecraft's literary style. In addition to presenting the GPT model's\ncapabilities, this paper provides a comprehensive description of its underlying\narchitecture and offers a comparative analysis with related work that simulates\nother notable authors and philosophers, such as Dennett. By exploring the\npotential of large language models in the context of literary emulation, our\nstudy contributes to the body of research on the applications and limitations\nof these models in various creative domains.", "published": "2023-05-05 11:03:03", "link": "http://arxiv.org/abs/2305.03429v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LMs stand their Ground: Investigating the Effect of Embodiment in\n  Figurative Language Interpretation by Language Models", "abstract": "Figurative language is a challenge for language models since its\ninterpretation is based on the use of words in a way that deviates from their\nconventional order and meaning. Yet, humans can easily understand and interpret\nmetaphors, similes or idioms as they can be derived from embodied metaphors.\nLanguage is a proxy for embodiment and if a metaphor is conventional and\nlexicalised, it becomes easier for a system without a body to make sense of\nembodied concepts. Yet, the intricate relation between embodiment and features\nsuch as concreteness or age of acquisition has not been studied in the context\nof figurative language interpretation concerning language models. Hence, the\npresented study shows how larger language models perform better at interpreting\nmetaphoric sentences when the action of the metaphorical sentence is more\nembodied. The analysis rules out multicollinearity with other features (e.g.\nword length or concreteness) and provides initial evidence that larger language\nmodels conceptualise embodied concepts to a degree that facilitates figurative\nlanguage understanding.", "published": "2023-05-05 11:44:12", "link": "http://arxiv.org/abs/2305.03445v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Mixed Large\n  Language Model Signals for Science Question Answering", "abstract": "Large Language Models (LLMs) have recently demonstrated exceptional\nperformance in various Natural Language Processing (NLP) tasks. They have also\nshown the ability to perform chain-of-thought (CoT) reasoning to solve complex\nproblems. Recent studies have explored CoT reasoning in complex multimodal\nscenarios, such as the science question answering task, by fine-tuning\nmultimodal models with high-quality human-annotated CoT rationales. However,\ncollecting high-quality COT rationales is usually time-consuming and costly.\nBesides, the annotated rationales are hardly accurate due to the external\nessential information missed. To address these issues, we propose a novel\nmethod termed T-SciQ that aims at teaching science question answering with LLM\nsignals. The T-SciQ approach generates high-quality CoT rationales as teaching\nsignals and is advanced to train much smaller models to perform CoT reasoning\nin complex modalities. Additionally, we introduce a novel data mixing strategy\nto produce more effective teaching data samples for simple and complex science\nquestion answer problems. Extensive experimental results show that our T-SciQ\nmethod achieves a new state-of-the-art performance on the ScienceQA benchmark,\nwith an accuracy of 96.18%. Moreover, our approach outperforms the most\npowerful fine-tuned baseline by 4.5%. The code is publicly available at\nhttps://github.com/T-SciQ/T-SciQ.", "published": "2023-05-05 11:56:30", "link": "http://arxiv.org/abs/2305.03453v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interactive Acquisition of Fine-grained Visual Concepts by Exploiting\n  Semantics of Generic Characterizations in Discourse", "abstract": "Interactive Task Learning (ITL) concerns learning about unforeseen domain\nconcepts via natural interactions with human users. The learner faces a number\nof significant constraints: learning should be online, incremental and\nfew-shot, as it is expected to perform tangible belief updates right after\nnovel words denoting unforeseen concepts are introduced. In this work, we\nexplore a challenging symbol grounding task--discriminating among object\nclasses that look very similar--within the constraints imposed by ITL. We\ndemonstrate empirically that more data-efficient grounding results from\nexploiting the truth-conditions of the teacher's generic statements (e.g., \"Xs\nhave attribute Z.\") and their implicatures in context (e.g., as an answer to\n\"How are Xs and Ys different?\", one infers Y lacks attribute Z).", "published": "2023-05-05 12:06:01", "link": "http://arxiv.org/abs/2305.03461v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Jointly Extracting Interventions, Outcomes, and Findings from RCT\n  Reports with LLMs", "abstract": "Results from Randomized Controlled Trials (RCTs) establish the comparative\neffectiveness of interventions, and are in turn critical inputs for\nevidence-based care. However, results from RCTs are presented in (often\nunstructured) natural language articles describing the design, execution, and\noutcomes of trials; clinicians must manually extract findings pertaining to\ninterventions and outcomes of interest from such articles. This onerous manual\nprocess has motivated work on (semi-)automating extraction of structured\nevidence from trial reports. In this work we propose and evaluate a\ntext-to-text model built on instruction-tuned Large Language Models (LLMs) to\njointly extract Interventions, Outcomes, and Comparators (ICO elements) from\nclinical abstracts, and infer the associated results reported. Manual (expert)\nand automated evaluations indicate that framing evidence extraction as a\nconditional generation task and fine-tuning LLMs for this purpose realizes\nconsiderable ($\\sim$20 point absolute F1 score) gains over the previous SOTA.\nWe perform ablations and error analyses to assess aspects that contribute to\nmodel performance, and to highlight potential directions for further\nimprovements. We apply our model to a collection of published RCTs through\nmid-2022, and release a searchable database of structured findings:\nhttp://ico-relations.ebm-nlp.com", "published": "2023-05-05 16:02:06", "link": "http://arxiv.org/abs/2305.03642v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DAMO-NLP at SemEval-2023 Task 2: A Unified Retrieval-augmented System\n  for Multilingual Named Entity Recognition", "abstract": "The MultiCoNER \\RNum{2} shared task aims to tackle multilingual named entity\nrecognition (NER) in fine-grained and noisy scenarios, and it inherits the\nsemantic ambiguity and low-context setting of the MultiCoNER \\RNum{1} task. To\ncope with these problems, the previous top systems in the MultiCoNER \\RNum{1}\neither incorporate the knowledge bases or gazetteers. However, they still\nsuffer from insufficient knowledge, limited context length, single retrieval\nstrategy. In this paper, our team \\textbf{DAMO-NLP} proposes a unified\nretrieval-augmented system (U-RaNER) for fine-grained multilingual NER. We\nperform error analysis on the previous top systems and reveal that their\nperformance bottleneck lies in insufficient knowledge. Also, we discover that\nthe limited context length causes the retrieval knowledge to be invisible to\nthe model. To enhance the retrieval context, we incorporate the entity-centric\nWikidata knowledge base, while utilizing the infusion approach to broaden the\ncontextual scope of the model. Also, we explore various search strategies and\nrefine the quality of retrieval knowledge. Our system\\footnote{We will release\nthe dataset, code, and scripts of our system at {\\small\n\\url{https://github.com/modelscope/AdaSeq/tree/master/examples/U-RaNER}}.} wins\n9 out of 13 tracks in the MultiCoNER \\RNum{2} shared task. Additionally, we\ncompared our system with ChatGPT, one of the large language models which have\nunlocked strong capabilities on many tasks. The results show that there is\nstill much room for improvement for ChatGPT on the extraction task.", "published": "2023-05-05 16:59:26", "link": "http://arxiv.org/abs/2305.03688v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models in Ambulatory Devices for Home Health Diagnostics:\n  A case study of Sickle Cell Anemia Management", "abstract": "This study investigates the potential of an ambulatory device that\nincorporates Large Language Models (LLMs) in cadence with other specialized ML\nmodels to assess anemia severity in sickle cell patients in real time. The\ndevice would rely on sensor data that measures angiogenic material levels to\nassess anemia severity, providing real-time information to patients and\nclinicians to reduce the frequency of vaso-occlusive crises because of the\nearly detection of anemia severity, allowing for timely interventions and\npotentially reducing the likelihood of serious complications. The main\nchallenges in developing such a device are the creation of a reliable\nnon-invasive tool for angiogenic level assessment, a biophysics model and the\npractical consideration of an LLM communicating with emergency personnel on\nbehalf of an incapacitated patient. A possible system is proposed, and the\nlimitations of this approach are discussed.", "published": "2023-05-05 17:55:49", "link": "http://arxiv.org/abs/2305.03715v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Harnessing the Power of BERT in the Turkish Clinical Domain: Pretraining\n  Approaches for Limited Data Scenarios", "abstract": "In recent years, major advancements in natural language processing (NLP) have\nbeen driven by the emergence of large language models (LLMs), which have\nsignificantly revolutionized research and development within the field.\nBuilding upon this progress, our study delves into the effects of various\npre-training methodologies on Turkish clinical language models' performance in\na multi-label classification task involving radiology reports, with a focus on\naddressing the challenges posed by limited language resources. Additionally, we\nevaluated the simultaneous pretraining approach by utilizing limited clinical\ntask data for the first time. We developed four models, including\nTurkRadBERT-task v1, TurkRadBERT-task v2, TurkRadBERT-sim v1, and\nTurkRadBERT-sim v2. Our findings indicate that the general Turkish BERT model\n(BERTurk) and TurkRadBERT-task v1, both of which utilize knowledge from a\nsubstantial general-domain corpus, demonstrate the best overall performance.\nAlthough the task-adaptive pre-training approach has the potential to capture\ndomain-specific patterns, it is constrained by the limited task-specific corpus\nand may be susceptible to overfitting. Furthermore, our results underscore the\nsignificance of domain-specific vocabulary during pre-training for enhancing\nmodel performance. Ultimately, we observe that the combination of\ngeneral-domain knowledge and task-specific fine-tuning is essential for\nachieving optimal performance across a range of categories. This study offers\nvaluable insights for developing effective Turkish clinical language models and\ncan guide future research on pre-training techniques for other low-resource\nlanguages within the clinical domain.", "published": "2023-05-05 18:39:07", "link": "http://arxiv.org/abs/2305.03788v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformer Working Memory Enables Regular Language Reasoning and\n  Natural Language Length Extrapolation", "abstract": "Unlike recurrent models, conventional wisdom has it that Transformers cannot\nperfectly model regular languages. Inspired by the notion of working memory, we\npropose a new Transformer variant named RegularGPT. With its novel combination\nof Weight-Sharing, Adaptive-Depth, and Sliding-Dilated-Attention, RegularGPT\nconstructs working memory along the depth dimension, thereby enabling efficient\nand successful modeling of regular languages such as PARITY. We further test\nRegularGPT on the task of natural language length extrapolation and\nsurprisingly find that it rediscovers the local windowed attention effect\ndeemed necessary in prior work for length extrapolation.", "published": "2023-05-05 18:54:40", "link": "http://arxiv.org/abs/2305.03796v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting Transformer Language Models for Predictive Typing in\n  Brain-Computer Interfaces", "abstract": "Brain-computer interfaces (BCI) are an important mode of alternative and\naugmentative communication for many people. Unlike keyboards, many BCI systems\ndo not display even the 26 letters of English at one time, let alone all the\nsymbols in more complex systems. Using language models to make character-level\npredictions, therefore, can greatly speed up BCI typing (Ghosh and Kristensson,\n2017). While most existing BCI systems employ character n-gram models or no LM\nat all, this paper adapts several wordpiece-level Transformer LMs to make\ncharacter predictions and evaluates them on typing tasks. GPT-2 fares best on\nclean text, but different LMs react differently to noisy histories. We further\nanalyze the effect of character positions in a word and context lengths.", "published": "2023-05-05 19:47:41", "link": "http://arxiv.org/abs/2305.03819v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models in Sport Science & Medicine: Opportunities, Risks\n  and Considerations", "abstract": "This paper explores the potential opportunities, risks, and challenges\nassociated with the use of large language models (LLMs) in sports science and\nmedicine. LLMs are large neural networks with transformer style architectures\ntrained on vast amounts of textual data, and typically refined with human\nfeedback. LLMs can perform a large range of natural language processing tasks.\nIn sports science and medicine, LLMs have the potential to support and augment\nthe knowledge of sports medicine practitioners, make recommendations for\npersonalised training programs, and potentially distribute high-quality\ninformation to practitioners in developing countries. However, there are also\npotential risks associated with the use and development of LLMs, including\nbiases in the dataset used to create the model, the risk of exposing\nconfidential data, the risk of generating harmful output, and the need to align\nthese models with human preferences through feedback. Further research is\nneeded to fully understand the potential applications of LLMs in sports science\nand medicine and to ensure that their use is ethical and beneficial to\nathletes, clients, patients, practitioners, and the general public.", "published": "2023-05-05 21:20:02", "link": "http://arxiv.org/abs/2305.03851v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Train Global, Tailor Local: Minimalist Multilingual Translation into\n  Endangered Languages", "abstract": "In many humanitarian scenarios, translation into severely low resource\nlanguages often does not require a universal translation engine, but a\ndedicated text-specific translation engine. For example, healthcare records,\nhygienic procedures, government communication, emergency procedures and\nreligious texts are all limited texts. While generic translation engines for\nall languages do not exist, translation of multilingually known limited texts\ninto new, endangered languages may be possible and reduce human translation\neffort. We attempt to leverage translation resources from many rich resource\nlanguages to efficiently produce best possible translation quality for a well\nknown text, which is available in multiple languages, in a new, severely low\nresource language. We examine two approaches: 1. best selection of seed\nsentences to jump start translations in a new language in view of best\ngeneralization to the remainder of a larger targeted text(s), and 2. we adapt\nlarge general multilingual translation engines from many other languages to\nfocus on a specific text in a new, unknown language. We find that adapting\nlarge pretrained multilingual models to the domain/text first and then to the\nseverely low resource language works best. If we also select a best set of seed\nsentences, we can improve average chrF performance on new test languages from a\nbaseline of 21.9 to 50.7, while reducing the number of seed sentences to only\naround 1,000 in the new, unknown language.", "published": "2023-05-05 23:22:16", "link": "http://arxiv.org/abs/2305.03873v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rescue Conversations from Dead-ends: Efficient Exploration for\n  Task-oriented Dialogue Policy Optimization", "abstract": "Training a dialogue policy using deep reinforcement learning requires a lot\nof exploration of the environment. The amount of wasted invalid exploration\nmakes their learning inefficient. In this paper, we find and define an\nimportant reason for the invalid exploration: dead-ends. When a conversation\nenters a dead-end state, regardless of the actions taken afterward, it will\ncontinue in a dead-end trajectory until the agent reaches a termination state\nor maximum turn. We propose a dead-end resurrection (DDR) algorithm that\ndetects the initial dead-end state in a timely and efficient manner and\nprovides a rescue action to guide and correct the exploration direction. To\nprevent dialogue policies from repeatedly making the same mistake, DDR also\nperforms dialogue data augmentation by adding relevant experiences containing\ndead-end states. We first validate the dead-end detection reliability and then\ndemonstrate the effectiveness and generality of the method by reporting\nexperimental results on several dialogue datasets from different domains.", "published": "2023-05-05 03:28:49", "link": "http://arxiv.org/abs/2305.03262v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Low-Resource Multi-Granularity Academic Function Recognition Based on\n  Multiple Prompt Knowledge", "abstract": "Fine-tuning pre-trained language models (PLMs), e.g., SciBERT, generally\nrequires large numbers of annotated data to achieve state-of-the-art\nperformance on a range of NLP tasks in the scientific domain. However,\nobtaining the fine-tune data for scientific NLP task is still challenging and\nexpensive. Inspired by recent advancement in prompt learning, in this paper, we\npropose the Mix Prompt Tuning (MPT), which is a semi-supervised method to\nalleviate the dependence on annotated data and improve the performance of\nmulti-granularity academic function recognition tasks with a small number of\nlabeled examples. Specifically, the proposed method provides multi-perspective\nrepresentations by combining manual prompt templates with automatically learned\ncontinuous prompt templates to help the given academic function recognition\ntask take full advantage of knowledge in PLMs. Based on these prompt templates\nand the fine-tuned PLM, a large number of pseudo labels are assigned to the\nunlabeled examples. Finally, we fine-tune the PLM using the pseudo training\nset. We evaluate our method on three academic function recognition tasks of\ndifferent granularity including the citation function, the abstract sentence\nfunction, and the keyword function, with datasets from computer science domain\nand biomedical domain. Extensive experiments demonstrate the effectiveness of\nour method and statistically significant improvements against strong baselines.\nIn particular, it achieves an average increase of 5% in Macro-F1 score compared\nwith fine-tuning, and 6% in Macro-F1 score compared with other semi-supervised\nmethod under low-resource settings. In addition, MPT is a general method that\ncan be easily applied to other low-resource scientific classification tasks.", "published": "2023-05-05 05:32:50", "link": "http://arxiv.org/abs/2305.03287v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Open Information Extraction via Chunks", "abstract": "Open Information Extraction (OIE) aims to extract relational tuples from\nopen-domain sentences. Existing OIE systems split a sentence into tokens and\nrecognize token spans as tuple relations and arguments. We instead propose\nSentence as Chunk sequence (SaC) and recognize chunk spans as tuple relations\nand arguments. We argue that SaC has better quantitative and qualitative\nproperties for OIE than sentence as token sequence, and evaluate four choices\nof chunks (i.e., CoNLL chunks, simple phrases, NP chunks, and spans from\nSpanOIE) against gold OIE tuples. Accordingly, we propose a simple BERT-based\nmodel for sentence chunking, and propose Chunk-OIE for tuple extraction on top\nof SaC. Chunk-OIE achieves state-of-the-art results on multiple OIE datasets,\nshowing that SaC benefits OIE task.", "published": "2023-05-05 06:03:54", "link": "http://arxiv.org/abs/2305.03299v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MindGames: Targeting Theory of Mind in Large Language Models with\n  Dynamic Epistemic Modal Logic", "abstract": "Theory of Mind (ToM) is a critical component of intelligence but its\nassessment remains the subject of heated debates. Prior research applied human\nToM assessments to natural language processing models using either\nhuman-created standardized tests or rule-based templates. However, these\nmethods primarily focus on simplistic reasoning and require further validation.\nHere, we leverage dynamic epistemic logic to isolate a particular component of\nToM and to generate controlled problems. We also introduce new verbalization\ntechniques to express these problems in English natural language. Our findings\nindicate that some language model scaling (from 70M to 6B and 350M to 174B)\ndoes not consistently yield results better than random chance. While GPT-4\ndemonstrates superior epistemic reasoning capabilities, there is still room for\nimprovement. Our code and datasets are publicly available\n(https://huggingface.co/datasets/sileod/mindgames ,\nhttps://github.com/sileod/llm-theory-of-mind )", "published": "2023-05-05 08:14:48", "link": "http://arxiv.org/abs/2305.03353v2", "categories": ["cs.CL", "cs.AI", "68T01, 68T27, 68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Multi-View Graph Representation Learning for Answering Hybrid Numerical\n  Reasoning Question", "abstract": "Hybrid question answering (HybridQA) over the financial report contains both\ntextual and tabular data, and requires the model to select the appropriate\nevidence for the numerical reasoning task. Existing methods based on\nencoder-decoder framework employ a expression tree-based decoder to solve\nnumerical reasoning problems. However, encoders rely more on Machine Reading\nComprehension (MRC) methods, which take table serialization and text splicing\nas input, damaging the granularity relationship between table and text as well\nas the spatial structure information of table itself. In order to solve these\nproblems, the paper proposes a Multi-View Graph (MVG) Encoder to take the\nrelations among the granularity into account and capture the relations from\nmultiple view. By utilizing MVGE as a module, we constuct Tabular View,\nRelation View and Numerical View which aim to retain the original\ncharacteristics of the hybrid data. We validate our model on the publicly\navailable table-text hybrid QA benchmark (TAT-QA) and outperform the\nstate-of-the-art model.", "published": "2023-05-05 12:00:58", "link": "http://arxiv.org/abs/2305.03458v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Context-Aware Semantic Similarity Measurement for Unsupervised Word\n  Sense Disambiguation", "abstract": "The issue of word sense ambiguity poses a significant challenge in natural\nlanguage processing due to the scarcity of annotated data to feed machine\nlearning models to face the challenge. Therefore, unsupervised word sense\ndisambiguation methods have been developed to overcome that challenge without\nrelying on annotated data. This research proposes a new context-aware approach\nto unsupervised word sense disambiguation, which provides a flexible mechanism\nfor incorporating contextual information into the similarity measurement\nprocess. We experiment with a popular benchmark dataset to evaluate the\nproposed strategy and compare its performance with state-of-the-art\nunsupervised word sense disambiguation techniques. The experimental results\nindicate that our approach substantially enhances disambiguation accuracy and\nsurpasses the performance of several existing techniques. Our findings\nunderscore the significance of integrating contextual information in semantic\nsimilarity measurements to manage word sense ambiguity in unsupervised\nscenarios effectively.", "published": "2023-05-05 13:50:04", "link": "http://arxiv.org/abs/2305.03520v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "In-context Learning as Maintaining Coherency: A Study of On-the-fly\n  Machine Translation Using Large Language Models", "abstract": "The phenomena of in-context learning has typically been thought of as\n\"learning from examples\". In this work which focuses on Machine Translation, we\npresent a perspective of in-context learning as the desired generation task\nmaintaining coherency with its context, i.e., the prompt examples. We first\ninvestigate randomly sampled prompts across 4 domains, and find that\ntranslation performance improves when shown in-domain prompts. Next, we\ninvestigate coherency for the in-domain setting, which uses prompt examples\nfrom a moving window. We study this with respect to other factors that have\npreviously been identified in the literature such as length, surface similarity\nand sentence embedding similarity. Our results across 3 models (GPTNeo2.7B,\nBloom3B, XGLM2.9B), and three translation directions\n(\\texttt{en}$\\rightarrow$\\{\\texttt{pt, de, fr}\\}) suggest that the long-term\ncoherency of the prompts and the test sentence is a good indicator of\ndownstream translation performance. In doing so, we demonstrate the efficacy of\nIn-context Machine Translation for on-the-fly adaptation.", "published": "2023-05-05 14:30:20", "link": "http://arxiv.org/abs/2305.03573v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Now It Sounds Like You: Learning Personalized Vocabulary On Device", "abstract": "In recent years, Federated Learning (FL) has shown significant advancements\nin its ability to perform various natural language processing (NLP) tasks. This\nwork focuses on applying personalized FL for on-device language modeling. Due\nto limitations of memory and latency, these models cannot support the\ncomplexity of sub-word tokenization or beam search decoding, resulting in the\ndecision to deploy a closed-vocabulary language model. However,\nclosed-vocabulary models are unable to handle out-of-vocabulary (OOV) words\nbelonging to specific users. To address this issue, We propose a novel\ntechnique called \"OOV expansion\" that improves OOV coverage and increases model\naccuracy while minimizing the impact on memory and latency. This method\nintroduces a personalized \"OOV adapter\" that effectively transfers knowledge\nfrom a central model and learns word embedding for personalized vocabulary. OOV\nexpansion significantly outperforms standard FL personalization methods on a\nset of common FL benchmarks.", "published": "2023-05-05 14:44:20", "link": "http://arxiv.org/abs/2305.03584v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Dual Semantic-Aware Recurrent Global-Adaptive Network For\n  Vision-and-Language Navigation", "abstract": "Vision-and-Language Navigation (VLN) is a realistic but challenging task that\nrequires an agent to locate the target region using verbal and visual cues.\nWhile significant advancements have been achieved recently, there are still two\nbroad limitations: (1) The explicit information mining for significant guiding\nsemantics concealed in both vision and language is still under-explored; (2)\nThe previously structured map method provides the average historical appearance\nof visited nodes, while it ignores distinctive contributions of various images\nand potent information retention in the reasoning process. This work proposes a\ndual semantic-aware recurrent global-adaptive network (DSRG) to address the\nabove problems. First, DSRG proposes an instruction-guidance linguistic module\n(IGL) and an appearance-semantics visual module (ASV) for boosting vision and\nlanguage semantic learning respectively. For the memory mechanism, a global\nadaptive aggregation module (GAA) is devised for explicit panoramic observation\nfusion, and a recurrent memory fusion module (RMF) is introduced to supply\nimplicit temporal hidden states. Extensive experimental results on the R2R and\nREVERIE datasets demonstrate that our method achieves better performance than\nexisting methods. Code is available at https://github.com/CrystalSixone/DSRG.", "published": "2023-05-05 15:06:08", "link": "http://arxiv.org/abs/2305.03602v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Predicting COVID-19 and pneumonia complications from admission texts", "abstract": "In this paper we present a novel approach to risk assessment for patients\nhospitalized with pneumonia or COVID-19 based on their admission reports. We\napplied a Longformer neural network to admission reports and other textual data\navailable shortly after admission to compute risk scores for the patients. We\nused patient data of multiple European hospitals to demonstrate that our\napproach outperforms the Transformer baselines. Our experiments show that the\nproposed model generalises across institutions and diagnoses. Also, our method\nhas several other advantages described in the paper.", "published": "2023-05-05 16:28:44", "link": "http://arxiv.org/abs/2305.03661v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Suite of Generative Tasks for Multi-Level Multimodal Webpage\n  Understanding", "abstract": "Webpages have been a rich, scalable resource for vision-language and language\nonly tasks. Yet only pieces of webpages are kept in existing datasets:\nimage-caption pairs, long text articles, or raw HTML, never all in one place.\nWebpage tasks have resultingly received little attention and structured\nimage-text data left underused. To study multimodal webpage understanding, we\nintroduce the Wikipedia Webpage suite (WikiWeb2M) containing 2M pages with all\nof the associated image, text, and structure data. We verify its utility on\nthree generative tasks: page description generation, section summarization, and\ncontextual image captioning. We design a novel attention mechanism Prefix\nGlobal, which selects the most relevant image and text content as global tokens\nto attend to the rest of the webpage for context. By using page structure to\nseparate such tokens, it performs better than full attention with lower\ncomputational complexity. Extensive experiments show that the new data in\nWikiWeb2M improves task performance compared to prior work.", "published": "2023-05-05 16:38:05", "link": "http://arxiv.org/abs/2305.03668v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Vera: A General-Purpose Plausibility Estimation Model for Commonsense\n  Statements", "abstract": "Despite the much discussed capabilities of today's language models, they are\nstill prone to silly and unexpected commonsense failures. We consider a\nretrospective verification approach that reflects on the correctness of LM\noutputs, and introduce Vera, a general-purpose model that estimates the\nplausibility of declarative statements based on commonsense knowledge. Trained\non ~7M commonsense statements created from 19 QA datasets and two large-scale\nknowledge bases, and with a combination of three training objectives, Vera is a\nversatile model that effectively separates correct from incorrect statements\nacross diverse commonsense domains. When applied to solving commonsense\nproblems in the verification format, Vera substantially outperforms existing\nmodels that can be repurposed for commonsense verification, and it further\nexhibits generalization capabilities to unseen tasks and provides\nwell-calibrated outputs. We find that Vera excels at filtering LM-generated\ncommonsense knowledge and is useful in detecting erroneous commonsense\nstatements generated by models like ChatGPT in real-world settings.", "published": "2023-05-05 17:15:32", "link": "http://arxiv.org/abs/2305.03695v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Otter: A Multi-Modal Model with In-Context Instruction Tuning", "abstract": "Large language models (LLMs) have demonstrated significant universal\ncapabilities as few/zero-shot learners in various tasks due to their\npre-training on vast amounts of text data, as exemplified by GPT-3, which\nboosted to InstrctGPT and ChatGPT, effectively following natural language\ninstructions to accomplish real-world tasks. In this paper, we propose to\nintroduce instruction tuning into multi-modal models, motivated by the Flamingo\nmodel's upstream interleaved format pretraining dataset. We adopt a similar\napproach to construct our MultI-Modal In-Context Instruction Tuning (MIMIC-IT)\ndataset. We then introduce Otter, a multi-modal model based on OpenFlamingo\n(open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and\nshowcasing improved instruction-following ability and in-context learning. We\nalso optimize OpenFlamingo's implementation for researchers, democratizing the\nrequired training resources from 1$\\times$ A100 GPU to 4$\\times$ RTX-3090 GPUs,\nand integrate both OpenFlamingo and Otter into Huggingface Transformers for\nmore researchers to incorporate the models into their customized training and\ninference pipelines.", "published": "2023-05-05 17:59:46", "link": "http://arxiv.org/abs/2305.03726v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Towards Zero-Shot Frame Semantic Parsing with Task Agnostic Ontologies\n  and Simple Labels", "abstract": "Frame semantic parsing is an important component of task-oriented dialogue\nsystems. Current models rely on a significant amount training data to\nsuccessfully identify the intent and slots in the user's input utterance. This\ncreates a significant barrier for adding new domains to virtual assistant\ncapabilities, as creation of this data requires highly specialized NLP\nexpertise. In this work we propose OpenFSP, a framework that allows for easy\ncreation of new domains from a handful of simple labels that can be generated\nwithout specific NLP knowledge. Our approach relies on creating a small, but\nexpressive, set of domain agnostic slot types that enables easy annotation of\nnew domains. Given such annotation, a matching algorithm relying on sentence\nencoders predicts the intent and slots for domains defined by end-users.\nExtensive experiments on the TopV2 dataset shows that our model outperforms\nstrong baselines in this simple labels setting.", "published": "2023-05-05 18:47:18", "link": "http://arxiv.org/abs/2305.03793v1", "categories": ["cs.CL", "cs.LG", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "Uncertainty-Aware Bootstrap Learning for Joint Extraction on\n  Distantly-Supervised Data", "abstract": "Jointly extracting entity pairs and their relations is challenging when\nworking on distantly-supervised data with ambiguous or noisy labels. To\nmitigate such impact, we propose uncertainty-aware bootstrap learning, which is\nmotivated by the intuition that the higher uncertainty of an instance, the more\nlikely the model confidence is inconsistent with the ground truths.\nSpecifically, we first explore instance-level data uncertainty to create an\ninitial high-confident examples. Such subset serves as filtering noisy\ninstances and facilitating the model to converge fast at the early stage.\nDuring bootstrap learning, we propose self-ensembling as a regularizer to\nalleviate inter-model uncertainty produced by noisy labels. We further define\nprobability variance of joint tagging probabilities to estimate inner-model\nparametric uncertainty, which is used to select and build up new reliable\ntraining instances for the next iteration. Experimental results on two large\ndatasets reveal that our approach outperforms existing strong baselines and\nrelated methods.", "published": "2023-05-05 20:06:11", "link": "http://arxiv.org/abs/2305.03827v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CLaC at SemEval-2023 Task 2: Comparing Span-Prediction and\n  Sequence-Labeling approaches for NER", "abstract": "This paper summarizes the CLaC submission for the MultiCoNER 2 task which\nconcerns the recognition of complex, fine-grained named entities. We compare\ntwo popular approaches for NER, namely Sequence Labeling and Span Prediction.\nWe find that our best Span Prediction system performs slightly better than our\nbest Sequence Labeling system on test data. Moreover, we find that using the\nlarger version of XLM RoBERTa significantly improves performance.\nPost-competition experiments show that Span Prediction and Sequence Labeling\napproaches improve when they use special input tokens (<s> and </s>) of\nXLM-RoBERTa. The code for training all models, preprocessing, and\npost-processing is available at\nhttps://github.com/harshshredding/semeval2023-multiconer-paper.", "published": "2023-05-05 20:49:40", "link": "http://arxiv.org/abs/2305.03845v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Implications of Multi-Word Expressions on English to Bharti Braille\n  Machine Translation", "abstract": "In this paper, we have shown the improvement of English to Bharti Braille\nmachine translation system. We have shown how we can improve a baseline NMT\nmodel by adding some linguistic knowledge to it. This was done for five\nlanguage pairs where English sentences were translated into five Indian\nlanguages and then subsequently to corresponding Bharti Braille. This has been\ndemonstrated by adding a sub-module for translating multi-word expressions. The\napproach shows promising results as across language pairs, we could see\nimprovement in the quality of NMT outputs. The least improvement was observed\nin English-Nepali language pair with 22.08% and the most improvement was\nobserved in the English-Hindi language pair with 23.30%.", "published": "2023-05-05 08:59:03", "link": "http://arxiv.org/abs/2305.06157v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Model for Translation of Text from Indian Languages to Bharti Braille\n  Characters", "abstract": "People who are visually impaired face a lot of difficulties while studying.\nOne of the major causes to this is lack of available text in Bharti Braille\nscript. In this paper, we have suggested a scheme to convert text in major\nIndian languages into Bharti Braille. The system uses a hybrid approach where\nat first the text in Indian language is given to a rule based system and in\ncase if there is any ambiguity then it is resolved by applying a LSTM based\nmodel. The developed model has also been tested and found to have produced near\naccurate results.", "published": "2023-05-05 09:21:13", "link": "http://arxiv.org/abs/2305.06475v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Survey on Out-of-Distribution Detection in NLP", "abstract": "Out-of-distribution (OOD) detection is essential for the reliable and safe\ndeployment of machine learning systems in the real world. Great progress has\nbeen made over the past years. This paper presents the first review of recent\nadvances in OOD detection with a particular focus on natural language\nprocessing approaches. First, we provide a formal definition of OOD detection\nand discuss several related fields. We then categorize recent algorithms into\nthree classes according to the data they used: (1) OOD data available, (2) OOD\ndata unavailable + in-distribution (ID) label available, and (3) OOD data\nunavailable + ID label unavailable. Third, we introduce datasets, applications,\nand metrics. Finally, we summarize existing work and present potential future\nresearch topics.", "published": "2023-05-05 01:38:49", "link": "http://arxiv.org/abs/2305.03236v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Out-of-Domain Intent Detection Considering Multi-Turn Dialogue Contexts", "abstract": "Out-of-Domain (OOD) intent detection is vital for practical dialogue systems,\nand it usually requires considering multi-turn dialogue contexts. However, most\nprevious OOD intent detection approaches are limited to single dialogue turns.\nIn this paper, we introduce a context-aware OOD intent detection (Caro)\nframework to model multi-turn contexts in OOD intent detection tasks.\nSpecifically, we follow the information bottleneck principle to extract robust\nrepresentations from multi-turn dialogue contexts. Two different views are\nconstructed for each input sample and the superfluous information not related\nto intent detection is removed using a multi-view information bottleneck loss.\nMoreover, we also explore utilizing unlabeled data in Caro. A two-stage\ntraining process is introduced to mine OOD samples from these unlabeled data,\nand these OOD samples are used to train the resulting model with a\nbootstrapping approach. Comprehensive experiments demonstrate that Caro\nestablishes state-of-the-art performances on multi-turn OOD detection tasks by\nimproving the F1-OOD score of over $29\\%$ compared to the previous best method.", "published": "2023-05-05 01:39:21", "link": "http://arxiv.org/abs/2305.03237v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "QCRI at SemEval-2023 Task 3: News Genre, Framing and Persuasion\n  Techniques Detection using Multilingual Models", "abstract": "Misinformation spreading in mainstream and social media has been misleading\nusers in different ways. Manual detection and verification efforts by\njournalists and fact-checkers can no longer cope with the great scale and quick\nspread of misleading information. This motivated research and industry efforts\nto develop systems for analyzing and verifying news spreading online. The\nSemEval-2023 Task 3 is an attempt to address several subtasks under this\noverarching problem, targeting writing techniques used in news articles to\naffect readers' opinions. The task addressed three subtasks with six languages,\nin addition to three ``surprise'' test languages, resulting in 27 different\ntest setups. This paper describes our participating system to this task. Our\nteam is one of the 6 teams that successfully submitted runs for all setups. The\nofficial results show that our system is ranked among the top 3 systems for 10\nout of the 27 setups.", "published": "2023-05-05 07:40:41", "link": "http://arxiv.org/abs/2305.03336v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "From Parse-Execute to Parse-Execute-Refine: Improving Semantic Parser\n  for Complex Question Answering over Knowledge Base", "abstract": "Parsing questions into executable logical forms has showed impressive results\nfor knowledge-base question answering (KBQA). However, complex KBQA is a more\nchallenging task that requires to perform complex multi-step reasoning.\nRecently, a new semantic parser called KoPL has been proposed to explicitly\nmodel the reasoning processes, which achieved the state-of-the-art on complex\nKBQA. In this paper, we further explore how to unlock the reasoning ability of\nsemantic parsers by a simple proposed parse-execute-refine paradigm. We refine\nand improve the KoPL parser by demonstrating the executed intermediate\nreasoning steps to the KBQA model. We show that such simple strategy can\nsignificantly improve the ability of complex reasoning. Specifically, we\npropose three components: a parsing stage, an execution stage and a refinement\nstage, to enhance the ability of complex reasoning. The parser uses the KoPL to\ngenerate the transparent logical forms. Then, the execution stage aligns and\nexecutes the logical forms over knowledge base to obtain intermediate reasoning\nprocesses. Finally, the intermediate step-by-step reasoning processes are\ndemonstrated to the KBQA model in the refinement stage. With the explicit\nreasoning processes, it is much easier to answer the complex questions.\nExperiments on benchmark dataset shows that the proposed PER-KBQA performs\nsignificantly better than the stage-of-the-art baselines on the complex KBQA.", "published": "2023-05-05 08:20:09", "link": "http://arxiv.org/abs/2305.03356v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The MuSe 2023 Multimodal Sentiment Analysis Challenge: Mimicked\n  Emotions, Cross-Cultural Humour, and Personalisation", "abstract": "The MuSe 2023 is a set of shared tasks addressing three different\ncontemporary multimodal affect and sentiment analysis problems: In the Mimicked\nEmotions Sub-Challenge (MuSe-Mimic), participants predict three continuous\nemotion targets. This sub-challenge utilises the Hume-Vidmimic dataset\ncomprising of user-generated videos. For the Cross-Cultural Humour Detection\nSub-Challenge (MuSe-Humour), an extension of the Passau Spontaneous Football\nCoach Humour (Passau-SFCH) dataset is provided. Participants predict the\npresence of spontaneous humour in a cross-cultural setting. The Personalisation\nSub-Challenge (MuSe-Personalisation) is based on the Ulm-Trier Social Stress\nTest (Ulm-TSST) dataset, featuring recordings of subjects in a stressed\nsituation. Here, arousal and valence signals are to be predicted, whereas parts\nof the test labels are made available in order to facilitate personalisation.\nMuSe 2023 seeks to bring together a broad audience from different research\ncommunities such as audio-visual emotion recognition, natural language\nprocessing, signal processing, and health informatics. In this baseline paper,\nwe introduce the datasets, sub-challenges, and provided feature sets. As a\ncompetitive baseline system, a Gated Recurrent Unit (GRU)-Recurrent Neural\nNetwork (RNN) is employed. On the respective sub-challenges' test datasets, it\nachieves a mean (across three continuous intensity targets) Pearson's\nCorrelation Coefficient of .4727 for MuSe-Mimic, an Area Under the Curve (AUC)\nvalue of .8310 for MuSe-Humor and Concordance Correlation Coefficient (CCC)\nvalues of .7482 for arousal and .7827 for valence in the MuSe-Personalisation\nsub-challenge.", "published": "2023-05-05 08:53:57", "link": "http://arxiv.org/abs/2305.03369v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.LG"}
{"title": "NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial\n  Reports", "abstract": "How can we interpret and retrieve medical evidence to support clinical\ndecisions? Clinical trial reports (CTR) amassed over the years contain\nindispensable information for the development of personalized medicine.\nHowever, it is practically infeasible to manually inspect over 400,000+\nclinical trial reports in order to find the best evidence for experimental\ntreatments. Natural Language Inference (NLI) offers a potential solution to\nthis problem, by allowing the scalable computation of textual entailment.\nHowever, existing NLI models perform poorly on biomedical corpora, and\npreviously published datasets fail to capture the full complexity of inference\nover CTRs. In this work, we present a novel resource to advance research on NLI\nfor reasoning on CTRs. The resource includes two main tasks. Firstly, to\ndetermine the inference relation between a natural language statement, and a\nCTR. Secondly, to retrieve supporting facts to justify the predicted relation.\nWe provide NLI4CT, a corpus of 2400 statements and CTRs, annotated for these\ntasks. Baselines on this corpus expose the limitations of existing NLI models,\nwith 6 state-of-the-art NLI models achieving a maximum F1 score of 0.627. To\nthe best of our knowledge, we are the first to design a task that covers the\ninterpretation of full CTRs. To encourage further work on this challenging\ndataset, we make the corpus, competition leaderboard, website and code to\nreplicate the baseline experiments available at:\nhttps://github.com/ai-systems/nli4ct", "published": "2023-05-05 15:03:01", "link": "http://arxiv.org/abs/2305.03598v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Role of Data Curation in Image Captioning", "abstract": "Image captioning models are typically trained by treating all samples\nequally, neglecting to account for mismatched or otherwise difficult data\npoints. In contrast, recent work has shown the effectiveness of training models\nby scheduling the data using curriculum learning strategies. This paper\ncontributes to this direction by actively curating difficult samples in\ndatasets without increasing the total number of samples. We explore the effect\nof using three data curation methods within the training process: complete\nremoval of an sample, caption replacement, or image replacement via a\ntext-to-image generation model. Experiments on the Flickr30K and COCO datasets\nwith the BLIP and BEiT-3 models demonstrate that these curation methods do\nindeed yield improved image captioning models, underscoring their efficacy.", "published": "2023-05-05 15:16:07", "link": "http://arxiv.org/abs/2305.03610v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "White-Box Multi-Objective Adversarial Attack on Dialogue Generation", "abstract": "Pre-trained transformers are popular in state-of-the-art dialogue generation\n(DG) systems. Such language models are, however, vulnerable to various\nadversarial samples as studied in traditional tasks such as text\nclassification, which inspires our curiosity about their robustness in DG\nsystems. One main challenge of attacking DG models is that perturbations on the\ncurrent sentence can hardly degrade the response accuracy because the unchanged\nchat histories are also considered for decision-making. Instead of merely\npursuing pitfalls of performance metrics such as BLEU, ROUGE, we observe that\ncrafting adversarial samples to force longer generation outputs benefits attack\neffectiveness -- the generated responses are typically irrelevant, lengthy, and\nrepetitive. To this end, we propose a white-box multi-objective attack method\ncalled DGSlow. Specifically, DGSlow balances two objectives -- generation\naccuracy and length, via a gradient-based multi-objective optimizer and applies\nan adaptive searching mechanism to iteratively craft adversarial samples with\nonly a few modifications. Comprehensive experiments on four benchmark datasets\ndemonstrate that DGSlow could significantly degrade state-of-the-art DG models\nwith a higher success rate than traditional accuracy-based methods. Besides,\nour crafted sentences also exhibit strong transferability in attacking other\nmodels.", "published": "2023-05-05 16:21:24", "link": "http://arxiv.org/abs/2305.03655v2", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Retrieval Augmented Chest X-Ray Report Generation using OpenAI GPT\n  models", "abstract": "We propose Retrieval Augmented Generation (RAG) as an approach for automated\nradiology report writing that leverages multimodally aligned embeddings from a\ncontrastively pretrained vision language model for retrieval of relevant\ncandidate radiology text for an input radiology image and a general domain\ngenerative model like OpenAI text-davinci-003, gpt-3.5-turbo and gpt-4 for\nreport generation using the relevant radiology text retrieved. This approach\nkeeps hallucinated generations under check and provides capabilities to\ngenerate report content in the format we desire leveraging the instruction\nfollowing capabilities of these generative models. Our approach achieves better\nclinical metrics with a BERTScore of 0.2865 ({\\Delta}+ 25.88%) and Semb score\nof 0.4026 ({\\Delta}+ 6.31%). Our approach can be broadly relevant for different\nclinical settings as it allows to augment the automated radiology report\ngeneration process with content relevant for that setting while also having the\nability to inject user intents and requirements in the prompts as part of the\nreport generation process to modulate the content and format of the generated\nreports as applicable for that clinical setting.", "published": "2023-05-05 16:28:03", "link": "http://arxiv.org/abs/2305.03660v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "I.2; J.3; H.3"], "primary_category": "cs.CL"}
{"title": "Improved Logical Reasoning of Language Models via Differentiable\n  Symbolic Programming", "abstract": "Pre-trained large language models (LMs) struggle to perform logical reasoning\nreliably despite advances in scale and compositionality. In this work, we\ntackle this challenge through the lens of symbolic programming. We propose\nDSR-LM, a Differentiable Symbolic Reasoning framework where pre-trained LMs\ngovern the perception of factual knowledge, and a symbolic module performs\ndeductive reasoning. In contrast to works that rely on hand-crafted logic\nrules, our differentiable symbolic reasoning framework efficiently learns\nweighted rules and applies semantic loss to further improve LMs. DSR-LM is\nscalable, interpretable, and allows easy integration of prior knowledge,\nthereby supporting extensive symbolic programming to robustly derive a logical\nconclusion. The results of our experiments suggest that DSR-LM improves the\nlogical reasoning abilities of pre-trained language models, resulting in a\nsignificant increase in accuracy of over 20% on deductive reasoning benchmarks.\nFurthermore, DSR-LM outperforms a variety of competitive baselines when faced\nwith systematic changes in sequence length.", "published": "2023-05-05 07:24:46", "link": "http://arxiv.org/abs/2305.03742v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Detecting and Reasoning of Deleted Tweets before they are Posted", "abstract": "Social media platforms empower us in several ways, from information\ndissemination to consumption. While these platforms are useful in promoting\ncitizen journalism, public awareness etc., they have misuse potentials.\nMalicious users use them to disseminate hate-speech, offensive content, rumor\netc. to gain social and political agendas or to harm individuals, entities and\norganizations. Often times, general users unconsciously share information\nwithout verifying it, or unintentionally post harmful messages. Some of such\ncontent often get deleted either by the platform due to the violation of terms\nand policies, or users themselves for different reasons, e.g., regrets. There\nis a wide range of studies in characterizing, understanding and predicting\ndeleted content. However, studies which aims to identify the fine-grained\nreasons (e.g., posts are offensive, hate speech or no identifiable reason)\nbehind deleted content, are limited. In this study we address this gap, by\nidentifying deleted tweets, particularly within the Arabic context, and\nlabeling them with a corresponding fine-grained disinformation category. We\nthen develop models that can predict the potentiality of tweets getting\ndeleted, as well as the potential reasons behind deletion. Such models can help\nin moderating social media posts before even posting.", "published": "2023-05-05 08:25:07", "link": "http://arxiv.org/abs/2305.04927v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "From Zero to Hero: Harnessing Transformers for Biomedical Named Entity\n  Recognition in Zero- and Few-shot Contexts", "abstract": "Supervised named entity recognition (NER) in the biomedical domain depends on\nlarge sets of annotated texts with the given named entities. The creation of\nsuch datasets can be time-consuming and expensive, while extraction of new\nentities requires additional annotation tasks and retraining the model. To\naddress these challenges, this paper proposes a method for zero- and few-shot\nNER in the biomedical domain. The method is based on transforming the task of\nmulti-class token classification into binary token classification and\npre-training on a large amount of datasets and biomedical entities, which allow\nthe model to learn semantic relations between the given and potentially novel\nnamed entity labels. We have achieved average F1 scores of 35.44% for zero-shot\nNER, 50.10% for one-shot NER, 69.94% for 10-shot NER, and 79.51% for 100-shot\nNER on 9 diverse evaluated biomedical entities with fine-tuned PubMedBERT-based\nmodel. The results demonstrate the effectiveness of the proposed method for\nrecognizing new biomedical entities with no or limited number of examples,\noutperforming previous transformer-based methods, and being comparable to\nGPT3-based models using models with over 1000 times fewer parameters. We make\nmodels and developed code publicly available.", "published": "2023-05-05 12:14:22", "link": "http://arxiv.org/abs/2305.04928v5", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Time-weighted Frequency Domain Audio Representation with GMM Estimator\n  for Anomalous Sound Detection", "abstract": "Although deep learning is the mainstream method in unsupervised anomalous\nsound detection, Gaussian Mixture Model (GMM) with statistical audio frequency\nrepresentation as input can achieve comparable results with much lower model\ncomplexity and fewer parameters. Existing statistical frequency\nrepresentations, e.g, the log-Mel spectrogram's average or maximum over time,\ndo not always work well for different machines. This paper presents\nTime-Weighted Frequency Domain Representation (TWFR) with the GMM method\n(TWFR-GMM) for anomalous sound detection. The TWFR is a generalized statistical\nfrequency domain representation that can adapt to different machine types,\nusing the global weighted ranking pooling over time-domain. This allows GMM\nestimator to recognize anomalies, even under domain-shift conditions, as\nvisualized with a Mahalanobis distance-based metric. Experiments on DCASE 2022\nChallenge Task2 dataset show that our method has better detection performance\nthan recent deep learning methods. TWFR-GMM is the core of our submission that\nachieved the 3rd place in DCASE 2022 Challenge Task2.", "published": "2023-05-05 07:17:21", "link": "http://arxiv.org/abs/2305.03328v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Compressing audio CNNs with graph centrality based filter pruning", "abstract": "Convolutional neural networks (CNNs) are commonplace in high-performing\nsolutions to many real-world problems, such as audio classification. CNNs have\nmany parameters and filters, with some having a larger impact on the\nperformance than others. This means that networks may contain many unnecessary\nfilters, increasing a CNN's computation and memory requirements while providing\nlimited performance benefits. To make CNNs more efficient, we propose a pruning\nframework that eliminates filters with the highest \"commonality\". We measure\nthis commonality using the graph-theoretic concept of \"centrality\". We\nhypothesise that a filter with a high centrality should be eliminated as it\nrepresents commonality and can be replaced by other filters without affecting\nthe performance of a network much. An experimental evaluation of the proposed\nframework is performed on acoustic scene classification and audio tagging. On\nthe DCASE 2021 Task 1A baseline network, our proposed method reduces\ncomputations per inference by 71\\% with 50\\% fewer parameters at less than a\ntwo percentage point drop in accuracy compared to the original network. For\nlarge-scale CNNs such as PANNs designed for audio tagging, our method reduces\n24\\% computations per inference with 41\\% fewer parameters at a slight\nimprovement in performance.", "published": "2023-05-05 09:38:05", "link": "http://arxiv.org/abs/2305.03391v1", "categories": ["cs.SD", "cs.AI", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Exploring Softly Masked Language Modelling for Controllable Symbolic\n  Music Generation", "abstract": "This document presents some early explorations of applying Softly Masked\nLanguage Modelling (SMLM) to symbolic music generation. SMLM can be seen as a\ngeneralisation of masked language modelling (MLM), where instead of each\nelement of the input set being either known or unknown, each element can be\nknown, unknown or partly known. We demonstrate some results of applying SMLM to\nconstrained symbolic music generation using a transformer encoder architecture.\nSeveral audio examples are available at\nhttps://erl-j.github.io/smlm-web-supplement/", "published": "2023-05-05 13:37:04", "link": "http://arxiv.org/abs/2305.03530v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Blind identification of Ambisonic reduced room impulse response", "abstract": "Recently proposed Generalized Time-domain Velocity Vector (GTVV) is a\ngeneralization of relative room impulse response in spherical harmonic (aka\nAmbisonic) domain that allows for blind estimation of early-echo parameters:\nthe directions and relative delays of individual reflections. However, the\nderived closed-form expression of GTVV mandates few assumptions to hold, most\nimportant being that the impulse response of the reference signal needs to be a\nminimum-phase filter. In practice, the reference is obtained by spatial\nfiltering towards the Direction-of-Arrival of the source, and the\naforementioned condition is bounded by the performance of the applied\nbeamformer (and thus, by the Ambisonic array order). In the present work, we\nsuggest to circumvent this problem by directly modeling the impulse responses\nconstituting the GTVV time series, which permits not only to relax the initial\nassumptions, but also to extract the information therein in a more consistent\nand efficient manner, entering the realm of blind system identification.\nExperiments using measured room impulse responses confirm the effectiveness of\nthe proposed approach.", "published": "2023-05-05 14:08:04", "link": "http://arxiv.org/abs/2305.03558v3", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "A vector quantized masked autoencoder for audiovisual speech emotion\n  recognition", "abstract": "The limited availability of labeled data is a major challenge in audiovisual\nspeech emotion recognition (SER). Self-supervised learning approaches have\nrecently been proposed to mitigate the need for labeled data in various\napplications. This paper proposes the VQ-MAE-AV model, a vector quantized\nmasked autoencoder (MAE) designed for audiovisual speech self-supervised\nrepresentation learning and applied to SER. Unlike previous approaches, the\nproposed method employs a self-supervised paradigm based on discrete audio and\nvisual speech representations learned by vector quantized variational\nautoencoders. A multimodal MAE with self- or cross-attention mechanisms is\nproposed to fuse the audio and visual speech modalities and to learn local and\nglobal representations of the audiovisual speech sequence, which are then used\nfor an SER downstream task. Experimental results show that the proposed\napproach, which is pre-trained on the VoxCeleb2 database and fine-tuned on\nstandard emotional audiovisual speech datasets, outperforms the\nstate-of-the-art audiovisual SER methods. Extensive ablation experiments are\nalso provided to assess the contribution of the different model components.", "published": "2023-05-05 14:19:46", "link": "http://arxiv.org/abs/2305.03568v2", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A multimodal dynamical variational autoencoder for audiovisual speech\n  representation learning", "abstract": "In this paper, we present a multimodal and dynamical VAE (MDVAE) applied to\nunsupervised audio-visual speech representation learning. The latent space is\nstructured to dissociate the latent dynamical factors that are shared between\nthe modalities from those that are specific to each modality. A static latent\nvariable is also introduced to encode the information that is constant over\ntime within an audiovisual speech sequence. The model is trained in an\nunsupervised manner on an audiovisual emotional speech dataset, in two stages.\nIn the first stage, a vector quantized VAE (VQ-VAE) is learned independently\nfor each modality, without temporal modeling. The second stage consists in\nlearning the MDVAE model on the intermediate representation of the VQ-VAEs\nbefore quantization. The disentanglement between static versus dynamical and\nmodality-specific versus modality-common information occurs during this second\ntraining stage. Extensive experiments are conducted to investigate how\naudiovisual speech latent factors are encoded in the latent space of MDVAE.\nThese experiments include manipulating audiovisual speech, audiovisual facial\nimage denoising, and audiovisual speech emotion recognition. The results show\nthat MDVAE effectively combines the audio and visual information in its latent\nspace. They also show that the learned static representation of audiovisual\nspeech can be used for emotion recognition with few labeled data, and with\nbetter accuracy compared with unimodal baselines and a state-of-the-art\nsupervised model based on an audiovisual transformer architecture.", "published": "2023-05-05 14:37:26", "link": "http://arxiv.org/abs/2305.03582v3", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Physics-Based Acoustic Holograms", "abstract": "Advances in additive manufacturing have enabled the realisation of\ninexpensive, scalable, diffractive acoustic lenses that can be used to generate\ncomplex acoustic fields via phase and/or amplitude modulation. However, the\ndesign of these holograms relies on a thin-element approximation adapted from\noptics which can severely limit the fidelity of the realised acoustic field.\nHere, we introduce physics-based acoustic holograms with a complex internal\nstructure. The structures are designed using a differentiable acoustic model\nwith manufacturing constraints via optimisation of the acoustic property\ndistribution within the hologram. The holograms can be fabricated simply and\ninexpensively using contemporary 3D printers. Experimental measurements\ndemonstrate a significant improvement compared to conventional thin-element\nholograms.", "published": "2023-05-05 15:37:17", "link": "http://arxiv.org/abs/2305.03625v1", "categories": ["cs.SD", "eess.AS", "physics.app-ph"], "primary_category": "cs.SD"}
{"title": "Mask The Bias: Improving Domain-Adaptive Generalization of CTC-based ASR\n  with Internal Language Model Estimation", "abstract": "End-to-end ASR models trained on large amount of data tend to be implicitly\nbiased towards language semantics of the training data. Internal language model\nestimation (ILME) has been proposed to mitigate this bias for autoregressive\nmodels such as attention-based encoder-decoder and RNN-T. Typically, ILME is\nperformed by modularizing the acoustic and language components of the model\narchitecture, and eliminating the acoustic input to perform log-linear\ninterpolation with the text-only posterior. However, for CTC-based ASR, it is\nnot as straightforward to decouple the model into such acoustic and language\ncomponents, as CTC log-posteriors are computed in a non-autoregressive manner.\nIn this work, we propose a novel ILME technique for CTC-based ASR models. Our\nmethod iteratively masks the audio timesteps to estimate a pseudo\nlog-likelihood of the internal LM by accumulating log-posteriors for only the\nmasked timesteps. Extensive evaluation across multiple out-of-domain datasets\nreveals that the proposed approach improves WER by up to 9.8% and OOV F1-score\nby up to 24.6% relative to Shallow Fusion, when only text data from target\ndomain is available. In the case of zero-shot domain adaptation, with no access\nto any target domain data, we demonstrate that removing the source domain bias\nwith ILME can still outperform Shallow Fusion to improve WER by up to 9.3%\nrelative.", "published": "2023-05-05 20:35:42", "link": "http://arxiv.org/abs/2305.03837v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Synthesizing Cough Audio with GAN for COVID-19 Detection", "abstract": "For this final year project, the goal is to add to the published works within\ndata synthesis for health care. The end product of this project is a trained\nmodel that generates synthesized images that can be used to expand a medical\ndataset (Pierre, 2021). The chosen domain for this project is the Covid-19\ncough recording which is have been proven to be a viable data source for\ndetecting Covid. This is an under-explored domain despite its huge importance\nbecause of the limited dataset available for the task. Once this model is\ndeveloped its impact will be illustrated by training state-of-the-art models\nwith and without the expanded dataset and measuring the difference in\nperformance. Lastly, everything will be put together by embedding the model\nwithin a web application to illustrate its power. To achieve the said goals, an\nextensive literature review will be conducted into the recent innovations for\nimage synthesis using generative models.", "published": "2023-05-05 10:36:07", "link": "http://arxiv.org/abs/2305.04810v1", "categories": ["cs.SD", "eess.AS", "q-bio.QM"], "primary_category": "cs.SD"}
