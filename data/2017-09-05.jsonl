{"title": "Using $k$-way Co-occurrences for Learning Word Embeddings", "abstract": "Co-occurrences between two words provide useful insights into the semantics\nof those words. Consequently, numerous prior work on word embedding learning\nhave used co-occurrences between two words as the training signal for learning\nword embeddings. However, in natural language texts it is common for multiple\nwords to be related and co-occurring in the same context. We extend the notion\nof co-occurrences to cover $k(\\geq\\!\\!2)$-way co-occurrences among a set of\n$k$-words. Specifically, we prove a theoretical relationship between the joint\nprobability of $k(\\geq\\!\\!2)$ words, and the sum of $\\ell_2$ norms of their\nembeddings. Next, we propose a learning objective motivated by our theoretical\nresult that utilises $k$-way co-occurrences for learning word embeddings. Our\nexperimental results show that the derived theoretical relationship does indeed\nhold empirically, and despite data sparsity, for some smaller $k$ values,\n$k$-way embeddings perform comparably or better than $2$-way embeddings in a\nrange of tasks.", "published": "2017-09-05 00:25:58", "link": "http://arxiv.org/abs/1709.01199v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Optimizing for Measure of Performance in Max-Margin Parsing", "abstract": "Many statistical learning problems in the area of natural language processing\nincluding sequence tagging, sequence segmentation and syntactic parsing has\nbeen successfully approached by means of structured prediction methods. An\nappealing property of the corresponding discriminative learning algorithms is\ntheir ability to integrate the loss function of interest directly into the\noptimization process, which potentially can increase the resulting performance\naccuracy. Here, we demonstrate on the example of constituency parsing how to\noptimize for F1-score in the max-margin framework of structural SVM. In\nparticular, the optimization is with respect to the original (not binarized)\ntrees.", "published": "2017-09-05 19:27:22", "link": "http://arxiv.org/abs/1709.01562v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Document Distance Measures and Unsupervised Document Revision\n  Detection", "abstract": "In this paper, we model the document revision detection problem as a minimum\ncost branching problem that relies on computing document distances.\nFurthermore, we propose two new document distance measures, word vector-based\nDynamic Time Warping (wDTW) and word vector-based Tree Edit Distance (wTED).\nOur revision detection system is designed for a large scale corpus and\nimplemented in Apache Spark. We demonstrate that our system can more precisely\ndetect revisions than state-of-the-art methods by utilizing the Wikipedia\nrevision dumps https://snap.stanford.edu/data/wiki-meta.html and simulated data\nsets.", "published": "2017-09-05 06:47:03", "link": "http://arxiv.org/abs/1709.01256v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Language Modeling by Clustering with Word Embeddings for Text\n  Readability Assessment", "abstract": "We present a clustering-based language model using word embeddings for text\nreadability prediction. Presumably, an Euclidean semantic space hypothesis\nholds true for word embeddings whose training is done by observing word\nco-occurrences. We argue that clustering with word embeddings in the metric\nspace should yield feature representations in a higher semantic space\nappropriate for text regression. Also, by representing features in terms of\nhistograms, our approach can naturally address documents of varying lengths. An\nempirical evaluation using the Common Core Standards corpus reveals that the\nfeatures formed on our clustering-based language model significantly improve\nthe previously known results for the same corpus in readability prediction. We\nalso evaluate the task of sentence matching based on semantic relatedness using\nthe Wiki-SimpleWiki corpus and find that our features lead to superior matching\nperformance.", "published": "2017-09-05 02:38:44", "link": "http://arxiv.org/abs/1709.01888v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sequence Prediction with Neural Segmental Models", "abstract": "Segments that span contiguous parts of inputs, such as phonemes in speech,\nnamed entities in sentences, actions in videos, occur frequently in sequence\nprediction problems. Segmental models, a class of models that explicitly\nhypothesizes segments, have allowed the exploration of rich segment features\nfor sequence prediction. However, segmental models suffer from slow decoding,\nhampering the use of computationally expensive features.\n  In this thesis, we introduce discriminative segmental cascades, a multi-pass\ninference framework that allows us to improve accuracy by adding higher-order\nfeatures and neural segmental features while maintaining efficiency. We also\nshow that instead of including more features to obtain better accuracy,\nsegmental cascades can be used to speed up training and decoding.\n  Segmental models, similarly to conventional speech recognizers, are typically\ntrained in multiple stages. In the first stage, a frame classifier is trained\nwith manual alignments, and then in the second stage, segmental models are\ntrained with manual alignments and the out- puts of the frame classifier.\nHowever, obtaining manual alignments are time-consuming and expensive. We\nexplore end-to-end training for segmental models with various loss functions,\nand show how end-to-end training with marginal log loss can eliminate the need\nfor detailed manual alignments.\n  We draw the connections between the marginal log loss and a popular\nend-to-end training approach called connectionist temporal classification. We\npresent a unifying framework for various end-to-end graph search-based models,\nsuch as hidden Markov models, connectionist temporal classification, and\nsegmental models. Finally, we discuss possible extensions of segmental models\nto large-vocabulary sequence prediction tasks.", "published": "2017-09-05 19:53:35", "link": "http://arxiv.org/abs/1709.01572v3", "categories": ["cs.CL", "cs.LG", "cs.SD"], "primary_category": "cs.CL"}
