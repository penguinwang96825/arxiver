{"title": "Differentially Private Synthetic Data via Foundation Model APIs 2: Text", "abstract": "Text data has become extremely valuable due to the emergence of machine\nlearning algorithms that learn from it. A lot of high-quality text data\ngenerated in the real world is private and therefore cannot be shared or used\nfreely due to privacy concerns. Generating synthetic replicas of private text\ndata with a formal privacy guarantee, i.e., differential privacy (DP), offers a\npromising and scalable solution. However, existing methods necessitate DP\nfinetuning of large language models (LLMs) on private data to generate DP\nsynthetic data. This approach is not viable for proprietary LLMs (e.g.,\nGPT-3.5) and also demands considerable computational resources for open-source\nLLMs. Lin et al. (2024) recently introduced the Private Evolution (PE)\nalgorithm to generate DP synthetic images with only API access to diffusion\nmodels. In this work, we propose an augmented PE algorithm, named Aug-PE, that\napplies to the complex setting of text. We use API access to an LLM and\ngenerate DP synthetic text without any model training. We conduct comprehensive\nexperiments on three benchmark datasets. Our results demonstrate that Aug-PE\nproduces DP synthetic text that yields competitive utility with the SOTA DP\nfinetuning baselines. This underscores the feasibility of relying solely on API\naccess of LLMs to produce high-quality DP synthetic texts, thereby facilitating\nmore accessible routes to privacy-preserving LLM applications. Our code and\ndata are available at https://github.com/AI-secure/aug-pe.", "published": "2024-03-04 05:57:50", "link": "http://arxiv.org/abs/2403.01749v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Derivative-Free Optimization for Low-Rank Adaptation in Large Language\n  Models", "abstract": "Parameter-efficient tuning methods such as LoRA could achieve comparable\nperformance to model tuning by tuning a small portion of the parameters.\nHowever, substantial computational resources are still required, as this\nprocess involves calculating gradients and performing back-propagation\nthroughout the model. Much effort has recently been devoted to utilizing the\nderivative-free optimization method to eschew the computation of gradients and\nshowcase an augmented level of robustness in few-shot settings. In this paper,\nwe prepend the low-rank modules into each self-attention layer of the model and\nemploy two derivative-free optimization methods to optimize these low-rank\nmodules at each layer alternately. Extensive results on various tasks and\nlanguage models demonstrate that our proposed method achieves substantial\nimprovement and exhibits clear advantages in memory usage and convergence speed\ncompared to existing gradient-based parameter-efficient tuning and\nderivative-free optimization methods in few-shot settings.", "published": "2024-03-04 06:20:31", "link": "http://arxiv.org/abs/2403.01754v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KeNet:Knowledge-enhanced Doc-Label Attention Network for Multi-label\n  text classification", "abstract": "Multi-Label Text Classification (MLTC) is a fundamental task in the field of\nNatural Language Processing (NLP) that involves the assignment of multiple\nlabels to a given text. MLTC has gained significant importance and has been\nwidely applied in various domains such as topic recognition, recommendation\nsystems, sentiment analysis, and information retrieval. However, traditional\nmachine learning and Deep neural network have not yet addressed certain issues,\nsuch as the fact that some documents are brief but have a large number of\nlabels and how to establish relationships between the labels. It is imperative\nto additionally acknowledge that the significance of knowledge is substantiated\nin the realm of MLTC. To address this issue, we provide a novel approach known\nas Knowledge-enhanced Doc-Label Attention Network (KeNet). Specifically, we\ndesign an Attention Network that incorporates external knowledge, label\nembedding, and a comprehensive attention mechanism. In contrast to conventional\nmethods, we use comprehensive representation of documents, knowledge and labels\nto predict all labels for each single text. Our approach has been validated by\ncomprehensive research conducted on three multi-label datasets. Experimental\nresults demonstrate that our method outperforms state-of-the-art MLTC method.\nAdditionally, a case study is undertaken to illustrate the practical\nimplementation of KeNet.", "published": "2024-03-04 06:52:19", "link": "http://arxiv.org/abs/2403.01767v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WebCiteS: Attributed Query-Focused Summarization on Chinese Web Search\n  Results with Citations", "abstract": "Enhancing the attribution in large language models (LLMs) is a crucial task.\nOne feasible approach is to enable LLMs to cite external sources that support\ntheir generations. However, existing datasets and evaluation methods in this\ndomain still exhibit notable limitations. In this work, we formulate the task\nof attributed query-focused summarization (AQFS) and present WebCiteS, a\nChinese dataset featuring 7k human-annotated summaries with citations. WebCiteS\nderives from real-world user queries and web search results, offering a\nvaluable resource for model training and evaluation. Prior works in attribution\nevaluation do not differentiate between groundedness errors and citation\nerrors. They also fall short in automatically verifying sentences that draw\npartial support from multiple sources. We tackle these issues by developing\ndetailed metrics and enabling the automatic evaluator to decompose the\nsentences into sub-claims for fine-grained verification. Our comprehensive\nevaluation of both open-source and proprietary models on WebCiteS highlights\nthe challenge LLMs face in correctly citing sources, underscoring the necessity\nfor further improvement. The dataset and code will be open-sourced to\nfacilitate further research in this crucial field.", "published": "2024-03-04 07:06:41", "link": "http://arxiv.org/abs/2403.01774v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Multi-Domain Automatic Short Answer Grading through an\n  Explainable Neuro-Symbolic Pipeline", "abstract": "Grading short answer questions automatically with interpretable reasoning\nbehind the grading decision is a challenging goal for current transformer\napproaches. Justification cue detection, in combination with logical reasoners,\nhas shown a promising direction for neuro-symbolic architectures in ASAG. But,\none of the main challenges is the requirement of annotated justification cues\nin the students' responses, which only exist for a few ASAG datasets. To\novercome this challenge, we contribute (1) a weakly supervised annotation\nprocedure for justification cues in ASAG datasets, and (2) a neuro-symbolic\nmodel for explainable ASAG based on justification cues. Our approach improves\nupon the RMSE by 0.24 to 0.3 compared to the state-of-the-art on the Short\nAnswer Feedback dataset in a bilingual, multi-domain, and multi-question\ntraining setup. This result shows that our approach provides a promising\ndirection for generating high-quality grades and accompanying explanations for\nfuture research in ASAG and educational NLP.", "published": "2024-03-04 07:58:26", "link": "http://arxiv.org/abs/2403.01811v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NusaBERT: Teaching IndoBERT to be Multilingual and Multicultural", "abstract": "Indonesia's linguistic landscape is remarkably diverse, encompassing over 700\nlanguages and dialects, making it one of the world's most linguistically rich\nnations. This diversity, coupled with the widespread practice of code-switching\nand the presence of low-resource regional languages, presents unique challenges\nfor modern pre-trained language models. In response to these challenges, we\ndeveloped NusaBERT, building upon IndoBERT by incorporating vocabulary\nexpansion and leveraging a diverse multilingual corpus that includes regional\nlanguages and dialects. Through rigorous evaluation across a range of\nbenchmarks, NusaBERT demonstrates state-of-the-art performance in tasks\ninvolving multiple languages of Indonesia, paving the way for future natural\nlanguage understanding research for under-represented languages.", "published": "2024-03-04 08:05:34", "link": "http://arxiv.org/abs/2403.01817v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CET2: Modelling Topic Transitions for Coherent and Engaging\n  Knowledge-Grounded Conversations", "abstract": "Knowledge-grounded dialogue systems aim to generate coherent and engaging\nresponses based on the dialogue contexts and selected external knowledge.\nPrevious knowledge selection methods tend to rely too heavily on the dialogue\ncontexts or over-emphasize the new information in the selected knowledge,\nresulting in the selection of repetitious or incongruous knowledge and further\ngenerating repetitive or incoherent responses, as the generation of the\nresponse depends on the chosen knowledge. To address these shortcomings, we\nintroduce a Coherent and Engaging Topic Transition (CET2) framework to model\ntopic transitions for selecting knowledge that is coherent to the context of\nthe conversations while providing adequate knowledge diversity for topic\ndevelopment. Our CET2 framework considers multiple factors for knowledge\nselection, including valid transition logic from dialogue contexts to the\nfollowing topics and systematic comparisons between available knowledge\ncandidates. Extensive experiments on two public benchmarks demonstrate the\nsuperiority and the better generalization ability of CET2 on knowledge\nselection. This is due to our well-designed transition features and comparative\nknowledge selection strategy, which are more transferable to conversations\nabout unseen topics. Analysis of fine-grained knowledge selection accuracy also\nshows that CET2 can better balance topic entailment (contextual coherence) and\ndevelopment (knowledge diversity) in dialogue than existing approaches.", "published": "2024-03-04 08:55:34", "link": "http://arxiv.org/abs/2403.01848v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Improved Traditional Chinese Evaluation Suite for Foundation Model", "abstract": "We present TMMLU+, a new benchmark designed for Traditional Chinese language\nunderstanding. TMMLU+ is a multi-choice question-answering dataset with 66\nsubjects from elementary to professional level. It is six times larger and\nboasts a more balanced subject distribution than its predecessor, Taiwan\nMassive Multitask Language Understanding (TMMLU). We also benchmark\nclosed-source models and 26 open-weight Chinese large language models (LLMs) of\nparameters ranging from 1.8B to 72B on the proposed TMMLU+. Our findings reveal\nthat (1.) Traditional Chinese models still trail behind their Simplified\nChinese counterparts, highlighting a need for more focused advancements in LLMs\ncatering to Traditional Chinese. (2.) Current LLMs still fall short of human\nperformance in average scores, indicating a potential need for future research\nto delve deeper into social science and humanities subjects. (3.) Among all the\ntokenization compression metrics examined, we identify that only the fertility\nscore uniquely demonstrates strong correlations with our benchmark results. We\nforesee that TMMLU+ will pinpoint areas for future model improvement, thereby\nnarrowing the gap between machine and human linguistic capabilities and\nsupporting researchers in developing Traditional Chinese LLMs. Our dataset,\nalong with the benchmark source code, is accessible at\nhuggingface.co/datasets/ikala/tmmluplus.", "published": "2024-03-04 09:13:33", "link": "http://arxiv.org/abs/2403.01858v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fostering the Ecosystem of Open Neural Encoders for Portuguese with\n  Albertina PT* Family", "abstract": "To foster the neural encoding of Portuguese, this paper contributes\nfoundation encoder models that represent an expansion of the still very scarce\necosystem of large language models specifically developed for this language\nthat are fully open, in the sense that they are open source and openly\ndistributed for free under an open license for any purpose, thus including\nresearch and commercial usages. Like most languages other than English,\nPortuguese is low-resourced in terms of these foundational language resources,\nthere being the inaugural 900 million parameter Albertina and 335 million\nBertimbau. Taking this couple of models as an inaugural set, we present the\nextension of the ecosystem of state-of-the-art open encoders for Portuguese\nwith a larger, top performance-driven model with 1.5 billion parameters, and a\nsmaller, efficiency-driven model with 100 million parameters. While achieving\nthis primary goal, further results that are relevant for this ecosystem were\nobtained as well, namely new datasets for Portuguese based on the SuperGLUE\nbenchmark, which we also distribute openly.", "published": "2024-03-04 09:56:47", "link": "http://arxiv.org/abs/2403.01897v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Arabic Text Sentiment Analysis: Reinforcing Human-Performed Surveys with\n  Wider Topic Analysis", "abstract": "Sentiment analysis (SA) has been, and is still, a thriving research area.\nHowever, the task of Arabic sentiment analysis (ASA) is still underrepresented\nin the body of research. This study offers the first in-depth and in-breadth\nanalysis of existing ASA studies of textual content and identifies their common\nthemes, domains of application, methods, approaches, technologies and\nalgorithms used. The in-depth study manually analyses 133 ASA papers published\nin the English language between 2002 and 2020 from four academic databases\n(SAGE, IEEE, Springer, WILEY) and from Google Scholar. The in-breadth study\nuses modern, automatic machine learning techniques, such as topic modelling and\ntemporal analysis, on Open Access resources, to reinforce themes and trends\nidentified by the prior study, on 2297 ASA publications between 2010-2020. The\nmain findings show the different approaches used for ASA: machine learning,\nlexicon-based and hybrid approaches. Other findings include ASA 'winning'\nalgorithms (SVM, NB, hybrid methods). Deep learning methods, such as LSTM can\nprovide higher accuracy, but for ASA sometimes the corpora are not large enough\nto support them. Additionally, whilst there are some ASA corpora and lexicons,\nmore are required. Specifically, Arabic tweets corpora and datasets are\ncurrently only moderately sized. Moreover, Arabic lexicons that have high\ncoverage contain only Modern Standard Arabic (MSA) words, and those with Arabic\ndialects are quite small. Thus, new corpora need to be created. On the other\nhand, ASA tools are stringently lacking. There is a need to develop ASA tools\nthat can be used in industry, as well as in academia, for Arabic text SA.\nHence, our study offers insights into the challenges associated with ASA\nresearch and provides suggestions for ways to move the field forward such as\nlack of Dialectical Arabic resource, Arabic tweets, corpora and data sets for\nSA.", "published": "2024-03-04 10:37:48", "link": "http://arxiv.org/abs/2403.01921v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IndicVoices: Towards building an Inclusive Multilingual Speech Dataset\n  for Indian Languages", "abstract": "We present INDICVOICES, a dataset of natural and spontaneous speech\ncontaining a total of 7348 hours of read (9%), extempore (74%) and\nconversational (17%) audio from 16237 speakers covering 145 Indian districts\nand 22 languages. Of these 7348 hours, 1639 hours have already been\ntranscribed, with a median of 73 hours per language. Through this paper, we\nshare our journey of capturing the cultural, linguistic and demographic\ndiversity of India to create a one-of-its-kind inclusive and representative\ndataset. More specifically, we share an open-source blueprint for data\ncollection at scale comprising of standardised protocols, centralised tools, a\nrepository of engaging questions, prompts and conversation scenarios spanning\nmultiple domains and topics of interest, quality control mechanisms,\ncomprehensive transcription guidelines and transcription tools. We hope that\nthis open source blueprint will serve as a comprehensive starter kit for data\ncollection efforts in other multilingual regions of the world. Using\nINDICVOICES, we build IndicASR, the first ASR model to support all the 22\nlanguages listed in the 8th schedule of the Constitution of India. All the\ndata, tools, guidelines, models and other materials developed as a part of this\nwork will be made publicly available", "published": "2024-03-04 10:42:08", "link": "http://arxiv.org/abs/2403.01926v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing and Adapting Large Language Models for Few-Shot Multilingual\n  NLU: Are We There Yet?", "abstract": "Supervised fine-tuning (SFT), supervised instruction tuning (SIT) and\nin-context learning (ICL) are three alternative, de facto standard approaches\nto few-shot learning. ICL has gained popularity recently with the advent of\nLLMs due to its simplicity and sample efficiency. Prior research has conducted\nonly limited investigation into how these approaches work for multilingual\nfew-shot learning, and the focus so far has been mostly on their performance.\nIn this work, we present an extensive and systematic comparison of the three\napproaches, testing them on 6 high- and low-resource languages, three different\nNLU tasks, and a myriad of language and domain setups. Importantly, performance\nis only one aspect of the comparison, where we also analyse the approaches\nthrough the optics of their computational, inference and financial costs. Our\nobservations show that supervised instruction tuning has the best trade-off\nbetween performance and resource requirements. As another contribution, we\nanalyse the impact of target language adaptation of pretrained LLMs and find\nthat the standard adaptation approaches can (superficially) improve target\nlanguage generation capabilities, but language understanding elicited through\nICL does not improve and remains limited, with low scores especially for\nlow-resource languages.", "published": "2024-03-04 10:48:13", "link": "http://arxiv.org/abs/2403.01929v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VariErr NLI: Separating Annotation Error from Human Label Variation", "abstract": "Human label variation arises when annotators assign different labels to the\nsame item for valid reasons, while annotation errors occur when labels are\nassigned for invalid reasons. These two issues are prevalent in NLP benchmarks,\nyet existing research has studied them in isolation. To the best of our\nknowledge, there exists no prior work that focuses on teasing apart error from\nsignal, especially in cases where signal is beyond black-and-white. To fill\nthis gap, we introduce a systematic methodology and a new dataset, VariErr\n(variation versus error), focusing on the NLI task in English. We propose a\n2-round annotation procedure with annotators explaining each label and\nsubsequently judging the validity of label-explanation pairs. VariErr contains\n7,732 validity judgments on 1,933 explanations for 500 re-annotated MNLI items.\nWe assess the effectiveness of various automatic error detection (AED) methods\nand GPTs in uncovering errors versus human label variation. We find that\nstate-of-the-art AED methods significantly underperform GPTs and humans. While\nGPT-4 is the best system, it still falls short of human performance. Our\nmethodology is applicable beyond NLI, offering fertile ground for future\nresearch on error versus plausible variation, which in turn can yield better\nand more trustworthy NLP systems.", "published": "2024-03-04 10:57:14", "link": "http://arxiv.org/abs/2403.01931v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AS-ES Learning: Towards Efficient CoT Learning in Small Models", "abstract": "Chain-of-Thought (CoT) serves as a critical emerging ability in LLMs,\nespecially when it comes to logical reasoning. Attempts have been made to\ninduce such ability in small models as well by distilling from the data with\nCoT generated by Large Language Models (LLMs). However, existing methods often\nsimply generate and incorporate more data from LLMs and fail to note the\nimportance of efficiently utilizing existing CoT data. We here propose a new\ntraining paradigm AS-ES (Abstractive Segments - Extractive Segments) learning,\nwhich exploits the inherent information in CoT for iterative generation.\nExperiments show that our methods surpass the direct seq2seq training on\nCoT-extensive tasks like MWP and PET summarization, without data augmentation\nor altering the model itself. Furthermore, we explore the reason behind the\ninefficiency of small models in learning CoT and provide an explanation of why\nAS-ES learning works, giving insights into the underlying mechanism of CoT.", "published": "2024-03-04 12:13:59", "link": "http://arxiv.org/abs/2403.01969v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-perspective Improvement of Knowledge Graph Completion with Large\n  Language Models", "abstract": "Knowledge graph completion (KGC) is a widely used method to tackle\nincompleteness in knowledge graphs (KGs) by making predictions for missing\nlinks. Description-based KGC leverages pre-trained language models to learn\nentity and relation representations with their names or descriptions, which\nshows promising results. However, the performance of description-based KGC is\nstill limited by the quality of text and the incomplete structure, as it lacks\nsufficient entity descriptions and relies solely on relation names, leading to\nsub-optimal results. To address this issue, we propose MPIKGC, a general\nframework to compensate for the deficiency of contextualized knowledge and\nimprove KGC by querying large language models (LLMs) from various perspectives,\nwhich involves leveraging the reasoning, explanation, and summarization\ncapabilities of LLMs to expand entity descriptions, understand relations, and\nextract structures, respectively. We conducted extensive evaluation of the\neffectiveness and improvement of our framework based on four description-based\nKGC models and four datasets, for both link prediction and triplet\nclassification tasks.", "published": "2024-03-04 12:16:15", "link": "http://arxiv.org/abs/2403.01972v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SciAssess: Benchmarking LLM Proficiency in Scientific Literature\n  Analysis", "abstract": "Recent breakthroughs in Large Language Models (LLMs) have revolutionized\nscientific literature analysis. However, existing benchmarks fail to adequately\nevaluate the proficiency of LLMs in this domain, particularly in scenarios\nrequiring higher-level abilities beyond mere memorization and the handling of\nmultimodal data. In response to this gap, we introduce SciAssess, a benchmark\nspecifically designed for the comprehensive evaluation of LLMs in scientific\nliterature analysis. It aims to thoroughly assess the efficacy of LLMs by\nevaluating their capabilities in Memorization (L1), Comprehension (L2), and\nAnalysis \\& Reasoning (L3). It encompasses a variety of tasks drawn from\ndiverse scientific fields, including biology, chemistry, material, and\nmedicine. To ensure the reliability of SciAssess, rigorous quality control\nmeasures have been implemented, ensuring accuracy, anonymization, and\ncompliance with copyright standards. SciAssess evaluates 11 LLMs, highlighting\ntheir strengths and areas for improvement. We hope this evaluation supports the\nongoing development of LLM applications in scientific literature analysis.\nSciAssess and its resources are available at\n\\url{https://github.com/sci-assess/SciAssess}.", "published": "2024-03-04 12:19:28", "link": "http://arxiv.org/abs/2403.01976v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language and Speech Technology for Central Kurdish Varieties", "abstract": "Kurdish, an Indo-European language spoken by over 30 million speakers, is\nconsidered a dialect continuum and known for its diversity in language\nvarieties. Previous studies addressing language and speech technology for\nKurdish handle it in a monolithic way as a macro-language, resulting in\ndisparities for dialects and varieties for which there are few resources and\ntools available. In this paper, we take a step towards developing resources for\nlanguage and speech technology for varieties of Central Kurdish, creating a\ncorpus by transcribing movies and TV series as an alternative to fieldwork.\nAdditionally, we report the performance of machine translation, automatic\nspeech recognition, and language identification as downstream tasks evaluated\non Central Kurdish varieties. Data and models are publicly available under an\nopen license at https://github.com/sinaahmadi/CORDI.", "published": "2024-03-04 12:27:32", "link": "http://arxiv.org/abs/2403.01983v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FKA-Owl: Advancing Multimodal Fake News Detection through\n  Knowledge-Augmented LVLMs", "abstract": "The massive generation of multimodal fake news involving both text and images\nexhibits substantial distribution discrepancies, prompting the need for\ngeneralized detectors. However, the insulated nature of training restricts the\ncapability of classical detectors to obtain open-world facts. While Large\nVision-Language Models (LVLMs) have encoded rich world knowledge, they are not\ninherently tailored for combating fake news and struggle to comprehend local\nforgery details. In this paper, we propose FKA-Owl, a novel framework that\nleverages forgery-specific knowledge to augment LVLMs, enabling them to reason\nabout manipulations effectively. The augmented forgery-specific knowledge\nincludes semantic correlation between text and images, and artifact trace in\nimage manipulation. To inject these two kinds of knowledge into the LVLM, we\ndesign two specialized modules to establish their representations,\nrespectively. The encoded knowledge embeddings are then incorporated into\nLVLMs. Extensive experiments on the public benchmark demonstrate that FKA-Owl\nachieves superior cross-domain performance compared to previous methods. Code\nis publicly available at https://liuxuannan.github.io/FKA_Owl.github.io/.", "published": "2024-03-04 12:35:09", "link": "http://arxiv.org/abs/2403.01988v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Vanilla Transformers are Transfer Capability Teachers", "abstract": "Recently, Mixture of Experts (MoE) Transformers have garnered increasing\nattention due to their advantages in model capacity and computational\nefficiency. However, studies have indicated that MoE Transformers underperform\nvanilla Transformers in many downstream tasks, significantly diminishing the\npractical value of MoE models. To explain this issue, we propose that the\npre-training performance and transfer capability of a model are joint\ndeterminants of its downstream task performance. MoE models, in comparison to\nvanilla models, have poorer transfer capability, leading to their subpar\nperformance in downstream tasks. To address this issue, we introduce the\nconcept of transfer capability distillation, positing that although vanilla\nmodels have weaker performance, they are effective teachers of transfer\ncapability. The MoE models guided by vanilla models can achieve both strong\npre-training performance and transfer capability, ultimately enhancing their\nperformance in downstream tasks. We design a specific distillation method and\nconduct experiments on the BERT architecture. Experimental results show a\nsignificant improvement in downstream performance of MoE models, and many\nfurther evidences also strongly support the concept of transfer capability\ndistillation. Finally, we attempt to interpret transfer capability distillation\nand provide some insights from the perspective of model feature.", "published": "2024-03-04 12:40:28", "link": "http://arxiv.org/abs/2403.01994v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM-Oriented Retrieval Tuner", "abstract": "Dense Retrieval (DR) is now considered as a promising tool to enhance the\nmemorization capacity of Large Language Models (LLM) such as GPT3 and GPT-4 by\nincorporating external memories. However, due to the paradigm discrepancy\nbetween text generation of LLM and DR, it is still an open challenge to\nintegrate the retrieval and generation tasks in a shared LLM. In this paper, we\npropose an efficient LLM-Oriented Retrieval Tuner, namely LMORT, which\ndecouples DR capacity from base LLM and non-invasively coordinates the\noptimally aligned and uniform layers of the LLM towards a unified DR space,\nachieving an efficient and effective DR without tuning the LLM itself. The\nextensive experiments on six BEIR datasets show that our approach could achieve\ncompetitive zero-shot retrieval performance compared to a range of strong DR\nmodels while maintaining the generation ability of LLM.", "published": "2024-03-04 12:50:25", "link": "http://arxiv.org/abs/2403.01999v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topic Aware Probing: From Sentence Length Prediction to Idiom\n  Identification how reliant are Neural Language Models on Topic?", "abstract": "Transformer-based Neural Language Models achieve state-of-the-art performance\non various natural language processing tasks. However, an open question is the\nextent to which these models rely on word-order/syntactic or word\nco-occurrence/topic-based information when processing natural language. This\nwork contributes to this debate by addressing the question of whether these\nmodels primarily use topic as a signal, by exploring the relationship between\nTransformer-based models' (BERT and RoBERTa's) performance on a range of\nprobing tasks in English, from simple lexical tasks such as sentence length\nprediction to complex semantic tasks such as idiom token identification, and\nthe sensitivity of these tasks to the topic information. To this end, we\npropose a novel probing method which we call topic-aware probing. Our initial\nresults indicate that Transformer-based models encode both topic and non-topic\ninformation in their intermediate layers, but also that the facility of these\nmodels to distinguish idiomatic usage is primarily based on their ability to\nidentify and encode topic. Furthermore, our analysis of these models'\nperformance on other standard probing tasks suggests that tasks that are\nrelatively insensitive to the topic information are also tasks that are\nrelatively difficult for these models.", "published": "2024-03-04 13:10:08", "link": "http://arxiv.org/abs/2403.02009v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Generation of Multiple-Choice Cloze Questions for Assessing\n  English Vocabulary Using GPT-turbo 3.5", "abstract": "A common way of assessing language learners' mastery of vocabulary is via\nmultiple-choice cloze (i.e., fill-in-the-blank) questions. But the creation of\ntest items can be laborious for individual teachers or in large-scale language\nprograms. In this paper, we evaluate a new method for automatically generating\nthese types of questions using large language models (LLM). The VocaTT\n(vocabulary teaching and training) engine is written in Python and comprises\nthree basic steps: pre-processing target word lists, generating sentences and\ncandidate word options using GPT, and finally selecting suitable word options.\nTo test the efficiency of this system, 60 questions were generated targeting\nacademic words. The generated items were reviewed by expert reviewers who\njudged the well-formedness of the sentences and word options, adding comments\nto items judged not well-formed. Results showed a 75% rate of well-formedness\nfor sentences and 66.85% rate for suitable word options. This is a marked\nimprovement over the generator used earlier in our research which did not take\nadvantage of GPT's capabilities. Post-hoc qualitative analysis reveals several\npoints for improvement in future work including cross-referencing\npart-of-speech tagging, better sentence validation, and improving GPT prompts.", "published": "2024-03-04 14:24:47", "link": "http://arxiv.org/abs/2403.02078v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using LLMs for the Extraction and Normalization of Product Attribute\n  Values", "abstract": "Product offers on e-commerce websites often consist of a product title and a\ntextual product description. In order to enable features such as faceted\nproduct search or to generate product comparison tables, it is necessary to\nextract structured attribute-value pairs from the unstructured product titles\nand descriptions and to normalize the extracted values to a single, unified\nscale for each attribute. This paper explores the potential of using large\nlanguage models (LLMs), such as GPT-3.5 and GPT-4, to extract and normalize\nattribute values from product titles and descriptions. We experiment with\ndifferent zero-shot and few-shot prompt templates for instructing LLMs to\nextract and normalize attribute-value pairs. We introduce the Web Data Commons\n- Product Attribute Value Extraction (WDC-PAVE) benchmark dataset for our\nexperiments. WDC-PAVE consists of product offers from 59 different websites\nwhich provide schema.org annotations. The offers belong to five different\nproduct categories, each with a specific set of attributes. The dataset\nprovides manually verified attribute-value pairs in two forms: (i) directly\nextracted values and (ii) normalized attribute values. The normalization of the\nattribute values requires systems to perform the following types of operations:\nname expansion, generalization, unit of measurement conversion, and string\nwrangling. Our experiments demonstrate that GPT-4 outperforms the PLM-based\nextraction methods SU-OpenTag, AVEQA, and MAVEQA by 10%, achieving an F1-score\nof 91%. For the extraction and normalization of product attribute values, GPT-4\nachieves a similar performance to the extraction scenario, while being\nparticularly strong at string wrangling and name expansion.", "published": "2024-03-04 15:39:59", "link": "http://arxiv.org/abs/2403.02130v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What has LeBenchmark Learnt about French Syntax?", "abstract": "The paper reports on a series of experiments aiming at probing LeBenchmark, a\npretrained acoustic model trained on 7k hours of spoken French, for syntactic\ninformation. Pretrained acoustic models are increasingly used for downstream\nspeech tasks such as automatic speech recognition, speech translation, spoken\nlanguage understanding or speech parsing. They are trained on very low level\ninformation (the raw speech signal), and do not have explicit lexical\nknowledge. Despite that, they obtained reasonable results on tasks that\nrequires higher level linguistic knowledge. As a result, an emerging question\nis whether these models encode syntactic information. We probe each\nrepresentation layer of LeBenchmark for syntax, using the Orf\\'eo treebank, and\nobserve that it has learnt some syntactic information. Our results show that\nsyntactic information is more easily extractable from the middle layers of the\nnetwork, after which a very sharp decrease is observed.", "published": "2024-03-04 16:20:14", "link": "http://arxiv.org/abs/2403.02173v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EEE-QA: Exploring Effective and Efficient Question-Answer\n  Representations", "abstract": "Current approaches to question answering rely on pre-trained language models\n(PLMs) like RoBERTa. This work challenges the existing question-answer encoding\nconvention and explores finer representations. We begin with testing various\npooling methods compared to using the begin-of-sentence token as a question\nrepresentation for better quality. Next, we explore opportunities to\nsimultaneously embed all answer candidates with the question. This enables\ncross-reference between answer choices and improves inference throughput via\nreduced memory usage. Despite their simplicity and effectiveness, these methods\nhave yet to be widely studied in current frameworks. We experiment with\ndifferent PLMs, and with and without the integration of knowledge graphs.\nResults prove that the memory efficacy of the proposed techniques with little\nsacrifice in performance. Practically, our work enhances 38-100% throughput\nwith 26-65% speedups on consumer-grade GPUs by allowing for considerably larger\nbatch sizes. Our work sends a message to the community with promising\ndirections in both representation quality and efficiency for the\nquestion-answering task in natural language processing.", "published": "2024-03-04 16:21:13", "link": "http://arxiv.org/abs/2403.02176v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ProTrix: Building Models for Planning and Reasoning over Tables with\n  Sentence Context", "abstract": "Tables play a crucial role in conveying information in various domains. We\npropose a Plan-then-Reason framework to answer different types of user queries\nover tables with sentence context. The framework first plans the reasoning\npaths over the context, then assigns each step to program-based or textual\nreasoning to reach the final answer. This framework enhances the table\nreasoning abilities for both in-context learning and fine-tuning methods.\nGPT-3.5-Turbo following Plan-then-Reason framework surpasses other prompting\nbaselines without self-consistency while using less API calls and in-context\ndemonstrations. We also construct an instruction tuning set TrixInstruct to\nevaluate the effectiveness of fine-tuning with this framework. We present\nProTrix model family by finetuning models on TrixInstruct. Our experiments show\nthat ProTrix family generalizes to diverse unseen tabular tasks with only 6k\ntraining instances. We further demonstrate that ProTrix can generate accurate\nand faithful explanations to answer complex free-form questions. Our work\nunderscores the importance of the planning and reasoning abilities towards a\nmodel over tabular tasks with generalizability and interpretability. We\nopen-source our dataset and models at https://github.com/WilliamZR/ProTrix.", "published": "2024-03-04 16:21:19", "link": "http://arxiv.org/abs/2403.02177v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PHAnToM: Persona-based Prompting Has An Effect on Theory-of-Mind\n  Reasoning in Large Language Models", "abstract": "The use of LLMs in natural language reasoning has shown mixed results,\nsometimes rivaling or even surpassing human performance in simpler\nclassification tasks while struggling with social-cognitive reasoning, a domain\nwhere humans naturally excel. These differences have been attributed to many\nfactors, such as variations in prompting and the specific LLMs used. However,\nno reasons appear conclusive, and no clear mechanisms have been established in\nprior work. In this study, we empirically evaluate how role-playing prompting\ninfluences Theory-of-Mind (ToM) reasoning capabilities. Grounding our rsearch\nin psychological theory, we propose the mechanism that, beyond the inherent\nvariance in the complexity of reasoning tasks, performance differences arise\nbecause of socially-motivated prompting differences. In an era where prompt\nengineering with role-play is a typical approach to adapt LLMs to new contexts,\nour research advocates caution as models that adopt specific personas might\npotentially result in errors in social-cognitive reasoning.", "published": "2024-03-04 17:34:34", "link": "http://arxiv.org/abs/2403.02246v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Birbal: An efficient 7B instruct-model fine-tuned with curated datasets", "abstract": "LLMOps incur significant costs due to hardware requirements, hindering their\nwidespread accessibility. Additionally, a lack of transparency in model\ntraining methods and data contributes to the majority of models being\nnon-reproducible. To tackle these challenges, the LLM Efficiency Challenge was\nintroduced at NeurIPS Workshop, aiming to adapt foundation models on a diverse\nset of tasks via fine-tuning on a single GPU (RTX 4090 or A100 with 40GB)\nwithin a 24-hour timeframe. In this system description paper, we introduce\nBirbal, our Mistral-7B based winning model, fine-tuned on a single RTX 4090 for\n16 hours. Birbal's success lies in curating high-quality instructions covering\ndiverse tasks, resulting in a 35% performance improvement over second-best\nQwen-14B based submission.", "published": "2024-03-04 17:34:46", "link": "http://arxiv.org/abs/2403.02247v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FENICE: Factuality Evaluation of summarization based on Natural language\n  Inference and Claim Extraction", "abstract": "Recent advancements in text summarization, particularly with the advent of\nLarge Language Models (LLMs), have shown remarkable performance. However, a\nnotable challenge persists as a substantial number of automatically-generated\nsummaries exhibit factual inconsistencies, such as hallucinations. In response\nto this issue, various approaches for the evaluation of consistency for\nsummarization have emerged. Yet, these newly-introduced metrics face several\nlimitations, including lack of interpretability, focus on short document\nsummaries (e.g., news articles), and computational impracticality, especially\nfor LLM-based metrics. To address these shortcomings, we propose Factuality\nEvaluation of summarization based on Natural language Inference and Claim\nExtraction (FENICE), a more interpretable and efficient factuality-oriented\nmetric. FENICE leverages an NLI-based alignment between information in the\nsource document and a set of atomic facts, referred to as claims, extracted\nfrom the summary. Our metric sets a new state of the art on AGGREFACT, the\nde-facto benchmark for factuality evaluation. Moreover, we extend our\nevaluation to a more challenging setting by conducting a human annotation\nprocess of long-form summarization. In the hope of fostering research in\nsummarization factuality evaluation, we release the code of our metric and our\nfactuality annotations of long-form summarization at\nhttps://github.com/Babelscape/FENICE.", "published": "2024-03-04 17:57:18", "link": "http://arxiv.org/abs/2403.02270v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emotion Granularity from Text: An Aggregate-Level Indicator of Mental\n  Health", "abstract": "We are united in how emotions are central to shaping our experiences; and\nyet, individuals differ greatly in how we each identify, categorize, and\nexpress emotions. In psychology, variation in the ability of individuals to\ndifferentiate between emotion concepts is called emotion granularity\n(determined through self-reports of one's emotions). High emotion granularity\nhas been linked with better mental and physical health; whereas low emotion\ngranularity has been linked with maladaptive emotion regulation strategies and\npoor health outcomes. In this work, we propose computational measures of\nemotion granularity derived from temporally-ordered speaker utterances in\nsocial media (in lieu of self-reports that suffer from various biases). We then\ninvestigate the effectiveness of such text-derived measures of emotion\ngranularity in functioning as markers of various mental health conditions\n(MHCs). We establish baseline measures of emotion granularity derived from\ntextual utterances, and show that, at an aggregate level, emotion granularities\nare significantly lower for people self-reporting as having an MHC than for the\ncontrol population. This paves the way towards a better understanding of the\nMHCs, and specifically the role emotions play in our well-being.", "published": "2024-03-04 18:12:10", "link": "http://arxiv.org/abs/2403.02281v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detection of Non-recorded Word Senses in English and Swedish", "abstract": "This study addresses the task of Unknown Sense Detection in English and\nSwedish. The primary objective of this task is to determine whether the meaning\nof a particular word usage is documented in a dictionary or not. For this\npurpose, sense entries are compared with word usages from modern and historical\ncorpora using a pre-trained Word-in-Context embedder that allows us to model\nthis task in a few-shot scenario. Additionally, we use human annotations on the\ntarget corpora to adapt hyperparameters and evaluate our models using 5-fold\ncross-validation. Compared to a random sample from a corpus, our model is able\nto considerably increase the detected number of word usages with non-recorded\nsenses.", "published": "2024-03-04 18:15:14", "link": "http://arxiv.org/abs/2403.02285v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How does Architecture Influence the Base Capabilities of Pre-trained\n  Language Models? A Case Study Based on FFN-Wider and MoE Transformers", "abstract": "Pre-trained language models have been proven to possess strong base\ncapabilities, which not only excel in in-distribution language modeling but\nalso show powerful abilities in out-of-distribution language modeling, transfer\nlearning and few-shot learning. Unlike existing work focusing on the influence\nof scale on base capabilities, our work examines the influence of architecture\non those. Specifically, our concern is: How does architecture influence the\nbase capabilities of pre-trained language models? In this work, we attempt to\nexplain and reverse the decline in base capabilities caused by the architecture\nof FFN-Wider Transformers, seeking to provide some insights. Through analysis,\nwe found the contribution ratio of Multi-Head Attention (a combination\nfunction) to pre-trained language modeling is a key factor affecting base\ncapabilities. FFN-Wider Transformers reduce the contribution ratio of this\ncombination function, leading to a decline in base capabilities. We confirmed\nthis by experiments and proposed Combination Enhanced Architecture (CEA) to\naddress the decline in base capabilities of such models. Significantly, we\nextended our explanation and CEA to Mixture of Experts (MoE) Transformers. We\nsuccessfully achieved significant improvements in base capabilities on a 14B\nparameter MoE model, demonstrating the practical application value of our work.\nThis also indicates that our analysis has a certain guiding significance for\narchitecture analysis, architecture improvement and architecture design.", "published": "2024-03-04 19:33:39", "link": "http://arxiv.org/abs/2403.02436v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Views Are My Own, but Also Yours: Benchmarking Theory of Mind Using\n  Common Ground", "abstract": "Evaluating the theory of mind (ToM) capabilities of language models (LMs) has\nrecently received a great deal of attention. However, many existing benchmarks\nrely on synthetic data, which risks misaligning the resulting experiments with\nhuman behavior. We introduce the first ToM dataset based on naturally occurring\nspoken dialogs, Common-ToM, and show that LMs struggle to demonstrate ToM. We\nthen show that integrating a simple, explicit representation of beliefs\nimproves LM performance on Common-ToM.", "published": "2024-03-04 20:07:17", "link": "http://arxiv.org/abs/2403.02451v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OffensiveLang: A Community Based Implicit Offensive Language Dataset", "abstract": "The widespread presence of hateful languages on social media has resulted in\nadverse effects on societal well-being. As a result, addressing this issue with\nhigh priority has become very important. Hate speech or offensive languages\nexist in both explicit and implicit forms, with the latter being more\nchallenging to detect. Current research in this domain encounters several\nchallenges. Firstly, the existing datasets primarily rely on the collection of\ntexts containing explicit offensive keywords, making it challenging to capture\nimplicitly offensive contents that are devoid of these keywords. Secondly,\ncommon methodologies tend to focus solely on textual analysis, neglecting the\nvaluable insights that community information can provide. In this research\npaper, we introduce a novel dataset OffensiveLang, a community based implicit\noffensive language dataset generated by ChatGPT 3.5 containing data for 38\ndifferent target groups. Despite limitations in generating offensive texts\nusing ChatGPT due to ethical constraints, we present a prompt-based approach\nthat effectively generates implicit offensive languages. To ensure data\nquality, we evaluate the dataset with human. Additionally, we employ a\nprompt-based zero-shot method with ChatGPT and compare the detection results\nbetween human annotation and ChatGPT annotation. We utilize existing\nstate-of-the-art models to see how effective they are in detecting such\nlanguages. The dataset is available here:\nhttps://github.com/AmitDasRup123/OffensiveLang", "published": "2024-03-04 20:34:58", "link": "http://arxiv.org/abs/2403.02472v8", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Emotion Dynamics of Literary Novels", "abstract": "Stories are rich in the emotions they exhibit in their narratives and evoke\nin the readers. The emotional journeys of the various characters within a story\nare central to their appeal. Computational analysis of the emotions of novels,\nhowever, has rarely examined the variation in the emotional trajectories of the\ndifferent characters within them, instead considering the entire novel to\nrepresent a single story arc. In this work, we use character dialogue to\ndistinguish between the emotion arcs of the narration and the various\ncharacters. We analyze the emotion arcs of the various characters in a dataset\nof English literary novels using the framework of Utterance Emotion Dynamics.\nOur findings show that the narration and the dialogue largely express disparate\nemotions through the course of a novel, and that the commonalities or\ndifferences in the emotional arcs of stories are more accurately captured by\nthose associated with individual characters.", "published": "2024-03-04 20:39:21", "link": "http://arxiv.org/abs/2403.02474v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Choose Your Own Adventure: Interactive E-Books to Improve Word Knowledge\n  and Comprehension Skills", "abstract": "The purpose of this feasibility study was to examine the potential impact of\nreading digital interactive e-books on essential skills that support reading\ncomprehension with third-fifth grade students. Students read two e-Books that\ntaught word learning and comprehension monitoring strategies in the service of\nlearning difficult vocabulary and targeted science concepts about hurricanes.\nWe investigated whether specific comprehension strategies including word\nlearning and strategies that supported general reading comprehension,\nsummarization, and question generation, show promise of effectiveness in\nbuilding vocabulary knowledge and comprehension skills in the e-Books. Students\nwere assigned to read one of three versions of each of the e-Books, each\nversion implemented one strategy. The books employed a choose-your-adventure\nformat with embedded comprehension questions that provided students with\nimmediate feedback on their responses. Paired samples t-tests were run to\nexamine pre-to-post differences in learning the targeted vocabulary and science\nconcepts taught in both e-Books. For both e-Books, students demonstrated\nsignificant gains in word learning and on the targeted hurricane concepts.\nAdditionally, Hierarchical Linear Modeling (HLM) revealed that no one strategy\nwas more associated with larger gains than the other. Performance on the\nembedded questions in the books was also associated with greater posttest\noutcomes for both e-Books. This work discusses important considerations for\nimplementation and future development of e-books that can enhance student\nengagement and improve reading comprehension.", "published": "2024-03-04 21:43:59", "link": "http://arxiv.org/abs/2403.02496v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Balancing Enhancement, Harmlessness, and General Capabilities: Enhancing\n  Conversational LLMs with Direct RLHF", "abstract": "In recent advancements in Conversational Large Language Models (LLMs), a\nconcerning trend has emerged, showing that many new base LLMs experience a\nknowledge reduction in their foundational capabilities following Supervised\nFine-Tuning (SFT). This process often leads to issues such as forgetting or a\ndecrease in the base model's abilities. Moreover, fine-tuned models struggle to\nalign with user preferences, inadvertently increasing the generation of toxic\noutputs when specifically prompted. To overcome these challenges, we adopted an\ninnovative approach by completely bypassing SFT and directly implementing\nHarmless Reinforcement Learning from Human Feedback (RLHF). Our method not only\npreserves the base model's general capabilities but also significantly enhances\nits conversational abilities, while notably reducing the generation of toxic\noutputs. Our approach holds significant implications for fields that demand a\nnuanced understanding and generation of responses, such as customer service. We\napplied this methodology to Mistral, the most popular base model, thereby\ncreating Mistral-Plus. Our validation across 11 general tasks demonstrates that\nMistral-Plus outperforms similarly sized open-source base models and their\ncorresponding instruct versions. Importantly, the conversational abilities of\nMistral-Plus were significantly improved, indicating a substantial advancement\nover traditional SFT models in both safety and user preference alignment.", "published": "2024-03-04 22:02:12", "link": "http://arxiv.org/abs/2403.02513v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topic Modeling Analysis of Aviation Accident Reports: A Comparative\n  Study between LDA and NMF Models", "abstract": "Aviation safety is paramount in the modern world, with a continuous\ncommitment to reducing accidents and improving safety standards. Central to\nthis endeavor is the analysis of aviation accident reports, rich textual\nresources that hold insights into the causes and contributing factors behind\naviation mishaps. This paper compares two prominent topic modeling techniques,\nLatent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF),\nin the context of aviation accident report analysis. The study leverages the\nNational Transportation Safety Board (NTSB) Dataset with the primary objective\nof automating and streamlining the process of identifying latent themes and\npatterns within accident reports. The Coherence Value (C_v) metric was used to\nevaluate the quality of generated topics. LDA demonstrates higher topic\ncoherence, indicating stronger semantic relevance among words within topics. At\nthe same time, NMF excelled in producing distinct and granular topics, enabling\na more focused analysis of specific aspects of aviation accidents.", "published": "2024-03-04 01:41:07", "link": "http://arxiv.org/abs/2403.04788v1", "categories": ["cs.CL", "Topic Modeling, Aviation Safety, Aviation Accident Reports, Machine\n  Learning, LDA, NMF"], "primary_category": "cs.CL"}
{"title": "Hypertext Entity Extraction in Webpage", "abstract": "Webpage entity extraction is a fundamental natural language processing task\nin both research and applications. Nowadays, the majority of webpage entity\nextraction models are trained on structured datasets which strive to retain\ntextual content and its structure information. However, existing datasets all\noverlook the rich hypertext features (e.g., font color, font size) which show\ntheir effectiveness in previous works. To this end, we first collect a\n\\textbf{H}ypertext \\textbf{E}ntity \\textbf{E}xtraction \\textbf{D}ataset\n(\\textit{HEED}) from the e-commerce domains, scraping both the text and the\ncorresponding explicit hypertext features with high-quality manual entity\nannotations. Furthermore, we present the \\textbf{Mo}E-based \\textbf{E}ntity\n\\textbf{E}xtraction \\textbf{F}ramework (\\textit{MoEEF}), which efficiently\nintegrates multiple features to enhance model performance by Mixture of Experts\nand outperforms strong baselines, including the state-of-the-art small-scale\nmodels and GPT-3.5-turbo. Moreover, the effectiveness of hypertext features in\n\\textit{HEED} and several model components in \\textit{MoEEF} are analyzed.", "published": "2024-03-04 03:21:40", "link": "http://arxiv.org/abs/2403.01698v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Self-Contained Answers: Entity-Based Answer Rewriting in\n  Conversational Search", "abstract": "Conversational information-seeking (CIS) is an emerging paradigm for\nknowledge acquisition and exploratory search. Traditional web search interfaces\nenable easy exploration of entities, but this is limited in conversational\nsettings due to the limited-bandwidth interface. This paper explore ways to\nrewrite answers in CIS, so that users can understand them without having to\nresort to external services or sources. Specifically, we focus on salient\nentities -- entities that are central to understanding the answer. As our first\ncontribution, we create a dataset of conversations annotated with entities for\nsaliency. Our analysis of the collected data reveals that the majority of\nanswers contain salient entities. As our second contribution, we propose two\nanswer rewriting strategies aimed at improving the overall user experience in\nCIS. One approach expands answers with inline definitions of salient entities,\nmaking the answer self-contained. The other approach complements answers with\nfollow-up questions, offering users the possibility to learn more about\nspecific entities. Results of a crowdsourcing-based study indicate that\nrewritten answers are clearly preferred over the original ones. We also find\nthat inline definitions tend to be favored over follow-up questions, but this\nchoice is highly subjective, thereby providing a promising future direction for\npersonalization.", "published": "2024-03-04 05:52:41", "link": "http://arxiv.org/abs/2403.01747v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "NeuSpeech: Decode Neural signal as Speech", "abstract": "Decoding language from brain dynamics is an important open direction in the\nrealm of brain-computer interface (BCI), especially considering the rapid\ngrowth of large language models. Compared to invasive-based signals which\nrequire electrode implantation surgery, non-invasive neural signals (e.g. EEG,\nMEG) have attracted increasing attention considering their safety and\ngenerality. However, the exploration is not adequate in three aspects: 1)\nprevious methods mainly focus on EEG but none of the previous works address\nthis problem on MEG with better signal quality; 2) prior works have\npredominantly used $``teacher-forcing\"$ during generative decoding, which is\nimpractical; 3) prior works are mostly $``BART-based\"$ not fully\nauto-regressive, which performs better in other sequence tasks. In this paper,\nwe explore the brain-to-text translation of MEG signals in a speech-decoding\nformation. Here we are the first to investigate a cross-attention-based\n``whisper\" model for generating text directly from MEG signals without teacher\nforcing. Our model achieves impressive BLEU-1 scores of 60.30 and 52.89 without\npretraining $\\&$ teacher-forcing on two major datasets ($\\textit{GWilliams}$\nand $\\textit{Schoffelen}$). This paper conducts a comprehensive review to\nunderstand how speech decoding formation performs on the neural decoding tasks,\nincluding pretraining initialization, training $\\&$ evaluation set splitting,\naugmentation, and scaling law. Code is available at\nhttps://github.com/NeuSpeech/NeuSpeech1$.", "published": "2024-03-04 05:55:01", "link": "http://arxiv.org/abs/2403.01748v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language\n  Models", "abstract": "Understanding the reasoning capabilities of Multimodal Large Language Models\n(MLLMs) is an important area of research. In this study, we introduce a dynamic\nbenchmark, NPHardEval4V, aimed at addressing the existing gaps in evaluating\nthe pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to\ndisentangle the effect of various factors such as image recognition and\ninstruction following, from the overall performance of the models, allowing us\nto focus solely on evaluating their reasoning abilities. It is built by\nconverting textual description of questions from NPHardEval to image\nrepresentations. Our findings reveal significant discrepancies in reasoning\nabilities across different models and highlight the relatively weak performance\nof MLLMs compared to LLMs in terms of reasoning. We also investigate the impact\nof different prompting styles, including visual, text, and combined visual and\ntext prompts, on the reasoning abilities of MLLMs, demonstrating the different\nimpacts of multimodal inputs in model performance. Unlike traditional\nbenchmarks, which focus primarily on static evaluations, our benchmark will be\nupdated monthly to prevent overfitting and ensure a more authentic and\nfine-grained evaluation of the models. We believe that this benchmark can aid\nin understanding and guide the further development of reasoning abilities in\nMLLMs. The benchmark dataset and code are available at\nhttps://github.com/lizhouf/NPHardEval4V", "published": "2024-03-04 07:10:31", "link": "http://arxiv.org/abs/2403.01777v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Model-Based Data-Centric AI: Bridging the Divide Between Academic Ideals\n  and Industrial Pragmatism", "abstract": "This paper delves into the contrasting roles of data within academic and\nindustrial spheres, highlighting the divergence between Data-Centric AI and\nModel-Agnostic AI approaches. We argue that while Data-Centric AI focuses on\nthe primacy of high-quality data for model performance, Model-Agnostic AI\nprioritizes algorithmic flexibility, often at the expense of data quality\nconsiderations. This distinction reveals that academic standards for data\nquality frequently do not meet the rigorous demands of industrial applications,\nleading to potential pitfalls in deploying academic models in real-world\nsettings. Through a comprehensive analysis, we address these disparities,\npresenting both the challenges they pose and strategies for bridging the gap.\nFurthermore, we propose a novel paradigm: Model-Based Data-Centric AI, which\naims to reconcile these differences by integrating model considerations into\ndata optimization processes. This approach underscores the necessity for\nevolving data requirements that are sensitive to the nuances of both academic\nresearch and industrial deployment. By exploring these discrepancies, we aim to\nfoster a more nuanced understanding of data's role in AI development and\nencourage a convergence of academic and industrial standards to enhance AI's\nreal-world applicability.", "published": "2024-03-04 08:29:15", "link": "http://arxiv.org/abs/2403.01832v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Making Pre-trained Language Models Great on Tabular Prediction", "abstract": "The transferability of deep neural networks (DNNs) has made significant\nprogress in image and language processing. However, due to the heterogeneity\namong tables, such DNN bonus is still far from being well exploited on tabular\ndata prediction (e.g., regression or classification tasks). Condensing\nknowledge from diverse domains, language models (LMs) possess the capability to\ncomprehend feature names from various tables, potentially serving as versatile\nlearners in transferring knowledge across distinct tables and diverse\nprediction tasks, but their discrete text representation space is inherently\nincompatible with numerical feature values in tables. In this paper, we present\nTP-BERTa, a specifically pre-trained LM for tabular data prediction.\nConcretely, a novel relative magnitude tokenization converts scalar numerical\nfeature values to finely discrete, high-dimensional tokens, and an\nintra-feature attention approach integrates feature values with the\ncorresponding feature names. Comprehensive experiments demonstrate that our\npre-trained TP-BERTa leads the performance among tabular DNNs and is\ncompetitive with Gradient Boosted Decision Tree models in typical tabular data\nregime.", "published": "2024-03-04 08:38:56", "link": "http://arxiv.org/abs/2403.01841v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Rethinking LLM Language Adaptation: A Case Study on Chinese Mixtral", "abstract": "Mixtral, a representative sparse mixture of experts (SMoE) language model,\nhas received significant attention due to its unique model design and superior\nperformance. Based on Mixtral-8x7B-v0.1, in this paper, we propose\nChinese-Mixtral and Chinese-Mixtral-Instruct with improved Chinese language\nabilities by adopting further pre-training and instruction fine-tuning.\nExperimental results show that our Chinese-Mixtral and Chinese-Mixtral-Instruct\nsuccessfully improve Chinese understanding and generation performance while\nretaining the original English abilities. Then, we discuss several key\nquestions when performing language adaptation on large language models,\nincluding the necessity of extending the language-specific vocabulary and the\nchoice of the initialization model (foundation model v.s. instruction model),\nby providing empirical results and analysis. We also present the visualizations\nof each expert to examine their importance on downstream tasks. Our resources\nare publicly available through \\url{https://github.com/ymcui/Chinese-Mixtral}.", "published": "2024-03-04 09:01:10", "link": "http://arxiv.org/abs/2403.01851v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FCDS: Fusing Constituency and Dependency Syntax into Document-Level\n  Relation Extraction", "abstract": "Document-level Relation Extraction (DocRE) aims to identify relation labels\nbetween entities within a single document. It requires handling several\nsentences and reasoning over them. State-of-the-art DocRE methods use a graph\nstructure to connect entities across the document to capture dependency syntax\ninformation. However, this is insufficient to fully exploit the rich syntax\ninformation in the document. In this work, we propose to fuse constituency and\ndependency syntax into DocRE. It uses constituency syntax to aggregate the\nwhole sentence information and select the instructive sentences for the pairs\nof targets. It exploits the dependency syntax in a graph structure with\nconstituency syntax enhancement and chooses the path between entity pairs based\non the dependency graph. The experimental results on datasets from various\ndomains demonstrate the effectiveness of the proposed method. The code is\npublicly available at this url.", "published": "2024-03-04 09:48:55", "link": "http://arxiv.org/abs/2403.01886v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "To Generate or to Retrieve? On the Effectiveness of Artificial Contexts\n  for Medical Open-Domain Question Answering", "abstract": "Medical open-domain question answering demands substantial access to\nspecialized knowledge. Recent efforts have sought to decouple knowledge from\nmodel parameters, counteracting architectural scaling and allowing for training\non common low-resource hardware. The retrieve-then-read paradigm has become\nubiquitous, with model predictions grounded on relevant knowledge pieces from\nexternal repositories such as PubMed, textbooks, and UMLS. An alternative path,\nstill under-explored but made possible by the advent of domain-specific large\nlanguage models, entails constructing artificial contexts through prompting. As\na result, \"to generate or to retrieve\" is the modern equivalent of Hamlet's\ndilemma. This paper presents MedGENIE, the first generate-then-read framework\nfor multiple-choice question answering in medicine. We conduct extensive\nexperiments on MedQA-USMLE, MedMCQA, and MMLU, incorporating a practical\nperspective by assuming a maximum of 24GB VRAM. MedGENIE sets a new\nstate-of-the-art in the open-book setting of each testbed, allowing a\nsmall-scale reader to outcompete zero-shot closed-book 175B baselines while\nusing up to 706$\\times$ fewer parameters. Our findings reveal that generated\npassages are more effective than retrieved ones in attaining higher accuracy.", "published": "2024-03-04 10:41:52", "link": "http://arxiv.org/abs/2403.01924v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transformers for Low-Resource Languages: Is F\u00e9idir Linn!", "abstract": "The Transformer model is the state-of-the-art in Machine Translation.\nHowever, in general, neural translation models often under perform on language\npairs with insufficient training data. As a consequence, relatively few\nexperiments have been carried out using this architecture on low-resource\nlanguage pairs. In this study, hyperparameter optimization of Transformer\nmodels in translating the low-resource English-Irish language pair is\nevaluated. We demonstrate that choosing appropriate parameters leads to\nconsiderable performance improvements. Most importantly, the correct choice of\nsubword model is shown to be the biggest driver of translation performance.\nSentencePiece models using both unigram and BPE approaches were appraised.\nVariations on model architectures included modifying the number of layers,\ntesting various regularisation techniques and evaluating the optimal number of\nheads for attention. A generic 55k DGT corpus and an in-domain 88k public admin\ncorpus were used for evaluation. A Transformer optimized model demonstrated a\nBLEU score improvement of 7.8 points when compared with a baseline RNN model.\nImprovements were observed across a range of metrics, including TER, indicating\na substantially reduced post editing effort for Transformer optimized models\nwith 16k BPE subword models. Bench-marked against Google Translate, our\ntranslation engines demonstrated significant improvements. The question of\nwhether or not Transformers can be used effectively in a low-resource setting\nof English-Irish translation has been addressed. Is f\\'eidir linn - yes we can.", "published": "2024-03-04 12:29:59", "link": "http://arxiv.org/abs/2403.01985v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Leveraging Weakly Annotated Data for Hate Speech Detection in Code-Mixed\n  Hinglish: A Feasibility-Driven Transfer Learning Approach with Large Language\n  Models", "abstract": "The advent of Large Language Models (LLMs) has advanced the benchmark in\nvarious Natural Language Processing (NLP) tasks. However, large amounts of\nlabelled training data are required to train LLMs. Furthermore, data annotation\nand training are computationally expensive and time-consuming. Zero and\nfew-shot learning have recently emerged as viable options for labelling data\nusing large pre-trained models. Hate speech detection in mix-code low-resource\nlanguages is an active problem area where the use of LLMs has proven\nbeneficial. In this study, we have compiled a dataset of 100 YouTube comments,\nand weakly labelled them for coarse and fine-grained misogyny classification in\nmix-code Hinglish. Weak annotation was applied due to the labor-intensive\nannotation process. Zero-shot learning, one-shot learning, and few-shot\nlearning and prompting approaches have then been applied to assign labels to\nthe comments and compare them to human-assigned labels. Out of all the\napproaches, zero-shot classification using the Bidirectional Auto-Regressive\nTransformers (BART) large model and few-shot prompting using Generative\nPre-trained Transformer- 3 (ChatGPT-3) achieve the best results", "published": "2024-03-04 15:27:49", "link": "http://arxiv.org/abs/2403.02121v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RIFF: Learning to Rephrase Inputs for Few-shot Fine-tuning of Language\n  Models", "abstract": "Pre-trained Language Models (PLMs) can be accurately fine-tuned for\ndownstream text processing tasks. Recently, researchers have introduced several\nparameter-efficient fine-tuning methods that optimize input prompts or adjust a\nsmall number of model parameters (e.g LoRA). In this study, we explore the\nimpact of altering the input text of the original task in conjunction with\nparameter-efficient fine-tuning methods. To most effectively rewrite the input\ntext, we train a few-shot paraphrase model with a Maximum-Marginal Likelihood\nobjective. Using six few-shot text classification datasets, we show that\nenriching data with paraphrases at train and test time enhances the performance\nbeyond what can be achieved with parameter-efficient fine-tuning alone. The\ncode used for our experiments can be found at\nhttps://github.com/SaeedNajafi/RIFF.", "published": "2024-03-04 17:58:09", "link": "http://arxiv.org/abs/2403.02271v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Key-Point-Driven Data Synthesis with its Enhancement on Mathematical\n  Reasoning", "abstract": "Large language models (LLMs) have shown great potential in complex reasoning\ntasks, yet their performance is often hampered by the scarcity of high-quality\nand reasoning-focused training datasets. Addressing this challenge, we propose\nKey-Point-Driven Data Synthesis (KPDDS), a novel data synthesis framework that\nsynthesizes question-answer pairs by leveraging key points and exemplar\npractices from authentic data sources. KPDDS ensures the generation of novel\nquestions with rigorous quality control and substantial scalability. As a\nresult, we present KPMath, an extensive synthetic dataset tailored for\nmathematical reasoning, comprising over 800K question-answer pairs. Utilizing\nKPMath and augmenting it with additional reasoning-intensive corpora, we create\nthe comprehensive KPMath-Plus dataset. The Qwen1.5-72B model, fine-tuned on\nKPMath-Plus, achieves 87.0% PASS@1 accuracy on GSM8K and 58.3% on MATH,\nsurpassing competitors in the 7B to 70B range and best commercial models like\nGPT-4 across multiple math reasoning datasets.", "published": "2024-03-04 18:58:30", "link": "http://arxiv.org/abs/2403.02333v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Human Evaluation of English--Irish Transformer-Based NMT", "abstract": "In this study, a human evaluation is carried out on how hyperparameter\nsettings impact the quality of Transformer-based Neural Machine Translation\n(NMT) for the low-resourced English--Irish pair. SentencePiece models using\nboth Byte Pair Encoding (BPE) and unigram approaches were appraised. Variations\nin model architectures included modifying the number of layers, evaluating the\noptimal number of heads for attention and testing various regularisation\ntechniques. The greatest performance improvement was recorded for a\nTransformer-optimized model with a 16k BPE subword model. Compared with a\nbaseline Recurrent Neural Network (RNN) model, a Transformer-optimized model\ndemonstrated a BLEU score improvement of 7.8 points. When benchmarked against\nGoogle Translate, our translation engines demonstrated significant\nimprovements. Furthermore, a quantitative fine-grained manual evaluation was\nconducted which compared the performance of machine translation systems. Using\nthe Multidimensional Quality Metrics (MQM) error taxonomy, a human evaluation\nof the error types generated by an RNN-based system and a Transformer-based\nsystem was explored. Our findings show the best-performing Transformer system\nsignificantly reduces both accuracy and fluency errors when compared with an\nRNN-based model.", "published": "2024-03-04 11:45:46", "link": "http://arxiv.org/abs/2403.02366v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "adaptNMT: an open-source, language-agnostic development environment for\n  Neural Machine Translation", "abstract": "adaptNMT streamlines all processes involved in the development and deployment\nof RNN and Transformer neural translation models. As an open-source\napplication, it is designed for both technical and non-technical users who work\nin the field of machine translation. Built upon the widely-adopted OpenNMT\necosystem, the application is particularly useful for new entrants to the field\nsince the setup of the development environment and creation of train,\nvalidation and test splits is greatly simplified. Graphing, embedded within the\napplication, illustrates the progress of model training, and SentencePiece is\nused for creating subword segmentation models. Hyperparameter customization is\nfacilitated through an intuitive user interface, and a single-click model\ndevelopment approach has been implemented. Models developed by adaptNMT can be\nevaluated using a range of metrics, and deployed as a translation service\nwithin the application. To support eco-friendly research in the NLP space, a\ngreen report also flags the power consumption and kgCO$_{2}$ emissions\ngenerated during model development. The application is freely available.", "published": "2024-03-04 12:10:17", "link": "http://arxiv.org/abs/2403.02367v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "adaptMLLM: Fine-Tuning Multilingual Language Models on Low-Resource\n  Languages with Integrated LLM Playgrounds", "abstract": "The advent of Multilingual Language Models (MLLMs) and Large Language Models\nhas spawned innovation in many areas of natural language processing. Despite\nthe exciting potential of this technology, its impact on developing\nhigh-quality Machine Translation (MT) outputs for low-resource languages\nremains relatively under-explored. Furthermore, an open-source application,\ndedicated to both fine-tuning MLLMs and managing the complete MT workflow for\nlow-resources languages, remains unavailable. We aim to address these\nimbalances through the development of adaptMLLM, which streamlines all\nprocesses involved in the fine-tuning of MLLMs for MT. This open-source\napplication is tailored for developers, translators, and users who are engaged\nin MT. An intuitive interface allows for easy customisation of hyperparameters,\nand the application offers a range of metrics for model evaluation and the\ncapability to deploy models as a translation service directly within the\napplication. As a multilingual tool, we used adaptMLLM to fine-tune models for\ntwo low-resource language pairs: English to Irish (EN$\\leftrightarrow$GA) and\nEnglish to Marathi (EN$\\leftrightarrow$MR). Compared with baselines from the\nLoResMT2021 Shared Task, the adaptMLLM system demonstrated significant\nimprovements. In the EN$\\rightarrow$GA direction, an improvement of 5.2 BLEU\npoints was observed and an increase of 40.5 BLEU points was recorded in the\nGA$\\rightarrow$EN direction. Significant improvements in the translation\nperformance of the EN$\\leftrightarrow$MR pair were also observed notably in the\nMR$\\rightarrow$EN direction with an increase of 21.3 BLEU points. Finally, a\nfine-grained human evaluation of the MLLM output on the EN$\\rightarrow$GA pair\nwas conducted using the Multidimensional Quality Metrics and Scalar Quality\nMetrics error taxonomies. The application and models are freely available.", "published": "2024-03-04 14:49:18", "link": "http://arxiv.org/abs/2403.02370v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing LLM Safety via Constrained Direct Preference Optimization", "abstract": "The rapidly increasing capabilities of large language models (LLMs) raise an\nurgent need to align AI systems with diverse human preferences to\nsimultaneously enhance their usefulness and safety, despite the often\nconflicting nature of these goals. To address this important problem, a\npromising approach is to enforce a safety constraint at the fine-tuning stage\nthrough a constrained Reinforcement Learning from Human Feedback (RLHF)\nframework. This approach, however, is computationally expensive and often\nunstable. In this work, we introduce Constrained DPO (C-DPO), a novel extension\nof the recently proposed Direct Preference Optimization (DPO) approach for\nfine-tuning LLMs that is both efficient and lightweight. By integrating dual\ngradient descent and DPO, our method identifies a nearly optimal trade-off\nbetween helpfulness and harmlessness without using reinforcement learning.\nEmpirically, our approach provides a safety guarantee to LLMs that is missing\nin DPO while achieving significantly higher rewards under the same safety\nconstraint compared to a recently proposed safe RLHF approach.\n  Warning: This paper contains example data that may be offensive or harmful.", "published": "2024-03-04 20:39:24", "link": "http://arxiv.org/abs/2403.02475v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Tutorial on the Pretrain-Finetune Paradigm for Natural Language\n  Processing", "abstract": "Given that natural language serves as the primary conduit for expressing\nthoughts and emotions, text analysis has become a key technique in\npsychological research. It enables the extraction of valuable insights from\nnatural language, facilitating endeavors like personality traits assessment,\nmental health monitoring, and sentiment analysis in interpersonal\ncommunications. In text analysis, existing studies often resort to either human\ncoding, which is time-consuming, using pre-built dictionaries, which often\nfails to cover all possible scenarios, or training models from scratch, which\nrequires large amounts of labeled data. In this tutorial, we introduce the\npretrain-finetune paradigm. The pretrain-finetune paradigm represents a\ntransformative approach in text analysis and natural language processing. This\nparadigm distinguishes itself through the use of large pretrained language\nmodels, demonstrating remarkable efficiency in finetuning tasks, even with\nlimited training data. This efficiency is especially beneficial for research in\nsocial sciences, where the number of annotated samples is often quite limited.\nOur tutorial offers a comprehensive introduction to the pretrain-finetune\nparadigm. We first delve into the fundamental concepts of pretraining and\nfinetuning, followed by practical exercises using real-world applications. We\ndemonstrate the application of the paradigm across various tasks, including\nmulti-class classification and regression. Emphasizing its efficacy and\nuser-friendliness, the tutorial aims to encourage broader adoption of this\nparadigm. To this end, we have provided open access to all our code and\ndatasets. The tutorial is highly beneficial across various psychology\ndisciplines, providing a comprehensive guide to employing text analysis in\ndiverse research settings.", "published": "2024-03-04 21:51:11", "link": "http://arxiv.org/abs/2403.02504v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SPUQ: Perturbation-Based Uncertainty Quantification for Large Language\n  Models", "abstract": "In recent years, large language models (LLMs) have become increasingly\nprevalent, offering remarkable text generation capabilities. However, a\npressing challenge is their tendency to make confidently wrong predictions,\nhighlighting the critical need for uncertainty quantification (UQ) in LLMs.\nWhile previous works have mainly focused on addressing aleatoric uncertainty,\nthe full spectrum of uncertainties, including epistemic, remains inadequately\nexplored. Motivated by this gap, we introduce a novel UQ method, sampling with\nperturbation for UQ (SPUQ), designed to tackle both aleatoric and epistemic\nuncertainties. The method entails generating a set of perturbations for LLM\ninputs, sampling outputs for each perturbation, and incorporating an\naggregation module that generalizes the sampling uncertainty approach for text\ngeneration tasks. Through extensive experiments on various datasets, we\ninvestigated different perturbation and aggregation techniques. Our findings\nshow a substantial improvement in model uncertainty calibration, with a\nreduction in Expected Calibration Error (ECE) by 50\\% on average. Our findings\nsuggest that our proposed UQ method offers promising steps toward enhancing the\nreliability and trustworthiness of LLMs.", "published": "2024-03-04 21:55:22", "link": "http://arxiv.org/abs/2403.02509v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DACO: Towards Application-Driven and Comprehensive Data Analysis via\n  Code Generation", "abstract": "Data analysis is a crucial analytical process to generate in-depth studies\nand conclusive insights to comprehensively answer a given user query for\ntabular data. In this work, we aim to propose new resources and benchmarks to\ninspire future research on this crucial yet challenging and under-explored\ntask. However, collecting data analysis annotations curated by experts can be\nprohibitively expensive. We propose to automatically generate high-quality\nanswer annotations leveraging the code-generation capabilities of LLMs with a\nmulti-turn prompting technique. We construct the DACO dataset, containing (1)\n440 databases (of tabular data) collected from real-world scenarios, (2) ~2k\nquery-answer pairs that can serve as weak supervision for model training, and\n(3) a concentrated but high-quality test set with human refined annotations\nthat serves as our main evaluation benchmark. We train a 6B supervised\nfine-tuning (SFT) model on DACO dataset, and find that the SFT model learns\nreasonable data analysis capabilities. To further align the models with human\npreference, we use reinforcement learning to encourage generating analysis\nperceived by human as helpful, and design a set of dense rewards to propagate\nthe sparse human preference reward to intermediate code generation steps. Our\nDACO-RL algorithm is evaluated by human annotators to produce more helpful\nanswers than SFT model in 57.72% cases, validating the effectiveness of our\nproposed algorithm. Data and code are released at\nhttps://github.com/shirley-wu/daco", "published": "2024-03-04 22:47:58", "link": "http://arxiv.org/abs/2403.02528v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Online Training of Large Language Models: Learn while chatting", "abstract": "Large Language Models(LLMs) have dramatically revolutionized the field of\nNatural Language Processing(NLP), offering remarkable capabilities that have\ngarnered widespread usage. However, existing interaction paradigms between LLMs\nand users are constrained by either inflexibility, limitations in\ncustomization, or a lack of persistent learning. This inflexibility is\nparticularly evident as users, especially those without programming skills,\nhave restricted avenues to enhance or personalize the model. Existing\nframeworks further complicate the model training and deployment process due to\ntheir computational inefficiencies and lack of user-friendly interfaces. To\novercome these challenges, this paper introduces a novel interaction\nparadigm-'Online Training using External Interactions'-that merges the benefits\nof persistent, real-time model updates with the flexibility for individual\ncustomization through external interactions such as AI agents or online/offline\nknowledge bases.", "published": "2024-03-04 10:00:55", "link": "http://arxiv.org/abs/2403.04790v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Breaking the Language Barrier: Can Direct Inference Outperform\n  Pre-Translation in Multilingual LLM Applications?", "abstract": "Large language models hold significant promise in multilingual applications.\nHowever, inherent biases stemming from predominantly English-centric\npre-training have led to the widespread practice of pre-translation, i.e.,\ntranslating non-English inputs to English before inference, leading to\ncomplexity and information loss. This study re-evaluates the need for\npre-translation in the context of PaLM2 models (Anil et al., 2023), which have\nbeen established as highly performant in multilingual tasks. We offer a\ncomprehensive investigation across 108 languages and 6 diverse benchmarks,\nincluding open-end generative tasks, which were excluded from previous similar\nstudies. Our findings challenge the pre-translation paradigm established in\nprior research, highlighting the advantages of direct inference in PaLM2.\nSpecifically, PaLM2-L consistently outperforms pre-translation in 94 out of 108\nlanguages. These findings pave the way for more efficient and effective\nmultilingual applications, alleviating the limitations associated with\npre-translation and unlocking linguistic authenticity.", "published": "2024-03-04 14:01:11", "link": "http://arxiv.org/abs/2403.04792v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Inference Acceleration for Large Language Models on CPUs", "abstract": "In recent years, large language models have demonstrated remarkable\nperformance across various natural language processing (NLP) tasks. However,\ndeploying these models for real-world applications often requires efficient\ninference solutions to handle the computational demands. In this paper, we\nexplore the utilization of CPUs for accelerating the inference of large\nlanguage models. Specifically, we introduce a parallelized approach to enhance\nthroughput by 1) Exploiting the parallel processing capabilities of modern CPU\narchitectures, 2) Batching the inference request. Our evaluation shows the\naccelerated inference engine gives an 18-22x improvement in the generated token\nper sec. The improvement is more with longer sequence and larger models. In\naddition to this, we can also run multiple workers in the same machine with\nNUMA node isolation to further improvement in tokens/s. Table 2, we have\nreceived 4x additional improvement with 4 workers. This would also make Gen-AI\nbased products and companies environment friendly, our estimates shows that CPU\nusage for Inference could reduce the power consumption of LLMs by 48.9% while\nproviding production ready throughput and latency.", "published": "2024-03-04 10:27:23", "link": "http://arxiv.org/abs/2406.07553v1", "categories": ["cs.DC", "cs.CL"], "primary_category": "cs.DC"}
{"title": "DECIDER: A Dual-System Rule-Controllable Decoding Framework for Language\n  Generation", "abstract": "Constrained decoding approaches aim to control the meaning or style of text\ngenerated by a Pre-trained Language Model (PLM) using specific target words\nduring inference. However, these methods often guide plausible continuations by\ngreedily selecting targets, which, while completing the task, may disrupt the\nnatural patterns of human language generation. In this work, we propose a novel\ndecoding framework, DECIDER, which enables us to program rules on how we\ncomplete tasks to control a PLM. Differing from previous work, our framework\ntransforms the encouragement of target words into the encouragement of all\nwords that satisfy the rule. Specifically, DECIDER is a dual system where a PLM\nis equipped with a First-OrderLogic (FOL) reasoner to express and evaluate the\nrules, and a decision function to merge the outputs from both systems to steer\nthe generation. Experiments on CommonGen and PersonaChat demonstrate that\nDECIDER can effectively follow given rules to achieve generation tasks in a\nmore human-like manner.", "published": "2024-03-04 11:49:08", "link": "http://arxiv.org/abs/2403.01954v3", "categories": ["cs.CL", "cs.AI", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Modeling Multimodal Social Interactions: New Challenges and Baselines\n  with Densely Aligned Representations", "abstract": "Understanding social interactions involving both verbal and non-verbal cues\nis essential for effectively interpreting social situations. However, most\nprior works on multimodal social cues focus predominantly on single-person\nbehaviors or rely on holistic visual representations that are not aligned to\nutterances in multi-party environments. Consequently, they are limited in\nmodeling the intricate dynamics of multi-party interactions. In this paper, we\nintroduce three new challenging tasks to model the fine-grained dynamics\nbetween multiple people: speaking target identification, pronoun coreference\nresolution, and mentioned player prediction. We contribute extensive data\nannotations to curate these new challenges in social deduction game settings.\nFurthermore, we propose a novel multimodal baseline that leverages densely\naligned language-visual representations by synchronizing visual features with\ntheir corresponding utterances. This facilitates concurrently capturing verbal\nand non-verbal cues pertinent to social reasoning. Experiments demonstrate the\neffectiveness of the proposed approach with densely aligned multimodal\nrepresentations in modeling fine-grained social interactions. Project website:\nhttps://sangmin-git.github.io/projects/MMSI.", "published": "2024-03-04 14:46:58", "link": "http://arxiv.org/abs/2403.02090v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "LOCR: Location-Guided Transformer for Optical Character Recognition", "abstract": "Academic documents are packed with texts, equations, tables, and figures,\nrequiring comprehensive understanding for accurate Optical Character\nRecognition (OCR). While end-to-end OCR methods offer improved accuracy over\nlayout-based approaches, they often grapple with significant repetition issues,\nespecially with complex layouts in Out-Of-Domain (OOD) documents.To tackle this\nissue, we propose LOCR, a model that integrates location guiding into the\ntransformer architecture during autoregression. We train the model on a dataset\ncomprising over 77M text-location pairs from 125K academic document pages,\nincluding bounding boxes for words, tables and mathematical symbols. LOCR\nadeptly handles various formatting elements and generates content in Markdown\nlanguage. It outperforms all existing methods in our test set constructed from\narXiv, as measured by edit distance, BLEU, METEOR and F-measure.LOCR also\nreduces repetition frequency from 4.4% of pages to 0.5% in the arXiv dataset,\nfrom 13.2% to 1.3% in OOD quantum physics documents and from 8.1% to 1.8% in\nOOD marketing documents. Additionally, LOCR features an interactive OCR mode,\nfacilitating the generation of complex documents through a few location prompts\nfrom human.", "published": "2024-03-04 15:34:12", "link": "http://arxiv.org/abs/2403.02127v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "EMOVOME: A Dataset for Emotion Recognition in Spontaneous Real-Life\n  Speech", "abstract": "Spontaneous datasets for Speech Emotion Recognition (SER) are scarce and\nfrequently derived from laboratory environments or staged scenarios, such as TV\nshows, limiting their application in real-world contexts. We developed and\npublicly released the Emotional Voice Messages (EMOVOME) dataset, including 999\nvoice messages from real conversations of 100 Spanish speakers on a messaging\napp, labeled in continuous and discrete emotions by expert and non-expert\nannotators. We evaluated speaker-independent SER models using acoustic features\nas baseline and transformer-based models. We compared the results with\nreference datasets including acted and elicited speech, and analyzed the\ninfluence of annotators and gender fairness. The pre-trained\nUniSpeech-SAT-Large model achieved the highest results, 61.64% and 55.57%\nUnweighted Accuracy (UA) for 3-class valence and arousal prediction\nrespectively on EMOVOME, a 10% improvement over baseline models. For the\nemotion categories, 42.58% UA was obtained. EMOVOME performed lower than the\nacted RAVDESS dataset. The elicited IEMOCAP dataset also outperformed EMOVOME\nin predicting emotion categories, while similar results were obtained in\nvalence and arousal. EMOVOME outcomes varied with annotator labels, showing\nbetter results and fairness when combining expert and non-expert annotations.\nThis study highlights the gap between controlled and real-life scenarios,\nsupporting further advancements in recognizing genuine emotions.", "published": "2024-03-04 16:13:39", "link": "http://arxiv.org/abs/2403.02167v3", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD", "I.5.1; I.5.4"], "primary_category": "eess.AS"}
{"title": "Masked Thought: Simply Masking Partial Reasoning Steps Can Improve\n  Mathematical Reasoning Learning of Language Models", "abstract": "In reasoning tasks, even a minor error can cascade into inaccurate results,\nleading to suboptimal performance of large language models in such domains.\nEarlier fine-tuning approaches sought to mitigate this by leveraging more\nprecise supervisory signals from human labeling, larger models, or\nself-sampling, although at a high cost. Conversely, we develop a method that\navoids external resources, relying instead on introducing perturbations to the\ninput. Our training approach randomly masks certain tokens within the chain of\nthought, a technique we found to be particularly effective for reasoning tasks.\nWhen applied to fine-tuning with GSM8K on Llama-2-7B, this method achieved a\n5\\% improvement in GSM8K accuracy and a 10\\% improvement in GSM-IC accuracy\nover standard supervised fine-tuning with a few codes modified. Furthermore, it\nis complementary to existing methods. When integrated with related explicit\ndata augmentation methods, it leads to improvements across five datasets of\nvarious augmentation methods, as well as two different base models. We further\ninvestigate the mechanisms behind this improvement through case studies and\nquantitative analysis, suggesting that our approach may provide superior\nsupport for the model in capturing long-distance dependencies, especially those\nrelated to questions. This enhancement could deepen understanding of the\npremises in questions and prior steps. Our code is available at Github.", "published": "2024-03-04 16:21:54", "link": "http://arxiv.org/abs/2403.02178v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Not All Layers of LLMs Are Necessary During Inference", "abstract": "Due to the large number of parameters, the inference phase of Large Language\nModels (LLMs) is resource-intensive. However, not all requests posed to LLMs\nare equally difficult to handle. Through analysis, we show that for some tasks,\nLLMs can achieve results comparable to the final output at some intermediate\nlayers. That is, not all layers of LLMs are necessary during inference. If we\ncan predict at which layer the inferred results match the final results\n(produced by evaluating all layers), we could significantly reduce the\ninference cost. To this end, we propose a simple yet effective algorithm named\nAdaInfer to adaptively terminate the inference process for an input instance.\nAdaInfer relies on easily obtainable statistical features and classic\nclassifiers like SVM. Experiments on well-known LLMs like the Llama2 series and\nOPT, show that AdaInfer can achieve an average of 17.8% pruning ratio, and up\nto 43% on sentiment tasks, with nearly no performance drop (<1%). Because\nAdaInfer does not alter LLM parameters, the LLMs incorporated with AdaInfer\nmaintain generalizability across tasks.", "published": "2024-03-04 16:23:58", "link": "http://arxiv.org/abs/2403.02181v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Distilled ChatGPT Topic & Sentiment Modeling with Applications in\n  Finance", "abstract": "In this study, ChatGPT is utilized to create streamlined models that generate\neasily interpretable features. These features are then used to evaluate\nfinancial outcomes from earnings calls. We detail a training approach that\nmerges knowledge distillation and transfer learning, resulting in lightweight\ntopic and sentiment classification models without significant loss in accuracy.\nThese models are assessed through a dataset annotated by experts. The paper\nalso delves into two practical case studies, highlighting how the generated\nfeatures can be effectively utilized in quantitative investing scenarios.", "published": "2024-03-04 16:27:21", "link": "http://arxiv.org/abs/2403.02185v1", "categories": ["cs.LG", "cs.CE", "cs.CL"], "primary_category": "cs.LG"}
{"title": "KnowPhish: Large Language Models Meet Multimodal Knowledge Graphs for\n  Enhancing Reference-Based Phishing Detection", "abstract": "Phishing attacks have inflicted substantial losses on individuals and\nbusinesses alike, necessitating the development of robust and efficient\nautomated phishing detection approaches. Reference-based phishing detectors\n(RBPDs), which compare the logos on a target webpage to a known set of logos,\nhave emerged as the state-of-the-art approach. However, a major limitation of\nexisting RBPDs is that they rely on a manually constructed brand knowledge\nbase, making it infeasible to scale to a large number of brands, which results\nin false negative errors due to the insufficient brand coverage of the\nknowledge base. To address this issue, we propose an automated knowledge\ncollection pipeline, using which we collect a large-scale multimodal brand\nknowledge base, KnowPhish, containing 20k brands with rich information about\neach brand. KnowPhish can be used to boost the performance of existing RBPDs in\na plug-and-play manner. A second limitation of existing RBPDs is that they\nsolely rely on the image modality, ignoring useful textual information present\nin the webpage HTML. To utilize this textual information, we propose a Large\nLanguage Model (LLM)-based approach to extract brand information of webpages\nfrom text. Our resulting multimodal phishing detection approach, KnowPhish\nDetector (KPD), can detect phishing webpages with or without logos. We evaluate\nKnowPhish and KPD on a manually validated dataset, and a field study under\nSingapore's local context, showing substantial improvements in effectiveness\nand efficiency compared to state-of-the-art baselines.", "published": "2024-03-04 17:38:32", "link": "http://arxiv.org/abs/2403.02253v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Subjective $\\textit{Isms}$? On the Danger of Conflating Hate and Offence\n  in Abusive Language Detection", "abstract": "Natural language processing research has begun to embrace the notion of\nannotator subjectivity, motivated by variations in labelling. This approach\nunderstands each annotator's view as valid, which can be highly suitable for\ntasks that embed subjectivity, e.g., sentiment analysis. However, this\nconstruction may be inappropriate for tasks such as hate speech detection, as\nit affords equal validity to all positions on e.g., sexism or racism. We argue\nthat the conflation of hate and offence can invalidate findings on hate speech,\nand call for future work to be situated in theory, disentangling hate from its\northogonal concept, offence.", "published": "2024-03-04 17:56:28", "link": "http://arxiv.org/abs/2403.02268v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Contrastive Region Guidance: Improving Grounding in Vision-Language\n  Models without Training", "abstract": "Highlighting particularly relevant regions of an image can improve the\nperformance of vision-language models (VLMs) on various vision-language (VL)\ntasks by guiding the model to attend more closely to these regions of interest.\nFor example, VLMs can be given a \"visual prompt\", where visual markers such as\nbounding boxes delineate key image regions. However, current VLMs that can\nincorporate visual guidance are either proprietary and expensive or require\ncostly training on curated data that includes visual prompts. We introduce\nContrastive Region Guidance (CRG), a training-free guidance method that enables\nopen-source VLMs to respond to visual prompts. CRG contrasts model outputs\nproduced with and without visual prompts, factoring out biases revealed by the\nmodel when answering without the information required to produce a correct\nanswer (i.e., the model's prior). CRG achieves substantial improvements in a\nwide variety of VL tasks: When region annotations are provided, CRG increases\nabsolute accuracy by up to 11.1% on ViP-Bench, a collection of six diverse\nregion-based tasks such as recognition, math, and object relationship\nreasoning. We also show CRG's applicability to spatial reasoning, with 10%\nimprovement on What'sUp, as well as to compositional generalization --\nimproving accuracy by 11.5% and 7.5% on two challenging splits from SugarCrepe\n-- and to image-text alignment for generated images, where we improve by up to\n8.4 AUROC and 6.8 F1 points on SeeTRUE. When reference regions are absent, CRG\nallows us to re-rank proposed regions in referring expression comprehension and\nphrase grounding benchmarks like RefCOCO/+/g and Flickr30K Entities, with an\naverage gain of 3.2% in accuracy. Our analysis explores alternative masking\nstrategies for CRG, quantifies CRG's probability shift, and evaluates the role\nof region guidance strength, empirically validating CRG's design choices.", "published": "2024-03-04 18:55:30", "link": "http://arxiv.org/abs/2403.02325v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "NeuroVoz: a Castillian Spanish corpus of parkinsonian speech", "abstract": "The screening of Parkinson's Disease (PD) through speech is hindered by a\nnotable lack of publicly available datasets in different languages. This fact\nlimits the reproducibility and further exploration of existing research.\n  To address this gap, this manuscript presents the NeuroVoz corpus consisting\nof 112 native Castilian-Spanish speakers, including 58 healthy controls and 54\nindividuals with PD, all recorded in ON state. The corpus showcases a diverse\narray of speech tasks: sustained vowels; diadochokinetic tests; 16\nListen-and-Repeat utterances; and spontaneous monologues.\n  The dataset is also complemented with subjective assessments of voice quality\nperformed by an expert according to the GRBAS scale\n(Grade/Roughness/Breathiness/Asthenia/Strain), as well as annotations with a\nthorough examination of phonation quality, intensity, speed, resonance,\nintelligibility, and prosody.\n  The corpus offers a substantial resource for the exploration of the impact of\nPD on speech. This data set has already supported several studies, achieving a\nbenchmark accuracy of 89% for the screening of PD. Despite these advances, the\nbroader challenge of conducting a language-agnostic, cross-corpora analysis of\nParkinsonian speech patterns remains open.", "published": "2024-03-04 16:17:39", "link": "http://arxiv.org/abs/2403.02371v3", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Trial and Error: Exploration-Based Trajectory Optimization for LLM\n  Agents", "abstract": "Large Language Models (LLMs) have become integral components in various\nautonomous agent systems. In this study, we present an exploration-based\ntrajectory optimization approach, referred to as ETO. This learning method is\ndesigned to enhance the performance of open LLM agents. Contrary to previous\nstudies that exclusively train on successful expert trajectories, our method\nallows agents to learn from their exploration failures. This leads to improved\nperformance through an iterative optimization framework. During the exploration\nphase, the agent interacts with the environment while completing given tasks,\ngathering failure trajectories to create contrastive trajectory pairs. In the\nsubsequent training phase, the agent utilizes these trajectory preference pairs\nto update its policy using contrastive learning methods like DPO. This\niterative cycle of exploration and training fosters continued improvement in\nthe agents. Our experiments on three complex tasks demonstrate that ETO\nconsistently surpasses baseline performance by a large margin. Furthermore, an\nexamination of task-solving efficiency and potential in scenarios lacking\nexpert trajectory underscores the effectiveness of our approach.", "published": "2024-03-04 21:50:29", "link": "http://arxiv.org/abs/2403.02502v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TopicDiff: A Topic-enriched Diffusion Approach for Multimodal\n  Conversational Emotion Detection", "abstract": "Multimodal Conversational Emotion (MCE) detection, generally spanning across\nthe acoustic, vision and language modalities, has attracted increasing interest\nin the multimedia community. Previous studies predominantly focus on learning\ncontextual information in conversations with only a few considering the topic\ninformation in single language modality, while always neglecting the acoustic\nand vision topic information. On this basis, we propose a model-agnostic\nTopic-enriched Diffusion (TopicDiff) approach for capturing multimodal topic\ninformation in MCE tasks. Particularly, we integrate the diffusion model into\nneural topic model to alleviate the diversity deficiency problem of neural\ntopic model in capturing topic information. Detailed evaluations demonstrate\nthe significant improvements of TopicDiff over the state-of-the-art MCE\nbaselines, justifying the importance of multimodal topic information to MCE and\nthe effectiveness of TopicDiff in capturing such information. Furthermore, we\nobserve an interesting finding that the topic information in acoustic and\nvision is more discriminative and robust compared to the language.", "published": "2024-03-04 08:38:53", "link": "http://arxiv.org/abs/2403.04789v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLM vs. Lawyers: Identifying a Subset of Summary Judgments in a Large UK\n  Case Law Dataset", "abstract": "To undertake computational research of the law, efficiently identifying\ndatasets of court decisions that relate to a specific legal issue is a crucial\nyet challenging endeavour. This study addresses the gap in the literature\nworking with large legal corpora about how to isolate cases, in our case\nsummary judgments, from a large corpus of UK court decisions. We introduce a\ncomparative analysis of two computational methods: (1) a traditional natural\nlanguage processing-based approach leveraging expert-generated keywords and\nlogical operators and (2) an innovative application of the Claude 2 large\nlanguage model to classify cases based on content-specific prompts. We use the\nCambridge Law Corpus of 356,011 UK court decisions and determine that the large\nlanguage model achieves a weighted F1 score of 0.94 versus 0.78 for keywords.\nDespite iterative refinement, the search logic based on keywords fails to\ncapture nuances in legal language. We identify and extract 3,102 summary\njudgment cases, enabling us to map their distribution across various UK courts\nover a temporal span. The paper marks a pioneering step in employing advanced\nnatural language processing to tackle core legal research tasks, demonstrating\nhow these technologies can bridge systemic gaps and enhance the accessibility\nof legal information. We share the extracted dataset metrics to support further\nresearch on summary judgments.", "published": "2024-03-04 10:13:30", "link": "http://arxiv.org/abs/2403.04791v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language Models in Fire Engineering: An Examination of Technical\n  Questions Against Domain Knowledge", "abstract": "This communication presents preliminary findings from comparing two recent\nchatbots, OpenAI's ChatGPT and Google's Bard, in the context of fire\nengineering by evaluating their responses in handling fire safety related\nqueries. A diverse range of fire engineering questions and scenarios were\ncreated and examined, including structural fire design, fire prevention\nstrategies, evacuation, building code compliance, and fire suppression systems\n(some of which resemble those commonly present in the Fire Protection exam\n(FPE)). The results reveal some key differences in the performance of the\nchatbots, with ChatGPT demonstrating a relatively superior performance. Then,\nthis communication highlights the potential for chatbot technology to\nrevolutionize fire engineering practices by providing instant access to\ncritical information while outlining areas for further improvement and\nresearch. Evidently, and when it matures, this technology will likely be\nelemental to our engineers' practice and education.", "published": "2024-03-04 16:18:36", "link": "http://arxiv.org/abs/2403.04795v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Predicting Learning Performance with Large Language Models: A Study in\n  Adult Literacy", "abstract": "Intelligent Tutoring Systems (ITSs) have significantly enhanced adult\nliteracy training, a key factor for societal participation, employment\nopportunities, and lifelong learning. Our study investigates the application of\nadvanced AI models, including Large Language Models (LLMs) like GPT-4, for\npredicting learning performance in adult literacy programs in ITSs. This\nresearch is motivated by the potential of LLMs to predict learning performance\nbased on its inherent reasoning and computational capabilities. By using\nreading comprehension datasets from the ITS, AutoTutor, we evaluate the\npredictive capabilities of GPT-4 versus traditional machine learning methods in\npredicting learning performance through five-fold cross-validation techniques.\nOur findings show that the GPT-4 presents the competitive predictive abilities\nwith traditional machine learning methods such as Bayesian Knowledge Tracing,\nPerformance Factor Analysis, Sparse Factor Analysis Lite (SPARFA-Lite), tensor\nfactorization and eXtreme Gradient Boosting (XGBoost). While XGBoost (trained\non local machine) outperforms GPT-4 in predictive accuracy, GPT-4-selected\nXGBoost and its subsequent tuning on the GPT-4 platform demonstrates superior\nperformance compared to local machine execution. Moreover, our investigation\ninto hyper-parameter tuning by GPT-4 versus grid-search suggests comparable\nperformance, albeit with less stability in the automated approach, using\nXGBoost as the case study. Our study contributes to the field by highlighting\nthe potential of integrating LLMs with traditional machine learning models to\nenhance predictive accuracy and personalize adult literacy education, setting a\nfoundation for future research in applying LLMs within ITSs.", "published": "2024-03-04 08:14:07", "link": "http://arxiv.org/abs/2403.14668v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Brilla AI: AI Contestant for the National Science and Maths Quiz", "abstract": "The African continent lacks enough qualified teachers which hampers the\nprovision of adequate learning support. An AI could potentially augment the\nefforts of the limited number of teachers, leading to better learning outcomes.\nTowards that end, this work describes and evaluates the first key output for\nthe NSMQ AI Grand Challenge, which proposes a robust, real-world benchmark for\nsuch an AI: \"Build an AI to compete live in Ghana's National Science and Maths\nQuiz (NSMQ) competition and win - performing better than the best contestants\nin all rounds and stages of the competition\". The NSMQ is an annual live\nscience and mathematics competition for senior secondary school students in\nGhana in which 3 teams of 2 students compete by answering questions across\nbiology, chemistry, physics, and math in 5 rounds over 5 progressive stages\nuntil a winning team is crowned for that year. In this work, we built Brilla\nAI, an AI contestant that we deployed to unofficially compete remotely and live\nin the Riddles round of the 2023 NSMQ Grand Finale, the first of its kind in\nthe 30-year history of the competition. Brilla AI is currently available as a\nweb app that livestreams the Riddles round of the contest, and runs 4 machine\nlearning systems: (1) speech to text (2) question extraction (3) question\nanswering and (4) text to speech that work together in real-time to quickly and\naccurately provide an answer, and then say it with a Ghanaian accent. In its\ndebut, our AI answered one of the 4 riddles ahead of the 3 human contesting\nteams, unofficially placing second (tied). Improvements and extensions of this\nAI could potentially be deployed to offer science tutoring to students and\neventually enable millions across Africa to have one-on-one learning\ninteractions, democratizing science education.", "published": "2024-03-04 03:24:18", "link": "http://arxiv.org/abs/2403.01699v3", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "How Multimodal Integration Boost the Performance of LLM for\n  Optimization: Case Study on Capacitated Vehicle Routing Problems", "abstract": "Recently, large language models (LLMs) have notably positioned them as\ncapable tools for addressing complex optimization challenges. Despite this\nrecognition, a predominant limitation of existing LLM-based optimization\nmethods is their struggle to capture the relationships among decision variables\nwhen relying exclusively on numerical text prompts, especially in\nhigh-dimensional problems. Keeping this in mind, we first propose to enhance\nthe optimization performance using multimodal LLM capable of processing both\ntextual and visual prompts for deeper insights of the processed optimization\nproblem. This integration allows for a more comprehensive understanding of\noptimization problems, akin to human cognitive processes. We have developed a\nmultimodal LLM-based optimization framework that simulates human\nproblem-solving workflows, thereby offering a more nuanced and effective\nanalysis. The efficacy of this method is evaluated through extensive empirical\nstudies focused on a well-known combinatorial optimization problem, i.e.,\ncapacitated vehicle routing problem. The results are compared against those\nobtained from the LLM-based optimization algorithms that rely solely on textual\nprompts, demonstrating the significant advantages of our multimodal approach.", "published": "2024-03-04 06:24:21", "link": "http://arxiv.org/abs/2403.01757v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.NE", "math.OC"], "primary_category": "cs.AI"}
{"title": "Are More LLM Calls All You Need? Towards Scaling Laws of Compound\n  Inference Systems", "abstract": "Many recent state-of-the-art results in language tasks were achieved using\ncompound systems that perform multiple Language Model (LM) calls and aggregate\ntheir responses. However, there is little understanding of how the number of LM\ncalls - e.g., when asking the LM to answer each question multiple times and\ntaking a majority vote - affects such a compound system's performance. In this\npaper, we initiate the study of scaling properties of compound inference\nsystems. We analyze, theoretically and empirically, how the number of LM calls\naffects the performance of Vote and Filter-Vote, two of the simplest compound\nsystem designs, which aggregate LM responses via majority voting, optionally\napplying LM filters. We find, surprisingly, that across multiple language\ntasks, the performance of both Vote and Filter-Vote can first increase but then\ndecrease as a function of the number of LM calls. Our theoretical results\nsuggest that this non-monotonicity is due to the diversity of query\ndifficulties within a task: more LM calls lead to higher performance on \"easy\"\nqueries, but lower performance on \"hard\" queries, and non-monotone behavior can\nemerge when a task contains both types of queries. This insight then allows us\nto compute, from a small number of samples, the number of LM calls that\nmaximizes system performance, and define an analytical scaling model for both\nsystems. Experiments show that our scaling model can accurately predict the\nperformance of Vote and Filter-Vote systems and thus find the optimal number of\nLM calls to make.", "published": "2024-03-04 19:12:48", "link": "http://arxiv.org/abs/2403.02419v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SY", "eess.SY"], "primary_category": "cs.LG"}
{"title": "JEP-KD: Joint-Embedding Predictive Architecture Based Knowledge\n  Distillation for Visual Speech Recognition", "abstract": "Visual Speech Recognition (VSR) tasks are generally recognized to have a\nlower theoretical performance ceiling than Automatic Speech Recognition (ASR),\nowing to the inherent limitations of conveying semantic information visually.\nTo mitigate this challenge, this paper introduces an advanced knowledge\ndistillation approach using a Joint-Embedding Predictive Architecture (JEPA),\nnamed JEP-KD, designed to more effectively utilize audio features during model\ntraining. Central to JEP-KD is the inclusion of a generative network within the\nembedding layer, which enhances the video encoder's capacity for semantic\nfeature extraction and brings it into closer alignment with the audio features\nfrom a pre-trained ASR model's encoder. This approach aims to progressively\nreduce the performance gap between VSR and ASR. Moreover, a comprehensive\nmultimodal, multistage training regimen for the JEP-KD framework is\nestablished, bolstering the robustness and efficacy of the training process.\nExperiment results demonstrate that JEP-KD significantly improves the\nperformance of VSR models and demonstrates versatility across different VSR\nplatforms, indicating its potential for broader application within other\nmultimodal tasks.", "published": "2024-03-04 00:30:24", "link": "http://arxiv.org/abs/2403.18843v1", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "PixIT: Joint Training of Speaker Diarization and Speech Separation from\n  Real-world Multi-speaker Recordings", "abstract": "A major drawback of supervised speech separation (SSep) systems is their\nreliance on synthetic data, leading to poor real-world generalization. Mixture\ninvariant training (MixIT) was proposed as an unsupervised alternative that\nuses real recordings, yet struggles with overseparation and adapting to\nlong-form audio. We introduce PixIT, a joint approach that combines permutation\ninvariant training (PIT) for speaker diarization (SD) and MixIT for SSep. With\na small extra requirement of needing SD labels, it solves the problem of\noverseparation and allows stitching local separated sources leveraging existing\nwork on clustering-based neural SD. We measure the quality of the separated\nsources via applying automatic speech recognition (ASR) systems to them. PixIT\nboosts the performance of various ASR systems across two meeting corpora both\nin terms of the speaker-attributed and utterance-based word error rates while\nnot requiring any fine-tuning.", "published": "2024-03-04 18:18:36", "link": "http://arxiv.org/abs/2403.02288v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "6DoF SELD: Sound Event Localization and Detection Using Microphones and\n  Motion Tracking Sensors on self-motioning human", "abstract": "We aim to perform sound event localization and detection (SELD) using\nwearable equipment for a moving human, such as a pedestrian. Conventional SELD\ntasks have dealt only with microphone arrays located in static positions.\nHowever, self-motion with three rotational and three translational degrees of\nfreedom (6DoF) shall be considered for wearable microphone arrays. A system\ntrained only with a dataset using microphone arrays in a fixed position would\nbe unable to adapt to the fast relative motion of sound events associated with\nself-motion, resulting in the degradation of SELD performance. To address this,\nwe designed 6DoF SELD Dataset for wearable systems, the first SELD dataset\nconsidering the self-motion of microphones. Furthermore, we proposed a\nmulti-modal SELD system that jointly utilizes audio and motion tracking sensor\nsignals. These sensor signals are expected to help the system find useful\nacoustic cues for SELD on the basis of the current self-motion state.\nExperimental results on our dataset show that the proposed method effectively\nimproves SELD performance with a mechanism to extract acoustic features\nconditioned by sensor signals.", "published": "2024-03-04 01:45:19", "link": "http://arxiv.org/abs/2403.01670v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "What do neural networks listen to? Exploring the crucial bands in Speech\n  Enhancement using Sinc-convolution", "abstract": "This study introduces a reformed Sinc-convolution (Sincconv) framework\ntailored for the encoder component of deep networks for speech enhancement\n(SE). The reformed Sincconv, based on parametrized sinc functions as band-pass\nfilters, offers notable advantages in terms of training efficiency, filter\ndiversity, and interpretability. The reformed Sinc-conv is evaluated in\nconjunction with various SE models, showcasing its ability to boost SE\nperformance. Furthermore, the reformed Sincconv provides valuable insights into\nthe specific frequency components that are prioritized in an SE scenario. This\nopens up a new direction of SE research and improving our knowledge of their\noperating dynamics.", "published": "2024-03-04 07:27:25", "link": "http://arxiv.org/abs/2403.01785v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ConSep: a Noise- and Reverberation-Robust Speech Separation Framework by\n  Magnitude Conditioning", "abstract": "Speech separation has recently made significant progress thanks to the\nfine-grained vision used in time-domain methods. However, several studies have\nshown that adopting Short-Time Fourier Transform (STFT) for feature extraction\ncould be beneficial when encountering harsher conditions, such as noise or\nreverberation. Therefore, we propose a magnitude-conditioned time-domain\nframework, ConSep, to inherit the beneficial characteristics. The experiment\nshows that ConSep promotes performance in anechoic, noisy, and reverberant\nsettings compared to two celebrated methods, SepFormer and Bi-Sep. Furthermore,\nwe visualize the components of ConSep to strengthen the advantages and cohere\nwith the actualities we have found in preliminary studies.", "published": "2024-03-04 07:34:24", "link": "http://arxiv.org/abs/2403.01792v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A robust audio deepfake detection system via multi-view feature", "abstract": "With the advancement of generative modeling techniques, synthetic human\nspeech becomes increasingly indistinguishable from real, and tricky challenges\nare elicited for the audio deepfake detection (ADD) system. In this paper, we\nexploit audio features to improve the generalizability of ADD systems.\nInvestigation of the ADD task performance is conducted over a broad range of\naudio features, including various handcrafted features and learning-based\nfeatures. Experiments show that learning-based audio features pretrained on a\nlarge amount of data generalize better than hand-crafted features on\nout-of-domain scenarios. Subsequently, we further improve the generalizability\nof the ADD system using proposed multi-feature approaches to incorporate\ncomplimentary information from features of different views. The model trained\non ASV2019 data achieves an equal error rate of 24.27\\% on the In-the-Wild\ndataset.", "published": "2024-03-04 11:57:32", "link": "http://arxiv.org/abs/2403.01960v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Fine-Grained Quantitative Emotion Editing for Speech Generation", "abstract": "It remains a significant challenge how to quantitatively control the\nexpressiveness of speech emotion in speech generation. In this work, we present\na novel approach for manipulating the rendering of emotions for speech\ngeneration. We propose a hierarchical emotion distribution extractor, i.e.\nHierarchical ED, that quantifies the intensity of emotions at different levels\nof granularity. Support vector machines (SVMs) are employed to rank emotion\nintensity, resulting in a hierarchical emotional embedding. Hierarchical ED is\nsubsequently integrated into the FastSpeech2 framework, guiding the model to\nlearn emotion intensity at phoneme, word, and utterance levels. During\nsynthesis, users can manually edit the emotional intensity of the generated\nvoices. Both objective and subjective evaluations demonstrate the effectiveness\nof the proposed network in terms of fine-grained quantitative emotion editing.", "published": "2024-03-04 12:53:15", "link": "http://arxiv.org/abs/2403.02002v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SA-SOT: Speaker-Aware Serialized Output Training for Multi-Talker ASR", "abstract": "Multi-talker automatic speech recognition plays a crucial role in scenarios\ninvolving multi-party interactions, such as meetings and conversations. Due to\nits inherent complexity, this task has been receiving increasing attention.\nNotably, the serialized output training (SOT) stands out among various\napproaches because of its simplistic architecture and exceptional performance.\nHowever, the frequent speaker changes in token-level SOT (t-SOT) present\nchallenges for the autoregressive decoder in effectively utilizing context to\npredict output sequences. To address this issue, we introduce a masked t-SOT\nlabel, which serves as the cornerstone of an auxiliary training loss.\nAdditionally, we utilize a speaker similarity matrix to refine the\nself-attention mechanism of the decoder. This strategic adjustment enhances\ncontextual relationships within the same speaker's tokens while minimizing\ninteractions between different speakers' tokens. We denote our method as\nspeaker-aware SOT (SA-SOT). Experiments on the Librispeech datasets demonstrate\nthat our SA-SOT obtains a relative cpWER reduction ranging from 12.75% to\n22.03% on the multi-talker test sets. Furthermore, with more extensive\ntraining, our method achieves an impressive cpWER of 3.41%, establishing a new\nstate-of-the-art result on the LibrispeechMix dataset.", "published": "2024-03-04 13:10:40", "link": "http://arxiv.org/abs/2403.02010v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Robust Wake Word Spotting With Frame-Level Cross-Modal Attention Based\n  Audio-Visual Conformer", "abstract": "In recent years, neural network-based Wake Word Spotting achieves good\nperformance on clean audio samples but struggles in noisy environments.\nAudio-Visual Wake Word Spotting (AVWWS) receives lots of attention because\nvisual lip movement information is not affected by complex acoustic scenes.\nPrevious works usually use simple addition or concatenation for multi-modal\nfusion. The inter-modal correlation remains relatively under-explored. In this\npaper, we propose a novel module called Frame-Level Cross-Modal Attention\n(FLCMA) to improve the performance of AVWWS systems. This module can help model\nmulti-modal information at the frame-level through synchronous lip movements\nand speech signals. We train the end-to-end FLCMA based Audio-Visual Conformer\nand further improve the performance by fine-tuning pre-trained uni-modal models\nfor the AVWWS task. The proposed system achieves a new state-of-the-art result\n(4.57% WWS score) on the far-field MISP dataset.", "published": "2024-03-04 03:25:42", "link": "http://arxiv.org/abs/2403.01700v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
