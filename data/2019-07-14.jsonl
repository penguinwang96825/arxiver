{"title": "Microsoft Translator at WMT 2019: Towards Large-Scale Document-Level\n  Neural Machine Translation", "abstract": "This paper describes the Microsoft Translator submissions to the WMT19 news\ntranslation shared task for English-German. Our main focus is document-level\nneural machine translation with deep transformer models. We start with strong\nsentence-level baselines, trained on large-scale data created via\ndata-filtering and noisy back-translation and find that back-translation seems\nto mainly help with translationese input. We explore fine-tuning techniques,\ndeeper models and different ensembling strategies to counter these effects.\nUsing document boundaries present in the authentic and synthetic parallel data,\nwe create sequences of up to 1000 subword segments and train transformer\ntranslation models. We experiment with data augmentation techniques for the\nsmaller authentic data with document-boundaries and for larger authentic data\nwithout boundaries. We further explore multi-task training for the\nincorporation of document-level source language monolingual data via the\nBERT-objective on the encoder and two-pass decoding for combinations of\nsentence-level and document-level systems. Based on preliminary human\nevaluation results, evaluators strongly prefer the document-level systems over\nour comparable sentence-level system. The document-level systems also seem to\nscore higher than the human references in source-based direct assessment.", "published": "2019-07-14 05:47:53", "link": "http://arxiv.org/abs/1907.06170v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simple Automatic Post-editing for Arabic-Japanese Machine Translation", "abstract": "A common bottleneck for developing machine translation (MT) systems for some\nlanguage pairs is the lack of direct parallel translation data sets, in general\nand in certain domains. Alternative solutions such as zero-shot models or\npivoting techniques are successful in getting a strong baseline, but are often\nbelow the more supported language-pair systems. In this paper, we focus on\nArabic-Japanese machine translation, a less studied language pair; and we work\nwith a unique parallel corpus of Arabic news articles that were manually\ntranslated to Japanese. We use this parallel corpus to adapt a state-of-the-art\ndomain/genre agnostic neural MT system via a simple automatic post-editing\ntechnique. Our results and detailed analysis suggest that this approach is\nquite viable for less supported language pairs in specific domains.", "published": "2019-07-14 11:56:20", "link": "http://arxiv.org/abs/1907.06210v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TWEETQA: A Social Media Focused Question Answering Dataset", "abstract": "With social media becoming increasingly pop-ular on which lots of news and\nreal-time eventsare reported, developing automated questionanswering systems is\ncritical to the effective-ness of many applications that rely on real-time\nknowledge. While previous datasets haveconcentrated on question answering (QA)\nforformal text like news and Wikipedia, wepresent the first large-scale dataset\nfor QA oversocial media data. To ensure that the tweetswe collected are useful,\nwe only gather tweetsused by journalists to write news articles. Wethen ask\nhuman annotators to write questionsand answers upon these tweets. Unlike\notherQA datasets like SQuAD in which the answersare extractive, we allow the\nanswers to be ab-stractive. We show that two recently proposedneural models\nthat perform well on formaltexts are limited in their performance when ap-plied\nto our dataset. In addition, even the fine-tuned BERT model is still lagging\nbehind hu-man performance with a large margin. Our re-sults thus point to the\nneed of improved QAsystems targeting social media text.", "published": "2019-07-14 22:20:59", "link": "http://arxiv.org/abs/1907.06292v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lexical Simplification with Pretrained Encoders", "abstract": "Lexical simplification (LS) aims to replace complex words in a given sentence\nwith their simpler alternatives of equivalent meaning. Recently unsupervised\nlexical simplification approaches only rely on the complex word itself\nregardless of the given sentence to generate candidate substitutions, which\nwill inevitably produce a large number of spurious candidates. We present a\nsimple LS approach that makes use of the Bidirectional Encoder Representations\nfrom Transformers (BERT) which can consider both the given sentence and the\ncomplex word during generating candidate substitutions for the complex word.\nSpecifically, we mask the complex word of the original sentence for feeding\ninto the BERT to predict the masked token. The predicted results will be used\nas candidate substitutions. Despite being entirely unsupervised, experimental\nresults show that our approach obtains obvious improvement compared with these\nbaselines leveraging linguistic databases and parallel corpus, outperforming\nthe state-of-the-art by more than 12 Accuracy points on three well-known\nbenchmarks.", "published": "2019-07-14 14:19:22", "link": "http://arxiv.org/abs/1907.06226v5", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Automatic Repair and Type Binding of Undeclared Variables using Neural\n  Networks", "abstract": "Deep learning had been used in program analysis for the prediction of hidden\nsoftware defects using software defect datasets, security vulnerabilities using\ngenerative adversarial networks as well as identifying syntax errors by\nlearning a trained neural machine translation on program codes. However, all\nthese approaches either require defect datasets or bug-free source codes that\nare executable for training the deep learning model. Our neural network model\nis neither trained with any defect datasets nor bug-free programming source\ncodes, instead it is trained using structural semantic details of Abstract\nSyntax Tree (AST) where each node represents a construct appearing in the\nsource code. This model is implemented to fix one of the most common semantic\nerrors, such as undeclared variable errors as well as infer their type\ninformation before program compilation. By this approach, the model has\nachieved in correctly locating and identifying 81% of the programs on prutor\ndataset of 1059 programs with only undeclared variable errors and also\ninferring their types correctly in 80% of the programs.", "published": "2019-07-14 11:14:14", "link": "http://arxiv.org/abs/1907.06205v1", "categories": ["cs.SE", "cs.CL", "cs.FL", "cs.LG", "cs.PL", "stat.ML"], "primary_category": "cs.SE"}
{"title": "Autoencoding sensory substitution", "abstract": "Tens of millions of people live blind, and their number is ever increasing.\nVisual-to-auditory sensory substitution (SS) encompasses a family of cheap,\ngeneric solutions to assist the visually impaired by conveying visual\ninformation through sound. The required SS training is lengthy: months of\neffort is necessary to reach a practical level of adaptation. There are two\nreasons for the tedious training process: the elongated substituting audio\nsignal, and the disregard for the compressive characteristics of the human\nhearing system. To overcome these obstacles, we developed a novel class of SS\nmethods, by training deep recurrent autoencoders for image-to-sound conversion.\nWe successfully trained deep learning models on different datasets to execute\nvisual-to-auditory stimulus conversion. By constraining the visual space, we\ndemonstrated the viability of shortened substituting audio signals, while\nproposing mechanisms, such as the integration of computational hearing models,\nto optimally convey visual features in the substituting stimulus as\nperceptually discernible auditory components. We tested our approach in two\nseparate cases. In the first experiment, the author went blindfolded for 5\ndays, while performing SS training on hand posture discrimination. The second\nexperiment assessed the accuracy of reaching movements towards objects on a\ntable. In both test cases, above-chance-level accuracy was attained after a few\nhours of training. Our novel SS architecture broadens the horizon of\nrehabilitation methods engineered for the visually impaired. Further\nimprovements on the proposed model shall yield hastened rehabilitation of the\nblind and a wider adaptation of SS devices as a consequence.", "published": "2019-07-14 21:58:10", "link": "http://arxiv.org/abs/1907.06286v1", "categories": ["q-bio.NC", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "q-bio.NC"}
{"title": "The Bach Doodle: Approachable music composition with machine learning at\n  scale", "abstract": "To make music composition more approachable, we designed the first AI-powered\nGoogle Doodle, the Bach Doodle, where users can create their own melody and\nhave it harmonized by a machine learning model Coconet (Huang et al., 2017) in\nthe style of Bach. For users to input melodies, we designed a simplified\nsheet-music based interface. To support an interactive experience at scale, we\nre-implemented Coconet in TensorFlow.js (Smilkov et al., 2019) to run in the\nbrowser and reduced its runtime from 40s to 2s by adopting dilated depth-wise\nseparable convolutions and fusing operations. We also reduced the model\ndownload size to approximately 400KB through post-training weight quantization.\nWe calibrated a speed test based on partial model evaluation time to determine\nif the harmonization request should be performed locally or sent to remote TPU\nservers. In three days, people spent 350 years worth of time playing with the\nBach Doodle, and Coconet received more than 55 million queries. Users could\nchoose to rate their compositions and contribute them to a public dataset,\nwhich we are releasing with this paper. We hope that the community finds this\ndataset useful for applications ranging from ethnomusicological studies, to\nmusic education, to improving machine learning models.", "published": "2019-07-14 23:39:12", "link": "http://arxiv.org/abs/1907.06637v1", "categories": ["cs.SD", "cs.HC", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
