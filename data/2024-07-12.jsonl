{"title": "Large Language Models as Biomedical Hypothesis Generators: A\n  Comprehensive Evaluation", "abstract": "The rapid growth of biomedical knowledge has outpaced our ability to\nefficiently extract insights and generate novel hypotheses. Large language\nmodels (LLMs) have emerged as a promising tool to revolutionize knowledge\ninteraction and potentially accelerate biomedical discovery. In this paper, we\npresent a comprehensive evaluation of LLMs as biomedical hypothesis generators.\nWe construct a dataset of background-hypothesis pairs from biomedical\nliterature, carefully partitioned into training, seen, and unseen test sets\nbased on publication date to mitigate data contamination. Using this dataset,\nwe assess the hypothesis generation capabilities of top-tier instructed models\nin zero-shot, few-shot, and fine-tuning settings. To enhance the exploration of\nuncertainty, a crucial aspect of scientific discovery, we incorporate tool use\nand multi-agent interactions in our evaluation framework. Furthermore, we\npropose four novel metrics grounded in extensive literature review to evaluate\nthe quality of generated hypotheses, considering both LLM-based and human\nassessments. Our experiments yield two key findings: 1) LLMs can generate novel\nand validated hypotheses, even when tested on literature unseen during\ntraining, and 2) Increasing uncertainty through multi-agent interactions and\ntool use can facilitate diverse candidate generation and improve zero-shot\nhypothesis generation performance. However, we also observe that the\nintegration of additional knowledge through few-shot learning and tool use may\nnot always lead to performance gains, highlighting the need for careful\nconsideration of the type and scope of external knowledge incorporated. These\nfindings underscore the potential of LLMs as powerful aids in biomedical\nhypothesis generation and provide valuable insights to guide further research\nin this area.", "published": "2024-07-12 02:55:13", "link": "http://arxiv.org/abs/2407.08940v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain-Hierarchy Adaptation via Chain of Iterative Reasoning for\n  Few-shot Hierarchical Text Classification", "abstract": "Recently, various pre-trained language models (PLMs) have been proposed to\nprove their impressive performances on a wide range of few-shot tasks. However,\nlimited by the unstructured prior knowledge in PLMs, it is difficult to\nmaintain consistent performance on complex structured scenarios, such as\nhierarchical text classification (HTC), especially when the downstream data is\nextremely scarce. The main challenge is how to transfer the unstructured\nsemantic space in PLMs to the downstream domain hierarchy. Unlike previous work\non HTC which directly performs multi-label classification or uses graph neural\nnetwork (GNN) to inject label hierarchy, in this work, we study the HTC problem\nunder a few-shot setting to adapt knowledge in PLMs from an unstructured manner\nto the downstream hierarchy. Technically, we design a simple yet effective\nmethod named Hierarchical Iterative Conditional Random Field (HierICRF) to\nsearch the most domain-challenging directions and exquisitely crafts\ndomain-hierarchy adaptation as a hierarchical iterative language modeling\nproblem, and then it encourages the model to make hierarchical consistency\nself-correction during the inference, thereby achieving knowledge transfer with\nhierarchical consistency preservation. We perform HierICRF on various\narchitectures, and extensive experiments on two popular HTC datasets\ndemonstrate that prompt with HierICRF significantly boosts the few-shot HTC\nperformance with an average Micro-F1 by 28.80% to 1.50% and Macro-F1 by 36.29%\nto 1.5% over the previous state-of-the-art (SOTA) baselines under few-shot\nsettings, while remaining SOTA hierarchical consistency performance.", "published": "2024-07-12 03:21:57", "link": "http://arxiv.org/abs/2407.08959v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Prompt Tuning: Enable Autonomous Role-Playing in LLMs", "abstract": "Recent advancements in LLMs have showcased their remarkable role-playing\ncapabilities, able to accurately simulate the dialogue styles and cognitive\nprocesses of various roles based on different instructions and contexts.\nStudies indicate that assigning LLMs the roles of experts, a strategy known as\nrole-play prompting, can enhance their performance in the corresponding\ndomains. However, the prompt needs to be manually designed for the given\nproblem, requiring certain expertise and iterative modifications. To this end,\nwe propose self-prompt tuning, making LLMs themselves generate role-play\nprompts through fine-tuning. Leveraging the LIMA dataset as our foundational\ncorpus, we employ GPT-4 to annotate role-play prompts for each data points,\nresulting in the creation of the LIMA-Role dataset. We then fine-tune LLMs like\nLlama-2-7B and Mistral-7B on LIMA-Role. Consequently, the self-prompt tuned\nLLMs can automatically generate expert role prompts for any given question. We\nextensively evaluate self-prompt tuned LLMs on widely used NLP benchmarks and\nopen-ended question test. Our empirical results illustrate that self-prompt\ntuned LLMs outperform standard instruction tuned baselines across most\ndatasets. This highlights the great potential of utilizing fine-tuning to\nenable LLMs to self-prompt, thereby automating complex prompting strategies. We\nrelease the dataset, models, and code at this\n\\href{https://anonymous.4open.science/r/Self-Prompt-Tuning-739E/}{url}.", "published": "2024-07-12 05:26:24", "link": "http://arxiv.org/abs/2407.08995v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking Language Model Creativity: A Case Study on Code Generation", "abstract": "As LLMs become increasingly prevalent, it is interesting to consider how\n``creative'' these models can be. From cognitive science, creativity consists\nof at least two key characteristics: \\emph{convergent} thinking (purposefulness\nto achieve a given goal) and \\emph{divergent} thinking (adaptability to explore\nnew environments or constraints) \\citep{runco2003critical}. In this work, we\nintroduce a framework for quantifying LLM creativity that incorporates the two\ndesign ingredients: (1) We introduce DENIAL PROMPTING which pushes LLMs to\ndevelop more creative solutions to a given problem by incrementally imposing\nnew constraints on the previous solution, compelling LLMs to adopt new\nstrategies. (2) We define NEOGAUGE, a metric that quantifies both convergent\nand divergent thinking in the generated creative responses by LLMs. We test the\nproposed framework on Codeforces problems, which serve as both a natural\ndataset for coding tasks and a collection of prior human solutions. We quantify\nNEOGAUGE for various proprietary and open-source models and find that even the\nmost creative model, GPT-4, still falls short of demonstrating human-like\ncreativity. We also experiment with advanced reasoning strategies (MCTS,\nself-correction, etc.) and observe no significant improvement in creativity. As\na by-product of our analysis, we release NEOCODER dataset for reproducing our\nresults on future models.", "published": "2024-07-12 05:55:22", "link": "http://arxiv.org/abs/2407.09007v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CompAct: Compressing Retrieved Documents Actively for Question Answering", "abstract": "Retrieval-augmented generation supports language models to strengthen their\nfactual groundings by providing external contexts. However, language models\noften face challenges when given extensive information, diminishing their\neffectiveness in solving questions. Context compression tackles this issue by\nfiltering out irrelevant information, but current methods still struggle in\nrealistic scenarios where crucial information cannot be captured with a\nsingle-step approach. To overcome this limitation, we introduce CompAct, a\nnovel framework that employs an active strategy to condense extensive documents\nwithout losing key information. Our experiments demonstrate that CompAct brings\nsignificant improvements in both performance and compression rate on multi-hop\nquestion-answering benchmarks. CompAct flexibly operates as a cost-efficient\nplug-in module with various off-the-shelf retrievers or readers, achieving\nexceptionally high compression rates (47x).", "published": "2024-07-12 06:06:54", "link": "http://arxiv.org/abs/2407.09014v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "3M-Health: Multimodal Multi-Teacher Knowledge Distillation for Mental\n  Health Detection", "abstract": "The significance of mental health classification is paramount in contemporary\nsociety, where digital platforms serve as crucial sources for monitoring\nindividuals' well-being. However, existing social media mental health datasets\nprimarily consist of text-only samples, potentially limiting the efficacy of\nmodels trained on such data. Recognising that humans utilise cross-modal\ninformation to comprehend complex situations or issues, we present a novel\napproach to address the limitations of current methodologies. In this work, we\nintroduce a Multimodal and Multi-Teacher Knowledge Distillation model for\nMental Health Classification, leveraging insights from cross-modal human\nunderstanding. Unlike conventional approaches that often rely on simple\nconcatenation to integrate diverse features, our model addresses the challenge\nof appropriately representing inputs of varying natures (e.g., texts and\nsounds). To mitigate the computational complexity associated with integrating\nall features into a single model, we employ a multimodal and multi-teacher\narchitecture. By distributing the learning process across multiple teachers,\neach specialising in a particular feature extraction aspect, we enhance the\noverall mental health classification performance. Through experimental\nvalidation, we demonstrate the efficacy of our model in achieving improved\nperformance.", "published": "2024-07-12 06:22:45", "link": "http://arxiv.org/abs/2407.09020v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "New Desiderata for Direct Preference Optimization", "abstract": "Large language models in the past have typically relied on some form of\nreinforcement learning with human feedback (RLHF) to better align model\nresponses with human preferences. However, because of oft-observed\ninstabilities when implementing these RLHF pipelines, various\nreparameterization techniques have recently been introduced to sidestep the\nneed for separately learning an RL reward model. Instead, directly fine-tuning\nfor human preferences is achieved via the minimization of a single closed-form\ntraining objective, a process originally referred to as direct preference\noptimization (DPO) and followed by several notable descendants. Although\neffective in certain real-world settings, we introduce new evaluation criteria\nthat serve to highlight unresolved shortcomings in the ability of existing DPO\nmethods to interpolate between a pre-trained reference model and empirical\nmeasures of human preferences, as well as unavoidable trade-offs in how low-\nand high-quality responses are regularized and constraints are handled. Our\ninsights then motivate an alternative DPO-like loss that provably mitigates\nthese limitations. Empirical results serve to corroborate notable aspects of\nour analyses.", "published": "2024-07-12 07:52:32", "link": "http://arxiv.org/abs/2407.09072v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Effectiveness of Methods for Persona Extraction", "abstract": "The paper presents a study of methods for extracting information about\ndialogue participants and evaluating their performance in Russian. To train\nmodels for this task, the Multi-Session Chat dataset was translated into\nRussian using multiple translation models, resulting in improved data quality.\nA metric based on the F-score concept is presented to evaluate the\neffectiveness of the extraction models. The metric uses a trained classifier to\nidentify the dialogue participant to whom the persona belongs. Experiments were\nconducted on MBart, FRED-T5, Starling-7B, which is based on the Mistral, and\nEncoder2Encoder models. The results demonstrated that all models exhibited an\ninsufficient level of recall in the persona extraction task. The incorporation\nof the NCE Loss improved the model's precision at the expense of its recall.\nFurthermore, increasing the model's size led to enhanced extraction of\npersonas.", "published": "2024-07-12 11:30:10", "link": "http://arxiv.org/abs/2407.09181v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does Incomplete Syntax Influence Korean Language Model? Focusing on Word\n  Order and Case Markers", "abstract": "Syntactic elements, such as word order and case markers, are fundamental in\nnatural language processing. Recent studies show that syntactic information\nboosts language model performance and offers clues for people to understand\ntheir learning mechanisms. Unlike languages with a fixed word order such as\nEnglish, Korean allows for varied word sequences, despite its canonical\nstructure, due to case markers that indicate the functions of sentence\ncomponents. This study explores whether Korean language models can accurately\ncapture this flexibility. We note that incomplete word orders and omitted case\nmarkers frequently appear in ordinary Korean communication. To investigate this\nfurther, we introduce the Syntactically Incomplete Korean (SIKO) dataset.\nThrough SIKO, we assessed Korean language models' flexibility with incomplete\nsyntax and confirmed the dataset's training value. Results indicate these\nmodels reflect Korean's inherent flexibility, accurately handling incomplete\ninputs. Moreover, fine-tuning with SIKO enhances the ability to handle common\nincomplete Korean syntactic forms. The dataset's simple construction process,\ncoupled with significant performance enhancements, solidifies its standing as\nan effective data augmentation technique.", "published": "2024-07-12 11:33:41", "link": "http://arxiv.org/abs/2407.09184v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Sociolinguistic Foundations of Language Modeling", "abstract": "In this paper, we introduce a sociolinguistic perspective on language\nmodeling. We claim that large language models are inherently models of\nvarieties of language, and we consider how this insight can inform the\ndevelopment and deployment of large language models. We begin by presenting a\ntechnical definition of the concept of a variety of language as developed in\nsociolinguistics. We then discuss how this perspective can help address five\nbasic challenges in language modeling: social bias, domain adaptation,\nalignment, language change, and scale. Ultimately, we argue that it is crucial\nto carefully define and compile training corpora that accurately represent the\nspecific varieties of language being modeled to maximize the performance and\nsocietal value of large language models.", "published": "2024-07-12 13:12:55", "link": "http://arxiv.org/abs/2407.09241v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformer Layers as Painters", "abstract": "Despite their nearly universal adoption for large language models, the\ninternal workings of transformers are not well understood. We aim to better\nunderstand the impact of removing or reorganizing information throughout the\nlayers of a pretrained transformer. Such an understanding could both yield\nbetter usage of existing models as well as to make architectural improvements\nto produce new variants. We present a series of empirical studies on frozen\nmodels that show that the lower and final layers of pretrained transformers\ndiffer from middle layers, but that middle layers have a surprising amount of\nuniformity. We further show that some classes of problems have robustness to\nskipping layers, running the layers in an order different from how they were\ntrained, or running the layers in parallel. Our observations suggest that even\nfrozen pretrained models may gracefully trade accuracy for latency by skipping\nlayers or running layers in parallel.", "published": "2024-07-12 14:31:05", "link": "http://arxiv.org/abs/2407.09298v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scalability of Bayesian Network Structure Elicitation with Large\n  Language Models: a Novel Methodology and Comparative Analysis", "abstract": "In this work, we propose a novel method for Bayesian Networks (BNs) structure\nelicitation that is based on the initialization of several LLMs with different\nexperiences, independently querying them to create a structure of the BN, and\nfurther obtaining the final structure by majority voting. We compare the method\nwith one alternative method on various widely and not widely known BNs of\ndifferent sizes and study the scalability of both methods on them. We also\npropose an approach to check the contamination of BNs in LLM, which shows that\nsome widely known BNs are inapplicable for testing the LLM usage for BNs\nstructure elicitation. We also show that some BNs may be inapplicable for such\nexperiments because their node names are indistinguishable. The experiments on\nthe other BNs show that our method performs better than the existing method\nwith one of the three studied LLMs; however, the performance of both methods\nsignificantly decreases with the increase in BN size.", "published": "2024-07-12 14:52:13", "link": "http://arxiv.org/abs/2407.09311v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Open (Clinical) LLMs are Sensitive to Instruction Phrasings", "abstract": "Instruction-tuned Large Language Models (LLMs) can perform a wide range of\ntasks given natural language instructions to do so, but they are sensitive to\nhow such instructions are phrased. This issue is especially concerning in\nhealthcare, as clinicians are unlikely to be experienced prompt engineers and\nthe potential consequences of inaccurate outputs are heightened in this domain.\n  This raises a practical question: How robust are instruction-tuned LLMs to\nnatural variations in the instructions provided for clinical NLP tasks? We\ncollect prompts from medical doctors across a range of tasks and quantify the\nsensitivity of seven LLMs -- some general, others specialized -- to natural\n(i.e., non-adversarial) instruction phrasings. We find that performance varies\nsubstantially across all models, and that -- perhaps surprisingly --\ndomain-specific models explicitly trained on clinical data are especially\nbrittle, compared to their general domain counterparts. Further, arbitrary\nphrasing differences can affect fairness, e.g., valid but distinct instructions\nfor mortality prediction yield a range both in overall performance, and in\nterms of differences between demographic groups.", "published": "2024-07-12 17:00:44", "link": "http://arxiv.org/abs/2407.09429v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ASTPrompter: Weakly Supervised Automated Language Model Red-Teaming to\n  Identify Low-Perplexity Toxic Prompts", "abstract": "Conventional approaches for the automated red-teaming of large language\nmodels (LLMs) aim to identify prompts that elicit toxic outputs from a frozen\nlanguage model (the defender). This often results in the prompting model (the\nadversary) producing text that is unlikely to arise during autoregression. In\nresponse, we propose a reinforcement learning formulation of LLM red-teaming\ndesigned to discover prompts that both (1) elicit toxic outputs from a defender\nand (2) have low perplexity as scored by that defender. These prompts are the\nmost pertinent in a red-teaming setting because the defender generates them\nwith high probability. We solve this formulation with an online and weakly\nsupervised form of Identity Preference Optimization (IPO), attacking models\nranging from 137M to 7.8B parameters. Our policy performs competitively,\nproducing prompts that induce defender toxicity at a rate of 2-23 times higher\nthan baseline across model scales. Importantly, these prompts have lower\nperplexity than both automatically generated and human-written attacks.\nFurthermore, our method creates black-box attacks with 5.4-14 times increased\ntoxicity. To assess the downstream utility of our method, we use rollouts from\nour policy as negative examples for downstream toxicity tuning and demonstrate\nimproved safety.", "published": "2024-07-12 17:33:34", "link": "http://arxiv.org/abs/2407.09447v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Chinese are Chinese Language Models? The Puzzling Lack of Language\n  Policy in China's LLMs", "abstract": "Contemporary language models are increasingly multilingual, but Chinese LLM\ndevelopers must navigate complex political and business considerations of\nlanguage diversity. Language policy in China aims at influencing the public\ndiscourse and governing a multi-ethnic society, and has gradually transitioned\nfrom a pluralist to a more assimilationist approach since 1949. We explore the\nimpact of these influences on current language technology. We evaluate six\nopen-source multilingual LLMs pre-trained by Chinese companies on 18 languages,\nspanning a wide range of Chinese, Asian, and Anglo-European languages. Our\nexperiments show Chinese LLMs performance on diverse languages is\nindistinguishable from international LLMs. Similarly, the models' technical\nreports also show lack of consideration for pretraining data language coverage\nexcept for English and Mandarin Chinese. Examining Chinese AI policy, model\nexperiments, and technical reports, we find no sign of any consistent policy,\neither for or against, language diversity in China's LLM development. This\nleaves a puzzling fact that while China regulates both the languages people use\ndaily as well as language model development, they do not seem to have any\npolicy on the languages in language models.", "published": "2024-07-12 19:21:40", "link": "http://arxiv.org/abs/2407.09652v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models for Integrating Social Determinant of Health Data:\n  A Case Study on Heart Failure 30-Day Readmission Prediction", "abstract": "Social determinants of health (SDOH) $-$ the myriad of circumstances in which\npeople live, grow, and age $-$ play an important role in health outcomes.\nHowever, existing outcome prediction models often only use proxies of SDOH as\nfeatures. Recent open data initiatives present an opportunity to construct a\nmore comprehensive view of SDOH, but manually integrating the most relevant\ndata for individual patients becomes increasingly challenging as the volume and\ndiversity of public SDOH data grows. Large language models (LLMs) have shown\npromise at automatically annotating structured data. Here, we conduct an\nend-to-end case study evaluating the feasibility of using LLMs to integrate\nSDOH data, and the utility of these SDOH features for clinical prediction. We\nfirst manually label 700+ variables from two publicly-accessible SDOH data\nsources to one of five semantic SDOH categories. Then, we benchmark performance\nof 9 open-source LLMs on this classification task. Finally, we train ML models\nto predict 30-day hospital readmission among 39k heart failure (HF) patients,\nand we compare the prediction performance of the categorized SDOH variables\nwith standard clinical variables. Additionally, we investigate the impact of\nfew-shot LLM prompting on LLM annotation performance, and perform a metadata\nablation study on prompts to evaluate which information helps LLMs accurately\nannotate these variables. We find that some open-source LLMs can effectively,\naccurately annotate SDOH variables with zero-shot prompting without the need\nfor fine-tuning. Crucially, when combined with standard clinical features, the\nLLM-annotated Neighborhood and Built Environment subset of the SDOH variables\nshows the best performance predicting 30-day readmission of HF patients.", "published": "2024-07-12 21:14:06", "link": "http://arxiv.org/abs/2407.09688v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What an Elegant Bridge: Multilingual LLMs are Biased Similarly in\n  Different Languages", "abstract": "This paper investigates biases of Large Language Models (LLMs) through the\nlens of grammatical gender. Drawing inspiration from seminal works in\npsycholinguistics, particularly the study of gender's influence on language\nperception, we leverage multilingual LLMs to revisit and expand upon the\nfoundational experiments of Boroditsky (2003). Employing LLMs as a novel method\nfor examining psycholinguistic biases related to grammatical gender, we prompt\na model to describe nouns with adjectives in various languages, focusing\nspecifically on languages with grammatical gender. In particular, we look at\nadjective co-occurrences across gender and languages, and train a binary\nclassifier to predict grammatical gender given adjectives an LLM uses to\ndescribe a noun. Surprisingly, we find that a simple classifier can not only\npredict noun gender above chance but also exhibit cross-language\ntransferability. We show that while LLMs may describe words differently in\ndifferent languages, they are biased similarly.", "published": "2024-07-12 22:10:16", "link": "http://arxiv.org/abs/2407.09704v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Evolving GPT: A Lifelong Autonomous Experiential Learner", "abstract": "To improve the performance of large language models (LLMs), researchers have\nexplored providing LLMs with textual task-solving experience via prompts.\nHowever, they rely on manual efforts to acquire and apply such experience for\neach task, which is not feasible for the growing demand for LLMs and the\nvariety of user questions. To address this issue, we design a lifelong\nautonomous experiential learning framework based on LLMs to explore whether\nLLMs can imitate human ability for learning and utilizing experience. It\nautonomously learns and accumulates experience through experience transfer and\ninduction, categorizing the types of input questions to select which\naccumulated experience to employ for them. Experimental results on six widely\nused NLP datasets show that our framework performs reliably in each\nintermediate step and effectively improves the performance of GPT-3.5 and\nGPT-4. This validates the feasibility of using LLMs to mimic human experiential\nlearning and application capabilities. Additionally, we provide a detailed\nanalysis of the behavior of our framework at each step.", "published": "2024-07-12 02:49:13", "link": "http://arxiv.org/abs/2407.08937v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Detect, Investigate, Judge and Determine: A Knowledge-guided Framework\n  for Few-shot Fake News Detection", "abstract": "Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news\nfrom real ones in extremely low-resource scenarios. This task has garnered\nincreased attention due to the widespread dissemination and harmful impact of\nfake news on social media. Large Language Models (LLMs) have demonstrated\ncompetitive performance with the help of their rich prior knowledge and\nexcellent in-context learning abilities. However, existing methods face\nsignificant limitations, such as the Understanding Ambiguity and Information\nScarcity, which significantly undermine the potential of LLMs. To address these\nshortcomings, we propose a Dual-perspective Knowledge-guided Fake News\nDetection (DKFND) model, designed to enhance LLMs from both inside and outside\nperspectives. Specifically, DKFND first identifies the knowledge concepts of\neach news article through a Detection Module. Subsequently, DKFND creatively\ndesigns an Investigation Module to retrieve inside and outside valuable\ninformation concerning to the current news, followed by another Judge Module to\nevaluate the relevance and confidence of them. Finally, a Determination Module\nfurther derives two respective predictions and obtain the final result.\nExtensive experiments on two public datasets show the efficacy of our proposed\nmethod, particularly in low-resource settings.", "published": "2024-07-12 03:15:01", "link": "http://arxiv.org/abs/2407.08952v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Empowering Few-Shot Relation Extraction with The Integration of\n  Traditional RE Methods and Large Language Models", "abstract": "Few-Shot Relation Extraction (FSRE), a subtask of Relation Extraction (RE)\nthat utilizes limited training instances, appeals to more researchers in\nNatural Language Processing (NLP) due to its capability to extract textual\ninformation in extremely low-resource scenarios. The primary methodologies\nemployed for FSRE have been fine-tuning or prompt tuning techniques based on\nPre-trained Language Models (PLMs). Recently, the emergence of Large Language\nModels (LLMs) has prompted numerous researchers to explore FSRE through\nIn-Context Learning (ICL). However, there are substantial limitations\nassociated with methods based on either traditional RE models or LLMs.\nTraditional RE models are hampered by a lack of necessary prior knowledge,\nwhile LLMs fall short in their task-specific capabilities for RE. To address\nthese shortcomings, we propose a Dual-System Augmented Relation Extractor\n(DSARE), which synergistically combines traditional RE models with LLMs.\nSpecifically, DSARE innovatively injects the prior knowledge of LLMs into\ntraditional RE models, and conversely enhances LLMs' task-specific aptitude for\nRE through relation extraction augmentation. Moreover, an Integrated Prediction\nmodule is employed to jointly consider these two respective predictions and\nderive the final results. Extensive experiments demonstrate the efficacy of our\nproposed method.", "published": "2024-07-12 03:31:11", "link": "http://arxiv.org/abs/2407.08967v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Chapter-to-Chapter Context-Aware Literary Translation via Large\n  Language Models", "abstract": "Discourse phenomena in existing document-level translation datasets are\nsparse, which has been a fundamental obstacle in the development of\ncontext-aware machine translation models. Moreover, most existing\ndocument-level corpora and context-aware machine translation methods rely on an\nunrealistic assumption on sentence-level alignments. To mitigate these issues,\nwe first curate a novel dataset of Chinese-English literature, which consists\nof 160 books with intricate discourse structures. Then, we propose a more\npragmatic and challenging setting for context-aware translation, termed\nchapter-to-chapter (Ch2Ch) translation, and investigate the performance of\ncommonly-used machine translation models under this setting. Furthermore, we\nintroduce a potential approach of finetuning large language models (LLMs)\nwithin the domain of Ch2Ch literary translation, yielding impressive\nimprovements over baselines. Through our comprehensive analysis, we unveil that\nliterary translation under the Ch2Ch setting is challenging in nature, with\nrespect to both model learning methods and translation decoding algorithms.", "published": "2024-07-12 04:18:22", "link": "http://arxiv.org/abs/2407.08978v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Robustness of LLMs to Perturbations in Text", "abstract": "Having a clean dataset has been the foundational assumption of most natural\nlanguage processing (NLP) systems. However, properly written text is rarely\nfound in real-world scenarios and hence, oftentimes invalidates the\naforementioned foundational assumption. Recently, Large language models (LLMs)\nhave shown impressive performance, but can they handle the inevitable noise in\nreal-world data? This work tackles this critical question by investigating\nLLMs' resilience against morphological variations in text. To that end, we\nartificially introduce varying levels of noise into a diverse set of datasets\nand systematically evaluate LLMs' robustness against the corrupt variations of\nthe original text. Our findings show that contrary to popular beliefs,\ngenerative LLMs are quiet robust to noisy perturbations in text. This is a\ndeparture from pre-trained models like BERT or RoBERTa whose performance has\nbeen shown to be sensitive to deteriorating noisy text. Additionally, we test\nLLMs' resilience on multiple real-world benchmarks that closely mimic commonly\nfound errors in the wild. With minimal prompting, LLMs achieve a new\nstate-of-the-art on the benchmark tasks of Grammar Error Correction (GEC) and\nLexical Semantic Change (LSC). To empower future research, we also release a\ndataset annotated by humans stating their preference for LLM vs.\nhuman-corrected outputs along with the code to reproduce our results.", "published": "2024-07-12 04:50:17", "link": "http://arxiv.org/abs/2407.08989v1", "categories": ["cs.CL", "cs.AI", "I.7; I.2.7; I.2.4"], "primary_category": "cs.CL"}
{"title": "Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled\n  Refusal Training", "abstract": "This study addresses a critical gap in safety tuning practices for Large\nLanguage Models (LLMs) by identifying and tackling a refusal position bias\nwithin safety tuning data, which compromises the models' ability to\nappropriately refuse generating unsafe content. We introduce a novel approach,\nDecoupled Refusal Training (DeRTa), designed to empower LLMs to refuse\ncompliance to harmful prompts at any response position, significantly enhancing\ntheir safety capabilities. DeRTa incorporates two novel components: (1) Maximum\nLikelihood Estimation (MLE) with Harmful Response Prefix, which trains models\nto recognize and avoid unsafe content by appending a segment of harmful\nresponse to the beginning of a safe response, and (2) Reinforced Transition\nOptimization (RTO), which equips models with the ability to transition from\npotential harm to safety refusal consistently throughout the harmful response\nsequence. Our empirical evaluation, conducted using LLaMA3 and Mistral model\nfamilies across six attack scenarios, demonstrates that our method not only\nimproves model safety without compromising performance but also surpasses\nwell-known models such as GPT-4 in defending against attacks. Importantly, our\napproach successfully defends recent advanced attack methods (e.g., CodeAttack)\nthat have jailbroken GPT-4 and LLaMA3-70B-Instruct. Our code and data can be\nfound at https://github.com/RobustNLP/DeRTa.", "published": "2024-07-12 09:36:33", "link": "http://arxiv.org/abs/2407.09121v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Look Into News Avoidance Through AWRS: An Avoidance-Aware Recommender\n  System", "abstract": "In recent years, journalists have expressed concerns about the increasing\ntrend of news article avoidance, especially within specific domains. This issue\nhas been exacerbated by the rise of recommender systems. Our research indicates\nthat recommender systems should consider avoidance as a fundamental factor. We\nargue that news articles can be characterized by three principal elements:\nexposure, relevance, and avoidance, all of which are closely interconnected. To\naddress these challenges, we introduce AWRS, an Avoidance-Aware Recommender\nSystem. This framework incorporates avoidance awareness when recommending news,\nbased on the premise that news article avoidance conveys significant\ninformation about user preferences. Evaluation results on three news datasets\nin different languages (English, Norwegian, and Japanese) demonstrate that our\nmethod outperforms existing approaches.", "published": "2024-07-12 10:16:03", "link": "http://arxiv.org/abs/2407.09137v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "The Two Sides of the Coin: Hallucination Generation and Detection with\n  LLMs as Evaluators for LLMs", "abstract": "Hallucination detection in Large Language Models (LLMs) is crucial for\nensuring their reliability. This work presents our participation in the CLEF\nELOQUENT HalluciGen shared task, where the goal is to develop evaluators for\nboth generating and detecting hallucinated content. We explored the\ncapabilities of four LLMs: Llama 3, Gemma, GPT-3.5 Turbo, and GPT-4, for this\npurpose. We also employed ensemble majority voting to incorporate all four\nmodels for the detection task. The results provide valuable insights into the\nstrengths and weaknesses of these LLMs in handling hallucination generation and\ndetection tasks.", "published": "2024-07-12 10:34:46", "link": "http://arxiv.org/abs/2407.09152v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Enhancing Depressive Post Detection in Bangla: A Comparative Study of\n  TF-IDF, BERT and FastText Embeddings", "abstract": "Due to massive adoption of social media, detection of users' depression\nthrough social media analytics bears significant importance, particularly for\nunderrepresented languages, such as Bangla. This study introduces a\nwell-grounded approach to identify depressive social media posts in Bangla, by\nemploying advanced natural language processing techniques. The dataset used in\nthis work, annotated by domain experts, includes both depressive and\nnon-depressive posts, ensuring high-quality data for model training and\nevaluation. To address the prevalent issue of class imbalance, we utilised\nrandom oversampling for the minority class, thereby enhancing the model's\nability to accurately detect depressive posts. We explored various numerical\nrepresentation techniques, including Term Frequency-Inverse Document Frequency\n(TF-IDF), Bidirectional Encoder Representations from Transformers (BERT)\nembedding and FastText embedding, by integrating them with a deep\nlearning-based Convolutional Neural Network-Bidirectional Long Short-Term\nMemory (CNN-BiLSTM) model. The results obtained through extensive\nexperimentation, indicate that the BERT approach performed better the others,\nachieving a F1-score of 84%. This indicates that BERT, in combination with the\nCNN-BiLSTM architecture, effectively recognises the nuances of Bangla texts\nrelevant to depressive contents. Comparative analysis with the existing\nstate-of-the-art methods demonstrates that our approach with BERT embedding\nperforms better than others in terms of evaluation metrics and the reliability\nof dataset annotations. Our research significantly contribution to the\ndevelopment of reliable tools for detecting depressive posts in the Bangla\nlanguage. By highlighting the efficacy of different embedding techniques and\ndeep learning models, this study paves the way for improved mental health\nmonitoring through social media platforms.", "published": "2024-07-12 11:40:17", "link": "http://arxiv.org/abs/2407.09187v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Chatbot for Asylum-Seeking Migrants in Europe", "abstract": "We present ACME: A Chatbot for asylum-seeking Migrants in Europe. ACME relies\non computational argumentation and aims to help migrants identify the highest\nlevel of protection they can apply for. This would contribute to a more\nsustainable migration by reducing the load on territorial commissions, Courts,\nand humanitarian organizations supporting asylum applicants. We describe the\nbackground context, system architecture, underlying technologies, and a case\nstudy used to validate the tool with domain experts.", "published": "2024-07-12 11:53:40", "link": "http://arxiv.org/abs/2407.09197v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Pronunciation Assessment with Multi-modal Large Language Models", "abstract": "Large language models (LLMs), renowned for their powerful conversational\nabilities, are widely recognized as exceptional tools in the field of\neducation, particularly in the context of automated intelligent instruction\nsystems for language learning. In this paper, we propose a scoring system based\non LLMs, motivated by their positive impact on text-related scoring tasks.\nSpecifically, the speech encoder first maps the learner's speech into\ncontextual features. The adapter layer then transforms these features to align\nwith the text embedding in latent space. The assessment task-specific prefix\nand prompt text are embedded and concatenated with the features generated by\nthe modality adapter layer, enabling the LLMs to predict accuracy and fluency\nscores. Our experiments demonstrate that the proposed scoring systems achieve\ncompetitive results compared to the baselines on the Speechocean762 datasets.\nMoreover, we also conducted an ablation study to better understand the\ncontributions of the prompt text and training strategy in the proposed scoring\nsystem.", "published": "2024-07-12 12:16:14", "link": "http://arxiv.org/abs/2407.09209v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Context Embeddings for Efficient Answer Generation in RAG", "abstract": "Retrieval-Augmented Generation (RAG) allows overcoming the limited knowledge\nof LLMs by extending the input with external information. As a consequence, the\ncontextual inputs to the model become much longer which slows down decoding\ntime directly translating to the time a user has to wait for an answer. We\naddress this challenge by presenting COCOM, an effective context compression\nmethod, reducing long contexts to only a handful of Context Embeddings speeding\nup the generation time by a large margin. Our method allows for different\ncompression rates trading off decoding time for answer quality. Compared to\nearlier methods, COCOM allows for handling multiple contexts more effectively,\nsignificantly reducing decoding time for long inputs. Our method demonstrates a\nspeed-up of up to 5.69 $\\times$ while achieving higher performance compared to\nexisting efficient context compression methods.", "published": "2024-07-12 13:30:44", "link": "http://arxiv.org/abs/2407.09252v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "H2O-Danube3 Technical Report", "abstract": "We present H2O-Danube3, a series of small language models consisting of\nH2O-Danube3-4B, trained on 6T tokens and H2O-Danube3-500M, trained on 4T\ntokens. Our models are pre-trained on high quality Web data consisting of\nprimarily English tokens in three stages with different data mixes before final\nsupervised tuning for chat version. The models exhibit highly competitive\nmetrics across a multitude of academic, chat, and fine-tuning benchmarks.\nThanks to its compact architecture, H2O-Danube3 can be efficiently run on a\nmodern smartphone, enabling local inference and rapid processing capabilities\neven on mobile devices. We make all models openly available under Apache 2.0\nlicense further democratizing LLMs to a wider audience economically.", "published": "2024-07-12 14:09:40", "link": "http://arxiv.org/abs/2407.09276v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DAHRS: Divergence-Aware Hallucination-Remediated SRL Projection", "abstract": "Semantic role labeling (SRL) enriches many downstream applications, e.g.,\nmachine translation, question answering, summarization, and stance/belief\ndetection. However, building multilingual SRL models is challenging due to the\nscarcity of semantically annotated corpora for multiple languages. Moreover,\nstate-of-the-art SRL projection (XSRL) based on large language models (LLMs)\nyields output that is riddled with spurious role labels. Remediation of such\nhallucinations is not straightforward due to the lack of explainability of\nLLMs. We show that hallucinated role labels are related to naturally occurring\ndivergence types that interfere with initial alignments. We implement\nDivergence-Aware Hallucination-Remediated SRL projection (DAHRS), leveraging\nlinguistically-informed alignment remediation followed by greedy First-Come\nFirst-Assign (FCFA) SRL projection. DAHRS improves the accuracy of SRL\nprojection without additional transformer-based machinery, beating XSRL in both\nhuman and automatic comparisons, and advancing beyond headwords to accommodate\nphrase-level SRL projection (e.g., EN-FR, EN-ES). Using CoNLL-2009 as our\nground truth, we achieve a higher word-level F1 over XSRL: 87.6% vs. 77.3%\n(EN-FR) and 89.0% vs. 82.7% (EN-ES). Human phrase-level assessments yield 89.1%\n(EN-FR) and 91.0% (EN-ES). We also define a divergence metric to adapt our\napproach to other language pairs (e.g., English-Tagalog).", "published": "2024-07-12 14:13:59", "link": "http://arxiv.org/abs/2407.09283v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sina at FigNews 2024: Multilingual Datasets Annotated with Bias and\n  Propaganda", "abstract": "The proliferation of bias and propaganda on social media is an increasingly\nsignificant concern, leading to the development of techniques for automatic\ndetection. This article presents a multilingual corpus of 12, 000 Facebook\nposts fully annotated for bias and propaganda. The corpus was created as part\nof the FigNews 2024 Shared Task on News Media Narratives for framing the\nIsraeli War on Gaza. It covers various events during the War from October 7,\n2023 to January 31, 2024. The corpus comprises 12, 000 posts in five languages\n(Arabic, Hebrew, English, French, and Hindi), with 2, 400 posts for each\nlanguage. The annotation process involved 10 graduate students specializing in\nLaw. The Inter-Annotator Agreement (IAA) was used to evaluate the annotations\nof the corpus, with an average IAA of 80.8% for bias and 70.15% for propaganda\nannotations. Our team was ranked among the bestperforming teams in both Bias\nand Propaganda subtasks. The corpus is open-source and available at\nhttps://sina.birzeit.edu/fada", "published": "2024-07-12 15:04:09", "link": "http://arxiv.org/abs/2407.09327v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Mitigating Entity-Level Hallucination in Large Language Models", "abstract": "The emergence of Large Language Models (LLMs) has revolutionized how users\naccess information, shifting from traditional search engines to direct\nquestion-and-answer interactions with LLMs. However, the widespread adoption of\nLLMs has revealed a significant challenge known as hallucination, wherein LLMs\ngenerate coherent yet factually inaccurate responses. This hallucination\nphenomenon has led to users' distrust in information retrieval systems based on\nLLMs. To tackle this challenge, this paper proposes Dynamic Retrieval\nAugmentation based on hallucination Detection (DRAD) as a novel method to\ndetect and mitigate hallucinations in LLMs. DRAD improves upon traditional\nretrieval augmentation by dynamically adapting the retrieval process based on\nreal-time hallucination detection. It features two main components: Real-time\nHallucination Detection (RHD) for identifying potential hallucinations without\nexternal models, and Self-correction based on External Knowledge (SEK) for\ncorrecting these errors using external knowledge. Experiment results show that\nDRAD demonstrates superior performance in both detecting and mitigating\nhallucinations in LLMs. All of our code and data are open-sourced at\nhttps://github.com/oneal2000/EntityHallucination.", "published": "2024-07-12 16:47:34", "link": "http://arxiv.org/abs/2407.09417v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Diversifying the Expert Knowledge for Task-Agnostic Pruning in Sparse\n  Mixture-of-Experts", "abstract": "By increasing model parameters but activating them sparsely when performing a\ntask, the use of Mixture-of-Experts (MoE) architecture significantly improves\nthe performance of Large Language Models (LLMs) without increasing the\ninference cost. However, the memory consumption due to the growing number of\nexperts presents a challenge to the deployment of these models in many real\nworld settings. Our empirical study reveals that some experts encode redundant\nknowledge during pre-training. We thus propose a method of grouping and pruning\nsimilar experts to improve the model's parameter efficiency. We validate the\neffectiveness of our method by pruning three state-of-the-art MoE\narchitectures, including Mixtral, Deepseek-MoE, and Qwen. The evaluation shows\nthat our method outperforms other model pruning methods on a range of natural\nlanguage tasks. We will release our code to facilitate future research.", "published": "2024-07-12 17:25:02", "link": "http://arxiv.org/abs/2407.09590v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bridging the Gap Between Information Seeking and Product Search Systems:\n  Q&A Recommendation for E-commerce", "abstract": "Consumers on a shopping mission often leverage both product search and\ninformation seeking systems, such as web search engines and Question Answering\n(QA) systems, in an iterative process to improve their understanding of\navailable products and reach a purchase decision. While product search is\nuseful for shoppers to find the actual products meeting their requirements in\nthe catalog, information seeking systems can be utilized to answer any\nquestions they may have to refine those requirements. The recent success of\nLarge Language Models (LLMs) has opened up an opportunity to bridge the gap\nbetween the two tasks to help customers achieve their goals quickly and\neffectively by integrating conversational QA within product search. In this\npaper, we propose to recommend users Question-Answer (Q&A) pairs that are\nrelevant to their product search and can help them make a purchase decision. We\ndiscuss the different aspects of the problem including the requirements and\ncharacteristics of the Q&A pairs, their generation, and the optimization of the\nQ&A recommendation task. We highlight the challenges, open problems, and\nsuggested solutions to encourage future research in this emerging area.", "published": "2024-07-12 19:22:17", "link": "http://arxiv.org/abs/2407.09653v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Bridging Dictionary: AI-Generated Dictionary of Partisan Language Use", "abstract": "Words often carry different meanings for people from diverse backgrounds.\nToday's era of social polarization demands that we choose words carefully to\nprevent miscommunication, especially in political communication and journalism.\nTo address this issue, we introduce the Bridging Dictionary, an interactive\ntool designed to illuminate how words are perceived by people with different\npolitical views. The Bridging Dictionary includes a static, printable document\nfeaturing 796 terms with summaries generated by a large language model. These\nsummaries highlight how the terms are used distinctively by Republicans and\nDemocrats. Additionally, the Bridging Dictionary offers an interactive\ninterface that lets users explore selected words, visualizing their frequency,\nsentiment, summaries, and examples across political divides. We present a use\ncase for journalists and emphasize the importance of human agency and trust in\nfurther enhancing this tool. The deployed version of Bridging Dictionary is\navailable at https://dictionary.ccc-mit.org/.", "published": "2024-07-12 19:44:40", "link": "http://arxiv.org/abs/2407.09661v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "GOFA: A Generative One-For-All Model for Joint Graph Language Modeling", "abstract": "Foundation models, such as Large Language Models (LLMs) or Large Vision\nModels (LVMs), have emerged as one of the most powerful tools in the respective\nfields. However, unlike text and image data, graph data do not have a\ndefinitive structure, posing great challenges to developing a Graph Foundation\nModel (GFM). For example, current attempts at designing general graph models\neither transform graph data into a language format for LLM-based prediction or\nstill train a GNN model with LLM as an assistant. The former can handle\nunlimited tasks, while the latter captures graph structure much better -- yet,\nno existing work can achieve both simultaneously. In this paper, we identify\nthree key desirable properties of a GFM: self-supervised pretraining, fluidity\nin tasks, and graph awareness. To account for these properties, we extend the\nconventional language modeling to the graph domain and propose a novel\ngenerative graph language model GOFA to solve the problem. The model\ninterleaves randomly initialized GNN layers into a frozen pre-trained LLM so\nthat the semantic and structural modeling abilities are organically combined.\nGOFA is pre-trained on newly proposed graph-level next-word prediction,\nquestion-answering, and structural tasks to obtain the above GFM properties.\nThe pre-trained model is further fine-tuned on downstream tasks to obtain\ntask-solving ability. The fine-tuned model is evaluated on various downstream\ntasks, demonstrating a strong ability to solve structural and contextual\nproblems in zero-shot scenarios. The code is available at\nhttps://github.com/JiaruiFeng/GOFA.", "published": "2024-07-12 22:23:51", "link": "http://arxiv.org/abs/2407.09709v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Optimized Multi-Token Joint Decoding with Auxiliary Model for LLM\n  Inference", "abstract": "Large language models (LLMs) have achieved remarkable success across diverse\ntasks, yet their inference processes are hindered by substantial time and\nenergy demands due to single-token generation at each decoding step. While\nprevious methods such as speculative decoding mitigate these inefficiencies by\nproducing multiple tokens per step, each token is still generated by its\nsingle-token distribution, thereby enhancing speed without improving\neffectiveness. In contrast, our work simultaneously enhances inference speed\nand improves the output effectiveness. We consider multi-token joint decoding\n(MTJD), which generates multiple tokens from their joint distribution at each\niteration, theoretically reducing perplexity and enhancing task performance.\nHowever, MTJD suffers from the high cost of sampling from the joint\ndistribution of multiple tokens. Inspired by speculative decoding, we introduce\nmulti-token assisted decoding (MTAD), a novel framework designed to accelerate\nMTJD. MTAD leverages a smaller auxiliary model to approximate the joint\ndistribution of a larger model, incorporating a verification mechanism that not\nonly ensures the accuracy of this approximation, but also improves the decoding\nefficiency over conventional speculative decoding. Theoretically, we\ndemonstrate that MTAD closely approximates exact MTJD with bounded error.\nEmpirical evaluations using Llama-2 and OPT models ranging from 13B to 70B\nparameters across various tasks reveal that MTAD reduces perplexity by 21.2%\nand improves downstream performance compared to standard single-token sampling.\nFurthermore, MTAD achieves a 1.42x speed-up and consumes 1.54x less energy than\nconventional speculative decoding methods. These results highlight MTAD's\nability to make multi-token joint decoding both effective and efficient,\npromoting more sustainable and high-performance deployment of LLMs.", "published": "2024-07-12 23:29:54", "link": "http://arxiv.org/abs/2407.09722v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Show, Don't Tell: Evaluating Large Language Models Beyond Textual\n  Understanding with ChildPlay", "abstract": "We developed a benchmark set to assess the generalization of state-of-the-art\nlarge language models on problems beyond linguistic tasks and evaluate it on a\nsystematic progression of GPT models (GPT-3.5, GPT-4, GPT-4o, GPT-4o-mini).\nUsing simple games like Tic-Tac-Toe, Connect Four, Battleship, and a Shape\nRecognition Game, all encoded in ASCII, we test strategic capabilities and\nspatial reasoning, core abilities any artificial intelligence would need to\nmaster for solving problems in chemistry. To probe generalization, we introduce\ntwo new games for spatial logic: LEGO Connect Language (LCL) and\nGuess-the-SMILES (GtS), a operationally simple chemistry benchmark. Our results\nshow that GPT models provide meaningful responses for several tasks but,\ngenerally, perform poorly. A systematic performance progression with increased\nmodel capabilities (GPT-3.5, GPT-4, GPT-4o) is only observed for 4 out of the 7\nbenchmark tasks. All models consistently struggle with Battleship, LCL, and\nGtS. This suggests that while GPT models can emulate conversational proficiency\nand basic rule comprehension, they have limited generalization with respect to\nstrategy and spatial reasoning. Particularly poor performance is observed for\ninterpreting molecular graphs when encoded in ASCII. The results provided by\nour open-source benchmark suite\n(\\href{https://github.com/BlueVelvetSackOfGoldPotatoes/child-play}{\\texttt{ChildPlay}\nGitHub Repository}) caution against claims of emergent intelligence in GPT\nmodels, which appear more specialized than general.", "published": "2024-07-12 14:17:26", "link": "http://arxiv.org/abs/2407.11068v5", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Token-Supervised Value Models for Enhancing Mathematical Problem-Solving\n  Capabilities of Large Language Models", "abstract": "With the rapid advancement of test-time compute search strategies to improve\nthe mathematical problem-solving capabilities of large language models (LLMs),\nthe need for building robust verifiers has become increasingly important.\nHowever, all these inference strategies rely on existing verifiers originally\ndesigned for Best-of-N search, which makes them sub-optimal for tree search\ntechniques at test time. During tree search, existing verifiers can only offer\nindirect and implicit assessments of partial solutions or under-value\nprospective intermediate steps, thus resulting in the premature pruning of\npromising intermediate steps. To overcome these limitations, we propose\ntoken-supervised value models (TVMs) - a new class of verifiers that assign\neach token a probability that reflects the likelihood of reaching the correct\nfinal answer. This new token-level supervision enables TVMs to directly and\nexplicitly evaluate partial solutions, effectively distinguishing between\npromising and incorrect intermediate steps during tree search at test time.\nExperimental results demonstrate that combining tree-search-based inference\nstrategies with TVMs significantly improves the accuracy of LLMs in\nmathematical problem-solving tasks, surpassing the performance of existing\nverifiers.", "published": "2024-07-12 13:16:50", "link": "http://arxiv.org/abs/2407.12863v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GRAD-SUM: Leveraging Gradient Summarization for Optimal Prompt\n  Engineering", "abstract": "Prompt engineering for large language models (LLMs) is often a manual\ntime-intensive process that involves generating, evaluating, and refining\nprompts iteratively to ensure high-quality outputs. While there has been work\non automating prompt engineering, the solutions generally are either tuned to\nspecific tasks with given answers or are quite costly. We introduce GRAD-SUM, a\nscalable and flexible method for automatic prompt engineering that builds on\ngradient-based optimization techniques. Our approach incorporates user-defined\ntask descriptions and evaluation criteria, and features a novel gradient\nsummarization module to generalize feedback effectively. Our results\ndemonstrate that GRAD-SUM consistently outperforms existing methods across\nvarious benchmarks, highlighting its versatility and effectiveness in automatic\nprompt optimization.", "published": "2024-07-12 19:11:21", "link": "http://arxiv.org/abs/2407.12865v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Honorific Effect: Exploring the Impact of Japanese Linguistic\n  Formalities on AI-Generated Physics Explanations", "abstract": "This study investigates the influence of Japanese honorifics on the responses\nof large language models (LLMs) when explaining the law of conservation of\nmomentum. We analyzed the outputs of six state-of-the-art AI models, including\nvariations of ChatGPT, Coral, and Gemini, using 14 different honorific forms.\nOur findings reveal that honorifics significantly affect the quality,\nconsistency, and formality of AI-generated responses, demonstrating LLMs'\nability to interpret and adapt to social context cues embedded in language.\nNotable variations were observed across different models, with some emphasizing\nhistorical context and derivations, while others focused on intuitive\nexplanations. The study highlights the potential for using honorifics to adjust\nthe depth and complexity of AI-generated explanations in educational contexts.\nFurthermore, the responsiveness of AI models to cultural linguistic elements\nunderscores the importance of considering cultural factors in AI development\nfor educational applications. These results open new avenues for research in\nAI-assisted education and cultural adaptation in AI systems, with significant\nimplications for personalizing learning experiences and developing culturally\nsensitive AI tools for global education.", "published": "2024-07-12 11:31:00", "link": "http://arxiv.org/abs/2407.13787v2", "categories": ["physics.ed-ph", "cs.CL"], "primary_category": "physics.ed-ph"}
{"title": "A Survey on Symbolic Knowledge Distillation of Large Language Models", "abstract": "This survey paper delves into the emerging and critical area of symbolic\nknowledge distillation in Large Language Models (LLMs). As LLMs like Generative\nPre-trained Transformer-3 (GPT-3) and Bidirectional Encoder Representations\nfrom Transformers (BERT) continue to expand in scale and complexity, the\nchallenge of effectively harnessing their extensive knowledge becomes\nparamount. This survey concentrates on the process of distilling the intricate,\noften implicit knowledge contained within these models into a more symbolic,\nexplicit form. This transformation is crucial for enhancing the\ninterpretability, efficiency, and applicability of LLMs. We categorize the\nexisting research based on methodologies and applications, focusing on how\nsymbolic knowledge distillation can be used to improve the transparency and\nfunctionality of smaller, more efficient Artificial Intelligence (AI) models.\nThe survey discusses the core challenges, including maintaining the depth of\nknowledge in a comprehensible format, and explores the various approaches and\ntechniques that have been developed in this field. We identify gaps in current\nresearch and potential opportunities for future advancements. This survey aims\nto provide a comprehensive overview of symbolic knowledge distillation in LLMs,\nspotlighting its significance in the progression towards more accessible and\nefficient AI systems.", "published": "2024-07-12 12:18:19", "link": "http://arxiv.org/abs/2408.10210v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "IDAT: A Multi-Modal Dataset and Toolkit for Building and Evaluating\n  Interactive Task-Solving Agents", "abstract": "Seamless interaction between AI agents and humans using natural language\nremains a key goal in AI research. This paper addresses the challenges of\ndeveloping interactive agents capable of understanding and executing grounded\nnatural language instructions through the IGLU competition at NeurIPS. Despite\nadvancements, challenges such as a scarcity of appropriate datasets and the\nneed for effective evaluation platforms persist. We introduce a scalable data\ncollection tool for gathering interactive grounded language instructions within\na Minecraft-like environment, resulting in a Multi-Modal dataset with around\n9,000 utterances and over 1,000 clarification questions. Additionally, we\npresent a Human-in-the-Loop interactive evaluation platform for qualitative\nanalysis and comparison of agent performance through multi-turn communication\nwith human annotators. We offer to the community these assets referred to as\nIDAT (IGLU Dataset And Toolkit) which aim to advance the development of\nintelligent, interactive AI agents and provide essential resources for further\nresearch.", "published": "2024-07-12 00:07:43", "link": "http://arxiv.org/abs/2407.08898v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "One Stone, Four Birds: A Comprehensive Solution for QA System Using\n  Supervised Contrastive Learning", "abstract": "This paper presents a novel and comprehensive solution to enhance both the\nrobustness and efficiency of question answering (QA) systems through supervised\ncontrastive learning (SCL). Training a high-performance QA system has become\nstraightforward with pre-trained language models, requiring only a small amount\nof data and simple fine-tuning. However, despite recent advances, existing QA\nsystems still exhibit significant deficiencies in functionality and training\nefficiency. We address the functionality issue by defining four key tasks: user\ninput intent classification, out-of-domain input detection, new intent\ndiscovery, and continual learning. We then leverage a unified SCL-based\nrepresentation learning method to efficiently build an intra-class compact and\ninter-class scattered feature space, facilitating both known intent\nclassification and unknown intent detection and discovery. Consequently, with\nminimal additional tuning on downstream tasks, our approach significantly\nimproves model efficiency and achieves new state-of-the-art performance across\nall tasks.", "published": "2024-07-12 06:01:51", "link": "http://arxiv.org/abs/2407.09011v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "URRL-IMVC: Unified and Robust Representation Learning for Incomplete\n  Multi-View Clustering", "abstract": "Incomplete multi-view clustering (IMVC) aims to cluster multi-view data that\nare only partially available. This poses two main challenges: effectively\nleveraging multi-view information and mitigating the impact of missing views.\nPrevailing solutions employ cross-view contrastive learning and missing view\nrecovery techniques. However, they either neglect valuable complementary\ninformation by focusing only on consensus between views or provide unreliable\nrecovered views due to the absence of supervision. To address these\nlimitations, we propose a novel Unified and Robust Representation Learning for\nIncomplete Multi-View Clustering (URRL-IMVC). URRL-IMVC directly learns a\nunified embedding that is robust to view missing conditions by integrating\ninformation from multiple views and neighboring samples. Firstly, to overcome\nthe limitations of cross-view contrastive learning, URRL-IMVC incorporates an\nattention-based auto-encoder framework to fuse multi-view information and\ngenerate unified embeddings. Secondly, URRL-IMVC directly enhances the\nrobustness of the unified embedding against view-missing conditions through KNN\nimputation and data augmentation techniques, eliminating the need for explicit\nmissing view recovery. Finally, incremental improvements are introduced to\nfurther enhance the overall performance, such as the Clustering Module and the\ncustomization of the Encoder. We extensively evaluate the proposed URRL-IMVC\nframework on various benchmark datasets, demonstrating its state-of-the-art\nperformance. Furthermore, comprehensive ablation studies are performed to\nvalidate the effectiveness of our design.", "published": "2024-07-12 09:35:25", "link": "http://arxiv.org/abs/2407.09120v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Stepwise Verification and Remediation of Student Reasoning Errors with\n  Large Language Model Tutors", "abstract": "Large language models (LLMs) present an opportunity to scale high-quality\npersonalized education to all. A promising approach towards this means is to\nbuild dialog tutoring models that scaffold students' problem-solving. However,\neven though existing LLMs perform well in solving reasoning questions, they\nstruggle to precisely detect student's errors and tailor their feedback to\nthese errors. Inspired by real-world teaching practice where teachers identify\nstudent errors and customize their response based on them, we focus on\nverifying student solutions and show how grounding to such verification\nimproves the overall quality of tutor response generation. We collect a dataset\nof 1K stepwise math reasoning chains with the first error step annotated by\nteachers. We show empirically that finding the mistake in a student solution is\nchallenging for current models. We propose and evaluate several verifiers for\ndetecting these errors. Using both automatic and human evaluation we show that\nthe student solution verifiers steer the generation model towards highly\ntargeted responses to student errors which are more often correct with less\nhallucinations compared to existing baselines.", "published": "2024-07-12 10:11:40", "link": "http://arxiv.org/abs/2407.09136v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deep Bag-of-Words Model: An Efficient and Interpretable Relevance\n  Architecture for Chinese E-Commerce", "abstract": "Text relevance or text matching of query and product is an essential\ntechnique for the e-commerce search system to ensure that the displayed\nproducts can match the intent of the query. Many studies focus on improving the\nperformance of the relevance model in search system. Recently, pre-trained\nlanguage models like BERT have achieved promising performance on the text\nrelevance task. While these models perform well on the offline test dataset,\nthere are still obstacles to deploy the pre-trained language model to the\nonline system as their high latency. The two-tower model is extensively\nemployed in industrial scenarios, owing to its ability to harmonize performance\nwith computational efficiency. Regrettably, such models present an opaque\n``black box'' nature, which prevents developers from making special\noptimizations. In this paper, we raise deep Bag-of-Words (DeepBoW) model, an\nefficient and interpretable relevance architecture for Chinese e-commerce. Our\napproach proposes to encode the query and the product into the sparse BoW\nrepresentation, which is a set of word-weight pairs. The weight means the\nimportant or the relevant score between the corresponding word and the raw\ntext. The relevance score is measured by the accumulation of the matched word\nbetween the sparse BoW representation of the query and the product. Compared to\npopular dense distributed representation that usually suffers from the drawback\nof black-box, the most advantage of the proposed representation model is highly\nexplainable and interventionable, which is a superior advantage to the\ndeployment and operation of online search engines. Moreover, the online\nefficiency of the proposed model is even better than the most efficient inner\nproduct form of dense representation ...", "published": "2024-07-12 16:18:05", "link": "http://arxiv.org/abs/2407.09395v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers", "abstract": "Seeking answers to questions within long scientific research articles is a\ncrucial area of study that aids readers in quickly addressing their inquiries.\nHowever, existing question-answering (QA) datasets based on scientific papers\nare limited in scale and focus solely on textual content. We introduce SPIQA\n(Scientific Paper Image Question Answering), the first large-scale QA dataset\nspecifically designed to interpret complex figures and tables within the\ncontext of scientific research articles across various domains of computer\nscience. Leveraging the breadth of expertise and ability of multimodal large\nlanguage models (MLLMs) to understand figures, we employ automatic and manual\ncuration to create the dataset. We craft an information-seeking task on\ninterleaved images and text that involves multiple images covering plots,\ncharts, tables, schematic diagrams, and result visualizations. SPIQA comprises\n270K questions divided into training, validation, and three different\nevaluation splits. Through extensive experiments with 12 prominent foundational\nmodels, we evaluate the ability of current multimodal systems to comprehend the\nnuanced aspects of research articles. Additionally, we propose a\nChain-of-Thought (CoT) evaluation strategy with in-context retrieval that\nallows fine-grained, step-by-step assessment and improves model performance. We\nfurther explore the upper bounds of performance enhancement with additional\ntextual information, highlighting its promising potential for future research\nand the dataset's impact on revolutionizing how we interact with scientific\nliterature.", "published": "2024-07-12 16:37:59", "link": "http://arxiv.org/abs/2407.09413v3", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Human-like Episodic Memory for Infinite Context LLMs", "abstract": "Large language models (LLMs) have shown remarkable capabilities, but still\nstruggle with processing extensive contexts, limiting their ability to maintain\ncoherence and accuracy over long sequences. In contrast, the human brain excels\nat organising and retrieving episodic experiences across vast temporal scales,\nspanning a lifetime. In this work, we introduce EM-LLM, a novel approach that\nintegrates key aspects of human episodic memory and event cognition into LLMs\nwith no fine-tuning, enabling them to handle practically infinite context\nlengths while maintaining computational efficiency. EM-LLM organises sequences\nof tokens into coherent episodic events using a combination of Bayesian\nsurprise and graph-theoretic boundary refinement in an online fashion. When\nneeded, these events are retrieved through a two-stage memory process,\ncombining similarity-based and temporally contiguous retrieval for efficient\nand human-like access to relevant information. Experiments on the LongBench and\nInfiniteBench benchmarks demonstrate EM-LLM's superior performance,\nconsistently outperforming the state-of-the-art retrieval model InfLLM across\nvarious baseline LLMs. In addition, EM-LLM outperforms its popular counterpart,\nRAG, in a wide range of tasks, while requiring similar resources. Notably,\nEM-LLM's performance even surpasses full-context models in most tasks, while\nsuccessfully performing retrieval across 10 million tokens - a scale\ncomputationally infeasible for such models. Finally, our analysis reveals\nstrong correlations between EM-LLM's event segmentation and human-perceived\nevents, suggesting a bridge between this artificial system and its biological\ncounterpart, thereby offering a novel computational framework for exploring\nhuman memory mechanisms.", "published": "2024-07-12 17:34:03", "link": "http://arxiv.org/abs/2407.09450v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "q-bio.NC"], "primary_category": "cs.AI"}
{"title": "Weight Block Sparsity: Training, Compilation, and AI Engine Accelerators", "abstract": "Nowadays, increasingly larger Deep Neural Networks (DNNs) are being\ndeveloped, trained, and utilized. These networks require significant\ncomputational resources, putting a strain on both advanced and limited devices.\nOur solution is to implement {\\em weight block sparsity}, which is a structured\nsparsity that is friendly to hardware. By zeroing certain sections of the\nconvolution and fully connected layers parameters of pre-trained DNN models, we\ncan efficiently speed up the DNN's inference process. This results in a smaller\nmemory footprint, faster communication, and fewer operations.\n  Our work presents a vertical system that allows for the training of\nconvolution and matrix multiplication weights to exploit 8x8 block sparsity on\na single GPU within a reasonable amount of time. Compilers recognize this\nsparsity and use it for both data compaction and computation splitting into\nthreads. Blocks like these take full advantage of both spatial and temporal\nlocality, paving the way for fast vector operations and memory reuse. By using\nthis system on a Resnet50 model, we were able to reduce the weight by half with\nminimal accuracy loss, resulting in a two-times faster inference speed. We will\npresent performance estimates using accurate and complete code generation for\nAIE2 configuration sets (AMD Versal FPGAs) with Resnet50, Inception V3, and\nVGG16 to demonstrate the necessary synergy between hardware overlay designs and\nsoftware stacks for compiling and executing machine learning applications.", "published": "2024-07-12 17:37:49", "link": "http://arxiv.org/abs/2407.09453v1", "categories": ["cs.LG", "cs.AR", "cs.CL", "C.5; D.3.4"], "primary_category": "cs.LG"}
{"title": "Is Contrasting All You Need? Contrastive Learning for the Detection and\n  Attribution of AI-generated Text", "abstract": "The significant progress in the development of Large Language Models has\ncontributed to blurring the distinction between human and AI-generated text.\nThe increasing pervasiveness of AI-generated text and the difficulty in\ndetecting it poses new challenges for our society. In this paper, we tackle the\nproblem of detecting and attributing AI-generated text by proposing WhosAI, a\ntriplet-network contrastive learning framework designed to predict whether a\ngiven input text has been generated by humans or AI and to unveil the\nauthorship of the text. Unlike most existing approaches, our proposed framework\nis conceived to learn semantic similarity representations from multiple\ngenerators at once, thus equally handling both detection and attribution tasks.\nFurthermore, WhosAI is model-agnostic and scalable to the release of new AI\ntext-generation models by incorporating their generated instances into the\nembedding space learned by our framework. Experimental results on the\nTuringBench benchmark of 200K news articles show that our proposed framework\nachieves outstanding results in both the Turing Test and Authorship Attribution\ntasks, outperforming all the methods listed in the TuringBench benchmark\nleaderboards.", "published": "2024-07-12 15:44:56", "link": "http://arxiv.org/abs/2407.09364v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Squeeze-and-Excite ResNet-Conformers for Sound Event Localization,\n  Detection, and Distance Estimation for DCASE 2024 Challenge", "abstract": "This technical report details our systems submitted for Task 3 of the DCASE\n2024 Challenge: Audio and Audiovisual Sound Event Localization and Detection\n(SELD) with Source Distance Estimation (SDE). We address only the audio-only\nSELD with SDE (SELDDE) task in this report. We propose to improve the existing\nResNet-Conformer architectures with Squeeze-and-Excitation blocks in order to\nintroduce additional forms of channel- and spatial-wise attention. In order to\nimprove SELD performance, we also utilize the Spatial Cue-Augmented\nLog-Spectrogram (SALSA) features over the commonly used log-mel spectra\nfeatures for polyphonic SELD. We complement the existing Sony-TAu Realistic\nSpatial Soundscapes 2023 (STARSS23) dataset with the audio channel swapping\ntechnique and synthesize additional data using the SpatialScaper generator. We\nalso perform distance scaling in order to prevent large distance errors from\ncontributing more towards the loss function. Finally, we evaluate our approach\non the evaluation subset of the STARSS23 dataset.", "published": "2024-07-12 06:23:21", "link": "http://arxiv.org/abs/2407.09021v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Audio Spotforming Using Nonnegative Tensor Factorization with\n  Attractor-Based Regularization", "abstract": "Spotforming is a target-speaker extraction technique that uses multiple\nmicrophone arrays. This method applies beamforming (BF) to each microphone\narray, and the common components among the BF outputs are estimated as the\ntarget source. This study proposes a new common component extraction method\nbased on nonnegative tensor factorization (NTF) for higher model\ninterpretability and more robust spotforming against hyperparameters. Moreover,\nattractor-based regularization was introduced to facilitate the automatic\nselection of optimal target bases in the NTF. Experimental results show that\nthe proposed method performs better than conventional methods in spotforming\nperformance and also shows some characteristics suitable for practical use.", "published": "2024-07-12 03:10:25", "link": "http://arxiv.org/abs/2407.08951v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Preliminary Investigation on Flexible Singing Voice Synthesis Through\n  Decomposed Framework with Inferrable Features", "abstract": "We investigate the feasibility of a singing voice synthesis (SVS) system by\nusing a decomposed framework to improve flexibility in generating singing\nvoices. Due to data-driven approaches, SVS performs a music score-to-waveform\nmapping; however, the direct mapping limits control, such as being able to only\nsynthesize in the language or the singers present in the labeled singing\ndatasets. As collecting large singing datasets labeled with music scores is an\nexpensive task, we investigate an alternative approach by decomposing the SVS\nsystem and inferring different singing voice features. We decompose the SVS\nsystem into three-stage modules of linguistic, pitch contour, and synthesis, in\nwhich singing voice features such as linguistic content, F0, voiced/unvoiced,\nsinger embeddings, and loudness are directly inferred from audio. Through this\ndecomposed framework, we show that we can alleviate the labeled dataset\nrequirements, adapt to different languages or singers, and inpaint the lyrical\ncontent of singing voices. Our investigations show that the framework has the\npotential to reach state-of-the-art in SVS, even though the model has\nadditional functionality and improved flexibility. The comprehensive analysis\nof our investigated framework's current capabilities sheds light on the ways\nthe research community can achieve a flexible and multifunctional SVS system.", "published": "2024-07-12 15:22:23", "link": "http://arxiv.org/abs/2407.09346v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Optimization of DNN-based speaker verification model through efficient\n  quantization technique", "abstract": "As Deep Neural Networks (DNNs) rapidly advance in various fields, including\nspeech verification, they typically involve high computational costs and\nsubstantial memory consumption, which can be challenging to manage on mobile\nsystems. Quantization of deep models offers a means to reduce both\ncomputational and memory expenses. Our research proposes an optimization\nframework for the quantization of the speaker verification model. By analyzing\nperformance changes and model size reductions in each layer of a pre-trained\nspeaker verification model, we have effectively minimized performance\ndegradation while significantly reducing the model size. Our quantization\nalgorithm is the first attempt to maintain the performance of the\nstate-of-the-art pre-trained speaker verification model, ECAPATDNN, while\nsignificantly compressing its model size. Overall, our quantization approach\nresulted in reducing the model size by half, with an increase in EER limited to\n0.07%.", "published": "2024-07-12 05:03:10", "link": "http://arxiv.org/abs/2407.08991v1", "categories": ["eess.AS", "cs.AI", "cs.CC"], "primary_category": "eess.AS"}
{"title": "Enhancing Emotion Recognition in Incomplete Data: A Novel Cross-Modal\n  Alignment, Reconstruction, and Refinement Framework", "abstract": "Multimodal emotion recognition systems rely heavily on the full availability\nof modalities, suffering significant performance declines when modal data is\nincomplete. To tackle this issue, we present the Cross-Modal Alignment,\nReconstruction, and Refinement (CM-ARR) framework, an innovative approach that\nsequentially engages in cross-modal alignment, reconstruction, and refinement\nphases to handle missing modalities and enhance emotion recognition. This\nframework utilizes unsupervised distribution-based contrastive learning to\nalign heterogeneous modal distributions, reducing discrepancies and modeling\nsemantic uncertainty effectively. The reconstruction phase applies normalizing\nflow models to transform these aligned distributions and recover missing\nmodalities. The refinement phase employs supervised point-based contrastive\nlearning to disrupt semantic correlations and accentuate emotional traits,\nthereby enriching the affective content of the reconstructed representations.\nExtensive experiments on the IEMOCAP and MSP-IMPROV datasets confirm the\nsuperior performance of CM-ARR under conditions of both missing and complete\nmodalities. Notably, averaged across six scenarios of missing modalities,\nCM-ARR achieves absolute improvements of 2.11% in WAR and 2.12% in UAR on the\nIEMOCAP dataset, and 1.71% and 1.96% in WAR and UAR, respectively, on the\nMSP-IMPROV dataset.", "published": "2024-07-12 06:44:42", "link": "http://arxiv.org/abs/2407.09029v1", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Music Proofreading with RefinPaint: Where and How to Modify Compositions\n  given Context", "abstract": "Autoregressive generative transformers are key in music generation, producing\ncoherent compositions but facing challenges in human-machine collaboration. We\npropose RefinPaint, an iterative technique that improves the sampling process.\nIt does this by identifying the weaker music elements using a feedback model,\nwhich then informs the choices for resampling by an inpainting model. This\ndual-focus methodology not only facilitates the machine's ability to improve\nits automatic inpainting generation through repeated cycles but also offers a\nvaluable tool for humans seeking to refine their compositions with automatic\nproofreading. Experimental results suggest RefinPaint's effectiveness in\ninpainting and proofreading tasks, demonstrating its value for refining music\ncreated by both machines and humans. This approach not only facilitates\ncreativity but also aids amateur composers in improving their work.", "published": "2024-07-12 08:52:27", "link": "http://arxiv.org/abs/2407.09099v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Informed FastICA: Semi-Blind Minimum Variance Distortionless Beamformer", "abstract": "Non-Gaussianity-based Independent Vector Extraction leads to the famous\none-unit FastICA/FastIVA algorithm when the likelihood function is optimized\nusing an approximate Newton-Raphson algorithm under the orthogonality\nconstraint. In this paper, we replace the constraint with the analytic form of\nthe minimum variance distortionless beamformer (MVDR), by which a semi-blind\nvariant of FastICA/FastIVA is obtained. The side information here is provided\nby a weighted covariance matrix replacing the noise covariance matrix, the\nestimation of which is a frequent goal of neural beamformers. The algorithm\nthus provides an intuitive connection between model-based blind extraction and\nlearning-based extraction. The algorithm is tested in simulations and speaker\nID-guided speaker extraction, showing fast convergence and promising\nperformance.", "published": "2024-07-12 13:41:57", "link": "http://arxiv.org/abs/2407.09259v1", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
