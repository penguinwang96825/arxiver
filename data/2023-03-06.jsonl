{"title": "Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning", "abstract": "Prompt tuning, in which a base pretrained model is adapted to each task via\nconditioning on learned prompt vectors, has emerged as a promising approach for\nefficiently adapting large language models to multiple downstream tasks.\nHowever, existing methods typically learn soft prompt vectors from scratch, and\nit has not been clear how to exploit the rich cross-task knowledge with prompt\nvectors in a multitask learning setting. We propose multitask prompt tuning\n(MPT), which first learns a single transferable prompt by distilling knowledge\nfrom multiple task-specific source prompts. We then learn multiplicative low\nrank updates to this shared prompt to efficiently adapt it to each downstream\ntarget task. Extensive experiments on 23 NLP datasets demonstrate that our\nproposed approach outperforms the state-of-the-art methods, including the full\nfinetuning baseline in some cases, despite only tuning 0.035% as many\ntask-specific parameters.", "published": "2023-03-06 03:25:59", "link": "http://arxiv.org/abs/2303.02861v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OpenICL: An Open-Source Framework for In-context Learning", "abstract": "In recent years, In-context Learning (ICL) has gained increasing attention\nand emerged as the new paradigm for large language model (LLM) evaluation.\nUnlike traditional fine-tuning methods, ICL instead adapts the pre-trained\nmodels to unseen tasks without any parameter updates. However, the\nimplementation of ICL is sophisticated due to the diverse retrieval and\ninference methods involved, as well as the varying pre-processing requirements\nfor different models, datasets, and tasks. A unified and flexible framework for\nICL is urgently needed to ease the implementation of the aforementioned\ncomponents. To facilitate ICL research, we introduce OpenICL, an open-source\ntoolkit for ICL and LLM evaluation. OpenICL is research-friendly with a highly\nflexible architecture that users can easily combine different components to\nsuit their needs. It also provides various state-of-the-art retrieval and\ninference methods to streamline the process of adapting ICL to cutting-edge\nresearch. The effectiveness of OpenICL has been validated on a wide range of\nNLP tasks, including classification, QA, machine translation, and semantic\nparsing. As a side-product, we found OpenICL to be an efficient yet robust tool\nfor LLMs evaluation. OpenICL is released at\nhttps://github.com/Shark-NLP/OpenICL", "published": "2023-03-06 06:20:25", "link": "http://arxiv.org/abs/2303.02913v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code\n  Understanding, Generation, Translation and Retrieval", "abstract": "Recently, pre-trained large language models (LLMs) have shown impressive\nabilities in generating codes from natural language descriptions, repairing\nbuggy codes, translating codes between languages, and retrieving relevant code\nsegments. However, the evaluation of these models has often been performed in a\nscattered way on only one or two specific tasks, in a few languages, at a\npartial granularity (e.g., function) level, and in many cases without proper\ntraining data. Even more concerning is that in most cases the evaluation of\ngenerated codes has been done in terms of mere lexical overlap with a reference\ncode rather than actual execution. We introduce xCodeEval, the largest\nexecutable multilingual multitask benchmark to date consisting of $25$M\ndocument-level coding examples ($16.5$B tokens) from about $7.5$K unique\nproblems covering up to $11$ programming languages with execution-level\nparallelism. It features a total of $7$ tasks involving code understanding,\ngeneration, translation and retrieval. xCodeEval adopts an execution-based\nevaluation and offers a multilingual code execution engine, ExecEval that\nsupports unit test based execution in all the $11$ languages. To address the\nchallenge of balancing the distributions of text-code samples over multiple\nattributes in validation/test sets, we propose a novel data splitting and a\ndata selection schema based on the geometric mean and graph-theoretic\nprinciple. Our experiments with OpenAI's LLMs (zero-shot) and open-LLMs\n(zero-shot and fine-tuned) on the tasks and languages demonstrate **xCodeEval**\nto be quite challenging as per the current advancements in language models.", "published": "2023-03-06 10:08:51", "link": "http://arxiv.org/abs/2303.03004v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NxPlain: Web-based Tool for Discovery of Latent Concepts", "abstract": "The proliferation of deep neural networks in various domains has seen an\nincreased need for the interpretability of these models, especially in\nscenarios where fairness and trust are as important as model performance. A lot\nof independent work is being carried out to: i) analyze what linguistic and\nnon-linguistic knowledge is learned within these models, and ii) highlight the\nsalient parts of the input. We present NxPlain, a web application that provides\nan explanation of a model's prediction using latent concepts. NxPlain discovers\nlatent concepts learned in a deep NLP model, provides an interpretation of the\nknowledge learned in the model, and explains its predictions based on the used\nconcepts. The application allows users to browse through the latent concepts in\nan intuitive order, letting them efficiently scan through the most salient\nconcepts with a global corpus level view and a local sentence-level view. Our\ntool is useful for debugging, unraveling model bias, and for highlighting\nspurious correlations in a model. A hosted demo is available here:\nhttps://nxplain.qcri.org.", "published": "2023-03-06 10:45:24", "link": "http://arxiv.org/abs/2303.03019v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Crowdsourcing on Sensitive Data with Privacy-Preserving Text Rewriting", "abstract": "Most tasks in NLP require labeled data. Data labeling is often done on\ncrowdsourcing platforms due to scalability reasons. However, publishing data on\npublic platforms can only be done if no privacy-relevant information is\nincluded. Textual data often contains sensitive information like person names\nor locations. In this work, we investigate how removing personally identifiable\ninformation (PII) as well as applying differential privacy (DP) rewriting can\nenable text with privacy-relevant information to be used for crowdsourcing. We\nfind that DP-rewriting before crowdsourcing can preserve privacy while still\nleading to good label quality for certain tasks and data. PII-removal led to\ngood label quality in all examined tasks, however, there are no privacy\nguarantees given.", "published": "2023-03-06 11:54:58", "link": "http://arxiv.org/abs/2303.03053v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Spelling convention sensitivity in neural language models", "abstract": "We examine whether large neural language models, trained on very large\ncollections of varied English text, learn the potentially long-distance\ndependency of British versus American spelling conventions, i.e., whether\nspelling is consistently one or the other within model-generated strings. In\ncontrast to long-distance dependencies in non-surface underlying structure\n(e.g., syntax), spelling consistency is easier to measure both in LMs and the\ntext corpora used to train them, which can provide additional insight into\ncertain observed model behaviors. Using a set of probe words unique to either\nBritish or American English, we first establish that training corpora exhibit\nsubstantial (though not total) consistency. A large T5 language model does\nappear to internalize this consistency, though only with respect to observed\nlexical items (not nonce words with British/American spelling patterns). We\nfurther experiment with correcting for biases in the training data by\nfine-tuning T5 on synthetic data that has been debiased, and find that\nfinetuned T5 remains only somewhat sensitive to spelling consistency. Further\nexperiments show GPT2 to be similarly limited.", "published": "2023-03-06 19:29:20", "link": "http://arxiv.org/abs/2303.03457v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Guilt Detection in Text: A Step Towards Understanding Complex Emotions", "abstract": "We introduce a novel Natural Language Processing (NLP) task called Guilt\ndetection, which focuses on detecting guilt in text. We identify guilt as a\ncomplex and vital emotion that has not been previously studied in NLP, and we\naim to provide a more fine-grained analysis of it. To address the lack of\npublicly available corpora for guilt detection, we created VIC, a dataset\ncontaining 4622 texts from three existing emotion detection datasets that we\nbinarized into guilt and no-guilt classes. We experimented with traditional\nmachine learning methods using bag-of-words and term frequency-inverse document\nfrequency features, achieving a 72% f1 score with the highest-performing model.\nOur study provides a first step towards understanding guilt in text and opens\nthe door for future research in this area.", "published": "2023-03-06 21:36:19", "link": "http://arxiv.org/abs/2303.03510v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contrastive variational information bottleneck for aspect-based\n  sentiment analysis", "abstract": "Deep learning techniques have dominated the literature on aspect-based\nsentiment analysis (ABSA), achieving state-of-the-art performance. However,\ndeep models generally suffer from spurious correlations between input features\nand output labels, which hurts the robustness and generalization capability by\na large margin. In this paper, we propose to reduce spurious correlations for\nABSA, via a novel Contrastive Variational Information Bottleneck framework\n(called CVIB). The proposed CVIB framework is composed of an original network\nand a self-pruned network, and these two networks are optimized simultaneously\nvia contrastive learning. Concretely, we employ the Variational Information\nBottleneck (VIB) principle to learn an informative and compressed network\n(self-pruned network) from the original network, which discards the superfluous\npatterns or spurious correlations between input features and prediction labels.\nThen, self-pruning contrastive learning is devised to pull together\nsemantically similar positive pairs and push away dissimilar pairs, where the\nrepresentations of the anchor learned by the original and self-pruned networks\nrespectively are regarded as a positive pair while the representations of two\ndifferent sentences within a mini-batch are treated as a negative pair. To\nverify the effectiveness of our CVIB method, we conduct extensive experiments\non five benchmark ABSA datasets and the experimental results show that our\napproach achieves better performance than the strong competitors in terms of\noverall prediction performance, robustness, and generalization. Code and data\nto reproduce the results in this paper is available at:\nhttps://github.com/shesshan/CVIB.", "published": "2023-03-06 02:52:37", "link": "http://arxiv.org/abs/2303.02846v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Multi-Grained Self-Interpretable Symbolic-Neural Model For\n  Single/Multi-Labeled Text Classification", "abstract": "Deep neural networks based on layer-stacking architectures have historically\nsuffered from poor inherent interpretability. Meanwhile, symbolic probabilistic\nmodels function with clear interpretability, but how to combine them with\nneural networks to enhance their performance remains to be explored. In this\npaper, we try to marry these two systems for text classification via a\nstructured language model. We propose a Symbolic-Neural model that can learn to\nexplicitly predict class labels of text spans from a constituency tree without\nrequiring any access to span-level gold labels. As the structured language\nmodel learns to predict constituency trees in a self-supervised manner, only\nraw texts and sentence-level labels are required as training data, which makes\nit essentially a general constituent-level self-interpretable classification\nmodel. Our experiments demonstrate that our approach could achieve good\nprediction accuracy in downstream tasks. Meanwhile, the predicted span labels\nare consistent with human rationales to a certain degree.", "published": "2023-03-06 03:25:43", "link": "http://arxiv.org/abs/2303.02860v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dynamic Prompting: A Unified Framework for Prompt Tuning", "abstract": "It has been demonstrated that the art of prompt tuning is highly effective in\nefficiently extracting knowledge from pretrained foundation models,\nencompassing pretrained language models (PLMs), vision pretrained models, and\nvision-language (V-L) models. However, the efficacy of employing fixed soft\nprompts with a predetermined position for concatenation with inputs for all\ninstances, irrespective of their inherent disparities, remains uncertain.\nVariables such as the position, length, and representations of prompts across\ndiverse instances and tasks can substantially influence the performance of\nprompt tuning. In this context, we provide a theoretical analysis, which\nreveals that optimizing the position of the prompt to encompass the input can\ncapture additional semantic information that traditional prefix or postfix\nprompt tuning methods fail to capture. Building upon our analysis, we present a\nunified dynamic prompt (DP) tuning strategy that dynamically determines\ndifferent factors of prompts based on specific tasks and instances. To\naccomplish this, we employ a lightweight learning network with Gumble-Softmax,\nallowing us to learn instance-dependent guidance. Experimental results\nunderscore the significant performance improvement achieved by dynamic prompt\ntuning across a wide range of tasks, including NLP tasks, vision recognition\ntasks, and vision-language tasks. Furthermore, we establish the universal\napplicability of our approach under full-data, few-shot, and multitask\nscenarios. Codes are available at https://github.com/Xianjun-Yang/DPT.", "published": "2023-03-06 06:04:46", "link": "http://arxiv.org/abs/2303.02909v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GlobalNER: Incorporating Non-local Information into Named Entity\n  Recognition", "abstract": "Nowadays, many Natural Language Processing (NLP) tasks see the demand for\nincorporating knowledge external to the local information to further improve\nthe performance. However, there is little related work on Named Entity\nRecognition (NER), which is one of the foundations of NLP. Specifically, no\nstudies were conducted on the query generation and re-ranking for retrieving\nthe related information for the purpose of improving NER. This work\ndemonstrates the effectiveness of a DNN-based query generation method and a\nmention-aware re-ranking architecture based on BERTScore particularly for NER.\nIn the end, a state-of-the-art performance of 61.56 micro-f1 score on WNUT17\ndataset is achieved.", "published": "2023-03-06 06:20:55", "link": "http://arxiv.org/abs/2303.02915v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Zero-Shot Functional Compositionality of Language Models", "abstract": "Large Pre-trained Language Models (PLM) have become the most desirable\nstarting point in the field of NLP, as they have become remarkably good at\nsolving many individual tasks. Despite such success, in this paper, we argue\nthat current paradigms of working with PLMs are neglecting a critical aspect of\nmodeling human intelligence: functional compositionality. Functional\ncompositionality - the ability to compose learned tasks - has been a\nlong-standing challenge in the field of AI (and many other fields) as it is\nconsidered one of the hallmarks of human intelligence. An illustrative example\nof such is cross-lingual summarization, where a bilingual person\n(English-French) could directly summarize an English document into French\nsentences without having to translate the English document or summary into\nFrench explicitly. We discuss why this matter is an important open problem that\nrequires further attention from the field. Then, we show that current PLMs\n(e.g., GPT-2 and T5) don't have functional compositionality yet and it is far\nfrom human-level generalizability. Finally, we suggest several research\ndirections that could push the field towards zero-shot functional\ncompositionality of language models.", "published": "2023-03-06 13:15:25", "link": "http://arxiv.org/abs/2303.03103v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "IFAN: An Explainability-Focused Interaction Framework for Humans and NLP\n  Models", "abstract": "Interpretability and human oversight are fundamental pillars of deploying\ncomplex NLP models into real-world applications. However, applying\nexplainability and human-in-the-loop methods requires technical proficiency.\nDespite existing toolkits for model understanding and analysis, options to\nintegrate human feedback are still limited. We propose IFAN, a framework for\nreal-time explanation-based interaction with NLP models. Through IFAN's\ninterface, users can provide feedback to selected model explanations, which is\nthen integrated through adapter layers to align the model with human rationale.\nWe show the system to be effective in debiasing a hate speech classifier with\nminimal impact on performance. IFAN also offers a visual admin system and API\nto manage models (and datasets) as well as control access rights. A demo is\nlive at https://ifan.ml.", "published": "2023-03-06 13:37:59", "link": "http://arxiv.org/abs/2303.03124v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Choice Over Control: How Users Write with Large Language Models using\n  Diegetic and Non-Diegetic Prompting", "abstract": "We propose a conceptual perspective on prompts for Large Language Models\n(LLMs) that distinguishes between (1) diegetic prompts (part of the narrative,\ne.g. \"Once upon a time, I saw a fox...\"), and (2) non-diegetic prompts\n(external, e.g. \"Write about the adventures of the fox.\"). With this lens, we\nstudy how 129 crowd workers on Prolific write short texts with different user\ninterfaces (1 vs 3 suggestions, with/out non-diegetic prompts; implemented with\nGPT-3): When the interface offered multiple suggestions and provided an option\nfor non-diegetic prompting, participants preferred choosing from multiple\nsuggestions over controlling them via non-diegetic prompts. When participants\nprovided non-diegetic prompts it was to ask for inspiration, topics or facts.\nSingle suggestions in particular were guided both with diegetic and\nnon-diegetic information. This work informs human-AI interaction with\ngenerative models by revealing that (1) writing non-diegetic prompts requires\neffort, (2) people combine diegetic and non-diegetic prompting, and (3) they\nuse their draft (i.e. diegetic information) and suggestion timing to\nstrategically guide LLMs.", "published": "2023-03-06 14:58:42", "link": "http://arxiv.org/abs/2303.03199v1", "categories": ["cs.HC", "cs.CL", "H.5.2; I.2.7"], "primary_category": "cs.HC"}
{"title": "The AI Ghostwriter Effect: When Users Do Not Perceive Ownership of\n  AI-Generated Text But Self-Declare as Authors", "abstract": "Human-AI interaction in text production increases complexity in authorship.\nIn two empirical studies (n1 = 30 & n2 = 96), we investigate authorship and\nownership in human-AI collaboration for personalized language generation. We\nshow an AI Ghostwriter Effect: Users do not consider themselves the owners and\nauthors of AI-generated text but refrain from publicly declaring AI authorship.\nPersonalization of AI-generated texts did not impact the AI Ghostwriter Effect,\nand higher levels of participants' influence on texts increased their sense of\nownership. Participants were more likely to attribute ownership to supposedly\nhuman ghostwriters than AI ghostwriters, resulting in a higher\nownership-authorship discrepancy for human ghostwriters. Rationalizations for\nauthorship in AI ghostwriters and human ghostwriters were similar. We discuss\nhow our findings relate to psychological ownership and human-AI interaction to\nlay the foundations for adapting authorship frameworks and user interfaces in\nAI in text-generation tasks.", "published": "2023-03-06 16:53:12", "link": "http://arxiv.org/abs/2303.03283v2", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Two-stage Pipeline for Multilingual Dialect Detection", "abstract": "Dialect Identification is a crucial task for localizing various Large\nLanguage Models. This paper outlines our approach to the VarDial 2023 shared\ntask. Here we have to identify three or two dialects from three languages each\nwhich results in a 9-way classification for Track-1 and 6-way classification\nfor Track-2 respectively. Our proposed approach consists of a two-stage system\nand outperforms other participants' systems and previous works in this domain.\nWe achieve a score of 58.54% for Track-1 and 85.61% for Track-2. Our codebase\nis available publicly (https://github.com/ankit-vaidya19/EACL_VarDial2023).", "published": "2023-03-06 20:35:51", "link": "http://arxiv.org/abs/2303.03487v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-resolution Interpretation and Diagnostics Tool for Natural\n  Language Classifiers", "abstract": "Developing explainability methods for Natural Language Processing (NLP)\nmodels is a challenging task, for two main reasons. First, the high\ndimensionality of the data (large number of tokens) results in low coverage and\nin turn small contributions for the top tokens, compared to the overall model\nperformance. Second, owing to their textual nature, the input variables, after\nappropriate transformations, are effectively binary (presence or absence of a\ntoken in an observation), making the input-output relationship difficult to\nunderstand. Common NLP interpretation techniques do not have flexibility in\nresolution, because they usually operate at word-level and provide fully local\n(message level) or fully global (over all messages) summaries. The goal of this\npaper is to create more flexible model explainability summaries by segments of\nobservation or clusters of words that are semantically related to each other.\nIn addition, we introduce a root cause analysis method for NLP models, by\nanalyzing representative False Positive and False Negative examples from\ndifferent segments. At the end, we illustrate, using a Yelp review data set\nwith three segments (Restaurant, Hotel, and Beauty), that exploiting\ngroup/cluster structures in words and/or messages can aid in the interpretation\nof decisions made by NLP models and can be utilized to assess the model's\nsensitivity or bias towards gender, syntax, and word meanings.", "published": "2023-03-06 22:59:02", "link": "http://arxiv.org/abs/2303.03542v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Data Portraits: Recording Foundation Model Training Data", "abstract": "Foundation models are trained on increasingly immense and opaque datasets.\nEven while these models are now key in AI system building, it can be difficult\nto answer the straightforward question: has the model already encountered a\ngiven example during training? We therefore propose a widespread adoption of\nData Portraits: artifacts that record training data and allow for downstream\ninspection. First we outline the properties of such an artifact and discuss how\nexisting solutions can be used to increase transparency. We then propose and\nimplement a solution based on data sketching, stressing fast and space\nefficient querying. Using our tools, we document a popular language modeling\ncorpus (The Pile) and a recently released code modeling dataset (The Stack). We\nshow that our solution enables answering questions about test set leakage and\nmodel plagiarism. Our tool is lightweight and fast, costing only 3% of the\ndataset size in overhead. We release a live interface of our tools at\nhttps://dataportraits.org/ and call on dataset and model creators to release\nData Portraits as a complement to current documentation practices.", "published": "2023-03-06 04:22:33", "link": "http://arxiv.org/abs/2303.03919v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Model-Agnostic Meta-Learning for Natural Language Understanding Tasks in\n  Finance", "abstract": "Natural language understanding(NLU) is challenging for finance due to the\nlack of annotated data and the specialized language in that domain. As a\nresult, researchers have proposed to use pre-trained language model and\nmulti-task learning to learn robust representations. However, aggressive\nfine-tuning often causes over-fitting and multi-task learning may favor tasks\nwith significantly larger amounts data, etc. To address these problems, in this\npaper, we investigate model-agnostic meta-learning algorithm(MAML) in\nlow-resource financial NLU tasks. Our contribution includes: 1. we explore the\nperformance of MAML method with multiple types of tasks: GLUE datasets, SNLI,\nSci-Tail and Financial PhraseBank; 2. we study the performance of MAML method\nwith multiple single-type tasks: a real scenario stock price prediction problem\nwith twitter text data. Our models achieve the state-of-the-art performance\naccording to the experimental results, which demonstrate that our method can\nadapt fast and well to low-resource situations.", "published": "2023-03-06 02:24:48", "link": "http://arxiv.org/abs/2303.02841v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HiCLIP: Contrastive Language-Image Pretraining with Hierarchy-aware\n  Attention", "abstract": "The success of large-scale contrastive vision-language pretraining (CLIP) has\nbenefited both visual recognition and multimodal content understanding. The\nconcise design brings CLIP the advantage in inference efficiency against other\nvision-language models with heavier cross-attention fusion layers, making it a\npopular choice for a wide spectrum of downstream tasks. However, CLIP does not\nexplicitly capture the hierarchical nature of high-level and fine-grained\nsemantics conveyed in images and texts, which is arguably critical to\nvision-language understanding and reasoning. To this end, we equip both the\nvisual and language branches in CLIP with hierarchy-aware attentions, namely\nHierarchy-aware CLIP (HiCLIP), to progressively discover semantic hierarchies\nlayer-by-layer from both images and texts in an unsupervised manner. As a\nresult, such hierarchical aggregation significantly improves the cross-modal\nalignment. To demonstrate the advantages of HiCLIP, we conduct qualitative\nanalysis on its unsupervised hierarchy induction during inference, as well as\nextensive quantitative experiments on both visual recognition and\nvision-language downstream tasks.", "published": "2023-03-06 09:44:01", "link": "http://arxiv.org/abs/2303.02995v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "DeCap: Decoding CLIP Latents for Zero-Shot Captioning via Text-Only\n  Training", "abstract": "Large-scale pre-trained multi-modal models (e.g., CLIP) demonstrate strong\nzero-shot transfer capability in many discriminative tasks. Their adaptation to\nzero-shot image-conditioned text generation tasks has drawn increasing\ninterest. Prior arts approach to zero-shot captioning by either utilizing the\nexisting large language models (e.g., GPT-2) or pre-training the\nencoder-decoder network in an end-to-end manner. In this work, we propose a\nsimple framework, named DeCap, for zero-shot captioning. We introduce a\nlightweight visual-aware language decoder. This decoder is both data-efficient\nand computation-efficient: 1) it only requires the text data for training,\neasing the burden on the collection of paired data. 2) it does not require\nend-to-end training. When trained with text-only data, the decoder takes the\ntext embedding extracted from the off-the-shelf CLIP encoder as a prefix\nembedding. The challenge is that the decoder is trained on the text corpus but\nat the inference stage, it needs to generate captions based on visual inputs.\nThe modality gap issue is widely observed in multi-modal contrastive models\nthat prevents us from directly taking the visual embedding as the prefix\nembedding. We propose a training-free mechanism to reduce the modality gap. We\nproject the visual embedding into the CLIP text embedding space, while the\nprojected embedding retains the information of the visual input. Taking the\nprojected embedding as the prefix embedding, the decoder generates high-quality\ndescriptions that match the visual input. The experiments show that DeCap\noutperforms other zero-shot captioning methods and unpaired captioning methods\non the typical image captioning benchmarks, i.e., MSCOCO and NoCaps.", "published": "2023-03-06 11:02:47", "link": "http://arxiv.org/abs/2303.03032v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "IPA-CLIP: Integrating Phonetic Priors into Vision and Language\n  Pretraining", "abstract": "Recently, large-scale Vision and Language (V\\&L) pretraining has become the\nstandard backbone of many multimedia systems. While it has shown remarkable\nperformance even in unseen situations, it often performs in ways not intuitive\nto humans. Particularly, they usually do not consider the pronunciation of the\ninput, which humans would utilize to understand language, especially when it\ncomes to unknown words. Thus, this paper inserts phonetic prior into\nContrastive Language-Image Pretraining (CLIP), one of the V\\&L pretrained\nmodels, to make it consider the pronunciation similarity among its\npronunciation inputs. To achieve this, we first propose a phoneme embedding\nthat utilizes the phoneme relationships provided by the International Phonetic\nAlphabet (IPA) chart as a phonetic prior. Next, by distilling the frozen CLIP\ntext encoder, we train a pronunciation encoder employing the IPA-based\nembedding. The proposed model named IPA-CLIP comprises this pronunciation\nencoder and the original CLIP encoders (image and text). Quantitative\nevaluation reveals that the phoneme distribution on the embedding space\nrepresents phonetic relationships more accurately when using the proposed\nphoneme embedding. Furthermore, in some multimodal retrieval tasks, we confirm\nthat the proposed pronunciation encoder enhances the performance of the text\nencoder and that the pronunciation encoder handles nonsense words in a more\nphonetic manner than the text encoder. Finally, qualitative evaluation verifies\nthe correlation between the pronunciation encoder and human perception\nregarding pronunciation similarity.", "published": "2023-03-06 13:59:37", "link": "http://arxiv.org/abs/2303.03144v1", "categories": ["cs.MM", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Neighborhood Contrastive Transformer for Change Captioning", "abstract": "Change captioning is to describe the semantic change between a pair of\nsimilar images in natural language. It is more challenging than general image\ncaptioning, because it requires capturing fine-grained change information while\nbeing immune to irrelevant viewpoint changes, and solving syntax ambiguity in\nchange descriptions. In this paper, we propose a neighborhood contrastive\ntransformer to improve the model's perceiving ability for various changes under\ndifferent scenes and cognition ability for complex syntax structure.\nConcretely, we first design a neighboring feature aggregating to integrate\nneighboring context into each feature, which helps quickly locate the\ninconspicuous changes under the guidance of conspicuous referents. Then, we\ndevise a common feature distilling to compare two images at neighborhood level\nand extract common properties from each image, so as to learn effective\ncontrastive information between them. Finally, we introduce the explicit\ndependencies between words to calibrate the transformer decoder, which helps\nbetter understand complex syntax structure during training. Extensive\nexperimental results demonstrate that the proposed method achieves the\nstate-of-the-art performance on three public datasets with different change\nscenarios. The code is available at https://github.com/tuyunbin/NCT.", "published": "2023-03-06 14:39:54", "link": "http://arxiv.org/abs/2303.03171v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Faithfulness-Aware Decoding Strategies for Abstractive Summarization", "abstract": "Despite significant progress in understanding and improving faithfulness in\nabstractive summarization, the question of how decoding strategies affect\nfaithfulness is less studied. We present a systematic study of the effect of\ngeneration techniques such as beam search and nucleus sampling on faithfulness\nin abstractive summarization. We find a consistent trend where beam search with\nlarge beam sizes produces the most faithful summaries while nucleus sampling\ngenerates the least faithful ones. We propose two faithfulness-aware generation\nmethods to further improve faithfulness over current generation techniques: (1)\nranking candidates generated by beam search using automatic faithfulness\nmetrics and (2) incorporating lookahead heuristics that produce a faithfulness\nscore on the future summary. We show that both generation methods significantly\nimprove faithfulness across two datasets as evaluated by four automatic\nfaithfulness metrics and human evaluation. To reduce computational cost, we\ndemonstrate a simple distillation approach that allows the model to generate\nfaithful summaries with just greedy decoding. Our code is publicly available at\nhttps://github.com/amazon-science/faithful-summarization-generation", "published": "2023-03-06 16:49:27", "link": "http://arxiv.org/abs/2303.03278v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AmQA: Amharic Question Answering Dataset", "abstract": "Question Answering (QA) returns concise answers or answer lists from natural\nlanguage text given a context document. Many resources go into curating QA\ndatasets to advance robust models' development. There is a surge of QA datasets\nfor languages like English, however, this is not true for Amharic. Amharic, the\nofficial language of Ethiopia, is the second most spoken Semitic language in\nthe world. There is no published or publicly available Amharic QA dataset.\nHence, to foster the research in Amharic QA, we present the first Amharic QA\n(AmQA) dataset. We crowdsourced 2628 question-answer pairs over 378 Wikipedia\narticles. Additionally, we run an XLMR Large-based baseline model to spark\nopen-domain QA research interest. The best-performing baseline achieves an\nF-score of 69.58 and 71.74 in reader-retriever QA and reading comprehension\nsettings respectively.", "published": "2023-03-06 17:06:50", "link": "http://arxiv.org/abs/2303.03290v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Enhancing Activity Prediction Models in Drug Discovery with the Ability\n  to Understand Human Language", "abstract": "Activity and property prediction models are the central workhorses in drug\ndiscovery and materials sciences, but currently they have to be trained or\nfine-tuned for new tasks. Without training or fine-tuning, scientific language\nmodels could be used for such low-data tasks through their announced zero- and\nfew-shot capabilities. However, their predictive quality at activity prediction\nis lacking. In this work, we envision a novel type of activity prediction model\nthat is able to adapt to new prediction tasks at inference time, via\nunderstanding textual information describing the task. To this end, we propose\na new architecture with separate modules for chemical and natural language\ninputs, and a contrastive pre-training objective on data from large biochemical\ndatabases. In extensive experiments, we show that our method CLAMP yields\nimproved predictive performance on few-shot learning benchmarks and zero-shot\nproblems in drug discovery. We attribute the advances of our method to the\nmodularized architecture and to our pre-training objective.", "published": "2023-03-06 18:49:09", "link": "http://arxiv.org/abs/2303.03363v2", "categories": ["q-bio.BM", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "q-bio.BM"}
{"title": "Can an Embodied Agent Find Your \"Cat-shaped Mug\"? LLM-Guided Exploration\n  for Zero-Shot Object Navigation", "abstract": "We present LGX (Language-guided Exploration), a novel algorithm for\nLanguage-Driven Zero-Shot Object Goal Navigation (L-ZSON), where an embodied\nagent navigates to a uniquely described target object in a previously unseen\nenvironment. Our approach makes use of Large Language Models (LLMs) for this\ntask by leveraging the LLM's commonsense reasoning capabilities for making\nsequential navigational decisions. Simultaneously, we perform generalized\ntarget object detection using a pre-trained Vision-Language grounding model. We\nachieve state-of-the-art zero-shot object navigation results on RoboTHOR with a\nsuccess rate (SR) improvement of over 27% over the current baseline of the\nOWL-ViT CLIP on Wheels (OWL CoW). Furthermore, we study the usage of LLMs for\nrobot navigation and present an analysis of various prompting strategies\naffecting the model output. Finally, we showcase the benefits of our approach\nvia \\textit{real-world} experiments that indicate the superior performance of\nLGX in detecting and navigating to visually unique objects.", "published": "2023-03-06 20:19:19", "link": "http://arxiv.org/abs/2303.03480v2", "categories": ["cs.RO", "cs.AI", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Large Language Models as Zero-Shot Human Models for Human-Robot\n  Interaction", "abstract": "Human models play a crucial role in human-robot interaction (HRI), enabling\nrobots to consider the impact of their actions on people and plan their\nbehavior accordingly. However, crafting good human models is challenging;\ncapturing context-dependent human behavior requires significant prior knowledge\nand/or large amounts of interaction data, both of which are difficult to\nobtain. In this work, we explore the potential of large-language models (LLMs)\n-- which have consumed vast amounts of human-generated text data -- to act as\nzero-shot human models for HRI. Our experiments on three social datasets yield\npromising results; the LLMs are able to achieve performance comparable to\npurpose-built models. That said, we also discuss current limitations, such as\nsensitivity to prompts and spatial/numerical reasoning mishaps. Based on our\nfindings, we demonstrate how LLM-based human models can be integrated into a\nsocial robot's planning process and applied in HRI scenarios. Specifically, we\npresent one case study on a simulated trust-based table-clearing task and\nreplicate past results that relied on custom models. Next, we conduct a new\nrobot utensil-passing experiment (n = 65) where preliminary results show that\nplanning with a LLM-based human model can achieve gains over a basic myopic\nplan. In summary, our results show that LLMs offer a promising (but incomplete)\napproach to human modeling for HRI.", "published": "2023-03-06 23:16:24", "link": "http://arxiv.org/abs/2303.03548v2", "categories": ["cs.RO", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.RO"}
{"title": "ChatGPT is on the Horizon: Could a Large Language Model be Suitable for\n  Intelligent Traffic Safety Research and Applications?", "abstract": "ChatGPT embarks on a new era of artificial intelligence and will\nrevolutionize the way we approach intelligent traffic safety systems. This\npaper begins with a brief introduction about the development of large language\nmodels (LLMs). Next, we exemplify using ChatGPT to address key traffic safety\nissues. Furthermore, we discuss the controversies surrounding LLMs, raise\ncritical questions for their deployment, and provide our solutions. Moreover,\nwe propose an idea of multi-modality representation learning for smarter\ntraffic safety decision-making and open more questions for application\nimprovement. We believe that LLM will both shape and potentially facilitate\ncomponents of traffic safety research.", "published": "2023-03-06 16:36:17", "link": "http://arxiv.org/abs/2303.05382v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Depression Detection Using Digital Traces on Social Media: A\n  Knowledge-aware Deep Learning Approach", "abstract": "Depression is a common disease worldwide. It is difficult to diagnose and\ncontinues to be underdiagnosed. Because depressed patients constantly share\ntheir symptoms, major life events, and treatments on social media, researchers\nare turning to user-generated digital traces on social media for depression\ndetection. Such methods have distinct advantages in combating depression\nbecause they can facilitate innovative approaches to fight depression and\nalleviate its social and economic burden. However, most existing studies lack\neffective means to incorporate established medical domain knowledge in\ndepression detection or suffer from feature extraction difficulties that impede\ngreater performance. Following the design science research paradigm, we propose\na Deep Knowledge-aware Depression Detection (DKDD) framework to accurately\ndetect social media users at risk of depression and explain the critical\nfactors that contribute to such detection. Extensive empirical studies with\nreal-world data demonstrate that, by incorporating domain knowledge, our method\noutperforms existing state-of-the-art methods. Our work has significant\nimplications for IS research in knowledge-aware machine learning, digital\ntraces utilization, and NLP research in IS. Practically, by providing early\ndetection and explaining the critical factors, DKDD can supplement clinical\ndepression screening and enable large-scale evaluations of a population's\nmental health status.", "published": "2023-03-06 20:08:07", "link": "http://arxiv.org/abs/2303.05389v2", "categories": ["cs.CL", "cs.AI", "cs.SI", "stat.AP", "H.4.m", "K.5"], "primary_category": "cs.CL"}
{"title": "wav2vec and its current potential to Automatic Speech Recognition in\n  German for the usage in Digital History: A comparative assessment of\n  available ASR-technologies for the use in cultural heritage contexts", "abstract": "In this case study we trained and published a state-of-the-art open-source\nmodel for Automatic Speech Recognition (ASR) for German to evaluate the current\npotential of this technology for the use in the larger context of Digital\nHumanities and cultural heritage indexation. Along with this paper we publish\nour wav2vec2 based speech to text model while we evaluate its performance on a\ncorpus of historical recordings we assembled compared against commercial\ncloud-based and proprietary services. While our model achieves moderate\nresults, we see that proprietary cloud services fare significantly better. As\nour results show, recognition rates over 90 percent can currently be achieved,\nhowever, these numbers drop quickly once the recordings feature limited audio\nquality or use of non-every day or outworn language. A big issue is the high\nvariety of different dialects and accents in the German language. Nevertheless,\nthis paper highlights that the currently available quality of recognition is\nhigh enough to address various use cases in the Digital Humanities. We argue\nthat ASR will become a key technology for the documentation and analysis of\naudio-visual sources and identify an array of important questions that the DH\ncommunity and cultural heritage stakeholders will have to address in the near\nfuture.", "published": "2023-03-06 22:24:31", "link": "http://arxiv.org/abs/2303.06026v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "FoundationTTS: Text-to-Speech for ASR Customization with Generative\n  Language Model", "abstract": "Neural text-to-speech (TTS) generally consists of cascaded architecture with\nseparately optimized acoustic model and vocoder, or end-to-end architecture\nwith continuous mel-spectrograms or self-extracted speech frames as the\nintermediate representations to bridge acoustic model and vocoder, which\nsuffers from two limitations: 1) the continuous acoustic frames are hard to\npredict with phoneme only, and acoustic information like duration or pitch is\nalso needed to solve the one-to-many problem, which is not easy to scale on\nlarge scale and noise datasets; 2) to achieve diverse speech output based on\ncontinuous speech features, complex VAE or flow-based models are usually\nrequired. In this paper, we propose FoundationTTS, a new speech synthesis\nsystem with a neural audio codec for discrete speech token extraction and\nwaveform reconstruction and a large language model for discrete token\ngeneration from linguistic (phoneme) tokens. Specifically, 1) we propose a\nhierarchical codec network based on vector-quantized auto-encoders with\nadversarial training (VQ-GAN), which first extracts continuous frame-level\nspeech representations with fine-grained codec, and extracts a discrete token\nfrom each continuous speech frame with coarse-grained codec; 2) we jointly\noptimize speech token, linguistic tokens, speaker token together with a large\nlanguage model and predict the discrete speech tokens autoregressively.\nExperiments show that FoundationTTS achieves a MOS gain of +0.14 compared to\nthe baseline system. In ASR customization tasks, our method achieves 7.09\\% and\n10.35\\% WERR respectively over two strong customized ASR baselines.", "published": "2023-03-06 07:17:15", "link": "http://arxiv.org/abs/2303.02939v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Variational Inference of Structured Line Spectra Exploiting\n  Group-Sparsity", "abstract": "In this paper, we present a variational inference algorithm that decomposes a\nsignal into multiple groups of related spectral lines. The spectral lines in\neach group are associated with a group parameter common to all spectral lines\nwithin the group. The proposed algorithm jointly estimates the group\nparameters, the number of spetral lines within a group, and the number of\ngroups exploiting a Bernoulli-Gamma-Gaussian hierarchical prior model which\npromotes sparse solutions. Aiming to maximize the evidence lower bound (ELBO),\nvariational inference provides analytic approximations of the posterior\nprobability density functions (PDFs) and also gives estimates of the additional\nmodel parameters such as the measurement noise variance. While the activation\nvariables of the groups and the associated group parameters (such as\nfundamental frequencies and the corresponding higher order harmonics) are\nestimated as point estimates, the remaining parameters such as the complex\namplitudes of the spectral lines and their precision parameters are estimated\nas approximate posterior PDFs. We demonstrate the versatility and performance\nof the proposed algorithm on three different inference problems. In particular,\nthe proposed algorithm is applied to the multi-pitch estimation problem, the\nradar signal-based extended object estimation problem, and variational mode\ndecomposition (VMD) using synthetic measurements and to real multi-pitch\nestimation problem using the Bach-10 dataset. The results show that the\nproposed algorithm outperforms state-of-the-art model-based and pre-trained\nalgorithms on all three inference problems.", "published": "2023-03-06 10:38:41", "link": "http://arxiv.org/abs/2303.03017v2", "categories": ["eess.SP", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Cross-Lingual Transfer Learning for Alzheimer's Detection From\n  Spontaneous Speech", "abstract": "Alzheimer's disease (AD) is a progressive neurodegenerative disease most\noften associated with memory deficits and cognitive decline. With the aging\npopulation, there has been much interest in automated methods for cognitive\nimpairment detection. One approach that has attracted attention in recent years\nis AD detection through spontaneous speech. While the results are promising, it\nis not certain whether the learned speech features can be generalized across\nlanguages. To fill this gap, the ADReSS-M challenge was organized. This paper\npresents our submission to this ICASSP-2023 Signal Processing Grand Challenge\n(SPGC). The model was trained on 228 English samples of a picture description\ntask and was transferred to Greek using only 8 samples. We obtained an accuracy\nof 82.6% for AD detection, a root-mean-square error of 4.345 for cognitive\nscore prediction, and ranked 2nd place in the competition out of 24\ncompetitors.", "published": "2023-03-06 11:46:22", "link": "http://arxiv.org/abs/2303.03049v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Scaling strategies for on-device low-complexity source separation with\n  Conv-Tasnet", "abstract": "Recently, several very effective neural approaches for single-channel speech\nseparation have been presented in the literature. However, due to the size and\ncomplexity of these models, their use on low-resource devices, e.g. for hearing\naids, and earphones, is still a challenge and established solutions are not\navailable yet. Although approaches based on either pruning or compressing\nneural models have been proposed, the design of a model architecture suitable\nfor a certain application domain often requires heuristic procedures not easily\nportable to different low-resource platforms. Given the modular nature of the\nwell-known Conv-Tasnet speech separation architecture, in this paper we\nconsider three parameters that directly control the overall size of the model,\nnamely: the number of residual blocks, the number of repetitions of the\nseparation blocks and the number of channels in the depth-wise convolutions,\nand experimentally evaluate how they affect the speech separation performance.\nIn particular, experiments carried out on the Libri2Mix show that the number of\ndilated 1D-Conv blocks is the most critical parameter and that the usage of\nextra-dilation in the residual blocks allows reducing the performance drop.", "published": "2023-03-06 10:15:14", "link": "http://arxiv.org/abs/2303.03005v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
