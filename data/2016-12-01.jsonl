{"title": "Multilingual Multiword Expressions", "abstract": "The project aims to provide a semi-supervised approach to identify Multiword\nExpressions in a multilingual context consisting of English and most of the\nmajor Indian languages. Multiword expressions are a group of words which refers\nto some conventional or regional way of saying things. If they are literally\ntranslated from one language to another the expression will lose its inherent\nmeaning.\n  To automatically extract multiword expressions from a corpus, an extraction\npipeline have been constructed which consist of a combination of rule based and\nstatistical approaches. There are several types of multiword expressions which\ndiffer from each other widely by construction. We employ different methods to\ndetect different types of multiword expressions. Given a POS tagged corpus in\nEnglish or any Indian language the system initially applies some regular\nexpression filters to narrow down the search space to certain patterns (like,\nreduplication, partial reduplication, compound nouns, compound verbs, conjunct\nverbs etc.). The word sequences matching the required pattern are subjected to\na series of linguistic tests which include verb filtering, named entity\nfiltering and hyphenation filtering test to exclude false positives. The\ncandidates are then checked for semantic relationships among themselves (using\nWordnet). In order to detect partial reduplication we make use of Wordnet as a\nlexical database as well as a tool for lemmatising. We detect complex\npredicates by investigating the features of the constituent words. Statistical\nmethods are applied to detect collocations. Finally, lexicographers examine the\nlist of automatically extracted candidates to validate whether they are true\nmultiword expressions or not and add them to the multiword dictionary\naccordingly.", "published": "2016-12-01 14:01:00", "link": "http://arxiv.org/abs/1612.00246v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Definition Modeling: Learning to define word embeddings in natural\n  language", "abstract": "Distributed representations of words have been shown to capture lexical\nsemantics, as demonstrated by their effectiveness in word similarity and\nanalogical relation tasks. But, these tasks only evaluate lexical semantics\nindirectly. In this paper, we study whether it is possible to utilize\ndistributed representations to generate dictionary definitions of words, as a\nmore direct and transparent representation of the embeddings' semantics. We\nintroduce definition modeling, the task of generating a definition for a given\nword and its embedding. We present several definition model architectures based\non recurrent neural networks, and experiment with the models over multiple data\nsets. Our results show that a model that controls dependencies between the word\nbeing defined and the definition words performs significantly better, and that\na character-level convolution layer designed to leverage morphology can\ncomplement word-level embeddings. Finally, an error analysis suggests that the\nerrors made by a definition model may provide insight into the shortcomings of\nword embeddings.", "published": "2016-12-01 19:42:37", "link": "http://arxiv.org/abs/1612.00394v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Document Embeddings for Intensive Care Patient Mortality\n  Prediction", "abstract": "We present an automatic mortality prediction scheme based on the unstructured\ntextual content of clinical notes. Proposing a convolutional document embedding\napproach, our empirical investigation using the MIMIC-III intensive care\ndatabase shows significant performance gains compared to previously employed\nmethods such as latent topic distributions or generic doc2vec embeddings. These\nimprovements are especially pronounced for the difficult problem of\npost-discharge mortality prediction.", "published": "2016-12-01 21:06:54", "link": "http://arxiv.org/abs/1612.00467v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ESE: Efficient Speech Recognition Engine with Sparse LSTM on FPGA", "abstract": "Long Short-Term Memory (LSTM) is widely used in speech recognition. In order\nto achieve higher prediction accuracy, machine learning scientists have built\nlarger and larger models. Such large model is both computation intensive and\nmemory intensive. Deploying such bulky model results in high power consumption\nand leads to high total cost of ownership (TCO) of a data center. In order to\nspeedup the prediction and make it energy efficient, we first propose a\nload-balance-aware pruning method that can compress the LSTM model size by 20x\n(10x from pruning and 2x from quantization) with negligible loss of the\nprediction accuracy. The pruned model is friendly for parallel processing.\nNext, we propose scheduler that encodes and partitions the compressed model to\neach PE for parallelism, and schedule the complicated LSTM data flow. Finally,\nwe design the hardware architecture, named Efficient Speech Recognition Engine\n(ESE) that works directly on the compressed model. Implemented on Xilinx\nXCKU060 FPGA running at 200MHz, ESE has a performance of 282 GOPS working\ndirectly on the compressed LSTM network, corresponding to 2.52 TOPS on the\nuncompressed one, and processes a full LSTM for speech recognition with a power\ndissipation of 41 Watts. Evaluated on the LSTM for speech recognition\nbenchmark, ESE is 43x and 3x faster than Core i7 5930k CPU and Pascal Titan X\nGPU implementations. It achieves 40x and 11.5x higher energy efficiency\ncompared with the CPU and GPU respectively.", "published": "2016-12-01 13:16:00", "link": "http://arxiv.org/abs/1612.00694v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain Adaptation for Named Entity Recognition in Online Media with Word\n  Embeddings", "abstract": "Content on the Internet is heterogeneous and arises from various domains like\nNews, Entertainment, Finance and Technology. Understanding such content\nrequires identifying named entities (persons, places and organizations) as one\nof the key steps. Traditionally Named Entity Recognition (NER) systems have\nbeen built using available annotated datasets (like CoNLL, MUC) and demonstrate\nexcellent performance. However, these models fail to generalize onto other\ndomains like Sports and Finance where conventions and language use can differ\nsignificantly. Furthermore, several domains do not have large amounts of\nannotated labeled data for training robust Named Entity Recognition models. A\nkey step towards this challenge is to adapt models learned on domains where\nlarge amounts of annotated training data are available to domains with scarce\nannotated data.\n  In this paper, we propose methods to effectively adapt models learned on one\ndomain onto other domains using distributed word representations. First we\nanalyze the linguistic variation present across domains to identify key\nlinguistic insights that can boost performance across domains. We propose\nmethods to capture domain specific semantics of word usage in addition to\nglobal semantics. We then demonstrate how to effectively use such domain\nspecific knowledge to learn NER models that outperform previous baselines in\nthe domain adaptation setting.", "published": "2016-12-01 05:08:53", "link": "http://arxiv.org/abs/1612.00148v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "On Coreferring Text-extracted Event Descriptions with the aid of\n  Ontological Reasoning", "abstract": "Systems for automatic extraction of semantic information about events from\nlarge textual resources are now available: these tools are capable to generate\nRDF datasets about text extracted events and this knowledge can be used to\nreason over the recognized events. On the other hand, text based tasks for\nevent recognition, as for example event coreference (i.e. recognizing whether\ntwo textual descriptions refer to the same event), do not take into account\nontological information of the extracted events in their process. In this\npaper, we propose a method to derive event coreference on text extracted event\ndata using semantic based rule reasoning. We demonstrate our method considering\na limited (yet representative) set of event types: we introduce a formal\nanalysis on their ontological properties and, on the base of this, we define a\nset of coreference criteria. We then implement these criteria as RDF-based\nreasoning rules to be applied on text extracted event data. We evaluate the\neffectiveness of our approach over a standard coreference benchmark dataset.", "published": "2016-12-01 12:58:02", "link": "http://arxiv.org/abs/1612.00227v1", "categories": ["cs.AI", "cs.CL", "I.2.4"], "primary_category": "cs.AI"}
{"title": "Improved Image Captioning via Policy Gradient optimization of SPIDEr", "abstract": "Current image captioning methods are usually trained via (penalized) maximum\nlikelihood estimation. However, the log-likelihood score of a caption does not\ncorrelate well with human assessments of quality. Standard syntactic evaluation\nmetrics, such as BLEU, METEOR and ROUGE, are also not well correlated. The\nnewer SPICE and CIDEr metrics are better correlated, but have traditionally\nbeen hard to optimize for. In this paper, we show how to use a policy gradient\n(PG) method to directly optimize a linear combination of SPICE and CIDEr (a\ncombination we call SPIDEr): the SPICE score ensures our captions are\nsemantically faithful to the image, while CIDEr score ensures our captions are\nsyntactically fluent. The PG method we propose improves on the prior MIXER\napproach, by using Monte Carlo rollouts instead of mixing MLE training with PG.\nWe show empirically that our algorithm leads to easier optimization and\nimproved results compared to MIXER. Finally, we show that using our PG method\nwe can optimize any of the metrics, including the proposed SPIDEr metric which\nresults in image captions that are strongly preferred by human raters compared\nto captions generated by the same model but trained to optimize MLE or the COCO\nmetrics.", "published": "2016-12-01 18:10:48", "link": "http://arxiv.org/abs/1612.00370v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Temporal Attention-Gated Model for Robust Sequence Classification", "abstract": "Typical techniques for sequence classification are designed for\nwell-segmented sequences which have been edited to remove noisy or irrelevant\nparts. Therefore, such methods cannot be easily applied on noisy sequences\nexpected in real-world applications. In this paper, we present the Temporal\nAttention-Gated Model (TAGM) which integrates ideas from attention models and\ngated recurrent networks to better deal with noisy or unsegmented sequences.\nSpecifically, we extend the concept of attention model to measure the relevance\nof each observation (time step) of a sequence. We then use a novel gated\nrecurrent network to learn the hidden representation for the final prediction.\nAn important advantage of our approach is interpretability since the temporal\nattention weights provide a meaningful value for the salience of each time step\nin the sequence. We demonstrate the merits of our TAGM approach, both for\nprediction accuracy and interpretability, on three different tasks: spoken\ndigit recognition, text-based sentiment analysis and visual event recognition.", "published": "2016-12-01 19:11:24", "link": "http://arxiv.org/abs/1612.00385v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Bootstrapping incremental dialogue systems: using linguistic knowledge\n  to learn from minimal data", "abstract": "We present a method for inducing new dialogue systems from very small amounts\nof unannotated dialogue data, showing how word-level exploration using\nReinforcement Learning (RL), combined with an incremental and semantic grammar\n- Dynamic Syntax (DS) - allows systems to discover, generate, and understand\nmany new dialogue variants. The method avoids the use of expensive and\ntime-consuming dialogue act annotations, and supports more natural\n(incremental) dialogues than turn-based systems. Here, language generation and\ndialogue management are treated as a joint decision/optimisation problem, and\nthe MDP model for RL is constructed automatically. With an implemented system,\nwe show that this method enables a wide range of dialogue variations to be\nautomatically captured, even when the system is trained from only a single\ndialogue. The variants include question-answer pairs, over- and\nunder-answering, self- and other-corrections, clarification interaction,\nsplit-utterances, and ellipsis. This generalisation property results from the\nstructural knowledge and constraints present within the DS grammar, and\nhighlights some limitations of recent systems built using machine learning\ntechniques only.", "published": "2016-12-01 16:49:04", "link": "http://arxiv.org/abs/1612.00347v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Piecewise Latent Variables for Neural Variational Text Processing", "abstract": "Advances in neural variational inference have facilitated the learning of\npowerful directed graphical models with continuous latent variables, such as\nvariational autoencoders. The hope is that such models will learn to represent\nrich, multi-modal latent factors in real-world data, such as natural language\ntext. However, current models often assume simplistic priors on the latent\nvariables - such as the uni-modal Gaussian distribution - which are incapable\nof representing complex latent factors efficiently. To overcome this\nrestriction, we propose the simple, but highly flexible, piecewise constant\ndistribution. This distribution has the capacity to represent an exponential\nnumber of modes of a latent target distribution, while remaining mathematically\ntractable. Our results demonstrate that incorporating this new latent\ndistribution into different models yields substantial improvements in natural\nlanguage processing tasks such as document modeling and natural language\ngeneration for dialogue.", "published": "2016-12-01 18:49:23", "link": "http://arxiv.org/abs/1612.00377v4", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "I.5.1; I.2.7"], "primary_category": "cs.CL"}
