{"title": "Probabilistic Token Alignment for Large Language Model Fusion", "abstract": "Training large language models (LLMs) from scratch can yield models with\nunique functionalities and strengths, but it is costly and often leads to\nredundant capabilities. A more cost-effective alternative is to fuse existing\npre-trained LLMs with different architectures into a more powerful model.\nHowever, a key challenge in existing model fusion is their dependence on\nmanually predefined vocabulary alignment, which may not generalize well across\ndiverse contexts, leading to performance degradation in several evaluation. To\nsolve this, we draw inspiration from distribution learning and propose the\nprobabilistic token alignment method as a general and soft mapping for\nalignment, named as PTA-LLM. Our approach innovatively reformulates token\nalignment into a classic mathematical problem: optimal transport, seamlessly\nleveraging distribution-aware learning to facilitate more coherent model\nfusion. Apart from its inherent generality, PTA-LLM exhibits interpretability\nfrom a distributional perspective, offering insights into the essence of the\ntoken alignment. Empirical results demonstrate that probabilistic token\nalignment enhances the target model's performance across multiple capabilities.\nOur code is avaliable at https://runjia.tech/neurips_pta-llm/.", "published": "2025-09-21 23:18:24", "link": "http://arxiv.org/abs/2509.17276v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Extending Automatic Machine Translation Evaluation to Book-Length Documents", "abstract": "Despite Large Language Models (LLMs) demonstrating superior translation\nperformance and long-context capabilities, evaluation methodologies remain\nconstrained to sentence-level assessment due to dataset limitations, token\nnumber restrictions in metrics, and rigid sentence boundary requirements. We\nintroduce SEGALE, an evaluation scheme that extends existing automatic metrics\nto long-document translation by treating documents as continuous text and\napplying sentence segmentation and alignment methods. Our approach enables\npreviously unattainable document-level evaluation, handling translations of\narbitrary length generated with document-level prompts while accounting for\nunder-/over-translations and varied sentence boundaries. Experiments show our\nscheme significantly outperforms existing long-form document evaluation\nschemes, while being comparable to evaluations performed with groundtruth\nsentence alignments. Additionally, we apply our scheme to book-length texts and\nnewly demonstrate that many open-weight LLMs fail to effectively translate\ndocuments at their reported maximum context lengths.", "published": "2025-09-21 21:46:58", "link": "http://arxiv.org/abs/2509.17249v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Agents Judge Systematic Reviews Like Humans? Evaluating SLRs with LLM-based Multi-Agent System", "abstract": "Systematic Literature Reviews (SLRs) are foundational to evidence-based\nresearch but remain labor-intensive and prone to inconsistency across\ndisciplines. We present an LLM-based SLR evaluation copilot built on a\nMulti-Agent System (MAS) architecture to assist researchers in assessing the\noverall quality of the systematic literature reviews. The system automates\nprotocol validation, methodological assessment, and topic relevance checks\nusing a scholarly database. Unlike conventional single-agent methods, our\ndesign integrates a specialized agentic approach aligned with PRISMA guidelines\nto support more structured and interpretable evaluations. We conducted an\ninitial study on five published SLRs from diverse domains, comparing system\noutputs to expert-annotated PRISMA scores, and observed 84% agreement. While\nearly results are promising, this work represents a first step toward scalable\nand accurate NLP-driven systems for interdisciplinary workflows and reveals\ntheir capacity for rigorous, domain-agnostic knowledge aggregation to\nstreamline the review process.", "published": "2025-09-21 21:17:23", "link": "http://arxiv.org/abs/2509.17240v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "primary_category": "cs.AI"}
{"title": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with RoE", "abstract": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction.To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters.", "published": "2025-09-21 21:05:29", "link": "http://arxiv.org/abs/2509.17238v1", "categories": ["cs.AI", "cs.CL", "cs.ET", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Causal Representation Learning from Multimodal Clinical Records under Non-Random Modality Missingness", "abstract": "Clinical notes contain rich patient information, such as diagnoses or\nmedications, making them valuable for patient representation learning. Recent\nadvances in large language models have further improved the ability to extract\nmeaningful representations from clinical texts. However, clinical notes are\noften missing. For example, in our analysis of the MIMIC-IV dataset, 24.5% of\npatients have no available discharge summaries. In such cases, representations\ncan be learned from other modalities such as structured data, chest X-rays, or\nradiology reports. Yet the availability of these modalities is influenced by\nclinical decision-making and varies across patients, resulting in modality\nmissing-not-at-random (MMNAR) patterns. We propose a causal representation\nlearning framework that leverages observed data and informative missingness in\nmultimodal clinical records. It consists of: (1) an MMNAR-aware modality fusion\ncomponent that integrates structured data, imaging, and text while conditioning\non missingness patterns to capture patient health and clinician-driven\nassignment; (2) a modality reconstruction component with contrastive learning\nto ensure semantic sufficiency in representation learning; and (3) a multitask\noutcome prediction model with a rectifier that corrects for residual bias from\nspecific modality observation patterns. Comprehensive evaluations across\nMIMIC-IV and eICU show consistent gains over the strongest baselines, achieving\nup to 13.8% AUC improvement for hospital readmission and 13.1% for ICU\nadmission.", "published": "2025-09-21 20:34:52", "link": "http://arxiv.org/abs/2509.17228v1", "categories": ["cs.LG", "cs.CL", "stat.ME"], "primary_category": "cs.LG"}
{"title": "Prompt-Based Simplification for Plain Language using Spanish Language Models", "abstract": "This paper describes the participation of HULAT-UC3M in CLEARS 2025 Subtask\n1: Adaptation of Text to Plain Language (PL) in Spanish. We explored strategies\nbased on models trained on Spanish texts, including a zero-shot configuration\nusing prompt engineering and a fine-tuned version with Low-Rank Adaptation\n(LoRA). Different strategies were evaluated on representative internal subsets\nof the training data, using the official task metrics, cosine similarity (SIM)\nand the Fern\\'andez-Huerta readability index (FH) to guide the selection of the\noptimal model and prompt combination. The final system was selected for its\nbalanced and consistent performance, combining normalization steps, the\nRigoChat-7B-v2 model, and a dedicated PL-oriented prompt. It ranked first in\nsemantic similarity (SIM = 0.75), however, fourth in readability (FH = 69.72).\nWe also discuss key challenges related to training data heterogeneity and the\nlimitations of current evaluation metrics in capturing both linguistic clarity\nand content preservation.", "published": "2025-09-21 19:28:37", "link": "http://arxiv.org/abs/2509.17209v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evolution of Concepts in Language Model Pre-Training", "abstract": "Language models obtain extensive capabilities through pre-training. However,\nthe pre-training process remains a black box. In this work, we track linear\ninterpretable feature evolution across pre-training snapshots using a sparse\ndictionary learning method called crosscoders. We find that most features begin\nto form around a specific point, while more complex patterns emerge in later\ntraining stages. Feature attribution analyses reveal causal connections between\nfeature evolution and downstream performance. Our feature-level observations\nare highly consistent with previous findings on Transformer's two-stage\nlearning process, which we term a statistical learning phase and a feature\nlearning phase. Our work opens up the possibility to track fine-grained\nrepresentation progress during language model learning dynamics.", "published": "2025-09-21 18:53:12", "link": "http://arxiv.org/abs/2509.17196v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery", "abstract": "Analyzing cultural-heritage artifacts remains challenging for MLLMs: general\nmodels lack domain expertise, and SFT often overfits superficial patterns,\nyielding brittle reasoning for authentication and historical attribution. This\nraises the question of how to equip MLLMs with robust, expert-level reasoning\nfor ancient Greek pottery. We present VaseVL, an SFT-then-RL system that turns\nevaluation into supervision: we construct a taxonomy of question types, probe\nthe SFT model to localize type-specific performance gaps, and optimize with\ntype-conditioned, compositionality-oriented rewards targeting those gaps. We\nalso release VaseVQA, a comprehensive benchmark of 31,773 images designed to\nprobe deep understanding. Experiments show state-of-the-art results on style\nclassification and historical attribution with marked gains in compositional\nrobustness over SFT-only baselines, validating diagnosis-guided,\ntaxonomy-conditioned reward engineering and providing a reusable resource for\nfuture research. Code and dataset will be available at\nhttps://github.com/AIGeeksGroup/VaseVQA.", "published": "2025-09-21 18:36:54", "link": "http://arxiv.org/abs/2509.17191v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "LifeAlign: Lifelong Alignment for Large Language Models with Memory-Augmented Focalized Preference Optimization", "abstract": "Alignment plays a crucial role in Large Language Models (LLMs) in aligning\nwith human preferences on a specific task/domain. Traditional alignment methods\nsuffer from catastrophic forgetting, where models lose previously acquired\nknowledge when adapting to new preferences or domains. We introduce LifeAlign,\na novel framework for lifelong alignment that enables LLMs to maintain\nconsistent human preference alignment across sequential learning tasks without\nforgetting previously learned knowledge. Our approach consists of two key\ninnovations. First, we propose a focalized preference optimization strategy\nthat aligns LLMs with new preferences while preventing the erosion of knowledge\nacquired from previous tasks. Second, we develop a short-to-long memory\nconsolidation mechanism that merges denoised short-term preference\nrepresentations into stable long-term memory using intrinsic dimensionality\nreduction, enabling efficient storage and retrieval of alignment patterns\nacross diverse domains. We evaluate LifeAlign across multiple sequential\nalignment tasks spanning different domains and preference types. Experimental\nresults demonstrate that our method achieves superior performance in\nmaintaining both preference alignment quality and knowledge retention compared\nto existing lifelong learning approaches. The codes and datasets will be\nreleased on GitHub.", "published": "2025-09-21 18:06:05", "link": "http://arxiv.org/abs/2509.17183v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Attention Consistency for LLMs Explanation", "abstract": "Understanding the decision-making processes of large language models (LLMs)\nis essential for their trustworthy development and deployment. However, current\ninterpretability methods often face challenges such as low resolution and high\ncomputational cost. To address these limitations, we propose the\n\\textbf{Multi-Layer Attention Consistency Score (MACS)}, a novel, lightweight,\nand easily deployable heuristic for estimating the importance of input tokens\nin decoder-based models. MACS measures contributions of input tokens based on\nthe consistency of maximal attention. Empirical evaluations demonstrate that\nMACS achieves a favorable trade-off between interpretability quality and\ncomputational efficiency, showing faithfulness comparable to complex techniques\nwith a 22\\% decrease in VRAM usage and 30\\% reduction in latency.", "published": "2025-09-21 17:55:30", "link": "http://arxiv.org/abs/2509.17178v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions", "abstract": "We conduct a moderate-scale contamination-free (to some extent) evaluation of\ncurrent large reasoning models (LRMs) with some preliminary findings. We also\nrelease ROME, our evaluation benchmark for vision language models intended to\ntest reasoning from visual clues. We attach links to the benchmark, evaluation\ndata, and other updates on this website:\nhttps://flageval-baai.github.io/LRM-Eval/", "published": "2025-09-21 17:53:30", "link": "http://arxiv.org/abs/2509.17177v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SFT-TA: Supervised Fine-Tuned Agents in Multi-Agent LLMs for Automated Inductive Thematic Analysis", "abstract": "Thematic Analysis (TA) is a widely used qualitative method that provides a\nstructured yet flexible framework for identifying and reporting patterns in\nclinical interview transcripts. However, manual thematic analysis is\ntime-consuming and limits scalability. Recent advances in LLMs offer a pathway\nto automate thematic analysis, but alignment with human results remains\nlimited. To address these limitations, we propose SFT-TA, an automated thematic\nanalysis framework that embeds supervised fine-tuned (SFT) agents within a\nmulti-agent system. Our framework outperforms existing frameworks and the\ngpt-4o baseline in alignment with human reference themes. We observed that SFT\nagents alone may underperform, but achieve better results than the baseline\nwhen embedded within a multi-agent system. Our results highlight that embedding\nSFT agents in specific roles within a multi-agent system is a promising pathway\nto improve alignment with desired outputs for thematic analysis.", "published": "2025-09-21 17:26:58", "link": "http://arxiv.org/abs/2509.17167v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distribution Testing in the Presence of Arbitrarily Dominant Noise with Verification Queries", "abstract": "We study distribution testing without direct access to a source of relevant\ndata, but rather to one where only a tiny fraction is relevant. To enable this,\nwe introduce the following verification query model. The goal is to perform a\nstatistical task on distribution $\\boldsymbol{p}$ given sample access to a\nmixture $\\boldsymbol{r} = \\lambda \\boldsymbol{p} + (1-\\lambda)\\boldsymbol{q}$\nand the ability to query whether a sample was generated by $\\boldsymbol{p}$ or\nby $\\boldsymbol{q}$. In general, if $m_0$ samples from $\\boldsymbol{p}$ suffice\nfor a task, then $O(m_0/\\lambda)$ samples and queries always suffice in our\nmodel. Are there tasks for which the number of queries can be significantly\nreduced?\n  We study the canonical problems in distribution testing, and obtain matching\nupper and lower bounds that reveal smooth trade-offs between sample and query\ncomplexity. For all $m \\leq n$, we obtain (i) a uniformity and identity tester\nusing $O(m + \\frac{\\sqrt{n}}{\\varepsilon^2 \\lambda})$ samples and $O(\\frac{n}{m\n\\varepsilon^4 \\lambda^2})$ queries, and (ii) a closeness tester using $O(m +\n\\frac{n^{2/3}}{\\varepsilon^{4/3} \\lambda} + \\frac{1}{\\varepsilon^4 \\lambda^3})$\nsamples and $O(\\frac{n^2}{m^2 \\varepsilon^4 \\lambda^3})$ queries. Moreover, we\nshow that these query complexities are tight for all testers using $m \\ll n$\nsamples.\n  Next, we show that for testing closeness using $m =\n\\widetilde{O}(\\frac{n}{\\varepsilon^2\\lambda})$ samples we can achieve query\ncomplexity $\\widetilde{O}(\\frac{1}{\\varepsilon^2\\lambda})$ which is nearly\noptimal even for the basic task of bias estimation with unbounded samples. Our\nuniformity testers work in the more challenging setting where the contaminated\nsamples are generated by an adaptive adversary (at the cost of a $\\log n$\nfactor). Finally, we show that our lower bounds can be circumvented if the\nalgorithm is provided with the PDF of the mixture.", "published": "2025-09-21 23:02:04", "link": "http://arxiv.org/abs/2509.17269v1", "categories": ["cs.DS", "cs.DM"], "primary_category": "cs.DS"}
{"title": "Distance Approximating Minors for Planar and Minor-Free Graphs", "abstract": "Given an edge-weighted graph $G$ and a subset of vertices $T$ called\nterminals, an $\\alpha$-distance-approximating minor ($\\alpha$-DAM) of $G$ is a\ngraph minor $H$ of $G$ that contains all terminals, such that the distance\nbetween every pair of terminals is preserved up to a factor of $\\alpha$.\nDistance-approximating minor would be an effective distance-sketching structure\non minor-closed family of graphs; in the constant-stretch regime it generalizes\nthe well-known Steiner Point Removal problem by allowing the existence of (a\nsmall number of) non-terminal vertices. Unfortunately, in the $(1+\\varepsilon)$\nregime the only known DAM construction for planar graphs relies on overlaying\n$\\tilde{O}_\\varepsilon(|T|)$ shortest paths in $G$, which naturally leads to a\nquadratic bound in the number of terminals [Cheung, Goranci, and Henzinger,\nICALP 2016].\n  We break the quadratic barrier and build the first\n$(1+\\varepsilon)$-distance-approximating minor for $k$-terminal planar graphs\nand minor-free graphs of near-linear size $\\tilde{O}_\\varepsilon(k)$. In\naddition to the near-optimality in size, the construction relies only on the\nexistence of shortest-path separators [Abraham and Gavoille, PODC 2006] and\n$\\varepsilon$-covers [Thorup, J.\\ ACM 2004]. Consequently, this provides an\nalternative and simpler construction to the near-linear-size emulator for\nplanar graphs [Chang, Krauthgamer, and Tan, STOC 2022], as well as the first\nnear-linear-size emulator for minor-free graphs. Our DAM can be constructed in\nnear-linear time.", "published": "2025-09-21 20:15:26", "link": "http://arxiv.org/abs/2509.17226v1", "categories": ["cs.DS", "cs.CG", "cs.DM"], "primary_category": "cs.DS"}
{"title": "Eccentric Connectivity Index of Strongly Connected Digraphs", "abstract": "Let $G = (V, E)$ be a graph with non-empty set of vertices $V$ and set of\nedges $E$. The \\emph{eccentric connectivity index} of the graph $G$ is defined\nas $$\\displaystyle{\\xi^C(G) = \\sum_{u \\in V} d_u \\;ecc(u)}$$ where $d_u$ is the\ndegree and $ecc(u)$ is the eccentricity of the vertex $u \\in V$. This article\nis an attempt to find the \\emph{eccentric connectivity index} of strongly\nconnected digraph $D$ with respect to the metric, \\textit{maximum distance}\ndefined by $md(u,v)=\\max\\{\\vec{d}(u,v),\\vec{d}(v,u)\\}$. An attempt is also made\nto find the extremal values for strongly connected digraphs.", "published": "2025-09-21 10:27:30", "link": "http://arxiv.org/abs/2509.17019v1", "categories": ["math.CO", "cs.DM", "05C12, 05C35, 05C90"], "primary_category": "math.CO"}
{"title": "Identifying and Upweighting Power-Niche Users to Mitigate Popularity Bias in Recommendations", "abstract": "Recommender systems have been shown to exhibit popularity bias by\nover-recommending popular items and under-recommending relevant niche items. We\nseek to understand interactions with niche items in benchmark recommendation\ndatasets as a step toward mitigating popularity bias. We find that, compared to\nmainstream users, niche-preferring users exhibit a longer-tailed activity-level\ndistribution, indicating the existence of users who both prefer niche items and\nexhibit high activity levels. We partition users along two axes: (1) activity\nlevel (\"power\" vs. \"light\") and (2) item-popularity preference (\"mainstream\"\nvs. \"niche\"), and show that in several benchmark datasets, the number of\npower-niche users (high activity and niche preference) is statistically\nsignificantly larger than expected under a null configuration model. Motivated\nby this observation, we propose a framework for reweighting the Bayesian\nPersonalized Ranking (BPR) loss that simultaneously reweights based on user\nactivity level and item popularity. Our method introduces two interpretable\nparameters: one controlling the significance of user activity level, and the\nother of item popularity. Experiments on benchmark datasets show that\nupweighting power-niche users reduces popularity bias and can increase overall\nperformance. In contrast to previous work that only considers user activity\nlevel or item popularity in isolation, our results suggest that considering\ntheir interaction leads to Pareto-dominant performance.", "published": "2025-09-21 22:41:07", "link": "http://arxiv.org/abs/2509.17265v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "RALLM-POI: Retrieval-Augmented LLM for Zero-shot Next POI Recommendation with Geographical Reranking", "abstract": "Next point-of-interest (POI) recommendation predicts a user's next\ndestination from historical movements. Traditional models require intensive\ntraining, while LLMs offer flexible and generalizable zero-shot solutions but\noften generate generic or geographically irrelevant results due to missing\ntrajectory and spatial context. To address these issues, we propose RALLM-POI,\na framework that couples LLMs with retrieval-augmented generation and\nself-rectification. We first propose a Historical Trajectory Retriever (HTR)\nthat retrieves relevant past trajectories to serve as contextual references,\nwhich are then reranked by a Geographical Distance Reranker (GDR) for\nprioritizing spatially relevant trajectories. Lastly, an Agentic LLM Rectifier\n(ALR) is designed to refine outputs through self-reflection. Without additional\ntraining, RALLM-POI achieves substantial accuracy gains across three real-world\nFoursquare datasets, outperforming both conventional and LLM-based baselines.\nCode is released at https://github.com/LKRcrocodile/RALLM-POI.", "published": "2025-09-21 12:52:28", "link": "http://arxiv.org/abs/2509.17066v1", "categories": ["cs.AI", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Equip Pre-ranking with Target Attention by Residual Quantization", "abstract": "The pre-ranking stage in industrial recommendation systems faces a\nfundamental conflict between efficiency and effectiveness. While powerful\nmodels like Target Attention (TA) excel at capturing complex feature\ninteractions in the ranking stage, their high computational cost makes them\ninfeasible for pre-ranking, which often relies on simplistic vector-product\nmodels. This disparity creates a significant performance bottleneck for the\nentire system. To bridge this gap, we propose TARQ, a novel pre-ranking\nframework. Inspired by generative models, TARQ's key innovation is to equip\npre-ranking with an architecture approximate to TA by Residual Quantization.\nThis allows us to bring the modeling power of TA into the latency-critical\npre-ranking stage for the first time, establishing a new state-of-the-art\ntrade-off between accuracy and efficiency. Extensive offline experiments and\nlarge-scale online A/B tests at Taobao demonstrate TARQ's significant\nimprovements in ranking performance. Consequently, our model has been fully\ndeployed in production, serving tens of millions of daily active users and\nyielding substantial business improvements.", "published": "2025-09-21 05:33:28", "link": "http://arxiv.org/abs/2509.16931v1", "categories": ["cs.IR", "cs.AI", "cs.LG", "I.2.0; I.5.0; I.7.0"], "primary_category": "cs.IR"}
{"title": "Temporal-Aware User Behaviour Simulation with Large Language Models for Recommender Systems", "abstract": "Large Language Models (LLMs) demonstrate human-like capabilities in language\nunderstanding, reasoning, and generation, driving interest in using LLM-based\nagents to simulate human feedback in recommender systems. However, most\nexisting approaches rely on static user profiling, neglecting the temporal and\ndynamic nature of user interests. This limitation stems from a disconnect\nbetween language modelling and behaviour modelling, which constrains the\ncapacity of agents to represent sequential patterns. To address this challenge,\nwe propose a Dynamic Temporal-aware Agent-based simulator for Recommender\nSystems, DyTA4Rec, which enables agents to model and utilise evolving user\nbehaviour based on historical interactions. DyTA4Rec features a dynamic updater\nfor real-time profile refinement, temporal-enhanced prompting for sequential\ncontext, and self-adaptive aggregation for coherent feedback. Experimental\nresults at group and individual levels show that DyTA4Rec significantly\nimproves the alignment between simulated and actual user behaviour by modelling\ndynamic characteristics and enhancing temporal awareness in LLM-based agents.", "published": "2025-09-21 03:10:02", "link": "http://arxiv.org/abs/2509.16895v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Privacy-Preserving State Estimation with Crowd Sensors: An Information-Theoretic Respective", "abstract": "Privacy-preserving state estimation for linear time-invariant dynamical\nsystems with crowd sensors is considered. At any time step, the estimator has\naccess to measurements from a randomly selected sensor from a pool of sensors\nwith pre-specified models and noise profiles. A Luenberger-like observer is\nused to fuse the measurements with the underlying model of the system to\nrecursively generate the state estimates. An additive privacy-preserving noise\nis used to constrain information leakage. Information leakage is measured via\nmutual information between the identity of the sensors and the state estimate\nconditioned on the actual state of the system. This captures an omnipotent\nadversary that not only can access state estimates but can also gather direct\nhigh-quality state measurements. Any prescribed level of information leakage is\nshown to be achievable by appropriately selecting the variance of the\nprivacy-preserving noise. Therefore, privacy-utility trade-off can be\nfine-tuned.", "published": "2025-09-21 22:44:34", "link": "http://arxiv.org/abs/2509.17266v1", "categories": ["cs.CR", "cs.IT", "cs.SY", "eess.SY", "math.IT"], "primary_category": "cs.CR"}
{"title": "Fundamental Mechanisms of Human Learning", "abstract": "Learning underlies nearly all human behavior and is central to education and\neducation reform. Although recent advances in neuroscience have revealed the\nfundamental structure of learning processes, these insights have yet to be\nintegrated into research and practice. Specifically, neuroscience has found\nthat decision-making is governed by a structured process of perception,\naction-selection, and execution, supported by multiple neural systems with\ndistinct memory stores and learning mechanisms. These systems extract different\ntypes of information (categorical, predictive, structural, and sequential)\nchallenging canonical models of memory used in learning and behavioral science\nresearch by providing a mechanistic account of how humans acquire and use\nknowledge. Because each system learns differently, effective teaching requires\nalignment with system-specific processes. We propose a unified model that\nintegrates these neuroscientific insights, bridging basic mechanisms with\noutcomes in education, identity, belonging, and wellbeing. By translating first\nprinciples of neural information processing into a generalizable framework,\nthis work advances theories of skill acquisition and transfer while\nestablishing a foundation for interdisciplinary research to refine how learning\nis understood and supported across domains of human behavior.", "published": "2025-09-21 19:17:13", "link": "http://arxiv.org/abs/2509.17202v1", "categories": ["cs.IT", "cs.HC", "math.IT"], "primary_category": "cs.IT"}
{"title": "Communication over LQG Control Systems: A Convex Optimization Approach to Capacity", "abstract": "We study communication over control systems, where a controller-encoder\nselects inputs to a dynamical system in order to simultaneously regulate the\nsystem and convey a message to an observer that has access to the system's\noutput measurements. This setup reflects implicit communication, as the\ncontroller embeds a message in the control signal. The capacity of a control\nsystem is the maximal reliable rate of the embedded message subject to a\nclosed-loop control-cost constraint. We focus on linear quadratic Gaussian\n(LQG) control systems, in which the dynamical system is given by a state-space\nmodel with Gaussian noise, and the control cost is a quadratic function of the\nsystem inputs and system states. Our main result is a convex optimization upper\nbound on the capacity of LQG systems. In the case of scalar systems, we prove\nthat the upper bound yields the exact LQG system capacity. The upper bound also\nrecovers all known results, including LQG control, feedback capacity of\nGaussian channels with memory, and the LQG system capacity with a\nstate-feedback. For vector LQG control systems, we provide a sufficient\ncondition for tightness of the upper bound, based on the Riccati equation.\nNumerical simulations indicate the upper bound tightness in all tested\nexamples, suggesting that the upper bound may be equal to the LQG system\ncapacity in the vector case as well.", "published": "2025-09-21 09:44:56", "link": "http://arxiv.org/abs/2509.17002v1", "categories": ["cs.IT", "cs.SY", "eess.SY", "math.IT", "math.OC"], "primary_category": "cs.IT"}
{"title": "Further results on bent partitions", "abstract": "Bent partitions of $V_{n}^{(p)}$ play an important role in constructing\n(vectorial) bent functions, partial difference sets, and association schemes,\nwhere $V_{n}^{(p)}$ denotes an $n$-dimensional vector space over the finite\nfield $\\mathbb{F}_{p}$, $n$ is an even positive integer, and $p$ is a prime.\nFor bent partitions, there remains a challenging open problem: Whether the\ndepth of any bent partition of $V_{n}^{(p)}$ is always a power of $p$. Notably,\nthe depths of all current known bent partitions of $V_{n}^{(p)}$ are powers of\n$p$. In this paper, we prove that for a bent partition $\\Gamma$ of\n$V_{n}^{(p)}$ for which all the $p$-ary bent functions generated by $\\Gamma$\nare regular or all are weakly regular but not regular, the depth of $\\Gamma$\nmust be a power of $p$. We present new constructions of bent partitions that\n(do not) correspond to vectorial dual-bent functions. In particular, a new\nconstruction of vectorial dual-bent functions is provided. Additionally, for\ngeneral bent partitions of $V_{n}^{(2)}$, we establish a characterization in\nterms of Hadamard matrices.", "published": "2025-09-21 04:18:43", "link": "http://arxiv.org/abs/2509.16911v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Quantum State Tomography for Tensor Networks in Two Dimensions", "abstract": "Recent work has shown that for one-dimensional quantum states that can be\neffectively approximated by matrix product operators (MPOs), a polynomial\nnumber of copies of the state suffices for reconstruction. Compared to MPOs in\none dimension, projected entangled-pair states (PEPSs) and projected\nentangled-pair operators (PEPOs), which represent typical low-dimensional\nstructures in two dimensions, are more prevalent as a looped tensor network.\nHowever, a formal analysis of the sample complexity required for estimating\nPEPS or PEPO has yet to be established. In this paper, we aim to address this\ngap by providing theoretical guarantees for the stable recovery of PEPS and\nPEPO. Our analysis primarily focuses on two quantum measurement schemes: $(i)$\ninformationally complete positive operator valued measures (IC-POVMs),\nspecifically the spherical $t$-designs ($t \\geq 3$), and $(ii)$ projective\nrank-one measurements, in particular Haar random projective measurements. We\nfirst establish stable embeddings for PEPSs (or PEPOs) to ensure that the\ninformation contained in the states can be preserved under these two\nmeasurement schemes. We then show that a constrained least-squares estimator\nachieves stable recovery for PEPSs (or PEPOs), with the recovery error bounded\nwhen the number of state copies scales linearly under spherical $t$-designs and\npolynomially under Haar-random projective measurements with respect to the\nnumber of qudits. These results provide theoretical support for the reliable\nuse of PEPS and PEPO in practical quantum information processing.", "published": "2025-09-21 00:42:29", "link": "http://arxiv.org/abs/2509.16852v1", "categories": ["quant-ph", "cs.IT", "eess.SP", "math.IT"], "primary_category": "quant-ph"}
{"title": "Analysis of the Impact of an Execution Algorithm with an Order Book Imbalance Strategy on a Financial Market Using an Agent-based Simulation", "abstract": "Order book imbalance (OBI) - buy orders minus sell orders near the best quote\n- measures supply-demand imbalance that can move prices. OBI is positively\ncorrelated with returns, and some investors try to use it to improve\nperformance. Large orders placed at once can reveal intent, invite\nfront-running, raise volatility, and cause losses. Execution algorithms\ntherefore split parent orders into smaller lots to limit price distortion. In\nprinciple, using OBI inside such algorithms could improve execution, but prior\nevidence is scarce because isolating OBI's effect in real markets is nearly\nimpossible amid many external factors.\n  Multi-agent simulation offers a way to study this. In an artificial market,\nindividual actors are agents whose rules and interactions form the model. This\nstudy builds an execution algorithm that accounts for OBI, tests it across\nseveral market patterns in artificial markets, and analyzes mechanisms,\ncomparing it with a conventional (OBI-agnostic) algorithm.\n  Results: (i) In stable markets, the OBI strategy's performance depends on the\nnumber of order slices; outcomes vary with how the parent order is partitioned.\n(ii) In markets with unstable prices, the OBI-based algorithm outperforms the\nconventional approach. (iii) Under spoofing manipulation, the OBI strategy is\nnot significantly worse than the conventional algorithm, indicating limited\nvulnerability to spoofing.\n  Overall, OBI provides a useful signal for execution. Incorporating OBI can\nadd value - especially in volatile conditions - while remaining reasonably\nrobust to spoofing; in calm markets, benefits are sensitive to slicing design.", "published": "2025-09-21 04:23:02", "link": "http://arxiv.org/abs/2509.16912v1", "categories": ["q-fin.CP", "cs.GT", "cs.MA"], "primary_category": "q-fin.CP"}
{"title": "On efficient block Krylov-solvers for $\\mathcal H^2$-matrices", "abstract": "Hierarchical matrices provide a highly memory-efficient way of storing dense\nlinear operators arising, for example, from boundary element methods,\nparticularly when stored in the H^2 format. In such data-sparse\nrepresentations, iterative solvers are preferred over direct ones due to the\ncost-efficient matrix-vector multiplications they enable. Solving multiple\nsystems of linear equations with the same hierarchical matrix naturally leads\nto block methods, which in turn make heavy use of BLAS level-3 functions such\nas GEMM. We present an efficient implementation of H^2-matrix-vector and\nH^2-matrix-matrix multiplication that fully exploits the potential of modern\nhardware in terms of memory and cache utilization. The latter is employed to\naccelerate block Krylov subspace methods, which we present later as the main\nresults of this paper.", "published": "2025-09-21 22:14:56", "link": "http://arxiv.org/abs/2509.17257v1", "categories": ["math.NA", "cs.NA", "65F55, 65F08, 65F10"], "primary_category": "math.NA"}
{"title": "Data-efficient Kernel Methods for Learning Hamiltonian Systems", "abstract": "Hamiltonian dynamics describe a wide range of physical systems. As such,\ndata-driven simulations of Hamiltonian systems are important for many\nscientific and engineering problems. In this work, we propose kernel-based\nmethods for identifying and forecasting Hamiltonian systems directly from data.\nWe present two approaches: a two-step method that reconstructs trajectories\nbefore learning the Hamiltonian, and a one-step method that jointly infers\nboth. Across several benchmark systems, including mass-spring dynamics, a\nnonlinear pendulum, and the Henon-Heiles system, we demonstrate that our\nframework achieves accurate, data-efficient predictions and outperforms\ntwo-step kernel-based baselines, particularly in scarce-data regimes, while\npreserving the conservation properties of Hamiltonian dynamics. Moreover, our\nmethodology provides theoretical a priori error estimates, ensuring reliability\nof the learned models. We also provide a more general, problem-agnostic\nnumerical framework that goes beyond Hamiltonian systems and can be used for\ndata-driven learning of arbitrary dynamical systems.", "published": "2025-09-21 16:50:17", "link": "http://arxiv.org/abs/2509.17154v1", "categories": ["math.NA", "cs.LG", "cs.NA", "math.DS", "stat.ML"], "primary_category": "math.NA"}
{"title": "Admissible convergence behavior and mirroring of stagnation in restarted (block) GMRES", "abstract": "In this work, we describe how to construct matrices and block right-hand\nsides the exhibit a specified restarted block GMRES convergence pattern, such\nthat the eigenvalues and Ritz values at each iteration can be chosen\nindependent of the specified convergence behavior. This work is a\ngeneralization of the work in [Meurant and Tebbens, Num. Alg. 2019] in which\nthe authors do the same for restarted non-block GMRES. We use the same tools as\nwere used in [Kub\\'inov\\'a and Soodhalter, SIMAX 2020], namely to analyze block\nGMRES as an iteration over a right vector space with scalars from the\n$^\\ast$-algebra of matrices. To facilitate our work, we also extend the work of\nMeurant and Tebbens and offer alternative proofs of some of their results, that\ncan be more easily generalized to the block setting.", "published": "2025-09-21 13:31:25", "link": "http://arxiv.org/abs/2509.17077v1", "categories": ["math.NA", "cs.NA", "65F10, 65N12, 15B57, 45B05, 45A05"], "primary_category": "math.NA"}
{"title": "Monte Carlo on a single sample", "abstract": "In this paper, we consider a Monte Carlo simulation method (MinMC) that\napproximates prices and risk measures for a range $\\Gamma$ of model parameters\nat once. The simulation method that we study has recently gained popularity\n[HS20, FPP22, BDG24], and we provide a theoretical framework and convergence\nrates for it. In particular, we show that sample-based approximations to\n$\\mathbb{E}_{\\theta}[X]$, where $\\theta$ denotes the model and\n$\\mathbb{E}_{\\theta}$ the expectation with respect to the distribution\n$P_\\theta$ of the model $\\theta$, can be obtained across all $\\theta \\in\n\\Gamma$ by minimizing a map $V:H\\rightarrow \\mathbb{R}$ with $H$ a suitable\nfunction space. The minimization can be achieved easily by fitting a standard\nfeedforward neural network with stochastic gradient descent. We show that\nMinMC, which uses only one sample for each model, significantly outperforms a\ntraditional Monte Carlo method performed for multiple values of $\\theta$, which\nare subsequently interpolated. Our case study suggests that MinMC might serve\nas a new benchmark for parameter-dependent Monte Carlo simulations, which\nappear not only in quantitative finance but also in many other areas of\nscientific computing.", "published": "2025-09-21 10:39:57", "link": "http://arxiv.org/abs/2509.17025v1", "categories": ["math.ST", "cs.NA", "math.NA", "math.PR", "stat.CO", "stat.TH", "91G60, 65C05"], "primary_category": "math.ST"}
{"title": "Multiscale solution decomposition of nonlocal-in-time problems with application in numerical computation", "abstract": "This work develops a multiscale solution decomposition (MSD) method for\nnonlocal-in-time problems to separate a series of known terms with multiscale\nsingularity from the original singular solution such that the remaining unknown\npart becomes smoother. We demonstrate that the MSD provides a scenario where\nthe smoothness assumption for solutions of weakly singular nonlocal-in-time\nproblems, a commonly encountered assumption in numerous literature of numerical\nmethods that is in general not true for original solutions, becomes appropriate\nsuch that abundant numerical analysis results therein become applicable. From\ncomputational aspect, instead of handling solution singularity, the MSD\nsignificantly reduces the numerical difficulties by separating and thus\ncircumventing the solution singularity. We consider typical problems, including\nthe fractional relaxation equation, Volterra integral equation, subdiffusion,\nintegrodifferential equation and diffusion-wave equation, to demonstrate the\nuniversality of MSD and its effectiveness in improving the numerical accuracy\nor stability in comparison with classical methods.", "published": "2025-09-21 10:28:36", "link": "http://arxiv.org/abs/2509.17020v1", "categories": ["math.NA", "cs.NA", "65M12, 65M60"], "primary_category": "math.NA"}
{"title": "Geometric Interpolation of Rigid Body Motions", "abstract": "The problem of interpolating a rigid body motion is to find a spatial\ntrajectory between a prescribed initial and terminal pose. Two variants of this\ninterpolation problem are addressed. The first is to find a solution that\nsatisfies initial conditions on the k-1 derivatives of the rigid body twist.\nThis is called the kth-order initial value trajectory interpolation problem\n(k-IV-TIP). The second is to find a solution that satisfies conditions on the\nrigid body twist and its k-1 derivatives at the initial and terminal pose. This\nis called the kth-order boundary value trajectory interpolation problem\n(k-BV-TIP). Solutions to the k-IV-TIP for k=1,...,4, i.e. the initial twist and\nup to the 4th time derivative are prescribed. Further, a solution to the\n1-IV-TBP is presented, i.e. the initial and terminal twist are prescribed. The\nlatter is a novel cubic interpolation between two spatial configurations with\ngiven initial and terminal twist. This interpolation is automatically identical\nto the minimum acceleration curve when the twists are set to zero. The general\napproach to derive higher-order solutions is presented. Numerical results are\nshown for two examples.", "published": "2025-09-21 07:55:35", "link": "http://arxiv.org/abs/2509.16966v1", "categories": ["cs.RO", "cs.NA", "math.DG", "math.GR", "math.NA", "math.OC"], "primary_category": "cs.RO"}
{"title": "Neural Network Dual Norms for Minimal Residual Finite Element Methods", "abstract": "Minimal-residual methods for PDEs with a residual in a dual space are\nnon-trivial to guarantee stability. We present a minimal-residual finite\nelement method in which the solution space is a standard finite element space,\nbut neural networks are used as test functions for the evaluation of residual\ndual norms. The use of a neural network improves the approximation of the\nresidual representer, and thereby improves the stability of the method. Our\nhybrid approach is implemented through a deep residual Uzawa algorithm that\nalternates finite element updates with neural network training. We prove\nconsistency and convergence results for the Uzawa methodology. We also prove an\na priori error estimate that relies on a suitable Fortin compatibility\ncondition. Numerical experiments on advection-reaction problems with singular\nor discontinuous data show that the proposed framework delivers robust and\naccurate approximations.", "published": "2025-09-21 07:47:36", "link": "http://arxiv.org/abs/2509.16961v1", "categories": ["math.NA", "cs.NA", "65N30, 65N12, 65N22, 65F10, 68T07"], "primary_category": "math.NA"}
{"title": "Numerical Reconstruction of Coefficients in Elliptic Equations Using Continuous Data Assimilation", "abstract": "We consider the numerical reconstruction of the spatially dependent\nconductivity coefficient and the source term in elliptic partial differential\nequations of in a two-dimensional convex polygonal domain, with the homogeneous\nDirichlet boundary condition and given interior observation of the solution.\nUsing data assimilation, some approximated gradients of our error functional\nare derived to update the reconstructed coefficients, and new $L^2$ error\nestimates for such minimization formulations are given for the spatially\ndiscretized reconstructions. Numerical examples are provided to show the\neffectiveness of the method and demonstrate the error estimates. The numerical\nresults also show that the reconstruction is very robust for the error in\ncertain inputted coefficients.", "published": "2025-09-21 07:25:25", "link": "http://arxiv.org/abs/2509.16954v1", "categories": ["math.NA", "cs.NA", "65N21 (Primary) 65N20 (Secondary)"], "primary_category": "math.NA"}
{"title": "A decoupled and structure-preserving direct discontinuous Galerkin method for the Keller-Segel Model", "abstract": "In this work, we develop a novel numerical scheme to solve the classical\nKeller--Segel (KS) model which simultaneously preserves its intrinsic\nmathematical structure and achieves optimal accuracy. The model is reformulated\ninto a gradient flow structure using the energy variational method, which\nreveals the inherent energy dissipative dynamics of the system. Based on this\nreformulation, we construct a structure-preserving discretization by\nsemi-implicit method in time and the direct discontinuous Galerkin (DDG) method\nin space, resulting in a stable and high-order accurate approximation. The\nproposed scheme enjoys several desirable properties: (i) energy stability,\nensuring discrete free energy dissipation; (ii) exact conservation of mass for\nthe cell density; (iii) positivity preservation of the numerical cell density,\nenforced via a carefully designed limiter; and (iv) optimal convergence rate,\nwith first-order accuracy in time and $(k+1)$-th order accuracy in space for\npolynomials of degree $k$. We provide rigorous theoretical analysis that\nsubstantiate these properties. In addition, extensive numerical experiments,\nincluding benchmark problems exhibiting pattern formation and near blow-up\nbehavior, are conducted to validate the theoretical results and demonstrate the\nrobustness, efficiency, and accuracy of the proposed method. The approach\noffers a flexible and reliable framework for structure-preserving numerical\nsimulation of chemotaxis models and other gradient flow-type systems.", "published": "2025-09-21 06:18:35", "link": "http://arxiv.org/abs/2509.16940v1", "categories": ["math.NA", "cs.NA", "65M60"], "primary_category": "math.NA"}
{"title": "Quantum Adaptive Self-Attention for Financial Rebalancing: An Empirical Study on Automated Market Makers in Decentralized Finance", "abstract": "We formulate automated market maker (AMM) \\emph{rebalancing} as a binary\ndetection problem and study a hybrid quantum--classical self-attention block,\n\\textbf{Quantum Adaptive Self-Attention (QASA)}. QASA constructs quantum\nqueries/keys/values via variational quantum circuits (VQCs) and applies\nstandard softmax attention over Pauli-$Z$ expectation vectors, yielding a\ndrop-in attention module for financial time-series decision making. Using daily\ndata for \\textbf{BTCUSDC} over \\textbf{Jan-2024--Jan-2025} with a 70/15/15\ntime-series split, we compare QASA against classical ensembles, a transformer,\nand pure quantum baselines under Return, Sharpe, and Max Drawdown. The\n\\textbf{QASA-Sequence} variant attains the \\emph{best single-model\nrisk-adjusted performance} (\\textbf{13.99\\%} return; \\textbf{Sharpe 1.76}),\nwhile hybrid models average \\textbf{11.2\\%} return (vs.\\ 9.8\\% classical; 4.4\\%\npure quantum), indicating a favorable performance--stability--cost trade-off.", "published": "2025-09-21 07:27:14", "link": "http://arxiv.org/abs/2509.16955v1", "categories": ["quant-ph", "cs.LG", "q-fin.CP"], "primary_category": "quant-ph"}
{"title": "An Ambit Field Framework for the Full Panel of Day-ahead Electricity Prices", "abstract": "This paper considers the often overlooked fact that electricity spot prices\nin individual European generation zones evolve as a high dimensional panel\nstructure. A general continuous time framework is developed by formulating the\npanel as an ambit field indexed by a cylinder surface, where the cross\nsectional dimension is represented by a circle. This requires a treatment of\nambit fields on manifolds, but the departure from Euclidean space allows for\nembedding intrinsic dependence structures into the index set in a flexible and\nparameter-free way, where the daily delivery periods have a canonical mapping\nonto the circle. The model is a natural space-time extension of volatility\nmodulated L\\'evy-driven Volterra processes, which have previously been studied\nin the context of energy markets, and the pricing of electricity derivatives\nturns out to be essentially as analytically tractable as in the null-spatial\nsetting. The space-time framework extends the scope of possible derivatives to\nproducts written on individual delivery periods, where spreads between these\nconstitute an interesting example. We establish useful formulas for the pricing\nof various derivatives along with a simulation scheme, and study specifications\nof the dependence structure in detail.", "published": "2025-09-21 21:01:03", "link": "http://arxiv.org/abs/2509.17236v1", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "Risk Comparisons in Linear Regression: Implicit Regularization Dominates Explicit Regularization", "abstract": "Existing theory suggests that for linear regression problems categorized by\ncapacity and source conditions, gradient descent (GD) is always minimax\noptimal, while both ridge regression and online stochastic gradient descent\n(SGD) are polynomially suboptimal for certain categories of such problems.\nMoving beyond minimax theory, this work provides instance-wise comparisons of\nthe finite-sample risks for these algorithms on any well-specified linear\nregression problem.\n  Our analysis yields three key findings. First, GD dominates ridge regression:\nwith comparable regularization, the excess risk of GD is always within a\nconstant factor of ridge, but ridge can be polynomially worse even when tuned\noptimally. Second, GD is incomparable with SGD. While it is known that for\ncertain problems GD can be polynomially better than SGD, the reverse is also\ntrue: we construct problems, inspired by benign overfitting theory, where\noptimally stopped GD is polynomially worse. Finally, GD dominates SGD for a\nsignificant subclass of problems -- those with fast and continuously decaying\ncovariance spectra -- which includes all problems satisfying the standard\ncapacity condition.", "published": "2025-09-21 22:02:38", "link": "http://arxiv.org/abs/2509.17251v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Enhancing Performance and Calibration in Quantile Hyperparameter Optimization", "abstract": "Bayesian hyperparameter optimization relies heavily on Gaussian Process (GP)\nsurrogates, due to robust distributional posteriors and strong performance on\nlimited training samples. GPs however underperform in categorical\nhyperparameter environments or when assumptions of normality,\nheteroskedasticity and symmetry are excessively challenged. Conformalized\nquantile regression can address these estimation weaknesses, while still\nproviding robust calibration guarantees. This study builds upon early work in\nthis area by addressing feedback covariate shift in sequential acquisition and\nintegrating a wider range of surrogate architectures and acquisition functions.\nProposed algorithms are rigorously benchmarked against a range of state of the\nart hyperparameter optimization methods (GP, TPE and SMAC). Findings identify\nquantile surrogate architectures and acquisition functions yielding superior\nperformance to the current quantile literature, while validating the beneficial\nimpact of conformalization on calibration and search performance.", "published": "2025-09-21 12:17:06", "link": "http://arxiv.org/abs/2509.17051v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Hessian-guided Perturbed Wasserstein Gradient Flows for Escaping Saddle Points", "abstract": "Wasserstein gradient flow (WGF) is a common method to perform optimization\nover the space of probability measures. While WGF is guaranteed to converge to\na first-order stationary point, for nonconvex functionals the converged\nsolution does not necessarily satisfy the second-order optimality condition;\ni.e., it could converge to a saddle point. In this work, we propose a new\nalgorithm for probability measure optimization, perturbed Wasserstein gradient\nflow (PWGF), that achieves second-order optimality for general nonconvex\nobjectives. PWGF enhances WGF by injecting noisy perturbations near saddle\npoints via a Gaussian process-based scheme. By pushing the measure forward\nalong a random vector field generated from a Gaussian process, PWGF helps the\nsolution escape saddle points efficiently by perturbing the solution towards\nthe smallest eigenvalue direction of the Wasserstein Hessian. We theoretically\nderive the computational complexity for PWGF to achieve a second-order\nstationary point. Furthermore, we prove that PWGF converges to a global optimum\nin polynomial time for strictly benign objectives.", "published": "2025-09-21 08:14:20", "link": "http://arxiv.org/abs/2509.16974v1", "categories": ["math.OC", "stat.ML"], "primary_category": "math.OC"}
{"title": "Gradient Interference-Aware Graph Coloring for Multitask Learning", "abstract": "When different objectives conflict with each other in multi-task learning,\ngradients begin to interfere and slow convergence, thereby reducing the final\nmodel's performance. To address this, we introduce a scheduler that computes\ngradient interference, constructs an interference graph, and then applies\ngreedy graph-coloring to partition tasks into groups that align well with each\nother. At each training step, only one group (color class) of tasks are\nactivated. The grouping partition is constantly recomputed as task\nrelationships evolve throughout training. By ensuring that each mini-batch\ncontains only tasks that pull the model in the same direction, our method\nimproves the effectiveness of any underlying multi-task learning optimizer\nwithout additional tuning. Since tasks within these groups will update in\ncompatible directions, model performance will be improved rather than impeded.\nEmpirical results on six different datasets show that this interference-aware\ngraph-coloring approach consistently outperforms baselines and state-of-the-art\nmulti-task optimizers.", "published": "2025-09-21 07:45:53", "link": "http://arxiv.org/abs/2509.16959v1", "categories": ["cs.LG", "cs.AI", "cs.NE", "stat.ML"], "primary_category": "cs.LG"}
{"title": "RADE for Land Mobile Radio: A Neural Codec for Transmission of Speech over Baseband FM Radio Channels", "abstract": "In the 1990s Land Mobile Radio (LMR) systems evolved from analog frequency\nmodulation (FM) to standardised digital systems. Both digital and analog FM\nsystems now co-exist in various services and exhibit similar speech quality.\nThe architecture of many digital radios retains the analog FM modulator and\ndemodulator from legacy analog radios, but driven by a multi-level digital\npulse train rather than an analog voice signal. We denote this architecture\nbaseband FM (BBFM). In this paper we describe a modern machine learning\napproach that uses an autoencoder to send high quality, 8 kHz bandwidth speech\nover the BBFM channel. The speech quality is shown to be superior to analog FM\nover simulated LMR channels in the presence of fading, and a demonstration of\nthe system running over commodity UHF radios is presented.", "published": "2025-09-21 23:52:11", "link": "http://arxiv.org/abs/2509.17286v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "BeepBank-500: A Synthetic Earcon Mini-Corpus for UI Sound Research and Psychoacoustics Research", "abstract": "We introduce BeepBank-500, a compact, fully synthetic earcon/alert dataset\n(300-500 clips) designed for rapid, rights-clean experimentation in\nhuman-computer interaction and audio machine learning. Each clip is generated\nfrom a parametric recipe controlling waveform family (sine, square, triangle,\nFM), fundamental frequency, duration, amplitude envelope, amplitude modulation\n(AM), and lightweight Schroeder-style reverberation. We use three reverberation\nsettings: dry, and two synthetic rooms denoted 'rir small' ('small') and 'rir\nmedium' ('medium') throughout the paper and in the metadata. We release mono 48\nkHz WAV audio (16-bit), a rich metadata table (signal/spectral features), and\ntiny reproducible baselines for (i) waveform-family classification and (ii) f0\nregression on single tones. The corpus targets tasks such as earcon\nclassification, timbre analyses, and onset detection, with clearly stated\nlicensing and limitations. Audio is dedicated to the public domain via CC0-1.0;\ncode is under MIT. Data DOI: https://doi.org/10.5281/zenodo.17172015. Code:\nhttps://github.com/mandip42/earcons-mini-500.", "published": "2025-09-21 23:26:10", "link": "http://arxiv.org/abs/2509.17277v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Reference-aware SFM layers for intrusive intelligibility prediction", "abstract": "Intrusive speech-intelligibility predictors that exploit explicit reference\nsignals are now widespread, yet they have not consistently surpassed\nnon-intrusive systems. We argue that a primary cause is the limited\nexploitation of speech foundation models (SFMs). This work revisits intrusive\nprediction by combining reference conditioning with multi-layer SFM\nrepresentations. Our final system achieves RMSE 22.36 on the development set\nand 24.98 on the evaluation set, ranking 1st on CPC3. These findings provide\npractical guidance for constructing SFM-based intrusive intelligibility\npredictors.", "published": "2025-09-21 23:06:31", "link": "http://arxiv.org/abs/2509.17270v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DeepASA: An Object-Oriented One-for-All Network for Auditory Scene Analysis", "abstract": "We propose DeepASA, a one-for-all model for auditory scene analysis that\nperforms multi-input multi-output (MIMO) source separation, dereverberation,\nsound event detection (SED), audio classification, and direction-of-arrival\nestimation (DoAE) within a unified framework. DeepASA is designed for complex\nauditory scenes where multiple, often similar, sound sources overlap in time\nand move dynamically in space. To achieve robust and consistent inference\nacross tasks, we introduce an object-oriented processing (OOP) strategy. This\napproach encapsulates diverse auditory features into object-centric\nrepresentations and refines them through a chain-of-inference (CoI) mechanism.\nThe pipeline comprises a dynamic temporal kernel-based feature extractor, a\ntransformer-based aggregator, and an object separator that yields per-object\nfeatures. These features feed into multiple task-specific decoders. Our\nobject-centric representations naturally resolve the parameter association\nambiguity inherent in traditional track-wise processing. However, early-stage\nobject separation can lead to failure in downstream ASA tasks. To address this,\nwe implement temporal coherence matching (TCM) within the chain-of-inference,\nenabling multi-task fusion and iterative refinement of object features using\nestimated auditory parameters. We evaluate DeepASA on representative spatial\naudio benchmark datasets, including ASA2, MC-FUSS, and STARSS23. Experimental\nresults show that our model achieves state-of-the-art performance across all\nevaluated tasks, demonstrating its effectiveness in both source separation and\nauditory parameter estimation under diverse spatial auditory scenes.", "published": "2025-09-21 21:44:10", "link": "http://arxiv.org/abs/2509.17247v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "STAR: Speech-to-Audio Generation via Representation Learning", "abstract": "This work presents STAR, the first end-to-end speech-to-audio generation\nframework, designed to enhance efficiency and address error propagation\ninherent in cascaded systems. Unlike prior approaches relying on text or\nvision, STAR leverages speech as it constitutes a natural modality for\ninteraction. As an initial step to validate the feasibility of the system, we\ndemonstrate through representation learning experiments that spoken sound event\nsemantics can be effectively extracted from raw speech, capturing both auditory\nevents and scene cues. Leveraging the semantic representations, STAR\nincorporates a bridge network for representation mapping and a two-stage\ntraining strategy to achieve end-to-end synthesis. With a 76.9% reduction in\nspeech processing latency, STAR demonstrates superior generation performance\nover the cascaded systems. Overall, STAR establishes speech as a direct\ninteraction signal for audio generation, thereby bridging representation\nlearning and multimodal synthesis. Generated samples are available at\nhttps://zeyuxie29.github.io/STAR.", "published": "2025-09-21 17:15:27", "link": "http://arxiv.org/abs/2509.17164v1", "categories": ["cs.SD", "eess.AS", "68Txx", "I.2"], "primary_category": "cs.SD"}
{"title": "FakeSound2: A Benchmark for Explainable and Generalizable Deepfake Sound Detection", "abstract": "The rapid development of generative audio raises ethical and security\nconcerns stemming from forged data, making deepfake sound detection an\nimportant safeguard against the malicious use of such technologies. Although\nprior studies have explored this task, existing methods largely focus on binary\nclassification and fall short in explaining how manipulations occur, tracing\nwhere the sources originated, or generalizing to unseen sources-thereby\nlimiting the explainability and reliability of detection. To address these\nlimitations, we present FakeSound2, a benchmark designed to advance deepfake\nsound detection beyond binary accuracy. FakeSound2 evaluates models across\nthree dimensions: localization, traceability, and generalization, covering 6\nmanipulation types and 12 diverse sources. Experimental results show that\nalthough current systems achieve high classification accuracy, they struggle to\nrecognize forged pattern distributions and provide reliable explanations. By\nhighlighting these gaps, FakeSound2 establishes a comprehensive benchmark that\nreveals key challenges and aims to foster robust, explainable, and\ngeneralizable approaches for trustworthy audio authentication.", "published": "2025-09-21 17:10:06", "link": "http://arxiv.org/abs/2509.17162v1", "categories": ["cs.SD", "eess.AS", "68Txx", "I.2"], "primary_category": "cs.SD"}
{"title": "MaskVCT: Masked Voice Codec Transformer for Zero-Shot Voice Conversion With Increased Controllability via Multiple Guidances", "abstract": "We introduce MaskVCT, a zero-shot voice conversion (VC) model that offers\nmulti-factor controllability through multiple classifier-free guidances (CFGs).\nWhile previous VC models rely on a fixed conditioning scheme, MaskVCT\nintegrates diverse conditions in a single model. To further enhance robustness\nand control, the model can leverage continuous or quantized linguistic features\nto enhance intellgibility and speaker similarity, and can use or omit pitch\ncontour to control prosody. These choices allow users to seamlessly balance\nspeaker identity, linguistic content, and prosodic factors in a zero-shot VC\nsetting. Extensive experiments demonstrate that MaskVCT achieves the best\ntarget speaker and accent similarities while obtaining competitive word and\ncharacter error rates compared to existing baselines. Audio samples are\navailable at https://maskvct.github.io/.", "published": "2025-09-21 16:14:51", "link": "http://arxiv.org/abs/2509.17143v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing", "abstract": "Large-scale text-to-speech (TTS) systems are limited by the scarcity of\nclean, multilingual recordings. We introduce Sidon, a fast, open-source speech\nrestoration model that converts noisy in-the-wild speech into studio-quality\nspeech and scales to dozens of languages. Sidon consists of two models:\nw2v-BERT 2.0 finetuned feature predictor to cleanse features from noisy speech\nand vocoder trained to synthesize restored speech from the cleansed features.\nSidon achieves restoration performance comparable to Miipher: Google's internal\nspeech restoration model with the aim of dataset cleansing for speech\nsynthesis. Sidon is also computationally efficient, running up to 3,390 times\nfaster than real time on a single GPU. We further show that training a TTS\nmodel using a Sidon-cleansed automatic speech recognition corpus improves the\nquality of synthetic speech in a zero-shot setting. Code and model are released\nto facilitate reproducible dataset cleansing for the research community.", "published": "2025-09-21 12:17:12", "link": "http://arxiv.org/abs/2509.17052v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "VAInpaint: Zero-Shot Video-Audio inpainting framework with LLMs-driven Module", "abstract": "Video and audio inpainting for mixed audio-visual content has become a\ncrucial task in multimedia editing recently. However, precisely removing an\nobject and its corresponding audio from a video without affecting the rest of\nthe scene remains a significant challenge. To address this, we propose\nVAInpaint, a novel pipeline that first utilizes a segmentation model to\ngenerate masks and guide a video inpainting model in removing objects. At the\nsame time, an LLM then analyzes the scene globally, while a region-specific\nmodel provides localized descriptions. Both the overall and regional\ndescriptions will be inputted into an LLM, which will refine the content and\nturn it into text queries for our text-driven audio separation model. Our audio\nseparation model is fine-tuned on a customized dataset comprising segmented\nMUSIC instrument images and VGGSound backgrounds to enhance its generalization\nperformance. Experiments show that our method achieves performance comparable\nto current benchmarks in both audio and video inpainting.", "published": "2025-09-21 10:31:56", "link": "http://arxiv.org/abs/2509.17022v1", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Bridging the gap between training and inference in LM-based TTS models", "abstract": "Recent advancements in text-to-speech (TTS) have shown that language model\n(LM) based systems offer competitive performance compared to traditional\napproaches. However, in training, TTS models use ground-truth (GT) tokens as\nprefixes to predict the next token, while in inference these tokens are not\navailable, a gap between training and inference that is often neglected. In\nthis study, we propose a prompt-guided hybrid training scheme to mitigate\nexposure bias in popular LM-based TTS systems. Our core idea is to adopt a\nhybrid training paradigm that combines teacher forcing with free running,\nthereby introducing self-generated tokens into the training process. This makes\nthe training mode more consistent with inference, reducing the\ntraining-inference gap. In addition, we incorporate an EOS prediction mechanism\nduring training to detect incorrect sequence termination and adaptively control\nthe free running process. Experimental results provide a comprehensive\nevaluation of the impact of exposure bias on LM-based TTS, and demonstrate that\nour method effectively narrows the training-inference gap, thereby improving\nthe quality of synthesized long-form speech.", "published": "2025-09-21 10:29:36", "link": "http://arxiv.org/abs/2509.17021v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MBCodec:Thorough disentangle for high-fidelity audio compression", "abstract": "High-fidelity neural audio codecs in Text-to-speech (TTS) aim to compress\nspeech signals into discrete representations for faithful reconstruction.\nHowever, prior approaches faced challenges in effectively disentangling\nacoustic and semantic information within tokens, leading to a lack of\nfine-grained details in synthesized speech. In this study, we propose MBCodec,\na novel multi-codebook audio codec based on Residual Vector Quantization (RVQ)\nthat learns a hierarchically structured representation. MBCodec leverages\nself-supervised semantic tokenization and audio subband features from the raw\nsignals to construct a functionally-disentangled latent space. In order to\nencourage comprehensive learning across various layers of the codec embedding\nspace, we introduce adaptive dropout depths to differentially train codebooks\nacross layers, and employ a multi-channel pseudo-quadrature mirror filter\n(PQMF) during training. By thoroughly decoupling semantic and acoustic\nfeatures, our method not only achieves near-lossless speech reconstruction but\nalso enables a remarkable 170x compression of 24 kHz audio, resulting in a low\nbit rate of just 2.2 kbps. Experimental evaluations confirm its consistent and\nsubstantial outperformance of baselines across all evaluations.", "published": "2025-09-21 09:52:45", "link": "http://arxiv.org/abs/2509.17006v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Attentive AV-FusionNet: Audio-Visual Quality Prediction with Hybrid Attention", "abstract": "We introduce a novel deep learning-based audio-visual quality (AVQ)\nprediction model that leverages internal features from state-of-the-art\nunimodal predictors. Unlike prior approaches that rely on simple fusion\nstrategies, our model employs a hybrid representation that combines learned\nGenerative Machine Listener (GML) audio features with hand-crafted Video\nMultimethod Assessment Fusion (VMAF) video features. Attention mechanisms\ncapture cross-modal interactions and intra-modal relationships, yielding\ncontext-aware quality representations. A modality relevance estimator\nquantifies each modality's contribution per content, potentially enabling\nadaptive bitrate allocation. Experiments demonstrate improved AVQ prediction\naccuracy and robustness across diverse content types.", "published": "2025-09-21 09:25:09", "link": "http://arxiv.org/abs/2509.16994v1", "categories": ["eess.AS", "cs.MM", "eess.IV"], "primary_category": "eess.AS"}
{"title": "Advancing Speech Understanding in Speech-Aware Language Models with GRPO", "abstract": "In this paper, we introduce a Group Relative Policy Optimization (GRPO)-based\nmethod for training Speech-Aware Large Language Models (SALLMs) on open-format\nspeech understanding tasks, such as Spoken Question Answering and Automatic\nSpeech Translation. SALLMs have proven highly effective for speech\nunderstanding tasks. GRPO has recently gained traction for its efficiency in\ntraining LLMs, and prior work has explored its application to SALLMs, primarily\nin multiple-choice tasks. Building on this, we focus on open-format tasks that\nbetter reflect the generative abilities of the models. Our approach leverages\nGRPO with BLEU as the reward signal to optimize SALLMs, and we demonstrate\nempirically that it surpasses standard SFT across several key metrics. Finally,\nwe explore the potential of incorporating off-policy samples within GRPO for\nthese tasks, highlighting avenues for further improvement and further research.", "published": "2025-09-21 09:09:36", "link": "http://arxiv.org/abs/2509.16990v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Leveraging Multiple Speech Enhancers for Non-Intrusive Intelligibility Prediction for Hearing-Impaired Listeners", "abstract": "Speech intelligibility evaluation for hearing-impaired (HI) listeners is\nessential for assessing hearing aid performance, traditionally relying on\nlistening tests or intrusive methods like HASPI. However, these methods require\nclean reference signals, which are often unavailable in real-world conditions,\ncreating a gap between lab-based and real-world assessments. To address this,\nwe propose a non-intrusive intelligibility prediction framework that leverages\nspeech enhancers to provide a parallel enhanced-signal pathway, enabling robust\npredictions without reference signals. We evaluate three state-of-the-art\nenhancers and demonstrate that prediction performance depends on the choice of\nenhancer, with ensembles of strong enhancers yielding the best results. To\nimprove cross-dataset generalization, we introduce a 2-clips augmentation\nstrategy that enhances listener-specific variability, boosting robustness on\nunseen datasets. Our approach consistently outperforms the non-intrusive\nbaseline, CPC2 Champion across multiple datasets, highlighting the potential of\nenhancer-guided non-intrusive intelligibility prediction for real-world\napplications.", "published": "2025-09-21 08:29:24", "link": "http://arxiv.org/abs/2509.16979v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Interpretable Audio Editing Evaluation via Chain-of-Thought Difference-Commonality Reasoning with Multimodal LLMs", "abstract": "Automatic mean opinion score (MOS) prediction provides a more perceptual\nalternative to objective metrics, offering deeper insights into the evaluated\nmodels. With the rapid progress of multimodal large language models (MLLMs),\ntheir enhanced perceptual and reasoning abilities enable more comprehensive and\ninterpretable audio quality assessment. In this work, we tackle the challenging\ntask of audio editing evaluation and propose the first natural language-based\nautomated evaluation framework built on MLLMs. Our approach introduces two\nfine-tuning tasks to boost multi-audio understanding, combined with\nChain-of-Thought prompting, and lightweight instruction tuning, to enhance\nstep-by-step reasoning. Experiment demonstrate that our framework delivers\naccurate, interpretable, and text-based editing evaluation, closely aligning\nwith human judgments and objective metrics while substantially improving over\nbaselines. The code and demo are available at\nhttps://github.com/NKU-HLT/Eval_Reasoning.", "published": "2025-09-21 08:18:48", "link": "http://arxiv.org/abs/2509.16975v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AudioGenie-Reasoner: A Training-Free Multi-Agent Framework for Coarse-to-Fine Audio Deep Reasoning", "abstract": "Audio deep reasoning is a challenging task that requires expert-level\nperception, multi-step logical inference, and the integration of contextual\nknowledge. However, existing models suffer from a gap between audio perception\nand reasoning abilities due to the lack of training data with explicit\nreasoning chains and the absence of mechanisms for active exploration and\niterative refinement. To address these challenges, we propose\nAudioGenie-Reasoner (AGR), the first unified training-free multi-agent system\nthat coordinates perception and reasoning over an evolving chain of textual\nevidence. Our key idea is a paradigm shift that transforms audio deep reasoning\ninto complex text understanding task from a new perspective, thereby unlocking\nthe full potential of large language models. Specifically, the design of AGR\nmimics the human coarse-to-fine cognitive process. It first transforms the\ninput audio into a coarse text-based document. Then, we design a novel\nproactive iterative document refinement loop, featuring tool-augmented routes\nand specialized agents, to continuously search for missing information and\naugment the evidence chain in a coarse-to-fine manner until sufficient\nquestion-related information is gathered for making final predictions.\nExperimental results show that AGR achieves state-of-the-art (SOTA) performance\nover existing open-source audio deep reasoning models across various\nbenchmarks. The code will be made publicly available.", "published": "2025-09-21 08:08:08", "link": "http://arxiv.org/abs/2509.16971v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DroFiT: A Lightweight Band-fused Frequency Attention Toward Real-time UAV Speech Enhancement", "abstract": "This paper proposes DroFiT (Drone Frequency lightweight Transformer for\nspeech enhancement, a single microphone speech enhancement network for severe\ndrone self-noise. DroFit integrates a frequency-wise Transformer with a\nfull/sub-band hybrid encoder-decoder and a TCN back-end for memory-efficient\nstreaming. A learnable skip-and-gate fusion with a combined spectral-temporal\nloss further refines reconstruction. The model is trained on VoiceBank-DEMAND\nmixed with recorded drone noise (-5 to -25 dB SNR) and evaluate using standard\nspeech enhancement metrics and computational efficiency. Experimental results\nshow that DroFiT achieves competitive enhancement performance while\nsignificantly reducing computational and memory demands, paving the way for\nreal-time processing on resource-constrained UAV platforms. Audio demo samples\nare available on our demo page.", "published": "2025-09-21 06:56:32", "link": "http://arxiv.org/abs/2509.16945v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cross-Attention with Confidence Weighting for Multi-Channel Audio Alignment", "abstract": "Multi-channel audio alignment is a key requirement in bioacoustic monitoring,\nspatial audio systems, and acoustic localization. However, existing methods\noften struggle to address nonlinear clock drift and lack mechanisms for\nquantifying uncertainty. Traditional methods like Cross-correlation and Dynamic\nTime Warping assume simple drift patterns and provide no reliability measures.\nMeanwhile, recent deep learning models typically treat alignment as a binary\nclassification task, overlooking inter-channel dependencies and uncertainty\nestimation. We introduce a method that combines cross-attention mechanisms with\nconfidence-weighted scoring to improve multi-channel audio synchronization. We\nextend BEATs encoders with cross-attention layers to model temporal\nrelationships between channels. We also develop a confidence-weighted scoring\nfunction that uses the full prediction distribution instead of binary\nthresholding. Our method achieved first place in the BioDCASE 2025 Task 1\nchallenge with 0.30 MSE average across test datasets, compared to 0.58 for the\ndeep learning baseline. On individual datasets, we achieved 0.14 MSE on ARU\ndata (77% reduction) and 0.45 MSE on zebra finch data (18% reduction). The\nframework supports probabilistic temporal alignment, moving beyond point\nestimates. While validated in a bioacoustic context, the approach is applicable\nto a broader range of multi-channel audio tasks where alignment confidence is\ncritical. Code available on: https://github.com/Ragib-Amin-Nihal/BEATsCA", "published": "2025-09-21 05:14:06", "link": "http://arxiv.org/abs/2509.16926v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automotive Sound Quality for EVs: Psychoacoustic Metrics with Reproducible AI/ML Baselines", "abstract": "We present an open, reproducible reference for automotive sound quality that\nconnects standardized psychoacoustic metrics with lightweight AI/ML baselines,\nwith a specific focus on electric vehicles (EVs). We implement loudness (ISO\n532-1/2), tonality (DIN 45681), and modulation-based descriptors (roughness,\nfluctuation strength), and document assumptions and parameterizations for\nreliable reuse. For modeling, we provide simple, fully reproducible baselines\n(logistic regression, random forest, SVM) on synthetic EV-like cases using\nfixed splits and seeds, reporting accuracy and rank correlations as examples of\nend-to-end workflows rather than a comparative benchmark. Program-level\nnormalization is reported in LUFS via ITU-R BS.1770, while psychoacoustic\nanalysis uses ISO-532 loudness (sones). All figures and tables are regenerated\nby scripts with pinned environments; code and minimal audio stimuli are\nreleased under permissive licenses to support teaching, replication, and\nextension to EV-specific noise phenomena (e.g., inverter whine, reduced\nmasking).", "published": "2025-09-21 03:25:09", "link": "http://arxiv.org/abs/2509.16901v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Drum-to-Vocal Percussion Sound Conversion and Its Evaluation Methodology", "abstract": "This paper defines the novel task of drum-to-vocal percussion (VP) sound\nconversion. VP imitates percussion instruments through human vocalization and\nis frequently employed in contemporary a cappella music. It exhibits acoustic\nproperties distinct from speech and singing (e.g., aperiodicity, noisy\ntransients, and the absence of linguistic structure), making conventional\nspeech or singing synthesis methods unsuitable. We thus formulate VP synthesis\nas a timbre transfer problem from drum sounds, leveraging their rhythmic and\ntimbral correspondence. To support this formulation, we define three\nrequirements for successful conversion: rhythmic fidelity, timbral consistency,\nand naturalness as VP. We also propose corresponding subjective evaluation\ncriteria. We implement two baseline conversion methods using a neural audio\nsynthesizer, the real-time audio variational autoencoder (RAVE), with and\nwithout vector quantization (VQ). Subjective experiments show that both methods\nproduce plausible VP outputs, with the VQ-based RAVE model yielding more\nconsistent conversion.", "published": "2025-09-21 01:27:07", "link": "http://arxiv.org/abs/2509.16862v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Estimation of Specific Gravity of Potato Tubers Using Dielectric Properties", "abstract": "Potatoes are an economically important crop, and their quality is closely\nrelated to the starch content, which is typically inferred from specific\ngravity (SG). Although microwave sensing technologies have been increasingly\ndeveloped for underground potato detection and quality assessment in recent\nyears, no accurate model has yet been established to link the dielectric\nproperties of potatoes with their key agronomic traits. To address this gap, we\ndeveloped a model for estimating potato tubers' SG based on their dielectric\nconstant. To construct and validate the model, we conducted SG measurements and\ndielectric spectroscopy measurements in the frequency range of 0.3 GHz to 3.0\nGHz on 250 potatoes of five different types (red, russet, yellow, purple, and\nchipping potatoes, with 50 samples per type). Out of the 250 data sets, 200\ndata sets were used for model development, and 50 data sets were used for model\nvalidation. A linear regression model was used to summarize the relationship\nbetween SG and dielectric constant, where the regression coefficients are\nexpressed as fourth-order polynomial functions of frequency. Experimental\nresults on the 50 validation data sets show that the model achieves high\nestimation accuracy with mean absolute errors (MAE) less than\n\\(4.8\\times10^{-3}\\) and mean absolute percentage errors (MAPE) less than\n0.45\\%. The study of the dielectric properties of potatoes, along with the\nderived SG estimation model, provides a foundation for the future development\nof microwave sensing technologies for agronomic trait assessment in the potato\nproduction and processing industries. All measured data will be made publicly\navailable upon acceptance of the paper.", "published": "2025-09-21 22:50:39", "link": "http://arxiv.org/abs/2509.17267v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Graph Signal Generative Diffusion Models", "abstract": "We introduce U-shaped encoder-decoder graph neural networks (U-GNNs) for\nstochastic graph signal generation using denoising diffusion processes. The\narchitecture learns node features at different resolutions with skip\nconnections between the encoder and decoder paths, analogous to the\nconvolutional U-Net for image generation. The U-GNN is prominent for a pooling\noperation that leverages zero-padding and avoids arbitrary graph coarsening,\nwith graph convolutions layered on top to capture local dependencies. This\ntechnique permits learning feature embeddings for sampled nodes at deeper\nlevels of the architecture that remain convolutional with respect to the\noriginal graph. Applied to stock price prediction -- where deterministic\nforecasts struggle to capture uncertainties and tail events that are paramount\n-- we demonstrate the effectiveness of the diffusion model in probabilistic\nforecasting of stock prices.", "published": "2025-09-21 21:57:27", "link": "http://arxiv.org/abs/2509.17250v1", "categories": ["cs.LG", "eess.SP"], "primary_category": "cs.LG"}
{"title": "SignalLLM: A General-Purpose LLM Agent Framework for Automated Signal Processing", "abstract": "Modern signal processing (SP) pipelines, whether model-based or data-driven,\noften constrained by complex and fragmented workflow, rely heavily on expert\nknowledge and manual engineering, and struggle with adaptability and\ngeneralization under limited data. In contrast, Large Language Models (LLMs)\noffer strong reasoning capabilities, broad general-purpose knowledge,\nin-context learning, and cross-modal transfer abilities, positioning them as\npowerful tools for automating and generalizing SP workflows. Motivated by these\npotentials, we introduce SignalLLM, the first general-purpose LLM-based agent\nframework for general SP tasks. Unlike prior LLM-based SP approaches that are\nlimited to narrow applications or tricky prompting, SignalLLM introduces a\nprincipled, modular architecture. It decomposes high-level SP goals into\nstructured subtasks via in-context learning and domain-specific retrieval,\nfollowed by hierarchical planning through adaptive retrieval-augmented\ngeneration (RAG) and refinement; these subtasks are then executed through\nprompt-based reasoning, cross-modal reasoning, code synthesis, model\ninvocation, or data-driven LLM-assisted modeling. Its generalizable design\nenables the flexible selection of problem solving strategies across different\nsignal modalities, task types, and data conditions. We demonstrate the\nversatility and effectiveness of SignalLLM through five representative tasks in\ncommunication and sensing, such as radar target detection, human activity\nrecognition, and text compression. Experimental results show superior\nperformance over traditional and existing LLM-based methods, particularly in\nfew-shot and zero-shot settings.", "published": "2025-09-21 18:54:54", "link": "http://arxiv.org/abs/2509.17197v1", "categories": ["cs.LG", "cs.AI", "eess.SP"], "primary_category": "cs.LG"}
{"title": "Resilient Signal Reflection under CSI Perturbations: A Robust Approach for Secure RIS Communication", "abstract": "Reconfigurable Intelligent Surfaces (RIS) have emerged as a transformative\ntechnology in wireless communication, enabling dynamic control over signal\npropagation. This paper tackles the challenge of mitigating Channel State\nInformation (CSI) perturbations in RIS-aided systems, particularly for secure\ncommunication scenarios. Leveraging a first-order approximation technique, we\ndevelop a robust approach that strengthens the resilience of RIS configurations\nagainst CSI imperfections. The study considers both untrusted user interception\nand stealth radar applications, focusing on optimizing signal reflection and\ntransmission in the presence of eavesdroppers. Simulation results demonstrate\nnotable gains in security and efficiency while maintaining low computational\ncomplexity. By extending the stability range, the proposed method updates RIS\nelements using only a few matrix-vector multiplications, eliminating the need\nfor repeated inverse or pseudo-inverse computations under small channel\nperturbations. Additionally, the framework provides a baseline for quantifying\nalgorithmic sensitivity to CSI variations. Overall, the findings underscore the\npotential of RIS to enable secure and reliable communication in next-generation\nnetworks such as 6G.", "published": "2025-09-21 18:05:43", "link": "http://arxiv.org/abs/2509.17181v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Functional WMMSE Algorithm for Continuous Aperture Array Systems", "abstract": "In this paper, we propose a functional extension of the weighted minimum\nmean-squared error (WMMSE) algorithm for downlink beamforming in multiuser\nmultiple-input multiple-output (MU-MIMO) systems where both the base station\n(BS) and the users employ continuous-aperture arrays (CAPAs). The method lifts\nthe matrices and vectors in the classical discrete WMMSE recursion to\ncontinuous functions by replacing matrix products and inner products with\nintegrals over the apertures. In practice, we apply a Galerkin projection to\nmap functions to coefficient matrices, solve the resulting discrete WMMSE\nproblem via closed-form updates, and then lift these updates back to the\nfunctional domain. All integrals are implemented using Gauss-Legendre\nquadrature, which preserves the closed-form structure through weighted matrix\nproducts. Simulations show that the proposed method outperforms baselines in\nboth spectral efficiency (SE) and computational complexity.", "published": "2025-09-21 14:41:51", "link": "http://arxiv.org/abs/2509.17101v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Asymptotic Scaling Law Analysis of Multicast Satellite Communications with Massive MIMO", "abstract": "In this paper, we consider a geostationary orbit (GEO) satellite\ncommunication system that employs massive multiple-input multiple-output (MIMO)\nfor multicast transmission. By modeling the spatial distribution of ground\nusers using a Poisson point process (PPP) and assuming a fixed-beam precoding\nis adopted, we find a closed-form expression for the asymptotical rate scaling\nlaw as a function of the number of antennas and the scaling factors of user\ndensity and multicast users. From the derived analytical expression, we reveal\nthat the rate degradation caused by multicast transmission can be precisely\ncompensated by increasing the user density accordingly.", "published": "2025-09-21 05:01:01", "link": "http://arxiv.org/abs/2509.16921v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Bi-modal Prediction and Transformation Coding for Compressing Complex Human Dynamics", "abstract": "For dynamic human motion sequences, the original KeyNode-Driven codec often\nstruggles to retain compression efficiency when confronted with rapid movements\nor strong non-rigid deformations. This paper proposes a novel Bi-modal coding\nframework that enhances the flexibility of motion representation by integrating\nsemantic segmentation and region-specific transformation modeling. The rigid\ntransformation model (rotation & translation) is extended with a hybrid scheme\nthat selectively applies affine transformations-rotation, translation, scaling,\nand shearing-only to deformation-rich regions (e.g., the torso, where loose\nclothing induces high variability), while retaining rigid models elsewhere. The\naffine model is decomposed into minimal parameter sets for efficient coding and\ncombined through a component selection strategy guided by a Lagrangian\nRate-Distortion optimization. The results show that the Bi-modal method\nachieves more accurate mesh deformation, especially in sequences involving\ncomplex non-rigid motion, without compromising compression efficiency in\nsimpler regions, with an average bit-rate saving of 33.81% compared to the\nbaseline.", "published": "2025-09-21 04:57:35", "link": "http://arxiv.org/abs/2509.16919v1", "categories": ["eess.SP", "cs.MM"], "primary_category": "eess.SP"}
{"title": "Graph Fractional Hilbert Transform: Theory and Application", "abstract": "The graph Hilbert transform (GHT) is a key tool in constructing analytic\nsignals and extracting envelope and phase information in graph signal\nprocessing. However, its utility is limited by confinement to the graph Fourier\ndomain, a fixed phase shift, information loss for real-valued spectral\ncomponents, and the absence of tunable parameters. The graph fractional Fourier\ntransform introduces domain flexibility through a fractional order parameter\n$\\alpha$ but does not resolve the issues of phase rigidity and information\nloss. Inspired by the dual-parameter fractional Hilbert transform (FRHT) in\nclassical signal processing, we propose the graph FRHT (GFRHT). The GFRHT\nincorporates a dual-parameter framework: the fractional order $\\alpha$ enables\nanalysis across arbitrary fractional domains, interpolating between vertex and\nspectral spaces, while the angle parameter $\\beta$ provides adjustable phase\nshifts and a non-zero real-valued response ($\\cos\\beta$) for real eigenvalues,\nthereby eliminating information loss. We formally define the GFRHT, establish\nits core properties, and design a method for graph analytic signal\nconstruction, enabling precise envelope extraction and demodulation.\nExperiments on edge detection, anomaly identification, and speech\nclassification demonstrate that GFRHT outperforms GHT, offering greater\nflexibility and superior performance in graph signal processing.", "published": "2025-09-21 04:16:09", "link": "http://arxiv.org/abs/2509.16910v1", "categories": ["eess.SP", "eess.IV"], "primary_category": "eess.SP"}
{"title": "On the Secrecy Performance of Pinching-Antenna Systems", "abstract": "Pinching-antenna systems have recently gained significant attention as a\nnovel reconfigurable-antenna technology due to its exceptional capability of\nmitigating signal-propagation path loss. In this letter, we investigate the\nsecrecy performance of a pinching-antenna system in the presence of an\neavesdropper. In particular, we derive an approximate expression of the\nsystem's secrecy outage probability (SOP) with respect to the random locations\nof the legitimate user and eavesdropper and analyze its asymptotic behavior.\nMoreover, we derive a constant performance lower bound on the SOP of the\nconsidered system, i.e., $\\frac{2\\pi-1}{24}$, which is significantly lower than\nthat of conventional fixed-position antenna systems, i.e., $0.5$. Finally,\nsimulation results are provided to validate the correctness of our analytical\nresults.", "published": "2025-09-21 00:44:42", "link": "http://arxiv.org/abs/2509.16854v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Change in Quantitative Bipolar Argumentation: Sufficient, Necessary, and Counterfactual Explanations", "abstract": "This paper presents a formal approach to explaining change of inference in\nQuantitative Bipolar Argumentation Frameworks (QBAFs). When drawing conclusions\nfrom a QBAF and updating the QBAF to then again draw conclusions (and so on),\nour approach traces changes -- which we call strength inconsistencies -- in the\npartial order over argument strengths that a semantics establishes on some\narguments of interest, called topic arguments. We trace the causes of strength\ninconsistencies to specific arguments, which then serve as explanations. We\nidentify sufficient, necessary, and counterfactual explanations for strength\ninconsistencies and show that strength inconsistency explanations exist if and\nonly if an update leads to strength inconsistency. We define a heuristic-based\napproach to facilitate the search for strength inconsistency explanations, for\nwhich we also provide an implementation.", "published": "2025-09-21 20:26:47", "link": "http://arxiv.org/abs/2509.18215v1", "categories": ["cs.AI", "cs.LO", "cs.MA"], "primary_category": "cs.AI"}
