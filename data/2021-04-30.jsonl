{"title": "Adapting Coreference Resolution for Processing Violent Death Narratives", "abstract": "Coreference resolution is an important component in analyzing narrative text\nfrom administrative data (e.g., clinical or police sources). However, existing\ncoreference models trained on general language corpora suffer from poor\ntransferability due to domain gaps, especially when they are applied to\ngender-inclusive data with lesbian, gay, bisexual, and transgender (LGBT)\nindividuals. In this paper, we analyzed the challenges of coreference\nresolution in an exemplary form of administrative text written in English:\nviolent death narratives from the USA's Centers for Disease Control's (CDC)\nNational Violent Death Reporting System. We developed a set of data\naugmentation rules to improve model performance using a probabilistic data\nprogramming framework. Experiments on narratives from an administrative\ndatabase, as well as existing gender-inclusive coreference datasets,\ndemonstrate the effectiveness of data augmentation in training coreference\nmodels that can better handle text data about LGBT individuals.", "published": "2021-04-30 00:16:42", "link": "http://arxiv.org/abs/2104.14703v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on sentiment analysis in Persian: A Comprehensive System\n  Perspective Covering Challenges and Advances in Resources, and Methods", "abstract": "Social media has been remarkably grown during the past few years. Nowadays,\nposting messages on social media websites has become one of the most popular\nInternet activities. The vast amount of user-generated content has made social\nmedia the most extensive data source of public opinion. Sentiment analysis is\none of the techniques used to analyze user-generated data. The Persian language\nhas specific features and thereby requires unique methods and models to be\nadopted for sentiment analysis, which are different from those in English\nlanguage. Sentiment analysis in each language has specified prerequisites;\nhence, the direct use of methods, tools, and resources developed for English\nlanguage in Persian has its limitations. The main target of this paper is to\nprovide a comprehensive literature survey for state-of-the-art advances in\nPersian sentiment analysis. In this regard, the present study aims to\ninvestigate and compare the previous sentiment analysis studies on Persian\ntexts and describe contributions presented in articles published in the last\ndecade. First, the levels, approaches, and tasks for sentiment analysis are\ndescribed. Then, a detailed survey of the sentiment analysis methods used for\nPersian texts is presented, and previous relevant works on Persian Language are\ndiscussed. Moreover, we present in this survey the authentic and published\nstandard sentiment analysis resources and advances that have been done for\nPersian sentiment analysis. Finally, according to the state-of-the-art\ndevelopment of English sentiment analysis, some issues and challenges not being\naddressed in Persian texts are listed, and some guidelines and trends are\nprovided for future research on Persian texts. The paper provides information\nto help new or established researchers in the field as well as industry\ndevelopers who aim to deploy an operational complete sentiment analysis system.", "published": "2021-04-30 04:31:21", "link": "http://arxiv.org/abs/2104.14751v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Out-of-Scope Domain and Intent Classification through Hierarchical Joint\n  Modeling", "abstract": "User queries for a real-world dialog system may sometimes fall outside the\nscope of the system's capabilities, but appropriate system responses will\nenable smooth processing throughout the human-computer interaction. This paper\nis concerned with the user's intent, and focuses on out-of-scope intent\nclassification in dialog systems. Although user intents are highly correlated\nwith the application domain, few studies have exploited such correlations for\nintent classification. Rather than developing a two-stage approach that first\nclassifies the domain and then the intent, we propose a hierarchical multi-task\nlearning approach based on a joint model to classify domain and intent\nsimultaneously. Novelties in the proposed approach include: (1) sharing\nsupervised out-of-scope signals in joint modeling of domain and intent\nclassification to replace a two-stage pipeline; and (2) introducing a\nhierarchical model that learns the intent and domain representations in the\nhigher and lower layers respectively. Experiments show that the model\noutperforms existing methods in terms of accuracy, out-of-scope recall and F1.\nAdditionally, threshold-based post-processing further improves performance by\nbalancing precision and recall in intent classification.", "published": "2021-04-30 06:38:23", "link": "http://arxiv.org/abs/2104.14781v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Factual Inconsistency Problem in Abstractive Text Summarization: A\n  Survey", "abstract": "Recently, various neural encoder-decoder models pioneered by Seq2Seq\nframework have been proposed to achieve the goal of generating more abstractive\nsummaries by learning to map input text to output text. At a high level, such\nneural models can freely generate summaries without any constraint on the words\nor phrases used. Moreover, their format is closer to human-edited summaries and\noutput is more readable and fluent. However, the neural model's abstraction\nability is a double-edged sword. A commonly observed problem with the generated\nsummaries is the distortion or fabrication of factual information in the\narticle. This inconsistency between the original text and the summary has\ncaused various concerns over its applicability, and the previous evaluation\nmethods of text summarization are not suitable for this issue. In response to\nthe above problems, the current research direction is predominantly divided\ninto two categories, one is to design fact-aware evaluation metrics to select\noutputs without factual inconsistency errors, and the other is to develop new\nsummarization systems towards factual consistency. In this survey, we focus on\npresenting a comprehensive review of these fact-specific evaluation methods and\ntext summarization models.", "published": "2021-04-30 08:46:13", "link": "http://arxiv.org/abs/2104.14839v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Summarization, Simplification, and Generation: The Case of Patents", "abstract": "We survey Natural Language Processing (NLP) approaches to summarizing,\nsimplifying, and generating patents' text. While solving these tasks has\nimportant practical applications - given patents' centrality in the R&D process\n- patents' idiosyncrasies open peculiar challenges to the current NLP state of\nthe art. This survey aims at a) describing patents' characteristics and the\nquestions they raise to the current NLP systems, b) critically presenting\nprevious work and its evolution, and c) drawing attention to directions of\nresearch in which further work is needed. To the best of our knowledge, this is\nthe first survey of generative approaches in the patent domain.", "published": "2021-04-30 09:28:29", "link": "http://arxiv.org/abs/2104.14860v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word-Level Alignment of Paper Documents with their Electronic Full-Text\n  Counterparts", "abstract": "We describe a simple procedure for the automatic creation of word-level\nalignments between printed documents and their respective full-text versions.\nThe procedure is unsupervised, uses standard, off-the-shelf components only,\nand reaches an F-score of 85.01 in the basic setup and up to 86.63 when using\npre- and post-processing. Potential areas of application are manual database\ncuration (incl. document triage) and biomedical expression OCR.", "published": "2021-04-30 11:43:05", "link": "http://arxiv.org/abs/2104.14925v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Paraphrastic Representations at Scale", "abstract": "We present a system that allows users to train their own state-of-the-art\nparaphrastic sentence representations in a variety of languages. We also\nrelease trained models for English, Arabic, German, French, Spanish, Russian,\nTurkish, and Chinese. We train these models on large amounts of data, achieving\nsignificantly improved performance from the original papers proposing the\nmethods on a suite of monolingual semantic similarity, cross-lingual semantic\nsimilarity, and bitext mining tasks. Moreover, the resulting models surpass all\nprior work on unsupervised semantic textual similarity, significantly\noutperforming even BERT-based models like Sentence-BERT (Reimers and Gurevych,\n2019). Additionally, our models are orders of magnitude faster than prior work\nand can be used on CPU with little difference in inference speed (even improved\nspeed over GPU when using more CPU cores), making these models an attractive\nchoice for users without access to GPUs or for use on embedded devices.\nFinally, we add significantly increased functionality to the code bases for\ntraining paraphrastic sentence models, easing their use for both inference and\nfor training them for any desired language with parallel data. We also include\ncode to automatically download and preprocess training data.", "published": "2021-04-30 16:55:28", "link": "http://arxiv.org/abs/2104.15114v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Attribution in Dialogue Systems: The BEGIN Benchmark", "abstract": "Knowledge-grounded dialogue systems powered by large language models often\ngenerate responses that, while fluent, are not attributable to a relevant\nsource of information. Progress towards models that do not exhibit this issue\nrequires evaluation metrics that can quantify its prevalence. To this end, we\nintroduce the Benchmark for Evaluation of Grounded INteraction (BEGIN),\ncomprised of 12k dialogue turns generated by neural dialogue systems trained on\nthree knowledge-grounded dialogue corpora. We collect human annotations\nassessing the extent to which the models' responses can be attributed to the\ngiven background information. We then use BEGIN to analyze eight evaluation\nmetrics. We find that these metrics rely on spurious correlations, do not\nreliably distinguish attributable abstractive responses from unattributable\nones, and perform substantially worse when the knowledge source is longer. Our\nfindings underscore the need for more sophisticated and robust evaluation\nmetrics for knowledge-grounded dialogue. We make BEGIN publicly available at\nhttps://github.com/google/BEGIN-dataset.", "published": "2021-04-30 20:17:52", "link": "http://arxiv.org/abs/2105.00071v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Response Quality with Backward Reasoning in Open-domain\n  Dialogue Systems", "abstract": "Being able to generate informative and coherent dialogue responses is crucial\nwhen designing human-like open-domain dialogue systems. Encoder-decoder-based\ndialogue models tend to produce generic and dull responses during the decoding\nstep because the most predictable response is likely to be a non-informative\nresponse instead of the most suitable one. To alleviate this problem, we\npropose to train the generation model in a bidirectional manner by adding a\nbackward reasoning step to the vanilla encoder-decoder training. The proposed\nbackward reasoning step pushes the model to produce more informative and\ncoherent content because the forward generation step's output is used to infer\nthe dialogue context in the backward direction. The advantage of our method is\nthat the forward generation and backward reasoning steps are trained\nsimultaneously through the use of a latent variable to facilitate bidirectional\noptimization. Our method can improve response quality without introducing side\ninformation (e.g., a pre-trained topic model). The proposed bidirectional\nresponse generation method achieves state-of-the-art performance for response\nquality.", "published": "2021-04-30 20:38:27", "link": "http://arxiv.org/abs/2105.00079v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual hate speech detection based on multilingual\n  domain-specific word embeddings", "abstract": "Automatic hate speech detection in online social networks is an important\nopen problem in Natural Language Processing (NLP). Hate speech is a\nmultidimensional issue, strongly dependant on language and cultural factors.\nDespite its relevance, research on this topic has been almost exclusively\ndevoted to English. Most supervised learning resources, such as labeled\ndatasets and NLP tools, have been created for this same language. Considering\nthat a large portion of users worldwide speak in languages other than English,\nthere is an important need for creating efficient approaches for multilingual\nhate speech detection. In this work we propose to address the problem of\nmultilingual hate speech detection from the perspective of transfer learning.\nOur goal is to determine if knowledge from one particular language can be used\nto classify other language, and to determine effective ways to achieve this. We\npropose a hate specific data representation and evaluate its effectiveness\nagainst general-purpose universal representations most of which, unlike our\nproposed model, have been trained on massive amounts of data. We focus on a\ncross-lingual setting, in which one needs to classify hate speech in one\nlanguage without having access to any labeled data for that language. We show\nthat the use of our simple yet specific multilingual hate representations\nimproves classification results. We explain this with a qualitative analysis\nshowing that our specific representation is able to capture some common\npatterns in how hate speech presents itself in different languages.\n  Our proposal constitutes, to the best of our knowledge, the first attempt for\nconstructing multilingual specific-task representations. Despite its\nsimplicity, our model outperformed the previous approaches for most of the\nexperimental setups. Our findings can orient future solutions toward the use of\ndomain-specific representations.", "published": "2021-04-30 02:24:50", "link": "http://arxiv.org/abs/2104.14728v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Adversarial Transfer Network for Knowledge Representation Learning", "abstract": "Knowledge representation learning has received a lot of attention in the past\nfew years. The success of existing methods heavily relies on the quality of\nknowledge graphs. The entities with few triplets tend to be learned with less\nexpressive power. Fortunately, there are many knowledge graphs constructed from\nvarious sources, the representations of which could contain much information.\nWe propose an adversarial embedding transfer network ATransN, which transfers\nknowledge from one or more teacher knowledge graphs to a target one through an\naligned entity set without explicit data leakage. Specifically, we add soft\nconstraints on aligned entity pairs and neighbours to the existing knowledge\nrepresentation learning methods. To handle the problem of possible distribution\ndifferences between teacher and target knowledge graphs, we introduce an\nadversarial adaption module. The discriminator of this module evaluates the\ndegree of consistency between the embeddings of an aligned entity pair. The\nconsistency score is then used as the weights of soft constraints. It is not\nnecessary to acquire the relations and triplets in teacher knowledge graphs\nbecause we only utilize the entity representations. Knowledge graph completion\nresults show that ATransN achieves better performance against baselines without\ntransfer on three datasets, CN3l, WK3l, and DWY100k. The ablation study\ndemonstrates that ATransN can bring steady and consistent improvement in\ndifferent settings. The extension of combining other knowledge graph embedding\nalgorithms and the extension with three teacher graphs display the promising\ngeneralization of the adversarial transfer network.", "published": "2021-04-30 05:07:25", "link": "http://arxiv.org/abs/2104.14757v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mitigating Political Bias in Language Models Through Reinforced\n  Calibration", "abstract": "Current large-scale language models can be politically biased as a result of\nthe data they are trained on, potentially causing serious problems when they\nare deployed in real-world settings. In this paper, we describe metrics for\nmeasuring political bias in GPT-2 generation and propose a reinforcement\nlearning (RL) framework for mitigating political biases in generated text. By\nusing rewards from word embeddings or a classifier, our RL framework guides\ndebiased generation without having access to the training data or requiring the\nmodel to be retrained. In empirical experiments on three attributes sensitive\nto political bias (gender, location, and topic), our methods reduced bias\naccording to both our metrics and human evaluation, while maintaining\nreadability and semantic coherence.", "published": "2021-04-30 07:21:30", "link": "http://arxiv.org/abs/2104.14795v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GTN-ED: Event Detection Using Graph Transformer Networks", "abstract": "Recent works show that the graph structure of sentences, generated from\ndependency parsers, has potential for improving event detection. However, they\noften only leverage the edges (dependencies) between words, and discard the\ndependency labels (e.g., nominal-subject), treating the underlying graph edges\nas homogeneous. In this work, we propose a novel framework for incorporating\nboth dependencies and their labels using a recently proposed technique called\nGraph Transformer Networks (GTN). We integrate GTNs to leverage dependency\nrelations on two existing homogeneous-graph-based models, and demonstrate an\nimprovement in the F1 score on the ACE dataset.", "published": "2021-04-30 16:35:29", "link": "http://arxiv.org/abs/2104.15104v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Leveraging Machine Learning to Detect Data Curation Activities", "abstract": "This paper describes a machine learning approach for annotating and analyzing\ndata curation work logs at ICPSR, a large social sciences data archive. The\nsystems we studied track curation work and coordinate team decision-making at\nICPSR. Repository staff use these systems to organize, prioritize, and document\ncuration work done on datasets, making them promising resources for studying\ncuration work and its impact on data reuse, especially in combination with data\nusage analytics. A key challenge, however, is classifying similar activities so\nthat they can be measured and associated with impact metrics. This paper\ncontributes: 1) a schema of data curation activities; 2) a computational model\nfor identifying curation actions in work log descriptions; and 3) an analysis\nof frequent data curation activities at ICPSR over time. We first propose a\nschema of data curation actions to help us analyze the impact of curation work.\nWe then use this schema to annotate a set of data curation logs, which contain\nrecords of data transformations and project management decisions completed by\nrepository staff. Finally, we train a text classifier to detect the frequency\nof curation actions in a large set of work logs. Our approach supports the\nanalysis of curation work documented in work log systems as an important step\ntoward studying the relationship between research data curation and data reuse.", "published": "2021-04-30 18:17:18", "link": "http://arxiv.org/abs/2105.00030v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Event-driven timeseries analysis and the comparison of public reactions\n  on COVID-19", "abstract": "The rapid spread of COVID-19 has already affected human lives throughout the\nglobe. Governments of different countries have taken various measures, but how\nthey affected people lives is not clear. In this study, a rule-based and a\nmachine-learning based models are applied to answer the above question using\npublic tweets from Japan, USA, UK, and Australia. Two polarity timeseries\n(meanPol and pnRatio) and two events, namely \"lockdown or emergency (LED)\" and\n\"the economic support package (ESP)\", are considered in this study. Statistical\ntesting on the sub-series around LED and ESP events showed their positive\nimpacts to the people of (UK and Australia) and (USA and UK), respectively\nunlike Japanese people that showed opposite effects. Manual validation with the\nrelevant tweets showed an agreement with the statistical results. A case study\nwith Japanese tweets using supervised logistic regression classifies tweets\ninto heath-worry, economy-worry and other classes with 83.11% accuracy.\nPredicted tweets around events re-confirm the statistical outcomes.", "published": "2021-04-30 06:14:53", "link": "http://arxiv.org/abs/2104.14777v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Deformable TDNN with adaptive receptive fields for speech recognition", "abstract": "Time Delay Neural Networks (TDNNs) are widely used in both DNN-HMM based\nhybrid speech recognition systems and recent end-to-end systems. Nevertheless,\nthe receptive fields of TDNNs are limited and fixed, which is not desirable for\ntasks like speech recognition, where the temporal dynamics of speech are varied\nand affected by many factors. This paper proposes to use deformable TDNNs for\nadaptive temporal dynamics modeling in end-to-end speech recognition. Inspired\nby deformable ConvNets, deformable TDNNs augment the temporal sampling\nlocations with additional offsets and learn the offsets automatically based on\nthe ASR criterion, without additional supervision. Experiments show that\ndeformable TDNNs obtain state-of-the-art results on WSJ benchmarks\n(1.42\\%/3.45\\% WER on WSJ eval92/dev93 respectively), outperforming standard\nTDNNs significantly. Furthermore, we propose the latency control mechanism for\ndeformable TDNNs, which enables deformable TDNNs to do streaming ASR without\naccuracy degradation.", "published": "2021-04-30 07:10:20", "link": "http://arxiv.org/abs/2104.14791v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Scaling End-to-End Models for Large-Scale Multilingual ASR", "abstract": "Building ASR models across many languages is a challenging multi-task\nlearning problem due to large variations and heavily unbalanced data. Existing\nwork has shown positive transfer from high resource to low resource languages.\nHowever, degradations on high resource languages are commonly observed due to\ninterference from the heterogeneous multilingual data and reduction in\nper-language capacity. We conduct a capacity study on a 15-language task, with\nthe amount of data per language varying from 7.6K to 53.5K hours. We adopt\nGShard [1] to efficiently scale up to 10B parameters. Empirically, we find that\n(1) scaling the number of model parameters is an effective way to solve the\ncapacity bottleneck - our 500M-param model already outperforms monolingual\nbaselines and scaling it to 1B and 10B brought further quality gains; (2)\nlarger models are not only more data efficient, but also more efficient in\nterms of training cost as measured in TPU days - the 1B-param model reaches the\nsame accuracy at 34% of training time as the 500M-param model; (3) given a\nfixed capacity budget, adding depth works better than width and large encoders\ndo better than large decoders; (4) with continuous training, they can be\nadapted to new languages and domains.", "published": "2021-04-30 08:24:11", "link": "http://arxiv.org/abs/2104.14830v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "BERT Meets Relational DB: Contextual Representations of Relational\n  Databases", "abstract": "In this paper, we address the problem of learning low dimension\nrepresentation of entities on relational databases consisting of multiple\ntables. Embeddings help to capture semantics encoded in the database and can be\nused in a variety of settings like auto-completion of tables, fully-neural\nquery processing of relational joins queries, seamlessly handling missing\nvalues, and more. Current work is restricted to working with just single table,\nor using pretrained embeddings over an external corpus making them unsuitable\nfor use in real-world databases. In this work, we look into ways of using these\nattention-based model to learn embeddings for entities in the relational\ndatabase. We are inspired by BERT style pretraining methods and are interested\nin observing how they can be extended for representation learning on structured\ndatabases. We evaluate our approach of the autocompletion of relational\ndatabases and achieve improvement over standard baselines.", "published": "2021-04-30 11:23:26", "link": "http://arxiv.org/abs/2104.14914v1", "categories": ["cs.CL", "cs.DB", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Explanation-Based Human Debugging of NLP Models: A Survey", "abstract": "Debugging a machine learning model is hard since the bug usually involves the\ntraining data and the learning process. This becomes even harder for an opaque\ndeep learning model if we have no clue about how the model actually works. In\nthis survey, we review papers that exploit explanations to enable humans to\ngive feedback and debug NLP models. We call this problem explanation-based\nhuman debugging (EBHD). In particular, we categorize and discuss existing work\nalong three dimensions of EBHD (the bug context, the workflow, and the\nexperimental setting), compile findings on how EBHD components affect the\nfeedback providers, and highlight open problems that could be future research\ndirections.", "published": "2021-04-30 17:53:07", "link": "http://arxiv.org/abs/2104.15135v3", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An analysis of full-size Russian complexly NER labelled corpus of\n  Internet user reviews on the drugs based on deep learning and language neural\n  nets", "abstract": "We present the full-size Russian complexly NER-labeled corpus of Internet\nuser reviews, along with an evaluation of accuracy levels reached on this\ncorpus by a set of advanced deep learning neural networks to extract the\npharmacologically meaningful entities from Russian texts. The corpus annotation\nincludes mentions of the following entities: Medication (33005 mentions),\nAdverse Drug Reaction (1778), Disease (17403), and Note (4490). Two of them -\nMedication and Disease - comprise a set of attributes. A part of the corpus has\nthe coreference annotation with 1560 coreference chains in 300 documents.\nSpecial multi-label model based on a language model and the set of features is\ndeveloped, appropriate for presented corpus labeling. The influence of the\nchoice of different modifications of the models: word vector representations,\ntypes of language models pre-trained for Russian, text normalization styles,\nand other preliminary processing are analyzed. The sufficient size of our\ncorpus allows to study the effects of particularities of corpus labeling and\nbalancing entities in the corpus. As a result, the state of the art for the\npharmacological entity extraction problem for Russian is established on a\nfull-size labeled corpus. In case of the adverse drug reaction (ADR)\nrecognition, it is 61.1 by the F1-exact metric that, as our analysis shows, is\non par with the accuracy level for other language corpora with similar\ncharacteristics and the ADR representativnes. The evaluated baseline precision\nof coreference relation extraction on the corpus is 71, that is higher the\nresults reached on other Russian corpora.", "published": "2021-04-30 19:46:24", "link": "http://arxiv.org/abs/2105.00059v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dance Generation with Style Embedding: Learning and Transferring Latent\n  Representations of Dance Styles", "abstract": "Choreography refers to creation of dance steps and motions for dances\naccording to the latent knowledge in human mind, where the created dance\nmotions are in general style-specific and consistent. So far, such latent\nstyle-specific knowledge about dance styles cannot be represented explicitly in\nhuman language and has not yet been learned in previous works on music-to-dance\ngeneration tasks. In this paper, we propose a novel music-to-dance synthesis\nframework with controllable style embeddings. These embeddings are learned\nrepresentations of style-consistent kinematic abstraction of reference dance\nclips, which act as controllable factors to impose style constraints on dance\ngeneration in a latent manner. Thus, the dance styles can be transferred to\ndance motions by merely modifying the style embeddings. To support this study,\nwe build a large music-to-dance dataset. The qualitative and quantitative\nevaluations demonstrate the advantage of our proposed framework, as well as the\nability of synthesizing diverse styles of dances from identical music via style\nembeddings.", "published": "2021-04-30 07:36:49", "link": "http://arxiv.org/abs/2104.14802v1", "categories": ["cs.MM", "cs.GR", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Crackle Detection In Lung Sounds Using Transfer Learning And Multi-Input\n  Convolitional Neural Networks", "abstract": "Large annotated lung sound databases are publicly available and might be used\nto train algorithms for diagnosis systems. However, it might be a challenge to\ndevelop a well-performing algorithm for small non-public data, which have only\na few subjects and show differences in recording devices and setup. In this\npaper, we use transfer learning to tackle the mismatch of the recording setup.\nThis allows us to transfer knowledge from one dataset to another dataset for\ncrackle detection in lung sounds. In particular, a single input convolutional\nneural network (CNN) model is pre-trained on a source domain using ICBHI 2017,\nthe largest publicly available database of lung sounds. We use log-mel\nspectrogram features of respiratory cycles of lung sounds. The pre-trained\nnetwork is used to build a multi-input CNN model, which shares the same network\narchitecture for respiratory cycles and their corresponding respiratory phases.\nThe multi-input model is then fine-tuned on the target domain of our\nself-collected lung sound database for classifying crackles and normal lung\nsounds. Our experimental results show significant performance improvements of\n9.84% (absolute) in F-score on the target domain using the multi-input CNN\nmodel based on transfer learning for crackle detection in adventitious lung\nsound classification task.", "published": "2021-04-30 11:32:42", "link": "http://arxiv.org/abs/2104.14921v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Simple and Cheap Setup for Timing Tapping Responses Synchronized to\n  Auditory Stimuli", "abstract": "Measuring human capabilities to synchronize in time, adapt to perturbations\nto timing sequences or reproduce time intervals often require experimental\nsetups that allow recording response times with millisecond precision. Most\nsetups present auditory stimuli using either MIDI devices or specialized\nhardware such as Arduino and are often expensive or require calibration and\nadvanced programming skills. Here, we present in detail an experimental setup\nthat only requires an external sound card and minor electronic skills, works on\na conventional PC, is cheaper than alternatives and requires almost no\nprogramming skills. It is intended for presenting any auditory stimuli and\nrecording tapping response times with within 2 milliseconds precision (up to\n-2ms lag). This paper shows why desired accuracy in recording response times\nagainst auditory stimuli is difficult to achieve in conventional computer\nsetups, presents an experimental setup to overcome this and explains in detail\nhow to set it up and use the provided code. Finally, the code for analyzing the\nrecorded tapping responses was evaluated, showing that no spurious or missing\nevents were found in 94% of the analyzed recordings.", "published": "2021-04-30 21:30:40", "link": "http://arxiv.org/abs/2105.01570v2", "categories": ["q-bio.NC", "cs.SD", "eess.AS"], "primary_category": "q-bio.NC"}
