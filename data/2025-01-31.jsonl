{"title": "TRADES: Generating Realistic Market Simulations with Diffusion Models", "abstract": "Financial markets are complex systems characterized by high statistical\nnoise, nonlinearity, and constant evolution. Thus, modeling them is extremely\nhard. We address the task of generating realistic and responsive Limit Order\nBook (LOB) market simulations, which are fundamental for calibrating and\ntesting trading strategies, performing market impact experiments, and\ngenerating synthetic market data. Previous works lack realism, usefulness, and\nresponsiveness of the generated simulations. To bridge this gap, we propose a\nnovel TRAnsformer-based Denoising Diffusion Probabilistic Engine for LOB\nSimulations (TRADES). TRADES generates realistic order flows conditioned on the\nstate of the market, leveraging a transformer-based architecture that captures\nthe temporal and spatial characteristics of high-frequency market data. There\nis a notable absence of quantitative metrics for evaluating generative market\nsimulation models in the literature. To tackle this problem, we adapt the\npredictive score, a metric measured as an MAE, by training a stock price\npredictive model on synthetic data and testing it on real data. We compare\nTRADES with previous works on two stocks, reporting an x3.27 and x3.47\nimprovement over SoTA according to the predictive score, demonstrating that we\ngenerate useful synthetic market data for financial downstream tasks. We assess\nTRADES's market simulation realism and responsiveness, showing that it\neffectively learns the conditional data distribution and successfully reacts to\nan experimental agent, giving sprout to possible calibrations and evaluations\nof trading strategies and market impact experiments. We developed DeepMarket,\nthe first open-source Python framework for market simulation with deep\nlearning. Our repository includes a synthetic LOB dataset composed of TRADES's\ngenerates simulations. We release the code at\ngithub.com/LeonardoBerti00/DeepMarket.", "published": "2025-01-31 19:43:13", "link": "http://arxiv.org/abs/2502.07071v2", "categories": ["q-fin.TR", "cs.AI", "cs.LG", "q-fin.CP"], "primary_category": "q-fin.TR"}
{"title": "Year-over-Year Developments in Financial Fraud Detection via Deep Learning: A Systematic Literature Review", "abstract": "This paper systematically reviews advancements in deep learning (DL)\ntechniques for financial fraud detection, a critical issue in the financial\nsector. Using the Kitchenham systematic literature review approach, 57 studies\npublished between 2019 and 2024 were analyzed. The review highlights the\neffectiveness of various deep learning models such as Convolutional Neural\nNetworks, Long Short-Term Memory, and transformers across domains such as\ncredit card transactions, insurance claims, and financial statement audits.\nPerformance metrics such as precision, recall, F1-score, and AUC-ROC were\nevaluated. Key themes explored include the impact of data privacy frameworks\nand advancements in feature engineering and data preprocessing. The study\nemphasizes challenges such as imbalanced datasets, model interpretability, and\nethical considerations, alongside opportunities for automation and\nprivacy-preserving techniques such as blockchain integration and Principal\nComponent Analysis. By examining trends over the past five years, this review\nidentifies critical gaps and promising directions for advancing DL applications\nin financial fraud detection, offering actionable insights for researchers and\npractitioners.", "published": "2025-01-31 22:31:50", "link": "http://arxiv.org/abs/2502.00201v1", "categories": ["cs.LG", "cs.AI", "q-fin.ST"], "primary_category": "cs.LG"}
{"title": "Structural Embedding Projection for Contextual Large Language Model\n  Inference", "abstract": "Structured embedding transformations offer a promising approach for enhancing\nthe efficiency and coherence of language model inference. The introduction of\nStructural Embedding Projection (SEP) provides a mechanism for refining token\nrepresentations through projection matrices that integrate hierarchical and\nrelational dependencies. The mathematical formulation of SEP enables embedding\nspaces to capture structured contextual relationships, thereby improving\nsemantic fidelity without significantly increasing computational overhead.\nExperimental evaluations conducted on a range of linguistic datasets revealed\nthat SEP contributed to reductions in perplexity and enhanced contextual\ncoherence, demonstrating its potential to refine language model outputs.\nComputational efficiency assessments highlighted variations across different\ndatasets, suggesting that the integration of structured embeddings introduced\ndataset-dependent trade-offs between inference speed and representational\nrichness. The qualitative analysis of generated responses indicated that SEP\nenhanced narrative consistency and topic alignment, leading to improved fluency\nin multi-sentence text generation. The modifications to embedding layers\nrequired precise optimization to ensure stable training dynamics, as the\nintroduction of structured transformations altered the traditional\nrepresentation-learning process. The architectural adjustments necessary for\nSEP implementation influenced inference latency and memory consumption,\nrequiring a balance between efficiency gains and additional processing demands.\nThe impact of SEP on lexical diversity suggested that embedding modifications\ninfluenced the model's vocabulary usage, reflecting a more context-aware\nselection of generated tokens.", "published": "2025-01-31 00:46:21", "link": "http://arxiv.org/abs/2501.18826v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Data Augmentation for Large Language Models: A Comprehensive Survey\n  of Methods, Challenges, and Opportunities", "abstract": "The increasing size and complexity of pre-trained language models have\ndemonstrated superior performance in many applications, but they usually\nrequire large training datasets to be adequately trained. Insufficient training\nsets could unexpectedly make the model overfit and fail to cope with complex\ntasks. Large language models (LLMs) trained on extensive corpora have prominent\ntext generation capabilities, which improve the quality and quantity of data\nand play a crucial role in data augmentation. Specifically, distinctive prompt\ntemplates are given in personalised tasks to guide LLMs in generating the\nrequired content. Recent promising retrieval-based techniques further improve\nthe expressive performance of LLMs in data augmentation by introducing external\nknowledge to enable them to produce more grounded-truth data. This survey\nprovides an in-depth analysis of data augmentation in LLMs, classifying the\ntechniques into Simple Augmentation, Prompt-based Augmentation, Retrieval-based\nAugmentation and Hybrid Augmentation. We summarise the post-processing\napproaches in data augmentation, which contributes significantly to refining\nthe augmented data and enabling the model to filter out unfaithful content.\nThen, we provide the common tasks and evaluation metrics. Finally, we introduce\nexisting challenges and future opportunities that could bring further\nimprovement to data augmentation.", "published": "2025-01-31 01:50:49", "link": "http://arxiv.org/abs/2501.18845v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Supernet Training with Orthogonal Softmax for Scalable ASR\n  Model Compression", "abstract": "ASR systems are deployed across diverse environments, each with specific\nhardware constraints. We use supernet training to jointly train multiple\nencoders of varying sizes, enabling dynamic model size adjustment to fit\nhardware constraints without redundant training. Moreover, we introduce a novel\nmethod called OrthoSoftmax, which applies multiple orthogonal softmax functions\nto efficiently identify optimal subnets within the supernet, avoiding\nresource-intensive search. This approach also enables more flexible and precise\nsubnet selection by allowing selection based on various criteria and levels of\ngranularity. Our results with CTC on Librispeech and TED-LIUM-v2 show that\nFLOPs-aware component-wise selection achieves the best overall performance.\nWith the same number of training updates from one single job, WERs for all\nmodel sizes are comparable to or slightly better than those of individually\ntrained models. Furthermore, we analyze patterns in the selected components and\nreveal interesting insights.", "published": "2025-01-31 05:23:03", "link": "http://arxiv.org/abs/2501.18895v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Intrinsic Tensor Field Propagation in Large Language Models: A Novel\n  Approach to Contextual Information Flow", "abstract": "Context propagation remains a central challenge in language model\narchitectures, particularly in tasks requiring the retention of long-range\ndependencies. Conventional attention mechanisms, while effective in many\napplications, exhibit limitations in maintaining coherent contextual\nrepresentations over extended sequences due to their reliance on discrete token\ninteractions. A novel approach is introduced through the formulation of\nIntrinsic Tensor Field Propagation (ITFP), which models contextual\nrelationships as continuous tensor fields distributed across token embeddings.\nThe propagation dynamics are governed through differential equations that\nenable a structured flow of contextual information, augmenting the standard\nattention mechanism to enhance coherence and recall. A series of experiments\nconducted on an open-source transformer-based model demonstrate that ITFP\nprovides measurable improvements in contextual retention, dependency\nresolution, and inference stability across various linguistic structures.\nComparisons with baseline models reveal a reduction in syntactic\ninconsistencies and factual errors, while ablation studies indicate that the\nchoice of propagation depth and integration strength significantly impacts\nmodel performance. Additional evaluations assessing domain generalization\nsuggest that ITFP effectively adapts across different text genres, reinforcing\nits applicability beyond conventional language modeling tasks. Although\ncomputational trade-offs are introduced through the inclusion of tensor field\ncomputations, empirical findings suggest that the benefits in accuracy and\ncoherence outweigh the increased processing demands.", "published": "2025-01-31 08:32:32", "link": "http://arxiv.org/abs/2501.18957v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Calling a Spade a Heart: Gaslighting Multimodal Large Language Models\n  via Negation", "abstract": "Multimodal Large Language Models (MLLMs) have exhibited remarkable\nadvancements in integrating different modalities, excelling in complex\nunderstanding and generation tasks. Despite their success, MLLMs remain\nvulnerable to conversational adversarial inputs, particularly negation\narguments. This paper systematically evaluates state-of-the-art MLLMs across\ndiverse benchmarks, revealing significant performance drops when negation\narguments are introduced to initially correct responses. Notably, we introduce\nthe first benchmark GaslightingBench, specifically designed to evaluate the\nvulnerability of MLLMs to negation arguments. GaslightingBench consists of\nmultiple-choice questions curated from existing datasets, along with generated\nnegation prompts across 20 diverse categories. Throughout extensive evaluation,\nwe find that proprietary models such as Gemini-1.5-flash, GPT-4o and\nClaude-3.5-Sonnet demonstrate better resilience compared to open-source\ncounterparts like Qwen2-VL and LLaVA. However, all evaluated MLLMs struggle to\nmaintain logical consistency under negation arguments during conversation. Our\nfindings provide critical insights for improving the robustness of MLLMs\nagainst negation inputs, contributing to the development of more reliable and\ntrustworthy multimodal AI systems.", "published": "2025-01-31 10:37:48", "link": "http://arxiv.org/abs/2501.19017v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Impact of Noise in Differentially Private Text Rewriting", "abstract": "The field of text privatization often leverages the notion of\n$\\textit{Differential Privacy}$ (DP) to provide formal guarantees in the\nrewriting or obfuscation of sensitive textual data. A common and nearly\nubiquitous form of DP application necessitates the addition of calibrated noise\nto vector representations of text, either at the data- or model-level, which is\ngoverned by the privacy parameter $\\varepsilon$. However, noise addition almost\nundoubtedly leads to considerable utility loss, thereby highlighting one major\ndrawback of DP in NLP. In this work, we introduce a new sentence infilling\nprivatization technique, and we use this method to explore the effect of noise\nin DP text rewriting. We empirically demonstrate that non-DP privatization\ntechniques excel in utility preservation and can find an acceptable empirical\nprivacy-utility trade-off, yet cannot outperform DP methods in empirical\nprivacy protections. Our results highlight the significant impact of noise in\ncurrent DP rewriting mechanisms, leading to a discussion of the merits and\nchallenges of DP in NLP, as well as the opportunities that non-DP methods\npresent.", "published": "2025-01-31 10:45:24", "link": "http://arxiv.org/abs/2501.19022v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Low-Resource Sequence Labeling with Knowledge Fusion and\n  Contextual Label Explanations", "abstract": "Sequence labeling remains a significant challenge in low-resource,\ndomain-specific scenarios, particularly for character-dense languages like\nChinese. Existing methods primarily focus on enhancing model comprehension and\nimproving data diversity to boost performance. However, these approaches still\nstruggle with inadequate model applicability and semantic distribution biases\nin domain-specific contexts. To overcome these limitations, we propose a novel\nframework that combines an LLM-based knowledge enhancement workflow with a\nspan-based Knowledge Fusion for Rich and Efficient Extraction (KnowFREE) model.\nOur workflow employs explanation prompts to generate precise contextual\ninterpretations of target entities, effectively mitigating semantic biases and\nenriching the model's contextual understanding. The KnowFREE model further\nintegrates extension label features, enabling efficient nested entity\nextraction without relying on external knowledge during inference. Experiments\non multiple Chinese domain-specific sequence labeling datasets demonstrate that\nour approach achieves state-of-the-art performance, effectively addressing the\nchallenges posed by low-resource settings.", "published": "2025-01-31 12:39:28", "link": "http://arxiv.org/abs/2501.19093v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mixed Feelings: Cross-Domain Sentiment Classification of Patient\n  Feedback", "abstract": "Sentiment analysis of patient feedback from the public health domain can aid\ndecision makers in evaluating the provided services. The current paper focuses\non free-text comments in patient surveys about general practitioners and\npsychiatric healthcare, annotated with four sentence-level polarity classes --\npositive, negative, mixed and neutral -- while also attempting to alleviate\ndata scarcity by leveraging general-domain sources in the form of reviews. For\nseveral different architectures, we compare in-domain and out-of-domain\neffects, as well as the effects of training joint multi-domain models.", "published": "2025-01-31 13:44:46", "link": "http://arxiv.org/abs/2501.19134v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving the Robustness of Representation Misdirection for Large\n  Language Model Unlearning", "abstract": "Representation Misdirection (RM) and variants are established large language\nmodel (LLM) unlearning methods with state-of-the-art performance. In this\npaper, we show that RM methods inherently reduce models' robustness, causing\nthem to misbehave even when a single non-adversarial forget-token is in the\nretain-query. Toward understanding underlying causes, we reframe the unlearning\nprocess as backdoor attacks and defenses: forget-tokens act as backdoor\ntriggers that, when activated in retain-queries, cause disruptions in RM\nmodels' behaviors, similar to successful backdoor attacks. To mitigate this\nvulnerability, we propose Random Noise Augmentation -- a model and method\nagnostic approach with theoretical guarantees for improving the robustness of\nRM methods. Extensive experiments demonstrate that RNA significantly improves\nthe robustness of RM models while enhancing the unlearning performances.", "published": "2025-01-31 15:12:20", "link": "http://arxiv.org/abs/2501.19202v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VisualSpeech: Enhance Prosody with Visual Context in TTS", "abstract": "Text-to-Speech (TTS) synthesis faces the inherent challenge of producing\nmultiple speech outputs with varying prosody from a single text input. While\nprevious research has addressed this by predicting prosodic information from\nboth text and speech, additional contextual information, such as visual\nfeatures, remains underutilized. This paper investigates the potential of\nintegrating visual context to enhance prosody prediction. We propose a novel\nmodel, VisualSpeech, which incorporates both visual and textual information for\nimproved prosody generation. Empirical results demonstrate that visual features\nprovide valuable prosodic cues beyond the textual input, significantly\nenhancing the naturalness and accuracy of the synthesized speech. Audio samples\nare available at https://ariameetgit.github.io/VISUALSPEECH-SAMPLES/.", "published": "2025-01-31 16:16:52", "link": "http://arxiv.org/abs/2501.19258v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pheromone-based Learning of Optimal Reasoning Paths", "abstract": "Large Language Models (LLMs) have demonstrated remarkable reasoning\ncapabilities through chain-of-thought prompting, yet discovering effective\nreasoning methods for complex problems remains challenging due to the vast\nspace of possible intermediate steps. We introduce Ant Colony\nOptimization-guided Tree of Thought (ACO-ToT), a novel algorithm that combines\nACO with LLMs to discover optimal reasoning paths for complex problems\nefficiently. Drawing inspiration from Hebbian learning in neurological systems,\nour method employs a collection of distinctly fine-tuned LLM \"ants\" to traverse\nand lay pheromone trails through a centralized tree of thought, with each ant's\nmovement governed by a weighted combination of existing pheromone trails and\nits own specialized expertise. The algorithm evaluates complete reasoning paths\nusing a mixture-of-experts-based scoring function, with pheromones reinforcing\nproductive reasoning paths across iterations. Experiments on three challenging\nreasoning tasks (GSM8K, ARC-Challenge, and MATH) demonstrate that ACO-ToT\nperforms significantly better than existing chain-of-thought optimization\napproaches, suggesting that incorporating biologically inspired collective\nsearch mechanisms into LLM inference can substantially enhance reasoning\ncapabilities.", "published": "2025-01-31 16:42:31", "link": "http://arxiv.org/abs/2501.19278v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Efficient Approach for Machine Translation on Low-resource Languages:\n  A Case Study in Vietnamese-Chinese", "abstract": "Despite the rise of recent neural networks in machine translation, those\nnetworks do not work well if the training data is insufficient. In this paper,\nwe proposed an approach for machine translation in low-resource languages such\nas Vietnamese-Chinese. Our proposed method leveraged the power of the\nmultilingual pre-trained language model (mBART) and both Vietnamese and Chinese\nmonolingual corpus. Firstly, we built an early bird machine translation model\nusing the bilingual training dataset. Secondly, we used TF-IDF technique to\nselect sentences from the monolingual corpus which are the most related to\ndomains of the parallel dataset. Finally, the first model was used to\nsynthesize the augmented training data from the selected monolingual corpus for\nthe translation model. Our proposed scheme showed that it outperformed 8%\ncompared to the transformer model. The augmented dataset also pushed the model\nperformance.", "published": "2025-01-31 17:11:45", "link": "http://arxiv.org/abs/2501.19314v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reverse Probing: Evaluating Knowledge Transfer via Finetuned Task\n  Embeddings for Coreference Resolution", "abstract": "In this work, we reimagine classical probing to evaluate knowledge transfer\nfrom simple source to more complex target tasks. Instead of probing frozen\nrepresentations from a complex source task on diverse simple target probing\ntasks (as usually done in probing), we explore the effectiveness of embeddings\nfrom multiple simple source tasks on a single target task. We select\ncoreference resolution, a linguistically complex problem requiring contextual\nunderstanding, as focus target task, and test the usefulness of embeddings from\ncomparably simpler tasks tasks such as paraphrase detection, named entity\nrecognition, and relation extraction. Through systematic experiments, we\nevaluate the impact of individual and combined task embeddings.\n  Our findings reveal that task embeddings vary significantly in utility for\ncoreference resolution, with semantic similarity tasks (e.g., paraphrase\ndetection) proving most beneficial. Additionally, representations from\nintermediate layers of fine-tuned models often outperform those from final\nlayers. Combining embeddings from multiple tasks consistently improves\nperformance, with attention-based aggregation yielding substantial gains. These\ninsights shed light on relationships between task-specific representations and\ntheir adaptability to complex downstream tasks, encouraging further exploration\nof embedding-level task transfer.", "published": "2025-01-31 17:12:53", "link": "http://arxiv.org/abs/2501.19316v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM-based Affective Text Generation Quality Based on Different\n  Quantization Values", "abstract": "Large language models exhibit a remarkable capacity in language generation\nand comprehension. These advances enable AI systems to produce more human-like\nand emotionally engaging text. However, these models rely on a large number of\nparameters, requiring significant computational resources for training and\ninference. In some scenarios, accessing these resources can be challenging\n(e.g., budget or hardware limitations). Techniques like reducing precision bits\ncan make models more memory-efficient, reducing the computational resources\nneeded, at the cost of reduced accuracy. This paper addresses the trade-off\nbetween different quantization values, GPU RAM utilization, and text quality in\naffective text generation (e.g., \"I really enjoy running in the snow-covered\nforest\"). To evaluate, we use an emotion classifier and ten seed prompts to\ngenerate affective text. We test three setups of precision bits (8, 16, and 32)\nacross five open-weight language models from two different families. Our\nfindings demonstrate that bit reductions lead to memory savings, achieving a\nreduction of 76%. However, this optimization comes with a trade-off, leading to\na decrease of up to 10 pp in F1 score for larger models and an increase of 10\npp for smaller models, along with roughly double the inference time. In terms\nof text quality, larger models at lower quantization levels generally\noutperform smaller, higher-precision models -- while requiring similar memory.", "published": "2025-01-31 17:12:55", "link": "http://arxiv.org/abs/2501.19317v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TableMaster: A Recipe to Advance Table Understanding with Language\n  Models", "abstract": "Tables serve as a fundamental format for representing structured relational\ndata. While current language models (LMs) excel at many text-based tasks, they\nstill face challenges in table understanding due to the complex characteristics\nof tabular data, such as their structured nature. In this paper, we aim to\nenhance LMs for improved table understanding. We identify four key challenges:\n1) difficulty in locating target data, 2) deficiency in table semantics, 3)\nnumerical inaccuracies in textual reasoning, and 4) semantic inflexibility in\nsymbolic reasoning. To address these issues, we propose TableMaster, a recipe\nand comprehensive framework that integrates multiple solutions to overcome\nthese obstacles. TableMaster first extracts relevant table content and\nverbalizes it with enriched semantic context. Additionally, we introduce\nadaptive reasoning, a flexible approach that dynamically adjusts between\ntextual and symbolic reasoning, tailoring the reasoning process to each query.\nExtensive analyses and experiments demonstrate our findings and the\neffectiveness of TableMaster. On the WikiTQ dataset, TableMaster achieves an\naccuracy of 78.13% using GPT-4o-mini, surpassing existing baselines.", "published": "2025-01-31 18:31:31", "link": "http://arxiv.org/abs/2501.19378v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding", "abstract": "In Transformer-based sequence-to-sequence generation, beam search has proven\neffective in enhancing the quality of generated sequences compared to greedy\ndecoding. Conventional beam search methods typically adopt either a sequential\nor batch-based approach. The sequential approach, while memory-efficient,\nrequires multiple decoding passes to construct a complete search tree, leading\nto significantly slower inference. On the other hand, the batch-based approach\nenables parallel computation across beams, but at the expense of high memory\nconsumption due to the need to maintain separate key-value (KV) caches for each\nbeam. In this study, we introduce a novel trie (prefix-tree)-based parallel\ndecoding method that addresses the memory inefficiency of batch-based beam\nsearch. By sharing a single KV cache among all beams that share the same\nprefix, the proposed method not only reduces memory consumption dramatically\nbut also enables parallel decoding across all branches. This innovative use of\na prefix tree offers an efficient alternative for beam search, achieving\nsignificant memory savings while preserving inference speed, making it\nparticularly well-suited for memory-constrained environments or large-scale\nmodel deployments.", "published": "2025-01-31 16:22:36", "link": "http://arxiv.org/abs/2502.00085v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Disambiguating Numeral Sequences to Decipher Ancient Accounting Corpora", "abstract": "A numeration system encodes abstract numeric quantities as concrete strings\nof written characters. The numeration systems used by modern scripts tend to be\nprecise and unambiguous, but this was not so for the ancient and\npartially-deciphered proto-Elamite (PE) script, where written numerals can have\nup to four distinct readings depending on the system that is used to read them.\nWe consider the task of disambiguating between these readings in order to\ndetermine the values of the numeric quantities recorded in this corpus. We\nalgorithmically extract a list of possible readings for each PE numeral\nnotation, and contribute two disambiguation techniques based on structural\nproperties of the original documents and classifiers learned with the\nbootstrapping algorithm. We also contribute a test set for evaluating\ndisambiguation techniques, as well as a novel approach to cautious rule\nselection for bootstrapped classifiers. Our analysis confirms existing\nintuitions about this script and reveals previously-unknown correlations\nbetween tablet content and numeral magnitude. This work is crucial to\nunderstanding and deciphering PE, as the corpus is heavily accounting-focused\nand contains many more numeric tokens than tokens of text.", "published": "2025-01-31 18:10:31", "link": "http://arxiv.org/abs/2502.00090v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sparse Autoencoder Insights on Voice Embeddings", "abstract": "Recent advances in explainable machine learning have highlighted the\npotential of sparse autoencoders in uncovering mono-semantic features in\ndensely encoded embeddings. While most research has focused on Large Language\nModel (LLM) embeddings, the applicability of this technique to other domains\nremains largely unexplored. This study applies sparse autoencoders to speaker\nembeddings generated from a Titanet model, demonstrating the effectiveness of\nthis technique in extracting mono-semantic features from non-textual embedded\ndata. The results show that the extracted features exhibit characteristics\nsimilar to those found in LLM embeddings, including feature splitting and\nsteering. The analysis reveals that the autoencoder can identify and manipulate\nfeatures such as language and music, which are not evident in the original\nembedding. The findings suggest that sparse autoencoders can be a valuable tool\nfor understanding and interpreting embedded data in many domains, including\naudio-based speaker recognition.", "published": "2025-01-31 19:21:43", "link": "http://arxiv.org/abs/2502.00127v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Resolving Editing-Unlearning Conflicts: A Knowledge Codebook Framework\n  for Large Language Model Updating", "abstract": "Large Language Models (LLMs) excel in natural language processing by encoding\nextensive human knowledge, but their utility relies on timely updates as\nknowledge evolves. Updating LLMs involves two key tasks simultaneously:\nunlearning to remove unwanted knowledge and editing to incorporate new\ninformation. Existing methods face two major challenges: ineffective knowledge\nstorage (either too sparse or too dense) and task conflicts between editing and\nunlearning, as validated through our theoretical and experimental results. To\naddress these issues, we propose LOKA, a conflict-free framework for LLM\nupdating based on a knowledge codebook. During training, updated knowledge is\nstored in multiple codebook memories. To optimize knowledge storage, a\nsimilarity-aware knowledge mapping ensures that related knowledge pieces are\nclustered and allocated to the same memory. Additionally, LOKA resolves task\nconflicts by employing task-specific and multi-task memories guided by a\nconflict score. In the inference stage, LOKA retrieves the most relevant memory\nfrom the codebook and plugs it into the original LLM to apply the updated\nknowledge. A learning-based router controls codebook activation to further\nimprove knowledge utilization. Extensive experiments demonstrate the\neffectiveness of LOKA in LLM knowledge updating tasks.", "published": "2025-01-31 20:48:46", "link": "http://arxiv.org/abs/2502.00158v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridging the Reasoning Gap: Small LLMs Can Plan with Generalised\n  Strategies", "abstract": "Recent advancements in the reasoning skills of Large Language Models (LLMs)\ndemonstrate an increase in the ability of LLMs to solve simple planning tasks.\nHowever, as long as the driving force behind improved reasoning capability is\nthe size and complexity of the model, the financial and computational costs\nassociated with running them will also increase. This trend raises questions\nabout continued accessibility and whether these improvements will increase at\nthe same pace as models continue to grow in size and expense. We propose two\napproaches to enhance the reasoning ability of less resource-intensive LLMs.\n(1) Provide them with a generalised strategy for solving tasks within a given\ndomain, generated by a more resource-intensive LLM. (2) Exploit their\ncost-effectiveness by iteratively prompting these models to correct errors in\ntheir proposed solutions. Our empirical results from planning and mathematical\nreasoning tasks demonstrate that these methods improve the performance of less\nresource-intensive LLMs to levels comparable with their more resource-intensive\ncounterparts, at a fraction of the cost. Additionally, we show that the\nutilisation of generalised strategies in our experiments reduced the cost of\nthe less resource-intensive model by nearly 30 percent on average.", "published": "2025-01-31 00:28:29", "link": "http://arxiv.org/abs/2501.18817v1", "categories": ["cs.AI", "cs.CL", "I.2.8; I.2.6"], "primary_category": "cs.AI"}
{"title": "Memory-Efficient Fine-Tuning of Transformers via Token Selection", "abstract": "Fine-tuning provides an effective means to specialize pre-trained models for\nvarious downstream tasks. However, fine-tuning often incurs high memory\noverhead, especially for large transformer-based models, such as LLMs. While\nexisting methods may reduce certain parts of the memory required for\nfine-tuning, they still require caching all intermediate activations computed\nin the forward pass to update weights during the backward pass. In this work,\nwe develop TokenTune, a method to reduce memory usage, specifically the memory\nto store intermediate activations, in the fine-tuning of transformer-based\nmodels. During the backward pass, TokenTune approximates the gradient\ncomputation by backpropagating through just a subset of input tokens. Thus,\nwith TokenTune, only a subset of intermediate activations are cached during the\nforward pass. Also, TokenTune can be easily combined with existing methods like\nLoRA, further reducing the memory cost. We evaluate our approach on pre-trained\ntransformer models with up to billions of parameters, considering the\nperformance on multiple downstream tasks such as text classification and\nquestion answering in a few-shot learning setup. Overall, TokenTune achieves\nperformance on par with full fine-tuning or representative memory-efficient\nfine-tuning methods, while greatly reducing the memory footprint, especially\nwhen combined with other methods with complementary memory reduction\nmechanisms. We hope that our approach will facilitate the fine-tuning of large\ntransformers, in specializing them for specific domains or co-training them\nwith other neural components from a larger system. Our code is available at\nhttps://github.com/facebookresearch/tokentune.", "published": "2025-01-31 00:43:50", "link": "http://arxiv.org/abs/2501.18824v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Partially Rewriting a Transformer in Natural Language", "abstract": "The greatest ambition of mechanistic interpretability is to completely\nrewrite deep neural networks in a format that is more amenable to human\nunderstanding, while preserving their behavior and performance. In this paper,\nwe attempt to partially rewrite a large language model using simple natural\nlanguage explanations. We first approximate one of the feedforward networks in\nthe LLM with a wider MLP with sparsely activating neurons - a transcoder - and\nuse an automated interpretability pipeline to generate explanations for these\nneurons. We then replace the first layer of this sparse MLP with an LLM-based\nsimulator, which predicts the activation of each neuron given its explanation\nand the surrounding context. Finally, we measure the degree to which these\nmodifications distort the model's final output. With our pipeline, the model's\nincrease in loss is statistically similar to entirely replacing the sparse MLP\noutput with the zero vector. We employ the same protocol, this time using a\nsparse autoencoder, on the residual stream of the same layer and obtain similar\nresults. These results suggest that more detailed explanations are needed to\nimprove performance substantially above the zero ablation baseline.", "published": "2025-01-31 01:12:50", "link": "http://arxiv.org/abs/2501.18838v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Scalable Multi-phase Word Embedding Using Conjunctive Propositional\n  Clauses", "abstract": "The Tsetlin Machine (TM) architecture has recently demonstrated effectiveness\nin Machine Learning (ML), particularly within Natural Language Processing\n(NLP). It has been utilized to construct word embedding using conjunctive\npropositional clauses, thereby significantly enhancing our understanding and\ninterpretation of machine-derived decisions. The previous approach performed\nthe word embedding over a sequence of input words to consolidate the\ninformation into a cohesive and unified representation. However, that approach\nencounters scalability challenges as the input size increases. In this study,\nwe introduce a novel approach incorporating two-phase training to discover\ncontextual embeddings of input sequences. Specifically, this method\nencapsulates the knowledge for each input word within the dataset's vocabulary,\nsubsequently constructing embeddings for a sequence of input words utilizing\nthe extracted knowledge. This technique not only facilitates the design of a\nscalable model but also preserves interpretability. Our experimental findings\nrevealed that the proposed method yields competitive performance compared to\nthe previous approaches, demonstrating promising results in contrast to\nhuman-generated benchmarks. Furthermore, we applied the proposed approach to\nsentiment analysis on the IMDB dataset, where the TM embedding and the TM\nclassifier, along with other interpretable classifiers, offered a transparent\nend-to-end solution with competitive performance.", "published": "2025-01-31 10:39:04", "link": "http://arxiv.org/abs/2501.19018v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Beyond checkmate: exploring the creative chokepoints in AI text", "abstract": "Large Language Models (LLMs) have revolutionized Natural Language Processing\n(NLP) and Artificial Intelligence (AI), unlocking unprecedented capabilities.\nThis rapid advancement has spurred research into various aspects of LLMs, their\ntext generation & reasoning capability, and potential misuse, fueling the\nnecessity for robust detection methods. While numerous prior research has\nfocused on detecting LLM-generated text (AI text) and thus checkmating them,\nour study investigates a relatively unexplored territory: portraying the\nnuanced distinctions between human and AI texts across text segments. Whether\nLLMs struggle with or excel at incorporating linguistic ingenuity across\ndifferent text segments carries substantial implications for determining their\npotential as effective creative assistants to humans. Through an analogy with\nthe structure of chess games-comprising opening, middle, and end games-we\nanalyze text segments (introduction, body, and conclusion) to determine where\nthe most significant distinctions between human and AI texts exist. While AI\ntexts can approximate the body segment better due to its increased length, a\ncloser examination reveals a pronounced disparity, highlighting the importance\nof this segment in AI text detection. Additionally, human texts exhibit higher\ncross-segment differences compared to AI texts. Overall, our research can shed\nlight on the intricacies of human-AI text distinctions, offering novel insights\nfor text detection and understanding.", "published": "2025-01-31 16:57:01", "link": "http://arxiv.org/abs/2501.19301v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SETS: Leveraging Self-Verification and Self-Correction for Improved\n  Test-Time Scaling", "abstract": "Recent advancements in Large Language Models (LLMs) have created new\nopportunities to enhance performance on complex reasoning tasks by leveraging\ntest-time computation. However, conventional approaches such as repeated\nsampling with majority voting or reward model scoring, often face diminishing\nreturns as test-time compute scales, in addition to requiring costly\ntask-specific reward model training. In this paper, we present Self-Enhanced\nTest-Time Scaling (SETS), a novel method that leverages the self-verification\nand self-correction capabilities of recent advanced LLMs to overcome these\nlimitations. SETS integrates sampling, self-verification, and self-correction\ninto a unified framework, enabling efficient and scalable test-time computation\nfor improved capabilities at complex tasks. Through extensive experiments on\nchallenging planning and reasoning benchmarks, compared to the alternatives, we\ndemonstrate that SETS achieves significant performance improvements and more\nfavorable test-time scaling laws.", "published": "2025-01-31 17:03:16", "link": "http://arxiv.org/abs/2501.19306v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model\n  Alignment", "abstract": "The performance of large language models (LLMs) is closely linked to their\nunderlying size, leading to ever-growing networks and hence slower inference.\nSpeculative decoding has been proposed as a technique to accelerate\nautoregressive generation, leveraging a fast draft model to propose candidate\ntokens, which are then verified in parallel based on their likelihood under the\ntarget model. While this approach guarantees to reproduce the target output, it\nincurs a substantial penalty: many high-quality draft tokens are rejected, even\nwhen they represent objectively valid continuations. Indeed, we show that even\npowerful draft models such as GPT-4o, as well as human text cannot achieve high\nacceptance rates under the standard verification scheme. This severely limits\nthe speedup potential of current speculative decoding methods, as an early\nrejection becomes overwhelmingly likely when solely relying on alignment of\ndraft and target.\n  We thus ask the following question: Can we adapt verification to recognize\ncorrect, but non-aligned replies? To this end, we draw inspiration from the\nLLM-as-a-judge framework, which demonstrated that LLMs are able to rate answers\nin a versatile way. We carefully design a dataset to elicit the same capability\nin the target model by training a compact module on top of the embeddings to\nproduce ``judgements\" of the current continuation. We showcase our strategy on\nthe Llama-3.1 family, where our 8b/405B-Judge achieves a speedup of 9x over\nLlama-405B, while maintaining its quality on a large range of benchmarks. These\nbenefits remain present even in optimized inference frameworks, where our\nmethod reaches up to 141 tokens/s for 8B/70B-Judge and 129 tokens/s for 8B/405B\non 2 and 8 H100s respectively.", "published": "2025-01-31 17:09:53", "link": "http://arxiv.org/abs/2501.19309v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Reward-Guided Speculative Decoding for Efficient LLM Reasoning", "abstract": "We introduce Reward-Guided Speculative Decoding (RSD), a novel framework\naimed at improving the efficiency of inference in large language models (LLMs).\nRSD synergistically combines a lightweight draft model with a more powerful\ntarget model, incorporating a controlled bias to prioritize high-reward\noutputs, in contrast to existing speculative decoding methods that enforce\nstrict unbiasedness. RSD employs a process reward model to evaluate\nintermediate decoding steps and dynamically decide whether to invoke the target\nmodel, optimizing the trade-off between computational cost and output quality.\nWe theoretically demonstrate that a threshold-based mixture strategy achieves\nan optimal balance between resource utilization and performance. Extensive\nevaluations on challenging reasoning benchmarks, including Olympiad-level\ntasks, show that RSD delivers significant efficiency gains against decoding\nwith the target model only (up to 4.4x fewer FLOPs), while achieving\nsignificant better accuracy than parallel decoding method on average (up to\n+3.5). These results highlight RSD as a robust and cost-effective approach for\ndeploying LLMs in resource-intensive scenarios. The code is available at\nhttps://github.com/BaohaoLiao/RSD.", "published": "2025-01-31 17:19:57", "link": "http://arxiv.org/abs/2501.19324v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Homogeneity Bias as Differential Sampling Uncertainty in Language Models", "abstract": "Prior research show that Large Language Models (LLMs) and Vision-Language\nModels (VLMs) represent marginalized groups more homogeneously than dominant\ngroups. However, the mechanisms underlying this homogeneity bias remain\nrelatively unexplored. We propose that this bias emerges from systematic\ndifferences in the probability distributions from which tokens are sampled at\ninference-time. Analyzing three measures of uncertainty in token sampling\ndistributions-entropy, perplexity, and probability of differentiation-we find\nthat in some models, specifically GPT-4 Turbo and Llama-3.2, tokens are sampled\nmore deterministically when generating texts about marginalized groups (i.e.,\nBlack Americans and women) compared to their dominant group counterparts (i.e.,\nWhite Americans and men). While these findings may help explain homogeneity\nbias in certain models, the patterns did not replicate across all VLMs tested,\nsuggesting multiple mechanisms may contribute to homogeneity bias in AI.", "published": "2025-01-31 17:36:12", "link": "http://arxiv.org/abs/2501.19337v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "PixelWorld: Towards Perceiving Everything as Pixels", "abstract": "Existing foundation models typically process visual input as pixels and\ntextual input as tokens, a paradigm that contrasts with human perception, where\nboth modalities are processed in a unified manner. With the rise of embodied\nand agentic AI, where inputs primarily come from camera pixels, the need for a\nunified perception framework becomes increasingly evident. In this paper, we\npropose to unify all modalities (text, tables, code, diagrams, images, etc) as\npixel inputs, i.e. \"Perceive Everything as Pixels\" (PEAP). We introduce\nPixelWorld, a novel evaluation suite that unifies all the mentioned modalities\ninto pixel space to gauge the existing models' performance. Our findings show\nthat (1) PEAP outperforms baseline with token-based input in multimodal\ndatasets, benefiting from unified input for better disambiguation, (2)\nsignificant declines in reasoning and coding capabilities across all models\nwhen processing pixel-based input, underscoring the need to enhance foundation\nmodels' perceptual abilities, (3) larger models can maintain strong performance\non non-reasoning tasks under PEAP, while smaller models like Phi-3.5-V suffer\nsignificant performance degradation, (4) the attention pattern of PEAP is\nhighly aligned with text token input, (5) PEAP can be accelerated significantly\nby exploiting the spatial sparsity. We conclude that the existing frontier\nmodels are competent in pixel perception, however, there is still headroom for\nimprovement. Our code, dataset will be released upon acceptance.", "published": "2025-01-31 17:39:21", "link": "http://arxiv.org/abs/2501.19339v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "BTS: Harmonizing Specialized Experts into a Generalist LLM", "abstract": "We present Branch-Train-Stitch (BTS), an efficient and flexible training\nalgorithm for combining independently trained large language model (LLM)\nexperts into a single, capable generalist model. Following Li et al., we start\nwith a single seed language model which is branched into domain-specific (e.g.,\ncoding or math) experts with continual pretraining. BTS combines experts into a\ngeneralist model using lightweight stitch layers, which are inserted between\nfrozen experts and the seed LLM, and trained on a small datamix of the expert\ndomains. Stitch layers enable the seed LLM to integrate representations from\nany number of experts during the forward pass, allowing it to generalize to new\ndomains, despite remaining frozen. Because BTS does not alter the constituent\nLLMs, BTS provides a modular and flexible approach: experts can be easily\nremoved and new experts can be added with only a small amount of training.\nCompared to alternative model merging approaches, BTS yields the best\ngeneralist performance on a variety of downstream tasks, retaining the\nspecialized capabilities of each of the experts.", "published": "2025-01-31 07:54:34", "link": "http://arxiv.org/abs/2502.00075v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Three-Branch Checks-and-Balances Frameworkfor Context-Aware Ethical\n  Alignment of Large Language Models", "abstract": "This paper introduces a three-branch checks-and-balances framework for\nethical alignment of Large Language Models (LLMs), inspired by governmental\nsystems. It implements three independent yet interacting components: LLMs as\nthe executive branch for knowledge generation, DIKE as the legislative branch\nestablishing ethical guardrails, and ERIS as the judicial branch for contextual\ninterpretation. The adversarial DIKE-ERIS duality enables adaptation to diverse\ncultural contexts while upholding consistent ethical principles. This\narchitecture addresses limitations of reinforcement learning with human\nfeedback (RLHF) by providing interpretable, adaptable, and culturally-aware\nethical reasoning. Through self-supervised learning and adversarial testing,\nour framework demonstrates how emotional modeling can guide linguistic\nbehaviors toward ethical outcomes while preserving independence across\nknowledge generation, ethical oversight, and contextual interpretation.", "published": "2025-01-31 19:41:28", "link": "http://arxiv.org/abs/2502.00136v1", "categories": ["cs.CL", "cs.AI", "F.2.2"], "primary_category": "cs.CL"}
{"title": "Fairshare Data Pricing for Large Language Models", "abstract": "Training data is a pivotal resource for building large language models\n(LLMs), but unfair pricing in data markets poses a serious challenge for both\ndata buyers (e.g., LLM builders) and sellers (e.g., human annotators), which\ndiscourages market participation, reducing data quantity and quality. In this\npaper, we propose a fairshare pricing framework that sets training data prices\nusing data valuation methods to quantify their contribution to LLMs. In our\nframework, buyers make purchasing decisions using data valuation and sellers\nset prices to maximize their profits based on the anticipated buyer purchases.\nWe theoretically show that pricing derived from our framework is tightly linked\nto data valuation and buyers' budget, optimal for both buyers and sellers.\nThrough market simulations using current LLMs and datasets (math problems,\nmedical diagnosis, and physical reasoning), we show that our framework is\nfairshare for buyers by ensuring their purchased data is reflective of model\ntraining value, leading to higher LLM task performances per-dollar spent on\ndata, and fairshare for sellers by ensuring they sell their data at optimal\nprices. Our framework lays the foundation for future research on equitable and\nsustainable data markets for large-scale AI.", "published": "2025-01-31 22:27:34", "link": "http://arxiv.org/abs/2502.00198v1", "categories": ["cs.GT", "cs.CL"], "primary_category": "cs.GT"}
{"title": "Reward-aware Preference Optimization: A Unified Mathematical Framework\n  for Model Alignment", "abstract": "The rapid development of large language model (LLM) alignment algorithms has\nresulted in a complex and fragmented landscape, with limited clarity on the\neffectiveness of different methods and their inter-connections. This paper\nintroduces Reward-Aware Preference Optimization (RPO), a mathematical framework\nthat unifies popular preference optimization techniques in LLM alignment,\nincluding DPO, IPO, SimPO, and REINFORCE (LOO), among others. RPO provides a\nstructured approach to disentangle and systematically study the impact of\nvarious design choices, such as the optimization objective, the number of\nresponses per prompt, and the use of implicit versus explicit reward models, on\nLLM preference optimization. We additionally propose a new experimental setup\nthat enables the clean and direct ablation of such design choices. Through an\nextensive series of ablation studies within the RPO framework, we gain insights\ninto the critical factors shaping model alignment, offering practical guidance\non the most effective strategies for improving LLM alignment.", "published": "2025-01-31 22:39:04", "link": "http://arxiv.org/abs/2502.00203v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Large Language Models as Common-Sense Heuristics", "abstract": "While systems designed for solving planning tasks vastly outperform Large\nLanguage Models (LLMs) in this domain, they usually discard the rich semantic\ninformation embedded within task descriptions. In contrast, LLMs possess\nparametrised knowledge across a wide range of topics, enabling them to leverage\nthe natural language descriptions of planning tasks in their solutions.\nHowever, current research in this direction faces challenges in generating\ncorrect and executable plans. Furthermore, these approaches depend on the LLM\nto output solutions in an intermediate language, which must be translated into\nthe representation language of the planning task. We introduce a novel planning\nmethod, which leverages the parametrised knowledge of LLMs by using their\noutput as a heuristic for Hill-Climbing Search. This approach is further\nenhanced by prompting the LLM to generate a solution estimate to guide the\nsearch. Our method outperforms the task success rate of similar systems within\na common household environment by 22 percentage points, with consistently\nexecutable plans. All actions are encoded in their original representation,\ndemonstrating that strong results can be achieved without an intermediate\nlanguage, thus eliminating the need for a translation step.", "published": "2025-01-31 00:26:38", "link": "http://arxiv.org/abs/2501.18816v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.8"], "primary_category": "cs.CL"}
{"title": "Constitutional Classifiers: Defending against Universal Jailbreaks\n  across Thousands of Hours of Red Teaming", "abstract": "Large language models (LLMs) are vulnerable to universal jailbreaks-prompting\nstrategies that systematically bypass model safeguards and enable users to\ncarry out harmful processes that require many model interactions, like\nmanufacturing illegal substances at scale. To defend against these attacks, we\nintroduce Constitutional Classifiers: safeguards trained on synthetic data,\ngenerated by prompting LLMs with natural language rules (i.e., a constitution)\nspecifying permitted and restricted content. In over 3,000 estimated hours of\nred teaming, no red teamer found a universal jailbreak that could extract\ninformation from an early classifier-guarded LLM at a similar level of detail\nto an unguarded model across most target queries. On automated evaluations,\nenhanced classifiers demonstrated robust defense against held-out\ndomain-specific jailbreaks. These classifiers also maintain deployment\nviability, with an absolute 0.38% increase in production-traffic refusals and a\n23.7% inference overhead. Our work demonstrates that defending against\nuniversal jailbreaks while maintaining practical deployment viability is\ntractable.", "published": "2025-01-31 01:09:32", "link": "http://arxiv.org/abs/2501.18837v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language\n  Model Reasoning", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex reasoning tasks, yet generating reliable reasoning processes remains a\nsignificant challenge. We present a unified probabilistic framework that\nformalizes LLM reasoning through a novel graphical model incorporating latent\nthinking processes and evaluation signals. Within this framework, we introduce\nthe Bootstrapping Reinforced Thinking Process (BRiTE) algorithm, which works in\ntwo steps. First, it generates high-quality rationales by approximating the\noptimal thinking process through reinforcement learning, using a novel reward\nshaping mechanism. Second, it enhances the base LLM by maximizing the joint\nprobability of rationale generation with respect to the model's parameters.\nTheoretically, we demonstrate BRiTE's convergence at a rate of $1/T$ with $T$\nrepresenting the number of iterations. Empirical evaluations on math and coding\nbenchmarks demonstrate that our approach consistently improves performance\nacross different base models without requiring human-annotated thinking\nprocesses. In addition, BRiTE demonstrates superior performance compared to\nexisting algorithms that bootstrap thinking processes use alternative methods\nsuch as rejection sampling, and can even match or exceed the results achieved\nthrough supervised fine-tuning with human-annotated data.", "published": "2025-01-31 02:39:07", "link": "http://arxiv.org/abs/2501.18858v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree\n  Search", "abstract": "Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions with a large-scale structured knowledge base (KB). Despite\nadvancements with large language models (LLMs), KBQA still faces challenges in\nweak KB awareness, imbalance between effectiveness and efficiency, and high\nreliance on annotated data. To address these challenges, we propose KBQA-o1, a\nnovel agentic KBQA method with Monte Carlo Tree Search (MCTS). It introduces a\nReAct-based agent process for stepwise logical form generation with KB\nenvironment exploration. Moreover, it employs MCTS, a heuristic search method\ndriven by policy and reward models, to balance agentic exploration's\nperformance and search space. With heuristic exploration, KBQA-o1 generates\nhigh-quality annotations for further improvement by incremental fine-tuning.\nExperimental results show that KBQA-o1 outperforms previous low-resource KBQA\nmethods with limited annotated data, boosting Llama-3.1-8B model's GrailQA F1\nperformance to 78.5% compared to 48.5% of the previous sota method with\nGPT-3.5-turbo.", "published": "2025-01-31 06:59:49", "link": "http://arxiv.org/abs/2501.18922v1", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Language Games as the Pathway to Artificial Superhuman Intelligence", "abstract": "The evolution of large language models (LLMs) toward artificial superhuman\nintelligence (ASI) hinges on data reproduction, a cyclical process in which\nmodels generate, curate and retrain on novel data to refine capabilities.\nCurrent methods, however, risk getting stuck in a data reproduction trap:\noptimizing outputs within fixed human-generated distributions in a closed loop\nleads to stagnation, as models merely recombine existing knowledge rather than\nexplore new frontiers. In this paper, we propose language games as a pathway to\nexpanded data reproduction, breaking this cycle through three mechanisms: (1)\n\\textit{role fluidity}, which enhances data diversity and coverage by enabling\nmulti-agent systems to dynamically shift roles across tasks; (2) \\textit{reward\nvariety}, embedding multiple feedback criteria that can drive complex\nintelligent behaviors; and (3) \\textit{rule plasticity}, iteratively evolving\ninteraction constraints to foster learnability, thereby injecting continual\nnovelty. By scaling language games into global sociotechnical ecosystems,\nhuman-AI co-evolution generates unbounded data streams that drive open-ended\nexploration. This framework redefines data reproduction not as a closed loop\nbut as an engine for superhuman intelligence.", "published": "2025-01-31 07:10:40", "link": "http://arxiv.org/abs/2501.18924v1", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Adversarial Attacks on AI-Generated Text Detection Models: A Token\n  Probability-Based Approach Using Embeddings", "abstract": "In recent years, text generation tools utilizing Artificial Intelligence (AI)\nhave occasionally been misused across various domains, such as generating\nstudent reports or creative writings. This issue prompts plagiarism detection\nservices to enhance their capabilities in identifying AI-generated content.\nAdversarial attacks are often used to test the robustness of AI-text generated\ndetectors. This work proposes a novel textual adversarial attack on the\ndetection models such as Fast-DetectGPT. The method employs embedding models\nfor data perturbation, aiming at reconstructing the AI generated texts to\nreduce the likelihood of detection of the true origin of the texts.\nSpecifically, we employ different embedding techniques, including the Tsetlin\nMachine (TM), an interpretable approach in machine learning for this purpose.\nBy combining synonyms and embedding similarity vectors, we demonstrates the\nstate-of-the-art reduction in detection scores against Fast-DetectGPT.\nParticularly, in the XSum dataset, the detection score decreased from 0.4431 to\n0.2744 AUROC, and in the SQuAD dataset, it dropped from 0.5068 to 0.3532 AUROC.", "published": "2025-01-31 10:06:27", "link": "http://arxiv.org/abs/2501.18998v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DyPCL: Dynamic Phoneme-level Contrastive Learning for Dysarthric Speech\n  Recognition", "abstract": "Dysarthric speech recognition often suffers from performance degradation due\nto the intrinsic diversity of dysarthric severity and extrinsic disparity from\nnormal speech. To bridge these gaps, we propose a Dynamic Phoneme-level\nContrastive Learning (DyPCL) method, which leads to obtaining invariant\nrepresentations across diverse speakers. We decompose the speech utterance into\nphoneme segments for phoneme-level contrastive learning, leveraging dynamic\nconnectionist temporal classification alignment. Unlike prior studies focusing\non utterance-level embeddings, our granular learning allows discrimination of\nsubtle parts of speech. In addition, we introduce dynamic curriculum learning,\nwhich progressively transitions from easy negative samples to\ndifficult-to-distinguishable negative samples based on phonetic similarity of\nphoneme. Our approach to training by difficulty levels alleviates the inherent\nvariability of speakers, better identifying challenging speeches. Evaluated on\nthe UASpeech dataset, DyPCL outperforms baseline models, achieving an average\n22.10\\% relative reduction in word error rate (WER) across the overall\ndysarthria group.", "published": "2025-01-31 10:25:42", "link": "http://arxiv.org/abs/2501.19010v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Importing Phantoms: Measuring LLM Package Hallucination Vulnerabilities", "abstract": "Large Language Models (LLMs) have become an essential tool in the\nprogrammer's toolkit, but their tendency to hallucinate code can be used by\nmalicious actors to introduce vulnerabilities to broad swathes of the software\nsupply chain. In this work, we analyze package hallucination behaviour in LLMs\nacross popular programming languages examining both existing package references\nand fictional dependencies. By analyzing this package hallucination behaviour\nwe find potential attacks and suggest defensive strategies to defend against\nthese attacks. We discover that package hallucination rate is predicated not\nonly on model choice, but also programming language, model size, and\nspecificity of the coding task request. The Pareto optimality boundary between\ncode generation performance and package hallucination is sparsely populated,\nsuggesting that coding models are not being optimized for secure code.\nAdditionally, we find an inverse correlation between package hallucination rate\nand the HumanEval coding benchmark, offering a heuristic for evaluating the\npropensity of a model to hallucinate packages. Our metrics, findings and\nanalyses provide a base for future models, securing AI-assisted software\ndevelopment workflows against package supply chain attacks.", "published": "2025-01-31 10:26:18", "link": "http://arxiv.org/abs/2501.19012v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Enabling Autonomic Microservice Management through Self-Learning Agents", "abstract": "The increasing complexity of modern software systems necessitates robust\nautonomic self-management capabilities. While Large Language Models (LLMs)\ndemonstrate potential in this domain, they often face challenges in adapting\ntheir general knowledge to specific service contexts. To address this\nlimitation, we propose ServiceOdyssey, a self-learning agent system that\nautonomously manages microservices without requiring prior knowledge of\nservice-specific configurations. By leveraging curriculum learning principles\nand iterative exploration, ServiceOdyssey progressively develops a deep\nunderstanding of operational environments, reducing dependence on human input\nor static documentation. A prototype built with the Sock Shop microservice\ndemonstrates the potential of this approach for autonomic microservice\nmanagement.", "published": "2025-01-31 11:32:05", "link": "http://arxiv.org/abs/2501.19056v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.SE"}
{"title": "Efficient Reasoning with Hidden Thinking", "abstract": "Chain-of-Thought (CoT) reasoning has become a powerful framework for\nimproving complex problem-solving capabilities in Multimodal Large Language\nModels (MLLMs). However, the verbose nature of textual reasoning introduces\nsignificant inefficiencies. In this work, we propose $\\textbf{Heima}$ (as\nhidden llama), an efficient reasoning framework that leverages reasoning CoTs\nat hidden latent space. We design the Heima Encoder to condense each\nintermediate CoT into a compact, higher-level hidden representation using a\nsingle thinking token, effectively minimizing verbosity and reducing the\noverall number of tokens required during the reasoning process. Meanwhile, we\ndesign corresponding Heima Decoder with traditional Large Language Models\n(LLMs) to adaptively interpret the hidden representations into variable-length\ntextual sequence, reconstructing reasoning processes that closely resemble the\noriginal CoTs. Experimental results across diverse reasoning MLLM benchmarks\ndemonstrate that Heima model achieves higher generation efficiency while\nmaintaining or even better zero-shot task accuracy. Moreover, the effective\nreconstruction of multimodal reasoning processes with Heima Decoder validates\nboth the robustness and interpretability of our approach.", "published": "2025-01-31 15:10:29", "link": "http://arxiv.org/abs/2501.19201v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "mFollowIR: a Multilingual Benchmark for Instruction Following in\n  Retrieval", "abstract": "Retrieval systems generally focus on web-style queries that are short and\nunderspecified. However, advances in language models have facilitated the\nnascent rise of retrieval models that can understand more complex queries with\ndiverse intents. However, these efforts have focused exclusively on English;\ntherefore, we do not yet understand how they work across languages. We\nintroduce mFollowIR, a multilingual benchmark for measuring\ninstruction-following ability in retrieval models. mFollowIR builds upon the\nTREC NeuCLIR narratives (or instructions) that span three diverse languages\n(Russian, Chinese, Persian) giving both query and instruction to the retrieval\nmodels. We make small changes to the narratives and isolate how well retrieval\nmodels can follow these nuanced changes. We present results for both\nmultilingual (XX-XX) and cross-lingual (En-XX) performance. We see strong\ncross-lingual performance with English-based retrievers that trained using\ninstructions, but find a notable drop in performance in the multilingual\nsetting, indicating that more work is needed in developing data for\ninstruction-based multilingual retrievers.", "published": "2025-01-31 16:24:46", "link": "http://arxiv.org/abs/2501.19264v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Do Large Multimodal Models Solve Caption Generation for Scientific\n  Figures? Lessons Learned from SciCap Challenge 2023", "abstract": "Since the SciCap datasets launch in 2021, the research community has made\nsignificant progress in generating captions for scientific figures in scholarly\narticles. In 2023, the first SciCap Challenge took place, inviting global teams\nto use an expanded SciCap dataset to develop models for captioning diverse\nfigure types across various academic fields. At the same time, text generation\nmodels advanced quickly, with many powerful pre-trained large multimodal models\n(LMMs) emerging that showed impressive capabilities in various\nvision-and-language tasks. This paper presents an overview of the first SciCap\nChallenge and details the performance of various models on its data, capturing\na snapshot of the fields state. We found that professional editors\noverwhelmingly preferred figure captions generated by GPT-4V over those from\nall other models and even the original captions written by authors. Following\nthis key finding, we conducted detailed analyses to answer this question: Have\nadvanced LMMs solved the task of generating captions for scientific figures?", "published": "2025-01-31 18:02:19", "link": "http://arxiv.org/abs/2501.19353v3", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "We're Different, We're the Same: Creative Homogeneity Across LLMs", "abstract": "Numerous powerful large language models (LLMs) are now available for use as\nwriting support tools, idea generators, and beyond. Although these LLMs are\nmarketed as helpful creative assistants, several works have shown that using an\nLLM as a creative partner results in a narrower set of creative outputs.\nHowever, these studies only consider the effects of interacting with a single\nLLM, begging the question of whether such narrowed creativity stems from using\na particular LLM -- which arguably has a limited range of outputs -- or from\nusing LLMs in general as creative assistants. To study this question, we elicit\ncreative responses from humans and a broad set of LLMs using standardized\ncreativity tests and compare the population-level diversity of responses. We\nfind that LLM responses are much more similar to other LLM responses than human\nresponses are to each other, even after controlling for response structure and\nother key variables. This finding of significant homogeneity in creative\noutputs across the LLMs we evaluate adds a new dimension to the ongoing\nconversation about creativity and LLMs. If today's LLMs behave similarly, using\nthem as a creative partners -- regardless of the model used -- may drive all\nusers towards a limited set of \"creative\" outputs.", "published": "2025-01-31 18:12:41", "link": "http://arxiv.org/abs/2501.19361v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "SELMA: A Speech-Enabled Language Model for Virtual Assistant\n  Interactions", "abstract": "In this work, we present and evaluate SELMA, a Speech-Enabled Language Model\nfor virtual Assistant interactions that integrates audio and text as inputs to\na Large Language Model (LLM). SELMA is designed to handle three primary and two\nauxiliary tasks related to interactions with virtual assistants simultaneously\nwithin a single end-to-end model. We employ low-rank adaptation modules for\nparameter-efficient training of both the audio encoder and the LLM.\nAdditionally, we implement a feature pooling strategy enabling the system to\nrecognize global patterns and improve accuracy on tasks less reliant on\nindividual sequence elements. Experimental results on Voice Trigger (VT)\ndetection, Device-Directed Speech Detection (DDSD), and Automatic Speech\nRecognition (ASR), demonstrate that our approach both simplifies the typical\ninput processing pipeline of virtual assistants significantly and also improves\nperformance compared to dedicated models for each individual task. SELMA yields\nrelative Equal-Error Rate improvements of 64% on the VT detection task, and 22%\non DDSD, while also achieving word error rates close to the baseline.", "published": "2025-01-31 18:30:36", "link": "http://arxiv.org/abs/2501.19377v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Decoding-based Regression", "abstract": "Language models have recently been shown capable of performing regression\ntasks wherein numeric predictions are represented as decoded strings. In this\nwork, we provide theoretical grounds for this capability and furthermore\ninvestigate the utility of causal auto-regressive sequence models when they are\napplied to any feature representation. We find that, despite being trained in\nthe usual way - for next-token prediction via cross-entropy loss -\ndecoding-based regression is as performant as traditional approaches for\ntabular regression tasks, while being flexible enough to capture arbitrary\ndistributions, such as in the task of density estimation.", "published": "2025-01-31 18:37:42", "link": "http://arxiv.org/abs/2501.19383v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "s1: Simple test-time scaling", "abstract": "Test-time scaling is a promising new approach to language modeling that uses\nextra test-time compute to improve performance. Recently, OpenAI's o1 model\nshowed this capability but did not publicly share its methodology, leading to\nmany replication efforts. We seek the simplest approach to achieve test-time\nscaling and strong reasoning performance. First, we curate a small dataset s1K\nof 1,000 questions paired with reasoning traces relying on three criteria we\nvalidate through ablations: difficulty, diversity, and quality. Second, we\ndevelop budget forcing to control test-time compute by forcefully terminating\nthe model's thinking process or lengthening it by appending \"Wait\" multiple\ntimes to the model's generation when it tries to end. This can lead the model\nto double-check its answer, often fixing incorrect reasoning steps. After\nsupervised finetuning the Qwen2.5-32B-Instruct language model on s1K and\nequipping it with budget forcing, our model s1-32B exceeds o1-preview on\ncompetition math questions by up to 27% (MATH and AIME24). Further, scaling\ns1-32B with budget forcing allows extrapolating beyond its performance without\ntest-time intervention: from 50% to 57% on AIME24. Our model, data, and code\nare open-source at https://github.com/simplescaling/s1", "published": "2025-01-31 18:48:08", "link": "http://arxiv.org/abs/2501.19393v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scalable-Softmax Is Superior for Attention", "abstract": "The maximum element of the vector output by the Softmax function approaches\nzero as the input vector size increases. Transformer-based language models rely\non Softmax to compute attention scores, causing the attention distribution to\nflatten as the context size grows. This reduces the model's ability to\nprioritize key information effectively and potentially limits its length\ngeneralization. To address this problem, we propose Scalable-Softmax (SSMax),\nwhich replaces Softmax in scenarios where the input vector size varies. SSMax\ncan be seamlessly integrated into existing Transformer-based architectures.\nExperimental results in language modeling show that models using SSMax not only\nachieve faster loss reduction during pretraining but also significantly improve\nperformance in long contexts and key information retrieval. Furthermore, an\nanalysis of attention scores reveals that SSMax enables the model to focus\nattention on key information even in long contexts. Additionally, although\nmodels that use SSMax from the beginning of pretraining achieve better length\ngeneralization, those that have already started pretraining can still gain some\nof this ability by replacing Softmax in the attention layers with SSMax, either\nduring or after pretraining.", "published": "2025-01-31 18:55:35", "link": "http://arxiv.org/abs/2501.19399v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLM Cyber Evaluations Don't Capture Real-World Risk", "abstract": "Large language models (LLMs) are demonstrating increasing prowess in\ncybersecurity applications, creating creating inherent risks alongside their\npotential for strengthening defenses. In this position paper, we argue that\ncurrent efforts to evaluate risks posed by these capabilities are misaligned\nwith the goal of understanding real-world impact. Evaluating LLM cybersecurity\nrisk requires more than just measuring model capabilities -- it demands a\ncomprehensive risk assessment that incorporates analysis of threat actor\nadoption behavior and potential for impact. We propose a risk assessment\nframework for LLM cyber capabilities and apply it to a case study of language\nmodels used as cybersecurity assistants. Our evaluation of frontier models\nreveals high compliance rates but moderate accuracy on realistic cyber\nassistance tasks. However, our framework suggests that this particular use case\npresents only moderate risk due to limited operational advantages and impact\npotential. Based on these findings, we recommend several improvements to align\nresearch priorities with real-world impact assessment, including closer\nacademia-industry collaboration, more realistic modeling of attacker behavior,\nand inclusion of economic metrics in evaluations. This work represents an\nimportant step toward more effective assessment and mitigation of LLM-enabled\ncybersecurity risks.", "published": "2025-01-31 05:33:48", "link": "http://arxiv.org/abs/2502.00072v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Ensembles of Low-Rank Expert Adapters", "abstract": "The training and fine-tuning of large language models (LLMs) often involve\ndiverse textual data from multiple sources, which poses challenges due to\nconflicting gradient directions, hindering optimization and specialization.\nThese challenges can undermine model generalization across tasks, resulting in\nreduced downstream performance. Recent research suggests that fine-tuning LLMs\non carefully selected, task-specific subsets of data can match or even surpass\nthe performance of using the entire dataset. Building on these insights, we\npropose the Ensembles of Low-Rank Expert Adapters (ELREA) framework to improve\nthe model's capability to handle diverse tasks. ELREA clusters the training\ninstructions based on their gradient directions, representing different areas\nof expertise and thereby reducing conflicts during optimization. Expert\nadapters are then trained on these clusters, utilizing the low-rank adaptation\n(LoRA) technique to ensure training efficiency and model scalability. During\ninference, ELREA combines predictions from the most relevant expert adapters\nbased on the input data's gradient similarity to the training clusters,\nensuring optimal adapter selection for each task. Experiments show that our\nmethod outperforms baseline LoRA adapters trained on the full dataset and other\nensemble approaches with similar training and inference complexity across a\nrange of domain-specific tasks.", "published": "2025-01-31 18:07:21", "link": "http://arxiv.org/abs/2502.00089v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DermaSynth: Rich Synthetic Image-Text Pairs Using Open Access\n  Dermatology Datasets", "abstract": "A major barrier to developing vision large language models (LLMs) in\ndermatology is the lack of large image--text pairs dataset. We introduce\nDermaSynth, a dataset comprising of 92,020 synthetic image--text pairs curated\nfrom 45,205 images (13,568 clinical and 35,561 dermatoscopic) for\ndermatology-related clinical tasks. Leveraging state-of-the-art LLMs, using\nGemini 2.0, we used clinically related prompts and self-instruct method to\ngenerate diverse and rich synthetic texts. Metadata of the datasets were\nincorporated into the input prompts by targeting to reduce potential\nhallucinations. The resulting dataset builds upon open access dermatological\nimage repositories (DERM12345, BCN20000, PAD-UFES-20, SCIN, and HIBA) that have\npermissive CC-BY-4.0 licenses. We also fine-tuned a preliminary\nLlama-3.2-11B-Vision-Instruct model, DermatoLlama 1.0, on 5,000 samples. We\nanticipate this dataset to support and accelerate AI research in dermatology.\nData and code underlying this work are accessible at\nhttps://github.com/abdurrahimyilmaz/DermaSynth.", "published": "2025-01-31 22:26:33", "link": "http://arxiv.org/abs/2502.00196v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Should You Use Your Large Language Model to Explore or Exploit?", "abstract": "We evaluate the ability of the current generation of large language models\n(LLMs) to help a decision-making agent facing an exploration-exploitation\ntradeoff. We use LLMs to explore and exploit in silos in various (contextual)\nbandit tasks. We find that while the current LLMs often struggle to exploit,\nin-context mitigations may be used to substantially improve performance for\nsmall-scale tasks. However even then, LLMs perform worse than a simple linear\nregression. On the other hand, we find that LLMs do help at exploring large\naction spaces with inherent semantics, by suggesting suitable candidates to\nexplore.", "published": "2025-01-31 23:42:53", "link": "http://arxiv.org/abs/2502.00225v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Large Language Models' Accuracy in Emulating Human Experts' Evaluation\n  of Public Sentiments about Heated Tobacco Products on Social Media", "abstract": "Sentiment analysis of alternative tobacco products on social media is\nimportant for tobacco control research. Large Language Models (LLMs) can help\nstreamline the labor-intensive human sentiment analysis process. This study\nexamined the accuracy of LLMs in replicating human sentiment evaluation of\nsocial media messages about heated tobacco products (HTPs).\n  The research used GPT-3.5 and GPT-4 Turbo to classify 500 Facebook and 500\nTwitter messages, including anti-HTPs, pro-HTPs, and neutral messages. The\nmodels evaluated each message up to 20 times, and their majority label was\ncompared to human evaluators.\n  Results showed that GPT-3.5 accurately replicated human sentiment 61.2% of\nthe time for Facebook messages and 57.0% for Twitter messages. GPT-4 Turbo\nperformed better, with 81.7% accuracy for Facebook and 77.0% for Twitter. Using\nthree response instances, GPT-4 Turbo achieved 99% of the accuracy of twenty\ninstances. GPT-4 Turbo also had higher accuracy for anti- and pro-HTPs messages\ncompared to neutral ones. Misclassifications by GPT-3.5 often involved anti- or\npro-HTPs messages being labeled as neutral or irrelevant, while GPT-4 Turbo\nshowed improvements across all categories.\n  In conclusion, LLMs can be used for sentiment analysis of HTP-related social\nmedia messages, with GPT-4 Turbo reaching around 80% accuracy compared to human\nexperts. However, there's a risk of misrepresenting overall sentiment due to\ndifferences in accuracy across sentiment categories.", "published": "2025-01-31 20:35:30", "link": "http://arxiv.org/abs/2502.01658v1", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Accelerating LLM Inference with Lossless Speculative Decoding Algorithms\n  for Heterogeneous Vocabularies", "abstract": "Accelerating the inference of large language models (LLMs) is a critical\nchallenge in generative AI. Speculative decoding (SD) methods offer substantial\nefficiency gains by generating multiple tokens using a single target forward\npass. However, existing SD approaches require the drafter and target models to\nshare the same vocabulary, thus limiting the pool of possible drafters, often\nnecessitating the training of a drafter from scratch. We present three new SD\nmethods that remove this shared-vocabulary constraint. All three methods\npreserve the target distribution (i.e., they are lossless) and work with\noff-the-shelf models without requiring additional training or modifications.\nEmpirically, on summarization, programming, and long-context tasks, our\nalgorithms achieve significant speedups over standard autoregressive decoding.\nBy enabling any off-the-shelf model to serve as drafter and requiring no\nretraining, this work substantially broadens the applicability of the SD\nframework in practice.", "published": "2025-01-31 19:13:58", "link": "http://arxiv.org/abs/2502.05202v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Bias in Self-Supervised Learning For Automatic Speech\n  Recognition", "abstract": "Self-supervised learning (SSL) is used in deep learning to train on large\ndatasets without the need for expensive labelling of the data. Recently, large\nAutomatic Speech Recognition (ASR) models such as XLS-R have utilised SSL to\ntrain on over one hundred different languages simultaneously. However, deeper\ninvestigation shows that the bulk of the training data for XLS-R comes from a\nsmall number of languages. Biases learned through SSL have been shown to exist\nin multiple domains, but language bias in multilingual SSL ASR has not been\nthoroughly examined. In this paper, we utilise the Lottery Ticket Hypothesis\n(LTH) to identify language-specific subnetworks within XLS-R and test the\nperformance of these subnetworks on a variety of different languages. We are\nable to show that when fine-tuning, XLS-R bypasses traditional linguistic\nknowledge and builds only on weights learned from the languages with the\nlargest data contribution to the pretraining data.", "published": "2025-01-31 17:16:45", "link": "http://arxiv.org/abs/2501.19321v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "eess.SP"], "primary_category": "eess.AS"}
{"title": "AIN: The Arabic INclusive Large Multimodal Model", "abstract": "Amid the swift progress of large language models (LLMs) and their evolution\ninto large multimodal models (LMMs), significant strides have been made in\nhigh-resource languages such as English and Chinese. While Arabic LLMs have\nseen notable progress, Arabic LMMs remain largely unexplored, often narrowly\nfocusing on a few specific aspects of the language and visual understanding. To\nbridge this gap, we introduce AIN-the Arabic Inclusive Multimodal\nModel-designed to excel across diverse domains. AIN is an English-Arabic\nbilingual LMM designed to excel in English and Arabic, leveraging carefully\nconstructed 3.6 million high-quality Arabic-English multimodal data samples.\nAIN demonstrates state-of-the-art Arabic performance, while also possessing\nstrong English-language visual capabilities. On the recent CAMEL-Bench\nbenchmark comprising 38 sub-domains including, multi-image understanding,\ncomplex visual perception, handwritten document understanding, video\nunderstanding, medical imaging, plant diseases, and remote sensing-based land\nuse understanding, our AIN demonstrates strong performance with the 7B model\noutperforming GPT-4o by an absolute gain of 3.4% averaged over eight domains\nand 38 sub-domains. AIN's superior capabilities position it as a significant\nstep toward empowering Arabic speakers with advanced multimodal generative AI\ntools across diverse applications.", "published": "2025-01-31 18:58:20", "link": "http://arxiv.org/abs/2502.00094v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Superhuman Game AI Disclosure: Expertise and Context Moderate Effects on\n  Trust and Fairness", "abstract": "As artificial intelligence surpasses human performance in select tasks,\ndisclosing superhuman capabilities poses distinct challenges for fairness,\naccountability, and trust. However, the impact of such disclosures on diverse\nuser attitudes and behaviors remains unclear, particularly concerning potential\nnegative reactions like discouragement or overreliance. This paper investigates\nthese effects by utilizing Persona Cards: a validated, standardized set of\nsynthetic personas designed to simulate diverse user reactions and fairness\nperspectives. We conducted an ethics board-approved study (N=32), utilizing\nthese personas to investigate how capability disclosure influenced behaviors\nwith a superhuman game AI in competitive StarCraft II scenarios. Our results\nreveal transparency is double-edged: while disclosure could alleviate\nsuspicion, it also provoked frustration and strategic defeatism among novices\nin cooperative scenarios, as well as overreliance in competitive contexts.\nExperienced and competitive players interpreted disclosure as confirmation of\nan unbeatable opponent, shifting to suboptimal goals. We release the Persona\nCards Dataset, including profiles, prompts, interaction logs, and protocols, to\nfoster reproducible research into human alignment AI design. This work\ndemonstrates that transparency is not a cure-all; successfully leveraging\ndisclosure to enhance trust and accountability requires careful tailoring to\nuser characteristics, domain norms, and specific fairness objectives.", "published": "2025-01-31 05:50:50", "link": "http://arxiv.org/abs/2503.15514v2", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.ET", "K.4.1; K.4.3; H.5.2; H.5.1; I.2.7"], "primary_category": "cs.HC"}
{"title": "Deepfake Detection of Singing Voices With Whisper Encodings", "abstract": "The deepfake generation of singing vocals is a concerning issue for artists\nin the music industry. In this work, we propose a singing voice deepfake\ndetection (SVDD) system, which uses noise-variant encodings of open-AI's\nWhisper model. As counter-intuitive as it may sound, even though the Whisper\nmodel is known to be noise-robust, the encodings are rich in non-speech\ninformation, and are noise-variant. This leads us to evaluate Whisper encodings\nas feature representations for the SVDD task. Therefore, in this work, the SVDD\ntask is performed on vocals and mixtures, and the performance is evaluated in\n\\%EER over varying Whisper model sizes and two classifiers- CNN and ResNet34,\nunder different testing conditions.", "published": "2025-01-31 06:43:50", "link": "http://arxiv.org/abs/2501.18919v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
