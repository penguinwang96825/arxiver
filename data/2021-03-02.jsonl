{"title": "Hindi-Urdu Adposition and Case Supersenses v1.0", "abstract": "These are the guidelines for the application of SNACS (Semantic Network of\nAdposition and Case Supersenses; Schneider et al. 2018) to Modern Standard\nHindi of Delhi. SNACS is an inventory of 50 supersenses (semantic labels) for\nlabelling the use of adpositions and case markers with respect to both\nlexical-semantic function and relation to the underlying context. The English\nguidelines (Schneider et al., 2020) were used as a model for this document.\n  Besides the case system, Hindi has an extremely rich adpositional system\nbuilt on the oblique genitive, with productive incorporation of loanwords even\nin present-day Hinglish.\n  This document is aligned with version 2.5 of the English guidelines.", "published": "2021-03-02 01:25:02", "link": "http://arxiv.org/abs/2103.01399v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Unsupervised Word Segmentation with Bi-directional Neural Language Model", "abstract": "We present an unsupervised word segmentation model, in which the learning\nobjective is to maximize the generation probability of a sentence given its all\npossible segmentation. Such generation probability can be factorized into the\nlikelihood of each possible segment given the context in a recursive way. In\norder to better capture the long- and short-term dependencies, we propose to\nuse bi-directional neural language models to better capture the features of\nsegment's context. Two decoding algorithms are also described to combine the\ncontext features from both directions to generate the final segmentation, which\nhelps to reconcile word boundary ambiguities. Experimental results showed that\nour context-sensitive unsupervised segmentation model achieved state-of-the-art\nat different evaluation settings on various data sets for Chinese, and the\ncomparable result for Thai.", "published": "2021-03-02 02:21:22", "link": "http://arxiv.org/abs/2103.01421v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing Product Description Generation via Posterior Distillation", "abstract": "In product description generation (PDG), the user-cared aspect is critical\nfor the recommendation system, which can not only improve user's experiences\nbut also obtain more clicks. High-quality customer reviews can be considered as\nan ideal source to mine user-cared aspects. However, in reality, a large number\nof new products (known as long-tailed commodities) cannot gather sufficient\namount of customer reviews, which brings a big challenge in the product\ndescription generation task. Existing works tend to generate the product\ndescription solely based on item information, i.e., product attributes or title\nwords, which leads to tedious contents and cannot attract customers\neffectively. To tackle this problem, we propose an adaptive posterior network\nbased on Transformer architecture that can utilize user-cared information from\ncustomer reviews. Specifically, we first extend the self-attentive Transformer\nencoder to encode product titles and attributes. Then, we apply an adaptive\nposterior distillation module to utilize useful review information, which\nintegrates user-cared aspects to the generation process. Finally, we apply a\nTransformer-based decoding phase with copy mechanism to automatically generate\nthe product description. Besides, we also collect a large-scare Chinese product\ndescription dataset to support our work and further research in this field.\nExperimental results show that our model is superior to traditional generative\nmodels in both automatic indicators and human evaluation.", "published": "2021-03-02 09:38:38", "link": "http://arxiv.org/abs/2103.01594v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hate Towards the Political Opponent: A Twitter Corpus Study of the 2020\n  US Elections on the Basis of Offensive Speech and Stance Detection", "abstract": "The 2020 US Elections have been, more than ever before, characterized by\nsocial media campaigns and mutual accusations. We investigate in this paper if\nthis manifests also in online communication of the supporters of the candidates\nBiden and Trump, by uttering hateful and offensive communication. We formulate\nan annotation task, in which we join the tasks of hateful/offensive speech\ndetection and stance detection, and annotate 3000 Tweets from the campaign\nperiod, if they express a particular stance towards a candidate. Next to the\nestablished classes of favorable and against, we add mixed and neutral stances\nand also annotate if a candidate is mentioned without an opinion expression.\nFurther, we annotate if the tweet is written in an offensive style. This\nenables us to analyze if supporters of Joe Biden and the Democratic Party\ncommunicate differently than supporters of Donald Trump and the Republican\nParty. A BERT baseline classifier shows that the detection if somebody is a\nsupporter of a candidate can be performed with high quality (.89 F1 for Trump\nand .91 F1 for Biden), while the detection that somebody expresses to be\nagainst a candidate is more challenging (.79 F1 and .64 F1, respectively). The\nautomatic detection of hate/offensive speech remains challenging (with .53 F1).\nOur corpus is publicly available and constitutes a novel resource for\ncomputational modelling of offensive language under consideration of stances.", "published": "2021-03-02 11:59:54", "link": "http://arxiv.org/abs/2103.01664v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emotion Ratings: How Intensity, Annotation Confidence and Agreements are\n  Entangled", "abstract": "When humans judge the affective content of texts, they also implicitly assess\nthe correctness of such judgment, that is, their confidence. We hypothesize\nthat people's (in)confidence that they performed well in an annotation task\nleads to (dis)agreements among each other. If this is true, confidence may\nserve as a diagnostic tool for systematic differences in annotations. To probe\nour assumption, we conduct a study on a subset of the Corpus of Contemporary\nAmerican English, in which we ask raters to distinguish neutral sentences from\nemotion-bearing ones, while scoring the confidence of their answers. Confidence\nturns out to approximate inter-annotator disagreements. Further, we find that\nconfidence is correlated to emotion intensity: perceiving stronger affect in\ntext prompts annotators to more certain classification performances. This\ninsight is relevant for modelling studies of intensity, as it opens the\nquestion wether automatic regressors or classifiers actually predict intensity,\nor rather human's self-perceived confidence.", "published": "2021-03-02 12:04:43", "link": "http://arxiv.org/abs/2103.01667v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AraBERT and Farasa Segmentation Based Approach For Sarcasm and Sentiment\n  Detection in Arabic Tweets", "abstract": "This paper presents our strategy to tackle the EACL WANLP-2021 Shared Task 2:\nSarcasm and Sentiment Detection. One of the subtasks aims at developing a\nsystem that identifies whether a given Arabic tweet is sarcastic in nature or\nnot, while the other aims to identify the sentiment of the Arabic tweet. We\napproach the task in two steps. The first step involves pre processing the\nprovided ArSarcasm-v2 dataset by performing insertions, deletions and\nsegmentation operations on various parts of the text. The second step involves\nexperimenting with multiple variants of two transformer based models,\nAraELECTRA and AraBERT. Our final approach was ranked seventh and fourth in the\nSarcasm and Sentiment Detection subtasks respectively.", "published": "2021-03-02 12:33:50", "link": "http://arxiv.org/abs/2103.01679v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conversational Norms for Human-Robot Dialogues", "abstract": "This paper describes a recently initiated research project aiming at\nsupporting development of computerised dialogue systems that handle breaches of\nconversational norms such as the Gricean maxims, which describe how dialogue\nparticipants ideally form their utterances in order to be informative,\nrelevant, brief, etc. Our approach is to model dialogue and norms with\nco-operating distributed grammar systems (CDGSs), and to develop methods to\ndetect breaches and to handle them in dialogue systems for verbal human-robot\ninteraction.", "published": "2021-03-02 13:28:18", "link": "http://arxiv.org/abs/2103.01706v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Rediscovery Hypothesis: Language Models Need to Meet Linguistics", "abstract": "There is an ongoing debate in the NLP community whether modern language\nmodels contain linguistic knowledge, recovered through so-called probes. In\nthis paper, we study whether linguistic knowledge is a necessary condition for\nthe good performance of modern language models, which we call the\n\\textit{rediscovery hypothesis}. In the first place, we show that language\nmodels that are significantly compressed but perform well on their pretraining\nobjectives retain good scores when probed for linguistic structures. This\nresult supports the rediscovery hypothesis and leads to the second contribution\nof our paper: an information-theoretic framework that relates language modeling\nobjectives with linguistic information. This framework also provides a metric\nto measure the impact of linguistic information on the word prediction task. We\nreinforce our analytical results with various experiments, both on synthetic\nand on real NLP tasks in English.", "published": "2021-03-02 15:57:39", "link": "http://arxiv.org/abs/2103.01819v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MultiSubs: A Large-scale Multimodal and Multilingual Dataset", "abstract": "This paper introduces a large-scale multimodal and multilingual dataset that\naims to facilitate research on grounding words to images in their contextual\nusage in language. The dataset consists of images selected to unambiguously\nillustrate concepts expressed in sentences from movie subtitles. The dataset is\na valuable resource as (i) the images are aligned to text fragments rather than\nwhole sentences; (ii) multiple images are possible for a text fragment and a\nsentence; (iii) the sentences are free-form and real-world like; (iv) the\nparallel texts are multilingual. We set up a fill-in-the-blank game for humans\nto evaluate the quality of the automatic image selection process of our\ndataset. We show the utility of the dataset on two automatic tasks: (i)\nfill-in-the-blank; (ii) lexical translation. Results of the human evaluation\nand automatic models demonstrate that images can be a useful complement to the\ntextual context. The dataset will benefit research on visual grounding of words\nespecially in the context of free-form sentences, and can be obtained from\nhttps://doi.org/10.5281/zenodo.5034604 under a Creative Commons licence.", "published": "2021-03-02 18:09:07", "link": "http://arxiv.org/abs/2103.01910v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Efficiently Diversifying Dialogue Generation via Embedding\n  Augmentation", "abstract": "Dialogue generation models face the challenge of producing generic and\nrepetitive responses. Unlike previous augmentation methods that mostly focus on\ntoken manipulation and ignore the essential variety within a single sample\nusing hard labels, we propose to promote the generation diversity of the neural\ndialogue models via soft embedding augmentation along with soft labels in this\npaper. Particularly, we select some key input tokens and fuse their embeddings\ntogether with embeddings from their semantic-neighbor tokens. The new\nembeddings serve as the input of the model to replace the original one.\nBesides, soft labels are used in loss calculation, resulting in multi-target\nsupervision for a given input. Our experimental results on two datasets\nillustrate that our proposed method is capable of generating more diverse\nresponses than raw models while remains a similar n-gram accuracy that ensures\nthe quality of generated responses.", "published": "2021-03-02 07:28:56", "link": "http://arxiv.org/abs/2103.01534v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Interpretable Multi-Modal Hate Speech Detection", "abstract": "With growing role of social media in shaping public opinions and beliefs\nacross the world, there has been an increased attention to identify and counter\nthe problem of hate speech on social media. Hate speech on online spaces has\nserious manifestations, including social polarization and hate crimes. While\nprior works have proposed automated techniques to detect hate speech online,\nthese techniques primarily fail to look beyond the textual content. Moreover,\nfew attempts have been made to focus on the aspects of interpretability of such\nmodels given the social and legal implications of incorrect predictions. In\nthis work, we propose a deep neural multi-modal model that can: (a) detect hate\nspeech by effectively capturing the semantics of the text along with\nsocio-cultural context in which a particular hate expression is made, and (b)\nprovide interpretable insights into decisions of our model. By performing a\nthorough evaluation of different modeling techniques, we demonstrate that our\nmodel is able to outperform the existing state-of-the-art hate speech\nclassification approaches. Finally, we show the importance of social and\ncultural context features towards unearthing clusters associated with different\ncategories of hate.", "published": "2021-03-02 10:12:26", "link": "http://arxiv.org/abs/2103.01616v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Data-Centric Framework for Composable NLP Workflows", "abstract": "Empirical natural language processing (NLP) systems in application domains\n(e.g., healthcare, finance, education) involve interoperation among multiple\ncomponents, ranging from data ingestion, human annotation, to text retrieval,\nanalysis, generation, and visualization. We establish a unified open-source\nframework to support fast development of such sophisticated NLP workflows in a\ncomposable manner. The framework introduces a uniform data representation to\nencode heterogeneous results by a wide range of NLP tasks. It offers a large\nrepository of processors for NLP tasks, visualization, and annotation, which\ncan be easily assembled with full interoperability under the unified\nrepresentation. The highly extensible framework allows plugging in custom\nprocessors from external off-the-shelf NLP and deep learning libraries. The\nwhole framework is delivered through two modularized yet integratable\nopen-source projects, namely Forte (for workflow infrastructure and NLP\nfunction processors) and Stave (for user interaction, visualization, and\nannotation).", "published": "2021-03-02 16:19:44", "link": "http://arxiv.org/abs/2103.01834v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Data Augmentation for Abstractive Query-Focused Multi-Document\n  Summarization", "abstract": "The progress in Query-focused Multi-Document Summarization (QMDS) has been\nlimited by the lack of sufficient largescale high-quality training datasets. We\npresent two QMDS training datasets, which we construct using two data\naugmentation methods: (1) transferring the commonly used single-document\nCNN/Daily Mail summarization dataset to create the QMDSCNN dataset, and (2)\nmining search-query logs to create the QMDSIR dataset. These two datasets have\ncomplementary properties, i.e., QMDSCNN has real summaries but queries are\nsimulated, while QMDSIR has real queries but simulated summaries. To cover both\nthese real summary and query aspects, we build abstractive end-to-end neural\nnetwork models on the combined datasets that yield new state-of-the-art\ntransfer results on DUC datasets. We also introduce new hierarchical encoders\nthat enable a more efficient encoding of the query together with multiple\ndocuments. Empirical results demonstrate that our data augmentation and\nencoding methods outperform baseline models on automatic metrics, as well as on\nhuman evaluations along multiple attributes.", "published": "2021-03-02 16:57:01", "link": "http://arxiv.org/abs/2103.01863v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Contrastive Explanations for Model Interpretability", "abstract": "Contrastive explanations clarify why an event occurred in contrast to\nanother. They are more inherently intuitive to humans to both produce and\ncomprehend. We propose a methodology to produce contrastive explanations for\nclassification models by modifying the representation to disregard\nnon-contrastive information, and modifying model behavior to only be based on\ncontrastive reasoning. Our method is based on projecting model representation\nto a latent space that captures only the features that are useful (to the\nmodel) to differentiate two potential decisions. We demonstrate the value of\ncontrastive explanations by analyzing two different scenarios, using both\nhigh-level abstract concept attribution and low-level input token/span\nattribution, on two widely used text classification tasks. Specifically, we\nproduce explanations for answering: for which label, and against which\nalternative label, is some aspect of the input useful? And which aspects of the\ninput are useful for and against particular decisions? Overall, our findings\nshed light on the ability of label-contrastive explanations to provide a more\naccurate and finer-grained interpretability of a model's decision.", "published": "2021-03-02 00:36:45", "link": "http://arxiv.org/abs/2103.01378v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An End-to-End Network for Emotion-Cause Pair Extraction", "abstract": "The task of Emotion-Cause Pair Extraction (ECPE) aims to extract all\npotential clause-pairs of emotions and their corresponding causes in a\ndocument. Unlike the more well-studied task of Emotion Cause Extraction (ECE),\nECPE does not require the emotion clauses to be provided as annotations.\nPrevious works on ECPE have either followed a multi-stage approach where\nemotion extraction, cause extraction, and pairing are done independently or use\ncomplex architectures to resolve its limitations. In this paper, we propose an\nend-to-end model for the ECPE task. Due to the unavailability of an English\nlanguage ECPE corpus, we adapt the NTCIR-13 ECE corpus and establish a baseline\nfor the ECPE task on this dataset. On this dataset, the proposed method\nproduces significant performance improvements (~6.5 increase in F1 score) over\nthe multi-stage approach and achieves comparable performance to the\nstate-of-the-art methods.", "published": "2021-03-02 08:03:03", "link": "http://arxiv.org/abs/2103.01544v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FinMatcher at FinSim-2: Hypernym Detection in the Financial Services\n  Domain using Knowledge Graphs", "abstract": "This paper presents the FinMatcher system and its results for the FinSim 2021\nshared task which is co-located with the Workshop on Financial Technology on\nthe Web (FinWeb) in conjunction with The Web Conference. The FinSim-2 shared\ntask consists of a set of concept labels from the financial services domain.\nThe goal is to find the most relevant top-level concept from a given set of\nconcepts. The FinMatcher system exploits three publicly available knowledge\ngraphs, namely WordNet, Wikidata, and WebIsALOD. The graphs are used to\ngenerate explicit features as well as latent features which are fed into a\nneural classifier to predict the closest hypernym.", "published": "2021-03-02 08:56:28", "link": "http://arxiv.org/abs/2103.01576v1", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Disentangling Syntax and Semantics in the Brain with Deep Networks", "abstract": "The activations of language transformers like GPT-2 have been shown to\nlinearly map onto brain activity during speech comprehension. However, the\nnature of these activations remains largely unknown and presumably conflate\ndistinct linguistic classes. Here, we propose a taxonomy to factorize the\nhigh-dimensional activations of language models into four combinatorial\nclasses: lexical, compositional, syntactic, and semantic representations. We\nthen introduce a statistical method to decompose, through the lens of GPT-2's\nactivations, the brain activity of 345 subjects recorded with functional\nmagnetic resonance imaging (fMRI) during the listening of ~4.6 hours of\nnarrated text. The results highlight two findings. First, compositional\nrepresentations recruit a more widespread cortical network than lexical ones,\nand encompass the bilateral temporal, parietal and prefrontal cortices. Second,\ncontrary to previous claims, syntax and semantics are not associated with\nseparated modules, but, instead, appear to share a common and distributed\nneural substrate. Overall, this study introduces a versatile framework to\nisolate, in the brain activity, the distributed representations of linguistic\nconstructs.", "published": "2021-03-02 10:24:05", "link": "http://arxiv.org/abs/2103.01620v2", "categories": ["cs.CL", "cs.LG", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Incorporating VAD into ASR System by Multi-task Learning", "abstract": "When we use End-to-end automatic speech recognition (E2E-ASR) system for\nreal-world applications, a voice activity detection (VAD) system is usually\nneeded to improve the performance and to reduce the computational cost by\ndiscarding non-speech parts in the audio. Usually ASR and VAD systems are\ntrained and utilized independently to each other. In this paper, we present a\nnovel multi-task learning (MTL) framework that incorporates VAD into the ASR\nsystem. The proposed system learns ASR and VAD jointly in the training stage.\nWith the assistance of VAD, the ASR performance improves as its connectionist\ntemporal classification (CTC) loss function can leverage the VAD alignment\ninformation. In the inference stage, the proposed system removes non-speech\nparts at low computational cost and recognizes speech parts with high\nrobustness. Experimental results on segmented speech data show that by\nutilizing VAD information, the proposed method outperforms the baseline ASR\nsystem on both English and Chinese datasets. On unsegmented speech data, we\nfind that the system outperforms the ASR systems that build an extra GMM-based\nor DNN-based voice activity detector.", "published": "2021-03-02 11:49:03", "link": "http://arxiv.org/abs/2103.01661v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Distributional Formal Semantics", "abstract": "Natural language semantics has recently sought to combine the complementary\nstrengths of formal and distributional approaches to meaning. More\nspecifically, proposals have been put forward to augment formal semantic\nmachinery with distributional meaning representations, thereby introducing the\nnotion of semantic similarity into formal semantics, or to define\ndistributional systems that aim to incorporate formal notions such as\nentailment and compositionality. However, given the fundamentally different\n'representational currency' underlying formal and distributional approaches -\nmodels of the world versus linguistic co-occurrence - their unification has\nproven extremely difficult. Here, we define a Distributional Formal Semantics\nthat integrates distributionality into a formal semantic system on the level of\nformal models. This approach offers probabilistic, distributed meaning\nrepresentations that are also inherently compositional, and that naturally\ncapture fundamental semantic notions such as quantification and entailment.\nFurthermore, we show how the probabilistic nature of these representations\nallows for probabilistic inference, and how the information-theoretic notion of\n\"information\" (measured in terms of Entropy and Surprisal) naturally follows\nfrom it. Finally, we illustrate how meaning representations can be derived\nincrementally from linguistic input using a recurrent neural network model, and\nhow the resultant incremental semantic construction procedure intuitively\ncaptures key semantic phenomena, including negation, presupposition, and\nanaphoricity.", "published": "2021-03-02 13:38:00", "link": "http://arxiv.org/abs/2103.01713v1", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "Dual Reinforcement-Based Specification Generation for Image De-Rendering", "abstract": "Advances in deep learning have led to promising progress in inferring\ngraphics programs by de-rendering computer-generated images. However, current\nmethods do not explore which decoding methods lead to better inductive bias for\ninferring graphics programs. In our work, we first explore the effectiveness of\nLSTM-RNN versus Transformer networks as decoders for order-independent graphics\nprograms. Since these are sequence models, we must choose an ordering of the\nobjects in the graphics programs for likelihood training. We found that the\nLSTM performance was highly sensitive to the sequence ordering (random order\nvs. pattern-based order), while Transformer performance was roughly independent\nof the sequence ordering. Further, we present a policy gradient based\nreinforcement learning approach for better inductive bias in the decoder via\nmultiple diverse rewards based both on the graphics program specification and\nthe rendered image. We also explore the combination of these complementary\nrewards. We achieve state-of-the-art results on two graphics program generation\ndatasets.", "published": "2021-03-02 17:04:56", "link": "http://arxiv.org/abs/2103.01867v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Listen, Read, and Identify: Multimodal Singing Language Identification\n  of Music", "abstract": "We propose a multimodal singing language classification model that uses both\naudio content and textual metadata. LRID-Net, the proposed model, takes an\naudio signal and a language probability vector estimated from the metadata and\noutputs the probabilities of the target languages. Optionally, LRID-Net is\nfacilitated with modality dropouts to handle a missing modality. In the\nexperiment, we trained several LRID-Nets with varying modality dropout\nconfiguration and tested them with various combinations of input modalities.\nThe experiment results demonstrate that using multimodal input improves\nperformance. The results also suggest that adopting modality dropout does not\ndegrade the performance of the model when there are full modality inputs while\nenabling the model to handle missing modality cases to some extent.", "published": "2021-03-02 17:45:04", "link": "http://arxiv.org/abs/2103.01893v4", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Investigations on Audiovisual Emotion Recognition in Noisy Conditions", "abstract": "In this paper we explore audiovisual emotion recognition under noisy acoustic\nconditions with a focus on speech features. We attempt to answer the following\nresearch questions: (i) How does speech emotion recognition perform on noisy\ndata? and (ii) To what extend does a multimodal approach improve the accuracy\nand compensate for potential performance degradation at different noise levels?\nWe present an analytical investigation on two emotion datasets with\nsuperimposed noise at different signal-to-noise ratios, comparing three types\nof acoustic features. Visual features are incorporated with a hybrid fusion\napproach: The first neural network layers are separate modality-specific ones,\nfollowed by at least one shared layer before the final prediction. The results\nshow a significant performance decrease when a model trained on clean audio is\napplied to noisy data and that the addition of visual features alleviates this\neffect.", "published": "2021-03-02 17:45:16", "link": "http://arxiv.org/abs/2103.01894v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual\n  Machine Learning", "abstract": "The milestone improvements brought about by deep representation learning and\npre-training techniques have led to large performance gains across downstream\nNLP, IR and Vision tasks. Multimodal modeling techniques aim to leverage large\nhigh-quality visio-linguistic datasets for learning complementary information\n(across image and text modalities). In this paper, we introduce the\nWikipedia-based Image Text (WIT) Dataset\n(https://github.com/google-research-datasets/wit) to better facilitate\nmultimodal, multilingual learning. WIT is composed of a curated set of 37.6\nmillion entity rich image-text examples with 11.5 million unique images across\n108 Wikipedia languages. Its size enables WIT to be used as a pretraining\ndataset for multimodal models, as we show when applied to downstream tasks such\nas image-text retrieval. WIT has four main and unique advantages. First, WIT is\nthe largest multimodal dataset by the number of image-text examples by 3x (at\nthe time of writing). Second, WIT is massively multilingual (first of its kind)\nwith coverage over 100+ languages (each of which has at least 12K examples) and\nprovides cross-lingual texts for many images. Third, WIT represents a more\ndiverse set of concepts and real world entities relative to what previous\ndatasets cover. Lastly, WIT provides a very challenging real-world test set, as\nwe empirically illustrate using an image-text retrieval task as an example.", "published": "2021-03-02 18:13:54", "link": "http://arxiv.org/abs/2103.01913v2", "categories": ["cs.CV", "cs.CL", "cs.IR"], "primary_category": "cs.CV"}
{"title": "Open Range Pitch Tracking for Carrier Frequency Difference Estimation\n  from HF Transmitted Speech", "abstract": "In this paper we investigate the task of detecting carrier frequency\ndifferences from demodulated single sideband signals by examining the pitch\ncontours of the received baseband speech signal in the short-time spectral\ndomain. From the detected pitch frequency trajectory and its harmonics a\ncarrier frequency difference, which is caused by demodulating the radio signal\nwith the wrong carrier frequency, can be deduced. A computationally efficient\nrealization in the power cepstral domain is presented. The core component,\ni.e., the pitch tracking algorithm, is shown to perform comparably to a state\nof the art algorithm. The full carrier frequency difference estimation system\nis tested on recordings of real transmissions over HF links. A comparison with\nan existing approach shows improved estimation accuracy, both on short and\nlonger speech utterances", "published": "2021-03-02 09:53:47", "link": "http://arxiv.org/abs/2103.01599v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio-Visual Speech Separation Using Cross-Modal Correspondence Loss", "abstract": "We present an audio-visual speech separation learning method that considers\nthe correspondence between the separated signals and the visual signals to\nreflect the speech characteristics during training. Audio-visual speech\nseparation is a technique to estimate the individual speech signals from a\nmixture using the visual signals of the speakers. Conventional studies on\naudio-visual speech separation mainly train the separation model on the\naudio-only loss, which reflects the distance between the source signals and the\nseparated signals. However, conventional losses do not reflect the\ncharacteristics of the speech signals, including the speaker's characteristics\nand phonetic information, which leads to distortion or remaining noise. To\naddress this problem, we propose the cross-modal correspondence (CMC) loss,\nwhich is based on the cooccurrence of the speech signal and the visual signal.\nSince the visual signal is not affected by background noise and contains\nspeaker and phonetic information, using the CMC loss enables the audio-visual\nspeech separation model to remove noise while preserving the speech\ncharacteristics. Experimental results demonstrate that the proposed method\nlearns the cooccurrence on the basis of CMC loss, which improves separation\nperformance.", "published": "2021-03-02 04:29:26", "link": "http://arxiv.org/abs/2103.01463v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Virufy: A Multi-Branch Deep Learning Network for Automated Detection of\n  COVID-19", "abstract": "Fast and affordable solutions for COVID-19 testing are necessary to contain\nthe spread of the global pandemic and help relieve the burden on medical\nfacilities. Currently, limited testing locations and expensive equipment pose\ndifficulties for individuals trying to be tested, especially in low-resource\nsettings. Researchers have successfully presented models for detecting COVID-19\ninfection status using audio samples recorded in clinical settings [5, 15],\nsuggesting that audio-based Artificial Intelligence models can be used to\nidentify COVID-19. Such models have the potential to be deployed on smartphones\nfor fast, widespread, and low-resource testing. However, while previous studies\nhave trained models on cleaned audio samples collected mainly from clinical\nsettings, audio samples collected from average smartphones may yield suboptimal\nquality data that is different from the clean data that models were trained on.\nThis discrepancy may add a bias that affects COVID-19 status predictions. To\ntackle this issue, we propose a multi-branch deep learning network that is\ntrained and tested on crowdsourced data where most of the data has not been\nmanually processed and cleaned. Furthermore, the model achieves state-of-art\nresults for the COUGHVID dataset [16]. After breaking down results for each\ncategory, we have shown an AUC of 0.99 for audio samples with COVID-19 positive\nlabels.", "published": "2021-03-02 15:31:09", "link": "http://arxiv.org/abs/2103.01806v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio scene monitoring using redundant ad-hoc microphone array networks", "abstract": "We present a system for localizing sound sources in a room with several\nad-hoc microphone arrays. Each circular array performs direction of arrival\n(DOA) estimation independently using commercial software. The DOAs are fed to a\nfusion center, concatenated, and used to perform the localization based on two\nproposed methods, which require only few labeled source locations (anchor\npoints) for training. The first proposed method is based on principal component\nanalysis (PCA) of the observed DOA and does not require any knowledge of anchor\npoints. The array cluster can then perform localization on a manifold defined\nby the PCA of concatenated DOAs over time. The second proposed method performs\nlocalization using an affine transformation between the DOA vectors and the\nroom manifold. The PCA has fewer requirements on the training sequence, but is\nless robust to missing DOAs from one of the arrays. The methods are\ndemonstrated with five IoT 8-microphone circular arrays, placed at unspecified\nfixed locations in an office. Both the PCA and the affine method can easily map\nout a rectangle based on a few anchor points with similar accuracy. The\nproposed methods provide a step towards monitoring activities in a smart home\nand require little installation effort as the array locations are not needed.", "published": "2021-03-02 16:10:43", "link": "http://arxiv.org/abs/2103.01830v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SoundCLR: Contrastive Learning of Representations For Improved\n  Environmental Sound Classification", "abstract": "Environmental Sound Classification (ESC) is a challenging field of research\nin non-speech audio processing. Most of current research in ESC focuses on\ndesigning deep models with special architectures tailored for specific audio\ndatasets, which usually cannot exploit the intrinsic patterns in the data.\nHowever recent studies have surprisingly shown that transfer learning from\nmodels trained on ImageNet is a very effective technique in ESC. Herein, we\npropose SoundCLR, a supervised contrastive learning method for effective\nenvironment sound classification with state-of-the-art performance, which works\nby learning representations that disentangle the samples of each class from\nthose of other classes. Our deep network models are trained by combining a\ncontrastive loss that contributes to a better probability output by the\nclassification layer with a cross-entropy loss on the output of the classifier\nlayer to map the samples to their respective 1-hot encoded labels. Due to the\ncomparatively small sizes of the available environmental sound datasets, we\npropose and exploit a transfer learning and strong data augmentation pipeline\nand apply the augmentations on both the sound signals and their log-mel\nspectrograms before inputting them to the model. Our experiments show that our\nmasking based augmentation technique on the log-mel spectrograms can\nsignificantly improve the recognition performance. Our extensive benchmark\nexperiments show that our hybrid deep network models trained with combined\ncontrastive and cross-entropy loss achieved the state-of-the-art performance on\nthree benchmark datasets ESC-10, ESC-50, and US8K with validation accuracies of\n99.75\\%, 93.4\\%, and 86.49\\% respectively. The ensemble version of our models\nalso outperforms other top ensemble methods. The code is available at\nhttps://github.com/alireza-nasiri/SoundCLR.", "published": "2021-03-02 18:42:45", "link": "http://arxiv.org/abs/2103.01929v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Tune-In: Training Under Negative Environments with Interference for\n  Attention Networks Simulating Cocktail Party Effect", "abstract": "We study the cocktail party problem and propose a novel attention network\ncalled Tune-In, abbreviated for training under negative environments with\ninterference. It firstly learns two separate spaces of speaker-knowledge and\nspeech-stimuli based on a shared feature space, where a new block structure is\ndesigned as the building block for all spaces, and then cooperatively solves\ndifferent tasks. Between the two spaces, information is cast towards each other\nvia a novel cross- and dual-attention mechanism, mimicking the bottom-up and\ntop-down processes of a human's cocktail party effect. It turns out that\nsubstantially discriminative and generalizable speaker representations can be\nlearnt in severely interfered conditions via our self-supervised training. The\nexperimental results verify this seeming paradox. The learnt speaker embedding\nhas superior discriminative power than a standard speaker verification method;\nmeanwhile, Tune-In achieves remarkably better speech separation performances in\nterms of SI-SNRi and SDRi consistently in all test modes, and especially at\nlower memory and computational consumption, than state-of-the-art benchmark\nsystems.", "published": "2021-03-02 04:03:37", "link": "http://arxiv.org/abs/2103.01461v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
