{"title": "Data Augmentation for Cross-Domain Named Entity Recognition", "abstract": "Current work in named entity recognition (NER) shows that data augmentation\ntechniques can produce more robust models. However, most existing techniques\nfocus on augmenting in-domain data in low-resource scenarios where annotated\ndata is quite limited. In contrast, we study cross-domain data augmentation for\nthe NER task. We investigate the possibility of leveraging data from\nhigh-resource domains by projecting it into the low-resource domains.\nSpecifically, we propose a novel neural architecture to transform the data\nrepresentation from a high-resource to a low-resource domain by learning the\npatterns (e.g. style, noise, abbreviations, etc.) in the text that\ndifferentiate them and a shared feature space where both domains are aligned.\nWe experiment with diverse datasets and show that transforming the data to the\nlow-resource domain representation achieves significant improvements over only\nusing data from high-resource domains.", "published": "2021-09-04 00:50:55", "link": "http://arxiv.org/abs/2109.01758v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Supervised Contrastive Learning for Multimodal Unreliable News Detection\n  in COVID-19 Pandemic", "abstract": "As the digital news industry becomes the main channel of information\ndissemination, the adverse impact of fake news is explosively magnified. The\ncredibility of a news report should not be considered in isolation. Rather,\npreviously published news articles on the similar event could be used to assess\nthe credibility of a news report. Inspired by this, we propose a BERT-based\nmultimodal unreliable news detection framework, which captures both textual and\nvisual information from unreliable articles utilising the contrastive learning\nstrategy. The contrastive learner interacts with the unreliable news classifier\nto push similar credible news (or similar unreliable news) closer while moving\nnews articles with similar content but opposite credibility labels away from\neach other in the multimodal embedding space. Experimental results on a\nCOVID-19 related dataset, ReCOVery, show that our model outperforms a number of\ncompetitive baseline in unreliable news detection.", "published": "2021-09-04 11:53:37", "link": "http://arxiv.org/abs/2109.01850v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Supervised Detection of Contextual Synonyms in a Multi-Class\n  Setting: Phenotype Annotation Use Case", "abstract": "Contextualised word embeddings is a powerful tool to detect contextual\nsynonyms. However, most of the current state-of-the-art (SOTA) deep learning\nconcept extraction methods remain supervised and underexploit the potential of\nthe context. In this paper, we propose a self-supervised pre-training approach\nwhich is able to detect contextual synonyms of concepts being training on the\ndata created by shallow matching. We apply our methodology in the sparse\nmulti-class setting (over 15,000 concepts) to extract phenotype information\nfrom electronic health records. We further investigate data augmentation\ntechniques to address the problem of the class sparsity. Our approach achieves\na new SOTA for the unsupervised phenotype concept annotation on clinical text\non F1 and Recall outperforming the previous SOTA with a gain of up to 4.5 and\n4.0 absolute points, respectively. After fine-tuning with as little as 20\\% of\nthe labelled data, we also outperform BioBERT and ClinicalBERT. The extrinsic\nevaluation on three ICU benchmarks also shows the benefit of using the\nphenotypes annotated by our model as features.", "published": "2021-09-04 21:35:01", "link": "http://arxiv.org/abs/2109.01935v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the ability of monolingual models to learn language-agnostic\n  representations", "abstract": "Pretrained multilingual models have become a de facto default approach for\nzero-shot cross-lingual transfer. Previous work has shown that these models are\nable to achieve cross-lingual representations when pretrained on two or more\nlanguages with shared parameters. In this work, we provide evidence that a\nmodel can achieve language-agnostic representations even when pretrained on a\nsingle language. That is, we find that monolingual models pretrained and\nfinetuned on different languages achieve competitive performance compared to\nthe ones that use the same target language. Surprisingly, the models show a\nsimilar performance on a same task regardless of the pretraining language. For\nexample, models pretrained on distant languages such as German and Portuguese\nperform similarly on English tasks.", "published": "2021-09-04 22:09:44", "link": "http://arxiv.org/abs/2109.01942v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Expressive Communication with Internet Memes: A New Multimodal\n  Conversation Dataset and Benchmark", "abstract": "As a kind of new expression elements, Internet memes are popular and\nextensively used in online chatting scenarios since they manage to make\ndialogues vivid, moving, and interesting. However, most current dialogue\nresearches focus on text-only dialogue tasks. In this paper, we propose a new\ntask named as \\textbf{M}eme incorporated \\textbf{O}pen-domain \\textbf{D}ialogue\n(MOD). Compared to previous dialogue tasks, MOD is much more challenging since\nit requires the model to understand the multimodal elements as well as the\nemotions behind them. To facilitate the MOD research, we construct a\nlarge-scale open-domain multimodal dialogue dataset incorporating abundant\nInternet memes into utterances. The dataset consists of $\\sim$45K Chinese\nconversations with $\\sim$606K utterances. Each conversation contains about $13$\nutterances with about $4$ Internet memes on average and each utterance equipped\nwith an Internet meme is annotated with the corresponding emotion. In addition,\nwe present a simple and effective method, which utilizes a unified generation\nnetwork to solve the MOD task. Experimental results demonstrate that our method\ntrained on the proposed corpus is able to achieve expressive communication\nincluding texts and memes. The corpus and models have been publicly available\nat https://github.com/lizekang/DSTC10-MOD.", "published": "2021-09-04 10:39:52", "link": "http://arxiv.org/abs/2109.01839v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Pushing Paraphrase Away from Original Sentence: A Multi-Round Paraphrase\n  Generation Approach", "abstract": "In recent years, neural paraphrase generation based on Seq2Seq has achieved\nsuperior performance, however, the generated paraphrase still has the problem\nof lack of diversity. In this paper, we focus on improving the diversity\nbetween the generated paraphrase and the original sentence, i.e., making\ngenerated paraphrase different from the original sentence as much as possible.\nWe propose BTmPG (Back-Translation guided multi-round Paraphrase Generation),\nwhich leverages multi-round paraphrase generation to improve diversity and\nemploys back-translation to preserve semantic information. We evaluate BTmPG on\ntwo benchmark datasets. Both automatic and human evaluation show BTmPG can\nimprove the diversity of paraphrase while preserving the semantics of the\noriginal sentence.", "published": "2021-09-04 13:12:01", "link": "http://arxiv.org/abs/2109.01862v1", "categories": ["cs.CL", "cs.AI", "68T50 (Primary), 68T07 (Secondary)", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Uncovering the Limits of Text-based Emotion Detection", "abstract": "Identifying emotions from text is crucial for a variety of real world tasks.\nWe consider the two largest now-available corpora for emotion classification:\nGoEmotions, with 58k messages labelled by readers, and Vent, with 33M\nwriter-labelled messages. We design a benchmark and evaluate several feature\nspaces and learning algorithms, including two simple yet novel models on top of\nBERT that outperform previous strong baselines on GoEmotions. Through an\nexperiment with human participants, we also analyze the differences between how\nwriters express emotions and how readers perceive them. Our results suggest\nthat emotions expressed by writers are harder to identify than emotions that\nreaders perceive. We share a public web interface for researchers to explore\nour models.", "published": "2021-09-04 16:40:06", "link": "http://arxiv.org/abs/2109.01900v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Neural Network-Based Linguistic Similarity Measure for Entrainment in\n  Conversations", "abstract": "Linguistic entrainment is a phenomenon where people tend to mimic each other\nin conversation. The core instrument to quantify entrainment is a linguistic\nsimilarity measure between conversational partners. Most of the current\nsimilarity measures are based on bag-of-words approaches that rely on\nlinguistic markers, ignoring the overall language structure and dialogue\ncontext. To address this issue, we propose to use a neural network model to\nperform the similarity measure for entrainment. Our model is context-aware, and\nit further leverages a novel component to learn the shared high-level\nlinguistic features across dialogues. We first investigate the effectiveness of\nour novel component. Then we use the model to perform similarity measure in a\ncorpus-based entrainment analysis. We observe promising results for both\nevaluation tasks.", "published": "2021-09-04 19:48:17", "link": "http://arxiv.org/abs/2109.01924v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Error Detection in Large-Scale Natural Language Understanding Systems\n  Using Transformer Models", "abstract": "Large-scale conversational assistants like Alexa, Siri, Cortana and Google\nAssistant process every utterance using multiple models for domain, intent and\nnamed entity recognition. Given the decoupled nature of model development and\nlarge traffic volumes, it is extremely difficult to identify utterances\nprocessed erroneously by such systems. We address this challenge to detect\ndomain classification errors using offline Transformer models. We combine\nutterance encodings from a RoBERTa model with the Nbest hypothesis produced by\nthe production system. We then fine-tune end-to-end in a multitask setting\nusing a small dataset of humanannotated utterances with domain classification\nerrors. We tested our approach for detecting misclassifications from one domain\nthat accounts for <0.5% of the traffic in a large-scale conversational AI\nsystem. Our approach achieves an F1 score of 30% outperforming a bi- LSTM\nbaseline by 16.9% and a standalone RoBERTa model by 4.8%. We improve this\nfurther by 2.2% to 32.2% by ensembling multiple models.", "published": "2021-09-04 00:10:48", "link": "http://arxiv.org/abs/2109.01754v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Representation Learning for Efficient and Effective Similarity Search\n  and Recommendation", "abstract": "How data is represented and operationalized is critical for building\ncomputational solutions that are both effective and efficient. A common\napproach is to represent data objects as binary vectors, denoted \\textit{hash\ncodes}, which require little storage and enable efficient similarity search\nthrough direct indexing into a hash table or through similarity computations in\nan appropriate space. Due to the limited expressibility of hash codes, compared\nto real-valued representations, a core open challenge is how to generate hash\ncodes that well capture semantic content or latent properties using a small\nnumber of bits, while ensuring that the hash codes are distributed in a way\nthat does not reduce their search efficiency. State of the art methods use\nrepresentation learning for generating such hash codes, focusing on neural\nautoencoder architectures where semantics are encoded into the hash codes by\nlearning to reconstruct the original inputs of the hash codes. This thesis\naddresses the above challenge and makes a number of contributions to\nrepresentation learning that (i) improve effectiveness of hash codes through\nmore expressive representations and a more effective similarity measure than\nthe current state of the art, namely the Hamming distance, and (ii) improve\nefficiency of hash codes by learning representations that are especially suited\nto the choice of search method. The contributions are empirically validated on\nseveral tasks related to similarity search and recommendation.", "published": "2021-09-04 08:19:01", "link": "http://arxiv.org/abs/2109.01815v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Frustratingly Simple Pretraining Alternatives to Masked Language\n  Modeling", "abstract": "Masked language modeling (MLM), a self-supervised pretraining objective, is\nwidely used in natural language processing for learning text representations.\nMLM trains a model to predict a random sample of input tokens that have been\nreplaced by a [MASK] placeholder in a multi-class setting over the entire\nvocabulary. When pretraining, it is common to use alongside MLM other auxiliary\nobjectives on the token or sequence level to improve downstream performance\n(e.g. next sentence prediction). However, no previous work so far has attempted\nin examining whether other simpler linguistically intuitive or not objectives\ncan be used standalone as main pretraining objectives. In this paper, we\nexplore five simple pretraining objectives based on token-level classification\ntasks as replacements of MLM. Empirical results on GLUE and SQuAD show that our\nproposed methods achieve comparable or better performance to MLM using a\nBERT-BASE architecture. We further validate our methods using smaller models,\nshowing that pretraining a model with 41% of the BERT-BASE's parameters,\nBERT-MEDIUM results in only a 1% drop in GLUE scores with our best objective.", "published": "2021-09-04 08:52:37", "link": "http://arxiv.org/abs/2109.01819v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Weakly Supervised Relative Spatial Reasoning for Visual Question\n  Answering", "abstract": "Vision-and-language (V\\&L) reasoning necessitates perception of visual\nconcepts such as objects and actions, understanding semantics and language\ngrounding, and reasoning about the interplay between the two modalities. One\ncrucial aspect of visual reasoning is spatial understanding, which involves\nunderstanding relative locations of objects, i.e.\\ implicitly learning the\ngeometry of the scene. In this work, we evaluate the faithfulness of V\\&L\nmodels to such geometric understanding, by formulating the prediction of\npair-wise relative locations of objects as a classification as well as a\nregression task. Our findings suggest that state-of-the-art transformer-based\nV\\&L models lack sufficient abilities to excel at this task. Motivated by this,\nwe design two objectives as proxies for 3D spatial reasoning (SR) -- object\ncentroid estimation, and relative position estimation, and train V\\&L with weak\nsupervision from off-the-shelf depth estimators. This leads to considerable\nimprovements in accuracy for the \"GQA\" visual question answering challenge (in\nfully supervised, few-shot, and O.O.D settings) as well as improvements in\nrelative spatial reasoning. Code and data will be released\n\\href{https://github.com/pratyay-banerjee/weak_sup_vqa}{here}.", "published": "2021-09-04 21:29:06", "link": "http://arxiv.org/abs/2109.01934v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "FewshotQA: A simple framework for few-shot learning of question\n  answering tasks using pre-trained text-to-text models", "abstract": "The task of learning from only a few examples (called a few-shot setting) is\nof key importance and relevance to a real-world setting. For question answering\n(QA), the current state-of-the-art pre-trained models typically need\nfine-tuning on tens of thousands of examples to obtain good results. Their\nperformance degrades significantly in a few-shot setting (< 100 examples). To\naddress this, we propose a simple fine-tuning framework that leverages\npre-trained text-to-text models and is directly aligned with their pre-training\nframework. Specifically, we construct the input as a concatenation of the\nquestion, a mask token representing the answer span and a context. Given this\ninput, the model is fine-tuned using the same objective as that of its\npre-training objective. Through experimental studies on various few-shot\nconfigurations, we show that this formulation leads to significant gains on\nmultiple QA benchmarks (an absolute gain of 34.2 F1 points on average when\nthere are only 16 training examples). The gains extend further when used with\nlarger models (Eg:- 72.3 F1 on SQuAD using BART-large with only 32 examples)\nand translate well to a multilingual setting . On the multilingual TydiQA\nbenchmark, our model outperforms the XLM-Roberta-large by an absolute margin of\nupto 40 F1 points and an average of 33 F1 points in a few-shot setting (<= 64\ntraining examples). We conduct detailed ablation studies to analyze factors\ncontributing to these gains.", "published": "2021-09-04 23:08:57", "link": "http://arxiv.org/abs/2109.01951v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LAViTeR: Learning Aligned Visual and Textual Representations Assisted by\n  Image and Caption Generation", "abstract": "Pre-training visual and textual representations from large-scale image-text\npairs is becoming a standard approach for many downstream vision-language\ntasks. The transformer-based models learn inter and intra-modal attention\nthrough a list of self-supervised learning tasks. This paper proposes LAViTeR,\na novel architecture for visual and textual representation learning. The main\nmodule, Visual Textual Alignment (VTA) will be assisted by two auxiliary tasks,\nGAN-based image synthesis and Image Captioning. We also propose a new\nevaluation metric measuring the similarity between the learnt visual and\ntextual embedding. The experimental results on two public datasets, CUB and\nMS-COCO, demonstrate superior visual and textual representation alignment in\nthe joint feature embedding space", "published": "2021-09-04 22:48:46", "link": "http://arxiv.org/abs/2109.04993v4", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Multi-Relational Graph based Heterogeneous Multi-Task Learning in\n  Community Question Answering", "abstract": "Various data mining tasks have been proposed to study Community Question\nAnswering (CQA) platforms like Stack Overflow. The relatedness between some of\nthese tasks provides useful learning signals to each other via Multi-Task\nLearning (MTL). However, due to the high heterogeneity of these tasks, few\nexisting works manage to jointly solve them in a unified framework. To tackle\nthis challenge, we develop a multi-relational graph based MTL model called\nHeterogeneous Multi-Task Graph Isomorphism Network (HMTGIN) which efficiently\nsolves heterogeneous CQA tasks. In each training forward pass, HMTGIN embeds\nthe input CQA forum graph by an extension of Graph Isomorphism Network and skip\nconnections. The embeddings are then shared across all task-specific output\nlayers to compute respective losses. Moreover, two cross-task constraints based\non the domain knowledge about tasks' relationships are used to regularize the\njoint learning. In the evaluation, the embeddings are shared among different\ntask-specific output layers to make corresponding predictions. To the best of\nour knowledge, HMTGIN is the first MTL model capable of tackling CQA tasks from\nthe aspect of multi-relational graphs. To evaluate HMTGIN's effectiveness, we\nbuild a novel large-scale multi-relational graph CQA dataset with over two\nmillion nodes from Stack Overflow. Extensive experiments show that: $(1)$\nHMTGIN is superior to all baselines on five tasks; $(2)$ The proposed MTL\nstrategy and cross-task constraints have substantial advantages.", "published": "2021-09-04 03:19:20", "link": "http://arxiv.org/abs/2110.02059v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Joint Learning of Chest X-Ray and Radiology Report by Word\n  Region Alignment", "abstract": "Self-supervised learning provides an opportunity to explore unlabeled chest\nX-rays and their associated free-text reports accumulated in clinical routine\nwithout manual supervision. This paper proposes a Joint Image Text\nRepresentation Learning Network (JoImTeRNet) for pre-training on chest X-ray\nimages and their radiology reports. The model was pre-trained on both the\nglobal image-sentence level and the local image region-word level for\nvisual-textual matching. Both are bidirectionally constrained on Cross-Entropy\nbased and ranking-based Triplet Matching Losses. The region-word matching is\ncalculated using the attention mechanism without direct supervision about their\nmapping. The pre-trained multi-modal representation learning paves the way for\ndownstream tasks concerning image and/or text encoding. We demonstrate the\nrepresentation learning quality by cross-modality retrievals and multi-label\nclassifications on two datasets: OpenI-IU and MIMIC-CXR", "published": "2021-09-04 22:58:35", "link": "http://arxiv.org/abs/2109.01949v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "eess.IV"], "primary_category": "cs.LG"}
{"title": "Network Modulation Synthesis: New Algorithms for Generating Musical\n  Audio Using Autoencoder Networks", "abstract": "A new framework is presented for generating musical audio using autoencoder\nneural networks. With the presented framework, called network modulation\nsynthesis, users can create synthesis architectures and use novel generative\nalgorithms to more easily move through the complex latent parameter space of an\nautoencoder model to create audio.\n  Implementations of the new algorithms are provided for the open-source CANNe\nsynthesizer network, and can be applied to other autoencoder networks for audio\nsynthesis. Spectrograms and time-series encoding analysis demonstrate that the\nnew algorithms provide simple mechanisms for users to generate time-varying\nparameter combinations, and therefore auditory possibilities, that are\ndifficult to create by generating audio from handcrafted encodings.", "published": "2021-09-04 22:58:08", "link": "http://arxiv.org/abs/2109.01948v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A New Non-Negative Matrix Co-Factorisation Approach for Noisy Neonatal\n  Chest Sound Separation", "abstract": "Obtaining high-quality heart and lung sounds enables clinicians to accurately\nassess a newborn's cardio-respiratory health and provide timely care. However,\nnoisy chest sound recordings are common, hindering timely and accurate\nassessment. A new Non-negative Matrix Co-Factorisation-based approach is\nproposed to separate noisy chest sound recordings into heart, lung, and noise\ncomponents to address this problem. This method is achieved through training\nwith 20 high-quality heart and lung sounds, in parallel with separating the\nsounds of the noisy recording. The method was tested on 68 10-second noisy\nrecordings containing both heart and lung sounds and compared to the current\nstate of the art Non-negative Matrix Factorisation methods. Results show\nsignificant improvements in heart and lung sound quality scores respectively,\nand improved accuracy of 3.6bpm and 1.2bpm in heart and breathing rate\nestimation respectively, when compared to existing methods.", "published": "2021-09-04 02:48:02", "link": "http://arxiv.org/abs/2109.03275v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "SEC4SR: A Security Analysis Platform for Speaker Recognition", "abstract": "Adversarial attacks have been expanded to speaker recognition (SR). However,\nexisting attacks are often assessed using different SR models, recognition\ntasks and datasets, and only few adversarial defenses borrowed from computer\nvision are considered. Yet,these defenses have not been thoroughly evaluated\nagainst adaptive attacks. Thus, there is still a lack of quantitative\nunderstanding about the strengths and limitations of adversarial attacks and\ndefenses. More effective defenses are also required for securing SR systems. To\nbridge this gap, we present SEC4SR, the first platform enabling researchers to\nsystematically and comprehensively evaluate adversarial attacks and defenses in\nSR. SEC4SR incorporates 4 white-box and 2 black-box attacks, 24 defenses\nincluding our novel feature-level transformations. It also contains techniques\nfor mounting adaptive attacks. Using SEC4SR, we conduct thus far the\nlargest-scale empirical study on adversarial attacks and defenses in SR,\ninvolving 23 defenses, 15 attacks and 4 attack settings. Our study provides\nlots of useful findings that may advance future research: such as (1) all the\ntransformations slightly degrade accuracy on benign examples and their\neffectiveness vary with attacks; (2) most transformations become less effective\nunder adaptive attacks, but some transformations become more effective; (3) few\ntransformations combined with adversarial training yield stronger defenses over\nsome but not all attacks, while our feature-level transformation combined with\nadversarial training yields the strongest defense over all the attacks.\nExtensive experiments demonstrate capabilities and advantages of SEC4SR which\ncan benefit future research in SR.", "published": "2021-09-04 02:04:25", "link": "http://arxiv.org/abs/2109.01766v1", "categories": ["cs.CR", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
