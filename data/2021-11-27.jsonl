{"title": "Partner Personas Generation for Diverse Dialogue Generation", "abstract": "Incorporating personas information allows diverse and engaging responses in\ndialogue response generation. Unfortunately, prior works have primarily focused\non self personas and have overlooked the value of partner personas. Moreover,\nin practical applications, the availability of ground truth partner personas is\noften not the case. This paper attempts to tackle these issues by offering a\nnovel framework that leverages automatic partner personas generation to enhance\nthe succeeding dialogue generation. We incorporate reinforcement learning with\na dedicatedly designed critic network for reward judgement. Experimental\nresults from both automatic and human evaluation demonstrate a) Our framework\nis capable of generating relevant, informative and coherent partner personas,\neven compared to the ground truth partner personas. b) Generated partner\npersonas enhance the succeeding response generation, thus surpassing our\nbaselines and comparison model when partner personas are missing during the\ninference stage. c) Our framework generates responses that are more informative\nand engaging than our baseline conditioned on the ground truth partner personas\nduring inference. d) Our dedicatedly designed critic network reinforces our\nframework effectively. Finally, our framework gives better explainability and\nreduces the demands for external databases for partner personas.", "published": "2021-11-27 06:54:16", "link": "http://arxiv.org/abs/2111.13833v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A novel knowledge graph development for industry design: A case study on\n  indirect coal liquefaction process", "abstract": "Hazard and operability analysis (HAZOP) is a remarkable representative in\nindustrial safety engineering. However, a great storehouse of industrial safety\nknowledge (ISK) in HAZOP reports has not been thoroughly exploited. In order to\nreuse and unlock the value of ISK and optimize HAZOP, we have developed a novel\nknowledge graph for industrial safety (ISKG) with HAZOP as the carrier through\nbridging data science and engineering design. Specifically, firstly,\nconsidering that the knowledge contained in HAZOP reports of different\nprocesses in industry is not the same, we creatively develope a general ISK\nstandardization framework, it provides a practical scheme for integrating HAZOP\nreports from various processes and uniformly representing the ISK with diverse\nexpressions. Secondly, we conceive a novel and reliable information extraction\nmodel based on deep learning combined with data science, it can effectively\nmine ISK from HAZOP reports, which alleviates the obstacle of ISK extraction\ncaused by the particularity of HAZOP text. Finally, we build ISK triples and\nstore them in the Neo4j graph database. We take indirect coal liquefaction\nprocess as a case study to develop ISKG, and its oriented applications can\noptimize HAZOP and mine the potential of ISK, which is of great significance to\nimprove the security of the system and enhance prevention awareness for people.\nISKG containing the ISK standardization framework and the information\nextraction model sets an example of the interaction between data science and\nengineering design, which can enlighten other researchers and extend the\nperspectives of industrial safety.", "published": "2021-11-27 09:37:56", "link": "http://arxiv.org/abs/2111.13854v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tapping BERT for Preposition Sense Disambiguation", "abstract": "Prepositions are frequently occurring polysemous words. Disambiguation of\nprepositions is crucial in tasks like semantic role labelling, question\nanswering, text entailment, and noun compound paraphrasing. In this paper, we\npropose a novel methodology for preposition sense disambiguation (PSD), which\ndoes not use any linguistic tools. In a supervised setting, the machine\nlearning model is presented with sentences wherein prepositions have been\nannotated with senses. These senses are IDs in what is called The Preposition\nProject (TPP). We use the hidden layer representations from pre-trained BERT\nand BERT variants. The latent representations are then classified into the\ncorrect sense ID using a Multi Layer Perceptron. The dataset used for this task\nis from SemEval-2007 Task-6. Our methodology gives an accuracy of 86.85% which\nis better than the state-of-the-art.", "published": "2021-11-27 19:17:29", "link": "http://arxiv.org/abs/2111.13972v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Exploring Transformer Based Models to Identify Hate Speech and Offensive\n  Content in English and Indo-Aryan Languages", "abstract": "Hate speech is considered to be one of the major issues currently plaguing\nonline social media. Repeated and repetitive exposure to hate speech has been\nshown to create physiological effects on the target users. Thus, hate speech,\nin all its forms, should be addressed on these platforms in order to maintain\ngood health. In this paper, we explored several Transformer based machine\nlearning models for the detection of hate speech and offensive content in\nEnglish and Indo-Aryan languages at FIRE 2021. We explore several models such\nas mBERT, XLMR-large, XLMR-base by team name \"Super Mario\". Our models came 2nd\nposition in Code-Mixed Data set (Macro F1: 0.7107), 2nd position in Hindi\ntwo-class classification(Macro F1: 0.7797), 4th in English four-class category\n(Macro F1: 0.8006) and 12th in English two-class category (Macro F1: 0.6447).", "published": "2021-11-27 19:26:14", "link": "http://arxiv.org/abs/2111.13974v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An analysis of document graph construction methods for AMR summarization", "abstract": "Meaning Representation (AMR) is a graph-based semantic representation for\nsentences, composed of collections of concepts linked by semantic relations.\nAMR-based approaches have found success in a variety of applications, but a\nchallenge to using it in tasks that require document-level context is that it\nonly represents individual sentences. Prior work in AMR-based summarization has\nautomatically merged the individual sentence graphs into a document graph, but\nthe method of merging and its effects on summary content selection have not\nbeen independently evaluated. In this paper, we present a novel dataset\nconsisting of human-annotated alignments between the nodes of paired documents\nand summaries which may be used to evaluate (1) merge strategies; and (2) the\nperformance of content selection methods over nodes of a merged or unmerged AMR\ngraph. We apply these two forms of evaluation to prior work as well as a new\nmethod for node merging and show that our new method has significantly better\nperformance than prior work.", "published": "2021-11-27 22:12:50", "link": "http://arxiv.org/abs/2111.13993v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Low-Cost Transformer Model Compression for Large-Scale\n  Commercial Reply Suggestions", "abstract": "Fine-tuning pre-trained language models improves the quality of commercial\nreply suggestion systems, but at the cost of unsustainable training times.\nPopular training time reduction approaches are resource intensive, thus we\nexplore low-cost model compression techniques like Layer Dropping and Layer\nFreezing. We demonstrate the efficacy of these techniques in large-data\nscenarios, enabling the training time reduction for a commercial email reply\nsuggestion system by 42%, without affecting the model relevance or user\nengagement. We further study the robustness of these techniques to pre-trained\nmodel and dataset size ablation, and share several insights and recommendations\nfor commercial applications.", "published": "2021-11-27 22:42:06", "link": "http://arxiv.org/abs/2111.13999v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Abusive and Threatening Language Detection in Urdu using Boosting based\n  and BERT based models: A Comparative Approach", "abstract": "Online hatred is a growing concern on many social media platforms. To address\nthis issue, different social media platforms have introduced moderation\npolicies for such content. They also employ moderators who can check the posts\nviolating moderation policies and take appropriate action. Academicians in the\nabusive language research domain also perform various studies to detect such\ncontent better. Although there is extensive research in abusive language\ndetection in English, there is a lacuna in abusive language detection in low\nresource languages like Hindi, Urdu etc. In this FIRE 2021 shared task -\n\"HASOC- Abusive and Threatening language detection in Urdu\" the organizers\npropose an abusive language detection dataset in Urdu along with threatening\nlanguage detection. In this paper, we explored several machine learning models\nsuch as XGboost, LGBM, m-BERT based models for abusive and threatening content\ndetection in Urdu based on the shared task. We observed the Transformer model\nspecifically trained on abusive language dataset in Arabic helps in getting the\nbest performance. Our model came First for both abusive and threatening content\ndetection with an F1scoreof 0.88 and 0.54, respectively.", "published": "2021-11-27 20:03:19", "link": "http://arxiv.org/abs/2111.14830v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Common Sense Knowledge Learning for Open Vocabulary Neural Reasoning: A\n  First View into Chronic Disease Literature", "abstract": "In this paper, we address reasoning tasks from open vocabulary Knowledge\nBases (openKBs) using state-of-the-art Neural Language Models (NLMs) with\napplications in scientific literature. For this purpose, self-attention based\nNLMs are trained using a common sense KB as a source task. The NLMs are then\ntested on a target KB for open vocabulary reasoning tasks involving scientific\nknowledge related to the most prevalent chronic diseases (also known as\nnon-communicable diseases, NCDs). Our results identified NLMs that performed\nconsistently and with significance in knowledge inference for both source and\ntarget tasks. Furthermore, in our analysis by inspection we discussed the\nsemantic regularities and reasoning capabilities learned by the models, while\nshowing a first insight into the potential benefits of our approach to aid NCD\nresearch.", "published": "2021-11-27 00:21:36", "link": "http://arxiv.org/abs/2111.13781v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language models in word sense disambiguation for Polish", "abstract": "In the paper, we test two different approaches to the {unsupervised} word\nsense disambiguation task for Polish. In both methods, we use neural language\nmodels to predict words similar to those being disambiguated and, on the basis\nof these words, we predict the partition of word senses in different ways. In\nthe first method, we cluster selected similar words, while in the second, we\ncluster vectors representing their subsets. The evaluation was carried out on\ntexts annotated with plWordNet senses and provided a relatively good result\n(F1=0.68 for all ambiguous words). The results are significantly better than\nthose obtained for the neural model-based unsupervised method proposed in\n\\cite{waw:myk:17:Sense} and are at the level of the supervised method presented\nthere. The proposed method may be a way of solving word sense disambiguation\nproblem for languages that lack sense annotated data.", "published": "2021-11-27 20:47:53", "link": "http://arxiv.org/abs/2111.13982v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Answer Generation for Questions With Multiple Information Sources in\n  E-Commerce", "abstract": "Automatic question answering is an important yet challenging task in\nE-commerce given the millions of questions posted by users about the product\nthat they are interested in purchasing. Hence, there is a great demand for\nautomatic answer generation systems that provide quick responses using related\ninformation about the product. There are three sources of knowledge available\nfor answering a user posted query, they are reviews, duplicate or similar\nquestions, and specifications. Effectively utilizing these information sources\nwill greatly aid us in answering complex questions. However, there are two main\nchallenges present in exploiting these sources: (i) The presence of irrelevant\ninformation and (ii) the presence of ambiguity of sentiment present in reviews\nand similar questions. Through this work we propose a novel pipeline (MSQAP)\nthat utilizes the rich information present in the aforementioned sources by\nseparately performing relevancy and ambiguity prediction before generating a\nresponse.\n  Experimental results show that our relevancy prediction model (BERT-QA)\noutperforms all other variants and has an improvement of 12.36% in F1 score\ncompared to the BERT-base baseline. Our generation model (T5-QA) outperforms\nthe baselines in all content preservation metrics such as BLEU, ROUGE and has\nan average improvement of 35.02% in ROUGE and 198.75% in BLEU compared to the\nhighest performing baseline (HSSC-q). Human evaluation of our pipeline shows us\nthat our method has an overall improvement in accuracy of 30.7% over the\ngeneration model (T5-QA), resulting in our full pipeline-based approach (MSQAP)\nproviding more accurate answers. To the best of our knowledge, this is the\nfirst work in the e-commerce domain that automatically generates natural\nlanguage answers combining the information present in diverse sources such as\nspecifications, similar questions, and reviews data.", "published": "2021-11-27 23:19:49", "link": "http://arxiv.org/abs/2111.14003v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Hierarchical Organization of Syntax", "abstract": "Hierarchies are the hidden backbones of complex systems and their analysis\nallows for a deeper understanding of their structure and how they evolve. We\nconsider languages also to be complex adaptive systems with several intricate\nnetworks that capture their structure and function. Hence, we decided to\nanalyze the hierarchical organization of historical syntactic networks to\nunderstand how syntax evolves over time. We created these networks from a\ncorpus of German texts from the 11th to 17th centuries, focusing on the\nhierarchical levels of these networks. diachronically and to map them to\nspecific communicative needs of speakers. We developed a framework to\nempirically track the emergence of syntactic structures diachronically,\nenabling us to map the communicative needs of speakers with these structures.\nWe named these syntactic structures \"syntactic communicative hierarchies.\" We\nshowed that the communicative needs of speakers are the organizational force of\nsyntax. Thus, we argue that the emergence of syntactic communicative\nhierarchies plays a crucial role in shaping syntax over time. This may indicate\nthat languages evolve not only to increase the efficiency of transferring\ninformation, but also to increase our capacity, as a species, to communicate\nour needs with more and more sophisticated abstractions.", "published": "2021-11-27 00:47:54", "link": "http://arxiv.org/abs/2112.05783v2", "categories": ["cs.CL", "nlin.AO"], "primary_category": "cs.CL"}
{"title": "Natural Language Processing in-and-for Design Research", "abstract": "We review the scholarly contributions that utilise Natural Language\nProcessing (NLP) techniques to support the design process. Using a heuristic\napproach, we gathered 223 articles that are published in 32 journals within the\nperiod 1991-present. We present state-of-the-art NLP in-and-for design research\nby reviewing these articles according to the type of natural language text\nsources: internal reports, design concepts, discourse transcripts, technical\npublications, consumer opinions, and others. Upon summarizing and identifying\nthe gaps in these contributions, we utilise an existing design innovation\nframework to identify the applications that are currently being supported by\nNLP. We then propose a few methodological and theoretical directions for future\nNLP in-and-for design research.", "published": "2021-11-27 06:32:54", "link": "http://arxiv.org/abs/2111.13827v3", "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Why KDAC? A general activation function for knowledge discovery", "abstract": "Deep learning oriented named entity recognition (DNER) has gradually become\nthe paradigm of knowledge discovery, which greatly promotes domain\nintelligence. However, the current activation function of DNER fails to treat\ngradient vanishing, no negative output or non-differentiable existence, which\nmay impede knowledge exploration caused by the omission and incomplete\nrepresentation of latent semantics. To break through the dilemma, we present a\nnovel activation function termed KDAC. Detailly, KDAC is an aggregation\nfunction with multiple conversion modes. The backbone of the activation region\nis the interaction between exponent and linearity, and the both ends extend\nthrough adaptive linear divergence, which surmounts the obstacle of gradient\nvanishing and no negative output. Crucially, the non-differentiable points are\nalerted and eliminated by an approximate smoothing algorithm. KDAC has a series\nof brilliant properties, including nonlinear, stable near-linear transformation\nand derivative, as well as dynamic style, etc. We perform experiments based on\nBERT-BiLSTM-CNN-CRF model on six benchmark datasets containing different domain\nknowledge, such as Weibo, Clinical, E-commerce, Resume, HAZOP and People's\ndaily. The evaluation results show that KDAC is advanced and effective, and can\nprovide more generalized activation to stimulate the performance of DNER. We\nhope that KDAC can be exploited as a promising activation function to devote\nitself to the construction of knowledge.", "published": "2021-11-27 10:05:12", "link": "http://arxiv.org/abs/2111.13858v4", "categories": ["cs.LG", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "A New Multifractal-based Deep Learning Model for Text Mining", "abstract": "In this world full of uncertainty, where the fabric of existence weaves\npatterns of complexity, multifractal emerges as beacons of insight,\nilluminating them. As we delve into the realm of text mining that underpins\nvarious natural language processing applications and powers a range of\nintelligent services, we recognize that behind the veil of text lies a\nmanifestation of human thought and cognition, intricately intertwined with the\ncomplexities. Building upon the foundation of perceiving text as a complex\nsystem, this study embarks on a journey to unravel the hidden treasures within,\narmed with the proposed multifractal method that deciphers the multifractal\nattributes embedded within the text landscape. This endeavor culminates in the\nbirth of our novel model, which also harnesses the power of the proposed\nactivation function to facilitate nonlinear information transmission within its\nneural network architecture. The success on experiments anchored in real-world\ntechnical reports covering the extraction of technical term and classification\nof hazard events, stands as a testament to our endeavors. This research venture\nnot only expands our understanding of text mining but also opens new horizons\nfor knowledge discovery across various domains.", "published": "2021-11-27 10:22:29", "link": "http://arxiv.org/abs/2111.13861v2", "categories": ["cs.LG", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Low-Latency Online Speaker Diarization with Graph-Based Label Generation", "abstract": "This paper introduces an online speaker diarization system that can handle\nlong-time audio with low latency. We enable Agglomerative Hierarchy Clustering\n(AHC) to work in an online fashion by introducing a label matching algorithm.\nThis algorithm solves the inconsistency between output labels and hidden labels\nthat are generated each turn. To ensure the low latency in the online setting,\nwe introduce a variant of AHC, namely chkpt-AHC, to cluster the speakers. In\naddition, we propose a speaker embedding graph to exploit a graph-based\nre-clustering method, further improving the performance. In the experiment, we\nevaluate our systems on both DIHARD3 and VoxConverse datasets. The experimental\nresults show that our proposed online systems have better performance than our\nbaseline online system and have comparable performance to our offline systems.\nWe find out that the framework combining the chkpt-AHC method and the label\nmatching algorithm works well in the online setting. Moreover, the chkpt-AHC\nmethod greatly reduces the time cost, while the graph-based re-clustering\nmethod helps improve the performance.", "published": "2021-11-27 03:34:34", "link": "http://arxiv.org/abs/2111.13803v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
