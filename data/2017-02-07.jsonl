{"title": "The Effect of Different Writing Tasks on Linguistic Style: A Case Study\n  of the ROC Story Cloze Task", "abstract": "A writer's style depends not just on personal traits but also on her intent\nand mental state. In this paper, we show how variants of the same writing task\ncan lead to measurable differences in writing style. We present a case study\nbased on the story cloze task (Mostafazadeh et al., 2016a), where annotators\nwere assigned similar writing tasks with different constraints: (1) writing an\nentire story, (2) adding a story ending for a given story context, and (3)\nadding an incoherent ending to a story. We show that a simple linear classifier\ninformed by stylistic features is able to successfully distinguish among the\nthree cases, without even looking at the story context. In addition, combining\nour stylistic features with language model predictions reaches state of the art\nperformance on the story cloze challenge. Our results demonstrate that\ndifferent task framings can dramatically affect the way people write.", "published": "2017-02-07 01:39:02", "link": "http://arxiv.org/abs/1702.01841v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparative Study of CNN and RNN for Natural Language Processing", "abstract": "Deep neural networks (DNN) have revolutionized the field of natural language\nprocessing (NLP). Convolutional neural network (CNN) and recurrent neural\nnetwork (RNN), the two main types of DNN architectures, are widely explored to\nhandle various NLP tasks. CNN is supposed to be good at extracting\nposition-invariant features and RNN at modeling units in sequence. The state of\nthe art on many NLP tasks often switches due to the battle between CNNs and\nRNNs. This work is the first systematic comparison of CNN and RNN on a wide\nrange of representative NLP tasks, aiming to give basic guidance for DNN\nselection.", "published": "2017-02-07 08:33:35", "link": "http://arxiv.org/abs/1702.01923v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Knowledge-Grounded Neural Conversation Model", "abstract": "Neural network models are capable of generating extremely natural sounding\nconversational interactions. Nevertheless, these models have yet to demonstrate\nthat they can incorporate content in the form of factual information or\nentity-grounded opinion that would enable them to serve in more task-oriented\nconversational applications. This paper presents a novel, fully data-driven,\nand knowledge-grounded neural conversation model aimed at producing more\ncontentful responses without slot filling. We generalize the widely-used\nSeq2Seq approach by conditioning responses on both conversation history and\nexternal \"facts\", allowing the model to be versatile and applicable in an\nopen-domain setting. Our approach yields significant improvements over a\ncompetitive Seq2Seq baseline. Human judges found that our outputs are\nsignificantly more informative.", "published": "2017-02-07 09:16:46", "link": "http://arxiv.org/abs/1702.01932v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EliXa: A Modular and Flexible ABSA Platform", "abstract": "This paper presents a supervised Aspect Based Sentiment Analysis (ABSA)\nsystem. Our aim is to develop a modular platform which allows to easily conduct\nexperiments by replacing the modules or adding new features. We obtain the best\nresult in the Opinion Target Extraction (OTE) task (slot 2) using an\noff-the-shelf sequence labeler. The target polarity classification (slot 3) is\naddressed by means of a multiclass SVM algorithm which includes lexical based\nfeatures such as the polarity values obtained from domain and open polarity\nlexicons. The system obtains accuracies of 0.70 and 0.73 for the restaurant and\nlaptop domain respectively, and performs second best in the out-of-domain\nhotel, achieving an accuracy of 0.80.", "published": "2017-02-07 10:18:07", "link": "http://arxiv.org/abs/1702.01944v1", "categories": ["cs.CL", "I.2.7; H.3.1"], "primary_category": "cs.CL"}
{"title": "Fast and Accurate Entity Recognition with Iterated Dilated Convolutions", "abstract": "Today when many practitioners run basic NLP on the entire web and\nlarge-volume traffic, faster methods are paramount to saving time and energy\ncosts. Recent advances in GPU hardware have led to the emergence of\nbi-directional LSTMs as a standard method for obtaining per-token vector\nrepresentations serving as input to labeling tasks such as NER (often followed\nby prediction in a linear-chain CRF). Though expressive and accurate, these\nmodels fail to fully exploit GPU parallelism, limiting their computational\nefficiency. This paper proposes a faster alternative to Bi-LSTMs for NER:\nIterated Dilated Convolutional Neural Networks (ID-CNNs), which have better\ncapacity than traditional CNNs for large context and structured prediction.\nUnlike LSTMs whose sequential processing on sentences of length N requires O(N)\ntime even in the face of parallelism, ID-CNNs permit fixed-depth convolutions\nto run in parallel across entire documents. We describe a distinct combination\nof network structure, parameter sharing and training procedures that enable\ndramatic 14-20x test-time speedups while retaining accuracy comparable to the\nBi-LSTM-CRF. Moreover, ID-CNNs trained to aggregate context from the entire\ndocument are even more accurate while maintaining 8x faster test time speeds.", "published": "2017-02-07 16:58:18", "link": "http://arxiv.org/abs/1702.02098v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to evaluate word embeddings? On importance of data efficiency and\n  simple supervised tasks", "abstract": "Maybe the single most important goal of representation learning is making\nsubsequent learning faster. Surprisingly, this fact is not well reflected in\nthe way embeddings are evaluated. In addition, recent practice in word\nembeddings points towards importance of learning specialized representations.\nWe argue that focus of word representation evaluation should reflect those\ntrends and shift towards evaluating what useful information is easily\naccessible. Specifically, we propose that evaluation should focus on data\nefficiency and simple supervised tasks, where the amount of available data is\nvaried and scores of a supervised model are reported for each subset (as\ncommonly done in transfer learning).\n  In order to illustrate significance of such analysis, a comprehensive\nevaluation of selected word embeddings is presented. Proposed approach yields a\nmore complete picture and brings new insight into performance characteristics,\nfor instance information about word similarity or analogy tends to be\nnon--linearly encoded in the embedding space, which questions the cosine-based,\nunsupervised, evaluation methods. All results and analysis scripts are\navailable online.", "published": "2017-02-07 19:21:50", "link": "http://arxiv.org/abs/1702.02170v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question Answering through Transfer Learning from Large Fine-grained\n  Supervision Data", "abstract": "We show that the task of question answering (QA) can significantly benefit\nfrom the transfer learning of models trained on a different large, fine-grained\nQA dataset. We achieve the state of the art in two well-studied QA datasets,\nWikiQA and SemEval-2016 (Task 3A), through a basic transfer learning technique\nfrom SQuAD. For WikiQA, our model outperforms the previous best model by more\nthan 8%. We demonstrate that finer supervision provides better guidance for\nlearning lexical and syntactic information than coarser supervision, through\nquantitative results and visual analysis. We also show that a similar transfer\nlearning procedure achieves the state of the art on an entailment task.", "published": "2017-02-07 19:22:06", "link": "http://arxiv.org/abs/1702.02171v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fixing the Infix: Unsupervised Discovery of Root-and-Pattern Morphology", "abstract": "We present an unsupervised and language-agnostic method for learning\nroot-and-pattern morphology in Semitic languages. This form of morphology,\nabundant in Semitic languages, has not been handled in prior unsupervised\napproaches. We harness the syntactico-semantic information in distributed word\nrepresentations to solve the long standing problem of root-and-pattern\ndiscovery in Semitic languages. Moreover, we construct an unsupervised root\nextractor based on the learned rules. We prove the validity of learned rules\nacross Arabic, Hebrew, and Amharic, alongside showing that our root extractor\ncompares favorably with a widely used, carefully engineered root extractor:\nISRI.", "published": "2017-02-07 21:43:21", "link": "http://arxiv.org/abs/1702.02211v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MORSE: Semantic-ally Drive-n MORpheme SEgment-er", "abstract": "We present in this paper a novel framework for morpheme segmentation which\nuses the morpho-syntactic regularities preserved by word representations, in\naddition to orthographic features, to segment words into morphemes. This\nframework is the first to consider vocabulary-wide syntactico-semantic\ninformation for this task. We also analyze the deficiencies of available\nbenchmarking datasets and introduce our own dataset that was created on the\nbasis of compositionality. We validate our algorithm across datasets and\npresent state-of-the-art results.", "published": "2017-02-07 21:49:13", "link": "http://arxiv.org/abs/1702.02212v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Discourse Structure for Text Categorization", "abstract": "We show that discourse structure, as defined by Rhetorical Structure Theory\nand provided by an existing discourse parser, benefits text categorization. Our\napproach uses a recursive neural network and a newly proposed attention\nmechanism to compute a representation of the text that focuses on salient\ncontent, from the perspective of both RST and the task. Experiments consider\nvariants of the approach and illustrate its strengths and weaknesses.", "published": "2017-02-07 00:26:56", "link": "http://arxiv.org/abs/1702.01829v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Effects of Stop Words Elimination for Arabic Information Retrieval: A\n  Comparative Study", "abstract": "The effectiveness of three stop words lists for Arabic Information\nRetrieval---General Stoplist, Corpus-Based Stoplist, Combined Stoplist ---were\ninvestigated in this study. Three popular weighting schemes were examined: the\ninverse document frequency weight, probabilistic weighting, and statistical\nlanguage modelling. The Idea is to combine the statistical approaches with\nlinguistic approaches to reach an optimal performance, and compare their effect\non retrieval. The LDC (Linguistic Data Consortium) Arabic Newswire data set was\nused with the Lemur Toolkit. The Best Match weighting scheme used in the Okapi\nretrieval system had the best overall performance of the three weighting\nalgorithms used in the study, stoplists improved retrieval effectiveness\nespecially when used with the BM25 weight. The overall performance of a general\nstoplist was better than the other two lists.", "published": "2017-02-07 08:49:58", "link": "http://arxiv.org/abs/1702.01925v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Knowledge Adaptation: Teaching to Adapt", "abstract": "Domain adaptation is crucial in many real-world applications where the\ndistribution of the training data differs from the distribution of the test\ndata. Previous Deep Learning-based approaches to domain adaptation need to be\ntrained jointly on source and target domain data and are therefore unappealing\nin scenarios where models need to be adapted to a large number of domains or\nwhere a domain is evolving, e.g. spam detection where attackers continuously\nchange their tactics.\n  To fill this gap, we propose Knowledge Adaptation, an extension of Knowledge\nDistillation (Bucilua et al., 2006; Hinton et al., 2015) to the domain\nadaptation scenario. We show how a student model achieves state-of-the-art\nresults on unsupervised domain adaptation from multiple sources on a standard\nsentiment analysis benchmark by taking into account the domain-specific\nexpertise of multiple teachers and the similarities between their domains.\n  When learning from a single teacher, using domain similarity to gauge\ntrustworthiness is inadequate. To this end, we propose a simple metric that\ncorrelates well with the teacher's accuracy in the target domain. We\ndemonstrate that incorporating high-confidence examples selected by this metric\nenables the student model to achieve state-of-the-art performance in the\nsingle-source scenario.", "published": "2017-02-07 14:59:45", "link": "http://arxiv.org/abs/1702.02052v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semi-Supervised QA with Generative Domain-Adaptive Nets", "abstract": "We study the problem of semi-supervised question answering----utilizing\nunlabeled text to boost the performance of question answering models. We\npropose a novel training framework, the Generative Domain-Adaptive Nets. In\nthis framework, we train a generative model to generate questions based on the\nunlabeled text, and combine model-generated questions with human-generated\nquestions for training question answering models. We develop novel domain\nadaptation algorithms, based on reinforcement learning, to alleviate the\ndiscrepancy between the model-generated data distribution and the\nhuman-generated data distribution. Experiments show that our proposed framework\nobtains substantial improvement from unlabeled text.", "published": "2017-02-07 21:23:01", "link": "http://arxiv.org/abs/1702.02206v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Representations of language in a model of visually grounded speech\n  signal", "abstract": "We present a visually grounded model of speech perception which projects\nspoken utterances and images to a joint semantic space. We use a multi-layer\nrecurrent highway network to model the temporal nature of spoken speech, and\nshow that it learns to extract both form and meaning-based linguistic knowledge\nfrom the input signal. We carry out an in-depth analysis of the representations\nused by different components of the trained model and show that encoding of\nsemantic aspects tends to become richer as we go up the hierarchy of layers,\nwhereas encoding of form-related aspects of the language input tends to\ninitially increase and then plateau or decrease.", "published": "2017-02-07 13:02:09", "link": "http://arxiv.org/abs/1702.01991v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
