{"title": "Grimoire is All You Need for Enhancing Large Language Models", "abstract": "In-context Learning (ICL) is one of the key methods for enhancing the\nperformance of large language models on specific tasks by providing a set of\nfew-shot examples. However, the ICL capability of different types of models\nshows significant variation due to factors such as model architecture, volume\nof learning data, and the size of parameters. Generally, the larger the model's\nparameter size and the more extensive the learning data, the stronger its ICL\ncapability. In this paper, we propose a method SLEICL that involves learning\nfrom examples using strong language models and then summarizing and\ntransferring these learned skills to weak language models for inference and\napplication. This ensures the stability and effectiveness of ICL. Compared to\ndirectly enabling weak language models to learn from prompt examples, SLEICL\nreduces the difficulty of ICL for these models. Our experiments, conducted on\nup to eight datasets with five language models, demonstrate that weak language\nmodels achieve consistent improvement over their own zero-shot or few-shot\ncapabilities using the SLEICL method. Some weak language models even surpass\nthe performance of GPT4-1106-preview (zero-shot) with the aid of SLEICL.", "published": "2024-01-07 04:32:29", "link": "http://arxiv.org/abs/2401.03385v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Empirical Study of Large Language Models as Automated Essay Scoring\n  Tools in English Composition__Taking TOEFL Independent Writing Task for\n  Example", "abstract": "Large language models have demonstrated exceptional capabilities in tasks\ninvolving natural language generation, reasoning, and comprehension. This study\naims to construct prompts and comments grounded in the diverse scoring criteria\ndelineated within the official TOEFL guide. The primary objective is to assess\nthe capabilities and constraints of ChatGPT, a prominent representative of\nlarge language models, within the context of automated essay scoring. The\nprevailing methodologies for automated essay scoring involve the utilization of\ndeep neural networks, statistical machine learning techniques, and fine-tuning\npre-trained models. However, these techniques face challenges when applied to\ndifferent contexts or subjects, primarily due to their substantial data\nrequirements and limited adaptability to small sample sizes. In contrast, this\nstudy employs ChatGPT to conduct an automated evaluation of English essays,\neven with a small sample size, employing an experimental approach. The\nempirical findings indicate that ChatGPT can provide operational functionality\nfor automated essay scoring, although the results exhibit a regression effect.\nIt is imperative to underscore that the effective design and implementation of\nChatGPT prompts necessitate a profound domain expertise and technical\nproficiency, as these prompts are subject to specific threshold criteria.\nKeywords: ChatGPT, Automated Essay Scoring, Prompt Learning, TOEFL Independent\nWriting Task", "published": "2024-01-07 07:13:50", "link": "http://arxiv.org/abs/2401.03401v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PEneo: Unifying Line Extraction, Line Grouping, and Entity Linking for\n  End-to-end Document Pair Extraction", "abstract": "Document pair extraction aims to identify key and value entities as well as\ntheir relationships from visually-rich documents. Most existing methods divide\nit into two separate tasks: semantic entity recognition (SER) and relation\nextraction (RE). However, simply concatenating SER and RE serially can lead to\nsevere error propagation, and it fails to handle cases like multi-line entities\nin real scenarios. To address these issues, this paper introduces a novel\nframework, PEneo (Pair Extraction new decoder option), which performs document\npair extraction in a unified pipeline, incorporating three concurrent\nsub-tasks: line extraction, line grouping, and entity linking. This approach\nalleviates the error accumulation problem and can handle the case of multi-line\nentities. Furthermore, to better evaluate the model's performance and to\nfacilitate future research on pair extraction, we introduce RFUND, a\nre-annotated version of the commonly used FUNSD and XFUND datasets, to make\nthem more accurate and cover realistic situations. Experiments on various\nbenchmarks demonstrate PEneo's superiority over previous pipelines, boosting\nthe performance by a large margin (e.g., 19.89%-22.91% F1 score on RFUND-EN)\nwhen combined with various backbones like LiLT and LayoutLMv3, showing its\neffectiveness and generality. Codes and the new annotations are available at\nhttps://github.com/ZeningLin/PEneo.", "published": "2024-01-07 12:48:07", "link": "http://arxiv.org/abs/2401.03472v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ROIC-DM: Robust Text Inference and Classification via Diffusion Model", "abstract": "While language models have made many milestones in text inference and\nclassification tasks, they remain susceptible to adversarial attacks that can\nlead to unforeseen outcomes. Existing works alleviate this problem by equipping\nlanguage models with defense patches. However, these defense strategies often\nrely on impractical assumptions or entail substantial sacrifices in model\nperformance. Consequently, enhancing the resilience of the target model using\nsuch defense mechanisms is a formidable challenge. This paper introduces an\ninnovative model for robust text inference and classification, built upon\ndiffusion models (ROIC-DM). Benefiting from its training involving denoising\nstages, ROIC-DM inherently exhibits greater robustness compared to conventional\nlanguage models. Moreover, ROIC-DM can attain comparable, and in some cases,\nsuperior performance to language models, by effectively incorporating them as\nadvisory components. Extensive experiments conducted with several strong\ntextual adversarial attacks on three datasets demonstrate that (1) ROIC-DM\noutperforms traditional language models in robustness, even when the latter are\nfortified with advanced defense mechanisms; (2) ROIC-DM can achieve comparable\nand even better performance than traditional language models by using them as\nadvisors.", "published": "2024-01-07 15:05:26", "link": "http://arxiv.org/abs/2401.03514v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building Efficient and Effective OpenQA Systems for Low-Resource\n  Languages", "abstract": "Question answering (QA) is the task of answering questions posed in natural\nlanguage with free-form natural language answers extracted from a given\npassage. In the OpenQA variant, only a question text is given, and the system\nmust retrieve relevant passages from an unstructured knowledge source and use\nthem to provide answers, which is the case in the mainstream QA systems on the\nWeb. QA systems currently are mostly limited to the English language due to the\nlack of large-scale labeled QA datasets in non-English languages. In this\npaper, we show that effective, low-cost OpenQA systems can be developed for\nlow-resource contexts. The key ingredients are (1) weak supervision using\nmachine-translated labeled datasets and (2) a relevant unstructured knowledge\nsource in the target language context. Furthermore, we show that only a few\nhundred gold assessment examples are needed to reliably evaluate these systems.\nWe apply our method to Turkish as a challenging case study, since English and\nTurkish are typologically very distinct and Turkish has limited resources for\nQA. We present SQuAD-TR, a machine translation of SQuAD2.0, and we build our\nOpenQA system by adapting ColBERT-QA and retraining it over Turkish resources\nand SQuAD-TR using two versions of Wikipedia dumps spanning two years. We\nobtain a performance improvement of 24-32% in the Exact Match (EM) score and\n22-29% in the F1 score compared to the BM25-based and DPR-based baseline QA\nreader models. Our results show that SQuAD-TR makes OpenQA feasible for\nTurkish, which we hope encourages researchers to build OpenQA systems in other\nlow-resource languages. We make all the code, models, and the dataset publicly\navailable at https://github.com/boun-tabi/SQuAD-TR.", "published": "2024-01-07 22:11:36", "link": "http://arxiv.org/abs/2401.03590v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Classification Based on Knowledge Graphs and Improved Attention\n  Mechanism", "abstract": "To resolve the semantic ambiguity in texts, we propose a model, which\ninnovatively combines a knowledge graph with an improved attention mechanism.\nAn existing knowledge base is utilized to enrich the text with relevant\ncontextual concepts. The model operates at both character and word levels to\ndeepen its understanding by integrating the concepts. We first adopt\ninformation gain to select import words. Then an encoder-decoder framework is\nused to encode the text along with the related concepts. The local attention\nmechanism adjusts the weight of each concept, reducing the influence of\nirrelevant or noisy concepts during classification. We improve the calculation\nformula for attention scores in the local self-attention mechanism, ensuring\nthat words with different frequencies of occurrence in the text receive higher\nattention scores. Finally, the model employs a Bi-directional Gated Recurrent\nUnit (Bi-GRU), which is effective in feature extraction from texts for improved\nclassification accuracy. Its performance is demonstrated on datasets such as\nAGNews, Ohsumed, and TagMyNews, achieving accuracy of 75.1%, 58.7%, and 68.5%\nrespectively, showing its effectiveness in classifying tasks.", "published": "2024-01-07 22:20:55", "link": "http://arxiv.org/abs/2401.03591v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GRAM: Global Reasoning for Multi-Page VQA", "abstract": "The increasing use of transformer-based large language models brings forward\nthe challenge of processing long sequences. In document visual question\nanswering (DocVQA), leading methods focus on the single-page setting, while\ndocuments can span hundreds of pages. We present GRAM, a method that seamlessly\nextends pre-trained single-page models to the multi-page setting, without\nrequiring computationally-heavy pretraining. To do so, we leverage a\nsingle-page encoder for local page-level understanding, and enhance it with\ndocument-level designated layers and learnable tokens, facilitating the flow of\ninformation across pages for global reasoning. To enforce our model to utilize\nthe newly introduced document tokens, we propose a tailored bias adaptation\nmethod. For additional computational savings during decoding, we introduce an\noptional compression stage using our compression-transformer\n(C-Former),reducing the encoded sequence length, thereby allowing a tradeoff\nbetween quality and latency. Extensive experiments showcase GRAM's\nstate-of-the-art performance on the benchmarks for multi-page DocVQA,\ndemonstrating the effectiveness of our approach.", "published": "2024-01-07 08:03:06", "link": "http://arxiv.org/abs/2401.03411v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "On Leveraging Large Language Models for Enhancing Entity Resolution: A\n  Cost-efficient Approach", "abstract": "Entity resolution, the task of identifying and merging records that refer to\nthe same real-world entity, is crucial in sectors like e-commerce, healthcare,\nand law enforcement. Large Language Models (LLMs) introduce an innovative\napproach to this task, capitalizing on their advanced linguistic capabilities\nand a ``pay-as-you-go'' model that provides significant advantages to those\nwithout extensive data science expertise. However, current LLMs are costly due\nto per-API request billing. Existing methods often either lack quality or\nbecome prohibitively expensive at scale. To address these problems, we propose\nan uncertainty reduction framework using LLMs to improve entity resolution\nresults. We first initialize possible partitions of the entity cluster, refer\nto the same entity, and define the uncertainty of the result. Then, we reduce\nthe uncertainty by selecting a few valuable matching questions for LLM\nverification. Upon receiving the answers, we update the probability\ndistribution of the possible partitions. To further reduce costs, we design an\nefficient algorithm to judiciously select the most valuable matching pairs to\nquery. Additionally, we create error-tolerant techniques to handle LLM mistakes\nand a dynamic adjustment method to reach truly correct partitions. Experimental\nresults show that our method is efficient and effective, offering promising\napplications in real-world tasks.", "published": "2024-01-07 09:06:58", "link": "http://arxiv.org/abs/2401.03426v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Long Context Compression with Activation Beacon", "abstract": "Long context compression is a critical research problem due to its\nsignificance in reducing the high computational and memory costs associated\nwith LLMs. In this paper, we propose Activation Beacon, a plug-in module for\ntransformer-based LLMs that targets effective, efficient, and flexible\ncompression of long contexts. To achieve this, our method introduces the\nfollowing technical designs. 1) We directly compress the activations (i.e. keys\nand values at every layer), rather than leveraging soft prompts to relay\ninformation (which constitute a major bottleneck to encapsulate the complex\ninformation within long contexts). 2) We tailor the compression workflow, where\neach fine-grained input unit is progressively compressed, enabling high-quality\ncompression and efficient computation during both training and inference. 3) We\ntrain the model through compression-based auto-regression, making full use of\nplain texts and instructional data to optimize the model's compression\nperformance. 4) During training, we randomly sample a compression ratio at each\nstep, teaching the model to support a wide range of compression configurations.\nExtensive evaluations are conducted on various long-context tasks whose lengths\n(e.g., 128K) may far exceed the maximum training length (20K), such as document\nunderstanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing\nmethods struggle to handle these challenging tasks, Activation Beacon maintains\na comparable performance to the uncompressed baseline across various scenarios,\nachieving a 2x acceleration in inference time and an 8x reduction of memory\ncosts for KV cache. Our data, model, and code have been released at\n\\url{https://github.com/FlagOpen/FlagEmbedding/}.", "published": "2024-01-07 11:57:40", "link": "http://arxiv.org/abs/2401.03462v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Maintaining Journalistic Integrity in the Digital Age: A Comprehensive\n  NLP Framework for Evaluating Online News Content", "abstract": "The rapid growth of online news platforms has led to an increased need for\nreliable methods to evaluate the quality and credibility of news articles. This\npaper proposes a comprehensive framework to analyze online news texts using\nnatural language processing (NLP) techniques, particularly a language model\nspecifically trained for this purpose, alongside other well-established NLP\nmethods. The framework incorporates ten journalism standards-objectivity,\nbalance and fairness, readability and clarity, sensationalism and clickbait,\nethical considerations, public interest and value, source credibility,\nrelevance and timeliness, factual accuracy, and attribution and transparency-to\nassess the quality of news articles. By establishing these standards,\nresearchers, media organizations, and readers can better evaluate and\nunderstand the content they consume and produce. The proposed method has some\nlimitations, such as potential difficulty in detecting subtle biases and the\nneed for continuous updating of the language model to keep pace with evolving\nlanguage patterns.", "published": "2024-01-07 12:27:14", "link": "http://arxiv.org/abs/2401.03467v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RoBERTurk: Adjusting RoBERTa for Turkish", "abstract": "We pretrain RoBERTa on a Turkish corpora using BPE tokenizer. Our model\noutperforms BERTurk family models on the BOUN dataset for the POS task while\nresulting in underperformance on the IMST dataset for the same task and\nachieving competitive scores on the Turkish split of the XTREME dataset for the\nNER task - all while being pretrained on smaller data than its competitors. We\nrelease our pretrained model and tokenizer.", "published": "2024-01-07 15:13:24", "link": "http://arxiv.org/abs/2401.03515v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CAPTAIN at COLIEE 2023: Efficient Methods for Legal Information\n  Retrieval and Entailment Tasks", "abstract": "The Competition on Legal Information Extraction/Entailment (COLIEE) is held\nannually to encourage advancements in the automatic processing of legal texts.\nProcessing legal documents is challenging due to the intricate structure and\nmeaning of legal language. In this paper, we outline our strategies for\ntackling Task 2, Task 3, and Task 4 in the COLIEE 2023 competition. Our\napproach involved utilizing appropriate state-of-the-art deep learning methods,\ndesigning methods based on domain characteristics observation, and applying\nmeticulous engineering practices and methodologies to the competition. As a\nresult, our performance in these tasks has been outstanding, with first places\nin Task 2 and Task 3, and promising results in Task 4. Our source code is\navailable at https://github.com/Nguyen2015/CAPTAIN-COLIEE2023/tree/coliee2023.", "published": "2024-01-07 17:23:27", "link": "http://arxiv.org/abs/2401.03551v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Data-CUBE: Data Curriculum for Instruction-based Sentence Representation\n  Learning", "abstract": "Recently, multi-task instruction tuning has been applied into sentence\nrepresentation learning, which endows the capability of generating specific\nrepresentations with the guidance of task instruction, exhibiting strong\ngeneralization ability on new tasks. However, these methods mostly neglect the\npotential interference problems across different tasks and instances, which may\naffect the training and convergence of the model. To address it, we propose a\ndata curriculum method, namely Data-CUBE, that arranges the orders of all the\nmulti-task data for training, to minimize the interference risks from the two\nviews. In the task level, we aim to find the optimal task order to minimize the\ntotal cross-task interference risk, which is exactly the traveling salesman\nproblem, hence we utilize a simulated annealing algorithm to find its solution.\nIn the instance level, we measure the difficulty of all instances per task,\nthen divide them into the easy-to-difficult mini-batches for training.\nExperiments on MTEB sentence representation evaluation tasks show that our\napproach can boost the performance of state-of-the-art methods. Our code and\ndata are publicly available at the link:\n\\url{https://github.com/RUCAIBox/Data-CUBE}.", "published": "2024-01-07 18:12:20", "link": "http://arxiv.org/abs/2401.03563v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "InFoBench: Evaluating Instruction Following Ability in Large Language\n  Models", "abstract": "This paper introduces the Decomposed Requirements Following Ratio (DRFR), a\nnew metric for evaluating Large Language Models' (LLMs) ability to follow\ninstructions. Addressing a gap in current methodologies, DRFR breaks down\ncomplex instructions into simpler criteria, facilitating a detailed analysis of\nLLMs' compliance with various aspects of tasks. Alongside this metric, we\npresent InFoBench, a benchmark comprising 500 diverse instructions and 2,250\ndecomposed questions across multiple constraint categories. Our experiments\ncompare DRFR with traditional scoring methods and explore annotation sources,\nincluding human experts, crowd-sourced workers, and GPT-4. The findings\ndemonstrate DRFR's higher reliability and the effectiveness of using GPT-4 as a\ncost-efficient annotator. The evaluation of several advanced LLMs using this\nframework reveals their strengths and areas needing improvement, particularly\nin complex instruction-following. This study contributes a novel metric and\nbenchmark, offering insights for future LLM development and evaluation.", "published": "2024-01-07 23:01:56", "link": "http://arxiv.org/abs/2401.03601v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLMs for Robotic Object Disambiguation", "abstract": "The advantages of pre-trained large language models (LLMs) are apparent in a\nvariety of language processing tasks. But can a language model's knowledge be\nfurther harnessed to effectively disambiguate objects and navigate\ndecision-making challenges within the realm of robotics? Our study reveals the\nLLM's aptitude for solving complex decision making challenges that are often\npreviously modeled by Partially Observable Markov Decision Processes (POMDPs).\nA pivotal focus of our research is the object disambiguation capability of\nLLMs. We detail the integration of an LLM into a tabletop environment\ndisambiguation task, a decision making problem where the robot's task is to\ndiscern and retrieve a user's desired object from an arbitrarily large and\ncomplex cluster of objects. Despite multiple query attempts with zero-shot\nprompt engineering (details can be found in the Appendix), the LLM struggled to\ninquire about features not explicitly provided in the scene description. In\nresponse, we have developed a few-shot prompt engineering system to improve the\nLLM's ability to pose disambiguating queries. The result is a model capable of\nboth using given features when they are available and inferring new relevant\nfeatures when necessary, to successfully generate and navigate down a precise\ndecision tree to the correct object--even when faced with identical options.", "published": "2024-01-07 04:46:23", "link": "http://arxiv.org/abs/2401.03388v1", "categories": ["cs.RO", "cs.CL", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Escalation Risks from Language Models in Military and Diplomatic\n  Decision-Making", "abstract": "Governments are increasingly considering integrating autonomous AI agents in\nhigh-stakes military and foreign-policy decision-making, especially with the\nemergence of advanced generative AI models like GPT-4. Our work aims to\nscrutinize the behavior of multiple AI agents in simulated wargames,\nspecifically focusing on their predilection to take escalatory actions that may\nexacerbate multilateral conflicts. Drawing on political science and\ninternational relations literature about escalation dynamics, we design a novel\nwargame simulation and scoring framework to assess the escalation risks of\nactions taken by these agents in different scenarios. Contrary to prior\nstudies, our research provides both qualitative and quantitative insights and\nfocuses on large language models (LLMs). We find that all five studied\noff-the-shelf LLMs show forms of escalation and difficult-to-predict escalation\npatterns. We observe that models tend to develop arms-race dynamics, leading to\ngreater conflict, and in rare cases, even to the deployment of nuclear weapons.\nQualitatively, we also collect the models' reported reasonings for chosen\nactions and observe worrying justifications based on deterrence and\nfirst-strike tactics. Given the high stakes of military and foreign-policy\ncontexts, we recommend further examination and cautious consideration before\ndeploying autonomous language model agents for strategic military or diplomatic\ndecision-making.", "published": "2024-01-07 07:59:10", "link": "http://arxiv.org/abs/2401.03408v1", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.MA"], "primary_category": "cs.AI"}
{"title": "CharPoet: A Chinese Classical Poetry Generation System Based on\n  Token-free LLM", "abstract": "Automatic Chinese classical poetry generation has attracted much research\ninterest, but achieving effective control over format and content\nsimultaneously remains challenging. Traditional systems usually accept keywords\nas user inputs, resulting in limited control over content. Large language\nmodels (LLMs) improve content control by allowing unrestricted user\ninstructions, but the token-by-token generation process frequently makes format\nerrors. Motivated by this, we propose CharPoet, a Chinese classical poetry\ngeneration system based on token-free LLM, which provides effective control\nover both format and content. Our token-free architecture generates in a\ncharacter-by-character manner, enabling precise control over the number of\ncharacters. Pruned from existing token-based LLMs, CharPoet inherits their\npretrained capabilities and can generate poetry following instructions like\n\"Write me a poem for my mother's birthday.\" CharPoet achieves format accuracy\nabove 0.96, outperforming Jiuge-GPT-2 (0.91) and GPT-4 (0.38). In terms of\ncontent quality, CharPoet surpasses traditional systems including Jiuge, and is\ncomparable to other LLMs. Our system is open source and available at\nhttps://modelscope.cn/models/CharPoet/CharPoet. A video demonstration of\nCharPoet is available at https://youtu.be/voZ25qEp3Dc.", "published": "2024-01-07 15:00:36", "link": "http://arxiv.org/abs/2401.03512v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transfer the linguistic representations from TTS to accent conversion\n  with non-parallel data", "abstract": "Accent conversion aims to convert the accent of a source speech to a target\naccent, meanwhile preserving the speaker's identity. This paper introduces a\nnovel non-autoregressive framework for accent conversion that learns\naccent-agnostic linguistic representations and employs them to convert the\naccent in the source speech. Specifically, the proposed system aligns speech\nrepresentations with linguistic representations obtained from Text-to-Speech\n(TTS) systems, enabling training of the accent voice conversion model on\nnon-parallel data. Furthermore, we investigate the effectiveness of a\npretraining strategy on native data and different acoustic features within our\nproposed framework. We conduct a comprehensive evaluation using both subjective\nand objective metrics to assess the performance of our approach. The evaluation\nresults highlight the benefits of the pretraining strategy and the\nincorporation of richer semantic features, resulting in significantly enhanced\naudio quality and intelligibility.", "published": "2024-01-07 16:39:34", "link": "http://arxiv.org/abs/2401.03538v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Is there really a Citation Age Bias in NLP?", "abstract": "Citations are a key ingredient of scientific research to relate a paper to\nothers published in the community. Recently, it has been noted that there is a\ncitation age bias in the Natural Language Processing (NLP) community, one of\nthe currently fastest growing AI subfields, in that the mean age of the\nbibliography of NLP papers has become ever younger in the last few years,\nleading to `citation amnesia' in which older knowledge is increasingly\nforgotten. In this work, we put such claims into perspective by analyzing the\nbibliography of $\\sim$300k papers across 15 different scientific fields\nsubmitted to the popular preprint server Arxiv in the time period from 2013 to\n2022. We find that all AI subfields (in particular: cs.AI, cs.CL, cs.CV, cs.LG)\nhave similar trends of citation amnesia, in which the age of the bibliography\nhas roughly halved in the last 10 years (from above 12 in 2013 to below 7 in\n2022), on average. Rather than diagnosing this as a citation age bias in the\nNLP community, we believe this pattern is an artefact of the dynamics of these\nresearch fields, in which new knowledge is produced in ever shorter time\nintervals.", "published": "2024-01-07 17:12:08", "link": "http://arxiv.org/abs/2401.03545v1", "categories": ["cs.DL", "cs.AI", "cs.CL"], "primary_category": "cs.DL"}
{"title": "ChatGPT for Conversational Recommendation: Refining Recommendations by\n  Reprompting with Feedback", "abstract": "Recommendation algorithms have been pivotal in handling the overwhelming\nvolume of online content. However, these algorithms seldom consider direct user\ninput, resulting in superficial interaction between them. Efforts have been\nmade to include the user directly in the recommendation process through\nconversation, but these systems too have had limited interactivity. Recently,\nLarge Language Models (LLMs) like ChatGPT have gained popularity due to their\nease of use and their ability to adapt dynamically to various tasks while\nresponding to feedback. In this paper, we investigate the effectiveness of\nChatGPT as a top-n conversational recommendation system. We build a rigorous\npipeline around ChatGPT to simulate how a user might realistically probe the\nmodel for recommendations: by first instructing and then reprompting with\nfeedback to refine a set of recommendations. We further explore the effect of\npopularity bias in ChatGPT's recommendations, and compare its performance to\nbaseline models. We find that reprompting ChatGPT with feedback is an effective\nstrategy to improve recommendation relevancy, and that popularity bias can be\nmitigated through prompt engineering.", "published": "2024-01-07 23:17:42", "link": "http://arxiv.org/abs/2401.03605v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "I.2.7; H.3.3"], "primary_category": "cs.IR"}
{"title": "Deep Learning Based Cyberbullying Detection in Bangla Language", "abstract": "The Internet is currently the largest platform for global communication\nincluding expressions of opinions, reviews, contents, images, videos and so\nforth. Moreover, social media has now become a very broad and highly engaging\nplatform due to its immense popularity and swift adoption trend. Increased\nsocial networking, however, also has detrimental impacts on the society leading\nto a range of unwanted phenomena, such as online assault, intimidation, digital\nbullying, criminality and trolling. Hence, cyberbullying has become a pervasive\nand worrying problem that poses considerable psychological and emotional harm\nto the people, particularly amongst the teens and the young adults. In order to\nlessen its negative effects and provide victims with prompt support, a great\ndeal of research to identify cyberbullying instances at various online\nplatforms is emerging. In comparison to other languages, Bangla (also known as\nBengali) has fewer research studies in this domain. This study demonstrates a\ndeep learning strategy for identifying cyberbullying in Bengali, using a\ndataset of 12282 versatile comments from multiple social media sites. In this\nstudy, a two-layer bidirectional long short-term memory (Bi-LSTM) model has\nbeen built to identify cyberbullying, using a variety of optimisers as well as\n5-fold cross validation. To evaluate the functionality and efficacy of the\nproposed system, rigorous assessment and validation procedures have been\nemployed throughout the project. The results of this study reveals that the\nproposed model's accuracy, using momentum-based stochastic gradient descent\n(SGD) optimiser, is 94.46%. It also reflects a higher accuracy of 95.08% and a\nF1 score of 95.23% using Adam optimiser as well as a better accuracy of 94.31%\nin 5-fold cross validation.", "published": "2024-01-07 04:58:59", "link": "http://arxiv.org/abs/2401.06787v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Information Retrieval and Classification of Real-Time Multi-Source\n  Hurricane Evacuation Notices", "abstract": "For an approaching disaster, the tracking of time-sensitive critical\ninformation such as hurricane evacuation notices is challenging in the United\nStates. These notices are issued and distributed rapidly by numerous local\nauthorities that may spread across multiple states. They often undergo frequent\nupdates and are distributed through diverse online portals lacking standard\nformats. In this study, we developed an approach to timely detect and track the\nlocally issued hurricane evacuation notices. The text data were collected\nmainly with a spatially targeted web scraping method. They were manually\nlabeled and then classified using natural language processing techniques with\ndeep learning models. The classification of mandatory evacuation notices\nachieved a high accuracy (recall = 96%). We used Hurricane Ian (2022) to\nillustrate how real-time evacuation notices extracted from local government\nsources could be redistributed with a Web GIS system. Our method applied to\nfuture hurricanes provides live data for situation awareness to higher-level\ngovernment agencies and news media. The archived data helps scholars to study\ngovernment responses toward weather warnings and individual behaviors\ninfluenced by evacuation history. The framework may be applied to other types\nof disasters for rapid and targeted retrieval, classification, redistribution,\nand archiving of real-time government orders and notifications.", "published": "2024-01-07 16:35:30", "link": "http://arxiv.org/abs/2401.06789v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "An Investigation of Large Language Models for Real-World Hate Speech\n  Detection", "abstract": "Hate speech has emerged as a major problem plaguing our social spaces today.\nWhile there have been significant efforts to address this problem, existing\nmethods are still significantly limited in effectively detecting hate speech\nonline. A major limitation of existing methods is that hate speech detection is\na highly contextual problem, and these methods cannot fully capture the context\nof hate speech to make accurate predictions. Recently, large language models\n(LLMs) have demonstrated state-of-the-art performance in several natural\nlanguage tasks. LLMs have undergone extensive training using vast amounts of\nnatural language data, enabling them to grasp intricate contextual details.\nHence, they could be used as knowledge bases for context-aware hate speech\ndetection. However, a fundamental problem with using LLMs to detect hate speech\nis that there are no studies on effectively prompting LLMs for context-aware\nhate speech detection. In this study, we conduct a large-scale study of hate\nspeech detection, employing five established hate speech datasets. We discover\nthat LLMs not only match but often surpass the performance of current benchmark\nmachine learning models in identifying hate speech. By proposing four diverse\nprompting strategies that optimize the use of LLMs in detecting hate speech.\nOur study reveals that a meticulously crafted reasoning prompt can effectively\ncapture the context of hate speech by fully utilizing the knowledge base in\nLLMs, significantly outperforming existing techniques. Furthermore, although\nLLMs can provide a rich knowledge base for the contextual detection of hate\nspeech, suitable prompting strategies play a crucial role in effectively\nleveraging this knowledge base for efficient detection.", "published": "2024-01-07 00:39:33", "link": "http://arxiv.org/abs/2401.03346v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CY"}
{"title": "EAT: Self-Supervised Pre-Training with Efficient Audio Transformer", "abstract": "Audio self-supervised learning (SSL) pre-training, which aims to learn good\nrepresentations from unlabeled audio, has made remarkable progress. However,\nthe extensive computational demands during pre-training pose a significant\nbarrier to the potential application and optimization of audio SSL models. In\nthis paper, inspired by the success of data2vec 2.0 in image modality and\nAudio-MAE in audio modality, we introduce Efficient Audio Transformer (EAT) to\nfurther improve the effectiveness and efficiency in audio SSL. The proposed EAT\nadopts the bootstrap self-supervised training paradigm to the audio domain. A\nnovel Utterance-Frame Objective (UFO) is designed to enhance the modeling\ncapability of acoustic events. Furthermore, we reveal that the masking strategy\nis critical in audio SSL pre-training, and superior audio representations can\nbe obtained with large inverse block masks. Experiment results demonstrate that\nEAT achieves state-of-the-art (SOTA) performance on a range of audio-related\ntasks, including AudioSet (AS-2M, AS-20K), ESC-50, and SPC-2, along with a\nsignificant pre-training speedup up to ~15x compared to existing audio SSL\nmodels.", "published": "2024-01-07 14:31:27", "link": "http://arxiv.org/abs/2401.03497v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Spatial Reverberation and Dereverberation using an Acoustic\n  Multiple-Input Multiple-Output System", "abstract": "Methods are proposed for modifying the reverberation characteristics of sound\nfields in rooms by employing a loudspeaker with adjustable directivity,\nrealized with a compact spherical loudspeaker array (SLA). These methods are\nbased on minimization and maximization of clarity and direct-to-reverberant\nsound ratio. Significant modification of reverberation is achieved by these\nmethods, as shown in simulation studies. The system under investigation\nincludes a spherical microphone array and an SLA comprising a multiple-input\nmultiple-output system. The robustness of these methods to system\nidentification errors is also investigated. Finally, reverberation and\ndereverberation results are validated by a listening experiment.", "published": "2024-01-07 09:59:11", "link": "http://arxiv.org/abs/2401.03441v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Single-Microphone Speaker Separation and Voice Activity Detection in\n  Noisy and Reverberant Environments", "abstract": "Speech separation involves extracting an individual speaker's voice from a\nmulti-speaker audio signal. The increasing complexity of real-world\nenvironments, where multiple speakers might converse simultaneously,\nunderscores the importance of effective speech separation techniques. This work\npresents a single-microphone speaker separation network with TF attention\naiming at noisy and reverberant environments. We dub this new architecture as\nSeparation TF Attention Network (Sep-TFAnet). In addition, we present a variant\nof the separation network, dubbed $ \\text{Sep-TFAnet}^{\\text{VAD}}$, which\nincorporates a voice activity detector (VAD) into the separation network.\n  The separation module is based on a temporal convolutional network (TCN)\nbackbone inspired by the Conv-Tasnet architecture with multiple modifications.\nRather than a learned encoder and decoder, we use short-time Fourier transform\n(STFT) and inverse short-time Fourier transform (iSTFT) for the analysis and\nsynthesis, respectively. Our system is specially developed for human-robotic\ninteractions and should support online mode. The separation capabilities of $\n\\text{Sep-TFAnet}^{\\text{VAD}}$ and Sep-TFAnet were evaluated and extensively\nanalyzed under several acoustic conditions, demonstrating their advantages over\ncompeting methods. Since separation networks trained on simulated data tend to\nperform poorly on real recordings, we also demonstrate the ability of the\nproposed scheme to better generalize to realistic examples recorded in our\nacoustic lab by a humanoid robot. Project page: https://Sep-TFAnet.github.io", "published": "2024-01-07 10:46:17", "link": "http://arxiv.org/abs/2401.03448v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Modal smoothing for analysis of room reflections measured with spherical\n  microphone and loudspeaker arrays", "abstract": "Spatial analysis of room acoustics is an ongoing research topic. Microphone\narrays have been employed for spatial analyses with an important objective\nbeing the estimation of the direction-of-arrival (DOA) of direct sound and\nearly room reflections using room impulse responses (RIRs). An optimal method\nfor DOA estimation is the multiple signal classification algorithm. When RIRs\nare considered, this method typically fails due to the correlation of room\nreflections, which leads to rank deficiency of the cross-spectrum matrix.\nPreprocessing methods for rank restoration, which may involve averaging over\nfrequency, for example, have been proposed exclusively for spherical arrays.\nHowever, these methods fail in the case of reflections with equal time delays,\nwhich may arise in practice and could be of interest. In this paper, a method\nis proposed for systems that combine a spherical microphone array and a\nspherical loudspeaker array, referred to as multiple-input multiple-output\nsystems. This method, referred to as modal smoothing, exploits the additional\nspatial diversity for rank restoration and succeeds where previous methods\nfail, as demonstrated in a simulation study. Finally, combining modal smoothing\nwith a preprocessing method is proposed in order to increase the number of DOAs\nthat can be estimated using low-order spherical loudspeaker arrays.", "published": "2024-01-07 11:38:02", "link": "http://arxiv.org/abs/2401.03458v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multichannel AV-wav2vec2: A Framework for Learning Multichannel\n  Multi-Modal Speech Representation", "abstract": "Self-supervised speech pre-training methods have developed rapidly in recent\nyears, which show to be very effective for many near-field single-channel\nspeech tasks. However, far-field multichannel speech processing is suffering\nfrom the scarcity of labeled multichannel data and complex ambient noises. The\nefficacy of self-supervised learning for far-field multichannel and multi-modal\nspeech processing has not been well explored. Considering that visual\ninformation helps to improve speech recognition performance in noisy scenes, in\nthis work we propose a multichannel multi-modal speech self-supervised learning\nframework AV-wav2vec2, which utilizes video and multichannel audio data as\ninputs. First, we propose a multi-path structure to process multichannel audio\nstreams and a visual stream in parallel, with intra- and inter-channel\ncontrastive losses as training targets to fully exploit the spatiotemporal\ninformation in multichannel speech data. Second, based on contrastive learning,\nwe use additional single-channel audio data, which is trained jointly to\nimprove the performance of speech representation. Finally, we use a Chinese\nmultichannel multi-modal dataset in real scenarios to validate the\neffectiveness of the proposed method on audio-visual speech recognition (AVSR),\nautomatic speech recognition (ASR), visual speech recognition (VSR) and\naudio-visual speaker diarization (AVSD) tasks.", "published": "2024-01-07 12:27:18", "link": "http://arxiv.org/abs/2401.03468v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Theory and investigation of acoustic multiple-input multiple-output\n  systems based on spherical arrays in a room", "abstract": "Spatial attributes of room acoustics have been widely studied using\nmicrophone and loudspeaker arrays. However, systems that combine both arrays,\nreferred to as multiple-input multiple-output (MIMO) systems, have only been\nstudied to a limited degree in this context. These systems can potentially\nprovide a powerful tool for room acoustics analysis due to the ability to\nsimultaneously control both arrays. This paper offers a theoretical framework\nfor the spatial analysis of enclosed sound fields using a MIMO system\ncomprising spherical loudspeaker and microphone arrays. A system transfer\nfunction is formulated in matrix form for free-field conditions, and its\nproperties are studied using tools from linear algebra. The system is shown to\nhave unit-rank, regardless of the array types, and its singular vectors are\nrelated to the directions of arrival and radiation at the microphone and\nloudspeaker arrays, respectively. The formulation is then generalized to apply\nto rooms, using an image source method. In this case, the rank of the system is\nrelated to the number of significant reflections. The paper ends with\nsimulation studies, which support the developed theory, and with an extensive\nreflection analysis of a room impulse response, using the platform of a MIMO\nsystem.", "published": "2024-01-07 14:15:07", "link": "http://arxiv.org/abs/2401.03493v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Hyperbolic Distance-Based Speech Separation", "abstract": "In this work, we explore the task of hierarchical distance-based speech\nseparation defined on a hyperbolic manifold. Based on the recent advent of\naudio-related tasks performed in non-Euclidean spaces, we propose to make use\nof the Poincar\\'e ball to effectively unveil the inherent hierarchical\nstructure found in complex speaker mixtures. We design two sets of experiments\nin which the distance-based parent sound classes, namely \"near\" and \"far\", can\ncontain up to two or three speakers (i.e., children) each. We show that our\nhyperbolic approach is suitable for unveiling hierarchical structure from the\nproblem definition, resulting in improved child-level separation. We further\nshow that a clear correlation emerges between the notion of hyperbolic\ncertainty (i.e., the distance to the ball's origin) and acoustic semantics such\nas speaker density, inter-source location, and microphone-to-speaker distance.", "published": "2024-01-07 18:26:34", "link": "http://arxiv.org/abs/2401.03567v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MLCA-AVSR: Multi-Layer Cross Attention Fusion based Audio-Visual Speech\n  Recognition", "abstract": "While automatic speech recognition (ASR) systems degrade significantly in\nnoisy environments, audio-visual speech recognition (AVSR) systems aim to\ncomplement the audio stream with noise-invariant visual cues and improve the\nsystem's robustness. However, current studies mainly focus on fusing the\nwell-learned modality features, like the output of modality-specific encoders,\nwithout considering the contextual relationship during the modality feature\nlearning. In this study, we propose a multi-layer cross-attention fusion based\nAVSR (MLCA-AVSR) approach that promotes representation learning of each\nmodality by fusing them at different levels of audio/visual encoders.\nExperimental results on the MISP2022-AVSR Challenge dataset show the efficacy\nof our proposed system, achieving a concatenated minimum permutation character\nerror rate (cpCER) of 30.57% on the Eval set and yielding up to 3.17% relative\nimprovement compared with our previous system which ranked the second place in\nthe challenge. Following the fusion of multiple systems, our proposed approach\nsurpasses the first-place system, establishing a new SOTA cpCER of 29.13% on\nthis dataset.", "published": "2024-01-07 08:59:32", "link": "http://arxiv.org/abs/2401.03424v3", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ICMC-ASR: The ICASSP 2024 In-Car Multi-Channel Automatic Speech\n  Recognition Challenge", "abstract": "To promote speech processing and recognition research in driving scenarios,\nwe build on the success of the Intelligent Cockpit Speech Recognition Challenge\n(ICSRC) held at ISCSLP 2022 and launch the ICASSP 2024 In-Car Multi-Channel\nAutomatic Speech Recognition (ICMC-ASR) Challenge. This challenge collects over\n100 hours of multi-channel speech data recorded inside a new energy vehicle and\n40 hours of noise for data augmentation. Two tracks, including automatic speech\nrecognition (ASR) and automatic speech diarization and recognition (ASDR) are\nset up, using character error rate (CER) and concatenated minimum permutation\ncharacter error rate (cpCER) as evaluation metrics, respectively. Overall, the\nICMC-ASR Challenge attracts 98 participating teams and receives 53 valid\nresults in both tracks. In the end, first-place team USTCiflytek achieves a CER\nof 13.16% in the ASR track and a cpCER of 21.48% in the ASDR track, showing an\nabsolute improvement of 13.08% and 51.4% compared to our challenge baseline,\nrespectively.", "published": "2024-01-07 12:51:42", "link": "http://arxiv.org/abs/2401.03473v3", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DiarizationLM: Speaker Diarization Post-Processing with Large Language\n  Models", "abstract": "In this paper, we introduce DiarizationLM, a framework to leverage large\nlanguage models (LLM) to post-process the outputs from a speaker diarization\nsystem. Various goals can be achieved with the proposed framework, such as\nimproving the readability of the diarized transcript, or reducing the word\ndiarization error rate (WDER). In this framework, the outputs of the automatic\nspeech recognition (ASR) and speaker diarization systems are represented as a\ncompact textual format, which is included in the prompt to an optionally\nfinetuned LLM. The outputs of the LLM can be used as the refined diarization\nresults with the desired enhancement. As a post-processing step, this framework\ncan be easily applied to any off-the-shelf ASR and speaker diarization systems\nwithout retraining existing components. Our experiments show that a finetuned\nPaLM 2-S model can reduce the WDER by rel. 55.5% on the Fisher telephone\nconversation dataset, and rel. 44.9% on the Callhome English dataset.", "published": "2024-01-07 14:54:57", "link": "http://arxiv.org/abs/2401.03506v11", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The NPU-ASLP-LiAuto System Description for Visual Speech Recognition in\n  CNVSRC 2023", "abstract": "This paper delineates the visual speech recognition (VSR) system introduced\nby the NPU-ASLP-LiAuto (Team 237) in the first Chinese Continuous Visual Speech\nRecognition Challenge (CNVSRC) 2023, engaging in the fixed and open tracks of\nSingle-Speaker VSR Task, and the open track of Multi-Speaker VSR Task. In terms\nof data processing, we leverage the lip motion extractor from the baseline1 to\nproduce multi-scale video data. Besides, various augmentation techniques are\napplied during training, encompassing speed perturbation, random rotation,\nhorizontal flipping, and color transformation. The VSR model adopts an\nend-to-end architecture with joint CTC/attention loss, comprising a ResNet3D\nvisual frontend, an E-Branchformer encoder, and a Transformer decoder.\nExperiments show that our system achieves 34.76% CER for the Single-Speaker\nTask and 41.06% CER for the Multi-Speaker Task after multi-system fusion,\nranking first place in all three tracks we participate.", "published": "2024-01-07 14:20:52", "link": "http://arxiv.org/abs/2401.06788v2", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Freetalker: Controllable Speech and Text-Driven Gesture Generation Based\n  on Diffusion Models for Enhanced Speaker Naturalness", "abstract": "Current talking avatars mostly generate co-speech gestures based on audio and\ntext of the utterance, without considering the non-speaking motion of the\nspeaker. Furthermore, previous works on co-speech gesture generation have\ndesigned network structures based on individual gesture datasets, which results\nin limited data volume, compromised generalizability, and restricted speaker\nmovements. To tackle these issues, we introduce FreeTalker, which, to the best\nof our knowledge, is the first framework for the generation of both spontaneous\n(e.g., co-speech gesture) and non-spontaneous (e.g., moving around the podium)\nspeaker motions. Specifically, we train a diffusion-based model for speaker\nmotion generation that employs unified representations of both speech-driven\ngestures and text-driven motions, utilizing heterogeneous data sourced from\nvarious motion datasets. During inference, we utilize classifier-free guidance\nto highly control the style in the clips. Additionally, to create smooth\ntransitions between clips, we utilize DoubleTake, a method that leverages a\ngenerative prior and ensures seamless motion blending. Extensive experiments\nshow that our method generates natural and controllable speaker movements. Our\ncode, model, and demo are are available at\n\\url{https://youngseng.github.io/FreeTalker/}.", "published": "2024-01-07 13:01:29", "link": "http://arxiv.org/abs/2401.03476v1", "categories": ["cs.MM", "cs.AI", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
