{"title": "Combining Spans into Entities: A Neural Two-Stage Approach for\n  Recognizing Discontiguous Entities", "abstract": "In medical documents, it is possible that an entity of interest not only\ncontains a discontiguous sequence of words but also overlaps with another\nentity. Entities of such structures are intrinsically hard to recognize due to\nthe large space of possible entity combinations. In this work, we propose a\nneural two-stage approach to recognize discontiguous and overlapping entities\nby decomposing this problem into two subtasks: 1) it first detects all the\noverlapping spans that either form entities on their own or present as segments\nof discontiguous entities, based on the representation of segmental hypergraph,\n2) next it learns to combine these segments into discontiguous entities with a\nclassifier, which filters out other incorrect combinations of segments. Two\nneural components are designed for these subtasks respectively and they are\nlearned jointly using a shared encoder for text. Our model achieves the\nstate-of-the-art performance in a standard dataset, even in the absence of\nexternal features that previous methods used.", "published": "2019-09-03 02:59:21", "link": "http://arxiv.org/abs/1909.00930v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Can you say more about the location?\" The Development of a Pedagogical\n  Reference Resolution Agent", "abstract": "In an increasingly globalized world, geographic literacy is crucial. In this\npaper, we present a collaborative two-player game to improve people's ability\nto locate countries on the world map. We discuss two implementations of the\ngame: First, we created a web-based version which can be played with the\nremote-controlled agent Nellie. With the knowledge we gained from a large\nonline data collection, we re-implemented the game so it can be played\nface-to-face with the Furhat robot Neil. Our analysis shows that participants\nfound the game not just engaging to play, they also believe they gained lasting\nknowledge about the world map.", "published": "2019-09-03 04:16:28", "link": "http://arxiv.org/abs/1909.00945v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unicoder: A Universal Language Encoder by Pre-training with Multiple\n  Cross-lingual Tasks", "abstract": "We present Unicoder, a universal language encoder that is insensitive to\ndifferent languages. Given an arbitrary NLP task, a model can be trained with\nUnicoder using training data in one language and directly applied to inputs of\nthe same task in other languages. Comparing to similar efforts such as\nMultilingual BERT and XLM, three new cross-lingual pre-training tasks are\nproposed, including cross-lingual word recovery, cross-lingual paraphrase\nclassification and cross-lingual masked language model. These tasks help\nUnicoder learn the mappings among different languages from more perspectives.\nWe also find that doing fine-tuning on multiple languages together can bring\nfurther improvement. Experiments are performed on two tasks: cross-lingual\nnatural language inference (XNLI) and cross-lingual question answering (XQA),\nwhere XLM is our baseline. On XNLI, 1.8% averaged accuracy improvement (on 15\nlanguages) is obtained. On XQA, which is a new cross-lingual dataset built by\nus, 5.5% averaged accuracy improvement (on French and German) is obtained.", "published": "2019-09-03 06:11:44", "link": "http://arxiv.org/abs/1909.00964v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Argument Quality Assessment -- New Datasets and Methods", "abstract": "We explore the task of automatic assessment of argument quality. To that end,\nwe actively collected 6.3k arguments, more than a factor of five compared to\npreviously examined data. Each argument was explicitly and carefully annotated\nfor its quality. In addition, 14k pairs of arguments were annotated\nindependently, identifying the higher quality argument in each pair. In spite\nof the inherent subjective nature of the task, both annotation schemes led to\nsurprisingly consistent results. We release the labeled datasets to the\ncommunity. Furthermore, we suggest neural methods based on a recently released\nlanguage model, for argument ranking as well as for argument-pair\nclassification. In the former task, our results are comparable to\nstate-of-the-art; in the latter task our results significantly outperform\nearlier methods.", "published": "2019-09-03 09:00:44", "link": "http://arxiv.org/abs/1909.01007v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Duality Regularization for Unsupervised Bilingual Lexicon Induction", "abstract": "Unsupervised bilingual lexicon induction naturally exhibits duality, which\nresults from symmetry in back-translation. For example, EN-IT and IT-EN\ninduction can be mutually primal and dual problems. Current state-of-the-art\nmethods, however, consider the two tasks independently. In this paper, we\npropose to train primal and dual models jointly, using regularizers to\nencourage consistency in back translation cycles. Experiments across 6 language\npairs show that the proposed method significantly outperforms competitive\nbaselines, obtaining the best-published results on a standard benchmark.", "published": "2019-09-03 09:22:11", "link": "http://arxiv.org/abs/1909.01013v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attention-based Pairwise Multi-Perspective Convolutional Neural Network\n  for Answer Selection in Question Answering", "abstract": "Over the past few years, question answering and information retrieval systems\nhave become widely used. These systems attempt to find the answer of the asked\nquestions from raw text sources. A component of these systems is Answer\nSelection which selects the most relevant from candidate answers. Syntactic\nsimilarities were mostly used to compute the similarity, but in recent works,\ndeep neural networks have been used, making a significant improvement in this\nfield. In this research, a model is proposed to select the most relevant\nanswers to the factoid question from the candidate answers. The proposed model\nranks the candidate answers in terms of semantic and syntactic similarity to\nthe question, using convolutional neural networks. In this research, Attention\nmechanism and Sparse feature vector use the context-sensitive interactions\nbetween questions and answer sentence. Wide convolution increases the\nimportance of the interrogative word. Pairwise ranking is used to learn\ndifferentiable representations to distinguish positive and negative answers.\nOur model indicates strong performance on the TrecQA Raw beating previous\nstate-of-the-art systems by 1.4% in MAP and 1.1% in MRR while using the\nbenefits of no additional syntactic parsers and external tools. The results\nshow that using context-sensitive interactions between question and answer\nsentences can help to find the correct answer more accurately.", "published": "2019-09-03 10:58:01", "link": "http://arxiv.org/abs/1909.01059v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Named Entity Embedding Distribution into Hypersphere", "abstract": "This work models named entity distribution from a way of visualizing\ntopological structure of embedding space, so that we make an assumption that\nmost, if not all, named entities (NEs) for a language tend to aggregate\ntogether to be accommodated by a specific hypersphere in embedding space. Thus\nwe present a novel open definition for NE which alleviates the obvious drawback\nin previous closed NE definition with a limited NE dictionary. Then, we show\ntwo applications with introducing the proposed named entity hypersphere model.\nFirst, using a generative adversarial neural network to learn a transformation\nmatrix of two embedding spaces, which results in a convenient determination of\nnamed entity distribution in the target language, indicating the potential of\nfast named entity discovery only using isomorphic relation between embedding\nspaces. Second, the named entity hypersphere model is directly integrated with\nvarious named entity recognition models over sentences to achieve\nstate-of-the-art results. Only assuming that embeddings are available, we show\na prior knowledge free approach on effective named entity distribution\ndepiction.", "published": "2019-09-03 11:09:12", "link": "http://arxiv.org/abs/1909.01065v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Models as Knowledge Bases?", "abstract": "Recent progress in pretraining language models on large textual corpora led\nto a surge of improvements for downstream NLP tasks. Whilst learning linguistic\nknowledge, these models may also be storing relational knowledge present in the\ntraining data, and may be able to answer queries structured as\n\"fill-in-the-blank\" cloze statements. Language models have many advantages over\nstructured knowledge bases: they require no schema engineering, allow\npractitioners to query about an open class of relations, are easy to extend to\nmore data, and require no human supervision to train. We present an in-depth\nanalysis of the relational knowledge already present (without fine-tuning) in a\nwide range of state-of-the-art pretrained language models. We find that (i)\nwithout fine-tuning, BERT contains relational knowledge competitive with\ntraditional NLP methods that have some access to oracle knowledge, (ii) BERT\nalso does remarkably well on open-domain question answering against a\nsupervised baseline, and (iii) certain types of factual knowledge are learned\nmuch more readily than others by standard language model pretraining\napproaches. The surprisingly strong ability of these models to recall factual\nknowledge without any fine-tuning demonstrates their potential as unsupervised\nopen-domain QA systems. The code to reproduce our analysis is available at\nhttps://github.com/facebookresearch/LAMA.", "published": "2019-09-03 11:11:08", "link": "http://arxiv.org/abs/1909.01066v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-agent Learning for Neural Machine Translation", "abstract": "Conventional Neural Machine Translation (NMT) models benefit from the\ntraining with an additional agent, e.g., dual learning, and bidirectional\ndecoding with one agent decoding from left to right and the other decoding in\nthe opposite direction. In this paper, we extend the training framework to the\nmulti-agent scenario by introducing diverse agents in an interactive updating\nprocess. At training time, each agent learns advanced knowledge from others,\nand they work together to improve translation quality. Experimental results on\nNIST Chinese-English, IWSLT 2014 German-English, WMT 2014 English-German and\nlarge-scale Chinese-English translation tasks indicate that our approach\nachieves absolute improvements over the strong baseline systems and shows\ncompetitive performance on all tasks.", "published": "2019-09-03 11:55:59", "link": "http://arxiv.org/abs/1909.01101v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Encode, Tag, Realize: High-Precision Text Editing", "abstract": "We propose LaserTagger - a sequence tagging approach that casts text\ngeneration as a text editing task. Target texts are reconstructed from the\ninputs using three main edit operations: keeping a token, deleting it, and\nadding a phrase before the token. To predict the edit operations, we propose a\nnovel model, which combines a BERT encoder with an autoregressive Transformer\ndecoder. This approach is evaluated on English text on four tasks: sentence\nfusion, sentence splitting, abstractive summarization, and grammar correction.\nLaserTagger achieves new state-of-the-art results on three of these tasks,\nperforms comparably to a set of strong seq2seq baselines with a large number of\ntraining examples, and outperforms them when the number of examples is limited.\nFurthermore, we show that at inference time tagging can be more than two orders\nof magnitude faster than comparable seq2seq models, making it more attractive\nfor running in a live environment.", "published": "2019-09-03 13:54:52", "link": "http://arxiv.org/abs/1909.01187v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Better Rewards Yield Better Summaries: Learning to Summarise Without\n  References", "abstract": "Reinforcement Learning (RL) based document summarisation systems yield\nstate-of-the-art performance in terms of ROUGE scores, because they directly\nuse ROUGE as the rewards during training. However, summaries with high ROUGE\nscores often receive low human judgement. To find a better reward function that\ncan guide RL to generate human-appealing summaries, we learn a reward function\nfrom human ratings on 2,500 summaries. Our reward function only takes the\ndocument and system summary as input. Hence, once trained, it can be used to\ntrain RL-based summarisation systems without using any reference summaries. We\nshow that our learned rewards have significantly higher correlation with human\nratings than previous approaches. Human evaluation experiments show that,\ncompared to the state-of-the-art supervised-learning systems and\nROUGE-as-rewards RL summarisation systems, the RL systems using our learned\nrewards during training generate summarieswith higher human ratings. The\nlearned reward function and our source code are available at\nhttps://github.com/yg211/summary-reward-no-reference.", "published": "2019-09-03 14:30:13", "link": "http://arxiv.org/abs/1909.01214v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Introducing RONEC -- the Romanian Named Entity Corpus", "abstract": "We present RONEC - the Named Entity Corpus for the Romanian language. The\ncorpus contains over 26000 entities in ~5000 annotated sentences, belonging to\n16 distinct classes. The sentences have been extracted from a copy-right free\nnewspaper, covering several styles. This corpus represents the first initiative\nin the Romanian language space specifically targeted for named entity\nrecognition. It is available in BRAT and CoNLL-U Plus formats, and it is free\nto use and extend at github.com/dumitrescustefan/ronec .", "published": "2019-09-03 15:20:44", "link": "http://arxiv.org/abs/1909.01247v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aspect Detection using Word and Char Embeddings with (Bi)LSTM and CRF", "abstract": "We proposed a~new accurate aspect extraction method that makes use of both\nword and character-based embeddings. We have conducted experiments of various\nmodels of aspect extraction using LSTM and BiLSTM including CRF enhancement on\nfive different pre-trained word embeddings extended with character embeddings.\nThe results revealed that BiLSTM outperforms regular LSTM, but also word\nembedding coverage in train and test sets profoundly impacted aspect detection\nperformance. Moreover, the additional CRF layer consistently improves the\nresults across different models and text embeddings. Summing up, we obtained\nstate-of-the-art F-score results for SemEval Restaurants (85%) and Laptops\n(80%).", "published": "2019-09-03 16:16:00", "link": "http://arxiv.org/abs/1909.01276v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PolyResponse: A Rank-based Approach to Task-Oriented Dialogue with\n  Application in Restaurant Search and Booking", "abstract": "We present PolyResponse, a conversational search engine that supports\ntask-oriented dialogue. It is a retrieval-based approach that bypasses the\ncomplex multi-component design of traditional task-oriented dialogue systems\nand the use of explicit semantics in the form of task-specific ontologies. The\nPolyResponse engine is trained on hundreds of millions of examples extracted\nfrom real conversations: it learns what responses are appropriate in different\nconversational contexts. It then ranks a large index of text and visual\nresponses according to their similarity to the given context, and narrows down\nthe list of relevant entities during the multi-turn conversation. We introduce\na restaurant search and booking system powered by the PolyResponse engine,\ncurrently available in 8 different languages.", "published": "2019-09-03 16:40:24", "link": "http://arxiv.org/abs/1909.01296v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Bottom-up Evolution of Representations in the Transformer: A Study\n  with Machine Translation and Language Modeling Objectives", "abstract": "We seek to understand how the representations of individual tokens and the\nstructure of the learned feature space evolve between layers in deep neural\nnetworks under different learning objectives. We focus on the Transformers for\nour analysis as they have been shown effective on various tasks, including\nmachine translation (MT), standard left-to-right language models (LM) and\nmasked language modeling (MLM). Previous work used black-box probing tasks to\nshow that the representations learned by the Transformer differ significantly\ndepending on the objective. In this work, we use canonical correlation analysis\nand mutual information estimators to study how information flows across\nTransformer layers and how this process depends on the choice of learning\nobjective. For example, as you go from bottom to top layers, information about\nthe past in left-to-right language models gets vanished and predictions about\nthe future get formed. In contrast, for MLM, representations initially acquire\ninformation about the context around the token, partially forgetting the token\nidentity and producing a more generalized token representation. The token\nidentity then gets recreated at the top MLM layers.", "published": "2019-09-03 18:06:03", "link": "http://arxiv.org/abs/1909.01380v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-Aware Monolingual Repair for Neural Machine Translation", "abstract": "Modern sentence-level NMT systems often produce plausible translations of\nisolated sentences. However, when put in context, these translations may end up\nbeing inconsistent with each other. We propose a monolingual DocRepair model to\ncorrect inconsistencies between sentence-level translations. DocRepair performs\nautomatic post-editing on a sequence of sentence-level translations, refining\ntranslations of sentences in context of each other. For training, the DocRepair\nmodel requires only monolingual document-level data in the target language. It\nis trained as a monolingual sequence-to-sequence model that maps inconsistent\ngroups of sentences into consistent ones. The consistent groups come from the\noriginal training data; the inconsistent groups are obtained by sampling\nround-trip translations for each isolated sentence. We show that this approach\nsuccessfully imitates inconsistencies we aim to fix: using contrastive\nevaluation, we show large improvements in the translation of several contextual\nphenomena in an English-Russian translation task, as well as improvements in\nthe BLEU score. We also conduct a human evaluation and show a strong preference\nof the annotators to corrected translations over the baseline ones. Moreover,\nwe analyze which discourse phenomena are hard to capture using monolingual data\nonly.", "published": "2019-09-03 18:12:36", "link": "http://arxiv.org/abs/1909.01383v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CrossWeigh: Training Named Entity Tagger from Imperfect Annotations", "abstract": "Everyone makes mistakes. So do human annotators when curating labels for\nnamed entity recognition (NER). Such label mistakes might hurt model training\nand interfere model comparison. In this study, we dive deep into one of the\nwidely-adopted NER benchmark datasets, CoNLL03 NER. We are able to identify\nlabel mistakes in about 5.38% test sentences, which is a significant ratio\nconsidering that the state-of-the-art test F1 score is already around 93%.\nTherefore, we manually correct these label mistakes and form a cleaner test\nset. Our re-evaluation of popular models on this corrected test set leads to\nmore accurate assessments, compared to those on the original test set. More\nimportantly, we propose a simple yet effective framework, CrossWeigh, to handle\nlabel mistakes during NER model training. Specifically, it partitions the\ntraining data into several folds and train independent NER models to identify\npotential mistakes in each fold. Then it adjusts the weights of training data\naccordingly to train the final NER model. Extensive experiments demonstrate\nsignificant improvements of plugging various NER models into our proposed\nframework on three datasets. All implementations and corrected test set are\navailable at our Github repo: https://github.com/ZihanWangKi/CrossWeigh.", "published": "2019-09-03 20:34:34", "link": "http://arxiv.org/abs/1909.01441v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting Specificity in Classroom Discussion", "abstract": "High quality classroom discussion is important to student development,\nenhancing abilities to express claims, reason about other students' claims, and\nretain information for longer periods of time. Previous small-scale studies\nhave shown that one indicator of classroom discussion quality is specificity.\nIn this paper we tackle the problem of predicting specificity for classroom\ndiscussions. We propose several methods and feature sets capable of\noutperforming the state of the art in specificity prediction. Additionally, we\nprovide a set of meaningful, interpretable features that can be used to analyze\nclassroom discussions at a pedagogical level.", "published": "2019-09-03 21:24:18", "link": "http://arxiv.org/abs/1909.01462v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Target Language-Aware Constrained Inference for Cross-lingual Dependency\n  Parsing", "abstract": "Prior work on cross-lingual dependency parsing often focuses on capturing the\ncommonalities between source and target languages and overlooks the potential\nof leveraging linguistic properties of the languages to facilitate the\ntransfer. In this paper, we show that weak supervisions of linguistic knowledge\nfor the target languages can improve a cross-lingual graph-based dependency\nparser substantially. Specifically, we explore several types of corpus\nlinguistic statistics and compile them into corpus-wise constraints to guide\nthe inference process during the test time. We adapt two techniques, Lagrangian\nrelaxation and posterior regularization, to conduct inference with\ncorpus-statistics constraints. Experiments show that the Lagrangian relaxation\nand posterior regularization inference improve the performances on 15 and 17\nout of 19 target languages, respectively. The improvements are especially\nsignificant for target languages that have different word order features from\nthe source language.", "published": "2019-09-03 22:34:42", "link": "http://arxiv.org/abs/1909.01482v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying Editor Roles in Argumentative Writing from Student Revision\n  Histories", "abstract": "We present a method for identifying editor roles from students' revision\nbehaviors during argumentative writing. We first develop a method for applying\na topic modeling algorithm to identify a set of editor roles from a vocabulary\ncapturing three aspects of student revision behaviors: operation, purpose, and\nposition. We validate the identified roles by showing that modeling the editor\nroles that students take when revising a paper not only accounts for the\nvariance in revision purposes in our data, but also relates to writing\nimprovement.", "published": "2019-09-03 20:47:32", "link": "http://arxiv.org/abs/1909.05308v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Annotation and Classification of Sentence-level Revision Improvement", "abstract": "Studies of writing revisions rarely focus on revision quality. To address\nthis issue, we introduce a corpus of between-draft revisions of student\nargumentative essays, annotated as to whether each revision improves essay\nquality. We demonstrate a potential usage of our annotations by developing a\nmachine learning model to predict revision improvement. With the goal of\nexpanding training data, we also extract revisions from a dataset edited by\nexpert proofreaders. Our results indicate that blending expert and non-expert\nrevisions increases model performance, with expert data particularly important\nfor predicting low-quality revisions.", "published": "2019-09-03 21:02:16", "link": "http://arxiv.org/abs/1909.05309v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attributed Rhetorical Structure Grammar for Domain Text Summarization", "abstract": "This paper presents a new approach of automatic text summarization which\ncombines domain oriented text analysis (DoTA) and rhetorical structure theory\n(RST) in a grammar form: the attributed rhetorical structure grammar (ARSG),\nwhere the non-terminal symbols are domain keywords, called domain relations,\nwhile the rhetorical relations serve as attributes. We developed machine\nlearning algorithms for learning such a grammar from a corpus of sample domain\ntexts, as well as parsing algorithms for the learned grammar, together with\nadjustable text summarization algorithms for generating domain specific\nsummaries. Our practical experiments have shown that with support of domain\nknowledge the drawback of missing very large training data set can be\neffectively compensated. We have also shown that the knowledge based approach\nmay be made more powerful by introducing grammar parsing and RST as inference\nengine. For checking the feasibility of model transfer, we introduced a\ntechnique for mapping a grammar from one domain to others with acceptable cost.\nWe have also made a comprehensive comparison of our approach with some others.", "published": "2019-09-03 02:31:47", "link": "http://arxiv.org/abs/1909.00923v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adversarial Bootstrapping for Dialogue Model Training", "abstract": "Open domain neural dialogue models, despite their successes, are known to\nproduce responses that lack relevance, diversity, and in many cases coherence.\nThese shortcomings stem from the limited ability of common training objectives\nto directly express these properties as well as their interplay with training\ndatasets and model architectures. Toward addressing these problems, this paper\nproposes bootstrapping a dialogue response generator with an adversarially\ntrained discriminator. The method involves training a neural generator in both\nautoregressive and traditional teacher-forcing modes, with the maximum\nlikelihood loss of the auto-regressive outputs weighted by the score from a\nmetric-based discriminator model. The discriminator input is a mixture of\nground truth labels, the teacher-forcing outputs of the generator, and\ndistractors sampled from the dataset, thereby allowing for richer feedback on\nthe autoregressive outputs of the generator. To improve the calibration of the\ndiscriminator output, we also bootstrap the discriminator with the matching of\nthe intermediate features of the ground truth and the generator's\nautoregressive output. We explore different sampling and adversarial policy\noptimization strategies during training in order to understand how to encourage\nresponse diversity without sacrificing relevance. Our experiments shows that\nadversarial bootstrapping is effective at addressing exposure bias, leading to\nimprovement in response relevance and coherence. The improvement is\ndemonstrated with the state-of-the-art results on the Movie and Ubuntu dialogue\ndatasets with respect to human evaluations and BLUE, ROGUE, and distinct n-gram\nscores.", "published": "2019-09-03 02:45:28", "link": "http://arxiv.org/abs/1909.00925v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transfer Fine-Tuning: A BERT Case Study", "abstract": "A semantic equivalence assessment is defined as a task that assesses semantic\nequivalence in a sentence pair by binary judgment (i.e., paraphrase\nidentification) or grading (i.e., semantic textual similarity measurement). It\nconstitutes a set of tasks crucial for research on natural language\nunderstanding. Recently, BERT realized a breakthrough in sentence\nrepresentation learning (Devlin et al., 2019), which is broadly transferable to\nvarious NLP tasks. While BERT's performance improves by increasing its model\nsize, the required computational power is an obstacle preventing practical\napplications from adopting the technology. Herein, we propose to inject phrasal\nparaphrase relations into BERT in order to generate suitable representations\nfor semantic equivalence assessment instead of increasing the model size.\nExperiments on standard natural language understanding tasks confirm that our\nmethod effectively improves a smaller BERT model while maintaining the model\nsize. The generated model exhibits superior performance compared to a larger\nBERT model on semantic equivalence assessment tasks. Furthermore, it achieves\nlarger performance gains on tasks with limited training datasets for\nfine-tuning, which is a property desirable for transfer learning.", "published": "2019-09-03 03:06:46", "link": "http://arxiv.org/abs/1909.00931v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Certified Robustness to Adversarial Word Substitutions", "abstract": "State-of-the-art NLP models can often be fooled by adversaries that apply\nseemingly innocuous label-preserving transformations (e.g., paraphrasing) to\ninput text. The number of possible transformations scales exponentially with\ntext length, so data augmentation cannot cover all transformations of an input.\nThis paper considers one exponentially large family of label-preserving\ntransformations, in which every word in the input can be replaced with a\nsimilar word. We train the first models that are provably robust to all word\nsubstitutions in this family. Our training procedure uses Interval Bound\nPropagation (IBP) to minimize an upper bound on the worst-case loss that any\ncombination of word substitutions can induce. To evaluate models' robustness to\nthese transformations, we measure accuracy on adversarially chosen word\nsubstitutions applied to test examples. Our IBP-trained models attain $75\\%$\nadversarial accuracy on both sentiment analysis on IMDB and natural language\ninference on SNLI. In comparison, on IMDB, models trained normally and ones\ntrained with data augmentation achieve adversarial accuracy of only $8\\%$ and\n$35\\%$, respectively.", "published": "2019-09-03 07:29:48", "link": "http://arxiv.org/abs/1909.00986v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Making a Dependency Parser See", "abstract": "We explore whether it is possible to leverage eye-tracking data in an RNN\ndependency parser (for English) when such information is only available during\ntraining, i.e., no aggregated or token-level gaze features are used at\ninference time. To do so, we train a multitask learning model that parses\nsentences as sequence labeling and leverages gaze features as auxiliary tasks.\nOur method also learns to train from disjoint datasets, i.e. it can be used to\ntest whether already collected gaze features are useful to improve the\nperformance on new non-gazed annotated treebanks. Accuracy gains are modest but\npositive, showing the feasibility of the approach. It can serve as a first step\ntowards architectures that can better leverage eye-tracking data or other\ncomplementary information available only for training sentences, possibly\nleading to improvements in syntactic parsing.", "published": "2019-09-03 10:42:35", "link": "http://arxiv.org/abs/1909.01053v1", "categories": ["cs.CL", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Smart Sliding Chinese Pinyin Input Method Editor on Touchscreen", "abstract": "This paper presents a smart sliding Chinese pinyin Input Method Editor (IME)\nfor touchscreen devices which allows user finger sliding from one key to\nanother on the touchscreen instead of tapping keys one by one, while the target\nChinese character sequence will be predicted during the sliding process to help\nuser input Chinese characters efficiently. Moreover, the layout of the virtual\nkeyboard of our IME adapts to user sliding for more efficient inputting. The\nlayout adaption process is utilized with Recurrent Neural Networks (RNN) and\ndeep reinforcement learning. The pinyin-to-character converter is implemented\nwith a sequence-to-sequence (Seq2Seq) model to predict the target Chinese\nsequence. A sliding simulator is built to automatically produce sliding samples\nfor model training and virtual keyboard test. The key advantage of our proposed\nIME is that nearly all its built-in tactics can be optimized automatically with\ndeep learning algorithms only following user behavior. Empirical studies verify\nthe effectiveness of the proposed model and show a better user input\nefficiency.", "published": "2019-09-03 11:04:19", "link": "http://arxiv.org/abs/1909.01063v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Neural Attentive Bag-of-Entities Model for Text Classification", "abstract": "This study proposes a Neural Attentive Bag-of-Entities model, which is a\nneural network model that performs text classification using entities in a\nknowledge base. Entities provide unambiguous and relevant semantic signals that\nare beneficial for capturing semantics in texts. We combine simple high-recall\nentity detection based on a dictionary, to detect entities in a document, with\na novel neural attention mechanism that enables the model to focus on a small\nnumber of unambiguous and relevant entities. We tested the effectiveness of our\nmodel using two standard text classification datasets (i.e., the 20 Newsgroups\nand R8 datasets) and a popular factoid question answering dataset based on a\ntrivia quiz game. As a result, our model achieved state-of-the-art results on\nall datasets. The source code of the proposed model is available online at\nhttps://github.com/wikipedia2vec/wikipedia2vec.", "published": "2019-09-03 15:50:34", "link": "http://arxiv.org/abs/1909.01259v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CMU GetGoing: An Understandable and Memorable Dialog System for Seniors", "abstract": "Voice-based technologies are typically developed for the average user, and\nthus generally not tailored to the specific needs of any subgroup of the\npopulation, like seniors. This paper presents CMU GetGoing, an accessible trip\nplanning dialog system designed for senior users. The GetGoing system design is\ndescribed in detail, with particular attention to the senior-tailored features.\nA user study is presented, demonstrating that the senior-tailored features\nsignificantly improve comprehension and retention of information.", "published": "2019-09-03 17:35:27", "link": "http://arxiv.org/abs/1909.01322v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "The Woman Worked as a Babysitter: On Biases in Language Generation", "abstract": "We present a systematic study of biases in natural language generation (NLG)\nby analyzing text generated from prompts that contain mentions of different\ndemographic groups. In this work, we introduce the notion of the regard towards\na demographic, use the varying levels of regard towards different demographics\nas a defining metric for bias in NLG, and analyze the extent to which sentiment\nscores are a relevant proxy metric for regard. To this end, we collect\nstrategically-generated text from language models and manually annotate the\ntext with both sentiment and regard scores. Additionally, we build an automatic\nregard classifier through transfer learning, so that we can analyze biases in\nunseen text. Together, these methods reveal the extent of the biased nature of\nlanguage model generations. Our analysis provides a study of biases in NLG,\nbias metrics and correlated human judgments, and empirical evidence on the\nusefulness of our annotated dataset.", "published": "2019-09-03 17:50:44", "link": "http://arxiv.org/abs/1909.01326v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How to Build User Simulators to Train RL-based Dialog Systems", "abstract": "User simulators are essential for training reinforcement learning (RL) based\ndialog models. The performance of the simulator directly impacts the RL policy.\nHowever, building a good user simulator that models real user behaviors is\nchallenging. We propose a method of standardizing user simulator building that\ncan be used by the community to compare dialog system quality using the same\nset of user simulators fairly. We present implementations of six user\nsimulators trained with different dialog planning and generation methods. We\nthen calculate a set of automatic metrics to evaluate the quality of these\nsimulators both directly and indirectly. We also ask human users to assess the\nsimulators directly and indirectly by rating the simulated dialogs and\ninteracting with the trained systems. This paper presents a comprehensive\nevaluation framework for user simulator study and provides a better\nunderstanding of the pros and cons of different user simulators, as well as\ntheir impacts on the trained systems.", "published": "2019-09-03 18:22:24", "link": "http://arxiv.org/abs/1909.01388v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Effective Strategies for Using Hashtags in Online Communication", "abstract": "The features of use of hashtags among students of Lviv were investigated. The\nlist of optimal strategies for using these communicative tools for personal\nbranding is determined. The effective strategy for using hashtags in online\ncommunication for the personal and company branding is considered. The results\nof calculation of effectiveness of hashtags related to #education is\ncalculated. The reports of using hashtag #education in social networks is\npresented.", "published": "2019-09-03 22:20:24", "link": "http://arxiv.org/abs/1909.01474v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "From Textual Information Sources to Linked Data in the Agatha Project", "abstract": "Automatic reasoning about textual information is a challenging task in modern\nNatural Language Processing (NLP) systems. In this work we describe our\nproposal for representing and reasoning about Portuguese documents by means of\nLinked Data like ontologies and thesauri. Our approach resorts to a specialized\npipeline of natural language processing (part-of-speech tagger, named entity\nrecognition, semantic role labeling) to populate an ontology for the domain of\ncriminal investigations. The provided architecture and ontology are language\nindependent. Although some of the NLP modules are language dependent, they can\nbe built using adequate AI methodologies.", "published": "2019-09-03 08:27:37", "link": "http://arxiv.org/abs/1909.05359v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Structuring Latent Spaces for Stylized Response Generation", "abstract": "Generating responses in a targeted style is a useful yet challenging task,\nespecially in the absence of parallel data. With limited data, existing methods\ntend to generate responses that are either less stylized or less\ncontext-relevant. We propose StyleFusion, which bridges conversation modeling\nand non-parallel style transfer by sharing a structured latent space. This\nstructure allows the system to generate stylized relevant responses by sampling\nin the neighborhood of the conversation model prediction, and continuously\ncontrol the style level. We demonstrate this method using dialogues from Reddit\ndata and two sets of sentences with distinct styles (arXiv and Sherlock Holmes\nnovels). Automatic and human evaluation show that, without sacrificing\nappropriateness, the system generates responses of the targeted style and\noutperforms competitive baselines.", "published": "2019-09-03 18:11:58", "link": "http://arxiv.org/abs/1909.05361v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PlotQA: Reasoning over Scientific Plots", "abstract": "Existing synthetic datasets (FigureQA, DVQA) for reasoning over plots do not\ncontain variability in data labels, real-valued data, or complex reasoning\nquestions. Consequently, proposed models for these datasets do not fully\naddress the challenge of reasoning over plots. In particular, they assume that\nthe answer comes either from a small fixed size vocabulary or from a bounding\nbox within the image. However, in practice, this is an unrealistic assumption\nbecause many questions require reasoning and thus have real-valued answers\nwhich appear neither in a small fixed size vocabulary nor in the image. In this\nwork, we aim to bridge this gap between existing datasets and real-world plots.\nSpecifically, we propose PlotQA with 28.9 million question-answer pairs over\n224,377 plots on data from real-world sources and questions based on\ncrowd-sourced question templates. Further, 80.76% of the out-of-vocabulary\n(OOV) questions in PlotQA have answers that are not in a fixed vocabulary.\nAnalysis of existing models on PlotQA reveals that they cannot deal with OOV\nquestions: their overall accuracy on our dataset is in single digits. This is\nnot surprising given that these models were not designed for such questions. As\na step towards a more holistic model which can address fixed vocabulary as well\nas OOV questions, we propose a hybrid approach: Specific questions are answered\nby choosing the answer from a fixed vocabulary or by extracting it from a\npredicted bounding box in the plot, while other questions are answered with a\ntable question-answering engine which is fed with a structured table generated\nby detecting visual elements from the image. On the existing DVQA dataset, our\nmodel has an accuracy of 58%, significantly improving on the highest reported\naccuracy of 46%. On PlotQA, our model has an accuracy of 22.52%, which is\nsignificantly better than state of the art models.", "published": "2019-09-03 08:23:51", "link": "http://arxiv.org/abs/1909.00997v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Local Embeddings for Relational Data Integration", "abstract": "Deep learning based techniques have been recently used with promising results\nfor data integration problems. Some methods directly use pre-trained embeddings\nthat were trained on a large corpus such as Wikipedia. However, they may not\nalways be an appropriate choice for enterprise datasets with custom vocabulary.\nOther methods adapt techniques from natural language processing to obtain\nembeddings for the enterprise's relational data. However, this approach blindly\ntreats a tuple as a sentence, thus losing a large amount of contextual\ninformation present in the tuple.\n  We propose algorithms for obtaining local embeddings that are effective for\ndata integration tasks on relational databases. We make four major\ncontributions. First, we describe a compact graph-based representation that\nallows the specification of a rich set of relationships inherent in the\nrelational world. Second, we propose how to derive sentences from such a graph\nthat effectively \"describe\" the similarity across elements (tokens, attributes,\nrows) in the two datasets. The embeddings are learned based on such sentences.\nThird, we propose effective optimization to improve the quality of the learned\nembeddings and the performance of integration tasks. Finally, we propose a\ndiverse collection of criteria to evaluate relational embeddings and perform an\nextensive set of experiments validating them against multiple baseline methods.\nOur experiments show that our framework, EmbDI, produces meaningful results for\ndata integration tasks such as schema matching and entity resolution both in\nsupervised and unsupervised settings.", "published": "2019-09-03 12:45:02", "link": "http://arxiv.org/abs/1909.01120v2", "categories": ["cs.DB", "cs.CL", "cs.LG"], "primary_category": "cs.DB"}
{"title": "Brain2Char: A Deep Architecture for Decoding Text from Brain Recordings", "abstract": "Decoding language representations directly from the brain can enable new\nBrain-Computer Interfaces (BCI) for high bandwidth human-human and\nhuman-machine communication. Clinically, such technologies can restore\ncommunication in people with neurological conditions affecting their ability to\nspeak. In this study, we propose a novel deep network architecture Brain2Char,\nfor directly decoding text (specifically character sequences) from direct brain\nrecordings (called Electrocorticography, ECoG). Brain2Char framework combines\nstate-of-the-art deep learning modules --- 3D Inception layers for multiband\nspatiotemporal feature extraction from neural data and bidirectional recurrent\nlayers, dilated convolution layers followed by language model weighted beam\nsearch to decode character sequences, optimizing a connectionist temporal\nclassification (CTC) loss. Additionally, given the highly non-linear\ntransformations that underlie the conversion of cortical function to character\nsequences, we perform regularizations on the network's latent representations\nmotivated by insights into cortical encoding of speech production and\nartifactual aspects specific to ECoG data acquisition. To do this, we impose\nauxiliary losses on latent representations for articulatory movements, speech\nacoustics and session specific non-linearities. In 3 participants tested here,\nBrain2Char achieves 10.6\\%, 8.5\\% and 7.0\\% Word Error Rates (WER) respectively\non vocabulary sizes ranging from 1200 to 1900 words. Brain2Char also performs\nwell when 2 participants silently mimed sentences. These results set a new\nstate-of-the-art on decoding text from brain and demonstrate the potential of\nBrain2Char as a high-performance communication BCI.", "published": "2019-09-03 18:54:43", "link": "http://arxiv.org/abs/1909.01401v1", "categories": ["cs.LG", "cs.CL", "q-bio.NC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Interpretable Word Embeddings via Informative Priors", "abstract": "Word embeddings have demonstrated strong performance on NLP tasks. However,\nlack of interpretability and the unsupervised nature of word embeddings have\nlimited their use within computational social science and digital humanities.\nWe propose the use of informative priors to create interpretable and\ndomain-informed dimensions for probabilistic word embeddings. Experimental\nresults show that sensible priors can capture latent semantic concepts better\nthan or on-par with the current state of the art, while retaining the\nsimplicity and generalizability of using priors.", "published": "2019-09-03 21:20:28", "link": "http://arxiv.org/abs/1909.01459v1", "categories": ["cs.CL", "cs.LG", "stat.ML", "68T50 (Primary) 62P25 (Secondary)", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Achieving Verified Robustness to Symbol Substitutions via Interval Bound\n  Propagation", "abstract": "Neural networks are part of many contemporary NLP systems, yet their\nempirical successes come at the price of vulnerability to adversarial attacks.\nPrevious work has used adversarial training and data augmentation to partially\nmitigate such brittleness, but these are unlikely to find worst-case\nadversaries due to the complexity of the search space arising from discrete\ntext perturbations. In this work, we approach the problem from the opposite\ndirection: to formally verify a system's robustness against a predefined class\nof adversarial attacks. We study text classification under synonym replacements\nor character flip perturbations. We propose modeling these input perturbations\nas a simplex and then using Interval Bound Propagation -- a formal model\nverification method. We modify the conventional log-likelihood training\nobjective to train models that can be efficiently verified, which would\notherwise come with exponential search complexity. The resulting models show\nonly little difference in terms of nominal accuracy, but have much improved\nverified accuracy under perturbations and come with an efficiently computable\nformal guarantee on worst case adversaries.", "published": "2019-09-03 23:03:10", "link": "http://arxiv.org/abs/1909.01492v2", "categories": ["cs.CL", "cs.CR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Neural Linguistic Steganography", "abstract": "Whereas traditional cryptography encrypts a secret message into an\nunintelligible form, steganography conceals that communication is taking place\nby encoding a secret message into a cover signal. Language is a particularly\npragmatic cover signal due to its benign occurrence and independence from any\none medium. Traditionally, linguistic steganography systems encode secret\nmessages in existing text via synonym substitution or word order\nrearrangements. Advances in neural language models enable previously\nimpractical generation-based techniques. We propose a steganography technique\nbased on arithmetic coding with large-scale neural language models. We find\nthat our approach can generate realistic looking cover sentences as evaluated\nby humans, while at the same time preserving security by matching the cover\nmessage distribution with the language model distribution.", "published": "2019-09-03 23:15:19", "link": "http://arxiv.org/abs/1909.01496v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Quantifying and Correlating Rhythm Formants in Speech", "abstract": "The objective of the present study is exploratory: to introduce and apply a\nnew theory of speech rhythm zones or rhythm formants (R-formants). R-formants\nare zones of high magnitude frequencies in the low frequency (LF) long-term\nspectrum (LTS), rather like formants in the short-term spectra of vowels and\nconsonants. After an illustration of the method, an R-formant analysis is made\nof non-elicited extracts from public speeches. The LF-LTS of three domains, the\namplitude modulated (AM) absolute (rectified) signal, the amplitude envelope\nmodulation (AEM) and frequency modulation (FM, F0, 'pitch') of the signal are\ncompared. The first two correlate well, but the third does not correlate\nconsistently with the other two, presumably due to variability of tone, pitch\naccent and intonation. Consequently, only the LF LTS of the absolute speech\nsignal is used in the empirical analysis. An informal discussion of the\nrelation between R-formant patterns and utterance structure and a selection of\npragmatic variables over the same utterances showed some trends for R-formant\nfunctionality and thus useful directions for future research.", "published": "2019-09-03 16:18:34", "link": "http://arxiv.org/abs/1909.05639v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Trouble on the Horizon: Forecasting the Derailment of Online\n  Conversations as they Develop", "abstract": "Online discussions often derail into toxic exchanges between participants.\nRecent efforts mostly focused on detecting antisocial behavior after the fact,\nby analyzing single comments in isolation. To provide more timely notice to\nhuman moderators, a system needs to preemptively detect that a conversation is\nheading towards derailment before it actually turns toxic. This means modeling\nderailment as an emerging property of a conversation rather than as an isolated\nutterance-level event.\n  Forecasting emerging conversational properties, however, poses several\ninherent modeling challenges. First, since conversations are dynamic, a\nforecasting model needs to capture the flow of the discussion, rather than\nproperties of individual comments. Second, real conversations have an unknown\nhorizon: they can end or derail at any time; thus a practical forecasting model\nneeds to assess the risk in an online fashion, as the conversation develops. In\nthis work we introduce a conversational forecasting model that learns an\nunsupervised representation of conversational dynamics and exploits it to\npredict future derailment as the conversation develops. By applying this model\nto two new diverse datasets of online conversations with labels for antisocial\nevents, we show that it outperforms state-of-the-art systems at forecasting\nderailment.", "published": "2019-09-03 18:00:05", "link": "http://arxiv.org/abs/1909.01362v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Voice Spoofing Detection Corpus for Single and Multi-order Audio Replays", "abstract": "The evolution of modern voice controlled devices (VCDs) in recent years has\nrevolutionized the Internet of Things, and resulted in increased realization of\nsmart homes, personalization and home automation through voice commands. The\nintroduction of VCDs in IoT is expected to give emergence of new subfield of\nIoT, called Multimedia of Thing (MoT). These VCDs can be exploited in IoT\ndriven environment to generate various spoofing attacks including the replays.\nReplay attacks are generated through replaying the recorded audio of legitimate\nhuman speaker with the intent of deceiving the VCDs having speaker verification\nsystem. The connectivity among the VCDs can easily be exploited in IoT driven\nenvironment to generate a chain of replay attacks (multi-order replay attacks).\nExisting spoofing detection datasets like ASVspoof and ReMASC contain only the\nfirst-order replay recordings against the bonafide audio samples. These\ndatasets can not offer evaluation of the anti-spoofing algorithms capable of\ndetecting the multi-order replay attacks. Additionally, these datasets do not\ncapture the characteristics of microphone arrays, which is an important\ncharacteristic of modern VCDs. We need a diverse replay spoofing detection\ncorpus that consists of multi-order replay recordings against the bonafide\nvoice samples. This paper presents a novel voice spoofing detection corpus\n(VSDC) to evaluate the performance of multi-order replay anti-spoofing methods.\nThe proposed VSDC consists of first and second-order-replay samples against the\nbonafide audio recordings. Additionally, the proposed VSDC can also be used to\nevaluate the performance of speaker verification systems as our corpus includes\nthe audio samples of fifteen different speakers. To the best of our knowledge,\nthis is the first publicly available replay spoofing detection corpus\ncomprising of first-order and second-order-replay samples.", "published": "2019-09-03 03:26:26", "link": "http://arxiv.org/abs/1909.00935v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-level Attention network using text, audio and video for Depression\n  Prediction", "abstract": "Depression has been the leading cause of mental-health illness worldwide.\nMajor depressive disorder (MDD), is a common mental health disorder that\naffects both psychologically as well as physically which could lead to loss of\nlives. Due to the lack of diagnostic tests and subjectivity involved in\ndetecting depression, there is a growing interest in using behavioural cues to\nautomate depression diagnosis and stage prediction. The absence of labelled\nbehavioural datasets for such problems and the huge amount of variations\npossible in behaviour makes the problem more challenging. This paper presents a\nnovel multi-level attention based network for multi-modal depression prediction\nthat fuses features from audio, video and text modalities while learning the\nintra and inter modality relevance. The multi-level attention reinforces\noverall learning by selecting the most influential features within each\nmodality for the decision making. We perform exhaustive experimentation to\ncreate different regression models for audio, video and text modalities.\nSeveral fusions models with different configurations are constructed to\nunderstand the impact of each feature and modality. We outperform the current\nbaseline by 17.52% in terms of root mean squared error.", "published": "2019-09-03 19:40:38", "link": "http://arxiv.org/abs/1909.01417v1", "categories": ["cs.CV", "eess.AS"], "primary_category": "cs.CV"}
{"title": "The LOCATA Challenge: Acoustic Source Localization and Tracking", "abstract": "The ability to localize and track acoustic events is a fundamental\nprerequisite for equipping machines with the ability to be aware of and engage\nwith humans in their surrounding environment. However, in realistic scenarios,\naudio signals are adversely affected by reverberation, noise, interference, and\nperiods of speech inactivity. In dynamic scenarios, where the sources and\nmicrophone platforms may be moving, the signals are additionally affected by\nvariations in the source-sensor geometries. In practice, approaches to sound\nsource localization and tracking are often impeded by missing estimates of\nactive sources, estimation errors, as well as false estimates. The aim of the\nLOCAlization and TrAcking (LOCATA) Challenge is an open-access framework for\nthe objective evaluation and benchmarking of broad classes of algorithms for\nsound source localization and tracking. This article provides a review of\nrelevant localization and tracking algorithms and, within the context of the\nexisting literature, a detailed evaluation and dissemination of the LOCATA\nsubmissions. The evaluation highlights achievements in the field, open\nchallenges, and identifies potential future directions.", "published": "2019-09-03 09:02:53", "link": "http://arxiv.org/abs/1909.01008v3", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "On Loss Functions for Supervised Monaural Time-Domain Speech Enhancement", "abstract": "Many deep learning-based speech enhancement algorithms are designed to\nminimize the mean-square error (MSE) in some transform domain between a\npredicted and a target speech signal. However, optimizing for MSE does not\nnecessarily guarantee high speech quality or intelligibility, which is the\nultimate goal of many speech enhancement algorithms. Additionally, only little\nis known about the impact of the loss function on the emerging class of\ntime-domain deep learning-based speech enhancement systems. We study how\npopular loss functions influence the performance of deep learning-based speech\nenhancement systems. First, we demonstrate that perceptually inspired loss\nfunctions might be advantageous if the receiver is the human auditory system.\nFurthermore, we show that the learning rate is a crucial design parameter even\nfor adaptive gradient-based optimizers, which has been generally overlooked in\nthe literature. Also, we found that waveform matching performance metrics must\nbe used with caution as they in certain situations can fail completely.\nFinally, we show that a loss function based on scale-invariant\nsignal-to-distortion ratio (SI-SDR) achieves good general performance across a\nrange of popular speech enhancement evaluation metrics, which suggests that\nSI-SDR is a good candidate as a general-purpose loss function for speech\nenhancement systems.", "published": "2019-09-03 09:32:11", "link": "http://arxiv.org/abs/1909.01019v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multimodal Deep Learning for Mental Disorders Prediction from Audio\n  Speech Samples", "abstract": "Key features of mental illnesses are reflected in speech. Our research\nfocuses on designing a multimodal deep learning structure that automatically\nextracts salient features from recorded speech samples for predicting various\nmental disorders including depression, bipolar, and schizophrenia. We adopt a\nvariety of pre-trained models to extract embeddings from both audio and text\nsegments. We use several state-of-the-art embedding techniques including BERT,\nFastText, and Doc2VecC for the text representation learning and WaveNet and\nVGG-ish models for audio encoding. We also leverage huge auxiliary\nemotion-labeled text and audio corpora to train emotion-specific embeddings and\nuse transfer learning in order to address the problem of insufficient annotated\nmultimodal data available. All these embeddings are then combined into a joint\nrepresentation in a multimodal fusion layer and finally a recurrent neural\nnetwork is used to predict the mental disorder. Our results show that mental\ndisorders can be predicted with acceptable accuracy through multimodal analysis\nof clinical interviews.", "published": "2019-09-03 11:15:19", "link": "http://arxiv.org/abs/1909.01067v5", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Feasibility of Using Automatic Speech Recognition with Voices of Deaf\n  and Hard-of-Hearing Individuals", "abstract": "Many personal devices have transitioned from visual-controlled interfaces to\nspeech-controlled interfaces to reduce device costs and interactive friction.\nThis transition has been hastened by the increasing capabilities of\nspeech-controlled interfaces, e.g., Amazon Echo or Apple's Siri. A consequence\nis that people who are deaf or hard of hearing (DHH) may be unable to use these\nspeech-controlled devices. We show that deaf speech has a high error rate\ncompared to hearing speech, in commercial speech-controlled interfaces. Deaf\nspeech had approximately a 78% word error rate (WER) compared to a hearing\nspeech 18% WER. Our findings show that current speech-controlled interfaces are\nnot usable by deaf and hard of hearing people. Therefore, it might be wise to\npursue other methods for deaf persons to deliver natural commands to computers.", "published": "2019-09-03 13:33:25", "link": "http://arxiv.org/abs/1909.01167v1", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Demucs: Deep Extractor for Music Sources with extra unlabeled data\n  remixed", "abstract": "We study the problem of source separation for music using deep learning with\nfour known sources: drums, bass, vocals and other accompaniments.\nState-of-the-art approaches predict soft masks over mixture spectrograms while\nmethods working on the waveform are lagging behind as measured on the standard\nMusDB benchmark. Our contribution is two fold. (i) We introduce a simple\nconvolutional and recurrent model that outperforms the state-of-the-art model\non waveforms, that is, Wave-U-Net, by 1.6 points of SDR (signal to distortion\nratio). (ii) We propose a new scheme to leverage unlabeled music. We train a\nfirst model to extract parts with at least one source silent in unlabeled\ntracks, for instance without bass. We remix this extract with a bass line taken\nfrom the supervised dataset to form a new weakly supervised training example.\nCombining our architecture and scheme, we show that waveform methods can play\nin the same ballpark as spectrogram ones.", "published": "2019-09-03 13:41:56", "link": "http://arxiv.org/abs/1909.01174v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Multiresolution analysis (discrete wavelet transform) through Daubechies\n  family for emotion recognition in speech", "abstract": "We propose a study of the mathematical properties of voice as an audio\nsignal. This work includes signals in which the channel conditions are not\nideal for emotion recognition. Multiresolution analysis discrete wavelet\ntransform was performed through the use of Daubechies Wavelet Family (Db1-Haar,\nDb 6, Db8, Db10) allowing the decomposition of the initial audio signal into\nsets of coefficients on which a set of features was extracted and analyzed\nstatistically in order to differentiate emotional states. ANNs proved to be a\nsystem that allows an appropriate classification of such states. This study\nshows that the extracted features using wavelet decomposition are enough to\nanalyze and extract emotional content in audio signals presenting a high\naccuracy rate in classification of emotional states without the need to use\nother kinds of classical frequency-time features. Accordingly, this paper seeks\nto characterize mathematically the six basic emotions in humans: boredom,\ndisgust, happiness, anxiety, anger and sadness, also included the neutrality,\nfor a total of seven states to identify.", "published": "2019-09-03 16:00:54", "link": "http://arxiv.org/abs/1909.01265v1", "categories": ["cs.SD", "eess.AS", "stat.AP"], "primary_category": "cs.SD"}
{"title": "An efficient and perceptually motivated auditory neural encoding and\n  decoding algorithm for spiking neural networks", "abstract": "Auditory front-end is an integral part of a spiking neural network (SNN) when\nperforming auditory cognitive tasks. It encodes the temporal dynamic stimulus,\nsuch as speech and audio, into an efficient, effective and reconstructable\nspike pattern to facilitate the subsequent processing. However, most of the\nauditory front-ends in current studies have not made use of recent findings in\npsychoacoustics and physiology concerning human listening. In this paper, we\npropose a neural encoding and decoding scheme that is optimized for speech\nprocessing. The neural encoding scheme, that we call Biologically plausible\nAuditory Encoding (BAE), emulates the functions of the perceptual components of\nthe human auditory system, that include the cochlear filter bank, the inner\nhair cells, auditory masking effects from psychoacoustic models, and the spike\nneural encoding by the auditory nerve. We evaluate the perceptual quality of\nthe BAE scheme using PESQ; the performance of the BAE based on speech\nrecognition experiments. Finally, we also built and published two spike-version\nof speech datasets: the Spike-TIDIGITS and the Spike-TIMIT, for researchers to\nuse and benchmarking of future SNN research.", "published": "2019-09-03 16:48:47", "link": "http://arxiv.org/abs/1909.01302v2", "categories": ["cs.SD", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automatic Speech Recognition Services: Deaf and Hard-of-Hearing\n  Usability", "abstract": "Nowadays, speech is becoming a more common, if not standard, interface to\ntechnology. This can be seen in the trend of technology changes over the years.\nIncreasingly, voice is used to control programs, appliances and personal\ndevices within homes, cars, workplaces, and public spaces through smartphones\nand home assistant devices using Amazon's Alexa, Google's Assistant and Apple's\nSiri, and other proliferating technologies. However, most speech interfaces are\nnot accessible for Deaf and Hard-of-Hearing (DHH) people. In this paper,\nperformances of current Automatic Speech Recognition (ASR) with voices of DHH\nspeakers are evaluated. ASR has improved over the years, and is able to reach\nWord Error Rates (WER) as low as 5-6% [1][2][3], with the help of\ncloud-computing and machine learning algorithms that take in custom vocabulary\nmodels. In this paper, a custom vocabulary model is used, and the significance\nof the improvement is evaluated when using DHH speech.", "published": "2019-09-03 12:52:21", "link": "http://arxiv.org/abs/1909.02853v1", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Translating Visual Art into Music", "abstract": "The Synesthetic Variational Autoencoder (SynVAE) introduced in this research\nis able to learn a consistent mapping between visual and auditive sensory\nmodalities in the absence of paired datasets. A quantitative evaluation on\nMNIST as well as the Behance Artistic Media dataset (BAM) shows that SynVAE is\ncapable of retaining sufficient information content during the translation\nwhile maintaining cross-modal latent space consistency. In a qualitative\nevaluation trial, human evaluators were furthermore able to match musical\nsamples with the images which generated them with accuracies of up to 73%.", "published": "2019-09-03 14:36:19", "link": "http://arxiv.org/abs/1909.01218v1", "categories": ["cs.CV", "cs.HC", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
