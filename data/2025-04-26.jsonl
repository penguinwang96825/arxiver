{"title": "Calibrating Translation Decoding with Quality Estimation on LLMs", "abstract": "Neural machine translation (NMT) systems typically employ maximum a\nposteriori (MAP) decoding to select the highest-scoring translation from the\ndistribution mass. However, recent evidence highlights the inadequacy of MAP\ndecoding, often resulting in low-quality or even pathological hypotheses -- the\ndecoding objective is not aligned with real-world translation quality. This\npaper proposes calibrating hypothesis likelihoods with translation quality from\na distribution view by directly optimizing their Pearson correlation -- thereby\nenhancing the effectiveness of translation decoding. With our method,\ntranslation on large language models (LLMs) improves substantially after\nlimited training (2K instances per direction). This improvement is orthogonal\nto those achieved through supervised fine-tuning, leading to substantial gains\nacross a broad range of metrics and human evaluations -- even when applied to\ntop-performing translation-specialized LLMs fine-tuned on high-quality\ntranslation data, such as Tower, or when compared to recent preference\noptimization methods, like CPO. Moreover, the calibrated translation likelihood\ncan directly serve as a strong proxy for translation quality, closely\napproximating or even surpassing some state-of-the-art translation quality\nestimation models, like CometKiwi. Lastly, our in-depth analysis demonstrates\nthat calibration enhances the effectiveness of MAP decoding, thereby enabling\ngreater efficiency in real-world deployment. The resulting state-of-the-art\ntranslation model, which covers 10 languages, along with the accompanying code\nand human evaluation data, has been released to the community:\nhttps://github.com/moore3930/calibrating-llm-mt.", "published": "2025-04-26 22:38:47", "link": "http://arxiv.org/abs/2504.19044v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KETCHUP: K-Step Return Estimation for Sequential Knowledge Distillation", "abstract": "We propose a novel k-step return estimation method (called KETCHUP) for\nReinforcement Learning(RL)-based knowledge distillation (KD) in text generation\ntasks. Our idea is to induce a K-step return by using the Bellman Optimality\nEquation for multiple steps. Theoretical analysis shows that this K-step\nformulation reduces the variance of the gradient estimates, thus leading to\nimproved RL optimization especially when the student model size is large.\nEmpirical evaluation on three text generation tasks demonstrates that our\napproach yields superior performance in both standard task metrics and large\nlanguage model (LLM)-based evaluation. These results suggest that our K-step\nreturn induction offers a promising direction for enhancing RL-based KD in LLM\nresearch.", "published": "2025-04-26 21:21:04", "link": "http://arxiv.org/abs/2504.19024v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Advancing Scientific Text Classification: Fine-Tuned Models with Dataset Expansion and Hard-Voting", "abstract": "Efficient text classification is essential for handling the increasing volume\nof academic publications. This study explores the use of pre-trained language\nmodels (PLMs), including BERT, SciBERT, BioBERT, and BlueBERT, fine-tuned on\nthe Web of Science (WoS-46985) dataset for scientific text classification. To\nenhance performance, we augment the dataset by executing seven targeted queries\nin the WoS database, retrieving 1,000 articles per category aligned with\nWoS-46985's main classes. PLMs predict labels for this unlabeled data, and a\nhard-voting strategy combines predictions for improved accuracy and confidence.\nFine-tuning on the expanded dataset with dynamic learning rates and early\nstopping significantly boosts classification accuracy, especially in\nspecialized domains. Domain-specific models like SciBERT and BioBERT\nconsistently outperform general-purpose models such as BERT. These findings\nunderscore the efficacy of dataset augmentation, inference-driven label\nprediction, hard-voting, and fine-tuning techniques in creating robust and\nscalable solutions for automated academic text classification.", "published": "2025-04-26 21:06:49", "link": "http://arxiv.org/abs/2504.19021v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Graph of Attacks: Improved Black-Box and Interpretable Jailbreaks for LLMs", "abstract": "The challenge of ensuring Large Language Models (LLMs) align with societal\nstandards is of increasing interest, as these models are still prone to\nadversarial jailbreaks that bypass their safety mechanisms. Identifying these\nvulnerabilities is crucial for enhancing the robustness of LLMs against such\nexploits. We propose Graph of ATtacks (GoAT), a method for generating\nadversarial prompts to test the robustness of LLM alignment using the Graph of\nThoughts framework [Besta et al., 2024]. GoAT excels at generating highly\neffective jailbreak prompts with fewer queries to the victim model than\nstate-of-the-art attacks, achieving up to five times better jailbreak success\nrate against robust models like Llama. Notably, GoAT creates high-quality,\nhuman-readable prompts without requiring access to the targeted model's\nparameters, making it a black-box attack. Unlike approaches constrained by\ntree-based reasoning, GoAT's reasoning is based on a more intricate graph\nstructure. By making simultaneous attack paths aware of each other's progress,\nthis dynamic framework allows a deeper integration and refinement of reasoning\npaths, significantly enhancing the collaborative exploration of adversarial\nvulnerabilities in LLMs. At a technical level, GoAT starts with a graph\nstructure and iteratively refines it by combining and improving thoughts,\nenabling synergy between different thought paths. The code for our\nimplementation can be found at: https://github.com/GoAT-pydev/Graph_of_Attacks.", "published": "2025-04-26 21:06:03", "link": "http://arxiv.org/abs/2504.19019v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Dynamic Fisher-weighted Model Merging via Bayesian Optimization", "abstract": "The fine-tuning of pre-trained language models has resulted in the widespread\navailability of task-specific models. Model merging offers an efficient way to\ncreate multi-task models by combining these fine-tuned models at the parameter\nlevel, without the need for training data or joint training on multiple\ndatasets. Existing merging approaches typically involve scaling the parameters\nmodel-wise or integrating parameter importance parameter-wise. Both approaches\nexhibit their own weaknesses, leading to a notable performance gap compared to\nmulti-task fine-tuning. In this paper, we unify these seemingly distinct\nstrategies into a more general merging framework, and introduce Dynamic\nFisher-weighted Merging (DF-Merge). Specifically, candidate models are\nassociated with a set of coefficients that linearly scale their fine-tuned\nparameters. Bayesian optimization is applied to dynamically adjust these\ncoefficients, aiming to maximize overall performance on validation sets. Each\niteration of this process integrates parameter importance based on the Fisher\ninformation conditioned by the coefficients. Experimental results show that\nDF-Merge outperforms strong baselines across models of different sizes and a\nvariety of tasks. Our analysis shows that the effectiveness of DF-Merge arises\nfrom the unified view of merging and that near-optimal performance is\nachievable in a few iterations, even with minimal validation data.", "published": "2025-04-26 18:31:14", "link": "http://arxiv.org/abs/2504.18992v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LINC: Supporting Language Independent Communication and Comprehension to Enhance Contribution in Multilingual Collaborative Meetings", "abstract": "Collaborative research often includes contributors with varied perspectives\nfrom diverse linguistic backgrounds. However, English as a Second Language\n(ESL) researchers often struggle to communicate during meetings in English and\ncomprehend discussions, leading to limited contribution. To investigate these\nchallenges, we surveyed 64 ESL researchers who frequently collaborate in\nmultilingual teams and identified four key design goals around participation,\ncomprehension, documentation, and feedback. Guided by these design goals, we\ndeveloped LINC, a multimodal Language INdependent Collaboration system with two\ncomponents: a real-time module for multilingual communication during meetings\nand a post-meeting dashboard for discussion analysis. We evaluated the system\nthrough a two-phased study with six triads of multilingual teams. We found that\nusing LINC, participants benefited from communicating in their preferred\nlanguage, recalled and reviewed actionable insights, and prepared for upcoming\nmeetings effectively. We discuss external factors that impact multilingual\nmeeting participation beyond language preferences and the implications of\nmultimodal systems in facilitating meetings in hybrid multilingual\ncollaborative settings beyond research.", "published": "2025-04-26 18:17:31", "link": "http://arxiv.org/abs/2504.18988v1", "categories": ["cs.HC", "cs.CL", "H.5.3"], "primary_category": "cs.HC"}
{"title": "LawFlow : Collecting and Simulating Lawyers' Thought Processes", "abstract": "Legal practitioners, particularly those early in their careers, face complex,\nhigh-stakes tasks that require adaptive, context-sensitive reasoning. While AI\nholds promise in supporting legal work, current datasets and models are\nnarrowly focused on isolated subtasks and fail to capture the end-to-end\ndecision-making required in real-world practice. To address this gap, we\nintroduce LawFlow, a dataset of complete end-to-end legal workflows collected\nfrom trained law students, grounded in real-world business entity formation\nscenarios. Unlike prior datasets focused on input-output pairs or linear chains\nof thought, LawFlow captures dynamic, modular, and iterative reasoning\nprocesses that reflect the ambiguity, revision, and client-adaptive strategies\nof legal practice. Using LawFlow, we compare human and LLM-generated workflows,\nrevealing systematic differences in structure, reasoning flexibility, and plan\nexecution. Human workflows tend to be modular and adaptive, while LLM workflows\nare more sequential, exhaustive, and less sensitive to downstream implications.\nOur findings also suggest that legal professionals prefer AI to carry out\nsupportive roles, such as brainstorming, identifying blind spots, and surfacing\nalternatives, rather than executing complex workflows end-to-end. Building on\nthese findings, we propose a set of design suggestions, rooted in empirical\nobservations, that align AI assistance with human goals of clarity,\ncompleteness, creativity, and efficiency, through hybrid planning, adaptive\nexecution, and decision-point support. Our results highlight both the current\nlimitations of LLMs in supporting complex legal workflows and opportunities for\ndeveloping more collaborative, reasoning-aware legal AI systems. All data and\ncode are available on our project page\n(https://minnesotanlp.github.io/LawFlow-website/).", "published": "2025-04-26 15:01:55", "link": "http://arxiv.org/abs/2504.18942v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MTCSC: Retrieval-Augmented Iterative Refinement for Chinese Spelling Correction", "abstract": "Chinese Spelling Correction (CSC) aims to detect and correct erroneous tokens\nin sentences. While Large Language Models (LLMs) have shown remarkable success\nin identifying and rectifying potential errors, they often struggle with\nmaintaining consistent output lengths and adapting to domain-specific\ncorrections. Furthermore, existing CSC task impose rigid constraints requiring\ninput and output lengths to be identical, limiting their applicability. In this\nwork, we extend traditional CSC to variable-length correction scenarios,\nincluding Chinese Splitting Error Correction (CSEC) and ASR N-best Error\nCorrection. To address domain adaptation and length consistency, we propose\nMTCSC (Multi-Turn CSC) framework based on RAG enhanced with a length reflection\nmechanism. Our approach constructs a retrieval database from domain-specific\ntraining data and dictionaries, fine-tuning retrievers to optimize performance\nfor error-containing inputs. Additionally, we introduce a multi-source\ncombination strategy with iterative length reflection to ensure output length\nfidelity. Experiments across diverse domain datasets demonstrate that our\nmethod significantly outperforms current approaches in correction quality,\nparticularly in handling domain-specific and variable-length error correction\ntasks.", "published": "2025-04-26 14:48:44", "link": "http://arxiv.org/abs/2504.18938v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Clinical knowledge in LLMs does not translate to human interactions", "abstract": "Global healthcare providers are exploring use of large language models (LLMs)\nto provide medical advice to the public. LLMs now achieve nearly perfect scores\non medical licensing exams, but this does not necessarily translate to accurate\nperformance in real-world settings. We tested if LLMs can assist members of the\npublic in identifying underlying conditions and choosing a course of action\n(disposition) in ten medical scenarios in a controlled study with 1,298\nparticipants. Participants were randomly assigned to receive assistance from an\nLLM (GPT-4o, Llama 3, Command R+) or a source of their choice (control). Tested\nalone, LLMs complete the scenarios accurately, correctly identifying conditions\nin 94.9% of cases and disposition in 56.3% on average. However, participants\nusing the same LLMs identified relevant conditions in less than 34.5% of cases\nand disposition in less than 44.2%, both no better than the control group. We\nidentify user interactions as a challenge to the deployment of LLMs for medical\nadvice. Standard benchmarks for medical knowledge and simulated patient\ninteractions do not predict the failures we find with human participants.\nMoving forward, we recommend systematic human user testing to evaluate\ninteractive capabilities prior to public deployments in healthcare.", "published": "2025-04-26 13:32:49", "link": "http://arxiv.org/abs/2504.18919v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "A Simple Ensemble Strategy for LLM Inference: Towards More Stable Text Classification", "abstract": "With the advance of large language models (LLMs), LLMs have been utilized for\nthe various tasks. However, the issues of variability and reproducibility of\nresults from each trial of LLMs have been largely overlooked in existing\nliterature while actual human annotation uses majority voting to resolve\ndisagreements among annotators. Therefore, this study introduces the\nstraightforward ensemble strategy to a sentiment analysis using LLMs. As the\nresults, we demonstrate that the ensemble of multiple inference using\nmedium-sized LLMs produces more robust and accurate results than using a large\nmodel with a single attempt with reducing RMSE by 18.6%.", "published": "2025-04-26 10:10:26", "link": "http://arxiv.org/abs/2504.18884v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Latent Adversarial Training Improves the Representation of Refusal", "abstract": "Recent work has shown that language models' refusal behavior is primarily\nencoded in a single direction in their latent space, making it vulnerable to\ntargeted attacks. Although Latent Adversarial Training (LAT) attempts to\nimprove robustness by introducing noise during training, a key question\nremains: How does this noise-based training affect the underlying\nrepresentation of refusal behavior? Understanding this encoding is crucial for\nevaluating LAT's effectiveness and limitations, just as the discovery of linear\nrefusal directions revealed vulnerabilities in traditional supervised safety\nfine-tuning (SSFT).\n  Through the analysis of Llama 2 7B, we examine how LAT reorganizes the\nrefusal behavior in the model's latent space compared to SSFT and embedding\nspace adversarial training (AT). By computing activation differences between\nharmful and harmless instruction pairs and applying Singular Value\nDecomposition (SVD), we find that LAT significantly alters the refusal\nrepresentation, concentrating it in the first two SVD components which explain\napproximately 75 percent of the activation differences variance - significantly\nhigher than in reference models. This concentrated representation leads to more\neffective and transferable refusal vectors for ablation attacks: LAT models\nshow improved robustness when attacked with vectors from reference models but\nbecome more vulnerable to self-generated vectors compared to SSFT and AT. Our\nfindings suggest that LAT's training perturbations enable a more comprehensive\nrepresentation of refusal behavior, highlighting both its potential strengths\nand vulnerabilities for improving model safety.", "published": "2025-04-26 09:40:31", "link": "http://arxiv.org/abs/2504.18872v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Effective Length Extrapolation via Dimension-Wise Positional Embeddings Manipulation", "abstract": "Large Language Models (LLMs) often struggle to process and generate coherent\ncontext when the number of input tokens exceeds the pre-trained length. Recent\nadvancements in long-context extension have significantly expanded the context\nwindow of LLMs but require expensive overhead to train the large-scale models\nwith longer context. In this work, we propose Dimension-Wise Positional\nEmbeddings Manipulation (DPE), a training-free framework to extrapolate the\ncontext window of LLMs by diving into RoPE's different hidden dimensions.\nInstead of manipulating all dimensions equally, DPE detects the effective\nlength for every dimension and finds the key dimensions for context extension.\nWe reuse the original position indices with their embeddings from the\npre-trained model and manipulate the key dimensions' position indices to their\nmost effective lengths. In this way, DPE adjusts the pre-trained models with\nminimal modifications while ensuring that each dimension reaches its optimal\nstate for extrapolation. DPE significantly surpasses well-known baselines such\nas YaRN and Self-Extend. DPE enables Llama3-8k 8B to support context windows of\n128k tokens without continual training and integrates seamlessly with Flash\nAttention 2. In addition to its impressive extrapolation capability, DPE also\ndramatically improves the models' performance within training length, such as\nLlama3.1 70B, by over 18 points on popular long-context benchmarks RULER. When\ncompared with commercial models, Llama 3.1 70B with DPE even achieves better\nperformance than GPT-4-128K.", "published": "2025-04-26 08:46:10", "link": "http://arxiv.org/abs/2504.18857v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "When2Call: When (not) to Call Tools", "abstract": "Leveraging external tools is a key feature for modern Language Models (LMs)\nto expand their capabilities and integrate them into existing systems. However,\nexisting benchmarks primarily focus on the accuracy of tool calling -- whether\nthe correct tool is called with the correct parameters -- and less on\nevaluating when LMs should (not) call tools. We develop a new benchmark,\nWhen2Call, which evaluates tool-calling decision-making: when to generate a\ntool call, when to ask follow-up questions and when to admit the question can't\nbe answered with the tools provided. We find that state-of-the-art tool-calling\nLMs show significant room for improvement on When2Call, indicating the\nimportance of this benchmark. We also develop a training set for When2Call and\nleverage the multiple-choice nature of the benchmark to develop a preference\noptimization training regime, which shows considerably more improvement than\ntraditional fine-tuning. We release the benchmark and training data as well as\nevaluation scripts at https://github.com/NVIDIA/When2Call.", "published": "2025-04-26 08:30:02", "link": "http://arxiv.org/abs/2504.18851v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Robust Dialogue Breakdown Detection: Addressing Disruptors in Large Language Models with Self-Guided Reasoning", "abstract": "Large language models (LLMs) are rapidly changing various domains. However,\ntheir capabilities in handling conversational breakdowns still require an\nin-depth exploration. This paper addresses the challenge of detecting and\nmitigating dialogue breakdowns within LLM-driven conversational systems. While\npowerful models from OpenAI and Anthropic excel in many dialogue tasks, they\ncan still produce incoherent or contradictory responses, commonly referred to\nas breakdowns, which undermine user trust. To tackle this, we propose an\napproach that combines specialized fine-tuning with advanced prompting\nstrategies, including few-shot learning, chain-of-thought reasoning, and\nanalogical prompting. In particular, we fine-tune a small 8B model and\ndemonstrate its robust classification and calibration capabilities in English\nand Japanese dialogue. We also validate its generalization on the BETOLD\ndataset, achieving a 7\\% accuracy improvement over its base model. Furthermore,\nwe introduce a real-time deployment architecture that selectively escalates\nsuspicious responses to more resource-intensive frontier models only when\nbreakdowns are detected, significantly cutting operational expenses and energy\nconsumption. Experimental results show our method surpasses prior\nstate-of-the-art specialized classifiers while also narrowing performance gaps\nbetween smaller open-source models and large proprietary ones. Our approach\noffers a scalable solution for robust conversational AI in high-impact domains\nby combining efficiency, interpretability, and reliability.", "published": "2025-04-26 07:51:05", "link": "http://arxiv.org/abs/2504.18839v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toward Generalizable Evaluation in the LLM Era: A Survey Beyond Benchmarks", "abstract": "Large Language Models (LLMs) are advancing at an amazing speed and have\nbecome indispensable across academia, industry, and daily applications. To keep\npace with the status quo, this survey probes the core challenges that the rise\nof LLMs poses for evaluation. We identify and analyze two pivotal transitions:\n(i) from task-specific to capability-based evaluation, which reorganizes\nbenchmarks around core competencies such as knowledge, reasoning, instruction\nfollowing, multi-modal understanding, and safety; and (ii) from manual to\nautomated evaluation, encompassing dynamic dataset curation and\n\"LLM-as-a-judge\" scoring.\n  Yet, even with these transitions, a crucial obstacle persists: the evaluation\ngeneralization issue. Bounded test sets cannot scale alongside models whose\nabilities grow seemingly without limit. We will dissect this issue, along with\nthe core challenges of the above two transitions, from the perspectives of\nmethods, datasets, evaluators, and metrics. Due to the fast evolving of this\nfield, we will maintain a living GitHub repository (links are in each section)\nto crowd-source updates and corrections, and warmly invite contributors and\ncollaborators.", "published": "2025-04-26 07:48:52", "link": "http://arxiv.org/abs/2504.18838v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stealing Creator's Workflow: A Creator-Inspired Agentic Framework with Iterative Feedback Loop for Improved Scientific Short-form Generation", "abstract": "Generating engaging, accurate short-form videos from scientific papers is\nchallenging due to content complexity and the gap between expert authors and\nreaders. Existing end-to-end methods often suffer from factual inaccuracies and\nvisual artifacts, limiting their utility for scientific dissemination. To\naddress these issues, we propose SciTalk, a novel multi-LLM agentic framework,\ngrounding videos in various sources, such as text, figures, visual styles, and\navatars. Inspired by content creators' workflows, SciTalk uses specialized\nagents for content summarization, visual scene planning, and text and layout\nediting, and incorporates an iterative feedback mechanism where video agents\nsimulate user roles to give feedback on generated videos from previous\niterations and refine generation prompts. Experimental evaluations show that\nSciTalk outperforms simple prompting methods in generating scientifically\naccurate and engaging content over the refined loop of video generation.\nAlthough preliminary results are still not yet matching human creators'\nquality, our framework provides valuable insights into the challenges and\nbenefits of feedback-driven video generation. Our code, data, and generated\nvideos will be publicly available.", "published": "2025-04-26 05:22:35", "link": "http://arxiv.org/abs/2504.18805v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SynLexLM: Scaling Legal LLMs with Synthetic Data and Curriculum Learning", "abstract": "Large Language Models (LLMs) are powerful but often require extensive\nfine-tuning and large datasets for specialized domains like law.\nGeneral-purpose pre-training may not capture legal nuances, and acquiring\nsufficient legal data is challenging. We introduce SynLexLM, a novel approach\nto efficiently pre-train a legal LLM. Our method employs curriculum learning,\nprogressing from simple to complex legal texts and queries, combined with\nsynthetic data augmentation using models like Gemini Pro to address data\nscarcity. We aim to achieve improved performance on legal benchmarks\n(BigLaw-Bench, EUR-Lex-Sum) compared to traditional models and fine-tuned\nversions. Preliminary work involves generating synthetic QA pairs reflecting\nlegal reasoning. This work aims to enhance legal document analysis and research\ntools, potentially democratizing access to advanced legal AI.", "published": "2025-04-26 01:42:22", "link": "http://arxiv.org/abs/2504.18762v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generative Product Recommendations for Implicit Superlative Queries", "abstract": "In Recommender Systems, users often seek the best products through indirect,\nvague, or under-specified queries, such as \"best shoes for trail running\". Such\nqueries, also referred to as implicit superlative queries, pose a significant\nchallenge for standard retrieval and ranking systems as they lack an explicit\nmention of attributes and require identifying and reasoning over complex\nfactors. We investigate how Large Language Models (LLMs) can generate implicit\nattributes for ranking as well as reason over them to improve product\nrecommendations for such queries. As a first step, we propose a novel\nfour-point schema for annotating the best product candidates for superlative\nqueries called SUPERB, paired with LLM-based product annotations. We then\nempirically evaluate several existing retrieval and ranking approaches on our\nnew dataset, providing insights and discussing their integration into\nreal-world e-commerce production systems.", "published": "2025-04-26 00:05:47", "link": "http://arxiv.org/abs/2504.18748v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A Group Theoretic Construction of Batch Codes", "abstract": "Batch codes serve as critical tools for load balancing in distributed storage\nsystems. While numerous constructions exist for specific batch sizes t, current\nmethodologies predominantly rely on code dimension parameters, limiting their\nadaptability. Practical implementations, however, demand versatile batch code\ndesigns capable of accommodating arbitrary batch sizes-a challenge that remains\nunderstudied in the literature. This paper introduces a novel framework for\nconstructing batch codes through finite groups and their subgroup structures,\nbuilding on the quasi-uniform group code framework proposed by Chan et al. By\nleveraging algebraic properties of groups, the proposed method enables\nsystematic code construction, streamlined decoding procedures, and efficient\nreconstruction of information symbols. Unlike traditional linear codes,\nquasi-uniform codes exhibit broader applicability due to their inherent\nstructural flexibility.\n  Focusing on abelian 2-groups, the work investigates their subgroup lattices\nand demonstrates their utility in code design-a contribution of independent\ntheoretical interest. The resulting batch codes achieve near-optimal code\nlengths and exhibit potential for dual application as locally repairable codes\n(LRCs), addressing redundancy and fault tolerance in distributed systems. This\nstudy not only advances batch code construction but also establishes\ngroup-theoretic techniques as a promising paradigm for future research in coded\nstorage systems. By bridging algebraic structures with practical coding\ndemands, the approach opens new directions for optimizing distributed storage\narchitectures.", "published": "2025-04-26 08:13:18", "link": "http://arxiv.org/abs/2504.18844v1", "categories": ["cs.IT", "cs.DM", "math.GR", "math.IT"], "primary_category": "cs.IT"}
{"title": "Feature Fusion Revisited: Multimodal CTR Prediction for MMCTR Challenge", "abstract": "With the rapid advancement of Multimodal Large Language Models (MLLMs), an\nincreasing number of researchers are exploring their application in\nrecommendation systems. However, the high latency associated with large models\npresents a significant challenge for such use cases. The EReL@MIR workshop\nprovided a valuable opportunity to experiment with various approaches aimed at\nimproving the efficiency of multimodal representation learning for information\nretrieval tasks. As part of the competition's requirements, participants were\nmandated to submit a technical report detailing their methodologies and\nfindings. Our team was honored to receive the award for Task 2 - Winner\n(Multimodal CTR Prediction). In this technical report, we present our methods\nand key findings. Additionally, we propose several directions for future work,\nparticularly focusing on how to effectively integrate recommendation signals\ninto multimodal representations. The codebase for our implementation is\npublicly available at: https://github.com/Lattice-zjj/MMCTR_Code, and the\ntrained model weights can be accessed at:\nhttps://huggingface.co/FireFlyCourageous/MMCTR_DIN_MicroLens_1M_x1.", "published": "2025-04-26 16:04:33", "link": "http://arxiv.org/abs/2504.18961v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "Speaker Retrieval in the Wild: Challenges, Effectiveness and Robustness", "abstract": "There is a growing abundance of publicly available or company-owned\naudio/video archives, highlighting the increasing importance of efficient\naccess to desired content and information retrieval from these archives. This\npaper investigates the challenges, solutions, effectiveness, and robustness of\nspeaker retrieval systems developed \"in the wild\" which involves addressing two\nprimary challenges: extraction of task-relevant labels from limited metadata\nfor system development and evaluation, as well as the unconstrained acoustic\nconditions encountered in the archive, ranging from quiet studios to adverse\nnoisy environments. While we focus on the publicly-available BBC Rewind archive\n(spanning 1948 to 1979), our framework addresses the broader issue of speaker\nretrieval on extensive and possibly aged archives with no control over the\ncontent and acoustic conditions. Typically, these archives offer a brief and\ngeneral file description, mostly inadequate for specific applications like\nspeaker retrieval, and manual annotation of such large-scale archives is\nunfeasible. We explore various aspects of system development (e.g., speaker\ndiarisation, embedding extraction, query selection) and analyse the challenges,\npossible solutions, and their functionality. To evaluate the performance, we\nconduct systematic experiments in both clean setup and against various\ndistortions simulating real-world applications. Our findings demonstrate the\neffectiveness and robustness of the developed speaker retrieval systems,\nestablishing the versatility and scalability of the proposed framework for a\nwide range of applications beyond the BBC Rewind corpus.", "published": "2025-04-26 15:21:14", "link": "http://arxiv.org/abs/2504.18950v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Masked Matrix Separation Problem: A First Analysis", "abstract": "Given a known matrix that is the sum of a low rank matrix and a masked sparse\nmatrix, we wish to recover both the low rank component and the sparse\ncomponent. The sparse matrix is masked in the sense that a linear\ntransformation has been applied on its left. We propose a convex optimization\nproblem to recover the low rank and sparse matrices, which generalizes the\nrobust PCA framework. We provide incoherence conditions for the success of the\nproposed convex optimizaiton problem, adapting to the masked setting. The\n``mask'' matrix can be quite general as long as a so-called restricted infinity\nnorm condition is satisfied. Further analysis on the incoherence condition is\nprovided and we conclude with promising numerical experiments.", "published": "2025-04-26 21:21:16", "link": "http://arxiv.org/abs/2504.19025v1", "categories": ["cs.IT", "cs.NA", "math.IT", "math.NA"], "primary_category": "cs.IT"}
{"title": "Secret Sharing for DNA Probability Vectors", "abstract": "Emerging DNA storage technologies use composite DNA letters, where\ninformation is represented by a probability vector, leading to higher\ninformation density and lower synthesis costs. However, it faces the problem of\ninformation leakage in sharing the DNA vessels among untrusted vendors. This\npaper introduces an asymptotic ramp secret sharing scheme (ARSSS) for secret\ninformation storage using composite DNA letters. This innovative scheme,\ninspired by secret sharing methods over finite fields and enhanced with a\nmodified matrix-vector multiplication operation for probability vectors,\nachieves asymptotic information-theoretic data security for a large alphabet\nsize. Moreover, this scheme reduces the number of reading operations for DNA\nsamples compared to traditional schemes, and therefore lowers the complexity\nand the cost of DNA-based secret sharing. We further explore the construction\nof the scheme, starting with a proof of the existence of a suitable generator,\nfollowed by practical examples. Finally, we demonstrate efficient constructions\nto support large information sizes, which utilize multiple vessels for each\nsecret share rather than a single vessel.", "published": "2025-04-26 16:43:06", "link": "http://arxiv.org/abs/2504.18970v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Deep Reinforcement Learning for MIMO Communication with Low-Resolution ADCs", "abstract": "Multiple-input multiple-output (MIMO) wireless systems conventionally use\nhigh-resolution analog-to-digital converters (ADCs) at the receiver side to\nfaithfully digitize received signals prior to digital signal processing.\nHowever, the power consumption of ADCs increases significantly as the bandwidth\nis increased, particularly in millimeter wave communications systems. A\ncombination of two mitigating approaches has been considered in the literature:\ni) to use hybrid beamforming to reduce the number of ADCs, and ii) to use\nlow-resolution ADCs to reduce per ADC power consumption. Lowering the number\nand resolution of the ADCs naturally reduces the communication rate of the\nsystem, leading to a tradeoff between ADC power consumption and communication\nrate. Prior works have shown that optimizing over the hybrid beamforming matrix\nand ADC thresholds may reduce the aforementioned rate-loss significantly. A key\nchallenge is the complexity of optimization over all choices of beamforming\nmatrices and threshold vectors. This work proposes a reinforcement learning\n(RL) architecture to perform the optimization. The proposed approach integrates\ndeep neural network-based mutual information estimators for reward calculation\nwith policy gradient methods for reinforcement learning. The approach is robust\nto dynamic channel statistics and noisy CSI estimates. It is shown\ntheoretically that greedy RL methods converge to the globally optimal policy.\nExtensive empirical evaluations are provided demonstrating that the performance\nof the RL-based approach closely matches exhaustive search optimization across\nthe solution space.", "published": "2025-04-26 15:42:01", "link": "http://arxiv.org/abs/2504.18957v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Closed-Form Expressions for I/O Relation in Zak-OTFS with Different Delay-Doppler Filters", "abstract": "The transceiver operations in the delay-Doppler (DD) domain in Zak-OTFS\nmodulation, including DD domain filtering at the transmitter and receiver,\ninvolve twisted convolution operation. The twisted convolution operations give\nrise to multiple integrals in the end-to-end DD domain input-output (I/O)\nrelation. The I/O relation plays a crucial role in performance evaluation and\nalgorithm development for transceiver implementation. In this paper, we derive\ndiscrete DD domain closed-form expressions for the I/O relation and noise\ncovariance in Zak-OTFS. We derive these expressions for sinc and Gaussian pulse\nshaping DD filters at the transmitter (Tx). On the receiver (Rx) side, three\ntypes of DD filters are considered, viz., $(i)$ Rx filter identical to Tx\nfilter (referred to as `identical filtering'), $(ii)$ Rx filter matched to the\nTx filter (referred to as `matched filtering'), and $(iii)$ Rx filter matched\nto both Tx filter and channel response (referred to as `channel matched\nfiltering'). For all the above cases, except for the case of sinc identical\nfiltering, we derive exact I/O relation and noise covariance expressions in\nclosed-form. For the sinc identical filtering case, we derive approximate\nclosed-form expressions which are shown to be accurate. Using the derived\nclosed-form expressions, we evaluate the bit error performance of Zak-OTFS for\ndifferent Tx/Rx filter configurations. Our results using Vehicular-A (Veh-A)\nchannel model with fractional DDs show that, while matched filtering achieves\nslightly better or almost same performance as identical filtering, channel\nmatched filtering achieves the best performance among the three.", "published": "2025-04-26 10:28:39", "link": "http://arxiv.org/abs/2504.18887v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Cross Far- and Near-Field Beam Management Technologies in Millimeter-Wave and Terahertz MIMO Systems", "abstract": "The evolution of wireless communication toward next-generation networks\nintroduces unprecedented demands on data rates, latency, and connectivity. To\nmeet these requirements, two key trends have emerged: the use of higher\ncommunication frequencies to provide broader bandwidth, and the deployment of\nmassive multiple-input multiple-output systems with large antenna arrays to\ncompensate for propagation losses and enhance spatial multiplexing. These\nadvancements significantly extend the Rayleigh distance, enabling near-field\n(NF) propagation alongside the traditional far-field (FF) regime. As user\ncommunication distances dynamically span both FF and NF regions, cross-field\n(CF) communication has also emerged as a practical consideration. Beam\nmanagement (BM)-including beam scanning, channel state information estimation,\nbeamforming, and beam tracking-plays a central role in maintaining reliable\ndirectional communications. While most existing BM techniques are developed for\nFF channels, recent works begin to address the unique characteristics of NF and\nCF regimes. This survey presents a comprehensive review of BM techniques from\nthe perspective of propagation fields. We begin by building the basic through\nanalyzing the modeling of FF, NF, and CF channels, along with the associated\nbeam patterns for alignment. Then, we categorize BM techniques by\nmethodologies, and discuss their operational differences across propagation\nregimes, highlighting how field-dependent channel characteristics influence\ndesign tradeoffs and implementation complexity. In addition, for each BM\nmethod, we identify open challenges and future research directions, including\nextending FF methods to NF or CF scenarios, developing unified BM strategies\nfor field-agnostic deployment, and designing low-overhead BM solutions for\ndynamic environments.", "published": "2025-04-26 08:43:12", "link": "http://arxiv.org/abs/2504.18855v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Covert Communication Over a Quantum MAC with a Helper", "abstract": "We study covert classical communication over a quantum multiple-access\nchannel (MAC) with a helper. Specifically, we consider three transmitters,\nwhere one transmitter helps the other two transmitters communicate covertly\nwith a receiver. We demonstrate the feasibility of achieving a positive covert\nrate over this channel and establish an achievable rate region. Our result\nrecovers as a special case known results for classical communication over\nclassical MACs with a degraded message set, classical communication over\nquantum MACs, and classical communication over MACs with a helper. To the best\nof our knowledge, our result is the first to achieve covert communication with\npositive rates over both classical and quantum MACs.", "published": "2025-04-26 00:00:14", "link": "http://arxiv.org/abs/2504.18747v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Entrywise Approximate Matrix Inversion", "abstract": "We study the bit complexity of inverting diagonally dominant matrices, which\nare associated with random walk quantities such as hitting times and escape\nprobabilities. Such quantities can be exponentially small, even on undirected\nunit-weighted graphs. However, their nonnegativity suggests that they can be\napproximated entrywise, leading to a stronger notion of approximation than\nvector norm-based error.\n  Under this notion of error, existing Laplacian solvers and fast matrix\nmultiplication approaches have bit complexities of $mn^2$ and $n^{\\omega+1}$,\nrespectively, where $m$ is the number of nonzero entries in the matrix, $n$ is\nits size, and $\\omega$ is the matrix multiplication exponent.\n  We present algorithms that compute entrywise $\\exp(\\epsilon)$-approximate\ninverses of row diagonally dominant $L$-matrices (RDDL) in two settings: (1)\nwhen the matrix entries are given in floating-point representation; (2) when\nthey are given in fixed-point representation.\n  For floating-point inputs, we present a cubic-time algorithm and show that it\nhas an optimal running time under the all-pairs shortest paths (APSP)\nconjecture.\n  For fixed-point inputs, we present several algorithms for solving linear\nsystems and inverting RDDL and SDDM matrices, all with high probability.\n  Omitting logarithmic factors:\n  (1) For SDDM matrices, we provide an algorithm for solving a linear system\nwith entrywise approximation guarantees using $\\tilde{O}(m\\sqrt{n})$ bit\noperations, and another for computing an entrywise approximate inverse using\n$\\tilde{O}(mn)$ bit operations.\n  (2) For RDDL matrices, we present an algorithm for solving a linear system\nusing $\\tilde{O}(mn^{1+o(1)})$ bit operations, and two algorithms for computing\nan entrywise approximate inverse: one using $\\tilde{O}(n^{\\omega+0.5})$ bit\noperations, and the other using $\\tilde{O}(mn^{1.5+o(1)})$ bit operations.", "published": "2025-04-26 23:57:58", "link": "http://arxiv.org/abs/2504.19054v1", "categories": ["cs.DS", "cs.NA", "math.NA", "65Y20, 65F10, 65F05, 15A09", "G.1.3; F.2.1"], "primary_category": "cs.DS"}
{"title": "On Symmetric Lanczos Quadrature for Trace Estimation", "abstract": "The Golub-Welsch algorithm computes Gauss quadrature rules with the nodes and\nweights generated from the symmetric tridiagonal matrix in the Lanczos process.\nWhile symmetric Lanczos quadrature (in exact arithmetic) theoretically reduces\ncomputational costs, its practical feasibility for trace estimation remains\nuncertain. This paper resolves this ambiguity by establishing sufficient and\nnecessary conditions for the symmetry of Lanczos quadratrure. For matrices of\nJordan-Wielandt type, we provide guidance on selecting initial vectors for the\nLanczos algorithm that guarantees symmetric quadrature nodes and weights. More\nimportantly, regarding Estrada index computations in bipartite graphs or\ndirected ones, our method would not only save computational costs, but also\nensure the unbiasedness of trace estimators.", "published": "2025-04-26 13:01:31", "link": "http://arxiv.org/abs/2504.18913v1", "categories": ["math.NA", "cs.NA", "65D32, 65F15"], "primary_category": "math.NA"}
{"title": "Numerical analysis of an H(div)-conforming divergence-free discontinuous Galerkin method with a second-order explicit Runge-Kutta scheme for the incompressible Euler equations", "abstract": "In this paper, we present an error analysis for an \\( H(\\text{div})\n\\)-conforming divergence-free discontinuous Galerkin (DG) method, combined with\na second-order explicit Runge-Kutta (RK) scheme, for the incompressible Euler\nequations. This work extends the error analysis of the second-order explicit\nRunge-Kutta discontinuous Galerkin (RKDG) type methods to incompressible flows,\nin which the exactly divergence-free constraint introduces additional\nchallenges to the analysis. For smooth solutions, we rigorously derive an a\npriori error estimate of $O(h^{k+1 / 2}+\\tau^2)$ under a restrictive CFL\ncondition $\\tau \\lesssim h^{4 / 3}$ for polynomials of degree $k \\geq 1$, where\n$h$ and $\\tau$ are the mesh size and time step size, respectively. For the case\nof linear polynomials, we further investigate whether existing analytical\ntechniques can relax the restrictive CFL condition to a standard CFL condition\n$\\tau \\lesssim h$. It is demonstrated that the exactly divergence-free\nconstraint prevents the application of these techniques. We conjecture that the\nerror estimates for linear polynomials cannot be derived under a standard CFL\ncondition. Numerical experiments are conducted, supporting our analytical\nresults and the conjecture for linear polynomials.", "published": "2025-04-26 12:20:04", "link": "http://arxiv.org/abs/2504.18903v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Adaptive Nonlinear Elimination Preconditioning for Transport in Fractured Porous Media", "abstract": "Sequential implicit (SI) formulations are gaining increasing interest due to\ntheir ability to decouple reservoir simulation problems into distinct flow and\ntransport subproblems, allowing for the use of specialized solvers tailored to\neach. This separation often improves solver efficiency and flexibility,\nespecially in weakly coupled systems. However, for fractured reservoirs, even\nthe decoupled subproblems may generate nonlinearly stiff systems. This is\nspecifically evident in the transport subproblem, where fracture-induced\nnon-linearity imbalances often lead to poor Newton convergence, including\nfailed iterations and frequent timestep cuts. To address this challenge, we\npropose and investigate an adaptive Nonlinear Elimination (NE) preconditioned\nexact Newton algorithm specifically tailored to transport subproblems that\narise from the sequential splitting of two-phase flow in fractured porous\nmedia. The proposed method is evaluated through a series of waterflooding test\ncases involving discrete fracture networks. The adaptive NE-preconditioned\nalgorithm consistently demonstrates improved convergence behavior and\ncomputational efficiency compared to standard Newton.", "published": "2025-04-26 12:10:00", "link": "http://arxiv.org/abs/2504.18900v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "A Dictionary of Closed-Form Kernel Mean Embeddings", "abstract": "Kernel mean embeddings -- integrals of a kernel with respect to a probability\ndistribution -- are essential in Bayesian quadrature, but also widely used in\nother computational tools for numerical integration or for statistical\ninference based on the maximum mean discrepancy. These methods often require,\nor are enhanced by, the availability of a closed-form expression for the kernel\nmean embedding. However, deriving such expressions can be challenging, limiting\nthe applicability of kernel-based techniques when practitioners do not have\naccess to a closed-form embedding. This paper addresses this limitation by\nproviding a comprehensive dictionary of known kernel mean embeddings, along\nwith practical tools for deriving new embeddings from known ones. We also\nprovide a Python library that includes minimal implementations of the\nembeddings.", "published": "2025-04-26 07:33:30", "link": "http://arxiv.org/abs/2504.18830v1", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA", "stat.CO"], "primary_category": "stat.ML"}
{"title": "Phase Transitions in Financial Markets Using the Ising Model: A Statistical Mechanics Perspective", "abstract": "This dissertation investigates the ability of the Ising model to replicate\nstatistical characteristics, or stylized facts, commonly observed in financial\nassets. The study specifically examines in the S&P500 index the following\nfeatures: volatility clustering, negative skewness, heavy tails, the absence of\nautocorrelation in returns, and the presence of autocorrelation in absolute\nreturns. A significant portion of the dissertation is dedicated to Ising\nmodel-based simulations. Due to the lack of an analytical or deterministic\nsolution, the Monte Carlo method was employed to explore the model's\nstatistical properties. The results demonstrate that the Ising model is capable\nof replicating the majority of the statistical features analyzed.", "published": "2025-04-26 23:19:44", "link": "http://arxiv.org/abs/2504.19050v1", "categories": ["q-fin.ST"], "primary_category": "q-fin.ST"}
{"title": "On Bitcoin Price Prediction", "abstract": "In recent years, cryptocurrencies have attracted growing attention from both\nprivate investors and institutions. Among them, Bitcoin stands out for its\nimpressive volatility and widespread influence. This paper explores the\npredictability of Bitcoin's price movements, drawing a parallel with\ntraditional financial markets. We examine whether the cryptocurrency market\noperates under the efficient market hypothesis (EMH) or if inefficiencies still\nallow opportunities for arbitrage. Our methodology combines theoretical\nreviews, empirical analyses, machine learning approaches, and time series\nmodeling to assess the extent to which Bitcoin's price can be predicted. We\nfind that while, in general, the Bitcoin market tends toward efficiency,\nspecific conditions, including information asymmetries and behavioral\nanomalies, occasionally create exploitable inefficiencies. However, these\nopportunities remain difficult to systematically identify and leverage. Our\nfindings have implications for both investors and policymakers, particularly\nregarding the regulation of cryptocurrency brokers and derivatives markets.", "published": "2025-04-26 17:48:11", "link": "http://arxiv.org/abs/2504.18982v1", "categories": ["q-fin.ST", "stat.OT"], "primary_category": "q-fin.ST"}
{"title": "Impact of the COVID-19 pandemic on the financial market efficiency of price returns, absolute returns, and volatility increment: Evidence from stock and cryptocurrency markets", "abstract": "This study examines the impact of the coronavirus disease 2019 (COVID-19)\npandemic on market efficiency by analyzing three time series -- price returns,\nabsolute returns, and volatility increments -- in stock (Deutscher Aktienindex,\nNikkei 225, Shanghai Stock Exchange (SSE), and Volatility Index) and\ncryptocurrency (Bitcoin and Ethereum) markets. The effect is found to vary by\nasset class and market. In the stock market, while the pandemic did not\ninfluence the Hurst exponent of volatility increments, it affected that of\nreturns and absolute returns (except in the SSE, where returns remained\nunaffected). In the cryptocurrency market, the pandemic did not alter the Hurst\nexponent for any time series but influenced the strength of multifractality in\nreturns and absolute returns. Some Hurst exponent time series exhibited a\ngradual decline over time, complicating the assessment of pandemic-related\neffects. Consequently, segmented analyses by pandemic periods may erroneously\nsuggest an impact, warranting caution in period-based studies.", "published": "2025-04-26 16:02:54", "link": "http://arxiv.org/abs/2504.18960v1", "categories": ["q-fin.ST"], "primary_category": "q-fin.ST"}
{"title": "Modeling Regime Structure and Informational Drivers of Stock Market Volatility via the Financial Chaos Index", "abstract": "This paper investigates the structural dynamics of stock market volatility\nthrough the Financial Chaos Index, a tensor- and eigenvalue-based measure\ndesigned to capture realized volatility via mutual fluctuations among asset\nprices. Motivated by empirical evidence of regime-dependent volatility behavior\nand perceptual time dilation during financial crises, we develop a\nregime-switching framework based on the Modified Lognormal Power-Law\ndistribution. Analysis of the FCIX from January 1990 to December 2023\nidentifies three distinct market regimes, low-chaos, intermediate-chaos, and\nhigh-chaos, each characterized by differing levels of systemic stress,\nstatistical dispersion and persistence characteristics. Building upon the\nsegmented regime structure, we further examine the informational forces that\nshape forward-looking market expectations. Using sentiment-based predictors\nderived from the Equity Market Volatility tracker, we employ an elastic net\nregression model to forecast implied volatility, as proxied by the VIX index.\nOur findings indicate that shifts in macroeconomic, financial, policy, and\ngeopolitical uncertainty exhibit strong predictive power for volatility\ndynamics across regimes. Together, these results offer a unified empirical\nperspective on how systemic uncertainty governs both the realized evolution of\nfinancial markets and the anticipatory behavior embedded in implied volatility\nmeasures.", "published": "2025-04-26 15:48:11", "link": "http://arxiv.org/abs/2504.18958v1", "categories": ["q-fin.ST", "cs.LG"], "primary_category": "q-fin.ST"}
{"title": "On learning functions over biological sequence space: relating Gaussian process priors, regularization, and gauge fixing", "abstract": "Mappings from biological sequences (DNA, RNA, protein) to quantitative\nmeasures of sequence functionality play an important role in contemporary\nbiology. We are interested in the related tasks of (i) inferring predictive\nsequence-to-function maps and (ii) decomposing sequence-function maps to\nelucidate the contributions of individual subsequences. Because each\nsequence-function map can be written as a weighted sum over subsequences in\nmultiple ways, meaningfully interpreting these weights requires \"gauge-fixing,\"\ni.e., defining a unique representation for each map. Recent work has\nestablished that most existing gauge-fixed representations arise as the unique\nsolutions to $L_2$-regularized regression in an overparameterized \"weight\nspace\" where the choice of regularizer defines the gauge. Here, we establish\nthe relationship between regularized regression in overparameterized weight\nspace and Gaussian process approaches that operate in \"function space,\" i.e.\nthe space of all real-valued functions on a finite set of sequences. We\ndisentangle how weight space regularizers both impose an implicit prior on the\nlearned function and restrict the optimal weights to a particular gauge. We\nalso show how to construct regularizers that correspond to arbitrary explicit\nGaussian process priors combined with a wide variety of gauges. Next, we derive\nthe distribution of gauge-fixed weights implied by the Gaussian process\nposterior and demonstrate that even for long sequences this distribution can be\nefficiently computed for product-kernel priors using a kernel trick. Finally,\nwe characterize the implicit function space priors associated with the most\ncommon weight space regularizers. Overall, our framework unifies and extends\nour ability to infer and interpret sequence-function relationships.", "published": "2025-04-26 22:00:42", "link": "http://arxiv.org/abs/2504.19034v1", "categories": ["cs.LG", "q-bio.GN", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Towards minimax optimal algorithms for Active Simple Hypothesis Testing", "abstract": "We study the Active Simple Hypothesis Testing (ASHT) problem, a simpler\nvariant of the Fixed Budget Best Arm Identification problem. In this work, we\nprovide novel game theoretic formulation of the upper bounds of the ASHT\nproblem. This formulation allows us to leverage tools of differential games and\nPartial Differential Equations (PDEs) to propose an approximately optimal\nalgorithm that is computationally tractable compared to prior work. However,\nthe optimal algorithm still suffers from a curse of dimensionality and instead\nwe use a novel link to Blackwell Approachability to propose an algorithm that\nis far more efficient computationally. We show that this new algorithm,\nalthough not proven to be optimal, is always better than static algorithms in\nall instances of ASHT and is numerically observed to attain the optimal\nexponent in various instances.", "published": "2025-04-26 20:03:53", "link": "http://arxiv.org/abs/2504.19014v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Geometry-aware Active Learning of Spatiotemporal Dynamic Systems", "abstract": "Rapid developments in advanced sensing and imaging have significantly\nenhanced information visibility, opening opportunities for predictive modeling\nof complex dynamic systems. However, sensing signals acquired from such complex\nsystems are often distributed across 3D geometries and rapidly evolving over\ntime, posing significant challenges in spatiotemporal predictive modeling. This\npaper proposes a geometry-aware active learning framework for modeling\nspatiotemporal dynamic systems. Specifically, we propose a geometry-aware\nspatiotemporal Gaussian Process (G-ST-GP) to effectively integrate the temporal\ncorrelations and geometric manifold features for reliable prediction of\nhigh-dimensional dynamic behaviors. In addition, we develop an adaptive active\nlearning strategy to strategically identify informative spatial locations for\ndata collection and further maximize the prediction accuracy. This strategy\nachieves the adaptive trade-off between the prediction uncertainty in the\nG-ST-GP model and the space-filling design guided by the geodesic distance\nacross the 3D geometry. We implement the proposed framework to model the\nspatiotemporal electrodynamics in a 3D heart geometry. Numerical experiments\nshow that our framework outperforms traditional methods lacking the mechanism\nof geometric information incorporation or effective data collection.", "published": "2025-04-26 19:56:38", "link": "http://arxiv.org/abs/2504.19012v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Learning Stochastic Thermodynamics Directly from Correlation and Trajectory-Fluctuation Currents", "abstract": "Markedly increased computational power and data acquisition have led to\ngrowing interest in data-driven inverse dynamics problems. These seek to answer\na fundamental question: What can we learn from time series measurements of a\ncomplex dynamical system? For small systems interacting with external\nenvironments, the effective dynamics are inherently stochastic, making it\ncrucial to properly manage noise in data. Here, we explore this for systems\nobeying Langevin dynamics and, using currents, we construct a learning\nframework for stochastic modeling. Currents have recently gained increased\nattention for their role in bounding entropy production (EP) from thermodynamic\nuncertainty relations (TURs). We introduce a fundamental relationship between\nthe cumulant currents there and standard machine-learning loss functions. Using\nthis, we derive loss functions for several key thermodynamic functions directly\nfrom the system dynamics without the (common) intermediate step of deriving a\nTUR. These loss functions reproduce results derived both from TURs and other\nmethods. More significantly, they open a path to discover new loss functions\nfor previously inaccessible quantities. Notably, this includes access to\nper-trajectory entropy production, even if the observed system is driven far\nfrom its steady-state. We also consider higher order estimation. Our method is\nstraightforward and unifies dynamic inference with recent approaches to entropy\nproduction estimation. Taken altogether, this reveals a deep connection between\ndiffusion models in machine learning and entropy production estimation in\nstochastic thermodynamics.", "published": "2025-04-26 19:42:09", "link": "http://arxiv.org/abs/2504.19007v1", "categories": ["cond-mat.stat-mech", "cond-mat.dis-nn", "cs.LG", "nlin.AO", "stat.ML"], "primary_category": "cond-mat.stat-mech"}
{"title": "Factor Analysis with Correlated Topic Model for Multi-Modal Data", "abstract": "Integrating various data modalities brings valuable insights into underlying\nphenomena. Multimodal factor analysis (FA) uncovers shared axes of variation\nunderlying different simple data modalities, where each sample is represented\nby a vector of features. However, FA is not suited for structured data\nmodalities, such as text or single cell sequencing data, where multiple data\npoints are measured per each sample and exhibit a clustering structure. To\novercome this challenge, we introduce FACTM, a novel, multi-view and\nmulti-structure Bayesian model that combines FA with correlated topic modeling\nand is optimized using variational inference. Additionally, we introduce a\nmethod for rotating latent factors to enhance interpretability with respect to\nbinary features. On text and video benchmarks as well as real-world music and\nCOVID-19 datasets, we demonstrate that FACTM outperforms other methods in\nidentifying clusters in structured data, and integrating them with simple\nmodalities via the inference of shared, interpretable factors.", "published": "2025-04-26 13:02:53", "link": "http://arxiv.org/abs/2504.18914v1", "categories": ["cs.LG", "stat.AP", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Langevin sampling algorithm inspired by the Adam optimizer", "abstract": "We present a framework for adaptive-stepsize MCMC sampling based on\ntime-rescaled Langevin dynamics, in which the stepsize variation is dynamically\ndriven by an additional degree of freedom. Our approach augments the phase\nspace by an additional variable which in turn defines a time\nreparameterization. The use of an auxiliary relaxation equation allows\naccumulation of a moving average of a local monitor function and provides for\nprecise control of the timestep while circumventing the need to modify the\ndrift term in the physical system. Our algorithm is straightforward to\nimplement and can be readily combined with any off-the-peg fixed-stepsize\nLangevin integrator. As a particular example, we consider control of the\nstepsize by monitoring the norm of the log-posterior gradient, which takes\ninspiration from the Adam optimizer, the stepsize being automatically reduced\nin regions of steep change of the log posterior and increased on plateaus,\nimproving numerical stability and convergence speed. As in Adam, the stepsize\nvariation depends on the recent history of the gradient norm, which enhances\nstability and improves accuracy compared to more immediate control approaches.\nWe demonstrate the potential benefit of this method--both in accuracy and in\nstability--in numerical experiments including Neal's funnel and a Bayesian\nneural network for classification of MNIST data.", "published": "2025-04-26 12:57:57", "link": "http://arxiv.org/abs/2504.18911v1", "categories": ["stat.CO", "cs.LG", "stat.ML", "60J22, 62-08, 82C31, 65C05, 65C30, 65C40"], "primary_category": "stat.CO"}
{"title": "ReLU integral probability metric and its applications", "abstract": "We propose a parametric integral probability metric (IPM) to measure the\ndiscrepancy between two probability measures. The proposed IPM leverages a\nspecific parametric family of discriminators, such as single-node neural\nnetworks with ReLU activation, to effectively distinguish between\ndistributions, making it applicable in high-dimensional settings. By optimizing\nover the parameters of the chosen discriminator class, the proposed IPM\ndemonstrates that its estimators have good convergence rates and can serve as a\nsurrogate for other IPMs that use smooth nonparametric discriminator classes.\nWe present an efficient algorithm for practical computation, offering a simple\nimplementation and requiring fewer hyperparameters. Furthermore, we explore its\napplications in various tasks, such as covariate balancing for causal inference\nand fair representation learning. Across such diverse applications, we\ndemonstrate that the proposed IPM provides strong theoretical guarantees, and\nempirical experiments show that it achieves comparable or even superior\nperformance to other methods.", "published": "2025-04-26 11:49:43", "link": "http://arxiv.org/abs/2504.18897v1", "categories": ["stat.ML", "cs.LG", "stat.ME"], "primary_category": "stat.ML"}
{"title": "Nonconvex Linear System Identification with Minimal State Representation", "abstract": "Low-order linear System IDentification (SysID) addresses the challenge of\nestimating the parameters of a linear dynamical system from finite samples of\nobservations and control inputs with minimal state representation. Traditional\napproaches often utilize Hankel-rank minimization, which relies on convex\nrelaxations that can require numerous, costly singular value decompositions\n(SVDs) to optimize. In this work, we propose two nonconvex reformulations to\ntackle low-order SysID (i) Burer-Monterio (BM) factorization of the Hankel\nmatrix for efficient nuclear norm minimization, and (ii) optimizing directly\nover system parameters for real, diagonalizable systems with an atomic norm\nstyle decomposition. These reformulations circumvent the need for repeated\nheavy SVD computations, significantly improving computational efficiency.\nMoreover, we prove that optimizing directly over the system parameters yields\nlower statistical error rates, and lower sample complexities that do not scale\nlinearly with trajectory length like in Hankel-nuclear norm minimization.\nAdditionally, while our proposed formulations are nonconvex, we provide\ntheoretical guarantees of achieving global optimality in polynomial time.\nFinally, we demonstrate algorithms that solve these nonconvex programs and\nvalidate our theoretical claims on synthetic data.", "published": "2025-04-26 04:11:02", "link": "http://arxiv.org/abs/2504.18791v1", "categories": ["eess.SY", "cs.LG", "cs.SY", "eess.SP", "stat.ML"], "primary_category": "eess.SY"}
{"title": "Enhancing Cochlear Implant Signal Coding with Scaled Dot-Product Attention", "abstract": "Cochlear implants (CIs) play a vital role in restoring hearing for\nindividuals with severe to profound sensorineural hearing loss by directly\nstimulating the auditory nerve with electrical signals. While traditional\ncoding strategies, such as the advanced combination encoder (ACE), have proven\neffective, they are constrained by their adaptability and precision. This paper\ninvestigates the use of deep learning (DL) techniques to generate\nelectrodograms for CIs, presenting our model as an advanced alternative. We\ncompared the performance of our model with the ACE strategy by evaluating the\nintelligibility of reconstructed audio signals using the short-time objective\nintelligibility (STOI) metric. The results indicate that our model achieves a\nSTOI score of 0.6031, closely approximating the 0.6126 score of the ACE\nstrategy, and offers potential advantages in flexibility and adaptability. This\nstudy underscores the benefits of incorporating artificial intelligent (AI)\ninto CI technology, such as enhanced personalization and efficiency.", "published": "2025-04-26 22:49:08", "link": "http://arxiv.org/abs/2504.19046v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Pretrained YAMNet for Enhanced Speech Command Detection via Transfer Learning", "abstract": "This work addresses the need for enhanced accuracy and efficiency in speech\ncommand recognition systems, a critical component for improving user\ninteraction in various smart applications. Leveraging the robust pretrained\nYAMNet model and transfer learning, this study develops a method that\nsignificantly improves speech command recognition. We adapt and train a YAMNet\ndeep learning model to effectively detect and interpret speech commands from\naudio signals. Using the extensively annotated Speech Commands dataset\n(speech_commands_v0.01), our approach demonstrates the practical application of\ntransfer learning to accurately recognize a predefined set of speech commands.\nThe dataset is meticulously augmented, and features are strategically extracted\nto boost model performance. As a result, the final model achieved a recognition\naccuracy of 95.28%, underscoring the impact of advanced machine learning\ntechniques on speech command recognition. This achievement marks substantial\nprogress in audio processing technologies and establishes a new benchmark for\nfuture research in the field.", "published": "2025-04-26 21:57:11", "link": "http://arxiv.org/abs/2504.19030v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Survey on Multimodal Music Emotion Recognition", "abstract": "Multimodal music emotion recognition (MMER) is an emerging discipline in\nmusic information retrieval that has experienced a surge in interest in recent\nyears. This survey provides a comprehensive overview of the current\nstate-of-the-art in MMER. Discussing the different approaches and techniques\nused in this field, the paper introduces a four-stage MMER framework, including\nmultimodal data selection, feature extraction, feature processing, and final\nemotion prediction. The survey further reveals significant advancements in deep\nlearning methods and the increasing importance of feature fusion techniques.\nDespite these advancements, challenges such as the need for large annotated\ndatasets, datasets with more modalities, and real-time processing capabilities\nremain. This paper also contributes to the field by identifying critical gaps\nin current research and suggesting potential directions for future research.\nThe gaps underscore the importance of developing robust, scalable, a\ninterpretable models for MMER, with implications for applications in music\nrecommendation systems, therapeutic tools, and entertainment.", "published": "2025-04-26 05:11:04", "link": "http://arxiv.org/abs/2504.18799v1", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Unveiling and Mitigating Adversarial Vulnerabilities in Iterative Optimizers", "abstract": "Machine learning (ML) models are often sensitive to carefully crafted yet\nseemingly unnoticeable perturbations. Such adversarial examples are considered\nto be a property of ML models, often associated with their black-box operation\nand sensitivity to features learned from data. This work examines the\nadversarial sensitivity of non-learned decision rules, and particularly of\niterative optimizers. Our analysis is inspired by the recent developments in\ndeep unfolding, which cast such optimizers as ML models. We show that\nnon-learned iterative optimizers share the sensitivity to adversarial examples\nof ML models, and that attacking iterative optimizers effectively alters the\noptimization objective surface in a manner that modifies the minima sought. We\nthen leverage the ability to cast iteration-limited optimizers as ML models to\nenhance robustness via adversarial training. For a class of proximal gradient\noptimizers, we rigorously prove how their learning affects adversarial\nsensitivity. We numerically back our findings, showing the vulnerability of\nvarious optimizers, as well as the robustness induced by unfolding and\nadversarial training.", "published": "2025-04-26 19:03:54", "link": "http://arxiv.org/abs/2504.19000v1", "categories": ["cs.LG", "eess.SP"], "primary_category": "cs.LG"}
{"title": "Two-Agent DRL for Power Allocation and IRS Orientation in Dynamic NOMA-based OWC Networks", "abstract": "Intelligent reflecting surfaces (IRSs) technology has been considered a\npromising solution in visible light communication (VLC) systems due to its\npotential to overcome the line-of-sight (LoS) blockage issue and enhance\ncoverage. Moreover, integrating IRS with a downlink non-orthogonal multiple\naccess (NOMA) transmission technique for multi-users is a smart solution to\nachieve a high sum rate and improve system performance. In this paper, a\ndynamic IRS-assisted NOMA-VLC system is modeled, and an optimization problem is\nformulated to maximize sum energy efficiency (SEE) and fairness among multiple\nmobile users under power allocation and IRS mirror orientation constraints. Due\nto the non-convex nature of the optimization problem and the non-linearity of\nthe constraints, conventional optimization methods are impractical for\nreal-time solutions. Therefore, a two-agent deep reinforcement learning (DRL)\nalgorithm is designed for optimizing power allocation and IRS orientation based\non centralized training with decentralized execution to obtain fast and\nreal-time solutions in dynamic environments. The results show the superior\nperformance of the proposed DRL algorithm compared to standard DRL algorithms\ntypically used for resource allocation in wireless communication. The results\nalso show that the proposed DRL algorithm achieves higher performance compared\nto deployments without IRS and with randomly oriented IRS elements.", "published": "2025-04-26 14:43:34", "link": "http://arxiv.org/abs/2504.18937v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "BEM-Assisted Low-Complexity Channel Estimation for AFDM Systems over Doubly Selective Channels", "abstract": "In this paper, we propose a low-complexity channel estimation scheme of\naffine frequency division multiplexing (AFDM) based on generalized complex\nexponential basis expansion model (GCE-BEM) over doubly selective channels. The\nGCE-BEM is used to solve fractional Doppler dispersion while significantly\nreducing the computational complexity of exhaustive search. Then, the\nclosed-form expression of channel estimation error is derived for the minimum\nmean square error (MMSE) estimation algorithm. Based on the estimated channel,\nthe MMSE detection is adopt to characterize the impacts of estimated channel on\nbit error rate (BER) by deriving the theoretical lower bound. Finally,\nnumerical results demonstrate that the proposed scheme effectively mitigates\nsevere inter-Doppler interference (IDoI). Our theoretical performance an alysis\ncan perfectly match the Monte-Carlo results, validating the effectiveness of\nour proposed channel estimation based on GCE-BEM.", "published": "2025-04-26 12:15:21", "link": "http://arxiv.org/abs/2504.18901v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Tracking-Aided Multi-User MIMO Communications with Hybrid Reconfigurable Intelligent Surfaces", "abstract": "Hybrid Reconfigurable Intelligent Surfaces (HRISs) constitute a new paradigm\nthat redefines smart metasurfaces, not only offering tunable reflections of\nincoming signals, but also incorporating signal reception and processing\ncapabilities. In this paper, leveraging the simultaneous dual-functionality of\nHRISs, we propose a novel framework for tracking-aided multi-user\nMultiple-Input Multiple-Output (MIMO) communications. In particular, a joint\ndesign of the transmit multi-user precoding matrix together with the HRIS\nreflection and analog combining configurations is presented, with the objective\nto maximize the accuracy of position estimation of multiple mobile users while\nmeeting their individual quality-of-service constraints for sensing-aided\ncommunications. The Cramer-Rao bound for the users' positioning parameters is\nderived together with a prediction approach based on the extended Kalman\nfilter. Our simulation results showcase the efficacy of the proposed Integrated\nSensing And Communications (ISAC) framework over various system configuration\nparameters.", "published": "2025-04-26 08:18:37", "link": "http://arxiv.org/abs/2504.18846v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "DMA Reception for Simultaneous Area-Wide Sensing and Multi-User Uplink Communications", "abstract": "The recent surge in deploying extremely large antenna arrays is expected to\nplay a vital role in future sixth generation wireless networks, enabling\nadvanced radar target localization with enhanced angular and range resolution.\nThis paper focuses on the promising technology of Dynamic Metasurface Antennas\n(DMAs), integrating numerous sub-wavelength-spaced metamaterials within a\nsingle aperture, and presents a novel framework for designing its analog\nreception beamforming weights with the goal to optimize sensing performance\nwithin a spatial Area of Interest (AoI), while simultaneously guaranteeing\ndesired multi-user uplink communication performance. We derive the Cramer-Rao\nBound (CRB) with DMA-based reception for both passive and active radar targets\nlying inside the AoI, which is then used as the optimization objective for\nconfiguring the discrete tunable phases of the metamaterials. Capitalizing on\nthe DMA partially-connected architecture, we formulate the design problem as\nconvex optimization and present both direct CRB minimization approaches and low\ncomplexity alternatives using a lower-bound approximation. Simulation results\nacross various scenarios validate the effectiveness of the proposed framework,\nshowing it consistently outperforms existing state-of-the-art methods.", "published": "2025-04-26 08:12:19", "link": "http://arxiv.org/abs/2504.18843v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Speaker Retrieval in the Wild: Challenges, Effectiveness and Robustness", "abstract": "There is a growing abundance of publicly available or company-owned\naudio/video archives, highlighting the increasing importance of efficient\naccess to desired content and information retrieval from these archives. This\npaper investigates the challenges, solutions, effectiveness, and robustness of\nspeaker retrieval systems developed \"in the wild\" which involves addressing two\nprimary challenges: extraction of task-relevant labels from limited metadata\nfor system development and evaluation, as well as the unconstrained acoustic\nconditions encountered in the archive, ranging from quiet studios to adverse\nnoisy environments. While we focus on the publicly-available BBC Rewind archive\n(spanning 1948 to 1979), our framework addresses the broader issue of speaker\nretrieval on extensive and possibly aged archives with no control over the\ncontent and acoustic conditions. Typically, these archives offer a brief and\ngeneral file description, mostly inadequate for specific applications like\nspeaker retrieval, and manual annotation of such large-scale archives is\nunfeasible. We explore various aspects of system development (e.g., speaker\ndiarisation, embedding extraction, query selection) and analyse the challenges,\npossible solutions, and their functionality. To evaluate the performance, we\nconduct systematic experiments in both clean setup and against various\ndistortions simulating real-world applications. Our findings demonstrate the\neffectiveness and robustness of the developed speaker retrieval systems,\nestablishing the versatility and scalability of the proposed framework for a\nwide range of applications beyond the BBC Rewind corpus.", "published": "2025-04-26 15:21:14", "link": "http://arxiv.org/abs/2504.18950v2", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MATCHA: Can Multi-Agent Collaboration Build a Trustworthy Conversational Recommender?", "abstract": "In this paper, we propose a multi-agent collaboration framework called MATCHA\nfor conversational recommendation system, leveraging large language models\n(LLMs) to enhance personalization and user engagement. Users can request\nrecommendations via free-form text and receive curated lists aligned with their\ninterests, preferences, and constraints. Our system introduces specialized\nagents for intent analysis, candidate generation, ranking, re-ranking,\nexplainability, and safeguards. These agents collaboratively improve\nrecommendations accuracy, diversity, and safety. On eight metrics, our model\nachieves superior or comparable performance to the current state-of-the-art.\nThrough comparisons with six baseline models, our approach addresses key\nchallenges in conversational recommendation systems for game recommendations,\nincluding: (1) handling complex, user-specific requests, (2) enhancing\npersonalization through multi-agent collaboration, (3) empirical evaluation and\ndeployment, and (4) ensuring safe and trustworthy interactions.", "published": "2025-04-26 00:55:43", "link": "http://arxiv.org/abs/2504.20094v1", "categories": ["cs.IR", "cs.CL", "cs.HC"], "primary_category": "cs.IR"}
{"title": "Decoding Latent Spaces: Assessing the Interpretability of Time Series Foundation Models for Visual Analytics", "abstract": "The present study explores the interpretability of latent spaces produced by\ntime series foundation models, focusing on their potential for visual analysis\ntasks. Specifically, we evaluate the MOMENT family of models, a set of\ntransformer-based, pre-trained architectures for multivariate time series tasks\nsuch as: imputation, prediction, classification, and anomaly detection. We\nevaluate the capacity of these models on five datasets to capture the\nunderlying structures in time series data within their latent space projection\nand validate whether fine tuning improves the clarity of the resulting\nembedding spaces. Notable performance improvements in terms of loss reduction\nwere observed after fine tuning. Visual analysis shows limited improvement in\nthe interpretability of the embeddings, requiring further work. Results suggest\nthat, although Time Series Foundation Models such as MOMENT are robust, their\nlatent spaces may require additional methodological refinements to be\nadequately interpreted, such as alternative projection techniques, loss\nfunctions, or data preprocessing strategies. Despite the limitations of MOMENT,\nfoundation models supose a big reduction in execution time and so a great\nadvance for interactive visual analytics.", "published": "2025-04-26 17:24:41", "link": "http://arxiv.org/abs/2504.20099v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Advancing Multi-Agent Systems Through Model Context Protocol: Architecture, Implementation, and Applications", "abstract": "Multi-agent systems represent a significant advancement in artificial\nintelligence, enabling complex problem-solving through coordinated specialized\nagents. However, these systems face fundamental challenges in context\nmanagement, coordination efficiency, and scalable operation. This paper\nintroduces a comprehensive framework for advancing multi-agent systems through\nModel Context Protocol (MCP), addressing these challenges through standardized\ncontext sharing and coordination mechanisms. We extend previous work on AI\nagent architectures by developing a unified theoretical foundation, advanced\ncontext management techniques, and scalable coordination patterns. Through\ndetailed implementation case studies across enterprise knowledge management,\ncollaborative research, and distributed problem-solving domains, we demonstrate\nsignificant performance improvements compared to traditional approaches. Our\nevaluation methodology provides a systematic assessment framework with\nbenchmark tasks and datasets specifically designed for multi-agent systems. We\nidentify current limitations, emerging research opportunities, and potential\ntransformative applications across industries. This work contributes to the\nevolution of more capable, collaborative, and context-aware artificial\nintelligence systems that can effectively address complex real-world\nchallenges.", "published": "2025-04-26 03:43:03", "link": "http://arxiv.org/abs/2504.21030v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
