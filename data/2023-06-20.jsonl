{"title": "GUMSum: Multi-Genre Data and Evaluation for English Abstractive\n  Summarization", "abstract": "Automatic summarization with pre-trained language models has led to\nimpressively fluent results, but is prone to 'hallucinations', low performance\non non-news genres, and outputs which are not exactly summaries. Targeting ACL\n2023's 'Reality Check' theme, we present GUMSum, a small but carefully crafted\ndataset of English summaries in 12 written and spoken genres for evaluation of\nabstractive summarization. Summaries are highly constrained, focusing on\nsubstitutive potential, factuality, and faithfulness. We present guidelines and\nevaluate human agreement as well as subjective judgments on recent system\noutputs, comparing general-domain untuned approaches, a fine-tuned one, and a\nprompt-based approach, to human performance. Results show that while GPT3\nachieves impressive scores, it still underperforms humans, with varying quality\nacross genres. Human judgments reveal different types of errors in supervised,\nprompted, and human-generated summaries, shedding light on the challenges of\nproducing a good summary.", "published": "2023-06-20 03:21:10", "link": "http://arxiv.org/abs/2306.11256v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Novel Counterfactual Data Augmentation Method for Aspect-Based\n  Sentiment Analysis", "abstract": "Aspect-based-sentiment-analysis (ABSA) is a fine-grained sentiment evaluation\ntask, which analyzes the emotional polarity of the evaluation aspects.\nGenerally, the emotional polarity of an aspect exists in the corresponding\nopinion expression, whose diversity has great impact on model's performance. To\nmitigate this problem, we propose a novel and simple counterfactual data\naugmentation method to generate opinion expressions with reversed sentiment\npolarity. In particular, the integrated gradients are calculated to locate and\nmask the opinion expression. Then, a prompt combined with the reverse\nexpression polarity is added to the original text, and a Pre-trained language\nmodel (PLM), T5, is finally was employed to predict the masks. The experimental\nresults shows the proposed counterfactual data augmentation method performs\nbetter than current augmentation methods on three ABSA datasets, i.e. Laptop,\nRestaurant, and MAMS.", "published": "2023-06-20 03:25:51", "link": "http://arxiv.org/abs/2306.11260v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Did the Models Understand Documents? Benchmarking Models for Language\n  Understanding in Document-Level Relation Extraction", "abstract": "Document-level relation extraction (DocRE) attracts more research interest\nrecently. While models achieve consistent performance gains in DocRE, their\nunderlying decision rules are still understudied: Do they make the right\npredictions according to rationales? In this paper, we take the first step\ntoward answering this question and then introduce a new perspective on\ncomprehensively evaluating a model. Specifically, we first conduct annotations\nto provide the rationales considered by humans in DocRE. Then, we conduct\ninvestigations and reveal the fact that: In contrast to humans, the\nrepresentative state-of-the-art (SOTA) models in DocRE exhibit different\ndecision rules. Through our proposed RE-specific attacks, we next demonstrate\nthat the significant discrepancy in decision rules between models and humans\nseverely damages the robustness of models and renders them inapplicable to\nreal-world RE scenarios. After that, we introduce mean average precision (MAP)\nto evaluate the understanding and reasoning capabilities of models. According\nto the extensive experimental results, we finally appeal to future work to\nconsider evaluating both performance and the understanding ability of models\nfor the development of their applications. We make our annotations and code\npublicly available.", "published": "2023-06-20 08:52:05", "link": "http://arxiv.org/abs/2306.11386v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Evaluating Multilingual Compositional Generalization with Translated\n  Datasets", "abstract": "Compositional generalization allows efficient learning and human-like\ninductive biases. Since most research investigating compositional\ngeneralization in NLP is done on English, important questions remain\nunderexplored. Do the necessary compositional generalization abilities differ\nacross languages? Can models compositionally generalize cross-lingually? As a\nfirst step to answering these questions, recent work used neural machine\ntranslation to translate datasets for evaluating compositional generalization\nin semantic parsing. However, we show that this entails critical semantic\ndistortion. To address this limitation, we craft a faithful rule-based\ntranslation of the MCWQ dataset from English to Chinese and Japanese. Even with\nthe resulting robust benchmark, which we call MCWQ-R, we show that the\ndistribution of compositions still suffers due to linguistic divergences, and\nthat multilingual models still struggle with cross-lingual compositional\ngeneralization. Our dataset and methodology will be useful resources for the\nstudy of cross-lingual compositional generalization in other tasks.", "published": "2023-06-20 10:03:57", "link": "http://arxiv.org/abs/2306.11420v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Blackbird language matrices (BLM), a new task for rule-like\n  generalization in neural networks: Motivations and Formal Specifications", "abstract": "We motivate and formally define a new task for fine-tuning rule-like\ngeneralization in large language models. It is conjectured that the\nshortcomings of current LLMs are due to a lack of ability to generalize. It has\nbeen argued that, instead, humans are better at generalization because they\nhave a tendency at extracting rules from complex data. We try to recreate this\ntendency to rule-based generalization. When exposed to tests of analytic\nintelligence, for example, the visual RAVEN IQ test, human problem-solvers\nidentify the relevant objects in the picture and their relevant attributes and\nreason based on rules applied to these objects and attributes. Based on the\ninduced rules, they are able to provide a solution to the test. We propose a\ntask that translates this IQ task into language. In this paper, we provide the\nformal specification for the task and the generative process of its datasets.", "published": "2023-06-20 10:45:56", "link": "http://arxiv.org/abs/2306.11444v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CATS: A Pragmatic Chinese Answer-to-Sequence Dataset with Large Scale\n  and High Quality", "abstract": "There are three problems existing in the popular data-to-text datasets.\nFirst, the large-scale datasets either contain noise or lack real application\nscenarios. Second, the datasets close to real applications are relatively small\nin size. Last, current datasets bias in the English language while leaving\nother languages underexplored. To alleviate these limitations, in this paper,\nwe present CATS, a pragmatic Chinese answer-to-sequence dataset with large\nscale and high quality. The dataset aims to generate textual descriptions for\nthe answer in the practical TableQA system. Further, to bridge the structural\ngap between the input SQL and table and establish better semantic alignments,\nwe propose a Unified Graph Transformation approach to establish a joint\nencoding space for the two hybrid knowledge resources and convert this task to\na graph-to-text problem. The experiment results demonstrate the effectiveness\nof our proposed method. Further analysis on CATS attests to both the high\nquality and challenges of the dataset.", "published": "2023-06-20 12:02:26", "link": "http://arxiv.org/abs/2306.11477v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explicit Syntactic Guidance for Neural Text Generation", "abstract": "Most existing text generation models follow the sequence-to-sequence\nparadigm. Generative Grammar suggests that humans generate natural language\ntexts by learning language grammar. We propose a syntax-guided generation\nschema, which generates the sequence guided by a constituency parse tree in a\ntop-down direction. The decoding process can be decomposed into two parts: (1)\npredicting the infilling texts for each constituent in the lexicalized syntax\ncontext given the source sentence; (2) mapping and expanding each constituent\nto construct the next-level syntax context. Accordingly, we propose a\nstructural beam search method to find possible syntax structures\nhierarchically. Experiments on paraphrase generation and machine translation\nshow that the proposed method outperforms autoregressive baselines, while also\ndemonstrating effectiveness in terms of interpretability, controllability, and\ndiversity.", "published": "2023-06-20 12:16:31", "link": "http://arxiv.org/abs/2306.11485v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "One model to rule them all: ranking Slovene summarizers", "abstract": "Text summarization is an essential task in natural language processing, and\nresearchers have developed various approaches over the years, ranging from\nrule-based systems to neural networks. However, there is no single model or\napproach that performs well on every type of text. We propose a system that\nrecommends the most suitable summarization model for a given text. The proposed\nsystem employs a fully connected neural network that analyzes the input content\nand predicts which summarizer should score the best in terms of ROUGE score for\na given input. The meta-model selects among four different summarization\nmodels, developed for the Slovene language, using different properties of the\ninput, in particular its Doc2Vec document representation. The four Slovene\nsummarization models deal with different challenges associated with text\nsummarization in a less-resourced language. We evaluate the proposed SloMetaSum\nmodel performance automatically and parts of it manually. The results show that\nthe system successfully automates the step of manually selecting the best\nmodel.", "published": "2023-06-20 13:12:58", "link": "http://arxiv.org/abs/2306.11518v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hallucination is the last thing you need", "abstract": "The legal profession necessitates a multidimensional approach that involves\nsynthesizing an in-depth comprehension of a legal issue with insightful\ncommentary based on personal experience, combined with a comprehensive\nunderstanding of pertinent legislation, regulation, and case law, in order to\ndeliver an informed legal solution. The present offering with generative AI\npresents major obstacles in replicating this, as current models struggle to\nintegrate and navigate such a complex interplay of understanding, experience,\nand fact-checking procedures. It is noteworthy that where generative AI outputs\nunderstanding and experience, which reflect the aggregate of various subjective\nviews on similar topics, this often deflects the model's attention from the\ncrucial legal facts, thereby resulting in hallucination. Hence, this paper\ndelves into the feasibility of three independent LLMs, each focused on\nunderstanding, experience, and facts, synthesising as one single ensemble model\nto effectively counteract the current challenges posed by the existing\nmonolithic generative AI models. We introduce an idea of mutli-length\ntokenisation to protect key information assets like common law judgements, and\nfinally we interrogate the most advanced publicly available models for legal\nhallucination, with some interesting results.", "published": "2023-06-20 13:14:15", "link": "http://arxiv.org/abs/2306.11520v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Ecological Fallacy in Annotation: Modelling Human Label Variation\n  goes beyond Sociodemographics", "abstract": "Many NLP tasks exhibit human label variation, where different annotators give\ndifferent labels to the same texts. This variation is known to depend, at least\nin part, on the sociodemographics of annotators. Recent research aims to model\nindividual annotator behaviour rather than predicting aggregated labels, and we\nwould expect that sociodemographic information is useful for these models. On\nthe other hand, the ecological fallacy states that aggregate group behaviour,\nsuch as the behaviour of the average female annotator, does not necessarily\nexplain individual behaviour. To account for sociodemographics in models of\nindividual annotator behaviour, we introduce group-specific layers to\nmulti-annotator models. In a series of experiments for toxic content detection,\nwe find that explicitly accounting for sociodemographic attributes in this way\ndoes not significantly improve model performance. This result shows that\nindividual annotation behaviour depends on much more than just\nsociodemographics.", "published": "2023-06-20 14:23:32", "link": "http://arxiv.org/abs/2306.11559v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FAIR: A Causal Framework for Accurately Inferring Judgments Reversals", "abstract": "Artificial intelligence researchers have made significant advances in legal\nintelligence in recent years. However, the existing studies have not focused on\nthe important value embedded in judgments reversals, which limits the\nimprovement of the efficiency of legal intelligence. In this paper, we propose\na causal Framework for Accurately Inferring case Reversals (FAIR), which models\nthe problem of judgments reversals based on real Chinese judgments. We mine the\ncauses of judgments reversals by causal inference methods and inject the\nobtained causal relationships into the neural network as a priori knowledge.\nAnd then, our framework is validated on a challenging dataset as a legal\njudgment prediction task. The experimental results show that our framework can\ntap the most critical factors in judgments reversal, and the obtained causal\nrelationships can effectively improve the neural network's performance. In\naddition, we discuss the generalization ability of large language models for\nlegal intelligence tasks using ChatGPT as an example. Our experiment has found\nthat the generalization ability of large language models still has defects, and\nmining causal relationships can effectively improve the accuracy and explain\nability of model predictions.", "published": "2023-06-20 15:02:25", "link": "http://arxiv.org/abs/2306.11585v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EvolveMT: an Ensemble MT Engine Improving Itself with Usage Only", "abstract": "This paper presents EvolveMT for efficiently combining multiple machine\ntranslation (MT) engines. The proposed system selects the output from a single\nengine for each segment by utilizing online learning techniques to predict the\nmost suitable system for every translation request. A neural quality estimation\nmetric supervises the method without requiring reference translations. The\nonline learning capability of this system allows for dynamic adaptation to\nalterations in the domain or machine translation engines, thereby obviating the\nnecessity for additional training. EvolveMT selects a subset of translation\nengines to be called based on the source sentence features. The degree of\nexploration is configurable according to the desired quality-cost trade-off.\nResults from custom datasets demonstrate that EvolveMT achieves similar\ntranslation accuracy at a lower cost than selecting the best translation of\neach segment from all translations using an MT quality estimator. To our\nknowledge, EvolveMT is the first meta MT system that adapts itself after\ndeployment to incoming translation requests from the production environment\nwithout needing costly retraining on human feedback.", "published": "2023-06-20 18:32:30", "link": "http://arxiv.org/abs/2306.11823v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DiNADO: Norm-Disentangled Neurally-Decomposed Oracles for Controlling\n  Language Models", "abstract": "NeurAlly-Decomposed Oracle (NADO) is a powerful approach for controllable\ngeneration with large language models. It is designed to avoid catastrophic\nforgetting while achieving guaranteed convergence to an entropy-maximized\nclosed-form optimal solution with reasonable modeling capacity. Despite the\nsuccess, several challenges arise when apply NADO to a wide range of scenarios.\nVanilla NADO suffers from gradient vanishing for low-probability control\nsignals and is highly reliant on a regularization to satisfy the stochastic\nversion of Bellman equation. In addition, the vanilla implementation of NADO\nintroduces a few additional transformer layers, suffering from a limited\ncapacity especially compared to other finetune-based model adaptation methods\nlike LoRA. In this paper, we propose a improved version of the NADO algorithm,\nnamely DiNADO (norm-Disentangled NeurAlly-Decomposed Oracles), which improves\nthe performance of the NADO algorithm through disentangling the step-wise\nglobal norm over the approximated oracle $R$-value for all potential\nnext-tokens, allowing DiNADO to be combined with finetuning methods like LoRA.\nWe discuss in depth how DiNADO achieves better capacity, stability and\nflexibility with both empirical and theoretical results. Experiments on\nformality control in machine translation and the lexically constrained\ngeneration task CommonGen demonstrates the significance of the improvements.", "published": "2023-06-20 18:36:52", "link": "http://arxiv.org/abs/2306.11825v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Machine Translation Corpus Generation", "abstract": "This paper proposes an efficient and semi-automated method for\nhuman-in-the-loop post-editing for machine translation (MT) corpus generation.\nThe method is based on online training of a custom MT quality estimation metric\non-the-fly as linguists perform post-edits. The online estimator is used to\nprioritize worse hypotheses for post-editing, and auto-close best hypotheses\nwithout post-editing. This way, significant improvements can be achieved in the\nresulting quality of post-edits at a lower cost due to reduced human\ninvolvement. The trained estimator can also provide an online sanity check\nmechanism for post-edits and remove the need for additional linguists to review\nthem or work on the same hypotheses. In this paper, the effect of prioritizing\nwith the proposed method on the resulting MT corpus quality is presented versus\nscheduling hypotheses randomly. As demonstrated by experiments, the proposed\nmethod improves the lifecycle of MT models by focusing the linguist effort on\nproduction samples and hypotheses, which matter most for expanding MT corpora\nto be used for re-training them.", "published": "2023-06-20 18:46:47", "link": "http://arxiv.org/abs/2306.11838v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Open-Domain Text Evaluation via Contrastive Distribution Methods", "abstract": "Recent advancements in open-domain text generation, driven by the power of\nlarge pre-trained language models (LLMs), have demonstrated remarkable\nperformance. However, assessing these models' generation quality remains a\nchallenge. In this paper, we introduce a novel method for evaluating\nopen-domain text generation called Contrastive Distribution Methods (CDM).\nLeveraging the connection between increasing model parameters and enhanced LLM\nperformance, CDM creates a mapping from the _contrast_ of two probabilistic\ndistributions -- one known to be superior to the other -- to quality measures.\nWe investigate CDM for open-domain text generation evaluation under two\nparadigms: 1) _Generative_ CDM, which harnesses the contrast of two language\nmodels' distributions to generate synthetic examples for training\ndiscriminator-based metrics; 2) _Discriminative_ CDM, which directly uses\ndistribution disparities between two language models for evaluation. Our\nexperiments on coherence evaluation for multi-turn dialogue and commonsense\nevaluation for controllable generation demonstrate CDM's superior correlate\nwith human judgment than existing automatic evaluation metrics, highlighting\nthe strong performance and generalizability of our approach.", "published": "2023-06-20 20:37:54", "link": "http://arxiv.org/abs/2306.11879v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring New Frontiers in Agricultural NLP: Investigating the Potential\n  of Large Language Models for Food Applications", "abstract": "This paper explores new frontiers in agricultural natural language processing\nby investigating the effectiveness of using food-related text corpora for\npretraining transformer-based language models. In particular, we focus on the\ntask of semantic matching, which involves establishing mappings between food\ndescriptions and nutrition data. To accomplish this, we fine-tune a pre-trained\ntransformer-based language model, AgriBERT, on this task, utilizing an external\nsource of knowledge, such as the FoodOn ontology. To advance the field of\nagricultural NLP, we propose two new avenues of exploration: (1) utilizing\nGPT-based models as a baseline and (2) leveraging ChatGPT as an external source\nof knowledge. ChatGPT has shown to be a strong baseline in many NLP tasks, and\nwe believe it has the potential to improve our model in the task of semantic\nmatching and enhance our model's understanding of food-related concepts and\nrelationships. Additionally, we experiment with other applications, such as\ncuisine prediction based on food ingredients, and expand the scope of our\nresearch to include other NLP tasks beyond semantic matching. Overall, this\npaper provides promising avenues for future research in this field, with\npotential implications for improving the performance of agricultural NLP\napplications.", "published": "2023-06-20 21:12:16", "link": "http://arxiv.org/abs/2306.11892v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation of Chinese-English Machine Translation of Emotion-Loaded\n  Microblog Texts: A Human Annotated Dataset for the Quality Assessment of\n  Emotion Translation", "abstract": "In this paper, we focus on how current Machine Translation (MT) tools perform\non the translation of emotion-loaded texts by evaluating outputs from Google\nTranslate according to a framework proposed in this paper. We propose this\nevaluation framework based on the Multidimensional Quality Metrics (MQM) and\nperform a detailed error analysis of the MT outputs. From our analysis, we\nobserve that about 50% of the MT outputs fail to preserve the original emotion.\nAfter further analysis of the errors, we find that emotion carrying words and\nlinguistic phenomena such as polysemous words, negation, abbreviation etc., are\ncommon causes for these translation errors.", "published": "2023-06-20 21:22:45", "link": "http://arxiv.org/abs/2306.11900v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LoSparse: Structured Compression of Large Language Models based on\n  Low-Rank and Sparse Approximation", "abstract": "Transformer models have achieved remarkable results in various natural\nlanguage tasks, but they are often prohibitively large, requiring massive\nmemories and computational resources. To reduce the size and complexity of\nthese models, we propose LoSparse (Low-Rank and Sparse approximation), a novel\nmodel compression technique that approximates a weight matrix by the sum of a\nlow-rank matrix and a sparse matrix. Our method combines the advantages of both\nlow-rank approximations and pruning, while avoiding their limitations. Low-rank\napproximation compresses the coherent and expressive parts in neurons, while\npruning removes the incoherent and non-expressive parts in neurons. Pruning\nenhances the diversity of low-rank approximations, and low-rank approximation\nprevents pruning from losing too many expressive neurons. We evaluate our\nmethod on natural language understanding, question answering, and natural\nlanguage generation tasks. We show that it significantly outperforms existing\ncompression methods.", "published": "2023-06-20 01:16:11", "link": "http://arxiv.org/abs/2306.11222v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "HK-LegiCoST: Leveraging Non-Verbatim Transcripts for Speech Translation", "abstract": "We introduce HK-LegiCoST, a new three-way parallel corpus of\nCantonese-English translations, containing 600+ hours of Cantonese audio, its\nstandard traditional Chinese transcript, and English translation, segmented and\naligned at the sentence level. We describe the notable challenges in corpus\npreparation: segmentation, alignment of long audio recordings, and\nsentence-level alignment with non-verbatim transcripts. Such transcripts make\nthe corpus suitable for speech translation research when there are significant\ndifferences between the spoken and written forms of the source language. Due to\nits large size, we are able to demonstrate competitive speech translation\nbaselines on HK-LegiCoST and extend them to promising cross-corpus results on\nthe FLEURS Cantonese subset. These results deliver insights into speech\nrecognition and translation research in languages for which non-verbatim or\n``noisy'' transcription is common due to various factors, including vernacular\nand dialectal speech.", "published": "2023-06-20 03:09:32", "link": "http://arxiv.org/abs/2306.11252v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating the Zero-shot Robustness of Instruction-tuned Language Models", "abstract": "Instruction fine-tuning has recently emerged as a promising approach for\nimproving the zero-shot capabilities of Large Language Models (LLMs) on new\ntasks. This technique has shown particular strength in improving the\nperformance of modestly sized LLMs, sometimes inducing performance competitive\nwith much larger model variants. In this paper we ask two questions: (1) How\nsensitive are instruction-tuned models to the particular phrasings of\ninstructions, and, (2) How can we make them more robust to such natural\nlanguage variation? To answer the former, we collect a set of 319 instructions\nmanually written by NLP practitioners for over 80 unique tasks included in\nwidely used benchmarks, and we evaluate the variance and average performance of\nthese instructions as compared to instruction phrasings observed during\ninstruction fine-tuning. We find that using novel (unobserved) but appropriate\ninstruction phrasings consistently degrades model performance, sometimes\nsubstantially so. Further, such natural instructions yield a wide variance in\ndownstream performance, despite their semantic equivalence. Put another way,\ninstruction-tuned models are not especially robust to instruction re-phrasings.\nWe propose a simple method to mitigate this issue by introducing ``soft\nprompt'' embedding parameters and optimizing these to maximize the similarity\nbetween representations of semantically equivalent instructions. We show that\nthis method consistently improves the robustness of instruction-tuned models.", "published": "2023-06-20 03:48:51", "link": "http://arxiv.org/abs/2306.11270v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "KiUT: Knowledge-injected U-Transformer for Radiology Report Generation", "abstract": "Radiology report generation aims to automatically generate a clinically\naccurate and coherent paragraph from the X-ray image, which could relieve\nradiologists from the heavy burden of report writing. Although various image\ncaption methods have shown remarkable performance in the natural image field,\ngenerating accurate reports for medical images requires knowledge of multiple\nmodalities, including vision, language, and medical terminology. We propose a\nKnowledge-injected U-Transformer (KiUT) to learn multi-level visual\nrepresentation and adaptively distill the information with contextual and\nclinical knowledge for word prediction. In detail, a U-connection schema\nbetween the encoder and decoder is designed to model interactions between\ndifferent modalities. And a symptom graph and an injected knowledge distiller\nare developed to assist the report generation. Experimentally, we outperform\nstate-of-the-art methods on two widely used benchmark datasets: IU-Xray and\nMIMIC-CXR. Further experimental results prove the advantages of our\narchitecture and the complementary benefits of the injected knowledge.", "published": "2023-06-20 07:27:28", "link": "http://arxiv.org/abs/2306.11345v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Visually grounded few-shot word learning in low-resource settings", "abstract": "We propose a visually grounded speech model that learns new words and their\nvisual depictions from just a few word-image example pairs. Given a set of test\nimages and a spoken query, we ask the model which image depicts the query word.\nPrevious work has simplified this few-shot learning problem by either using an\nartificial setting with digit word-image pairs or by using a large number of\nexamples per class. Moreover, all previous studies were performed using English\nspeech-image data. We propose an approach that can work on natural word-image\npairs but with less examples, i.e. fewer shots, and then illustrate how this\napproach can be applied for multimodal few-shot learning in a real low-resource\nlanguage, Yor\\`ub\\'a. Our approach involves using the given word-image example\npairs to mine new unsupervised word-image training pairs from large collections\nof unlabelled speech and images. Additionally, we use a word-to-image attention\nmechanism to determine word-image similarity. With this new model, we achieve\nbetter performance with fewer shots than previous approaches on an existing\nEnglish benchmark. Many of the model's mistakes are due to confusion between\nvisual concepts co-occurring in similar contexts. The experiments on Yor\\`ub\\'a\nshow the benefit of transferring knowledge from a multimodal model trained on a\nlarger set of English speech-image data.", "published": "2023-06-20 08:27:42", "link": "http://arxiv.org/abs/2306.11371v3", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Democratizing LLMs for Low-Resource Languages by Leveraging their\n  English Dominant Abilities with Linguistically-Diverse Prompts", "abstract": "Large language models (LLMs) are known to effectively perform tasks by simply\nobserving few exemplars. However, in low-resource languages, obtaining such\nhand-picked exemplars can still be challenging, where unsupervised techniques\nmay be necessary. Moreover, competent generative capabilities of LLMs are\nobserved only in high-resource languages, while their performances among\nunder-represented languages fall behind due to pre-training data imbalance. To\nelicit LLMs' ability onto low-resource languages without any supervised data,\nwe propose to assemble synthetic exemplars from a diverse set of high-resource\nlanguages to prompt the LLMs to translate from any language into English. These\nprompts are then used to create intra-lingual exemplars to perform tasks in the\ntarget languages. Our unsupervised prompting method performs on par with\nsupervised few-shot learning in LLMs of different sizes for translations\nbetween English and 13 Indic and 21 African low-resource languages. We also\nshow that fine-tuning a 7B model on data generated from our method helps it\nperform competitively with a 175B model. In non-English translation tasks, our\nmethod even outperforms supervised prompting by up to 3 chrF++ in many\nlow-resource languages. When evaluated on zero-shot multilingual summarization,\nour method surpasses other English-pivoting baselines by up to 4 ROUGE-L and is\nalso favored by GPT-4.", "published": "2023-06-20 08:27:47", "link": "http://arxiv.org/abs/2306.11372v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MuDPT: Multi-modal Deep-symphysis Prompt Tuning for Large Pre-trained\n  Vision-Language Models", "abstract": "Prompt tuning, like CoOp, has recently shown promising vision recognizing and\ntransfer learning ability on various downstream tasks with the emergence of\nlarge pre-trained vision-language models like CLIP. However, we identify that\nexisting uni-modal prompt tuning approaches may result in sub-optimal\nperformance since this uni-modal design breaks the original alignment of\ntextual and visual representations in the pre-trained model. Inspired by the\nnature of pre-trained vision-language models, we aim to achieve completeness in\nprompt tuning and propose a novel approach called Multi-modal Deep-symphysis\nPrompt Tuning, dubbed as MuDPT, which extends independent multi-modal prompt\ntuning by additionally learning a model-agnostic transformative network to\nallow deep hierarchical bi-directional prompt fusion. We evaluate the\neffectiveness of MuDPT on few-shot vision recognition and out-of-domain\ngeneralization tasks. Compared with the state-of-the-art methods, MuDPT\nachieves better recognition and generalization ability with an apparent margin\nthanks to synergistic alignment of textual and visual representations. Our code\nis available at: https://github.com/Mechrev0/MuDPT.", "published": "2023-06-20 09:15:52", "link": "http://arxiv.org/abs/2306.11400v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Exploring the Performance and Efficiency of Transformer Models for NLP\n  on Mobile Devices", "abstract": "Deep learning (DL) is characterised by its dynamic nature, with new deep\nneural network (DNN) architectures and approaches emerging every few years,\ndriving the field's advancement. At the same time, the ever-increasing use of\nmobile devices (MDs) has resulted in a surge of DNN-based mobile applications.\nAlthough traditional architectures, like CNNs and RNNs, have been successfully\nintegrated into MDs, this is not the case for Transformers, a relatively new\nmodel family that has achieved new levels of accuracy across AI tasks, but\nposes significant computational challenges. In this work, we aim to make steps\ntowards bridging this gap by examining the current state of Transformers'\non-device execution. To this end, we construct a benchmark of representative\nmodels and thoroughly evaluate their performance across MDs with different\ncomputational capabilities. Our experimental results show that Transformers are\nnot accelerator-friendly and indicate the need for software and hardware\noptimisations to achieve efficient deployment.", "published": "2023-06-20 10:15:01", "link": "http://arxiv.org/abs/2306.11426v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Towards Theory-based Moral AI: Moral AI with Aggregating Models Based on\n  Normative Ethical Theory", "abstract": "Moral AI has been studied in the fields of philosophy and artificial\nintelligence. Although most existing studies are only theoretical, recent\ndevelopments in AI have made it increasingly necessary to implement AI with\nmorality. On the other hand, humans are under the moral uncertainty of not\nknowing what is morally right. In this paper, we implement the Maximizing\nExpected Choiceworthiness (MEC) algorithm, which aggregates outputs of models\nbased on three normative theories of normative ethics to generate the most\nappropriate output. MEC is a method for making appropriate moral judgments\nunder moral uncertainty. Our experimental results suggest that the output of\nMEC correlates to some extent with commonsense morality and that MEC can\nproduce equally or more appropriate output than existing methods.", "published": "2023-06-20 10:22:24", "link": "http://arxiv.org/abs/2306.11432v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Timestamped Embedding-Matching Acoustic-to-Word CTC ASR", "abstract": "In this work, we describe a novel method of training an embedding-matching\nword-level connectionist temporal classification (CTC) automatic speech\nrecognizer (ASR) such that it directly produces word start times and durations,\nrequired by many real-world applications, in addition to the transcription. The\nword timestamps enable the ASR to output word segmentations and word confusion\nnetworks without relying on a secondary model or forced alignment process when\ntesting. Our proposed system has similar word segmentation accuracy as a hybrid\nDNN-HMM (Deep Neural Network-Hidden Markov Model) system, with less than 3ms\ndifference in mean absolute error in word start times on TIMIT data. At the\nsame time, we observed less than 5% relative increase in the word error rate\ncompared to the non-timestamped system when using the same audio training data\nand nearly identical model size. We also contribute more rigorous analysis of\nmultiple-hypothesis embedding-matching ASR in general.", "published": "2023-06-20 11:53:43", "link": "http://arxiv.org/abs/2306.11473v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Give Us the Facts: Enhancing Large Language Models with Knowledge Graphs\n  for Fact-aware Language Modeling", "abstract": "Recently, ChatGPT, a representative large language model (LLM), has gained\nconsiderable attention due to its powerful emergent abilities. Some researchers\nsuggest that LLMs could potentially replace structured knowledge bases like\nknowledge graphs (KGs) and function as parameterized knowledge bases. However,\nwhile LLMs are proficient at learning probabilistic language patterns based on\nlarge corpus and engaging in conversations with humans, they, like previous\nsmaller pre-trained language models (PLMs), still have difficulty in recalling\nfacts while generating knowledge-grounded contents. To overcome these\nlimitations, researchers have proposed enhancing data-driven PLMs with\nknowledge-based KGs to incorporate explicit factual knowledge into PLMs, thus\nimproving their performance to generate texts requiring factual knowledge and\nproviding more informed responses to user queries. This paper reviews the\nstudies on enhancing PLMs with KGs, detailing existing knowledge graph enhanced\npre-trained language models (KGPLMs) as well as their applications. Inspired by\nexisting studies on KGPLM, this paper proposes to enhance LLMs with KGs by\ndeveloping knowledge graph-enhanced large language models (KGLLMs). KGLLM\nprovides a solution to enhance LLMs' factual reasoning ability, opening up new\navenues for LLM research.", "published": "2023-06-20 12:21:06", "link": "http://arxiv.org/abs/2306.11489v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TrustGPT: A Benchmark for Trustworthy and Responsible Large Language\n  Models", "abstract": "Large Language Models (LLMs) such as ChatGPT, have gained significant\nattention due to their impressive natural language processing capabilities. It\nis crucial to prioritize human-centered principles when utilizing these models.\nSafeguarding the ethical and moral compliance of LLMs is of utmost importance.\nHowever, individual ethical issues have not been well studied on the latest\nLLMs. Therefore, this study aims to address these gaps by introducing a new\nbenchmark -- TrustGPT. TrustGPT provides a comprehensive evaluation of LLMs in\nthree crucial areas: toxicity, bias, and value-alignment. Initially, TrustGPT\nexamines toxicity in language models by employing toxic prompt templates\nderived from social norms. It then quantifies the extent of bias in models by\nmeasuring quantifiable toxicity values across different groups. Lastly,\nTrustGPT assesses the value of conversation generation models from both active\nvalue-alignment and passive value-alignment tasks. Through the implementation\nof TrustGPT, this research aims to enhance our understanding of the performance\nof conversation generation models and promote the development of language\nmodels that are more ethical and socially responsible.", "published": "2023-06-20 12:53:39", "link": "http://arxiv.org/abs/2306.11507v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Recent Advances in Direct Speech-to-text Translation", "abstract": "Recently, speech-to-text translation has attracted more and more attention\nand many studies have emerged rapidly. In this paper, we present a\ncomprehensive survey on direct speech translation aiming to summarize the\ncurrent state-of-the-art techniques. First, we categorize the existing research\nwork into three directions based on the main challenges -- modeling burden,\ndata scarcity, and application issues. To tackle the problem of modeling\nburden, two main structures have been proposed, encoder-decoder framework\n(Transformer and the variants) and multitask frameworks. For the challenge of\ndata scarcity, recent work resorts to many sophisticated techniques, such as\ndata augmentation, pre-training, knowledge distillation, and multilingual\nmodeling. We analyze and summarize the application issues, which include\nreal-time, segmentation, named entity, gender bias, and code-switching.\nFinally, we discuss some promising directions for future work.", "published": "2023-06-20 16:14:27", "link": "http://arxiv.org/abs/2306.11646v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Lingua Manga: A Generic Large Language Model Centric System for Data\n  Curation", "abstract": "Data curation is a wide-ranging area which contains many critical but\ntime-consuming data processing tasks. However, the diversity of such tasks\nmakes it challenging to develop a general-purpose data curation system. To\naddress this issue, we present Lingua Manga, a user-friendly and versatile\nsystem that utilizes pre-trained large language models. Lingua Manga offers\nautomatic optimization for achieving high performance and label efficiency\nwhile facilitating flexible and rapid development. Through three example\napplications with distinct objectives and users of varying levels of technical\nproficiency, we demonstrate that Lingua Manga can effectively assist both\nskilled programmers and low-code or even no-code users in addressing data\ncuration challenges.", "published": "2023-06-20 17:30:02", "link": "http://arxiv.org/abs/2306.11702v2", "categories": ["cs.DB", "cs.CL"], "primary_category": "cs.DB"}
{"title": "QuOTeS: Query-Oriented Technical Summarization", "abstract": "Abstract. When writing an academic paper, researchers often spend\nconsiderable time reviewing and summarizing papers to extract relevant\ncitations and data to compose the Introduction and Related Work sections. To\naddress this problem, we propose QuOTeS, an interactive system designed to\nretrieve sentences related to a summary of the research from a collection of\npotential references and hence assist in the composition of new papers. QuOTeS\nintegrates techniques from Query-Focused Extractive Summarization and\nHigh-Recall Information Retrieval to provide Interactive Query-Focused\nSummarization of scientific documents. To measure the performance of our\nsystem, we carried out a comprehensive user study where participants uploaded\npapers related to their research and evaluated the system in terms of its\nusability and the quality of the summaries it produces. The results show that\nQuOTeS provides a positive user experience and consistently provides\nquery-focused summaries that are relevant, concise, and complete. We share the\ncode of our system and the novel Query-Focused Summarization dataset collected\nduring our experiments at https://github.com/jarobyte91/quotes.", "published": "2023-06-20 18:43:24", "link": "http://arxiv.org/abs/2306.11832v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "State space models can express n-gram languages", "abstract": "Recent advancements in recurrent neural networks (RNNs) have reinvigorated\ninterest in their application to natural language processing tasks,\nparticularly with the development of more efficient and parallelizable variants\nknown as state space models (SSMs), which have shown competitive performance\nagainst transformer models while maintaining a lower memory footprint. While\nRNNs and SSMs (e.g., Mamba) have been empirically more successful than\nrule-based systems based on n-gram models, a rigorous theoretical explanation\nfor this success has not yet been developed, as it is unclear how these models\nencode the combinatorial rules that govern the next-word prediction task. In\nthis paper, we construct state space language models that can solve the\nnext-word prediction task for languages generated from n-gram rules, thereby\nshowing that the former are more expressive. Our proof shows how SSMs can\nencode n-gram rules using new theoretical results on their memorization\ncapacity, and demonstrates how their context window can be controlled by\nrestricting the spectrum of the state transition matrix. We conduct experiments\nwith a small dataset generated from n-gram rules to show how our framework can\nbe applied to SSMs and RNNs obtained through gradient-based optimization.", "published": "2023-06-20 10:41:23", "link": "http://arxiv.org/abs/2306.17184v3", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Quilt-1M: One Million Image-Text Pairs for Histopathology", "abstract": "Recent accelerations in multi-modal applications have been made possible with\nthe plethora of image and text data available online. However, the scarcity of\nanalogous data in the medical field, specifically in histopathology, has slowed\ncomparable progress. To enable similar representation learning for\nhistopathology, we turn to YouTube, an untapped resource of videos, offering\n$1,087$ hours of valuable educational histopathology videos from expert\nclinicians. From YouTube, we curate QUILT: a large-scale vision-language\ndataset consisting of $802, 144$ image and text pairs. QUILT was automatically\ncurated using a mixture of models, including large language models, handcrafted\nalgorithms, human knowledge databases, and automatic speech recognition. In\ncomparison, the most comprehensive datasets curated for histopathology amass\nonly around $200$K samples. We combine QUILT with datasets from other sources,\nincluding Twitter, research papers, and the internet in general, to create an\neven larger dataset: QUILT-1M, with $1$M paired image-text samples, marking it\nas the largest vision-language histopathology dataset to date. We demonstrate\nthe value of QUILT-1M by fine-tuning a pre-trained CLIP model. Our model\noutperforms state-of-the-art models on both zero-shot and linear probing tasks\nfor classifying new histopathology images across $13$ diverse patch-level\ndatasets of $8$ different sub-pathologies and cross-modal retrieval tasks.", "published": "2023-06-20 00:14:47", "link": "http://arxiv.org/abs/2306.11207v4", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Eight challenges in developing theory of intelligence", "abstract": "A good theory of mathematical beauty is more practical than any current\nobservation, as new predictions of physical reality can be verified\nself-consistently. This belief applies to the current status of understanding\ndeep neural networks including large language models and even the biological\nintelligence. Toy models provide a metaphor of physical reality, allowing\nmathematically formulating that reality (i.e., the so-called theory), which can\nbe updated as more conjectures are justified or refuted. One does not need to\npack all details into a model, but rather, more abstract models are\nconstructed, as complex systems like brains or deep networks have many sloppy\ndimensions but much less stiff dimensions that strongly impact macroscopic\nobservables. This kind of bottom-up mechanistic modeling is still promising in\nthe modern era of understanding the natural or artificial intelligence. Here,\nwe shed light on eight challenges in developing theory of intelligence\nfollowing this theoretical paradigm. Theses challenges are representation\nlearning, generalization, adversarial robustness, continual learning, causal\nlearning, internal model of the brain, next-token prediction, and finally the\nmechanics of subjective experience.", "published": "2023-06-20 01:45:42", "link": "http://arxiv.org/abs/2306.11232v2", "categories": ["q-bio.NC", "cond-mat.stat-mech", "cs.AI", "cs.CL"], "primary_category": "q-bio.NC"}
{"title": "ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF\n  Synthesis", "abstract": "We use prompt engineering to guide ChatGPT in the automation of text mining\nof metal-organic frameworks (MOFs) synthesis conditions from diverse formats\nand styles of the scientific literature. This effectively mitigates ChatGPT's\ntendency to hallucinate information -- an issue that previously made the use of\nLarge Language Models (LLMs) in scientific fields challenging. Our approach\ninvolves the development of a workflow implementing three different processes\nfor text mining, programmed by ChatGPT itself. All of them enable parsing,\nsearching, filtering, classification, summarization, and data unification with\ndifferent tradeoffs between labor, speed, and accuracy. We deploy this system\nto extract 26,257 distinct synthesis parameters pertaining to approximately 800\nMOFs sourced from peer-reviewed research articles. This process incorporates\nour ChemPrompt Engineering strategy to instruct ChatGPT in text mining,\nresulting in impressive precision, recall, and F1 scores of 90-99%.\nFurthermore, with the dataset built by text mining, we constructed a\nmachine-learning model with over 86% accuracy in predicting MOF experimental\ncrystallization outcomes and preliminarily identifying important factors in MOF\ncrystallization. We also developed a reliable data-grounded MOF chatbot to\nanswer questions on chemical reactions and synthesis procedures. Given that the\nprocess of using ChatGPT reliably mines and tabulates diverse MOF synthesis\ninformation in a unified format, while using only narrative language requiring\nno coding expertise, we anticipate that our ChatGPT Chemistry Assistant will be\nvery useful across various other chemistry sub-disciplines.", "published": "2023-06-20 05:20:29", "link": "http://arxiv.org/abs/2306.11296v2", "categories": ["cs.IR", "cond-mat.mtrl-sci", "cs.CL", "physics.chem-ph"], "primary_category": "cs.IR"}
{"title": "RS5M and GeoRSCLIP: A Large Scale Vision-Language Dataset and A Large\n  Vision-Language Model for Remote Sensing", "abstract": "Pre-trained Vision-Language Models (VLMs) utilizing extensive image-text\npaired data have demonstrated unprecedented image-text association\ncapabilities, achieving remarkable results across various downstream tasks. A\ncritical challenge is how to make use of existing large-scale pre-trained VLMs,\nwhich are trained on common objects, to perform the domain-specific transfer\nfor accomplishing domain-related downstream tasks. A critical challenge is how\nto make use of existing large-scale pre-trained VLMs, which are trained on\ncommon objects, to perform the domain-specific transfer for accomplishing\ndomain-related downstream tasks. In this paper, we propose a new framework that\nincludes the Domain pre-trained Vision-Language Model (DVLM), bridging the gap\nbetween the General Vision-Language Model (GVLM) and domain-specific downstream\ntasks. Moreover, we present an image-text paired dataset in the field of remote\nsensing (RS), RS5M, which has 5 million RS images with English descriptions.\nThe dataset is obtained from filtering publicly available image-text paired\ndatasets and captioning label-only RS datasets with pre-trained VLM. These\nconstitute the first large-scale RS image-text paired dataset. Additionally, we\nfine-tuned the CLIP model and tried several Parameter-Efficient Fine-Tuning\nmethods on RS5M to implement the DVLM. Experimental results show that our\nproposed dataset is highly effective for various tasks, and our model GeoRSCLIP\nimproves upon the baseline or previous state-of-the-art model by $3\\%\\sim20\\%$\nin Zero-shot Classification (ZSC), $3\\%\\sim6\\%$ in Remote Sensing Cross-Modal\nText-Image Retrieval (RSCTIR) and $4\\%\\sim5\\%$ in Semantic Localization (SeLo)\ntasks. Dataset and models have been released in:\n\\url{https://github.com/om-ai-lab/RS5M}.", "published": "2023-06-20 05:30:59", "link": "http://arxiv.org/abs/2306.11300v5", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Multi-pass Training and Cross-information Fusion for Low-resource\n  End-to-end Accented Speech Recognition", "abstract": "Low-resource accented speech recognition is one of the important challenges\nfaced by current ASR technology in practical applications. In this study, we\npropose a Conformer-based architecture, called Aformer, to leverage both the\nacoustic information from large non-accented and limited accented training\ndata. Specifically, a general encoder and an accent encoder are designed in the\nAformer to extract complementary acoustic information. Moreover, we propose to\ntrain the Aformer in a multi-pass manner, and investigate three\ncross-information fusion methods to effectively combine the information from\nboth general and accent encoders. All experiments are conducted on both the\naccented English and Mandarin ASR tasks. Results show that our proposed methods\noutperform the strong Conformer baseline by relative 10.2% to 24.5%\nword/character error rate reduction on six in-domain and out-of-domain accented\ntest sets.", "published": "2023-06-20 06:08:09", "link": "http://arxiv.org/abs/2306.11309v1", "categories": ["cs.SD", "cs.CL", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Textbooks Are All You Need", "abstract": "We introduce phi-1, a new large language model for code, with significantly\nsmaller size than competing models: phi-1 is a Transformer-based model with\n1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook\nquality\" data from the web (6B tokens) and synthetically generated textbooks\nand exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains\npass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays\nsurprising emergent properties compared to phi-1-base, our model before our\nfinetuning stage on a dataset of coding exercises, and phi-1-small, a smaller\nmodel with 350M parameters trained with the same pipeline as phi-1 that still\nachieves 45% on HumanEval.", "published": "2023-06-20 16:14:25", "link": "http://arxiv.org/abs/2306.11644v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Harnessing the Power of Adversarial Prompting and Large Language Models\n  for Robust Hypothesis Generation in Astronomy", "abstract": "This study investigates the application of Large Language Models (LLMs),\nspecifically GPT-4, within Astronomy. We employ in-context prompting, supplying\nthe model with up to 1000 papers from the NASA Astrophysics Data System, to\nexplore the extent to which performance can be improved by immersing the model\nin domain-specific literature. Our findings point towards a substantial boost\nin hypothesis generation when using in-context prompting, a benefit that is\nfurther accentuated by adversarial prompting. We illustrate how adversarial\nprompting empowers GPT-4 to extract essential details from a vast knowledge\nbase to produce meaningful hypotheses, signaling an innovative step towards\nemploying LLMs for scientific research in Astronomy.", "published": "2023-06-20 16:16:56", "link": "http://arxiv.org/abs/2306.11648v1", "categories": ["astro-ph.IM", "astro-ph.GA", "cs.AI", "cs.CL"], "primary_category": "astro-ph.IM"}
{"title": "A Simple and Effective Pruning Approach for Large Language Models", "abstract": "As their size increases, Large Languages Models (LLMs) are natural candidates\nfor network pruning methods: approaches that drop a subset of network weights\nwhile striving to preserve performance. Existing methods, however, require\neither retraining, which is rarely affordable for billion-scale LLMs, or\nsolving a weight reconstruction problem reliant on second-order information,\nwhich may also be computationally expensive. In this paper, we introduce a\nnovel, straightforward yet effective pruning method, termed Wanda (Pruning by\nWeights and activations), designed to induce sparsity in pretrained LLMs.\nMotivated by the recent observation of emergent large magnitude features in\nLLMs, our approach prunes weights with the smallest magnitudes multiplied by\nthe corresponding input activations, on a per-output basis. Notably, Wanda\nrequires no retraining or weight update, and the pruned LLM can be used as is.\nWe conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2\nacross various language benchmarks. Wanda significantly outperforms the\nestablished baseline of magnitude pruning and performs competitively against\nrecent method involving intensive weight update. Code is available at\nhttps://github.com/locuslab/wanda.", "published": "2023-06-20 17:18:20", "link": "http://arxiv.org/abs/2306.11695v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT\n  Models", "abstract": "Generative Pre-trained Transformer (GPT) models have exhibited exciting\nprogress in their capabilities, capturing the interest of practitioners and the\npublic alike. Yet, while the literature on the trustworthiness of GPT models\nremains limited, practitioners have proposed employing capable GPT models for\nsensitive applications such as healthcare and finance -- where mistakes can be\ncostly. To this end, this work proposes a comprehensive trustworthiness\nevaluation for large language models with a focus on GPT-4 and GPT-3.5,\nconsidering diverse perspectives -- including toxicity, stereotype bias,\nadversarial robustness, out-of-distribution robustness, robustness on\nadversarial demonstrations, privacy, machine ethics, and fairness. Based on our\nevaluations, we discover previously unpublished vulnerabilities to\ntrustworthiness threats. For instance, we find that GPT models can be easily\nmisled to generate toxic and biased outputs and leak private information in\nboth training data and conversation history. We also find that although GPT-4\nis usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more\nvulnerable given jailbreaking system or user prompts, potentially because GPT-4\nfollows (misleading) instructions more precisely. Our work illustrates a\ncomprehensive trustworthiness evaluation of GPT models and sheds light on the\ntrustworthiness gaps. Our benchmark is publicly available at\nhttps://decodingtrust.github.io/ ; our dataset can be previewed at\nhttps://huggingface.co/datasets/AI-Secure/DecodingTrust ; a concise version of\nthis work is at https://openreview.net/pdf?id=kaHpo8OZw2 .", "published": "2023-06-20 17:24:23", "link": "http://arxiv.org/abs/2306.11698v5", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Learning to Generate Better Than Your LLM", "abstract": "Reinforcement learning (RL) has emerged as a powerful paradigm for\nfine-tuning Large Language Models (LLMs) for text generation. In particular,\nrecent LLMs such as ChatGPT and GPT-4 can engage in fluent conversations with\nusers after finetuning with RL. Capitalizing on key properties of text\ngeneration, we seek to investigate RL algorithms beyond general purpose\nalgorithms like Proximal Policy Optimization (PPO). In particular, we extend RL\nalgorithms to allow them to interact with a dynamic black-box guide LLM and\npropose RL with guided feedback (RLGF), a suite of RL algorithms for LLM\nfine-tuning. We provide two ways for the guide LLM to interact with the LLM to\nbe optimized for maximizing rewards. The guide LLM can generate text which\nserves as additional starting states for the RL optimization procedure. The\nguide LLM can also be used to complete the partial sentences generated by the\nLLM that is being optimized, treating the guide LLM as an expert to imitate and\nsurpass eventually. We experiment on the IMDB positive sentiment, CommonGen,\nand TL;DR summarization tasks. We show that our RL algorithms achieve higher\nperformance than supervised learning (SL) and the RL baseline PPO,\ndemonstrating the benefit of interaction with the guide LLM. On both CommonGen\nand TL;DR, we not only outperform our SL baselines but also improve upon PPO\nacross a variety of metrics beyond the one we optimized for. Our code can be\nfound at https://github.com/Cornell-RL/tril.", "published": "2023-06-20 18:19:17", "link": "http://arxiv.org/abs/2306.11816v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Retrieval-Based Transformer for Table Augmentation", "abstract": "Data preparation, also called data wrangling, is considered one of the most\nexpensive and time-consuming steps when performing analytics or building\nmachine learning models. Preparing data typically involves collecting and\nmerging data from complex heterogeneous, and often large-scale data sources,\nsuch as data lakes. In this paper, we introduce a novel approach toward\nautomatic data wrangling in an attempt to alleviate the effort of end-users,\ne.g. data analysts, in structuring dynamic views from data lakes in the form of\ntabular data. We aim to address table augmentation tasks, including row/column\npopulation and data imputation. Given a corpus of tables, we propose a\nretrieval augmented self-trained transformer model. Our self-learning strategy\nconsists in randomly ablating tables from the corpus and training the\nretrieval-based model to reconstruct the original values or headers given the\npartial tables as input. We adopt this strategy to first train the dense neural\nretrieval model encoding table-parts to vectors, and then the end-to-end model\ntrained to perform table augmentation tasks. We test on EntiTables, the\nstandard benchmark for table augmentation, as well as introduce a new benchmark\nto advance further research: WebTables. Our model consistently and\nsubstantially outperforms both supervised statistical methods and the current\nstate-of-the-art transformer-based models.", "published": "2023-06-20 18:51:21", "link": "http://arxiv.org/abs/2306.11843v1", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis", "abstract": "Polis is a platform that leverages machine intelligence to scale up\ndeliberative processes. In this paper, we explore the opportunities and risks\nassociated with applying Large Language Models (LLMs) towards challenges with\nfacilitating, moderating and summarizing the results of Polis engagements. In\nparticular, we demonstrate with pilot experiments using Anthropic's Claude that\nLLMs can indeed augment human intelligence to help more efficiently run Polis\nconversations. In particular, we find that summarization capabilities enable\ncategorically new methods with immense promise to empower the public in\ncollective meaning-making exercises. And notably, LLM context limitations have\na significant impact on insight and quality of these results.\n  However, these opportunities come with risks. We discuss some of these risks,\nas well as principles and techniques for characterizing and mitigating them,\nand the implications for other deliberative or political systems that may\nemploy LLMs. Finally, we conclude with several open future research directions\nfor augmenting tools like Polis with LLMs.", "published": "2023-06-20 22:52:51", "link": "http://arxiv.org/abs/2306.11932v1", "categories": ["cs.SI", "cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.SI"}
{"title": "Towards Understanding What Code Language Models Learned", "abstract": "Pre-trained language models are effective in a variety of natural language\ntasks, but it has been argued their capabilities fall short of fully learning\nmeaning or understanding language. To understand the extent to which language\nmodels can learn some form of meaning, we investigate their ability to capture\nsemantics of code beyond superficial frequency and co-occurrence. In contrast\nto previous research on probing models for linguistic features, we study\npre-trained models in a setting that allows for objective and straightforward\nevaluation of a model's ability to learn semantics. In this paper, we examine\nwhether such models capture the semantics of code, which is precisely and\nformally defined. Through experiments involving the manipulation of code\nfragments, we show that code pre-trained models of code learn a robust\nrepresentation of the computational semantics of code that goes beyond\nsuperficial features of form alone", "published": "2023-06-20 23:42:14", "link": "http://arxiv.org/abs/2306.11943v2", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "DEPAC: a Corpus for Depression and Anxiety Detection from Speech", "abstract": "Mental distress like depression and anxiety contribute to the largest\nproportion of the global burden of diseases. Automated diagnosis systems of\nsuch disorders, empowered by recent innovations in Artificial Intelligence, can\npave the way to reduce the sufferings of the affected individuals. Development\nof such systems requires information-rich and balanced corpora. In this work,\nwe introduce a novel mental distress analysis audio dataset DEPAC, labeled\nbased on established thresholds on depression and anxiety standard screening\ntools. This large dataset comprises multiple speech tasks per individual, as\nwell as relevant demographic information. Alongside, we present a feature set\nconsisting of hand-curated acoustic and linguistic features, which were found\neffective in identifying signs of mental illnesses in human speech. Finally, we\njustify the quality and effectiveness of our proposed audio corpus and feature\nset in predicting depression severity by comparing the performance of baseline\nmachine learning models built on this dataset with baseline models trained on\nother well-known depression corpora.", "published": "2023-06-20 12:21:06", "link": "http://arxiv.org/abs/2306.12443v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MSVD-Indonesian: A Benchmark for Multimodal Video-Text Tasks in\n  Indonesian", "abstract": "Multimodal learning on video and text data has been receiving growing\nattention from many researchers in various research tasks, including\ntext-to-video retrieval, video-to-text retrieval, and video captioning.\nAlthough many algorithms have been proposed for those challenging tasks, most\nof them are developed on English language datasets. Despite Indonesian being\none of the most spoken languages in the world, the research progress on the\nmultimodal video-text with Indonesian sentences is still under-explored, likely\ndue to the absence of the public benchmark dataset. To address this issue, we\nconstruct the first public Indonesian video-text dataset by translating English\nsentences from the MSVD dataset to Indonesian sentences. Using our dataset, we\nthen train neural network models which were developed for the English\nvideo-text dataset on three tasks, i.e., text-to-video retrieval, video-to-text\nretrieval, and video captioning. The recent neural network-based approaches to\nvideo-text tasks often utilized a feature extractor that is primarily\npretrained on an English vision-language dataset. Since the availability of the\npretraining resources with Indonesian sentences is relatively limited, the\napplicability of those approaches to our dataset is still questionable. To\novercome the lack of pretraining resources, we apply cross-lingual transfer\nlearning by utilizing the feature extractors pretrained on the English dataset,\nand we then fine-tune the models on our Indonesian dataset. Our experimental\nresults show that this approach can help to improve the performance for the\nthree tasks on all metrics. Finally, we discuss potential future works using\nour dataset, inspiring further research in the Indonesian multimodal video-text\ntasks. We believe that our dataset and our experimental results could provide\nvaluable contributions to the community. Our dataset is available on GitHub.", "published": "2023-06-20 07:19:36", "link": "http://arxiv.org/abs/2306.11341v1", "categories": ["cs.MM", "cs.CL", "cs.CV", "cs.LG", "eess.IV"], "primary_category": "cs.MM"}
{"title": "Improving Image Captioning Descriptiveness by Ranking and LLM-based\n  Fusion", "abstract": "State-of-The-Art (SoTA) image captioning models often rely on the Microsoft\nCOCO (MS-COCO) dataset for training. This dataset contains annotations provided\nby human annotators, who typically produce captions averaging around ten\ntokens. However, this constraint presents a challenge in effectively capturing\ncomplex scenes and conveying detailed information. Furthermore, captioning\nmodels tend to exhibit bias towards the ``average'' caption, which captures\nonly the more general aspects. What would happen if we were able to\nautomatically generate longer captions, thereby making them more detailed?\nWould these captions, evaluated by humans, be more or less representative of\nthe image content compared to the original MS-COCO captions? In this paper, we\npresent a novel approach to address previous challenges by showcasing how\ncaptions generated from different SoTA models can be effectively fused,\nresulting in richer captions. Our proposed method leverages existing models\nfrom the literature, eliminating the need for additional training. Instead, it\nutilizes an image-text based metric to rank the captions generated by SoTA\nmodels for a given image. Subsequently, the top two captions are fused using a\nLarge Language Model (LLM). Experimental results demonstrate the effectiveness\nof our approach, as the captions generated by our model exhibit higher\nconsistency with human judgment when evaluated on the MS-COCO test set. By\ncombining the strengths of various SoTA models, our method enhances the quality\nand appeal of image captions, bridging the gap between automated systems and\nthe rich, informative nature of human-generated descriptions. This advance\nopens up new possibilities for generating captions that are more suitable for\nthe training of both vision-language and captioning models.", "published": "2023-06-20 15:13:02", "link": "http://arxiv.org/abs/2306.11593v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.DB", "cs.LG"], "primary_category": "cs.CV"}
{"title": "An empirical study of using radiology reports and images to improve ICU\n  mortality prediction", "abstract": "Background: The predictive Intensive Care Unit (ICU) scoring system plays an\nimportant role in ICU management because it predicts important outcomes,\nespecially mortality. Many scoring systems have been developed and used in the\nICU. These scoring systems are primarily based on the structured clinical data\nin the electronic health record (EHR), which may suffer the loss of important\nclinical information in the narratives and images. Methods: In this work, we\nbuild a deep learning based survival prediction model with multi-modality data\nto predict ICU mortality. Four sets of features are investigated: (1)\nphysiological measurements of Simplified Acute Physiology Score (SAPS) II, (2)\ncommon thorax diseases pre-defined by radiologists, (3) BERT-based text\nrepresentations, and (4) chest X-ray image features. We use the Medical\nInformation Mart for Intensive Care IV (MIMIC-IV) dataset to evaluate the\nproposed model. Results: Our model achieves the average C-index of 0.7829 (95%\nconfidence interval, 0.7620-0.8038), which substantially exceeds that of the\nbaseline with SAPS-II features (0.7470 (0.7263-0.7676)). Ablation studies\nfurther demonstrate the contributions of pre-defined labels (2.00%), text\nfeatures (2.44%), and image features (2.82%).", "published": "2023-06-20 15:43:28", "link": "http://arxiv.org/abs/2307.07513v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "eess.IV"], "primary_category": "cs.AI"}
{"title": "A Computation-efficient Online Secondary Path Modeling Technique for\n  Modified FXLMS Algorithm", "abstract": "This paper proposes an online secondary path modelling (SPM) technique to\nimprove the performance of the modified filtered reference Least Mean Square\n(FXLMS) algorithm. It can effectively respond to a time-varying secondary path,\nwhich refers to the path from a secondary source to an error sensor. Unlike\ntraditional methods, the proposed approach switches modes between adaptive ANC\nand online SPM, eliminating the use of destabilizing components such as\nauxiliary white noise or additional filters, which can negatively impact the\ncomplexity, stability, and noise reduction performance of the ANC system. The\nsystem operates in adaptive ANC mode until divergence is detected due to\nsecondary path changes. At this moment, it switches to SPM mode until the path\nis remodeled and then returns to ANC mode. Furthermore, numerical simulations\nin the paper demonstrate that the proposed online technique effectively copes\nwith the secondary path variations.", "published": "2023-06-20 09:32:13", "link": "http://arxiv.org/abs/2306.11408v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Auditory Neural Response Inspired Sound Event Detection Based on\n  Spectro-temporal Receptive Field", "abstract": "Sound event detection (SED) is one of tasks to automate function by human\nauditory system which listens and understands auditory scenes. Therefore, we\nwere inspired to make SED recognize sound events in the way human auditory\nsystem does. Spectro-temporal receptive field (STRF), an approach to describe\nthe relationship between perceived sound at ear and transformed neural response\nin the auditory cortex, is closely related to recognition of sound. In this\nwork, we utilized STRF as a kernel of the first convolutional layer in SED\nmodel to extract neural response from input sound to make SED model similar to\nhuman auditory system. In addition, we constructed two-branched SED model named\nas Two Branch STRFNet (TB-STRFNet) composed of STRF branch and baseline branch.\nWhile STRF branch extracts sound event information from auditory neural\nresponse, baseline branch extracts sound event information directly from the\nmel spectrogram just as conventional SED models do. TB-STRFNet outperformed the\nDCASE baseline by 4.3% in terms of threshold-independent macro F1 score,\nachieving 4th rank in DCASE Challenge 2023 Task 4b. We further improved\nTB-STRFNet by applying frequency dynamic convolution (FDYConv) which also\nleveraged domain knowledge on acoustics. As a result, two branch model applied\nwith FDYConv on both branches outperformed the DCASE baseline by 6.2% in terms\nof the same metric.", "published": "2023-06-20 10:15:24", "link": "http://arxiv.org/abs/2306.11427v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Cross-lingual Prosody Transfer for Expressive Machine Dubbing", "abstract": "Prosody transfer is well-studied in the context of expressive speech\nsynthesis. Cross-lingual prosody transfer, however, is challenging and has been\nunder-explored to date. In this paper, we present a novel solution to learn\nprosody representations that are transferable across languages and speakers for\nmachine dubbing of expressive multimedia contents. Multimedia contents often\ncontain field recordings. To enable prosody transfer from noisy audios, we\nintroduce a novel noise modelling module that disentangles noise conditioning\nfrom prosody conditioning, and thereby gains independent control of noise\nlevels in the synthesised speech. We augment noisy training data with clean\ndata to improve the ability of the model to map the denoised reference audio to\nclean speech. Our proposed system can generate speech with context-matching\nprosody and closes the gap between a strong baseline and human expressive\ndialogs by 11.2%.", "published": "2023-06-20 16:28:03", "link": "http://arxiv.org/abs/2306.11658v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Expressive Machine Dubbing Through Phrase-level Cross-lingual Prosody\n  Transfer", "abstract": "Speech generation for machine dubbing adds complexity to conventional\nText-To-Speech solutions as the generated output is required to match the\nexpressiveness, emotion and speaking rate of the source content. Capturing and\ntransferring details and variations in prosody is a challenge. We introduce\nphrase-level cross-lingual prosody transfer for expressive multi-lingual\nmachine dubbing. The proposed phrase-level prosody transfer delivers a\nsignificant 6.2% MUSHRA score increase over a baseline with utterance-level\nglobal prosody transfer, thereby closing the gap between the baseline and\nexpressive human dubbing by 23.2%, while preserving intelligibility of the\nsynthesised speech.", "published": "2023-06-20 16:32:56", "link": "http://arxiv.org/abs/2306.11662v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Frequency & Channel Attention for Computationally Efficient Sound Event\n  Detection", "abstract": "We explore on various attention methods on frequency and channel dimensions\nfor sound event detection (SED) in order to enhance performance with minimal\nincrease in computational cost while leveraging domain knowledge to address the\nfrequency dimension of audio data. We have introduced frequency dynamic\nconvolution (FDY conv) in a previous work to release the translational\nequivariance issue associated with 2D convolution on the frequency dimension of\n2D audio data. Although this approach demonstrated state-of-the-art SED\nperformance, it resulted in a model with 150% more trainable parameters. To\nachieve comparable SED performance with computationally efficient methods for\npracticality, we explore on lighter alternative attention methods. In addition,\nwe focus on attention methods applied to frequency and channel dimensions.\nJoint application Squeeze-and-excitation (SE) module and time-frame\nfrequency-wise SE (tfwSE) to apply attention on both frequency and channel\ndimensions shows comparable performance to SED model with FDY conv with only\n2.7% more trainable parameters compared to the baseline model. In addition, we\nperformed class-wise comparison of various attention methods to further discuss\nvarious attention methods' characteristics.", "published": "2023-06-20 04:08:34", "link": "http://arxiv.org/abs/2306.11277v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Phase Repair for Time-Domain Convolutional Neural Networks in Music\n  Super-Resolution", "abstract": "Audio Super-Resolution (SR) is an important topic as low-resolution\nrecordings are ubiquitous in daily life. In this paper, we focus on the music\nSR task, which is challenging due to the wide frequency response and dynamic\nrange of music. Many models are designed in time domain to jointly process\nmagnitude and phase of audio signals. However, prior works show that approaches\nusing Time-Domain Convolutional Neural Network (TD-CNN) tend to produce\nannoying artifacts in their waveform outputs, and the cause of the artifacts is\nyet to be identified. To the best of our knowledge, this work is the first to\ndemonstrate the artifacts in TD-CNNs are caused by the phase distortion via a\nsubjective experiment. We further propose Time-Domain Phase Repair (TD-PR),\nwhich uses a neural vocoder pre-trained on the wide-band data to repair the\nphase components in the waveform outputs of TD-CNNs. Although the vocoder and\nTD-CNNs are independently trained, the proposed TD-PR obtained better mean\nopinion score, significantly improving the perceptual quality of TD-CNN\nbaselines. Since the proposed TD-PR only repairs the phase components of the\nwaveforms, the improved perceptual quality in turn indicates that phase\ndistortion has been the cause of the annoying artifacts of TD-CNNs. Moreover, a\nsingle pretrained vocoder can be directly applied to arbitrary TD-CNNs without\nadditional adaptation. Therefore, we apply TD-PR to three TD-CNNs that have\ndifferent architecture and parameter amount. Consistent improvements are\nobserved when TD-PR is applied to all three TD-CNN baselines. Audio samples are\navailable on the demo page.", "published": "2023-06-20 04:26:02", "link": "http://arxiv.org/abs/2306.11282v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "eCat: An End-to-End Model for Multi-Speaker TTS & Many-to-Many\n  Fine-Grained Prosody Transfer", "abstract": "We present eCat, a novel end-to-end multispeaker model capable of: a)\ngenerating long-context speech with expressive and contextually appropriate\nprosody, and b) performing fine-grained prosody transfer between any pair of\nseen speakers. eCat is trained using a two-stage training approach. In Stage I,\nthe model learns speaker-independent word-level prosody representations in an\nend-to-end fashion from speech. In Stage II, we learn to predict the prosody\nrepresentations using the contextual information available in text. We compare\neCat to CopyCat2, a model capable of both fine-grained prosody transfer (FPT)\nand multi-speaker TTS. We show that eCat statistically significantly reduces\nthe gap in naturalness between CopyCat2 and human recordings by an average of\n46.7% across 2 languages, 3 locales, and 7 speakers, along with better\ntarget-speaker similarity in FPT. We also compare eCat to VITS, and show a\nstatistically significant preference.", "published": "2023-06-20 06:50:52", "link": "http://arxiv.org/abs/2306.11327v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Pipeline for recording datasets and running neural networks on the Bela\n  embedded hardware platform", "abstract": "Deploying deep learning models on embedded devices is an arduous task:\noftentimes, there exist no platform-specific instructions, and compilation\ntimes can be considerably large due to the limited computational resources\navailable on-device. Moreover, many music-making applications demand real-time\ninference. Embedded hardware platforms for audio, such as Bela, offer an entry\npoint for beginners into physical audio computing; however, the need for\ncross-compilation environments and low-level software development tools for\ndeploying embedded deep learning models imposes high entry barriers on\nnon-expert users. We present a pipeline for deploying neural networks in the\nBela embedded hardware platform. In our pipeline, we include a tool to record a\nmultichannel dataset of sensor signals. Additionally, we provide a dockerised\ncross-compilation environment for faster compilation. With this pipeline, we\naim to provide a template for programmers and makers to prototype and\nexperiment with neural networks for real-time embedded musical applications.", "published": "2023-06-20 08:55:04", "link": "http://arxiv.org/abs/2306.11389v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Align, Adapt and Inject: Sound-guided Unified Image Generation", "abstract": "Text-guided image generation has witnessed unprecedented progress due to the\ndevelopment of diffusion models. Beyond text and image, sound is a vital\nelement within the sphere of human perception, offering vivid representations\nand naturally coinciding with corresponding scenes. Taking advantage of sound\ntherefore presents a promising avenue for exploration within image generation\nresearch. However, the relationship between audio and image supervision remains\nsignificantly underdeveloped, and the scarcity of related, high-quality\ndatasets brings further obstacles. In this paper, we propose a unified\nframework 'Align, Adapt, and Inject' (AAI) for sound-guided image generation,\nediting, and stylization. In particular, our method adapts input sound into a\nsound token, like an ordinary word, which can plug and play with existing\npowerful diffusion-based Text-to-Image (T2I) models. Specifically, we first\ntrain a multi-modal encoder to align audio representation with the pre-trained\ntextual manifold and visual manifold, respectively. Then, we propose the audio\nadapter to adapt audio representation into an audio token enriched with\nspecific semantics, which can be injected into a frozen T2I model flexibly. In\nthis way, we are able to extract the dynamic information of varied sounds,\nwhile utilizing the formidable capability of existing T2I models to facilitate\nsound-guided image generation, editing, and stylization in a convenient and\ncost-effective manner. The experiment results confirm that our proposed AAI\noutperforms other text and sound-guided state-of-the-art methods. And our\naligned multi-modal encoder is also competitive with other approaches in the\naudio-visual retrieval and audio-text retrieval tasks.", "published": "2023-06-20 12:50:49", "link": "http://arxiv.org/abs/2306.11504v1", "categories": ["cs.GR", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.GR"}
{"title": "Implicit neural representation with physics-informed neural networks for\n  the reconstruction of the early part of room impulse responses", "abstract": "Recently deep learning and machine learning approaches have been widely\nemployed for various applications in acoustics. Nonetheless, in the area of\nsound field processing and reconstruction classic methods based on the\nsolutions of wave equation are still widespread. Recently, physics-informed\nneural networks have been proposed as a deep learning paradigm for solving\npartial differential equations which govern physical phenomena, bridging the\ngap between purely data-driven and model based methods. Here, we exploit\nphysics-informed neural networks to reconstruct the early part of missing room\nimpulse responses in an uniform linear array. This methodology allows us to\nexploit the underlying law of acoustics, i.e., the wave equation, forcing the\nneural network to generate physically meaningful solutions given only a limited\nnumber of data points. The results on real measurements show that the proposed\nmodel achieves accurate reconstruction and performance in line with respect to\nstate-of-the-art deep-learning and compress sensing techniques while\nmaintaining a lightweight architecture.", "published": "2023-06-20 13:01:00", "link": "http://arxiv.org/abs/2306.11509v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Sound reconstruction from human brain activity via a generative model\n  with brain-like auditory features", "abstract": "The successful reconstruction of perceptual experiences from human brain\nactivity has provided insights into the neural representations of sensory\nexperiences. However, reconstructing arbitrary sounds has been avoided due to\nthe complexity of temporal sequences in sounds and the limited resolution of\nneuroimaging modalities. To overcome these challenges, leveraging the\nhierarchical nature of brain auditory processing could provide a path toward\nreconstructing arbitrary sounds. Previous studies have indicated a hierarchical\nhomology between the human auditory system and deep neural network (DNN)\nmodels. Furthermore, advancements in audio-generative models enable to\ntransform compressed representations back into high-resolution sounds. In this\nstudy, we introduce a novel sound reconstruction method that combines brain\ndecoding of auditory features with an audio-generative model. Using fMRI\nresponses to natural sounds, we found that the hierarchical sound features of a\nDNN model could be better decoded than spectrotemporal features. We then\nreconstructed the sound using an audio transformer that disentangled compressed\ntemporal information in the decoded DNN features. Our method shows\nunconstrained sounds reconstruction capturing sound perceptual contents and\nquality and generalizability by reconstructing sound categories not included in\nthe training dataset. Reconstructions from different auditory regions remain\nsimilar to actual sounds, highlighting the distributed nature of auditory\nrepresentations. To see whether the reconstructions mirrored actual subjective\nperceptual experiences, we performed an experiment involving selective auditory\nattention to one of overlapping sounds. The results tended to resemble the\nattended sound than the unattended. These findings demonstrate that our\nproposed model provides a means to externalize experienced auditory contents\nfrom human brain activity.", "published": "2023-06-20 15:59:09", "link": "http://arxiv.org/abs/2306.11629v1", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On Frequency-Wise Normalizations for Better Recording Device\n  Generalization in Audio Spectrogram Transformers", "abstract": "Varying conditions between the data seen at training and at application time\nremain a major challenge for machine learning. We study this problem in the\ncontext of Acoustic Scene Classification (ASC) with mismatching recording\ndevices. Previous works successfully employed frequency-wise normalization of\ninputs and hidden layer activations in convolutional neural networks to reduce\nthe recording device discrepancy. The main objective of this work was to adopt\nfrequency-wise normalization for Audio Spectrogram Transformers (ASTs), which\nhave recently become the dominant model architecture in ASC. To this end, we\nfirst investigate how recording device characteristics are encoded in the\nhidden layer activations of ASTs. We find that recording device information is\ninitially encoded in the frequency dimension; however, after the first\nself-attention block, it is largely transformed into the token dimension. Based\non this observation, we conjecture that suppressing recording device\ncharacteristics in the input spectrogram is the most effective. We propose a\nfrequency-centering operation for spectrograms that improves the ASC\nperformance on unseen recording devices on average by up to 18.2 percentage\npoints.", "published": "2023-06-20 09:52:59", "link": "http://arxiv.org/abs/2306.11764v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Factors Affecting the Performance of Automated Speaker Verification in\n  Alzheimer's Disease Clinical Trials", "abstract": "Detecting duplicate patient participation in clinical trials is a major\nchallenge because repeated patients can undermine the credibility and accuracy\nof the trial's findings and result in significant health and financial risks.\nDeveloping accurate automated speaker verification (ASV) models is crucial to\nverify the identity of enrolled individuals and remove duplicates, but the size\nand quality of data influence ASV performance. However, there has been limited\ninvestigation into the factors that can affect ASV capabilities in clinical\nenvironments. In this paper, we bridge the gap by conducting analysis of how\nparticipant demographic characteristics, audio quality criteria, and severity\nlevel of Alzheimer's disease (AD) impact the performance of ASV utilizing a\ndataset of speech recordings from 659 participants with varying levels of AD,\nobtained through multiple speech tasks. Our results indicate that ASV\nperformance: 1) is slightly better on male speakers than on female speakers; 2)\ndegrades for individuals who are above 70 years old; 3) is comparatively better\nfor non-native English speakers than for native English speakers; 4) is\nnegatively affected by clinician interference, noisy background, and unclear\nparticipant speech; 5) tends to decrease with an increase in the severity level\nof AD. Our study finds that voice biometrics raise fairness concerns as certain\nsubgroups exhibit different ASV performances owing to their inherent voice\ncharacteristics. Moreover, the performance of ASV is influenced by the quality\nof speech recordings, which underscores the importance of improving the data\ncollection settings in clinical trials.", "published": "2023-06-20 12:24:46", "link": "http://arxiv.org/abs/2306.12444v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
