{"title": "Named Entity Recognition via Machine Reading Comprehension: A Multi-Task\n  Learning Approach", "abstract": "Named Entity Recognition (NER) aims to extract and classify entity mentions\nin the text into pre-defined types (e.g., organization or person name).\nRecently, many works have been proposed to shape the NER as a machine reading\ncomprehension problem (also termed MRC-based NER), in which entity recognition\nis achieved by answering the formulated questions related to pre-defined entity\ntypes through MRC, based on the contexts. However, these works ignore the label\ndependencies among entity types, which are critical for precisely recognizing\nnamed entities. In this paper, we propose to incorporate the label dependencies\namong entity types into a multi-task learning framework for better MRC-based\nNER. We decompose MRC-based NER into multiple tasks and use a self-attention\nmodule to capture label dependencies. Comprehensive experiments on both nested\nNER and flat NER datasets are conducted to validate the effectiveness of the\nproposed Multi-NER. Experimental results show that Multi-NER can achieve better\nperformance on all datasets.", "published": "2023-09-20 03:15:05", "link": "http://arxiv.org/abs/2309.11027v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Heterogeneous Entity Matching with Complex Attribute Associations using\n  BERT and Neural Networks", "abstract": "Across various domains, data from different sources such as Baidu Baike and\nWikipedia often manifest in distinct forms. Current entity matching\nmethodologies predominantly focus on homogeneous data, characterized by\nattributes that share the same structure and concise attribute values. However,\nthis orientation poses challenges in handling data with diverse formats.\nMoreover, prevailing approaches aggregate the similarity of attribute values\nbetween corresponding attributes to ascertain entity similarity. Yet, they\noften overlook the intricate interrelationships between attributes, where one\nattribute may have multiple associations. The simplistic approach of pairwise\nattribute comparison fails to harness the wealth of information encapsulated\nwithin entities.To address these challenges, we introduce a novel entity\nmatching model, dubbed Entity Matching Model for Capturing Complex Attribute\nRelationships(EMM-CCAR),built upon pre-trained models. Specifically, this model\ntransforms the matching task into a sequence matching problem to mitigate the\nimpact of varying data formats. Moreover, by introducing attention mechanisms,\nit identifies complex relationships between attributes, emphasizing the degree\nof matching among multiple attributes rather than one-to-one correspondences.\nThrough the integration of the EMM-CCAR model, we adeptly surmount the\nchallenges posed by data heterogeneity and intricate attribute\ninterdependencies. In comparison with the prevalent DER-SSM and Ditto\napproaches, our model achieves improvements of approximately 4% and 1% in F1\nscores, respectively. This furnishes a robust solution for addressing the\nintricacies of attribute complexity in entity matching.", "published": "2023-09-20 03:49:57", "link": "http://arxiv.org/abs/2309.11046v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Localize, Retrieve and Fuse: A Generalized Framework for Free-Form\n  Question Answering over Tables", "abstract": "Question answering on tabular data (a.k.a TableQA), which aims at generating\nanswers to questions grounded on a provided table, has gained significant\nattention recently. Prior work primarily produces concise factual responses\nthrough information extraction from individual or limited table cells, lacking\nthe ability to reason across diverse table cells. Yet, the realm of free-form\nTableQA, which demands intricate strategies for selecting relevant table cells\nand the sophisticated integration and inference of discrete data fragments,\nremains mostly unexplored. To this end, this paper proposes a generalized\nthree-stage approach: Table-to- Graph conversion and cell localizing, external\nknowledge retrieval, and the fusion of table and text (called TAG-QA), to\naddress the challenge of inferring long free-form answers in generative\nTableQA. In particular, TAG-QA (1) locates relevant table cells using a graph\nneural network to gather intersecting cells between relevant rows and columns,\n(2) leverages external knowledge from Wikipedia, and (3) generates answers by\nintegrating both tabular data and natural linguistic information. Experiments\nshowcase the superior capabilities of TAG-QA in generating sentences that are\nboth faithful and coherent, particularly when compared to several\nstate-of-the-art baselines. Notably, TAG-QA surpasses the robust pipeline-based\nbaseline TAPAS by 17% and 14% in terms of BLEU-4 and PARENT F-score,\nrespectively. Furthermore, TAG-QA outperforms the end-to-end model T5 by 16%\nand 12% on BLEU-4 and PARENT F-score, respectively.", "published": "2023-09-20 03:52:34", "link": "http://arxiv.org/abs/2309.11049v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XATU: A Fine-grained Instruction-based Benchmark for Explainable Text\n  Updates", "abstract": "Text editing is a crucial task of modifying text to better align with user\nintents. However, existing text editing benchmark datasets contain only\ncoarse-grained instructions and lack explainability, thus resulting in outputs\nthat deviate from the intended changes outlined in the gold reference. To\ncomprehensively investigate the text editing capabilities of large language\nmodels (LLMs), this paper introduces XATU, the first benchmark specifically\ndesigned for fine-grained instruction-based explainable text editing. XATU\nconsiders finer-grained text editing tasks of varying difficulty\n(simplification, grammar check, fact-check, etc.), incorporating lexical,\nsyntactic, semantic, and knowledge-intensive edit aspects. To enhance\ninterpretability, we combine LLM-based annotation and human annotation,\nresulting in a benchmark that includes fine-grained instructions and\ngold-standard edit explanations. By evaluating existing LLMs against our\nbenchmark, we demonstrate the effectiveness of instruction tuning and the\nimpact of underlying architecture across various editing tasks. Furthermore,\nextensive experimentation reveals the significant role of explanations in\nfine-tuning language models for text editing tasks. The benchmark will be\nopen-sourced to support reproduction and facilitate future research\nat~\\url{https://github.com/megagonlabs/xatu}.", "published": "2023-09-20 04:58:59", "link": "http://arxiv.org/abs/2309.11063v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UniPCM: Universal Pre-trained Conversation Model with Task-aware\n  Automatic Prompt", "abstract": "Recent research has shown that multi-task pre-training greatly improves the\nmodel's robustness and transfer ability, which is crucial for building a\nhigh-quality dialog system. However, most previous works on multi-task\npre-training rely heavily on human-defined input format or prompt, which is not\noptimal in quality and quantity. In this work, we propose to use Task-based\nAutomatic Prompt generation (TAP) to automatically generate high-quality\nprompts. Using the high-quality prompts generated, we scale the corpus of the\npre-trained conversation model to 122 datasets from 15 dialog-related tasks,\nresulting in Universal Pre-trained Conversation Model (UniPCM), a powerful\nfoundation model for various conversational tasks and different dialog systems.\nExtensive experiments have shown that UniPCM is robust to input prompts and\ncapable of various dialog-related tasks. Moreover, UniPCM has strong transfer\nability and excels at low resource scenarios, achieving SOTA results on 9\ndifferent datasets ranging from task-oriented dialog to open-domain\nconversation. Furthermore, we are amazed to find that TAP can generate prompts\non par with those collected with crowdsourcing. The code is released with the\npaper.", "published": "2023-09-20 05:05:40", "link": "http://arxiv.org/abs/2309.11065v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prototype of a robotic system to assist the learning process of English\n  language with text-generation through DNN", "abstract": "In the last ongoing years, there has been a significant ascending on the\nfield of Natural Language Processing (NLP) for performing multiple tasks\nincluding English Language Teaching (ELT). An effective strategy to favor the\nlearning process uses interactive devices to engage learners in their\nself-learning process. In this work, we present a working prototype of a\nhumanoid robotic system to assist English language self-learners through text\ngeneration using Long Short Term Memory (LSTM) Neural Networks. The learners\ninteract with the system using a Graphic User Interface that generates text\naccording to the English level of the user. The experimentation was conducted\nusing English learners and the results were measured accordingly to\nInternational English Language Testing System (IELTS) rubric. Preliminary\nresults show an increment in the Grammatical Range of learners who interacted\nwith the system.", "published": "2023-09-20 08:39:51", "link": "http://arxiv.org/abs/2309.11142v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessment of Pre-Trained Models Across Languages and Grammars", "abstract": "We present an approach for assessing how multilingual large language models\n(LLMs) learn syntax in terms of multi-formalism syntactic structures. We aim to\nrecover constituent and dependency structures by casting parsing as sequence\nlabeling. To do so, we select a few LLMs and study them on 13 diverse UD\ntreebanks for dependency parsing and 10 treebanks for constituent parsing. Our\nresults show that: (i) the framework is consistent across encodings, (ii)\npre-trained word vectors do not favor constituency representations of syntax\nover dependencies, (iii) sub-word tokenization is needed to represent syntax,\nin contrast to character-based models, and (iv) occurrence of a language in the\npretraining data is more important than the amount of task data when recovering\nsyntax from the word vectors.", "published": "2023-09-20 09:23:36", "link": "http://arxiv.org/abs/2309.11165v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data", "abstract": "Nowadays, open-source large language models like LLaMA have emerged. Recent\ndevelopments have incorporated supervised fine-tuning (SFT) and reinforcement\nlearning fine-tuning (RLFT) to align these models with human goals. However,\nSFT methods treat all training data with mixed quality equally, while RLFT\nmethods require high-quality pairwise or ranking-based preference data. In this\nstudy, we present a novel framework, named OpenChat, to advance open-source\nlanguage models with mixed-quality data. Specifically, we consider the general\nSFT training data, consisting of a small amount of expert data mixed with a\nlarge proportion of sub-optimal data, without any preference labels. We propose\nthe C(onditioned)-RLFT, which regards different data sources as coarse-grained\nreward labels and learns a class-conditioned policy to leverage complementary\ndata quality information. Interestingly, the optimal policy in C-RLFT can be\neasily solved through single-stage, RL-free supervised learning, which is\nlightweight and avoids costly human preference labeling. Through extensive\nexperiments on three standard benchmarks, our openchat-13b fine-tuned with\nC-RLFT achieves the highest average performance among all 13b open-source\nlanguage models. Moreover, we use AGIEval to validate the model generalization\nperformance, in which only openchat-13b surpasses the base model. Finally, we\nconduct a series of analyses to shed light on the effectiveness and robustness\nof OpenChat. Our code, data, and models are publicly available at\nhttps://github.com/imoneoi/openchat and https://huggingface.co/openchat.", "published": "2023-09-20 11:54:40", "link": "http://arxiv.org/abs/2309.11235v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Wizard of Curiosities: Enriching Dialogues with Fun Facts", "abstract": "Introducing curiosities in a conversation is a way to teach something new to\nthe person in a pleasant and enjoyable way. Enriching dialogues with\ncontextualized curiosities can improve the users' perception of a dialog system\nand their overall user experience. In this paper, we introduce a set of curated\ncuriosities, targeting dialogues in the cooking and DIY domains. In particular,\nwe use real human-agent conversations collected in the context of the Amazon\nAlexa TaskBot challenge, a multimodal and multi-turn conversational setting.\nAccording to an A/B test with over 1000 conversations, curiosities not only\nincrease user engagement, but provide an average relative rating improvement of\n9.7%.", "published": "2023-09-20 13:07:32", "link": "http://arxiv.org/abs/2309.11283v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal\n  Services", "abstract": "We propose DISC-LawLLM, an intelligent legal system utilizing large language\nmodels (LLMs) to provide a wide range of legal services. We adopt legal\nsyllogism prompting strategies to construct supervised fine-tuning datasets in\nthe Chinese Judicial domain and fine-tune LLMs with legal reasoning capability.\nWe augment LLMs with a retrieval module to enhance models' ability to access\nand utilize external legal knowledge. A comprehensive legal benchmark,\nDISC-Law-Eval, is presented to evaluate intelligent legal systems from both\nobjective and subjective dimensions. Quantitative and qualitative results on\nDISC-Law-Eval demonstrate the effectiveness of our system in serving various\nusers across diverse legal scenarios. The detailed resources are available at\nhttps://github.com/FudanDISC/DISC-LawLLM.", "published": "2023-09-20 13:50:26", "link": "http://arxiv.org/abs/2309.11325v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Safurai 001: New Qualitative Approach for Code LLM Evaluation", "abstract": "This paper presents Safurai-001, a new Large Language Model (LLM) with\nsignificant potential in the domain of coding assistance. Driven by recent\nadvancements in coding LLMs, Safurai-001 competes in performance with the\nlatest models like WizardCoder [Xu et al., 2023], PanguCoder [Shen et al.,\n2023] and Phi-1 [Gunasekar et al., 2023] but aims to deliver a more\nconversational interaction. By capitalizing on the progress in data engineering\n(including latest techniques of data transformation and prompt engineering) and\ninstruction tuning, this new model promises to stand toe-to-toe with recent\nclosed and open source developments. Recognizing the need for an efficacious\nevaluation metric for coding LLMs, this paper also introduces GPT4-based\nMultiParameters, an evaluation benchmark that harnesses varied parameters to\npresent a comprehensive insight into the models functioning and performance.\nOur assessment shows that Safurai-001 can outperform GPT-3.5 by 1.58% and\nWizardCoder by 18.78% in the Code Readability parameter and more.", "published": "2023-09-20 15:11:32", "link": "http://arxiv.org/abs/2309.11385v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controlled Generation with Prompt Insertion for Natural Language\n  Explanations in Grammatical Error Correction", "abstract": "In Grammatical Error Correction (GEC), it is crucial to ensure the user's\ncomprehension of a reason for correction. Existing studies present tokens,\nexamples, and hints as to the basis for correction but do not directly explain\nthe reasons for corrections. Although methods that use Large Language Models\n(LLMs) to provide direct explanations in natural language have been proposed\nfor various tasks, no such method exists for GEC. Generating explanations for\nGEC corrections involves aligning input and output tokens, identifying\ncorrection points, and presenting corresponding explanations consistently.\nHowever, it is not straightforward to specify a complex format to generate\nexplanations, because explicit control of generation is difficult with prompts.\nThis study introduces a method called controlled generation with Prompt\nInsertion (PI) so that LLMs can explain the reasons for corrections in natural\nlanguage. In PI, LLMs first correct the input text, and then we automatically\nextract the correction points based on the rules. The extracted correction\npoints are sequentially inserted into the LLM's explanation output as prompts,\nguiding the LLMs to generate explanations for the correction points. We also\ncreate an Explainable GEC (XGEC) dataset of correction reasons by annotating\nNUCLE, CoNLL2013, and CoNLL2014. Although generations from GPT-3 and ChatGPT\nusing original prompts miss some correction points, the generation control\nusing PI can explicitly guide to describe explanations for all correction\npoints, contributing to improved performance in generating correction reasons.", "published": "2023-09-20 16:14:10", "link": "http://arxiv.org/abs/2309.11439v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SignBank+: Preparing a Multilingual Sign Language Dataset for Machine\n  Translation Using Large Language Models", "abstract": "We introduce SignBank+, a clean version of the SignBank dataset, optimized\nfor machine translation between spoken language text and SignWriting, a\nphonetic sign language writing system. In addition to previous work that\nemploys complex factorization techniques to enable translation between text and\nSignWriting, we show that a traditional text-to-text translation approach\nperforms equally effectively on the cleaned SignBank+ dataset. Our evaluation\nresults indicate that models trained on SignBank+ surpass those on the original\ndataset, establishing a new benchmark for SignWriting-based sign language\ntranslation and providing an open resource for future research.", "published": "2023-09-20 18:08:28", "link": "http://arxiv.org/abs/2309.11566v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Examining the Limitations of Computational Rumor Detection Models\n  Trained on Static Datasets", "abstract": "A crucial aspect of a rumor detection model is its ability to generalize,\nparticularly its ability to detect emerging, previously unknown rumors. Past\nresearch has indicated that content-based (i.e., using solely source posts as\ninput) rumor detection models tend to perform less effectively on unseen\nrumors. At the same time, the potential of context-based models remains largely\nuntapped. The main contribution of this paper is in the in-depth evaluation of\nthe performance gap between content and context-based models specifically on\ndetecting new, unseen rumors. Our empirical findings demonstrate that\ncontext-based models are still overly dependent on the information derived from\nthe rumors' source post and tend to overlook the significant role that\ncontextual information can play. We also study the effect of data split\nstrategies on classifier performance. Based on our experimental results, the\npaper also offers practical suggestions on how to minimize the effects of\ntemporal concept drift in static datasets during the training of rumor\ndetection methods.", "published": "2023-09-20 18:27:19", "link": "http://arxiv.org/abs/2309.11576v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incorporating Singletons and Mention-based Features in Coreference\n  Resolution via Multi-task Learning for Better Generalization", "abstract": "Previous attempts to incorporate a mention detection step into end-to-end\nneural coreference resolution for English have been hampered by the lack of\nsingleton mention span data as well as other entity information. This paper\npresents a coreference model that learns singletons as well as features such as\nentity type and information status via a multi-task learning-based approach.\nThis approach achieves new state-of-the-art scores on the OntoGUM benchmark\n(+2.7 points) and increases robustness on multiple out-of-domain datasets (+2.3\npoints on average), likely due to greater generalizability for mention\ndetection and utilization of more data from singletons when compared to only\ncoreferent mention pair matching.", "published": "2023-09-20 18:44:24", "link": "http://arxiv.org/abs/2309.11582v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SpeechAlign: a Framework for Speech Translation Alignment Evaluation", "abstract": "Speech-to-Speech and Speech-to-Text translation are currently dynamic areas\nof research. In our commitment to advance these fields, we present SpeechAlign,\na framework designed to evaluate the underexplored field of source-target\nalignment in speech models. The SpeechAlign framework has two core components.\nFirst, to tackle the absence of suitable evaluation datasets, we introduce the\nSpeech Gold Alignment dataset, built upon a English-German text translation\ngold alignment dataset. Secondly, we introduce two novel metrics, Speech\nAlignment Error Rate (SAER) and Time-weighted Speech Alignment Error Rate\n(TW-SAER), which enable the evaluation of alignment quality within speech\nmodels. While the former gives equal importance to each word, the latter\nassigns weights based on the length of the words in the speech signal. By\npublishing SpeechAlign we provide an accessible evaluation framework for model\nassessment, and we employ it to benchmark open-source Speech Translation\nmodels. In doing so, we contribute to the ongoing research progress within the\nfields of Speech-to-Speech and Speech-to-Text translation.", "published": "2023-09-20 18:46:37", "link": "http://arxiv.org/abs/2309.11585v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hate speech detection in algerian dialect using deep learning", "abstract": "With the proliferation of hate speech on social networks under different\nformats, such as abusive language, cyberbullying, and violence, etc., people\nhave experienced a significant increase in violence, putting them in\nuncomfortable situations and threats. Plenty of efforts have been dedicated in\nthe last few years to overcome this phenomenon to detect hate speech in\ndifferent structured languages like English, French, Arabic, and others.\nHowever, a reduced number of works deal with Arabic dialects like Tunisian,\nEgyptian, and Gulf, mainly the Algerian ones. To fill in the gap, we propose in\nthis work a complete approach for detecting hate speech on online Algerian\nmessages. Many deep learning architectures have been evaluated on the corpus we\ncreated from some Algerian social networks (Facebook, YouTube, and Twitter).\nThis corpus contains more than 13.5K documents in Algerian dialect written in\nArabic, labeled as hateful or non-hateful. Promising results are obtained,\nwhich show the efficiency of our approach.", "published": "2023-09-20 19:54:48", "link": "http://arxiv.org/abs/2309.11611v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Effective Disambiguation for Machine Translation with Large\n  Language Models", "abstract": "Resolving semantic ambiguity has long been recognised as a central challenge\nin the field of Machine Translation. Recent work on benchmarking translation\nperformance on ambiguous sentences has exposed the limitations of conventional\nNeural Machine Translation (NMT) systems, which fail to handle many such cases.\nLarge language models (LLMs) have emerged as a promising alternative,\ndemonstrating comparable performance to traditional NMT models while\nintroducing new paradigms for controlling the target outputs. In this paper, we\nstudy the capabilities of LLMs to translate \"ambiguous sentences\" - i.e. those\ncontaining highly polysemous words and/or rare word senses. We also propose two\nways to improve their disambiguation capabilities, through a) in-context\nlearning and b) fine-tuning on carefully curated ambiguous datasets.\nExperiments show that our methods can match or outperform state-of-the-art\nsystems such as DeepL and NLLB in four out of five language directions. Our\nresearch provides valuable insights into effectively adapting LLMs to become\nbetter disambiguators during Machine Translation. We release our curated\ndisambiguation corpora and resources at\nhttps://data.statmt.org/ambiguous-europarl.", "published": "2023-09-20 22:22:52", "link": "http://arxiv.org/abs/2309.11668v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Construction of Paired Knowledge Graph-Text Datasets Informed by Cyclic\n  Evaluation", "abstract": "Datasets that pair Knowledge Graphs (KG) and text together (KG-T) can be used\nto train forward and reverse neural models that generate text from KG and vice\nversa. However models trained on datasets where KG and text pairs are not\nequivalent can suffer from more hallucination and poorer recall. In this paper,\nwe verify this empirically by generating datasets with different levels of\nnoise and find that noisier datasets do indeed lead to more hallucination. We\nargue that the ability of forward and reverse models trained on a dataset to\ncyclically regenerate source KG or text is a proxy for the equivalence between\nthe KG and the text in the dataset. Using cyclic evaluation we find that\nmanually created WebNLG is much better than automatically created TeKGen and\nT-REx. Guided by these observations, we construct a new, improved dataset\ncalled LAGRANGE using heuristics meant to improve equivalence between KG and\ntext and show the impact of each of the heuristics on cyclic evaluation. We\nalso construct two synthetic datasets using large language models (LLMs), and\nobserve that these are conducive to models that perform significantly well on\ncyclic generation of text, but less so on cyclic generation of KGs, probably\nbecause of a lack of a consistent underlying ontology.", "published": "2023-09-20 22:30:20", "link": "http://arxiv.org/abs/2309.11669v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Paradigm Shift in Machine Translation: Boosting Translation\n  Performance of Large Language Models", "abstract": "Generative Large Language Models (LLMs) have achieved remarkable advancements\nin various NLP tasks. However, these advances have not been reflected in the\ntranslation task, especially those with moderate model sizes (i.e., 7B or 13B\nparameters), which still lag behind conventional supervised encoder-decoder\ntranslation models. Previous studies have attempted to improve the translation\ncapabilities of these moderate LLMs, but their gains have been limited. In this\nstudy, we propose a novel fine-tuning approach for LLMs that is specifically\ndesigned for the translation task, eliminating the need for the abundant\nparallel data that traditional translation models usually depend on. Our\napproach consists of two fine-tuning stages: initial fine-tuning on monolingual\ndata followed by subsequent fine-tuning on a small set of high-quality parallel\ndata. We introduce the LLM developed through this strategy as Advanced Language\nModel-based trAnslator (ALMA). Based on LLaMA-2 as our underlying model, our\nresults show that the model can achieve an average improvement of more than 12\nBLEU and 12 COMET over its zero-shot performance across 10 translation\ndirections from the WMT'21 (2 directions) and WMT'22 (8 directions) test\ndatasets. The performance is significantly better than all prior work and even\nsuperior to the NLLB-54B model and GPT-3.5-text-davinci-003, with only 7B or\n13B parameters. This method establishes the foundation for a novel training\nparadigm in machine translation.", "published": "2023-09-20 22:53:15", "link": "http://arxiv.org/abs/2309.11674v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-supervised News Discourse Profiling with Contrastive Learning", "abstract": "News Discourse Profiling seeks to scrutinize the event-related role of each\nsentence in a news article and has been proven useful across various downstream\napplications. Specifically, within the context of a given news discourse, each\nsentence is assigned to a pre-defined category contingent upon its depiction of\nthe news event structure. However, existing approaches suffer from an\ninadequacy of available human-annotated data, due to the laborious and\ntime-intensive nature of generating discourse-level annotations. In this paper,\nwe present a novel approach, denoted as Intra-document Contrastive Learning\nwith Distillation (ICLD), for addressing the news discourse profiling task,\ncapitalizing on its unique structural characteristics. Notably, we are the\nfirst to apply a semi-supervised methodology within this task paradigm, and\nevaluation demonstrates the effectiveness of the presented approach.", "published": "2023-09-20 23:51:34", "link": "http://arxiv.org/abs/2309.11692v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Making Small Language Models Better Multi-task Learners with\n  Mixture-of-Task-Adapters", "abstract": "Recently, Large Language Models (LLMs) have achieved amazing zero-shot\nlearning performance over a variety of Natural Language Processing (NLP) tasks,\nespecially for text generative tasks. Yet, the large size of LLMs often leads\nto the high computational cost of model training and online deployment. In our\nwork, we present ALTER, a system that effectively builds the multi-tAsk\nLearners with mixTure-of-task-adaptERs upon small language models (with <1B\nparameters) to address multiple NLP tasks simultaneously, capturing the\ncommonalities and differences between tasks, in order to support\ndomain-specific applications. Specifically, in ALTER, we propose the\nMixture-of-Task-Adapters (MTA) module as an extension to the transformer\narchitecture for the underlying model to capture the intra-task and inter-task\nknowledge. A two-stage training method is further proposed to optimize the\ncollaboration between adapters at a small computational cost. Experimental\nresults over a mixture of NLP tasks show that our proposed MTA architecture and\nthe two-stage training method achieve good performance. Based on ALTER, we have\nalso produced MTA-equipped language models for various domains.", "published": "2023-09-20 03:39:56", "link": "http://arxiv.org/abs/2309.11042v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AttentionMix: Data augmentation method that relies on BERT attention\n  mechanism", "abstract": "The Mixup method has proven to be a powerful data augmentation technique in\nComputer Vision, with many successors that perform image mixing in a guided\nmanner. One of the interesting research directions is transferring the\nunderlying Mixup idea to other domains, e.g. Natural Language Processing (NLP).\nEven though there already exist several methods that apply Mixup to textual\ndata, there is still room for new, improved approaches. In this work, we\nintroduce AttentionMix, a novel mixing method that relies on attention-based\ninformation. While the paper focuses on the BERT attention mechanism, the\nproposed approach can be applied to generally any attention-based model.\nAttentionMix is evaluated on 3 standard sentiment classification datasets and\nin all three cases outperforms two benchmark approaches that utilize Mixup\nmechanism, as well as the vanilla BERT method. The results confirm that the\nattention-based information can be effectively used for data augmentation in\nthe NLP domain.", "published": "2023-09-20 07:18:53", "link": "http://arxiv.org/abs/2309.11104v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CoT-BERT: Enhancing Unsupervised Sentence Representation through\n  Chain-of-Thought", "abstract": "Unsupervised sentence representation learning aims to transform input\nsentences into fixed-length vectors enriched with intricate semantic\ninformation while obviating the reliance on labeled data. Recent strides within\nthis domain have been significantly propelled by breakthroughs in contrastive\nlearning and prompt engineering. Despite these advancements, the field has\nreached a plateau, leading some researchers to incorporate external components\nto enhance the quality of sentence embeddings. Such integration, though\nbeneficial, complicates solutions and inflates demands for computational\nresources. In response to these challenges, this paper presents CoT-BERT, an\ninnovative method that harnesses the progressive thinking of Chain-of-Thought\nreasoning to tap into the latent potential of pre-trained models like BERT.\nAdditionally, we develop an advanced contrastive learning loss function and\npropose a novel template denoising strategy. Rigorous experimentation\ndemonstrates that CoT-BERT surpasses a range of well-established baselines by\nrelying exclusively on the intrinsic strengths of pre-trained models.", "published": "2023-09-20 08:42:06", "link": "http://arxiv.org/abs/2309.11143v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Are Large Language Models Really Robust to Word-Level Perturbations?", "abstract": "The swift advancement in the scales and capabilities of Large Language Models\n(LLMs) positions them as promising tools for a variety of downstream tasks. In\naddition to the pursuit of better performance and the avoidance of violent\nfeedback on a certain prompt, to ensure the responsibility of the LLM, much\nattention is drawn to the robustness of LLMs. However, existing evaluation\nmethods mostly rely on traditional question answering datasets with predefined\nsupervised labels, which do not align with the superior generation capabilities\nof contemporary LLMs. To address this issue, we propose a novel rational\nevaluation approach that leverages pre-trained reward models as diagnostic\ntools to evaluate the longer conversation generated from more challenging open\nquestions by LLMs, which we refer to as the Reward Model for Reasonable\nRobustness Evaluation (TREvaL). Longer conversations manifest the comprehensive\ngrasp of language models in terms of their proficiency in understanding\nquestions, a capability not entirely encompassed by individual words or\nletters, which may exhibit oversimplification and inherent biases. Our\nextensive empirical experiments demonstrate that TREvaL provides an innovative\nmethod for evaluating the robustness of an LLM. Furthermore, our results\ndemonstrate that LLMs frequently exhibit vulnerability to word-level\nperturbations that are commonplace in daily language usage. Notably, we are\nsurprised to discover that robustness tends to decrease as fine-tuning (SFT and\nRLHF) is conducted. The code of TREval is available in\nhttps://github.com/Harry-mic/TREvaL.", "published": "2023-09-20 09:23:46", "link": "http://arxiv.org/abs/2309.11166v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Languini Kitchen: Enabling Language Modelling Research at Different\n  Scales of Compute", "abstract": "The Languini Kitchen serves as both a research collective and codebase\ndesigned to empower researchers with limited computational resources to\ncontribute meaningfully to the field of language modelling. We introduce an\nexperimental protocol that enables model comparisons based on equivalent\ncompute, measured in accelerator hours. The number of tokens on which a model\nis trained is defined by the model's throughput and the chosen compute class.\nNotably, this approach avoids constraints on critical hyperparameters which\naffect total parameters or floating-point operations. For evaluation, we\npre-process an existing large, diverse, and high-quality dataset of books that\nsurpasses existing academic benchmarks in quality, diversity, and document\nlength. On it, we compare methods based on their empirical scaling trends which\nare estimated through experiments at various levels of compute. This work also\nprovides two baseline models: a feed-forward model derived from the GPT-2\narchitecture and a recurrent model in the form of a novel LSTM with ten-fold\nthroughput. While the GPT baseline achieves better perplexity throughout all\nour levels of compute, our LSTM baseline exhibits a predictable and more\nfavourable scaling law. This is due to the improved throughput and the need for\nfewer training tokens to achieve the same decrease in test perplexity.\nExtrapolating the scaling laws leads of both models results in an intersection\nat roughly 50,000 accelerator hours. We hope this work can serve as the\nfoundation for meaningful and reproducible language modelling research.", "published": "2023-09-20 10:31:17", "link": "http://arxiv.org/abs/2309.11197v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for\n  Knowledge Graph Question Answering", "abstract": "Despite their competitive performance on knowledge-intensive tasks, large\nlanguage models (LLMs) still have limitations in memorizing all world knowledge\nespecially long tail knowledge. In this paper, we study the KG-augmented\nlanguage model approach for solving the knowledge graph question answering\n(KGQA) task that requires rich world knowledge. Existing work has shown that\nretrieving KG knowledge to enhance LLMs prompting can significantly improve\nLLMs performance in KGQA. However, their approaches lack a well-formed\nverbalization of KG knowledge, i.e., they ignore the gap between KG\nrepresentations and textual representations. To this end, we propose an\nanswer-sensitive KG-to-Text approach that can transform KG knowledge into\nwell-textualized statements most informative for KGQA. Based on this approach,\nwe propose a KG-to-Text enhanced LLMs framework for solving the KGQA task.\nExperiments on several KGQA benchmarks show that the proposed KG-to-Text\naugmented LLMs approach outperforms previous KG-augmented LLMs approaches\nregarding answer accuracy and usefulness of knowledge statements.", "published": "2023-09-20 10:42:08", "link": "http://arxiv.org/abs/2309.11206v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Scenario Refiner: Grounding subjects in images at the morphological\n  level", "abstract": "Derivationally related words, such as \"runner\" and \"running\", exhibit\nsemantic differences which also elicit different visual scenarios. In this\npaper, we ask whether Vision and Language (V\\&L) models capture such\ndistinctions at the morphological level, using a a new methodology and dataset.\nWe compare the results from V\\&L models to human judgements and find that\nmodels' predictions differ from those of human participants, in particular\ndisplaying a grammatical bias. We further investigate whether the human-model\nmisalignment is related to model architecture. Our methodology, developed on\none specific morphological contrast, can be further extended for testing models\non capturing other nuanced language features.", "published": "2023-09-20 12:23:06", "link": "http://arxiv.org/abs/2309.11252v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Grounded Complex Task Segmentation for Conversational Assistants", "abstract": "Following complex instructions in conversational assistants can be quite\ndaunting due to the shorter attention and memory spans when compared to reading\nthe same instructions. Hence, when conversational assistants walk users through\nthe steps of complex tasks, there is a need to structure the task into\nmanageable pieces of information of the right length and complexity. In this\npaper, we tackle the recipes domain and convert reading structured instructions\ninto conversational structured ones. We annotated the structure of instructions\naccording to a conversational scenario, which provided insights into what is\nexpected in this setting. To computationally model the conversational step's\ncharacteristics, we tested various Transformer-based architectures, showing\nthat a token-based approach delivers the best results. A further user study\nshowed that users tend to favor steps of manageable complexity and length, and\nthat the proposed methodology can improve the original web-based instructional\ntext. Specifically, 86% of the evaluated tasks were improved from a\nconversational suitability point of view.", "published": "2023-09-20 12:55:46", "link": "http://arxiv.org/abs/2309.11271v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Rating Prediction in Conversational Task Assistants with Behavioral and\n  Conversational-Flow Features", "abstract": "Predicting the success of Conversational Task Assistants (CTA) can be\ncritical to understand user behavior and act accordingly. In this paper, we\npropose TB-Rater, a Transformer model which combines conversational-flow\nfeatures with user behavior features for predicting user ratings in a CTA\nscenario. In particular, we use real human-agent conversations and ratings\ncollected in the Alexa TaskBot challenge, a novel multimodal and multi-turn\nconversational context. Our results show the advantages of modeling both the\nconversational-flow and behavioral aspects of the conversation in a single\nmodel for offline rating prediction. Additionally, an analysis of the\nCTA-specific behavioral features brings insights into this setting and can be\nused to bootstrap future systems.", "published": "2023-09-20 13:34:03", "link": "http://arxiv.org/abs/2309.11307v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "TRAVID: An End-to-End Video Translation Framework", "abstract": "In today's globalized world, effective communication with people from diverse\nlinguistic backgrounds has become increasingly crucial. While traditional\nmethods of language translation, such as written text or voice-only\ntranslations, can accomplish the task, they often fail to capture the complete\ncontext and nuanced information conveyed through nonverbal cues like facial\nexpressions and lip movements. In this paper, we present an end-to-end video\ntranslation system that not only translates spoken language but also\nsynchronizes the translated speech with the lip movements of the speaker. Our\nsystem focuses on translating educational lectures in various Indian languages,\nand it is designed to be effective even in low-resource system settings. By\nincorporating lip movements that align with the target language and matching\nthem with the speaker's voice using voice cloning techniques, our application\noffers an enhanced experience for students and users. This additional feature\ncreates a more immersive and realistic learning environment, ultimately making\nthe learning process more effective and engaging.", "published": "2023-09-20 14:13:05", "link": "http://arxiv.org/abs/2309.11338v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Article Classification with Graph Neural Networks and Multigraphs", "abstract": "Classifying research output into context-specific label taxonomies is a\nchallenging and relevant downstream task, given the volume of existing and\nnewly published articles. We propose a method to enhance the performance of\narticle classification by enriching simple Graph Neural Network (GNN) pipelines\nwith multi-graph representations that simultaneously encode multiple signals of\narticle relatedness, e.g. references, co-authorship, shared publication source,\nshared subject headings, as distinct edge types. Fully supervised transductive\nnode classification experiments are conducted on the Open Graph Benchmark\nOGBN-arXiv dataset and the PubMed diabetes dataset, augmented with additional\nmetadata from Microsoft Academic Graph and PubMed Central, respectively. The\nresults demonstrate that multi-graphs consistently improve the performance of a\nvariety of GNN models compared to the default graphs. When deployed with SOTA\ntextual node embedding methods, the transformed multi-graphs enable simple and\nshallow 2-layer GNN pipelines to achieve results on par with more complex\narchitectures.", "published": "2023-09-20 14:18:04", "link": "http://arxiv.org/abs/2309.11341v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "GECTurk: Grammatical Error Correction and Detection Dataset for Turkish", "abstract": "Grammatical Error Detection and Correction (GEC) tools have proven useful for\nnative speakers and second language learners. Developing such tools requires a\nlarge amount of parallel, annotated data, which is unavailable for most\nlanguages. Synthetic data generation is a common practice to overcome the\nscarcity of such data. However, it is not straightforward for morphologically\nrich languages like Turkish due to complex writing rules that require\nphonological, morphological, and syntactic information. In this work, we\npresent a flexible and extensible synthetic data generation pipeline for\nTurkish covering more than 20 expert-curated grammar and spelling rules\n(a.k.a., writing rules) implemented through complex transformation functions.\nUsing this pipeline, we derive 130,000 high-quality parallel sentences from\nprofessionally edited articles. Additionally, we create a more realistic test\nset by manually annotating a set of movie reviews. We implement three baselines\nformulating the task as i) neural machine translation, ii) sequence tagging,\nand iii) prefix tuning with a pretrained decoder-only model, achieving strong\nresults. Furthermore, we perform exhaustive experiments on out-of-domain\ndatasets to gain insights on the transferability and robustness of the proposed\napproaches. Our results suggest that our corpus, GECTurk, is high-quality and\nallows knowledge transfer for the out-of-domain setting. To encourage further\nresearch on Turkish GEC, we release our datasets, baseline models, and the\nsynthetic data generation pipeline at https://github.com/GGLAB-KU/gecturk.", "published": "2023-09-20 14:25:44", "link": "http://arxiv.org/abs/2309.11346v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "KOSMOS-2.5: A Multimodal Literate Model", "abstract": "The automatic reading of text-intensive images represents a significant\nadvancement toward achieving Artificial General Intelligence (AGI). In this\npaper we present KOSMOS-2.5, a multimodal literate model for machine reading of\ntext-intensive images. Pre-trained on a large-scale corpus of text-intensive\nimages, KOSMOS-2.5 excels in two distinct yet complementary transcription\ntasks: (1) generating spatially-aware text blocks, where each block of text is\nassigned spatial coordinates within the image, and (2) producing structured\ntext output that captures both style and structure in markdown format. This\nunified multimodal literate capability is achieved through a shared\ndecoder-only autoregressive Transformer architecture and task-specific prompts.\nBuilding on this foundation, we fine-tune KOSMOS-2.5 for document understanding\ntasks, resulting in a document understanding generalist named KOSMOS-2.5-CHAT.\nAdditionally, a large corpus of 357.4 million document pages spanning diverse\ndomains was curated for pre-training. We evaluate KOSMOS-2.5 on two newly\nproposed benchmarks, OCREval and MarkdownEval, for document-level text\nrecognition and image-to-markdown generation, demonstrating impressive literate\ncapabilities comparable to GPT-4o. KOSMOS-2.5-CHAT achieves performance\ncomparable to other state-of-the-art generalists that are five times larger\n(1.3B vs. 7B) across nine text-rich visual question answering benchmarks.\nModels and code have been available at \\url{https://aka.ms/kosmos25}.", "published": "2023-09-20 15:50:08", "link": "http://arxiv.org/abs/2309.11419v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Chain-of-Verification Reduces Hallucination in Large Language Models", "abstract": "Generation of plausible yet incorrect factual information, termed\nhallucination, is an unsolved issue in large language models. We study the\nability of language models to deliberate on the responses they give in order to\ncorrect their mistakes. We develop the Chain-of-Verification (CoVe) method\nwhereby the model first (i) drafts an initial response; then (ii) plans\nverification questions to fact-check its draft; (iii) answers those questions\nindependently so the answers are not biased by other responses; and (iv)\ngenerates its final verified response. In experiments, we show CoVe decreases\nhallucinations across a variety of tasks, from list-based questions from\nWikidata, closed book MultiSpanQA and longform text generation.", "published": "2023-09-20 17:50:55", "link": "http://arxiv.org/abs/2309.11495v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hierarchical reinforcement learning with natural language subgoals", "abstract": "Hierarchical reinforcement learning has been a compelling approach for\nachieving goal directed behavior over long sequences of actions. However, it\nhas been challenging to implement in realistic or open-ended environments. A\nmain challenge has been to find the right space of sub-goals over which to\ninstantiate a hierarchy. We present a novel approach where we use data from\nhumans solving these tasks to softly supervise the goal space for a set of long\nrange tasks in a 3D embodied environment. In particular, we use unconstrained\nnatural language to parameterize this space. This has two advantages: first, it\nis easy to generate this data from naive human participants; second, it is\nflexible enough to represent a vast range of sub-goals in human-relevant tasks.\nOur approach outperforms agents that clone expert behavior on these tasks, as\nwell as HRL from scratch without this supervised sub-goal space. Our work\npresents a novel approach to combining human expert supervision with the\nbenefits and flexibility of reinforcement learning.", "published": "2023-09-20 18:03:04", "link": "http://arxiv.org/abs/2309.11564v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LLM Guided Inductive Inference for Solving Compositional Problems", "abstract": "While large language models (LLMs) have demonstrated impressive performance\nin question-answering tasks, their performance is limited when the questions\nrequire knowledge that is not included in the model's training data and can\nonly be acquired through direct observation or interaction with the real world.\nExisting methods decompose reasoning tasks through the use of modules invoked\nsequentially, limiting their ability to answer deep reasoning tasks. We\nintroduce a method, Recursion based extensible LLM (REBEL), which handles\nopen-world, deep reasoning tasks by employing automated reasoning techniques\nlike dynamic planning and forward-chaining strategies. REBEL allows LLMs to\nreason via recursive problem decomposition and utilization of external tools.\nThe tools that REBEL uses are specified only by natural language description.\nWe further demonstrate REBEL capabilities on a set of problems that require a\ndeeply nested use of external tools in a compositional and conversational\nsetting.", "published": "2023-09-20 23:44:16", "link": "http://arxiv.org/abs/2309.11688v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Joint Modeling of Dialogue Response and Speech Synthesis based\n  on Large Language Model", "abstract": "This paper explores the potential of constructing an AI spoken dialogue\nsystem that \"thinks how to respond\" and \"thinks how to speak\" simultaneously,\nwhich more closely aligns with the human speech production process compared to\nthe current cascade pipeline of independent chatbot and Text-to-Speech (TTS)\nmodules. We hypothesize that Large Language Models (LLMs) with billions of\nparameters possess significant speech understanding capabilities and can\njointly model dialogue responses and linguistic features. We conduct two sets\nof experiments: 1) Prosodic structure prediction, a typical front-end task in\nTTS, demonstrating the speech understanding ability of LLMs, and 2) Further\nintegrating dialogue response and a wide array of linguistic features using a\nunified encoding format. Our results indicate that the LLM-based approach is a\npromising direction for building unified spoken dialogue systems.", "published": "2023-09-20 01:48:27", "link": "http://arxiv.org/abs/2309.11000v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "fakenewsbr: A Fake News Detection Platform for Brazilian Portuguese", "abstract": "The proliferation of fake news has become a significant concern in recent\ntimes due to its potential to spread misinformation and manipulate public\nopinion. This paper presents a comprehensive study on detecting fake news in\nBrazilian Portuguese, focusing on journalistic-type news. We propose a machine\nlearning-based approach that leverages natural language processing techniques,\nincluding TF-IDF and Word2Vec, to extract features from textual data. We\nevaluate the performance of various classification algorithms, such as logistic\nregression, support vector machine, random forest, AdaBoost, and LightGBM, on a\ndataset containing both true and fake news articles. The proposed approach\nachieves high accuracy and F1-Score, demonstrating its effectiveness in\nidentifying fake news. Additionally, we developed a user-friendly web platform,\nfakenewsbr.com, to facilitate the verification of news articles' veracity. Our\nplatform provides real-time analysis, allowing users to assess the likelihood\nof fake news articles. Through empirical analysis and comparative studies, we\ndemonstrate the potential of our approach to contribute to the fight against\nthe spread of fake news and promote more informed media consumption.", "published": "2023-09-20 04:10:03", "link": "http://arxiv.org/abs/2309.11052v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Design of Chain-of-Thought in Math Problem Solving", "abstract": "Chain-of-Thought (CoT) plays a crucial role in reasoning for math problem\nsolving. We conduct a comprehensive examination of methods for designing CoT,\ncomparing conventional natural language CoT with various program CoTs,\nincluding the self-describing program, the comment-describing program, and the\nnon-describing program. Furthermore, we investigate the impact of programming\nlanguage on program CoTs, comparing Python and Wolfram Language. Through\nextensive experiments on GSM8K, MATHQA, and SVAMP, we find that program CoTs\noften have superior effectiveness in math problem solving. Notably, the best\nperforming combination with 30B parameters beats GPT-3.5-turbo by a significant\nmargin. The results show that self-describing program offers greater diversity\nand thus can generally achieve higher performance. We also find that Python is\na better choice of language than Wolfram for program CoTs. The experimental\nresults provide a valuable guideline for future CoT designs that take into\naccount both programming language and coding style for further advancements.\nOur datasets and code are publicly available.", "published": "2023-09-20 04:17:28", "link": "http://arxiv.org/abs/2309.11054v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PL"], "primary_category": "cs.CL"}
{"title": "Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial\n  Margin Contrastive Learning", "abstract": "In recent years, the explosion of web videos makes text-video retrieval\nincreasingly essential and popular for video filtering, recommendation, and\nsearch. Text-video retrieval aims to rank relevant text/video higher than\nirrelevant ones. The core of this task is to precisely measure the cross-modal\nsimilarity between texts and videos. Recently, contrastive learning methods\nhave shown promising results for text-video retrieval, most of which focus on\nthe construction of positive and negative pairs to learn text and video\nrepresentations. Nevertheless, they do not pay enough attention to hard\nnegative pairs and lack the ability to model different levels of semantic\nsimilarity. To address these two issues, this paper improves contrastive\nlearning using two novel techniques. First, to exploit hard examples for robust\ndiscriminative power, we propose a novel Dual-Modal Attention-Enhanced Module\n(DMAE) to mine hard negative pairs from textual and visual clues. By further\nintroducing a Negative-aware InfoNCE (NegNCE) loss, we are able to adaptively\nidentify all these hard negatives and explicitly highlight their impacts in the\ntraining loss. Second, our work argues that triplet samples can better model\nfine-grained semantic similarity compared to pairwise samples. We thereby\npresent a new Triplet Partial Margin Contrastive Learning (TPM-CL) module to\nconstruct partial order triplet samples by automatically generating\nfine-grained hard negatives for matched text-video pairs. The proposed TPM-CL\ndesigns an adaptive token masking strategy with cross-modal interaction to\nmodel subtle semantic differences. Extensive experiments demonstrate that the\nproposed approach outperforms existing methods on four widely-used text-video\nretrieval datasets, including MSR-VTT, MSVD, DiDeMo and ActivityNet.", "published": "2023-09-20 06:08:11", "link": "http://arxiv.org/abs/2309.11082v3", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling", "abstract": "Lyric translation, a field studied for over a century, is now attracting\ncomputational linguistics researchers. We identified two limitations in\nprevious studies. Firstly, lyric translation studies have predominantly focused\non Western genres and languages, with no previous study centering on K-pop\ndespite its popularity. Second, the field of lyric translation suffers from a\nlack of publicly available datasets; to the best of our knowledge, no such\ndataset exists. To broaden the scope of genres and languages in lyric\ntranslation studies, we introduce a novel singable lyric translation dataset,\napproximately 89\\% of which consists of K-pop song lyrics. This dataset aligns\nKorean and English lyrics line-by-line and section-by-section. We leveraged\nthis dataset to unveil unique characteristics of K-pop lyric translation,\ndistinguishing it from other extensively studied genres, and to construct a\nneural lyric translation model, thereby underscoring the importance of a\ndedicated dataset for singable lyric translations.", "published": "2023-09-20 06:54:55", "link": "http://arxiv.org/abs/2309.11093v4", "categories": ["cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Language-Oriented Communication with Semantic Coding and Knowledge\n  Distillation for Text-to-Image Generation", "abstract": "By integrating recent advances in large language models (LLMs) and generative\nmodels into the emerging semantic communication (SC) paradigm, in this article\nwe put forward to a novel framework of language-oriented semantic communication\n(LSC). In LSC, machines communicate using human language messages that can be\ninterpreted and manipulated via natural language processing (NLP) techniques\nfor SC efficiency. To demonstrate LSC's potential, we introduce three\ninnovative algorithms: 1) semantic source coding (SSC) which compresses a text\nprompt into its key head words capturing the prompt's syntactic essence while\nmaintaining their appearance order to keep the prompt's context; 2) semantic\nchannel coding (SCC) that improves robustness against errors by substituting\nhead words with their lenghthier synonyms; and 3) semantic knowledge\ndistillation (SKD) that produces listener-customized prompts via in-context\nlearning the listener's language style. In a communication task for progressive\ntext-to-image generation, the proposed methods achieve higher perceptual\nsimilarities with fewer transmissions while enhancing robustness in noisy\ncommunication channels.", "published": "2023-09-20 08:19:05", "link": "http://arxiv.org/abs/2309.11127v1", "categories": ["eess.SP", "cs.AI", "cs.CL"], "primary_category": "eess.SP"}
{"title": "Speak While You Think: Streaming Speech Synthesis During Text Generation", "abstract": "Large Language Models (LLMs) demonstrate impressive capabilities, yet\ninteraction with these models is mostly facilitated through text. Using\nText-To-Speech to synthesize LLM outputs typically results in notable latency,\nwhich is impractical for fluent voice conversations. We propose LLM2Speech, an\narchitecture to synthesize speech while text is being generated by an LLM which\nyields significant latency reduction. LLM2Speech mimics the predictions of a\nnon-streaming teacher model while limiting the exposure to future context in\norder to enable streaming. It exploits the hidden embeddings of the LLM, a\nby-product of the text generation that contains informative semantic context.\nExperimental results show that LLM2Speech maintains the teacher's quality while\nreducing the latency to enable natural conversations.", "published": "2023-09-20 11:00:15", "link": "http://arxiv.org/abs/2309.11210v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Sequence-to-Sequence Spanish Pre-trained Language Models", "abstract": "In recent years, significant advancements in pre-trained language models have\ndriven the creation of numerous non-English language variants, with a\nparticular emphasis on encoder-only and decoder-only architectures. While\nSpanish language models based on BERT and GPT have demonstrated proficiency in\nnatural language understanding and generation, there remains a noticeable\nscarcity of encoder-decoder models explicitly designed for sequence-to-sequence\ntasks, which aim to map input sequences to generate output sequences\nconditionally. This paper breaks new ground by introducing the implementation\nand evaluation of renowned encoder-decoder architectures exclusively\npre-trained on Spanish corpora. Specifically, we present Spanish versions of\nBART, T5, and BERT2BERT-style models and subject them to a comprehensive\nassessment across various sequence-to-sequence tasks, including summarization,\nquestion answering, split-and-rephrase, dialogue, and translation. Our findings\nunderscore the competitive performance of all models, with the BART- and\nT5-based models emerging as top performers across all tasks. We have made all\nmodels publicly available to the research community to foster future\nexplorations and advancements in Spanish NLP:\nhttps://github.com/vgaraujov/Seq2Seq-Spanish-PLMs.", "published": "2023-09-20 12:35:19", "link": "http://arxiv.org/abs/2309.11259v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Overview of AuTexTification at IberLEF 2023: Detection and Attribution\n  of Machine-Generated Text in Multiple Domains", "abstract": "This paper presents the overview of the AuTexTification shared task as part\nof the IberLEF 2023 Workshop in Iberian Languages Evaluation Forum, within the\nframework of the SEPLN 2023 conference. AuTexTification consists of two\nsubtasks: for Subtask 1, participants had to determine whether a text is\nhuman-authored or has been generated by a large language model. For Subtask 2,\nparticipants had to attribute a machine-generated text to one of six different\ntext generation models. Our AuTexTification 2023 dataset contains more than\n160.000 texts across two languages (English and Spanish) and five domains\n(tweets, reviews, news, legal, and how-to articles). A total of 114 teams\nsigned up to participate, of which 36 sent 175 runs, and 20 of them sent their\nworking notes. In this overview, we present the AuTexTification dataset and\ntask, the submitted participating systems, and the results.", "published": "2023-09-20 13:10:06", "link": "http://arxiv.org/abs/2309.11285v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CPLLM: Clinical Prediction with Large Language Models", "abstract": "We present Clinical Prediction with Large Language Models (CPLLM), a method\nthat involves fine-tuning a pre-trained Large Language Model (LLM) for clinical\ndisease and readmission prediction. We utilized quantization and fine-tuned the\nLLM using prompts. For diagnosis prediction, we predict whether patients will\nbe diagnosed with a target disease during their next visit or in the subsequent\ndiagnosis, leveraging their historical diagnosis records. We compared our\nresults to various baselines, including RETAIN, and Med-BERT, the current\nstate-of-the-art model for disease prediction using temporal structured EHR\ndata. In addition, We also evaluated CPLLM for patient hospital readmission\nprediction and compared our method's performance with benchmark baselines. Our\nexperiments have shown that our proposed method, CPLLM, surpasses all the\ntested models in terms of PR-AUC and ROC-AUC metrics, showing state-of-the-art\nresults for diagnosis prediction and patient hospital readmission prediction.\nSuch a method can be easily implemented and integrated into the clinical\nprocess to help care providers estimate the next steps of patients", "published": "2023-09-20 13:24:12", "link": "http://arxiv.org/abs/2309.11295v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Data Collection and Unsupervised Learning for Code-switched\n  Tunisian Arabic Automatic Speech Recognition", "abstract": "Crafting an effective Automatic Speech Recognition (ASR) solution for\ndialects demands innovative approaches that not only address the data scarcity\nissue but also navigate the intricacies of linguistic diversity. In this paper,\nwe address the aforementioned ASR challenge, focusing on the Tunisian dialect.\nFirst, textual and audio data is collected and in some cases annotated. Second,\nwe explore self-supervision, semi-supervision and few-shot code-switching\napproaches to push the state-of-the-art on different Tunisian test sets;\ncovering different acoustic, linguistic and prosodic conditions. Finally, and\ngiven the absence of conventional spelling, we produce a human evaluation of\nour transcripts to avoid the noise coming from spelling inadequacies in our\ntesting references. Our models, allowing to transcribe audio samples in a\nlinguistic mix involving Tunisian Arabic, English and French, and all the data\nused during training and testing are released for public use and further\nimprovements.", "published": "2023-09-20 13:56:27", "link": "http://arxiv.org/abs/2309.11327v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Incremental Blockwise Beam Search for Simultaneous Speech Translation\n  with Controllable Quality-Latency Tradeoff", "abstract": "Blockwise self-attentional encoder models have recently emerged as one\npromising end-to-end approach to simultaneous speech translation. These models\nemploy a blockwise beam search with hypothesis reliability scoring to determine\nwhen to wait for more input speech before translating further. However, this\nmethod maintains multiple hypotheses until the entire speech input is consumed\n-- this scheme cannot directly show a single \\textit{incremental} translation\nto users. Further, this method lacks mechanisms for \\textit{controlling} the\nquality vs. latency tradeoff. We propose a modified incremental blockwise beam\nsearch incorporating local agreement or hold-$n$ policies for quality-latency\ncontrol. We apply our framework to models trained for online or offline\ntranslation and demonstrate that both types can be effectively used in online\nmode.\n  Experimental results on MuST-C show 0.6-3.6 BLEU improvement without changing\nlatency or 0.8-1.4 s latency improvement without changing quality.", "published": "2023-09-20 14:59:06", "link": "http://arxiv.org/abs/2309.11379v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Studying Lobby Influence in the European Parliament", "abstract": "We present a method based on natural language processing (NLP), for studying\nthe influence of interest groups (lobbies) in the law-making process in the\nEuropean Parliament (EP). We collect and analyze novel datasets of lobbies'\nposition papers and speeches made by members of the EP (MEPs). By comparing\nthese texts on the basis of semantic similarity and entailment, we are able to\ndiscover interpretable links between MEPs and lobbies. In the absence of a\nground-truth dataset of such links, we perform an indirect validation by\ncomparing the discovered links with a dataset, which we curate, of retweet\nlinks between MEPs and lobbies, and with the publicly disclosed meetings of\nMEPs. Our best method achieves an AUC score of 0.77 and performs significantly\nbetter than several baselines. Moreover, an aggregate analysis of the\ndiscovered links, between groups of related lobbies and political groups of\nMEPs, correspond to the expectations from the ideology of the groups (e.g.,\ncenter-left groups are associated with social causes). We believe that this\nwork, which encompasses the methodology, datasets, and results, is a step\ntowards enhancing the transparency of the intricate decision-making processes\nwithin democratic institutions.", "published": "2023-09-20 15:03:30", "link": "http://arxiv.org/abs/2309.11381v1", "categories": ["cs.CL", "cs.CE", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Discuss Before Moving: Visual Language Navigation via Multi-expert\n  Discussions", "abstract": "Visual language navigation (VLN) is an embodied task demanding a wide range\nof skills encompassing understanding, perception, and planning. For such a\nmultifaceted challenge, previous VLN methods totally rely on one model's own\nthinking to make predictions within one round. However, existing models, even\nthe most advanced large language model GPT4, still struggle with dealing with\nmultiple tasks by single-round self-thinking. In this work, drawing inspiration\nfrom the expert consultation meeting, we introduce a novel zero-shot VLN\nframework. Within this framework, large models possessing distinct abilities\nare served as domain experts. Our proposed navigation agent, namely DiscussNav,\ncan actively discuss with these experts to collect essential information before\nmoving at every step. These discussions cover critical navigation subtasks like\ninstruction understanding, environment perception, and completion estimation.\nThrough comprehensive experiments, we demonstrate that discussions with domain\nexperts can effectively facilitate navigation by perceiving\ninstruction-relevant information, correcting inadvertent errors, and sifting\nthrough in-consistent movement decisions. The performances on the\nrepresentative VLN task R2R show that our method surpasses the leading\nzero-shot VLN model by a large margin on all metrics. Additionally, real-robot\nexperiments display the obvious advantages of our method over single-round\nself-thinking.", "published": "2023-09-20 15:04:49", "link": "http://arxiv.org/abs/2309.11382v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Long-Form End-to-End Speech Translation via Latent Alignment\n  Segmentation", "abstract": "Current simultaneous speech translation models can process audio only up to a\nfew seconds long. Contemporary datasets provide an oracle segmentation into\nsentences based on human-annotated transcripts and translations. However, the\nsegmentation into sentences is not available in the real world. Current speech\nsegmentation approaches either offer poor segmentation quality or have to trade\nlatency for quality. In this paper, we propose a novel segmentation approach\nfor a low-latency end-to-end speech translation. We leverage the existing\nspeech translation encoder-decoder architecture with ST CTC and show that it\ncan perform the segmentation task without supervision or additional parameters.\nTo the best of our knowledge, our method is the first that allows an actual\nend-to-end simultaneous speech translation, as the same model is used for\ntranslation and segmentation at the same time. On a diverse set of language\npairs and in- and out-of-domain data, we show that the proposed approach\nachieves state-of-the-art quality at no additional computational cost.", "published": "2023-09-20 15:10:12", "link": "http://arxiv.org/abs/2309.11384v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "You Only Look at Screens: Multimodal Chain-of-Action Agents", "abstract": "Autonomous graphical user interface (GUI) agents aim to facilitate task\nautomation by interacting with the user interface without manual intervention.\nRecent studies have investigated eliciting the capabilities of large language\nmodels (LLMs) for effective engagement in diverse environments. To align with\nthe input-output requirement of LLMs, most existing approaches are developed\nunder a sandbox setting where they rely on external tools and\napplication-specific APIs to parse the environment into textual elements and\ninterpret the predicted actions. Consequently, those approaches often grapple\nwith inference inefficiency and error propagation risks. To mitigate the\nchallenges, we introduce Auto-GUI, a multimodal solution that directly\ninteracts with the interface, bypassing the need for environment parsing or\nreliance on application-dependent APIs. Moreover, we propose a chain-of-action\ntechnique -- leveraging a series of intermediate previous action histories and\nfuture action plans -- to help the agent decide what action to execute. We\nevaluate our approach on a new device-control benchmark AITW with 30$K$ unique\ninstructions, spanning multi-step tasks such as application operation, web\nsearching, and web shopping. Experimental results show that Auto-GUI achieves\nstate-of-the-art performance with an action type prediction accuracy of 90\\%\nand an overall action success rate of 74\\%. Code is publicly available at\nhttps://github.com/cooelf/Auto-GUI.", "published": "2023-09-20 16:12:32", "link": "http://arxiv.org/abs/2309.11436v4", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Text2Reward: Reward Shaping with Language Models for Reinforcement\n  Learning", "abstract": "Designing reward functions is a longstanding challenge in reinforcement\nlearning (RL); it requires specialized knowledge or domain data, leading to\nhigh costs for development. To address this, we introduce Text2Reward, a\ndata-free framework that automates the generation and shaping of dense reward\nfunctions based on large language models (LLMs). Given a goal described in\nnatural language, Text2Reward generates shaped dense reward functions as an\nexecutable program grounded in a compact representation of the environment.\nUnlike inverse RL and recent work that uses LLMs to write sparse reward codes\nor unshaped dense rewards with a constant function across timesteps,\nText2Reward produces interpretable, free-form dense reward codes that cover a\nwide range of tasks, utilize existing packages, and allow iterative refinement\nwith human feedback. We evaluate Text2Reward on two robotic manipulation\nbenchmarks (ManiSkill2, MetaWorld) and two locomotion environments of MuJoCo.\nOn 13 of the 17 manipulation tasks, policies trained with generated reward\ncodes achieve similar or better task success rates and convergence speed than\nexpert-written reward codes. For locomotion tasks, our method learns six novel\nlocomotion behaviors with a success rate exceeding 94%. Furthermore, we show\nthat the policies trained in the simulator with our method can be deployed in\nthe real world. Finally, Text2Reward further improves the policies by refining\ntheir reward functions with human feedback. Video results are available at\nhttps://text-to-reward.github.io/ .", "published": "2023-09-20 17:39:13", "link": "http://arxiv.org/abs/2309.11489v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.LG"}
{"title": "DreamLLM: Synergistic Multimodal Comprehension and Creation", "abstract": "This paper presents DreamLLM, a learning framework that first achieves\nversatile Multimodal Large Language Models (MLLMs) empowered with frequently\noverlooked synergy between multimodal comprehension and creation. DreamLLM\noperates on two fundamental principles. The first focuses on the generative\nmodeling of both language and image posteriors by direct sampling in the raw\nmultimodal space. This approach circumvents the limitations and information\nloss inherent to external feature extractors like CLIP, and a more thorough\nmultimodal understanding is obtained. Second, DreamLLM fosters the generation\nof raw, interleaved documents, modeling both text and image contents, along\nwith unstructured layouts. This allows DreamLLM to learn all conditional,\nmarginal, and joint multimodal distributions effectively. As a result, DreamLLM\nis the first MLLM capable of generating free-form interleaved content.\nComprehensive experiments highlight DreamLLM's superior performance as a\nzero-shot multimodal generalist, reaping from the enhanced learning synergy.\nProject page: https://dreamllm.github.io.", "published": "2023-09-20 17:58:05", "link": "http://arxiv.org/abs/2309.11499v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model", "abstract": "We introduce the Bittensor Language Model, called \"BTLM-3B-8K\", a new\nstate-of-the-art 3 billion parameter open-source language model. BTLM-3B-8K was\ntrained on 627B tokens from the SlimPajama dataset with a mixture of 2,048 and\n8,192 context lengths. BTLM-3B-8K outperforms all existing 3B parameter models\nby 2-5.5% across downstream tasks. BTLM-3B-8K is even competitive with some 7B\nparameter models. Additionally, BTLM-3B-8K provides excellent long context\nperformance, outperforming MPT-7B-8K and XGen-7B-8K on tasks up to 8,192\ncontext length. We trained the model on a cleaned and deduplicated SlimPajama\ndataset; aggressively tuned the \\textmu P hyperparameters and schedule; used\nALiBi position embeddings; and adopted the SwiGLU nonlinearity.\n  On Hugging Face, the most popular models have 7B parameters, indicating that\nusers prefer the quality-size ratio of 7B models. Compacting the 7B parameter\nmodel to one with 3B parameters, with little performance impact, is an\nimportant milestone. BTLM-3B-8K needs only 3GB of memory with 4-bit precision\nand takes 2.5x less inference compute than 7B models, helping to open up access\nto a powerful language model on mobile and edge devices. BTLM-3B-8K is\navailable under an Apache 2.0 license on Hugging Face:\nhttps://huggingface.co/cerebras/btlm-3b-8k-base.", "published": "2023-09-20 18:12:56", "link": "http://arxiv.org/abs/2309.11568v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "SCREWS: A Modular Framework for Reasoning with Revisions", "abstract": "Large language models (LLMs) can improve their accuracy on various tasks\nthrough iteratively refining and revising their output based on feedback. We\nobserve that these revisions can introduce errors, in which case it is better\nto roll back to a previous result. Further, revisions are typically\nhomogeneous: they use the same reasoning method that produced the initial\nanswer, which may not correct errors. To enable exploration in this space, we\npresent SCREWS, a modular framework for reasoning with revisions. It is\ncomprised of three main modules: Sampling, Conditional Resampling, and\nSelection, each consisting of sub-modules that can be hand-selected per task.\nWe show that SCREWS not only unifies several previous approaches under a common\nframework, but also reveals several novel strategies for identifying improved\nreasoning chains. We evaluate our framework with state-of-the-art LLMs (ChatGPT\nand GPT-4) on a diverse set of reasoning tasks and uncover useful new reasoning\nstrategies for each: arithmetic word problems, multi-hop question answering,\nand code debugging. Heterogeneous revision strategies prove to be important, as\ndoes selection between original and revised candidates.", "published": "2023-09-20 15:59:54", "link": "http://arxiv.org/abs/2309.13075v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "A Neural TTS System with Parallel Prosody Transfer from Unseen Speakers", "abstract": "Modern neural TTS systems are capable of generating natural and expressive\nspeech when provided with sufficient amounts of training data. Such systems can\nbe equipped with prosody-control functionality, allowing for more direct\nshaping of the speech output at inference time. In some TTS applications, it\nmay be desirable to have an option that guides the TTS system with an ad-hoc\nspeech recording exemplar to impose an implicit fine-grained, user-preferred\nprosodic realization for certain input prompts. In this work we present a\nfirst-of-its-kind neural TTS system equipped with such functionality to\ntransfer the prosody from a parallel text recording from an unseen speaker. We\ndemonstrate that the proposed system can precisely transfer the speech prosody\nfrom novel speakers to various trained TTS voices with no quality degradation,\nwhile preserving the target TTS speakers' identity, as evaluated by a set of\nsubjective listening experiments.", "published": "2023-09-20 17:33:47", "link": "http://arxiv.org/abs/2309.11487v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Deep Complex U-Net with Conformer for Audio-Visual Speech Enhancement", "abstract": "Recent studies have increasingly acknowledged the advantages of incorporating\nvisual data into speech enhancement (SE) systems. In this paper, we introduce a\nnovel audio-visual SE approach, termed DCUC-Net (deep complex U-Net with\nconformer network). The proposed DCUC-Net leverages complex domain features and\na stack of conformer blocks. The encoder and decoder of DCUC-Net are designed\nusing a complex U-Net-based framework. The audio and visual signals are\nprocessed using a complex encoder and a ResNet-18 model, respectively. These\nprocessed signals are then fused using the conformer blocks and transformed\ninto enhanced speech waveforms via a complex decoder. The conformer blocks\nconsist of a combination of self-attention mechanisms and convolutional\noperations, enabling DCUC-Net to effectively capture both global and local\naudio-visual dependencies. Our experimental results demonstrate the\neffectiveness of DCUC-Net, as it outperforms the baseline model from the\nCOG-MHEAR AVSE Challenge 2023 by a notable margin of 0.14 in terms of PESQ.\nAdditionally, the proposed DCUC-Net performs comparably to a state-of-the-art\nmodel and outperforms all other compared models on the Taiwan Mandarin speech\nwith video (TMSV) dataset.", "published": "2023-09-20 04:39:28", "link": "http://arxiv.org/abs/2309.11059v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Joint Minimum Processing Beamforming and Near-end Listening Enhancement", "abstract": "We consider speech enhancement for signals picked up in one noisy environment\nthat must be rendered to a listener in another noisy environment. For both\nfar-end noise reduction and near-end listening enhancement, it has been shown\nthat excessive focus on noise suppression or intelligibility maximization may\nlead to excessive speech distortions and quality degradations in favorable\nnoise conditions, where intelligibility is already at ceiling level. Recently\n[1,2] propose to remedy this with a minimum processing framework that either\nreduces noise or enhances listening a minimum amount given that a certain\nintelligibility criterion is still satisfied Additionally, it has been shown\nthat joint consideration of both environments improves speech enhancement\nperformance. In this paper, we formulate a joint far- and near-end minimum\nprocessing framework, that improves intelligibility while limiting speech\ndistortions in favorable noise conditions. We provide closed-form solutions to\nspecific boundary scenarios and investigate performance for the general case\nusing numerical optimization. We also show concatenating existing minimum\nprocessing far- and near-end enhancement methods preserves the effects of the\ninitial methods. Results show that the joint optimization can further improve\nperformance compared to the concatenated approach.", "published": "2023-09-20 12:11:06", "link": "http://arxiv.org/abs/2309.11243v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Directional Source Separation for Robust Speech Recognition on Smart\n  Glasses", "abstract": "Modern smart glasses leverage advanced audio sensing and machine learning\ntechnologies to offer real-time transcribing and captioning services,\nconsiderably enriching human experiences in daily communications. However, such\nsystems frequently encounter challenges related to environmental noises,\nresulting in degradation to speech recognition and speaker change detection. To\nimprove voice quality, this work investigates directional source separation\nusing the multi-microphone array. We first explore multiple beamformers to\nassist source separation modeling by strengthening the directional properties\nof speech signals. In addition to relying on predetermined beamformers, we\ninvestigate neural beamforming in multi-channel source separation,\ndemonstrating that automatic learning directional characteristics effectively\nimproves separation quality. We further compare the ASR performance leveraging\nseparated outputs to noisy inputs. Our results show that directional source\nseparation benefits ASR for the wearer but not for the conversation partner.\nLastly, we perform the joint training of the directional source separation and\nASR model, achieving the best overall ASR performance.", "published": "2023-09-20 01:23:16", "link": "http://arxiv.org/abs/2309.10993v1", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Ensembling Multilingual Pre-Trained Models for Predicting Multi-Label\n  Regression Emotion Share from Speech", "abstract": "Speech emotion recognition has evolved from research to practical\napplications. Previous studies of emotion recognition from speech have focused\non developing models on certain datasets like IEMOCAP. The lack of data in the\ndomain of emotion modeling emerges as a challenge to evaluate models in the\nother dataset, as well as to evaluate speech emotion recognition models that\nwork in a multilingual setting. This paper proposes an ensemble learning to\nfuse results of pre-trained models for emotion share recognition from speech.\nThe models were chosen to accommodate multilingual data from English and\nSpanish. The results show that ensemble learning can improve the performance of\nthe baseline model with a single model and the previous best model from the\nlate fusion. The performance is measured using the Spearman rank correlation\ncoefficient since the task is a regression problem with ranking values. A\nSpearman rank correlation coefficient of 0.537 is reported for the test set,\nwhile for the development set, the score is 0.524. These scores are higher than\nthe previous study of a fusion method from monolingual data, which achieved\nscores of 0.476 for the test and 0.470 for the development.", "published": "2023-09-20 02:28:00", "link": "http://arxiv.org/abs/2309.11014v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Investigating Personalization Methods in Text to Music Generation", "abstract": "In this work, we investigate the personalization of text-to-music diffusion\nmodels in a few-shot setting. Motivated by recent advances in the computer\nvision domain, we are the first to explore the combination of pre-trained\ntext-to-audio diffusers with two established personalization methods. We\nexperiment with the effect of audio-specific data augmentation on the overall\nsystem performance and assess different training strategies. For evaluation, we\nconstruct a novel dataset with prompts and music clips. We consider both\nembedding-based and music-specific metrics for quantitative evaluation, as well\nas a user study for qualitative evaluation. Our analysis shows that similarity\nmetrics are in accordance with user preferences and that current\npersonalization approaches tend to learn rhythmic music constructs more easily\nthan melody. The code, dataset, and example material of this study are open to\nthe research community.", "published": "2023-09-20 08:36:34", "link": "http://arxiv.org/abs/2309.11140v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automatic Bat Call Classification using Transformer Networks", "abstract": "Automatically identifying bat species from their echolocation calls is a\ndifficult but important task for monitoring bats and the ecosystem they live\nin. Major challenges in automatic bat call identification are high call\nvariability, similarities between species, interfering calls and lack of\nannotated data. Many currently available models suffer from relatively poor\nperformance on real-life data due to being trained on single call datasets and,\nmoreover, are often too slow for real-time classification. Here, we propose a\nTransformer architecture for multi-label classification with potential\napplications in real-time classification scenarios. We train our model on\nsynthetically generated multi-species recordings by merging multiple bats calls\ninto a single recording with multiple simultaneous calls. Our approach achieves\na single species accuracy of 88.92% (F1-score of 84.23%) and a multi species\nmacro F1-score of 74.40% on our test set. In comparison to three other tools on\nthe independent and publicly available dataset ChiroVox, our model achieves at\nleast 25.82% better accuracy for single species classification and at least\n6.9% better macro F1-score for multi species classification.", "published": "2023-09-20 11:15:56", "link": "http://arxiv.org/abs/2309.11218v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "AudioFool: Fast, Universal and synchronization-free Cross-Domain Attack\n  on Speech Recognition", "abstract": "Automatic Speech Recognition systems have been shown to be vulnerable to\nadversarial attacks that manipulate the command executed on the device. Recent\nresearch has focused on exploring methods to create such attacks, however, some\nissues relating to Over-The-Air (OTA) attacks have not been properly addressed.\nIn our work, we examine the needed properties of robust attacks compatible with\nthe OTA model, and we design a method of generating attacks with arbitrary such\ndesired properties, namely the invariance to synchronization, and the\nrobustness to filtering: this allows a Denial-of-Service (DoS) attack against\nASR systems. We achieve these characteristics by constructing attacks in a\nmodified frequency domain through an inverse Fourier transform. We evaluate our\nmethod on standard keyword classification tasks and analyze it in OTA, and we\nanalyze the properties of the cross-domain attacks to explain the efficiency of\nthe approach.", "published": "2023-09-20 16:59:22", "link": "http://arxiv.org/abs/2309.11462v1", "categories": ["cs.CR", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.CR"}
{"title": "Auto-ACD: A Large-scale Dataset for Audio-Language Representation\n  Learning", "abstract": "Recently, the AI community has made significant strides in developing\npowerful foundation models, driven by large-scale multimodal datasets. However,\nfor audio representation learning, existing datasets suffer from limitations in\nthe following aspects: insufficient volume, simplistic content, and arduous\ncollection procedures. To establish an audio dataset with high-quality\ncaptions, we propose an innovative, automatic approach leveraging multimodal\ninputs, such as video frames, audio streams. Specifically, we construct a\nlarge-scale, high-quality, audio-language dataset, named as Auto-ACD,\ncomprising over 1.5M audio-text pairs. We exploit a series of pre-trained\nmodels or APIs, to determine audio-visual synchronisation, generate image\ncaptions, object detection, or audio tags for specific videos. Subsequently, we\nemploy LLM to paraphrase a congruent caption for each audio, guided by the\nextracted multi-modality clues. To demonstrate the effectiveness of the\nproposed dataset, we train widely used models on our dataset and show\nperformance improvement on various downstream tasks, for example,\naudio-language retrieval, audio captioning, zero-shot classification. In\naddition, we establish a novel benchmark with environmental information and\nprovide a benchmark for audio-text tasks.", "published": "2023-09-20 17:59:32", "link": "http://arxiv.org/abs/2309.11500v4", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
