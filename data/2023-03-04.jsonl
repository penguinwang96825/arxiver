{"title": "DiTTO: A Feature Representation Imitation Approach for Improving\n  Cross-Lingual Transfer", "abstract": "Zero-shot cross-lingual transfer is promising, however has been shown to be\nsub-optimal, with inferior transfer performance across low-resource languages.\nIn this work, we envision languages as domains for improving zero-shot transfer\nby jointly reducing the feature incongruity between the source and the target\nlanguage and increasing the generalization capabilities of pre-trained\nmultilingual transformers. We show that our approach, DiTTO, significantly\noutperforms the standard zero-shot fine-tuning method on multiple datasets\nacross all languages using solely unlabeled instances in the target language.\nEmpirical results show that jointly reducing feature incongruity for multiple\ntarget languages is vital for successful cross-lingual transfer. Moreover, our\nmodel enables better cross-lingual transfer than standard fine-tuning methods,\neven in the few-shot setting.", "published": "2023-03-04 08:42:50", "link": "http://arxiv.org/abs/2303.02357v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RweetMiner: Automatic identification and categorization of help requests\n  on twitter during disasters", "abstract": "Catastrophic events create uncertain situations for humanitarian\norganizations locating and providing aid to affected people. Many people turn\nto social media during disasters for requesting help and/or providing relief to\nothers. However, the majority of social media posts seeking help could not\nproperly be detected and remained concealed because often they are noisy and\nill-formed. Existing systems lack in planning an effective strategy for tweet\npreprocessing and grasping the contexts of tweets. This research, first of all,\nformally defines request tweets in the context of social networking sites,\nhereafter rweets, along with their different primary types and sub-types. Our\nmain contributions are the identification and categorization of rweets. For\nrweet identification, we employ two approaches, namely a rule-based and\nlogistic regression, and show their high precision and F1 scores. The rweets\nclassification into sub-types such as medical, food, and shelter, using\nlogistic regression shows promising results and outperforms existing works.\nFinally, we introduce an architecture to store intermediate data to accelerate\nthe development process of the machine learning classifiers.", "published": "2023-03-04 12:21:45", "link": "http://arxiv.org/abs/2303.02399v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Self-tuning hyper-parameters for unsupervised cross-lingual tokenization", "abstract": "We explore the possibility of meta-learning for the language-independent\nunsupervised tokenization problem for English, Russian, and Chinese. We\nimplement the meta-learning approach for automatic determination of\nhyper-parameters of the unsupervised tokenization model proposed in earlier\nworks, relying on various human-independent fitness functions such as\nnormalised anti-entropy, compression factor and cross-split F1 score, as well\nas additive and multiplicative composite combinations of the three metrics,\ntesting them against the conventional F1 tokenization score. We find a fairly\ngood correlation between the latter and the additive combination of the former\nthree metrics for English and Russian. In case of Chinese, we find a\nsignificant correlation between the F 1 score and the compression factor. Our\nresults suggest the possibility of robust unsupervised tokenization of\nlow-resource and dead languages and allow us to think about human languages in\nterms of the evolution of efficient symbolic communication codes with different\nstructural optimisation schemes that have evolved in different human cultures.", "published": "2023-03-04 14:23:02", "link": "http://arxiv.org/abs/2303.02427v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Lon-ea at SemEval-2023 Task 11: A Comparison of Activation Functions for\n  Soft and Hard Label Prediction", "abstract": "We study the influence of different activation functions in the output layer\nof deep neural network models for soft and hard label prediction in the\nlearning with disagreement task. In this task, the goal is to quantify the\namount of disagreement via predicting soft labels. To predict the soft labels,\nwe use BERT-based preprocessors and encoders and vary the activation function\nused in the output layer, while keeping other parameters constant. The soft\nlabels are then used for the hard label prediction. The activation functions\nconsidered are sigmoid as well as a step-function that is added to the model\npost-training and a sinusoidal activation function, which is introduced for the\nfirst time in this paper.", "published": "2023-03-04 17:59:43", "link": "http://arxiv.org/abs/2303.02468v4", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Variational Quantum Classifiers for Natural-Language Text", "abstract": "As part of the recent research effort on quantum natural language processing\n(QNLP), variational quantum sentence classifiers (VQSCs) have been implemented\nand supported in lambeq / DisCoPy, based on the DisCoCat model of sentence\nmeaning. We discuss in some detail VQSCs, including category theory, DisCoCat\nfor modeling sentence as string diagram, and DisCoPy for encoding string\ndiagram as parameterized quantum circuit. Many NLP tasks, however, require the\nhandling of text consisting of multiple sentences, which is not supported in\nlambeq / DisCoPy. A good example is sentiment classification of customer\nfeedback or product review. We discuss three potential approaches to\nvariational quantum text classifiers (VQTCs), in line with VQSCs. The first is\na weighted bag-of-sentences approach which treats text as a group of\nindependent sentences with task-specific sentence weighting. The second is a\ncoreference resolution approach which treats text as a consolidation of its\nmember sentences with coreferences among them resolved. Both approaches are\nbased on the DisCoCat model and should be implementable in lambeq / DisCoCat.\nThe third approach, on the other hand, is based on the DisCoCirc model which\nconsiders both ordering of sentences and interaction of words in composing text\nmeaning from word and sentence meanings. DisCoCirc makes fundamental\nmodification of DisCoCat since a sentence in DisCoCirc updates meanings of\nwords, whereas all meanings are static in DisCoCat. It is not clear if\nDisCoCirc can be implemented in lambeq / DisCoCat without breaking DisCoCat.", "published": "2023-03-04 18:00:05", "link": "http://arxiv.org/abs/2303.02469v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Model-Agnostic Meta-Learning for Multilingual Hate Speech Detection", "abstract": "Hate speech in social media is a growing phenomenon, and detecting such toxic\ncontent has recently gained significant traction in the research community.\nExisting studies have explored fine-tuning language models (LMs) to perform\nhate speech detection, and these solutions have yielded significant\nperformance. However, most of these studies are limited to detecting hate\nspeech only in English, neglecting the bulk of hateful content that is\ngenerated in other languages, particularly in low-resource languages.\nDeveloping a classifier that captures hate speech and nuances in a low-resource\nlanguage with limited data is extremely challenging. To fill the research gap,\nwe propose HateMAML, a model-agnostic meta-learning-based framework that\neffectively performs hate speech detection in low-resource languages. HateMAML\nutilizes a self-supervision strategy to overcome the limitation of data\nscarcity and produces better LM initialization for fast adaptation to an unseen\ntarget language (i.e., cross-lingual transfer) or other hate speech datasets\n(i.e., domain generalization). Extensive experiments are conducted on five\ndatasets across eight different low-resource languages. The results show that\nHateMAML outperforms the state-of-the-art baselines by more than 3% in the\ncross-domain multilingual transfer setting. We also conduct ablation studies to\nanalyze the characteristics of HateMAML.", "published": "2023-03-04 22:28:29", "link": "http://arxiv.org/abs/2303.02513v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "MathPrompter: Mathematical Reasoning using Large Language Models", "abstract": "Large Language Models (LLMs) have limited performance when solving arithmetic\nreasoning tasks and often provide incorrect answers. Unlike natural language\nunderstanding, math problems typically have a single correct answer, making the\ntask of generating accurate solutions more challenging for LLMs. To the best of\nour knowledge, we are not aware of any LLMs that indicate their level of\nconfidence in their responses which fuels a trust deficit in these models\nimpeding their adoption. To address this deficiency, we propose `MathPrompter',\na technique that improves performance of LLMs on arithmetic problems along with\nincreased reliance in the predictions. MathPrompter uses the Zero-shot\nchain-of-thought prompting technique to generate multiple Algebraic expressions\nor Python functions to solve the same math problem in different ways and\nthereby raise the confidence level in the output results. This is in contrast\nto other prompt based CoT methods, where there is no check on the validity of\nthe intermediate steps followed. Our technique improves over state-of-the-art\non the MultiArith dataset ($78.7\\%\\rightarrow92.5\\%$) evaluated using 175B\nparameter GPT-based LLM.", "published": "2023-03-04 04:43:49", "link": "http://arxiv.org/abs/2303.05398v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Contribution of Knowledge in Visiolinguistic Learning: A Survey on\n  Tasks and Challenges", "abstract": "Recent advancements in visiolinguistic (VL) learning have allowed the\ndevelopment of multiple models and techniques that offer several impressive\nimplementations, able to currently resolve a variety of tasks that require the\ncollaboration of vision and language. Current datasets used for VL pre-training\nonly contain a limited amount of visual and linguistic knowledge, thus\nsignificantly limiting the generalization capabilities of many VL models.\nExternal knowledge sources such as knowledge graphs (KGs) and Large Language\nModels (LLMs) are able to cover such generalization gaps by filling in missing\nknowledge, resulting in the emergence of hybrid architectures. In the current\nsurvey, we analyze tasks that have benefited from such hybrid approaches.\nMoreover, we categorize existing knowledge sources and types, proceeding to\ndiscussion regarding the KG vs LLM dilemma and its potential impact to future\nhybrid approaches.", "published": "2023-03-04 13:12:18", "link": "http://arxiv.org/abs/2303.02411v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "ESD: Expected Squared Difference as a Tuning-Free Trainable Calibration\n  Measure", "abstract": "Studies have shown that modern neural networks tend to be poorly calibrated\ndue to over-confident predictions. Traditionally, post-processing methods have\nbeen used to calibrate the model after training. In recent years, various\ntrainable calibration measures have been proposed to incorporate them directly\ninto the training process. However, these methods all incorporate internal\nhyperparameters, and the performance of these calibration objectives relies on\ntuning these hyperparameters, incurring more computational costs as the size of\nneural networks and datasets become larger. As such, we present Expected\nSquared Difference (ESD), a tuning-free (i.e., hyperparameter-free) trainable\ncalibration objective loss, where we view the calibration error from the\nperspective of the squared difference between the two expectations. With\nextensive experiments on several architectures (CNNs, Transformers) and\ndatasets, we demonstrate that (1) incorporating ESD into the training improves\nmodel calibration in various batch size settings without the need for internal\nhyperparameter tuning, (2) ESD yields the best-calibrated results compared with\nprevious approaches, and (3) ESD drastically improves the computational costs\nrequired for calibration during training due to the absence of internal\nhyperparameter. The code is publicly accessible at\nhttps://github.com/hee-suk-yoon/ESD.", "published": "2023-03-04 18:06:36", "link": "http://arxiv.org/abs/2303.02472v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Could a Large Language Model be Conscious?", "abstract": "There has recently been widespread discussion of whether large language\nmodels might be sentient. Should we take this idea seriously? I will break down\nthe strongest reasons for and against. Given mainstream assumptions in the\nscience of consciousness, there are significant obstacles to consciousness in\ncurrent models: for example, their lack of recurrent processing, a global\nworkspace, and unified agency. At the same time, it is quite possible that\nthese obstacles will be overcome in the next decade or so. I conclude that\nwhile it is somewhat unlikely that current large language models are conscious,\nwe should take seriously the possibility that successors to large language\nmodels may be conscious in the not-too-distant future.", "published": "2023-03-04 19:14:20", "link": "http://arxiv.org/abs/2303.07103v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "The DKU Post-Challenge Audio-Visual Wake Word Spotting System for the\n  2021 MISP Challenge: Deep Analysis", "abstract": "This paper further explores our previous wake word spotting system ranked\n2-nd in Track 1 of the MISP Challenge 2021. First, we investigate a robust\nunimodal approach based on 3D and 2D convolution and adopt the simple attention\nmodule (SimAM) for our system to improve performance. Second, we explore\ndifferent combinations of data augmentation methods for better performance.\nFinally, we study the fusion strategies, including score-level, cascaded and\nneural fusion. Our proposed multimodal system leverages multimodal features and\nuses the complementary visual information to mitigate the performance\ndegradation of audio-only systems in complex acoustic scenarios. Our system\nobtains a false reject rate of 2.15% and a false alarm rate of 3.44% in the\nevaluation set of the competition database, which achieves the new\nstate-of-the-art performance by 21% relative improvement compared to previous\nsystems.", "published": "2023-03-04 07:30:39", "link": "http://arxiv.org/abs/2303.02348v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Requirements for Mass Adoption of Assistive Listening Technology by the\n  General Public", "abstract": "Assistive listening systems (ALSs) dramatically increase speech\nintelligibility and reduce listening effort. It is very likely that essentially\neveryone, not only individuals with hearing loss, would benefit from the\nincreased signal-to-noise ratio an ALS provides in almost any listening\nscenario. However, ALSs are rarely used by anyone other than people with severe\nto profound hearing losses. To date, the reasons for this poor adoption have\nnot been systematically investigated.\n  The authors hypothesize that the reasons for poor adoption of assistive\nlistening technology include (1) an inability to use personally owned receiving\ndevices, (2) a lack of high-fidelity stereo sound, (3) receiving devices not\nproviding an unoccluded listening experience, (4) distortion from alignment\ndelay and (5) a lack of automatic connectivity to an available assistive\nlistening audio signal.\n  We propose solutions to each of these problems in an effort to pave the way\nfor mass adoption of assistive listening technology by the general public.", "published": "2023-03-04 23:02:50", "link": "http://arxiv.org/abs/2303.02523v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Fixed-point quantization aware training for on-device keyword-spotting", "abstract": "Fixed-point (FXP) inference has proven suitable for embedded devices with\nlimited computational resources, and yet model training is continually\nperformed in floating-point (FLP). FXP training has not been fully explored and\nthe non-trivial conversion from FLP to FXP presents unavoidable performance\ndrop. We propose a novel method to train and obtain FXP convolutional\nkeyword-spotting (KWS) models. We combine our methodology with two\nquantization-aware-training (QAT) techniques - squashed weight distribution and\nabsolute cosine regularization for model parameters, and propose techniques for\nextending QAT over transient variables, otherwise neglected by previous\nparadigms. Experimental results on the Google Speech Commands v2 dataset show\nthat we can reduce model precision up to 4-bit with no loss in accuracy.\nFurthermore, on an in-house KWS dataset, we show that our 8-bit FXP-QAT models\nhave a 4-6% improvement in relative false discovery rate at fixed false reject\nrate compared to full precision FLP models. During inference we argue that\nFXP-QAT eliminates q-format normalization and enables the use of low-bit\naccumulators while maximizing SIMD throughput to reduce user perceived latency.\nWe demonstrate that we can reduce execution time by 68% without compromising\nKWS model's predictive performance or requiring model architectural changes.\nOur work provides novel findings that aid future research in this area and\nenable accurate and efficient models.", "published": "2023-03-04 01:06:16", "link": "http://arxiv.org/abs/2303.02284v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "eess.SP"], "primary_category": "eess.AS"}
{"title": "A General Framework for Learning Procedural Audio Models of\n  Environmental Sounds", "abstract": "This paper introduces the Procedural (audio) Variational autoEncoder (ProVE)\nframework as a general approach to learning Procedural Audio PA models of\nenvironmental sounds with an improvement to the realism of the synthesis while\nmaintaining provision of control over the generated sound through adjustable\nparameters. The framework comprises two stages: (i) Audio Class Representation,\nin which a latent representation space is defined by training an audio\nautoencoder, and (ii) Control Mapping, in which a joint function of\nstatic/temporal control variables derived from the audio and a random sample of\nuniform noise is learned to replace the audio encoder. We demonstrate the use\nof ProVE through the example of footstep sound effects on various surfaces. Our\nresults show that ProVE models outperform both classical PA models and an\nadversarial-based approach in terms of sound fidelity, as measured by Fr\\'echet\nAudio Distance (FAD), Maximum Mean Discrepancy (MMD), and subjective\nevaluations, making them feasible tools for sound design workflows.", "published": "2023-03-04 12:12:26", "link": "http://arxiv.org/abs/2303.02396v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
