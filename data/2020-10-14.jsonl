{"title": "Google Crowdsourced Speech Corpora and Related Open-Source Resources for\n  Low-Resource Languages and Dialects: An Overview", "abstract": "This paper presents an overview of a program designed to address the growing\nneed for developing freely available speech resources for under-represented\nlanguages. At present we have released 38 datasets for building text-to-speech\nand automatic speech recognition applications for languages and dialects of\nSouth and Southeast Asia, Africa, Europe and South America. The paper describes\nthe methodology used for developing such corpora and presents some of our\nfindings that could benefit under-represented language communities.", "published": "2020-10-14 02:24:04", "link": "http://arxiv.org/abs/2010.06778v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Self-supervised Representation Learning of Sentence Structure for\n  Authorship Attribution", "abstract": "Syntactic structure of sentences in a document substantially informs about\nits authorial writing style. Sentence representation learning has been widely\nexplored in recent years and it has been shown that it improves the\ngeneralization of different downstream tasks across many domains. Even though\nutilizing probing methods in several studies suggests that these learned\ncontextual representations implicitly encode some amount of syntax, explicit\nsyntactic information further improves the performance of deep neural models in\nthe domain of authorship attribution. These observations have motivated us to\ninvestigate the explicit representation learning of syntactic structure of\nsentences. In this paper, we propose a self-supervised framework for learning\nstructural representations of sentences. The self-supervised network contains\ntwo components; a lexical sub-network and a syntactic sub-network which take\nthe sequence of words and their corresponding structural labels as the input,\nrespectively. Due to the n-to-1 mapping of words to their structural labels,\neach word will be embedded into a vector representation which mainly carries\nstructural information. We evaluate the learned structural representations of\nsentences using different probing tasks, and subsequently utilize them in the\nauthorship attribution task. Our experimental results indicate that the\nstructural embeddings significantly improve the classification tasks when\nconcatenated with the existing pre-trained word embeddings.", "published": "2020-10-14 02:57:10", "link": "http://arxiv.org/abs/2010.06786v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Relation Extraction from Language Models using Constrained\n  Cloze Completion", "abstract": "We show that state-of-the-art self-supervised language models can be readily\nused to extract relations from a corpus without the need to train a fine-tuned\nextractive head. We introduce RE-Flex, a simple framework that performs\nconstrained cloze completion over pretrained language models to perform\nunsupervised relation extraction. RE-Flex uses contextual matching to ensure\nthat language model predictions matches supporting evidence from the input\ncorpus that is relevant to a target relation. We perform an extensive\nexperimental study over multiple relation extraction benchmarks and demonstrate\nthat RE-Flex outperforms competing unsupervised relation extraction methods\nbased on pretrained language models by up to 27.8 $F_1$ points compared to the\nnext-best method. Our results show that constrained inference queries against a\nlanguage model can enable accurate unsupervised relation extraction.", "published": "2020-10-14 04:21:57", "link": "http://arxiv.org/abs/2010.06804v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantically-Aligned Universal Tree-Structured Solver for Math Word\n  Problems", "abstract": "A practical automatic textual math word problems (MWPs) solver should be able\nto solve various textual MWPs while most existing works only focused on\none-unknown linear MWPs. Herein, we propose a simple but efficient method\ncalled Universal Expression Tree (UET) to make the first attempt to represent\nthe equations of various MWPs uniformly. Then a semantically-aligned universal\ntree-structured solver (SAU-Solver) based on an encoder-decoder framework is\nproposed to resolve multiple types of MWPs in a unified model, benefiting from\nour UET representation. Our SAU-Solver generates a universal expression tree\nexplicitly by deciding which symbol to generate according to the generated\nsymbols' semantic meanings like human solving MWPs. Besides, our SAU-Solver\nalso includes a novel subtree-level semanticallyaligned regularization to\nfurther enforce the semantic constraints and rationality of the generated\nexpression tree by aligning with the contextual information. Finally, to\nvalidate the universality of our solver and extend the research boundary of\nMWPs, we introduce a new challenging Hybrid Math Word Problems dataset (HMWP),\nconsisting of three types of MWPs. Experimental results on several MWPs\ndatasets show that our model can solve universal types of MWPs and outperforms\nseveral state-of-the-art models.", "published": "2020-10-14 06:27:07", "link": "http://arxiv.org/abs/2010.06823v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Word Representations for Tunisian Sentiment Analysis", "abstract": "Tunisians on social media tend to express themselves in their local dialect\nusing Latin script (TUNIZI). This raises an additional challenge to the process\nof exploring and recognizing online opinions. To date, very little work has\naddressed TUNIZI sentiment analysis due to scarce resources for training an\nautomated system. In this paper, we focus on the Tunisian dialect sentiment\nanalysis used on social media. Most of the previous work used machine learning\ntechniques combined with handcrafted features. More recently, Deep Neural\nNetworks were widely used for this task, especially for the English language.\nIn this paper, we explore the importance of various unsupervised word\nrepresentations (word2vec, BERT) and we investigate the use of Convolutional\nNeural Networks and Bidirectional Long Short-Term Memory. Without using any\nkind of handcrafted features, our experimental results on two publicly\navailable datasets showed comparable performances to other languages.", "published": "2020-10-14 07:47:33", "link": "http://arxiv.org/abs/2010.06857v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "fugashi, a Tool for Tokenizing Japanese in Python", "abstract": "Recent years have seen an increase in the number of large-scale multilingual\nNLP projects. However, even in such projects, languages with special processing\nrequirements are often excluded. One such language is Japanese. Japanese is\nwritten without spaces, tokenization is non-trivial, and while high quality\nopen source tokenizers exist they can be hard to use and lack English\ndocumentation. This paper introduces fugashi, a MeCab wrapper for Python, and\ngives an introduction to tokenizing Japanese.", "published": "2020-10-14 07:52:47", "link": "http://arxiv.org/abs/2010.06858v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Memformer: A Memory-Augmented Transformer for Sequence Modeling", "abstract": "Transformers have reached remarkable success in sequence modeling. However,\nthese models have efficiency issues as they need to store all the history\ntoken-level representations as memory. We present Memformer, an efficient\nneural network for sequence modeling, that utilizes an external dynamic memory\nto encode and retrieve past information. Our model achieves linear time\ncomplexity and constant memory space complexity when processing long sequences.\nWe also propose a new optimization scheme, memory replay back-propagation\n(MRBP), which promotes long-range back-propagation through time with a\nsignificantly reduced memory requirement. Experimental results show that\nMemformer has achieved comparable performance compared to the baselines by\nusing 8.1x less memory space and 3.2x faster on inference. Analysis of the\nattention pattern shows that our external memory slots can encode and retain\nimportant information through timesteps.", "published": "2020-10-14 09:03:36", "link": "http://arxiv.org/abs/2010.06891v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DA-Transformer: Distance-aware Transformer", "abstract": "Transformer has achieved great success in the NLP field by composing various\nadvanced models like BERT and GPT. However, Transformer and its existing\nvariants may not be optimal in capturing token distances because the position\nor distance embeddings used by these methods usually cannot keep the precise\ninformation of real distances, which may not be beneficial for modeling the\norders and relations of contexts. In this paper, we propose DA-Transformer,\nwhich is a distance-aware Transformer that can exploit the real distance. We\npropose to incorporate the real distances between tokens to re-scale the raw\nself-attention weights, which are computed by the relevance between attention\nquery and key. Concretely, in different self-attention heads the relative\ndistance between each pair of tokens is weighted by different learnable\nparameters, which control the different preferences on long- or short-term\ninformation of these heads. Since the raw weighted real distances may not be\noptimal for adjusting self-attention weights, we propose a learnable sigmoid\nfunction to map them into re-scaled coefficients that have proper ranges. We\nfirst clip the raw self-attention weights via the ReLU function to keep\nnon-negativity and introduce sparsity, and then multiply them with the\nre-scaled coefficients to encode real distance information into self-attention.\nExtensive experiments on five benchmark datasets show that DA-Transformer can\neffectively improve the performance of many tasks and outperform the vanilla\nTransformer and its several variants.", "published": "2020-10-14 10:09:01", "link": "http://arxiv.org/abs/2010.06925v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chinese Lexical Simplification", "abstract": "Lexical simplification has attracted much attention in many languages, which\nis the process of replacing complex words in a given sentence with simpler\nalternatives of equivalent meaning. Although the richness of vocabulary in\nChinese makes the text very difficult to read for children and non-native\nspeakers, there is no research work for Chinese lexical simplification (CLS)\ntask. To circumvent difficulties in acquiring annotations, we manually create\nthe first benchmark dataset for CLS, which can be used for evaluating the\nlexical simplification systems automatically. In order to acquire more thorough\ncomparison, we present five different types of methods as baselines to generate\nsubstitute candidates for the complex word that include synonym-based approach,\nword embedding-based approach, pretrained language model-based approach,\nsememe-based approach, and a hybrid approach. Finally, we design the\nexperimental evaluation of these baselines and discuss their advantages and\ndisadvantages. To our best knowledge, this is the first study for CLS task.", "published": "2020-10-14 12:55:36", "link": "http://arxiv.org/abs/2010.07048v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Summarize, Outline, and Elaborate: Long-Text Generation via Hierarchical\n  Supervision from Extractive Summaries", "abstract": "The difficulty of generating coherent long texts lies in the fact that\nexisting models overwhelmingly focus on predicting local words, and cannot make\nhigh level plans on what to generate or capture the high-level discourse\ndependencies between chunks of texts. Inspired by human writing processes,\nwhere a list of bullet points or a catalog is first outlined, and then each\nbullet point is expanded to form the whole article, we propose {\\it SOE}, a\npipelined system that involves of summarizing, outlining and elaborating for\nlong text generation: the model first outlines the summaries for different\nsegments of long texts, and then elaborates on each bullet point to generate\nthe corresponding segment. To avoid the labor-intensive process of summary\nsoliciting, we propose the {\\it reconstruction} strategy, which extracts\nsegment summaries in an unsupervised manner by selecting its most informative\npart to reconstruct the segment. The proposed generation system comes with the\nfollowing merits: (1) the summary provides high-level guidance for text\ngeneration and avoids the local minimum of individual word predictions; (2) the\nhigh-level discourse dependencies are captured in the conditional dependencies\nbetween summaries and are preserved during the summary expansion process and\n(3) additionally, we are able to consider significantly more contexts by\nrepresenting contexts as concise summaries. Extensive experiments demonstrate\nthat SOE produces long texts with significantly better quality, along with\nfaster convergence speed.", "published": "2020-10-14 13:22:20", "link": "http://arxiv.org/abs/2010.07074v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-Supervised Bilingual Lexicon Induction with Two-way Interaction", "abstract": "Semi-supervision is a promising paradigm for Bilingual Lexicon Induction\n(BLI) with limited annotations. However, previous semisupervised methods do not\nfully utilize the knowledge hidden in annotated and nonannotated data, which\nhinders further improvement of their performance. In this paper, we propose a\nnew semi-supervised BLI framework to encourage the interaction between the\nsupervised signal and unsupervised alignment. We design two message-passing\nmechanisms to transfer knowledge between annotated and non-annotated data,\nnamed prior optimal transport and bi-directional lexicon update respectively.\nThen, we perform semi-supervised learning based on a cyclic or a parallel\nparameter feeding routine to update our models. Our framework is a general\nframework that can incorporate any supervised and unsupervised BLI methods\nbased on optimal transport. Experimental results on MUSE and VecMap datasets\nshow significant improvement of our models. Ablation study also proves that the\ntwo-way interaction between the supervised signal and unsupervised alignment\naccounts for the gain of the overall performance. Results on distant language\npairs further illustrate the advantage and robustness of our proposed method.", "published": "2020-10-14 13:59:07", "link": "http://arxiv.org/abs/2010.07101v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Investigation on Different Underlying Quantization Schemes for\n  Pre-trained Language Models", "abstract": "Recently, pre-trained language models like BERT have shown promising\nperformance on multiple natural language processing tasks. However, the\napplication of these models has been limited due to their huge size. To reduce\nits size, a popular and efficient way is quantization. Nevertheless, most of\nthe works focusing on BERT quantization adapted primary linear clustering as\nthe quantization scheme, and few works try to upgrade it. That limits the\nperformance of quantization significantly. In this paper, we implement k-means\nquantization and compare its performance on the fix-precision quantization of\nBERT with linear quantization. Through the comparison, we verify that the\neffect of the underlying quantization scheme upgrading is underestimated and\nthere is a huge development potential of k-means quantization. Besides, we also\ncompare the two quantization schemes on ALBERT models to explore the robustness\ndifferences between different pre-trained models.", "published": "2020-10-14 14:05:06", "link": "http://arxiv.org/abs/2010.07109v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The EOS Decision and Length Extrapolation", "abstract": "Extrapolation to unseen sequence lengths is a challenge for neural generative\nmodels of language. In this work, we characterize the effect on length\nextrapolation of a modeling decision often overlooked: predicting the end of\nthe generative process through the use of a special end-of-sequence (EOS)\nvocabulary item. We study an oracle setting - forcing models to generate to the\ncorrect sequence length at test time - to compare the length-extrapolative\nbehavior of networks trained to predict EOS (+EOS) with networks not trained to\n(-EOS). We find that -EOS substantially outperforms +EOS, for example\nextrapolating well to lengths 10 times longer than those seen at training time\nin a bracket closing task, as well as achieving a 40% improvement over +EOS in\nthe difficult SCAN dataset length generalization task. By comparing the hidden\nstates and dynamics of -EOS and +EOS models, we observe that +EOS models fail\nto generalize because they (1) unnecessarily stratify their hidden states by\ntheir linear position is a sequence (structures we call length manifolds) or\n(2) get stuck in clusters (which we refer to as length attractors) once the EOS\ntoken is the highest-probability prediction.", "published": "2020-10-14 15:46:17", "link": "http://arxiv.org/abs/2010.07174v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Positioning yourself in the maze of Neural Text Generation: A\n  Task-Agnostic Survey", "abstract": "Neural text generation metamorphosed into several critical natural language\napplications ranging from text completion to free form narrative generation. In\norder to progress research in text generation, it is critical to absorb the\nexisting research works and position ourselves in this massively growing field.\nSpecifically, this paper surveys the fundamental components of modeling\napproaches relaying task agnostic impacts across various generation tasks such\nas storytelling, summarization, translation etc., In this context, we present\nan abstraction of the imperative techniques with respect to learning paradigms,\npretraining, modeling approaches, decoding and the key challenges outstanding\nin the field in each of them. Thereby, we deliver a one-stop destination for\nresearchers in the field to facilitate a perspective on where to situate their\nwork and how it impacts other closely related generation tasks.", "published": "2020-10-14 17:54:42", "link": "http://arxiv.org/abs/2010.07279v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decoding Methods for Neural Narrative Generation", "abstract": "Narrative generation is an open-ended NLP task in which a model generates a\nstory given a prompt. The task is similar to neural response generation for\nchatbots; however, innovations in response generation are often not applied to\nnarrative generation, despite the similarity between these tasks. We aim to\nbridge this gap by applying and evaluating advances in decoding methods for\nneural response generation to neural narrative generation. In particular, we\nemploy GPT-2 and perform ablations across nucleus sampling thresholds and\ndiverse decoding hyperparameters -- specifically, maximum mutual information --\nanalyzing results over multiple criteria with automatic and human evaluation.\nWe find that (1) nucleus sampling is generally best with thresholds between 0.7\nand 0.9; (2) a maximum mutual information objective can improve the quality of\ngenerated stories; and (3) established automatic metrics do not correlate well\nwith human judgments of narrative quality on any qualitative metric.", "published": "2020-10-14 19:32:56", "link": "http://arxiv.org/abs/2010.07375v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A new approach for extracting the conceptual schema of texts based on\n  the linguistic Thematic Progression theory", "abstract": "The purpose of this article is to present a new approach for the discovery\nand labelling of the implicit conceptual schema of texts through the\napplication of the Thematic Progression theory. The underlying conceptual\nschema is the core component for the generation of summaries that are genuinely\nconsistent with the semantics of the text.", "published": "2020-10-14 23:50:25", "link": "http://arxiv.org/abs/2010.07440v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Summarizing Text on Any Aspects: A Knowledge-Informed Weakly-Supervised\n  Approach", "abstract": "Given a document and a target aspect (e.g., a topic of interest),\naspect-based abstractive summarization attempts to generate a summary with\nrespect to the aspect. Previous studies usually assume a small pre-defined set\nof aspects and fall short of summarizing on other diverse topics. In this work,\nwe study summarizing on arbitrary aspects relevant to the document, which\nsignificantly expands the application of the task in practice. Due to the lack\nof supervision data, we develop a new weak supervision construction method and\nan aspect modeling scheme, both of which integrate rich external knowledge\nsources such as ConceptNet and Wikipedia. Experiments show our approach\nachieves performance boosts on summarizing both real and synthetic documents\ngiven pre-defined or arbitrary aspects.", "published": "2020-10-14 03:20:46", "link": "http://arxiv.org/abs/2010.06792v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Graph Representation of Semi-structured Data for Web Question\n  Answering", "abstract": "The abundant semi-structured data on the Web, such as HTML-based tables and\nlists, provide commercial search engines a rich information source for question\nanswering (QA). Different from plain text passages in Web documents, Web tables\nand lists have inherent structures, which carry semantic correlations among\nvarious elements in tables and lists. Many existing studies treat tables and\nlists as flat documents with pieces of text and do not make good use of\nsemantic information hidden in structures. In this paper, we propose a novel\ngraph representation of Web tables and lists based on a systematic\ncategorization of the components in semi-structured data as well as their\nrelations. We also develop pre-training and reasoning techniques on the graph\nmodel for the QA task. Extensive experiments on several real datasets collected\nfrom a commercial engine verify the effectiveness of our approach. Our method\nimproves F1 score by 3.90 points over the state-of-the-art baselines.", "published": "2020-10-14 04:01:54", "link": "http://arxiv.org/abs/2010.06801v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Modeling Protagonist Emotions for Emotion-Aware Storytelling", "abstract": "Emotions and their evolution play a central role in creating a captivating\nstory. In this paper, we present the first study on modeling the emotional\ntrajectory of the protagonist in neural storytelling. We design methods that\ngenerate stories that adhere to given story titles and desired emotion arcs for\nthe protagonist. Our models include Emotion Supervision (EmoSup) and two\nEmotion-Reinforced (EmoRL) models. The EmoRL models use special rewards\ndesigned to regularize the story generation process through reinforcement\nlearning. Our automatic and manual evaluations demonstrate that these models\nare significantly better at generating stories that follow the desired emotion\narcs compared to baseline methods, without sacrificing story quality.", "published": "2020-10-14 06:24:25", "link": "http://arxiv.org/abs/2010.06822v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pair the Dots: Jointly Examining Training History and Test Stimuli for\n  Model Interpretability", "abstract": "Any prediction from a model is made by a combination of learning history and\ntest stimuli. This provides significant insights for improving model\ninterpretability: {\\it because of which part(s) of which training example(s),\nthe model attends to which part(s) of a test example}. Unfortunately, existing\nmethods to interpret a model's predictions are only able to capture a single\naspect of either test stimuli or learning history, and evidences from both are\nnever combined or integrated. In this paper, we propose an efficient and\ndifferentiable approach to make it feasible to interpret a model's prediction\nby jointly examining training history and test stimuli. Test stimuli is first\nidentified by gradient-based methods, signifying {\\it the part of a test\nexample that the model attends to}. The gradient-based saliency scores are then\npropagated to training examples using influence functions to identify {\\it\nwhich part(s) of which training example(s)} make the model attends to the test\nstimuli. The system is differentiable and time efficient: the adoption of\nsaliency scores from gradient-based methods allows us to efficiently trace a\nmodel's prediction through test stimuli, and then back to training examples\nthrough influence functions. We demonstrate that the proposed methodology\noffers clear explanations about neural model decisions, along with being useful\nfor performing error analysis, crafting adversarial examples and fixing\nerroneously classified examples.", "published": "2020-10-14 10:45:01", "link": "http://arxiv.org/abs/2010.06943v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Medical Code Assignment with Gated Convolution and Note-Code Interaction", "abstract": "Medical code assignment from clinical text is a fundamental task in clinical\ninformation system management. As medical notes are typically lengthy and the\nmedical coding system's code space is large, this task is a long-standing\nchallenge. Recent work applies deep neural network models to encode the medical\nnotes and assign medical codes to clinical documents. However, these methods\nare still ineffective as they do not fully encode and capture the lengthy and\nrich semantic information of medical notes nor explicitly exploit the\ninteractions between the notes and codes. We propose a novel method, gated\nconvolutional neural networks, and a note-code interaction (GatedCNN-NCI), for\nautomatic medical code assignment to overcome these challenges. Our methods\ncapture the rich semantic information of the lengthy clinical text for better\nrepresentation by utilizing embedding injection and gated information\npropagation in the medical note encoding module. With a novel note-code\ninteraction design and a graph message passing mechanism, we explicitly capture\nthe underlying dependency between notes and codes, enabling effective code\nprediction. A weight sharing scheme is further designed to decrease the number\nof trainable parameters. Empirical experiments on real-world clinical datasets\nshow that our proposed model outperforms state-of-the-art models in most cases,\nand our model size is on par with light-weighted baselines.", "published": "2020-10-14 11:37:24", "link": "http://arxiv.org/abs/2010.06975v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime\n  with Search", "abstract": "Despite transformers' impressive accuracy, their computational cost is often\nprohibitive to use with limited computational resources. Most previous\napproaches to improve inference efficiency require a separate model for each\npossible computational budget. In this paper, we extend PoWER-BERT (Goyal et\nal., 2020) and propose Length-Adaptive Transformer that can be used for various\ninference scenarios after one-shot training. We train a transformer with\nLengthDrop, a structural variant of dropout, which stochastically determines a\nsequence length at each layer. We then conduct a multi-objective evolutionary\nsearch to find a length configuration that maximizes the accuracy and minimizes\nthe efficiency metric under any given computational budget. Additionally, we\nsignificantly extend the applicability of PoWER-BERT beyond sequence-level\nclassification into token-level classification with Drop-and-Restore process\nthat drops word-vectors temporarily in intermediate layers and restores at the\nlast layer if necessary. We empirically verify the utility of the proposed\napproach by demonstrating the superior accuracy-efficiency trade-off under\nvarious setups, including span-based question answering and text\nclassification. Code is available at\nhttps://github.com/clovaai/length-adaptive-transformer.", "published": "2020-10-14 12:28:08", "link": "http://arxiv.org/abs/2010.07003v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AutoADR: Automatic Model Design for Ad Relevance", "abstract": "Large-scale pre-trained models have attracted extensive attention in the\nresearch community and shown promising results on various tasks of natural\nlanguage processing. However, these pre-trained models are memory and\ncomputation intensive, hindering their deployment into industrial online\nsystems like Ad Relevance. Meanwhile, how to design an effective yet efficient\nmodel architecture is another challenging problem in online Ad Relevance.\nRecently, AutoML shed new lights on architecture design, but how to integrate\nit with pre-trained language models remains unsettled. In this paper, we\npropose AutoADR (Automatic model design for AD Relevance) -- a novel end-to-end\nframework to address this challenge, and share our experience to ship these\ncutting-edge techniques into online Ad Relevance system at Microsoft Bing.\nSpecifically, AutoADR leverages a one-shot neural architecture search algorithm\nto find a tailored network architecture for Ad Relevance. The search process is\nsimultaneously guided by knowledge distillation from a large pre-trained\nteacher model (e.g. BERT), while taking the online serving constraints (e.g.\nmemory and latency) into consideration. We add the model designed by AutoADR as\na sub-model into the production Ad Relevance model. This additional sub-model\nimproves the Precision-Recall AUC (PR AUC) on top of the original Ad Relevance\nmodel by 2.65X of the normalized shipping bar. More importantly, adding this\nautomatically designed sub-model leads to a statistically significant 4.6%\nBad-Ad ratio reduction in online A/B testing. This model has been shipped into\nMicrosoft Bing Ad Relevance Production model.", "published": "2020-10-14 13:24:43", "link": "http://arxiv.org/abs/2010.07075v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Recipes for Safety in Open-domain Chatbots", "abstract": "Models trained on large unlabeled corpora of human interactions will learn\npatterns and mimic behaviors therein, which include offensive or otherwise\ntoxic behavior and unwanted biases. We investigate a variety of methods to\nmitigate these issues in the context of open-domain generative dialogue models.\nWe introduce a new human-and-model-in-the-loop framework for both training\nsafer models and for evaluating them, as well as a novel method to distill\nsafety considerations inside generative models without the use of an external\nclassifier at deployment time. We conduct experiments comparing these methods\nand find our new techniques are (i) safer than existing models as measured by\nautomatic and human evaluations while (ii) maintaining usability metrics such\nas engagingness relative to the state of the art. We then discuss the\nlimitations of this work by analyzing failure cases of our models.", "published": "2020-10-14 13:26:39", "link": "http://arxiv.org/abs/2010.07079v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Relaxed Matching Procedure for Unsupervised BLI", "abstract": "Recently unsupervised Bilingual Lexicon Induction (BLI) without any parallel\ncorpus has attracted much research interest. One of the crucial parts in\nmethods for the BLI task is the matching procedure. Previous works impose a too\nstrong constraint on the matching and lead to many counterintuitive translation\npairings. Thus, We propose a relaxed matching procedure to find a more precise\nmatching between two languages. We also find that aligning source and target\nlanguage embedding space bidirectionally will bring significant improvement. We\nfollow the previous iterative framework to conduct experiments. Results on\nstandard benchmark demonstrate the effectiveness of our proposed method, which\nsubstantially outperforms previous unsupervised methods.", "published": "2020-10-14 13:53:08", "link": "http://arxiv.org/abs/2010.07095v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Geometry matters: Exploring language examples at the decision boundary", "abstract": "A growing body of recent evidence has highlighted the limitations of natural\nlanguage processing (NLP) datasets and classifiers. These include the presence\nof annotation artifacts in datasets, classifiers relying on shallow features\nlike a single word (e.g., if a movie review has the word \"romantic\", the review\ntends to be positive), or unnecessary words (e.g., learning a proper noun to\nclassify a movie as positive or negative). The presence of such artifacts has\nsubsequently led to the development of challenging datasets to force the model\nto generalize better. While a variety of heuristic strategies, such as\ncounterfactual examples and contrast sets, have been proposed, the theoretical\njustification about what makes these examples difficult for the classifier is\noften lacking or unclear. In this paper, using tools from information geometry,\nwe propose a theoretical way to quantify the difficulty of an example in NLP.\nUsing our approach, we explore difficult examples for several deep learning\narchitectures. We discover that both BERT, CNN and fasttext are susceptible to\nword substitutions in high difficulty examples. These classifiers tend to\nperform poorly on the FIM test set. (generated by sampling and perturbing\ndifficult examples, with accuracy dropping below 50%). We replicate our\nexperiments on 5 NLP datasets (YelpReviewPolarity, AGNEWS, SogouNews,\nYelpReviewFull and Yahoo Answers). On YelpReviewPolarity we observe a\ncorrelation coefficient of -0.4 between resilience to perturbations and the\ndifficulty score. Similarly we observe a correlation of 0.35 between the\ndifficulty score and the empirical success probability of random substitutions.\nOur approach is simple, architecture agnostic and can be used to study the\nfragilities of text classification models. All the code used will be made\npublicly available, including a tool to explore the difficult examples for\nother datasets.", "published": "2020-10-14 16:26:13", "link": "http://arxiv.org/abs/2010.07212v3", "categories": ["cs.CL", "stat.ML", "F.2.0; I.2.7"], "primary_category": "cs.CL"}
{"title": "Text Classification Using Label Names Only: A Language Model\n  Self-Training Approach", "abstract": "Current text classification methods typically require a good number of\nhuman-labeled documents as training data, which can be costly and difficult to\nobtain in real applications. Humans can perform classification without seeing\nany labeled examples but only based on a small set of words describing the\ncategories to be classified. In this paper, we explore the potential of only\nusing the label name of each class to train classification models on unlabeled\ndata, without using any labeled documents. We use pre-trained neural language\nmodels both as general linguistic knowledge sources for category understanding\nand as representation learning models for document classification. Our method\n(1) associates semantically related words with the label names, (2) finds\ncategory-indicative words and trains the model to predict their implied\ncategories, and (3) generalizes the model via self-training. We show that our\nmodel achieves around 90% accuracy on four benchmark datasets including topic\nand sentiment classification without using any labeled documents but learning\nfrom unlabeled data supervised by at most 3 words (1 in most cases) per class\nas the label name.", "published": "2020-10-14 17:06:41", "link": "http://arxiv.org/abs/2010.07245v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "My Team Will Go On: Differentiating High and Low Viability Teams through\n  Team Interaction", "abstract": "Understanding team viability -- a team's capacity for sustained and future\nsuccess -- is essential for building effective teams. In this study, we\naggregate features drawn from the organizational behavior literature to train a\nviability classification model over a dataset of 669 10-minute text\nconversations of online teams. We train classifiers to identify teams at the\ntop decile (most viable teams), 50th percentile (above a median split), and\nbottom decile (least viable teams), then characterize the attributes of teams\nat each of these viability levels. We find that a lasso regression model\nachieves an accuracy of .74--.92 AUC ROC under different thresholds of\nclassifying viability scores. From these models, we identify the use of\nexclusive language such as `but' and `except', and the use of second person\npronouns, as the most predictive features for detecting the most viable teams,\nsuggesting that active engagement with others' ideas is a crucial signal of a\nviable team. Only a small fraction of the 10-minute discussion, as little as 70\nseconds, is required for predicting the viability of team interaction. This\nwork suggests opportunities for teams to assess, track, and visualize their own\nviability in real time as they collaborate.", "published": "2020-10-14 21:33:36", "link": "http://arxiv.org/abs/2010.07292v2", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Six Attributes of Unhealthy Conversation", "abstract": "We present a new dataset of approximately 44000 comments labeled by\ncrowdworkers. Each comment is labelled as either 'healthy' or 'unhealthy', in\naddition to binary labels for the presence of six potentially 'unhealthy'\nsub-attributes: (1) hostile; (2) antagonistic, insulting, provocative or\ntrolling; (3) dismissive; (4) condescending or patronising; (5) sarcastic;\nand/or (6) an unfair generalisation. Each label also has an associated\nconfidence score. We argue that there is a need for datasets which enable\nresearch based on a broad notion of 'unhealthy online conversation'. We build\nthis typology to encompass a substantial proportion of the individual comments\nwhich contribute to unhealthy online conversation. For some of these\nattributes, this is the first publicly available dataset of this scale. We\nexplore the quality of the dataset, present some summary statistics and initial\nmodels to illustrate the utility of this data, and highlight limitations and\ndirections for further research.", "published": "2020-10-14 21:28:06", "link": "http://arxiv.org/abs/2010.07410v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "On Cross-Dataset Generalization in Automatic Detection of Online Abuse", "abstract": "NLP research has attained high performances in abusive language detection as\na supervised classification task. While in research settings, training and test\ndatasets are usually obtained from similar data samples, in practice systems\nare often applied on data that are different from the training set in topic and\nclass distributions. Also, the ambiguity in class definitions inherited in this\ntask aggravates the discrepancies between source and target datasets. We\nexplore the topic bias and the task formulation bias in cross-dataset\ngeneralization. We show that the benign examples in the Wikipedia Detox dataset\nare biased towards platform-specific topics. We identify these examples using\nunsupervised topic modeling and manual inspection of topics' keywords. Removing\nthese topics increases cross-dataset generalization, without reducing in-domain\nclassification performance. For a robust dataset design, we suggest applying\ninexpensive unsupervised methods to inspect the collected data and downsize the\nnon-generalizable content before manually annotating for class labels.", "published": "2020-10-14 21:47:03", "link": "http://arxiv.org/abs/2010.07414v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From Language to Language-ish: How Brain-Like is an LSTM's\n  Representation of Nonsensical Language Stimuli?", "abstract": "The representations generated by many models of language (word embeddings,\nrecurrent neural networks and transformers) correlate to brain activity\nrecorded while people read. However, these decoding results are usually based\non the brain's reaction to syntactically and semantically sound language\nstimuli. In this study, we asked: how does an LSTM (long short term memory)\nlanguage model, trained (by and large) on semantically and syntactically intact\nlanguage, represent a language sample with degraded semantic or syntactic\ninformation? Does the LSTM representation still resemble the brain's reaction?\nWe found that, even for some kinds of nonsensical language, there is a\nstatistically significant relationship between the brain's activity and the\nrepresentations of an LSTM. This indicates that, at least in some instances,\nLSTMs and the human brain handle nonsensical data similarly.", "published": "2020-10-14 23:26:28", "link": "http://arxiv.org/abs/2010.07435v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Vokenization: Improving Language Understanding with Contextualized,\n  Visual-Grounded Supervision", "abstract": "Humans learn language by listening, speaking, writing, reading, and also, via\ninteraction with the multimodal real world. Existing language pre-training\nframeworks show the effectiveness of text-only self-supervision while we\nexplore the idea of a visually-supervised language model in this paper. We find\nthat the main reason hindering this exploration is the large divergence in\nmagnitude and distributions between the visually-grounded language datasets and\npure-language corpora. Therefore, we develop a technique named \"vokenization\"\nthat extrapolates multimodal alignments to language-only data by contextually\nmapping language tokens to their related images (which we call \"vokens\"). The\n\"vokenizer\" is trained on relatively small image captioning datasets and we\nthen apply it to generate vokens for large language corpora. Trained with these\ncontextually generated vokens, our visually-supervised language models show\nconsistent improvements over self-supervised alternatives on multiple\npure-language tasks such as GLUE, SQuAD, and SWAG. Code and pre-trained models\npublicly available at https://github.com/airsplay/vokenization", "published": "2020-10-14 02:11:51", "link": "http://arxiv.org/abs/2010.06775v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "No Rumours Please! A Multi-Indic-Lingual Approach for COVID Fake-Tweet\n  Detection", "abstract": "The sudden widespread menace created by the present global pandemic COVID-19\nhas had an unprecedented effect on our lives. Man-kind is going through\nhumongous fear and dependence on social media like never before. Fear\ninevitably leads to panic, speculations, and the spread of misinformation. Many\ngovernments have taken measures to curb the spread of such misinformation for\npublic well being. Besides global measures, to have effective outreach, systems\nfor demographically local languages have an important role to play in this\neffort. Towards this, we propose an approach to detect fake news about COVID-19\nearly on from social media, such as tweets, for multiple Indic-Languages\nbesides English. In addition, we also create an annotated dataset of Hindi and\nBengali tweet for fake news detection. We propose a BERT based model augmented\nwith additional relevant features extracted from Twitter to identify fake\ntweets. To expand our approach to multiple Indic languages, we resort to mBERT\nbased model which is fine-tuned over created dataset in Hindi and Bengali. We\nalso propose a zero-shot learning approach to alleviate the data scarcity issue\nfor such low resource languages. Through rigorous experiments, we show that our\napproach reaches around 89% F-Score in fake tweet detection which supercedes\nthe state-of-the-art (SOTA) results. Moreover, we establish the first benchmark\nfor two Indic-Languages, Hindi and Bengali. Using our annotated data, our model\nachieves about 79% F-Score in Hindi and 81% F-Score for Bengali Tweets. Our\nzero-shot model achieves about 81% F-Score in Hindi and 78% F-Score for Bengali\nTweets without any annotated data, which clearly indicates the efficacy of our\napproach.", "published": "2020-10-14 09:37:51", "link": "http://arxiv.org/abs/2010.06906v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Neural Databases", "abstract": "In recent years, neural networks have shown impressive performance gains on\nlong-standing AI problems, and in particular, answering queries from natural\nlanguage text. These advances raise the question of whether they can be\nextended to a point where we can relax the fundamental assumption of database\nmanagement, namely, that our data is represented as fields of a pre-defined\nschema.\n  This paper presents a first step in answering that question. We describe\nNeuralDB, a database system with no pre-defined schema, in which updates and\nqueries are given in natural language. We develop query processing techniques\nthat build on the primitives offered by the state of the art Natural Language\nProcessing methods.\n  We begin by demonstrating that at the core, recent NLP transformers, powered\nby pre-trained language models, can answer select-project-join queries if they\nare given the exact set of relevant facts. However, they cannot scale to\nnon-trivial databases and cannot perform aggregation queries. Based on these\nfindings, we describe a NeuralDB architecture that runs multiple Neural SPJ\noperators in parallel, each with a set of database sentences that can produce\none of the answers to the query. The result of these operators is fed to an\naggregation operator if needed. We describe an algorithm that learns how to\ncreate the appropriate sets of facts to be fed into each of the Neural SPJ\noperators. Importantly, this algorithm can be trained by the Neural SPJ\noperator itself. We experimentally validate the accuracy of NeuralDB and its\ncomponents, showing that we can answer queries over thousands of sentences with\nvery high accuracy.", "published": "2020-10-14 11:31:53", "link": "http://arxiv.org/abs/2010.06973v1", "categories": ["cs.CL", "cs.DB", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Weight Squeezing: Reparameterization for Knowledge Transfer and Model\n  Compression", "abstract": "In this work, we present a novel approach for simultaneous knowledge transfer\nand model compression called Weight Squeezing. With this method, we perform\nknowledge transfer from a teacher model by learning the mapping from its\nweights to smaller student model weights.\n  We applied Weight Squeezing to a pre-trained text classification model based\non BERT-Medium model and compared our method to various other knowledge\ntransfer and model compression methods on GLUE multitask benchmark. We observed\nthat our approach produces better results while being significantly faster than\nother methods for training student models.\n  We also proposed a variant of Weight Squeezing called Gated Weight Squeezing,\nfor which we combined fine-tuning of BERT-Medium model and learning mapping\nfrom BERT-Base weights. We showed that fine-tuning with Gated Weight Squeezing\noutperforms plain fine-tuning of BERT-Medium model as well as other concurrent\nSoTA approaches while much being easier to implement.", "published": "2020-10-14 12:13:28", "link": "http://arxiv.org/abs/2010.06993v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Re-evaluating Evaluation in Text Summarization", "abstract": "Automated evaluation metrics as a stand-in for manual evaluation are an\nessential part of the development of text-generation tasks such as text\nsummarization. However, while the field has progressed, our standard metrics\nhave not -- for nearly 20 years ROUGE has been the standard evaluation in most\nsummarization papers. In this paper, we make an attempt to re-evaluate the\nevaluation method for text summarization: assessing the reliability of\nautomatic metrics using top-scoring system outputs, both abstractive and\nextractive, on recently popular datasets for both system-level and\nsummary-level evaluation settings. We find that conclusions about evaluation\nmetrics on older datasets do not necessarily hold on modern datasets and\nsystems.", "published": "2020-10-14 13:58:53", "link": "http://arxiv.org/abs/2010.07100v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploiting Spectral Augmentation for Code-Switched Spoken Language\n  Identification", "abstract": "Spoken language Identification (LID) systems are needed to identify the\nlanguage(s) present in a given audio sample, and typically could be the first\nstep in many speech processing related tasks such as automatic speech\nrecognition (ASR). Automatic identification of the languages present in a\nspeech signal is not only scientifically interesting, but also of practical\nimportance in a multilingual country such as India. In many of the Indian\ncities, when people interact with each other, as many as three languages may\nget mixed. These may include the official language of that province, Hindi and\nEnglish (at times the languages of the neighboring provinces may also get mixed\nduring these interactions). This makes the spoken LID task extremely\nchallenging in Indian context. While quite a few LID systems in the context of\nIndian languages have been implemented, most such systems have used small scale\nspeech data collected internally within an organization. In the current work,\nwe perform spoken LID on three Indian languages (Gujarati, Telugu, and Tamil)\ncode-mixed with English. This task was organized by the Microsoft research team\nas a spoken LID challenge. In our work, we modify the usual spectral\naugmentation approach and propose a language mask that discriminates the\nlanguage ID pairs, which leads to a noise robust spoken LID system. The\nproposed method gives a relative improvement of approximately 3-5% in the LID\naccuracy over a baseline system proposed by Microsoft on the three language\npairs for two shared tasks suggested in the challenge.", "published": "2020-10-14 14:37:03", "link": "http://arxiv.org/abs/2010.07130v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "MulDE: Multi-teacher Knowledge Distillation for Low-dimensional\n  Knowledge Graph Embeddings", "abstract": "Link prediction based on knowledge graph embeddings (KGE) aims to predict new\ntriples to automatically construct knowledge graphs (KGs). However, recent KGE\nmodels achieve performance improvements by excessively increasing the embedding\ndimensions, which may cause enormous training costs and require more storage\nspace. In this paper, instead of training high-dimensional models, we propose\nMulDE, a novel knowledge distillation framework, which includes multiple\nlow-dimensional hyperbolic KGE models as teachers and two student components,\nnamely Junior and Senior. Under a novel iterative distillation strategy, the\nJunior component, a low-dimensional KGE model, asks teachers actively based on\nits preliminary prediction results, and the Senior component integrates\nteachers' knowledge adaptively to train the Junior component based on two\nmechanisms: relation-specific scaling and contrast attention. The experimental\nresults show that MulDE can effectively improve the performance and training\nspeed of low-dimensional KGE models. The distilled 32-dimensional model is\ncompetitive compared to the state-of-the-art high-dimensional methods on\nseveral widely-used datasets.", "published": "2020-10-14 15:09:27", "link": "http://arxiv.org/abs/2010.07152v4", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Learning Improvised Chatbots from Adversarial Modifications of Natural\n  Language Feedback", "abstract": "The ubiquitous nature of chatbots and their interaction with users generate\nan enormous amount of data. Can we improve chatbots using this data? A\nself-feeding chatbot improves itself by asking natural language feedback when a\nuser is dissatisfied with its response and uses this feedback as an additional\ntraining sample. However, user feedback in most cases contains extraneous\nsequences hindering their usefulness as a training sample. In this work, we\npropose a generative adversarial model that converts noisy feedback into a\nplausible natural response in a conversation. The generator's goal is to\nconvert the feedback into a response that answers the user's previous utterance\nand to fool the discriminator which distinguishes feedback from natural\nresponses. We show that augmenting original training data with these modified\nfeedback responses improves the original chatbot performance from 69.94% to\n75.96% in ranking correct responses on the Personachat dataset, a large\nimprovement given that the original model is already trained on 131k samples.", "published": "2020-10-14 17:33:37", "link": "http://arxiv.org/abs/2010.07261v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Viewmaker Networks: Learning Views for Unsupervised Representation\n  Learning", "abstract": "Many recent methods for unsupervised representation learning train models to\nbe invariant to different \"views,\" or distorted versions of an input. However,\ndesigning these views requires considerable trial and error by human experts,\nhindering widespread adoption of unsupervised representation learning methods\nacross domains and modalities. To address this, we propose viewmaker networks:\ngenerative models that learn to produce useful views from a given input.\nViewmakers are stochastic bounded adversaries: they produce views by generating\nand then adding an $\\ell_p$-bounded perturbation to the input, and are trained\nadversarially with respect to the main encoder network. Remarkably, when\npretraining on CIFAR-10, our learned views enable comparable transfer accuracy\nto the well-tuned SimCLR augmentations -- despite not including transformations\nlike cropping or color jitter. Furthermore, our learned views significantly\noutperform baseline augmentations on speech recordings (+9% points, on average)\nand wearable sensor data (+17% points). Viewmakers can also be combined with\nhandcrafted views: they improve robustness to common image corruptions and can\nincrease transfer performance in cases where handcrafted views are less\nexplored. These results suggest that viewmakers may provide a path towards more\ngeneral representation learning algorithms -- reducing the domain expertise and\neffort needed to pretrain on a much wider set of domains. Code is available at\nhttps://github.com/alextamkin/viewmaker.", "published": "2020-10-14 23:03:31", "link": "http://arxiv.org/abs/2010.07432v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Towards Resistant Audio Adversarial Examples", "abstract": "Adversarial examples tremendously threaten the availability and integrity of\nmachine learning-based systems. While the feasibility of such attacks has been\nobserved first in the domain of image processing, recent research shows that\nspeech recognition is also susceptible to adversarial attacks. However,\nreliably bridging the air gap (i.e., making the adversarial examples work when\nrecorded via a microphone) has so far eluded researchers. We find that due to\nflaws in the generation process, state-of-the-art adversarial example\ngeneration methods cause overfitting because of the binning operation in the\ntarget speech recognition system (e.g., Mozilla Deepspeech). We devise an\napproach to mitigate this flaw and find that our method improves generation of\nadversarial examples with varying offsets. We confirm the significant\nimprovement with our approach by empirical comparison of the edit distance in a\nrealistic over-the-air setting. Our approach states a significant step towards\nover-the-air attacks. We publish the code and an applicable implementation of\nour approach.", "published": "2020-10-14 16:04:02", "link": "http://arxiv.org/abs/2010.07190v1", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS", "I.2"], "primary_category": "cs.SD"}
