{"title": "TAP-DLND 1.0 : A Corpus for Document Level Novelty Detection", "abstract": "Detecting novelty of an entire document is an Artificial Intelligence (AI)\nfrontier problem that has widespread NLP applications, such as extractive\ndocument summarization, tracking development of news events, predicting impact\nof scholarly articles, etc. Important though the problem is, we are unaware of\nany benchmark document level data that correctly addresses the evaluation of\nautomatic novelty detection techniques in a classification framework. To bridge\nthis gap, we present here a resource for benchmarking the techniques for\ndocument level novelty detection. We create the resource via event-specific\ncrawling of news documents across several domains in a periodic manner. We\nrelease the annotated corpus with necessary statistics and show its use with a\ndeveloped system for the problem in concern.", "published": "2018-02-20 03:42:11", "link": "http://arxiv.org/abs/1802.06950v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combining Textual Content and Structure to Improve Dialog Similarity", "abstract": "Chatbots, taking advantage of the success of the messaging apps and recent\nadvances in Artificial Intelligence, have become very popular, from helping\nbusiness to improve customer services to chatting to users for the sake of\nconversation and engagement (celebrity or personal bots). However, developing\nand improving a chatbot requires understanding their data generated by its\nusers. Dialog data has a different nature of a simple question and answering\ninteraction, in which context and temporal properties (turn order) creates a\ndifferent understanding of such data. In this paper, we propose a novelty\nmetric to compute dialogs' similarity based not only on the text content but\nalso on the information related to the dialog structure. Our experimental\nresults performed over the Switchboard dataset show that using evidence from\nboth textual content and the dialog structure leads to more accurate results\nthan using each measure in isolation.", "published": "2018-02-20 14:05:48", "link": "http://arxiv.org/abs/1802.07117v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Implicit Argument Prediction with Event Knowledge", "abstract": "Implicit arguments are not syntactically connected to their predicates, and\nare therefore hard to extract. Previous work has used models with large numbers\nof features, evaluated on very small datasets. We propose to train models for\nimplicit argument prediction on a simple cloze task, for which data can be\ngenerated automatically at scale. This allows us to use a neural model, which\ndraws on narrative coherence and entity salience for predictions. We show that\nour model has superior performance on both synthetic and natural data.", "published": "2018-02-20 18:04:46", "link": "http://arxiv.org/abs/1802.07226v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stability of meanings versus rate of replacement of words: an\n  experimental test", "abstract": "The words of a language are randomly replaced in time by new ones, but it has\nlong been known that words corresponding to some items (meanings) are less\nfrequently replaced than others. Usually, the rate of replacement for a given\nitem is not directly observable, but it is inferred by the estimated stability\nwhich, on the contrary, is observable. This idea goes back a long way in the\nlexicostatistical literature, nevertheless nothing ensures that it gives the\ncorrect answer. The family of Romance languages allows for a direct test of the\nestimated stabilities against the replacement rates since the proto-language\n(Latin) is known and the replacement rates can be explicitly computed. The\noutput of the test is threefold:first, we prove that the standard approach\nwhich tries to infer the replacement rates trough the estimated stabilities is\nsound; second, we are able to rewrite the fundamental formula of\nGlottochronology for a non universal replacement rate (a rate which depends on\nthe item); third, we give indisputable evidence that the stability ranking is\nfar from being the same for different families of languages. This last result\nis also supported by comparison with the Malagasy family of dialects. As a side\nresult we also provide some evidence that Vulgar Latin and not Late Classical\nLatin is at the root of modern Romance languages.", "published": "2018-02-20 17:45:53", "link": "http://arxiv.org/abs/1802.06764v2", "categories": ["cs.CL", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "SufiSent - Universal Sentence Representations Using Suffix Encodings", "abstract": "Computing universal distributed representations of sentences is a fundamental\ntask in natural language processing. We propose a method to learn such\nrepresentations by encoding the suffixes of word sequences in a sentence and\ntraining on the Stanford Natural Language Inference (SNLI) dataset. We\ndemonstrate the effectiveness of our approach by evaluating it on the SentEval\nbenchmark, improving on existing approaches on several transfer tasks.", "published": "2018-02-20 23:08:19", "link": "http://arxiv.org/abs/1802.07370v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the scaling of polynomial features for representation matching", "abstract": "In many neural models, new features as polynomial functions of existing ones\nare used to augment representations. Using the natural language inference task\nas an example, we investigate the use of scaled polynomials of degree 2 and\nabove as matching features. We find that scaling degree 2 features has the\nhighest impact on performance, reducing classification error by 5% in the best\nmodels.", "published": "2018-02-20 23:22:25", "link": "http://arxiv.org/abs/1802.07374v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Distilling Knowledge Using Parallel Data for Far-field Speech\n  Recognition", "abstract": "In order to improve the performance for far-field speech recognition, this\npaper proposes to distill knowledge from the close-talking model to the\nfar-field model using parallel data. The close-talking model is called the\nteacher model. The far-field model is called the student model. The student\nmodel is trained to imitate the output distributions of the teacher model. This\nconstraint can be realized by minimizing the Kullback-Leibler (KL) divergence\nbetween the output distribution of the student model and the teacher model.\nExperimental results on AMI corpus show that the best student model achieves up\nto 4.7% absolute word error rate (WER) reduction when compared with the\nconventionally-trained baseline models.", "published": "2018-02-20 02:51:34", "link": "http://arxiv.org/abs/1802.06941v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Attentive Tensor Product Learning", "abstract": "This paper proposes a new architecture - Attentive Tensor Product Learning\n(ATPL) - to represent grammatical structures in deep learning models. ATPL is a\nnew architecture to bridge this gap by exploiting Tensor Product\nRepresentations (TPR), a structured neural-symbolic model developed in\ncognitive science, aiming to integrate deep learning with explicit language\nstructures and rules. The key ideas of ATPL are: 1) unsupervised learning of\nrole-unbinding vectors of words via TPR-based deep neural network; 2) employing\nattention modules to compute TPR; and 3) integration of TPR with typical deep\nlearning architectures including Long Short-Term Memory (LSTM) and Feedforward\nNeural Network (FFNN). The novelty of our approach lies in its ability to\nextract the grammatical structure of a sentence by using role-unbinding\nvectors, which are obtained in an unsupervised manner. This ATPL approach is\napplied to 1) image captioning, 2) part of speech (POS) tagging, and 3)\nconstituency parsing of a sentence. Experimental results demonstrate the\neffectiveness of the proposed approach.", "published": "2018-02-20 12:42:07", "link": "http://arxiv.org/abs/1802.07089v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Fitting New Speakers Based on a Short Untranscribed Sample", "abstract": "Learning-based Text To Speech systems have the potential to generalize from\none speaker to the next and thus require a relatively short sample of any new\nvoice. However, this promise is currently largely unrealized. We present a\nmethod that is designed to capture a new speaker from a short untranscribed\naudio sample. This is done by employing an additional network that given an\naudio sample, places the speaker in the embedding space. This network is\ntrained as part of the speech synthesis system using various consistency\nlosses. Our results demonstrate a greatly improved performance on both the\ndataset speakers, and, more importantly, when fitting new voices, even from\nvery short samples.", "published": "2018-02-20 07:06:13", "link": "http://arxiv.org/abs/1802.06984v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
