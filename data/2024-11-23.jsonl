{"title": "Online High-Frequency Trading Stock Forecasting with Automated Feature Clustering and Radial Basis Function Neural Networks", "abstract": "This study presents an autonomous experimental machine learning protocol for\nhigh-frequency trading (HFT) stock price forecasting that involves a dual\ncompetitive feature importance mechanism and clustering via shallow neural\nnetwork topology for fast training. By incorporating the k-means algorithm into\nthe radial basis function neural network (RBFNN), the proposed method addresses\nthe challenges of manual clustering and the reliance on potentially\nuninformative features. More specifically, our approach involves a dual\ncompetitive mechanism for feature importance, combining the mean-decrease\nimpurity (MDI) method and a gradient descent (GD) based feature importance\nmechanism. This approach, tested on HFT Level 1 order book data for 20 S&P 500\nstocks, enhances the forecasting ability of the RBFNN regressor. Our findings\nsuggest that an autonomous approach to feature selection and clustering is\ncrucial, as each stock requires a different input feature space. Overall, by\nautomating the feature selection and clustering processes, we remove the need\nfor manual topological grid search and provide a more efficient way to predict\nLOB's mid-price.", "published": "2024-11-23 18:30:04", "link": "http://arxiv.org/abs/2412.16160v2", "categories": ["q-fin.ST", "cs.LG"], "primary_category": "q-fin.ST"}
{"title": "From Jack of All Trades to Master of One: Specializing LLM-based\n  Autoraters to a Test Set", "abstract": "As LLMs continue to become more powerful and versatile, human evaluation has\nquickly become intractable at scale and reliance on automatic metrics has\nbecome the norm. Recently, it has been shown that LLMs are themselves\nstate-of-the-art evaluators for many tasks. These Autoraters are typically\ndesigned so that they generalize to new systems and test sets. In practice,\nhowever, evaluation is performed on a small set of fixed, canonical test sets,\nwhich are carefully curated to measure certain capabilities of interest and are\nnot changed frequently. In this work, we design a method which specializes a\nprompted Autorater to a given test set, by leveraging historical ratings on the\ntest set to construct in-context learning (ICL) examples. We evaluate our\nSpecialist method on the task of fine-grained machine translation evaluation,\nand show that it dramatically outperforms the state-of-the-art XCOMET metric by\n54% and 119% on the WMT'23 and WMT'24 test sets, respectively. We perform\nextensive analyses to understand the representations learned by our Specialist\nmetrics, and how variability in rater behavior affects their performance. We\nalso verify the generalizability and robustness of our Specialist method for\ndesigning automatic metrics across different numbers of ICL examples, LLM\nbackbones, systems to evaluate, and evaluation tasks.", "published": "2024-11-23 00:02:21", "link": "http://arxiv.org/abs/2411.15387v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Ternary Weight Embedding Model: Bridging Scalability and\n  Performance", "abstract": "Embedding models have become essential tools in both natural language\nprocessing and computer vision, enabling efficient semantic search,\nrecommendation, clustering, and more. However, the high memory and\ncomputational demands of full-precision embeddings pose challenges for\ndeployment in resource-constrained environments, such as real-time\nrecommendation systems. In this work, we propose a novel finetuning framework\nto ternary-weight embedding models, which reduces memory and computational\noverhead while maintaining high performance. To apply ternarization to\npre-trained embedding models, we introduce self-taught knowledge distillation\nto finalize the ternary-weights of the linear layers. With extensive\nexperiments on public text and vision datasets, we demonstrated that without\nsacrificing effectiveness, the ternarized model consumes low memory usage and\nhas low latency in the inference stage with great efficiency. In practical\nimplementations, embedding models are typically integrated with Approximate\nNearest Neighbor (ANN) search. Our experiments combining ternary embedding with\nANN search yielded impressive improvement in both accuracy and computational\nefficiency. The repository is available at here.", "published": "2024-11-23 03:44:56", "link": "http://arxiv.org/abs/2411.15438v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HateDay: Insights from a Global Hate Speech Dataset Representative of a\n  Day on Twitter", "abstract": "To tackle the global challenge of online hate speech, a large body of\nresearch has developed detection models to flag hate speech in the sea of\nonline content. Yet, due to systematic biases in evaluation datasets, detection\nperformance in real-world settings remains unclear, let alone across\ngeographies. To address this issue, we introduce HateDay, the first global hate\nspeech dataset representative of social media settings, randomly sampled from\nall tweets posted on September 21, 2022 for eight languages and four\nEnglish-speaking countries. Using HateDay, we show how the prevalence and\ncomposition of hate speech varies across languages and countries. We also find\nthat evaluation on academic hate speech datasets overestimates real-world\ndetection performance, which we find is very low, especially for non-European\nlanguages. We identify several factors explaining poor performance, including\nmodels' inability to distinguish between hate and offensive speech, and the\nmisalignment between academic target focus and real-world target prevalence. We\nfinally argue that such low performance renders hate speech moderation with\npublic detection models unfeasible, even in a human-in-the-loop setting which\nwe find is prohibitively costly. Overall, we emphasize the need to evaluate\nfuture detection models from academia and platforms in real-world settings to\naddress this global challenge.", "published": "2024-11-23 05:54:30", "link": "http://arxiv.org/abs/2411.15462v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Seed-Free Synthetic Data Generation Framework for Instruction-Tuning\n  LLMs: A Case Study in Thai", "abstract": "We present a synthetic data approach for instruction-tuning large language\nmodels (LLMs) for low-resource languages in a data-efficient manner,\nspecifically focusing on Thai. We identify three key properties that contribute\nto the effectiveness of instruction-tuning datasets: fluency, diversity, and\ncultural context. We propose a seed-data-free framework for generating\nsynthetic instruction-tuning data that incorporates these essential properties.\nOur framework employs an LLM to generate diverse topics, retrieve relevant\ncontexts from Wikipedia, and create instructions for various tasks, such as\nquestion answering, summarization, and conversation. The experimental results\nshow that our best-performing synthetic dataset, which incorporates all three\nkey properties, achieves competitive performance using only 5,000 instructions\nwhen compared to state-of-the-art Thai LLMs trained on hundreds of thousands of\ninstructions. Our code and dataset are publicly available at\nhttps://github.com/parinzee/seed-free-synthetic-instruct.", "published": "2024-11-23 07:50:59", "link": "http://arxiv.org/abs/2411.15484v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Traditional Chinese Medicine Case Analysis System for High-Level\n  Semantic Abstraction: Optimized with Prompt and RAG", "abstract": "This paper details a technical plan for building a clinical case database for\nTraditional Chinese Medicine (TCM) using web scraping. Leveraging multiple\nplatforms, including 360doc, we gathered over 5,000 TCM clinical cases,\nperformed data cleaning, and structured the dataset with crucial fields such as\npatient details, pathogenesis, syndromes, and annotations. Using the\n$Baidu\\_ERNIE\\_Speed\\_128K$ API, we removed redundant information and generated\nthe final answers through the $DeepSeekv2$ API, outputting results in standard\nJSON format. We optimized data recall with RAG and rerank techniques during\nretrieval and developed a hybrid matching scheme. By combining two-stage\nretrieval method with keyword matching via Jieba, we significantly enhanced the\naccuracy of model outputs.", "published": "2024-11-23 08:24:15", "link": "http://arxiv.org/abs/2411.15491v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From MTEB to MTOB: Retrieval-Augmented Classification for Descriptive\n  Grammars", "abstract": "Recent advances in language modeling have demonstrated significant\nimprovements in zero-shot capabilities, including in-context learning,\ninstruction following, and machine translation for extremely under-resourced\nlanguages (Tanzer et al., 2024). However, many languages with limited written\nresources rely primarily on formal descriptions of grammar and vocabulary.\n  In this paper, we introduce a set of benchmarks to evaluate how well models\ncan extract and classify information from the complex descriptions found in\nlinguistic grammars. We present a Retrieval-Augmented Generation (RAG)-based\napproach that leverages these descriptions for downstream tasks such as machine\ntranslation. Our benchmarks encompass linguistic descriptions for 248 languages\nacross 142 language families, focusing on typological features from WALS and\nGrambank.\n  This set of benchmarks offers the first comprehensive evaluation of language\nmodels' in-context ability to accurately interpret and extract linguistic\nfeatures, providing a critical resource for scaling NLP to low-resource\nlanguages. The code and data are publicly available at\n\\url{https://github.com/al-the-eigenvalue/RAG-on-grammars}.", "published": "2024-11-23 14:47:10", "link": "http://arxiv.org/abs/2411.15577v2", "categories": ["cs.CL", "68-06, 68T50, 68T01", "G.3; I.2.7"], "primary_category": "cs.CL"}
{"title": "Transparent but Powerful: Explainability, Accuracy, and Generalizability\n  in ADHD Detection from Social Media Data", "abstract": "Attention-deficit/hyperactivity disorder (ADHD) is a prevalent mental health\ncondition affecting both children and adults, yet it remains severely\nunderdiagnosed. Recent advances in artificial intelligence, particularly in\nNatural Language Processing (NLP) and Machine Learning (ML), offer promising\nsolutions for scalable and non-invasive ADHD screening methods using social\nmedia data. This paper presents a comprehensive study on ADHD detection,\nleveraging both shallow machine learning models and deep learning approaches,\nincluding BiLSTM and transformer-based models, to analyze linguistic patterns\nin ADHD-related social media text. Our results highlight the trade-offs between\ninterpretability and performance across different models, with BiLSTM offering\na balance of transparency and accuracy. Additionally, we assess the\ngeneralizability of these models using cross-platform data from Reddit and\nTwitter, uncovering key linguistic features associated with ADHD that could\ncontribute to more effective digital screening tools.", "published": "2024-11-23 15:26:01", "link": "http://arxiv.org/abs/2411.15586v1", "categories": ["cs.CL", "68T50", "I.2.7; I.5.1"], "primary_category": "cs.CL"}
{"title": "Multi-label Sequential Sentence Classification via Large Language Model", "abstract": "Sequential sentence classification (SSC) in scientific publications is\ncrucial for supporting downstream tasks such as fine-grained information\nretrieval and extractive summarization. However, current SSC methods are\nconstrained by model size, sequence length, and single-label setting. To\naddress these limitations, this paper proposes LLM-SSC, a large language model\n(LLM)-based framework for both single- and multi-label SSC tasks. Unlike\nprevious approaches that employ small- or medium-sized language models, the\nproposed framework utilizes LLMs to generate SSC labels through designed\nprompts, which enhance task understanding by incorporating demonstrations and a\nquery to describe the prediction target. We also present a multi-label\ncontrastive learning loss with auto-weighting scheme, enabling the multi-label\nclassification task. To support our multi-label SSC analysis, we introduce and\nrelease a new dataset, biorc800, which mainly contains unstructured abstracts\nin the biomedical domain with manual annotations. Experiments demonstrate\nLLM-SSC's strong performance in SSC under both in-context learning and\ntask-specific tuning settings. We release biorc800 and our code at:\nhttps://github.com/ScienceNLP-Lab/LLM-SSC.", "published": "2024-11-23 18:27:35", "link": "http://arxiv.org/abs/2411.15623v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AfriMed-QA: A Pan-African, Multi-Specialty, Medical Question-Answering\n  Benchmark Dataset", "abstract": "Recent advancements in large language model(LLM) performance on medical\nmultiple choice question (MCQ) benchmarks have stimulated interest from\nhealthcare providers and patients globally. Particularly in low-and\nmiddle-income countries (LMICs) facing acute physician shortages and lack of\nspecialists, LLMs offer a potentially scalable pathway to enhance healthcare\naccess and reduce costs. However, their effectiveness in the Global South,\nespecially across the African continent, remains to be established. In this\nwork, we introduce AfriMed-QA, the first large scale Pan-African English\nmulti-specialty medical Question-Answering (QA) dataset, 15,000 questions (open\nand closed-ended) sourced from over 60 medical schools across 16 countries,\ncovering 32 medical specialties. We further evaluate 30 LLMs across multiple\naxes including correctness and demographic bias. Our findings show significant\nperformance variation across specialties and geographies, MCQ performance\nclearly lags USMLE (MedQA). We find that biomedical LLMs underperform general\nmodels and smaller edge-friendly LLMs struggle to achieve a passing score.\nInterestingly, human evaluations show a consistent consumer preference for LLM\nanswers and explanations when compared with clinician answers.", "published": "2024-11-23 19:43:02", "link": "http://arxiv.org/abs/2411.15640v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ML-SPEAK: A Theory-Guided Machine Learning Method for Studying and\n  Predicting Conversational Turn-taking Patterns", "abstract": "Predicting team dynamics from personality traits remains a fundamental\nchallenge for the psychological sciences and team-based organizations.\nUnderstanding how team composition generates team processes can significantly\nadvance team-based research along with providing practical guidelines for team\nstaffing and training. Although the Input-Process-Output (IPO) model has been\nuseful for studying these connections, the complex nature of team member\ninteractions demands a more dynamic approach. We develop a computational model\nof conversational turn-taking within self-organized teams that can provide\ninsight into the relationships between team member personality traits and team\ncommunication dynamics. We focus on turn-taking patterns between team members,\nindependent of content, which can significantly influence team emergent states\nand outcomes while being objectively measurable and quantifiable. As our model\nis trained on conversational data from teams of given trait compositions, it\ncan learn the relationships between individual traits and speaking behaviors\nand predict group-wide patterns of communication based on team trait\ncomposition alone. We first evaluate the performance of our model using\nsimulated data and then apply it to real-world data collected from\nself-organized student teams. In comparison to baselines, our model is more\naccurate at predicting speaking turn sequences and can reveal new relationships\nbetween team member traits and their communication patterns. Our approach\noffers a more data-driven and dynamic understanding of team processes. By\nbridging the gap between individual personality traits and team communication\npatterns, our model has the potential to inform theories of team processes and\nprovide powerful insights into optimizing team staffing and training.", "published": "2024-11-23 01:27:01", "link": "http://arxiv.org/abs/2411.15405v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring Large Language Models for Multimodal Sentiment Analysis:\n  Challenges, Benchmarks, and Future Directions", "abstract": "Multimodal Aspect-Based Sentiment Analysis (MABSA) aims to extract aspect\nterms and their corresponding sentiment polarities from multimodal information,\nincluding text and images. While traditional supervised learning methods have\nshown effectiveness in this task, the adaptability of large language models\n(LLMs) to MABSA remains uncertain. Recent advances in LLMs, such as Llama2,\nLLaVA, and ChatGPT, demonstrate strong capabilities in general tasks, yet their\nperformance in complex and fine-grained scenarios like MABSA is underexplored.\nIn this study, we conduct a comprehensive investigation into the suitability of\nLLMs for MABSA. To this end, we construct a benchmark to evaluate the\nperformance of LLMs on MABSA tasks and compare them with state-of-the-art\nsupervised learning methods. Our experiments reveal that, while LLMs\ndemonstrate potential in multimodal understanding, they face significant\nchallenges in achieving satisfactory results for MABSA, particularly in terms\nof accuracy and inference time. Based on these findings, we discuss the\nlimitations of current LLMs and outline directions for future research to\nenhance their capabilities in multimodal sentiment analysis.", "published": "2024-11-23 02:17:10", "link": "http://arxiv.org/abs/2411.15408v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Lifelong Knowledge Editing for Vision Language Models with Low-Rank\n  Mixture-of-Experts", "abstract": "Model editing aims to correct inaccurate knowledge, update outdated\ninformation, and incorporate new data into Large Language Models (LLMs) without\nthe need for retraining. This task poses challenges in lifelong scenarios where\nedits must be continuously applied for real-world applications. While some\neditors demonstrate strong robustness for lifelong editing in pure LLMs, Vision\nLLMs (VLLMs), which incorporate an additional vision modality, are not directly\nadaptable to existing LLM editors. In this paper, we propose LiveEdit, a\nLIfelong Vision language modEl Edit to bridge the gap between lifelong LLM\nediting and VLLMs. We begin by training an editing expert generator to\nindependently produce low-rank experts for each editing instance, with the goal\nof correcting the relevant responses of the VLLM. A hard filtering mechanism is\ndeveloped to utilize visual semantic knowledge, thereby coarsely eliminating\nvisually irrelevant experts for input queries during the inference stage of the\npost-edited model. Finally, to integrate visually relevant experts, we\nintroduce a soft routing mechanism based on textual semantic relevance to\nachieve multi-expert fusion. For evaluation, we establish a benchmark for\nlifelong VLLM editing. Extensive experiments demonstrate that LiveEdit offers\nsignificant advantages in lifelong VLLM editing scenarios. Further experiments\nvalidate the rationality and effectiveness of each module design in LiveEdit.", "published": "2024-11-23 03:19:40", "link": "http://arxiv.org/abs/2411.15432v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "MolMetaLM: a Physicochemical Knowledge-Guided Molecular Meta Language\n  Model", "abstract": "Most current molecular language models transfer the masked language model or\nimage-text generation model from natural language processing to molecular\nfield. However, molecules are not solely characterized by atom/bond symbols;\nthey encapsulate important physical/chemical properties. Moreover, normal\nlanguage models bring grammar rules that are irrelevant for understanding\nmolecules. In this study, we propose a novel physicochemical knowledge-guided\nmolecular meta language framework MolMetaLM. We design a molecule-specialized\nmeta language paradigm, formatted as multiple <S,P,O> (subject, predicate,\nobject) knowledge triples sharing the same S (i.e., molecule) to enhance\nlearning the semantic relationships between physicochemical knowledge and\nmolecules. By introducing different molecular knowledge and noises, the meta\nlanguage paradigm generates tens of thousands of pretraining tasks. By\nrecovering the token/sequence/order-level noises, MolMetaLM exhibits\nproficiency in large-scale benchmark evaluations involving property prediction,\nmolecule generation, conformation inference, and molecular optimization.\nThrough MolMetaLM, we offer a new insight for designing language models.", "published": "2024-11-23 09:27:38", "link": "http://arxiv.org/abs/2411.15500v1", "categories": ["cs.ET", "cs.CL"], "primary_category": "cs.ET"}
{"title": "Enhancing Grammatical Error Detection using BERT with Cleaned Lang-8\n  Dataset", "abstract": "This paper presents an improved LLM based model for Grammatical Error\nDetection (GED), which is a very challenging and equally important problem for\nmany applications. The traditional approach to GED involved hand-designed\nfeatures, but recently, Neural Networks (NN) have automated the discovery of\nthese features, improving performance in GED. Traditional rule-based systems\nhave an F1 score of 0.50-0.60 and earlier machine learning models give an F1\nscore of 0.65-0.75, including decision trees and simple neural networks.\nPrevious deep learning models, for example, Bi-LSTM, have reported F1 scores\nwithin the range from 0.80 to 0.90. In our study, we have fine-tuned various\ntransformer models using the Lang8 dataset rigorously cleaned by us. In our\nexperiments, the BERT-base-uncased model gave an impressive performance with an\nF1 score of 0.91 and accuracy of 98.49% on training data and 90.53% on testing\ndata, also showcasing the importance of data cleaning. Increasing model size\nusing BERT-large-uncased or RoBERTa-large did not give any noticeable\nimprovements in performance or advantage for this task, underscoring that\nlarger models are not always better. Our results clearly show how far rigorous\ndata cleaning and simple transformer-based models can go toward significantly\nimproving the quality of GED.", "published": "2024-11-23 10:57:41", "link": "http://arxiv.org/abs/2411.15523v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do LLMs Agree on the Creativity Evaluation of Alternative Uses?", "abstract": "This paper investigates whether large language models (LLMs) show agreement\nin assessing creativity in responses to the Alternative Uses Test (AUT). While\nLLMs are increasingly used to evaluate creative content, previous studies have\nprimarily focused on a single model assessing responses generated by the same\nmodel or humans. This paper explores whether LLMs can impartially and\naccurately evaluate creativity in outputs generated by both themselves and\nother models. Using an oracle benchmark set of AUT responses, categorized by\ncreativity level (common, creative, and highly creative), we experiment with\nfour state-of-the-art LLMs evaluating these outputs. We test both scoring and\nranking methods and employ two evaluation settings (comprehensive and\nsegmented) to examine if LLMs agree on the creativity evaluation of alternative\nuses. Results reveal high inter-model agreement, with Spearman correlations\naveraging above 0.7 across models and reaching over 0.77 with respect to the\noracle, indicating a high level of agreement and validating the reliability of\nLLMs in creativity assessment of alternative uses. Notably, models do not\nfavour their own responses, instead they provide similar creativity assessment\nscores or rankings for alternative uses generated by other models. These\nfindings suggest that LLMs exhibit impartiality and high alignment in\ncreativity evaluation, offering promising implications for their use in\nautomated creativity assessment.", "published": "2024-11-23 13:34:50", "link": "http://arxiv.org/abs/2411.15560v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "A Survey on LLM-as-a-Judge", "abstract": "Accurate and consistent evaluation is crucial for decision-making across\nnumerous fields, yet it remains a challenging task due to inherent\nsubjectivity, variability, and scale. Large Language Models (LLMs) have\nachieved remarkable success across diverse domains, leading to the emergence of\n\"LLM-as-a-Judge,\" where LLMs are employed as evaluators for complex tasks. With\ntheir ability to process diverse data types and provide scalable,\ncost-effective, and consistent assessments, LLMs present a compelling\nalternative to traditional expert-driven evaluations. However, ensuring the\nreliability of LLM-as-a-Judge systems remains a significant challenge that\nrequires careful design and standardization. This paper provides a\ncomprehensive survey of LLM-as-a-Judge, addressing the core question: How can\nreliable LLM-as-a-Judge systems be built? We explore strategies to enhance\nreliability, including improving consistency, mitigating biases, and adapting\nto diverse assessment scenarios. Additionally, we propose methodologies for\nevaluating the reliability of LLM-as-a-Judge systems, supported by a novel\nbenchmark designed for this purpose. To advance the development and real-world\ndeployment of LLM-as-a-Judge systems, we also discussed practical applications,\nchallenges, and future directions. This survey serves as a foundational\nreference for researchers and practitioners in this rapidly evolving field.", "published": "2024-11-23 16:03:35", "link": "http://arxiv.org/abs/2411.15594v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Next Tokens via Second-to-Last Predictions with Generate and\n  Refine", "abstract": "Autoregressive language models like GPT aim to predict next tokens, while\nautoencoding models such as BERT are trained on tasks such as predicting masked\ntokens. We train a decoder-only architecture for predicting the second to last\ntoken for a sequence of tokens. Our approach yields higher computational\ntraining efficiency than BERT-style models by employing a structured\ndeterministic approach to masking tokens. We use our model to improve the next\ntoken predictions of a standard GPT by combining both predictions in a\n``generate-then-refine'' approach. We demonstrate on different variants of\nGPT-2 and different datasets that (not unexpectedly) second to last token\npredictions are much more accurate, i.e., more than 15\\% higher accuracy than\nstandard next token predictions. The ``generate-then-refine'' approach also\ndemonstrates notable improvements in next-token predictions, yielding smaller\nyet consistent and significant gains.", "published": "2024-11-23 22:09:58", "link": "http://arxiv.org/abs/2411.15661v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Ontology-Constrained Generation of Domain-Specific Clinical Summaries", "abstract": "Large Language Models (LLMs) offer promising solutions for text\nsummarization. However, some domains require specific information to be\navailable in the summaries. Generating these domain-adapted summaries is still\nan open challenge. Similarly, hallucinations in generated content is a major\ndrawback of current approaches, preventing their deployment. This study\nproposes a novel approach that leverages ontologies to create domain-adapted\nsummaries both structured and unstructured. We employ an ontology-guided\nconstrained decoding process to reduce hallucinations while improving\nrelevance. When applied to the medical domain, our method shows potential in\nsummarizing Electronic Health Records (EHRs) across different specialties,\nallowing doctors to focus on the most relevant information to their domain.\nEvaluation on the MIMIC-III dataset demonstrates improvements in generating\ndomain-adapted summaries of clinical notes and hallucination reduction.", "published": "2024-11-23 23:05:48", "link": "http://arxiv.org/abs/2411.15666v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Multi-Reranker: Maximizing performance of retrieval-augmented generation\n  in the FinanceRAG challenge", "abstract": "As Large Language Models (LLMs) increasingly address domain-specific\nproblems, their application in the financial sector has expanded rapidly. Tasks\nthat are both highly valuable and time-consuming, such as analyzing financial\nstatements, disclosures, and related documents, are now being effectively\ntackled using LLMs. This paper details the development of a high-performance,\nfinance-specific Retrieval-Augmented Generation (RAG) system for the ACM-ICAIF\n'24 FinanceRAG competition. We optimized performance through ablation studies\non query expansion and corpus refinement during the pre-retrieval phase. To\nenhance retrieval accuracy, we employed multiple reranker models. Notably, we\nintroduced an efficient method for managing long context sizes during the\ngeneration phase, significantly improving response quality without sacrificing\nperformance. We ultimately achieve 2nd place in the FinanceRAG Challenge. Our\nkey contributions include: (1) pre-retrieval ablation analysis, (2) an enhanced\nretrieval algorithm, and (3) a novel approach for long-context management. This\nwork demonstrates the potential of LLMs in effectively processing and analyzing\ncomplex financial data to generate accurate and valuable insights. The source\ncode and further details are available at https://github.com/cv-lee/FinanceRAG.", "published": "2024-11-23 09:56:21", "link": "http://arxiv.org/abs/2411.16732v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Comparative Analysis of Transformer and LSTM Models for Detecting\n  Suicidal Ideation on Reddit", "abstract": "Suicide is a critical global health problem involving more than 700,000\ndeaths yearly, particularly among young adults. Many people express their\nsuicidal thoughts on social media platforms such as Reddit. This paper\nevaluates the effectiveness of the deep learning transformer-based models BERT,\nRoBERTa, DistilBERT, ALBERT, and ELECTRA and various Long Short-Term Memory\n(LSTM) based models in detecting suicidal ideation from user posts on Reddit.\nToward this objective, we curated an extensive dataset from diverse subreddits\nand conducted linguistic, topic modeling, and statistical analyses to ensure\ndata quality. Our results indicate that each model could reach high accuracy\nand F1 scores, but among them, RoBERTa emerged as the most effective model with\nan accuracy of 93.22% and F1 score of 93.14%. An LSTM model that uses attention\nand BERT embeddings performed as the second best, with an accuracy of 92.65%\nand an F1 score of 92.69%. Our findings show that transformer-based models have\nthe potential to improve suicide ideation detection, thereby providing a path\nto develop robust mental health monitoring tools from social media. This\nresearch, therefore, underlines the undeniable prospect of advanced techniques\nin Natural Language Processing (NLP) while improving suicide prevention\nefforts.", "published": "2024-11-23 01:17:43", "link": "http://arxiv.org/abs/2411.15404v1", "categories": ["cs.LG", "cs.CL", "cs.SI"], "primary_category": "cs.LG"}
{"title": "Towards Robust Evaluation of Unlearning in LLMs via Data Transformations", "abstract": "Large Language Models (LLMs) have shown to be a great success in a wide range\nof applications ranging from regular NLP-based use cases to AI agents. LLMs\nhave been trained on a vast corpus of texts from various sources; despite the\nbest efforts during the data pre-processing stage while training the LLMs, they\nmay pick some undesirable information such as personally identifiable\ninformation (PII). Consequently, in recent times research in the area of\nMachine Unlearning (MUL) has become active, the main idea is to force LLMs to\nforget (unlearn) certain information (e.g., PII) without suffering from\nperformance loss on regular tasks. In this work, we examine the robustness of\nthe existing MUL techniques for their ability to enable leakage-proof\nforgetting in LLMs. In particular, we examine the effect of data transformation\non forgetting, i.e., is an unlearned LLM able to recall forgotten information\nif there is a change in the format of the input? Our findings on the TOFU\ndataset highlight the necessity of using diverse data formats to quantify\nunlearning in LLMs more reliably.", "published": "2024-11-23 07:20:36", "link": "http://arxiv.org/abs/2411.15477v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transition Network Analysis: A Novel Framework for Modeling,\n  Visualizing, and Identifying the Temporal Patterns of Learners and Learning\n  Processes", "abstract": "This paper presents a novel learning analytics method: Transition Network\nAnalysis (TNA), a method that integrates Stochastic Process Mining and\nprobabilistic graph representation to model, visualize, and identify transition\npatterns in the learning process data. Combining the relational and temporal\naspects into a single lens offers capabilities beyond either framework,\nincluding centralities to capture important learning events, community\ndetection to identify behavior patterns, and clustering to reveal temporal\npatterns. Furthermore, TNA introduces several significance tests that go beyond\neither method and add rigor to the analysis. Here, we introduce the theoretical\nand mathematical foundations of TNA and we demonstrate the functionalities of\nTNA with a case study where students (n=191) engaged in small-group\ncollaboration to map patterns of group dynamics using the theories of\nco-regulation and socially-shared regulated learning. The analysis revealed\nthat TNA can map the regulatory processes as well as identify important events,\npatterns, and clusters. Bootstrap validation established the significant\ntransitions and eliminated spurious transitions. As such, TNA can capture\nlearning dynamics and provide a robust framework for investigating the temporal\nevolution of learning processes. Future directions include -- inter alia --\nexpanding estimation methods, reliability assessment, and building longitudinal\nTNA.", "published": "2024-11-23 07:54:15", "link": "http://arxiv.org/abs/2411.15486v2", "categories": ["cs.SI", "cs.CL", "cs.CY"], "primary_category": "cs.SI"}
{"title": "Automatic Evaluation for Text-to-image Generation: Task-decomposed\n  Framework, Distilled Training, and Meta-evaluation Benchmark", "abstract": "Driven by the remarkable progress in diffusion models, text-to-image\ngeneration has made significant strides, creating a pressing demand for\nautomatic quality evaluation of generated images. Current state-of-the-art\nautomatic evaluation methods heavily rely on Multi-modal Large Language Models\n(MLLMs), particularly powerful commercial models like GPT-4o. While these\nmodels are highly effective, their substantial costs limit scalability in\nlarge-scale evaluations. Adopting open-source MLLMs is an alternative; however,\ntheir performance falls short due to significant limitations in processing\nmulti-modal data compared to commercial MLLMs. To tackle these problems, we\nfirst propose a task decomposition evaluation framework based on GPT-4o to\nautomatically construct a new training dataset, where the complex evaluation\ntask is decoupled into simpler sub-tasks, effectively reducing the learning\ncomplexity. Based on this dataset, we design innovative training strategies to\neffectively distill GPT-4o's evaluation capabilities into a 7B open-source\nMLLM, MiniCPM-V-2.6. Furthermore, to reliably and comprehensively assess prior\nworks and our proposed model, we manually annotate a meta-evaluation benchmark\nthat includes chain-of-thought explanations alongside quality scores for\ngenerated images. Experimental results demonstrate that our distilled\nopen-source MLLM significantly outperforms the current state-of-the-art\nGPT-4o-base baseline, VIEScore, with over 4.6\\% improvement in Spearman and\nKendall correlations with human judgments.", "published": "2024-11-23 08:06:06", "link": "http://arxiv.org/abs/2411.15488v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "QEQR: An Exploration of Query Expansion Methods for Question Retrieval\n  in CQA Services", "abstract": "CQA services are valuable sources of knowledge that can be used to find\nanswers to users' information needs. In these services, question retrieval aims\nto help users with their information needs by finding similar questions to\ntheirs. However, finding similar questions is obstructed by the lexical gap\nthat exists between relevant questions. In this work, we target this problem by\nusing query expansion methods. We use word-similarity-based methods, propose a\nquestion-similarity-based method and selective expansion of these methods to\nexpand a question that's been submitted and mitigate the lexical gap problem.\nOur best method achieves a significant relative improvement of 1.8\\% compared\nto the best-performing baseline without query expansion.", "published": "2024-11-23 11:47:03", "link": "http://arxiv.org/abs/2411.15530v1", "categories": ["cs.IR", "cs.CL", "cs.HC"], "primary_category": "cs.IR"}
{"title": "\"All that Glitters\": Approaches to Evaluations with Unreliable Model and\n  Human Annotations", "abstract": "\"Gold\" and \"ground truth\" human-mediated labels have error. The effects of\nthis error can escape commonly reported metrics of label quality or obscure\nquestions of accuracy, bias, fairness, and usefulness during model evaluation.\nThis study demonstrates methods for answering such questions even in the\ncontext of very low reliabilities from expert humans. We analyze human labels,\nGPT model ratings, and transformer encoder model annotations describing the\nquality of classroom teaching, an important, expensive, and currently only\nhuman task. We answer the question of whether such a task can be automated\nusing two Large Language Model (LLM) architecture families--encoders and GPT\ndecoders, using novel approaches to evaluating label quality across six\ndimensions: Concordance, Confidence, Validity, Bias, Fairness, and Helpfulness.\nFirst, we demonstrate that using standard metrics in the presence of poor\nlabels can mask both label and model quality: the encoder family of models\nachieve state-of-the-art, even \"super-human\", results across all classroom\nannotation tasks. But not all these positive results remain after using more\nrigorous evaluation measures which reveal spurious correlations and nonrandom\nracial biases across models and humans. This study then expands these methods\nto estimate how model use would change to human label quality if models were\nused in a human-in-the-loop context, finding that the variance captured in GPT\nmodel labels would worsen reliabilities for humans influenced by these models.\nWe identify areas where some LLMs, within the generalizability of the current\ndata, could improve the quality of expensive human ratings of classroom\ninstruction.", "published": "2024-11-23 19:18:08", "link": "http://arxiv.org/abs/2411.15634v1", "categories": ["cs.CL", "cs.AI", "stat.AP"], "primary_category": "cs.CL"}
{"title": "\"Moralized\" Multi-Step Jailbreak Prompts: Black-Box Testing of\n  Guardrails in Large Language Models for Verbal Attacks", "abstract": "As the application of large language models continues to expand in various\nfields, it poses higher challenges to the effectiveness of identifying harmful\ncontent generation and guardrail mechanisms. This research aims to evaluate the\nguardrail effectiveness of GPT-4o, Grok-2 Beta, Llama 3.1 (405B), Gemini 1.5,\nand Claude 3.5 Sonnet through black-box testing of seemingly ethical multi-step\njailbreak prompts. It conducts ethical attacks by designing an identical\nmulti-step prompts that simulates the scenario of \"corporate middle managers\ncompeting for promotions.\" The data results show that the guardrails of the\nabove-mentioned LLMs were bypassed and the content of verbal attacks was\ngenerated. Claude 3.5 Sonnet's resistance to multi-step jailbreak prompts is\nmore obvious. To ensure objectivity, the experimental process, black box test\ncode, and enhanced guardrail code are uploaded to the GitHub repository:\nhttps://github.com/brucewang123456789/GeniusTrail.git.", "published": "2024-11-23 09:32:44", "link": "http://arxiv.org/abs/2411.16730v4", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "ChemSafetyBench: Benchmarking LLM Safety on Chemistry Domain", "abstract": "The advancement and extensive application of large language models (LLMs)\nhave been remarkable, including their use in scientific research assistance.\nHowever, these models often generate scientifically incorrect or unsafe\nresponses, and in some cases, they may encourage users to engage in dangerous\nbehavior. To address this issue in the field of chemistry, we introduce\nChemSafetyBench, a benchmark designed to evaluate the accuracy and safety of\nLLM responses. ChemSafetyBench encompasses three key tasks: querying chemical\nproperties, assessing the legality of chemical uses, and describing synthesis\nmethods, each requiring increasingly deeper chemical knowledge. Our dataset has\nmore than 30K samples across various chemical materials. We incorporate\nhandcrafted templates and advanced jailbreaking scenarios to enhance task\ndiversity. Our automated evaluation framework thoroughly assesses the safety,\naccuracy, and appropriateness of LLM responses. Extensive experiments with\nstate-of-the-art LLMs reveal notable strengths and critical vulnerabilities,\nunderscoring the need for robust safety measures. ChemSafetyBench aims to be a\npivotal tool in developing safer AI technologies in chemistry. Our code and\ndataset are available at https://github.com/HaochenZhao/SafeAgent4Chem.\nWarning: this paper contains discussions on the synthesis of controlled\nchemicals using AI models.", "published": "2024-11-23 12:50:33", "link": "http://arxiv.org/abs/2411.16736v1", "categories": ["cs.CL", "cs.AI", "physics.chem-ph"], "primary_category": "cs.CL"}
{"title": "ChatBCI: A P300 Speller BCI Leveraging Large Language Models for\n  Improved Sentence Composition in Realistic Scenarios", "abstract": "P300 speller BCIs allow users to compose sentences by selecting target keys\non a GUI through the detection of P300 component in their EEG signals following\nvisual stimuli. Most P300 speller BCIs require users to spell words letter by\nletter, or the first few initial letters, resulting in high keystroke demands\nthat increase time, cognitive load, and fatigue. This highlights the need for\nmore efficient, user-friendly methods for faster sentence composition. In this\nwork, we introduce ChatBCI, a P300 speller BCI that leverages the zero-shot\nlearning capabilities of large language models (LLMs) to suggest words from\nuser-spelled initial letters or predict the subsequent word(s), reducing\nkeystrokes and accelerating sentence composition. ChatBCI retrieves word\nsuggestions through remote queries to the GPT-3.5 API. A new GUI, displaying\nGPT-3.5 word suggestions as extra keys is designed. SWLDA is used for the P300\nclassification. Seven subjects completed two online spelling tasks: 1)\ncopy-spelling a self-composed sentence using ChatBCI, and 2) improvising a\nsentence using ChatBCI's word suggestions. Results demonstrate that in Task 1,\non average, ChatBCI outperforms letter-by-letter BCI spellers, reducing time\nand keystrokes by 62.14% and 53.22%, respectively, and increasing information\ntransfer rate by 198.96%. In Task 2, ChatBCI achieves 80.68% keystroke savings\nand a record 8.53 characters/min for typing speed. Overall, ChatBCI, by\nemploying remote LLM queries, enhances sentence composition in realistic\nscenarios, significantly outperforming traditional spellers without requiring\nlocal model training or storage. ChatBCI's (multi-) word predictions, combined\nwith its new GUI, pave the way for developing next-generation speller BCIs that\nare efficient and effective for real-time communication, especially for users\nwith communication and motor disabilities.", "published": "2024-11-23 00:42:12", "link": "http://arxiv.org/abs/2411.15395v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.SY", "eess.SP", "eess.SY"], "primary_category": "cs.HC"}
{"title": "Gotta Hear Them All: Sound Source Aware Vision to Audio Generation", "abstract": "Vision-to-audio (V2A) synthesis has broad applications in multimedia. Recent\nadvancements of V2A methods have made it possible to generate relevant audios\nfrom inputs of videos or still images. However, the immersiveness and\nexpressiveness of the generation are limited. One possible problem is that\nexisting methods solely rely on the global scene and overlook details of local\nsounding objects (i.e., sound sources). To address this issue, we propose a\nSound Source-Aware V2A (SSV2A) generator. SSV2A is able to locally perceive\nmultimodal sound sources from a scene with visual detection and cross-modality\ntranslation. It then contrastively learns a Cross-Modal Sound Source (CMSS)\nManifold to semantically disambiguate each source. Finally, we attentively mix\ntheir CMSS semantics into a rich audio representation, from which a pretrained\naudio generator outputs the sound. To model the CMSS manifold, we curate a\nnovel single-sound-source visual-audio dataset VGGS3 from VGGSound. We also\ndesign a Sound Source Matching Score to measure localized audio relevance. By\naddressing V2A generation at the sound-source level, SSV2A surpasses\nstate-of-the-art methods in both generation fidelity and relevance as evidenced\nby extensive experiments. We further demonstrate SSV2A's ability to achieve\nintuitive V2A control by compositing vision, text, and audio conditions. Our\ngeneration can be tried and heard at https://ssv2a.github.io/SSV2A-demo .", "published": "2024-11-23 04:27:19", "link": "http://arxiv.org/abs/2411.15447v3", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Hindi audio-video-Deepfake (HAV-DF): A Hindi language-based Audio-video\n  Deepfake Dataset", "abstract": "Deepfakes offer great potential for innovation and creativity, but they also\npose significant risks to privacy, trust, and security. With a vast\nHindi-speaking population, India is particularly vulnerable to deepfake-driven\nmisinformation campaigns. Fake videos or speeches in Hindi can have an enormous\nimpact on rural and semi-urban communities, where digital literacy tends to be\nlower and people are more inclined to trust video content. The development of\neffective frameworks and detection tools to combat deepfake misuse requires\nhigh-quality, diverse, and extensive datasets. The existing popular datasets\nlike FF-DF (FaceForensics++), and DFDC (DeepFake Detection Challenge) are based\non English language.. Hence, this paper aims to create a first novel Hindi deep\nfake dataset, named ``Hindi audio-video-Deepfake'' (HAV-DF). The dataset has\nbeen generated using the faceswap, lipsyn and voice cloning methods. This\nmulti-step process allows us to create a rich, varied dataset that captures the\nnuances of Hindi speech and facial expressions, providing a robust foundation\nfor training and evaluating deepfake detection models in a Hindi language\ncontext. It is unique of its kind as all of the previous datasets contain\neither deepfake videos or synthesized audio. This type of deepfake dataset can\nbe used for training a detector for both deepfake video and audio datasets.\nNotably, the newly introduced HAV-DF dataset demonstrates lower detection\naccuracy's across existing detection methods like Headpose, Xception-c40, etc.\nCompared to other well-known datasets FF-DF, and DFDC. This trend suggests that\nthe HAV-DF dataset presents deeper challenges to detect, possibly due to its\nfocus on Hindi language content and diverse manipulation techniques. The HAV-DF\ndataset fills the gap in Hindi-specific deepfake datasets, aiding multilingual\ndeepfake detection development.", "published": "2024-11-23 05:18:43", "link": "http://arxiv.org/abs/2411.15457v1", "categories": ["cs.SD", "cs.AI", "cs.CR", "cs.GR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DiM-Gestor: Co-Speech Gesture Generation with Adaptive Layer\n  Normalization Mamba-2", "abstract": "Speech-driven gesture generation using transformer-based generative models\nrepresents a rapidly advancing area within virtual human creation. However,\nexisting models face significant challenges due to their quadratic time and\nspace complexities, limiting scalability and efficiency. To address these\nlimitations, we introduce DiM-Gestor, an innovative end-to-end generative model\nleveraging the Mamba-2 architecture. DiM-Gestor features a dual-component\nframework: (1) a fuzzy feature extractor and (2) a speech-to-gesture mapping\nmodule, both built on the Mamba-2. The fuzzy feature extractor, integrated with\na Chinese Pre-trained Model and Mamba-2, autonomously extracts implicit,\ncontinuous speech features. These features are synthesized into a unified\nlatent representation and then processed by the speech-to-gesture mapping\nmodule. This module employs an Adaptive Layer Normalization (AdaLN)-enhanced\nMamba-2 mechanism to uniformly apply transformations across all sequence\ntokens. This enables precise modeling of the nuanced interplay between speech\nfeatures and gesture dynamics. We utilize a diffusion model to train and infer\ndiverse gesture outputs. Extensive subjective and objective evaluations\nconducted on the newly released Chinese Co-Speech Gestures dataset corroborate\nthe efficacy of our proposed model. Compared with Transformer-based\narchitecture, the assessments reveal that our approach delivers competitive\nresults and significantly reduces memory usage, approximately 2.4 times, and\nenhances inference speeds by 2 to 4 times. Additionally, we released the CCG\ndataset, a Chinese Co-Speech Gestures dataset, comprising 15.97 hours (six\nstyles across five scenarios) of 3D full-body skeleton gesture motion performed\nby professional Chinese TV broadcasters.", "published": "2024-11-23 08:02:03", "link": "http://arxiv.org/abs/2411.16729v1", "categories": ["cs.SD", "cs.AI", "cs.GR", "cs.HC", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
