{"title": "Multimodal Stock Price Prediction", "abstract": "In an era where financial markets are heavily influenced by many static and\ndynamic factors, it has become increasingly critical to carefully integrate\ndiverse data sources with machine learning for accurate stock price prediction.\nThis paper explores a multimodal machine learning approach for stock price\nprediction by combining data from diverse sources, including traditional\nfinancial metrics, tweets, and news articles. We capture real-time market\ndynamics and investor mood through sentiment analysis on these textual data\nusing both ChatGPT-4o and FinBERT models. We look at how these integrated data\nstreams augment predictions made with a standard Long Short-Term Memory (LSTM\nmodel) to illustrate the extent of performance gains. Our study's results\nindicate that incorporating the mentioned data sources considerably increases\nthe forecast effectiveness of the reference model by up to 5%. We also provide\ninsights into the individual and combined predictive capacities of these\nmodalities, highlighting the substantial impact of incorporating sentiment\nanalysis from tweets and news articles. This research offers a systematic and\neffective framework for applying multimodal data analytics techniques in\nfinancial time series forecasting that provides a new view for investors to\nleverage data for decision-making.", "published": "2025-01-23 16:38:46", "link": "http://arxiv.org/abs/2502.05186v1", "categories": ["q-fin.ST", "cs.AI", "cs.LG"], "primary_category": "q-fin.ST"}
{"title": "Task-Oriented Automatic Fact-Checking with Frame-Semantics", "abstract": "We propose a novel paradigm for automatic fact-checking that leverages frame\nsemantics to enhance the structured understanding of claims and guide the\nprocess of fact-checking them. To support this, we introduce a pilot dataset of\nreal-world claims extracted from PolitiFact, specifically annotated for\nlarge-scale structured data. This dataset underpins two case studies: the first\ninvestigates voting-related claims using the Vote semantic frame, while the\nsecond explores various semantic frames based on data sources from the\nOrganisation for Economic Co-operation and Development (OECD). Our findings\ndemonstrate the effectiveness of frame semantics in improving evidence\nretrieval and explainability for fact-checking. Finally, we conducted a survey\nof frames evoked in fact-checked claims, identifying high-impact frames to\nguide future work in this direction.", "published": "2025-01-23 00:26:09", "link": "http://arxiv.org/abs/2501.13288v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hypothesis Generation for Materials Discovery and Design Using\n  Goal-Driven and Constraint-Guided LLM Agents", "abstract": "Materials discovery and design are essential for advancing technology across\nvarious industries by enabling the development of application-specific\nmaterials. Recent research has leveraged Large Language Models (LLMs) to\naccelerate this process. We explore the potential of LLMs to generate viable\nhypotheses that, once validated, can expedite materials discovery.\nCollaborating with materials science experts, we curated a novel dataset from\nrecent journal publications, featuring real-world goals, constraints, and\nmethods for designing real-world applications. Using this dataset, we test\nLLM-based agents that generate hypotheses for achieving given goals under\nspecific constraints. To assess the relevance and quality of these hypotheses,\nwe propose a novel scalable evaluation metric that emulates the process a\nmaterials scientist would use to evaluate a hypothesis critically. Our curated\ndataset, proposed method, and evaluation framework aim to advance future\nresearch in accelerating materials discovery and design with LLMs.", "published": "2025-01-23 01:01:05", "link": "http://arxiv.org/abs/2501.13299v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do as We Do, Not as You Think: the Conformity of Large Language Models", "abstract": "Recent advancements in large language models (LLMs) revolutionize the field\nof intelligent agents, enabling collaborative multi-agent systems capable of\ntackling complex problems across various domains. However, the potential of\nconformity within these systems, analogous to phenomena like conformity bias\nand groupthink in human group dynamics, remains largely unexplored, raising\nconcerns about their collective problem-solving capabilities and possible\nethical implications. This paper presents a comprehensive study on conformity\nin LLM-driven multi-agent systems, focusing on three aspects: the existence of\nconformity, the factors influencing conformity, and potential mitigation\nstrategies. In particular, we introduce BenchForm, a new conformity-oriented\nbenchmark, featuring reasoning-intensive tasks and five distinct interaction\nprotocols designed to probe LLMs' behavior in collaborative scenarios. Several\nrepresentative LLMs are evaluated on BenchForm, using metrics such as\nconformity rate and independence rate to quantify conformity's impact. Our\nanalysis delves into factors influencing conformity, including interaction time\nand majority size, and examines how the subject agent rationalizes its\nconforming behavior. Furthermore, we explore two strategies to mitigate\nconformity effects, i.e., developing enhanced personas and implementing a\nreflection mechanism. Several interesting findings regarding LLMs' conformity\nare derived from empirical results and case studies. We hope that these\ninsights can pave the way for more robust and ethically-aligned collaborative\nAI systems. Our benchmark and code are available at BenchForm.", "published": "2025-01-23 04:50:03", "link": "http://arxiv.org/abs/2501.13381v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models Understand Preferences in Personalized\n  Recommendation?", "abstract": "Large Language Models (LLMs) excel in various tasks, including personalized\nrecommendations. Existing evaluation methods often focus on rating prediction,\nrelying on regression errors between actual and predicted ratings. However,\nuser rating bias and item quality, two influential factors behind rating\nscores, can obscure personal preferences in user-item pair data. To address\nthis, we introduce PerRecBench, disassociating the evaluation from these two\nfactors and assessing recommendation techniques on capturing the personal\npreferences in a grouped ranking manner. We find that the LLM-based\nrecommendation techniques that are generally good at rating prediction fail to\nidentify users' favored and disfavored items when the user rating bias and item\nquality are eliminated by grouping users. With PerRecBench and 19 LLMs, we find\nthat while larger models generally outperform smaller ones, they still struggle\nwith personalized recommendation. Our findings reveal the superiority of\npairwise and listwise ranking approaches over pointwise ranking, PerRecBench's\nlow correlation with traditional regression metrics, the importance of user\nprofiles, and the role of pretraining data distributions. We further explore\nthree supervised fine-tuning strategies, finding that merging weights from\nsingle-format training is promising but improving LLMs' understanding of user\npreferences remains an open research problem. Code and data are available at\nhttps://github.com/TamSiuhin/PerRecBench", "published": "2025-01-23 05:24:18", "link": "http://arxiv.org/abs/2501.13391v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey of Code-switched Arabic NLP: Progress, Challenges, and Future\n  Directions", "abstract": "Language in the Arab world presents a complex diglossic and multilingual\nsetting, involving the use of Modern Standard Arabic, various dialects and\nsub-dialects, as well as multiple European languages. This diverse linguistic\nlandscape has given rise to code-switching, both within Arabic varieties and\nbetween Arabic and foreign languages. The widespread occurrence of\ncode-switching across the region makes it vital to address these linguistic\nneeds when developing language technologies. In this paper, we provide a review\nof the current literature in the field of code-switched Arabic NLP, offering a\nbroad perspective on ongoing efforts, challenges, research gaps, and\nrecommendations for future research directions.", "published": "2025-01-23 06:46:23", "link": "http://arxiv.org/abs/2501.13419v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Level Attention and Contrastive Learning for Enhanced Text\n  Classification with an Optimized Transformer", "abstract": "This paper studies a text classification algorithm based on an improved\nTransformer to improve the performance and efficiency of the model in text\nclassification tasks. Aiming at the shortcomings of the traditional Transformer\nmodel in capturing deep semantic relationships and optimizing computational\ncomplexity, this paper introduces a multi-level attention mechanism and a\ncontrastive learning strategy. The multi-level attention mechanism effectively\nmodels the global semantics and local features in the text by combining global\nattention with local attention; the contrastive learning strategy enhances the\nmodel's ability to distinguish between different categories by constructing\npositive and negative sample pairs while improving the classification effect.\nIn addition, in order to improve the training and inference efficiency of the\nmodel on large-scale text data, this paper designs a lightweight module to\noptimize the feature transformation process and reduce the computational cost.\nExperimental results on the dataset show that the improved Transformer model\noutperforms the comparative models such as BiLSTM, CNN, standard Transformer,\nand BERT in terms of classification accuracy, F1 score, and recall rate,\nshowing stronger semantic representation ability and generalization\nperformance. The method proposed in this paper provides a new idea for\nalgorithm optimization in the field of text classification and has good\napplication potential and practical value. Future work will focus on studying\nthe performance of this model in multi-category imbalanced datasets and\ncross-domain tasks and explore the integration wi", "published": "2025-01-23 08:32:27", "link": "http://arxiv.org/abs/2501.13467v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Contextual Faithfulness of Large Language Models via Retrieval\n  Heads-Induced Optimization", "abstract": "Ensuring contextual faithfulness in retrieval-augmented large language models\n(LLMs) is crucial for building trustworthy information-seeking systems,\nparticularly in long-form question-answering (LFQA) scenarios. In this work, we\nidentify a salient correlation between LFQA faithfulness and retrieval heads, a\nset of attention heads responsible for retrieving contextual information.\nLeveraging this insight, we propose RHIO, a framework designed to teach LLMs to\nexplicitly discriminate between faithful and unfaithful generations. RHIO first\naugments unfaithful samples that simulate realistic model-intrinsic errors by\nselectively masking retrieval heads. Then, these samples are incorporated into\njoint training, enabling the model to distinguish unfaithful outputs from\nfaithful ones conditioned on control tokens. Furthermore, these control tokens\nare leveraged to self-induce contrastive outputs, amplifying their difference\nthrough contrastive decoding. Additionally, to facilitate the evaluation of\ncontextual faithfulness, we also introduce GroundBench, a comprehensive\nbenchmark compiled from five existing LFQA datasets. Extensive experimental\nresults on GroundBench demonstrate that RHIO significantly improves\nfaithfulness, even outperforming GPT-4o.", "published": "2025-01-23 11:23:25", "link": "http://arxiv.org/abs/2501.13573v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain-Specific Machine Translation to Translate Medicine Brochures in\n  English to Sorani Kurdish", "abstract": "Access to Kurdish medicine brochures is limited, depriving Kurdish-speaking\ncommunities of critical health information. To address this problem, we\ndeveloped a specialized Machine Translation (MT) model to translate English\nmedicine brochures into Sorani Kurdish using a parallel corpus of 22,940\naligned sentence pairs from 319 brochures, sourced from two pharmaceutical\ncompanies in the Kurdistan Region of Iraq (KRI). We trained a Statistical\nMachine Translation (SMT) model using the Moses toolkit, conducting seven\nexperiments that resulted in BLEU scores ranging from 22.65 to 48.93. We\ntranslated three new brochures to improve the evaluation process and\nencountered unknown words. We addressed unknown words through post-processing\nwith a medical dictionary, resulting in BLEU scores of 56.87, 31.05, and 40.01.\nHuman evaluation by native Kurdish-speaking pharmacists, physicians, and\nmedicine users showed that 50% of professionals found the translations\nconsistent, while 83.3% rated them accurate. Among users, 66.7% considered the\ntranslations clear and felt confident using the medications.", "published": "2025-01-23 12:28:36", "link": "http://arxiv.org/abs/2501.13609v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models", "abstract": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%.", "published": "2025-01-23 12:58:14", "link": "http://arxiv.org/abs/2501.13629v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LVPruning: An Effective yet Simple Language-Guided Vision Token Pruning\n  Approach for Multi-modal Large Language Models", "abstract": "Multi-modal Large Language Models (MLLMs) have achieved remarkable success by\nintegrating visual and textual modalities. However, they incur significant\ncomputational overhead due to the large number of vision tokens processed,\nlimiting their practicality in resource-constrained environments. We introduce\nLanguage-Guided Vision Token Pruning (LVPruning) for MLLMs, an effective yet\nsimple method that significantly reduces the computational burden while\npreserving model performance. LVPruning employs cross-attention modules to\ncompute the importance of vision tokens based on their interaction with\nlanguage tokens, determining which to prune. Importantly, LVPruning can be\nintegrated without modifying the original MLLM parameters, which makes\nLVPruning simple to apply or remove. Our experiments show that LVPruning can\neffectively reduce up to 90% of vision tokens by the middle layer of LLaVA-1.5,\nresulting in a 62.1% decrease in inference Tera Floating-Point Operations Per\nSecond (TFLOPs), with an average performance loss of just 0.45% across nine\nmulti-modal benchmarks.", "published": "2025-01-23 13:31:51", "link": "http://arxiv.org/abs/2501.13652v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Collective Memory and Narrative Cohesion: A Computational Study of\n  Palestinian Refugee Oral Histories in Lebanon", "abstract": "This study uses the Palestinian Oral History Archive (POHA) to investigate\nhow Palestinian refugee groups in Lebanon sustain a cohesive collective memory\nof the Nakba through shared narratives. Grounded in Halbwachs' theory of group\nmemory, we employ statistical analysis of pairwise similarity of narratives,\nfocusing on the influence of shared gender and location. We use textual\nrepresentation and semantic embeddings of narratives to represent the\ninterviews themselves. Our analysis demonstrates that shared origin is a\npowerful determinant of narrative similarity across thematic keywords,\nlandmarks, and significant figures, as well as in semantic embeddings of the\nnarratives. Meanwhile, shared residence fosters cohesion, with its impact\nsignificantly amplified when paired with shared origin. Additionally, women's\nnarratives exhibit heightened thematic cohesion, particularly in recounting\nexperiences of the British occupation, underscoring the gendered dimensions of\nmemory formation. This research deepens the understanding of collective memory\nin diasporic settings, emphasizing the critical role of oral histories in\nsafeguarding Palestinian identity and resisting erasure.", "published": "2025-01-23 14:07:49", "link": "http://arxiv.org/abs/2501.13682v1", "categories": ["cs.CL", "J.4; I.2.7"], "primary_category": "cs.CL"}
{"title": "RPO: Retrieval Preference Optimization for Robust Retrieval-Augmented\n  Generation", "abstract": "While Retrieval-Augmented Generation (RAG) has exhibited promise in utilizing\nexternal knowledge, its generation process heavily depends on the quality and\naccuracy of the retrieved context. Large language models (LLMs) struggle to\nevaluate the correctness of non-parametric knowledge retrieved externally when\nit differs from internal memorization, leading to knowledge conflicts during\nresponse generation. To this end, we introduce the Retrieval Preference\nOptimization (RPO), a lightweight and effective alignment method to adaptively\nleverage multi-source knowledge based on retrieval relevance. An implicit\nrepresentation of retrieval relevance is derived and incorporated into the\nreward model to integrate retrieval evaluation and response generation into a\nsingle model, solving the problem that previous methods necessitate the\nadditional procedure to assess the retrieval quality. Notably, RPO is the only\nRAG-dedicated alignment approach that quantifies the awareness of retrieval\nrelevance in training, overcoming mathematical obstacles. Experiments on four\ndatasets demonstrate that RPO outperforms RAG by 4-10% in accuracy without any\nextra component, exhibiting its robust generalization.", "published": "2025-01-23 14:58:56", "link": "http://arxiv.org/abs/2501.13726v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Study of the Plausibility of Attention between RNN Encoders in Natural\n  Language Inference", "abstract": "Attention maps in neural models for NLP are appealing to explain the decision\nmade by a model, hopefully emphasizing words that justify the decision. While\nmany empirical studies hint that attention maps can provide such justification\nfrom the analysis of sound examples, only a few assess the plausibility of\nexplanations based on attention maps, i.e., the usefulness of attention maps\nfor humans to understand the decision. These studies furthermore focus on text\nclassification. In this paper, we report on a preliminary assessment of\nattention maps in a sentence comparison task, namely natural language\ninference. We compare the cross-attention weights between two RNN encoders with\nhuman-based and heuristic-based annotations on the eSNLI corpus. We show that\nthe heuristic reasonably correlates with human annotations and can thus\nfacilitate evaluation of plausible explanations in sentence comparison tasks.\nRaw attention weights however remain only loosely related to a plausible\nexplanation.", "published": "2025-01-23 15:11:27", "link": "http://arxiv.org/abs/2501.13735v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Large Language Models Truly Understand Geometric Structures?", "abstract": "Geometric ability is a significant challenge for large language models (LLMs)\ndue to the need for advanced spatial comprehension and abstract thinking.\nExisting datasets primarily evaluate LLMs on their final answers, but they\ncannot truly measure their true understanding of geometric structures, as LLMs\ncan arrive at correct answers by coincidence. To fill this gap, we introduce\nthe GeomRel dataset, designed to evaluate LLMs' understanding of geometric\nstructures by isolating the core step of geometric relationship identification\nin problem-solving. Using this benchmark, we conduct thorough evaluations of\ndiverse LLMs and identify key limitations in understanding geometric\nstructures. We further propose the Geometry Chain-of-Thought (GeoCoT) method,\nwhich enhances LLMs' ability to identify geometric relationships, resulting in\nsignificant performance improvements.", "published": "2025-01-23 15:52:34", "link": "http://arxiv.org/abs/2501.13773v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A RAG-Based Institutional Assistant", "abstract": "Although large language models (LLMs) demonstrate strong text generation\ncapabilities, they struggle in scenarios requiring access to structured\nknowledge bases or specific documents, limiting their effectiveness in\nknowledge-intensive tasks. To address this limitation, retrieval-augmented\ngeneration (RAG) models have been developed, enabling generative models to\nincorporate relevant document fragments into their inputs. In this paper, we\ndesign and evaluate a RAG-based virtual assistant specifically tailored for the\nUniversity of S\\~ao Paulo. Our system architecture comprises two key modules: a\nretriever and a generative model. We experiment with different types of models\nfor both components, adjusting hyperparameters such as chunk size and the\nnumber of retrieved documents. Our optimal retriever model achieves a Top-5\naccuracy of 30%, while our most effective generative model scores 22.04\\%\nagainst ground truth answers. Notably, when the correct document chunks are\nsupplied to the LLMs, accuracy significantly improves to 54.02%, an increase of\nover 30 percentage points. Conversely, without contextual input, performance\ndeclines to 13.68%. These findings highlight the critical role of database\naccess in enhancing LLM performance. They also reveal the limitations of\ncurrent semantic search methods in accurately identifying relevant documents\nand underscore the ongoing challenges LLMs face in generating precise\nresponses.", "published": "2025-01-23 17:54:19", "link": "http://arxiv.org/abs/2501.13880v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analysis of Indic Language Capabilities in LLMs", "abstract": "This report evaluates the performance of text-in text-out Large Language\nModels (LLMs) to understand and generate Indic languages. This evaluation is\nused to identify and prioritize Indic languages suited for inclusion in safety\nbenchmarks. We conduct this study by reviewing existing evaluation studies and\ndatasets; and a set of twenty-eight LLMs that support Indic languages. We\nanalyze the LLMs on the basis of the training data, license for model and data,\ntype of access and model developers. We also compare Indic language performance\nacross evaluation datasets and find that significant performance disparities in\nperformance across Indic languages. Hindi is the most widely represented\nlanguage in models. While model performance roughly correlates with number of\nspeakers for the top five languages, the assessment after that varies.", "published": "2025-01-23 18:49:33", "link": "http://arxiv.org/abs/2501.13912v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Breeze 2 Herd of Models: Traditional Chinese LLMs Based on Llama\n  with Vision-Aware and Function-Calling Capabilities", "abstract": "Llama-Breeze2 (hereinafter referred to as Breeze2) is a suite of advanced\nmulti-modal language models, available in 3B and 8B parameter configurations,\nspecifically designed to enhance Traditional Chinese language representation.\nBuilding upon the Llama 3.2 model family, we continue the pre-training of\nBreeze2 on an extensive corpus to enhance the linguistic and cultural heritage\nof Traditional Chinese. In addition to language modeling capabilities, we\nsignificantly augment the models with function calling and vision understanding\ncapabilities. At the time of this publication, as far as we are aware, absent\nreasoning-inducing prompts, Breeze2 are the strongest performing models in\nTraditional Chinese function calling and image understanding in its size class.\nThe effectiveness of Breeze2 is benchmarked across various tasks, including\nTaiwan general knowledge, instruction-following, long context, function\ncalling, and vision understanding. We are publicly releasing all Breeze2 models\nunder the Llama 3.2 Community License. We also showcase the capabilities of the\nmodel running on mobile platform with a mobile application which we also open\nsource.", "published": "2025-01-23 18:59:02", "link": "http://arxiv.org/abs/2501.13921v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Large Language Models to Analyze Emotional and Contextual\n  Drivers of Teen Substance Use in Online Discussions", "abstract": "Adolescence is a critical stage often linked to risky behaviors, including\nsubstance use, with significant developmental and public health implications.\nSocial media provides a lens into adolescent self-expression, but interpreting\nemotional and contextual signals remains complex. This study applies Large\nLanguage Models (LLMs) to analyze adolescents' social media posts, uncovering\nemotional patterns (e.g., sadness, guilt, fear, joy) and contextual factors\n(e.g., family, peers, school) related to substance use. Heatmap and machine\nlearning analyses identified key predictors of substance use-related posts.\nNegative emotions like sadness and guilt were significantly more frequent in\nsubstance use contexts, with guilt acting as a protective factor, while shame\nand peer influence heightened substance use risk. Joy was more common in\nnon-substance use discussions. Peer influence correlated strongly with sadness,\nfear, and disgust, while family and school environments aligned with\nnon-substance use. Findings underscore the importance of addressing emotional\nvulnerabilities and contextual influences, suggesting that collaborative\ninterventions involving families, schools, and communities can reduce risk\nfactors and foster healthier adolescent development.", "published": "2025-01-23 19:06:26", "link": "http://arxiv.org/abs/2501.14037v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMs are Vulnerable to Malicious Prompts Disguised as Scientific\n  Language", "abstract": "As large language models (LLMs) have been deployed in various real-world\nsettings, concerns about the harm they may propagate have grown. Various\njailbreaking techniques have been developed to expose the vulnerabilities of\nthese models and improve their safety. This work reveals that many\nstate-of-the-art LLMs are vulnerable to malicious requests hidden behind\nscientific language. Specifically, our experiments with GPT4o, GPT4o-mini,\nGPT-4, LLama3-405B-Instruct, Llama3-70B-Instruct, Cohere, Gemini models\ndemonstrate that, the models' biases and toxicity substantially increase when\nprompted with requests that deliberately misinterpret social science and\npsychological studies as evidence supporting the benefits of stereotypical\nbiases. Alarmingly, these models can also be manipulated to generate fabricated\nscientific arguments claiming that biases are beneficial, which can be used by\nill-intended actors to systematically jailbreak these strong LLMs. Our analysis\nstudies various factors that contribute to the models' vulnerabilities to\nmalicious requests in academic language. Mentioning author names and venues\nenhances the persuasiveness of models, and the bias scores increase as\ndialogues progress. Our findings call for a more careful investigation on the\nuse of scientific data for training LLMs.", "published": "2025-01-23 20:20:20", "link": "http://arxiv.org/abs/2501.14073v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Biomedical Relation Extraction with Directionality", "abstract": "Biological relation networks contain rich information for understanding the\nbiological mechanisms behind the relationship of entities such as genes,\nproteins, diseases, and chemicals. The vast growth of biomedical literature\nposes significant challenges updating the network knowledge. The recent\nBiomedical Relation Extraction Dataset (BioRED) provides valuable manual\nannotations, facilitating the develop-ment of machine-learning and pre-trained\nlanguage model approaches for automatically identifying novel document-level\n(inter-sentence context) relationships. Nonetheless, its annotations lack\ndirectionality (subject/object) for the entity roles, essential for studying\ncomplex biological networks. Herein we annotate the entity roles of the\nrelationships in the BioRED corpus and subsequently propose a novel multi-task\nlanguage model with soft-prompt learning to jointly identify the relationship,\nnovel findings, and entity roles. Our results in-clude an enriched BioRED\ncorpus with 10,864 directionality annotations. Moreover, our proposed method\noutperforms existing large language models such as the state-of-the-art GPT-4\nand Llama-3 on two benchmarking tasks. Our source code and dataset are\navailable at https://github.com/ncbi-nlp/BioREDirect.", "published": "2025-01-23 20:36:11", "link": "http://arxiv.org/abs/2501.14079v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoPERLex: Content Planning with Event-based Representations for Legal\n  Case Summarization", "abstract": "Legal professionals often struggle with lengthy judgments and require\nefficient summarization for quick comprehension. To address this challenge, we\ninvestigate the need for structured planning in legal case summarization,\nparticularly through event-centric representations that reflect the narrative\nnature of legal case documents. We propose our framework, CoPERLex, which\noperates in three stages: first, it performs content selection to identify\ncrucial information from the judgment; second, the selected content is utilized\nto generate intermediate plans through event-centric representations modeled as\nSubject-Verb-Object tuples; and finally, it generates coherent summaries based\non both the content and the structured plan. Our experiments on four legal\nsummarization datasets demonstrate the effectiveness of integrating content\nselection and planning components, highlighting the advantages of event-centric\nplans over traditional entity-centric approaches in the context of legal\njudgements.", "published": "2025-01-23 22:03:45", "link": "http://arxiv.org/abs/2501.14112v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RELexED: Retrieval-Enhanced Legal Summarization with Exemplar Diversity", "abstract": "This paper addresses the task of legal summarization, which involves\ndistilling complex legal documents into concise, coherent summaries. Current\napproaches often struggle with content theme deviation and inconsistent writing\nstyles due to their reliance solely on source documents. We propose RELexED, a\nretrieval-augmented framework that utilizes exemplar summaries along with the\nsource document to guide the model. RELexED employs a two-stage exemplar\nselection strategy, leveraging a determinantal point process to balance the\ntrade-off between similarity of exemplars to the query and diversity among\nexemplars, with scores computed via influence functions. Experimental results\non two legal summarization datasets demonstrate that RELexED significantly\noutperforms models that do not utilize exemplars and those that rely solely on\nsimilarity-based exemplar selection.", "published": "2025-01-23 22:05:03", "link": "http://arxiv.org/abs/2501.14113v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LeCoPCR: Legal Concept-guided Prior Case Retrieval for European Court of\n  Human Rights cases", "abstract": "Prior case retrieval (PCR) is crucial for legal practitioners to find\nrelevant precedent cases given the facts of a query case. Existing approaches\noften overlook the underlying semantic intent in determining relevance with\nrespect to the query case. In this work, we propose LeCoPCR, a novel approach\nthat explicitly generate intents in the form of legal concepts from a given\nquery case facts and then augments the query with these concepts to enhance\nmodels understanding of semantic intent that dictates relavance. To overcome\nthe unavailability of annotated legal concepts, we employ a weak supervision\napproach to extract key legal concepts from the reasoning section using\nDeterminantal Point Process (DPP) to balance quality and diversity.\nExperimental results on the ECtHR-PCR dataset demonstrate the effectiveness of\nleveraging legal concepts and DPP-based key concept extraction.", "published": "2025-01-23 22:10:00", "link": "http://arxiv.org/abs/2501.14114v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Watching the AI Watchdogs: A Fairness and Robustness Analysis of AI\n  Safety Moderation Classifiers", "abstract": "AI Safety Moderation (ASM) classifiers are designed to moderate content on\nsocial media platforms and to serve as guardrails that prevent Large Language\nModels (LLMs) from being fine-tuned on unsafe inputs. Owing to their potential\nfor disparate impact, it is crucial to ensure that these classifiers: (1) do\nnot unfairly classify content belonging to users from minority groups as unsafe\ncompared to those from majority groups and (2) that their behavior remains\nrobust and consistent across similar inputs. In this work, we thus examine the\nfairness and robustness of four widely-used, closed-source ASM classifiers:\nOpenAI Moderation API, Perspective API, Google Cloud Natural Language (GCNL)\nAPI, and Clarifai API. We assess fairness using metrics such as demographic\nparity and conditional statistical parity, comparing their performance against\nASM models and a fair-only baseline. Additionally, we analyze robustness by\ntesting the classifiers' sensitivity to small and natural input perturbations.\nOur findings reveal potential fairness and robustness gaps, highlighting the\nneed to mitigate these issues in future versions of these models.", "published": "2025-01-23 01:04:00", "link": "http://arxiv.org/abs/2501.13302v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ExLM: Rethinking the Impact of [MASK] Tokens in Masked Language Models", "abstract": "Masked Language Models (MLMs) have achieved remarkable success in many\nself-supervised representation learning tasks. MLMs are trained by randomly\nmasking portions of the input sequences with [MASK] tokens and learning to\nreconstruct the original content based on the remaining context. This paper\nexplores the impact of [MASK] tokens on MLMs. Analytical studies show that\nmasking tokens can introduce the corrupted semantics problem, wherein the\ncorrupted context may convey multiple, ambiguous meanings. This problem is also\na key factor affecting the performance of MLMs on downstream tasks. Based on\nthese findings, we propose a novel enhanced-context MLM, ExLM. Our approach\nexpands [MASK] tokens in the input context and models the dependencies between\nthese expanded states. This enhancement increases context capacity and enables\nthe model to capture richer semantic information, effectively mitigating the\ncorrupted semantics problem during pre-training. Experimental results\ndemonstrate that ExLM achieves significant performance improvements in both\ntext modeling and SMILES modeling tasks. Further analysis confirms that ExLM\nenriches semantic representations through context enhancement, and effectively\nreduces the semantic multimodality commonly observed in MLMs.", "published": "2025-01-23 05:46:50", "link": "http://arxiv.org/abs/2501.13397v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RECALL: Library-Like Behavior In Language Models is Enhanced by\n  Self-Referencing Causal Cycles", "abstract": "We introduce the concept of the self-referencing causal cycle (abbreviated\nRECALL) - a mechanism that enables large language models (LLMs) to bypass the\nlimitations of unidirectional causality, which underlies a phenomenon known as\nthe reversal curse. When an LLM is prompted with sequential data, it often\nfails to recall preceding context. For example, when we ask an LLM to recall\nthe line preceding \"O say does that star-spangled banner yet wave\" in the U.S.\nNational Anthem, it often fails to correctly return \"Gave proof through the\nnight that our flag was still there\" - this is due to the reversal curse. It\noccurs because language models such as ChatGPT and Llama generate text based on\npreceding tokens, requiring facts to be learned and reproduced in a consistent\ntoken order. While the reversal curse is often viewed as a limitation, we offer\nevidence of an alternative view: it is not always an obstacle in practice. We\nfind that RECALL is driven by what we designate as cycle tokens - sequences\nthat connect different parts of the training data, enabling recall of preceding\ntokens from succeeding ones. Through rigorous probabilistic formalization and\ncontrolled experiments, we demonstrate how the cycles they induce influence a\nmodel's ability to reproduce information. To facilitate reproducibility, we\nprovide our code and experimental details at\nhttps://anonymous.4open.science/r/remember-B0B8/.", "published": "2025-01-23 09:14:07", "link": "http://arxiv.org/abs/2501.13491v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ReasVQA: Advancing VideoQA with Imperfect Reasoning Process", "abstract": "Video Question Answering (VideoQA) is a challenging task that requires\nunderstanding complex visual and temporal relationships within videos to answer\nquestions accurately. In this work, we introduce \\textbf{ReasVQA}\n(Reasoning-enhanced Video Question Answering), a novel approach that leverages\nreasoning processes generated by Multimodal Large Language Models (MLLMs) to\nimprove the performance of VideoQA models. Our approach consists of three\nphases: reasoning generation, reasoning refinement, and learning from\nreasoning. First, we generate detailed reasoning processes using additional\nMLLMs, and second refine them via a filtering step to ensure data quality.\nFinally, we use the reasoning data, which might be in an imperfect form, to\nguide the VideoQA model via multi-task learning, on how to interpret and answer\nquestions based on a given video. We evaluate ReasVQA on three popular\nbenchmarks, and our results establish new state-of-the-art performance with\nsignificant improvements of +2.9 on NExT-QA, +7.3 on STAR, and +5.9 on\nIntentQA. Our findings demonstrate the supervising benefits of integrating\nreasoning processes into VideoQA. Further studies validate each component of\nour method, also with different backbones and MLLMs, and again highlight the\nadvantages of this simple but effective method. We offer a new perspective on\nenhancing VideoQA performance by utilizing advanced reasoning techniques,\nsetting a new benchmark in this research field.", "published": "2025-01-23 10:35:22", "link": "http://arxiv.org/abs/2501.13536v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "LLMs Can Plan Only If We Tell Them", "abstract": "Large language models (LLMs) have demonstrated significant capabilities in\nnatural language processing and reasoning, yet their effectiveness in\nautonomous planning has been under debate. While existing studies have utilized\nLLMs with external feedback mechanisms or in controlled environments for\nplanning, these approaches often involve substantial computational and\ndevelopment resources due to the requirement for careful design and iterative\nbackprompting. Moreover, even the most advanced LLMs like GPT-4 struggle to\nmatch human performance on standard planning benchmarks, such as the\nBlocksworld, without additional support. This paper investigates whether LLMs\ncan independently generate long-horizon plans that rival human baselines. Our\nnovel enhancements to Algorithm-of-Thoughts (AoT), which we dub AoT+, help\nachieve state-of-the-art results in planning benchmarks out-competing prior\nmethods and human baselines all autonomously.", "published": "2025-01-23 10:46:14", "link": "http://arxiv.org/abs/2501.13545v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "K-COMP: Retrieval-Augmented Medical Domain Question Answering With\n  Knowledge-Injected Compressor", "abstract": "Retrieval-augmented question answering (QA) integrates external information\nand thereby increases the QA accuracy of reader models that lack domain\nknowledge. However, documents retrieved for closed domains require high\nexpertise, so the reader model may have difficulty fully comprehending the\ntext. Moreover, the retrieved documents contain thousands of tokens, some\nunrelated to the question. As a result, the documents include some inaccurate\ninformation, which could lead the reader model to mistrust the passages and\ncould result in hallucinations. To solve these problems, we propose K-comp\n(Knowledge-injected compressor) which provides the knowledge required to answer\ncorrectly. The compressor automatically generates the prior knowledge necessary\nto facilitate the answer process prior to compression of the retrieved\npassages. Subsequently, the passages are compressed autoregressively, with the\ngenerated knowledge being integrated into the compression process. This process\nensures alignment between the question intent and the compressed context. By\naugmenting this prior knowledge and concise context, the reader models are\nguided toward relevant answers and trust the context.", "published": "2025-01-23 11:14:21", "link": "http://arxiv.org/abs/2501.13567v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How to Alleviate Catastrophic Forgetting in LLMs Finetuning?\n  Hierarchical Layer-Wise and Element-Wise Regularization", "abstract": "Large Language Models (LLMs) exhibit strong general language capabilities.\nHowever, fine-tuning these models on domain-specific tasks often leads to\ncatastrophic forgetting, where the model overwrites or loses essential\nknowledge acquired during pretraining. This phenomenon significantly limits the\nbroader applicability of LLMs. To address this challenge, we propose a novel\napproach to compute the element-wise importance of model parameters crucial for\npreserving general knowledge during fine-tuning. Our method utilizes a\ndual-objective optimization strategy: (1) regularization loss based on\nelement-wise parameter importance, which constrains the updates to parameters\ncrucial for general knowledge; (2) cross-entropy loss to adapt to\ndomain-specific tasks. Additionally, we introduce layer-wise coefficients to\naccount for the varying contributions of different layers, dynamically\nbalancing the dual-objective optimization. Extensive experiments on scientific,\nmedical, and physical tasks using GPT-J and LLaMA-3 demonstrate that our\napproach mitigates catastrophic forgetting while enhancing model adaptability.\nCompared to previous methods, our solution is approximately 20 times faster and\nrequires only 10-15% of the storage, highlighting the practical efficiency. The\ncode will be released.", "published": "2025-01-23 13:54:53", "link": "http://arxiv.org/abs/2501.13669v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Question Answering on Patient Medical Records with Private Fine-Tuned\n  LLMs", "abstract": "Healthcare systems continuously generate vast amounts of electronic health\nrecords (EHRs), commonly stored in the Fast Healthcare Interoperability\nResources (FHIR) standard. Despite the wealth of information in these records,\ntheir complexity and volume make it difficult for users to retrieve and\ninterpret crucial health insights. Recent advances in Large Language Models\n(LLMs) offer a solution, enabling semantic question answering (QA) over medical\ndata, allowing users to interact with their health records more effectively.\nHowever, ensuring privacy and compliance requires edge and private deployments\nof LLMs.\n  This paper proposes a novel approach to semantic QA over EHRs by first\nidentifying the most relevant FHIR resources for a user query (Task1) and\nsubsequently answering the query based on these resources (Task2). We explore\nthe performance of privately hosted, fine-tuned LLMs, evaluating them against\nbenchmark models such as GPT-4 and GPT-4o. Our results demonstrate that\nfine-tuned LLMs, while 250x smaller in size, outperform GPT-4 family models by\n0.55% in F1 score on Task1 and 42% on Meteor Task in Task2. Additionally, we\nexamine advanced aspects of LLM usage, including sequential fine-tuning, model\nself-evaluation (narcissistic evaluation), and the impact of training data size\non performance. The models and datasets are available here:\nhttps://huggingface.co/genloop", "published": "2025-01-23 14:13:56", "link": "http://arxiv.org/abs/2501.13687v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DI-BENCH: Benchmarking Large Language Models on Dependency Inference\n  with Testable Repositories at Scale", "abstract": "Large Language Models have advanced automated software development, however,\nit remains a challenge to correctly infer dependencies, namely, identifying the\ninternal components and external packages required for a repository to\nsuccessfully run. Existing studies highlight that dependency-related issues\ncause over 40\\% of observed runtime errors on the generated repository. To\naddress this, we introduce DI-BENCH, a large-scale benchmark and evaluation\nframework specifically designed to assess LLMs' capability on dependency\ninference. The benchmark features 581 repositories with testing environments\nacross Python, C#, Rust, and JavaScript. Extensive experiments with textual and\nexecution-based metrics reveal that the current best-performing model achieves\nonly a 42.9% execution pass rate, indicating significant room for improvement.\nDI-BENCH establishes a new viewpoint for evaluating LLM performance on\nrepositories, paving the way for more robust end-to-end software synthesis.", "published": "2025-01-23 14:27:11", "link": "http://arxiv.org/abs/2501.13699v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Pseudocode-Injection Magic: Enabling LLMs to Tackle Graph Computational\n  Tasks", "abstract": "Graph computational tasks are inherently challenging and often demand the\ndevelopment of advanced algorithms for effective solutions. With the emergence\nof large language models (LLMs), researchers have begun investigating their\npotential to address these tasks. However, existing approaches are constrained\nby LLMs' limited capability to comprehend complex graph structures and their\nhigh inference costs, rendering them impractical for handling large-scale\ngraphs. Inspired by human approaches to graph problems, we introduce a novel\nframework, PIE (Pseudocode-Injection-Enhanced LLM Reasoning for Graph\nComputational Tasks), which consists of three key steps: problem understanding,\nprompt design, and code generation. In this framework, LLMs are tasked with\nunderstanding the problem and extracting relevant information to generate\ncorrect code. The responsibility for analyzing the graph structure and\nexecuting the code is delegated to the interpreter. We inject task-related\npseudocodes into the prompts to further assist the LLMs in generating efficient\ncode. We also employ cost-effective trial-and-error techniques to ensure that\nthe LLM-generated code executes correctly. Unlike other methods that require\ninvoking LLMs for each individual test case, PIE only calls the LLM during the\ncode generation phase, allowing the generated code to be reused and\nsignificantly reducing inference costs. Extensive experiments demonstrate that\nPIE outperforms existing baselines in terms of both accuracy and computational\nefficiency.", "published": "2025-01-23 15:04:22", "link": "http://arxiv.org/abs/2501.13731v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "UGMathBench: A Diverse and Dynamic Benchmark for Undergraduate-Level\n  Mathematical Reasoning with Large Language Models", "abstract": "Large Language Models (LLMs) have made significant strides in mathematical\nreasoning, underscoring the need for a comprehensive and fair evaluation of\ntheir capabilities. However, existing benchmarks often fall short, either\nlacking extensive coverage of undergraduate-level mathematical problems or\nprobably suffering from test-set contamination. To address these issues, we\nintroduce UGMathBench, a diverse and dynamic benchmark specifically designed\nfor evaluating undergraduate-level mathematical reasoning with LLMs.\nUGMathBench comprises 5,062 problems across 16 subjects and 111 topics,\nfeaturing 10 distinct answer types. Each problem includes three randomized\nversions, with additional versions planned for release as leading open-source\nLLMs become saturated in UGMathBench. Furthermore, we propose two key metrics:\neffective accuracy (EAcc), which measures the percentage of correctly solved\nproblems across all three versions, and reasoning gap ($\\Delta$), which\nassesses reasoning robustness by calculating the difference between the average\naccuracy across all versions and EAcc. Our extensive evaluation of 23 leading\nLLMs reveals that the highest EAcc achieved is 56.3\\% by OpenAI-o1-mini, with\nlarge $\\Delta$ values observed across different models. This highlights the\nneed for future research aimed at developing \"large reasoning models\" with high\nEAcc and $\\Delta = 0$. We anticipate that the release of UGMathBench, along\nwith its detailed evaluation codes, will serve as a valuable resource to\nadvance the development of LLMs in solving mathematical problems. Codes and\ndata are available at https://github.com/YangLabHKUST/UGMathBench", "published": "2025-01-23 15:46:43", "link": "http://arxiv.org/abs/2501.13766v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Explainable XR: Understanding User Behaviors of XR Environments using\n  LLM-assisted Analytics Framework", "abstract": "We present Explainable XR, an end-to-end framework for analyzing user\nbehavior in diverse eXtended Reality (XR) environments by leveraging Large\nLanguage Models (LLMs) for data interpretation assistance. Existing XR user\nanalytics frameworks face challenges in handling cross-virtuality - AR, VR, MR\n- transitions, multi-user collaborative application scenarios, and the\ncomplexity of multimodal data. Explainable XR addresses these challenges by\nproviding a virtuality-agnostic solution for the collection, analysis, and\nvisualization of immersive sessions. We propose three main components in our\nframework: (1) A novel user data recording schema, called User Action\nDescriptor (UAD), that can capture the users' multimodal actions, along with\ntheir intents and the contexts; (2) a platform-agnostic XR session recorder,\nand (3) a visual analytics interface that offers LLM-assisted insights tailored\nto the analysts' perspectives, facilitating the exploration and analysis of the\nrecorded XR session data. We demonstrate the versatility of Explainable XR by\ndemonstrating five use-case scenarios, in both individual and collaborative XR\napplications across virtualities. Our technical evaluation and user studies\nshow that Explainable XR provides a highly usable analytics solution for\nunderstanding user actions and delivering multifaceted, actionable insights\ninto user behaviors in immersive environments.", "published": "2025-01-23 15:55:07", "link": "http://arxiv.org/abs/2501.13778v2", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Generation of reusable learning objects from digital medical\n  collections: An analysis based on the MASMDOA framework", "abstract": "Learning Objects represent a widespread approach to structuring instructional\nmaterials in a large variety of educational contexts. The main aim of this work\nconsists of analyzing from a qualitative point of view the process of\ngenerating reusable learning objects (RLOs) followed by Clavy, a tool that can\nbe used to retrieve data from multiple medical knowledge sources and\nreconfigure such sources in diverse multimedia-based structures and\norganizations. From these organizations, Clavy is able to generate learning\nobjects which can be adapted to various instructional healthcare scenarios with\nseveral types of user profiles and distinct learning requirements. Moreover,\nClavy provides the capability of exporting these learning objects through\neducational standard specifications, which improves their reusability features.\nThe analysis insights highlight the importance of having a tool able to\ntransfer knowledge from the available digital medical collections to learning\nobjects that can be easily accessed by medical students and healthcare\npractitioners through the most popular e-learning platforms.", "published": "2025-01-23 16:27:15", "link": "http://arxiv.org/abs/2501.13806v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Hallucinations Can Improve Large Language Models in Drug Discovery", "abstract": "Concerns about hallucinations in Large Language Models (LLMs) have been\nraised by researchers, yet their potential in areas where creativity is vital,\nsuch as drug discovery, merits exploration. In this paper, we come up with the\nhypothesis that hallucinations can improve LLMs in drug discovery. To verify\nthis hypothesis, we use LLMs to describe the SMILES string of molecules in\nnatural language and then incorporate these descriptions as part of the prompt\nto address specific tasks in drug discovery. Evaluated on seven LLMs and five\nclassification tasks, our findings confirm the hypothesis: LLMs can achieve\nbetter performance with text containing hallucinations. Notably, Llama-3.1-8B\nachieves an 18.35% gain in ROC-AUC compared to the baseline without\nhallucination. Furthermore, hallucinations generated by GPT-4o provide the most\nconsistent improvements across models. Additionally, we conduct empirical\nanalyses and a case study to investigate key factors affecting performance and\nthe underlying reasons. Our research sheds light on the potential use of\nhallucinations for LLMs and offers new perspectives for future research\nleveraging LLMs in drug discovery.", "published": "2025-01-23 16:45:51", "link": "http://arxiv.org/abs/2501.13824v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline\n  Professional Videos", "abstract": "Humans acquire knowledge through three cognitive stages: perceiving\ninformation, comprehending knowledge, and adapting knowledge to solve novel\nproblems. Videos serve as an effective medium for this learning process,\nfacilitating a progression through these cognitive stages. However, existing\nvideo benchmarks fail to systematically evaluate the knowledge acquisition\ncapabilities in Large Multimodal Models (LMMs). To address this gap, we\nintroduce Video-MMMU, a multi-modal, multi-disciplinary benchmark designed to\nassess LMMs' ability to acquire and utilize knowledge from videos. Video-MMMU\nfeatures a curated collection of 300 expert-level videos and 900\nhuman-annotated questions across six disciplines, evaluating knowledge\nacquisition through stage-aligned question-answer pairs: Perception,\nComprehension, and Adaptation. A proposed knowledge gain metric,\n{\\Delta}knowledge, quantifies improvement in performance after video viewing.\nEvaluation of LMMs reveals a steep decline in performance as cognitive demands\nincrease and highlights a significant gap between human and model knowledge\nacquisition, underscoring the need for methods to enhance LMMs' capability to\nlearn and adapt from videos.", "published": "2025-01-23 16:51:47", "link": "http://arxiv.org/abs/2501.13826v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Think Outside the Data: Colonial Biases and Systemic Issues in Automated\n  Moderation Pipelines for Low-Resource Languages", "abstract": "Most social media users come from non-English speaking countries in the\nGlobal South. Despite the widespread prevalence of harmful content in these\nregions, current moderation systems repeatedly struggle in low-resource\nlanguages spoken there. In this work, we examine the challenges AI researchers\nand practitioners face when building moderation tools for low-resource\nlanguages. We conducted semi-structured interviews with 22 AI researchers and\npractitioners specializing in automatic detection of harmful content in four\ndiverse low-resource languages from the Global South. These are: Tamil from\nSouth Asia, Swahili from East Africa, Maghrebi Arabic from North Africa, and\nQuechua from South America. Our findings reveal that social media companies'\nrestrictions on researchers' access to data exacerbate the historical\nmarginalization of these languages, which have long lacked datasets for\nstudying online harms. Moreover, common preprocessing techniques and language\nmodels, predominantly designed for data-rich English, fail to account for the\nlinguistic complexity of low-resource languages. This leads to critical errors\nwhen moderating content in Tamil, Swahili, Arabic, and Quechua, which are\nmorphologically richer than English. Based on our findings, we establish that\nthe precarities in current moderation pipelines are rooted in deep systemic\ninequities and continue to reinforce historical power imbalances. We conclude\nby discussing multi-stakeholder approaches to improve moderation for\nlow-resource languages.", "published": "2025-01-23 17:01:53", "link": "http://arxiv.org/abs/2501.13836v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "AdEval: Alignment-based Dynamic Evaluation to Mitigate Data\n  Contamination in Large Language Models", "abstract": "As Large Language Models (LLMs) are pretrained on massive-scale corpora, the\nissue of data contamination has become increasingly severe, leading to\npotential overestimation of model performance during evaluation. To address\nthis, we propose AdEval (Alignment-based Dynamic Evaluation), a dynamic data\nevaluation method aimed at mitigating the impact of data contamination on\nevaluation reliability. Experimental results on multiple datasets demonstrate\nthat AdEval effectively reduces the impact of data contamination on evaluation\noutcomes, enhancing both the fairness and reliability of the evaluation\nprocess.", "published": "2025-01-23 06:57:24", "link": "http://arxiv.org/abs/2501.13983v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Framework for Progressive Knowledge Fusion in Large Language Models\n  Through Structured Conceptual Redundancy Analysis", "abstract": "The organization of latent knowledge within large-scale models poses unique\nchallenges when addressing overlapping representations and optimizing\ncontextual accuracy. Conceptual redundancies embedded across layers often\nresult in inefficiencies that affect both computational demands and\ntask-specific outcomes. A framework was proposed to restructure these\nredundancies through advanced clustering techniques and dynamic thresholding,\nensuring that critical semantic relationships are preserved while removing\nunnecessary overlaps. Evaluations revealed improved memory efficiency and\nfaster inference times, alongside better alignment in latent knowledge clusters\nthat enhanced interpretability. Improvements in error rates and adversarial\nrobustness suggest that restructuring redundancies has broader implications for\nincreasing model reliability across diverse applications. Comparative analyses\nhighlighted reductions in resource consumption and notable gains in\nperformance, particularly in translation and summarization tasks. Energy\nmetrics demonstrated significant savings during training phases, further\nvalidating the practicality of the approach for real-world deployments.\nRepresentational fidelity was also enhanced, with latent space evaluations\nindicating better cluster alignment and higher semantic consistency. The\nmethodology bridges a key gap in model optimization through directly addressing\nredundancies at the structural level. Its application opens avenues for\nscalable, efficient, and contextually aware systems that can adapt to complex,\ndomain-specific tasks without compromising on performance.", "published": "2025-01-23 11:34:04", "link": "http://arxiv.org/abs/2501.13999v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Advancing Mathematical Reasoning in Language Models: The Impact of\n  Problem-Solving Data, Data Synthesis Methods, and Training Stages", "abstract": "Mathematical reasoning remains a challenging area for large language models\n(LLMs), prompting the development of math-specific LLMs such as LLEMMA,\nDeepSeekMath, and Qwen2-Math, among others. These models typically follow a\ntwo-stage training paradigm: pre-training with math-related corpora and\npost-training with problem datasets for supervised fine-tuning (SFT). Despite\nthese efforts, the improvements in mathematical reasoning achieved through\ncontinued pre-training (CPT) are often less significant compared to those\nobtained via SFT. This study addresses this discrepancy by exploring\nalternative strategies during the pre-training phase, focusing on the use of\nproblem-solving data over general mathematical corpora. We investigate three\nprimary research questions: (1) Can problem-solving data enhance the model's\nmathematical reasoning capabilities more effectively than general mathematical\ncorpora during CPT? (2) Are synthetic data from the same source equally\neffective, and which synthesis methods are most efficient? (3) How do the\ncapabilities developed from the same problem-solving data differ between the\nCPT and SFT stages, and what factors contribute to these differences? Our\nfindings indicate that problem-solving data significantly enhances the model's\nmathematical capabilities compared to general mathematical corpora. We also\nidentify effective data synthesis methods, demonstrating that the tutorship\namplification synthesis method achieves the best performance. Furthermore,\nwhile SFT facilitates instruction-following abilities, it underperforms\ncompared to CPT with the same data, which can be partially attributed to its\npoor learning capacity for more challenging problem-solving data. These\ninsights provide valuable guidance for optimizing the mathematical reasoning\ncapabilities of LLMs, culminating in our development of a powerful mathematical\nbase model called MathGPT-8B.", "published": "2025-01-23 12:14:57", "link": "http://arxiv.org/abs/2501.14002v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "QuanTaxo: A Quantum Approach to Self-Supervised Taxonomy Expansion", "abstract": "A taxonomy is a hierarchical graph containing knowledge to provide valuable\ninsights for various web applications. Online retail organizations like\nMicrosoft and Amazon utilize taxonomies to improve product recommendations and\noptimize advertisement by enhancing query interpretation. However, the manual\nconstruction of taxonomies requires significant human effort. As web content\ncontinues to expand at an unprecedented pace, existing taxonomies risk becoming\noutdated, struggling to incorporate new and emerging information effectively.\nAs a consequence, there is a growing need for dynamic taxonomy expansion to\nkeep them relevant and up-to-date. Existing taxonomy expansion methods often\nrely on classical word embeddings to represent entities. However, these\nembeddings fall short in capturing hierarchical polysemy, where an entity's\nmeaning can vary based on its position in the hierarchy and its surrounding\ncontext. To address this challenge, we introduce QuanTaxo, an innovative\nquantum-inspired framework for taxonomy expansion. QuanTaxo encodes entity\nrepresentations in quantum space, effectively modeling hierarchical polysemy by\nleveraging the principles of Hilbert space to capture interference effects\nbetween entities, yielding richer and more nuanced representations.\nComprehensive experiments on four real-world benchmark datasets show that\nQuanTaxo significantly outperforms classical embedding models, achieving\nsubstantial improvements of 18.45% in accuracy, 20.5% in Mean Reciprocal Rank,\nand 17.87% in Wu & Palmer metrics across eight classical embedding-based\nbaselines. We further highlight the superiority of QuanTaxo through extensive\nablation and case studies.", "published": "2025-01-23 18:40:02", "link": "http://arxiv.org/abs/2501.14011v2", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Autonomous Structural Memory Manipulation for Large Language Models\n  Using Hierarchical Embedding Augmentation", "abstract": "Transformative innovations in model architectures have introduced\nhierarchical embedding augmentation as a means to redefine the representation\nof tokens through multi-level semantic structures, offering enhanced\nadaptability to complex linguistic inputs. Autonomous structural memory\nmanipulation further advances this paradigm through dynamic memory reallocation\nmechanisms that prioritize critical contextual features while suppressing less\nrelevant information, enabling scalable and efficient performance across\ndiverse tasks. Experimental results reveal substantial improvements in\ncomputational efficiency, with marked reductions in processing overhead for\nlonger input sequences, achieved through memory reorganization strategies that\nadapt to evolving contextual requirements. Hierarchical embeddings not only\nimproved contextual alignment but also facilitated task generalization by\ncapturing relationships at varying semantic granularities, ensuring coherence\nacross layers without introducing significant computational redundancies.\nComparative analysis against baseline models demonstrated unique advantages in\naccuracy, efficiency, and interpretability, particularly in tasks requiring\ncomplex contextual understanding or domain-specific adaptability. The ability\nto dynamically adjust token representations and memory configurations\ncontributed to the model's robustness under varied and unpredictable input\nconditions. Applications benefiting from these advancements include\nmulti-domain generalization, interactive systems, and scenarios involving\nreal-time decision-making, where traditional static memory architectures often\nface limitations. The proposed methodology combines advanced embedding and\nmemory management strategies into a cohesive framework that addresses\nscalability challenges while preserving task-specific relevance.", "published": "2025-01-23 22:20:36", "link": "http://arxiv.org/abs/2501.14119v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Toyteller: AI-powered Visual Storytelling Through Toy-Playing with\n  Character Symbols", "abstract": "We introduce Toyteller, an AI-powered storytelling system where users\ngenerate a mix of story text and visuals by directly manipulating character\nsymbols like they are toy-playing. Anthropomorphized symbol motions can convey\nrich and nuanced social interactions; Toyteller leverages these motions (1) to\nlet users steer story text generation and (2) as a visual output format that\naccompanies story text. We enabled motion-steered text generation and\ntext-steered motion generation by mapping motions and text onto a shared\nsemantic space so that large language models and motion generation models can\nuse it as a translational layer. Technical evaluations showed that Toyteller\noutperforms a competitive baseline, GPT-4o. Our user study identified that\ntoy-playing helps express intentions difficult to verbalize. However, only\nmotions could not express all user intentions, suggesting combining it with\nother modalities like language. We discuss the design space of toy-playing\ninteractions and implications for technical HCI research on human-AI\ninteraction.", "published": "2025-01-23 00:20:38", "link": "http://arxiv.org/abs/2501.13284v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "RAMQA: A Unified Framework for Retrieval-Augmented Multi-Modal Question\n  Answering", "abstract": "Multi-modal retrieval-augmented Question Answering (MRAQA), integrating text\nand images, has gained significant attention in information retrieval (IR) and\nnatural language processing (NLP). Traditional ranking methods rely on small\nencoder-based language models, which are incompatible with modern decoder-based\ngenerative large language models (LLMs) that have advanced various NLP tasks.\nTo bridge this gap, we propose RAMQA, a unified framework combining\nlearning-to-rank methods with generative permutation-enhanced ranking\ntechniques. We first train a pointwise multi-modal ranker using LLaVA as the\nbackbone. Then, we apply instruction tuning to train a LLaMA model for\nre-ranking the top-k documents using an innovative autoregressive multi-task\nlearning approach. Our generative ranking model generates re-ranked document\nIDs and specific answers from document candidates in various permutations.\nExperiments on two MRAQA benchmarks, WebQA and MultiModalQA, show significant\nimprovements over strong baselines, highlighting the effectiveness of our\napproach. Code and data are available at: https://github.com/TonyBY/RAMQA", "published": "2025-01-23 00:50:33", "link": "http://arxiv.org/abs/2501.13297v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "OSUM: Advancing Open Speech Understanding Models with Limited Resources\n  in Academia", "abstract": "Large Language Models (LLMs) have made significant progress in various\ndownstream tasks, inspiring the development of Speech Understanding Language\nModels (SULMs) to enable comprehensive speech-based interactions. However, most\nadvanced SULMs are developed by the industry, leveraging large-scale datasets\nand computational resources that are not readily available to the academic\ncommunity. Moreover, the lack of transparency in training details creates\nadditional barriers to further innovation. In this study, we present OSUM, an\nOpen Speech Understanding Model designed to explore the potential of training\nSLUMs under constrained academic resources. The OSUM model combines a Whisper\nencoder with a Qwen2 LLM and supports a wide range of speech tasks, including\nspeech recognition (ASR), speech recognition with timestamps (SRWT), vocal\nevent detection (VED), speech emotion recognition (SER), speaking style\nrecognition (SSR), speaker gender classification (SGC), speaker age prediction\n(SAP), and speech-to-text chat (STTC). By employing an ASR+X training strategy,\nOSUM achieves efficient and stable multi-task training by simultaneously\noptimizing ASR alongside target tasks. Beyond delivering strong performance,\nOSUM emphasizes transparency by providing openly available data preparation and\ntraining methodologies, offering valuable insights and practical guidance for\nthe academic community. By doing so, we aim to accelerate research and\ninnovation in advanced SULM technologies.", "published": "2025-01-23 01:27:46", "link": "http://arxiv.org/abs/2501.13306v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AgentRec: Agent Recommendation Using Sentence Embeddings Aligned to\n  Human Feedback", "abstract": "Multi-agent systems must decide which agent is the most appropriate for a\ngiven task. We propose a novel architecture for recommending which LLM agent\nout of many should perform a task given a natural language prompt by extending\nthe Sentence-BERT (SBERT) encoder model. On test data, we are able to achieve a\ntop-1 accuracy of 92.2% with each classification taking less than 300\nmilliseconds. In contrast to traditional classification methods, our\narchitecture is computationally cheap, adaptive to new classes, interpretable,\nand controllable with arbitrary metrics through reinforcement learning. By\nencoding natural language prompts into sentence embeddings, our model captures\nthe semantic content relevant to recommending an agent. The distance between\nsentence embeddings that belong to the same agent is then minimized through\nfine-tuning and aligned to human values through reinforcement learning from\nhuman feedback. This allows the classification of natural language prompts\nbased on their nearest neighbors by measuring the cosine similarity between\nembeddings. This work is made possible through the generation of a synthetic\ndataset for agent recommendation, which we have open-sourced to the public\nalong with the code for AgentRec recommendation system at\nhttps://github.com/joshprk/agentrec.", "published": "2025-01-23 02:25:44", "link": "http://arxiv.org/abs/2501.13333v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.LG"}
{"title": "Softplus Attention with Re-weighting Boosts Length Extrapolation in\n  Large Language Models", "abstract": "Large language models have achieved remarkable success in recent years,\nprimarily due to the implementation of self-attention mechanisms. However,\ntraditional Softmax attention suffers from numerical instability and reduced\nperformance as the length of inference tokens increases. This paper addresses\nthese issues by decomposing the Softmax operation into a non-linear\ntransformation and the $l_1$-norm. We identify the latter as essential for\nmaintaining model performance. By replacing the non-linear transformation with\nthe Softplus activation function and introducing a dynamic scale factor for\ndifferent token lengths based on invariance entropy, we create a novel\nattention mechanism with performance better than conventional Softmax attention\nacross various inference lengths. To further improve the length extrapolation\nability of the proposed attention mechanism, we introduce a fine-tuning-free\nre-weighting mechanism that amplifies significant attention weights while\ndiminishing weaker ones, enabling the model to concentrate more effectively on\nrelevant tokens without requiring retraining. When combined with our proposed\nattention mechanism, this approach demonstrates significant promise in managing\nlonger sequences, maintaining nearly constant validation loss even at\n16$\\times$ the training token length while ensuring numerical stability. Our\ncode is available at: https://github.com/iminfine/freeatten.", "published": "2025-01-23 07:21:08", "link": "http://arxiv.org/abs/2501.13428v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MambaQuant: Quantizing the Mamba Family with Variance Aligned Rotation\n  Methods", "abstract": "Mamba is an efficient sequence model that rivals Transformers and\ndemonstrates significant potential as a foundational architecture for various\ntasks. Quantization is commonly used in neural networks to reduce model size\nand computational latency. However, applying quantization to Mamba remains\nunderexplored, and existing quantization methods, which have been effective for\nCNN and Transformer models, appear inadequate for Mamba models (e.g., Quarot\nsuffers a 21% accuracy drop on Vim-T$^\\dagger$ even under W8A8). We have\npioneered the exploration of this issue and identified several key challenges.\nFirst, significant outliers are present in gate projections, output\nprojections, and matrix multiplications. Second, Mamba's unique parallel scan\nfurther amplifies these outliers, leading to uneven and heavy-tailed data\ndistributions. Third, even with the application of the Hadamard transform, the\nvariance across channels in weights and activations still remains inconsistent.\nTo these ends, we propose MambaQuant, a post-training quantization (PTQ)\nframework consisting of: 1) Karhunen-Loeve Transformation (KLT) enhanced\nrotation, rendering the rotation matrix adaptable to diverse channel\ndistributions. 2) Smooth-Fused rotation, which equalizes channel variances and\ncan merge additional parameters into model weights. Experiments show that\nMambaQuant can quantize both weights and activations into 8-bit with less than\n1% accuracy loss for Mamba-based vision and language tasks. To the best of our\nknowledge, MambaQuant is the first comprehensive PTQ design for the Mamba\nfamily, paving the way for further advancements in its application.", "published": "2025-01-23 08:57:33", "link": "http://arxiv.org/abs/2501.13484v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "DQ-Data2vec: Decoupling Quantization for Multilingual Speech Recognition", "abstract": "Data2vec is a self-supervised learning (SSL) approach that employs a\nteacher-student architecture for contextual representation learning via masked\nprediction, demonstrating remarkable performance in monolingual ASR. Previous\nstudies have revealed that data2vec's shallow layers capture speaker and\nlanguage information, middle layers encode phoneme and word features, while\ndeep layers are responsible for reconstruction. Language and phoneme features\nare crucial for multilingual ASR. However, data2vec's masked representation\ngeneration relies on multi-layer averaging, inevitably coupling these features.\nTo address this limitation, we propose a decoupling quantization based data2vec\n(DQ-Data2vec) for multilingual ASR, which includes a data2vec backbone and two\nimproved online K-means quantizers. Our core idea is using the K-means\nquantizer with specified cluster numbers to decouple language and phoneme\ninformation for masked prediction. Specifically, in the language quantization,\nconsidering that the number of languages is significantly different from other\nirrelevant features (e.g., speakers), we assign the cluster number to match the\nnumber of languages, explicitly decoupling shallow layers' language-related\ninformation from irrelevant features. This strategy is also applied to\ndecoupling middle layers' phoneme and word features. In a self-supervised\nscenario, experiments on the CommonVoice dataset demonstrate that DQ-Data2vec\nachieves a relative reduction of 9.51% in phoneme error rate (PER) and 11.58%\nin word error rate (WER) compared to data2vec and UniData2vec. Moreover, in a\nweakly-supervised scenario incorporating language labels and high-resource\nlanguage text labels, the relative reduction is 18.09% and 1.55%, respectively.", "published": "2025-01-23 09:25:31", "link": "http://arxiv.org/abs/2501.13497v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Certified Robustness Under Bounded Levenshtein Distance", "abstract": "Text classifiers suffer from small perturbations, that if chosen\nadversarially, can dramatically change the output of the model. Verification\nmethods can provide robustness certificates against such adversarial\nperturbations, by computing a sound lower bound on the robust accuracy.\nNevertheless, existing verification methods incur in prohibitive costs and\ncannot practically handle Levenshtein distance constraints. We propose the\nfirst method for computing the Lipschitz constant of convolutional classifiers\nwith respect to the Levenshtein distance. We use these Lipschitz constant\nestimates for training 1-Lipschitz classifiers. This enables computing the\ncertified radius of a classifier in a single forward pass. Our method, LipsLev,\nis able to obtain $38.80$% and $13.93$% verified accuracy at distance $1$ and\n$2$ respectively in the AG-News dataset, while being $4$ orders of magnitude\nfaster than existing approaches. We believe our work can open the door to more\nefficient verification in the text domain.", "published": "2025-01-23 13:58:53", "link": "http://arxiv.org/abs/2501.13676v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Musical ethnocentrism in Large Language Models", "abstract": "Large Language Models (LLMs) reflect the biases in their training data and,\nby extension, those of the people who created this training data. Detecting,\nanalyzing, and mitigating such biases is becoming a focus of research. One type\nof bias that has been understudied so far are geocultural biases. Those can be\ncaused by an imbalance in the representation of different geographic regions\nand cultures in the training data, but also by value judgments contained\ntherein. In this paper, we make a first step towards analyzing musical biases\nin LLMs, particularly ChatGPT and Mixtral. We conduct two experiments. In the\nfirst, we prompt LLMs to provide lists of the \"Top 100\" musical contributors of\nvarious categories and analyze their countries of origin. In the second\nexperiment, we ask the LLMs to numerically rate various aspects of the musical\ncultures of different countries. Our results indicate a strong preference of\nthe LLMs for Western music cultures in both experiments.", "published": "2025-01-23 14:50:37", "link": "http://arxiv.org/abs/2501.13720v2", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "2-Tier SimCSE: Elevating BERT for Robust Sentence Embeddings", "abstract": "Effective sentence embeddings that capture semantic nuances and generalize\nwell across diverse contexts are crucial for natural language processing tasks.\nWe address this challenge by applying SimCSE (Simple Contrastive Learning of\nSentence Embeddings) using contrastive learning to fine-tune the minBERT model\nfor sentiment analysis, semantic textual similarity (STS), and paraphrase\ndetection. Our contributions include experimenting with three different dropout\ntechniques, namely standard dropout, curriculum dropout, and adaptive dropout,\nto tackle overfitting, proposing a novel 2-Tier SimCSE Fine-tuning Model that\ncombines both unsupervised and supervised SimCSE on STS task, and exploring\ntransfer learning potential for Paraphrase and SST tasks. Our findings\ndemonstrate the effectiveness of SimCSE, with the 2-Tier model achieving\nsuperior performance on the STS task, with an average test score of 0.742\nacross all three downstream tasks. The results of error analysis reveals\nchallenges in handling complex sentiments and reliance on lexical overlap for\nparaphrase detection, highlighting areas for future research. The ablation\nstudy revealed that removing Adaptive Dropout in the Single-Task Unsupervised\nSimCSE Model led to improved performance on the STS task, indicating\noverfitting due to added parameters. Transfer learning from SimCSE models on\nParaphrase and SST tasks did not enhance performance, suggesting limited\ntransferability of knowledge from the STS task.", "published": "2025-01-23 15:36:35", "link": "http://arxiv.org/abs/2501.13758v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Parameter-Efficient Fine-Tuning for Foundation Models", "abstract": "This survey delves into the realm of Parameter-Efficient Fine-Tuning (PEFT)\nwithin the context of Foundation Models (FMs). PEFT, a cost-effective\nfine-tuning technique, minimizes parameters and computational complexity while\nstriving for optimal downstream task performance. FMs, like ChatGPT, DALL-E,\nand LLaVA specialize in language understanding, generative tasks, and\nmultimodal tasks, trained on diverse datasets spanning text, images, and\nvideos. The diversity of FMs guides various adaptation strategies for PEFT.\nTherefore, this survey aims to provide a comprehensive overview of PEFT\ntechniques applied to diverse FMs and address critical gaps in understanding\nthe techniques, trends, and applications. We start by providing a detailed\ndevelopment of FMs and PEFT. Subsequently, we systematically review the key\ncategories and core mechanisms of PEFT across diverse FMs to offer a\ncomprehensive understanding of trends. We also explore the most recent\napplications across various FMs to demonstrate the versatility of PEFT,\nshedding light on the integration of systematic PEFT methods with a range of\nFMs. Furthermore, we identify potential research and development directions for\nimproving PEFTs in the future. This survey provides a valuable resource for\nboth newcomers and experts seeking to understand and use the power of PEFT\nacross FMs. All reviewed papers are listed at\n\\url{https://github.com/THUDM/Awesome-Parameter-Efficient-Fine-Tuning-for-Foundation-Models}.", "published": "2025-01-23 16:04:23", "link": "http://arxiv.org/abs/2501.13787v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Predicting Compact Phrasal Rewrites with Large Language Models for ASR\n  Post Editing", "abstract": "Large Language Models (LLMs) excel at rewriting tasks such as text style\ntransfer and grammatical error correction. While there is considerable overlap\nbetween the inputs and outputs in these tasks, the decoding cost still\nincreases with output length, regardless of the amount of overlap. By\nleveraging the overlap between the input and the output, Kaneko and Okazaki\n(2023) proposed model-agnostic edit span representations to compress the\nrewrites to save computation. They reported an output length reduction rate of\nnearly 80% with minimal accuracy impact in four rewriting tasks. In this paper,\nwe propose alternative edit phrase representations inspired by phrase-based\nstatistical machine translation. We systematically compare our phrasal\nrepresentations with their span representations. We apply the LLM rewriting\nmodel to the task of Automatic Speech Recognition (ASR) post editing and show\nthat our target-phrase-only edit representation has the best\nefficiency-accuracy trade-off. On the LibriSpeech test set, our method closes\n50-60% of the WER gap between the edit span model and the full rewrite model\nwhile losing only 10-20% of the length reduction rate of the edit span model.", "published": "2025-01-23 16:54:27", "link": "http://arxiv.org/abs/2501.13831v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Reasoning Capacity of AI Models and How to Quantify It", "abstract": "Recent advances in Large Language Models (LLMs) have intensified the debate\nsurrounding the fundamental nature of their reasoning capabilities. While\nachieving high performance on benchmarks such as GPQA and MMLU, these models\nexhibit limitations in more complex reasoning tasks, highlighting the need for\nmore rigorous evaluation methodologies. We propose a novel phenomenological\napproach that goes beyond traditional accuracy metrics to probe the underlying\nmechanisms of model behavior, establishing a framework that could broadly\nimpact how we analyze and understand AI systems. Using positional bias in\nmultiple-choice reasoning tasks as a case study, we demonstrate how systematic\nperturbations can reveal fundamental aspects of model decision-making. To\nanalyze these behaviors, we develop two complementary phenomenological models:\na Probabilistic Mixture Model (PMM) that decomposes model responses into\nreasoning, memorization, and guessing components and an Information-Theoretic\nConsistency (ITC) analysis that quantifies the relationship between model\nconfidence and strategy selection. Through controlled experiments on reasoning\nbenchmarks, we show that true reasoning remains challenging for current models,\nwith apparent success often relying on sophisticated combinations of\nmemorization and pattern matching rather than genuine logical deduction. More\nfundamentally, we demonstrate that accuracy alone often overstates a model's\nreasoning abilities, as model behavior can be characterized through underlying\nmechanisms in the phase space of cognitive strategies, revealing how models\ndynamically balance different approaches when responding to queries. This\nframework enables quantitative criteria for real-world deployments, allowing\napplications to specify reliability thresholds based on strategy distributions\nrather than aggregate performance metrics.", "published": "2025-01-23 16:58:18", "link": "http://arxiv.org/abs/2501.13833v1", "categories": ["cs.AI", "cs.CL", "cs.IT", "math.IT"], "primary_category": "cs.AI"}
{"title": "GUI-Bee: Align GUI Action Grounding to Novel Environments via Autonomous\n  Exploration", "abstract": "Graphical User Interface (GUI) action grounding is a critical step in GUI\nautomation that maps language instructions to actionable elements on GUI\nscreens. Most recent works of GUI action grounding leverage large GUI datasets\nto fine-tune MLLMs. However, the fine-tuning data always covers limited GUI\nenvironments, and we find the performance of the resulting model deteriorates\nin novel environments. We argue that the GUI grounding models should be further\naligned to the novel environments to reveal their full potential, when the\ninference is known to involve novel environments, i.e., environments not used\nduring the previous fine-tuning. To realize this, we first propose GUI-Bee, an\nMLLM-based autonomous agent, to collect high-quality, environment-specific data\nthrough exploration and then continuously fine-tune GUI grounding models with\nthe collected data. Our agent leverages a novel Q-value-Incentive In-Context\nReinforcement Learning (Q-ICRL) method to optimize exploration efficiency and\ndata quality. Additionally, we introduce NovelScreenSpot, a benchmark for\ntesting how well the data can help align GUI action grounding models to novel\nenvironments and demonstrate the effectiveness of data collected by GUI-Bee in\nthe experiments. Furthermore, we conduct an ablation study to validate the\nQ-ICRL method in enhancing the efficiency of GUI-Bee. Project page:\nhttps://gui-bee.github.io", "published": "2025-01-23 18:16:21", "link": "http://arxiv.org/abs/2501.13896v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "IMAGINE-E: Image Generation Intelligence Evaluation of State-of-the-art\n  Text-to-Image Models", "abstract": "With the rapid development of diffusion models, text-to-image(T2I) models\nhave made significant progress, showcasing impressive abilities in prompt\nfollowing and image generation. Recently launched models such as FLUX.1 and\nIdeogram2.0, along with others like Dall-E3 and Stable Diffusion 3, have\ndemonstrated exceptional performance across various complex tasks, raising\nquestions about whether T2I models are moving towards general-purpose\napplicability. Beyond traditional image generation, these models exhibit\ncapabilities across a range of fields, including controllable generation, image\nediting, video, audio, 3D, and motion generation, as well as computer vision\ntasks like semantic segmentation and depth estimation. However, current\nevaluation frameworks are insufficient to comprehensively assess these models'\nperformance across expanding domains. To thoroughly evaluate these models, we\ndeveloped the IMAGINE-E and tested six prominent models: FLUX.1, Ideogram2.0,\nMidjourney, Dall-E3, Stable Diffusion 3, and Jimeng. Our evaluation is divided\ninto five key domains: structured output generation, realism, and physical\nconsistency, specific domain generation, challenging scenario generation, and\nmulti-style creation tasks. This comprehensive assessment highlights each\nmodel's strengths and limitations, particularly the outstanding performance of\nFLUX.1 and Ideogram2.0 in structured and specific domain tasks, underscoring\nthe expanding applications and potential of T2I models as foundational AI\ntools. This study provides valuable insights into the current state and future\ntrajectory of T2I models as they evolve towards general-purpose usability.\nEvaluation scripts will be released at https://github.com/jylei16/Imagine-e.", "published": "2025-01-23 18:58:33", "link": "http://arxiv.org/abs/2501.13920v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Can We Generate Images with CoT? Let's Verify and Reinforce Image\n  Generation Step by Step", "abstract": "Chain-of-Thought (CoT) reasoning has been extensively explored in large\nmodels to tackle complex understanding tasks. However, it still remains an open\nquestion whether such strategies can be applied to verifying and reinforcing\nimage generation scenarios. In this paper, we provide the first comprehensive\ninvestigation of the potential of CoT reasoning to enhance autoregressive image\ngeneration. We focus on three techniques: scaling test-time computation for\nverification, aligning model preferences with Direct Preference Optimization\n(DPO), and integrating these techniques for complementary effects. Our results\ndemonstrate that these approaches can be effectively adapted and combined to\nsignificantly improve image generation performance. Furthermore, given the\npivotal role of reward models in our findings, we propose the Potential\nAssessment Reward Model (PARM) and PARM++, specialized for autoregressive image\ngeneration. PARM adaptively assesses each generation step through a potential\nassessment approach, merging the strengths of existing reward models, and\nPARM++ further introduces a reflection mechanism to self-correct the generated\nunsatisfactory image. Using our investigated reasoning strategies, we enhance a\nbaseline model, Show-o, to achieve superior results, with a significant +24%\nimprovement on the GenEval benchmark, surpassing Stable Diffusion 3 by +15%. We\nhope our study provides unique insights and paves a new path for integrating\nCoT reasoning with autoregressive image generation. Code and models are\nreleased at https://github.com/ZiyuGuo99/Image-Generation-CoT", "published": "2025-01-23 18:59:43", "link": "http://arxiv.org/abs/2501.13926v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "CRPO: Confidence-Reward Driven Preference Optimization for Machine\n  Translation", "abstract": "Large language models (LLMs) have shown great potential in natural language\nprocessing tasks, but their application to machine translation (MT) remains\nchallenging due to pretraining on English-centric data and the complexity of\nreinforcement learning from human feedback (RLHF). Direct Preference\nOptimization (DPO) has emerged as a simpler and more efficient alternative, but\nits performance depends heavily on the quality of preference data. To address\nthis, we propose Confidence-Reward driven Preference Optimization (CRPO), a\nnovel method that combines reward scores with model confidence to improve data\nselection for fine-tuning. CRPO selects challenging sentence pairs where the\nmodel is uncertain or underperforms, leading to more effective learning. While\nprimarily designed for LLMs, CRPO also generalizes to encoder-decoder models\nlike NLLB, demonstrating its versatility. Empirical results show that CRPO\noutperforms existing methods such as RS-DPO, RSO and MBR score in both\ntranslation accuracy and data efficiency.", "published": "2025-01-23 18:59:47", "link": "http://arxiv.org/abs/2501.13927v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Towards Safer Social Media Platforms: Scalable and Performant Few-Shot\n  Harmful Content Moderation Using Large Language Models", "abstract": "The prevalence of harmful content on social media platforms poses significant\nrisks to users and society, necessitating more effective and scalable content\nmoderation strategies. Current approaches rely on human moderators, supervised\nclassifiers, and large volumes of training data, and often struggle with\nscalability, subjectivity, and the dynamic nature of harmful content (e.g.,\nviolent content, dangerous challenge trends, etc.). To bridge these gaps, we\nutilize Large Language Models (LLMs) to undertake few-shot dynamic content\nmoderation via in-context learning. Through extensive experiments on multiple\nLLMs, we demonstrate that our few-shot approaches can outperform existing\nproprietary baselines (Perspective and OpenAI Moderation) as well as prior\nstate-of-the-art few-shot learning methods, in identifying harm. We also\nincorporate visual information (video thumbnails) and assess if different\nmultimodal techniques improve model performance. Our results underscore the\nsignificant benefits of employing LLM based methods for scalable and dynamic\nharmful content moderation online.", "published": "2025-01-23 00:19:14", "link": "http://arxiv.org/abs/2501.13976v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Re-ranking Using Large Language Models for Mitigating Exposure to\n  Harmful Content on Social Media Platforms", "abstract": "Social media platforms utilize Machine Learning (ML) and Artificial\nIntelligence (AI) powered recommendation algorithms to maximize user\nengagement, which can result in inadvertent exposure to harmful content.\nCurrent moderation efforts, reliant on classifiers trained with extensive\nhuman-annotated data, struggle with scalability and adapting to new forms of\nharm. To address these challenges, we propose a novel re-ranking approach using\nLarge Language Models (LLMs) in zero-shot and few-shot settings. Our method\ndynamically assesses and re-ranks content sequences, effectively mitigating\nharmful content exposure without requiring extensive labeled data. Alongside\ntraditional ranking metrics, we also introduce two new metrics to evaluate the\neffectiveness of re-ranking in reducing exposure to harmful content. Through\nexperiments on three datasets, three models and across three configurations, we\ndemonstrate that our LLM-based approach significantly outperforms existing\nproprietary moderation approaches, offering a scalable and adaptable solution\nfor harm mitigation.", "published": "2025-01-23 00:26:32", "link": "http://arxiv.org/abs/2501.13977v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Chain of Grounded Objectives: Bridging Process and Goal-oriented\n  Prompting for Code Generation", "abstract": "The use of Large Language Models (LLMs) for code generation has gained\nsignificant attention in recent years. Existing methods often aim to improve\nthe quality of generated code by incorporating additional contextual\ninformation or guidance into input prompts. Many of these approaches adopt\nsequential reasoning strategies, mimicking human-like step-by-step thinking.\nHowever, such strategies may constrain flexibility, as they do not always align\nwith the structured characteristics of programming languages. This paper\nintroduces the Chain of Grounded Objectives (CGO), a method that embeds\nfunctional objectives into input prompts to enhance code generation. By\nleveraging appropriately structured objectives as input and avoiding explicit\nsequential procedures, CGO adapts effectively to the structured nature of\nprogramming tasks. Empirical evaluations demonstrate that CGO effectively\nenhances code generation, addressing limitations of existing approaches.", "published": "2025-01-23 01:45:09", "link": "http://arxiv.org/abs/2501.13978v1", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Comprehensive Modeling and Question Answering of Cancer Clinical\n  Practice Guidelines using LLMs", "abstract": "The updated recommendations on diagnostic procedures and treatment pathways\nfor a medical condition are documented as graphical flows in Clinical Practice\nGuidelines (CPGs). For effective use of the CPGs in helping medical\nprofessionals in the treatment decision process, it is necessary to fully\ncapture the guideline knowledge, particularly the contexts and their\nrelationships in the graph. While several existing works have utilized these\nguidelines to create rule bases for Clinical Decision Support Systems, limited\nwork has been done toward directly capturing the full medical knowledge\ncontained in CPGs. This work proposes an approach to create a contextually\nenriched, faithful digital representation of National Comprehensive Cancer\nNetwork (NCCN) Cancer CPGs in the form of graphs using automated extraction and\nnode & relationship classification. We also implement semantic enrichment of\nthe model by using Large Language Models (LLMs) for node classification,\nachieving an accuracy of 80.86% and 88.47% with zero-shot learning and few-shot\nlearning, respectively. Additionally, we introduce a methodology for answering\nnatural language questions with constraints to guideline text by leveraging\nLLMs to extract the relevant subgraph from the guideline knowledge base. By\ngenerating natural language answers based on subgraph paths and semantic\ninformation, we mitigate the risk of incorrect answers and hallucination\nassociated with LLMs, ensuring factual accuracy in medical domain Question\nAnswering.", "published": "2025-01-23 07:06:26", "link": "http://arxiv.org/abs/2501.13984v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CAPRAG: A Large Language Model Solution for Customer Service and\n  Automatic Reporting using Vector and Graph Retrieval-Augmented Generation", "abstract": "The introduction of new features and services in the banking sector often\noverwhelms customers, creating an opportunity for banks to enhance user\nexperience through financial chatbots powered by large language models (LLMs).\nWe initiated an AI agent designed to provide customers with relevant\ninformation about banking services and insights from annual reports. We\nproposed a hybrid Customer Analysis Pipeline Retrieval-Augmented Generation\n(CAPRAG) that effectively addresses both relationship-based and contextual\nqueries, thereby improving customer engagement in the digital banking\nlandscape. To implement this, we developed a processing pipeline to refine text\ndata, which we utilized in two main frameworks: Vector RAG and Graph RAG. This\ndual approach enables us to populate both vector and graph databases with\nprocessed data for efficient retrieval. The Cypher query component is employed\nto effectively query the graph database. When a user submits a query, it is\nfirst expanded by a query expansion module before being routed to construct a\nfinal query from the hybrid Knowledge Base (KB). This final query is then sent\nto an open-source LLM for response generation. Overall, our innovative,\ndesigned to international banks, serves bank's customers in an increasingly\ncomplex digital environment, enhancing clarity and accessibility of\ninformation.", "published": "2025-01-23 10:38:20", "link": "http://arxiv.org/abs/2501.13993v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Communicating Activations Between Language Model Agents", "abstract": "Communication between multiple language model (LM) agents has been shown to\nscale up the reasoning ability of LMs. While natural language has been the\ndominant medium for inter-LM communication, it is not obvious this should be\nthe standard: not only does natural language communication incur high inference\ncosts that scale quickly with the number of both agents and messages, but also\nthe decoding process abstracts away too much rich information that could be\notherwise accessed from the internal activations. In this work, we propose a\nsimple technique whereby LMs communicate via activations; concretely, we pause\nan LM $\\textit{B}$'s computation at an intermediate layer, combine its current\nactivation with another LM $\\textit{A}$'s intermediate activation via some\nfunction $\\textit{f}$, then pass $\\textit{f}$'s output into the next layer of\n$\\textit{B}$ and continue the forward pass till decoding is complete. This\napproach scales up LMs on new tasks with zero additional parameters and data,\nand saves a substantial amount of compute over natural language communication.\nWe test our method with various functional forms $\\textit{f}$ on two\nexperimental setups--multi-player coordination games and reasoning\nbenchmarks--and find that it achieves up to $27.0\\%$ improvement over natural\nlanguage communication across datasets with $<$$1/4$ the compute, illustrating\nthe superiority and robustness of activations as an alternative \"language\" for\ncommunication between LMs.", "published": "2025-01-23 20:41:07", "link": "http://arxiv.org/abs/2501.14082v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MedSlice: Fine-Tuned Large Language Models for Secure Clinical Note\n  Sectioning", "abstract": "Extracting sections from clinical notes is crucial for downstream analysis\nbut is challenging due to variability in formatting and labor-intensive nature\nof manual sectioning. While proprietary large language models (LLMs) have shown\npromise, privacy concerns limit their accessibility. This study develops a\npipeline for automated note sectioning using open-source LLMs, focusing on\nthree sections: History of Present Illness, Interval History, and Assessment\nand Plan. We fine-tuned three open-source LLMs to extract sections using a\ncurated dataset of 487 progress notes, comparing results relative to\nproprietary models (GPT-4o, GPT-4o mini). Internal and external validity were\nassessed via precision, recall and F1 score. Fine-tuned Llama 3.1 8B\noutperformed GPT-4o (F1=0.92). On the external validity test set, performance\nremained high (F1= 0.85). Fine-tuned open-source LLMs can surpass proprietary\nmodels in clinical note sectioning, offering advantages in cost, performance,\nand accessibility.", "published": "2025-01-23 21:32:09", "link": "http://arxiv.org/abs/2501.14105v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Low-Rank Adapters Meet Neural Architecture Search for LLM Compression", "abstract": "The rapid expansion of Large Language Models (LLMs) has posed significant\nchallenges regarding the computational resources required for fine-tuning and\ndeployment. Recent advancements in low-rank adapters have demonstrated their\nefficacy in parameter-efficient fine-tuning (PEFT) of these models. This\nretrospective paper comprehensively discusses innovative approaches that\nsynergize low-rank representations with Neural Architecture Search (NAS)\ntechniques, particularly weight-sharing super-networks. Robust solutions for\ncompressing and fine-tuning large pre-trained models are developed by\nintegrating these methodologies. Our analysis highlights the potential of these\ncombined strategies to democratize the use of LLMs, making them more accessible\nfor deployment in resource-constrained environments. The resulting models\nexhibit reduced memory footprints and faster inference times, paving the way\nfor more practical and scalable applications of LLMs. Models and code are\navailable at\nhttps://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning.", "published": "2025-01-23 02:14:08", "link": "http://arxiv.org/abs/2501.16372v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Extractive Schema Linking for Text-to-SQL", "abstract": "Text-to-SQL is emerging as a practical interface for real world databases.\nThe dominant paradigm for Text-to-SQL is cross-database or schema-independent,\nsupporting application schemas unseen during training. The schema of a database\ndefines the tables, columns, column types and foreign key connections between\ntables. Real world schemas can be large, containing hundreds of columns, but\nfor any particular query only a small fraction will be relevant. Placing the\nentire schema in the prompt for an LLM can be impossible for models with\nsmaller token windows and expensive even when the context window is large\nenough to allow it. Even apart from computational considerations, the accuracy\nof the model can be improved by focusing the SQL generation on only the\nrelevant portion of the database. Schema linking identifies the portion of the\ndatabase schema useful for the question. Previous work on schema linking has\nused graph neural networks, generative LLMs, and cross encoder classifiers. We\nintroduce a new approach to adapt decoder-only LLMs to schema linking that is\nboth computationally more efficient and more accurate than the generative\napproach. Additionally our extractive approach permits fine-grained control\nover the precision-recall trade-off for schema linking.", "published": "2025-01-23 19:57:08", "link": "http://arxiv.org/abs/2501.17174v1", "categories": ["cs.DB", "cs.AI", "cs.CL"], "primary_category": "cs.DB"}
{"title": "Document-Level Sentiment Analysis of Urdu Text Using Deep Learning\n  Techniques", "abstract": "Document level Urdu Sentiment Analysis (SA) is a challenging Natural Language\nProcessing (NLP) task as it deals with large documents in a resource-poor\nlanguage. In large documents, there are ample amounts of words that exhibit\ndifferent viewpoints. Deep learning (DL) models comprise of complex neural\nnetwork architectures that have the ability to learn diverse features of the\ndata to classify various sentiments. Besides audio, image and video\nclassification; DL algorithms are now extensively used in text-based\nclassification problems. To explore the powerful DL techniques for Urdu SA, we\nhave applied five different DL architectures namely, Bidirectional Long Short\nTerm Memory (BiLSTM), Convolutional Neural Network (CNN), Convolutional Neural\nNetwork with Bidirectional Long Short Term Memory (CNN-BiLSTM), Bidirectional\nEncoder Representation from Transformer (BERT). In this paper, we have proposed\na DL hybrid model that integrates BiLSTM with Single Layer Multi Filter\nConvolutional Neural Network (BiLSTM-SLMFCNN). The proposed and baseline\ntechniques are applied on Urdu Customer Support data set and IMDB Urdu movie\nreview data set by using pretrained Urdu word embeddings that are suitable for\n(SA) at the document level. Results of these techniques are evaluated and our\nproposed model outperforms all other DL techniques for Urdu SA. BiLSTM-SLMFCNN\noutperformed the baseline DL models and achieved 83{\\%}, 79{\\%}, 83{\\%} and\n94{\\%} accuracy on small, medium and large sized IMDB Urdu movie review data\nset and Urdu Customer Support data set respectively.", "published": "2025-01-23 21:25:37", "link": "http://arxiv.org/abs/2501.17175v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Mining Social Determinants of Health for Heart Failure Patient 30-Day\n  Readmission via Large Language Model", "abstract": "Heart Failure (HF) affects millions of Americans and leads to high\nreadmission rates, posing significant healthcare challenges. While Social\nDeterminants of Health (SDOH) such as socioeconomic status and housing\nstability play critical roles in health outcomes, they are often\nunderrepresented in structured EHRs and hidden in unstructured clinical notes.\nThis study leverages advanced large language models (LLMs) to extract SDOHs\nfrom clinical text and uses logistic regression to analyze their association\nwith HF readmissions. By identifying key SDOHs (e.g. tobacco usage, limited\ntransportation) linked to readmission risk, this work also offers actionable\ninsights for reducing readmissions and improving patient care.", "published": "2025-01-23 23:05:53", "link": "http://arxiv.org/abs/2502.12158v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Temporal Preference Optimization for Long-Form Video Understanding", "abstract": "Despite significant advancements in video large multimodal models\n(video-LMMs), achieving effective temporal grounding in long-form videos\nremains a challenge for existing models. To address this limitation, we propose\nTemporal Preference Optimization (TPO), a novel post-training framework\ndesigned to enhance the temporal grounding capabilities of video-LMMs through\npreference learning. TPO adopts a self-training approach that enables models to\ndifferentiate between well-grounded and less accurate temporal responses by\nleveraging curated preference datasets at two granularities: localized temporal\ngrounding, which focuses on specific video segments, and comprehensive temporal\ngrounding, which captures extended temporal dependencies across entire video\nsequences. By optimizing on these preference datasets, TPO significantly\nenhances temporal understanding while reducing reliance on manually annotated\ndata. Extensive experiments on three long-form video understanding\nbenchmarks--LongVideoBench, MLVU, and Video-MME--demonstrate the effectiveness\nof TPO across two state-of-the-art video-LMMs. Notably, LLaVA-Video-TPO\nestablishes itself as the leading 7B model on the Video-MME benchmark,\nunderscoring the potential of TPO as a scalable and efficient solution for\nadvancing temporal reasoning in long-form video understanding. Project page:\nhttps://ruili33.github.io/tpo_website.", "published": "2025-01-23 18:58:03", "link": "http://arxiv.org/abs/2501.13919v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.CV"}
{"title": "AlphaSharpe: LLM-Driven Discovery of Robust Risk-Adjusted Metrics", "abstract": "Financial metrics like the Sharpe ratio are pivotal in evaluating investment\nperformance by balancing risk and return. However, traditional metrics often\nstruggle with robustness and generalization, particularly in dynamic and\nvolatile market conditions. This paper introduces AlphaSharpe, a novel\nframework leveraging large language models (LLMs) to iteratively evolve and\noptimize financial metrics to discover enhanced risk-return metrics that\noutperform traditional approaches in robustness and correlation with future\nperformance metrics by employing iterative crossover, mutation, and evaluation.\nKey contributions of this work include: (1) a novel use of LLMs to generate and\nrefine financial metrics with implicit domain-specific knowledge, (2) a scoring\nmechanism to ensure that evolved metrics generalize effectively to unseen data,\nand (3) an empirical demonstration of 3x predictive power for future\nrisk-returns, and 2x portfolio performance. Experimental results in a\nreal-world dataset highlight the superiority of discovered metrics, making them\nhighly relevant to portfolio managers and financial decision-makers. This\nframework not only addresses the limitations of existing metrics but also\nshowcases the potential of LLMs in advancing financial analytics, paving the\nway for informed and robust investment strategies.", "published": "2025-01-23 11:35:17", "link": "http://arxiv.org/abs/2502.00029v2", "categories": ["q-fin.PM", "cs.AI", "cs.CL", "cs.NE", "q-fin.RM"], "primary_category": "q-fin.PM"}
{"title": "TrueReason: An Exemplar Personalised Learning System Integrating\n  Reasoning with Foundational Models", "abstract": "Personalised education is one of the domains that can greatly benefit from\nthe most recent advances in Artificial Intelligence (AI) and Large Language\nModels (LLM). However, it is also one of the most challenging applications due\nto the cognitive complexity of teaching effectively while personalising the\nlearning experience to suit independent learners. We hypothesise that one\npromising approach to excelling in such demanding use cases is using a\n\\emph{society of minds}. In this chapter, we present TrueReason, an exemplar\npersonalised learning system that integrates a multitude of specialised AI\nmodels that can mimic micro skills that are composed together by a LLM to\noperationalise planning and reasoning. The architecture of the initial\nprototype is presented while describing two micro skills that have been\nincorporated in the prototype. The proposed system demonstrates the first step\nin building sophisticated AI systems that can take up very complex cognitive\ntasks that are demanded by domains such as education.", "published": "2025-01-23 13:25:44", "link": "http://arxiv.org/abs/2502.10411v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.IR", "cs.MA", "I.2.4; I.2.7; H.3.3; I.2.6; K.3.1"], "primary_category": "cs.CY"}
{"title": "Machine Learning-Driven Convergence Analysis in Multijurisdictional\n  Compliance Using BERT and K-Means Clustering", "abstract": "Digital data continues to grow, there has been a shift towards using\neffective regulatory mechanisms to safeguard personal information. The CCPA of\nCalifornia and the General Data Protection Regulation (GDPR) of the European\nUnion are two of the most important privacy laws. The regulation is intended to\nsafeguard consumer privacy, but it varies greatly in scope, definitions, and\nmethods of enforcement. This paper presents a fresh approach to adaptive\ncompliance, using machine learning and emphasizing natural language processing\n(NLP) as the primary focus of comparison between the GDPR and CCPA. Using NLP,\nthis study compares various regulations to identify areas where they overlap or\ndiverge. This includes the \"right to be forgotten\" provision in the GDPR and\nthe \"opt-out of sale\" provision under CCPA. International companies can learn\nvaluable lessons from this report, as it outlines strategies for better\nenforcement of laws across different nations. Additionally, the paper discusses\nthe challenges of utilizing NLP in legal literature and proposes methods to\nenhance the model-ability of machine learning models for studying regulations.\nThe study's objective is to \"bridge the gap between legal knowledge and\ntechnical expertise\" by developing regulatory compliance strategies that are\nmore efficient in operation and more effective in data protection.", "published": "2025-01-23 22:11:18", "link": "http://arxiv.org/abs/2502.10413v1", "categories": ["cs.CY", "cs.AI", "cs.CE", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Learning-based A Posteriori Speech Presence Probability Estimation and\n  Applications", "abstract": "The a posteriori speech presence probability (SPP) is the fundamental\ncomponent of noise power spectral density (PSD) estimation, which can\ncontribute to speech enhancement and speech recognition systems. Most existing\nSPP estimators can estimate SPP accurately from the background noise.\nNevertheless, numerous challenges persist, including the difficulty of\naccurately estimating SPP from non-stationary noise with statistics-based\nmethods and the high latency associated with deep learning-based approaches.\nThis paper presents an improved SPP estimation approach based on deep learning\nto achieve higher SPP estimation accuracy, especially in non-stationary noise\nconditions. To promote the information extraction performance of the DNN, the\nglobal information of the observed signal and the local information of the\ndecoupled frequency bins from the observed signal are connected as hybrid\nglobal-local information. The global information is extracted by one encoder.\nThen, one decoder and two fully connected layers are used to estimate SPP from\nthe information of residual connection. To evaluate the performance of our\nproposed SPP estimator, the noise PSD estimation and speech enhancement tasks\nare performed. In contrast to existing minimum mean-square error (MMSE)-based\nnoise PSD estimation approaches, the noise PSD is estimated by the sub-optimal\nMMSE based on the current frame SPP estimate without smoothing. Directed by the\nnoise PSD estimate, a standard speech enhancement framework, the log spectral\namplitude estimator, is employed to extract clean speech from the observed\nsignal. From the experimental results, we can confirm that our proposed SPP\nestimator can achieve high noise PSD estimation accuracy and speech enhancement\nperformance while requiring low model complexity.", "published": "2025-01-23 13:19:26", "link": "http://arxiv.org/abs/2501.13642v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Generative Data Augmentation Challenge: Zero-Shot Speech Synthesis for\n  Personalized Speech Enhancement", "abstract": "This paper presents a new challenge that calls for zero-shot text-to-speech\n(TTS) systems to augment speech data for the downstream task, personalized\nspeech enhancement (PSE), as part of the Generative Data Augmentation workshop\nat ICASSP 2025. Collecting high-quality personalized data is challenging due to\nprivacy concerns and technical difficulties in recording audio from the test\nscene. To address these issues, synthetic data generation using generative\nmodels has gained significant attention. In this challenge, participants are\ntasked first with building zero-shot TTS systems to augment personalized data.\nSubsequently, PSE systems are asked to be trained with this augmented\npersonalized dataset. Through this challenge, we aim to investigate how the\nquality of augmented data generated by zero-shot TTS models affects PSE model\nperformance. We also provide baseline experiments using open-source zero-shot\nTTS models to encourage participation and benchmark advancements. Our baseline\ncode implementation and checkpoints are available online.", "published": "2025-01-23 04:27:37", "link": "http://arxiv.org/abs/2501.13372v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Neural Vocoders as Speech Enhancers", "abstract": "Speech enhancement (SE) and neural vocoding are traditionally viewed as\nseparate tasks. In this work, we observe them under a common thread: the rank\nbehavior of these processes. This observation prompts two key questions:\n\\textit{Can a model designed for one task's rank degradation be adapted for the\nother?} and \\textit{Is it possible to address both tasks using a unified\nmodel?} Our empirical findings demonstrate that existing speech enhancement\nmodels can be successfully trained to perform vocoding tasks, and a single\nmodel, when jointly trained, can effectively handle both tasks with performance\ncomparable to separately trained models. These results suggest that speech\nenhancement and neural vocoding can be unified under a broader framework of\nspeech restoration. Code:\nhttps://github.com/Andong-Li-speech/Neural-Vocoders-as-Speech-Enhancers.", "published": "2025-01-23 08:27:01", "link": "http://arxiv.org/abs/2501.13465v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Everyone-Can-Sing: Zero-Shot Singing Voice Synthesis and Conversion with\n  Speech Reference", "abstract": "We propose a unified framework for Singing Voice Synthesis (SVS) and\nConversion (SVC), addressing the limitations of existing approaches in\ncross-domain SVS/SVC, poor output musicality, and scarcity of singing data. Our\nframework enables control over multiple aspects, including language content\nbased on lyrics, performance attributes based on a musical score, singing style\nand vocal techniques based on a selector, and voice identity based on a speech\nsample. The proposed zero-shot learning paradigm consists of one SVS model and\ntwo SVC models, utilizing pre-trained content embeddings and a diffusion-based\ngenerator. The proposed framework is also trained on mixed datasets comprising\nboth singing and speech audio, allowing singing voice cloning based on speech\nreference. Experiments show substantial improvements in timbre similarity and\nmusicality over state-of-the-art baselines, providing insights into other\nlow-data music tasks such as instrumental style transfer. Examples can be found\nat: everyone-can-sing.github.io.", "published": "2025-01-23 17:41:40", "link": "http://arxiv.org/abs/2501.13870v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Bridging The Multi-Modality Gaps of Audio, Visual and Linguistic for\n  Speech Enhancement", "abstract": "Speech Enhancement (SE) aims to improve the quality of noisy speech. It has\nbeen shown that additional visual cues can further improve performance. Given\nthat speech communication involves audio, visual, and linguistic modalities, it\nis natural to expect another performance boost by incorporating linguistic\ninformation. However, bridging the modality gaps to efficiently incorporate\nlinguistic information, along with audio and visual modalities during knowledge\ntransfer, is a challenging task. In this paper, we propose a novel\nmulti-modality learning framework for SE. In the model framework, a\nstate-of-the-art diffusion Model backbone is utilized for Audio-Visual Speech\nEnhancement (AVSE) modeling where both audio and visual information are\ndirectly captured by microphones and video cameras. Based on this AVSE, the\nlinguistic modality employs a PLM to transfer linguistic knowledge to the\nvisual acoustic modality through a process termed Cross-Modal Knowledge\nTransfer (CMKT) during AVSE model training. After the model is trained, it is\nsupposed that linguistic knowledge is encoded in the feature processing of the\nAVSE model by the CMKT, and the PLM will not be involved during inference\nstage. We carry out SE experiments to evaluate the proposed model framework.\nExperimental results demonstrate that our proposed AVSE system significantly\nenhances speech quality and reduces generative artifacts, such as phonetic\nconfusion compared to the state-of-the-art. Moreover, our visualization results\ndemonstrate that our Cross-Modal Knowledge Transfer method further improves the\ngenerated speech quality of our AVSE system. These findings not only suggest\nthat Diffusion Model-based techniques hold promise for advancing the\nstate-of-the-art in AVSE but also justify the effectiveness of incorporating\nlinguistic information to improve the performance of Diffusion-based AVSE\nsystems.", "published": "2025-01-23 04:36:29", "link": "http://arxiv.org/abs/2501.13375v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploring Finetuned Audio-LLM on Heart Murmur Features", "abstract": "Large language models (LLMs) for audio have excelled in recognizing and\nanalyzing human speech, music, and environmental sounds. However, their\npotential for understanding other types of sounds, particularly biomedical\nsounds, remains largely underexplored despite significant scientific interest.\nIn this study, we focus on diagnosing cardiovascular diseases using\nphonocardiograms, i.e., heart sounds. Most existing deep neural network (DNN)\nparadigms are restricted to heart murmur classification (healthy vs unhealthy)\nand do not predict other acoustic features of the murmur such as timing,\ngrading, harshness, pitch, and quality, which are important in helping\nphysicians diagnose the underlying heart conditions. We propose to finetune an\naudio LLM, Qwen2-Audio, on the PhysioNet CirCor DigiScope phonocardiogram (PCG)\ndataset and evaluate its performance in classifying 11 expert-labeled murmur\nfeatures. Additionally, we aim to achieve more noise-robust and generalizable\nsystem by exploring a preprocessing segmentation algorithm using an audio\nrepresentation model, SSAMBA. Our results indicate that the LLM-based model\noutperforms state-of-the-art methods in 8 of the 11 features and performs\ncomparably in the remaining 3. Moreover, the LLM successfully classifies\nlong-tail murmur features with limited training data, a task that all previous\nmethods have failed to classify. These findings underscore the potential of\naudio LLMs as assistants to human cardiologists in enhancing heart disease\ndiagnosis.", "published": "2025-01-23 17:57:18", "link": "http://arxiv.org/abs/2501.13884v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "What Does an Audio Deepfake Detector Focus on? A Study in the Time\n  Domain", "abstract": "Adding explanations to audio deepfake detection (ADD) models will boost their\nreal-world application by providing insight on the decision making process. In\nthis paper, we propose a relevancy-based explainable AI (XAI) method to analyze\nthe predictions of transformer-based ADD models. We compare against standard\nGrad-CAM and SHAP-based methods, using quantitative faithfulness metrics as\nwell as a partial spoof test, to comprehensively analyze the relative\nimportance of different temporal regions in an audio. We consider large\ndatasets, unlike previous works where only limited utterances are studied, and\nfind that the XAI methods differ in their explanations. The proposed\nrelevancy-based XAI method performs the best overall on a variety of metrics.\nFurther investigation on the relative importance of speech/non-speech, phonetic\ncontent, and voice onsets/offsets suggest that the XAI results obtained from\nanalyzing limited utterances don't necessarily hold when evaluated on large\ndatasets.", "published": "2025-01-23 18:00:14", "link": "http://arxiv.org/abs/2501.13887v2", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Evolving Performance Practices in Beethoven's Cello Sonatas: Tempo,\n  Portamento, and Historical Interpretation of the First Movements", "abstract": "This paper examines the evolving performance practices of Ludwig van\nBeethoven's cello sonatas, with a particular focus on tempo and portamento\nbetween 1930 and 2012. It integrates analyses of 22 historical recordings,\nadvancements in recording technology to shed light on changes in interpretative\napproaches. By comparing Beethoven's metronome markings, as understood through\ncontemporaries such as Czerny and Moscheles, with their application in modern\nperformances, my research highlights notable deviations. These differences\nprove the challenges performers face in reconciling historical tempos with the\ndemands of contemporary performance practice. My study pays special attention\nto the diminishing use of audible portamento in the latter half of the 20th\ncentury, contrasted with a gradual increase in tempo after 1970. This\ndevelopment is linked to broader cultural and pedagogical shifts, including the\nadoption of fingering techniques that reduce hand shifts, thereby facilitating\ngreater technical precision at faster tempos. Nonetheless, my study identifies\nthe persistence of 'silent portamento' as an expressive device, allowing\nperformers to retain stylistic expression without compromising rhythmic\nintegrity. My paper offers valuable insights for performers and scholars alike,\nadvocating a critical reassessment of Beethoven's tempo markings and the\nnuanced application of portamento in modern performance practice.", "published": "2025-01-23 13:49:57", "link": "http://arxiv.org/abs/2502.00030v1", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Tune In, Act Up: Exploring the Impact of Audio Modality-Specific Edits\n  on Large Audio Language Models in Jailbreak", "abstract": "Large Language Models (LLMs) demonstrate remarkable zero-shot performance\nacross various natural language processing tasks. The integration of multimodal\nencoders extends their capabilities, enabling the development of Multimodal\nLarge Language Models that process vision, audio, and text. However, these\ncapabilities also raise significant security concerns, as these models can be\nmanipulated to generate harmful or inappropriate content through jailbreak.\nWhile extensive research explores the impact of modality-specific input edits\non text-based LLMs and Large Vision-Language Models in jailbreak, the effects\nof audio-specific edits on Large Audio-Language Models (LALMs) remain\nunderexplored. Hence, this paper addresses this gap by investigating how\naudio-specific edits influence LALMs inference regarding jailbreak. We\nintroduce the Audio Editing Toolbox (AET), which enables audio-modality edits\nsuch as tone adjustment, word emphasis, and noise injection, and the Edited\nAudio Datasets (EADs), a comprehensive audio jailbreak benchmark. We also\nconduct extensive evaluations of state-of-the-art LALMs to assess their\nrobustness under different audio edits. This work lays the groundwork for\nfuture explorations on audio-modality interactions in LALMs security.", "published": "2025-01-23 15:51:38", "link": "http://arxiv.org/abs/2501.13772v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
