{"title": "Automatic Noisy Label Correction for Fine-Grained Entity Typing", "abstract": "Fine-grained entity typing (FET) aims to assign proper semantic types to\nentity mentions according to their context, which is a fundamental task in\nvarious entity-leveraging applications. Current FET systems usually establish\non large-scale weakly-supervised/distantly annotation data, which may contain\nabundant noise and thus severely hinder the performance of the FET task.\nAlthough previous studies have made great success in automatically identifying\nthe noisy labels in FET, they usually rely on some auxiliary resources which\nmay be unavailable in real-world applications (e.g. pre-defined hierarchical\ntype structures, human-annotated subsets). In this paper, we propose a novel\napproach to automatically correct noisy labels for FET without external\nresources. Specifically, it first identifies the potentially noisy labels by\nestimating the posterior probability of a label being positive or negative\naccording to the logits output by the model, and then relabel candidate noisy\nlabels by training a robust model over the remaining clean labels. Experiments\non two popular benchmarks prove the effectiveness of our method. Our source\ncode can be obtained from https://github.com/CCIIPLab/DenoiseFET.", "published": "2022-05-06 04:39:02", "link": "http://arxiv.org/abs/2205.03011v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aksharantar: Open Indic-language Transliteration datasets and models for\n  the Next Billion Users", "abstract": "Transliteration is very important in the Indian language context due to the\nusage of multiple scripts and the widespread use of romanized inputs. However,\nfew training and evaluation sets are publicly available. We introduce\nAksharantar, the largest publicly available transliteration dataset for Indian\nlanguages created by mining from monolingual and parallel corpora, as well as\ncollecting data from human annotators. The dataset contains 26 million\ntransliteration pairs for 21 Indic languages from 3 language families using 12\nscripts. Aksharantar is 21 times larger than existing datasets and is the first\npublicly available dataset for 7 languages and 1 language family. We also\nintroduce the Aksharantar testset comprising 103k word pairs spanning 19\nlanguages that enables a fine-grained analysis of transliteration models on\nnative origin words, foreign words, frequent words, and rare words. Using the\ntraining set, we trained IndicXlit, a multilingual transliteration model that\nimproves accuracy by 15% on the Dakshina test set, and establishes strong\nbaselines on the Aksharantar testset introduced in this work. The models,\nmining scripts, transliteration guidelines, and datasets are available at\nhttps://github.com/AI4Bharat/IndicXlit under open-source licenses. We hope the\navailability of these large-scale, open resources will spur innovation for\nIndic language transliteration and downstream applications. We hope the\navailability of these large-scale, open resources will spur innovation for\nIndic language transliteration and downstream applications.", "published": "2022-05-06 05:13:12", "link": "http://arxiv.org/abs/2205.03018v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hearing voices at the National Library -- a speech corpus and acoustic\n  model for the Swedish language", "abstract": "This paper explains our work in developing new acoustic models for automated\nspeech recognition (ASR) at KBLab, the infrastructure for data-driven research\nat the National Library of Sweden (KB). We evaluate different approaches for a\nviable speech-to-text pipeline for audiovisual resources in Swedish, using the\nwav2vec 2.0 architecture in combination with speech corpuses created from KB's\ncollections. These approaches include pretraining an acoustic model for Swedish\nfrom the ground up, and fine-tuning existing monolingual and multilingual\nmodels. The collections-based corpuses we use have been sampled from millions\nof hours of speech, with a conscious attempt to balance regional dialects to\nproduce a more representative, and thus more democratic, model. The acoustic\nmodel this enabled, \"VoxRex\", outperforms existing models for Swedish ASR. We\nalso evaluate combining this model with various pretrained language models,\nwhich further enhanced performance. We conclude by highlighting the potential\nof such technology for cultural heritage institutions with vast collections of\npreviously unlabelled audiovisual data. Our models are released for further\nexploration and research here: https://huggingface.co/KBLab.", "published": "2022-05-06 06:06:00", "link": "http://arxiv.org/abs/2205.03026v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridging the Domain Gap for Stance Detection for the Zulu language", "abstract": "Misinformation has become a major concern in recent last years given its\nspread across our information sources. In the past years, many NLP tasks have\nbeen introduced in this area, with some systems reaching good results on\nEnglish language datasets. Existing AI based approaches for fighting\nmisinformation in literature suggest automatic stance detection as an integral\nfirst step to success. Our paper aims at utilizing this progress made for\nEnglish to transfers that knowledge into other languages, which is a\nnon-trivial task due to the domain gap between English and the target\nlanguages. We propose a black-box non-intrusive method that utilizes techniques\nfrom Domain Adaptation to reduce the domain gap, without requiring any human\nexpertise in the target language, by leveraging low-quality data in both a\nsupervised and unsupervised manner. This allows us to rapidly achieve similar\nresults for stance detection for the Zulu language, the target language in this\nwork, as are found for English. We also provide a stance detection dataset in\nthe Zulu language. Our experimental results show that by leveraging English\ndatasets and machine translation we can increase performances on both English\ndata along with other languages.", "published": "2022-05-06 11:44:35", "link": "http://arxiv.org/abs/2205.03153v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GlobEnc: Quantifying Global Token Attribution by Incorporating the Whole\n  Encoder Layer in Transformers", "abstract": "There has been a growing interest in interpreting the underlying dynamics of\nTransformers. While self-attention patterns were initially deemed as the\nprimary option, recent studies have shown that integrating other components can\nyield more accurate explanations. This paper introduces a novel token\nattribution analysis method that incorporates all the components in the encoder\nblock and aggregates this throughout layers. Through extensive quantitative and\nqualitative experiments, we demonstrate that our method can produce faithful\nand meaningful global token attributions. Our experiments reveal that\nincorporating almost every encoder component results in increasingly more\naccurate analysis in both local (single layer) and global (the whole model)\nsettings. Our global attribution analysis significantly outperforms previous\nmethods on various tasks regarding correlation with gradient-based saliency\nscores. Our code is freely available at\nhttps://github.com/mohsenfayyaz/GlobEnc.", "published": "2022-05-06 15:13:34", "link": "http://arxiv.org/abs/2205.03286v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Disentangled Learning of Stance and Aspect Topics for Vaccine Attitude\n  Detection in Social Media", "abstract": "Building models to detect vaccine attitudes on social media is challenging\nbecause of the composite, often intricate aspects involved, and the limited\navailability of annotated data. Existing approaches have relied heavily on\nsupervised training that requires abundant annotations and pre-defined aspect\ncategories. Instead, with the aim of leveraging the large amount of unannotated\ndata now available on vaccination, we propose a novel semi-supervised approach\nfor vaccine attitude detection, called VADet. A variational autoencoding\narchitecture based on language models is employed to learn from unlabelled data\nthe topical information of the domain. Then, the model is fine-tuned with a few\nmanually annotated examples of user attitudes. We validate the effectiveness of\nVADet on our annotated data and also on an existing vaccination corpus\nannotated with opinions on vaccines. Our results show that VADet is able to\nlearn disentangled stance and aspect topics, and outperforms existing\naspect-based sentiment analysis models on both stance detection and tweet\nclustering.", "published": "2022-05-06 15:24:33", "link": "http://arxiv.org/abs/2205.03296v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Necessity and Sufficiency for Explaining Text Classifiers: A Case Study\n  in Hate Speech Detection", "abstract": "We present a novel feature attribution method for explaining text\nclassifiers, and analyze it in the context of hate speech detection. Although\nfeature attribution models usually provide a single importance score for each\ntoken, we instead provide two complementary and theoretically-grounded scores\n-- necessity and sufficiency -- resulting in more informative explanations. We\npropose a transparent method that calculates these values by generating\nexplicit perturbations of the input text, allowing the importance scores\nthemselves to be explainable. We employ our method to explain the predictions\nof different hate speech detection models on the same set of curated examples\nfrom a test suite, and show that different values of necessity and sufficiency\nfor identity terms correspond to different kinds of false positive errors,\nexposing sources of classifier bias against marginalized groups.", "published": "2022-05-06 15:34:48", "link": "http://arxiv.org/abs/2205.03302v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combining Humor and Sarcasm for Improving Political Parody Detection", "abstract": "Parody is a figurative device used for mimicking entities for comedic or\ncritical purposes. Parody is intentionally humorous and often involves sarcasm.\nThis paper explores jointly modelling these figurative tropes with the goal of\nimproving performance of political parody detection in tweets. To this end, we\npresent a multi-encoder model that combines three parallel encoders to enrich\nparody-specific representations with humor and sarcasm information. Experiments\non a publicly available data set of political parody tweets demonstrate that\nour approach outperforms previous state-of-the-art methods.", "published": "2022-05-06 15:48:22", "link": "http://arxiv.org/abs/2205.03313v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Example-Based Machine Translation from Text to a Hierarchical\n  Representation of Sign Language", "abstract": "This article presents an original method for Text-to-Sign Translation. It\ncompensates data scarcity using a domain-specific parallel corpus of alignments\nbetween text and hierarchical formal descriptions of Sign Language videos in\nAZee. Based on the detection of similarities present in the source text, the\nproposed algorithm recursively exploits matches and substitutions of aligned\nsegments to build multiple candidate translations for a novel statement. This\nhelps preserving Sign Language structures as much as possible before falling\nback on literal translations too quickly, in a generative way. The resulting\ntranslations are in the form of AZee expressions, designed to be used as input\nto avatar synthesis systems. We present a test set tailored to showcase its\npotential for expressiveness and generation of idiomatic target language, and\nobserved limitations. This work finally opens prospects on how to evaluate\ntranslation and linguistic aspects, such as accuracy and grammatical fluency.", "published": "2022-05-06 15:48:43", "link": "http://arxiv.org/abs/2205.03314v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Unreliability of Explanations in Few-shot Prompting for Textual\n  Reasoning", "abstract": "Does prompting a large language model (LLM) like GPT-3 with explanations\nimprove in-context learning? We study this question on two NLP tasks that\ninvolve reasoning over text, namely question answering and natural language\ninference. We test the performance of four LLMs on three textual reasoning\ndatasets using prompts that include explanations in multiple different styles.\nFor these tasks, we find that including explanations in the prompts for OPT,\nGPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to\nmoderate accuracy improvements over standard few-show learning. However,\ntext-davinci-002 is able to benefit more substantially.\n  We further show that explanations generated by the LLMs may not entail the\nmodels' predictions nor be factually grounded in the input, even on simple\ntasks with extractive explanations. However, these flawed explanations can\nstill be useful as a way to verify LLMs' predictions post-hoc. Through analysis\nin our three settings, we show that explanations judged by humans to be\ngood--logically consistent with the input and the prediction--more likely\ncooccur with accurate predictions. Following these observations, we train\ncalibrators using automatically extracted scores that assess the reliability of\nexplanations, allowing us to improve performance post-hoc across all of our\ndatasets.", "published": "2022-05-06 17:57:58", "link": "http://arxiv.org/abs/2205.03401v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Data Cartography based MixUp for Pre-trained Language Models", "abstract": "MixUp is a data augmentation strategy where additional samples are generated\nduring training by combining random pairs of training samples and their labels.\nHowever, selecting random pairs is not potentially an optimal choice. In this\nwork, we propose TDMixUp, a novel MixUp strategy that leverages Training\nDynamics and allows more informative samples to be combined for generating new\ndata samples. Our proposed TDMixUp first measures confidence, variability,\n(Swayamdipta et al., 2020), and Area Under the Margin (AUM) (Pleiss et al.,\n2020) to identify the characteristics of training samples (e.g., as\neasy-to-learn or ambiguous samples), and then interpolates these characterized\nsamples. We empirically validate that our method not only achieves competitive\nperformance using a smaller subset of the training data compared with strong\nbaselines, but also yields lower expected calibration error on the pre-trained\nlanguage model, BERT, on both in-domain and out-of-domain settings in a wide\nrange of NLP tasks. We publicly release our code.", "published": "2022-05-06 17:59:19", "link": "http://arxiv.org/abs/2205.03403v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When a sentence does not introduce a discourse entity, Transformer-based\n  models still sometimes refer to it", "abstract": "Understanding longer narratives or participating in conversations requires\ntracking of discourse entities that have been mentioned. Indefinite noun\nphrases (NPs), such as 'a dog', frequently introduce discourse entities but\nthis behavior is modulated by sentential operators such as negation. For\nexample, 'a dog' in 'Arthur doesn't own a dog' does not introduce a discourse\nentity due to the presence of negation. In this work, we adapt the\npsycholinguistic assessment of language models paradigm to higher-level\nlinguistic phenomena and introduce an English evaluation suite that targets the\nknowledge of the interactions between sentential operators and indefinite NPs.\nWe use this evaluation suite for a fine-grained investigation of the entity\ntracking abilities of the Transformer-based models GPT-2 and GPT-3. We find\nthat while the models are to a certain extent sensitive to the interactions we\ninvestigate, they are all challenged by the presence of multiple NPs and their\nbehavior is not systematic, which suggests that even models at the scale of\nGPT-3 do not fully acquire basic entity tracking abilities.", "published": "2022-05-06 20:49:27", "link": "http://arxiv.org/abs/2205.03472v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KECP: Knowledge Enhanced Contrastive Prompting for Few-shot Extractive\n  Question Answering", "abstract": "Extractive Question Answering (EQA) is one of the most important tasks in\nMachine Reading Comprehension (MRC), which can be solved by fine-tuning the\nspan selecting heads of Pre-trained Language Models (PLMs). However, most\nexisting approaches for MRC may perform poorly in the few-shot learning\nscenario. To solve this issue, we propose a novel framework named Knowledge\nEnhanced Contrastive Prompt-tuning (KECP). Instead of adding pointer heads to\nPLMs, we introduce a seminal paradigm for EQA that transform the task into a\nnon-autoregressive Masked Language Modeling (MLM) generation problem.\nSimultaneously, rich semantics from the external knowledge base (KB) and the\npassage context are support for enhancing the representations of the query. In\naddition, to boost the performance of PLMs, we jointly train the model by the\nMLM and contrastive learning objectives. Experiments on multiple benchmarks\ndemonstrate that our method consistently outperforms state-of-the-art\napproaches in few-shot settings by a large margin.", "published": "2022-05-06 08:31:02", "link": "http://arxiv.org/abs/2205.03071v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "QLEVR: A Diagnostic Dataset for Quantificational Language and Elementary\n  Visual Reasoning", "abstract": "Synthetic datasets have successfully been used to probe visual\nquestion-answering datasets for their reasoning abilities. CLEVR\n(johnson2017clevr), for example, tests a range of visual reasoning abilities.\nThe questions in CLEVR focus on comparisons of shapes, colors, and sizes,\nnumerical reasoning, and existence claims. This paper introduces a minimally\nbiased, diagnostic visual question-answering dataset, QLEVR, that goes beyond\nexistential and numerical quantification and focus on more complex quantifiers\nand their combinations, e.g., asking whether there are more than two red balls\nthat are smaller than at least three blue balls in an image. We describe how\nthe dataset was created and present a first evaluation of state-of-the-art\nvisual question-answering models, showing that QLEVR presents a formidable\nchallenge to our current models. Code and Dataset are available at\nhttps://github.com/zechenli03/QLEVR", "published": "2022-05-06 08:51:13", "link": "http://arxiv.org/abs/2205.03075v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Arabic Fake News Detection Based on Deep Contextualized Embedding Models", "abstract": "Social media is becoming a source of news for many people due to its ease and\nfreedom of use. As a result, fake news has been spreading quickly and easily\nregardless of its credibility, especially in the last decade. Fake news\npublishers take advantage of critical situations such as the Covid-19 pandemic\nand the American presidential elections to affect societies negatively. Fake\nnews can seriously impact society in many fields including politics, finance,\nsports, etc. Many studies have been conducted to help detect fake news in\nEnglish, but research conducted on fake news detection in the Arabic language\nis scarce. Our contribution is twofold: first, we have constructed a large and\ndiverse Arabic fake news dataset. Second, we have developed and evaluated\ntransformer-based classifiers to identify fake news while utilizing eight\nstate-of-the-art Arabic contextualized embedding models. The majority of these\nmodels had not been previously used for Arabic fake news detection. We conduct\na thorough analysis of the state-of-the-art Arabic contextualized embedding\nmodels as well as comparison with similar fake news detection systems.\nExperimental results confirm that these state-of-the-art models are robust,\nwith accuracy exceeding 98%.", "published": "2022-05-06 09:54:35", "link": "http://arxiv.org/abs/2205.03114v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Quantifying Synthesis and Fusion and their Impact on Machine Translation", "abstract": "Theoretical work in morphological typology offers the possibility of\nmeasuring morphological diversity on a continuous scale. However, literature in\nNatural Language Processing (NLP) typically labels a whole language with a\nstrict type of morphology, e.g. fusional or agglutinative. In this work, we\npropose to reduce the rigidity of such claims, by quantifying morphological\ntypology at the word and segment level. We consider Payne (2017)'s approach to\nclassify morphology using two indices: synthesis (e.g. analytic to\npolysynthetic) and fusion (agglutinative to fusional). For computing synthesis,\nwe test unsupervised and supervised morphological segmentation methods for\nEnglish, German and Turkish, whereas for fusion, we propose a semi-automatic\nmethod using Spanish as a case study. Then, we analyse the relationship between\nmachine translation quality and the degree of synthesis and fusion at word\n(nouns and verbs for English-Turkish, and verbs in English-Spanish) and segment\nlevel (previous language pairs plus English-German in both directions). We\ncomplement the word-level analysis with human evaluation, and overall, we\nobserve a consistent impact of both indexes on machine translation quality.", "published": "2022-05-06 17:04:58", "link": "http://arxiv.org/abs/2205.03369v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Explaining the Effectiveness of Multi-Task Learning for Efficient\n  Knowledge Extraction from Spine MRI Reports", "abstract": "Pretrained Transformer based models finetuned on domain specific corpora have\nchanged the landscape of NLP. However, training or fine-tuning these models for\nindividual tasks can be time consuming and resource intensive. Thus, a lot of\ncurrent research is focused on using transformers for multi-task learning\n(Raffel et al.,2020) and how to group the tasks to help a multi-task model to\nlearn effective representations that can be shared across tasks (Standley et\nal., 2020; Fifty et al., 2021). In this work, we show that a single\nmulti-tasking model can match the performance of task specific models when the\ntask specific models show similar representations across all of their hidden\nlayers and their gradients are aligned, i.e. their gradients follow the same\ndirection. We hypothesize that the above observations explain the effectiveness\nof multi-task learning. We validate our observations on our internal\nradiologist-annotated datasets on the cervical and lumbar spine. Our method is\nsimple and intuitive, and can be used in a wide range of NLP problems.", "published": "2022-05-06 01:51:19", "link": "http://arxiv.org/abs/2205.02979v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Characterizing Multi-Domain False News and Underlying User Effects on\n  Chinese Weibo", "abstract": "False news that spreads on social media has proliferated over the past years\nand has led to multi-aspect threats in the real world. While there are studies\nof false news on specific domains (like politics or health care), little work\nis found comparing false news across domains. In this article, we investigate\nfalse news across nine domains on Weibo, the largest Twitter-like social media\nplatform in China, from 2009 to 2019. The newly collected data comprise 44,728\nposts in the nine domains, published by 40,215 users, and reposted over 3.4\nmillion times. Based on the distributions and spreads of the multi-domain\ndataset, we observe that false news in domains that are close to daily life\nlike health and medicine generated more posts but diffused less effectively\nthan those in other domains like politics, and that political false news had\nthe most effective capacity for diffusion. The widely diffused false news posts\non Weibo were associated strongly with certain types of users -- by gender,\nage, etc. Further, these posts provoked strong emotions in the reposts and\ndiffused further with the active engagement of false-news starters. Our\nfindings have the potential to help design false news detection systems in\nsuspicious news discovery, veracity prediction, and display and explanation.\nThe comparison of the findings on Weibo with those of existing work\ndemonstrates nuanced patterns, suggesting the need for more research on data\nfrom diverse platforms, countries, or languages to tackle the global issue of\nfalse news. The code and new anonymized dataset are available at\nhttps://github.com/ICTMCG/Characterizing-Weibo-Multi-Domain-False-News.", "published": "2022-05-06 08:23:26", "link": "http://arxiv.org/abs/2205.03068v2", "categories": ["cs.SI", "cs.CL", "cs.CY"], "primary_category": "cs.SI"}
{"title": "Federated Learning with Noisy User Feedback", "abstract": "Machine Learning (ML) systems are getting increasingly popular, and drive\nmore and more applications and services in our daily life. This has led to\ngrowing concerns over user privacy, since human interaction data typically\nneeds to be transmitted to the cloud in order to train and improve such\nsystems. Federated learning (FL) has recently emerged as a method for training\nML models on edge devices using sensitive user data and is seen as a way to\nmitigate concerns over data privacy. However, since ML models are most commonly\ntrained with label supervision, we need a way to extract labels on edge to make\nFL viable. In this work, we propose a strategy for training FL models using\npositive and negative user feedback. We also design a novel framework to study\ndifferent noise patterns in user feedback, and explore how well standard\nnoise-robust objectives can help mitigate this noise when training models in a\nfederated setting. We evaluate our proposed training setup through detailed\nexperiments on two text classification datasets and analyze the effects of\nvarying levels of user reliability and feedback noise on model performance. We\nshow that our method improves substantially over a self-training baseline,\nachieving performance closer to models trained with full supervision.", "published": "2022-05-06 09:14:24", "link": "http://arxiv.org/abs/2205.03092v1", "categories": ["cs.LG", "cs.CL", "cs.HC"], "primary_category": "cs.LG"}
{"title": "Emp-RFT: Empathetic Response Generation via Recognizing Feature\n  Transitions between Utterances", "abstract": "Each utterance in multi-turn empathetic dialogues has features such as\nemotion, keywords, and utterance-level meaning. Feature transitions between\nutterances occur naturally. However, existing approaches fail to perceive the\ntransitions because they extract features for the context at the coarse-grained\nlevel. To solve the above issue, we propose a novel approach of recognizing\nfeature transitions between utterances, which helps understand the dialogue\nflow and better grasp the features of utterance that needs attention. Also, we\nintroduce a response generation strategy to help focus on emotion and keywords\nrelated to appropriate features when generating responses. Experimental results\nshow that our approach outperforms baselines and especially, achieves\nsignificant improvements on multi-turn dialogues.", "published": "2022-05-06 09:52:39", "link": "http://arxiv.org/abs/2205.03112v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Collective Relevance Labeling for Passage Retrieval", "abstract": "Deep learning for Information Retrieval (IR) requires a large amount of\nhigh-quality query-document relevance labels, but such labels are inherently\nsparse. Label smoothing redistributes some observed probability mass over\nunobserved instances, often uniformly, uninformed of the true distribution. In\ncontrast, we propose knowledge distillation for informed labeling, without\nincurring high computation overheads at evaluation time. Our contribution is\ndesigning a simple but efficient teacher model which utilizes collective\nknowledge, to outperform state-of-the-arts distilled from a more complex\nteacher model. Specifically, we train up to x8 faster than the state-of-the-art\nteacher, while distilling the rankings better. Our code is publicly available\nat https://github.com/jihyukkim-nlp/CollectiveKD", "published": "2022-05-06 14:47:15", "link": "http://arxiv.org/abs/2205.03273v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Fine-grained Intent Classification in the Legal Domain", "abstract": "A law practitioner has to go through a lot of long legal case proceedings. To\nunderstand the motivation behind the actions of different parties/individuals\nin a legal case, it is essential that the parts of the document that express an\nintent corresponding to the case be clearly understood. In this paper, we\nintroduce a dataset of 93 legal documents, belonging to the case categories of\neither Murder, Land Dispute, Robbery, or Corruption, where phrases expressing\nintent same as the category of the document are annotated. Also, we annotate\nfine-grained intents for each such phrase to enable a deeper understanding of\nthe case for a reader. Finally, we analyze the performance of several\ntransformer-based models in automating the process of extracting intent phrases\n(both at a coarse and a fine-grained level), and classifying a document into\none of the possible 4 categories, and observe that, our dataset is challenging,\nespecially in the case of fine-grained intent classification.", "published": "2022-05-06 23:57:17", "link": "http://arxiv.org/abs/2205.03509v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Musical Score Following and Audio Alignment", "abstract": "Real-time tracking of the position of a musical performance on a musical\nscore, i.e. score following, can be useful in music practice, performance and\nproduction. Example applications of such technology include computer-aided\naccompaniment and automatic page turning. Score following is a challenging\ntask, especially when considering deviations in performance data from the score\nstemming from mistakes or expressive choices.\n  In this project, the extensive research present in the field is first\nexplored before two open-source evaluation testbenches for score following--one\nquantitative and the other qualitative--are introduced. A new way of obtaining\nquantitative testbench data is proposed, and the QualScofo dataset for\nqualitative benchmarking is introduced. Subsequently, three different score\nfollowers, each of a different class, are implemented. First, a beat-based\nfollower for an interactive conductor application--the TuneApp Conductor--is\ncreated to demonstrate an entertaining application of score following. Then, an\nApproximate String Matching (ASM) non-real-time follower is implemented to\ncomplement the quantitative testbench and provide more technical background\ndetails of score following. Finally, a Constant Q-Transform (CQT) Dynamic Time\nWarping (DTW) score follower robust against major challenges in score following\n(such as polyphonic music and performance deviations) is outlined and\nimplemented; it is shown that this CQT-based approach consistently and\nsignificantly outperforms a commonly used FFT-based approach in extracting\naudio features for score following.", "published": "2022-05-06 14:03:51", "link": "http://arxiv.org/abs/2205.03247v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Robustness of Neural Architectures for Audio Event Detection", "abstract": "Traditionally, in Audio Recognition pipeline, noise is suppressed by the\n\"frontend\", relying on preprocessing techniques such as speech enhancement.\nHowever, it is not guaranteed that noise will not cascade into downstream\npipelines. To understand the actual influence of noise on the entire audio\npipeline, in this paper, we directly investigate the impact of noise on a\ndifferent types of neural models without the preprocessing step. We measure the\nrecognition performances of 4 different neural network models on the task of\nenvironment sound classification under the 3 types of noises: \\emph{occlusion}\n(to emulate intermittent noise), \\emph{Gaussian} noise (models continuous\nnoise), and \\emph{adversarial perturbations} (worst case scenario). Our\nintuition is that the different ways in which these models process their input\n(i.e. CNNs have strong locality inductive biases, which Transformers do not\nhave) should lead to observable differences in performance and/ or robustness,\nan understanding of which will enable further improvements. We perform\nextensive experiments on AudioSet which is the largest weakly-labeled sound\nevent dataset available. We also seek to explain the behaviors of different\nmodels through output distribution change and weight visualization.", "published": "2022-05-06 14:43:32", "link": "http://arxiv.org/abs/2205.03268v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sound2Synth: Interpreting Sound via FM Synthesizer Parameters Estimation", "abstract": "Synthesizer is a type of electronic musical instrument that is now widely\nused in modern music production and sound design. Each parameters configuration\nof a synthesizer produces a unique timbre and can be viewed as a unique\ninstrument. The problem of estimating a set of parameters configuration that\nbest restore a sound timbre is an important yet complicated problem, i.e.: the\nsynthesizer parameters estimation problem. We proposed a multi-modal\ndeep-learning-based pipeline Sound2Synth, together with a network structure\nPrime-Dilated Convolution (PDC) specially designed to solve this problem. Our\nmethod achieved not only SOTA but also the first real-world applicable results\non Dexed synthesizer, a popular FM synthesizer.", "published": "2022-05-06 06:55:29", "link": "http://arxiv.org/abs/2205.03043v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Transformer-Based Multi-Aspect Multi-Granularity Non-Native English\n  Speaker Pronunciation Assessment", "abstract": "Automatic pronunciation assessment is an important technology to help\nself-directed language learners. While pronunciation quality has multiple\naspects including accuracy, fluency, completeness, and prosody, previous\nefforts typically only model one aspect (e.g., accuracy) at one granularity\n(e.g., at the phoneme-level). In this work, we explore modeling multi-aspect\npronunciation assessment at multiple granularities. Specifically, we train a\nGoodness Of Pronunciation feature-based Transformer (GOPT) with multi-task\nlearning. Experiments show that GOPT achieves the best results on\nspeechocean762 with a public automatic speech recognition (ASR) acoustic model\ntrained on Librispeech.", "published": "2022-05-06 18:07:44", "link": "http://arxiv.org/abs/2205.03432v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Vocalsound: A Dataset for Improving Human Vocal Sounds Recognition", "abstract": "Recognizing human non-speech vocalizations is an important task and has broad\napplications such as automatic sound transcription and health condition\nmonitoring. However, existing datasets have a relatively small number of vocal\nsound samples or noisy labels. As a consequence, state-of-the-art audio event\nclassification models may not perform well in detecting human vocal sounds. To\nsupport research on building robust and accurate vocal sound recognition, we\nhave created a VocalSound dataset consisting of over 21,000 crowdsourced\nrecordings of laughter, sighs, coughs, throat clearing, sneezes, and sniffs\nfrom 3,365 unique subjects. Experiments show that the vocal sound recognition\nperformance of a model can be significantly improved by 41.9% by adding\nVocalSound dataset to an existing dataset as training material. In addition,\ndifferent from previous datasets, the VocalSound dataset contains meta\ninformation such as speaker age, gender, native language, country, and health\ncondition.", "published": "2022-05-06 18:08:18", "link": "http://arxiv.org/abs/2205.03433v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Conformer-based Waveform-domain Neural Acoustic Echo Canceller\n  Optimized for ASR Accuracy", "abstract": "Acoustic Echo Cancellation (AEC) is essential for accurate recognition of\nqueries spoken to a smart speaker that is playing out audio. Previous work has\nshown that a neural AEC model operating on log-mel spectral features (denoted\n\"logmel\" hereafter) can greatly improve Automatic Speech Recognition (ASR)\naccuracy when optimized with an auxiliary loss utilizing a pre-trained ASR\nmodel encoder. In this paper, we develop a conformer-based waveform-domain\nneural AEC model inspired by the \"TasNet\" architecture. The model is trained by\njointly optimizing Negative Scale-Invariant SNR (SISNR) and ASR losses on a\nlarge speech dataset. On a realistic rerecorded test set, we find that\ncascading a linear adaptive AEC and a waveform-domain neural AEC is very\neffective, giving 56-59% word error rate (WER) reduction over the linear AEC\nalone. On this test set, the 1.6M parameter waveform-domain neural AEC also\nimproves over a larger 6.5M parameter logmel-domain neural AEC model by 20-29%\nin easy to moderate conditions. By operating on smaller frames, the waveform\nneural model is able to perform better at smaller sizes and is better suited\nfor applications where memory is limited.", "published": "2022-05-06 21:35:25", "link": "http://arxiv.org/abs/2205.03481v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
