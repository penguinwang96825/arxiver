{"title": "Zipf's law in 50 languages: its structural pattern, linguistic\n  interpretation, and cognitive motivation", "abstract": "Zipf's law has been found in many human-related fields, including language,\nwhere the frequency of a word is persistently found as a power law function of\nits frequency rank, known as Zipf's law. However, there is much dispute whether\nit is a universal law or a statistical artifact, and little is known about what\nmechanisms may have shaped it. To answer these questions, this study conducted\na large scale cross language investigation into Zipf's law. The statistical\nresults show that Zipf's laws in 50 languages all share a 3-segment structural\npattern, with each segment demonstrating distinctive linguistic properties and\nthe lower segment invariably bending downwards to deviate from theoretical\nexpectation. This finding indicates that this deviation is a fundamental and\nuniversal feature of word frequency distributions in natural languages, not the\nstatistical error of low frequency words. A computer simulation based on the\ndual-process theory yields Zipf's law with the same structural pattern,\nsuggesting that Zipf's law of natural languages are motivated by common\ncognitive mechanisms. These results show that Zipf's law in languages is\nmotivated by cognitive mechanisms like dual-processing that govern human verbal\nbehaviors.", "published": "2018-07-05 06:03:39", "link": "http://arxiv.org/abs/1807.01855v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chinese Lexical Analysis with Deep Bi-GRU-CRF Network", "abstract": "Lexical analysis is believed to be a crucial step towards natural language\nunderstanding and has been widely studied. Recent years, end-to-end lexical\nanalysis models with recurrent neural networks have gained increasing\nattention. In this report, we introduce a deep Bi-GRU-CRF network that jointly\nmodels word segmentation, part-of-speech tagging and named entity recognition\ntasks. We trained the model using several massive corpus pre-tagged by our best\nChinese lexical analysis tool, together with a small, yet high-quality human\nannotated corpus. We conducted balanced sampling between different corpora to\nguarantee the influence of human annotations, and fine-tune the CRF decoding\nlayer regularly during the training progress. As evaluated by linguistic\nexperts, the model achieved a 95.5% accuracy on the test set, roughly 13%\nrelative error reduction over our (previously) best Chinese lexical analysis\ntool. The model is computationally efficient, achieving the speed of 2.3K\ncharacters per second with one thread.", "published": "2018-07-05 07:45:25", "link": "http://arxiv.org/abs/1807.01882v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sanity Check: A Strong Alignment and Information Retrieval Baseline for\n  Question Answering", "abstract": "While increasingly complex approaches to question answering (QA) have been\nproposed, the true gain of these systems, particularly with respect to their\nexpensive training requirements, can be inflated when they are not compared to\nadequate baselines. Here we propose an unsupervised, simple, and fast alignment\nand information retrieval baseline that incorporates two novel contributions: a\n\\textit{one-to-many alignment} between query and document terms and\n\\textit{negative alignment} as a proxy for discriminative information. Our\napproach not only outperforms all conventional baselines as well as many\nsupervised recurrent neural networks, but also approaches the state of the art\nfor supervised systems on three QA datasets. With only three hyperparameters,\nwe achieve 47\\% P@1 on an 8th grade Science QA dataset, 32.9\\% P@1 on a Yahoo!\nanswers QA dataset and 64\\% MAP on WikiQA. We also achieve 26.56\\% and 58.36\\%\non ARC challenge and easy dataset respectively. In addition to including the\nadditional ARC results in this version of the paper, for the ARC easy set only\nwe also experimented with one additional parameter -- number of justifications\nretrieved.", "published": "2018-07-05 03:33:53", "link": "http://arxiv.org/abs/1807.01836v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Feature Assisted bi-directional LSTM Model for Protein-Protein\n  Interaction Identification from Biomedical Texts", "abstract": "Knowledge about protein-protein interactions is essential in understanding\nthe biological processes such as metabolic pathways, DNA replication, and\ntranscription etc. However, a majority of the existing Protein-Protein\nInteraction (PPI) systems are dependent primarily on the scientific literature,\nwhich is yet not accessible as a structured database. Thus, efficient\ninformation extraction systems are required for identifying PPI information\nfrom the large collection of biomedical texts. Most of the existing systems\nmodel the PPI extraction task as a classification problem and are tailored to\nthe handcrafted feature set including domain dependent features. In this paper,\nwe present a novel method based on deep bidirectional long short-term memory\n(B-LSTM) technique that exploits word sequences and dependency path related\ninformation to identify PPI information from text. This model leverages joint\nmodeling of proteins and relations in a single unified framework, which we name\nas Shortest Dependency Path B-LSTM (sdpLSTM) model. We perform experiments on\ntwo popular benchmark PPI datasets, namely AiMed & BioInfer. The evaluation\nshows the F1-score values of 86.45% and 77.35% on AiMed and BioInfer,\nrespectively. Comparisons with the existing systems show that our proposed\napproach attains state-of-the-art performance.", "published": "2018-07-05 19:37:29", "link": "http://arxiv.org/abs/1807.02162v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Neural Language Codes for Multilingual Acoustic Models", "abstract": "Multilingual Speech Recognition is one of the most costly AI problems,\nbecause each language (7,000+) and even different accents require their own\nacoustic models to obtain best recognition performance. Even though they all\nuse the same phoneme symbols, each language and accent imposes its own coloring\nor \"twang\". Many adaptive approaches have been proposed, but they require\nfurther training, additional data and generally are inferior to monolingually\ntrained models. In this paper, we propose a different approach that uses a\nlarge multilingual model that is \\emph{modulated} by the codes generated by an\nancillary network that learns to code useful differences between the \"twangs\"\nor human language.\n  We use Meta-Pi networks to have one network (the language code net) gate the\nactivity of neurons in another (the acoustic model nets). Our results show that\nduring recognition multilingual Meta-Pi networks quickly adapt to the proper\nlanguage coloring without retraining or new data, and perform better than\nmonolingually trained networks. The model was evaluated by training acoustic\nmodeling nets and modulating language code nets jointly and optimize them for\nbest recognition performance.", "published": "2018-07-05 12:15:34", "link": "http://arxiv.org/abs/1807.01956v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Extracting Actionable Knowledge from Domestic Violence Discourses on\n  Social Media", "abstract": "Domestic Violence (DV) is considered as big social issue and there exists a\nstrong relationship between DV and health impacts of the public. Existing\nresearch studies have focused on social media to track and analyse real world\nevents like emerging trends, natural disasters, user sentiment analysis,\npolitical opinions, and health care. However there is less attention given on\nsocial welfare issues like DV and its impact on public health. Recently, the\nvictims of DV turned to social media platforms to express their feelings in the\nform of posts and seek the social and emotional support, for sympathetic\nencouragement, to show compassion and empathy among public. But, it is\ndifficult to mine the actionable knowledge from large conversational datasets\nfrom social media due to the characteristics of high dimensions, short, noisy,\nhuge volume, high velocity, and so on. Hence, this paper will propose a novel\nframework to model and discover the various themes related to DV from the\npublic domain. The proposed framework would possibly provide unprecedentedly\nvaluable information to the public health researchers, national family health\norganizations, government and public with data enrichment and consolidation to\nimprove the social welfare of the community. Thus provides actionable knowledge\nby monitoring and analysing continuous and rich user generated content.", "published": "2018-07-05 03:34:22", "link": "http://arxiv.org/abs/1807.02391v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "A Review of Different Word Embeddings for Sentiment Classification using\n  Deep Learning", "abstract": "The web is loaded with textual content, and Natural Language Processing is a\nstandout amongst the most vital fields in Machine Learning. But when data is\nhuge simple Machine Learning algorithms are not able to handle it and that is\nwhen Deep Learning comes into play which based on Neural Networks. However\nsince neural networks cannot process raw text, we have to change over them\nthrough some diverse strategies of word embedding. This paper demonstrates\nthose distinctive word embedding strategies implemented on an Amazon Review\nDataset, which has two sentiments to be classified: Happy and Unhappy based on\nnumerous customer reviews. Moreover we demonstrate the distinction in accuracy\nwith a discourse about which word embedding to apply when.", "published": "2018-07-05 07:17:21", "link": "http://arxiv.org/abs/1807.02471v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "Denoising Auto-encoder with Recurrent Skip Connections and Residual\n  Regression for Music Source Separation", "abstract": "Convolutional neural networks with skip connections have shown good\nperformance in music source separation. In this work, we propose a denoising\nAuto-encoder with Recurrent skip Connections (ARC). We use 1D convolution along\nthe temporal axis of the time-frequency feature map in all layers of the\nfully-convolutional network. The use of 1D convolution makes it possible to\napply recurrent layers to the intermediate outputs of the convolution layers.\nIn addition, we also propose an enhancement network and a residual regression\nmethod to further improve the separation result. The recurrent skip\nconnections, the enhancement module, and the residual regression all improve\nthe separation quality. The ARC model with residual regression achieves 5.74\nsiganl-to-distoration ratio (SDR) in vocals with MUSDB in SiSEC 2018. We also\nevaluate the ARC model alone on the older dataset DSD100 (used in SiSEC 2016)\nand it achieves 5.91 SDR in vocals.", "published": "2018-07-05 08:54:32", "link": "http://arxiv.org/abs/1807.01898v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
