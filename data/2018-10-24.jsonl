{"title": "Learn to Code-Switch: Data Augmentation using Copy Mechanism on Language\n  Modeling", "abstract": "Building large-scale datasets for training code-switching language models is\nchallenging and very expensive. To alleviate this problem using parallel corpus\nhas been a major workaround. However, existing solutions use linguistic\nconstraints which may not capture the real data distribution. In this work, we\npropose a novel method for learning how to generate code-switching sentences\nfrom parallel corpora. Our model uses a Seq2Seq model in combination with\npointer networks to align and choose words from the monolingual sentences and\nform a grammatical code-switching sentence. In our experiment, we show that by\ntraining a language model using the augmented sentences we improve the\nperplexity score by 10% compared to the LSTM baseline.", "published": "2018-10-24 09:02:17", "link": "http://arxiv.org/abs/1810.10254v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Discriminate Noises for Incorporating External Information\n  in Neural Machine Translation", "abstract": "Previous studies show that incorporating external information could improve\nthe translation quality of Neural Machine Translation (NMT) systems. However,\nthere are inevitably noises in the external information, severely reducing the\nbenefit that the existing methods could receive from the incorporation. To\ntackle the problem, this study pays special attention to the discrimination of\nthe noises during the incorporation. We argue that there exist two kinds of\nnoise in this external information, i.e. global noise and local noise, which\naffect the translations for the whole sentence and for some specific words,\nrespectively. Accordingly, we propose a general framework that learns to\njointly discriminate both the global and local noises, so that the external\ninformation could be better leveraged. Our model is trained on the dataset\nderived from the original parallel corpus without any external labeled data or\nannotation. Experimental results in various real-world scenarios, language\npairs, and neural architectures indicate that discriminating noises contributes\nto significant improvements in translation quality by being able to better\nincorporate the external information, even in very noisy conditions.", "published": "2018-10-24 12:16:55", "link": "http://arxiv.org/abs/1810.10317v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The MeMAD Submission to the IWSLT 2018 Speech Translation Task", "abstract": "This paper describes the MeMAD project entry to the IWSLT Speech Translation\nShared Task, addressing the translation of English audio into German text.\nBetween the pipeline and end-to-end model tracks, we participated only in the\nformer, with three contrastive systems. We tried also the latter, but were not\nable to finish our end-to-end model in time.\n  All of our systems start by transcribing the audio into text through an\nautomatic speech recognition (ASR) model trained on the TED-LIUM English Speech\nRecognition Corpus (TED-LIUM). Afterwards, we feed the transcripts into\nEnglish-German text-based neural machine translation (NMT) models. Our systems\nemploy three different translation models trained on separate training sets\ncompiled from the English-German part of the TED Speech Translation Corpus\n(TED-Trans) and the OpenSubtitles2018 section of the OPUS collection.\n  In this paper, we also describe the experiments leading up to our final\nsystems. Our experiments indicate that using OpenSubtitles2018 in training\nsignificantly improves translation performance. We also experimented with\nvarious pre- and postprocessing routines for the NMT module, but we did not\nhave much success with these.\n  Our best-scoring system attains a BLEU score of 16.45 on the test set for\nthis year's task.", "published": "2018-10-24 12:18:44", "link": "http://arxiv.org/abs/1810.10320v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Image-based Natural Language Understanding Using 2D Convolutional Neural\n  Networks", "abstract": "We propose a new approach to natural language understanding in which we\nconsider the input text as an image and apply 2D Convolutional Neural Networks\nto learn the local and global semantics of the sentences from the variations\nofthe visual patterns of words. Our approach demonstrates that it is possible\nto get semantically meaningful features from images with text without using\noptical character recognition and sequential processing pipelines, techniques\nthat traditional Natural Language Understanding algorithms require. To validate\nour approach, we present results for two applications: text classification and\ndialog modeling. Using a 2D Convolutional Neural Network, we were able to\noutperform the state-of-art accuracy results of non-Latin alphabet-based text\nclassification and achieved promising results for eight text classification\ndatasets. Furthermore, our approach outperformed the memory networks when using\nout of vocabulary entities fromtask 4 of the bAbI dialog dataset.", "published": "2018-10-24 13:46:58", "link": "http://arxiv.org/abs/1810.10401v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Variational Semi-supervised Aspect-term Sentiment Analysis via\n  Transformer", "abstract": "Aspect-term sentiment analysis (ATSA) is a longstanding challenge in natural\nlanguage understanding. It requires fine-grained semantical reasoning about a\ntarget entity appeared in the text. As manual annotation over the aspects is\nlaborious and time-consuming, the amount of labeled data is limited for\nsupervised learning. This paper proposes a semi-supervised method for the ATSA\nproblem by using the Variational Autoencoder based on Transformer (VAET), which\nmodels the latent distribution via variational inference. By disentangling the\nlatent representation into the aspect-specific sentiment and the lexical\ncontext, our method induces the underlying sentiment prediction for the\nunlabeled data, which then benefits the ATSA classifier. Our method is\nclassifier agnostic, i.e., the classifier is an independent module and various\nadvanced supervised models can be integrated. Experimental results are obtained\non the SemEval 2014 task 4 and show that our method is effective with four\nclassical classifiers. The proposed method outperforms two general\nsemisupervised methods and achieves state-of-the-art performance.", "published": "2018-10-24 15:07:19", "link": "http://arxiv.org/abs/1810.10437v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Clinical Concept Extraction with Contextual Word Embedding", "abstract": "Automatic extraction of clinical concepts is an essential step for turning\nthe unstructured data within a clinical note into structured and actionable\ninformation. In this work, we propose a clinical concept extraction model for\nautomatic annotation of clinical problems, treatments, and tests in clinical\nnotes utilizing domain-specific contextual word embedding. A contextual word\nembedding model is first trained on a corpus with a mixture of clinical reports\nand relevant Wikipedia pages in the clinical domain. Next, a bidirectional\nLSTM-CRF model is trained for clinical concept extraction using the contextual\nword embedding model. We tested our proposed model on the I2B2 2010 challenge\ndataset. Our proposed model achieved the best performance among reported\nbaseline models and outperformed the state-of-the-art models by 3.4% in terms\nof F1-score.", "published": "2018-10-24 18:18:19", "link": "http://arxiv.org/abs/1810.10566v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Multilingual Study of Compressive Cross-Language Text Summarization", "abstract": "Cross-Language Text Summarization (CLTS) generates summaries in a language\ndifferent from the language of the source documents. Recent methods use\ninformation from both languages to generate summaries with the most informative\nsentences. However, these methods have performance that can vary according to\nlanguages, which can reduce the quality of summaries. In this paper, we propose\na compressive framework to generate cross-language summaries. In order to\nanalyze performance and especially stability, we tested our system and\nextractive baselines on a dataset available in four languages (English, French,\nPortuguese, and Spanish) to generate English and French summaries. An automatic\nevaluation showed that our method outperformed extractive state-of-art CLTS\nmethods with better and more stable ROUGE scores for all languages.", "published": "2018-10-24 21:58:48", "link": "http://arxiv.org/abs/1810.10639v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting the Semantic Textual Similarity with Siamese CNN and LSTM", "abstract": "Semantic Textual Similarity (STS) is the basis of many applications in\nNatural Language Processing (NLP). Our system combines convolution and\nrecurrent neural networks to measure the semantic similarity of sentences. It\nuses a convolution network to take account of the local context of words and an\nLSTM to consider the global context of sentences. This combination of networks\nhelps to preserve the relevant information of sentences and improves the\ncalculation of the similarity between sentences. Our model has achieved good\nresults and is competitive with the best state-of-the-art systems.", "published": "2018-10-24 22:08:26", "link": "http://arxiv.org/abs/1810.10641v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-level Memory for Task Oriented Dialogs", "abstract": "Recent end-to-end task oriented dialog systems use memory architectures to\nincorporate external knowledge in their dialogs. Current work makes simplifying\nassumptions about the structure of the knowledge base, such as the use of\ntriples to represent knowledge, and combines dialog utterances (context) as\nwell as knowledge base (KB) results as part of the same memory. This causes an\nexplosion in the memory size, and makes the reasoning over memory harder. In\naddition, such a memory design forces hierarchical properties of the data to be\nfit into a triple structure of memory. This requires the memory reader to infer\nrelationships across otherwise connected attributes. In this paper we relax the\nstrong assumptions made by existing architectures and separate memories used\nfor modeling dialog context and KB results. Instead of using triples to store\nKB results, we introduce a novel multi-level memory architecture consisting of\ncells for each query and their corresponding results. The multi-level memory\nfirst addresses queries, followed by results and finally each key-value pair\nwithin a result. We conduct detailed experiments on three publicly available\ntask oriented dialog data sets and we find that our method conclusively\noutperforms current state-of-the-art models. We report a 15-25% increase in\nboth entity F1 and BLEU scores.", "published": "2018-10-24 22:49:32", "link": "http://arxiv.org/abs/1810.10647v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Local Homology of Word Embeddings", "abstract": "Topological data analysis (TDA) has been widely used to make progress on a\nnumber of problems. However, it seems that TDA application in natural language\nprocessing (NLP) is at its infancy. In this paper we try to bridge the gap by\narguing why TDA tools are a natural choice when it comes to analysing word\nembedding data. We describe a parallelisable unsupervised learning algorithm\nbased on local homology of datapoints and show some experimental results on\nword embedding data. We see that local homology of datapoints in word embedding\ndata contains some information that can potentially be used to solve the word\nsense disambiguation problem.", "published": "2018-10-24 00:24:03", "link": "http://arxiv.org/abs/1810.10136v1", "categories": ["math.AT", "cs.CL"], "primary_category": "math.AT"}
{"title": "Resolving Referring Expressions in Images With Labeled Elements", "abstract": "Images may have elements containing text and a bounding box associated with\nthem, for example, text identified via optical character recognition on a\ncomputer screen image, or a natural image with labeled objects. We present an\nend-to-end trainable architecture to incorporate the information from these\nelements and the image to segment/identify the part of the image a natural\nlanguage expression is referring to. We calculate an embedding for each element\nand then project it onto the corresponding location (i.e., the associated\nbounding box) of the image feature map. We show that this architecture gives an\nimprovement in resolving referring expressions, over only using the image, and\nother methods that incorporate the element information. We demonstrate\nexperimental results on the referring expression datasets based on COCO, and on\na webpage image referring expression dataset that we developed.", "published": "2018-10-24 03:22:08", "link": "http://arxiv.org/abs/1810.10165v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Exploiting Deep Representations for Neural Machine Translation", "abstract": "Advanced neural machine translation (NMT) models generally implement encoder\nand decoder as multiple layers, which allows systems to model complex functions\nand capture complicated linguistic structures. However, only the top layers of\nencoder and decoder are leveraged in the subsequent process, which misses the\nopportunity to exploit the useful information embedded in other layers. In this\nwork, we propose to simultaneously expose all of these signals with layer\naggregation and multi-layer attention mechanisms. In addition, we introduce an\nauxiliary regularization term to encourage different layers to capture diverse\ninformation. Experimental results on widely-used WMT14 English-German and WMT17\nChinese-English translation data demonstrate the effectiveness and universality\nof the proposed approach.", "published": "2018-10-24 04:08:22", "link": "http://arxiv.org/abs/1810.10181v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Modeling Localness for Self-Attention Networks", "abstract": "Self-attention networks have proven to be of profound value for its strength\nof capturing global dependencies. In this work, we propose to model localness\nfor self-attention networks, which enhances the ability of capturing useful\nlocal context. We cast localness modeling as a learnable Gaussian bias, which\nindicates the central and scope of the local region to be paid more attention.\nThe bias is then incorporated into the original attention distribution to form\na revised distribution. To maintain the strength of capturing long distance\ndependencies and enhance the ability of capturing short-range dependencies, we\nonly apply localness modeling to lower layers of self-attention networks.\nQuantitative and qualitative analyses on Chinese-English and English-German\ntranslation tasks demonstrate the effectiveness and universality of the\nproposed approach.", "published": "2018-10-24 04:08:25", "link": "http://arxiv.org/abs/1810.10182v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-Head Attention with Disagreement Regularization", "abstract": "Multi-head attention is appealing for the ability to jointly attend to\ninformation from different representation subspaces at different positions. In\nthis work, we introduce a disagreement regularization to explicitly encourage\nthe diversity among multiple attention heads. Specifically, we propose three\ntypes of disagreement regularization, which respectively encourage the\nsubspace, the attended positions, and the output representation associated with\neach attention head to be different from other heads. Experimental results on\nwidely-used WMT14 English-German and WMT17 Chinese-English translation tasks\ndemonstrate the effectiveness and universality of the proposed approach.", "published": "2018-10-24 04:08:27", "link": "http://arxiv.org/abs/1810.10183v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Proof-Theoretic Approach to Scope Ambiguity in Compositional Vector\n  Space Models", "abstract": "We investigate the extent to which compositional vector space models can be\nused to account for scope ambiguity in quantified sentences (of the form \"Every\nman loves some woman\"). Such sentences containing two quantifiers introduce two\nreadings, a direct scope reading and an inverse scope reading. This ambiguity\nhas been treated in a vector space model using bialgebras by (Hedges and\nSadrzadeh, 2016) and (Sadrzadeh, 2016), though without an explanation of the\nmechanism by which the ambiguity arises. We combine a polarised focussed\nsequent calculus for the non-associative Lambek calculus NL, as described in\n(Moortgat and Moot, 2011), with the vector based approach to quantifier scope\nambiguity. In particular, we establish a procedure for obtaining a vector space\nmodel for quantifier scope ambiguity in a derivational way.", "published": "2018-10-24 11:20:02", "link": "http://arxiv.org/abs/1810.10297v2", "categories": ["cs.CL", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Multi-Multi-View Learning: Multilingual and Multi-Representation Entity\n  Typing", "abstract": "Knowledge bases (KBs) are paramount in NLP. We employ multiview learning for\nincreasing accuracy and coverage of entity type information in KBs. We rely on\ntwo metaviews: language and representation. For language, we consider\nhigh-resource and low-resource languages from Wikipedia. For representation, we\nconsider representations based on the context distribution of the entity (i.e.,\non its embedding), on the entity's name (i.e., on its surface form) and on its\ndescription in Wikipedia. The two metaviews language and representation can be\nfreely combined: each pair of language and representation (e.g., German\nembedding, English description, Spanish name) is a distinct view. Our\nexperiments on entity typing with fine-grained classes demonstrate the\neffectiveness of multiview learning. We release MVET, a large multiview - and,\nin particular, multilingual - entity typing dataset we created. Mono- and\nmultilingual fine-grained entity typing systems can be evaluated on this\ndataset.", "published": "2018-10-24 17:08:36", "link": "http://arxiv.org/abs/1810.10499v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FewRel: A Large-Scale Supervised Few-Shot Relation Classification\n  Dataset with State-of-the-Art Evaluation", "abstract": "We present a Few-Shot Relation Classification Dataset (FewRel), consisting of\n70, 000 sentences on 100 relations derived from Wikipedia and annotated by\ncrowdworkers. The relation of each sentence is first recognized by distant\nsupervision methods, and then filtered by crowdworkers. We adapt the most\nrecent state-of-the-art few-shot learning methods for relation classification\nand conduct a thorough evaluation of these methods. Empirical results show that\neven the most competitive few-shot learning models struggle on this task,\nespecially as compared with humans. We also show that a range of different\nreasoning skills are needed to solve our task. These results indicate that\nfew-shot relation classification remains an open problem and still requires\nfurther research. Our detailed analysis points multiple directions for future\nresearch. All details and resources about the dataset and baselines are\nreleased on http://zhuhao.me/fewrel.", "published": "2018-10-24 01:18:08", "link": "http://arxiv.org/abs/1810.10147v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Universal Language Model Fine-Tuning with Subword Tokenization for\n  Polish", "abstract": "Universal Language Model for Fine-tuning [arXiv:1801.06146] (ULMFiT) is one\nof the first NLP methods for efficient inductive transfer learning.\nUnsupervised pretraining results in improvements on many NLP tasks for English.\nIn this paper, we describe a new method that uses subword tokenization to adapt\nULMFiT to languages with high inflection. Our approach results in a new\nstate-of-the-art for the Polish language, taking first place in Task 3 of\nPolEval'18. After further training, our final model outperformed the second\nbest model by 35%. We have open-sourced our pretrained models and code.", "published": "2018-10-24 07:34:45", "link": "http://arxiv.org/abs/1810.10222v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Effective extractive summarization using frequency-filtered entity\n  relationship graphs", "abstract": "Word frequency-based methods for extractive summarization are easy to\nimplement and yield reasonable results across languages. However, they have\nsignificant limitations - they ignore the role of context, they offer uneven\ncoverage of topics in a document, and sometimes are disjointed and hard to\nread. We use a simple premise from linguistic typology - that English sentences\nare complete descriptors of potential interactions between entities, usually in\nthe order subject-verb-object - to address a subset of these difficulties. We\nhave developed a hybrid model of extractive summarization that combines\nword-frequency based keyword identification with information from automatically\ngenerated entity relationship graphs to select sentences for summaries.\nComparative evaluation with word-frequency and topic word-based methods shows\nthat the proposed method is competitive by conventional ROUGE standards, and\nyields moderately more informative summaries on average, as assessed by a large\npanel (N=94) of human raters.", "published": "2018-10-24 14:30:39", "link": "http://arxiv.org/abs/1810.10419v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Text Classification Application: Poet Detection from Poetry", "abstract": "With the widespread use of the internet, the size of the text data increases\nday by day. Poems can be given as an example of the growing text. In this\nstudy, we aim to classify poetry according to poet. Firstly, data set\nconsisting of three different poetry of poets written in English have been\nconstructed. Then, text categorization techniques are implemented on it.\nChi-Square technique are used for feature selection. In addition, five\ndifferent classification algorithms are tried. These algorithms are Sequential\nminimal optimization, Naive Bayes, C4.5 decision tree, Random Forest and\nk-nearest neighbors. Although each classifier showed very different results,\nover the 70% classification success rate was taken by sequential minimal\noptimization technique.", "published": "2018-10-24 17:44:57", "link": "http://arxiv.org/abs/1810.11414v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "The speaker-independent lipreading play-off; a survey of lipreading\n  machines", "abstract": "Lipreading is a difficult gesture classification task. One problem in\ncomputer lipreading is speaker-independence. Speaker-independence means to\nachieve the same accuracy on test speakers not included in the training set as\nspeakers within the training set. Current literature is limited on\nspeaker-independent lipreading, the few independent test speaker accuracy\nscores are usually aggregated within dependent test speaker accuracies for an\naveraged performance. This leads to unclear independent results. Here we\nundertake a systematic survey of experiments with the TCD-TIMIT dataset using\nboth conventional approaches and deep learning methods to provide a series of\nwholly speaker-independent benchmarks and show that the best\nspeaker-independent machine scores 69.58% accuracy with CNN features and an SVM\nclassifier. This is less than state of the art speaker-dependent lipreading\nmachines, but greater than previously reported in independence experiments.", "published": "2018-10-24 20:06:19", "link": "http://arxiv.org/abs/1810.10597v1", "categories": ["cs.CV", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Training neural audio classifiers with few data", "abstract": "We investigate supervised learning strategies that improve the training of\nneural network audio classifiers on small annotated collections. In particular,\nwe study whether (i) a naive regularization of the solution space, (ii)\nprototypical networks, (iii) transfer learning, or (iv) their combination, can\nfoster deep learning models to better leverage a small amount of training\nexamples. To this end, we evaluate (i-iv) for the tasks of acoustic event\nrecognition and acoustic scene classification, considering from 1 to 100\nlabeled examples per class. Results indicate that transfer learning is a\npowerful strategy in such scenarios, but prototypical networks show promising\nresults when one does not count with external or validation data.", "published": "2018-10-24 09:59:17", "link": "http://arxiv.org/abs/1810.10274v3", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
