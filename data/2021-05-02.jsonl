{"title": "Event Argument Extraction using Causal Knowledge Structures", "abstract": "Event Argument extraction refers to the task of extracting structured\ninformation from unstructured text for a particular event of interest. The\nexisting works exhibit poor capabilities to extract causal event arguments like\nReason and After Effects. Furthermore, most of the existing works model this\ntask at a sentence level, restricting the context to a local scope. While it\nmay be effective for short spans of text, for longer bodies of text such as\nnews articles, it has often been observed that the arguments for an event do\nnot necessarily occur in the same sentence as that containing an event trigger.\nTo tackle the issue of argument scattering across sentences, the use of global\ncontext becomes imperative in this task. In our work, we propose an external\nknowledge aided approach to infuse document-level event information to aid the\nextraction of complex event arguments. We develop a causal network for our\nevent-annotated dataset by extracting relevant event causal structures from\nConceptNet and phrases from Wikipedia. We use the extracted event causal\nfeatures in a bi-directional transformer encoder to effectively capture\nlong-range inter-sentence dependencies. We report the effectiveness of our\nproposed approach through both qualitative and quantitative analysis. In this\ntask, we establish our findings on an event annotated dataset in 5 Indian\nlanguages. This dataset adds further complexity to the task by labelling\narguments of entity type (like Time, Place) as well as more complex argument\ntypes (like Reason, After-Effect). Our approach achieves state-of-the-art\nperformance across all the five languages. Since our work does not rely on any\nlanguage-specific features, it can be easily extended to other languages.", "published": "2021-05-02 13:59:07", "link": "http://arxiv.org/abs/2105.00477v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Larger-Scale Transformers for Multilingual Masked Language Modeling", "abstract": "Recent work has demonstrated the effectiveness of cross-lingual language\nmodel pretraining for cross-lingual understanding. In this study, we present\nthe results of two larger multilingual masked language models, with 3.5B and\n10.7B parameters. Our two new models dubbed XLM-R XL and XLM-R XXL outperform\nXLM-R by 1.8% and 2.4% average accuracy on XNLI. Our model also outperforms the\nRoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on\naverage while handling 99 more languages. This suggests pretrained models with\nlarger capacity may obtain both strong performance on high-resource languages\nwhile greatly improving low-resource languages. We make our code and models\npublicly available.", "published": "2021-05-02 23:15:02", "link": "http://arxiv.org/abs/2105.00572v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MathBERT: A Pre-Trained Model for Mathematical Formula Understanding", "abstract": "Large-scale pre-trained models like BERT, have obtained a great success in\nvarious Natural Language Processing (NLP) tasks, while it is still a challenge\nto adapt them to the math-related tasks. Current pre-trained models neglect the\nstructural features and the semantic correspondence between formula and its\ncontext. To address these issues, we propose a novel pre-trained model, namely\n\\textbf{MathBERT}, which is jointly trained with mathematical formulas and\ntheir corresponding contexts. In addition, in order to further capture the\nsemantic-level structural features of formulas, a new pre-training task is\ndesigned to predict the masked formula substructures extracted from the\nOperator Tree (OPT), which is the semantic structural representation of\nformulas. We conduct various experiments on three downstream tasks to evaluate\nthe performance of MathBERT, including mathematical information retrieval,\nformula topic classification and formula headline generation. Experimental\nresults demonstrate that MathBERT significantly outperforms existing methods on\nall those three tasks. Moreover, we qualitatively show that this pre-trained\nmodel effectively captures the semantic-level structural information of\nformulas. To the best of our knowledge, MathBERT is the first pre-trained model\nfor mathematical formula understanding.", "published": "2021-05-02 02:10:31", "link": "http://arxiv.org/abs/2105.00377v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Searchable Hidden Intermediates for End-to-End Models of Decomposable\n  Sequence Tasks", "abstract": "End-to-end approaches for sequence tasks are becoming increasingly popular.\nYet for complex sequence tasks, like speech translation, systems that cascade\nseveral models trained on sub-tasks have shown to be superior, suggesting that\nthe compositionality of cascaded systems simplifies learning and enables\nsophisticated search capabilities. In this work, we present an end-to-end\nframework that exploits compositionality to learn searchable hidden\nrepresentations at intermediate stages of a sequence model using decomposed\nsub-tasks. These hidden intermediates can be improved using beam search to\nenhance the overall performance and can also incorporate external models at\nintermediate stages of the network to re-score or adapt towards out-of-domain\ndata. One instance of the proposed framework is a Multi-Decoder model for\nspeech translation that extracts the searchable hidden intermediates from a\nspeech recognition sub-task. The model demonstrates the aforementioned benefits\nand outperforms the previous state-of-the-art by around +6 and +3 BLEU on the\ntwo test sets of Fisher-CallHome and by around +3 and +4 BLEU on the\nEnglish-German and English-French test sets of MuST-C.", "published": "2021-05-02 23:22:49", "link": "http://arxiv.org/abs/2105.00573v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Intelligent Conversational Android ERICA Applied to Attentive Listening\n  and Job Interview", "abstract": "Following the success of spoken dialogue systems (SDS) in smartphone\nassistants and smart speakers, a number of communicative robots are developed\nand commercialized. Compared with the conventional SDSs designed as a\nhuman-machine interface, interaction with robots is expected to be in a closer\nmanner to talking to a human because of the anthropomorphism and physical\npresence. The goal or task of dialogue may not be information retrieval, but\nthe conversation itself. In order to realize human-level \"long and deep\"\nconversation, we have developed an intelligent conversational android ERICA. We\nset up several social interaction tasks for ERICA, including attentive\nlistening, job interview, and speed dating. To allow for spontaneous,\nincremental multiple utterances, a robust turn-taking model is implemented\nbased on TRP (transition-relevance place) prediction, and a variety of\nbackchannels are generated based on time frame-wise prediction instead of\nIPU-based prediction. We have realized an open-domain attentive listening\nsystem with partial repeats and elaborating questions on focus words as well as\nassessment responses. It has been evaluated with 40 senior people, engaged in\nconversation of 5-7 minutes without a conversation breakdown. It was also\ncompared against the WOZ setting. We have also realized a job interview system\nwith a set of base questions followed by dynamic generation of elaborating\nquestions. It has also been evaluated with student subjects, showing promising\nresults.", "published": "2021-05-02 06:37:23", "link": "http://arxiv.org/abs/2105.00403v1", "categories": ["cs.CL", "cs.AI", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Adapting CRISP-DM for Idea Mining: A Data Mining Process for Generating\n  Ideas Using a Textual Dataset", "abstract": "Data mining project managers can benefit from using standard data mining\nprocess models. The benefits of using standard process models for data mining,\nsuch as the de facto and the most popular, Cross-Industry-Standard-Process\nmodel for Data Mining (CRISP-DM) are reduced cost and time. Also, standard\nmodels facilitate knowledge transfer, reuse of best practices, and minimize\nknowledge requirements. On the other hand, to unlock the potential of\never-growing textual data such as publications, patents, social media data, and\ndocuments of various forms, digital innovation is increasingly needed.\nFurthermore, the introduction of cutting-edge machine learning tools and\ntechniques enable the elicitation of ideas. The processing of unstructured\ntextual data to generate new and useful ideas is referred to as idea mining.\nExisting literature about idea mining merely overlooks the utilization of\nstandard data mining process models. Therefore, the purpose of this paper is to\npropose a reusable model to generate ideas, CRISP-DM, for Idea Mining\n(CRISP-IM). The design and development of the CRISP-IM are done following the\ndesign science approach. The CRISP-IM facilitates idea generation, through the\nuse of Dynamic Topic Modeling (DTM), unsupervised machine learning, and\nsubsequent statistical analysis on a dataset of scholarly articles. The adapted\nCRISP-IM can be used to guide the process of identifying trends using scholarly\nliterature datasets or temporally organized patent or any other textual dataset\nof any domain to elicit ideas. The ex-post evaluation of the CRISP-IM is left\nfor future study.", "published": "2021-05-02 23:24:25", "link": "http://arxiv.org/abs/2105.00574v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
