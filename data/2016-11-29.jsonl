{"title": "Geometry of Compositionality", "abstract": "This paper proposes a simple test for compositionality (i.e., literal usage)\nof a word or phrase in a context-specific way. The test is computationally\nsimple, relying on no external resources and only uses a set of trained word\nvectors. Experiments show that the proposed method is competitive with state of\nthe art and displays high accuracy in context-specific compositionality\ndetection of a variety of natural language phenomena (idiomaticity, sarcasm,\nmetaphor) for different datasets in multiple languages. The key insight is to\nconnect compositionality to a curious geometric property of word embeddings,\nwhich is of independent interest.", "published": "2016-11-29 19:23:41", "link": "http://arxiv.org/abs/1611.09799v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-aware Natural Language Generation with Recurrent Neural Networks", "abstract": "This paper studied generating natural languages at particular contexts or\nsituations. We proposed two novel approaches which encode the contexts into a\ncontinuous semantic representation and then decode the semantic representation\ninto text sequences with recurrent neural networks. During decoding, the\ncontext information are attended through a gating mechanism, addressing the\nproblem of long-range dependency caused by lengthy sequences. We evaluate the\neffectiveness of the proposed approaches on user review data, in which rich\ncontexts are available and two informative contexts, sentiments and products,\nare selected for evaluation. Experiments show that the fake reviews generated\nby our approaches are very natural. Results of fake review detection with human\njudges show that more than 50\\% of the fake reviews are misclassified as the\nreal reviews, and more than 90\\% are misclassified by existing state-of-the-art\nfake review detection algorithm.", "published": "2016-11-29 21:45:42", "link": "http://arxiv.org/abs/1611.09900v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis for Twitter : Going Beyond Tweet Text", "abstract": "Analysing sentiment of tweets is important as it helps to determine the\nusers' opinion. Knowing people's opinion is crucial for several purposes\nstarting from gathering knowledge about customer base, e-governance,\ncampaigning and many more. In this report, we aim to develop a system to detect\nthe sentiment from tweets. We employ several linguistic features along with\nsome other external sources of information to detect the sentiment of a tweet.\nWe show that augmenting the 140 character-long tweet with information harvested\nfrom external urls shared in the tweet as well as Social Media features\nenhances the sentiment prediction accuracy significantly.", "published": "2016-11-29 00:22:13", "link": "http://arxiv.org/abs/1611.09441v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Is a picture worth a thousand words? A Deep Multi-Modal Fusion\n  Architecture for Product Classification in e-commerce", "abstract": "Classifying products into categories precisely and efficiently is a major\nchallenge in modern e-commerce. The high traffic of new products uploaded daily\nand the dynamic nature of the categories raise the need for machine learning\nmodels that can reduce the cost and time of human editors. In this paper, we\npropose a decision level fusion approach for multi-modal product classification\nusing text and image inputs. We train input specific state-of-the-art deep\nneural networks for each input source, show the potential of forging them\ntogether into a multi-modal architecture and train a novel policy network that\nlearns to choose between them. Finally, we demonstrate that our multi-modal\nnetwork improves the top-1 accuracy % over both networks on a real-world\nlarge-scale product classification dataset that we collected fromWalmart.com.\nWhile we focus on image-text fusion that characterizes e-commerce domains, our\nalgorithms can be easily applied to other modalities such as audio, video,\nphysical sensors, etc.", "published": "2016-11-29 09:05:11", "link": "http://arxiv.org/abs/1611.09534v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Semantic Parsing of Mathematics by Context-based Learning from Aligned\n  Corpora and Theorem Proving", "abstract": "We study methods for automated parsing of informal mathematical expressions\ninto formal ones, a main prerequisite for deep computer understanding of\ninformal mathematical texts. We propose a context-based parsing approach that\ncombines efficient statistical learning of deep parse trees with their semantic\npruning by type checking and large-theory automated theorem proving. We show\nthat the methods very significantly improve on previous results in parsing\ntheorems from the Flyspeck corpus.", "published": "2016-11-29 16:20:24", "link": "http://arxiv.org/abs/1611.09703v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dialogue Learning With Human-In-The-Loop", "abstract": "An important aspect of developing conversational agents is to give a bot the\nability to improve through communicating with humans and to learn from the\nmistakes that it makes. Most research has focused on learning from fixed\ntraining sets of labeled data rather than interacting with a dialogue partner\nin an online fashion. In this paper we explore this direction in a\nreinforcement learning setting where the bot improves its question-answering\nability from feedback a teacher gives following its generated responses. We\nbuild a simulator that tests various aspects of such learning in a synthetic\nenvironment, and introduce models that work in this regime. Finally, real\nexperiments with Mechanical Turk validate the approach.", "published": "2016-11-29 20:16:44", "link": "http://arxiv.org/abs/1611.09823v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "NewsQA: A Machine Comprehension Dataset", "abstract": "We present NewsQA, a challenging machine comprehension dataset of over\n100,000 human-generated question-answer pairs. Crowdworkers supply questions\nand answers based on a set of over 10,000 news articles from CNN, with answers\nconsisting of spans of text from the corresponding articles. We collect this\ndataset through a four-stage process designed to solicit exploratory questions\nthat require reasoning. A thorough analysis confirms that NewsQA demands\nabilities beyond simple word matching and recognizing textual entailment. We\nmeasure human performance on the dataset and compare it to several strong\nneural models. The performance gap between humans and machines (0.198 in F1)\nindicates that significant progress can be made on NewsQA through future\nresearch. The dataset is freely available at\nhttps://datasets.maluuba.com/NewsQA.", "published": "2016-11-29 20:38:07", "link": "http://arxiv.org/abs/1611.09830v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Concept Hierarchies through Probabilistic Topic Modeling", "abstract": "With the advent of semantic web, various tools and techniques have been\nintroduced for presenting and organizing knowledge. Concept hierarchies are one\nsuch technique which gained significant attention due to its usefulness in\ncreating domain ontologies that are considered as an integral part of semantic\nweb. Automated concept hierarchy learning algorithms focus on extracting\nrelevant concepts from unstructured text corpus and connect them together by\nidentifying some potential relations exist between them. In this paper, we\npropose a novel approach for identifying relevant concepts from plain text and\nthen learns hierarchy of concepts by exploiting subsumption relation between\nthem. To start with, we model topics using a probabilistic topic model and then\nmake use of some lightweight linguistic process to extract semantically rich\nconcepts. Then we connect concepts by identifying an \"is-a\" relationship\nbetween pair of concepts. The proposed method is completely unsupervised and\nthere is no need for a domain specific training corpus for concept extraction\nand learning. Experiments on large and real-world text corpora such as BBC News\ndataset and Reuters News corpus shows that the proposed method outperforms some\nof the existing methods for concept extraction and efficient concept hierarchy\nlearning is possible if the overall task is guided by a probabilistic topic\nmodeling algorithm.", "published": "2016-11-29 11:28:59", "link": "http://arxiv.org/abs/1611.09573v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Identity-sensitive Word Embedding through Heterogeneous Networks", "abstract": "Most existing word embedding approaches do not distinguish the same words in\ndifferent contexts, therefore ignoring their contextual meanings. As a result,\nthe learned embeddings of these words are usually a mixture of multiple\nmeanings. In this paper, we acknowledge multiple identities of the same word in\ndifferent contexts and learn the \\textbf{identity-sensitive} word embeddings.\nBased on an identity-labeled text corpora, a heterogeneous network of words and\nword identities is constructed to model different-levels of word\nco-occurrences. The heterogeneous network is further embedded into a\nlow-dimensional space through a principled network embedding approach, through\nwhich we are able to obtain the embeddings of words and the embeddings of word\nidentities. We study three different types of word identities including topics,\nsentiments and categories. Experimental results on real-world data sets show\nthat the identity-sensitive word embeddings learned by our approach indeed\ncapture different meanings of words and outperforms competitive methods on\ntasks including text classification and word similarity computation.", "published": "2016-11-29 21:12:04", "link": "http://arxiv.org/abs/1611.09878v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Less is More: Learning Prominent and Diverse Topics for Data\n  Summarization", "abstract": "Statistical topic models efficiently facilitate the exploration of\nlarge-scale data sets. Many models have been developed and broadly used to\nsummarize the semantic structure in news, science, social media, and digital\nhumanities. However, a common and practical objective in data exploration tasks\nis not to enumerate all existing topics, but to quickly extract representative\nones that broadly cover the content of the corpus, i.e., a few topics that\nserve as a good summary of the data. Most existing topic models fit exactly the\nsame number of topics as a user specifies, which have imposed an unnecessary\nburden to the users who have limited prior knowledge. We instead propose new\nmodels that are able to learn fewer but more representative topics for the\npurpose of data summarization. We propose a reinforced random walk that allows\nprominent topics to absorb tokens from similar and smaller topics, thus\nenhances the diversity among the top topics extracted. With this reinforced\nrandom walk as a general process embedded in classical topic models, we obtain\n\\textit{diverse topic models} that are able to extract the most prominent and\ndiverse topics from data. The inference procedures of these diverse topic\nmodels remain as simple and efficient as the classical models. Experimental\nresults demonstrate that the diverse topic models not only discover topics that\nbetter summarize the data, but also require minimal prior knowledge of the\nusers.", "published": "2016-11-29 22:24:30", "link": "http://arxiv.org/abs/1611.09921v2", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
