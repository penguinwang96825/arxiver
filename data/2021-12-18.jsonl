{"title": "Assessing Post-editing Effort in the English-Hindi Direction", "abstract": "We present findings from a first in-depth post-editing effort estimation\nstudy in the English-Hindi direction along multiple effort indicators. We\nconduct a controlled experiment involving professional translators, who\ncomplete assigned tasks alternately, in a translation from scratch and a\npost-edit condition. We find that post-editing reduces translation time (by\n63%), utilizes fewer keystrokes (by 59%), and decreases the number of pauses\n(by 63%) when compared to translating from scratch. We further verify the\nquality of translations thus produced via a human evaluation task in which we\ndo not detect any discernible quality differences.", "published": "2021-12-18 04:26:19", "link": "http://arxiv.org/abs/2112.09841v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Graph Guided Summarization for Radiology Findings", "abstract": "Radiology reports play a critical role in communicating medical findings to\nphysicians. In each report, the impression section summarizes essential\nradiology findings. In clinical practice, writing impression is highly demanded\nyet time-consuming and prone to errors for radiologists. Therefore, automatic\nimpression generation has emerged as an attractive research direction to\nfacilitate such clinical practice. Existing studies mainly focused on\nintroducing salient word information to the general text summarization\nframework to guide the selection of the key content in radiology findings.\nHowever, for this task, a model needs not only capture the important words in\nfindings but also accurately describe their relations so as to generate\nhigh-quality impressions. In this paper, we propose a novel method for\nautomatic impression generation, where a word graph is constructed from the\nfindings to record the critical words and their relations, then a Word Graph\nguided Summarization model (WGSum) is designed to generate impressions with the\nhelp of the word graph. Experimental results on two datasets, OpenI and\nMIMIC-CXR, confirm the validity and effectiveness of our proposed approach,\nwhere the state-of-the-art results are achieved on both datasets. Further\nexperiments are also conducted to analyze the impact of different graph designs\nto the performance of our method.", "published": "2021-12-18 13:20:18", "link": "http://arxiv.org/abs/2112.09925v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Morpheme Boundary Detection & Grammatical Feature Prediction for\n  Gujarati : Dataset & Model", "abstract": "Developing Natural Language Processing resources for a low resource language\nis a challenging but essential task. In this paper, we present a Morphological\nAnalyzer for Gujarati. We have used a Bi-Directional LSTM based approach to\nperform morpheme boundary detection and grammatical feature tagging. We have\ncreated a data set of Gujarati words with lemma and grammatical features. The\nBi-LSTM based model of Morph Analyzer discussed in the paper handles the\nlanguage morphology effectively without the knowledge of any hand-crafted\nsuffix rules. To the best of our knowledge, this is the first dataset and morph\nanalyzer model for the Gujarati language which performs both grammatical\nfeature tagging and morpheme boundary detection tasks.", "published": "2021-12-18 06:58:36", "link": "http://arxiv.org/abs/2112.09860v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF"], "primary_category": "cs.CL"}
{"title": "The Web Is Your Oyster - Knowledge-Intensive NLP against a Very Large\n  Web Corpus", "abstract": "In order to address increasing demands of real-world applications, the\nresearch for knowledge-intensive NLP (KI-NLP) should advance by capturing the\nchallenges of a truly open-domain environment: web-scale knowledge, lack of\nstructure, inconsistent quality and noise. To this end, we propose a new setup\nfor evaluating existing knowledge intensive tasks in which we generalize the\nbackground corpus to a universal web snapshot. We investigate a slate of NLP\ntasks which rely on knowledge - either factual or common sense, and ask systems\nto use a subset of CCNet - the Sphere corpus - as a knowledge source. In\ncontrast to Wikipedia, otherwise a common background corpus in KI-NLP, Sphere\nis orders of magnitude larger and better reflects the full diversity of\nknowledge on the web. Despite potential gaps in coverage, challenges of scale,\nlack of structure and lower quality, we find that retrieval from Sphere enables\na state of the art system to match and even outperform Wikipedia-based models\non several tasks. We also observe that while a dense index can outperform a\nsparse BM25 baseline on Wikipedia, on Sphere this is not yet possible. To\nfacilitate further research and minimise the community's reliance on\nproprietary, black-box search engines, we share our indices, evaluation metrics\nand infrastructure.", "published": "2021-12-18 13:15:34", "link": "http://arxiv.org/abs/2112.09924v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Syntactic-GCN Bert based Chinese Event Extraction", "abstract": "With the rapid development of information technology, online platforms (e.g.,\nnews portals and social media) generate enormous web information every moment.\nTherefore, it is crucial to extract structured representations of events from\nsocial streams. Generally, existing event extraction research utilizes pattern\nmatching, machine learning, or deep learning methods to perform event\nextraction tasks. However, the performance of Chinese event extraction is not\nas good as English due to the unique characteristics of the Chinese language.\nIn this paper, we propose an integrated framework to perform Chinese event\nextraction. The proposed approach is a multiple channel input neural framework\nthat integrates semantic features and syntactic features. The semantic features\nare captured by BERT architecture. The Part of Speech (POS) features and\nDependency Parsing (DP) features are captured by profiling embeddings and Graph\nConvolutional Network (GCN), respectively. We also evaluate our model on a\nreal-world dataset. Experimental results show that the proposed method\noutperforms the benchmark approaches significantly.", "published": "2021-12-18 14:07:54", "link": "http://arxiv.org/abs/2112.09939v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "stat.AP"], "primary_category": "cs.CL"}
{"title": "Leveraging Transformers for Hate Speech Detection in Conversational\n  Code-Mixed Tweets", "abstract": "In the current era of the internet, where social media platforms are easily\naccessible for everyone, people often have to deal with threats, identity\nattacks, hate, and bullying due to their association with a cast, creed,\ngender, religion, or even acceptance or rejection of a notion. Existing works\nin hate speech detection primarily focus on individual comment classification\nas a sequence labeling task and often fail to consider the context of the\nconversation. The context of a conversation often plays a substantial role when\ndetermining the author's intent and sentiment behind the tweet. This paper\ndescribes the system proposed by team MIDAS-IIITD for HASOC 2021 subtask 2, one\nof the first shared tasks focusing on detecting hate speech from Hindi-English\ncode-mixed conversations on Twitter. We approach this problem using neural\nnetworks, leveraging the transformer's cross-lingual embeddings and further\nfinetuning them for low-resource hate-speech classification in transliterated\nHindi text. Our best performing system, a hard voting ensemble of Indic-BERT,\nXLM-RoBERTa, and Multilingual BERT, achieved a macro F1 score of 0.7253,\nplacing us first on the overall leaderboard standings.", "published": "2021-12-18 19:27:33", "link": "http://arxiv.org/abs/2112.09986v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Continual Learning with Knowledge Transfer for Sentiment Classification", "abstract": "This paper studies continual learning (CL) for sentiment classification (SC).\nIn this setting, the CL system learns a sequence of SC tasks incrementally in a\nneural network, where each task builds a classifier to classify the sentiment\nof reviews of a particular product category or domain. Two natural questions\nare: Can the system transfer the knowledge learned in the past from the\nprevious tasks to the new task to help it learn a better model for the new\ntask? And, can old models for previous tasks be improved in the process as\nwell? This paper proposes a novel technique called KAN to achieve these\nobjectives. KAN can markedly improve the SC accuracy of both the new task and\nthe old tasks via forward and backward knowledge transfer. The effectiveness of\nKAN is demonstrated through extensive experiments.", "published": "2021-12-18 22:58:21", "link": "http://arxiv.org/abs/2112.10021v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Cascading Adaptors to Leverage English Data to Improve Performance of\n  Question Answering for Low-Resource Languages", "abstract": "Transformer based architectures have shown notable results on many down\nstreaming tasks including question answering. The availability of data, on the\nother hand, impedes obtaining legitimate performance for low-resource\nlanguages. In this paper, we investigate the applicability of pre-trained\nmultilingual models to improve the performance of question answering in\nlow-resource languages. We tested four combinations of language and task\nadapters using multilingual transformer architectures on seven languages\nsimilar to MLQA dataset. Additionally, we have also proposed zero-shot transfer\nlearning of low-resource question answering using language and task adapters.\nWe observed that stacking the language and the task adapters improves the\nmultilingual transformer models' performance significantly for low-resource\nlanguages.", "published": "2021-12-18 07:40:37", "link": "http://arxiv.org/abs/2112.09866v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Noisy Speech Based Temporal Decomposition to Improve Fundamental\n  Frequency Estimation", "abstract": "This paper introduces a novel method to separate noisy speech into low or\nhigh frequency frames, in order to improve fundamental frequency (F0)\nestimation accuracy. In this proposal, the target signal is analyzed by means\nof the ensemble empirical mode decomposition. Next, the pitch information is\nextracted from the first decomposition modes. This feature indicates the\nfrequency region where the F0 of speech should be located, thus separating the\nframes into low-frequency (LF) or high-frequency (HF). The separation is\napplied to correct candidates extracted from a conventional fundamental\nfrequency detection method, and hence improving the accuracy of F0 estimate.\nThe proposed method is evaluated in experiments with CSTR and TIMIT databases,\nconsidering six acoustic noises under various signal-to-noise ratios. A pitch\nenhancement algorithm is adopted as baseline in the evaluation analysis\nconsidering three conventional estimators. Results show that the proposed\nmethod outperforms the competing strategies, in terms of low/high frequency\nseparation accuracy. Moreover, the performance metrics of the F0 estimation\ntechniques show that the novel solution is able to better improve F0 detection\naccuracy when compared to competitive approaches under different noisy\nconditions.", "published": "2021-12-18 10:31:43", "link": "http://arxiv.org/abs/2112.09896v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
