{"title": "Learning Multiscale Transformer Models for Sequence Generation", "abstract": "Multiscale feature hierarchies have been witnessed the success in the\ncomputer vision area. This further motivates researchers to design multiscale\nTransformer for natural language processing, mostly based on the self-attention\nmechanism. For example, restricting the receptive field across heads or\nextracting local fine-grained features via convolutions. However, most of\nexisting works directly modeled local features but ignored the word-boundary\ninformation. This results in redundant and ambiguous attention distributions,\nwhich lacks of interpretability. In this work, we define those scales in\ndifferent linguistic units, including sub-words, words and phrases. We built a\nmultiscale Transformer model by establishing relationships among scales based\non word-boundary information and phrase-level prior knowledge. The proposed\n\\textbf{U}niversal \\textbf{M}ulti\\textbf{S}cale \\textbf{T}ransformer, namely\n\\textsc{Umst}, was evaluated on two sequence generation tasks. Notably, it\nyielded consistent performance gains over the strong baseline on several test\nsets without sacrificing the efficiency.", "published": "2022-06-19 07:28:54", "link": "http://arxiv.org/abs/2206.09337v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MME-CRS: Multi-Metric Evaluation Based on Correlation Re-Scaling for\n  Evaluating Open-Domain Dialogue", "abstract": "Automatic open-domain dialogue evaluation is a crucial component of dialogue\nsystems. Recently, learning-based evaluation metrics have achieved\nstate-of-the-art performance in open-domain dialogue evaluation. However, these\nmetrics, which only focus on a few qualities, are hard to evaluate dialogue\ncomprehensively. Furthermore, these metrics lack an effective score composition\napproach for diverse evaluation qualities. To address the above problems, we\npropose a Multi-Metric Evaluation based on Correlation Re-Scaling (MME-CRS) for\nevaluating open-domain dialogue. Firstly, we build an evaluation metric\ncomposed of 5 groups of parallel sub-metrics called Multi-Metric Evaluation\n(MME) to evaluate the quality of dialogue comprehensively. Furthermore, we\npropose a novel score composition method called Correlation Re-Scaling (CRS) to\nmodel the relationship between sub-metrics and diverse qualities. Our approach\nMME-CRS ranks first on the final test data of DSTC10 track5 subtask1 Automatic\nOpen-domain Dialogue Evaluation Challenge with a large margin, which proved the\neffectiveness of our proposed approach.", "published": "2022-06-19 13:43:59", "link": "http://arxiv.org/abs/2206.09403v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Unified Understanding of Deep NLP Models for Text Classification", "abstract": "The rapid development of deep natural language processing (NLP) models for\ntext classification has led to an urgent need for a unified understanding of\nthese models proposed individually. Existing methods cannot meet the need for\nunderstanding different models in one framework due to the lack of a unified\nmeasure for explaining both low-level (e.g., words) and high-level (e.g.,\nphrases) features. We have developed a visual analysis tool, DeepNLPVis, to\nenable a unified understanding of NLP models for text classification. The key\nidea is a mutual information-based measure, which provides quantitative\nexplanations on how each layer of a model maintains the information of input\nwords in a sample. We model the intra- and inter-word information at each layer\nmeasuring the importance of a word to the final prediction as well as the\nrelationships between words, such as the formation of phrases. A multi-level\nvisualization, which consists of a corpus-level, a sample-level, and a\nword-level visualization, supports the analysis from the overall training set\nto individual samples. Two case studies on classification tasks and comparison\nbetween models demonstrate that DeepNLPVis can help users effectively identify\npotential problems caused by samples and model architectures and then make\ninformed improvements.", "published": "2022-06-19 08:55:07", "link": "http://arxiv.org/abs/2206.09355v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Self-Guided Framework for Radiology Report Generation", "abstract": "Automatic radiology report generation is essential to computer-aided\ndiagnosis. Through the success of image captioning, medical report generation\nhas been achievable. However, the lack of annotated disease labels is still the\nbottleneck of this area. In addition, the image-text data bias problem and\ncomplex sentences make it more difficult to generate accurate reports. To\naddress these gaps, we pre-sent a self-guided framework (SGF), a suite of\nunsupervised and supervised deep learning methods to mimic the process of human\nlearning and writing. In detail, our framework obtains the domain knowledge\nfrom medical reports with-out extra disease labels and guides itself to extract\nfined-grain visual features as-sociated with the text. Moreover, SGF\nsuccessfully improves the accuracy and length of medical report generation by\nincorporating a similarity comparison mechanism that imitates the process of\nhuman self-improvement through compar-ative practice. Extensive experiments\ndemonstrate the utility of our SGF in the majority of cases, showing its\nsuperior performance over state-of-the-art meth-ods. Our results highlight the\ncapacity of the proposed framework to distinguish fined-grained visual details\nbetween words and verify its advantage in generating medical reports.", "published": "2022-06-19 11:09:27", "link": "http://arxiv.org/abs/2206.09378v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Towards Unified Conversational Recommender Systems via\n  Knowledge-Enhanced Prompt Learning", "abstract": "Conversational recommender systems (CRS) aim to proactively elicit user\npreference and recommend high-quality items through natural language\nconversations. Typically, a CRS consists of a recommendation module to predict\npreferred items for users and a conversation module to generate appropriate\nresponses. To develop an effective CRS, it is essential to seamlessly integrate\nthe two modules. Existing works either design semantic alignment strategies, or\nshare knowledge resources and representations between the two modules. However,\nthese approaches still rely on different architectures or techniques to develop\nthe two modules, making it difficult for effective module integration.\n  To address this problem, we propose a unified CRS model named UniCRS based on\nknowledge-enhanced prompt learning. Our approach unifies the recommendation and\nconversation subtasks into the prompt learning paradigm, and utilizes\nknowledge-enhanced prompts based on a fixed pre-trained language model (PLM) to\nfulfill both subtasks in a unified approach. In the prompt design, we include\nfused knowledge representations, task-specific soft tokens, and the dialogue\ncontext, which can provide sufficient contextual information to adapt the PLM\nfor the CRS task. Besides, for the recommendation subtask, we also incorporate\nthe generated response template as an important part of the prompt, to enhance\nthe information interaction between the two subtasks. Extensive experiments on\ntwo public CRS datasets have demonstrated the effectiveness of our approach.", "published": "2022-06-19 09:21:27", "link": "http://arxiv.org/abs/2206.09363v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Towards Adversarial Attack on Vision-Language Pre-training Models", "abstract": "While vision-language pre-training model (VLP) has shown revolutionary\nimprovements on various vision-language (V+L) tasks, the studies regarding its\nadversarial robustness remain largely unexplored. This paper studied the\nadversarial attack on popular VLP models and V+L tasks. First, we analyzed the\nperformance of adversarial attacks under different settings. By examining the\ninfluence of different perturbed objects and attack targets, we concluded some\nkey observations as guidance on both designing strong multimodal adversarial\nattack and constructing robust VLP models. Second, we proposed a novel\nmultimodal attack method on the VLP models called Collaborative Multimodal\nAdversarial Attack (Co-Attack), which collectively carries out the attacks on\nthe image modality and the text modality. Experimental results demonstrated\nthat the proposed method achieves improved attack performances on different V+L\ndownstream tasks and VLP models. The analysis observations and novel attack\nmethod hopefully provide new understanding into the adversarial robustness of\nVLP models, so as to contribute their safe and reliable deployment in more\nreal-world scenarios. Code is available at\nhttps://github.com/adversarial-for-goodness/Co-Attack.", "published": "2022-06-19 12:55:45", "link": "http://arxiv.org/abs/2206.09391v2", "categories": ["cs.LG", "cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.LG"}
{"title": "Transfer Learning for Robust Low-Resource Children's Speech ASR with\n  Transformers and Source-Filter Warping", "abstract": "Automatic Speech Recognition (ASR) systems are known to exhibit difficulties\nwhen transcribing children's speech. This can mainly be attributed to the\nabsence of large children's speech corpora to train robust ASR models and the\nresulting domain mismatch when decoding children's speech with systems trained\non adult data. In this paper, we propose multiple enhancements to alleviate\nthese issues. First, we propose a data augmentation technique based on the\nsource-filter model of speech to close the domain gap between adult and\nchildren's speech. This enables us to leverage the data availability of adult\nspeech corpora by making these samples perceptually similar to children's\nspeech. Second, using this augmentation strategy, we apply transfer learning on\na Transformer model pre-trained on adult data. This model follows the recently\nintroduced XLS-R architecture, a wav2vec 2.0 model pre-trained on several\ncross-lingual adult speech corpora to learn general and robust acoustic\nframe-level representations. Adopting this model for the ASR task using adult\ndata augmented with the proposed source-filter warping strategy and a limited\namount of in-domain children's speech significantly outperforms previous\nstate-of-the-art results on the PF-STAR British English Children's Speech\ncorpus with a 4.86% WER on the official test set.", "published": "2022-06-19 12:57:47", "link": "http://arxiv.org/abs/2206.09396v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Universal Adversarial Policy for Text Classifiers", "abstract": "Discovering the existence of universal adversarial perturbations had large\ntheoretical and practical impacts on the field of adversarial learning. In the\ntext domain, most universal studies focused on adversarial prefixes which are\nadded to all texts. However, unlike the vision domain, adding the same\nperturbation to different inputs results in noticeably unnatural inputs.\nTherefore, we introduce a new universal adversarial setup - a universal\nadversarial policy, which has many advantages of other universal attacks but\nalso results in valid texts - thus making it relevant in practice. We achieve\nthis by learning a single search policy over a predefined set of semantics\npreserving text alterations, on many texts. This formulation is universal in\nthat the policy is successful in finding adversarial examples on new texts\nefficiently. Our approach uses text perturbations which were extensively shown\nto produce natural attacks in the non-universal setup (specific synonym\nreplacements). We suggest a strong baseline approach for this formulation which\nuses reinforcement learning. It's ability to generalise (from as few as 500\ntraining texts) shows that universal adversarial patterns exist in the text\ndomain as well.", "published": "2022-06-19 17:55:47", "link": "http://arxiv.org/abs/2206.09458v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "GMM based multi-stage Wiener filtering for low SNR speech enhancement", "abstract": "This paper proposes a single-channel speech enhancement method to reduce the\nnoise and enhance speech at low signal-to-noise ratio (SNR) levels and\nnon-stationary noise conditions. Specifically, we focus on modeling the noise\nusing a Gaussian mixture model (GMM) based on a multi-stage process with a\nparametric Wiener filter. The proposed noise model estimates a more accurate\nnoise power spectral density (PSD), and allows for better generalization under\nvarious noise conditions compared to traditional Wiener filtering methods.\nSimulations show that the proposed approach can achieve better performance in\nterms of speech quality (PESQ) and intelligibility (STOI) at low SNR levels.", "published": "2022-06-19 00:03:18", "link": "http://arxiv.org/abs/2206.09298v2", "categories": ["cs.SD", "cs.RO", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Resource-Efficient Separation Transformer", "abstract": "Transformers have recently achieved state-of-the-art performance in speech\nseparation. These models, however, are computationally demanding and require a\nlot of learnable parameters. This paper explores Transformer-based speech\nseparation with a reduced computational cost. Our main contribution is the\ndevelopment of the Resource-Efficient Separation Transformer (RE-SepFormer), a\nself-attention-based architecture that reduces the computational burden in two\nways. First, it uses non-overlapping blocks in the latent space. Second, it\noperates on compact latent summaries calculated from each chunk. The\nRE-SepFormer reaches a competitive performance on the popular WSJ0-2Mix and\nWHAM! datasets in both causal and non-causal settings. Remarkably, it scales\nsignificantly better than the previous Transformer-based architectures in terms\nof memory and inference time, making it more suitable for processing long\nmixtures.", "published": "2022-06-19 23:37:24", "link": "http://arxiv.org/abs/2206.09507v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
