{"title": "Personalized Machine Translation: Preserving Original Author Traits", "abstract": "The language that we produce reflects our personality, and various personal\nand demographic characteristics can be detected in natural language texts. We\nfocus on one particular personal trait of the author, gender, and study how it\nis manifested in original texts and in translations. We show that author's\ngender has a powerful, clear signal in originals texts, but this signal is\nobfuscated in human and machine translation. We then propose simple\ndomain-adaptation techniques that help retain the original gender traits in the\ntranslation, without harming the quality of the translation, thereby creating\nmore personalized machine translation systems.", "published": "2016-10-18 07:39:40", "link": "http://arxiv.org/abs/1610.05461v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Addressing Community Question Answering in English and Arabic", "abstract": "This paper studies the impact of different types of features applied to\nlearning to re-rank questions in community Question Answering. We tested our\nmodels on two datasets released in SemEval-2016 Task 3 on \"Community Question\nAnswering\". Task 3 targeted real-life Web fora both in English and Arabic. Our\nmodels include bag-of-words features (BoW), syntactic tree kernels (TKs), rank\nfeatures, embeddings, and machine translation evaluation features. To the best\nof our knowledge, structural kernels have barely been applied to the question\nreranking task, where they have to model paraphrase relations. In the case of\nthe English question re-ranking task, we compare our learning to rank (L2R)\nalgorithms against a strong baseline given by the Google-generated ranking\n(GR). The results show that i) the shallow structures used in our TKs are\nrobust enough to noisy data and ii) improving GR is possible, but effective BoW\nfeatures and TKs along with an accurate model of GR features in the used L2R\nalgorithm are required. In the case of the Arabic question re-ranking task, for\nthe first time we applied tree kernels on syntactic trees of Arabic sentences.\nOur approaches to both tasks obtained the second best results on SemEval-2016\nsubtasks B on English and D on Arabic.", "published": "2016-10-18 10:22:46", "link": "http://arxiv.org/abs/1610.05522v1", "categories": ["cs.CL", "I.2.7; H.3.4"], "primary_category": "cs.CL"}
{"title": "SYSTRAN's Pure Neural Machine Translation Systems", "abstract": "Since the first online demonstration of Neural Machine Translation (NMT) by\nLISA, NMT development has recently moved from laboratory to production systems\nas demonstrated by several entities announcing roll-out of NMT engines to\nreplace their existing technologies. NMT systems have a large number of\ntraining configurations and the training process of such systems is usually\nvery long, often a few weeks, so role of experimentation is critical and\nimportant to share. In this work, we present our approach to production-ready\nsystems simultaneously with release of online demonstrators covering a large\nvariety of languages (12 languages, for 32 language pairs). We explore\ndifferent practical choices: an efficient and evolutive open-source framework;\ndata preparation; network architecture; additional implemented features; tuning\nfor production; etc. We discuss about evaluation methodology, present our first\nfindings and we finally outline further work.\n  Our ultimate goal is to share our expertise to build competitive production\nsystems for \"generic\" translation. We aim at contributing to set up a\ncollaborative framework to speed-up adoption of the technology, foster further\nresearch efforts and enable the delivery and adoption to/by industry of\nuse-case specific engines integrated in real production workflows. Mastering of\nthe technology would allow us to build translation engines suited for\nparticular needs, outperforming current simplest/uniform systems.", "published": "2016-10-18 11:32:42", "link": "http://arxiv.org/abs/1610.05540v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Vietnamese Named Entity Recognition using Token Regular Expressions and\n  Bidirectional Inference", "abstract": "This paper describes an efficient approach to improve the accuracy of a named\nentity recognition system for Vietnamese. The approach combines regular\nexpressions over tokens and a bidirectional inference method in a sequence\nlabelling model. The proposed method achieves an overall $F_1$ score of 89.66%\non a test set of an evaluation campaign, organized in late 2016 by the\nVietnamese Language and Speech Processing (VLSP) community.", "published": "2016-10-18 14:44:01", "link": "http://arxiv.org/abs/1610.05652v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The infochemical core", "abstract": "Vocalizations and less often gestures have been the object of linguistic\nresearch over decades. However, the development of a general theory of\ncommunication with human language as a particular case requires a clear\nunderstanding of the organization of communication through other means.\nInfochemicals are chemical compounds that carry information and are employed by\nsmall organisms that cannot emit acoustic signals of optimal frequency to\nachieve successful communication. Here the distribution of infochemicals across\nspecies is investigated when they are ranked by their degree or the number of\nspecies with which it is associated (because they produce or they are sensitive\nto it). The quality of the fit of different functions to the dependency between\ndegree and rank is evaluated with a penalty for the number of parameters of the\nfunction. Surprisingly, a double Zipf (a Zipf distribution with two regimes\nwith a different exponent each) is the model yielding the best fit although it\nis the function with the largest number of parameters. This suggests that the\nworld wide repertoire of infochemicals contains a chemical nucleus shared by\nmany species and reminiscent of the core vocabularies found for human language\nin dictionaries or large corpora.", "published": "2016-10-18 14:53:20", "link": "http://arxiv.org/abs/1610.05654v2", "categories": ["q-bio.NC", "cs.CL"], "primary_category": "q-bio.NC"}
{"title": "Stylometric Analysis of Early Modern Period English Plays", "abstract": "Function word adjacency networks (WANs) are used to study the authorship of\nplays from the Early Modern English period. In these networks, nodes are\nfunction words and directed edges between two nodes represent the relative\nfrequency of directed co-appearance of the two words. For every analyzed play,\na WAN is constructed and these are aggregated to generate author profile\nnetworks. We first study the similarity of writing styles between Early English\nplaywrights by comparing the profile WANs. The accuracy of using WANs for\nauthorship attribution is then demonstrated by attributing known plays among\nsix popular playwrights. Moreover, the WAN method is shown to outperform other\nfrequency-based methods on attributing Early English plays. In addition, WANs\nare shown to be reliable classifiers even when attributing collaborative plays.\nFor several plays of disputed co-authorship, a deeper analysis is performed by\nattributing every act and scene separately, in which we both corroborate\nexisting breakdowns and provide evidence of new assignments.", "published": "2016-10-18 15:22:14", "link": "http://arxiv.org/abs/1610.05670v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Small-footprint Highway Deep Neural Networks for Speech Recognition", "abstract": "State-of-the-art speech recognition systems typically employ neural network\nacoustic models. However, compared to Gaussian mixture models, deep neural\nnetwork (DNN) based acoustic models often have many more model parameters,\nmaking it challenging for them to be deployed on resource-constrained\nplatforms, such as mobile devices. In this paper, we study the application of\nthe recently proposed highway deep neural network (HDNN) for training\nsmall-footprint acoustic models. HDNNs are a depth-gated feedforward neural\nnetwork, which include two types of gate functions to facilitate the\ninformation flow through different layers. Our study demonstrates that HDNNs\nare more compact than regular DNNs for acoustic modeling, i.e., they can\nachieve comparable recognition accuracy with many fewer model parameters.\nFurthermore, HDNNs are more controllable than DNNs: the gate functions of an\nHDNN can control the behavior of the whole network using a very small number of\nmodel parameters. Finally, we show that HDNNs are more adaptable than DNNs. For\nexample, simply updating the gate functions using adaptation data can result in\nconsiderable gains in accuracy. We demonstrate these aspects by experiments\nusing the publicly available AMI corpus, which has around 80 hours of training\ndata.", "published": "2016-10-18 22:06:01", "link": "http://arxiv.org/abs/1610.05812v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Low-rank and Sparse Soft Targets to Learn Better DNN Acoustic Models", "abstract": "Conventional deep neural networks (DNN) for speech acoustic modeling rely on\nGaussian mixture models (GMM) and hidden Markov model (HMM) to obtain binary\nclass labels as the targets for DNN training. Subword classes in speech\nrecognition systems correspond to context-dependent tied states or senones. The\npresent work addresses some limitations of GMM-HMM senone alignments for DNN\ntraining. We hypothesize that the senone probabilities obtained from a DNN\ntrained with binary labels can provide more accurate targets to learn better\nacoustic models. However, DNN outputs bear inaccuracies which are exhibited as\nhigh dimensional unstructured noise, whereas the informative components are\nstructured and low-dimensional. We exploit principle component analysis (PCA)\nand sparse coding to characterize the senone subspaces. Enhanced probabilities\nobtained from low-rank and sparse reconstructions are used as soft-targets for\nDNN acoustic modeling, that also enables training with untranscribed data.\nExperiments conducted on AMI corpus shows 4.6% relative reduction in word error\nrate.", "published": "2016-10-18 16:02:10", "link": "http://arxiv.org/abs/1610.05688v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
