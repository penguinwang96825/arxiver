{"title": "Suspicious News Detection Using Micro Blog Text", "abstract": "We present a new task, suspicious news detection using micro blog text. This\ntask aims to support human experts to detect suspicious news articles to be\nverified, which is costly but a crucial step before verifying the truthfulness\nof the articles. Specifically, in this task, given a set of posts on SNS\nreferring to a news article, the goal is to judge whether the article is to be\nverified or not. For this task, we create a publicly available dataset in\nJapanese and provide benchmark results by using several basic machine learning\ntechniques. Experimental results show that our models can reduce the cost of\nmanual fact-checking process.", "published": "2018-10-27 15:24:15", "link": "http://arxiv.org/abs/1810.11663v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Handling Imbalanced Dataset in Multi-label Text Categorization using\n  Bagging and Adaptive Boosting", "abstract": "Imbalanced dataset is occurred due to uneven distribution of data available\nin the real world such as disposition of complaints on government offices in\nBandung. Consequently, multi-label text categorization algorithms may not\nproduce the best performance because classifiers tend to be weighed down by the\nmajority of the data and ignore the minority. In this paper, Bagging and\nAdaptive Boosting algorithms are employed to handle the issue and improve the\nperformance of text categorization. The result is evaluated with four\nevaluation metrics such as hamming loss, subset accuracy, example-based\naccuracy and micro-averaged f-measure. Bagging ML-LP with SMO weak classifier\nis the best performer in terms of subset accuracy and example-based accuracy.\nBagging ML-BR with SMO weak classifier has the best micro-averaged f-measure\namong all. In other hand, AdaBoost MH with J48 weak classifier has the lowest\nhamming loss value. Thus, both algorithms have high potential in boosting the\nperformance of text categorization, but only for certain weak classifiers.\nHowever, bagging has more potential than adaptive boosting in increasing the\naccuracy of minority labels.", "published": "2018-10-27 07:27:18", "link": "http://arxiv.org/abs/1810.11612v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Short-segment heart sound classification using an ensemble of deep\n  convolutional neural networks", "abstract": "This paper proposes a framework based on deep convolutional neural networks\n(CNNs) for automatic heart sound classification using short-segments of\nindividual heart beats. We design a 1D-CNN that directly learns features from\nraw heart-sound signals, and a 2D-CNN that takes inputs of two- dimensional\ntime-frequency feature maps based on Mel-frequency cepstral coefficients\n(MFCC). We further develop a time-frequency CNN ensemble (TF-ECNN) combining\nthe 1D-CNN and 2D-CNN based on score-level fusion of the class probabilities.\nOn the large PhysioNet CinC challenge 2016 database, the proposed CNN models\noutperformed traditional classifiers based on support vector machine and hidden\nMarkov models with various hand-crafted time- and frequency-domain features.\nBest classification scores with 89.22% accuracy and 89.94% sensitivity were\nachieved by the ECNN, and 91.55% specificity and 88.82% modified accuracy by\nthe 2D-CNN alone on the test set.", "published": "2018-10-27 01:32:27", "link": "http://arxiv.org/abs/1810.11573v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP", "stat.ML"], "primary_category": "cs.SD"}
