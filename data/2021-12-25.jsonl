{"title": "Combining Improvements for Exploiting Dependency Trees in Neural\n  Semantic Parsing", "abstract": "The dependency tree of a natural language sentence can capture the\ninteractions between semantics and words. However, it is unclear whether those\nmethods which exploit such dependency information for semantic parsing can be\ncombined to achieve further improvement and the relationship of those methods\nwhen they combine. In this paper, we examine three methods to incorporate such\ndependency information in a Transformer based semantic parser and empirically\nstudy their combinations. We first replace standard self-attention heads in the\nencoder with parent-scaled self-attention (PASCAL) heads, i.e., the ones that\ncan attend to the dependency parent of each token. Then we concatenate\nsyntax-aware word representations (SAWRs), i.e., the intermediate hidden\nrepresentations of a neural dependency parser, with ordinary word embedding to\nenhance the encoder. Later, we insert the constituent attention (CA) module to\nthe encoder, which adds an extra constraint to attention heads that can better\ncapture the inherent dependency structure of input sentences. Transductive\nensemble learning (TEL) is used for model aggregation, and an ablation study is\nconducted to show the contribution of each method. Our experiments show that CA\nis complementary to PASCAL or SAWRs, and PASCAL + CA provides state-of-the-art\nperformance among neural approaches on ATIS, GEO, and JOBS.", "published": "2021-12-25 03:41:42", "link": "http://arxiv.org/abs/2112.13179v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PerCQA: Persian Community Question Answering Dataset", "abstract": "Community Question Answering (CQA) forums provide answers for many real-life\nquestions. Thanks to the large size, these forums are very popular among\nmachine learning researchers. Automatic answer selection, answer ranking,\nquestion retrieval, expert finding, and fact-checking are example learning\ntasks performed using CQA data. In this paper, we present PerCQA, the first\nPersian dataset for CQA. This dataset contains the questions and answers\ncrawled from the most well-known Persian forum. After data acquisition, we\nprovide rigorous annotation guidelines in an iterative process, and then the\nannotation of question-answer pairs in SemEvalCQA format. PerCQA contains 989\nquestions and 21,915 annotated answers. We make PerCQA publicly available to\nencourage more research in Persian CQA. We also build strong benchmarks for the\ntask of answer selection in PerCQA by using mono- and multi-lingual pre-trained\nlanguage models", "published": "2021-12-25 14:06:41", "link": "http://arxiv.org/abs/2112.13238v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stance Quantification: Definition of the Problem", "abstract": "Stance detection is commonly defined as the automatic process of determining\nthe positions of text producers, towards a target. In this paper, we define a\nresearch problem closely related to stance detection, namely, stance\nquantification, for the first time. We define stance quantification on a pair\nincluding (1) a set of natural language text items and (2) a target. At the end\nof the stance quantification process, a triple is obtained which consists of\nthe percentages of the number of text items classified as Favor, Against,\nNeither, respectively, towards the target in the input pair. Also defined in\nthe current paper is a significant subproblem of the stance quantification\nproblem, namely, multi-target stance quantification. We believe that stance\nquantification at the aggregate level can lead to fruitful results in many\napplication settings, and furthermore, stance quantification might be the sole\nstance related analysis alternative in settings where privacy concerns prevent\nresearchers from applying generic stance detection.", "published": "2021-12-25 21:19:42", "link": "http://arxiv.org/abs/2112.13288v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Preliminary Study for Literary Rhyme Generation based on Neuronal\n  Representation, Semantics and Shallow Parsing", "abstract": "In recent years, researchers in the area of Computational Creativity have\nstudied the human creative process proposing different approaches to reproduce\nit with a formal procedure. In this paper, we introduce a model for the\ngeneration of literary rhymes in Spanish, combining structures of language and\nneural network models %(\\textit{Word2vec}).%, into a structure for semantic\nassimilation. The results obtained with a manual evaluation of the texts\ngenerated by our algorithm are encouraging.", "published": "2021-12-25 14:40:09", "link": "http://arxiv.org/abs/2112.13241v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CABACE: Injecting Character Sequence Information and Domain Knowledge\n  for Enhanced Acronym and Long-Form Extraction", "abstract": "Acronyms and long-forms are commonly found in research documents, more so in\ndocuments from scientific and legal domains. Many acronyms used in such\ndocuments are domain-specific and are very rarely found in normal text corpora.\nOwing to this, transformer-based NLP models often detect OOV (Out of\nVocabulary) for acronym tokens, especially for non-English languages, and their\nperformance suffers while linking acronyms to their long forms during\nextraction. Moreover, pretrained transformer models like BERT are not\nspecialized to handle scientific and legal documents. With these points being\nthe overarching motivation behind this work, we propose a novel framework\nCABACE: Character-Aware BERT for ACronym Extraction, which takes into account\ncharacter sequences in text and is adapted to scientific and legal domains by\nmasked language modelling. We further use an objective with an augmented loss\nfunction, adding the max loss and mask loss terms to the standard cross-entropy\nloss for training CABACE. We further leverage pseudo labelling and adversarial\ndata generation to improve the generalizability of the framework. Experimental\nresults prove the superiority of the proposed framework in comparison to\nvarious baselines. Additionally, we show that the proposed framework is better\nsuited than baseline models for zero-shot generalization to non-English\nlanguages, thus reinforcing the effectiveness of our approach. Our team\nBacKGProp secured the highest scores on the French dataset, second-highest on\nDanish and Vietnamese, and third-highest in the English-Legal dataset on the\nglobal leaderboard for the acronym extraction (AE) shared task at SDU AAAI-22.", "published": "2021-12-25 14:03:09", "link": "http://arxiv.org/abs/2112.13237v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Deeper Clinical Document Understanding Using Relation Extraction", "abstract": "The surging amount of biomedical literature & digital clinical records\npresents a growing need for text mining techniques that can not only identify\nbut also semantically relate entities in unstructured data. In this paper we\npropose a text mining framework comprising of Named Entity Recognition (NER)\nand Relation Extraction (RE) models, which expands on previous work in three\nmain ways. First, we introduce two new RE model architectures -- an\naccuracy-optimized one based on BioBERT and a speed-optimized one utilizing\ncrafted features over a Fully Connected Neural Network (FCNN). Second, we\nevaluate both models on public benchmark datasets and obtain new\nstate-of-the-art F1 scores on the 2012 i2b2 Clinical Temporal Relations\nchallenge (F1 of 73.6, +1.2% over the previous SOTA), the 2010 i2b2 Clinical\nRelations challenge (F1 of 69.1, +1.2%), the 2019 Phenotype-Gene Relations\ndataset (F1 of 87.9, +8.5%), the 2012 Adverse Drug Events Drug-Reaction dataset\n(F1 of 90.0, +6.3%), and the 2018 n2c2 Posology Relations dataset (F1 of 96.7,\n+0.6%). Third, we show two practical applications of this framework -- for\nbuilding a biomedical knowledge graph and for improving the accuracy of mapping\nentities to clinical codes. The system is built using the Spark NLP library\nwhich provides a production-grade, natively scalable, hardware-optimized,\ntrainable & tunable NLP framework.", "published": "2021-12-25 17:14:13", "link": "http://arxiv.org/abs/2112.13259v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.4; I.2.11"], "primary_category": "cs.CL"}
{"title": "Multi-Dialect Arabic Speech Recognition", "abstract": "This paper presents the design and development of multi-dialect automatic\nspeech recognition for Arabic. Deep neural networks are becoming an effective\ntool to solve sequential data problems, particularly, adopting an end-to-end\ntraining of the system. Arabic speech recognition is a complex task because of\nthe existence of multiple dialects, non-availability of large corpora, and\nmissing vocalization. Thus, the first contribution of this work is the\ndevelopment of a large multi-dialectal corpus with either full or at least\npartially vocalized transcription. Additionally, the open-source corpus has\nbeen gathered from multiple sources that bring non-standard Arabic alphabets in\ntranscription which are normalized by defining a common character-set. The\nsecond contribution is the development of a framework to train an acoustic\nmodel achieving state-of-the-art performance. The network architecture\ncomprises of a combination of convolutional and recurrent layers. The\nspectrogram features of the audio data are extracted in the frequency vs time\ndomain and fed in the network. The output frames, produced by the recurrent\nmodel, are further trained to align the audio features with its corresponding\ntranscription sequences. The sequence alignment is performed using a beam\nsearch decoder with a tetra-gram language model. The proposed system achieved a\n14% error rate which outperforms previous systems.", "published": "2021-12-25 20:55:57", "link": "http://arxiv.org/abs/2112.14678v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
