{"title": "Extractive and Abstractive Explanations for Fact-Checking and Evaluation\n  of News", "abstract": "In this paper, we explore the construction of natural language explanations\nfor news claims, with the goal of assisting fact-checking and news evaluation\napplications. We experiment with two methods: (1) an extractive method based on\nBiased TextRank -- a resource-effective unsupervised graph-based algorithm for\ncontent extraction; and (2) an abstractive method based on the GPT-2 language\nmodel. We perform comparative evaluations on two misinformation datasets in the\npolitical and health news domains, and find that the extractive method shows\nthe most promise.", "published": "2021-04-27 00:21:55", "link": "http://arxiv.org/abs/2104.12918v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SE-DAE: Style-Enhanced Denoising Auto-Encoder for Unsupervised Text\n  Style Transfer", "abstract": "Text style transfer aims to change the style of sentences while preserving\nthe semantic meanings. Due to the lack of parallel data, the Denoising\nAuto-Encoder (DAE) is widely used in this task to model distributions of\ndifferent sentence styles. However, because of the conflict between the target\nof the conventional denoising procedure and the target of style transfer task,\nthe vanilla DAE can not produce satisfying enough results. To improve the\ntransferability of the model, most of the existing works combine DAE with\nvarious complicated unsupervised networks, which makes the whole system become\nover-complex. In this work, we design a novel DAE model named Style-Enhanced\nDAE (SE-DAE), which is specifically designed for the text style transfer task.\nCompared with previous complicated style-transfer models, our model do not\nconsist of any complicated unsupervised networks, but only relies on the\nhigh-quality pseudo-parallel data generated by a novel data refinement\nmechanism. Moreover, to alleviate the conflict between the targets of the\nconventional denoising procedure and the style transfer task, we propose\nanother novel style denoising mechanism, which is more compatible with the\ntarget of the style transfer task. We validate the effectiveness of our model\non two style benchmark datasets. Both automatic evaluation and human evaluation\nshow that our proposed model is highly competitive compared with previous\nstrong the state of the art (SOTA) approaches and greatly outperforms the\nvanilla DAE.", "published": "2021-04-27 04:41:18", "link": "http://arxiv.org/abs/2104.12977v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LAST at CMCL 2021 Shared Task: Predicting Gaze Data During Reading with\n  a Gradient Boosting Decision Tree Approach", "abstract": "A LightGBM model fed with target word lexical characteristics and features\nobtained from word frequency lists, psychometric data and bigram association\nmeasures has been optimized for the 2021 CMCL Shared Task on Eye-Tracking Data\nPrediction. It obtained the best performance of all teams on two of the five\neye-tracking measures to predict, allowing it to rank first on the official\nchallenge criterion and to outperform all deep-learning based systems\nparticipating in the challenge.", "published": "2021-04-27 08:39:52", "link": "http://arxiv.org/abs/2104.13043v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-Supervised Joint Estimation of Word and Document Readability", "abstract": "Readability or difficulty estimation of words and documents has been\ninvestigated independently in the literature, often assuming the existence of\nextensive annotated resources for the other. Motivated by our analysis showing\nthat there is a recursive relationship between word and document difficulty, we\npropose to jointly estimate word and document difficulty through a graph\nconvolutional network (GCN) in a semi-supervised fashion. Our experimental\nresults reveal that the GCN-based method can achieve higher accuracy than\nstrong baselines, and stays robust even with a smaller amount of labeled data.", "published": "2021-04-27 10:56:47", "link": "http://arxiv.org/abs/2104.13103v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UoT-UWF-PartAI at SemEval-2021 Task 5: Self Attention Based Bi-GRU with\n  Multi-Embedding Representation for Toxicity Highlighter", "abstract": "Toxic Spans Detection(TSD) task is defined as highlighting spans that make a\ntext toxic. Many works have been done to classify a given comment or document\nas toxic or non-toxic. However, none of those proposed models work at the token\nlevel. In this paper, we propose a self-attention-based bidirectional gated\nrecurrent unit(BiGRU) with a multi-embedding representation of the tokens. Our\nproposed model enriches the representation by a combination of GPT-2, GloVe,\nand RoBERTa embeddings, which led to promising results. Experimental results\nshow that our proposed approach is very effective in detecting span tokens.", "published": "2021-04-27 13:18:28", "link": "http://arxiv.org/abs/2104.13164v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question-Aware Memory Network for Multi-hop Question Answering in\n  Human-Robot Interaction", "abstract": "Knowledge graph question answering is an important technology in intelligent\nhuman-robot interaction, which aims at automatically giving answer to human\nnatural language question with the given knowledge graph. For the\nmulti-relation question with higher variety and complexity, the tokens of the\nquestion have different priority for the triples selection in the reasoning\nsteps. Most existing models take the question as a whole and ignore the\npriority information in it. To solve this problem, we propose question-aware\nmemory network for multi-hop question answering, named QA2MN, to update the\nattention on question timely in the reasoning process. In addition, we\nincorporate graph context information into knowledge graph embedding model to\nincrease the ability to represent entities and relations. We use it to\ninitialize the QA2MN model and fine-tune it in the training process. We\nevaluate QA2MN on PathQuestion and WorldCup2014, two representative datasets\nfor complex multi-hop question answering. The result demonstrates that QA2MN\nachieves state-of-the-art Hits@1 accuracy on the two datasets, which validates\nthe effectiveness of our model.", "published": "2021-04-27 13:32:41", "link": "http://arxiv.org/abs/2104.13173v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding Factuality in Abstractive Summarization with FRANK: A\n  Benchmark for Factuality Metrics", "abstract": "Modern summarization models generate highly fluent but often factually\nunreliable outputs. This motivated a surge of metrics attempting to measure the\nfactuality of automatically generated summaries. Due to the lack of common\nbenchmarks, these metrics cannot be compared. Moreover, all these methods treat\nfactuality as a binary concept and fail to provide deeper insights into the\nkinds of inconsistencies made by different systems. To address these\nlimitations, we devise a typology of factual errors and use it to collect human\nannotations of generated summaries from state-of-the-art summarization systems\nfor the CNN/DM and XSum datasets. Through these annotations, we identify the\nproportion of different categories of factual errors in various summarization\nmodels and benchmark factuality metrics, showing their correlation with human\njudgment as well as their specific strengths and weaknesses.", "published": "2021-04-27 17:28:07", "link": "http://arxiv.org/abs/2104.13346v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Named Entity Recognition and Linking Augmented with Large-Scale\n  Structured Data", "abstract": "In this paper we describe our submissions to the 2nd and 3rd SlavNER Shared\nTasks held at BSNLP 2019 and BSNLP 2021, respectively. The tasks focused on the\nanalysis of Named Entities in multilingual Web documents in Slavic languages\nwith rich inflection. Our solution takes advantage of large collections of both\nunstructured and structured documents. The former serve as data for\nunsupervised training of language models and embeddings of lexical units. The\nlatter refers to Wikipedia and its structured counterpart - Wikidata, our\nsource of lemmatization rules, and real-world entities. With the aid of those\nresources, our system could recognize, normalize and link entities, while being\ntrained with only small amounts of labeled data.", "published": "2021-04-27 20:10:18", "link": "http://arxiv.org/abs/2104.13456v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Document Structure aware Relational Graph Convolutional Networks for\n  Ontology Population", "abstract": "Ontologies comprising of concepts, their attributes, and relationships are\nused in many knowledge based AI systems. While there have been efforts towards\npopulating domain specific ontologies, we examine the role of document\nstructure in learning ontological relationships between concepts in any\ndocument corpus. Inspired by ideas from hypernym discovery and explainability,\nour method performs about 15 points more accurate than a stand-alone R-GCN\nmodel for this task.", "published": "2021-04-27 02:50:39", "link": "http://arxiv.org/abs/2104.12950v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Relational Learning with Gated and Attentive Neighbor Aggregator for\n  Few-Shot Knowledge Graph Completion", "abstract": "Aiming at expanding few-shot relations' coverage in knowledge graphs (KGs),\nfew-shot knowledge graph completion (FKGC) has recently gained more research\ninterests. Some existing models employ a few-shot relation's multi-hop neighbor\ninformation to enhance its semantic representation. However, noise neighbor\ninformation might be amplified when the neighborhood is excessively sparse and\nno neighbor is available to represent the few-shot relation. Moreover, modeling\nand inferring complex relations of one-to-many (1-N), many-to-one (N-1), and\nmany-to-many (N-N) by previous knowledge graph completion approaches requires\nhigh model complexity and a large amount of training instances. Thus, inferring\ncomplex relations in the few-shot scenario is difficult for FKGC models due to\nlimited training instances. In this paper, we propose a few-shot relational\nlearning with global-local framework to address the above issues. At the global\nstage, a novel gated and attentive neighbor aggregator is built for accurately\nintegrating the semantics of a few-shot relation's neighborhood, which helps\nfiltering the noise neighbors even if a KG contains extremely sparse\nneighborhoods. For the local stage, a meta-learning based TransH (MTransH)\nmethod is designed to model complex relations and train our model in a few-shot\nlearning fashion. Extensive experiments show that our model outperforms the\nstate-of-the-art FKGC approaches on the frequently-used benchmark datasets\nNELL-One and Wiki-One. Compared with the strong baseline model MetaR, our model\nachieves 5-shot FKGC performance improvements of 8.0% on NELL-One and 2.8% on\nWiki-One by the metric Hits@10.", "published": "2021-04-27 10:38:44", "link": "http://arxiv.org/abs/2104.13095v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Shellcode_IA32: A Dataset for Automatic Shellcode Generation", "abstract": "We take the first step to address the task of automatically generating\nshellcodes, i.e., small pieces of code used as a payload in the exploitation of\na software vulnerability, starting from natural language comments. We assemble\nand release a novel dataset (Shellcode_IA32), consisting of challenging but\ncommon assembly instructions with their natural language descriptions. We\nexperiment with standard methods in neural machine translation (NMT) to\nestablish baseline performance levels on this task.", "published": "2021-04-27 10:50:47", "link": "http://arxiv.org/abs/2104.13100v4", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Semi-supervised Interactive Intent Labeling", "abstract": "Building the Natural Language Understanding (NLU) modules of task-oriented\nSpoken Dialogue Systems (SDS) involves a definition of intents and entities,\ncollection of task-relevant data, annotating the data with intents and\nentities, and then repeating the same process over and over again for adding\nany functionality/enhancement to the SDS. In this work, we showcase an Intent\nBulk Labeling system where SDS developers can interactively label and augment\ntraining data from unlabeled utterance corpora using advanced clustering and\nvisual labeling methods. We extend the Deep Aligned Clustering work with a\nbetter backbone BERT model, explore techniques to select the seed data for\nlabeling, and develop a data balancing method using an oversampling technique\nthat utilizes paraphrasing models. We also look at the effect of data\naugmentation on the clustering process. Our results show that we can achieve\nover 10% gain in clustering accuracy on some datasets using the combination of\nthe above techniques. Finally, we extract utterance embeddings from the\nclustering model and plot the data to interactively bulk label the samples,\nreducing the time and effort for data labeling of the whole dataset\nsignificantly.", "published": "2021-04-27 18:06:55", "link": "http://arxiv.org/abs/2104.13406v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Towards Clinical Encounter Summarization: Learning to Compose Discharge\n  Summaries from Prior Notes", "abstract": "The records of a clinical encounter can be extensive and complex, thus\nplacing a premium on tools that can extract and summarize relevant information.\nThis paper introduces the task of generating discharge summaries for a clinical\nencounter. Summaries in this setting need to be faithful, traceable, and scale\nto multiple long documents, motivating the use of extract-then-abstract\nsummarization cascades. We introduce two new measures, faithfulness and\nhallucination rate for evaluation in this task, which complement existing\nmeasures for fluency and informativeness. Results across seven medical sections\nand five models show that a summarization architecture that supports\ntraceability yields promising results, and that a sentence-rewriting approach\nperforms consistently on the measure used for faithfulness\n(faithfulness-adjusted $F_3$) over a diverse range of generated sections.", "published": "2021-04-27 22:45:54", "link": "http://arxiv.org/abs/2104.13498v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improved and Efficient Text Adversarial Attacks using Target Information", "abstract": "There has been recently a growing interest in studying adversarial examples\non natural language models in the black-box setting. These methods attack\nnatural language classifiers by perturbing certain important words until the\nclassifier label is changed. In order to find these important words, these\nmethods rank all words by importance by querying the target model word by word\nfor each input sentence, resulting in high query inefficiency. A new\ninteresting approach was introduced that addresses this problem through\ninterpretable learning to learn the word ranking instead of previous expensive\nsearch. The main advantage of using this approach is that it achieves\ncomparable attack rates to the state-of-the-art methods, yet faster and with\nfewer queries, where fewer queries are desirable to avoid suspicion towards the\nattacking agent. Nonetheless, this approach sacrificed the useful information\nthat could be leveraged from the target classifier for that sake of query\nefficiency. In this paper we study the effect of leveraging the target model\noutputs and data on both attack rates and average number of queries, and we\nshow that both can be improved, with a limited overhead of additional queries.", "published": "2021-04-27 21:25:55", "link": "http://arxiv.org/abs/2104.13484v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "I.5.0, I.2.0"], "primary_category": "cs.LG"}
{"title": "Text Generation with Deep Variational GAN", "abstract": "Generating realistic sequences is a central task in many machine learning\napplications. There has been considerable recent progress on building deep\ngenerative models for sequence generation tasks. However, the issue of\nmode-collapsing remains a main issue for the current models. In this paper we\npropose a GAN-based generic framework to address the problem of mode-collapse\nin a principled approach. We change the standard GAN objective to maximize a\nvariational lower-bound of the log-likelihood while minimizing the\nJensen-Shanon divergence between data and model distributions. We experiment\nour model with text generation task and show that it can generate realistic\ntext with high diversity.", "published": "2021-04-27 21:42:13", "link": "http://arxiv.org/abs/2104.13488v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.0; I.2.7; I.5.0"], "primary_category": "cs.LG"}
{"title": "On Addressing Practical Challenges for RNN-Transducer", "abstract": "In this paper, several works are proposed to address practical challenges for\ndeploying RNN Transducer (RNN-T) based speech recognition system. These\nchallenges are adapting a well-trained RNN-T model to a new domain without\ncollecting the audio data, obtaining time stamps and confidence scores at word\nlevel. The first challenge is solved with a splicing data method which\nconcatenates the speech segments extracted from the source domain data. To get\nthe time stamp, a phone prediction branch is added to the RNN-T model by\nsharing the encoder for the purpose of force alignment. Finally, we obtain\nword-level confidence scores by utilizing several types of features calculated\nduring decoding and from confusion network. Evaluated with Microsoft production\ndata, the splicing data adaptation method improves the baseline and adaptation\nwith the text to speech method by 58.03% and 15.25% relative word error rate\nreduction, respectively. The proposed time stamping method can get less than\n50ms word timing difference from the ground truth alignment on average while\nmaintaining the recognition accuracy of the RNN-T model. We also obtain high\nconfidence annotation performance with limited computation cost.", "published": "2021-04-27 23:31:43", "link": "http://arxiv.org/abs/2105.00858v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Visually grounded models of spoken language: A survey of datasets,\n  architectures and evaluation techniques", "abstract": "This survey provides an overview of the evolution of visually grounded models\nof spoken language over the last 20 years. Such models are inspired by the\nobservation that when children pick up a language, they rely on a wide range of\nindirect and noisy clues, crucially including signals from the visual modality\nco-occurring with spoken utterances. Several fields have made important\ncontributions to this approach to modeling or mimicking the process of learning\nlanguage: Machine Learning, Natural Language and Speech Processing, Computer\nVision and Cognitive Science. The current paper brings together these\ncontributions in order to provide a useful introduction and overview for\npractitioners in all these areas. We discuss the central research questions\naddressed, the timeline of developments, and the datasets which enabled much of\nthis work. We then summarize the main modeling architectures and offer an\nexhaustive overview of the evaluation metrics and analysis techniques.", "published": "2021-04-27 14:32:22", "link": "http://arxiv.org/abs/2104.13225v3", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
{"title": "DPT-FSNet: Dual-path Transformer Based Full-band and Sub-band Fusion\n  Network for Speech Enhancement", "abstract": "Sub-band models have achieved promising results due to their ability to model\nlocal patterns in the spectrogram. Some studies further improve the performance\nby fusing sub-band and full-band information. However, the structure for the\nfull-band and sub-band fusion model was not fully explored. This paper proposes\na dual-path transformer-based full-band and sub-band fusion network (DPT-FSNet)\nfor speech enhancement in the frequency domain. The intra and inter parts of\nthe dual-path transformer model sub-band and full-band information,\nrespectively. The features utilized by our proposed method are more\ninterpretable than those utilized by the time-domain dual-path transformer. We\nconducted experiments on the Voice Bank + DEMAND and Interspeech 2020 Deep\nNoise Suppression (DNS) datasets to evaluate the proposed method. Experimental\nresults show that the proposed method outperforms the current state-of-the-art.", "published": "2021-04-27 06:49:56", "link": "http://arxiv.org/abs/2104.13002v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "dEchorate: a Calibrated Room Impulse Response Database for Echo-aware\n  Signal Processing", "abstract": "This paper presents dEchorate: a new database of measured multichannel Room\nImpulse Responses (RIRs) including annotations of early echo timings and 3D\npositions of microphones, real sources and image sources under different wall\nconfigurations in a cuboid room. These data provide a tool for benchmarking\nrecent methods in echo-aware speech enhancement, room geometry estimation, RIR\nestimation, acoustic echo retrieval, microphone calibration, echo labeling and\nreflectors estimation. The database is accompanied with software utilities to\neasily access, manipulate and visualize the data as well as baseline methods\nfor echo-related tasks.", "published": "2021-04-27 13:23:56", "link": "http://arxiv.org/abs/2104.13168v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "IATos: AI-powered pre-screening tool for COVID-19 from cough audio\n  samples", "abstract": "OBJECTIVE: Our objective is to evaluate the possibility of using cough audio\nrecordings (spontaneous or simulated) to detect sound patterns in people who\nare diagnosed with COVID-19. The research question that led our work was: what\nis the sensitivity and specificity of a machine learning based COVID-19 cough\nclassifier, using RT-PCR tests as gold standard?\n  SETTING: The audio samples that were collected for this study belong to\nindividuals who were swabbed in the City of Buenos Aires in 20 public and 1\nprivate facilities where RT-PCR studies were carried out on patients suspected\nof COVID, and 14 out-of-hospital isolation units for patients with confirmed\nCOVID mild cases. The audios were collected through the Buenos Aires city\ngovernment WhatsApp chatbot that was specifically designed to address citizen\ninquiries related to the coronavirus pandemic (COVID-19).\n  PARTICIPANTS: The data collected corresponds to 2821 individuals who were\nswabbed in the City of Buenos Aires, between August 11 and December 2, 2020.\nIndividuals were divided into 1409 that tested positive for COVID-19 and 1412\nthat tested negative. From this sample group, 52.6% of the individuals were\nfemale and 47.4% were male. 2.5% were between the age of 0 and 20 , 61.1%\nbetween the age of 21 and 40 , 30.3% between the age of 41 and 60 and 6.1% were\nover 61 years of age.\n  RESULTS: Using the dataset of 2821 individuals our results showed that the\nneural network classifier was able to discriminate between the COVID-19\npositive and the healthy coughs with an accuracy of 86%. This accuracy obtained\nduring the training process was later tested and confirmed with a second\ndataset corresponding to 492 individuals.", "published": "2021-04-27 14:57:44", "link": "http://arxiv.org/abs/2104.13247v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "BeamLearning: an end-to-end Deep Learning approach for the angular\n  localization of sound sources using raw multichannel acoustic pressure data", "abstract": "Sound sources localization using multichannel signal processing has been a\nsubject of active research for decades. In recent years, the use of deep\nlearning in audio signal processing has allowed to drastically improve\nperformances for machine hearing. This has motivated the scientific community\nto also develop machine learning strategies for source localization\napplications. In this paper, we present BeamLearning, a multi-resolution deep\nlearning approach that allows to encode relevant information contained in\nunprocessed time domain acoustic signals captured by microphone arrays. The use\nof raw data aims at avoiding simplifying hypothesis that most traditional\nmodel-based localization methods rely on. Benefits of its use are shown for\nrealtime sound source 2D-localization tasks in reverberating and noisy\nenvironments. Since supervised machine learning approaches require large-sized,\nphysically realistic, precisely labelled datasets, we also developed a fast\nGPU-based computation of room impulse responses using fractional delays for\nimage source models. A thorough analysis of the network representation and\nextensive performance tests are carried out using the BeamLearning network with\nsynthetic and experimental datasets. Obtained results demonstrate that the\nBeamLearning approach significantly outperforms the wideband MUSIC and SRP-PHAT\nmethods in terms of localization accuracy and computational efficiency in\npresence of heavy measurement noise and reverberation.", "published": "2021-04-27 17:28:08", "link": "http://arxiv.org/abs/2104.13347v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The music box operad: Random generation of musical phrases from patterns", "abstract": "We introduce the notion of multi-patterns, a combinatorial abstraction of\npolyphonic musical phrases. The interest of this approach in encoding musical\nphrases lies in the fact that it becomes possible to compose multi-patterns in\norder to produce new ones. This composition is parameterized by a monoid\nstructure on the scale degrees. This embeds the set of the musical phrases into\nan algebraic framework since the set of the multi-patterns is endowed with the\nstructure of an operad. Operads are algebraic structures offering a\nformalization and an abstraction of the notion of operators and their\ncompositions. Seeing musical phrases as operators allows us to perform\ncomputations on phrases and admits applications in generative music. Indeed,\ngiven a set of initial multi-patterns, we propose various algorithms to\nrandomly generate a new and longer phrase emulating the style suggested by the\ninputted multi-patterns. The designed algorithms use types of grammars working\nwith operads and colored operads, known as bud generating systems.", "published": "2021-04-27 08:32:09", "link": "http://arxiv.org/abs/2104.13040v2", "categories": ["cs.SD", "eess.AS", "math.CO", "math.QA", "00A65, 18M60, 68Q42"], "primary_category": "cs.SD"}
{"title": "Generating Lead Sheets with Affect: A Novel Conditional seq2seq\n  Framework", "abstract": "The field of automatic music composition has seen great progress in the last\nfew years, much of which can be attributed to advances in deep neural networks.\nThere are numerous studies that present different strategies for generating\nsheet music from scratch. The inclusion of high-level musical characteristics\n(e.g., perceived emotional qualities), however, as conditions for controlling\nthe generation output remains a challenge. In this paper, we present a novel\napproach for calculating the valence (the positivity or negativity of the\nperceived emotion) of a chord progression within a lead sheet, using\npre-defined mood tags proposed by music experts. Based on this approach, we\npropose a novel strategy for conditional lead sheet generation that allows us\nto steer the music generation in terms of valence, phrasing, and time\nsignature. Our approach is similar to a Neural Machine Translation (NMT)\nproblem, as we include high-level conditions in the encoder part of the\nsequence-to-sequence architectures used (i.e., long-short term memory networks,\nand a Transformer network). We conducted experiments to thoroughly analyze\nthese two architectures. The results show that the proposed strategy is able to\ngenerate lead sheets in a controllable manner, resulting in distributions of\nmusical attributes similar to those of the training dataset. We also verified\nthrough a subjective listening test that our approach is effective in\ncontrolling the valence of a generated chord progression.", "published": "2021-04-27 09:04:21", "link": "http://arxiv.org/abs/2104.13056v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Visualization of Linear Operations in the Spherical Harmonics Domain", "abstract": "Linear operations on coefficients in the spherical harmonics (SH) transform\ndomain that again yield SH-domain coefficients are an important toolset in many\ndisciplines of research and engineering. They comprise rotations, spatially\nselective filters, and many other modifications for various applications, or\ndescribe the response of a MIMO system to an excitation. It is of particular\nimportance to characterize these operations both qualitatively and\nquantitatively, and make them accessible for people to work with. In this\npaper, we identify different key properties of such operations and propose a\nmethod for their visualization. With our proposed method, we succeed to show\nmany important aspects of an operation in a single plot and give rise to a\ncomprehensive interpretation of the behavior of a system. In our evaluation, we\nshow the potential of the proposed method on the basis of various practical\nexamples from spatial audio signal processing, where SH-domain filtering is\nused to modify acoustic scenes given by higher-order Ambisonics signals.", "published": "2021-04-27 09:36:06", "link": "http://arxiv.org/abs/2104.13069v3", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Batebit Controller: Popularizing Digital Musical Instruments Development\n  Process", "abstract": "In this paper, we present an ongoing research project related to popularizing\nthe mindset of building new digital musical instruments. We developed a\nphysical kit and software intended to provide beginner users with the first\ngrasp on the development process of a digital musical instrument. We expect\nthat, by using the kit and the software, the users could experiment in a short\nperiod the various steps in developing a DMI such as physical structure,\nelectronics, programming, mapping, and sound design. Our approach to\npopularizing the DMI development process is twofold: reducing the cognitive\nload for beginners by encapsulating technical details and lowering the costs of\nthe kit by using simple components and open-source software. In the end, we\nexpect that by increasing the interest of beginners in the building process of\ndigital musical instruments, we could make the community of new interfaces for\nmusical expression stronger.", "published": "2021-04-27 15:34:53", "link": "http://arxiv.org/abs/2104.13266v1", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "End-to-End Video-To-Speech Synthesis using Generative Adversarial\n  Networks", "abstract": "Video-to-speech is the process of reconstructing the audio speech from a\nvideo of a spoken utterance. Previous approaches to this task have relied on a\ntwo-step process where an intermediate representation is inferred from the\nvideo, and is then decoded into waveform audio using a vocoder or a waveform\nreconstruction algorithm. In this work, we propose a new end-to-end\nvideo-to-speech model based on Generative Adversarial Networks (GANs) which\ntranslates spoken video to waveform end-to-end without using any intermediate\nrepresentation or separate waveform synthesis algorithm. Our model consists of\nan encoder-decoder architecture that receives raw video as input and generates\nspeech, which is then fed to a waveform critic and a power critic. The use of\nan adversarial loss based on these two critics enables the direct synthesis of\nraw audio waveform and ensures its realism. In addition, the use of our three\ncomparative losses helps establish direct correspondence between the generated\naudio and the input video. We show that this model is able to reconstruct\nspeech with remarkable realism for constrained datasets such as GRID, and that\nit is the first end-to-end model to produce intelligible speech for LRW (Lip\nReading in the Wild), featuring hundreds of speakers recorded entirely `in the\nwild'. We evaluate the generated samples in two different scenarios -- seen and\nunseen speakers -- using four objective metrics which measure the quality and\nintelligibility of artificial speech. We demonstrate that the proposed approach\noutperforms all previous works in most metrics on GRID and LRW.", "published": "2021-04-27 17:12:30", "link": "http://arxiv.org/abs/2104.13332v3", "categories": ["cs.LG", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "DASEE A Synthetic Database of Domestic Acoustic Scenes and Events in\n  Dementia Patients Environment", "abstract": "Access to informative databases is a crucial part of notable research\ndevelopments. In the field of domestic audio classification, there have been\nsignificant advances in recent years. Although several audio databases exist,\nthese can be limited in terms of the amount of information they provide, such\nas the exact location of the sound sources, and the associated noise levels. In\nthis work, we detail our approach on generating an unbiased synthetic domestic\naudio database, consisting of sound scenes and events, emulated in both quiet\nand noisy environments. Data is carefully curated such that it reflects issues\ncommonly faced in a dementia patients environment, and recreate scenarios that\ncould occur in real-world settings. Similarly, the room impulse response\ngenerated is based on a typical one-bedroom apartment at Hebrew SeniorLife\nFacility. As a result, we present an 11-class database containing excerpts of\nclean and noisy signals at 5-seconds duration each, uniformly sampled at 16\nkHz. Using our baseline model using Continues Wavelet Transform Scalograms and\nAlexNet, this yielded a weighted F1-score of 86.24 percent.", "published": "2021-04-27 18:51:44", "link": "http://arxiv.org/abs/2104.13423v2", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "One Billion Audio Sounds from GPU-enabled Modular Synthesis", "abstract": "We release synth1B1, a multi-modal audio corpus consisting of 1 billion\n4-second synthesized sounds, paired with the synthesis parameters used to\ngenerate them. The dataset is 100x larger than any audio dataset in the\nliterature. We also introduce torchsynth, an open source modular synthesizer\nthat generates the synth1B1 samples on-the-fly at 16200x faster than real-time\n(714MHz) on a single GPU. Finally, we release two new audio datasets: FM synth\ntimbre and subtractive synth pitch. Using these datasets, we demonstrate new\nrank-based evaluation criteria for existing audio representations. Finally, we\npropose a novel approach to synthesizer hyperparameter optimization.", "published": "2021-04-27 00:38:52", "link": "http://arxiv.org/abs/2104.12922v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "MULTIMODAL ANALYSIS: Informed content estimation and audio source\n  separation", "abstract": "This dissertation proposes the study of multimodal learning in the context of\nmusical signals. Throughout, we focus on the interaction between audio signals\nand text information. Among the many text sources related to music that can be\nused (e.g. reviews, metadata, or social network feedback), we concentrate on\nlyrics. The singing voice directly connects the audio signal and the text\ninformation in a unique way, combining melody and lyrics where a linguistic\ndimension complements the abstraction of musical instruments. Our study focuses\non the audio and lyrics interaction for targeting source separation and\ninformed content estimation.", "published": "2021-04-27 15:45:21", "link": "http://arxiv.org/abs/2104.13276v3", "categories": ["cs.SD", "cs.DB", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
