{"title": "Document Sub-structure in Neural Machine Translation", "abstract": "Current approaches to machine translation (MT) either translate sentences in\nisolation, disregarding the context they appear in, or model context at the\nlevel of the full document, without a notion of any internal structure the\ndocument may have. In this work we consider the fact that documents are rarely\nhomogeneous blocks of text, but rather consist of parts covering different\ntopics. Some documents, such as biographies and encyclopedia entries, have\nhighly predictable, regular structures in which sections are characterised by\ndifferent topics. We draw inspiration from Louis and Webber (2014) who use this\ninformation to improve statistical MT and transfer their proposal into the\nframework of neural MT. We compare two different methods of including\ninformation about the topic of the section within which each sentence is found:\none using side constraints and the other using a cache-based model. We create\nand release the data on which we run our experiments - parallel corpora for\nthree language pairs (Chinese-English, French-English, Bulgarian-English) from\nWikipedia biographies, which we extract automatically, preserving the\nboundaries of sections within the articles.", "published": "2019-12-13 16:49:16", "link": "http://arxiv.org/abs/1912.06598v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WaLDORf: Wasteless Language-model Distillation On Reading-comprehension", "abstract": "Transformer based Very Large Language Models (VLLMs) like BERT, XLNet and\nRoBERTa, have recently shown tremendous performance on a large variety of\nNatural Language Understanding (NLU) tasks. However, due to their size, these\nVLLMs are extremely resource intensive and cumbersome to deploy at production\ntime. Several recent publications have looked into various ways to distil\nknowledge from a transformer based VLLM (most commonly BERT-Base) into a\nsmaller model which can run much faster at inference time. Here, we propose a\nnovel set of techniques which together produce a task-specific hybrid\nconvolutional and transformer model, WaLDORf, that achieves state-of-the-art\ninference speed while still being more accurate than previous distilled models.", "published": "2019-12-13 18:15:37", "link": "http://arxiv.org/abs/1912.06638v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Common Voice: A Massively-Multilingual Speech Corpus", "abstract": "The Common Voice corpus is a massively-multilingual collection of transcribed\nspeech intended for speech technology research and development. Common Voice is\ndesigned for Automatic Speech Recognition purposes but can be useful in other\ndomains (e.g. language identification). To achieve scale and sustainability,\nthe Common Voice project employs crowdsourcing for both data collection and\ndata validation. The most recent release includes 29 languages, and as of\nNovember 2019 there are a total of 38 languages collecting data. Over 50,000\nindividuals have participated so far, resulting in 2,500 hours of collected\naudio. To our knowledge this is the largest audio corpus in the public domain\nfor speech recognition, both in terms of number of hours and number of\nlanguages. As an example use case for Common Voice, we present speech\nrecognition experiments using Mozilla's DeepSpeech Speech-to-Text toolkit. By\napplying transfer learning from a source English model, we find an average\nCharacter Error Rate improvement of 5.99 +/- 5.48 for twelve target languages\n(German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton,\nTatar, Chuvash, and Kabyle). For most of these languages, these are the first\never published results on end-to-end Automatic Speech Recognition.", "published": "2019-12-13 19:22:44", "link": "http://arxiv.org/abs/1912.06670v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Network Surgery with Sets", "abstract": "The cost to train machine learning models has been increasing exponentially,\nmaking exploration and research into the correct features and architecture a\ncostly or intractable endeavor at scale. However, using a technique named\n\"surgery\" OpenAI Five was continuously trained to play the game DotA 2 over the\ncourse of 10 months through 20 major changes in features and architecture.\nSurgery transfers trained weights from one network to another after a selection\nprocess to determine which sections of the model are unchanged and which must\nbe re-initialized. In the past, the selection process relied on heuristics,\nmanual labor, or pre-existing boundaries in the structure of the model,\nlimiting the ability to salvage experiments after modifications of the feature\nset or input reorderings.\n  We propose a solution to automatically determine which components of a neural\nnetwork model should be salvaged and which require retraining. We achieve this\nby allowing the model to operate over discrete sets of features and use\nset-based operations to determine the exact relationship between inputs and\noutputs, and how they change across tweaks in model architecture. In this\npaper, we introduce the methodology for enabling neural networks to operate on\nsets, derive two methods for detecting feature-parameter interaction maps, and\nshow their equivalence. We empirically validate that we can surgery weights\nacross feature and architecture changes to the OpenAI Five model.", "published": "2019-12-13 21:41:39", "link": "http://arxiv.org/abs/1912.06719v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Long-Term Planning and Situational Awareness in OpenAI Five", "abstract": "Understanding how knowledge about the world is represented within model-free\ndeep reinforcement learning methods is a major challenge given the black box\nnature of its learning process within high-dimensional observation and action\nspaces. AlphaStar and OpenAI Five have shown that agents can be trained without\nany explicit hierarchical macro-actions to reach superhuman skill in games that\nrequire taking thousands of actions before reaching the final goal. Assessing\nthe agent's plans and game understanding becomes challenging given the lack of\nhierarchy or explicit representations of macro-actions in these models, coupled\nwith the incomprehensible nature of the internal representations.\n  In this paper, we study the distributed representations learned by OpenAI\nFive to investigate how game knowledge is gradually obtained over the course of\ntraining. We also introduce a general technique for learning a model from the\nagent's hidden states to identify the formation of plans and subgoals. We show\nthat the agent can learn situational similarity across actions, and find\nevidence of planning towards accomplishing subgoals minutes before they are\nexecuted. We perform a qualitative analysis of these predictions during the\ngames against the DotA 2 world champions OG in April 2019.", "published": "2019-12-13 21:49:30", "link": "http://arxiv.org/abs/1912.06721v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Short-duration Speaker Verification (SdSV) Challenge 2021: the Challenge\n  Evaluation Plan", "abstract": "This document describes the Short-duration Speaker Verification (SdSV)\nChallenge 2021. The main goal of the challenge is to evaluate new technologies\nfor text-dependent (TD) and text-independent (TI) speaker verification (SV) in\na short duration scenario. The proposed challenge evaluates SdSV with varying\ndegree of phonetic overlap between the enrollment and test utterances\n(cross-lingual). It is the first challenge with a broad focus on systematic\nbenchmark and analysis on varying degrees of phonetic variability on\nshort-duration speaker recognition. We expect that modern methods (deep neural\nnetworks in particular) will play a key role.", "published": "2019-12-13 03:38:14", "link": "http://arxiv.org/abs/1912.06311v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Associating Natural Language Comment and Source Code Entities", "abstract": "Comments are an integral part of software development; they are natural\nlanguage descriptions associated with source code elements. Understanding\nexplicit associations can be useful in improving code comprehensibility and\nmaintaining the consistency between code and comments. As an initial step\ntowards this larger goal, we address the task of associating entities in\nJavadoc comments with elements in Java source code. We propose an approach for\nautomatically extracting supervised data using revision histories of open\nsource projects and present a manually annotated evaluation dataset for this\ntask. We develop a binary classifier and a sequence labeling model by crafting\na rich feature set which encompasses various aspects of code, comments, and the\nrelationships between them. Experiments show that our systems outperform\nseveral baselines learning from the proposed supervision.", "published": "2019-12-13 22:06:59", "link": "http://arxiv.org/abs/1912.06728v1", "categories": ["cs.CL", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "An Unsupervised Domain-Independent Framework for Automated Detection of\n  Persuasion Tactics in Text", "abstract": "With the increasing growth of social media, people have started relying\nheavily on the information shared therein to form opinions and make decisions.\nWhile such a reliance is motivation for a variety of parties to promote\ninformation, it also makes people vulnerable to exploitation by slander,\nmisinformation, terroristic and predatorial advances. In this work, we aim to\nunderstand and detect such attempts at persuasion. Existing works on detecting\npersuasion in text make use of lexical features for detecting persuasive\ntactics, without taking advantage of the possible structures inherent in the\ntactics used. We formulate the task as a multi-class classification problem and\npropose an unsupervised, domain-independent machine learning framework for\ndetecting the type of persuasion used in text, which exploits the inherent\nsentence structure present in the different persuasion tactics. Our work shows\npromising results as compared to existing work.", "published": "2019-12-13 23:32:38", "link": "http://arxiv.org/abs/1912.06745v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
