{"title": "Quantification of BERT Diagnosis Generalizability Across Medical\n  Specialties Using Semantic Dataset Distance", "abstract": "Deep learning models in healthcare may fail to generalize on data from unseen\ncorpora. Additionally, no quantitative metric exists to tell how existing\nmodels will perform on new data. Previous studies demonstrated that NLP models\nof medical notes generalize variably between institutions, but ignored other\nlevels of healthcare organization. We measured SciBERT diagnosis sentiment\nclassifier generalizability between medical specialties using EHR sentences\nfrom MIMIC-III. Models trained on one specialty performed better on internal\ntest sets than mixed or external test sets (mean AUCs 0.92, 0.87, and 0.83,\nrespectively; p = 0.016). When models are trained on more specialties, they\nhave better test performances (p < 1e-4). Model performance on new corpora is\ndirectly correlated to the similarity between train and test sentence content\n(p < 1e-4). Future studies should assess additional axes of generalization to\nensure deep learning models fulfil their intended purpose across institutions,\nspecialties, and practices.", "published": "2020-08-14 23:44:11", "link": "http://arxiv.org/abs/2008.06606v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Speech To Semantics: Improve ASR and NLU Jointly via All-Neural\n  Interfaces", "abstract": "We consider the problem of spoken language understanding (SLU) of extracting\nnatural language intents and associated slot arguments or named entities from\nspeech that is primarily directed at voice assistants. Such a system subsumes\nboth automatic speech recognition (ASR) as well as natural language\nunderstanding (NLU). An end-to-end joint SLU model can be built to a required\nspecification opening up the opportunity to deploy on hardware constrained\nscenarios like devices enabling voice assistants to work offline, in a privacy\npreserving manner, whilst also reducing server costs.\n  We first present models that extract utterance intent directly from speech\nwithout intermediate text output. We then present a compositional model, which\ngenerates the transcript using the Listen Attend Spell ASR system and then\nextracts interpretation using a neural NLU model. Finally, we contrast these\nmethods to a jointly trained end-to-end joint SLU model, consisting of ASR and\nNLU subsystems which are connected by a neural network based interface instead\nof text, that produces transcripts as well as NLU interpretation. We show that\nthe jointly trained model shows improvements to ASR incorporating semantic\ninformation from NLU and also improves NLU by exposing it to ASR confusion\nencoded in the hidden layer.", "published": "2020-08-14 02:43:57", "link": "http://arxiv.org/abs/2008.06173v1", "categories": ["cs.CL", "eess.AS", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Hybrid BERT and LightGBM based Model for Predicting Emotion GIF\n  Categories on Twitter", "abstract": "The animated Graphical Interchange Format (GIF) images have been widely used\non social media as an intuitive way of expression emotion. Given their\nexpressiveness, GIFs offer a more nuanced and precise way to convey emotions.\nIn this paper, we present our solution for the EmotionGIF 2020 challenge, the\nshared task of SocialNLP 2020. To recommend GIF categories for unlabeled\ntweets, we regarded this problem as a kind of matching tasks and proposed a\nlearning to rank framework based on Bidirectional Encoder Representations from\nTransformer (BERT) and LightGBM. Our team won the 4th place with a Mean Average\nPrecision @ 6 (MAP@6) score of 0.5394 on the round 1 leaderboard.", "published": "2020-08-14 03:23:09", "link": "http://arxiv.org/abs/2008.06176v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Annotating for Hate Speech: The MaNeCo Corpus and Some Input from\n  Critical Discourse Analysis", "abstract": "This paper presents a novel scheme for the annotation of hate speech in\ncorpora of Web 2.0 commentary. The proposed scheme is motivated by the critical\nanalysis of posts made in reaction to news reports on the Mediterranean\nmigration crisis and LGBTIQ+ matters in Malta, which was conducted under the\nauspices of the EU-funded C.O.N.T.A.C.T. project. Based on the realization that\nhate speech is not a clear-cut category to begin with, appears to belong to a\ncontinuum of discriminatory discourse and is often realized through the use of\nindirect linguistic means, it is argued that annotation schemes for its\ndetection should refrain from directly including the label 'hate speech,' as\ndifferent annotators might have different thresholds as to what constitutes\nhate speech and what not. In view of this, we suggest a multi-layer annotation\nscheme, which is pilot-tested against a binary +/- hate speech classification\nand appears to yield higher inter-annotator agreement. Motivating the\npostulation of our scheme, we then present the MaNeCo corpus on which it will\neventually be used; a substantial corpus of on-line newspaper comments spanning\n10 years.", "published": "2020-08-14 07:39:21", "link": "http://arxiv.org/abs/2008.06222v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Language Models as Few-Shot Learner for Task-Oriented Dialogue Systems", "abstract": "Task-oriented dialogue systems use four connected modules, namely, Natural\nLanguage Understanding (NLU), a Dialogue State Tracking (DST), Dialogue Policy\n(DP) and Natural Language Generation (NLG). A research challenge is to learn\neach module with the least amount of samples (i.e., few-shots) given the high\ncost related to the data collection. The most common and effective technique to\nsolve this problem is transfer learning, where large language models, either\npre-trained on text or task-specific data, are fine-tuned on the few samples.\nThese methods require fine-tuning steps and a set of parameters for each task.\nDifferently, language models, such as GPT-2 (Radford et al., 2019) and GPT-3\n(Brown et al., 2020), allow few-shot learning by priming the model with few\nexamples. In this paper, we evaluate the priming few-shot ability of language\nmodels in the NLU, DST, DP and NLG tasks. Importantly, we highlight the current\nlimitations of this approach, and we discuss the possible implication for\nfuture work.", "published": "2020-08-14 08:23:21", "link": "http://arxiv.org/abs/2008.06239v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Efficient Model Inference Algorithm for Learning-based Testing of\n  Reactive Systems", "abstract": "Learning-based testing (LBT) is an emerging methodology to automate iterative\nblack-box requirements testing of software systems. The methodology involves\ncombining model inference with model checking techniques. However, a variety of\noptimisations on model inference are necessary in order to achieve scalable\ntesting for large systems. In this paper we describe the IKL learning algorithm\nwhich is an active incremental learning algorithm for deterministic Kripke\nstructures. We formally prove the correctness of IKL. We discuss the\noptimisations it incorporates to achieve scalability of testing. We also\nevaluate a black box heuristic for test termination based on convergence of IKL\nlearning.", "published": "2020-08-14 09:48:58", "link": "http://arxiv.org/abs/2008.06268v1", "categories": ["cs.FL", "cs.CL"], "primary_category": "cs.FL"}
{"title": "Graph-based Modeling of Online Communities for Fake News Detection", "abstract": "Over the past few years, there has been a substantial effort towards\nautomated detection of fake news on social media platforms. Existing research\nhas modeled the structure, style, content, and patterns in dissemination of\nonline posts, as well as the demographic traits of users who interact with\nthem. However, no attention has been directed towards modeling the properties\nof online communities that interact with the posts. In this work, we propose a\nnovel social context-aware fake news detection framework, SAFER, based on graph\nneural networks (GNNs). The proposed framework aggregates information with\nrespect to: 1) the nature of the content disseminated, 2) content-sharing\nbehavior of users, and 3) the social network of those users. We furthermore\nperform a systematic comparison of several GNN models for this task and\nintroduce novel methods based on relational and hyperbolic GNNs, which have not\nbeen previously used for user or community modeling within NLP. We empirically\ndemonstrate that our framework yields significant improvements over existing\ntext-based techniques and achieves state-of-the-art results on fake news\ndatasets from two different domains.", "published": "2020-08-14 10:01:34", "link": "http://arxiv.org/abs/2008.06274v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Predicting Event Time by Classifying Sub-Level Temporal Relations\n  Induced from a Unified Representation of Time Anchors", "abstract": "Extracting event time from news articles is a challenging but attractive\ntask. In contrast to the most existing pair-wised temporal link annotation,\nReimers et al.(2016) proposed to annotate the time anchor (a.k.a. the exact\ntime) of each event. Their work represents time anchors with discrete\nrepresentations of Single-Day/Multi-Day and Certain/Uncertain. This increases\nthe complexity of modeling the temporal relations between two time anchors,\nwhich cannot be categorized into the relations of Allen's interval algebra\n(Allen, 1990).\n  In this paper, we propose an effective method to decompose such complex\ntemporal relations into sub-level relations by introducing a unified quadruple\nrepresentation for both Single-Day/Multi-Day and Certain/Uncertain time\nanchors. The temporal relation classifiers are trained in a multi-label\nclassification manner. The system structure of our approach is much simpler\nthan the existing decision tree model (Reimers et al., 2018), which is composed\nby a dozen of node classifiers. Another contribution of this work is to\nconstruct a larger event time corpus (256 news documents) with a reasonable\nInter-Annotator Agreement (IAA), for the purpose of overcoming the data\nshortage of the existing event time corpus (36 news documents). The empirical\nresults show our approach outperforms the state-of-the-art decision tree model\nand the increase of data size obtained a significant improvement of\nperformance.", "published": "2020-08-14 16:30:07", "link": "http://arxiv.org/abs/2008.06452v1", "categories": ["cs.CL", "cs.LG", "68T01", "I.2"], "primary_category": "cs.CL"}
{"title": "End-to-End Trainable Self-Attentive Shallow Network for Text-Independent\n  Speaker Verification", "abstract": "Generalized end-to-end (GE2E) model is widely used in speaker verification\n(SV) fields due to its expandability and generality regardless of specific\nlanguages. However, the long-short term memory (LSTM) based on GE2E has two\nlimitations: First, the embedding of GE2E suffers from vanishing gradient,\nwhich leads to performance degradation for very long input sequences. Secondly,\nutterances are not represented as a properly fixed dimensional vector. In this\npaper, to overcome issues mentioned above, we propose a novel framework for SV,\nend-to-end trainable self-attentive shallow network (SASN), incorporating a\ntime-delay neural network (TDNN) and a self-attentive pooling mechanism based\non the self-attentive x-vector system during an utterance embedding phase. We\ndemonstrate that the proposed model is highly efficient, and provides more\naccurate speaker verification than GE2E. For VCTK dataset, with just less than\nhalf the size of GE2E, the proposed model showed significant performance\nimprovement over GE2E of about 63%, 67%, and 85% in EER (Equal error rate), DCF\n(Detection cost function), and AUC (Area under the curve), respectively.\nNotably, when the input length becomes longer, the DCF score improvement of the\nproposed model is about 17 times greater than that of GE2E.", "published": "2020-08-14 00:46:50", "link": "http://arxiv.org/abs/2008.06146v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Adaptable Multi-Domain Language Model for Transformer ASR", "abstract": "We propose an adapter based multi-domain Transformer based language model\n(LM) for Transformer ASR. The model consists of a big size common LM and small\nsize adapters. The model can perform multi-domain adaptation with only the\nsmall size adapters and its related layers. The proposed model can reuse the\nfull fine-tuned LM which is fine-tuned using all layers of an original model.\nThe proposed LM can be expanded to new domains by adding about 2% of parameters\nfor a first domain and 13% parameters for after second domain. The proposed\nmodel is also effective in reducing the model maintenance cost because it is\npossible to omit the costly and time-consuming common LM pre-training process.\nUsing proposed adapter based approach, we observed that a general LM with\nadapter can outperform a dedicated music domain LM in terms of word error rate\n(WER).", "published": "2020-08-14 06:33:26", "link": "http://arxiv.org/abs/2008.06208v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised vs. transfer learning for multimodal one-shot matching of\n  speech and images", "abstract": "We consider the task of multimodal one-shot speech-image matching. An agent\nis shown a picture along with a spoken word describing the object in the\npicture, e.g. cookie, broccoli and ice-cream. After observing one paired\nspeech-image example per class, it is shown a new set of unseen pictures, and\nasked to pick the \"ice-cream\". Previous work attempted to tackle this problem\nusing transfer learning: supervised models are trained on labelled background\ndata not containing any of the one-shot classes. Here we compare transfer\nlearning to unsupervised models trained on unlabelled in-domain data. On a\ndataset of paired isolated spoken and visual digits, we specifically compare\nunsupervised autoencoder-like models to supervised classifier and Siamese\nneural networks. In both unimodal and multimodal few-shot matching experiments,\nwe find that transfer learning outperforms unsupervised training. We also\npresent experiments towards combining the two methodologies, but find that\ntransfer learning still performs best (despite idealised experiments showing\nthe benefits of unsupervised learning).", "published": "2020-08-14 09:13:37", "link": "http://arxiv.org/abs/2008.06258v1", "categories": ["cs.CL", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Partial Orders, Residuation, and First-Order Linear Logic", "abstract": "We will investigate proof-theoretic and linguistic aspects of first-order\nlinear logic. We will show that adding partial order constraints in such a way\nthat each sequent defines a unique linear order on the antecedent formulas of a\nsequent allows us to define many useful logical operators. In addition, the\npartial order constraints improve the efficiency of proof search.", "published": "2020-08-14 13:06:21", "link": "http://arxiv.org/abs/2008.06351v1", "categories": ["cs.LO", "cs.CL", "math.LO"], "primary_category": "cs.LO"}
{"title": "Hate Speech Detection and Racial Bias Mitigation in Social Media based\n  on BERT model", "abstract": "Disparate biases associated with datasets and trained classifiers in hateful\nand abusive content identification tasks have raised many concerns recently.\nAlthough the problem of biased datasets on abusive language detection has been\naddressed more frequently, biases arising from trained classifiers have not yet\nbeen a matter of concern. Here, we first introduce a transfer learning approach\nfor hate speech detection based on an existing pre-trained language model\ncalled BERT and evaluate the proposed model on two publicly available datasets\nannotated for racism, sexism, hate or offensive content on Twitter. Next, we\nintroduce a bias alleviation mechanism in hate speech detection task to\nmitigate the effect of bias in training set during the fine-tuning of our\npre-trained BERT-based model. Toward that end, we use an existing\nregularization method to reweight input samples, thereby decreasing the effects\nof high correlated training set' s n-grams with class labels, and then\nfine-tune our pre-trained BERT-based model with the new re-weighted samples. To\nevaluate our bias alleviation mechanism, we employ a cross-domain approach in\nwhich we use the trained classifiers on the aforementioned datasets to predict\nthe labels of two new datasets from Twitter, AAE-aligned and White-aligned\ngroups, which indicate tweets written in African-American English (AAE) and\nStandard American English (SAE) respectively. The results show the existence of\nsystematic racial bias in trained classifiers as they tend to assign tweets\nwritten in AAE from AAE-aligned group to negative classes such as racism,\nsexism, hate, and offensive more often than tweets written in SAE from\nWhite-aligned. However, the racial bias in our classifiers reduces\nsignificantly after our bias alleviation mechanism is incorporated. This work\ncould institute the first step towards debiasing hate speech and abusive\nlanguage detection systems.", "published": "2020-08-14 16:47:25", "link": "http://arxiv.org/abs/2008.06460v2", "categories": ["cs.SI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Adaptation Algorithms for Neural Network-Based Speech Recognition: An\n  Overview", "abstract": "We present a structured overview of adaptation algorithms for neural\nnetwork-based speech recognition, considering both hybrid hidden Markov model /\nneural network systems and end-to-end neural network systems, with a focus on\nspeaker adaptation, domain adaptation, and accent adaptation. The overview\ncharacterizes adaptation algorithms as based on embeddings, model parameter\nadaptation, or data augmentation. We present a meta-analysis of the performance\nof speech recognition adaptation algorithms, based on relative error rate\nreductions as reported in the literature.", "published": "2020-08-14 21:50:03", "link": "http://arxiv.org/abs/2008.06580v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Online Speaker Adaptation for WaveNet-based Neural Vocoders", "abstract": "In this paper, we propose an online speaker adaptation method for\nWaveNet-based neural vocoders in order to improve their performance on\nspeaker-independent waveform generation. In this method, a speaker encoder is\nfirst constructed using a large speaker-verification dataset which can extract\na speaker embedding vector from an utterance pronounced by an arbitrary\nspeaker. At the training stage, a speaker-aware WaveNet vocoder is then built\nusing a multi-speaker dataset which adopts both acoustic feature sequences and\nspeaker embedding vectors as conditions.At the generation stage, we first feed\nthe acoustic feature sequence from a test speaker into the speaker encoder to\nobtain the speaker embedding vector of the utterance. Then, both the speaker\nembedding vector and acoustic features pass the speaker-aware WaveNet vocoder\nto reconstruct speech waveforms. Experimental results demonstrate that our\nmethod can achieve a better objective and subjective performance on\nreconstructing waveforms of unseen speakers than the conventional\nspeaker-independent WaveNet vocoder.", "published": "2020-08-14 03:55:20", "link": "http://arxiv.org/abs/2008.06182v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Data augmentation and loss normalization for deep noise suppression", "abstract": "Speech enhancement using neural networks is recently receiving large\nattention in research and being integrated in commercial devices and\napplications. In this work, we investigate data augmentation techniques for\nsupervised deep learning-based speech enhancement. We show that not only\naugmenting SNR values to a broader range and a continuous distribution helps to\nregularize training, but also augmenting the spectral and dynamic level\ndiversity. However, to not degrade training by level augmentation, we propose a\nmodification to signal-based loss functions by applying sequence level\nnormalization. We show in experiments that this normalization overcomes the\ndegradation caused by training on sequences with imbalanced signal levels, when\nusing a level-dependent loss function.", "published": "2020-08-14 15:16:50", "link": "http://arxiv.org/abs/2008.06412v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Semi-supervised learning using teacher-student models for vocal melody\n  extraction", "abstract": "The lack of labeled data is a major obstacle in many music information\nretrieval tasks such as melody extraction, where labeling is extremely\nlaborious or costly. Semi-supervised learning (SSL) provides a solution to\nalleviate the issue by leveraging a large amount of unlabeled data. In this\npaper, we propose an SSL method using teacher-student models for vocal melody\nextraction. The teacher model is pre-trained with labeled data and guides the\nstudent model to make identical predictions given unlabeled input in a\nself-training setting. We examine three setups of teacher-student models with\ndifferent data augmentation schemes and loss functions. Also, considering the\nscarcity of labeled data in the test phase, we artificially generate\nlarge-scale testing data with pitch labels from unlabeled data using an\nanalysis-synthesis method. The results show that the SSL method significantly\nincreases the performance against supervised learning only and the improvement\ndepends on the teacher-student models, the size of unlabeled data, the number\nof self-training iterations, and other training details. We also find that it\nis essential to ensure that the unlabeled audio has vocal parts. Finally, we\nshow that the proposed SSL method enables a baseline convolutional recurrent\nneural network model to achieve performance comparable to state-of-the-arts.", "published": "2020-08-14 13:24:37", "link": "http://arxiv.org/abs/2008.06358v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Impact of Label Noise on a Music Tagger", "abstract": "We explore how much can be learned from noisy labels in audio music tagging.\nOur experiments show that carefully annotated labels result in highest figures\nof merit, but even high amounts of noisy labels contain enough information for\nsuccessful learning. Artificial corruption of curated data allows us to\nquantize this contribution of noisy labels.", "published": "2020-08-14 10:00:30", "link": "http://arxiv.org/abs/2008.06273v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio-Visual Event Localization via Recursive Fusion by Joint\n  Co-Attention", "abstract": "The major challenge in audio-visual event localization task lies in how to\nfuse information from multiple modalities effectively. Recent works have shown\nthat attention mechanism is beneficial to the fusion process. In this paper, we\npropose a novel joint attention mechanism with multimodal fusion methods for\naudio-visual event localization. Particularly, we present a concise yet valid\narchitecture that effectively learns representations from multiple modalities\nin a joint manner. Initially, visual features are combined with auditory\nfeatures and then turned into joint representations. Next, we make use of the\njoint representations to attend to visual features and auditory features,\nrespectively. With the help of this joint co-attention, new visual and auditory\nfeatures are produced, and thus both features can enjoy the mutually improved\nbenefits from each other. It is worth noting that the joint co-attention unit\nis recursive meaning that it can be performed multiple times for obtaining\nbetter joint representations progressively. Extensive experiments on the public\nAVE dataset have shown that the proposed method achieves significantly better\nresults than the state-of-the-art methods.", "published": "2020-08-14 21:50:26", "link": "http://arxiv.org/abs/2008.06581v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
