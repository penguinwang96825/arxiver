{"title": "Argument Mining with Structured SVMs and RNNs", "abstract": "We propose a novel factor graph model for argument mining, designed for\nsettings in which the argumentative relations in a document do not necessarily\nform a tree structure. (This is the case in over 20% of the web comments\ndataset we release.) Our model jointly learns elementary unit type\nclassification and argumentative relation prediction. Moreover, our model\nsupports SVM and RNN parametrizations, can enforce structure constraints (e.g.,\ntransitivity), and can express dependencies between adjacent relations and\npropositions. Our approaches outperform unstructured baselines in both web\ncomments and argumentative essay datasets.", "published": "2017-04-23 01:14:55", "link": "http://arxiv.org/abs/1704.06869v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Deep Keyphrase Generation", "abstract": "Keyphrase provides highly-condensed information that can be effectively used\nfor understanding, organizing and retrieving text content. Though previous\nstudies have provided many workable solutions for automated keyphrase\nextraction, they commonly divided the to-be-summarized content into multiple\ntext chunks, then ranked and selected the most meaningful ones. These\napproaches could neither identify keyphrases that do not appear in the text,\nnor capture the real semantic meaning behind the text. We propose a generative\nmodel for keyphrase prediction with an encoder-decoder framework, which can\neffectively overcome the above drawbacks. We name it as deep keyphrase\ngeneration since it attempts to capture the deep semantic meaning of the\ncontent with a deep learning method. Empirical analysis on six datasets\ndemonstrates that our proposed model not only achieves a significant\nperformance boost on extracting keyphrases that appear in the source text, but\nalso can generate absent keyphrases based on the semantic meaning of the text.\nCode and dataset are available at\nhttps://github.com/memray/OpenNMT-kpg-release.", "published": "2017-04-23 04:34:26", "link": "http://arxiv.org/abs/1704.06879v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation via Binary Code Prediction", "abstract": "In this paper, we propose a new method for calculating the output layer in\nneural machine translation systems. The method is based on predicting a binary\ncode for each word and can reduce computation time/memory requirements of the\noutput layer to be logarithmic in vocabulary size in the best case. In\naddition, we also introduce two advanced approaches to improve the robustness\nof the proposed model: using error-correcting codes and combining softmax and\nbinary codes. Experiments on two English-Japanese bidirectional translation\ntasks show proposed models achieve BLEU scores that approach the softmax, while\nreducing memory usage to the order of less than 1/10 and improving decoding\nspeed on CPUs by x5 to x10.", "published": "2017-04-23 12:38:13", "link": "http://arxiv.org/abs/1704.06918v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A* CCG Parsing with a Supertag and Dependency Factored Model", "abstract": "We propose a new A* CCG parsing model in which the probability of a tree is\ndecomposed into factors of CCG categories and its syntactic dependencies both\ndefined on bi-directional LSTMs. Our factored model allows the precomputation\nof all probabilities and runs very efficiently, while modeling sentence\nstructures explicitly via dependencies. Our model achieves the state-of-the-art\nresults on English and Japanese CCG parsing.", "published": "2017-04-23 15:16:53", "link": "http://arxiv.org/abs/1704.06936v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Create and Reuse Words in Open-Vocabulary Neural Language\n  Modeling", "abstract": "Fixed-vocabulary language models fail to account for one of the most\ncharacteristic statistical facts of natural language: the frequent creation and\nreuse of new word types. Although character-level language models offer a\npartial solution in that they can create word types not attested in the\ntraining corpus, they do not capture the \"bursty\" distribution of such words.\nIn this paper, we augment a hierarchical LSTM language model that generates\nsequences of word tokens character by character with a caching mechanism that\nlearns to reuse previously generated words. To validate our model we construct\na new open-vocabulary language modeling corpus (the Multilingual Wikipedia\nCorpus, MWC) from comparable Wikipedia articles in 7 typologically diverse\nlanguages and demonstrate the effectiveness of our model across this range of\nlanguages.", "published": "2017-04-23 21:31:22", "link": "http://arxiv.org/abs/1704.06986v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Skim Text", "abstract": "Recurrent Neural Networks are showing much promise in many sub-areas of\nnatural language processing, ranging from document classification to machine\ntranslation to automatic question answering. Despite their promise, many\nrecurrent models have to read the whole text word by word, making it slow to\nhandle long documents. For example, it is difficult to use a recurrent network\nto read a book and answer questions about it. In this paper, we present an\napproach of reading text while skipping irrelevant information if needed. The\nunderlying model is a recurrent network that learns how far to jump after\nreading a few words of the input text. We employ a standard policy gradient\nmethod to train the model to make discrete jumping decisions. In our benchmarks\non four different tasks, including number prediction, sentiment analysis, news\narticle classification and automatic Q\\&A, our proposed model, a modified LSTM\nwith jumping, is up to 6 times faster than the standard sequential LSTM, while\nmaintaining the same or even better accuracy.", "published": "2017-04-23 03:54:22", "link": "http://arxiv.org/abs/1704.06877v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning weakly supervised multimodal phoneme embeddings", "abstract": "Recent works have explored deep architectures for learning multimodal speech\nrepresentation (e.g. audio and images, articulation and audio) in a supervised\nway. Here we investigate the role of combining different speech modalities,\ni.e. audio and visual information representing the lips movements, in a weakly\nsupervised way using Siamese networks and lexical same-different side\ninformation. In particular, we ask whether one modality can benefit from the\nother to provide a richer representation for phone recognition in a weakly\nsupervised setting. We introduce mono-task and multi-task methods for merging\nspeech and visual modalities for phone recognition. The mono-task learning\nconsists in applying a Siamese network on the concatenation of the two\nmodalities, while the multi-task learning receives several different\ncombinations of modalities at train time. We show that multi-task learning\nenhances discriminability for visual and multimodal inputs while minimally\nimpacting auditory inputs. Furthermore, we present a qualitative analysis of\nthe obtained phone embeddings, and show that cross-modal visual input can\nimprove the discriminability of phonological features which are visually\ndiscernable (rounding, open/close, labial place of articulation), resulting in\nrepresentations that are closer to abstract linguistic features than those\nbased on audio only.", "published": "2017-04-23 11:27:53", "link": "http://arxiv.org/abs/1704.06913v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Translating Neuralese", "abstract": "Several approaches have recently been proposed for learning decentralized\ndeep multiagent policies that coordinate via a differentiable communication\nchannel. While these policies are effective for many tasks, interpretation of\ntheir induced communication strategies has remained a challenge. Here we\npropose to interpret agents' messages by translating them. Unlike in typical\nmachine translation problems, we have no parallel data to learn from. Instead\nwe develop a translation model based on the insight that agent messages and\nnatural language strings mean the same thing if they induce the same belief\nabout the world in a listener. We present theoretical guarantees and empirical\nevidence that our approach preserves both the semantics and pragmatics of\nmessages by ensuring that players communicating through a translation layer do\nnot suffer a substantial loss in reward relative to players with a common\nlanguage.", "published": "2017-04-23 18:46:42", "link": "http://arxiv.org/abs/1704.06960v5", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Discourse-Based Objectives for Fast Unsupervised Sentence Representation\n  Learning", "abstract": "This work presents a novel objective function for the unsupervised training\nof neural network sentence encoders. It exploits signals from paragraph-level\ndiscourse coherence to train these models to understand text. Our objective is\npurely discriminative, allowing us to train models many times faster than was\npossible under prior methods, and it yields models which perform well in\nextrinsic evaluations.", "published": "2017-04-23 09:15:35", "link": "http://arxiv.org/abs/1705.00557v1", "categories": ["cs.CL", "cs.LG", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Naturalizing a Programming Language via Interactive Learning", "abstract": "Our goal is to create a convenient natural language interface for performing\nwell-specified but complex actions such as analyzing data, manipulating text,\nand querying databases. However, existing natural language interfaces for such\ntasks are quite primitive compared to the power one wields with a programming\nlanguage. To bridge this gap, we start with a core programming language and\nallow users to \"naturalize\" the core language incrementally by defining\nalternative, more natural syntax and increasingly complex concepts in terms of\ncompositions of simpler ones. In a voxel world, we show that a community of\nusers can simultaneously teach a common system a diverse language and use it to\nbuild hundreds of complex voxel structures. Over the course of three days,\nthese users went from using only the core language to using the naturalized\nlanguage in 85.9\\% of the last 10K utterances.", "published": "2017-04-23 18:13:10", "link": "http://arxiv.org/abs/1704.06956v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG", "I.2.7; I.2.6; I.2.1"], "primary_category": "cs.CL"}
{"title": "Differentiable Scheduled Sampling for Credit Assignment", "abstract": "We demonstrate that a continuous relaxation of the argmax operation can be\nused to create a differentiable approximation to greedy decoding for\nsequence-to-sequence (seq2seq) models. By incorporating this approximation into\nthe scheduled sampling training procedure (Bengio et al., 2015)--a well-known\ntechnique for correcting exposure bias--we introduce a new training objective\nthat is continuous and differentiable everywhere and that can provide\ninformative gradients near points where previous decoding decisions change\ntheir value. In addition, by using a related approximation, we demonstrate a\nsimilar approach to sampled-based training. Finally, we show that our approach\noutperforms cross-entropy training and scheduled sampling procedures in two\nsequence prediction tasks: named entity recognition and machine translation.", "published": "2017-04-23 20:05:36", "link": "http://arxiv.org/abs/1704.06970v1", "categories": ["cs.CL", "cs.LG", "cs.NE", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
