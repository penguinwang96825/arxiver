{"title": "OpenViDial: A Large-Scale, Open-Domain Dialogue Dataset with Visual\n  Contexts", "abstract": "When humans converse, what a speaker will say next significantly depends on\nwhat he sees. Unfortunately, existing dialogue models generate dialogue\nutterances only based on preceding textual contexts, and visual contexts are\nrarely considered. This is due to a lack of a large-scale multi-module dialogue\ndataset with utterances paired with visual contexts. In this paper, we release\n{\\bf OpenViDial}, a large-scale multi-module dialogue dataset. The dialogue\nturns and visual contexts are extracted from movies and TV series, where each\ndialogue turn is paired with the corresponding visual context in which it takes\nplace. OpenViDial contains a total number of 1.1 million dialogue turns, and\nthus 1.1 million visual contexts stored in images. Based on this dataset, we\npropose a family of encoder-decoder models leveraging both textual and visual\ncontexts, from coarse-grained image features extracted from CNNs to\nfine-grained object features extracted from Faster R-CNNs. We observe that\nvisual information significantly improves dialogue generation qualities,\nverifying the necessity of integrating multi-modal features for dialogue\nlearning. Our work marks an important step towards large-scale multi-modal\ndialogue learning.", "published": "2020-12-30 03:02:50", "link": "http://arxiv.org/abs/2012.15015v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Identification of Devanagari Poems", "abstract": "Language Identification is a very important part of several text processing\npipelines. Extensive research has been done in this field. This paper proposes\na procedure for automatic language identification of poems for poem analysis\ntask, consisting of 10 Devanagari based languages of India i.e. Angika, Awadhi,\nBraj, Bhojpuri, Chhattisgarhi, Garhwali, Haryanvi, Hindi, Magahi, and Maithili.\nWe collated corpora of poems of varying length and studied the similarity of\npoems among the 10 languages at the lexical level. Finally, various language\nidentification systems based on supervised machine learning and deep learning\ntechniques are applied and evaluated.", "published": "2020-12-30 03:36:18", "link": "http://arxiv.org/abs/2012.15023v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reservoir Transformers", "abstract": "We demonstrate that transformers obtain impressive performance even when some\nof the layers are randomly initialized and never updated. Inspired by old and\nwell-established ideas in machine learning, we explore a variety of non-linear\n\"reservoir\" layers interspersed with regular transformer layers, and show\nimprovements in wall-clock compute time until convergence, as well as overall\nperformance, on various machine translation and (masked) language modelling\ntasks.", "published": "2020-12-30 05:20:16", "link": "http://arxiv.org/abs/2012.15045v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Pre-trained Language Model with Lexical Simplification", "abstract": "For both human readers and pre-trained language models (PrLMs), lexical\ndiversity may lead to confusion and inaccuracy when understanding the\nunderlying semantic meanings of given sentences. By substituting complex words\nwith simple alternatives, lexical simplification (LS) is a recognized method to\nreduce such lexical diversity, and therefore to improve the understandability\nof sentences. In this paper, we leverage LS and propose a novel approach which\ncan effectively improve the performance of PrLMs in text classification. A\nrule-based simplification process is applied to a given sentence. PrLMs are\nencouraged to predict the real label of the given sentence with auxiliary\ninputs from the simplified version. Using strong PrLMs (BERT and ELECTRA) as\nbaselines, our approach can still further improve the performance in various\ntext classification tasks.", "published": "2020-12-30 07:49:00", "link": "http://arxiv.org/abs/2012.15070v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Human Evaluation of Spoken vs. Visual Explanations for Open-Domain QA", "abstract": "While research on explaining predictions of open-domain QA systems (ODQA) to\nusers is gaining momentum, most works have failed to evaluate the extent to\nwhich explanations improve user trust. While few works evaluate explanations\nusing user studies, they employ settings that may deviate from the end-user's\nusage in-the-wild: ODQA is most ubiquitous in voice-assistants, yet current\nresearch only evaluates explanations using a visual display, and may\nerroneously extrapolate conclusions about the most performant explanations to\nother modalities. To alleviate these issues, we conduct user studies that\nmeasure whether explanations help users correctly decide when to accept or\nreject an ODQA system's answer. Unlike prior work, we control for explanation\nmodality, e.g., whether they are communicated to users through a spoken or\nvisual interface, and contrast effectiveness across modalities. Our results\nshow that explanations derived from retrieved evidence passages can outperform\nstrong baselines (calibrated confidence) across modalities but the best\nexplanation strategy in fact changes with the modality. We show common failure\ncases of current explanations, emphasize end-to-end evaluation of explanations,\nand caution against evaluating them in proxy modalities that are different from\ndeployment.", "published": "2020-12-30 08:19:02", "link": "http://arxiv.org/abs/2012.15075v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Zero-Shot Translation by Disentangling Positional Information", "abstract": "Multilingual neural machine translation has shown the capability of directly\ntranslating between language pairs unseen in training, i.e. zero-shot\ntranslation. Despite being conceptually attractive, it often suffers from low\noutput quality. The difficulty of generalizing to new translation directions\nsuggests the model representations are highly specific to those language pairs\nseen in training. We demonstrate that a main factor causing the\nlanguage-specific representations is the positional correspondence to input\ntokens. We show that this can be easily alleviated by removing residual\nconnections in an encoder layer. With this modification, we gain up to 18.5\nBLEU points on zero-shot translation while retaining quality on supervised\ndirections. The improvements are particularly prominent between related\nlanguages, where our proposed model outperforms pivot-based translation.\nMoreover, our approach allows easy integration of new languages, which\nsubstantially expands translation coverage. By thorough inspections of the\nhidden layer outputs, we show that our approach indeed leads to more\nlanguage-independent representations.", "published": "2020-12-30 12:20:41", "link": "http://arxiv.org/abs/2012.15127v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving BERT with Syntax-aware Local Attention", "abstract": "Pre-trained Transformer-based neural language models, such as BERT, have\nachieved remarkable results on varieties of NLP tasks. Recent works have shown\nthat attention-based models can benefit from more focused attention over local\nregions. Most of them restrict the attention scope within a linear span, or\nconfine to certain tasks such as machine translation and question answering. In\nthis paper, we propose a syntax-aware local attention, where the attention\nscopes are restrained based on the distances in the syntactic structure. The\nproposed syntax-aware local attention can be integrated with pretrained\nlanguage models, such as BERT, to render the model to focus on syntactically\nrelevant words. We conduct experiments on various single-sentence benchmarks,\nincluding sentence classification and sequence labeling tasks. Experimental\nresults show consistent gains over BERT on all benchmark datasets. The\nextensive studies verify that our model achieves better performance owing to\nmore focused attention over syntactically relevant words.", "published": "2020-12-30 13:29:58", "link": "http://arxiv.org/abs/2012.15150v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Memory Efficient Baseline for Open Domain Question Answering", "abstract": "Recently, retrieval systems based on dense representations have led to\nimportant improvements in open-domain question answering, and related tasks.\nWhile very effective, this approach is also memory intensive, as the dense\nvectors for the whole knowledge source need to be kept in memory. In this\npaper, we study how the memory footprint of dense retriever-reader systems can\nbe reduced. We consider three strategies to reduce the index size: dimension\nreduction, vector quantization and passage filtering. We evaluate our approach\non two question answering benchmarks: TriviaQA and NaturalQuestions, showing\nthat it is possible to get competitive systems using less than 6Gb of memory.", "published": "2020-12-30 13:46:06", "link": "http://arxiv.org/abs/2012.15156v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Introducing Orthogonal Constraint in Structural Probes", "abstract": "With the recent success of pre-trained models in NLP, a significant focus was\nput on interpreting their representations. One of the most prominent approaches\nis structural probing (Hewitt and Manning, 2019), where a linear projection of\nword embeddings is performed in order to approximate the topology of dependency\nstructures. In this work, we introduce a new type of structural probing, where\nthe linear projection is decomposed into 1. isomorphic space rotation; 2.\nlinear scaling that identifies and scales the most relevant dimensions. In\naddition to syntactic dependency, we evaluate our method on novel tasks\n(lexical hypernymy and position in a sentence). We jointly train the probes for\nmultiple tasks and experimentally show that lexical and syntactic information\nis separated in the representations. Moreover, the orthogonal constraint makes\nthe Structural Probes less vulnerable to memorization.", "published": "2020-12-30 17:14:25", "link": "http://arxiv.org/abs/2012.15228v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Sequence-to-Sequence Models Crack Substitution Ciphers?", "abstract": "Decipherment of historical ciphers is a challenging problem. The language of\nthe target plaintext might be unknown, and ciphertext can have a lot of noise.\nState-of-the-art decipherment methods use beam search and a neural language\nmodel to score candidate plaintext hypotheses for a given cipher, assuming the\nplaintext language is known. We propose an end-to-end multilingual model for\nsolving simple substitution ciphers. We test our model on synthetic and real\nhistorical ciphers and show that our proposed method can decipher text without\nexplicit language identification while still being robust to noise.", "published": "2020-12-30 17:16:33", "link": "http://arxiv.org/abs/2012.15229v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Label-aware Event Trigger and Argument Classification", "abstract": "Identifying events and mapping them to pre-defined event types has long been\nan important natural language processing problem. Most previous work has been\nheavily relying on labor-intensive and domain-specific annotations while\nignoring the semantic meaning contained in the labels of the event types. As a\nresult, the learned models cannot effectively generalize to new domains, where\nnew event types could be introduced. In this paper, we propose an unsupervised\nevent extraction pipeline, which first identifies events with available tools\n(e.g., SRL) and then automatically maps them to pre-defined event types with\nour proposed unsupervised classification model. Rather than relying on\nannotated data, our model matches the semantics of identified events with those\nof event type labels. Specifically, we leverage pre-trained language models to\ncontextually represent pre-defined types for both event triggers and arguments.\nAfter we map identified events to the target types via representation\nsimilarity, we use the event ontology (e.g., argument type \"Victim\" can only\nappear as the argument of event type \"Attack\") as global constraints to\nregularize the prediction. The proposed approach is shown to be very effective\nwhen tested on the ACE-2005 dataset, which has 33 trigger and 22 argument\ntypes. Without using any annotation, we successfully map 83% of the triggers\nand 54% of the arguments to the correct types, almost doubling the performance\nof previous zero-shot approaches.", "published": "2020-12-30 17:47:24", "link": "http://arxiv.org/abs/2012.15243v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting cross-linguistic adjective order with information gain", "abstract": "Languages vary in their placement of multiple adjectives before, after, or\nsurrounding the noun, but they typically exhibit strong intra-language\ntendencies on the relative order of those adjectives (e.g., the preference for\n`big blue box' in English, `grande bo\\^{i}te bleue' in French, and\n`alsund\\={u}q al'azraq alkab\\={\\i}r' in Arabic). We advance a new quantitative\naccount of adjective order across typologically-distinct languages based on\nmaximizing information gain. Our model addresses the left-right asymmetry of\nFrench-type ANA sequences with the same approach as AAN and NAA orderings,\nwithout appeal to other mechanisms. We find that, across 32 languages, the\npreferred order of adjectives largely mirrors an efficient algorithm of\nmaximizing information gain.", "published": "2020-12-30 18:21:55", "link": "http://arxiv.org/abs/2012.15263v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ECONET: Effective Continual Pretraining of Language Models for Event\n  Temporal Reasoning", "abstract": "While pre-trained language models (PTLMs) have achieved noticeable success on\nmany NLP tasks, they still struggle for tasks that require event temporal\nreasoning, which is essential for event-centric applications. We present a\ncontinual pre-training approach that equips PTLMs with targeted knowledge about\nevent temporal relations. We design self-supervised learning objectives to\nrecover masked-out event and temporal indicators and to discriminate sentences\nfrom their corrupted counterparts (where event or temporal indicators got\nreplaced). By further pre-training a PTLM with these objectives jointly, we\nreinforce its attention to event and temporal information, yielding enhanced\ncapability on event temporal reasoning. This effective continual pre-training\nframework for event temporal reasoning (ECONET) improves the PTLMs' fine-tuning\nperformances across five relation extraction and question answering tasks and\nachieves new or on-par state-of-the-art performances in most of our downstream\ntasks.", "published": "2020-12-30 18:57:16", "link": "http://arxiv.org/abs/2012.15283v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DynaSent: A Dynamic Benchmark for Sentiment Analysis", "abstract": "We introduce DynaSent ('Dynamic Sentiment'), a new English-language benchmark\ntask for ternary (positive/negative/neutral) sentiment analysis. DynaSent\ncombines naturally occurring sentences with sentences created using the\nopen-source Dynabench Platform, which facilities human-and-model-in-the-loop\ndataset creation. DynaSent has a total of 121,634 sentences, each validated by\nfive crowdworkers, and its development and test splits are designed to produce\nchance performance for even the best models we have been able to develop; when\nfuture models solve this task, we will use them to create DynaSent version 2,\ncontinuing the dynamic evolution of this benchmark. Here, we report on the\ndataset creation effort, focusing on the steps we took to increase quality and\nreduce artifacts. We also present evidence that DynaSent's Neutral category is\nmore coherent than the comparable category in other benchmarks, and we motivate\ntraining models from scratch for each round over successive fine-tuning.", "published": "2020-12-30 22:38:21", "link": "http://arxiv.org/abs/2012.15349v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deriving Contextualised Semantic Features from BERT (and Other\n  Transformer Model) Embeddings", "abstract": "Models based on the transformer architecture, such as BERT, have marked a\ncrucial step forward in the field of Natural Language Processing. Importantly,\nthey allow the creation of word embeddings that capture important semantic\ninformation about words in context. However, as single entities, these\nembeddings are difficult to interpret and the models used to create them have\nbeen described as opaque. Binder and colleagues proposed an intuitive embedding\nspace where each dimension is based on one of 65 core semantic features.\nUnfortunately, the space only exists for a small dataset of 535 words, limiting\nits uses. Previous work (Utsumi, 2018, 2020, Turton, Vinson & Smith, 2020) has\nshown that Binder features can be derived from static embeddings and\nsuccessfully extrapolated to a large new vocabulary. Taking the next step, this\npaper demonstrates that Binder features can be derived from the BERT embedding\nspace. This provides contextualised Binder embeddings, which can aid in\nunderstanding semantic differences between words in context. It additionally\nprovides insights into how semantic features are represented across the\ndifferent layers of the BERT model.", "published": "2020-12-30 22:52:29", "link": "http://arxiv.org/abs/2012.15353v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UnNatural Language Inference", "abstract": "Recent investigations into the inner-workings of state-of-the-art large-scale\npre-trained Transformer-based Natural Language Understanding (NLU) models\nindicate that they appear to know humanlike syntax, at least to some extent. We\nprovide novel evidence that complicates this claim: we find that\nstate-of-the-art Natural Language Inference (NLI) models assign the same labels\nto permuted examples as they do to the original, i.e. they are largely\ninvariant to random word-order permutations. This behavior notably differs from\nthat of humans; we struggle with ungrammatical sentences. To measure the\nseverity of this issue, we propose a suite of metrics and investigate which\nproperties of particular permutations lead models to be word-order invariant.\nIn the MNLI dataset, for example, we find almost all (98.7%) examples contain\nat least one permutation which elicits the gold label. Models are sometimes\neven able to assign gold labels to permutations that they originally failed to\npredict correctly. We provide a comprehensive empirical evaluation of this\nphenomenon, and further show that this issue exists for both Transformers and\npre-Transformer RNN / ConvNet based encoders, as well as across multiple\nlanguages (English and Mandarin Chinese). Our code and data are available at\nhttps://github.com/facebookresearch/unlu.", "published": "2020-12-30 20:40:48", "link": "http://arxiv.org/abs/2101.00010v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ERICA: Improving Entity and Relation Understanding for Pre-trained\n  Language Models via Contrastive Learning", "abstract": "Pre-trained Language Models (PLMs) have shown superior performance on various\ndownstream Natural Language Processing (NLP) tasks. However, conventional\npre-training objectives do not explicitly model relational facts in text, which\nare crucial for textual understanding. To address this issue, we propose a\nnovel contrastive learning framework ERICA to obtain a deep understanding of\nthe entities and their relations in text. Specifically, we define two novel\npre-training tasks to better understand entities and relations: (1) the entity\ndiscrimination task to distinguish which tail entity can be inferred by the\ngiven head entity and relation; (2) the relation discrimination task to\ndistinguish whether two relations are close or not semantically, which involves\ncomplex relational reasoning. Experimental results demonstrate that ERICA can\nimprove typical PLMs (BERT and RoBERTa) on several language understanding\ntasks, including relation extraction, entity typing and question answering,\nespecially under low-resource settings.", "published": "2020-12-30 03:35:22", "link": "http://arxiv.org/abs/2012.15022v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Sindhi Word Segmentation using Subword Representation Learning\n  and Position-aware Self-attention", "abstract": "Sindhi word segmentation is a challenging task due to space omission and\ninsertion issues. The Sindhi language itself adds to this complexity. It's\ncursive and consists of characters with inherent joining and non-joining\nproperties, independent of word boundaries. Existing Sindhi word segmentation\nmethods rely on designing and combining hand-crafted features. However, these\nmethods have limitations, such as difficulty handling out-of-vocabulary words,\nlimited robustness for other languages, and inefficiency with large amounts of\nnoisy or raw text. Neural network-based models, in contrast, can automatically\ncapture word boundary information without requiring prior knowledge. In this\npaper, we propose a Subword-Guided Neural Word Segmenter (SGNWS) that addresses\nword segmentation as a sequence labeling task. The SGNWS model incorporates\nsubword representation learning through a bidirectional long short-term memory\nencoder, position-aware self-attention, and a conditional random field. Our\nempirical results demonstrate that the SGNWS model achieves state-of-the-art\nperformance in Sindhi word segmentation on six datasets.", "published": "2020-12-30 08:31:31", "link": "http://arxiv.org/abs/2012.15079v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Synthetic Source Language Augmentation for Colloquial Neural Machine\n  Translation", "abstract": "Neural machine translation (NMT) is typically domain-dependent and\nstyle-dependent, and it requires lots of training data. State-of-the-art NMT\nmodels often fall short in handling colloquial variations of its source\nlanguage and the lack of parallel data in this regard is a challenging hurdle\nin systematically improving the existing models. In this work, we develop a\nnovel colloquial Indonesian-English test-set collected from YouTube transcript\nand Twitter. We perform synthetic style augmentation to the source of formal\nIndonesian language and show that it improves the baseline Id-En models (in\nBLEU) over the new test data.", "published": "2020-12-30 14:52:15", "link": "http://arxiv.org/abs/2012.15178v1", "categories": ["cs.CL", "cs.LG", "68T50", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "SemGloVe: Semantic Co-occurrences for GloVe from BERT", "abstract": "GloVe learns word embeddings by leveraging statistical information from word\nco-occurrence matrices. However, word pairs in the matrices are extracted from\na predefined local context window, which might lead to limited word pairs and\npotentially semantic irrelevant word pairs. In this paper, we propose SemGloVe,\nwhich distills semantic co-occurrences from BERT into static GloVe word\nembeddings. Particularly, we propose two models to extract co-occurrence\nstatistics based on either the masked language model or the multi-head\nattention weights of BERT. Our methods can extract word pairs without limiting\nby the local window assumption and can define the co-occurrence weights by\ndirectly considering the semantic distance between word pairs. Experiments on\nseveral word similarity datasets and four external tasks show that SemGloVe can\noutperform GloVe.", "published": "2020-12-30 15:38:26", "link": "http://arxiv.org/abs/2012.15197v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Robustness Testing of Language Understanding in Task-Oriented Dialog", "abstract": "Most language understanding models in task-oriented dialog systems are\ntrained on a small amount of annotated training data, and evaluated in a small\nset from the same distribution. However, these models can lead to system\nfailure or undesirable output when being exposed to natural language\nperturbation or variation in practice. In this paper, we conduct comprehensive\nevaluation and analysis with respect to the robustness of natural language\nunderstanding models, and introduce three important aspects related to language\nunderstanding in real-world dialog systems, namely, language variety, speech\ncharacteristics, and noise perturbation. We propose a model-agnostic toolkit\nLAUG to approximate natural language perturbations for testing the robustness\nissues in task-oriented dialog. Four data augmentation approaches covering the\nthree aspects are assembled in LAUG, which reveals critical robustness issues\nin state-of-the-art models. The augmented dataset through LAUG can be used to\nfacilitate future research on the robustness testing of language understanding\nin task-oriented dialog.", "published": "2020-12-30 18:18:47", "link": "http://arxiv.org/abs/2012.15262v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generating Landmark Navigation Instructions from Maps as a Graph-to-Text\n  Problem", "abstract": "Car-focused navigation services are based on turns and distances of named\nstreets, whereas navigation instructions naturally used by humans are centered\naround physical objects called landmarks. We present a neural model that takes\nOpenStreetMap representations as input and learns to generate navigation\ninstructions that contain visible and salient landmarks from human natural\nlanguage instructions. Routes on the map are encoded in a location- and\nrotation-invariant graph representation that is decoded into natural language\ninstructions. Our work is based on a novel dataset of 7,672 crowd-sourced\ninstances that have been verified by human navigation in Street View. Our\nevaluation shows that the navigation instructions generated by our system have\nsimilar properties as human-generated instructions, and lead to successful\nhuman navigation in Street View.", "published": "2020-12-30 21:22:04", "link": "http://arxiv.org/abs/2012.15329v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Corrected CBOW Performs as well as Skip-gram", "abstract": "Mikolov et al. (2013a) observed that continuous bag-of-words (CBOW) word\nembeddings tend to underperform Skip-gram (SG) embeddings, and this finding has\nbeen reported in subsequent works. We find that these observations are driven\nnot by fundamental differences in their training objectives, but more likely on\nfaulty negative sampling CBOW implementations in popular libraries such as the\nofficial implementation, word2vec.c, and Gensim. We show that after correcting\na bug in the CBOW gradient update, one can learn CBOW word embeddings that are\nfully competitive with SG on various intrinsic and extrinsic tasks, while being\nmany times faster to train.", "published": "2020-12-30 21:37:28", "link": "http://arxiv.org/abs/2012.15332v2", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Optimizing Deeper Transformers on Small Datasets", "abstract": "It is a common belief that training deep transformers from scratch requires\nlarge datasets. Consequently, for small datasets, people usually use shallow\nand simple additional layers on top of pre-trained models during fine-tuning.\nThis work shows that this does not always need to be the case: with proper\ninitialization and optimization, the benefits of very deep transformers can\ncarry over to challenging tasks with small datasets, including Text-to-SQL\nsemantic parsing and logical reading comprehension. In particular, we\nsuccessfully train $48$ layers of transformers, comprising $24$ fine-tuned\nlayers from pre-trained RoBERTa and $24$ relation-aware layers trained from\nscratch. With fewer training steps and no task-specific pre-training, we obtain\nthe state-of-the-art performance on the challenging cross-domain Text-to-SQL\nparsing benchmark Spider. We achieve this by deriving a novel Data-dependent\nTransformer Fixed-update initialization scheme (DT-Fixup), inspired by the\nprior T-Fixup work. Further error analysis shows that increasing depth can help\nimprove generalization on small datasets for hard cases that require reasoning\nand structural understanding.", "published": "2020-12-30 22:53:49", "link": "http://arxiv.org/abs/2012.15355v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reducing conversational agents' overconfidence through linguistic\n  calibration", "abstract": "While improving neural dialogue agents' factual accuracy is the object of\nmuch research, another important aspect of communication, less studied in the\nsetting of neural dialogue, is transparency about ignorance. In this work, we\nanalyze to what extent state-of-the-art chit-chat models are linguistically\ncalibrated in the sense that their verbalized expression of doubt (or\nconfidence) matches the likelihood that the model's responses are factually\nincorrect (or correct). We find that these models are poorly calibrated, yet we\nshow that likelihood of correctness can accurately be predicted. By\nincorporating such metacognitive features into the training of a controllable\ngeneration model, we obtain a dialogue agent with greatly improved linguistic\ncalibration. While improving neural dialogue agents' factual accuracy is the\nobject of much research, another important aspect of communication, less\nstudied in the setting of neural dialogue, is transparency about ignorance. In\nthis work, we analyze to what extent state-of-the-art chit-chat models are\nlinguistically calibrated in the sense that their verbalized expression of\ndoubt (or confidence) matches the likelihood that the model's responses are\nfactually incorrect (or correct). We find that these models are poorly\ncalibrated, yet we show that likelihood of correctness can accurately be\npredicted. By incorporating such metacognitive features into the training of a\ncontrollable generation model, we obtain a dialogue agent with greatly improved\nlinguistic calibration.", "published": "2020-12-30 00:12:36", "link": "http://arxiv.org/abs/2012.14983v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Accurate Word Representations with Universal Visual Guidance", "abstract": "Word representation is a fundamental component in neural language\nunderstanding models. Recently, pre-trained language models (PrLMs) offer a new\nperformant method of contextualized word representations by leveraging the\nsequence-level context for modeling. Although the PrLMs generally give more\naccurate contextualized word representations than non-contextualized models do,\nthey are still subject to a sequence of text contexts without diverse hints for\nword representation from multimodality. This paper thus proposes a visual\nrepresentation method to explicitly enhance conventional word embedding with\nmultiple-aspect senses from visual guidance. In detail, we build a small-scale\nword-image dictionary from a multimodal seed dataset where each word\ncorresponds to diverse related images. The texts and paired images are encoded\nin parallel, followed by an attention layer to integrate the multimodal\nrepresentations. We show that the method substantially improves the accuracy of\ndisambiguation. Experiments on 12 natural language understanding and machine\ntranslation tasks further verify the effectiveness and the generalization\ncapability of the proposed approach.", "published": "2020-12-30 09:11:50", "link": "http://arxiv.org/abs/2012.15086v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Joint Verification and Reranking for Open Fact Checking Over Tables", "abstract": "Structured information is an important knowledge source for automatic\nverification of factual claims. Nevertheless, the majority of existing research\ninto this task has focused on textual data, and the few recent inquiries into\nstructured data have been for the closed-domain setting where appropriate\nevidence for each claim is assumed to have already been retrieved. In this\npaper, we investigate verification over structured data in the open-domain\nsetting, introducing a joint reranking-and-verification model which fuses\nevidence documents in the verification component. Our open-domain model\nachieves performance comparable to the closed-domain state-of-the-art on the\nTabFact dataset, and demonstrates performance gains from the inclusion of\nmultiple tables as well as a significant improvement over a heuristic retrieval\nbaseline.", "published": "2020-12-30 11:22:31", "link": "http://arxiv.org/abs/2012.15115v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Out of Order: How Important Is The Sequential Order of Words in a\n  Sentence in Natural Language Understanding Tasks?", "abstract": "Do state-of-the-art natural language understanding models care about word\norder - one of the most important characteristics of a sequence? Not always! We\nfound 75% to 90% of the correct predictions of BERT-based classifiers, trained\non many GLUE tasks, remain constant after input words are randomly shuffled.\nDespite BERT embeddings are famously contextual, the contribution of each\nindividual word to downstream tasks is almost unchanged even after the word's\ncontext is shuffled. BERT-based models are able to exploit superficial cues\n(e.g. the sentiment of keywords in sentiment analysis; or the word-wise\nsimilarity between sequence-pair inputs in natural language inference) to make\ncorrect decisions when tokens are arranged in random orders. Encouraging\nclassifiers to capture word order information improves the performance on most\nGLUE tasks, SQuAD 2.0 and out-of-samples. Our work suggests that many GLUE\ntasks are not challenging machines to understand the meaning of a sentence.", "published": "2020-12-30 14:56:12", "link": "http://arxiv.org/abs/2012.15180v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-view Temporal Alignment for Non-parallel Articulatory-to-Acoustic\n  Speech Synthesis", "abstract": "Articulatory-to-acoustic (A2A) synthesis refers to the generation of audible\nspeech from captured movement of the speech articulators. This technique has\nnumerous applications, such as restoring oral communication to people who\ncannot longer speak due to illness or injury. Most successful techniques so far\nadopt a supervised learning framework, in which time-synchronous\narticulatory-and-speech recordings are used to train a supervised machine\nlearning algorithm that can be used later to map articulator movements to\nspeech. This, however, prevents the application of A2A techniques in cases\nwhere parallel data is unavailable, e.g., a person has already lost her/his\nvoice and only articulatory data can be captured. In this work, we propose a\nsolution to this problem based on the theory of multi-view learning. The\nproposed algorithm attempts to find an optimal temporal alignment between pairs\nof non-aligned articulatory-and-acoustic sequences with the same phonetic\ncontent by projecting them into a common latent space where both views are\nmaximally correlated and then applying dynamic time warping. Several variants\nof this idea are discussed and explored. We show that the quality of speech\ngenerated in the non-aligned scenario is comparable to that obtained in the\nparallel scenario.", "published": "2020-12-30 15:09:02", "link": "http://arxiv.org/abs/2012.15184v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
