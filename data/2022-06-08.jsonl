{"title": "1Cademy at Semeval-2022 Task 1: Investigating the Effectiveness of\n  Multilingual, Multitask, and Language-Agnostic Tricks for the Reverse\n  Dictionary Task", "abstract": "This paper describes our system for the SemEval2022 task of matching\ndictionary glosses to word embeddings. We focus on the Reverse Dictionary Track\nof the competition, which maps multilingual glosses to reconstructed vector\nrepresentations. More specifically, models convert the input of sentences to\nthree types of embeddings: SGNS, Char, and Electra. We propose several\nexperiments for applying neural network cells, general multilingual and\nmultitask structures, and language-agnostic tricks to the task. We also provide\ncomparisons over different types of word embeddings and ablation studies to\nsuggest helpful strategies. Our initial transformer-based model achieves\nrelatively low performance. However, trials on different retokenization\nmethodologies indicate improved performance. Our proposed Elmobased monolingual\nmodel achieves the highest outcome, and its multitask, and multilingual\nvarieties show competitive results as well.", "published": "2022-06-08 06:39:04", "link": "http://arxiv.org/abs/2206.03702v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Realistic Zero-Shot Cross-Lingual Transfer in Legal Topic Classification", "abstract": "We consider zero-shot cross-lingual transfer in legal topic classification\nusing the recent MultiEURLEX dataset. Since the original dataset contains\nparallel documents, which is unrealistic for zero-shot cross-lingual transfer,\nwe develop a new version of the dataset without parallel documents. We use it\nto show that translation-based methods vastly outperform cross-lingual\nfine-tuning of multilingually pre-trained models, the best previous zero-shot\ntransfer method for MultiEURLEX. We also develop a bilingual teacher-student\nzero-shot transfer approach, which exploits additional unlabeled documents of\nthe target language and performs better than a model fine-tuned directly on\nlabeled target language documents.", "published": "2022-06-08 10:02:11", "link": "http://arxiv.org/abs/2206.03785v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Open corpus of the Veps and Karelian languages: overview and\n  applications", "abstract": "A growing priority in the study of Baltic-Finnic languages of the Republic of\nKarelia has been the methods and tools of corpus linguistics. Since 2016,\nlinguists, mathematicians, and programmers at the Karelian Research Centre have\nbeen working with the Open Corpus of the Veps and Karelian Languages (VepKar),\nwhich is an extension of the Veps Corpus created in 2009. The VepKar corpus\ncomprises texts in Karelian and Veps, multifunctional dictionaries linked to\nthem, and software with an advanced system of search using various criteria of\nthe texts (language, genre, etc.) and numerous linguistic categories (lexical\nand grammatical search in texts was implemented thanks to the generator of word\nforms that we created earlier). A corpus of 3000 texts was compiled, texts were\nuploaded and marked up, the system for classifying texts into languages,\ndialects, types and genres was introduced, and the word-form generator was\ncreated. Future plans include developing a speech module for working with audio\nrecordings and a syntactic tagging module using morphological analysis outputs.\nOwing to continuous functional advancements in the corpus manager and ongoing\nVepKar enrichment with new material and text markup, users can handle a wide\nrange of scientific and applied tasks. In creating the universal national\nVepKar corpus, its developers and managers strive to preserve and exhibit as\nfully as possible the state of the Veps and Karelian languages in the 19th-21st\ncenturies.", "published": "2022-06-08 13:05:50", "link": "http://arxiv.org/abs/2206.03870v1", "categories": ["cs.CL", "68T50", "H.3.1; H.3.6"], "primary_category": "cs.CL"}
{"title": "Counseling Summarization using Mental Health Knowledge Guided Utterance\n  Filtering", "abstract": "The psychotherapy intervention technique is a multifaceted conversation\nbetween a therapist and a patient. Unlike general clinical discussions,\npsychotherapy's core components (viz. symptoms) are hard to distinguish, thus\nbecoming a complex problem to summarize later. A structured counseling\nconversation may contain discussions about symptoms, history of mental health\nissues, or the discovery of the patient's behavior. It may also contain\ndiscussion filler words irrelevant to a clinical summary. We refer to these\nelements of structured psychotherapy as counseling components. In this paper,\nthe aim is mental health counseling summarization to build upon domain\nknowledge and to help clinicians quickly glean meaning. We create a new dataset\nafter annotating 12.9K utterances of counseling components and reference\nsummaries for each dialogue. Further, we propose ConSum, a novel\ncounseling-component guided summarization model. ConSum undergoes three\nindependent modules. First, to assess the presence of depressive symptoms, it\nfilters utterances utilizing the Patient Health Questionnaire (PHQ-9), while\nthe second and third modules aim to classify counseling components. At last, we\npropose a problem-specific Mental Health Information Capture (MHIC) evaluation\nmetric for counseling summaries. Our comparative study shows that we improve on\nperformance and generate cohesive, semantic, and coherent summaries. We\ncomprehensively analyze the generated summaries to investigate the capturing of\npsychotherapy elements. Human and clinical evaluations on the summary show that\nConSum generates quality summary. Further, mental health experts validate the\nclinical acceptability of the ConSum. Lastly, we discuss the uniqueness in\nmental health counseling summarization in the real world and show evidences of\nits deployment on an online application with the support of mpathic.ai", "published": "2022-06-08 13:38:47", "link": "http://arxiv.org/abs/2206.03886v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Challenges in Applying Explainability Methods to Improve the Fairness of\n  NLP Models", "abstract": "Motivations for methods in explainable artificial intelligence (XAI) often\ninclude detecting, quantifying and mitigating bias, and contributing to making\nmachine learning models fairer. However, exactly how an XAI method can help in\ncombating biases is often left unspecified. In this paper, we briefly review\ntrends in explainability and fairness in NLP research, identify the current\npractices in which explainability methods are applied to detect and mitigate\nbias, and investigate the barriers preventing XAI methods from being used more\nwidely in tackling fairness issues.", "published": "2022-06-08 15:09:04", "link": "http://arxiv.org/abs/2206.03945v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Proactively Reducing the Hate Intensity of Online Posts via Hate Speech\n  Normalization", "abstract": "Curbing online hate speech has become the need of the hour; however, a\nblanket ban on such activities is infeasible for several geopolitical and\ncultural reasons. To reduce the severity of the problem, in this paper, we\nintroduce a novel task, hate speech normalization, that aims to weaken the\nintensity of hatred exhibited by an online post. The intention of hate speech\nnormalization is not to support hate but instead to provide the users with a\nstepping stone towards non-hate while giving online platforms more time to\nmonitor any improvement in the user's behavior.\n  To this end, we manually curated a parallel corpus - hate texts and their\nnormalized counterparts (a normalized text is less hateful and more benign). We\nintroduce NACL, a simple yet efficient hate speech normalization model that\noperates in three stages - first, it measures the hate intensity of the\noriginal sample; second, it identifies the hate span(s) within it; and finally,\nit reduces hate intensity by paraphrasing the hate spans. We perform extensive\nexperiments to measure the efficacy of NACL via three-way evaluation\n(intrinsic, extrinsic, and human-study). We observe that NACL outperforms six\nbaselines - NACL yields a score of 0.1365 RMSE for the intensity prediction,\n0.622 F1-score in the span identification, and 82.27 BLEU and 80.05 perplexity\nfor the normalized text generation. We further show the generalizability of\nNACL across other platforms (Reddit, Facebook, Gab). An interactive prototype\nof NACL was put together for the user study. Further, the tool is being\ndeployed in a real-world setting at Wipro AI as a part of its mission to tackle\nharmful content on online platforms.", "published": "2022-06-08 16:43:06", "link": "http://arxiv.org/abs/2206.04007v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adversarial Text Normalization", "abstract": "Text-based adversarial attacks are becoming more commonplace and accessible\nto general internet users. As these attacks proliferate, the need to address\nthe gap in model robustness becomes imminent. While retraining on adversarial\ndata may increase performance, there remains an additional class of\ncharacter-level attacks on which these models falter. Additionally, the process\nto retrain a model is time and resource intensive, creating a need for a\nlightweight, reusable defense. In this work, we propose the Adversarial Text\nNormalizer, a novel method that restores baseline performance on attacked\ncontent with low computational overhead. We evaluate the efficacy of the\nnormalizer on two problem areas prone to adversarial attacks, i.e. Hate Speech\nand Natural Language Inference. We find that text normalization provides a\ntask-agnostic defense against character-level attacks that can be implemented\nsupplementary to adversarial retraining solutions, which are more suited for\nsemantic alterations.", "published": "2022-06-08 19:44:03", "link": "http://arxiv.org/abs/2206.04137v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Abstraction not Memory: BERT and the English Article System", "abstract": "Article prediction is a task that has long defied accurate linguistic\ndescription. As such, this task is ideally suited to evaluate models on their\nability to emulate native-speaker intuition. To this end, we compare the\nperformance of native English speakers and pre-trained models on the task of\narticle prediction set up as a three way choice (a/an, the, zero). Our\nexperiments with BERT show that BERT outperforms humans on this task across all\narticles. In particular, BERT is far superior to humans at detecting the zero\narticle, possibly because we insert them using rules that the deep neural model\ncan easily pick up. More interestingly, we find that BERT tends to agree more\nwith annotators than with the corpus when inter-annotator agreement is high but\nswitches to agreeing more with the corpus as inter-annotator agreement drops.\nWe contend that this alignment with annotators, despite being trained on the\ncorpus, suggests that BERT is not memorising article use, but captures a high\nlevel generalisation of article use akin to human intuition.", "published": "2022-06-08 22:36:54", "link": "http://arxiv.org/abs/2206.04184v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Few-shot Question Generation for Personalized Feedback in Intelligent\n  Tutoring Systems", "abstract": "Existing work on generating hints in Intelligent Tutoring Systems (ITS)\nfocuses mostly on manual and non-personalized feedback. In this work, we\nexplore automatically generated questions as personalized feedback in an ITS.\nOur personalized feedback can pinpoint correct and incorrect or missing phrases\nin student answers as well as guide them towards correct answer by asking a\nquestion in natural language. Our approach combines cause-effect analysis to\nbreak down student answers using text similarity-based NLP Transformer models\nto identify correct and incorrect or missing parts. We train a few-shot Neural\nQuestion Generation and Question Re-ranking models to show questions addressing\ncomponents missing in the student answers which steers students towards the\ncorrect answer. Our model vastly outperforms both simple and strong baselines\nin terms of student learning gains by 45% and 23% respectively when tested in a\nreal dialogue-based ITS. Finally, we show that our personalized corrective\nfeedback system has the potential to improve Generative Question Answering\nsystems.", "published": "2022-06-08 22:59:23", "link": "http://arxiv.org/abs/2206.04187v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Set Interdependence Transformer: Set-to-Sequence Neural Networks for\n  Permutation Learning and Structure Prediction", "abstract": "The task of learning to map an input set onto a permuted sequence of its\nelements is challenging for neural networks. Set-to-sequence problems occur in\nnatural language processing, computer vision and structure prediction, where\ninteractions between elements of large sets define the optimal output. Models\nmust exhibit relational reasoning, handle varying cardinalities and manage\ncombinatorial complexity. Previous attention-based methods require $n$ layers\nof their set transformations to explicitly represent $n$-th order relations.\nOur aim is to enhance their ability to efficiently model higher-order\ninteractions through an additional interdependence component. We propose a\nnovel neural set encoding method called the Set Interdependence Transformer,\ncapable of relating the set's permutation invariant representation to its\nelements within sets of any cardinality. We combine it with a permutation\nlearning module into a complete, 3-part set-to-sequence model and demonstrate\nits state-of-the-art performance on a number of tasks. These range from\ncombinatorial optimization problems, through permutation learning challenges on\nboth synthetic and established NLP datasets for sentence ordering, to a novel\ndomain of product catalog structure prediction. Additionally, the network's\nability to generalize to unseen sequence lengths is investigated and a\ncomparative empirical analysis of the existing methods' ability to learn\nhigher-order interactions is provided.", "published": "2022-06-08 07:46:49", "link": "http://arxiv.org/abs/2206.03720v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "STable: Table Generation Framework for Encoder-Decoder Models", "abstract": "The output structure of database-like tables, consisting of values structured\nin horizontal rows and vertical columns identifiable by name, can cover a wide\nrange of NLP tasks. Following this constatation, we propose a framework for\ntext-to-table neural models applicable to problems such as extraction of line\nitems, joint entity and relation extraction, or knowledge base population. The\npermutation-based decoder of our proposal is a generalized sequential method\nthat comprehends information from all cells in the table. The training\nmaximizes the expected log-likelihood for a table's content across all random\npermutations of the factorization order. During the content inference, we\nexploit the model's ability to generate cells in any order by searching over\npossible orderings to maximize the model's confidence and avoid substantial\nerror accumulation, which other sequential models are prone to. Experiments\ndemonstrate a high practical value of the framework, which establishes\nstate-of-the-art results on several challenging datasets, outperforming\nprevious solutions by up to 15%.", "published": "2022-06-08 17:59:02", "link": "http://arxiv.org/abs/2206.04045v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Key Event Detection from Massive Text Corpora", "abstract": "Automated event detection from news corpora is a crucial task towards mining\nfast-evolving structured knowledge. As real-world events have different\ngranularities, from the top-level themes to key events and then to event\nmentions corresponding to concrete actions, there are generally two lines of\nresearch: (1) theme detection identifies from a news corpus major themes (e.g.,\n\"2019 Hong Kong Protests\" vs. \"2020 U.S. Presidential Election\") that have very\ndistinct semantics; and (2) action extraction extracts from one document\nmention-level actions (e.g., \"the police hit the left arm of the protester\")\nthat are too fine-grained for comprehending the event. In this paper, we\npropose a new task, key event detection at the intermediate level, aiming to\ndetect from a news corpus key events (e.g., \"HK Airport Protest on Aug.\n12-14\"), each happening at a particular time/location and focusing on the same\ntopic. This task can bridge event understanding and structuring and is\ninherently challenging because of the thematic and temporal closeness of key\nevents and the scarcity of labeled data due to the fast-evolving nature of news\narticles. To address these challenges, we develop an unsupervised key event\ndetection framework, EvMine, that (1) extracts temporally frequent peak phrases\nusing a novel ttf-itf score, (2) merges peak phrases into event-indicative\nfeature sets by detecting communities from our designed peak phrase graph that\ncaptures document co-occurrences, semantic similarities, and temporal closeness\nsignals, and (3) iteratively retrieves documents related to each key event by\ntraining a classifier with automatically generated pseudo labels from the\nevent-indicative feature sets and refining the detected key events using the\nretrieved documents. Extensive experiments and case studies show EvMine\noutperforms all the baseline methods and its ablations on two real-world news\ncorpora.", "published": "2022-06-08 20:31:02", "link": "http://arxiv.org/abs/2206.04153v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Modularized Transfer Learning with Multiple Knowledge Graphs for\n  Zero-shot Commonsense Reasoning", "abstract": "Commonsense reasoning systems should be able to generalize to diverse\nreasoning cases. However, most state-of-the-art approaches depend on expensive\ndata annotations and overfit to a specific benchmark without learning how to\nperform general semantic reasoning. To overcome these drawbacks, zero-shot QA\nsystems have shown promise as a robust learning scheme by transforming a\ncommonsense knowledge graph (KG) into synthetic QA-form samples for model\ntraining. Considering the increasing type of different commonsense KGs, this\npaper aims to extend the zero-shot transfer learning scenario into\nmultiple-source settings, where different KGs can be utilized synergetically.\nTowards this goal, we propose to mitigate the loss of knowledge from the\ninterference among the different knowledge sources, by developing a modular\nvariant of the knowledge aggregation as a new zero-shot commonsense reasoning\nframework. Results on five commonsense reasoning benchmarks demonstrate the\nefficacy of our framework, improving the performance with multiple KGs.", "published": "2022-06-08 07:36:31", "link": "http://arxiv.org/abs/2206.03715v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Learning to Generate Prompts for Dialogue Generation through\n  Reinforcement Learning", "abstract": "Much literature has shown that prompt-based learning is an efficient method\nto make use of the large pre-trained language model. Recent works also exhibit\nthe possibility of steering a chatbot's output by plugging in an appropriate\nprompt. Gradient-based methods are often used to perturb the prompts. However,\nsome language models are not even available to the public. In this work, we\nfirst explored the combination of prompting and reinforcement learning (RL) to\nsteer models' generation without accessing any of the models' parameters.\nSecond, to reduce the training effort and enhance the generalizability to the\nunseen task, we apply multi-task learning to make the model learn to generalize\nto new tasks better. The experiment results show that our proposed method can\nsuccessfully control several state-of-the-art (SOTA) dialogue models without\naccessing their parameters. Furthermore, the model demonstrates the strong\nability to quickly adapt to an unseen task in fewer steps than the baseline\nmodel.", "published": "2022-06-08 14:48:06", "link": "http://arxiv.org/abs/2206.03931v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Words are all you need? Language as an approximation for human\n  similarity judgments", "abstract": "Human similarity judgments are a powerful supervision signal for machine\nlearning applications based on techniques such as contrastive learning,\ninformation retrieval, and model alignment, but classical methods for\ncollecting human similarity judgments are too expensive to be used at scale.\nRecent methods propose using pre-trained deep neural networks (DNNs) to\napproximate human similarity, but pre-trained DNNs may not be available for\ncertain domains (e.g., medical images, low-resource languages) and their\nperformance in approximating human similarity has not been extensively tested.\nWe conducted an evaluation of 611 pre-trained models across three domains --\nimages, audio, video -- and found that there is a large gap in performance\nbetween human similarity judgments and pre-trained DNNs. To address this gap,\nwe propose a new class of similarity approximation methods based on language.\nTo collect the language data required by these new methods, we also developed\nand validated a novel adaptive tag collection pipeline. We find that our\nproposed language-based methods are significantly cheaper, in the number of\nhuman judgments, than classical methods, but still improve performance over the\nDNN-based methods. Finally, we also develop `stacked' methods that combine\nlanguage embeddings with DNN embeddings, and find that these consistently\nprovide the best approximations for human similarity across all three of our\nmodalities. Based on the results of this comprehensive study, we provide a\nconcise guide for researchers interested in collecting or approximating human\nsimilarity data. To accompany this guide, we also release all of the similarity\nand language data, a total of 206,339 human judgments, that we collected in our\nexperiments, along with a detailed breakdown of all modeling results.", "published": "2022-06-08 18:09:19", "link": "http://arxiv.org/abs/2206.04105v3", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Improved two-stage hate speech classification for twitter based on Deep\n  Neural Networks", "abstract": "Hate speech is a form of online harassment that involves the use of abusive\nlanguage, and it is commonly seen in social media posts. This sort of\nharassment mainly focuses on specific group characteristics such as religion,\ngender, ethnicity, etc and it has both societal and economic consequences\nnowadays. The automatic detection of abusive language in text postings has\nalways been a difficult task, but it is lately receiving much interest from the\nscientific community. This paper addresses the important problem of discerning\nhateful content in social media. The model we propose in this work is an\nextension of an existing approach based on LSTM neural network architectures,\nwhich we appropriately enhanced and fine-tuned to detect certain forms of\nhatred language, such as racism or sexism, in a short text. The most\nsignificant enhancement is the conversion to a two-stage scheme consisting of\nRecurrent Neural Network (RNN) classifiers. The output of all One-vs-Rest (OvR)\nclassifiers from the first stage are combined and used to train the second\nstage classifier, which finally determines the type of harassment. Our study\nincludes a performance comparison of several proposed alternative methods for\nthe second stage evaluated on a public corpus of 16k tweets, followed by a\ngeneralization study on another dataset. The reported results show the superior\nclassification quality of the proposed scheme in the task of hate speech\ndetection as compared to the current state-of-the-art.", "published": "2022-06-08 20:57:41", "link": "http://arxiv.org/abs/2206.04162v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Resolving the Human Subjects Status of Machine Learning's Crowdworkers", "abstract": "In recent years, machine learning (ML) has relied heavily on crowdworkers\nboth for building datasets and for addressing research questions requiring\nhuman interaction or judgment. The diverse tasks performed and uses of the data\nproduced render it difficult to determine when crowdworkers are best thought of\nas workers (versus human subjects). These difficulties are compounded by\nconflicting policies, with some institutions and researchers regarding all ML\ncrowdworkers as human subjects and others holding that they rarely constitute\nhuman subjects. Notably few ML papers involving crowdwork mention IRB\noversight, raising the prospect of non-compliance with ethical and regulatory\nrequirements. We investigate the appropriate designation of ML crowdsourcing\nstudies, focusing our inquiry on natural language processing to expose unique\nchallenges for research oversight. Crucially, under the U.S. Common Rule, these\njudgments hinge on determinations of aboutness, concerning both whom (or what)\nthe collected data is about and whom (or what) the analysis is about. We\nhighlight two challenges posed by ML: the same set of workers can serve\nmultiple roles and provide many sorts of information; and ML research tends to\nembrace a dynamic workflow, where research questions are seldom stated ex ante\nand data sharing opens the door for future studies to aim questions at\ndifferent targets. Our analysis exposes a potential loophole in the Common\nRule, where researchers can elude research ethics oversight by splitting data\ncollection and analysis into distinct studies. Finally, we offer several policy\nrecommendations to address these concerns.", "published": "2022-06-08 17:55:01", "link": "http://arxiv.org/abs/2206.04039v2", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CY"}
{"title": "Low-complexity acoustic scene classification in DCASE 2022 Challenge", "abstract": "This paper presents an analysis of the Low-Complexity Acoustic Scene\nClassification task in DCASE 2022 Challenge. The task was a continuation from\nthe previous years, but the low-complexity requirements were changed to the\nfollowing: the maximum number of allowed parameters, including the zero-valued\nones, was 128 K, with parameters being represented using INT8 numerical format;\nand the maximum number of multiply-accumulate operations at inference time was\n30 million. The provided baseline system is a convolutional neural network\nwhich employs post-training quantization of parameters, resulting in 46.5 K\nparameters, and 29.23 million multiply-and-accumulate operations (MMACs). Its\nperformance on the evaluation data is 44.2% accuracy and 1.532 log-loss. In\ncomparison, the top system in the challenge obtained an accuracy of 59.6% and a\nlog loss of 1.091, having 121 K parameters and 28 MMACs. The task received 48\nsubmissions from 19 different teams, most of which outperformed the baseline\nsystem.", "published": "2022-06-08 12:16:51", "link": "http://arxiv.org/abs/2206.03835v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "On the Integration of Acoustics and LiDAR: a Multi-Modal Approach to\n  Acoustic Reflector Estimation", "abstract": "Having knowledge on the room acoustic properties, e.g., the location of\nacoustic reflectors, allows to better reproduce the sound field as intended.\nCurrent state-of-the-art methods for room boundary detection using microphone\nmeasurements typically focus on a two-dimensional setting, causing a model\nmismatch when employed in real-life scenarios. Detection of arbitrary\nreflectors in three dimensions encounters practical limitations, e.g., the need\nfor a spherical array and the increased computational complexity. Moreover,\nloudspeakers may not have an omnidirectional directivity pattern, as usually\nassumed in the literature, making the detection of acoustic reflectors in some\ndirections more challenging. In the proposed method, a LiDAR sensor is added to\na loudspeaker to improve wall detection accuracy and robustness. This is done\nin two ways. First, the model mismatch introduced by horizontal reflectors can\nbe resolved by detecting reflectors with the LiDAR sensor to enable elimination\nof their detrimental influence from the 2D problem in pre-processing. Second, a\nLiDAR-based method is proposed to compensate for the challenging directions\nwhere the directive loudspeaker emits little energy. We show via simulations\nthat this multi-modal approach, i.e., combining microphone and LiDAR sensors,\nimproves the robustness and accuracy of wall detection.", "published": "2022-06-08 13:36:44", "link": "http://arxiv.org/abs/2206.03885v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Few-Shot Audio-Visual Learning of Environment Acoustics", "abstract": "Room impulse response (RIR) functions capture how the surrounding physical\nenvironment transforms the sounds heard by a listener, with implications for\nvarious applications in AR, VR, and robotics. Whereas traditional methods to\nestimate RIRs assume dense geometry and/or sound measurements throughout the\nenvironment, we explore how to infer RIRs based on a sparse set of images and\nechoes observed in the space. Towards that goal, we introduce a\ntransformer-based method that uses self-attention to build a rich acoustic\ncontext, then predicts RIRs of arbitrary query source-receiver locations\nthrough cross-attention. Additionally, we design a novel training objective\nthat improves the match in the acoustic signature between the RIR predictions\nand the targets. In experiments using a state-of-the-art audio-visual simulator\nfor 3D environments, we demonstrate that our method successfully generates\narbitrary RIRs, outperforming state-of-the-art methods and -- in a major\ndeparture from traditional methods -- generalizing to novel environments in a\nfew-shot manner. Project: http://vision.cs.utexas.edu/projects/fs_rir.", "published": "2022-06-08 16:38:24", "link": "http://arxiv.org/abs/2206.04006v2", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Motif Mining and Unsupervised Representation Learning for BirdCLEF 2022", "abstract": "We build a classification model for the BirdCLEF 2022 challenge using\nunsupervised methods. We implement an unsupervised representation of the\ntraining dataset using a triplet loss on spectrogram representation of audio\nmotifs. Our best model performs with a score of 0.48 on the public leaderboard.", "published": "2022-06-08 06:58:54", "link": "http://arxiv.org/abs/2206.04805v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
