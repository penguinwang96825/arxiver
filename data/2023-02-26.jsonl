{"title": "CLICKER: Attention-Based Cross-Lingual Commonsense Knowledge Transfer", "abstract": "Recent advances in cross-lingual commonsense reasoning (CSR) are facilitated\nby the development of multilingual pre-trained models (mPTMs). While mPTMs show\nthe potential to encode commonsense knowledge for different languages,\ntransferring commonsense knowledge learned in large-scale English corpus to\nother languages is challenging. To address this problem, we propose the\nattention-based Cross-LIngual Commonsense Knowledge transfER (CLICKER)\nframework, which minimizes the performance gaps between English and non-English\nlanguages in commonsense question-answering tasks. CLICKER effectively improves\ncommonsense reasoning for non-English languages by differentiating\nnon-commonsense knowledge from commonsense knowledge. Experimental results on\npublic benchmarks demonstrate that CLICKER achieves remarkable improvements in\nthe cross-lingual CSR task for languages other than English.", "published": "2023-02-26 00:57:29", "link": "http://arxiv.org/abs/2302.13201v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Question Answering over Knowledge Base as Reading\n  Comprehension", "abstract": "Although many large-scale knowledge bases (KBs) claim to contain multilingual\ninformation, their support for many non-English languages is often incomplete.\nThis incompleteness gives birth to the task of cross-lingual question answering\nover knowledge base (xKBQA), which aims to answer questions in languages\ndifferent from that of the provided KB. One of the major challenges facing\nxKBQA is the high cost of data annotation, leading to limited resources\navailable for further exploration. Another challenge is mapping KB schemas and\nnatural language expressions in the questions under cross-lingual settings. In\nthis paper, we propose a novel approach for xKBQA in a reading comprehension\nparadigm. We convert KB subgraphs into passages to narrow the gap between KB\nschemas and questions, which enables our model to benefit from recent advances\nin multilingual pre-trained language models (MPLMs) and cross-lingual machine\nreading comprehension (xMRC). Specifically, we use MPLMs, with considerable\nknowledge of cross-lingual mappings, for cross-lingual reading comprehension.\nExisting high-quality xMRC datasets can be further utilized to finetune our\nmodel, greatly alleviating the data scarcity issue in xKBQA. Extensive\nexperiments on two xKBQA datasets in 12 languages show that our approach\noutperforms various baselines and achieves strong few-shot and zero-shot\nperformance. Our dataset and code are released for further research.", "published": "2023-02-26 05:52:52", "link": "http://arxiv.org/abs/2302.13241v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tailoring Language Generation Models under Total Variation Distance", "abstract": "The standard paradigm of neural language generation adopts maximum likelihood\nestimation (MLE) as the optimizing method. From a distributional view, MLE in\nfact minimizes the Kullback-Leibler divergence (KLD) between the distribution\nof the real data and that of the model. However, this approach forces the model\nto distribute non-zero (sometimes large) probability mass to all training\nsamples regardless of their quality. Moreover, in the attempt to cover the\nlow-probability regions in the data distribution, the model systematically\noverestimates the probability of corrupted text sequences, which we conjecture\nis one of the main reasons for text degeneration during autoregressive\ndecoding. To remedy this problem, we leverage the total variation distance\n(TVD) with its robustness to outliers, and develop practical bounds to apply it\nto language generation. Then, we introduce the TaiLr objective that balances\nthe tradeoff of estimating TVD. Intuitively, TaiLr downweights real data\nsamples that have low model probabilities with tunable penalization intensity.\nExperimental results show that our method alleviates the overestimation of\ndegenerated sequences without sacrificing diversity and improves generation\nquality on a wide range of text generation tasks.", "published": "2023-02-26 16:32:52", "link": "http://arxiv.org/abs/2302.13344v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "User-Centric Evaluation of OCR Systems for Kwak'wala", "abstract": "There has been recent interest in improving optical character recognition\n(OCR) for endangered languages, particularly because a large number of\ndocuments and books in these languages are not in machine-readable formats. The\nperformance of OCR systems is typically evaluated using automatic metrics such\nas character and word error rates. While error rates are useful for the\ncomparison of different models and systems, they do not measure whether and how\nthe transcriptions produced from OCR tools are useful to downstream users. In\nthis paper, we present a human-centric evaluation of OCR systems, focusing on\nthe Kwak'wala language as a case study. With a user study, we show that\nutilizing OCR reduces the time spent in the manual transcription of culturally\nvaluable documents -- a task that is often undertaken by endangered language\ncommunity members and researchers -- by over 50%. Our results demonstrate the\npotential benefits that OCR tools can have on downstream language documentation\nand revitalization efforts.", "published": "2023-02-26 21:41:15", "link": "http://arxiv.org/abs/2302.13410v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparing Sentence-Level Suggestions to Message-Level Suggestions in\n  AI-Mediated Communication", "abstract": "Traditionally, writing assistance systems have focused on short or even\nsingle-word suggestions. Recently, large language models like GPT-3 have made\nit possible to generate significantly longer natural-sounding suggestions,\noffering more advanced assistance opportunities. This study explores the\ntrade-offs between sentence- vs. message-level suggestions for AI-mediated\ncommunication. We recruited 120 participants to act as staffers from\nlegislators' offices who often need to respond to large volumes of constituent\nconcerns. Participants were asked to reply to emails with different types of\nassistance. The results show that participants receiving message-level\nsuggestions responded faster and were more satisfied with the experience, as\nthey mainly edited the suggested drafts. In addition, the texts they wrote were\nevaluated as more helpful by others. In comparison, participants receiving\nsentence-level assistance retained a higher sense of agency, but took longer\nfor the task as they needed to plan the flow of their responses and decide when\nto use suggestions. Our findings have implications for designing\ntask-appropriate communication assistance systems.", "published": "2023-02-26 18:40:38", "link": "http://arxiv.org/abs/2302.13382v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Navigating the Grey Area: How Expressions of Uncertainty and\n  Overconfidence Affect Language Models", "abstract": "The increased deployment of LMs for real-world tasks involving knowledge and\nfacts makes it important to understand model epistemology: what LMs think they\nknow, and how their attitudes toward that knowledge are affected by language\nuse in their inputs. Here, we study an aspect of model epistemology: how\nepistemic markers of certainty, uncertainty, or evidentiality like \"I'm sure\nit's\", \"I think it's\", or \"Wikipedia says it's\" affect models, and whether they\ncontribute to model failures. We develop a typology of epistemic markers and\ninject 50 markers into prompts for question answering. We find that LMs are\nhighly sensitive to epistemic markers in prompts, with accuracies varying more\nthan 80%. Surprisingly, we find that expressions of high certainty result in a\n7% decrease in accuracy as compared to low certainty expressions; similarly,\nfactive verbs hurt performance, while evidentials benefit performance. Our\nanalysis of a popular pretraining dataset shows that these markers of\nuncertainty are associated with answers on question-answering websites, while\nmarkers of certainty are associated with questions. These associations may\nsuggest that the behavior of LMs is based on mimicking observed language use,\nrather than truly reflecting epistemic uncertainty.", "published": "2023-02-26 23:46:29", "link": "http://arxiv.org/abs/2302.13439v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Speech Corpora Divergence Based Unsupervised Data Selection for ASR", "abstract": "Selecting application scenarios matching data is important for the automatic\nspeech recognition (ASR) training, but it is difficult to measure the matching\ndegree of the training corpus. This study proposes a unsupervised target-aware\ndata selection method based on speech corpora divergence (SCD), which can\nmeasure the similarity between two speech corpora. We first use the\nself-supervised Hubert model to discretize the speech corpora into label\nsequence and calculate the N-gram probability distribution. Then we calculate\nthe Kullback-Leibler divergence between the N-grams as the SCD. Finally, we can\nchoose the subset which has minimum SCD to the target corpus for annotation and\ntraining. Compared to previous data selection method, the SCD data selection\nmethod can focus on more acoustic details and guarantee the diversity of the\nselected set. We evaluate our method on different accents from Common Voice.\nExperiments show that the proposed SCD data selection can realize 14.8%\nrelative improvements to the random selection, comparable or even superior to\nthe result of supervised selection.", "published": "2023-02-26 03:26:26", "link": "http://arxiv.org/abs/2302.13222v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Understanding Social Media Cross-Modality Discourse in Linguistic Space", "abstract": "The multimedia communications with texts and images are popular on social\nmedia. However, limited studies concern how images are structured with texts to\nform coherent meanings in human cognition. To fill in the gap, we present a\nnovel concept of cross-modality discourse, reflecting how human readers couple\nimage and text understandings. Text descriptions are first derived from images\n(named as subtitles) in the multimedia contexts. Five labels -- entity-level\ninsertion, projection and concretization and scene-level restatement and\nextension -- are further employed to shape the structure of subtitles and texts\nand present their joint meanings. As a pilot study, we also build the very\nfirst dataset containing 16K multimedia tweets with manually annotated\ndiscourse labels. The experimental results show that the multimedia encoder\nbased on multi-head attention with captions is able to obtain\nthe-state-of-the-art results.", "published": "2023-02-26 13:04:04", "link": "http://arxiv.org/abs/2302.13311v1", "categories": ["cs.MM", "cs.CL", "cs.SI"], "primary_category": "cs.MM"}
{"title": "Multi-Modality in Music: Predicting Emotion in Music from High-Level\n  Audio Features and Lyrics", "abstract": "This paper aims to test whether a multi-modal approach for music emotion\nrecognition (MER) performs better than a uni-modal one on high-level song\nfeatures and lyrics. We use 11 song features retrieved from the Spotify API,\ncombined lyrics features including sentiment, TF-IDF, and Anew to predict\nvalence and arousal (Russell, 1980) scores on the Deezer Mood Detection Dataset\n(DMDD) (Delbouys et al., 2018) with 4 different regression models. We find that\nout of the 11 high-level song features, mainly 5 contribute to the performance,\nmulti-modal features do better than audio alone when predicting valence. We\nmade our code publically available.", "published": "2023-02-26 13:38:42", "link": "http://arxiv.org/abs/2302.13321v1", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Tweets Under the Rubble: Detection of Messages Calling for Help in\n  Earthquake Disaster", "abstract": "The importance of social media is again exposed in the recent tragedy of the\n2023 Turkey and Syria earthquake. Many victims who were trapped under the\nrubble called for help by posting messages in Twitter. We present an\ninteractive tool to provide situational awareness for missing and trapped\npeople, and disaster relief for rescue and donation efforts. The system (i)\ncollects tweets, (ii) classifies the ones calling for help, (iii) extracts\nimportant entity tags, and (iv) visualizes them in an interactive map screen.\nOur initial experiments show that the performance in terms of the F1 score is\nup to 98.30 for tweet classification, and 84.32 for entity extraction. The\ndemonstration, dataset, and other related files can be accessed at\nhttps://github.com/avaapm/deprem", "published": "2023-02-26 20:55:19", "link": "http://arxiv.org/abs/2302.13403v1", "categories": ["cs.SI", "cs.CL", "cs.IR"], "primary_category": "cs.SI"}
{"title": "The Lindstrom's Characterizability of Abstract Logic Systems for\n  Analytic Structures Based on Measures", "abstract": "In 1969, Per Lindstrom proved his celebrated theorem characterising the\nfirst-order logic and established criteria for the first-order definability of\nformal theories for discrete structures. K. J. Barwise, S. Shelah, J. Vaananen\nand others extended Lindstrom's characterizability program to classes of\ninfinitary logic systems, including a recent paper by M. Dzamonja and J.\nVaananen on Karp's chain logic, which satisfies interpolation, undefinability\nof well-order, and is maximal in the class of logic systems with these\nproperties. The novelty of the chain logic is in its new definition of\nsatisfability. In our paper, we give a framework for Lindstrom's type\ncharacterizability of predicate logic systems interpreted semantically in\nmodels with objects based on measures (analytic structures). In particular,\nHajek's Logic of Integral is redefined as an abstract logic with a new type of\nHajek's satisfiability and constitutes a maximal logic in the class of logic\nsystems for describing analytic structures with Lebesgue integrals and\nsatisfying compactness, elementary chain condition, and weak negation.", "published": "2023-02-26 21:48:25", "link": "http://arxiv.org/abs/2302.13412v1", "categories": ["math.LO", "cs.CL", "cs.LO"], "primary_category": "math.LO"}
{"title": "Efficient Ensemble for Multimodal Punctuation Restoration using\n  Time-Delay Neural Network", "abstract": "Punctuation restoration plays an essential role in the post-processing\nprocedure of automatic speech recognition, but model efficiency is a key\nrequirement for this task. To that end, we present EfficientPunct, an ensemble\nmethod with a multimodal time-delay neural network that outperforms the current\nbest model by 1.0 F1 points, using less than a tenth of its inference network\nparameters. We streamline a speech recognizer to efficiently output hidden\nlayer acoustic embeddings for punctuation restoration, as well as BERT to\nextract meaningful text embeddings. By using forced alignment and temporal\nconvolutions, we eliminate the need for attention-based fusion, greatly\nincreasing computational efficiency and raising performance. EfficientPunct\nsets a new state of the art with an ensemble that weights BERT's purely\nlanguage-based predictions slightly more than the multimodal network's\npredictions. Our code is available at\nhttps://github.com/lxy-peter/EfficientPunct.", "published": "2023-02-26 18:28:20", "link": "http://arxiv.org/abs/2302.13376v2", "categories": ["cs.CL", "cs.HC", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "I-MSV 2022: Indic-Multilingual and Multi-sensor Speaker Verification\n  Challenge", "abstract": "Speaker Verification (SV) is a task to verify the claimed identity of the\nclaimant using his/her voice sample. Though there exists an ample amount of\nresearch in SV technologies, the development concerning a multilingual\nconversation is limited. In a country like India, almost all the speakers are\npolyglot in nature. Consequently, the development of a Multilingual SV (MSV)\nsystem on the data collected in the Indian scenario is more challenging. With\nthis motivation, the Indic- Multilingual Speaker Verification (I-MSV) Challenge\n2022 has been designed for understanding and comparing the state-of-the-art SV\ntechniques. For the challenge, approximately $100$ hours of data spoken by\n$100$ speakers has been collected using $5$ different sensors in $13$ Indian\nlanguages. The data is divided into development, training, and testing sets and\nhas been made publicly available for further research. The goal of this\nchallenge is to make the SV system robust to language and sensor variations\nbetween enrollment and testing. In the challenge, participants were asked to\ndevelop the SV system in two scenarios, viz. constrained and unconstrained. The\nbest system in the constrained and unconstrained scenario achieved a\nperformance of $2.12\\%$ and $0.26\\%$ in terms of Equal Error Rate (EER),\nrespectively.", "published": "2023-02-26 02:26:02", "link": "http://arxiv.org/abs/2302.13209v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Mingling or Misalignment? Temporal Shift for Speech Emotion Recognition\n  with Pre-trained Representations", "abstract": "Fueled by recent advances of self-supervised models, pre-trained speech\nrepresentations proved effective for the downstream speech emotion recognition\n(SER) task. Most prior works mainly focus on exploiting pre-trained\nrepresentations and just adopt a linear head on top of the pre-trained model,\nneglecting the design of the downstream network. In this paper, we propose a\ntemporal shift module to mingle channel-wise information without introducing\nany parameter or FLOP. With the temporal shift module, three designed baseline\nbuilding blocks evolve into corresponding shift variants, i.e. ShiftCNN,\nShiftLSTM, and Shiftformer. Moreover, to balance the trade-off between mingling\nand misalignment, we propose two technical strategies, placement of shift and\nproportion of shift. The family of temporal shift models all outperforms the\nstate-of-the-art methods on the benchmark IEMOCAP dataset under both finetuning\nand feature extraction settings. Our code is available at\nhttps://github.com/ECNU-Cross-Innovation-Lab/ShiftSER.", "published": "2023-02-26 09:35:24", "link": "http://arxiv.org/abs/2302.13277v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Contrast-PLC: Contrastive Learning for Packet Loss Concealment", "abstract": "Packet loss concealment (PLC) is challenging in concealing missing contents\nboth plausibly and naturally when there are only limited available context to\nuse. Recently deep-learning based PLC algorithms have demonstrated their\nsuperiority over traditional counterparts; but their concealment ability is\nstill mostly limited to a maximum of 120ms loss. Even with strong GAN-based\ngenerative models, it is still very challenging to predict long burst losses\nthat could happen within/in-between phonemes. In this paper, we propose to use\ncontrastive learning to learn a loss-robust semantic representation for PLC. A\nhybrid neural PLC architecture combining the semantic prediction and GAN-based\ngenerative model is designed to verify its effectiveness. Results on the blind\ntest set of Interspeech2022 PLC Challenge show its superiority over commonly\nused UNet-style framework and the one without contrastive learning, especially\nfor the longer burst loss at (120, 220] ms.", "published": "2023-02-26 10:24:59", "link": "http://arxiv.org/abs/2302.13284v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Implementation of an aeroacoustic simulation pipeline using\n  openCFS-Acoustics and openCFS-Data applied to human phonation", "abstract": "The human phonation process be modeled using the Finite Element Method (FEM)\nwhich provides a detailed representation of the voice production process. A\nsoftware implementation in C++ using FEM (openCFS) has been used to simulate\nthe phonation process. The FEM model consists of a 3D mesh of the upper human\nairways. The simVoice model provides an accurate representation of the\nphonation process and was valid in several publications. In this article, we\nshow how to set up the model using openCFS and openCFS-Data.", "published": "2023-02-26 10:46:15", "link": "http://arxiv.org/abs/2302.13290v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DFSNet: A Steerable Neural Beamformer Invariant to Microphone Array\n  Configuration for Real-Time, Low-Latency Speech Enhancement", "abstract": "Invariance to microphone array configuration is a rare attribute in neural\nbeamformers. Filter-and-sum (FS) methods in this class define the target signal\nwith respect to a reference channel. However, this not only complicates\nformulation in reverberant conditions but also the network, which must have a\nmechanism to infer what the reference channel is. To address these issues, this\nstudy presents Delay Filter-and-Sum Network (DFSNet), a steerable neural\nbeamformer invariant to microphone number and array geometry for causal speech\nenhancement. In DFSNet, acquired signals are first steered toward the speech\nsource direction prior to the FS operation, which simplifies the task into the\nestimation of delay-and-summed reverberant clean speech. The proposed model is\ndesigned to incur low latency, distortion, and memory and computational burden,\ngiving rise to high potential in hearing aid applications. Simulation results\nreveal comparable performance to noncausal state-of-the-art.", "published": "2023-02-26 21:30:33", "link": "http://arxiv.org/abs/2302.13407v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Two-Stream Joint-Training for Speaker Independent\n  Acoustic-to-Articulatory Inversion", "abstract": "Acoustic-to-articulatory inversion (AAI) aims to estimate the parameters of\narticulators from speech audio. There are two common challenges in AAI, which\nare the limited data and the unsatisfactory performance in speaker independent\nscenario. Most current works focus on extracting features directly from speech\nand ignoring the importance of phoneme information which may limit the\nperformance of AAI. To this end, we propose a novel network called SPN that\nuses two different streams to carry out the AAI task. Firstly, to improve the\nperformance of speaker-independent experiment, we propose a new phoneme stream\nnetwork to estimate the articulatory parameters as the phoneme features. To the\nbest of our knowledge, this is the first work that extracts the\nspeaker-independent features from phonemes to improve the performance of AAI.\nSecondly, in order to better represent the speech information, we train a\nspeech stream network to combine the local features and the global features.\nCompared with state-of-the-art (SOTA), the proposed method reduces 0.18mm on\nRMSE and increases 6.0% on Pearson correlation coefficient in the\nspeaker-independent experiment. The code has been released at\nhttps://github.com/liujinyu123/AAINetwork-SPN.", "published": "2023-02-26 08:53:20", "link": "http://arxiv.org/abs/2302.13273v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "From Audio to Symbolic Encoding", "abstract": "Automatic music transcription (AMT) aims to convert raw audio to symbolic\nmusic representation. As a fundamental problem of music information retrieval\n(MIR), AMT is considered a difficult task even for trained human experts due to\noverlap of multiple harmonics in the acoustic signal. On the other hand, speech\nrecognition, as one of the most popular tasks in natural language processing,\naims to translate human spoken language to texts. Based on the similar nature\nof AMT and speech recognition (as they both deal with tasks of translating\naudio signal to symbolic encoding), this paper investigated whether a generic\nneural network architecture could possibly work on both tasks. In this paper,\nwe introduced our new neural network architecture built on top of the current\nstate-of-the-art Onsets and Frames, and compared the performances of its\nmultiple variations on AMT task. We also tested our architecture with the task\nof speech recognition. For AMT, our models were able to produce better results\ncompared to the model trained using the state-of-art architecture; however,\nalthough similar architecture was able to be trained on the speech recognition\ntask, it did not generate very ideal result compared to other task-specific\nmodels.", "published": "2023-02-26 20:15:00", "link": "http://arxiv.org/abs/2302.13401v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
