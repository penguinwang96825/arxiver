{"title": "A Survey on Knowledge-Enhanced Pre-trained Language Models", "abstract": "Natural Language Processing (NLP) has been revolutionized by the use of\nPre-trained Language Models (PLMs) such as BERT. Despite setting new records in\nnearly every NLP task, PLMs still face a number of challenges including poor\ninterpretability, weak reasoning capability, and the need for a lot of\nexpensive annotated data when applied to downstream tasks. By integrating\nexternal knowledge into PLMs,\n\\textit{\\underline{K}nowledge-\\underline{E}nhanced \\underline{P}re-trained\n\\underline{L}anguage \\underline{M}odels} (KEPLMs) have the potential to\novercome the above-mentioned limitations. In this paper, we examine KEPLMs\nsystematically through a series of studies. Specifically, we outline the common\ntypes and different formats of knowledge to be integrated into KEPLMs, detail\nthe existing methods for building and evaluating KEPLMS, present the\napplications of KEPLMs in downstream tasks, and discuss the future research\ndirections. Researchers will benefit from this survey by gaining a quick and\ncomprehensive overview of the latest developments in this field.", "published": "2022-12-27 09:54:14", "link": "http://arxiv.org/abs/2212.13428v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TegFormer: Topic-to-Essay Generation with Good Topic Coverage and High\n  Text Coherence", "abstract": "Creating an essay based on a few given topics is a challenging NLP task.\nAlthough several effective methods for this problem, topic-to-essay generation,\nhave appeared recently, there is still much room for improvement, especially in\nterms of the coverage of the given topics and the coherence of the generated\ntext. In this paper, we propose a novel approach called TegFormer which\nutilizes the Transformer architecture where the encoder is enriched with\ndomain-specific contexts while the decoder is enhanced by a large-scale\npre-trained language model. Specifically, a \\emph{Topic-Extension} layer\ncapturing the interaction between the given topics and their domain-specific\ncontexts is plugged into the encoder. Since the given topics are usually\nconcise and sparse, such an additional layer can bring more topic-related\nsemantics in to facilitate the subsequent natural language generation.\nMoreover, an \\emph{Embedding-Fusion} module that combines the domain-specific\nword embeddings learnt from the given corpus and the general-purpose word\nembeddings provided by a GPT-2 model pre-trained on massive text data is\nintegrated into the decoder. Since GPT-2 is at a much larger scale, it contains\na lot more implicit linguistic knowledge which would help the decoder to\nproduce more grammatical and readable text. Extensive experiments have shown\nthat the pieces of text generated by TegFormer have better topic coverage and\nhigher text coherence than those from SOTA topic-to-essay techniques, according\nto automatic and human evaluations. As revealed by ablation studies, both the\nTopic-Extension layer and the Embedding-Fusion module contribute substantially\nto TegFormer's performance advantage.", "published": "2022-12-27 11:50:14", "link": "http://arxiv.org/abs/2212.13456v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MultiSpider: Towards Benchmarking Multilingual Text-to-SQL Semantic\n  Parsing", "abstract": "Text-to-SQL semantic parsing is an important NLP task, which greatly\nfacilitates the interaction between users and the database and becomes the key\ncomponent in many human-computer interaction systems. Much recent progress in\ntext-to-SQL has been driven by large-scale datasets, but most of them are\ncentered on English. In this work, we present MultiSpider, the largest\nmultilingual text-to-SQL dataset which covers seven languages (English, German,\nFrench, Spanish, Japanese, Chinese, and Vietnamese). Upon MultiSpider, we\nfurther identify the lexical and structural challenges of text-to-SQL (caused\nby specific language properties and dialect sayings) and their intensity across\ndifferent languages. Experimental results under three typical settings\n(zero-shot, monolingual and multilingual) reveal a 6.1% absolute drop in\naccuracy in non-English languages. Qualitative and quantitative analyses are\nconducted to understand the reason for the performance drop of each language.\nBesides the dataset, we also propose a simple schema augmentation framework\nSAVe (Schema-Augmentation-with-Verification), which significantly boosts the\noverall performance by about 1.8% and closes the 29.5% performance gap across\nlanguages.", "published": "2022-12-27 13:58:30", "link": "http://arxiv.org/abs/2212.13492v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NEEDED: Introducing Hierarchical Transformer to Eye Diseases Diagnosis", "abstract": "With the development of natural language processing techniques(NLP),\nautomatic diagnosis of eye diseases using ophthalmology electronic medical\nrecords (OEMR) has become possible. It aims to evaluate the condition of both\neyes of a patient respectively, and we formulate it as a particular multi-label\nclassification task in this paper. Although there are a few related studies in\nother diseases, automatic diagnosis of eye diseases exhibits unique\ncharacteristics. First, descriptions of both eyes are mixed up in OEMR\ndocuments, with both free text and templated asymptomatic descriptions,\nresulting in sparsity and clutter of information. Second, OEMR documents\ncontain multiple parts of descriptions and have long document lengths. Third,\nit is critical to provide explainability to the disease diagnosis model. To\novercome those challenges, we present an effective automatic eye disease\ndiagnosis framework, NEEDED. In this framework, a preprocessing module is\nintegrated to improve the density and quality of information. Then, we design a\nhierarchical transformer structure for learning the contextualized\nrepresentations of each sentence in the OEMR document. For the diagnosis part,\nwe propose an attention-based predictor that enables traceable diagnosis by\nobtaining disease-specific information. Experiments on the real dataset and\ncomparison with several baseline models show the advantage and explainability\nof our framework.", "published": "2022-12-27 08:37:57", "link": "http://arxiv.org/abs/2212.13408v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Survey on Table-and-Text HybridQA: Concepts, Methods, Challenges and\n  Future Directions", "abstract": "Table-and-text hybrid question answering (HybridQA) is a widely used and\nchallenging NLP task commonly applied in the financial and scientific domain.\nThe early research focuses on migrating other QA task methods to HybridQA,\nwhile with further research, more and more HybridQA-specific methods have been\npresent. With the rapid development of HybridQA, the systematic survey is still\nunder-explored to summarize the main techniques and advance further research.\nSo we present this work to summarize the current HybridQA benchmarks and\nmethods, then analyze the challenges and future directions of this task. The\ncontributions of this paper can be summarized in three folds: (1) first survey,\nto our best knowledge, including benchmarks, methods and challenges for\nHybridQA; (2) systematic investigation with the reasonable comparison of the\nexisting systems to articulate their advantages and shortcomings; (3) detailed\nanalysis of challenges in four important dimensions to shed light on future\ndirections.", "published": "2022-12-27 12:34:57", "link": "http://arxiv.org/abs/2212.13465v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Gold Standard and Benchmark for Comics Text Detection\n  and Recognition", "abstract": "This study focuses on improving the optical character recognition (OCR) data\nfor panels in the COMICS dataset, the largest dataset containing text and\nimages from comic books. To do this, we developed a pipeline for OCR processing\nand labeling of comic books and created the first text detection and\nrecognition datasets for western comics, called \"COMICS Text+: Detection\" and\n\"COMICS Text+: Recognition\". We evaluated the performance of state-of-the-art\ntext detection and recognition models on these datasets and found significant\nimprovement in word accuracy and normalized edit distance compared to the text\nin COMICS. We also created a new dataset called \"COMICS Text+\", which contains\nthe extracted text from the textboxes in the COMICS dataset. Using the improved\ntext data of COMICS Text+ in the comics processing model from resulted in\nstate-of-the-art performance on cloze-style tasks without changing the model\narchitecture. The COMICS Text+ dataset can be a valuable resource for\nresearchers working on tasks including text detection, recognition, and\nhigh-level processing of comics, such as narrative understanding, character\nrelations, and story generation. All the data and inference instructions can be\naccessed in https://github.com/gsoykan/comics_text_plus.", "published": "2022-12-27 12:05:23", "link": "http://arxiv.org/abs/2212.14674v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Don't Be So Sure! Boosting ASR Decoding via Confidence Relaxation", "abstract": "Automatic Speech Recognition (ASR) systems frequently use a search-based\ndecoding strategy aiming to find the best attainable transcript by considering\nmultiple candidates. One prominent speech recognition decoding heuristic is\nbeam search, which seeks the transcript with the greatest likelihood computed\nusing the predicted distribution. While showing substantial performance gains\nin various tasks, beam search loses some of its effectiveness when the\npredicted probabilities are highly confident, i.e., the predicted distribution\nis massed for a single or very few classes. We show that recently proposed\nSelf-Supervised Learning (SSL)-based ASR models tend to yield exceptionally\nconfident predictions that may hamper beam search from truly considering a\ndiverse set of candidates. We perform a layer analysis to reveal and visualize\nhow predictions evolve, and propose a decoding procedure that improves the\nperformance of fine-tuned ASR models. Our proposed approach does not require\nfurther training beyond the original fine-tuning, nor additional model\nparameters. In fact, we find that our proposed method requires significantly\nless inference computation than current approaches. We propose aggregating the\ntop M layers, potentially leveraging useful information encoded in intermediate\nlayers, and relaxing model confidence. We demonstrate the effectiveness of our\napproach by conducting an empirical study on varying amounts of labeled\nresources and different model sizes, showing consistent improvements in\nparticular when applied to low-resource scenarios.", "published": "2022-12-27 06:42:26", "link": "http://arxiv.org/abs/2212.13378v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "DeepCuts: Single-Shot Interpretability based Pruning for BERT", "abstract": "As language models have grown in parameters and layers, it has become much\nharder to train and infer with them on single GPUs. This is severely\nrestricting the availability of large language models such as GPT-3,\nBERT-Large, and many others. A common technique to solve this problem is\npruning the network architecture by removing transformer heads, fully-connected\nweights, and other modules. The main challenge is to discern the important\nparameters from the less important ones. Our goal is to find strong metrics for\nidentifying such parameters. We thus propose two strategies: Cam-Cut based on\nthe GradCAM interpretations, and Smooth-Cut based on the SmoothGrad, for\ncalculating the importance scores. Through this work, we show that our scoring\nfunctions are able to assign more relevant task-based scores to the network\nparameters, and thus both our pruning approaches significantly outperform the\nstandard weight and gradient-based strategies, especially at higher compression\nratios in BERT-based models. We also analyze our pruning masks and find them to\nbe significantly different from the ones obtained using standard metrics.", "published": "2022-12-27 07:21:41", "link": "http://arxiv.org/abs/2212.13392v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Using Large Language Models to Generate Engaging Captions for Data\n  Visualizations", "abstract": "Creating compelling captions for data visualizations has been a longstanding\nchallenge. Visualization researchers are typically untrained in journalistic\nreporting and hence the captions that are placed below data visualizations tend\nto be not overly engaging and rather just stick to basic observations about the\ndata. In this work we explore the opportunities offered by the newly emerging\ncrop of large language models (LLM) which use sophisticated deep learning\ntechnology to produce human-like prose. We ask, can these powerful software\ndevices be purposed to produce engaging captions for generic data\nvisualizations like a scatterplot. It turns out that the key challenge lies in\ndesigning the most effective prompt for the LLM, a task called prompt\nengineering. We report on first experiments using the popular LLM GPT-3 and\ndeliver some promising results.", "published": "2022-12-27 23:56:57", "link": "http://arxiv.org/abs/2212.14047v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Countering Malicious Content Moderation Evasion in Online Social\n  Networks: Simulation and Detection of Word Camouflage", "abstract": "Content moderation is the process of screening and monitoring user-generated\ncontent online. It plays a crucial role in stopping content resulting from\nunacceptable behaviors such as hate speech, harassment, violence against\nspecific groups, terrorism, racism, xenophobia, homophobia, or misogyny, to\nmention some few, in Online Social Platforms. These platforms make use of a\nplethora of tools to detect and manage malicious information; however,\nmalicious actors also improve their skills, developing strategies to surpass\nthese barriers and continuing to spread misleading information. Twisting and\ncamouflaging keywords are among the most used techniques to evade platform\ncontent moderation systems. In response to this recent ongoing issue, this\npaper presents an innovative approach to address this linguistic trend in\nsocial networks through the simulation of different content evasion techniques\nand a multilingual Transformer model for content evasion detection. In this\nway, we share with the rest of the scientific community a multilingual public\ntool, named \"pyleetspeak\" to generate/simulate in a customizable way the\nphenomenon of content evasion through automatic word camouflage and a\nmultilingual Named-Entity Recognition (NER) Transformer-based model tuned for\nits recognition and detection. The multilingual NER model is evaluated in\ndifferent textual scenarios, detecting different types and mixtures of\ncamouflage techniques, achieving an overall weighted F1 score of 0.8795. This\narticle contributes significantly to countering malicious information by\ndeveloping multilingual tools to simulate and detect new methods of evasion of\ncontent on social networks, making the fight against information disorders more\neffective.", "published": "2022-12-27 16:08:49", "link": "http://arxiv.org/abs/2212.14727v1", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Voice conversion with limited data and limitless data augmentations", "abstract": "Applying changes to an input speech signal to change the perceived speaker of\nspeech to a target while maintaining the content of the input is a challenging\nbut interesting task known as Voice conversion (VC). Over the last few years,\nthis task has gained significant interest where most systems use data-driven\nmachine learning models. Doing the conversion in a low-latency real-world\nscenario is even more challenging constrained by the availability of\nhigh-quality data. Data augmentations such as pitch shifting and noise addition\nare often used to increase the amount of data used for training machine\nlearning based models for this task. In this paper we explore the efficacy of\ncommon data augmentation techniques for real-time voice conversion and\nintroduce novel techniques for data augmentation based on audio and voice\ntransformation effects as well. We evaluate the conversions for both male and\nfemale target speakers using objective and subjective evaluation methodologies.", "published": "2022-12-27 18:37:21", "link": "http://arxiv.org/abs/2212.13581v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Feature Selection Approaches for Optimising Music Emotion Recognition\n  Methods", "abstract": "The high feature dimensionality is a challenge in music emotion recognition.\nThere is no common consensus on a relation between audio features and emotion.\nThe MER system uses all available features to recognize emotion; however, this\nis not an optimal solution since it contains irrelevant data acting as noise.\nIn this paper, we introduce a feature selection approach to eliminate redundant\nfeatures for MER. We created a Selected Feature Set (SFS) based on the feature\nselection algorithm (FSA) and benchmarked it by training with two models,\nSupport Vector Regression (SVR) and Random Forest (RF) and comparing them\nagainst with using the Complete Feature Set (CFS). The result indicates that\nthe performance of MER has improved for both Random Forest (RF) and Support\nVector Regression (SVR) models by using SFS. We found using FSA can improve\nperformance in all scenarios, and it has potential benefits for model\nefficiency and stability for MER task.", "published": "2022-12-27 05:55:34", "link": "http://arxiv.org/abs/2212.13369v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audiovisual Database with 360 Video and Higher-Order Ambisonics Audio\n  for Perception, Cognition, Behavior, and QoE Evaluation Research", "abstract": "Research into multi-modal perception, human cognition, behavior, and\nattention can benefit from high-fidelity content that may recreate\nreal-life-like scenes when rendered on head-mounted displays. Moreover, aspects\nof audiovisual perception, cognitive processes, and behavior may complement\nquestionnaire-based Quality of Experience (QoE) evaluation of interactive\nvirtual environments. Currently, there is a lack of high-quality open-source\naudiovisual databases that can be used to evaluate such aspects or systems\ncapable of reproducing high-quality content. With this paper, we provide a\npublicly available audiovisual database consisting of twelve scenes capturing\nreal-life nature and urban environments with a video resolution of 7680x3840 at\n60 frames-per-second and with 4th-order Ambisonics audio. These 360 video\nsequences, with an average duration of 60 seconds, represent real-life settings\nfor systematically evaluating various dimensions of uni-/multi-modal\nperception, cognition, behavior, and QoE. The paper provides details of the\nscene requirements, recording approach, and scene descriptions. The database\nprovides high-quality reference material with a balanced focus on auditory and\nvisual sensory information. The database will be continuously updated with\nadditional scenes and further metadata such as human ratings and saliency\ninformation.", "published": "2022-12-27 10:47:08", "link": "http://arxiv.org/abs/2212.13442v1", "categories": ["eess.IV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "eess.IV"}
