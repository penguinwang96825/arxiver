{"title": "Automated Market Makers: Toward More Profitable Liquidity Provisioning Strategies", "abstract": "To trade tokens in cryptoeconomic systems, automated market makers (AMMs)\ntypically rely on liquidity providers (LPs) that deposit tokens in exchange for\nrewards. To profit from such rewards, LPs must use effective liquidity\nprovisioning strategies. However, LPs lack guidance for developing such\nstrategies, which often leads them to financial losses. We developed a\nmeasurement model based on impermanent loss to analyze the influences of key\nparameters (i.e., liquidity pool type, position duration, position range size,\nand position size) of liquidity provisioning strategies on LPs' returns. To\nreveal the influences of those key parameters on LPs' profits, we used the\nmeasurement model to analyze 700 days of historical liquidity provision data of\nUniswap v3. By uncovering the influences of key parameters of liquidity\nprovisioning strategies on profitability, this work supports LPs in developing\nmore profitable strategies.", "published": "2025-01-14 04:12:50", "link": "http://arxiv.org/abs/2501.07828v1", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "Reasoning with Graphs: Structuring Implicit Knowledge to Enhance LLMs\n  Reasoning", "abstract": "Large language models (LLMs) have demonstrated remarkable success across a\nwide range of tasks; however, they still encounter challenges in reasoning\ntasks that require understanding and inferring relationships between distinct\npieces of information within text sequences. This challenge is particularly\npronounced in tasks involving multi-step processes, such as logical reasoning\nand multi-hop question answering, where understanding implicit relationships\nbetween entities and leveraging multi-hop connections in the given context are\ncrucial. Graphs, as fundamental data structures, explicitly represent pairwise\nrelationships between entities, thereby offering the potential to enhance LLMs'\nreasoning capabilities. External graphs have proven effective in supporting\nLLMs across multiple tasks. However, in many reasoning tasks, no pre-existing\ngraph structure is provided. Can we structure implicit knowledge derived from\ncontext into graphs to assist LLMs in reasoning? In this paper, we propose\nReasoning with Graphs (RwG) by first constructing explicit graphs from the\ncontext and then leveraging these graphs to enhance LLM reasoning performance\non reasoning tasks. Extensive experiments demonstrate the effectiveness of the\nproposed method in improving both logical reasoning and multi-hop question\nanswering tasks.", "published": "2025-01-14 05:18:20", "link": "http://arxiv.org/abs/2501.07845v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ReARTeR: Retrieval-Augmented Reasoning with Trustworthy Process\n  Rewarding", "abstract": "Retrieval-Augmented Generation (RAG) systems for Large Language Models (LLMs)\nhold promise in knowledge-intensive tasks but face limitations in complex\nmulti-step reasoning. While recent methods have integrated RAG with\nchain-of-thought reasoning or test-time search using Process Reward Models\n(PRMs), these approaches encounter challenges such as a lack of explanations,\nbias in PRM training data, early-step bias in PRM scores, and insufficient\npost-training optimization of reasoning potential. To address these issues, we\npropose Retrieval-Augmented Reasoning through Trustworthy Process Rewarding\n(ReARTeR), a framework that enhances RAG systems' reasoning capabilities\nthrough post-training and test-time scaling. At test time, ReARTeR introduces\nTrustworthy Process Rewarding via a Process Reward Model for accurate scalar\nscoring and a Process Explanation Model (PEM) for generating natural language\nexplanations, enabling step refinement. During post-training, it utilizes Monte\nCarlo Tree Search guided by Trustworthy Process Rewarding to collect\nhigh-quality step-level preference data, optimized through Iterative Preference\nOptimization. ReARTeR addresses three core challenges: (1) misalignment between\nPRM and PEM, tackled through off-policy preference learning; (2) bias in PRM\ntraining data, mitigated by balanced annotation methods and stronger\nannotations for challenging examples; and (3) early-step bias in PRM, resolved\nthrough a temporal-difference-based look-ahead search strategy. Experimental\nresults on multi-step reasoning benchmarks demonstrate significant\nimprovements, underscoring ReARTeR's potential to advance the reasoning\ncapabilities of RAG systems.", "published": "2025-01-14 05:56:26", "link": "http://arxiv.org/abs/2501.07861v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Wait, did you mean the doctor?\": Collecting a Dialogue Corpus for\n  Topical Analysis", "abstract": "Dialogue is at the core of human behaviour and being able to identify the\ntopic at hand is crucial to take part in conversation. Yet, there are few\naccounts of the topical organisation in casual dialogue and of how people\nrecognise the current topic in the literature. Moreover, analysing topics in\ndialogue requires conversations long enough to contain several topics and types\nof topic shifts. Such data is complicated to collect and annotate. In this\npaper we present a dialogue collection experiment which aims to build a corpus\nsuitable for topical analysis. We will carry out the collection with a\nmessaging tool we developed.", "published": "2025-01-14 09:00:45", "link": "http://arxiv.org/abs/2501.07947v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Formalising lexical and syntactic diversity for data sampling in French", "abstract": "Diversity is an important property of datasets and sampling data for\ndiversity is useful in dataset creation. Finding the optimally diverse sample\nis expensive, we therefore present a heuristic significantly increasing\ndiversity relative to random sampling. We also explore whether different kinds\nof diversity -- lexical and syntactic -- correlate, with the purpose of\nsampling for expensive syntactic diversity through inexpensive lexical\ndiversity. We find that correlations fluctuate with different datasets and\nversions of diversity measures. This shows that an arbitrarily chosen measure\nmay fall short of capturing diversity-related properties of datasets.", "published": "2025-01-14 10:47:33", "link": "http://arxiv.org/abs/2501.08003v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OpenCSG Chinese Corpus: A Series of High-quality Chinese Datasets for\n  LLM Training", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, but\ntheir success heavily relies on the quality of pretraining corpora. For Chinese\nLLMs, the scarcity of high-quality Chinese datasets presents a significant\nchallenge, often limiting their performance. To address this issue, we propose\nthe OpenCSG Chinese Corpus, a series of high-quality datasets specifically\ndesigned for LLM pretraining, post-training, and fine-tuning. This corpus\nincludes Fineweb-edu-chinese, Fineweb-edu-chinese-v2, Cosmopedia-chinese, and\nSmoltalk-chinese, each with distinct characteristics: Fineweb-edu datasets\nfocus on filtered, high-quality content derived from diverse Chinese web\nsources; Cosmopedia-chinese provides synthetic, textbook-style data for\nknowledge-intensive training; and Smoltalk-chinese emphasizes stylistic and\ndiverse chat-format data. The OpenCSG Chinese Corpus is characterized by its\nhigh-quality text, diverse coverage across domains, and scalable, reproducible\ndata curation processes. Additionally, we conducted extensive experimental\nanalyses, including evaluations on smaller parameter models, which demonstrated\nsignificant performance improvements in tasks such as C-Eval, showcasing the\neffectiveness of the corpus for training Chinese LLMs.", "published": "2025-01-14 15:22:47", "link": "http://arxiv.org/abs/2501.08197v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math\n  Problem Solving", "abstract": "While Large Language Models (LLMs) have shown impressive capabilities in math\nproblem-solving tasks, their robustness to noisy inputs is not well-studied. In\nthis work, we propose ArithmAttack to examine how robust the LLMs are when they\nencounter noisy prompts that contain extra noise in the form of punctuation\nmarks. While being easy to implement, ArithmAttack does not cause any\ninformation loss since words are not added or deleted from the context. We\nevaluate the robustness of seven LLMs, including LLama3, Mistral, and\nMathstral, on noisy GSM8K and MultiArith datasets. Our experiments suggest that\nall the studied models show vulnerability to such noise, with more noise\nleading to poorer performances.", "published": "2025-01-14 15:38:41", "link": "http://arxiv.org/abs/2501.08203v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Robustness of LLMs to Sociodemographically-Conditioned\n  Paraphrasing", "abstract": "Large Language Models (LLMs) have shown impressive performance in various NLP\ntasks. However, there are concerns about their reliability in different domains\nof linguistic variations. Many works have proposed robustness evaluation\nmeasures for local adversarial attacks, but we need globally robust models\nunbiased to different language styles. We take a broader approach to explore a\nwider range of variations across sociodemographic dimensions to perform\nstructured reliability tests on the reasoning capacity of language models. We\nextend the SocialIQA dataset to create diverse paraphrased sets conditioned on\nsociodemographic styles. The assessment aims to provide a deeper understanding\nof LLMs in (a) their capability of generating demographic paraphrases with\nengineered prompts and (b) their reasoning capabilities in real-world, complex\nlanguage scenarios. We also explore measures such as perplexity,\nexplainability, and ATOMIC performance of paraphrases for fine-grained\nreliability analysis of LLMs on these sets. We find that demographic-specific\nparaphrasing significantly impacts the performance of language models,\nindicating that the subtleties of language variations remain a significant\nchallenge. The code and dataset will be made available for reproducibility and\nfuture research.", "published": "2025-01-14 17:50:06", "link": "http://arxiv.org/abs/2501.08276v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AfriHate: A Multilingual Collection of Hate Speech and Abusive Language\n  Datasets for African Languages", "abstract": "Hate speech and abusive language are global phenomena that need\nsocio-cultural background knowledge to be understood, identified, and\nmoderated. However, in many regions of the Global South, there have been\nseveral documented occurrences of (1) absence of moderation and (2) censorship\ndue to the reliance on keyword spotting out of context. Further, high-profile\nindividuals have frequently been at the center of the moderation process, while\nlarge and targeted hate speech campaigns against minorities have been\noverlooked. These limitations are mainly due to the lack of high-quality data\nin the local languages and the failure to include local communities in the\ncollection, annotation, and moderation processes. To address this issue, we\npresent AfriHate: a multilingual collection of hate speech and abusive language\ndatasets in 15 African languages. Each instance in AfriHate is annotated by\nnative speakers familiar with the local culture. We report the challenges\nrelated to the construction of the datasets and present various classification\nbaseline results with and without using LLMs. The datasets, individual\nannotations, and hate speech and offensive language lexicons are available on\nhttps://github.com/AfriHate/AfriHate", "published": "2025-01-14 18:00:07", "link": "http://arxiv.org/abs/2501.08284v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Pedophile Attribution Techniques for Online Platforms", "abstract": "Reliance on anonymity in social media has increased its popularity on these\nplatforms among all ages. The availability of public Wi-Fi networks has\nfacilitated a vast variety of online content, including social media\napplications. Although anonymity and ease of access can be a convenient means\nof communication for their users, it is difficult to manage and protect its\nvulnerable users against sexual predators. Using an automated identification\nsystem that can attribute predators to their text would make the solution more\nattainable. In this survey, we provide a review of the methods of pedophile\nattribution used in social media platforms. We examine the effect of the size\nof the suspect set and the length of the text on the task of attribution.\nMoreover, we review the most-used datasets, features, classification techniques\nand performance measures for attributing sexual predators. We found that few\nstudies have proposed tools to mitigate the risk of online sexual predators,\nbut none of them can provide suspect attribution. Finally, we list several open\nresearch problems.", "published": "2025-01-14 18:25:07", "link": "http://arxiv.org/abs/2501.08296v1", "categories": ["cs.CL", "A.1, I.7.5"], "primary_category": "cs.CL"}
{"title": "Everybody Likes to Sleep: A Computer-Assisted Comparison of Object\n  Naming Data from 30 Languages", "abstract": "Object naming - the act of identifying an object with a word or a phrase - is\na fundamental skill in interpersonal communication, relevant to many\ndisciplines, such as psycholinguistics, cognitive linguistics, or language and\nvision research. Object naming datasets, which consist of concept lists with\npicture pairings, are used to gain insights into how humans access and select\nnames for objects in their surroundings and to study the cognitive processes\ninvolved in converting visual stimuli into semantic concepts. Unfortunately,\nobject naming datasets often lack transparency and have a highly idiosyncratic\nstructure. Our study tries to make current object naming data transparent and\ncomparable by using a multilingual, computer-assisted approach that links\nindividual items of object naming lists to unified concepts. Our current sample\nlinks 17 object naming datasets that cover 30 languages from 10 different\nlanguage families. We illustrate how the comparative dataset can be explored by\nsearching for concepts that recur across the majority of datasets and comparing\nthe conceptual spaces of covered object naming datasets with classical basic\nvocabulary lists from historical linguistics and linguistic typology. Our\nfindings can serve as a basis for enhancing cross-linguistic object naming\nresearch and as a guideline for future studies dealing with object naming\ntasks.", "published": "2025-01-14 18:50:00", "link": "http://arxiv.org/abs/2501.08312v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Automated Interpretability with Output-Centric Feature\n  Descriptions", "abstract": "Automated interpretability pipelines generate natural language descriptions\nfor the concepts represented by features in large language models (LLMs), such\nas plants or the first word in a sentence. These descriptions are derived using\ninputs that activate the feature, which may be a dimension or a direction in\nthe model's representation space. However, identifying activating inputs is\ncostly, and the mechanistic role of a feature in model behavior is determined\nboth by how inputs cause a feature to activate and by how feature activation\naffects outputs. Using steering evaluations, we reveal that current pipelines\nprovide descriptions that fail to capture the causal effect of the feature on\noutputs. To fix this, we propose efficient, output-centric methods for\nautomatically generating feature descriptions. These methods use the tokens\nweighted higher after feature stimulation or the highest weight tokens after\napplying the vocabulary \"unembedding\" head directly to the feature. Our\noutput-centric descriptions better capture the causal effect of a feature on\nmodel outputs than input-centric descriptions, but combining the two leads to\nthe best performance on both input and output evaluations. Lastly, we show that\noutput-centric descriptions can be used to find inputs that activate features\npreviously thought to be \"dead\".", "published": "2025-01-14 18:53:00", "link": "http://arxiv.org/abs/2501.08319v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Robustness of Multilingual LLMs on Real-World Noisy Data", "abstract": "Large Language Models (LLMs) are trained on Web data that might contain\nspelling errors made by humans. But do they become robust to similar real-world\nnoise? In this paper, we investigate the effect of real-world spelling mistakes\non the performance of 9 language models, with parameters ranging from 0.2B to\n13B, in 3 different NLP tasks, namely Natural Language Inference (NLI), Name\nEntity Recognition (NER), and Intent Classification (IC). We perform our\nexperiments on 6 different languages and build a dictionary of real-world noise\nfor them using the Wikipedia edit history. We show that the performance gap of\nthe studied models on the clean and noisy test data averaged across all the\ndatasets and languages ranges from 2.3 to 4.3 absolute percentage points. In\naddition, mT5 models, in general, show more robustness compared to BLOOM,\nFalcon, and BERT-like models. In particular, mT5 (13B), was the most robust on\naverage overall, across the 3 tasks, and in 4 of the 6 languages.", "published": "2025-01-14 18:55:35", "link": "http://arxiv.org/abs/2501.08322v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ensemble of Large Language Models for Curated Labeling and Rating of\n  Free-text Data", "abstract": "Free-text responses are commonly collected in psychological studies,\nproviding rich qualitative insights that quantitative measures may not capture.\nLabeling curated topics of research interest in free-text data by multiple\ntrained human coders is typically labor-intensive and time-consuming. Though\nlarge language models (LLMs) excel in language processing, LLM-assisted\nlabeling techniques relying on closed-source LLMs cannot be directly applied to\nfree-text data, without explicit consent for external use.\n  In this study, we propose a framework of assembling locally-deployable LLMs\nto enhance the labeling of predetermined topics in free-text data under privacy\nconstraints. Analogous to annotation by multiple human raters, this framework\nleverages the heterogeneity of diverse open-source LLMs. The ensemble approach\nseeks a balance between the agreement and disagreement across LLMs, guided by a\nrelevancy scoring methodology that utilizes embedding distances between topic\ndescriptions and LLMs' reasoning. We evaluated the ensemble approach using both\npublicly accessible Reddit data from eating disorder related forums, and\nfree-text responses from eating disorder patients, both complemented by human\nannotations.\n  We found that: (1) there is heterogeneity in the performance of labeling\namong same-sized LLMs, with some showing low sensitivity but high precision,\nwhile others exhibit high sensitivity but low precision. (2) Compared to\nindividual LLMs, the ensemble of LLMs achieved the highest accuracy and optimal\nprecision-sensitivity trade-off in predicting human annotations. (3) The\nrelevancy scores across LLMs showed greater agreement than dichotomous labels,\nindicating that the relevancy scoring method effectively mitigates the\nheterogeneity in LLMs' labeling.", "published": "2025-01-14 20:08:16", "link": "http://arxiv.org/abs/2501.08413v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Religious Bias Landscape in Language and Text-to-Image Models: Analysis,\n  Detection, and Debiasing Strategies", "abstract": "Note: This paper includes examples of potentially offensive content related\nto religious bias, presented solely for academic purposes. The widespread\nadoption of language models highlights the need for critical examinations of\ntheir inherent biases, particularly concerning religion. This study\nsystematically investigates religious bias in both language models and\ntext-to-image generation models, analyzing both open-source and closed-source\nsystems. We construct approximately 400 unique, naturally occurring prompts to\nprobe language models for religious bias across diverse tasks, including mask\nfilling, prompt completion, and image generation. Our experiments reveal\nconcerning instances of underlying stereotypes and biases associated\ndisproportionately with certain religions. Additionally, we explore\ncross-domain biases, examining how religious bias intersects with demographic\nfactors such as gender, age, and nationality. This study further evaluates the\neffectiveness of targeted debiasing techniques by employing corrective prompts\ndesigned to mitigate the identified biases. Our findings demonstrate that\nlanguage models continue to exhibit significant biases in both text and image\ngeneration tasks, emphasizing the urgent need to develop fairer language models\nto achieve global acceptability.", "published": "2025-01-14 21:10:08", "link": "http://arxiv.org/abs/2501.08441v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Jochre 3 and the Yiddish OCR corpus", "abstract": "We describe the construction of a publicly available Yiddish OCR Corpus, and\ndescribe and evaluate the open source OCR tool suite Jochre 3, including an\nAlto editor for corpus annotation, OCR software for Alto OCR layer generation,\nand a customizable OCR search engine. The current version of the Yiddish OCR\ncorpus contains 658 pages, 186K tokens and 840K glyphs. The Jochre 3 OCR tool\nuses various fine-tuned YOLOv8 models for top-down page layout analysis, and a\ncustom CNN network for glyph recognition. It attains a CER of 1.5% on our test\ncorpus, far out-performing all other existing public models for Yiddish. We\nanalyzed the full 660M word Yiddish Book Center with Jochre 3 OCR, and the new\nOCR is searchable through the Yiddish Book Center OCR search engine.", "published": "2025-01-14 21:21:39", "link": "http://arxiv.org/abs/2501.08442v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Theater Stage as Laboratory: Review of Real-Time Comedy LLM Systems\n  for Live Performance", "abstract": "In this position paper, we review the eclectic recent history of academic and\nartistic works involving computational systems for humor generation, and focus\nspecifically on live performance. We make the case that AI comedy should be\nevaluated in live conditions, in front of audiences sharing either physical or\nonline spaces, and under real-time constraints. We further suggest that\nimprovised comedy is therefore the perfect substrate for deploying and\nassessing computational humor systems. Using examples of successful AI-infused\nshows, we demonstrate that live performance raises three sets of challenges for\ncomputational humor generation: 1) questions around robotic embodiment,\nanthropomorphism and competition between humans and machines, 2) questions\naround comedic timing and the nature of audience interaction, and 3) questions\nabout the human interpretation of seemingly absurd AI-generated humor. We argue\nthat these questions impact the choice of methodologies for evaluating\ncomputational humor, as any such method needs to work around the constraints of\nlive audiences and performance spaces. These interrogations also highlight\ndifferent types of collaborative relationship of human comedians towards AI\ntools.", "published": "2025-01-14 22:38:55", "link": "http://arxiv.org/abs/2501.08474v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models for Knowledge Graph Embedding: A Survey", "abstract": "Large language models (LLMs) have garnered significant attention for their\nsuperior performance in many knowledge-driven applications on the world wide\nweb.These models are designed to train hundreds of millions or more parameters\non large amounts of text data, enabling them to understand and generate\nnaturallanguage effectively. As the superior performance of LLMs becomes\napparent,they are increasingly being applied to knowledge graph embedding (KGE)\nrelated tasks to improve the processing results. Traditional KGE representation\nlearning methods map entities and relations into a low-dimensional vector\nspace, enablingthe triples in the knowledge graph to satisfy a specific scoring\nfunction in thevector space. However, based on the powerful language\nunderstanding and seman-tic modeling capabilities of LLMs, that have recently\nbeen invoked to varying degrees in different types of KGE related scenarios\nsuch as multi-modal KGE andopen KGE according to their task characteristics. In\nthis paper, we investigate awide range of approaches for performing\nLLMs-related tasks in different types of KGE scenarios. To better compare the\nvarious approaches, we summarize each KGE scenario in a classification.\nFinally, we discuss the applications in which the methods are mainly used and\nsuggest several forward-looking directions for the development of this new\nresearch area.", "published": "2025-01-14 00:47:24", "link": "http://arxiv.org/abs/2501.07766v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Parameter-Inverted Image Pyramid Networks for Visual Perception and\n  Multimodal Understanding", "abstract": "Image pyramids are widely adopted in top-performing methods to obtain\nmulti-scale features for precise visual perception and understanding. However,\ncurrent image pyramids use the same large-scale model to process multiple\nresolutions of images, leading to significant computational cost. To address\nthis challenge, we propose a novel network architecture, called\nParameter-Inverted Image Pyramid Networks (PIIP). Specifically, PIIP uses\npretrained models (ViTs or CNNs) as branches to process multi-scale images,\nwhere images of higher resolutions are processed by smaller network branches to\nbalance computational cost and performance. To integrate information from\ndifferent spatial scales, we further propose a novel cross-branch feature\ninteraction mechanism. To validate PIIP, we apply it to various perception\nmodels and a representative multimodal large language model called LLaVA, and\nconduct extensive experiments on various tasks such as object detection,\nsegmentation, image classification and multimodal understanding. PIIP achieves\nsuperior performance compared to single-branch and existing multi-resolution\napproaches with lower computational cost. When applied to InternViT-6B, a\nlarge-scale vision foundation model, PIIP can improve its performance by 1%-2%\non detection and segmentation with only 40%-60% of the original computation,\nfinally achieving 60.0 box AP on MS COCO and 59.7 mIoU on ADE20K. For\nmultimodal understanding, our PIIP-LLaVA achieves 73.0% accuracy on TextVQA and\n74.5% on MMBench with only 2.8M training data. Our code is released at\nhttps://github.com/OpenGVLab/PIIP.", "published": "2025-01-14 01:57:41", "link": "http://arxiv.org/abs/2501.07783v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Optimizing Language Models for Grammatical Acceptability: A Comparative\n  Study of Fine-Tuning Techniques", "abstract": "This study explores the fine-tuning (FT) of the Open Pre-trained Transformer\n(OPT-125M) for grammatical acceptability tasks using the CoLA dataset. By\ncomparing Vanilla-Fine-Tuning (VFT), Pattern-Based-Fine-Tuning (PBFT), and\nParameter-Efficient Fine-Tuning techniques (PEFT) like Low-Rank Adaptation\n(LoRA), we demonstrate significant improvements in computational efficiency\nwhile maintaining high accuracy. Our experiments reveal that while VFT achieves\nthe highest accuracy (81.2%), LoRA enhancing FT by reducing memory usage and\niteration time by more than 50%, and increases accuracy in PBFT case. Context\nDistillation (CD), though computationally efficient, underperformed with\naccuracy around 31%. Our findings contribute to democratizing access to large\nlanguage models (LLM) by reducing computational barriers.", "published": "2025-01-14 05:41:09", "link": "http://arxiv.org/abs/2501.07853v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Continual Learning with Embedding Layer Surgery and Task-wise Beam\n  Search using Whisper", "abstract": "Current Multilingual ASR models only support a fraction of the world's\nlanguages. Continual Learning (CL) aims to tackle this problem by adding new\nlanguages to pre-trained models while avoiding the loss of performance on\nexisting languages, also known as Catastrophic Forgetting (CF). However,\nexisting CL methods overlook the adaptation of the token embedding lookup table\nat the decoder, despite its significant contribution to CF. We propose\nEmbedding Layer Surgery where separate copies of the token embeddings are\ncreated for each new languages, and one of the copies is selected to replace\nthe old languages embeddings when transcribing the corresponding new language.\nUnfortunately, this approach means LID errors also cause incorrect ASR\nembedding selection. Our Task-wise Beam Search allows self-correction for such\nmistakes. By adapting Whisper to 10 hours of data for each of 10 unseen\nlanguages from Common Voice, results show that our method reduces the Average\nWER (AWER) of pre-trained languages from 14.2% to 11.9% compared with\nExperience Replay, without compromising the AWER of the unseen languages.", "published": "2025-01-14 06:33:40", "link": "http://arxiv.org/abs/2501.07875v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GRAPHMOE: Amplifying Cognitive Depth of Mixture-of-Experts Network via\n  Introducing Self-Rethinking Mechanism", "abstract": "Traditional Mixture-of-Experts (MoE) networks benefit from utilizing multiple\nsmaller expert models as opposed to a single large network. However, these\nexperts typically operate independently, leaving a question open about whether\ninterconnecting these models could enhance the performance of MoE networks. In\nresponse, we introduce GRAPHMOE, a novel method aimed at augmenting the\ncognitive depth of language models via a self-rethinking mechanism constructed\non Pseudo GraphMoE networks. GRAPHMOE employs a recurrent routing strategy to\nsimulate iterative thinking steps, thereby facilitating the flow of information\namong expert nodes. We implement the GRAPHMOE architecture using Low-Rank\nAdaptation techniques (LoRA) and conduct extensive experiments on various\nbenchmark datasets. The experimental results reveal that GRAPHMOE outperforms\nother LoRA based models, achieving state-of-the-art (SOTA) performance.\nAdditionally, this study explores a novel recurrent routing strategy that may\ninspire further advancements in enhancing the reasoning capabilities of\nlanguage models.", "published": "2025-01-14 06:59:51", "link": "http://arxiv.org/abs/2501.07890v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Aviation Safety Enhancement via NLP & Deep Learning: Classifying Flight\n  Phases in ATSB Safety Reports", "abstract": "Aviation safety is paramount, demanding precise analysis of safety\noccurrences during different flight phases. This study employs Natural Language\nProcessing (NLP) and Deep Learning models, including LSTM, CNN, Bidirectional\nLSTM (BLSTM), and simple Recurrent Neural Networks (sRNN), to classify flight\nphases in safety reports from the Australian Transport Safety Bureau (ATSB).\nThe models exhibited high accuracy, precision, recall, and F1 scores, with LSTM\nachieving the highest performance of 87%, 88%, 87%, and 88%, respectively. This\nperformance highlights their effectiveness in automating safety occurrence\nanalysis. The integration of NLP and Deep Learning technologies promises\ntransformative enhancements in aviation safety analysis, enabling targeted\nsafety measures and streamlined report handling.", "published": "2025-01-14 08:18:41", "link": "http://arxiv.org/abs/2501.07923v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Exploring Aviation Incident Narratives Using Topic Modeling and\n  Clustering Techniques", "abstract": "Aviation safety is a global concern, requiring detailed investigations into\nincidents to understand contributing factors comprehensively. This study uses\nthe National Transportation Safety Board (NTSB) dataset. It applies advanced\nnatural language processing (NLP) techniques, including Latent Dirichlet\nAllocation (LDA), Non-Negative Matrix Factorization (NMF), Latent Semantic\nAnalysis (LSA), Probabilistic Latent Semantic Analysis (pLSA), and K-means\nclustering. The main objectives are identifying latent themes, exploring\nsemantic relationships, assessing probabilistic connections, and cluster\nincidents based on shared characteristics. This research contributes to\naviation safety by providing insights into incident narratives and\ndemonstrating the versatility of NLP and topic modelling techniques in\nextracting valuable information from complex datasets. The results, including\ntopics identified from various techniques, provide an understanding of\nrecurring themes. Comparative analysis reveals that LDA performed best with a\ncoherence value of 0.597, pLSA of 0.583, LSA of 0.542, and NMF of 0.437.\nK-means clustering further reveals commonalities and unique insights into\nincident narratives. In conclusion, this study uncovers latent patterns and\nthematic structures within incident narratives, offering a comparative analysis\nof multiple-topic modelling techniques. Future research avenues include\nexploring temporal patterns, incorporating additional datasets, and developing\npredictive models for early identification of safety issues. This research lays\nthe groundwork for enhancing the understanding and improvement of aviation\nsafety by utilising the wealth of information embedded in incident narratives.", "published": "2025-01-14 08:23:15", "link": "http://arxiv.org/abs/2501.07924v1", "categories": ["cs.AI", "cs.CL", "Topic Modelling, narratives, clustering, Aviation Incidents, NTSB"], "primary_category": "cs.AI"}
{"title": "TriAdaptLoRA: Brain-Inspired Triangular Adaptive Low-Rank Adaptation for\n  Parameter-Efficient Fine-Tuning", "abstract": "The fine-tuning of Large Language Models (LLMs) is pivotal for achieving\noptimal performance across diverse downstream tasks. However, while full\nfine-tuning delivers superior results, it entails significant computational and\nresource costs. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA,\naddress these challenges by reducing the number of trainable parameters, but\nthey often struggle with rank adjustment efficiency and task-specific\nadaptability. We propose Triangular Adaptive Low-Rank Adaptation\n(TriAdaptLoRA), a novel PEFT framework inspired by neuroscience principles,\nwhich dynamically optimizes the allocation of trainable parameters.\nTriAdaptLoRA introduces three key innovations: 1) a triangular split of\ntransformation matrices into lower and upper triangular components to maximize\nparameter utilization, 2) a parameter importance metric based on normalized\nFrobenius norms for efficient adaptation, and 3) an adaptive rank-growth\nstrategy governed by dynamic thresholds, allowing flexible parameter allocation\nacross training steps. Experiments conducted on a variety of natural language\nunderstanding and generation tasks demonstrate that TriAdaptLoRA consistently\noutperforms existing PEFT methods. It achieves superior performance, enhanced\nstability, and reduced computational overhead, particularly under linear\nthreshold-driven rank growth. These results highlight its efficacy as a\nscalable and resource-efficient solution for fine-tuning LLMs.", "published": "2025-01-14 10:51:31", "link": "http://arxiv.org/abs/2501.08008v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "READ: Reinforcement-based Adversarial Learning for Text Classification\n  with Limited Labeled Data", "abstract": "Pre-trained transformer models such as BERT have shown massive gains across\nmany text classification tasks. However, these models usually need enormous\nlabeled data to achieve impressive performances. Obtaining labeled data is\noften expensive and time-consuming, whereas collecting unlabeled data using\nsome heuristics is relatively much cheaper for any task. Therefore, this paper\nproposes a method that encapsulates reinforcement learning-based text\ngeneration and semi-supervised adversarial learning approaches in a novel way\nto improve the model's performance. Our method READ, Reinforcement-based\nAdversarial learning, utilizes an unlabeled dataset to generate diverse\nsynthetic text through reinforcement learning, improving the model's\ngeneralization capability using adversarial learning. Our experimental results\nshow that READ outperforms the existing state-of-art methods on multiple\ndatasets.", "published": "2025-01-14 11:39:55", "link": "http://arxiv.org/abs/2501.08035v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Narrative Clustering in Large Language Models: A Layerwise\n  Analysis of BERT", "abstract": "This study investigates the internal mechanisms of BERT, a transformer-based\nlarge language model, with a focus on its ability to cluster narrative content\nand authorial style across its layers. Using a dataset of narratives developed\nvia GPT-4, featuring diverse semantic content and stylistic variations, we\nanalyze BERT's layerwise activations to uncover patterns of localized neural\nprocessing. Through dimensionality reduction techniques such as Principal\nComponent Analysis (PCA) and Multidimensional Scaling (MDS), we reveal that\nBERT exhibits strong clustering based on narrative content in its later layers,\nwith progressively compact and distinct clusters. While strong stylistic\nclustering might occur when narratives are rephrased into different text types\n(e.g., fables, sci-fi, kids' stories), minimal clustering is observed for\nauthorial style specific to individual writers. These findings highlight BERT's\nprioritization of semantic content over stylistic features, offering insights\ninto its representational capabilities and processing hierarchy. This study\ncontributes to understanding how transformer models like BERT encode linguistic\ninformation, paving the way for future interdisciplinary research in artificial\nintelligence and cognitive neuroscience.", "published": "2025-01-14 12:01:54", "link": "http://arxiv.org/abs/2501.08053v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dynamic Multimodal Sentiment Analysis: Leveraging Cross-Modal Attention\n  for Enabled Classification", "abstract": "This paper explores the development of a multimodal sentiment analysis model\nthat integrates text, audio, and visual data to enhance sentiment\nclassification. The goal is to improve emotion detection by capturing the\ncomplex interactions between these modalities, thereby enabling more accurate\nand nuanced sentiment interpretation. The study evaluates three feature fusion\nstrategies -- late stage fusion, early stage fusion, and multi-headed attention\n-- within a transformer-based architecture. Experiments were conducted using\nthe CMU-MOSEI dataset, which includes synchronized text, audio, and visual\ninputs labeled with sentiment scores. Results show that early stage fusion\nsignificantly outperforms late stage fusion, achieving an accuracy of 71.87\\%,\nwhile the multi-headed attention approach offers marginal improvement, reaching\n72.39\\%. The findings suggest that integrating modalities early in the process\nenhances sentiment classification, while attention mechanisms may have limited\nimpact within the current framework. Future work will focus on refining feature\nfusion techniques, incorporating temporal data, and exploring dynamic feature\nweighting to further improve model performance.", "published": "2025-01-14 12:54:19", "link": "http://arxiv.org/abs/2501.08085v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Refusal Behavior in Large Language Models: A Nonlinear Perspective", "abstract": "Refusal behavior in large language models (LLMs) enables them to decline\nresponding to harmful, unethical, or inappropriate prompts, ensuring alignment\nwith ethical standards. This paper investigates refusal behavior across six\nLLMs from three architectural families. We challenge the assumption of refusal\nas a linear phenomenon by employing dimensionality reduction techniques,\nincluding PCA, t-SNE, and UMAP. Our results reveal that refusal mechanisms\nexhibit nonlinear, multidimensional characteristics that vary by model\narchitecture and layer. These findings highlight the need for nonlinear\ninterpretability to improve alignment research and inform safer AI deployment\nstrategies.", "published": "2025-01-14 14:23:18", "link": "http://arxiv.org/abs/2501.08145v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ASTRID -- An Automated and Scalable TRIaD for the Evaluation of\n  RAG-based Clinical Question Answering Systems", "abstract": "Large Language Models (LLMs) have shown impressive potential in clinical\nquestion answering (QA), with Retrieval Augmented Generation (RAG) emerging as\na leading approach for ensuring the factual accuracy of model responses.\nHowever, current automated RAG metrics perform poorly in clinical and\nconversational use cases. Using clinical human evaluations of responses is\nexpensive, unscalable, and not conducive to the continuous iterative\ndevelopment of RAG systems. To address these challenges, we introduce ASTRID -\nan Automated and Scalable TRIaD for evaluating clinical QA systems leveraging\nRAG - consisting of three metrics: Context Relevance (CR), Refusal Accuracy\n(RA), and Conversational Faithfulness (CF). Our novel evaluation metric, CF, is\ndesigned to better capture the faithfulness of a model's response to the\nknowledge base without penalising conversational elements. To validate our\ntriad, we curate a dataset of over 200 real-world patient questions posed to an\nLLM-based QA agent during surgical follow-up for cataract surgery - the highest\nvolume operation in the world - augmented with clinician-selected questions for\nemergency, clinical, and non-clinical out-of-domain scenarios. We demonstrate\nthat CF can predict human ratings of faithfulness better than existing\ndefinitions for conversational use cases. Furthermore, we show that evaluation\nusing our triad consisting of CF, RA, and CR exhibits alignment with clinician\nassessment for inappropriate, harmful, or unhelpful responses. Finally, using\nnine different LLMs, we demonstrate that the three metrics can closely agree\nwith human evaluations, highlighting the potential of these metrics for use in\nLLM-driven automated evaluation pipelines. We also publish the prompts and\ndatasets for these experiments, providing valuable resources for further\nresearch and development.", "published": "2025-01-14 15:46:39", "link": "http://arxiv.org/abs/2501.08208v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Comparative Analysis of Efficient Adapter-Based Fine-Tuning of\n  State-of-the-Art Transformer Models", "abstract": "In this work, we investigate the efficacy of various adapter architectures on\nsupervised binary classification tasks from the SuperGLUE benchmark as well as\na supervised multi-class news category classification task from Kaggle.\nSpecifically, we compare classification performance and time complexity of\nthree transformer models, namely DistilBERT, ELECTRA, and BART, using\nconventional fine-tuning as well as nine state-of-the-art (SoTA) adapter\narchitectures. Our analysis reveals performance differences across adapter\narchitectures, highlighting their ability to achieve comparable or better\nperformance relative to fine-tuning at a fraction of the training time. Similar\nresults are observed on the new classification task, further supporting our\nfindings and demonstrating adapters as efficient and flexible alternatives to\nfine-tuning. This study provides valuable insights and guidelines for selecting\nand implementing adapters in diverse natural language processing (NLP)\napplications.", "published": "2025-01-14 17:37:40", "link": "http://arxiv.org/abs/2501.08271v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HALoGEN: Fantastic LLM Hallucinations and Where to Find Them", "abstract": "Despite their impressive ability to generate high-quality and fluent text,\ngenerative large language models (LLMs) also produce hallucinations: statements\nthat are misaligned with established world knowledge or provided input context.\nHowever, measuring hallucination can be challenging, as having humans verify\nmodel generations on-the-fly is both expensive and time-consuming. In this\nwork, we release HALoGEN, a comprehensive hallucination benchmark consisting\nof: (1) 10,923 prompts for generative models spanning nine domains including\nprogramming, scientific attribution, and summarization, and (2) automatic\nhigh-precision verifiers for each use case that decompose LLM generations into\natomic units, and verify each unit against a high-quality knowledge source. We\nuse this framework to evaluate ~150,000 generations from 14 language models,\nfinding that even the best-performing models are riddled with hallucinations\n(sometimes up to 86% of generated atomic facts depending on the domain). We\nfurther define a novel error classification for LLM hallucinations based on\nwhether they likely stem from incorrect recollection of training data (Type A\nerrors), or incorrect knowledge in training data (Type B errors), or are\nfabrication (Type C errors). We hope our framework provides a foundation to\nenable the principled study of why generative models hallucinate, and advances\nthe development of trustworthy large language models.", "published": "2025-01-14 18:13:08", "link": "http://arxiv.org/abs/2501.08292v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MiniMax-01: Scaling Foundation Models with Lightning Attention", "abstract": "We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01,\nwhich are comparable to top-tier models while offering superior capabilities in\nprocessing longer contexts. The core lies in lightning attention and its\nefficient scaling. To maximize computational capacity, we integrate it with\nMixture of Experts (MoE), creating a model with 32 experts and 456 billion\ntotal parameters, of which 45.9 billion are activated for each token. We\ndevelop an optimized parallel strategy and highly efficient\ncomputation-communication overlap techniques for MoE and lightning attention.\nThis approach enables us to conduct efficient training and inference on models\nwith hundreds of billions of parameters across contexts spanning millions of\ntokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens\nduring training and extrapolate to 4 million tokens during inference at an\naffordable cost. Our vision-language model, MiniMax-VL-01 is built through\ncontinued training with 512 billion vision-language tokens. Experiments on both\nstandard and in-house benchmarks show that our models match the performance of\nstate-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32\ntimes longer context window. We publicly release MiniMax-01 at\nhttps://github.com/MiniMax-AI.", "published": "2025-01-14 18:50:05", "link": "http://arxiv.org/abs/2501.08313v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Tag&Tab: Pretraining Data Detection in Large Language Models Using\n  Keyword-Based Membership Inference Attack", "abstract": "Large language models (LLMs) have become essential digital task assistance\ntools. Their training relies heavily on the collection of vast amounts of data,\nwhich may include copyright-protected or sensitive information. Recent studies\non the detection of pretraining data in LLMs have primarily focused on\nsentence-level or paragraph-level membership inference attacks (MIAs), usually\ninvolving probability analysis of the target model prediction tokens. However,\nthe proposed methods often demonstrate poor performance, specifically in terms\nof accuracy, failing to account for the semantic importance of textual content\nand word significance. To address these shortcomings, we propose Tag&Tab, a\nnovel approach for detecting data that has been used as part of the LLM\npretraining. Our method leverages advanced natural language processing (NLP)\ntechniques to tag keywords in the input text - a process we term Tagging. Then,\nthe LLM is used to obtain the probabilities of these keywords and calculate\ntheir average log-likelihood to determine input text membership, a process we\nrefer to as Tabbing. Our experiments on three benchmark datasets (BookMIA,\nMIMIR, and the Pile) and several open-source LLMs of varying sizes demonstrate\nan average increase in the AUC scores ranging from 4.1% to 12.1% over\nstate-of-the-art methods. Tag&Tab not only sets a new standard for data leakage\ndetection in LLMs, but its outstanding performance is a testament to the\nimportance of words in MIAs on LLMs.", "published": "2025-01-14 21:55:37", "link": "http://arxiv.org/abs/2501.08454v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Large Language Models For Text Classification: Case Study And\n  Comprehensive Review", "abstract": "Unlocking the potential of Large Language Models (LLMs) in data\nclassification represents a promising frontier in natural language processing.\nIn this work, we evaluate the performance of different LLMs in comparison with\nstate-of-the-art deep-learning and machine-learning models, in two different\nclassification scenarios: i) the classification of employees' working locations\nbased on job reviews posted online (multiclass classification), and 2) the\nclassification of news articles as fake or not (binary classification). Our\nanalysis encompasses a diverse range of language models differentiating in\nsize, quantization, and architecture. We explore the impact of alternative\nprompting techniques and evaluate the models based on the weighted F1-score.\nAlso, we examine the trade-off between performance (F1-score) and time\n(inference response time) for each language model to provide a more nuanced\nunderstanding of each model's practical applicability. Our work reveals\nsignificant variations in model responses based on the prompting strategies. We\nfind that LLMs, particularly Llama3 and GPT-4, can outperform traditional\nmethods in complex classification tasks, such as multiclass classification,\nthough at the cost of longer inference times. In contrast, simpler ML models\noffer better performance-to-time trade-offs in simpler binary classification\ntasks.", "published": "2025-01-14 22:02:38", "link": "http://arxiv.org/abs/2501.08457v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing the De-identification of Personally Identifiable Information\n  in Educational Data", "abstract": "Protecting Personally Identifiable Information (PII), such as names, is a\ncritical requirement in learning technologies to safeguard student and teacher\nprivacy and maintain trust. Accurate PII detection is an essential step toward\nanonymizing sensitive information while preserving the utility of educational\ndata. Motivated by recent advancements in artificial intelligence, our study\ninvestigates the GPT-4o-mini model as a cost-effective and efficient solution\nfor PII detection tasks. We explore both prompting and fine-tuning approaches\nand compare GPT-4o-mini's performance against established frameworks, including\nMicrosoft Presidio and Azure AI Language. Our evaluation on two public\ndatasets, CRAPII and TSCC, demonstrates that the fine-tuned GPT-4o-mini model\nachieves superior performance, with a recall of 0.9589 on CRAPII. Additionally,\nfine-tuned GPT-4o-mini significantly improves precision scores (a threefold\nincrease) while reducing computational costs to nearly one-tenth of those\nassociated with Azure AI Language. Furthermore, our bias analysis reveals that\nthe fine-tuned GPT-4o-mini model consistently delivers accurate results across\ndiverse cultural backgrounds and genders. The generalizability analysis using\nthe TSCC dataset further highlights its robustness, achieving a recall of\n0.9895 with minimal additional training data from TSCC. These results emphasize\nthe potential of fine-tuned GPT-4o-mini as an accurate and cost-effective tool\nfor PII detection in educational data. It offers robust privacy protection\nwhile preserving the data's utility for research and pedagogical analysis. Our\ncode is available on GitHub: https://github.com/AnonJD/PrivacyAI", "published": "2025-01-14 20:53:38", "link": "http://arxiv.org/abs/2501.09765v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Talk to Right Specialists: Routing and Planning in Multi-agent System\n  for Question Answering", "abstract": "Leveraging large language models (LLMs), an agent can utilize\nretrieval-augmented generation (RAG) techniques to integrate external knowledge\nand increase the reliability of its responses. Current RAG-based agents\nintegrate single, domain-specific knowledge sources, limiting their ability and\nleading to hallucinated or inaccurate responses when addressing cross-domain\nqueries. Integrating multiple knowledge bases into a unified RAG-based agent\nraises significant challenges, including increased retrieval overhead and data\nsovereignty when sensitive data is involved. In this work, we propose RopMura,\na novel multi-agent system that addresses these limitations by incorporating\nhighly efficient routing and planning mechanisms. RopMura features two key\ncomponents: a router that intelligently selects the most relevant agents based\non knowledge boundaries and a planner that decomposes complex multi-hop queries\ninto manageable steps, allowing for coordinating cross-domain responses.\nExperimental results demonstrate that RopMura effectively handles both\nsingle-hop and multi-hop queries, with the routing mechanism enabling precise\nanswers for single-hop queries and the combined routing and planning mechanisms\nachieving accurate, multi-step resolutions for complex queries.", "published": "2025-01-14 03:25:26", "link": "http://arxiv.org/abs/2501.07813v1", "categories": ["cs.MA", "cs.AI", "cs.CL"], "primary_category": "cs.MA"}
{"title": "Agent-Centric Projection of Prompting Techniques and Implications for\n  Synthetic Training Data for Large Language Models", "abstract": "Recent advances in prompting techniques and multi-agent systems for Large\nLanguage Models (LLMs) have produced increasingly complex approaches. However,\nwe lack a framework for characterizing and comparing prompting techniques or\nunderstanding their relationship to multi-agent LLM systems. This position\npaper introduces and explains the concepts of linear contexts (a single,\ncontinuous sequence of interactions) and non-linear contexts (branching or\nmulti-path) in LLM systems. These concepts enable the development of an\nagent-centric projection of prompting techniques, a framework that can reveal\ndeep connections between prompting strategies and multi-agent systems. We\npropose three conjectures based on this framework: (1) results from non-linear\nprompting techniques can predict outcomes in equivalent multi-agent systems,\n(2) multi-agent system architectures can be replicated through single-LLM\nprompting techniques that simulate equivalent interaction patterns, and (3)\nthese equivalences suggest novel approaches for generating synthetic training\ndata. We argue that this perspective enables systematic cross-pollination of\nresearch findings between prompting and multi-agent domains, while providing\nnew directions for improving both the design and training of future LLM\nsystems.", "published": "2025-01-14 03:26:43", "link": "http://arxiv.org/abs/2501.07815v1", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI"}
{"title": "A Multi-Encoder Frozen-Decoder Approach for Fine-Tuning Large Language\n  Models", "abstract": "Among parameter-efficient fine-tuning methods, freezing has emerged as a\npopular strategy for speeding up training, reducing catastrophic forgetting,\nand improving downstream performance. We investigate the impact of freezing the\ndecoder in a multi-task setup comprising diverse natural language tasks, aiming\nto reduce deployment overhead and enhance portability to novel tasks. Our\nexperiments, conducted by fine-tuning both individual and multi-task setups on\nthe AlexaTM model, reveal that freezing decoders is highly effective for tasks\nwith natural language outputs and mitigates catastrophic forgetting in\nmultilingual tasks. However, we find that pairing frozen decoders with a larger\nmodel can effectively maintain or even enhance performance in structured and QA\ntasks, making it a viable strategy for a broader range of task types.", "published": "2025-01-14 03:43:23", "link": "http://arxiv.org/abs/2501.07818v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.5.1; I.5.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Real-time Verification and Refinement of Language Model Text Generation", "abstract": "Large language models (LLMs) have shown remarkable performance across a wide\nrange of natural language tasks. However, a critical challenge remains in that\nthey sometimes generate factually incorrect answers. To address this, while\nmany previous work has focused on identifying errors in their generation and\nfurther refining them, they are slow in deployment since they are designed to\nverify the response from LLMs only after their entire generation (from the\nfirst to last tokens) is done. Further, we observe that once LLMs generate\nincorrect tokens early on, there is a higher likelihood that subsequent tokens\nwill also be factually incorrect. To this end, in this work, we propose\nStreaming-VR (Streaming Verification and Refinement), a novel approach designed\nto enhance the efficiency of verification and refinement of LLM outputs.\nSpecifically, the proposed Streaming-VR enables on-the-fly verification and\ncorrection of tokens as they are being generated, similar to a streaming\nprocess, ensuring that each subset of tokens is checked and refined in\nreal-time by another LLM as the LLM constructs its response. Through\ncomprehensive evaluations on multiple datasets, we demonstrate that our\napproach not only enhances the factual accuracy of LLMs, but also offers a more\nefficient solution compared to prior refinement methods.", "published": "2025-01-14 03:59:48", "link": "http://arxiv.org/abs/2501.07824v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Iterative Label Refinement Matters More than Preference Optimization\n  under Weak Supervision", "abstract": "Language model (LM) post-training relies on two stages of human supervision:\ntask demonstrations for supervised finetuning (SFT), followed by preference\ncomparisons for reinforcement learning from human feedback (RLHF). As LMs\nbecome more capable, the tasks they are given become harder to supervise. Will\npost-training remain effective under unreliable supervision? To test this, we\nsimulate unreliable demonstrations and comparison feedback using small LMs and\ntime-constrained humans. We find that in the presence of unreliable\nsupervision, SFT still retains some effectiveness, but DPO (a common RLHF\nalgorithm) fails to improve the model beyond SFT. To address this, we propose\niterative label refinement (ILR) as an alternative to RLHF. ILR improves the\nSFT data by using comparison feedback to decide whether human demonstrations\nshould be replaced by model-generated alternatives, then retrains the model via\nSFT on the updated data. SFT+ILR outperforms SFT+DPO on several tasks with\nunreliable supervision (math, coding, and safe instruction-following). Our\nfindings suggest that as LMs are used for complex tasks where human supervision\nis unreliable, RLHF may no longer be the best use of human comparison feedback;\ninstead, it is better to direct feedback towards improving the training data\nrather than continually training the model. Our code and data are available at\nhttps://github.com/helloelwin/iterative-label-refinement.", "published": "2025-01-14 06:54:17", "link": "http://arxiv.org/abs/2501.07886v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Gandalf the Red: Adaptive Security for LLMs", "abstract": "Current evaluations of defenses against prompt attacks in large language\nmodel (LLM) applications often overlook two critical factors: the dynamic\nnature of adversarial behavior and the usability penalties imposed on\nlegitimate users by restrictive defenses. We propose D-SEC (Dynamic Security\nUtility Threat Model), which explicitly separates attackers from legitimate\nusers, models multi-step interactions, and expresses the security-utility in an\noptimizable form. We further address the shortcomings in existing evaluations\nby introducing Gandalf, a crowd-sourced, gamified red-teaming platform designed\nto generate realistic, adaptive attack. Using Gandalf, we collect and release a\ndataset of 279k prompt attacks. Complemented by benign user data, our analysis\nreveals the interplay between security and utility, showing that defenses\nintegrated in the LLM (e.g., system prompts) can degrade usability even without\nblocking requests. We demonstrate that restricted application domains,\ndefense-in-depth, and adaptive defenses are effective strategies for building\nsecure and useful LLM applications.", "published": "2025-01-14 08:30:49", "link": "http://arxiv.org/abs/2501.07927v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Optimizing Speech Multi-View Feature Fusion through Conditional\n  Computation", "abstract": "Recent advancements have highlighted the efficacy of self-supervised learning\n(SSL) features in various speech-related tasks, providing lightweight and\nversatile multi-view speech representations. However, our study reveals that\nwhile SSL features expedite model convergence, they conflict with traditional\nspectral features like FBanks in terms of update directions. In response, we\npropose a novel generalized feature fusion framework grounded in conditional\ncomputation, featuring a gradient-sensitive gating network and a multi-stage\ndropout strategy. This framework mitigates feature conflicts and bolsters model\nrobustness to multi-view input features. By integrating SSL and spectral\nfeatures, our approach accelerates convergence and maintains performance on par\nwith spectral models across multiple speech translation tasks on the MUSTC\ndataset.", "published": "2025-01-14 12:12:06", "link": "http://arxiv.org/abs/2501.08057v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Consistency of Responses and Continuations Generated by Large Language\n  Models on Social Media", "abstract": "Large Language Models (LLMs) demonstrate remarkable capabilities in text\ngeneration, yet their emotional consistency and semantic coherence in social\nmedia contexts remain insufficiently understood. This study investigates how\nLLMs handle emotional content and maintain semantic relationships through\ncontinuation and response tasks using two open-source models: Gemma and Llama.\nBy analyzing climate change discussions from Twitter and Reddit, we examine\nemotional transitions, intensity patterns, and semantic similarity between\nhuman-authored and LLM-generated content. Our findings reveal that while both\nmodels maintain high semantic coherence, they exhibit distinct emotional\npatterns: Gemma shows a tendency toward negative emotion amplification,\nparticularly anger, while maintaining certain positive emotions like optimism.\nLlama demonstrates superior emotional preservation across a broader spectrum of\naffects. Both models systematically generate responses with attenuated\nemotional intensity compared to human-authored content and show a bias toward\npositive emotions in response tasks. Additionally, both models maintain strong\nsemantic similarity with original texts, though performance varies between\ncontinuation and response tasks. These findings provide insights into LLMs'\nemotional and semantic processing capabilities, with implications for their\ndeployment in social media contexts and human-AI interaction design.", "published": "2025-01-14 13:19:47", "link": "http://arxiv.org/abs/2501.08102v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "In-situ graph reasoning and knowledge expansion using Graph-PReFLexOR", "abstract": "The pursuit of automated scientific discovery has fueled progress from\nsymbolic logic to modern AI, forging new frontiers in reasoning and pattern\nrecognition. Transformers function as potential systems, where every possible\nrelationship remains latent potentiality until tasks impose constraints, akin\nto measurement. Yet, refining their sampling requires more than probabilistic\nselection: solutions must conform to specific structures or rules, ensuring\nconsistency and the invocation of general principles. We present\nGraph-PReFLexOR (Graph-based Preference-based Recursive Language Modeling for\nExploratory Optimization of Reasoning), a framework that combines graph\nreasoning with symbolic abstraction to dynamically expand domain knowledge.\nInspired by reinforcement learning, Graph-PReFLexOR defines reasoning as a\nstructured mapping, where tasks yield knowledge graphs, abstract patterns, and\nultimately, final answers. Inspired by category theory, it encodes concepts as\nnodes and their relationships as edges, supporting hierarchical inference and\nadaptive learning through isomorphic representations. Demonstrations include\nhypothesis generation, materials design, and creative reasoning, such as\ndiscovering relationships between mythological concepts like 'thin places' with\nmaterials science. We propose a 'knowledge garden growth' strategy that\nintegrates insights across domains, promoting interdisciplinary connections.\nResults with a 3-billion-parameter Graph-PReFLexOR model show superior\nreasoning depth and adaptability, underscoring the potential for transparent,\nmultidisciplinary AI-driven discovery. It lays the groundwork for general\nautonomous reasoning solutions.", "published": "2025-01-14 13:52:41", "link": "http://arxiv.org/abs/2501.08120v1", "categories": ["cs.AI", "cond-mat.dis-nn", "cond-mat.mtrl-sci", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Potential and Perils of Large Language Models as Judges of Unstructured\n  Textual Data", "abstract": "Rapid advancements in large language models have unlocked remarkable\ncapabilities when it comes to processing and summarizing unstructured text\ndata. This has implications for the analysis of rich, open-ended datasets, such\nas survey responses, where LLMs hold the promise of efficiently distilling key\nthemes and sentiments. However, as organizations increasingly turn to these\npowerful AI systems to make sense of textual feedback, a critical question\narises, can we trust LLMs to accurately represent the perspectives contained\nwithin these text based datasets? While LLMs excel at generating human-like\nsummaries, there is a risk that their outputs may inadvertently diverge from\nthe true substance of the original responses. Discrepancies between the\nLLM-generated outputs and the actual themes present in the data could lead to\nflawed decision-making, with far-reaching consequences for organizations. This\nresearch investigates the effectiveness of LLM-as-judge models to evaluate the\nthematic alignment of summaries generated by other LLMs. We utilized an\nAnthropic Claude model to generate thematic summaries from open-ended survey\nresponses, with Amazon's Titan Express, Nova Pro, and Meta's Llama serving as\njudges. This LLM-as-judge approach was compared to human evaluations using\nCohen's kappa, Spearman's rho, and Krippendorff's alpha, validating a scalable\nalternative to traditional human centric evaluation methods. Our findings\nreveal that while LLM-as-judge offer a scalable solution comparable to human\nraters, humans may still excel at detecting subtle, context-specific nuances.\nOur research contributes to the growing body of knowledge on AI assisted text\nanalysis. Further, we provide recommendations for future research, emphasizing\nthe need for careful consideration when generalizing LLM-as-judge models across\nvarious contexts and use cases.", "published": "2025-01-14 14:49:14", "link": "http://arxiv.org/abs/2501.08167v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "CWEval: Outcome-driven Evaluation on Functionality and Security of LLM\n  Code Generation", "abstract": "Large Language Models (LLMs) have significantly aided developers by\ngenerating or assisting in code writing, enhancing productivity across various\ntasks. While identifying incorrect code is often straightforward, detecting\nvulnerabilities in functionally correct code is more challenging, especially\nfor developers with limited security knowledge, which poses considerable\nsecurity risks of using LLM-generated code and underscores the need for robust\nevaluation benchmarks that assess both functional correctness and security.\nCurrent benchmarks like CyberSecEval and SecurityEval attempt to solve it but\nare hindered by unclear and impractical specifications, failing to assess both\nfunctionality and security accurately. To tackle these deficiencies, we\nintroduce CWEval, a novel outcome-driven evaluation framework designed to\nenhance the evaluation of secure code generation by LLMs. This framework not\nonly assesses code functionality but also its security simultaneously with\nhigh-quality task specifications and outcome-driven test oracles which provides\nhigh accuracy. Coupled with CWEval-bench, a multilingual, security-critical\ncoding benchmark, CWEval provides a rigorous empirical security evaluation on\nLLM-generated code, overcoming previous benchmarks' shortcomings. Through our\nevaluations, CWEval reveals a notable portion of functional but insecure code\nproduced by LLMs, and shows a serious inaccuracy of previous evaluations,\nultimately contributing significantly to the field of secure code generation.\nWe open-source our artifact at: https://github.com/Co1lin/CWEval .", "published": "2025-01-14 15:27:01", "link": "http://arxiv.org/abs/2501.08200v1", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Eliciting In-context Retrieval and Reasoning for Long-context Large\n  Language Models", "abstract": "Recent advancements in long-context language models (LCLMs) promise to\ntransform Retrieval-Augmented Generation (RAG) by simplifying pipelines. With\ntheir expanded context windows, LCLMs can process entire knowledge bases and\nperform retrieval and reasoning directly -- a capability we define as\nIn-Context Retrieval and Reasoning (ICR^2). However, existing benchmarks like\nLOFT often overestimate LCLM performance by providing overly simplified\ncontexts. To address this, we introduce ICR^2, a benchmark that evaluates LCLMs\nin more realistic scenarios by including confounding passages retrieved with\nstrong retrievers. We then propose three methods to enhance LCLM performance:\n(1) retrieve-then-generate fine-tuning, (2) retrieval-attention-probing, which\nuses attention heads to filter and de-noise long contexts during decoding, and\n(3) joint retrieval head training alongside the generation head. Our evaluation\nof five well-known LCLMs on LOFT and ICR^2 demonstrates significant gains with\nour best approach applied to Mistral-7B: +17 and +15 points by Exact Match on\nLOFT, and +13 and +2 points on ICR^2, compared to vanilla RAG and supervised\nfine-tuning, respectively. It even outperforms GPT-4-Turbo on most tasks\ndespite being a much smaller model.", "published": "2025-01-14 16:38:33", "link": "http://arxiv.org/abs/2501.08248v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PokerBench: Training Large Language Models to become Professional Poker\n  Players", "abstract": "We introduce PokerBench - a benchmark for evaluating the poker-playing\nabilities of large language models (LLMs). As LLMs excel in traditional NLP\ntasks, their application to complex, strategic games like poker poses a new\nchallenge. Poker, an incomplete information game, demands a multitude of skills\nsuch as mathematics, reasoning, planning, strategy, and a deep understanding of\ngame theory and human psychology. This makes Poker the ideal next frontier for\nlarge language models. PokerBench consists of a comprehensive compilation of\n11,000 most important scenarios, split between pre-flop and post-flop play,\ndeveloped in collaboration with trained poker players. We evaluate prominent\nmodels including GPT-4, ChatGPT 3.5, and various Llama and Gemma series models,\nfinding that all state-of-the-art LLMs underperform in playing optimal poker.\nHowever, after fine-tuning, these models show marked improvements. We validate\nPokerBench by having models with different scores compete with each other,\ndemonstrating that higher scores on PokerBench lead to higher win rates in\nactual poker games. Through gameplay between our fine-tuned model and GPT-4, we\nalso identify limitations of simple supervised fine-tuning for learning optimal\nplaying strategy, suggesting the need for more advanced methodologies for\neffectively training language models to excel in games. PokerBench thus\npresents a unique benchmark for a quick and reliable evaluation of the\npoker-playing ability of LLMs as well as a comprehensive benchmark to study the\nprogress of LLMs in complex game-playing scenarios.", "published": "2025-01-14 18:59:03", "link": "http://arxiv.org/abs/2501.08328v2", "categories": ["cs.CL", "cs.AI", "cs.GT"], "primary_category": "cs.CL"}
{"title": "Towards Best Practices for Open Datasets for LLM Training", "abstract": "Many AI companies are training their large language models (LLMs) on data\nwithout the permission of the copyright owners. The permissibility of doing so\nvaries by jurisdiction: in countries like the EU and Japan, this is allowed\nunder certain restrictions, while in the United States, the legal landscape is\nmore ambiguous. Regardless of the legal status, concerns from creative\nproducers have led to several high-profile copyright lawsuits, and the threat\nof litigation is commonly cited as a reason for the recent trend towards\nminimizing the information shared about training datasets by both corporate and\npublic interest actors. This trend in limiting data information causes harm by\nhindering transparency, accountability, and innovation in the broader ecosystem\nby denying researchers, auditors, and impacted individuals access to the\ninformation needed to understand AI models.\n  While this could be mitigated by training language models on open access and\npublic domain data, at the time of writing, there are no such models (trained\nat a meaningful scale) due to the substantial technical and sociological\nchallenges in assembling the necessary corpus. These challenges include\nincomplete and unreliable metadata, the cost and complexity of digitizing\nphysical records, and the diverse set of legal and technical skills required to\nensure relevance and responsibility in a quickly changing landscape. Building\ntowards a future where AI systems can be trained on openly licensed data that\nis responsibly curated and governed requires collaboration across legal,\ntechnical, and policy domains, along with investments in metadata standards,\ndigitization, and fostering a culture of openness.", "published": "2025-01-14 17:18:05", "link": "http://arxiv.org/abs/2501.08365v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "OptiChat: Bridging Optimization Models and Practitioners with Large\n  Language Models", "abstract": "Optimization models have been applied to solve a wide variety of\ndecision-making problems. These models are usually developed by optimization\nexperts but are used by practitioners without optimization expertise in various\napplication domains. As a result, practitioners often struggle to interact with\nand draw useful conclusions from optimization models independently. To fill\nthis gap, we introduce OptiChat, a natural language dialogue system designed to\nhelp practitioners interpret model formulation, diagnose infeasibility, analyze\nsensitivity, retrieve information, evaluate modifications, and provide\ncounterfactual explanations. By augmenting large language models (LLMs) with\nfunctional calls and code generation tailored for optimization models, we\nenable seamless interaction and minimize the risk of hallucinations in\nOptiChat. We develop a new dataset to evaluate OptiChat's performance in\nexplaining optimization models. Experiments demonstrate that OptiChat\neffectively bridges the gap between optimization models and practitioners,\ndelivering autonomous, accurate, and instant responses.", "published": "2025-01-14 19:53:58", "link": "http://arxiv.org/abs/2501.08406v1", "categories": ["cs.HC", "cs.CL", "cs.LG", "math.OC"], "primary_category": "cs.HC"}
{"title": "Towards Zero-Shot & Explainable Video Description by Reasoning over\n  Graphs of Events in Space and Time", "abstract": "In the current era of Machine Learning, Transformers have become the de facto\napproach across a variety of domains, such as computer vision and natural\nlanguage processing. Transformer-based solutions are the backbone of current\nstate-of-the-art methods for language generation, image and video\nclassification, segmentation, action and object recognition, among many others.\nInterestingly enough, while these state-of-the-art methods produce impressive\nresults in their respective domains, the problem of understanding the\nrelationship between vision and language is still beyond our reach. In this\nwork, we propose a common ground between vision and language based on events in\nspace and time in an explainable and programmatic way, to connect\nlearning-based vision and language state of the art models and provide a\nsolution to the long standing problem of describing videos in natural language.\nWe validate that our algorithmic approach is able to generate coherent, rich\nand relevant textual descriptions on videos collected from a variety of\ndatasets, using both standard metrics (e.g. Bleu, ROUGE) and the modern\nLLM-as-a-Jury approach.", "published": "2025-01-14 22:09:06", "link": "http://arxiv.org/abs/2501.08460v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Selective Attention Merging for low resource tasks: A case study of\n  Child ASR", "abstract": "While Speech Foundation Models (SFMs) excel in various speech tasks, their\nperformance for low-resource tasks such as child Automatic Speech Recognition\n(ASR) is hampered by limited pretraining data. To address this, we explore\ndifferent model merging techniques to leverage knowledge from models trained on\nlarger, more diverse speech corpora. This paper also introduces Selective\nAttention (SA) Merge, a novel method that selectively merges task vectors from\nattention matrices to enhance SFM performance on low-resource tasks.\nExperiments on the MyST database show significant reductions in relative word\nerror rate of up to 14%, outperforming existing model merging and data\naugmentation techniques. By combining data augmentation techniques with SA\nMerge, we achieve a new state-of-the-art WER of 8.69 on the MyST database for\nthe Whisper-small model, highlighting the potential of SA Merge for improving\nlow-resource ASR.", "published": "2025-01-14 22:27:48", "link": "http://arxiv.org/abs/2501.08468v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Quantifying the Importance of Data Alignment in Downstream Model\n  Performance", "abstract": "Contrary to the conventional emphasis on dataset size, we explore the role of\ndata alignment -- an often overlooked aspect of data quality -- in training\ncapable Large Language Models (LLMs). To do so, we use the Task2Vec-based\nalignment coefficient, a quantitative measure of the similarity between two\ndatasets, to quantify the impact of alignment between training data and\nevaluation data on downstream performance. In particular, we conduct controlled\n\\textit{interventional} experiments for two settings: 1. the impact of\nincreased alignment coefficients between various pre-training (pt) against\nevaluation datasets, and 2. the impact of increased alignment coefficients\nbetween domain specific fine-tuning (ft) against domain specific evaluation.\nThe domain specific task we explore is Autoformalization -- the machine\ntranslation task between natural language and code for formal verification. In\nboth settings, we find a strong, predictable negative correlation between the\nalignment coefficient of a model's training and evaluation data and the model's\nloss/perplexity on the respective downstream task. These findings suggest a\nre-evaluation of LLM training approaches, demonstrating the relevance of data\nalignment compared to data quantity, especially in specialized downstream tasks\nsuch as Autoformalization.", "published": "2025-01-14 23:59:23", "link": "http://arxiv.org/abs/2501.08496v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PL"], "primary_category": "cs.CL"}
{"title": "Evaluating Computational Accuracy of Large Language Models in Numerical\n  Reasoning Tasks for Healthcare Applications", "abstract": "Large Language Models (LLMs) have emerged as transformative tools in the\nhealthcare sector, demonstrating remarkable capabilities in natural language\nunderstanding and generation. However, their proficiency in numerical\nreasoning, particularly in high-stakes domains like in clinical applications,\nremains underexplored. Numerical reasoning is critical in healthcare\napplications, influencing patient outcomes, treatment planning, and resource\nallocation. This study investigates the computational accuracy of LLMs in\nnumerical reasoning tasks within healthcare contexts. Using a curated dataset\nof 1,000 numerical problems, encompassing real-world scenarios such as dosage\ncalculations and lab result interpretations, the performance of a refined LLM\nbased on the GPT-3 architecture was evaluated. The methodology includes prompt\nengineering, integration of fact-checking pipelines, and application of\nregularization techniques to enhance model accuracy and generalization. Key\nmetrics such as precision, recall, and F1-score were utilized to assess the\nmodel's efficacy. The results indicate an overall accuracy of 84.10%, with\nimproved performance in straightforward numerical tasks and challenges in\nmulti-step reasoning. The integration of a fact-checking pipeline improved\naccuracy by 11%, underscoring the importance of validation mechanisms. This\nresearch highlights the potential of LLMs in healthcare numerical reasoning and\nidentifies avenues for further refinement to support critical decision-making\nin clinical environments. The findings aim to contribute to the development of\nreliable, interpretable, and contextually relevant AI tools for healthcare.", "published": "2025-01-14 04:29:43", "link": "http://arxiv.org/abs/2501.13936v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Developing Enhanced Conversational Agents for Social Virtual Worlds", "abstract": "In this paper, we present a methodology for the development of embodied\nconversational agents for social virtual worlds. The agents provide multimodal\ncommunication with their users in which speech interaction is included. Our\nproposal combines different techniques related to Artificial Intelligence,\nNatural Language Processing, Affective Computing, and User Modeling. Firstly,\nthe developed conversational agents. A statistical methodology has been\ndeveloped to model the system conversational behavior, which is learned from an\ninitial corpus and improved with the knowledge acquired from the successive\ninteractions. In addition, the selection of the next system response is adapted\nconsidering information stored into users profiles and also the emotional\ncontents detected in the users utterances. Our proposal has been evaluated with\nthe successful development of an embodied conversational agent which has been\nplaced in the Second Life social virtual world. The avatar includes the\ndifferent models and interacts with the users who inhabit the virtual world in\norder to provide academic information. The experimental results show that the\nagents conversational behavior adapts successfully to the specific\ncharacteristics of users interacting in such environments.", "published": "2025-01-14 11:15:16", "link": "http://arxiv.org/abs/2501.16341v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SEAL: Speaker Error Correction using Acoustic-conditioned Large Language\n  Models", "abstract": "Speaker Diarization (SD) is a crucial component of modern end-to-end ASR\npipelines. Traditional SD systems, which are typically audio-based and operate\nindependently of ASR, often introduce speaker errors, particularly during\nspeaker transitions and overlapping speech. Recently, language models including\nfine-tuned large language models (LLMs) have shown to be effective as a\nsecond-pass speaker error corrector by leveraging lexical context in the\ntranscribed output. In this work, we introduce a novel acoustic conditioning\napproach to provide more fine-grained information from the acoustic diarizer to\nthe LLM. We also show that a simpler constrained decoding strategy reduces LLM\nhallucinations, while avoiding complicated post-processing. Our approach\nsignificantly reduces the speaker error rates by 24-43% across Fisher,\nCallhome, and RT03-CTS datasets, compared to the first-pass Acoustic SD.", "published": "2025-01-14 20:24:12", "link": "http://arxiv.org/abs/2501.08421v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction\n  Following", "abstract": "Large language models excel at interpreting complex natural language\ninstructions, enabling them to perform a wide range of tasks. In the life\nsciences, single-cell RNA sequencing (scRNA-seq) data serves as the \"language\nof cellular biology\", capturing intricate gene expression patterns at the\nsingle-cell level. However, interacting with this \"language\" through\nconventional tools is often inefficient and unintuitive, posing challenges for\nresearchers. To address these limitations, we present InstructCell, a\nmulti-modal AI copilot that leverages natural language as a medium for more\ndirect and flexible single-cell analysis. We construct a comprehensive\nmulti-modal instruction dataset that pairs text-based instructions with\nscRNA-seq profiles from diverse tissues and species. Building on this, we\ndevelop a multi-modal cell language architecture capable of simultaneously\ninterpreting and processing both modalities. InstructCell empowers researchers\nto accomplish critical tasks-such as cell type annotation, conditional\npseudo-cell generation, and drug sensitivity prediction-using straightforward\nnatural language commands. Extensive evaluations demonstrate that InstructCell\nconsistently meets or exceeds the performance of existing single-cell\nfoundation models, while adapting to diverse experimental conditions. More\nimportantly, InstructCell provides an accessible and intuitive tool for\nexploring complex single-cell data, lowering technical barriers and enabling\ndeeper biological insights.", "published": "2025-01-14 15:12:19", "link": "http://arxiv.org/abs/2501.08187v2", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.HC", "cs.LG", "q-bio.CB"], "primary_category": "cs.CL"}
{"title": "Bridge-SR: Schr\u00f6dinger Bridge for Efficient SR", "abstract": "Speech super-resolution (SR), which generates a waveform at a higher sampling\nrate from its low-resolution version, is a long-standing critical task in\nspeech restoration. Previous works have explored speech SR in different data\nspaces, but these methods either require additional compression networks or\nexhibit limited synthesis quality and inference speed. Motivated by recent\nadvances in probabilistic generative models, we present Bridge-SR, a novel and\nefficient any-to-48kHz SR system in the speech waveform domain. Using tractable\nSchr\\\"odinger Bridge models, we leverage the observed low-resolution waveform\nas a prior, which is intrinsically informative for the high-resolution target.\nBy optimizing a lightweight network to learn the score functions from the prior\nto the target, we achieve efficient waveform SR through a data-to-data\ngeneration process that fully exploits the instructive content contained in the\nlow-resolution observation. Furthermore, we identify the importance of the\nnoise schedule, data scaling, and auxiliary loss functions, which further\nimprove the SR quality of bridge-based systems. The experiments conducted on\nthe benchmark dataset VCTK demonstrate the efficiency of our system: (1) in\nterms of sample quality, Bridge-SR outperforms several strong baseline methods\nunder different SR settings, using a lightweight network backbone (1.7M); (2)\nin terms of inference speed, our 4-step synthesis achieves better performance\nthan the 8-step conditional diffusion counterpart (LSD: 0.911 vs 0.927). Demo\nat https://bridge-sr.github.io.", "published": "2025-01-14 07:26:05", "link": "http://arxiv.org/abs/2501.07897v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Loudspeaker Beamforming to Enhance Speech Recognition Performance of\n  Voice Driven Applications", "abstract": "In this paper we propose a robust loudspeaker beamforming algorithm which is\nused to enhance the performance of voice driven applications in scenarios where\nthe loudspeakers introduce the majority of the noise, e.g. when music is\nplaying loudly. The loudspeaker beamformer modifies the loudspeaker playback\nsignals to create a low-acoustic-energy region around the device that\nimplements automatic speech recognition for a voice driven application (VDA).\nThe algorithm utilises a distortion measure based on human auditory perception\nto limit the distortion perceived by human listeners. Simulations and\nreal-world experiments show that the proposed loudspeaker beamformer improves\nthe speech recognition performance in all tested scenarios. Moreover, the\nalgorithm allows to further reduce the acoustic energy around the VDA device at\nthe expense of reduced objective audio quality at the listener's location.", "published": "2025-01-14 13:28:14", "link": "http://arxiv.org/abs/2501.08104v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Neural Speech Tracking in a Virtual Acoustic Environment: Audio-Visual\n  Benefit for Unscripted Continuous Speech", "abstract": "The audio visual benefit in speech perception, where congruent visual input\nenhances auditory processing, is well documented across age groups,\nparticularly in challenging listening conditions and among individuals with\nvarying hearing abilities. However, most studies rely on highly controlled\nlaboratory environments with scripted stimuli. Here, we examine the audio\nvisual benefit using unscripted, natural speech from untrained speakers within\na virtual acoustic environment. Using electroencephalography (EEG) and cortical\nspeech tracking, we assessed neural responses across audio visual, audio only,\nvisual only, and masked lip conditions to isolate the role of lip movements.\nAdditionally, we analysed individual differences in acoustic and visual\nfeatures of the speakers, including pitch, jitter, and lip openness, to explore\ntheir influence on the audio visual speech tracking benefit. Results showed a\nsignificant audio visual enhancement in speech tracking with background noise,\nwith the masked lip condition performing similarly to the audio-only condition,\nemphasizing the importance of lip movements in adverse listening situations.\nOur findings reveal the feasibility of cortical speech tracking with\nnaturalistic stimuli and underscore the impact of individual speaker\ncharacteristics on audio-visual integration in real world listening contexts.", "published": "2025-01-14 14:00:57", "link": "http://arxiv.org/abs/2501.08124v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Automatic Live Music Song Identification Using Multi-level Deep Sequence\n  Similarity Learning", "abstract": "This paper studies the novel problem of automatic live music song\nidentification, where the goal is, given a live recording of a song, to\nretrieve the corresponding studio version of the song from a music database. We\npropose a system based on similarity learning and a Siamese convolutional\nneural network-based model. The model uses cross-similarity matrices of\nmulti-level deep sequences to measure musical similarity between different\naudio tracks. A manually collected custom live music dataset is used to test\nthe performance of the system with live music. The results of the experiments\nshow that the system is able to identify 87.4% of the given live music queries.", "published": "2025-01-14 14:06:51", "link": "http://arxiv.org/abs/2501.08129v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CodecFake+: A Large-Scale Neural Audio Codec-Based Deepfake Speech\n  Dataset", "abstract": "With the rapid advancement of neural audio codecs, codec-based speech\ngeneration (CoSG) systems have become highly powerful. Unfortunately, CoSG also\nenables the creation of highly realistic deepfake speech, making it easier to\nmimic an individual's voice and spread misinformation. We refer to this\nemerging deepfake speech generated by CoSG systems as CodecFake. Detecting such\nCodecFake is an urgent challenge, yet most existing systems primarily focus on\ndetecting fake speech generated by traditional speech synthesis models. In this\npaper, we introduce CodecFake+, a large-scale dataset designed to advance\nCodecFake detection. To our knowledge, CodecFake+ is the largest dataset\nencompassing the most diverse range of codec architectures. The training set is\ngenerated through re-synthesis using 31 publicly available open-source codec\nmodels, while the evaluation set includes web-sourced data from 17 advanced\nCoSG models. We also propose a comprehensive taxonomy that categorizes codecs\nby their root components: vector quantizer, auxiliary objectives, and decoder\ntypes. Our proposed dataset and taxonomy enable detailed analysis at multiple\nlevels to discern the key factors for successful CodecFake detection. At the\nindividual codec level, we validate the effectiveness of using codec\nre-synthesized speech (CoRS) as training data for large-scale CodecFake\ndetection. At the taxonomy level, we show that detection performance is\nstrongest when the re-synthesis model incorporates disentanglement auxiliary\nobjectives or a frequency-domain decoder. Furthermore, from the perspective of\nusing all the CoRS training data, we show that our proposed taxonomy can be\nused to select better training data for improving detection performance.\nOverall, we envision that CodecFake+ will be a valuable resource for both\ngeneral and fine-grained exploration to develop better anti-spoofing models\nagainst CodecFake.", "published": "2025-01-14 16:26:14", "link": "http://arxiv.org/abs/2501.08238v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Gen-A: Generalizing Ambisonics Neural Encoding to Unseen Microphone\n  Arrays", "abstract": "Using deep neural networks (DNNs) for encoding of microphone array (MA)\nsignals to the Ambisonics spatial audio format can surpass certain limitations\nof established conventional methods, but existing DNN-based methods need to be\ntrained separately for each MA. This paper proposes a DNN-based method for\nAmbisonics encoding that can generalize to arbitrary MA geometries unseen\nduring training. The method takes as inputs the MA geometry and MA signals and\nuses a multi-level encoder consisting of separate paths for geometry and signal\ndata, where geometry features inform the signal encoder at each level. The\nmethod is validated in simulated anechoic and reverberant conditions with one\nand two sources. The results indicate improvement over conventional encoding\nacross the whole frequency range for dry scenes, while for reverberant scenes\nthe improvement is frequency-dependent.", "published": "2025-01-14 11:54:45", "link": "http://arxiv.org/abs/2501.08047v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio-Visual Deepfake Detection With Local Temporal Inconsistencies", "abstract": "This paper proposes an audio-visual deepfake detection approach that aims to\ncapture fine-grained temporal inconsistencies between audio and visual\nmodalities. To achieve this, both architectural and data synthesis strategies\nare introduced. From an architectural perspective, a temporal distance map,\ncoupled with an attention mechanism, is designed to capture these\ninconsistencies while minimizing the impact of irrelevant temporal\nsubsequences. Moreover, we explore novel pseudo-fake generation techniques to\nsynthesize local inconsistencies. Our approach is evaluated against\nstate-of-the-art methods using the DFDC and FakeAVCeleb datasets, demonstrating\nits effectiveness in detecting audio-visual deepfakes.", "published": "2025-01-14 14:15:10", "link": "http://arxiv.org/abs/2501.08137v4", "categories": ["cs.CV", "cs.CR", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
