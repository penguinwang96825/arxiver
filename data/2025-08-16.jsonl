{"title": "LLM-as-a-Judge for Privacy Evaluation? Exploring the Alignment of Human and LLM Perceptions of Privacy in Textual Data", "abstract": "Despite advances in the field of privacy-preserving Natural Language\nProcessing (NLP), a significant challenge remains the accurate evaluation of\nprivacy. As a potential solution, using LLMs as a privacy evaluator presents a\npromising approach $\\unicode{x2013}$ a strategy inspired by its success in\nother subfields of NLP. In particular, the so-called $\\textit{LLM-as-a-Judge}$\nparadigm has achieved impressive results on a variety of natural language\nevaluation tasks, demonstrating high agreement rates with human annotators.\nRecognizing that privacy is both subjective and difficult to define, we\ninvestigate whether LLM-as-a-Judge can also be leveraged to evaluate the\nprivacy sensitivity of textual data. Furthermore, we measure how closely LLM\nevaluations align with human perceptions of privacy in text. Resulting from a\nstudy involving 10 datasets, 13 LLMs, and 677 human survey participants, we\nconfirm that privacy is indeed a difficult concept to measure empirically,\nexhibited by generally low inter-human agreement rates. Nevertheless, we find\nthat LLMs can accurately model a global human privacy perspective, and through\nan analysis of human and LLM reasoning patterns, we discuss the merits and\nlimitations of LLM-as-a-Judge for privacy evaluation in textual data. Our\nfindings pave the way for exploring the feasibility of LLMs as privacy\nevaluators, addressing a core challenge in solving pressing privacy issues with\ninnovative technical solutions.", "published": "2025-08-16 20:49:41", "link": "http://arxiv.org/abs/2508.12158v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Efficiency Frontiers of Thinking Budget in Medical Reasoning: Scaling Laws between Computational Resources and Reasoning Quality", "abstract": "This study presents the first comprehensive evaluation of thinking budget\nmechanisms in medical reasoning tasks, revealing fundamental scaling laws\nbetween computational resources and reasoning quality. We systematically\nevaluated two major model families, Qwen3 (1.7B to 235B parameters) and\nDeepSeek-R1 (1.5B to 70B parameters), across 15 medical datasets spanning\ndiverse specialties and difficulty levels. Through controlled experiments with\nthinking budgets ranging from zero to unlimited tokens, we establish\nlogarithmic scaling relationships where accuracy improvements follow a\npredictable pattern with both thinking budget and model size. Our findings\nidentify three distinct efficiency regimes: high-efficiency (0 to 256 tokens)\nsuitable for real-time applications, balanced (256 to 512 tokens) offering\noptimal cost-performance tradeoffs for routine clinical support, and\nhigh-accuracy (above 512 tokens) justified only for critical diagnostic tasks.\nNotably, smaller models demonstrate disproportionately larger benefits from\nextended thinking, with 15 to 20% improvements compared to 5 to 10% for larger\nmodels, suggesting a complementary relationship where thinking budget provides\ngreater relative benefits for capacity-constrained models. Domain-specific\npatterns emerge clearly, with neurology and gastroenterology requiring\nsignificantly deeper reasoning processes than cardiovascular or respiratory\nmedicine. The consistency between Qwen3 native thinking budget API and our\nproposed truncation method for DeepSeek-R1 validates the generalizability of\nthinking budget concepts across architectures. These results establish thinking\nbudget control as a critical mechanism for optimizing medical AI systems,\nenabling dynamic resource allocation aligned with clinical needs while\nmaintaining the transparency essential for healthcare deployment.", "published": "2025-08-16 19:25:06", "link": "http://arxiv.org/abs/2508.12140v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DynamixSFT: Dynamic Mixture Optimization of Instruction Tuning Collections", "abstract": "As numerous instruction-tuning datasets continue to emerge during the\npost-training stage, dynamically balancing and optimizing their mixtures has\nbecome a critical challenge. To address this, we propose DynamixSFT, a dynamic\nand automated method for instruction-tuning dataset mixture optimization. We\nformulate the problem as a multi-armed bandit setup and introduce a\nPrior-scaled Boltzmann Exploration that softly anchors the updated sampling\ndistribution to the original dataset proportions, thereby preserving the\ninherent diversity and coverage of the collection. Sampling probabilities are\nupdated using a lightweight 1-Step Look-ahead Reward, reflecting how much the\ndataset contributes to improving the model's performance at its current state.\nWhen applied to the Tulu-v2-mixture collection comprising 16 instruction-tuning\ndatasets, DynamixSFT achieves up to a 2.2% performance improvement across 10\nbenchmarks. Furthermore, we provide a comprehensive analysis and visualizations\nto offer deeper insights into the adaptive dynamics of our method.", "published": "2025-08-16 18:01:39", "link": "http://arxiv.org/abs/2508.12116v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Generative Medical Event Models Improve with Scale", "abstract": "Realizing personalized medicine at scale calls for methods that distill\ninsights from longitudinal patient journeys, which can be viewed as a sequence\nof medical events. Foundation models pretrained on large-scale medical event\ndata represent a promising direction for scaling real-world evidence generation\nand generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset with\nmedical events from de-identified longitudinal health records for 16.3 billion\nencounters over 300 million unique patient records from 310 health systems, we\nintroduce the Cosmos Medical Event Transformer ( CoMET) models, a family of\ndecoder-only transformer models pretrained on 118 million patients representing\n115 billion discrete medical events (151 billion tokens). We present the\nlargest scaling-law study for medical event data, establishing a methodology\nfor pretraining and revealing power-law scaling relationships for compute,\ntokens, and model size. Based on this, we pretrained a series of\ncompute-optimal models with up to 1 billion parameters. Conditioned on a\npatient's real-world history, CoMET autoregressively generates the next medical\nevent, simulating patient health timelines. We studied 78 real-world tasks,\nincluding diagnosis prediction, disease prognosis, and healthcare operations.\nRemarkably for a foundation model with generic pretraining and simulation-based\ninference, CoMET generally outperformed or matched task-specific supervised\nmodels on these tasks, without requiring task-specific fine-tuning or few-shot\nexamples. CoMET's predictive power consistently improves as the model and\npretraining scale. Our results show that CoMET, a generative medical event\nfoundation model, can effectively capture complex clinical dynamics, providing\nan extensible and generalizable framework to support clinical decision-making,\nstreamline healthcare operations, and improve patient outcomes.", "published": "2025-08-16 17:00:51", "link": "http://arxiv.org/abs/2508.12104v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "STEM: Efficient Relative Capability Evaluation of LLMs through Structured Transition Samples", "abstract": "Evaluating large language models (LLMs) has become increasingly challenging\nas model capabilities advance rapidly. While recent models often achieve higher\nscores on standard benchmarks, these improvements do not consistently reflect\nenhanced real-world reasoning capabilities. Moreover, widespread overfitting to\npublic benchmarks and the high computational cost of full evaluations have made\nit both expensive and less effective to distinguish meaningful differences\nbetween models. To address these challenges, we propose the \\textbf{S}tructured\n\\textbf{T}ransition \\textbf{E}valuation \\textbf{M}ethod (STEM), a lightweight\nand interpretable evaluation framework for efficiently estimating the relative\ncapabilities of LLMs. STEM identifies \\textit{significant transition samples}\n(STS) by analyzing consistent performance transitions among LLMs of the same\narchitecture but varying parameter scales. These samples enable STEM to\neffectively estimate the capability position of an unknown model. Qwen3 model\nfamily is applied to construct the STS pool on six diverse and representative\nbenchmarks. To assess generalizability. Experimental results indicate that STEM\nreliably captures performance trends, aligns with ground-truth rankings of\nmodel capability. These findings highlight STEM as a practical and scalable\nmethod for fine-grained, architecture-agnostic evaluation of LLMs.", "published": "2025-08-16 16:36:43", "link": "http://arxiv.org/abs/2508.12096v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "J6: Jacobian-Driven Role Attribution for Multi-Objective Prompt Optimization in LLMs", "abstract": "In large language model (LLM) adaptation, balancing multiple optimization\nobjectives such as improving factuality (heat) and increasing confidence (via\nlow entropy) poses a fundamental challenge, especially when prompt parameters\n(e.g., hidden-layer insertions h and embedding modifications w) interact in\nnon-trivial ways. Existing multi-objective optimization strategies often rely\non scalar gradient aggregation, ignoring the deeper geometric structure between\nobjectives and parameters. We propose J6, a structured Jacobian-based method\nthat decomposes the gradient interaction matrix into six interpretable\ncomponents. This decomposition enables both hard decision-making (e.g.,\nchoosing the dominant update direction via argmax) and soft strategies (e.g.,\nattention-style weighting via softmax over J6), forming a dynamic update\nframework that adapts to local conflict and synergy. Moreover, the\ninterpretable structure of J6 provides insight into parameter attribution, task\ninterference, and geometry-aligned adaptation. Our work introduces a principled\nand extensible mechanism for conflict-aware prompt optimization, and opens a\nnew avenue for incorporating structured Jacobian reasoning into multi-objective\nneural tuning.", "published": "2025-08-16 15:47:47", "link": "http://arxiv.org/abs/2508.12086v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50, 90C29, 62F07", "I.2.7; I.2.6; G.1.6"], "primary_category": "cs.CL"}
{"title": "VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models", "abstract": "This paper introduces VimoRAG, a novel video-based retrieval-augmented motion\ngeneration framework for motion large language models (LLMs). As motion LLMs\nface severe out-of-domain/out-of-vocabulary issues due to limited annotated\ndata, VimoRAG leverages large-scale in-the-wild video databases to enhance 3D\nmotion generation by retrieving relevant 2D human motion signals. While\nvideo-based motion RAG is nontrivial, we address two key bottlenecks: (1)\ndeveloping an effective motion-centered video retrieval model that\ndistinguishes human poses and actions, and (2) mitigating the issue of error\npropagation caused by suboptimal retrieval results. We design the Gemini Motion\nVideo Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer,\nenabling effective retrieval and generation processes. Experimental results\nshow that VimoRAG significantly boosts the performance of motion LLMs\nconstrained to text-only input.", "published": "2025-08-16 15:31:14", "link": "http://arxiv.org/abs/2508.12081v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Mitigating Jailbreaks with Intent-Aware LLMs", "abstract": "Despite extensive safety-tuning, large language models (LLMs) remain\nvulnerable to jailbreak attacks via adversarially crafted instructions,\nreflecting a persistent trade-off between safety and task performance. In this\nwork, we propose Intent-FT, a simple and lightweight fine-tuning approach that\nexplicitly trains LLMs to infer the underlying intent of an instruction before\nresponding. By fine-tuning on a targeted set of adversarial instructions,\nIntent-FT enables LLMs to generalize intent deduction to unseen attacks,\nthereby substantially improving their robustness. We comprehensively evaluate\nboth parametric and non-parametric attacks across open-source and proprietary\nmodels, considering harmfulness from attacks, utility, over-refusal, and impact\nagainst white-box threats. Empirically, Intent-FT consistently mitigates all\nevaluated attack categories, with no single attack exceeding a 50\\% success\nrate -- whereas existing defenses remain only partially effective. Importantly,\nour method preserves the model's general capabilities and reduces excessive\nrefusals on benign instructions containing superficially harmful keywords.\nFurthermore, models trained with Intent-FT accurately identify hidden harmful\nintent in adversarial attacks, and these learned intentions can be effectively\ntransferred to enhance vanilla model defenses.", "published": "2025-08-16 15:03:33", "link": "http://arxiv.org/abs/2508.12072v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation", "abstract": "While large language models (LLMs) have demonstrated remarkable performance\nacross diverse tasks, they fundamentally lack self-awareness and frequently\nexhibit overconfidence, assigning high confidence scores to incorrect\npredictions. Accurate confidence estimation is therefore critical for enhancing\nthe trustworthiness and reliability of LLM-generated outputs. However, existing\napproaches suffer from coarse-grained scoring mechanisms that fail to provide\nfine-grained, continuous confidence estimates throughout the generation\nprocess. To address these limitations, we introduce FineCE, a novel confidence\nestimation method that delivers accurate, fine-grained confidence scores during\ntext generation. Specifically, we first develop a comprehensive pipeline for\nconstructing training data that effectively captures the underlying\nprobabilistic distribution of LLM responses, and then train a model to predict\nconfidence scores for arbitrary text sequences in a supervised manner.\nFurthermore, we propose a Backward Confidence Integration (BCI) strategy that\nleverages information from the subsequent text to enhance confidence estimation\nfor the current sequence during inference. We also introduce three strategies\nfor identifying optimal positions to perform confidence estimation within the\ngeneration process. Extensive experiments on multiple benchmark datasets\ndemonstrate that FineCE consistently outperforms existing classical confidence\nestimation methods. Our code and all baselines used in the paper are available\non GitHub.", "published": "2025-08-16 13:29:35", "link": "http://arxiv.org/abs/2508.12040v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Wisdom from Errors: Promoting LLM's Continual Relation Learning through Exploiting Error Cases", "abstract": "Continual Relation Extraction (CRE) aims to continually learn new emerging\nrelations while avoiding catastrophic forgetting. Existing CRE methods mainly\nuse memory replay and contrastive learning to mitigate catastrophic forgetting.\nHowever, these methods do not attach importance to the error cases that can\nreveal the model's cognitive biases more effectively. To address this issue, we\npropose an instruction-based continual contrastive tuning approach for Large\nLanguage Models (LLMs) in CRE. Different from existing CRE methods that\ntypically handle the training and memory data in a unified manner, this\napproach splits the training and memory data of each task into two parts\nrespectively based on the correctness of the initial responses and treats them\ndifferently through dual-task fine-tuning. In addition, leveraging the\nadvantages of LLM's instruction-following ability, we propose a novel\ninstruction-based contrastive tuning strategy for LLM to continuously correct\ncurrent cognitive biases with the guidance of previous data in an\ninstruction-tuning manner, which mitigates the gap between old and new\nrelations in a more suitable way for LLMs. We experimentally evaluate our model\non TACRED and FewRel, and the results show that our model achieves new\nstate-of-the-art CRE performance with significant improvements, demonstrating\nthe importance of specializing in exploiting error cases.", "published": "2025-08-16 12:49:11", "link": "http://arxiv.org/abs/2508.12031v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CHBench: A Cognitive Hierarchy Benchmark for Evaluating Strategic Reasoning Capability of LLMs", "abstract": "Game-playing ability serves as an indicator for evaluating the strategic\nreasoning capability of large language models (LLMs). While most existing\nstudies rely on utility performance metrics, which are not robust enough due to\nvariations in opponent behavior and game structure. To address this limitation,\nwe propose \\textbf{Cognitive Hierarchy Benchmark (CHBench)}, a novel evaluation\nframework inspired by the cognitive hierarchy models from behavioral economics.\nWe hypothesize that agents have bounded rationality -- different agents behave\nat varying reasoning depths/levels. We evaluate LLMs' strategic reasoning\nthrough a three-phase systematic framework, utilizing behavioral data from six\nstate-of-the-art LLMs across fifteen carefully selected normal-form games.\nExperiments show that LLMs exhibit consistent strategic reasoning levels across\ndiverse opponents, confirming the framework's robustness and generalization\ncapability. We also analyze the effects of two key mechanisms (Chat Mechanism\nand Memory Mechanism) on strategic reasoning performance. Results indicate that\nthe Chat Mechanism significantly degrades strategic reasoning, whereas the\nMemory Mechanism enhances it. These insights position CHBench as a promising\ntool for evaluating LLM capabilities, with significant potential for future\nresearch and practical applications.", "published": "2025-08-16 07:10:26", "link": "http://arxiv.org/abs/2508.11944v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "CAMF: Collaborative Adversarial Multi-agent Framework for Machine Generated Text Detection", "abstract": "Detecting machine-generated text (MGT) from contemporary Large Language\nModels (LLMs) is increasingly crucial amid risks like disinformation and\nthreats to academic integrity. Existing zero-shot detection paradigms, despite\ntheir practicality, often exhibit significant deficiencies. Key challenges\ninclude: (1) superficial analyses focused on limited textual attributes, and\n(2) a lack of investigation into consistency across linguistic dimensions such\nas style, semantics, and logic. To address these challenges, we introduce the\n\\textbf{C}ollaborative \\textbf{A}dversarial \\textbf{M}ulti-agent\n\\textbf{F}ramework (\\textbf{CAMF}), a novel architecture using multiple\nLLM-based agents. CAMF employs specialized agents in a synergistic three-phase\nprocess: \\emph{Multi-dimensional Linguistic Feature Extraction},\n\\emph{Adversarial Consistency Probing}, and \\emph{Synthesized Judgment\nAggregation}. This structured collaborative-adversarial process enables a deep\nanalysis of subtle, cross-dimensional textual incongruities indicative of\nnon-human origin. Empirical evaluations demonstrate CAMF's significant\nsuperiority over state-of-the-art zero-shot MGT detection techniques.", "published": "2025-08-16 06:25:27", "link": "http://arxiv.org/abs/2508.11933v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMs Struggle with NLI for Perfect Aspect: A Cross-Linguistic Study in Chinese and Japanese", "abstract": "Unlike English, which uses distinct forms (e.g., had, has, will have) to mark\nthe perfect aspect across tenses, Chinese and Japanese lack separate\ngrammatical forms for tense within the perfect aspect, which complicates\nNatural Language Inference (NLI). Focusing on the perfect aspect in these\nlanguages, we construct a linguistically motivated, template-based NLI dataset\n(1,350 pairs per language). Experiments reveal that even advanced LLMs struggle\nwith temporal inference, particularly in detecting subtle tense and\nreference-time shifts. These findings highlight model limitations and\nunderscore the need for cross-linguistic evaluation in temporal semantics. Our\ndataset is available at https://github.com/Lujie2001/CrossNLI.", "published": "2025-08-16 06:16:56", "link": "http://arxiv.org/abs/2508.11927v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Optimizing Token Choice for Code Watermarking: A RL Approach", "abstract": "The need for detecting LLM-generated code necessitates watermarking systems\ncapable of operating within its highly structured and syntactically constrained\nenvironment. To address this, we introduce CodeTracer, an innovative adaptive\ncode watermarking framework underpinned by a novel reinforcement learning\ntraining paradigm. At its core, CodeTracer features a policy-driven approach\nthat utilizes a parameterized model to intelligently bias token choices during\nnext-token prediction. This strategy ensures that embedded watermarks maintain\ncode functionality while exhibiting subtle yet statistically detectable\ndeviations from typical token distributions. To facilitate policy learning, we\ndevise a comprehensive reward system that seamlessly integrates execution\nfeedback with watermark embedding signals, balancing process-level and\noutcome-level rewards. Additionally, we employ Gumbel Top-k reparameterization\nto enable gradient-based optimization of discrete watermarking decisions.\nExtensive comparative evaluations demonstrate CodeTracer's significant\nsuperiority over state-of-the-art baselines in both watermark detectability and\nthe preservation of generated code's functionality.", "published": "2025-08-16 06:11:29", "link": "http://arxiv.org/abs/2508.11925v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "CORE: Measuring Multi-Agent LLM Interaction Quality under Game-Theoretic Pressures", "abstract": "Game-theoretic interactions between agents with Large Language Models (LLMs)\nhave revealed many emergent capabilities, yet the linguistic diversity of these\ninteractions has not been sufficiently quantified. In this paper, we present\nthe Conversational Robustness Evaluation Score: CORE, a metric to quantify the\neffectiveness of language use within multi-agent systems across different\ngame-theoretic interactions. CORE integrates measures of cluster entropy,\nlexical repetition, and semantic similarity, providing a direct lens of dialog\nquality. We apply CORE to pairwise LLM dialogs across competitive, cooperative,\nand neutral settings, further grounding our analysis in Zipf's and Heaps' Laws\nto characterize word frequency distributions and vocabulary growth. Our\nfindings show that cooperative settings exhibit both steeper Zipf distributions\nand higher Heap exponents, indicating more repetition alongside greater\nvocabulary expansion. In contrast, competitive interactions display lower Zipf\nand Heaps exponents, reflecting less repetition and more constrained\nvocabularies. These results provide new insights into how social incentives\ninfluence language adaptation, and highlight CORE as a robust diagnostic for\nmeasuring linguistic robustness in multi-agent LLM systems. Our code is\navailable at https://github.com/psyonp/core.", "published": "2025-08-16 05:26:36", "link": "http://arxiv.org/abs/2508.11915v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "In-Context Examples Matter: Improving Emotion Recognition in Conversation with Instruction Tuning", "abstract": "Emotion recognition in conversation (ERC) aims to identify the emotion of\neach utterance in a conversation, playing a vital role in empathetic artificial\nintelligence. With the growing of large language models (LLMs), instruction\ntuning has emerged as a critical paradigm for ERC. Existing studies mainly\nfocus on multi-stage instruction tuning, which first endows LLMs with speaker\ncharacteristics, and then conducts context-aware instruction tuning to\ncomprehend emotional states. However, these methods inherently constrains the\ncapacity to jointly capture the dynamic interaction between speaker\ncharacteristics and conversational context, resulting in weak alignment among\nspeaker identity, contextual cues, and emotion states within a unified\nframework. In this paper, we propose InitERC, a simple yet effective one-stage\nin-context instruction tuning framework for ERC. InitERC adapts LLMs to learn\nspeaker-context-emotion alignment from context examples via in-context\ninstruction tuning. Specifically, InitERC comprises four components, i.e.,\ndemonstration pool construction, in-context example selection, prompt template\ndesign, and in-context instruction tuning. To explore the impact of in-context\nexamples, we conduct a comprehensive study on three key factors: retrieval\nstrategy, example ordering, and the number of examples. Extensive experiments\non three widely used datasets demonstrate that our proposed InitERC achieves\nsubstantial improvements over the state-of-the-art baselines.", "published": "2025-08-16 03:23:48", "link": "http://arxiv.org/abs/2508.11889v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EVTP-IVS: Effective Visual Token Pruning For Unifying Instruction Visual Segmentation In Multi-Modal Large Language Models", "abstract": "Instructed Visual Segmentation (IVS) tasks require segmenting objects in\nimages or videos based on natural language instructions. While recent\nmultimodal large language models (MLLMs) have achieved strong performance on\nIVS, their inference cost remains a major bottleneck, particularly in video. We\nempirically analyze visual token sampling in MLLMs and observe a strong\ncorrelation between subset token coverage and segmentation performance. This\nmotivates our design of a simple and effective token pruning method that\nselects a compact yet spatially representative subset of tokens to accelerate\ninference. In this paper, we introduce a novel visual token pruning method for\nIVS, called EVTP-IV, which builds upon the k-center by integrating spatial\ninformation to ensure better coverage. We further provide an\ninformation-theoretic analysis to support our design. Experiments on standard\nIVS benchmarks show that our method achieves up to 5X speed-up on video tasks\nand 3.5X on image tasks, while maintaining comparable accuracy using only 20%\nof the tokens. Our method also consistently outperforms state-of-the-art\npruning baselines under varying pruning ratios.", "published": "2025-08-16 03:16:33", "link": "http://arxiv.org/abs/2508.11886v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "eess.IV"], "primary_category": "cs.CV"}
{"title": "LARC: Towards Human-level Constrained Retrosynthesis Planning through an Agentic Framework", "abstract": "Large language model (LLM) agent evaluators leverage specialized tools to\nground the rational decision-making of LLMs, making them well-suited to aid in\nscientific discoveries, such as constrained retrosynthesis planning.\nConstrained retrosynthesis planning is an essential, yet challenging, process\nwithin chemistry for identifying synthetic routes from commercially available\nstarting materials to desired target molecules, subject to practical\nconstraints. Here, we present LARC, the first LLM-based Agentic framework for\nRetrosynthesis planning under Constraints. LARC incorporates agentic constraint\nevaluation, through an Agent-as-a-Judge, directly into the retrosynthesis\nplanning process, using agentic feedback grounded in tool-based reasoning to\nguide and constrain route generation. We rigorously evaluate LARC on a\ncarefully curated set of 48 constrained retrosynthesis planning tasks across 3\nconstraint types. LARC achieves a 72.9% success rate on these tasks, vastly\noutperforming LLM baselines and approaching human expert-level success in\nsubstantially less time. The LARC framework is extensible, and serves as a\nfirst step towards an effective agentic tool or a co-scientist to human experts\nfor constrained retrosynthesis.", "published": "2025-08-16 01:05:26", "link": "http://arxiv.org/abs/2508.11860v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "SupraTok: Cross-Boundary Tokenization for Enhanced Language Model Performance", "abstract": "Tokenization remains a fundamental yet underexplored bottleneck in natural\nlanguage processing, with strategies largely static despite remarkable progress\nin model architectures. We present SupraTok, a novel tokenization architecture\nthat reimagines subword segmentation through three innovations: cross-boundary\npattern learning that discovers multi-word semantic units, entropy-driven data\ncuration that optimizes training corpus quality, and multi-phase curriculum\nlearning for stable convergence. Our approach extends Byte-Pair Encoding by\nlearning \"superword\" tokens, coherent multi-word expressions that preserve\nsemantic unity while maximizing compression efficiency. SupraTok achieves 31%\nimprovement in English tokenization efficiency (5.91 versus 4.51 characters per\ntoken) compared to OpenAI's o200k tokenizer and 30% improvement over Google's\nGemma 3 tokenizer (256k vocabulary), while maintaining competitive performance\nacross 38 languages. When integrated with a GPT-2 scale model (124M parameters)\ntrained on 10 billion tokens from the FineWeb-Edu dataset, SupraTok yields 8.4%\nimprovement on HellaSWAG and 9.5% on MMLU benchmarks without architectural\nmodifications. While these results are promising at this scale, further\nvalidation at larger model scales is needed. These findings suggest that\nefficient tokenization can complement architectural innovations as a path to\nimproved language model performance.", "published": "2025-08-16 00:54:20", "link": "http://arxiv.org/abs/2508.11857v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Parameterized Perspective on Uniquely Restricted Matchings", "abstract": "Given a graph G, a matching is a subset of edges of G that do not share an\nendpoint. A matching M is uniquely restricted if the subgraph induced by the\nendpoints of the edges of M has exactly one perfect matching. Given a graph G\nand a positive integer \\ell, Uniquely Restricted Matching asks whether G has a\nuniquely restricted matching of size at least \\ell. In this paper, we study the\nparameterized complexity of Uniquely Restricted Matching under various\nparameters. Specifically, we show that Uniquely Restricted Matching admits a\nfixed-parameter tractable (FPT) algorithm on line graphs when parameterized by\nthe solution size. We also establish that the problem is FPT when parameterized\nby the treewidth of the input graph. Furthermore, we show that Uniquely\nRestricted Matching does not admit a polynomial kernel with respect to the\nvertex cover number plus the size of the matching unless NP \\subseteq\ncoNP/poly.", "published": "2025-08-16 10:11:46", "link": "http://arxiv.org/abs/2508.12004v1", "categories": ["cs.DS", "cs.DM"], "primary_category": "cs.DS"}
{"title": "Queen Domination by SAT Solving", "abstract": "The queen domination problem asks for the minimum number of queens needed to\nattack all squares on an $n\\times n$ chessboard. Once this optimal number is\nknown, determining the number of distinct solutions up to isomorphism has also\nattracted considerable attention. Previous work has introduced specialized and\nhighly optimized search procedures to address open instances of the problem.\nWhile efficient in terms of runtime, these approaches have not provided proofs\nthat can be independently verified by third-party checkers. In contrast, this\npaper aims to combine efficiency with verifiability. We reduce the problem to a\npropositional satisfiability problem (SAT) using a straightforward encoding,\nand solve the resulting formulas with modern SAT solvers capable of generating\nproof certificates. By improving the SAT encoding with a novel literal ordering\nstrategy, and leveraging established techniques such as static symmetry\nbreaking and the Cube-and-Conquer paradigm, this paper achieves both\nperformance and trustworthiness. Our approach discovers and corrects a\ndiscrepancy in previous results for $n=16$ and resolves the previously open\ncase $n=19$.", "published": "2025-08-16 07:11:57", "link": "http://arxiv.org/abs/2508.11945v1", "categories": ["cs.LO", "cs.DM", "math.CO"], "primary_category": "cs.LO"}
{"title": "MOON: Generative MLLM-based Multimodal Representation Learning for E-commerce Product Understanding", "abstract": "With the rapid advancement of e-commerce, exploring general representations\nrather than task-specific ones has attracted increasing research attention. For\nproduct understanding, although existing discriminative dual-flow architectures\ndrive progress in this field, they inherently struggle to model the many-to-one\nalignment between multiple images and texts of products. Therefore, we argue\nthat generative Multimodal Large Language Models (MLLMs) hold significant\npotential for improving product representation learning. Nevertheless,\nachieving this goal still remains non-trivial due to several key challenges:\nthe lack of multimodal and aspect-aware modeling modules in typical LLMs; the\ncommon presence of background noise in product images; and the absence of a\nstandard benchmark for evaluation. To address these issues, we propose the\nfirst generative MLLM-based model named MOON for product representation\nlearning. Our method (1) employs a guided Mixture-of-Experts (MoE) module for\ntargeted modeling of multimodal and aspect-specific product content; (2)\neffectively detects core semantic regions in product images to mitigate the\ndistraction and interference caused by background noise; and (3) introduces the\nspecialized negative sampling strategy to increase the difficulty and diversity\nof negative samples. In addition, we release a large-scale multimodal benchmark\nMBE for various product understanding tasks. Experimentally, our model\ndemonstrates competitive zero-shot performance on both our benchmark and the\npublic dataset, showcasing strong generalization across various downstream\ntasks, including cross-modal retrieval, product classification, and attribute\nprediction. Furthermore, the case study and visualization illustrate the\neffectiveness of MOON for product understanding.", "published": "2025-08-16 09:59:25", "link": "http://arxiv.org/abs/2508.11999v1", "categories": ["cs.CV", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Leveraging Geometric Insights in Hyperbolic Triplet Loss for Improved Recommendations", "abstract": "Recent studies have demonstrated the potential of hyperbolic geometry for\ncapturing complex patterns from interaction data in recommender systems. In\nthis work, we introduce a novel hyperbolic recommendation model that uses\ngeometrical insights to improve representation learning and increase\ncomputational stability at the same time. We reformulate the notion of\nhyperbolic distances to unlock additional representation capacity over\nconventional Euclidean space and learn more expressive user and item\nrepresentations. To better capture user-items interactions, we construct a\ntriplet loss that models ternary relations between users and their\ncorresponding preferred and nonpreferred choices through a mix of pairwise\ninteraction terms driven by the geometry of data. Our hyperbolic approach not\nonly outperforms existing Euclidean and hyperbolic models but also reduces\npopularity bias, leading to more diverse and personalized recommendations.", "published": "2025-08-16 08:34:17", "link": "http://arxiv.org/abs/2508.11978v1", "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR"}
{"title": "TBGRecall: A Generative Retrieval Model for E-commerce Recommendation Scenarios", "abstract": "Recommendation systems are essential tools in modern e-commerce, facilitating\npersonalized user experiences by suggesting relevant products. Recent\nadvancements in generative models have demonstrated potential in enhancing\nrecommendation systems; however, these models often exhibit limitations in\noptimizing retrieval tasks, primarily due to their reliance on autoregressive\ngeneration mechanisms. Conventional approaches introduce sequential\ndependencies that impede efficient retrieval, as they are inherently unsuitable\nfor generating multiple items without positional constraints within a single\nrequest session. To address these limitations, we propose TBGRecall, a\nframework integrating Next Session Prediction (NSP), designed to enhance\ngenerative retrieval models for e-commerce applications. Our framework\nreformulation involves partitioning input samples into multi-session sequences,\nwhere each sequence comprises a session token followed by a set of item tokens,\nand then further incorporate multiple optimizations tailored to the generative\ntask in retrieval scenarios. In terms of training methodology, our pipeline\nintegrates limited historical data pre-training with stochastic partial\nincremental training, significantly improving training efficiency and\nemphasizing the superiority of data recency over sheer data volume. Our\nextensive experiments, conducted on public benchmarks alongside a large-scale\nindustrial dataset from TaoBao, show TBGRecall outperforms the state-of-the-art\nrecommendation methods, and exhibits a clear scaling law trend. Ultimately, NSP\nrepresents a significant advancement in the effectiveness of generative\nrecommendation systems for e-commerce applications.", "published": "2025-08-16 08:31:11", "link": "http://arxiv.org/abs/2508.11977v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "A Law of Emergence: Maximum Causal Power at the Mesoscale", "abstract": "Complex systems universally exhibit emergence, where macroscopic dynamics\narise from local interactions, but a predictive law governing this process has\nbeen absent. We establish and verify such a law. We define a system's causal\npower at a spatial scale, $\\ell$, as its Effective Information (EI$_\\ell$),\nmeasured by the mutual information between a targeted, maximum-entropy\nintervention and its outcome. From this, we derive and prove a Middle-Scale\nPeak Theorem: for a broad class of systems with local interactions, EI$_\\ell$\nis not monotonic but exhibits a strict maximum at a mesoscopic scale $\\ell^*$.\nThis peak is a necessary consequence of a fundamental trade-off between\nnoise-averaging at small scales and locality-limited response at large scales.\nWe provide quantitative, reproducible evidence for this law in two distinct\ndomains: a 2D Ising model near criticality and a model of agent-based\ncollective behavior. In both systems, the predicted unimodal peak is decisively\nconfirmed by statistical model selection. Our work establishes a falsifiable,\nfirst-principles law that identifies the natural scale of emergence, providing\na quantitative foundation for the discovery of effective theories.", "published": "2025-08-16 11:33:37", "link": "http://arxiv.org/abs/2508.12016v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Higher and extended Jacobi polynomials for codes", "abstract": "In this paper, we introduce Jacobi polynomial generalizations of several\nclassical invariants in coding theory over finite fields, specifically, the\nhigher and extended weight enumerators, and we establish explicit\ncorrespondences between the resulting Jacobi polynomials. Moreover, we present\nthe Jacobi analogue of MacWilliams identity for both higher and extended weight\nenumerators. We also present that the higher Jacobi polynomials for linear\ncodes whose subcode supports form $t$-designs can be uniquely determined from\nthe higher weight enumerators of the codes via polarization technique. Finally,\nwe demonstrate how higher Jacobi polynomials can be computed from harmonic\nhigher weight enumerators with the help of Hahn polynomials.", "published": "2025-08-16 04:52:06", "link": "http://arxiv.org/abs/2508.11909v1", "categories": ["math.CO", "cs.IT", "math.IT", "math.NT", "Primary 11T71, Secondary 94B05, 11F11"], "primary_category": "math.CO"}
{"title": "On Balancing Sparsity with Reliable Connectivity in Distributed Network Design with Random K-out Graphs", "abstract": "In several applications in distributed systems, an important design criterion\nis ensuring that the network is sparse, i.e., does not contain too many edges,\nwhile achieving reliable connectivity. Sparsity ensures communication overhead\nremains low, while reliable connectivity is tied to reliable communication and\ninference on decentralized data reservoirs and computational resources. A class\nof network models called random K-out graphs appear widely as a heuristic to\nbalance connectivity and sparsity, especially in settings with limited trust,\ne.g., privacy-preserving aggregation of networked data in which networks are\ndeployed. However, several questions remain regarding how to choose network\nparameters in response to different operational requirements, including the\nneed to go beyond asymptotic results and the ability to model the stochastic\nand adversarial environments. To address this gap, we present theorems to\ninform the choice of network parameters that guarantee reliable connectivity in\nregimes where nodes can be finite or unreliable. We first derive upper and\nlower bounds for probability of connectivity in random K-out graphs when the\nnumber of nodes is finite. Next, we analyze the property of r-robustness, a\nstronger notion than connectivity that enables resilient consensus in the\npresence of malicious nodes. Finally, motivated by aggregation mechanisms based\non pairwise masking, we model and analyze the impact of a subset of adversarial\nnodes, modeled as deletions, on connectivity and giant component size - metrics\nthat are closely tied to privacy guarantees. Together, our results pave the way\nfor end-to-end performance guarantees for a suite of algorithms for reliable\ninference on networks.", "published": "2025-08-16 01:29:16", "link": "http://arxiv.org/abs/2508.11863v1", "categories": ["cs.SI", "cs.IT", "cs.LG", "cs.NI", "math.IT", "math.OC"], "primary_category": "cs.SI"}
{"title": "MAPF-World: Action World Model for Multi-Agent Path Finding", "abstract": "Multi-agent path finding (MAPF) is the problem of planning conflict-free\npaths from the designated start locations to goal positions for multiple\nagents. It underlies a variety of real-world tasks, including multi-robot\ncoordination, robot-assisted logistics, and social navigation. Recent\ndecentralized learnable solvers have shown great promise for large-scale MAPF,\nespecially when leveraging foundation models and large datasets. However, these\nagents are reactive policy models and exhibit limited modeling of environmental\ntemporal dynamics and inter-agent dependencies, resulting in performance\ndegradation in complex, long-term planning scenarios. To address these\nlimitations, we propose MAPF-World, an autoregressive action world model for\nMAPF that unifies situation understanding and action generation, guiding\ndecisions beyond immediate local observations. It improves situational\nawareness by explicitly modeling environmental dynamics, including spatial\nfeatures and temporal dependencies, through future state and actions\nprediction. By incorporating these predicted futures, MAPF-World enables more\ninformed, coordinated, and far-sighted decision-making, especially in complex\nmulti-agent settings. Furthermore, we augment MAPF benchmarks by introducing an\nautomatic map generator grounded in real-world scenarios, capturing practical\nmap layouts for training and evaluating MAPF solvers. Extensive experiments\ndemonstrate that MAPF-World outperforms state-of-the-art learnable solvers,\nshowcasing superior zero-shot generalization to out-of-distribution cases.\nNotably, MAPF-World is trained with a 96.5% smaller model size and 92% reduced\ndata.", "published": "2025-08-16 15:50:26", "link": "http://arxiv.org/abs/2508.12087v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI"}
{"title": "AgentCDM: Enhancing Multi-Agent Collaborative Decision-Making via ACH-Inspired Structured Reasoning", "abstract": "Multi-agent systems (MAS) powered by large language models (LLMs) hold\nsignificant promise for solving complex decision-making tasks. However, the\ncore process of collaborative decision-making (CDM) within these systems\nremains underexplored. Existing approaches often rely on either ``dictatorial\"\nstrategies that are vulnerable to the cognitive biases of a single agent, or\n``voting-based\" methods that fail to fully harness collective intelligence. To\naddress these limitations, we propose \\textbf{AgentCDM}, a structured framework\nfor enhancing collaborative decision-making in LLM-based multi-agent systems.\nDrawing inspiration from the Analysis of Competing Hypotheses (ACH) in\ncognitive science, AgentCDM introduces a structured reasoning paradigm that\nsystematically mitigates cognitive biases and shifts decision-making from\npassive answer selection to active hypothesis evaluation and construction. To\ninternalize this reasoning process, we develop a two-stage training paradigm:\nthe first stage uses explicit ACH-inspired scaffolding to guide the model\nthrough structured reasoning, while the second stage progressively removes this\nscaffolding to encourage autonomous generalization. Experiments on multiple\nbenchmark datasets demonstrate that AgentCDM achieves state-of-the-art\nperformance and exhibits strong generalization, validating its effectiveness in\nimproving the quality and robustness of collaborative decisions in MAS.", "published": "2025-08-16 09:46:04", "link": "http://arxiv.org/abs/2508.11995v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI"}
{"title": "A Comprehensive Review of AI Agents: Transforming Possibilities in Technology and Beyond", "abstract": "Artificial Intelligence (AI) agents have rapidly evolved from specialized,\nrule-based programs to versatile, learning-driven autonomous systems capable of\nperception, reasoning, and action in complex environments. The explosion of\ndata, advances in deep learning, reinforcement learning, and multi-agent\ncoordination have accelerated this transformation. Yet, designing and deploying\nunified AI agents that seamlessly integrate cognition, planning, and\ninteraction remains a grand challenge. In this review, we systematically\nexamine the architectural principles, foundational components, and emergent\nparadigms that define the landscape of contemporary AI agents. We synthesize\ninsights from cognitive science-inspired models, hierarchical reinforcement\nlearning frameworks, and large language model-based reasoning. Moreover, we\ndiscuss the pressing ethical, safety, and interpretability concerns associated\nwith deploying these agents in real-world scenarios. By highlighting major\nbreakthroughs, persistent challenges, and promising research directions, this\nreview aims to guide the next generation of AI agent systems toward more\nrobust, adaptable, and trustworthy autonomous intelligence.", "published": "2025-08-16 07:38:45", "link": "http://arxiv.org/abs/2508.11957v1", "categories": ["cs.MA", "cs.AI", "cs.LG"], "primary_category": "cs.MA"}
{"title": "Reduced-order modeling of Hamiltonian dynamics based on symplectic neural networks", "abstract": "We introduce a novel data-driven symplectic induced-order modeling (ROM)\nframework for high-dimensional Hamiltonian systems that unifies latent-space\ndiscovery and dynamics learning within a single, end-to-end neural\narchitecture. The encoder-decoder is built from Henon neural networks\n(HenonNets) and may be augmented with linear SGS-reflector layers. This yields\nan exact symplectic map between full and latent phase spaces. Latent dynamics\nare advanced by a symplectic flow map implemented as a HenonNet. This unified\nneural architecture ensures exact preservation of the underlying symplectic\nstructure at the reduced-order level, significantly enhancing the fidelity and\nlong-term stability of the resulting ROM. We validate our method through\ncomprehensive numerical experiments on canonical Hamiltonian systems. The\nresults demonstrate the method's capability for accurate trajectory\nreconstruction, robust predictive performance beyond the training horizon, and\naccurate Hamiltonian preservation. These promising outcomes underscore the\neffectiveness and potential applicability of our symplectic ROM framework for\ncomplex dynamical systems across a broad range of scientific and engineering\ndisciplines.", "published": "2025-08-16 05:09:28", "link": "http://arxiv.org/abs/2508.11911v1", "categories": ["math.NA", "cs.LG", "cs.NA", "physics.comp-ph"], "primary_category": "math.NA"}
{"title": "Equilibrium Mean-Variance Dividend Rate Strategies", "abstract": "This paper studies an optimal dividend problem for a company that aims to\nmaximize the mean-variance (MV) objective of the accumulated discounted\ndividend payments up to its ruin time. The MV objective involves an integral\nform over a random horizon that depends endogenously on the company's dividend\nstrategy, and these features lead to a novel time-inconsistent control problem.\nTo address the time inconsistency, we seek a time-consistent equilibrium\ndividend rate strategy. We first develop and prove a new verification lemma\nthat characterizes the value function and equilibrium strategy by an extended\nHamilton-Jacobi-Bellman system. Next, we apply the verification lemma to obtain\nthe equilibrium strategy and show that it is a barrier strategy for small\nlevels of risk aversion.", "published": "2025-08-16 13:49:25", "link": "http://arxiv.org/abs/2508.12047v1", "categories": ["math.OC", "q-fin.MF", "93E20, 91G50, 91B30, 91G05"], "primary_category": "math.OC"}
{"title": "Unified Conformalized Multiple Testing with Full Data Efficiency", "abstract": "Conformalized multiple testing offers a model-free way to control predictive\nuncertainty in decision-making. Existing methods typically use only part of the\navailable data to build score functions tailored to specific settings. We\npropose a unified framework that puts data utilization at the center: it uses\nall available data-null, alternative, and unlabeled-to construct scores and\ncalibrate p-values through a full permutation strategy. This unified use of all\navailable data significantly improves power by enhancing non-conformity score\nquality and maximizing calibration set size while rigorously controlling the\nfalse discovery rate. Crucially, our framework provides a systematic design\nprinciple for conformal testing and enables automatic selection of the best\nconformal procedure among candidates without extra data splitting. Extensive\nnumerical experiments demonstrate that our enhanced methods deliver superior\nefficiency and adaptability across diverse scenarios.", "published": "2025-08-16 15:45:29", "link": "http://arxiv.org/abs/2508.12085v1", "categories": ["stat.ME", "stat.ML"], "primary_category": "stat.ME"}
{"title": "Robust Data Fusion via Subsampling", "abstract": "Data fusion and transfer learning are rapidly growing fields that enhance\nmodel performance for a target population by leveraging other related data\nsources or tasks. The challenges lie in the various potential heterogeneities\nbetween the target and external data, as well as various practical concerns\nthat prevent a na\\\"ive data integration. We consider a realistic scenario where\nthe target data is limited in size while the external data is large but\ncontaminated with outliers; such data contamination, along with other\ncomputational and operational constraints, necessitates proper selection or\nsubsampling of the external data for transfer learning. To our\nknowledge,transfer learning and subsampling under data contamination have not\nbeen thoroughly investigated. We address this gap by studying various transfer\nlearning methods with subsamples of the external data, accounting for outliers\ndeviating from the underlying true model due to arbitrary mean shifts. Two\nsubsampling strategies are investigated: one aimed at reducing biases and the\nother at minimizing variances. Approaches to combine these strategies are also\nintroduced to enhance the performance of the estimators. We provide\nnon-asymptotic error bounds for the transfer learning estimators, clarifying\nthe roles of sample sizes, signal strength, sampling rates, magnitude of\noutliers, and tail behaviors of model error distributions, among other factors.\nExtensive simulations show the superior performance of the proposed methods.\nAdditionally, we apply our methods to analyze the risk of hard landings in A380\nairplanes by utilizing data from other airplane types,demonstrating that robust\ntransfer learning can improve estimation efficiency for relatively rare\nairplane types with the help of data from other types of airplanes.", "published": "2025-08-16 13:49:26", "link": "http://arxiv.org/abs/2508.12048v1", "categories": ["stat.ML", "cs.LG", "62K05"], "primary_category": "stat.ML"}
{"title": "Universal Learning of Nonlinear Dynamics", "abstract": "We study the fundamental problem of learning a marginally stable unknown\nnonlinear dynamical system. We describe an algorithm for this problem, based on\nthe technique of spectral filtering, which learns a mapping from past\nobservations to the next based on a spectral representation of the system.\nUsing techniques from online convex optimization, we prove vanishing prediction\nerror for any nonlinear dynamical system that has finitely many marginally\nstable modes, with rates governed by a novel quantitative control-theoretic\nnotion of learnability. The main technical component of our method is a new\nspectral filtering algorithm for linear dynamical systems, which incorporates\npast observations and applies to general noisy and marginally stable systems.\nThis significantly generalizes the original spectral filtering algorithm to\nboth asymmetric dynamics as well as incorporating noise correction, and is of\nindependent interest.", "published": "2025-08-16 09:14:47", "link": "http://arxiv.org/abs/2508.11990v1", "categories": ["cs.LG", "math.OC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A note on simulation methods for the Dirichlet-Laplace prior", "abstract": "Bhattacharya et al. (2015, Journal of the American Statistical Association\n110(512): 1479-1490) introduce a novel prior, the Dirichlet-Laplace (DL) prior,\nand propose a Markov chain Monte Carlo (MCMC) method to simulate posterior\ndraws under this prior in a conditionally Gaussian setting. The original\nalgorithm samples from conditional distributions in the wrong order, i.e., it\ndoes not correctly sample from the joint posterior distribution of all latent\nvariables. This note details the issue and provides two simple solutions: A\ncorrection to the original algorithm and a new algorithm based on an\nalternative, yet equivalent, formulation of the prior. This corrigendum does\nnot affect the theoretical results in Bhattacharya et al. (2015).", "published": "2025-08-16 08:44:27", "link": "http://arxiv.org/abs/2508.11982v1", "categories": ["stat.CO", "econ.EM", "stat.ME", "stat.ML"], "primary_category": "stat.CO"}
{"title": "Dropping Just a Handful of Preferences Can Change Top Large Language Model Rankings", "abstract": "We propose a method for evaluating the robustness of a widely used LLM\nranking system -- the Bradley--Terry ranking system -- to dropping a worst-case\nvery small fraction of evaluation data. Our approach is computationally fast\nand easy to adopt. When we apply our method to matchups from two popular\nhuman-preference platforms, Chatbot Arena and MT-Bench, we find that the\nBradley--Terry rankings of top-performing models are remarkably sensitive to\nthe removal of a small fraction of evaluations. Our framework also identifies\nthe specific evaluations most responsible for such ranking flips, allowing for\ninspections of these influential preferences. We observe that the rankings\nderived from MT-Bench preferences are notably more robust than those from\nChatbot Arena, likely due to MT-bench's use of expert annotators and carefully\nconstructed prompts. Finally, we find that rankings based on crowdsourced\nhuman-evaluated systems are just as sensitive as those based on LLM-as-a-judge\nevaluations, where in both, dropping as little as 0.02% of the total\nevaluations in the dataset can change the top-ranked model.", "published": "2025-08-16 00:01:33", "link": "http://arxiv.org/abs/2508.11847v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "MASSLOC: A Massive Sound Source Localization System based on Direction-of-Arrival Estimation", "abstract": "Acoustic indoor localization offers the potential for highly accurate\nposition estimation while generally exhibiting low hardware requirements\ncompared to Radio Frequency (RF)-based solutions. Furthermore, angular-based\nlocalization significantly reduces installation effort by minimizing the number\nof required fixed anchor nodes. In this contribution, we propose the so-called\nMASSLOC system, which leverages sparse two-dimensional array geometries to\nlocalize and identify a large number of concurrently active sources.\nAdditionally, the use of complementary Zadoff-Chu sequences is introduced to\nenable efficient, beamforming-based source identification. These sequences\nprovide a trade-off between favorable correlation properties and accurate,\nunsynchronized direction-of-arrival estimation by exhibiting a spectrally\nbalanced waveform. The system is evaluated in both a controlled anechoic\nchamber and a highly reverberant lobby environment with a reverberation time of\n1.6 s. In a laboratory setting, successful direction-of-arrival estimation and\nidentification of up to 14 simultaneously emitting sources are demonstrated.\nAdopting a Perspective-n-Point (PnP) calibration approach, the system achieves\na median three-dimensional localization error of 55.7 mm and a median angular\nerror of 0.84 deg with dynamic source movement of up to 1.9 mps in the\nchallenging reverberant environment. The multi-source capability is also\ndemonstrated and evaluated in that environment with a total of three tags.\nThese results indicate the scalability and robustness of the MASSLOC system,\neven under challenging acoustic conditions.", "published": "2025-08-16 11:58:34", "link": "http://arxiv.org/abs/2508.12024v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "FNH-TTS: A Fast, Natural, and Human-Like Speech Synthesis System with advanced prosodic modeling based on Mixture of Experts", "abstract": "Achieving natural and human-like speech synthesis with low inference costs\nremains a major challenge in speech synthesis research. This study focuses on\nhuman prosodic patterns and synthesized spectrum harmony, addressing the\nchallenges of prosody modeling and artifact issues in non-autoregressive\nmodels. To enhance prosody modeling and synthesis quality, we introduce a new\nDuration Predictor based on the Mixture of Experts alongside a new Vocoder with\ntwo advanced multi-scale discriminators. We integrated the these new modules\ninto the VITS system, forming our FNH-TTS system. Our experiments on LJSpeech,\nVCTK, and LibriTTS demonstrate the system's superiority in synthesis quality,\nphoneme duration prediction, Vocoder results, and synthesis speed. Our prosody\nvisualization results show that FNH-TTS produces duration predictions that more\nclosely align with natural human beings than other systems.", "published": "2025-08-16 10:04:21", "link": "http://arxiv.org/abs/2508.12001v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Effect of Phase Shift Errors on the Security of UAV-assisted STAR-RIS IoT Networks", "abstract": "Unmanned aerial vehicles (UAV)-mounted simultaneous transmitting and\nreflecting reconfigurable intelligent surface (STAR-RIS) systems can provide\nfull-dimensional coverage and flexible deployment opportunities in future\n6G-enabled IoT networks. However, practical imperfections such as jittering and\nairflow of UAV could affect the phase shift of STAR-RIS, and consequently\ndegrade network security. In this respect, this paper investigates the impact\nof phase shift errors on the secrecy performance of UAV-mounted\nSTAR-RIS-assisted IoT systems. More specifically, we consider a UAV-mounted\nSTAR-RIS-assisted non-orthogonal multiple access (NOMA) system where IoT\ndevices are grouped into two groups: one group on each side of the STAR-RIS.\nThe nodes in each group are considered as potential Malicious nodes for the\nones on the other side. By modeling phase estimation errors using a von Mises\ndistribution, analytical closed-form expressions for the ergodic secrecy rates\nunder imperfect phase adjustment are derived. An optimization problem to\nmaximize the weighted sum secrecy rate (WSSR) by optimizing the UAV placement\nis formulated and is then solved using a linear grid-based algorithm. Monte\nCarlo simulations are provided to validate the analytical derivations. The\nimpact of phase estimation errors on system's secrecy performance is analyzed,\nproviding critical insights for the practical realisation of STAR-RIS\ndeployments for secure UAV-enabled IoT networks.", "published": "2025-08-16 17:55:26", "link": "http://arxiv.org/abs/2508.12114v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "RFSS: A Comprehensive Multi-Standard RF Signal Source Separation Dataset with Advanced Channel Modeling", "abstract": "The rapid evolution of wireless communication systems has created complex\nelectromagnetic environments where multiple cellular standards (2G/3G/4G/5G)\ncoexist, necessitating advanced signal source separation techniques. We present\nRFSS (RF Signal Source Separation), a comprehensive open-source dataset\ncontaining 52,847 realistic multi-standard RF signal samples with complete 3GPP\nstandards compliance. Our framework generates authentic baseband signals for\nGSM, UMTS, LTE, and 5G NR with advanced channel modeling including multipath\nfading, MIMO processing up to 8 by 8 antennas, and realistic interference\nscenarios. Experimental validation demonstrates superior performance of\nCNN-LSTM architectures achieving 26.7 dB SINR improvement in source separation\ntasks, significantly outperforming traditional ICA (15.2 dB) and NMF (18.3 dB)\napproaches. The RFSS dataset enables reproducible research in RF source\nseparation, cognitive radio, and machine learning applications while\nmaintaining complete open-source accessibility", "published": "2025-08-16 17:05:24", "link": "http://arxiv.org/abs/2508.12106v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "A Generalized Multidimensional Chinese Remainder Theorem (MD-CRT) for Multiple Integer Vectors", "abstract": "Chinese remainder theorem (CRT) is widely applied in cryptography, coding\ntheory, and signal processing. It has been extended to the multidimensional CRT\n(MD-CRT), which reconstructs an integer vector from its vector remainders\nmodulo multiple integer matrices. This paper investigates a generalized MD-CRT\nfor multiple integer vectors, where the goal is to determine multiple integer\nvectors from multiple vector residue sets modulo multiple integer\nmatrices.Comparing to the existing generalized CRT for multiple scalar\nintegers, the challenge is that the moduli in MD-CRT are matrices that do not\ncommute and the corresponding uniquely determinable range is multidimensional\nand the inclusion relationship is much more complicated. In this paper,we\naddress two fundamental questions regarding the generalized MD-CRT. The first\nquestion concerns the uniquely determinable range of multiple integer vectors\nwhen no prior information about them is available. The second question is about\nthe conditions under which the maximal possible dynamic range can be\nachieved.To answer these two questions, we first derive a uniquely determinable\nrange without prior information and accordingly propose an algorithm to achieve\nit. A special case involving only two integer vectors is investigated for the\nsecond question, leading to a new condition for achieving the maximal possible\ndynamic range. Interestingly, this newly obtained condition, when the dimension\nis reduced to $1$, is even better than the existing ones for the conventional\ngeneralized CRT for scalar integers.These results may have applications for\nfrequency detection in multidimensional signal processing.", "published": "2025-08-16 16:40:03", "link": "http://arxiv.org/abs/2508.12099v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Means of Random Variables in Lie Groups", "abstract": "The concepts of mean (i.e., average) and covariance of a random variable are\nfundamental in statistics, and are used to solve real-world problems such as\nthose that arise in robotics, computer vision, and medical imaging. On matrix\nLie groups, multiple competing definitions of the mean arise, including the\nEuclidean, projected, distance-based (i.e., Fr\\'echet and Karcher),\ngroup-theoretic, and parametric means. This article provides a comprehensive\nreview of these definitions, investigates their relationships to each other,\nand determines the conditions under which the group-theoretic means minimize a\nleast-squares type cost function. We also highlight the dependence of these\ndefinitions on the choice of inner product on the Lie algebra. The goal of this\narticle is to guide practitioners in selecting an appropriate notion of the\nmean in applications involving matrix Lie groups.", "published": "2025-08-16 12:38:09", "link": "http://arxiv.org/abs/2508.12030v1", "categories": ["math.ST", "eess.SP", "stat.TH"], "primary_category": "math.ST"}
{"title": "Autonomous Driving with RSMA-Enabled Finite Blocklength Transmissions: Ergodic Performance Analysis and Optimization", "abstract": "Rate-splitting multiple access (RSMA) is a key technology for next-generation\nmultiple access systems due to its robustness against imperfect channel state\ninformation (CSI). This makes RSMA particularly suitable for high-mobility\nautonomous driving, where ultra-reliable and low-latency communication (URLLC)\nis essential. To address the stringent requirements, this study enables RSMA\nfinite blocklength (FBL) transmissions and explicitly evaluates the ergodic\nperformance. We derive the closed-form lower bound for the ergodic sum-rate of\nRSMA, considering vital factors such as the vehicle velocities, vehicle\npositions, power allocation of each stream, blocklengths, and block error rates\n(BLERs). To further enhance the ergodic sum-rate while complying with quality\nof service (QoS) rate constraints, we jointly optimize the global power\ncoefficient, private power distribution, and common rate splitting. Guided by\ngradient descent, we first adjust the global power coefficient based on its\nsum-rate solution. This parameter regulates the power state of the common\nstream, allowing for dynamic activation or deactivation: if active, we optimize\nthe private power distribution and adjust the common rate splitting to meet\nminimum transmission constraints; if inactive, we use the sequential quadratic\nprogramming for private power distribution optimization. Simulation results\nconfirm that our RSMA scheme significantly improves the ergodic performance,\nreduces blocklength and BLER, surpassing the RSMA counterpart with average\nprivate power and space division multiple access (SDMA). Furthermore, our\napproach is validated to guarantee the rates for users with the poorest channel\nconditions, thereby enhancing fairness across the network.", "published": "2025-08-16 11:03:45", "link": "http://arxiv.org/abs/2508.12012v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Research on Conversational Recommender System Considering Consumer Types", "abstract": "Conversational Recommender Systems (CRS) provide personalized services\nthrough multi-turn interactions, yet most existing methods overlook users'\nheterogeneous decision-making styles and knowledge levels, which constrains\nboth accuracy and efficiency. To address this gap, we propose CT-CRS (Consumer\nType-Enhanced Conversational Recommender System), a framework that integrates\nconsumer type modeling into dialogue recommendation. Based on consumer type\ntheory, we define four user categories--dependent, efficient, cautious, and\nexpert--derived from two dimensions: decision-making style (maximizers vs.\nsatisficers) and knowledge level (high vs. low). CT-CRS employs interaction\nhistories and fine-tunes the large language model to automatically infer user\ntypes in real time, avoiding reliance on static questionnaires. We incorporate\nuser types into state representation and design a type-adaptive policy that\ndynamically adjusts recommendation granularity, diversity, and attribute query\ncomplexity. To further optimize the dialogue policy, we adopt Inverse\nReinforcement Learning (IRL), enabling the agent to approximate expert-like\nstrategies conditioned on consumer type. Experiments on LastFM, Amazon-Book,\nand Yelp show that CTCRS improves recommendation success rate and reduces\ninteraction turns compared to strong baselines. Ablation studies confirm that\nboth consumer type modeling and IRL contribute significantly to performance\ngains. These results demonstrate that CT-CRS offers a scalable and\ninterpretable solution for enhancing CRS personalization through the\nintegration of psychological modeling and advanced policy optimization.", "published": "2025-08-16 15:15:52", "link": "http://arxiv.org/abs/2508.13209v1", "categories": ["cs.IR", "cs.AI", "cs.SI", "J.4; I.2; K.4"], "primary_category": "cs.IR"}
{"title": "Benchmarking LLM-based Agents for Single-cell Omics Analysis", "abstract": "The surge in multimodal single-cell omics data exposes limitations in\ntraditional, manually defined analysis workflows. AI agents offer a paradigm\nshift, enabling adaptive planning, executable code generation, traceable\ndecisions, and real-time knowledge fusion. However, the lack of a comprehensive\nbenchmark critically hinders progress. We introduce a novel benchmarking\nevaluation system to rigorously assess agent capabilities in single-cell omics\nanalysis. This system comprises: a unified platform compatible with diverse\nagent frameworks and LLMs; multidimensional metrics assessing cognitive program\nsynthesis, collaboration, execution efficiency, bioinformatics knowledge\nintegration, and task completion quality; and 50 diverse real-world single-cell\nomics analysis tasks spanning multi-omics, species, and sequencing\ntechnologies. Our evaluation reveals that Grok-3-beta achieves state-of-the-art\nperformance among tested agent frameworks. Multi-agent frameworks significantly\nenhance collaboration and execution efficiency over single-agent approaches\nthrough specialized role division. Attribution analyses of agent capabilities\nidentify that high-quality code generation is crucial for task success, and\nself-reflection has the most significant overall impact, followed by\nretrieval-augmented generation (RAG) and planning. This work highlights\npersistent challenges in code generation, long-context handling, and\ncontext-aware knowledge retrieval, providing a critical empirical foundation\nand best practices for developing robust AI agents in computational biology.", "published": "2025-08-16 04:26:18", "link": "http://arxiv.org/abs/2508.13201v1", "categories": ["q-bio.GN", "cs.AI", "cs.MA"], "primary_category": "q-bio.GN"}
{"title": "FNH-TTS: A Fast, Natural, and Human-Like Speech Synthesis System with advanced prosodic modeling based on Mixture of Experts", "abstract": "Achieving natural and human-like speech synthesis with low inference costs\nremains a major challenge in speech synthesis research. This study focuses on\nhuman prosodic patterns and synthesized spectrum harmony, addressing the\nchallenges of prosody modeling and artifact issues in non-autoregressive\nmodels. To enhance prosody modeling and synthesis quality, we introduce a new\nDuration Predictor based on the Mixture of Experts alongside a new Vocoder with\ntwo advanced multi-scale discriminators. We integrated the these new modules\ninto the VITS system, forming our FNH-TTS system. Our experiments on LJSpeech,\nVCTK, and LibriTTS demonstrate the system's superiority in synthesis quality,\nphoneme duration prediction, Vocoder results, and synthesis speed. Our prosody\nvisualization results show that FNH-TTS produces duration predictions that more\nclosely align with natural human beings than other systems.", "published": "2025-08-16 10:04:21", "link": "http://arxiv.org/abs/2508.12001v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
