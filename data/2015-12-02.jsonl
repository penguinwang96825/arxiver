{"title": "Benchmarking sentiment analysis methods for large-scale texts: A case\n  for using continuum-scored words and word shift graphs", "abstract": "The emergence and global adoption of social media has rendered possible the\nreal-time estimation of population-scale sentiment, bearing profound\nimplications for our understanding of human behavior. Given the growing\nassortment of sentiment measuring instruments, comparisons between them are\nevidently required. Here, we perform detailed tests of 6 dictionary-based\nmethods applied to 4 different corpora, and briefly examine a further 20\nmethods. We show that a dictionary-based method will only perform both reliably\nand meaningfully if (1) the dictionary covers a sufficiently large enough\nportion of a given text's lexicon when weighted by word usage frequency; and\n(2) words are scored on a continuous scale.", "published": "2015-12-02 00:34:51", "link": "http://arxiv.org/abs/1512.00531v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Annotating Character Relationships in Literary Texts", "abstract": "We present a dataset of manually annotated relationships between characters\nin literary texts, in order to support the training and evaluation of automatic\nmethods for relation type prediction in this domain (Makazhanov et al., 2014;\nKokkinakis, 2013) and the broader computational analysis of literary character\n(Elson et al., 2010; Bamman et al., 2014; Vala et al., 2015; Flekova and\nGurevych, 2015). In this work, we solicit annotations from workers on Amazon\nMechanical Turk for 109 texts ranging from Homer's _Iliad_ to Joyce's _Ulysses_\non four dimensions of interest: for a given pair of characters, we collect\njudgments as to the coarse-grained category (professional, social, familial),\nfine-grained category (friend, lover, parent, rival, employer), and affinity\n(positive, negative, neutral) that describes their primary relationship in a\ntext. We do not assume that this relationship is static; we also collect\njudgments as to whether it changes at any point in the course of the text.", "published": "2015-12-02 15:09:31", "link": "http://arxiv.org/abs/1512.00728v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probabilistic Latent Semantic Analysis (PLSA) untuk Klasifikasi Dokumen\n  Teks Berbahasa Indonesia", "abstract": "One task that is included in managing documents is how to find substantial\ninformation inside. Topic modeling is a technique that has been developed to\nproduce document representation in form of keywords. The keywords will be used\nin the indexing process and document retrieval as needed by users. In this\nresearch, we will discuss specifically about Probabilistic Latent Semantic\nAnalysis (PLSA). It will cover PLSA mechanism which involves Expectation\nMaximization (EM) as the training algorithm, how to conduct testing, and obtain\nthe accuracy result.", "published": "2015-12-02 04:41:58", "link": "http://arxiv.org/abs/1512.00576v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Klasifikasi Komponen Argumen Secara Otomatis pada Dokumen Teks berbentuk\n  Esai Argumentatif", "abstract": "By automatically recognize argument component, essay writers can do some\ninspections to texts that they have written. It will assist essay scoring\nprocess objectively and precisely because essay grader is able to see how well\nthe argument components are constructed. Some reseachers have tried to do\nargument detection and classification along with its implementation in some\ndomains. The common approach is by doing feature extraction to the text.\nGenerally, the features are structural, lexical, syntactic, indicator, and\ncontextual. In this research, we add new feature to the existing features. It\nadopts keywords list by Knott and Dale (1993). The experiment result shows the\nargument classification achieves 72.45% accuracy. Moreover, we still get the\nsame accuracy without the keyword lists. This concludes that the keyword lists\ndo not affect significantly to the features. All features are still weak to\nclassify major claim and claim, so we need other features which are useful to\ndifferentiate those two kind of argument components.", "published": "2015-12-02 04:58:38", "link": "http://arxiv.org/abs/1512.00578v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Learning Semantic Similarity for Very Short Texts", "abstract": "Levering data on social media, such as Twitter and Facebook, requires\ninformation retrieval algorithms to become able to relate very short text\nfragments to each other. Traditional text similarity methods such as tf-idf\ncosine-similarity, based on word overlap, mostly fail to produce good results\nin this case, since word overlap is little or non-existent. Recently,\ndistributed word representations, or word embeddings, have been shown to\nsuccessfully allow words to match on the semantic level. In order to pair short\ntext fragments - as a concatenation of separate words - an adequate distributed\nsentence representation is needed, in existing literature often obtained by\nnaively combining the individual word representations. We therefore\ninvestigated several text representations as a combination of word embeddings\nin the context of semantic pair matching. This paper investigates the\neffectiveness of several such naive techniques, as well as traditional tf-idf\nsimilarity, for fragments of different lengths. Our main contribution is a\nfirst step towards a hybrid method that combines the strength of dense\ndistributed representations - as opposed to sparse term matching - with the\nstrength of tf-idf based methods to automatically reduce the impact of less\ninformative terms. Our new approach outperforms the existing techniques in a\ntoy experimental set-up, leading to the conclusion that the combination of word\nembeddings and tf-idf information might lead to a better model for semantic\ncontent within very short text fragments.", "published": "2015-12-02 16:31:20", "link": "http://arxiv.org/abs/1512.00765v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Zero-Shot Event Detection by Multimodal Distributional Semantic\n  Embedding of Videos", "abstract": "We propose a new zero-shot Event Detection method by Multi-modal\nDistributional Semantic embedding of videos. Our model embeds object and action\nconcepts as well as other available modalities from videos into a\ndistributional semantic space. To our knowledge, this is the first Zero-Shot\nevent detection model that is built on top of distributional semantics and\nextends it in the following directions: (a) semantic embedding of multimodal\ninformation in videos (with focus on the visual modalities), (b) automatically\ndetermining relevance of concepts/attributes to a free text query, which could\nbe useful for other applications, and (c) retrieving videos by free text event\nquery (e.g., \"changing a vehicle tire\") based on their content. We embed videos\ninto a distributional semantic space and then measure the similarity between\nvideos and the event query in a free text form. We validated our method on the\nlarge TRECVID MED (Multimedia Event Detection) challenge. Using only the event\ntitle as a query, our method outperformed the state-of-the-art that uses big\ndescriptions from 12.6% to 13.5% with MAP metric and 0.73 to 0.83 with ROC-AUC\nmetric. It is also an order of magnitude faster.", "published": "2015-12-02 19:34:00", "link": "http://arxiv.org/abs/1512.00818v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
