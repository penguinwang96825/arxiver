{"title": "Latent Tree Language Model", "abstract": "In this paper we introduce Latent Tree Language Model (LTLM), a novel\napproach to language modeling that encodes syntax and semantics of a given\nsentence as a tree of word roles.\n  The learning phase iteratively updates the trees by moving nodes according to\nGibbs sampling. We introduce two algorithms to infer a tree for a given\nsentence. The first one is based on Gibbs sampling. It is fast, but does not\nguarantee to find the most probable tree. The second one is based on dynamic\nprogramming. It is slower, but guarantees to find the most probable tree. We\nprovide comparison of both algorithms.\n  We combine LTLM with 4-gram Modified Kneser-Ney language model via linear\ninterpolation. Our experiments with English and Czech corpora show significant\nperplexity reductions (up to 46% for English and 49% for Czech) compared with\nstandalone 4-gram Modified Kneser-Ney language model.", "published": "2016-07-24 15:40:36", "link": "http://arxiv.org/abs/1607.07057v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
