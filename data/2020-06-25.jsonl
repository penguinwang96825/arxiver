{"title": "Normalizing Text using Language Modelling based on Phonetics and String\n  Similarity", "abstract": "Social media networks and chatting platforms often use an informal version of\nnatural text. Adversarial spelling attacks also tend to alter the input text by\nmodifying the characters in the text. Normalizing these texts is an essential\nstep for various applications like language translation and text to speech\nsynthesis where the models are trained over clean regular English language. We\npropose a new robust model to perform text normalization.\n  Our system uses the BERT language model to predict the masked words that\ncorrespond to the unnormalized words. We propose two unique masking strategies\nthat try to replace the unnormalized words in the text with their root form\nusing a unique score based on phonetic and string similarity metrics.We use\nhuman-centric evaluations where volunteers were asked to rank the normalized\ntext. Our strategies yield an accuracy of 86.7% and 83.2% which indicates the\neffectiveness of our system in dealing with text normalization.", "published": "2020-06-25 00:42:39", "link": "http://arxiv.org/abs/2006.14116v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation for Multilingual Grapheme-to-Phoneme\n  Conversion", "abstract": "Grapheme-to-phoneme (G2P) models are a key component in Automatic Speech\nRecognition (ASR) systems, such as the ASR system in Alexa, as they are used to\ngenerate pronunciations for out-of-vocabulary words that do not exist in the\npronunciation lexicons (mappings like \"e c h o\" to \"E k oU\"). Most G2P systems\nare monolingual and based on traditional joint-sequence based n-gram models\n[1,2]. As an alternative, we present a single end-to-end trained neural G2P\nmodel that shares same encoder and decoder across multiple languages. This\nallows the model to utilize a combination of universal symbol inventories of\nLatin-like alphabets and cross-linguistically shared feature representations.\nSuch model is especially useful in the scenarios of low resource languages and\ncode switching/foreign words, where the pronunciations in one language need to\nbe adapted to other locales or accents. We further experiment with word\nlanguage distribution vector as an additional training target in order to\nimprove system performance by helping the model decouple pronunciations across\na variety of languages in the parameter space. We show 7.2% average improvement\nin phoneme error rate over low resource languages and no degradation over high\nresource ones compared to monolingual baselines.", "published": "2020-06-25 06:16:29", "link": "http://arxiv.org/abs/2006.14194v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Simple Approach to Case-Based Reasoning in Knowledge Bases", "abstract": "We present a surprisingly simple yet accurate approach to reasoning in\nknowledge graphs (KGs) that requires \\emph{no training}, and is reminiscent of\ncase-based reasoning in classical artificial intelligence (AI). Consider the\ntask of finding a target entity given a source entity and a binary relation.\nOur non-parametric approach derives crisp logical rules for each query by\nfinding multiple \\textit{graph path patterns} that connect similar source\nentities through the given relation. Using our method, we obtain new\nstate-of-the-art accuracy, outperforming all previous models, on NELL-995 and\nFB-122. We also demonstrate that our model is robust in low data settings,\noutperforming recently proposed meta-learning approaches", "published": "2020-06-25 06:28:09", "link": "http://arxiv.org/abs/2006.14198v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Domain Adaptation Outperforms Manual Domain Adaptation for\n  Predicting Financial Outcomes", "abstract": "In this paper, we automatically create sentiment dictionaries for predicting\nfinancial outcomes. We compare three approaches: (I) manual adaptation of the\ndomain-general dictionary H4N, (ii) automatic adaptation of H4N and (iii) a\ncombination consisting of first manual, then automatic adaptation. In our\nexperiments, we demonstrate that the automatically adapted sentiment dictionary\noutperforms the previous state of the art in predicting the financial outcomes\nexcess return and volatility. In particular, automatic adaptation performs\nbetter than manual adaptation. In our analysis, we find that annotation based\non an expert's a priori belief about a word's meaning can be incorrect --\nannotation should be performed based on the word's contexts in the target\ndomain instead.", "published": "2020-06-25 07:11:07", "link": "http://arxiv.org/abs/2006.14209v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Effect of Repeated Reading on Oral Fluency and Narrative\n  Production for Computer-Assisted Language Learning", "abstract": "Repeated reading (RR) helps learners, who have little to no experience with\nreading fluently to gain confidence, speed and process words automatically. The\nbenefits of repeated readings include helping all learners with fact recall,\naiding identification of learners' main ideas and vocabulary, increasing\ncomprehension, leading to faster reading as well as increasing word recognition\naccuracy, and assisting struggling learners as they transition from\nword-by-word reading to more meaningful phrasing. Thus, RR ultimately helps in\nimprovements of learners' oral fluency and narrative production. However, there\nare no open audio datasets available on oral responses of learners based on\ntheir RR practices. Therefore, in this paper, we present our dataset, discuss\nits properties, and propose a method to assess oral fluency and narrative\nproduction for learners of English using acoustic, prosodic, lexical and\nsyntactical characteristics. The results show that a CALL system can be\ndeveloped for assessing the improvements in learners' oral fluency and\nnarrative production.", "published": "2020-06-25 11:51:08", "link": "http://arxiv.org/abs/2006.14320v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Source Phrase Representations for Neural Machine Translation", "abstract": "The Transformer translation model (Vaswani et al., 2017) based on a\nmulti-head attention mechanism can be computed effectively in parallel and has\nsignificantly pushed forward the performance of Neural Machine Translation\n(NMT). Though intuitively the attentional network can connect distant words via\nshorter network paths than RNNs, empirical analysis demonstrates that it still\nhas difficulty in fully capturing long-distance dependencies (Tang et al.,\n2018). Considering that modeling phrases instead of words has significantly\nimproved the Statistical Machine Translation (SMT) approach through the use of\nlarger translation blocks (\"phrases\") and its reordering ability, modeling NMT\nat phrase level is an intuitive proposal to help the model capture\nlong-distance relationships. In this paper, we first propose an attentive\nphrase representation generation mechanism which is able to generate phrase\nrepresentations from corresponding token representations. In addition, we\nincorporate the generated phrase representations into the Transformer\ntranslation model to enhance its ability to capture long-distance\nrelationships. In our experiments, we obtain significant improvements on the\nWMT 14 English-German and English-French tasks on top of the strong Transformer\nbaseline, which shows the effectiveness of our approach. Our approach helps\nTransformer Base models perform at the level of Transformer Big models, and\neven significantly better for long sentences, but with substantially fewer\nparameters and training steps. The fact that phrase representations help even\nin the big setting further supports our conjecture that they make a valuable\ncontribution to long-distance relations.", "published": "2020-06-25 13:43:11", "link": "http://arxiv.org/abs/2006.14405v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IIT Gandhinagar at SemEval-2020 Task 9: Code-Mixed Sentiment\n  Classification Using Candidate Sentence Generation and Selection", "abstract": "Code-mixing is the phenomenon of using multiple languages in the same\nutterance of a text or speech. It is a frequently used pattern of communication\non various platforms such as social media sites, online gaming, product\nreviews, etc. Sentiment analysis of the monolingual text is a well-studied\ntask. Code-mixing adds to the challenge of analyzing the sentiment of the text\ndue to the non-standard writing style. We present a candidate sentence\ngeneration and selection based approach on top of the Bi-LSTM based neural\nclassifier to classify the Hinglish code-mixed text into one of the three\nsentiment classes positive, negative, or neutral. The proposed approach shows\nan improvement in the system performance as compared to the Bi-LSTM based\nneural classifier. The results present an opportunity to understand various\nother nuances of code-mixing in the textual data, such as humor-detection,\nintent classification, etc.", "published": "2020-06-25 14:59:47", "link": "http://arxiv.org/abs/2006.14465v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "THEaiTRE: Artificial Intelligence to Write a Theatre Play", "abstract": "We present THEaiTRE, a starting project aimed at automatic generation of\ntheatre play scripts. This paper reviews related work and drafts an approach we\nintend to follow. We plan to adopt generative neural language models and\nhierarchical generation approaches, supported by summarization and machine\ntranslation methods, and complemented with a human-in-the-loop approach.", "published": "2020-06-25 19:24:57", "link": "http://arxiv.org/abs/2006.14668v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explainable CNN-attention Networks (C-Attention Network) for Automated\n  Detection of Alzheimer's Disease", "abstract": "In this work, we propose three explainable deep learning architectures to\nautomatically detect patients with Alzheimer`s disease based on their language\nabilities. The architectures use: (1) only the part-of-speech features; (2)\nonly language embedding features and (3) both of these feature classes via a\nunified architecture. We use self-attention mechanisms and interpretable\n1-dimensional ConvolutionalNeural Network (CNN) to generate two types of\nexplanations of the model`s action: intra-class explanation and inter-class\nexplanation. The inter-class explanation captures the relative importance of\neach of the different features in that class, while the inter-class explanation\ncaptures the relative importance between the classes. Note that although we\nhave considered two classes of features in this paper, the architecture is\neasily expandable to more classes because of its modularity. Extensive\nexperimentation and comparison with several recent models show that our method\noutperforms these methods with an accuracy of 92.2% and F1 score of 0.952on the\nDementiaBank dataset while being able to generate explanations. We show by\nexamples, how to generate these explanations using attention values.", "published": "2020-06-25 02:10:38", "link": "http://arxiv.org/abs/2006.14135v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LSBert: A Simple Framework for Lexical Simplification", "abstract": "Lexical simplification (LS) aims to replace complex words in a given sentence\nwith their simpler alternatives of equivalent meaning, to simplify the\nsentence. Recently unsupervised lexical simplification approaches only rely on\nthe complex word itself regardless of the given sentence to generate candidate\nsubstitutions, which will inevitably produce a large number of spurious\ncandidates. In this paper, we propose a lexical simplification framework LSBert\nbased on pretrained representation model Bert, that is capable of (1) making\nuse of the wider context when both detecting the words in need of\nsimplification and generating substitue candidates, and (2) taking five\nhigh-quality features into account for ranking candidates, including Bert\nprediction order, Bert-based language model, and the paraphrase database PPDB,\nin addition to the word frequency and word similarity commonly used in other LS\nmethods. We show that our system outputs lexical simplifications that are\ngrammatically correct and semantically appropriate, and obtains obvious\nimprovement compared with these baselines, outperforming the state-of-the-art\nby 29.8 Accuracy points on three well-known benchmarks.", "published": "2020-06-25 09:15:42", "link": "http://arxiv.org/abs/2006.14939v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Lest We Forget: A Dataset of Coronavirus-Related News Headlines in Swiss\n  Media", "abstract": "We release our COVID-19 news dataset, containing more than 10,000 links to\nnews articles related to the Coronavirus pandemic published in the Swiss media\nsince early January 2020. This collection can prove beneficial in mining and\nanalysis of the reaction of the Swiss media and the COVID-19 pandemic and\nextracting insightful information for further research. We hope this dataset\nhelps researchers and the public deliver results that will help analyse the\npandemic and potentially lead to a better understanding of the events.", "published": "2020-06-25 19:43:13", "link": "http://arxiv.org/abs/2006.16967v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "Towards Differentially Private Text Representations", "abstract": "Most deep learning frameworks require users to pool their local data or model\nupdates to a trusted server to train or maintain a global model. The assumption\nof a trusted server who has access to user information is ill-suited in many\napplications. To tackle this problem, we develop a new deep learning framework\nunder an untrusted server setting, which includes three modules: (1) embedding\nmodule, (2) randomization module, and (3) classifier module. For the\nrandomization module, we propose a novel local differentially private (LDP)\nprotocol to reduce the impact of privacy parameter $\\epsilon$ on accuracy, and\nprovide enhanced flexibility in choosing randomization probabilities for LDP.\nAnalysis and experiments show that our framework delivers comparable or even\nbetter performance than the non-private framework and existing LDP protocols,\ndemonstrating the advantages of our LDP protocol.", "published": "2020-06-25 04:42:18", "link": "http://arxiv.org/abs/2006.14170v1", "categories": ["cs.LG", "cs.CL", "cs.CR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Neural Machine Translation For Paraphrase Generation", "abstract": "Training a spoken language understanding system, as the one in Alexa,\ntypically requires a large human-annotated corpus of data. Manual annotations\nare expensive and time consuming. In Alexa Skill Kit (ASK) user experience with\nthe skill greatly depends on the amount of data provided by skill developer. In\nthis work, we present an automatic natural language generation system, capable\nof generating both human-like interactions and annotations by the means of\nparaphrasing. Our approach consists of machine translation (MT) inspired\nencoder-decoder deep recurrent neural network. We evaluate our model on the\nimpact it has on ASK skill, intent, named entity classification accuracy and\nsentence level coverage, all of which demonstrate significant improvements for\nunseen skills on natural language understanding (NLU) models, trained on the\ndata augmented with paraphrases.", "published": "2020-06-25 07:38:00", "link": "http://arxiv.org/abs/2006.14223v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SACT: Self-Aware Multi-Space Feature Composition Transformer for\n  Multinomial Attention for Video Captioning", "abstract": "Video captioning works on the two fundamental concepts, feature detection and\nfeature composition. While modern day transformers are beneficial in composing\nfeatures, they lack the fundamental problems of selecting and understanding of\nthe contents. As the feature length increases, it becomes increasingly\nimportant to include provisions for improved capturing of the pertinent\ncontents. In this work, we have introduced a new concept of Self-Aware\nComposition Transformer (SACT) that is capable of generating Multinomial\nAttention (MultAtt) which is a way of generating distributions of various\ncombinations of frames. Also, multi-head attention transformer works on the\nprinciple of combining all possible contents for attention, which is good for\nnatural language classification, but has limitations for video captioning.\nVideo contents have repetitions and require parsing of important contents for\nbetter content composition. In this work, we have introduced SACT for more\nselective attention and combined them for different attention heads for better\ncapturing of the usable contents for any applications. To address the problem\nof diversification and encourage selective utilization, we propose the\nSelf-Aware Composition Transformer model for dense video captioning and apply\nthe technique on two benchmark datasets like ActivityNet and YouCookII.", "published": "2020-06-25 09:11:49", "link": "http://arxiv.org/abs/2006.14262v1", "categories": ["cs.CV", "cs.CL", "cs.NE"], "primary_category": "cs.CV"}
{"title": "Self-Segregating and Coordinated-Segregating Transformer for Focused\n  Deep Multi-Modular Network for Visual Question Answering", "abstract": "Attention mechanism has gained huge popularity due to its effectiveness in\nachieving high accuracy in different domains. But attention is opportunistic\nand is not justified by the content or usability of the content. Transformer\nlike structure creates all/any possible attention(s). We define segregating\nstrategies that can prioritize the contents for the applications for\nenhancement of performance. We defined two strategies: Self-Segregating\nTransformer (SST) and Coordinated-Segregating Transformer (CST) and used it to\nsolve visual question answering application. Self-segregation strategy for\nattention contributes in better understanding and filtering the information\nthat can be most helpful for answering the question and create diversity of\nvisual-reasoning for attention. This work can easily be used in many other\napplications that involve repetition and multiple frames of features and would\nreduce the commonality of the attentions to a great extent. Visual Question\nAnswering (VQA) requires understanding and coordination of both images and\ntextual interpretations. Experiments demonstrate that segregation strategies\nfor cascaded multi-head transformer attention outperforms many previous works\nand achieved considerable improvement for VQA-v2 dataset benchmark.", "published": "2020-06-25 09:17:03", "link": "http://arxiv.org/abs/2006.14264v1", "categories": ["cs.CV", "cs.CL", "cs.NE"], "primary_category": "cs.CV"}
{"title": "LPar -- A Distributed Multi Agent platform for building Polyglot, Omni\n  Channel and Industrial grade Natural Language Interfaces", "abstract": "The goal of serving and delighting customers in a personal and near human\nlike manner is very high on automation agendas of most Enterprises. Last few\nyears, have seen huge progress in Natural Language Processing domain which has\nled to deployments of conversational agents in many enterprises. Most of the\ncurrent industrial deployments tend to use Monolithic Single Agent designs that\nmodel the entire knowledge and skill of the Domain. While this approach is one\nof the fastest to market, the monolithic design makes it very hard to scale\nbeyond a point. There are also challenges in seamlessly leveraging many tools\noffered by sub fields of Natural Language Processing and Information Retrieval\nin a single solution. The sub fields that can be leveraged to provide relevant\ninformation are, Question and Answer system, Abstractive Summarization,\nSemantic Search, Knowledge Graph etc. Current deployments also tend to be very\ndependent on the underlying Conversational AI platform (open source or\ncommercial) , which is a challenge as this is a fast evolving space and no one\nplatform can be considered future proof even in medium term of 3-4 years.\nLately,there is also work done to build multi agent solutions that tend to\nleverage a concept of master agent. While this has shown promise, this approach\nstill makes the master agent in itself difficult to scale. To address these\nchallenges, we introduce LPar, a distributed multi agent platform for large\nscale industrial deployment of polyglot, diverse and inter-operable agents. The\nasynchronous design of LPar supports dynamically expandable domain. We also\nintroduce multiple strategies available in the LPar system to elect the most\nsuitable agent to service a customer query.", "published": "2020-06-25 19:20:07", "link": "http://arxiv.org/abs/2006.14666v1", "categories": ["cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Speaker-Conditional Chain Model for Speech Separation and Extraction", "abstract": "Speech separation has been extensively explored to tackle the cocktail party\nproblem. However, these studies are still far from having enough generalization\ncapabilities for real scenarios. In this work, we raise a common strategy named\nSpeaker-Conditional Chain Model to process complex speech recordings. In the\nproposed method, our model first infers the identities of variable numbers of\nspeakers from the observation based on a sequence-to-sequence model. Then, it\ntakes the information from the inferred speakers as conditions to extract their\nspeech sources. With the predicted speaker information from whole observation,\nour model is helpful to solve the problem of conventional speech separation and\nspeaker extraction for multi-round long recordings. The experiments from\nstandard fully-overlapped speech separation benchmarks show comparable results\nwith prior studies, while our proposed model gets better adaptability for\nmulti-round long recordings.", "published": "2020-06-25 03:13:41", "link": "http://arxiv.org/abs/2006.14149v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Sequence to Multi-Sequence Learning via Conditional Chain Mapping for\n  Mixture Signals", "abstract": "Neural sequence-to-sequence models are well established for applications\nwhich can be cast as mapping a single input sequence into a single output\nsequence. In this work, we focus on one-to-many sequence transduction problems,\nsuch as extracting multiple sequential sources from a mixture sequence. We\nextend the standard sequence-to-sequence model to a conditional multi-sequence\nmodel, which explicitly models the relevance between multiple output sequences\nwith the probabilistic chain rule. Based on this extension, our model can\nconditionally infer output sequences one-by-one by making use of both input and\npreviously-estimated contextual output sequences. This model additionally has a\nsimple and efficient stop criterion for the end of the transduction, making it\nable to infer the variable number of output sequences. We take speech data as a\nprimary test field to evaluate our methods since the observed speech data is\noften composed of multiple sources due to the nature of the superposition\nprinciple of sound waves. Experiments on several different tasks including\nspeech separation and multi-speaker speech recognition show that our\nconditional multi-sequence models lead to consistent improvements over the\nconventional non-conditional models.", "published": "2020-06-25 03:16:13", "link": "http://arxiv.org/abs/2006.14150v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Dialogue Enhancement in Object-based Audio -- Evaluating the Benefit on\n  People above 65", "abstract": "Due to age-related hearing loss, elderly people often struggle with following\nthe language on TV. Because they form an increasing part of the audience, this\nproblem will become even more important in the future and needs to be addressed\nby research and development. Object-based audio is a promising approach to\nsolve this issue as it offers the possibility of customizable dialogue\nenhancement (DE). For this thesis an Adjustment / Satisfaction Test (A/ST) was\nconducted to evaluate the preferred loudness difference (LD) between speech and\nbackground in people above 65. Two different types of DE were tested: DE with\nseparately available audio components (speech and background) and DE with\ncomponents created by blind source separation (BSS). The preferred LDs compared\nto the original, differences of the preferred LDs between the two DE methods\nand the listener satisfaction were tested. It was observed that the preferred\nLDs were larger than the original LDs, that customizable DE increases listener\nsatisfaction and that the two DE methods performed comparably well in terms of\npreferred LD and listener satisfaction. Based on the results, it can be assumed\nthat elderly viewers above 65 will benefit equally from user-adjustable DE by\navailable components and by dialogue separation.", "published": "2020-06-25 09:57:00", "link": "http://arxiv.org/abs/2006.14282v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Sound Event Localization and Detection using Squeeze-Excitation Residual\n  CNNs", "abstract": "Sound Event Localization and Detection (SELD) is a problem related to the\nfield of machine listening whose objective is to recognize individual sound\nevents, detect their temporal activity, and estimate their spatial location.\nThanks to the emergence of more hard-labeled audio datasets, deep learning\ntechniques have become state-of-the-art solutions. The most common ones are\nthose that implement a convolutional recurrent network (CRNN) having previously\ntransformed the audio signal into multichannel 2D representation. The\nsqueeze-excitation technique can be considered as a convolution enhancement\nthat aims to learn spatial and channel feature maps independently rather than\ntogether as standard convolutions do. This is usually achieved by combining\nsome global clustering operators, linear operators and a final calibration\nbetween the block input and its learned relationships. This work aims to\nimprove the accuracy results of the baseline CRNN presented in DCASE 2020 Task\n3 by adding residual squeeze-excitation (SE) blocks in the convolutional part\nof the CRNN. The followed procedure involves a grid search of the ratio\nparameter (used in the linear relationships) of the residual SE block, whereas\nthe hyperparameters of the network remain the same as in the baseline.\nExperiments show that by simply introducing the residual SE blocks, the results\nobtained are able to improve the baseline considerably.", "published": "2020-06-25 14:18:55", "link": "http://arxiv.org/abs/2006.14436v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Streaming Transformer ASR with Blockwise Synchronous Beam Search", "abstract": "The Transformer self-attention network has shown promising performance as an\nalternative to recurrent neural networks in end-to-end (E2E) automatic speech\nrecognition (ASR) systems. However, Transformer has a drawback in that the\nentire input sequence is required to compute both self-attention and\nsource--target attention. In this paper, we propose a novel blockwise\nsynchronous beam search algorithm based on blockwise processing of encoder to\nperform streaming E2E Transformer ASR. In the beam search, encoded feature\nblocks are synchronously aligned using a block boundary detection technique,\nwhere a reliability score of each predicted hypothesis is evaluated based on\nthe end-of-sequence and repeated tokens in the hypothesis. Evaluations of the\nHKUST and AISHELL-1 Mandarin, LibriSpeech English, and CSJ Japanese tasks show\nthat the proposed streaming Transformer algorithm outperforms conventional\nonline approaches, including monotonic chunkwise attention (MoChA), especially\nwhen using the knowledge distillation technique. An ablation study indicates\nthat our streaming approach contributes to reducing the response time, and the\nrepetition criterion contributes significantly in certain tasks. Our streaming\nASR models achieve comparable or superior performance to batch models and other\nstreaming-based Transformer methods in all tasks considered.", "published": "2020-06-25 06:49:00", "link": "http://arxiv.org/abs/2006.14941v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Modeling Baroque Two-Part Counterpoint with Neural Machine Translation", "abstract": "We propose a system for contrapuntal music generation based on a Neural\nMachine Translation (NMT) paradigm. We consider Baroque counterpoint and are\ninterested in modeling the interaction between any two given parts as a mapping\nbetween a given source material and an appropriate target material. Like in\ntranslation, the former imposes some constraints on the latter, but doesn't\ndefine it completely. We collate and edit a bespoke dataset of Baroque pieces,\nuse it to train an attention-based neural network model, and evaluate the\ngenerated output via BLEU score and musicological analysis. We show that our\nmodel is able to respond with some idiomatic trademarks, such as imitation and\nappropriate rhythmic offset, although it falls short of having learned\nstylistically correct contrapuntal motion (e.g., avoidance of parallel fifths)\nor stricter imitative rules, such as canon.", "published": "2020-06-25 07:34:37", "link": "http://arxiv.org/abs/2006.14221v4", "categories": ["cs.SD", "cs.LG", "eess.AS", "I.2.1"], "primary_category": "cs.SD"}
{"title": "Dynamically Mitigating Data Discrepancy with Balanced Focal Loss for\n  Replay Attack Detection", "abstract": "It becomes urgent to design effective anti-spoofing algorithms for vulnerable\nautomatic speaker verification systems due to the advancement of high-quality\nplayback devices. Current studies mainly treat anti-spoofing as a binary\nclassification problem between bonafide and spoofed utterances, while lack of\nindistinguishable samples makes it difficult to train a robust spoofing\ndetector. In this paper, we argue that for anti-spoofing, it needs more\nattention for indistinguishable samples over easily-classified ones in the\nmodeling process, to make correct discrimination a top priority. Therefore, to\nmitigate the data discrepancy between training and inference, we propose D3M,\nto leverage a balanced focal loss function as the training objective to\ndynamically scale the loss based on the traits of the sample itself. Besides,\nin the experiments, we select three kinds of features that contain both\nmagnitude-based and phase-based information to form complementary and\ninformative features. Experimental results on the ASVspoof2019 dataset\ndemonstrate the superiority of the proposed methods by comparison between our\nsystems and top-performing ones. Systems trained with the balanced focal loss\nperform significantly better than conventional cross-entropy loss. With\ncomplementary features, our fusion system with only three kinds of features\noutperforms other systems containing five or more complex single models by\n22.5% for min-tDCF and 7% for EER, achieving a min-tDCF and an EER of 0.0124\nand 0.55% respectively. Furthermore, we present and discuss the evaluation\nresults on real replay data apart from the simulated ASVspoof2019 data,\nindicating that research for anti-spoofing still has a long way to go. Source\ncode, analysis data, and other details are publicly available at\nhttps://github.com/asvspoof/D3M.", "published": "2020-06-25 17:06:47", "link": "http://arxiv.org/abs/2006.14563v3", "categories": ["cs.CV", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.CV"}
