{"title": "The Low-Dimensional Linear Geometry of Contextualized Word\n  Representations", "abstract": "Black-box probing models can reliably extract linguistic features like tense,\nnumber, and syntactic role from pretrained word representations. However, the\nmanner in which these features are encoded in representations remains poorly\nunderstood. We present a systematic study of the linear geometry of\ncontextualized word representations in ELMO and BERT. We show that a variety of\nlinguistic features (including structured dependency relationships) are encoded\nin low-dimensional subspaces. We then refine this geometric picture, showing\nthat there are hierarchical relations between the subspaces encoding general\nlinguistic categories and more specific ones, and that low-dimensional feature\nencodings are distributed rather than aligned to individual neurons. Finally,\nwe demonstrate that these linear subspaces are causally related to model\nbehavior, and can be used to perform fine-grained manipulation of BERT's output\ndistribution.", "published": "2021-05-15 00:58:08", "link": "http://arxiv.org/abs/2105.07109v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Premise-based Multimodal Reasoning: Conditional Inference on Joint\n  Textual and Visual Clues", "abstract": "It is a common practice for recent works in vision language cross-modal\nreasoning to adopt a binary or multi-choice classification formulation taking\nas input a set of source image(s) and textual query. In this work, we take a\nsober look at such an unconditional formulation in the sense that no prior\nknowledge is specified with respect to the source image(s). Inspired by the\ndesigns of both visual commonsense reasoning and natural language inference\ntasks, we propose a new task termed Premise-based Multi-modal Reasoning(PMR)\nwhere a textual premise is the background presumption on each source image. The\nPMR dataset contains 15,360 manually annotated samples which are created by a\nmulti-phase crowd-sourcing process. With selected high-quality movie\nscreenshots and human-curated premise templates from 6 pre-defined categories,\nwe ask crowd-source workers to write one true hypothesis and three distractors\n(4 choices) given the premise and image through a cross-check procedure.\nBesides, we generate adversarial samples to alleviate the annotation artifacts\nand double the size of PMR. We benchmark various state-of-the-art (pretrained)\nmulti-modal inference models on PMR and conduct comprehensive experimental\nanalyses to showcase the utility of our dataset.", "published": "2021-05-15 03:25:42", "link": "http://arxiv.org/abs/2105.07122v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Cognitive Regularizer for Language Modeling", "abstract": "The uniform information density (UID) hypothesis, which posits that speakers\nbehaving optimally tend to distribute information uniformly across a linguistic\nsignal, has gained traction in psycholinguistics as an explanation for certain\nsyntactic, morphological, and prosodic choices. In this work, we explore\nwhether the UID hypothesis can be operationalized as an inductive bias for\nstatistical language modeling. Specifically, we augment the canonical MLE\nobjective for training language models with a regularizer that encodes UID. In\nexperiments on ten languages spanning five language families, we find that\nusing UID regularization consistently improves perplexity in language models,\nhaving a larger effect when training data is limited. Moreover, via an analysis\nof generated sequences, we find that UID-regularized language models have other\ndesirable properties, e.g., they generate text that is more lexically diverse.\nOur results not only suggest that UID is a reasonable inductive bias for\nlanguage modeling, but also provide an alternative validation of the UID\nhypothesis using modern-day NLP tools.", "published": "2021-05-15 05:37:42", "link": "http://arxiv.org/abs/2105.07144v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lexicon Enhanced Chinese Sequence Labeling Using BERT Adapter", "abstract": "Lexicon information and pre-trained models, such as BERT, have been combined\nto explore Chinese sequence labelling tasks due to their respective strengths.\nHowever, existing methods solely fuse lexicon features via a shallow and random\ninitialized sequence layer and do not integrate them into the bottom layers of\nBERT. In this paper, we propose Lexicon Enhanced BERT (LEBERT) for Chinese\nsequence labelling, which integrates external lexicon knowledge into BERT\nlayers directly by a Lexicon Adapter layer. Compared with the existing methods,\nour model facilitates deep lexicon knowledge fusion at the lower layers of\nBERT. Experiments on ten Chinese datasets of three tasks including Named Entity\nRecognition, Word Segmentation, and Part-of-Speech tagging, show that LEBERT\nachieves the state-of-the-art results.", "published": "2021-05-15 06:13:39", "link": "http://arxiv.org/abs/2105.07148v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DirectQE: Direct Pretraining for Machine Translation Quality Estimation", "abstract": "Machine Translation Quality Estimation (QE) is a task of predicting the\nquality of machine translations without relying on any reference. Recently, the\npredictor-estimator framework trains the predictor as a feature extractor,\nwhich leverages the extra parallel corpora without QE labels, achieving\npromising QE performance. However, we argue that there are gaps between the\npredictor and the estimator in both data quality and training objectives, which\npreclude QE models from benefiting from a large number of parallel corpora more\ndirectly. We propose a novel framework called DirectQE that provides a direct\npretraining for QE tasks. In DirectQE, a generator is trained to produce pseudo\ndata that is closer to the real QE data, and a detector is pretrained on these\ndata with novel objectives that are akin to the QE task. Experiments on widely\nused benchmarks show that DirectQE outperforms existing methods, without using\nany pretraining models such as BERT. We also give extensive analyses showing\nhow fixing the two gaps contributes to our improvements.", "published": "2021-05-15 06:18:49", "link": "http://arxiv.org/abs/2105.07149v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "String Theories involving Regular Membership Predicates: From Practice\n  to Theory and Back", "abstract": "Widespread use of string solvers in formal analysis of string-heavy programs\nhas led to a growing demand for more efficient and reliable techniques which\ncan be applied in this context, especially for real-world cases. Designing an\nalgorithm for the (generally undecidable) satisfiability problem for systems of\nstring constraints requires a thorough understanding of the structure of\nconstraints present in the targeted cases. In this paper, we investigate\nbenchmarks presented in the literature containing regular expression membership\npredicates, extract different first order logic theories, and prove their\ndecidability, resp. undecidability. Notably, the most common theories in\nreal-world benchmarks are PSPACE-complete and directly lead to the\nimplementation of a more efficient algorithm to solving string constraints.", "published": "2021-05-15 13:13:50", "link": "http://arxiv.org/abs/2105.07220v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "STAGE: Tool for Automated Extraction of Semantic Time Cues to Enrich\n  Neural Temporal Ordering Models", "abstract": "Despite achieving state-of-the-art accuracy on temporal ordering of events,\nneural models showcase significant gaps in performance. Our work seeks to fill\none of these gaps by leveraging an under-explored dimension of textual\nsemantics: rich semantic information provided by explicit textual time cues. We\ndevelop STAGE, a system that consists of a novel temporal framework and a\nparser that can automatically extract time cues and convert them into\nrepresentations suitable for integration with neural models. We demonstrate the\nutility of extracted cues by integrating them with an event ordering model\nusing a joint BiLSTM and ILP constraint architecture. We outline the\nfunctionality of the 3-part STAGE processing approach, and show two methods of\nintegrating its representations with the BiLSTM-ILP model: (i) incorporating\nsemantic cues as additional features, and (ii) generating new constraints from\nsemantic cues to be enforced in the ILP. We demonstrate promising results on\ntwo event ordering datasets, and highlight important issues in semantic cue\nrepresentation and integration for future research.", "published": "2021-05-15 23:34:02", "link": "http://arxiv.org/abs/2105.07314v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Masked Language Modeling to Translation: Non-English Auxiliary\n  Tasks Improve Zero-shot Spoken Language Understanding", "abstract": "The lack of publicly available evaluation data for low-resource languages\nlimits progress in Spoken Language Understanding (SLU). As key tasks like\nintent classification and slot filling require abundant training data, it is\ndesirable to reuse existing data in high-resource languages to develop models\nfor low-resource scenarios. We introduce xSID, a new benchmark for\ncross-lingual Slot and Intent Detection in 13 languages from 6 language\nfamilies, including a very low-resource dialect. To tackle the challenge, we\npropose a joint learning approach, with English SLU training data and\nnon-English auxiliary tasks from raw text, syntax and translation for transfer.\nWe study two setups which differ by type and language coverage of the\npre-trained embeddings. Our results show that jointly learning the main tasks\nwith masked language modeling is effective for slots, while machine translation\ntransfer works best for intent classification.", "published": "2021-05-15 23:51:11", "link": "http://arxiv.org/abs/2105.07316v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Annotation Uncertainty in the Context of Grammatical Change", "abstract": "This paper elaborates on the notion of uncertainty in the context of\nannotation in large text corpora, specifically focusing on (but not limited to)\nhistorical languages. Such uncertainty might be due to inherent properties of\nthe language, for example, linguistic ambiguity and overlapping categories of\nlinguistic description, but could also be caused by lacking annotation\nexpertise. By examining annotation uncertainty in more detail, we identify the\nsources and deepen our understanding of the nature and different types of\nuncertainty encountered in daily annotation practice. Moreover, some practical\nimplications of our theoretical findings are also discussed. Last but not\nleast, this article can be seen as an attempt to reconcile the perspectives of\nthe main scientific disciplines involved in corpus projects, linguistics and\ncomputer science, to develop a unified view and to highlight the potential\nsynergies between these disciplines.", "published": "2021-05-15 17:45:29", "link": "http://arxiv.org/abs/2105.07270v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rethinking Skip Connection with Layer Normalization in Transformers and\n  ResNets", "abstract": "Skip connection, is a widely-used technique to improve the performance and\nthe convergence of deep neural networks, which is believed to relieve the\ndifficulty in optimization due to non-linearity by propagating a linear\ncomponent through the neural network layers. However, from another point of\nview, it can also be seen as a modulating mechanism between the input and the\noutput, with the input scaled by a pre-defined value one. In this work, we\ninvestigate how the scale factors in the effectiveness of the skip connection\nand reveal that a trivial adjustment of the scale will lead to spurious\ngradient exploding or vanishing in line with the deepness of the models, which\ncould be addressed by normalization, in particular, layer normalization, which\ninduces consistent improvements over the plain skip connection. Inspired by the\nfindings, we further propose to adaptively adjust the scale of the input by\nrecursively applying skip connection with layer normalization, which promotes\nthe performance substantially and generalizes well across diverse tasks\nincluding both machine translation and image classification datasets.", "published": "2021-05-15 11:44:49", "link": "http://arxiv.org/abs/2105.07205v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "A Deep Metric Learning Approach to Account Linking", "abstract": "We consider the task of linking social media accounts that belong to the same\nauthor in an automated fashion on the basis of the content and metadata of\ntheir corresponding document streams. We focus on learning an embedding that\nmaps variable-sized samples of user activity -- ranging from single posts to\nentire months of activity -- to a vector space, where samples by the same\nauthor map to nearby points. The approach does not require human-annotated data\nfor training purposes, which allows us to leverage large amounts of social\nmedia content. The proposed model outperforms several competitive baselines\nunder a novel evaluation framework modeled after established recognition\nbenchmarks in other domains. Our method achieves high linking accuracy, even\nwith small samples from accounts not seen at training time, a prerequisite for\npractical applications of the proposed linking framework.", "published": "2021-05-15 17:06:31", "link": "http://arxiv.org/abs/2105.07263v1", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "1D CNN Architectures for Music Genre Classification", "abstract": "This paper proposes a 1D residual convolutional neural network (CNN)\narchitecture for music genre classification and compares it with other recent\n1D CNN architectures. The 1D CNNs learn a representation and a discriminant\ndirectly from the raw audio signal. Several convolutional layers capture the\ntime-frequency characteristics of the audio signal and learn various filters\nrelevant to the music genre recognition task. The proposed approach splits the\naudio signal into overlapped segments using a sliding window to comply with the\nfixed-length input constraint of the 1D CNNs. As a result, music genre\nclassification can be carried out on a single audio segment or on the\naggregation of the predictions on several audio segments, which improves the\nfinal accuracy. The performance of the proposed 1D residual CNN is assessed on\na public dataset of 1,000 audio clips. The experimental results have shown that\nit achieves 80.93% of mean accuracy in classifying music genres and outperforms\nother 1D CNN architectures.", "published": "2021-05-15 22:33:48", "link": "http://arxiv.org/abs/2105.07302v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Analyzing Images for Music Recommendation", "abstract": "Experiencing images with suitable music can greatly enrich the overall user\nexperience. The proposed image analysis method treats an artwork image\ndifferently from a photograph image. Automatic image classification is\nperformed using deep-learning based models. An illustrative analysis showcasing\nthe ability of our deep-models to inherently learn and utilize perceptually\nrelevant features when classifying artworks is also presented. The Mean Opinion\nScore (MOS) obtained from subjective assessments of the respective image and\nrecommended music pairs supports the effectiveness of our approach.", "published": "2021-05-15 04:14:47", "link": "http://arxiv.org/abs/2105.07135v1", "categories": ["cs.MM", "cs.AI", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.MM"}
{"title": "Move2Hear: Active Audio-Visual Source Separation", "abstract": "We introduce the active audio-visual source separation problem, where an\nagent must move intelligently in order to better isolate the sounds coming from\nan object of interest in its environment. The agent hears multiple audio\nsources simultaneously (e.g., a person speaking down the hall in a noisy\nhousehold) and it must use its eyes and ears to automatically separate out the\nsounds originating from a target object within a limited time budget. Towards\nthis goal, we introduce a reinforcement learning approach that trains movement\npolicies controlling the agent's camera and microphone placement over time,\nguided by the improvement in predicted audio separation quality. We demonstrate\nour approach in scenarios motivated by both augmented reality (system is\nalready co-located with the target object) and mobile robotics (agent begins\narbitrarily far from the target object). Using state-of-the-art realistic\naudio-visual simulations in 3D environments, we demonstrate our model's ability\nto find minimal movement sequences with maximal payoff for audio source\nseparation. Project: http://vision.cs.utexas.edu/projects/move2hear.", "published": "2021-05-15 04:58:08", "link": "http://arxiv.org/abs/2105.07142v2", "categories": ["cs.CV", "cs.LG", "cs.RO", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
