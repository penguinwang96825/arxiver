{"title": "Construction of a Quality Estimation Dataset for Automatic Evaluation of\n  Japanese Grammatical Error Correction", "abstract": "In grammatical error correction (GEC), automatic evaluation is an important\nfactor for research and development of GEC systems. Previous studies on\nautomatic evaluation have demonstrated that quality estimation models built\nfrom datasets with manual evaluation can achieve high performance in automatic\nevaluation of English GEC without using reference sentences.. However, quality\nestimation models have not yet been studied in Japanese, because there are no\ndatasets for constructing quality estimation models. Therefore, in this study,\nwe created a quality estimation dataset with manual evaluation to build an\nautomatic evaluation model for Japanese GEC. Moreover, we conducted a\nmeta-evaluation to verify the dataset's usefulness in building the Japanese\nquality estimation model.", "published": "2022-01-20 08:07:42", "link": "http://arxiv.org/abs/2201.08038v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VISA: An Ambiguous Subtitles Dataset for Visual Scene-Aware Machine\n  Translation", "abstract": "Existing multimodal machine translation (MMT) datasets consist of images and\nvideo captions or general subtitles, which rarely contain linguistic ambiguity,\nmaking visual information not so effective to generate appropriate\ntranslations. We introduce VISA, a new dataset that consists of 40k\nJapanese-English parallel sentence pairs and corresponding video clips with the\nfollowing key features: (1) the parallel sentences are subtitles from movies\nand TV episodes; (2) the source subtitles are ambiguous, which means they have\nmultiple possible translations with different meanings; (3) we divide the\ndataset into Polysemy and Omission according to the cause of ambiguity. We show\nthat VISA is challenging for the latest MMT system, and we hope that the\ndataset can facilitate MMT research. The VISA dataset is available at:\nhttps://github.com/ku-nlp/VISA.", "published": "2022-01-20 08:38:31", "link": "http://arxiv.org/abs/2201.08054v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistically-driven Multi-task Pre-training for Low-resource Neural\n  Machine Translation", "abstract": "In the present study, we propose novel sequence-to-sequence pre-training\nobjectives for low-resource machine translation (NMT): Japanese-specific\nsequence to sequence (JASS) for language pairs involving Japanese as the source\nor target language, and English-specific sequence to sequence (ENSS) for\nlanguage pairs involving English. JASS focuses on masking and reordering\nJapanese linguistic units known as bunsetsu, whereas ENSS is proposed based on\nphrase structure masking and reordering tasks. Experiments on ASPEC\nJapanese--English & Japanese--Chinese, Wikipedia Japanese--Chinese, News\nEnglish--Korean corpora demonstrate that JASS and ENSS outperform MASS and\nother existing language-agnostic pre-training methods by up to +2.9 BLEU points\nfor the Japanese--English tasks, up to +7.0 BLEU points for the\nJapanese--Chinese tasks and up to +1.3 BLEU points for English--Korean tasks.\nEmpirical analysis, which focuses on the relationship between individual parts\nin JASS and ENSS, reveals the complementary nature of the subtasks of JASS and\nENSS. Adequacy evaluation using LASER, human evaluation, and case studies\nreveals that our proposed methods significantly outperform pre-training methods\nwithout injected linguistic knowledge and they have a larger positive impact on\nthe adequacy as compared to the fluency. We release codes here:\nhttps://github.com/Mao-KU/JASS/tree/master/linguistically-driven-pretraining.", "published": "2022-01-20 09:10:08", "link": "http://arxiv.org/abs/2201.08070v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LEMON: Language-Based Environment Manipulation via Execution-Guided\n  Pre-training", "abstract": "Language-based environment manipulation requires agents to manipulate the\nenvironment following natural language instructions, which is challenging due\nto the huge space of the environments. To address this challenge, various\napproaches have been proposed in recent work. Although these approaches work\nwell for their intended environments, they are difficult to generalize across\nenvironments. In this work, we propose LEMON, a general framework for\nlanguage-based environment manipulation tasks. Specifically, we first specify a\ntask-agnostic approach for language-based environment manipulation tasks, which\ncan deal with various environments using the same generative language model.\nThen we propose an execution-guided pre-training strategy to inject prior\nknowledge of environments to the language model with a pure synthetic\npre-training corpus. Experimental results on tasks including Alchemy, Scene,\nTangrams, ProPara and Recipes demonstrate the effectiveness of LEMON: it\nachieves new state-of-the-art results on four of the tasks, and the\nexecution-guided pre-training strategy brings remarkable improvements on all\nexperimental tasks.", "published": "2022-01-20 09:29:34", "link": "http://arxiv.org/abs/2201.08081v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Why Did You Not Compare With That? Identifying Papers for Use as\n  Baselines", "abstract": "We propose the task of automatically identifying papers used as baselines in\na scientific article. We frame the problem as a binary classification task\nwhere all the references in a paper are to be classified as either baselines or\nnon-baselines. This is a challenging problem due to the numerous ways in which\na baseline reference can appear in a paper. We develop a dataset of $2,075$\npapers from ACL anthology corpus with all their references manually annotated\nas one of the two classes. We develop a multi-module attention-based neural\nclassifier for the baseline classification task that outperforms four\nstate-of-the-art citation role classification methods when applied to the\nbaseline classification task. We also present an analysis of the errors made by\nthe proposed classifier, eliciting the challenges that make baseline\nidentification a challenging problem.", "published": "2022-01-20 09:44:29", "link": "http://arxiv.org/abs/2201.08089v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Latent-Variable Model for Intrinsic Probing", "abstract": "The success of pre-trained contextualized representations has prompted\nresearchers to analyze them for the presence of linguistic information. Indeed,\nit is natural to assume that these pre-trained representations do encode some\nlevel of linguistic knowledge as they have brought about large empirical\nimprovements on a wide variety of NLP tasks, which suggests they are learning\ntrue linguistic generalization. In this work, we focus on intrinsic probing, an\nanalysis technique where the goal is not only to identify whether a\nrepresentation encodes a linguistic attribute but also to pinpoint where this\nattribute is encoded. We propose a novel latent-variable formulation for\nconstructing intrinsic probes and derive a tractable variational approximation\nto the log-likelihood. Our results show that our model is versatile and yields\ntighter mutual information estimates than two intrinsic probes previously\nproposed in the literature. Finally, we find empirical evidence that\npre-trained representations develop a cross-lingually entangled notion of\nmorphosyntax.", "published": "2022-01-20 15:01:12", "link": "http://arxiv.org/abs/2201.08214v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cheating Automatic Short Answer Grading: On the Adversarial Usage of\n  Adjectives and Adverbs", "abstract": "Automatic grading models are valued for the time and effort saved during the\ninstruction of large student bodies. Especially with the increasing\ndigitization of education and interest in large-scale standardized testing, the\npopularity of automatic grading has risen to the point where commercial\nsolutions are widely available and used. However, for short answer formats,\nautomatic grading is challenging due to natural language ambiguity and\nversatility. While automatic short answer grading models are beginning to\ncompare to human performance on some datasets, their robustness, especially to\nadversarially manipulated data, is questionable. Exploitable vulnerabilities in\ngrading models can have far-reaching consequences ranging from cheating\nstudents receiving undeserved credit to undermining automatic grading\naltogether - even when most predictions are valid. In this paper, we devise a\nblack-box adversarial attack tailored to the educational short answer grading\nscenario to investigate the grading models' robustness. In our attack, we\ninsert adjectives and adverbs into natural places of incorrect student answers,\nfooling the model into predicting them as correct. We observed a loss of\nprediction accuracy between 10 and 22 percentage points using the\nstate-of-the-art models BERT and T5. While our attack made answers appear less\nnatural to humans in our experiments, it did not significantly increase the\ngraders' suspicions of cheating. Based on our experiments, we provide\nrecommendations for utilizing automatic grading systems more safely in\npractice.", "published": "2022-01-20 17:34:33", "link": "http://arxiv.org/abs/2201.08318v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis: Predicting Yelp Scores", "abstract": "In this work, we predict the sentiment of restaurant reviews based on a\nsubset of the Yelp Open Dataset. We utilize the meta features and text\navailable in the dataset and evaluate several machine learning and\nstate-of-the-art deep learning approaches for the prediction task. Through\nseveral qualitative experiments, we show the success of the deep models with\nattention mechanism in learning a balanced model for reviews across different\nrestaurants. Finally, we propose a novel Multi-tasked joint BERT model that\nimproves the overall classification performance.", "published": "2022-01-20 04:47:12", "link": "http://arxiv.org/abs/2201.07999v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "TextHacker: Learning based Hybrid Local Search Algorithm for Text\n  Hard-label Adversarial Attack", "abstract": "Existing textual adversarial attacks usually utilize the gradient or\nprediction confidence to generate adversarial examples, making it hard to be\ndeployed in real-world applications. To this end, we consider a rarely\ninvestigated but more rigorous setting, namely hard-label attack, in which the\nattacker can only access the prediction label. In particular, we find we can\nlearn the importance of different words via the change on prediction label\ncaused by word substitutions on the adversarial examples. Based on this\nobservation, we propose a novel adversarial attack, termed Text Hard-label\nattacker (TextHacker). TextHacker randomly perturbs lots of words to craft an\nadversarial example. Then, TextHacker adopts a hybrid local search algorithm\nwith the estimation of word importance from the attack history to minimize the\nadversarial perturbation. Extensive evaluations for text classification and\ntextual entailment show that TextHacker significantly outperforms existing\nhard-label attacks regarding the attack performance as well as adversary\nquality.", "published": "2022-01-20 14:16:07", "link": "http://arxiv.org/abs/2201.08193v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LaMDA: Language Models for Dialog Applications", "abstract": "We present LaMDA: Language Models for Dialog Applications. LaMDA is a family\nof Transformer-based neural language models specialized for dialog, which have\nup to 137B parameters and are pre-trained on 1.56T words of public dialog data\nand web text. While model scaling alone can improve quality, it shows less\nimprovements on safety and factual grounding. We demonstrate that fine-tuning\nwith annotated data and enabling the model to consult external knowledge\nsources can lead to significant improvements towards the two key challenges of\nsafety and factual grounding. The first challenge, safety, involves ensuring\nthat the model's responses are consistent with a set of human values, such as\npreventing harmful suggestions and unfair bias. We quantify safety using a\nmetric based on an illustrative set of human values, and we find that filtering\ncandidate responses using a LaMDA classifier fine-tuned with a small amount of\ncrowdworker-annotated data offers a promising approach to improving model\nsafety. The second challenge, factual grounding, involves enabling the model to\nconsult external knowledge sources, such as an information retrieval system, a\nlanguage translator, and a calculator. We quantify factuality using a\ngroundedness metric, and we find that our approach enables the model to\ngenerate responses grounded in known sources, rather than responses that merely\nsound plausible. Finally, we explore the use of LaMDA in the domains of\neducation and content recommendations, and analyze their helpfulness and role\nconsistency.", "published": "2022-01-20 15:44:37", "link": "http://arxiv.org/abs/2201.08239v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NaijaSenti: A Nigerian Twitter Sentiment Corpus for Multilingual\n  Sentiment Analysis", "abstract": "Sentiment analysis is one of the most widely studied applications in NLP, but\nmost work focuses on languages with large amounts of data. We introduce the\nfirst large-scale human-annotated Twitter sentiment dataset for the four most\nwidely spoken languages in Nigeria (Hausa, Igbo, Nigerian-Pidgin, and\nYor\\`ub\\'a ) consisting of around 30,000 annotated tweets per language (and\n14,000 for Nigerian-Pidgin), including a significant fraction of code-mixed\ntweets. We propose text collection, filtering, processing and labeling methods\nthat enable us to create datasets for these low-resource languages. We evaluate\na rangeof pre-trained models and transfer strategies on the dataset. We find\nthat language-specific models and language-adaptivefine-tuning generally\nperform best. We release the datasets, trained models, sentiment lexicons, and\ncode to incentivizeresearch on sentiment analysis in under-represented\nlanguages.", "published": "2022-01-20 16:28:06", "link": "http://arxiv.org/abs/2201.08277v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Signature Entrenchment and Conceptual Changes in Automated Theory Repair", "abstract": "Human beliefs change, but so do the concepts that underpin them. The recent\nAbduction, Belief Revision and Conceptual Change (ABC) repair system combines\nseveral methods from automated theory repair to expand, contract, or reform\nlogical structures representing conceptual knowledge in artificial agents. In\nthis paper we focus on conceptual change: repair not only of the membership of\nlogical concepts, such as what animals can fly, but also concepts themselves,\nsuch that birds may be divided into flightless and flying birds, by changing\nthe signature of the logical theory used to represent them. We offer a method\nfor automatically evaluating entrenchment in the signature of a Datalog theory,\nin order to constrain automated theory repair to succinct and intuitive\noutcomes. Formally, signature entrenchment measures the inferential\ncontributions of every logical language element used to express conceptual\nknowledge, i.e., predicates and the arguments, ranking possible repairs to\nretain valuable logical concepts and reject redundant or implausible\nalternatives. This quantitative measurement of signature entrenchment offers a\nguide to the plausibility of conceptual changes, which we aim to contrast with\nhuman judgements of concept entrenchment in future work.", "published": "2022-01-20 18:11:58", "link": "http://arxiv.org/abs/2201.08340v1", "categories": ["cs.CL", "cs.AI", "68T30", "I.2.4"], "primary_category": "cs.CL"}
{"title": "Transfer Learning Approaches for Building Cross-Language Dense Retrieval\n  Models", "abstract": "The advent of transformer-based models such as BERT has led to the rise of\nneural ranking models. These models have improved the effectiveness of\nretrieval systems well beyond that of lexical term matching models such as\nBM25. While monolingual retrieval tasks have benefited from large-scale\ntraining collections such as MS MARCO and advances in neural architectures,\ncross-language retrieval tasks have fallen behind these advancements. This\npaper introduces ColBERT-X, a generalization of the ColBERT\nmulti-representation dense retrieval model that uses the XLM-RoBERTa (XLM-R)\nencoder to support cross-language information retrieval (CLIR). ColBERT-X can\nbe trained in two ways. In zero-shot training, the system is trained on the\nEnglish MS MARCO collection, relying on the XLM-R encoder for cross-language\nmappings. In translate-train, the system is trained on the MS MARCO English\nqueries coupled with machine translations of the associated MS MARCO passages.\nResults on ad hoc document ranking tasks in several languages demonstrate\nsubstantial and statistically significant improvements of these trained dense\nretrieval models over traditional lexical CLIR baselines.", "published": "2022-01-20 22:11:38", "link": "http://arxiv.org/abs/2201.08471v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Temporal Sentence Grounding in Videos: A Survey and Future Directions", "abstract": "Temporal sentence grounding in videos (TSGV), \\aka natural language video\nlocalization (NLVL) or video moment retrieval (VMR), aims to retrieve a\ntemporal moment that semantically corresponds to a language query from an\nuntrimmed video. Connecting computer vision and natural language, TSGV has\ndrawn significant attention from researchers in both communities. This survey\nattempts to provide a summary of fundamental concepts in TSGV and current\nresearch status, as well as future research directions. As the background, we\npresent a common structure of functional components in TSGV, in a tutorial\nstyle: from feature extraction from raw video and language query, to answer\nprediction of the target moment. Then we review the techniques for multimodal\nunderstanding and interaction, which is the key focus of TSGV for effective\nalignment between the two modalities. We construct a taxonomy of TSGV\ntechniques and elaborate the methods in different categories with their\nstrengths and weaknesses. Lastly, we discuss issues with the current TSGV\nresearch and share our insights about promising research directions.", "published": "2022-01-20 09:10:20", "link": "http://arxiv.org/abs/2201.08071v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Knowledge Graph Question Answering Leaderboard: A Community Resource to\n  Prevent a Replication Crisis", "abstract": "Data-driven systems need to be evaluated to establish trust in the scientific\napproach and its applicability. In particular, this is true for Knowledge Graph\n(KG) Question Answering (QA), where complex data structures are made accessible\nvia natural-language interfaces. Evaluating the capabilities of these systems\nhas been a driver for the community for more than ten years while establishing\ndifferent KGQA benchmark datasets. However, comparing different approaches is\ncumbersome. The lack of existing and curated leaderboards leads to a missing\nglobal view over the research field and could inject mistrust into the results.\nIn particular, the latest and most-used datasets in the KGQA community, LC-QuAD\nand QALD, miss providing central and up-to-date points of trust. In this paper,\nwe survey and analyze a wide range of evaluation results with significant\ncoverage of 100 publications and 98 systems from the last decade. We provide a\nnew central and open leaderboard for any KGQA benchmark dataset as a focal\npoint for the community - https://kgqa.github.io/leaderboard. Our analysis\nhighlights existing problems during the evaluation of KGQA systems. Thus, we\nwill point to possible improvements for future evaluations.", "published": "2022-01-20 13:46:01", "link": "http://arxiv.org/abs/2201.08174v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "End-to-end Generative Pretraining for Multimodal Video Captioning", "abstract": "Recent video and language pretraining frameworks lack the ability to generate\nsentences. We present Multimodal Video Generative Pretraining (MV-GPT), a new\npretraining framework for learning from unlabelled videos which can be\neffectively used for generative tasks such as multimodal video captioning.\nUnlike recent video-language pretraining frameworks, our framework trains both\na multimodal video encoder and a sentence decoder jointly. To overcome the lack\nof captions in unlabelled videos, we leverage the future utterance as an\nadditional text source and propose a bidirectional generation objective -- we\ngenerate future utterances given the present mulitmodal context, and also the\npresent utterance given future observations. With this objective, we train an\nencoder-decoder model end-to-end to generate a caption from raw pixels and\ntranscribed speech directly. Our model achieves state-of-the-art performance\nfor multimodal video captioning on four standard benchmarks, as well as for\nother video understanding tasks such as VideoQA, video retrieval and action\nclassification.", "published": "2022-01-20 16:16:21", "link": "http://arxiv.org/abs/2201.08264v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.CV"}
{"title": "Regional Negative Bias in Word Embeddings Predicts Racial Animus--but\n  only via Name Frequency", "abstract": "The word embedding association test (WEAT) is an important method for\nmeasuring linguistic biases against social groups such as ethnic minorities in\nlarge text corpora. It does so by comparing the semantic relatedness of words\nprototypical of the groups (e.g., names unique to those groups) and attribute\nwords (e.g., 'pleasant' and 'unpleasant' words). We show that anti-black WEAT\nestimates from geo-tagged social media data at the level of metropolitan\nstatistical areas strongly correlate with several measures of racial\nanimus--even when controlling for sociodemographic covariates. However, we also\nshow that every one of these correlations is explained by a third variable: the\nfrequency of Black names in the underlying corpora relative to White names.\nThis occurs because word embeddings tend to group positive (negative) words and\nfrequent (rare) words together in the estimated semantic space. As the\nfrequency of Black names on social media is strongly correlated with Black\nAmericans' prevalence in the population, this results in spurious anti-Black\nWEAT estimates wherever few Black Americans live. This suggests that research\nusing the WEAT to measure bias should consider term frequency, and also\ndemonstrates the potential consequences of using black-box models like word\nembeddings to study human cognition and behavior.", "published": "2022-01-20 20:52:12", "link": "http://arxiv.org/abs/2201.08451v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Text-to-Speech Using Multi-Task Learning and Speaker\n  Classifier Joint Training", "abstract": "In cross-lingual speech synthesis, the speech in various languages can be\nsynthesized for a monoglot speaker. Normally, only the data of monoglot\nspeakers are available for model training, thus the speaker similarity is\nrelatively low between the synthesized cross-lingual speech and the native\nlanguage recordings. Based on the multilingual transformer text-to-speech\nmodel, this paper studies a multi-task learning framework to improve the\ncross-lingual speaker similarity. To further improve the speaker similarity,\njoint training with a speaker classifier is proposed. Here, a scheme similar to\nparallel scheduled sampling is proposed to train the transformer model\nefficiently to avoid breaking the parallel training mechanism when introducing\njoint training. By using multi-task learning and speaker classifier joint\ntraining, in subjective and objective evaluations, the cross-lingual speaker\nsimilarity can be consistently improved for both the seen and unseen speakers\nin the training set.", "published": "2022-01-20 12:02:58", "link": "http://arxiv.org/abs/2201.08124v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Kinit Classification in Ethiopian Chants, Azmaris and Modern Music: A\n  New Dataset and CNN Benchmark", "abstract": "In this paper, we create EMIR, the first-ever Music Information Retrieval\ndataset for Ethiopian music. EMIR is freely available for research purposes and\ncontains 600 sample recordings of Orthodox Tewahedo chants, traditional Azmari\nsongs and contemporary Ethiopian secular music. Each sample is classified by\nfive expert judges into one of four well-known Ethiopian Kinits, Tizita, Bati,\nAmbassel and Anchihoye. Each Kinit uses its own pentatonic scale and also has\nits own stylistic characteristics. Thus, Kinit classification needs to combine\nscale identification with genre recognition. After describing the dataset, we\npresent the Ethio Kinits Model (EKM), based on VGG, for classifying the EMIR\nclips. In Experiment 1, we investigated whether Filterbank, Mel-spectrogram,\nChroma, or Mel-frequency Cepstral coefficient (MFCC) features work best for\nKinit classification using EKM. MFCC was found to be superior and was therefore\nadopted for Experiment 2, where the performance of EKM models using MFCC was\ncompared using three different audio sample lengths. 3s length gave the best\nresults. In Experiment 3, EKM and four existing models were compared on the\nEMIR dataset: AlexNet, ResNet50, VGG16 and LSTM. EKM was found to have the best\naccuracy (95.00%) as well as the fastest training time. We hope this work will\nencourage others to explore Ethiopian music and to experiment with other models\nfor Kinit classification.", "published": "2022-01-20 20:48:07", "link": "http://arxiv.org/abs/2201.08448v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
