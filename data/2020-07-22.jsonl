{"title": "Better Early than Late: Fusing Topics with Word Embeddings for Neural\n  Question Paraphrase Identification", "abstract": "Question paraphrase identification is a key task in Community Question\nAnswering (CQA) to determine if an incoming question has been previously asked.\nMany current models use word embeddings to identify duplicate questions, but\nthe use of topic models in feature-engineered systems suggests that they can be\nhelpful for this task, too. We therefore propose two ways of merging topics\nwith word embeddings (early vs. late fusion) in a new neural architecture for\nquestion paraphrase identification. Our results show that our system\noutperforms neural baselines on multiple CQA datasets, while an ablation study\nhighlights the importance of topics and especially early topic-embedding fusion\nin our architecture.", "published": "2020-07-22 10:09:26", "link": "http://arxiv.org/abs/2007.11314v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Massive Multi-Document Summarization of Product Reviews with Weak\n  Supervision", "abstract": "Product reviews summarization is a type of Multi-Document Summarization (MDS)\ntask in which the summarized document sets are often far larger than in\ntraditional MDS (up to tens of thousands of reviews). We highlight this\ndifference and coin the term \"Massive Multi-Document Summarization\" (MMDS) to\ndenote an MDS task that involves hundreds of documents or more. Prior work on\nproduct reviews summarization considered small samples of the reviews, mainly\ndue to the difficulty of handling massive document sets. We show that\nsummarizing small samples can result in loss of important information and\nprovide misleading evaluation results. We propose a schema for summarizing a\nmassive set of reviews on top of a standard summarization algorithm. Since\nwriting large volumes of reference summaries needed for advanced neural network\nmodels is impractical, our solution relies on weak supervision. Finally, we\npropose an evaluation scheme that is based on multiple crowdsourced reference\nsummaries and aims to capture the massive review collection. We show that an\ninitial implementation of our schema significantly improves over several\nbaselines in ROUGE scores, and exhibits strong coherence in a manual linguistic\nquality assessment.", "published": "2020-07-22 11:22:57", "link": "http://arxiv.org/abs/2007.11348v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "To Be or Not To Be a Verbal Multiword Expression: A Quest for\n  Discriminating Features", "abstract": "Automatic identification of mutiword expressions (MWEs) is a pre-requisite\nfor semantically-oriented downstream applications. This task is challenging\nbecause MWEs, especially verbal ones (VMWEs), exhibit surface variability.\nHowever, this variability is usually more restricted than in regular (non-VMWE)\nconstructions, which leads to various variability profiles. We use this fact to\ndetermine the optimal set of features which could be used in a supervised\nclassification setting to solve a subproblem of VMWE identification: the\nidentification of occurrences of previously seen VMWEs. Surprisingly, a simple\ncustom frequency-based feature selection method proves more efficient than\nother standard methods such as Chi-squared test, information gain or decision\ntrees. An SVM classifier using the optimal set of only 6 features outperforms\nthe best systems from a recent shared task on the French seen data.", "published": "2020-07-22 12:47:11", "link": "http://arxiv.org/abs/2007.11381v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "SemEval-2020 Task 1: Unsupervised Lexical Semantic Change Detection", "abstract": "Lexical Semantic Change detection, i.e., the task of identifying words that\nchange meaning over time, is a very active research area, with applications in\nNLP, lexicography, and linguistics. Evaluation is currently the most pressing\nproblem in Lexical Semantic Change detection, as no gold standards are\navailable to the community, which hinders progress. We present the results of\nthe first shared task that addresses this gap by providing researchers with an\nevaluation framework and manually annotated, high-quality datasets for English,\nGerman, Latin, and Swedish. 33 teams submitted 186 systems, which were\nevaluated on two subtasks.", "published": "2020-07-22 14:37:42", "link": "http://arxiv.org/abs/2007.11464v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effects of Language Relatedness for Cross-lingual Transfer Learning in\n  Character-Based Language Models", "abstract": "Character-based Neural Network Language Models (NNLM) have the advantage of\nsmaller vocabulary and thus faster training times in comparison to NNLMs based\non multi-character units. However, in low-resource scenarios, both the\ncharacter and multi-character NNLMs suffer from data sparsity. In such\nscenarios, cross-lingual transfer has improved multi-character NNLM performance\nby allowing information transfer from a source to the target language. In the\nsame vein, we propose to use cross-lingual transfer for character NNLMs applied\nto low-resource Automatic Speech Recognition (ASR). However, applying\ncross-lingual transfer to character NNLMs is not as straightforward. We observe\nthat relatedness of the source language plays an important role in\ncross-lingual pretraining of character NNLMs. We evaluate this aspect on ASR\ntasks for two target languages: Finnish (with English and Estonian as source)\nand Swedish (with Danish, Norwegian, and English as source). Prior work has\nobserved no difference between using the related or unrelated language for\nmulti-character NNLMs. We, however, show that for character-based NNLMs, only\npretraining with a related language improves the ASR performance, and using an\nunrelated language may deteriorate it. We also observe that the benefits are\nlarger when there is much lesser target data than source data.", "published": "2020-07-22 19:52:34", "link": "http://arxiv.org/abs/2007.11648v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When Classical Chinese Meets Machine Learning: Explaining the Relative\n  Performances of Word and Sentence Segmentation Tasks", "abstract": "We consider three major text sources about the Tang Dynasty of China in our\nexperiments that aim to segment text written in classical Chinese. These\ncorpora include a collection of Tang Tomb Biographies, the New Tang Book, and\nthe Old Tang Book. We show that it is possible to achieve satisfactory\nsegmentation results with the deep learning approach. More interestingly, we\nfound that some of the relative superiority that we observed among different\ndesigns of experiments may be explainable. The relative relevance among the\ntraining corpora provides hints/explanation for the observed differences in\nsegmentation results that were achieved when we employed different combinations\nof corpora to train the classifiers.", "published": "2020-07-22 02:42:01", "link": "http://arxiv.org/abs/2007.11171v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Predicting Job-Hopping Motive of Candidates Using Answers to Open-ended\n  Interview Questions", "abstract": "A significant proportion of voluntary employee turnover includes people who\nfrequently move from job to job, known as job-hopping. Our work shows that\nlanguage used in responding to interview questions on past behaviour and\nsituational judgement is predictive of job-hopping motive as measured by the\nJob-Hopping Motives (JHM) Scale. The study is based on responses from over\n45,000 job applicants who completed an online chat interview and self-rated\nthemselves on JHM Scale. Five different methods of text representation were\nevaluated, namely four open-vocabulary approaches (TF-IDF, LDA, Glove word\nembeddings and Doc2Vec document embeddings) and one closed-vocabulary approach\n(LIWC). The Glove embeddings provided the best results with a correlation of r\n= 0.35 between sequences of words used and the JHM Scale. Further analysis also\nshowed a correlation of r = 0.25 between language-based job-hopping motive and\nthe personality trait Openness to experience and a correlation of r = -0.09\nwith the trait Agreeableness.", "published": "2020-07-22 03:41:32", "link": "http://arxiv.org/abs/2007.11189v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Exploratory Search with Sentence Embeddings", "abstract": "Exploratory search aims to guide users through a corpus rather than\npinpointing exact information. We propose an exploratory search system based on\nhierarchical clusters and document summaries using sentence embeddings. With\nsentence embeddings, we represent documents as the mean of their embedded\nsentences, extract summaries containing sentences close to this document\nrepresentation and extract keyphrases close to the document representation. To\nevaluate our search system, we scrape our personal search history over the past\nyear and report our experience with the system. We then discuss motivating use\ncases of an exploratory search system of this nature and conclude with possible\ndirections of future work.", "published": "2020-07-22 04:46:54", "link": "http://arxiv.org/abs/2007.11198v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Multi-task learning for natural language processing in the 2020s: where\n  are we going?", "abstract": "Multi-task learning (MTL) significantly pre-dates the deep learning era, and\nit has seen a resurgence in the past few years as researchers have been\napplying MTL to deep learning solutions for natural language tasks. While\nsteady MTL research has always been present, there is a growing interest driven\nby the impressive successes published in the related fields of transfer\nlearning and pre-training, such as BERT, and the release of new challenge\nproblems, such as GLUE and the NLP Decathlon (decaNLP). These efforts place\nmore focus on how weights are shared across networks, evaluate the re-usability\nof network components and identify use cases where MTL can significantly\noutperform single-task solutions. This paper strives to provide a comprehensive\nsurvey of the numerous recent MTL contributions to the field of natural\nlanguage processing and provide a forum to focus efforts on the hardest\nunsolved problems in the next decade. While novel models that improve\nperformance on NLP benchmarks are continually produced, lasting MTL challenges\nremain unsolved which could hold the key to better language understanding,\nknowledge discovery and natural language interfaces.", "published": "2020-07-22 13:44:57", "link": "http://arxiv.org/abs/2007.16008v1", "categories": ["cs.CL", "cs.LG", "I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "IITK at the FinSim Task: Hypernym Detection in Financial Domain via\n  Context-Free and Contextualized Word Embeddings", "abstract": "In this paper, we present our approaches for the FinSim 2020 shared task on\n\"Learning Semantic Representations for the Financial Domain\". The goal of this\ntask is to classify financial terms into the most relevant hypernym (or\ntop-level) concept in an external ontology. We leverage both context-dependent\nand context-independent word embeddings in our analysis. Our systems deploy\nWord2vec embeddings trained from scratch on the corpus (Financial Prospectus in\nEnglish) along with pre-trained BERT embeddings. We divide the test dataset\ninto two subsets based on a domain rule. For one subset, we use unsupervised\ndistance measures to classify the term. For the second subset, we use simple\nsupervised classifiers like Naive Bayes, on top of the embeddings, to arrive at\na final prediction. Finally, we combine both the results. Our system ranks 1st\nbased on both the metrics, i.e., mean rank and accuracy.", "published": "2020-07-22 04:56:23", "link": "http://arxiv.org/abs/2007.11201v1", "categories": ["cs.CL", "cs.LG", "q-fin.CP"], "primary_category": "cs.CL"}
{"title": "Analogical Reasoning for Visually Grounded Language Acquisition", "abstract": "Children acquire language subconsciously by observing the surrounding world\nand listening to descriptions. They can discover the meaning of words even\nwithout explicit language knowledge, and generalize to novel compositions\neffortlessly. In this paper, we bring this ability to AI, by studying the task\nof Visually grounded Language Acquisition (VLA). We propose a multimodal\ntransformer model augmented with a novel mechanism for analogical reasoning,\nwhich approximates novel compositions by learning semantic mapping and\nreasoning operations from previously seen compositions. Our proposed method,\nAnalogical Reasoning Transformer Networks (ARTNet), is trained on raw\nmultimedia data (video frames and transcripts), and after observing a set of\ncompositions such as \"washing apple\" or \"cutting carrot\", it can generalize and\nrecognize new compositions in new video frames, such as \"washing carrot\" or\n\"cutting apple\". To this end, ARTNet refers to relevant instances in the\ntraining data and uses their visual features and captions to establish\nanalogies with the query image. Then it chooses the suitable verb and noun to\ncreate a new composition that describes the new image best. Extensive\nexperiments on an instructional video dataset demonstrate that the proposed\nmethod achieves significantly better generalization capability and recognition\naccuracy compared to state-of-the-art transformer models.", "published": "2020-07-22 20:51:58", "link": "http://arxiv.org/abs/2007.11668v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.RO", "68T07, 68T45, 68T50, 68T40, 68T27", "I.2.10; I.2.6; I.2.7; I.2.9"], "primary_category": "cs.CL"}
{"title": "Integrating Image Captioning with Rule-based Entity Masking", "abstract": "Given an image, generating its natural language description (i.e., caption)\nis a well studied problem. Approaches proposed to address this problem usually\nrely on image features that are difficult to interpret. Particularly, these\nimage features are subdivided into global and local features, where global\nfeatures are extracted from the global representation of the image, while local\nfeatures are extracted from the objects detected locally in an image. Although,\nlocal features extract rich visual information from the image, existing models\ngenerate captions in a blackbox manner and humans have difficulty interpreting\nwhich local objects the caption is aimed to represent. Hence in this paper, we\npropose a novel framework for the image captioning with an explicit object\n(e.g., knowledge graph entity) selection process while still maintaining its\nend-to-end training ability. The model first explicitly selects which local\nentities to include in the caption according to a human-interpretable mask,\nthen generate proper captions by attending to selected entities. Experiments\nconducted on the MSCOCO dataset demonstrate that our method achieves good\nperformance in terms of the caption quality and diversity with a more\ninterpretable generating process than previous counterparts.", "published": "2020-07-22 21:27:12", "link": "http://arxiv.org/abs/2007.11690v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Rethinking CNN Models for Audio Classification", "abstract": "In this paper, we show that ImageNet-Pretrained standard deep CNN models can\nbe used as strong baseline networks for audio classification. Even though there\nis a significant difference between audio Spectrogram and standard ImageNet\nimage samples, transfer learning assumptions still hold firmly. To understand\nwhat enables the ImageNet pretrained models to learn useful audio\nrepresentations, we systematically study how much of pretrained weights is\nuseful for learning spectrograms. We show (1) that for a given standard model\nusing pretrained weights is better than using randomly initialized weights (2)\nqualitative results of what the CNNs learn from the spectrograms by visualizing\nthe gradients. Besides, we show that even though we use the pretrained model\nweights for initialization, there is variance in performance in various output\nruns of the same model. This variance in performance is due to the random\ninitialization of linear classification layer and random mini-batch orderings\nin multiple runs. This brings significant diversity to build stronger ensemble\nmodels with an overall improvement in accuracy. An ensemble of ImageNet\npretrained DenseNet achieves 92.89% validation accuracy on the ESC-50 dataset\nand 87.42% validation accuracy on the UrbanSound8K dataset which is the current\nstate-of-the-art on both of these datasets.", "published": "2020-07-22 01:31:44", "link": "http://arxiv.org/abs/2007.11154v2", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Resource-Efficient Speech Mask Estimation for Multi-Channel Speech\n  Enhancement", "abstract": "While machine learning techniques are traditionally resource intensive, we\nare currently witnessing an increased interest in hardware and energy efficient\napproaches. This need for resource-efficient machine learning is primarily\ndriven by the demand for embedded systems and their usage in ubiquitous\ncomputing and IoT applications. In this article, we provide a\nresource-efficient approach for multi-channel speech enhancement based on Deep\nNeural Networks (DNNs). In particular, we use reduced-precision DNNs for\nestimating a speech mask from noisy, multi-channel microphone observations.\nThis speech mask is used to obtain either the Minimum Variance Distortionless\nResponse (MVDR) or Generalized Eigenvalue (GEV) beamformer. In the extreme case\nof binary weights and reduced precision activations, a significant reduction of\nexecution time and memory footprint is possible while still obtaining an audio\nquality almost on par to single-precision DNNs and a slightly larger Word Error\nRate (WER) for single speaker scenarios using the WSJ0 speech corpus.", "published": "2020-07-22 14:58:29", "link": "http://arxiv.org/abs/2007.11477v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Transfer Learning End-to-End ArabicText-To-Speech (TTS) Deep\n  Architecture", "abstract": "Speech synthesis is the artificial production of human speech. A typical\ntext-to-speech system converts a language text into a waveform. There exist\nmany English TTS systems that produce mature, natural, and human-like speech\nsynthesizers. In contrast, other languages, including Arabic, have not been\nconsidered until recently. Existing Arabic speech synthesis solutions are slow,\nof low quality, and the naturalness of synthesized speech is inferior to the\nEnglish synthesizers. They also lack essential speech key factors such as\nintonation, stress, and rhythm. Different works were proposed to solve those\nissues, including the use of concatenative methods such as unit selection or\nparametric methods. However, they required a lot of laborious work and domain\nexpertise. Another reason for such poor performance of Arabic speech\nsynthesizers is the lack of speech corpora, unlike English that has many\npublicly available corpora and audiobooks. This work describes how to generate\nhigh quality, natural, and human-like Arabic speech using an end-to-end neural\ndeep network architecture. This work uses just $\\langle$ text, audio $\\rangle$\npairs with a relatively small amount of recorded audio samples with a total of\n2.41 hours. It illustrates how to use English character embedding despite using\ndiacritic Arabic characters as input and how to preprocess these audio samples\nto achieve the best results.", "published": "2020-07-22 17:03:18", "link": "http://arxiv.org/abs/2007.11541v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
