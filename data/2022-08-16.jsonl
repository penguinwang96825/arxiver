{"title": "BERT(s) to Detect Multiword Expressions", "abstract": "Multiword expressions (MWEs) present groups of words in which the meaning of\nthe whole is not derived from the meaning of its parts. The task of processing\nMWEs is crucial in many natural language processing (NLP) applications,\nincluding machine translation and terminology extraction. Therefore, detecting\nMWEs is a popular research theme. In this paper, we explore state-of-the-art\nneural transformers in the task of detecting MWEs.We empirically evaluate\nseveral transformer models in the dataset for SemEval-2016 Task 10: Detecting\nMinimal Semantic Units and their Meanings (DiMSUM). We show that transformer\nmodels outperform the previous neural models based on long short-term memory\n(LSTM). The code and pre-trained model will be made freely available to the\ncommunity.", "published": "2022-08-16 16:32:23", "link": "http://arxiv.org/abs/2208.07832v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Temporal Concept Drift and Alignment: An empirical approach to comparing\n  Knowledge Organization Systems over time", "abstract": "This research explores temporal concept drift and temporal alignment in\nknowledge organization systems (KOS). A comparative analysis is pursued using\nthe 1910 Library of Congress Subject Headings, 2020 FAST Topical, and automatic\nindexing. The use case involves a sample of 90 nineteenth-century Encyclopedia\nBritannica entries. The entries were indexed using two approaches: 1) full-text\nindexing; 2) Named Entity Recognition was performed upon the entries with\nStanza, Stanford's NLP toolkit, and entities were automatically indexed with\nthe Helping Interdisciplinary Vocabulary application (HIVE), using both 1910\nLCSH and FAST Topical. The analysis focused on three goals: 1) identifying\nresults that were exclusive to the 1910 LCSH output; 2) identifying terms in\nthe exclusive set that have been deprecated from the contemporary LCSH,\ndemonstrating temporal concept drift; and 3) exploring the historical\nsignificance of these deprecated terms. Results confirm that historical\nvocabularies can be used to generate anachronistic subject headings\nrepresenting conceptual drift across time in KOS and historical resources. A\nmethodological contribution is made demonstrating how to study changes in KOS\nover time and improve the contextualization of historical humanities resources.", "published": "2022-08-16 16:37:17", "link": "http://arxiv.org/abs/2208.07835v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TexPrax: A Messaging Application for Ethical, Real-time Data Collection\n  and Annotation", "abstract": "Collecting and annotating task-oriented dialog data is difficult, especially\nfor highly specific domains that require expert knowledge. At the same time,\ninformal communication channels such as instant messengers are increasingly\nbeing used at work. This has led to a lot of work-relevant information that is\ndisseminated through those channels and needs to be post-processed manually by\nthe employees. To alleviate this problem, we present TexPrax, a messaging\nsystem to collect and annotate problems, causes, and solutions that occur in\nwork-related chats. TexPrax uses a chatbot to directly engage the employees to\nprovide lightweight annotations on their conversation and ease their\ndocumentation work. To comply with data privacy and security regulations, we\nuse an end-to-end message encryption and give our users full control over their\ndata which has various advantages over conventional annotation tools. We\nevaluate TexPrax in a user-study with German factory employees who ask their\ncolleagues for solutions on problems that arise during their daily work.\nOverall, we collect 202 task-oriented German dialogues containing 1,027\nsentences with sentence-level expert annotations. Our data analysis also\nreveals that real-world conversations frequently contain instances with\ncode-switching, varying abbreviations for the same entity, and dialects which\nNLP systems should be able to handle.", "published": "2022-08-16 17:04:58", "link": "http://arxiv.org/abs/2208.07846v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERTifying Sinhala -- A Comprehensive Analysis of Pre-trained Language\n  Models for Sinhala Text Classification", "abstract": "This research provides the first comprehensive analysis of the performance of\npre-trained language models for Sinhala text classification. We test on a set\nof different Sinhala text classification tasks and our analysis shows that out\nof the pre-trained multilingual models that include Sinhala (XLM-R, LaBSE, and\nLASER), XLM-R is the best model by far for Sinhala text classification. We also\npre-train two RoBERTa-based monolingual Sinhala models, which are far superior\nto the existing pre-trained language models for Sinhala. We show that when\nfine-tuned, these pre-trained language models set a very strong baseline for\nSinhala text classification and are robust in situations where labeled data is\ninsufficient for fine-tuning. We further provide a set of recommendations for\nusing pre-trained models for Sinhala text classification. We also introduce new\nannotated datasets useful for future research in Sinhala text classification\nand publicly release our pre-trained models.", "published": "2022-08-16 17:47:42", "link": "http://arxiv.org/abs/2208.07864v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Manual-Guided Dialogue for Flexible Conversational Agents", "abstract": "How to build and use dialogue data efficiently, and how to deploy models in\ndifferent domains at scale can be two critical issues in building a\ntask-oriented dialogue system. In this paper, we propose a novel manual-guided\ndialogue scheme to alleviate these problems, where the agent learns the tasks\nfrom both dialogue and manuals. The manual is an unstructured textual document\nthat guides the agent in interacting with users and the database during the\nconversation. Our proposed scheme reduces the dependence of dialogue models on\nfine-grained domain ontology, and makes them more flexible to adapt to various\ndomains. We then contribute a fully-annotated multi-domain dataset MagDial to\nsupport our scheme. It introduces three dialogue modeling subtasks: instruction\nmatching, argument filling, and response generation. Modeling these subtasks is\nconsistent with the human agent's behavior patterns. Experiments demonstrate\nthat the manual-guided dialogue scheme improves data efficiency and domain\nscalability in building dialogue systems. The dataset and benchmark will be\npublicly available for promoting future research.", "published": "2022-08-16 08:21:12", "link": "http://arxiv.org/abs/2208.07597v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CorpusBrain: Pre-train a Generative Retrieval Model for\n  Knowledge-Intensive Language Tasks", "abstract": "Knowledge-intensive language tasks (KILT) usually require a large body of\ninformation to provide correct answers. A popular paradigm to solve this\nproblem is to combine a search system with a machine reader, where the former\nretrieves supporting evidences and the latter examines them to produce answers.\nRecently, the reader component has witnessed significant advances with the help\nof large-scale pre-trained generative models. Meanwhile most existing solutions\nin the search component rely on the traditional ``index-retrieve-then-rank''\npipeline, which suffers from large memory footprint and difficulty in\nend-to-end optimization. Inspired by recent efforts in constructing model-based\nIR models, we propose to replace the traditional multi-step search pipeline\nwith a novel single-step generative model, which can dramatically simplify the\nsearch process and be optimized in an end-to-end manner. We show that a strong\ngenerative retrieval model can be learned with a set of adequately designed\npre-training tasks, and be adopted to improve a variety of downstream KILT\ntasks with further fine-tuning. We name the pre-trained generative retrieval\nmodel as CorpusBrain as all information about the corpus is encoded in its\nparameters without the need of constructing additional index. Empirical results\nshow that CorpusBrain can significantly outperform strong baselines for the\nretrieval task on the KILT benchmark and establish new state-of-the-art\ndownstream performances. We also show that CorpusBrain works well under zero-\nand low-resource settings.", "published": "2022-08-16 10:22:49", "link": "http://arxiv.org/abs/2208.07652v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "ConTextual Masked Auto-Encoder for Dense Passage Retrieval", "abstract": "Dense passage retrieval aims to retrieve the relevant passages of a query\nfrom a large corpus based on dense representations (i.e., vectors) of the query\nand the passages. Recent studies have explored improving pre-trained language\nmodels to boost dense retrieval performance. This paper proposes CoT-MAE\n(ConTextual Masked Auto-Encoder), a simple yet effective generative\npre-training method for dense passage retrieval. CoT-MAE employs an asymmetric\nencoder-decoder architecture that learns to compress the sentence semantics\ninto a dense vector through self-supervised and context-supervised masked\nauto-encoding. Precisely, self-supervised masked auto-encoding learns to model\nthe semantics of the tokens inside a text span, and context-supervised masked\nauto-encoding learns to model the semantical correlation between the text\nspans. We conduct experiments on large-scale passage retrieval benchmarks and\nshow considerable improvements over strong baselines, demonstrating the high\nefficiency of CoT-MAE. Our code is available at\nhttps://github.com/caskcsg/ir/tree/main/cotmae.", "published": "2022-08-16 11:17:22", "link": "http://arxiv.org/abs/2208.07670v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Parallel Hierarchical Transformer with Attention Alignment for\n  Abstractive Multi-Document Summarization", "abstract": "In comparison to single-document summarization, abstractive Multi-Document\nSummarization (MDS) brings challenges on the representation and coverage of its\nlengthy and linked sources. This study develops a Parallel Hierarchical\nTransformer (PHT) with attention alignment for MDS. By incorporating word- and\nparagraph-level multi-head attentions, the hierarchical architecture of PHT\nallows better processing of dependencies at both token and document levels. To\nguide the decoding towards a better coverage of the source documents, the\nattention-alignment mechanism is then introduced to calibrate beam search with\npredicted optimal attention distributions. Based on the WikiSum data, a\ncomprehensive evaluation is conducted to test improvements on MDS by the\nproposed architecture. By better handling the inner- and cross-document\ninformation, results in both ROUGE and human evaluation suggest that our\nhierarchical model generates summaries of higher quality relative to other\nTransformer-based baselines at relatively low computational cost.", "published": "2022-08-16 17:02:48", "link": "http://arxiv.org/abs/2208.07845v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficient Multimodal Transformer with Dual-Level Feature Restoration for\n  Robust Multimodal Sentiment Analysis", "abstract": "With the proliferation of user-generated online videos, Multimodal Sentiment\nAnalysis (MSA) has attracted increasing attention recently. Despite significant\nprogress, there are still two major challenges on the way towards robust MSA:\n1) inefficiency when modeling cross-modal interactions in unaligned multimodal\ndata; and 2) vulnerability to random modality feature missing which typically\noccurs in realistic settings. In this paper, we propose a generic and unified\nframework to address them, named Efficient Multimodal Transformer with\nDual-Level Feature Restoration (EMT-DLFR). Concretely, EMT employs\nutterance-level representations from each modality as the global multimodal\ncontext to interact with local unimodal features and mutually promote each\nother. It not only avoids the quadratic scaling cost of previous local-local\ncross-modal interaction methods but also leads to better performance. To\nimprove model robustness in the incomplete modality setting, on the one hand,\nDLFR performs low-level feature reconstruction to implicitly encourage the\nmodel to learn semantic information from incomplete data. On the other hand, it\ninnovatively regards complete and incomplete data as two different views of one\nsample and utilizes siamese representation learning to explicitly attract their\nhigh-level representations. Comprehensive experiments on three popular datasets\ndemonstrate that our method achieves superior performance in both complete and\nincomplete modality settings.", "published": "2022-08-16 08:02:30", "link": "http://arxiv.org/abs/2208.07589v2", "categories": ["cs.LG", "cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.LG"}
{"title": "American cultural regions mapped through the lexical analysis of social\n  media", "abstract": "Cultural areas represent a useful concept that cross-fertilizes diverse\nfields in social sciences. Knowledge of how humans organize and relate their\nideas and behavior within a society helps to understand their actions and\nattitudes towards different issues. However, the selection of common traits\nthat shape a cultural area is somewhat arbitrary. What is needed is a method\nthat can leverage the massive amounts of data coming online, especially through\nsocial media, to identify cultural regions without ad-hoc assumptions, biases\nor prejudices. This work takes a crucial step in this direction by introducing\na method to infer cultural regions based on the automatic analysis of large\ndatasets from microblogging posts. The approach presented here is based on the\nprinciple that cultural affiliation can be inferred from the topics that people\ndiscuss among themselves. Specifically, regional variations in written\ndiscourse are measured in American social media. From the frequency\ndistributions of content words in geotagged Tweets, the regional hotspots of\nwords' usage are found, and from there, principal components of regional\nvariation are derived. Through a hierarchical clustering of the data in this\nlower-dimensional space, this method yields clear cultural areas and the topics\nof discussion that define them. It uncovers a manifest North-South separation,\nwhich is primarily influenced by the African American culture, and further\ncontiguous (East-West) and non-contiguous divisions that provide a\ncomprehensive picture of today's cultural areas in the US.", "published": "2022-08-16 10:18:47", "link": "http://arxiv.org/abs/2208.07649v2", "categories": ["cs.CL", "cs.CY", "cs.SI", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation\n  with Large Language Models", "abstract": "State-of-the-art neural language models can now be used to solve ad-hoc\nlanguage tasks through zero-shot prompting without the need for supervised\ntraining. This approach has gained popularity in recent years, and researchers\nhave demonstrated prompts that achieve strong accuracy on specific NLP tasks.\nHowever, finding a prompt for new tasks requires experimentation. Different\nprompt templates with different wording choices lead to significant accuracy\ndifferences. PromptIDE allows users to experiment with prompt variations,\nvisualize prompt performance, and iteratively optimize prompts. We developed a\nworkflow that allows users to first focus on model feedback using small data\nbefore moving on to a large data regime that allows empirical grounding of\npromising prompts using quantitative measures of the task. The tool then allows\neasy deployment of the newly created ad-hoc models. We demonstrate the utility\nof PromptIDE (demo at http://prompt.vizhub.ai) and our workflow using several\nreal-world use cases.", "published": "2022-08-16 17:17:53", "link": "http://arxiv.org/abs/2208.07852v1", "categories": ["cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DICE: Data-Efficient Clinical Event Extraction with Generative Models", "abstract": "Event extraction for the clinical domain is an under-explored research area.\nThe lack of training data along with the high volume of domain-specific\nterminologies with vague entity boundaries makes the task especially\nchallenging. In this paper, we introduce DICE, a robust and data-efficient\ngenerative model for clinical event extraction. DICE frames event extraction as\na conditional generation problem and introduces a contrastive learning\nobjective to accurately decide the boundaries of biomedical mentions. DICE also\ntrains an auxiliary mention identification task jointly with event extraction\ntasks to better identify entity mention boundaries, and further introduces\nspecial markers to incorporate identified entity mentions as trigger and\nargument candidates for their respective tasks. To benchmark clinical event\nextraction, we compose MACCROBAT-EE, the first clinical event extraction\ndataset with argument annotation, based on an existing clinical information\nextraction dataset MACCROBAT. Our experiments demonstrate state-of-the-art\nperformances of DICE for clinical and news domain event extraction, especially\nunder low data settings.", "published": "2022-08-16 23:12:04", "link": "http://arxiv.org/abs/2208.07989v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "\"Are you okay, honey?\": Recognizing Emotions among Couples Managing\n  Diabetes in Daily Life using Multimodal Real-World Smartwatch Data", "abstract": "Couples generally manage chronic diseases together and the management takes\nan emotional toll on both patients and their romantic partners. Consequently,\nrecognizing the emotions of each partner in daily life could provide an insight\ninto their emotional well-being in chronic disease management. Currently, the\nprocess of assessing each partner's emotions is manual, time-intensive, and\ncostly. Despite the existence of works on emotion recognition among couples,\nnone of these works have used data collected from couples' interactions in\ndaily life. In this work, we collected 85 hours (1,021 5-minute samples) of\nreal-world multimodal smartwatch sensor data (speech, heart rate,\naccelerometer, and gyroscope) and self-reported emotion data (n=612) from 26\npartners (13 couples) managing diabetes mellitus type 2 in daily life. We\nextracted physiological, movement, acoustic, and linguistic features, and\ntrained machine learning models (support vector machine and random forest) to\nrecognize each partner's self-reported emotions (valence and arousal). Our\nresults from the best models (balanced accuracies of 63.8% and 78.1% for\narousal and valence respectively) are better than chance and our prior work\nthat also used data from German-speaking, Swiss-based couples, albeit, in the\nlab. This work contributes toward building automated emotion recognition\nsystems that would eventually enable partners to monitor their emotions in\ndaily life and enable the delivery of interventions to improve their emotional\nwell-being.", "published": "2022-08-16 22:04:12", "link": "http://arxiv.org/abs/2208.08909v2", "categories": ["cs.HC", "cs.CL", "cs.CY"], "primary_category": "cs.HC"}
{"title": "How Should We Evaluate Synthesized Environmental Sounds", "abstract": "Although several methods of environmental sound synthesis have been proposed,\nthere has been no discussion on how synthesized environmental sounds should be\nevaluated. Only either subjective or objective evaluations have been conducted\nin conventional evaluations, and it is not clear what type of evaluation should\nbe carried out. In this paper, we investigate how to evaluate synthesized\nenvironmental sounds. We also propose a subjective evaluation methodology to\nevaluate whether the synthesized sound appropriately represents the information\ninput to the environmental sound synthesis system. In our experiments, we\ncompare the proposed and conventional evaluation methods and show that the\nresults of subjective evaluations tended to differ from those of objective\nevaluations. From these results, we conclude that it is necessary to conduct\nnot only objective evaluation but also subjective evaluation.", "published": "2022-08-16 11:32:09", "link": "http://arxiv.org/abs/2208.07679v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Uconv-Conformer: High Reduction of Input Sequence Length for End-to-End\n  Speech Recognition", "abstract": "Optimization of modern ASR architectures is among the highest priority tasks\nsince it saves many computational resources for model training and inference.\nThe work proposes a new Uconv-Conformer architecture based on the standard\nConformer model. It consistently reduces the input sequence length by 16 times,\nwhich results in speeding up the work of the intermediate layers. To solve the\nconvergence issue connected with such a significant reduction of the time\ndimension, we use upsampling blocks like in the U-Net architecture to ensure\nthe correct CTC loss calculation and stabilize network training. The\nUconv-Conformer architecture appears to be not only faster in terms of training\nand inference speed but also shows better WER compared to the baseline\nConformer. Our best Uconv-Conformer model shows 47.8% and 23.5% inference\nacceleration on the CPU and GPU, respectively. Relative WER reduction is 7.3%\nand 9.2% on LibriSpeech test_clean and test_other respectively.", "published": "2022-08-16 10:40:15", "link": "http://arxiv.org/abs/2208.07657v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Enhancing Audio Perception of Music By AI Picked Room Acoustics", "abstract": "Every sound that we hear is the result of successive convolutional operations\n(e.g. room acoustics, microphone characteristics, resonant properties of the\ninstrument itself, not to mention characteristics and limitations of the sound\nreproduction system). In this work we seek to determine the best room in which\nto perform a particular piece using AI. Additionally, we use room acoustics as\na way to enhance the perceptual qualities of a given sound. Historically, rooms\n(particularly Churches and concert halls) were designed to host and serve\nspecific musical functions. In some cases the architectural acoustical\nqualities enhanced the music performed there. We try to mimic this, as a first\nstep, by designating room impulse responses that would correlate to producing\nenhanced sound quality for particular music. A convolutional architecture is\nfirst trained to take in an audio sample and mimic the ratings of experts with\nabout 78 % accuracy for various instrument families and notes for perceptual\nqualities. This gives us a scoring function for any audio sample which can rate\nthe perceptual pleasantness of a note automatically. Now, via a library of\nabout 60,000 synthetic impulse responses mimicking all kinds of room,\nmaterials, etc, we use a simple convolution operation, to transform the sound\nas if it was played in a particular room. The perceptual evaluator is used to\nrank the musical sounds, and yield the \"best room or the concert hall\" to play\na sound. As a byproduct it can also use room acoustics to turn a poor quality\nsound into a \"good\" sound.", "published": "2022-08-16 23:47:43", "link": "http://arxiv.org/abs/2208.07994v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
