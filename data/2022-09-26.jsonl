{"title": "Smells like Teen Spirit: An Exploration of Sensorial Style in Literary\n  Genres", "abstract": "It is well recognized that sensory perceptions and language have\ninterconnections through numerous studies in psychology, neuroscience, and\nsensorial linguistics. Set in this rich context we ask whether the use of\nsensorial language in writings is part of linguistic style? This question is\nimportant from the view of stylometrics research where a rich set of language\nfeatures have been explored, but with insufficient attention given to features\nrelated to sensorial language.\n  Taking this as the goal we explore several angles about sensorial language\nand style in collections of lyrics, novels, and poetry. We find, for example,\nthat individual use of sensorial language is not a random phenomenon; choice is\nlikely involved. Also, sensorial style is generally stable over time - the\nshifts are extremely small. Moreover, style can be extracted from just a few\nhundred sentences that have sensorial terms. We also identify representative\nand distinctive features within each genre.\n  For example, we observe that 4 of the top 6 representative features in novels\ncollection involved individuals using olfactory language where we expected them\nto use non-olfactory language.", "published": "2022-09-26 00:17:10", "link": "http://arxiv.org/abs/2209.12352v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "News Summarization and Evaluation in the Era of GPT-3", "abstract": "The recent success of prompting large language models like GPT-3 has led to a\nparadigm shift in NLP research. In this paper, we study its impact on text\nsummarization, focusing on the classic benchmark domain of news summarization.\nFirst, we investigate how GPT-3 compares against fine-tuned models trained on\nlarge summarization datasets. We show that not only do humans overwhelmingly\nprefer GPT-3 summaries, prompted using only a task description, but these also\ndo not suffer from common dataset-specific issues such as poor factuality.\nNext, we study what this means for evaluation, particularly the role of gold\nstandard test sets. Our experiments show that both reference-based and\nreference-free automatic metrics cannot reliably evaluate GPT-3 summaries.\nFinally, we evaluate models on a setting beyond generic summarization,\nspecifically keyword-based summarization, and show how dominant fine-tuning\napproaches compare to prompting.\n  To support further research, we release: (a) a corpus of 10K generated\nsummaries from fine-tuned and prompt-based models across 4 standard\nsummarization benchmarks, (b) 1K human preference judgments comparing different\nsystems for generic- and keyword-based summarization.", "published": "2022-09-26 01:04:52", "link": "http://arxiv.org/abs/2209.12356v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entailment Semantics Can Be Extracted from an Ideal Language Model", "abstract": "Language models are often trained on text alone, without additional\ngrounding. There is debate as to how much of natural language semantics can be\ninferred from such a procedure. We prove that entailment judgments between\nsentences can be extracted from an ideal language model that has perfectly\nlearned its target distribution, assuming the training sentences are generated\nby Gricean agents, i.e., agents who follow fundamental principles of\ncommunication from the linguistic theory of pragmatics. We also show entailment\njudgments can be decoded from the predictions of a language model trained on\nsuch Gricean data. Our results reveal a pathway for understanding the semantic\ninformation encoded in unlabeled linguistic data and a potential framework for\nextracting semantics from language models.", "published": "2022-09-26 04:16:02", "link": "http://arxiv.org/abs/2209.12407v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Content-Emotion Duality via Disentanglement for Empathetic\n  Conversation", "abstract": "The task of empathetic response generation aims to understand what feelings a\nspeaker expresses on his/her experiences and then reply to the speaker\nappropriately. To solve the task, it is essential to model the content-emotion\nduality of a dialogue, which is composed of the content view (i.e., what\npersonal experiences are described) and the emotion view (i.e., the feelings of\nthe speaker on these experiences). To this end, we design a framework to model\nthe Content-Emotion Duality (CEDual) via disentanglement for empathetic\nresponse generation. With disentanglement, we encode the dialogue history from\nboth the content and emotion views, and then generate the empathetic response\nbased on the disentangled representations, thereby both the content and emotion\ninformation of the dialogue history can be embedded in the generated response.\nThe experiments on the benchmark dataset EMPATHETICDIALOGUES show that the\nCEDual model achieves state-of-the-art performance on both automatic and human\nmetrics, and it also generates more empathetic responses than previous methods.", "published": "2022-09-26 08:07:55", "link": "http://arxiv.org/abs/2209.12495v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-end Multilingual Coreference Resolution with Mention Head\n  Prediction", "abstract": "This paper describes our approach to the CRAC 2022 Shared Task on\nMultilingual Coreference Resolution. Our model is based on a state-of-the-art\nend-to-end coreference resolution system. Apart from joined multilingual\ntraining, we improved our results with mention head prediction. We also tried\nto integrate dependency information into our model. Our system ended up in\n$3^{rd}$ place. Moreover, we reached the best performance on two datasets out\nof 13.", "published": "2022-09-26 08:45:09", "link": "http://arxiv.org/abs/2209.12516v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meta-Learning a Cross-lingual Manifold for Semantic Parsing", "abstract": "Localizing a semantic parser to support new languages requires effective\ncross-lingual generalization. Recent work has found success with\nmachine-translation or zero-shot methods although these approaches can struggle\nto model how native speakers ask questions. We consider how to effectively\nleverage minimal annotated examples in new languages for few-shot cross-lingual\nsemantic parsing. We introduce a first-order meta-learning algorithm to train a\nsemantic parser with maximal sample efficiency during cross-lingual transfer.\nOur algorithm uses high-resource languages to train the parser and\nsimultaneously optimizes for cross-lingual generalization for lower-resource\nlanguages. Results across six languages on ATIS demonstrate that our\ncombination of generalization steps yields accurate semantic parsers sampling\n$\\le$10% of source training data in each new language. Our approach also trains\na competitive model on Spider using English with generalization to Chinese\nsimilarly sampling $\\le$10% of training data.", "published": "2022-09-26 10:42:17", "link": "http://arxiv.org/abs/2209.12577v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models Truly Understand Prompts? A Case Study with\n  Negated Prompts", "abstract": "Previous work has shown that there exists a scaling law between the size of\nLanguage Models (LMs) and their zero-shot performance on different downstream\nNLP tasks. In this work, we show that this phenomenon does not hold when\nevaluating large LMs on tasks with negated prompts, but instead shows an\ninverse scaling law. We evaluate 9 different tasks with negated prompts on (1)\npretrained LMs (OPT & GPT-3) of varying sizes (125M - 175B), (2) LMs further\npretrained to generalize to novel prompts (InstructGPT), (3) LMs provided with\nfew-shot examples, and (4) LMs fine-tuned specifically on negated prompts; all\nLM types perform worse on negated prompts as they scale and show a huge\nperformance gap between the human performance when comparing the average score\non both original and negated prompts. By highlighting a critical limitation of\nexisting LMs and methods, we urge the community to develop new approaches of\ndeveloping LMs that actually follow the given instructions. We provide the code\nand the datasets to explore negated prompts at\nhttps://github.com/joeljang/negated-prompts-for-llms", "published": "2022-09-26 14:05:10", "link": "http://arxiv.org/abs/2209.12711v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Informative Text Generation from Knowledge Triples", "abstract": "As the development of the encoder-decoder architecture, researchers are able\nto study the text generation tasks with broader types of data. Among them,\nKB-to-text aims at converting a set of knowledge triples into human readable\nsentences. In the original setting, the task assumes that the input triples and\nthe text are exactly aligned in the perspective of the embodied\nknowledge/information. In this paper, we extend this setting and explore how to\nfacilitate the trained model to generate more informative text, namely,\ncontaining more information about the triple entities but not conveyed by the\ninput triples. To solve this problem, we propose a novel memory augmented\ngenerator that employs a memory network to memorize the useful knowledge\nlearned during the training and utilizes such information together with the\ninput triples to generate text in the operational or testing phase. We derive a\ndataset from WebNLG for our new setting and conduct extensive experiments to\ninvestigate the effectiveness of our model as well as uncover the intrinsic\ncharacteristics of the setting.", "published": "2022-09-26 14:35:57", "link": "http://arxiv.org/abs/2209.12733v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lex2Sent: A bagging approach to unsupervised sentiment analysis", "abstract": "Unsupervised text classification, with its most common form being sentiment\nanalysis, used to be performed by counting words in a text that were stored in\na lexicon, which assigns each word to one class or as a neutral word. In recent\nyears, these lexicon-based methods fell out of favor and were replaced by\ncomputationally demanding fine-tuning techniques for encoder-only models such\nas BERT and zero-shot classification using decoder-only models such as GPT-4.\nIn this paper, we propose an alternative approach: Lex2Sent, which provides\nimprovement over classic lexicon methods but does not require any GPU or\nexternal hardware. To classify texts, we train embedding models to determine\nthe distances between document embeddings and the embeddings of the parts of a\nsuitable lexicon. We employ resampling, which results in a bagging effect,\nboosting the performance of the classification. We show that our model\noutperforms lexica and provides a basis for a high performing few-shot\nfine-tuning approach in the task of binary sentiment analysis.", "published": "2022-09-26 20:49:18", "link": "http://arxiv.org/abs/2209.13023v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "5-Star Hotel Customer Satisfaction Analysis Using Hybrid Methodology", "abstract": "Due to the rapid development of non-face-to-face services due to the corona\nvirus, commerce through the Internet, such as sales and reservations, is\nincreasing very rapidly. Consumers also post reviews, suggestions, or judgments\nabout goods or services on the website. The review data directly used by\nconsumers provides positive feedback and nice impact to consumers, such as\ncreating business value. Therefore, analysing review data is very important\nfrom a marketing point of view. Our research suggests a new way to find factors\nfor customer satisfaction through review data. We applied a method to find\nfactors for customer satisfaction by mixing and using the data mining\ntechnique, which is a big data analysis method, and the natural language\nprocessing technique, which is a language processing method, in our research.\nUnlike many studies on customer satisfaction that have been conducted in the\npast, our research has a novelty of the thesis by using various techniques. And\nas a result of the analysis, the results of our experiments were very accurate.", "published": "2022-09-26 04:53:10", "link": "http://arxiv.org/abs/2209.12417v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Text Summarization with Oracle Expectation", "abstract": "Extractive summarization produces summaries by identifying and concatenating\nthe most important sentences in a document. Since most summarization datasets\ndo not come with gold labels indicating whether document sentences are\nsummary-worthy, different labeling algorithms have been proposed to extrapolate\noracle extracts for model training. In this work, we identify two flaws with\nthe widely used greedy labeling approach: it delivers suboptimal and\ndeterministic oracles. To alleviate both issues, we propose a simple yet\neffective labeling algorithm that creates soft, expectation-based sentence\nlabels. We define a new learning objective for extractive summarization which\nincorporates learning signals from multiple oracle summaries and prove it is\nequivalent to estimating the oracle expectation for each document sentence.\nWithout any architectural modifications, the proposed labeling scheme achieves\nsuperior performance on a variety of summarization benchmarks across domains\nand languages, in both supervised and zero-shot settings.", "published": "2022-09-26 14:10:08", "link": "http://arxiv.org/abs/2209.12714v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Overcoming Referential Ambiguity in Language-Guided Goal-Conditioned\n  Reinforcement Learning", "abstract": "Teaching an agent to perform new tasks using natural language can easily be\nhindered by ambiguities in interpretation. When a teacher provides an\ninstruction to a learner about an object by referring to its features, the\nlearner can misunderstand the teacher's intentions, for instance if the\ninstruction ambiguously refer to features of the object, a phenomenon called\nreferential ambiguity. We study how two concepts derived from cognitive\nsciences can help resolve those referential ambiguities: pedagogy (selecting\nthe right instructions) and pragmatism (learning the preferences of the other\nagents using inductive reasoning). We apply those ideas to a teacher/learner\nsetup with two artificial agents on a simulated robotic task (block-stacking).\nWe show that these concepts improve sample efficiency for training the learner.", "published": "2022-09-26 15:07:59", "link": "http://arxiv.org/abs/2209.12758v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Do ever larger octopi still amplify reporting biases? Evidence from\n  judgments of typical colour", "abstract": "Language models (LMs) trained on raw texts have no direct access to the\nphysical world. Gordon and Van Durme (2013) point out that LMs can thus suffer\nfrom reporting bias: texts rarely report on common facts, instead focusing on\nthe unusual aspects of a situation. If LMs are only trained on text corpora and\nnaively memorise local co-occurrence statistics, they thus naturally would\nlearn a biased view of the physical world. While prior studies have repeatedly\nverified that LMs of smaller scales (e.g., RoBERTa, GPT-2) amplify reporting\nbias, it remains unknown whether such trends continue when models are scaled\nup. We investigate reporting bias from the perspective of colour in larger\nlanguage models (LLMs) such as PaLM and GPT-3. Specifically, we query LLMs for\nthe typical colour of objects, which is one simple type of perceptually\ngrounded physical common sense. Surprisingly, we find that LLMs significantly\noutperform smaller LMs in determining an object's typical colour and more\nclosely track human judgments, instead of overfitting to surface patterns\nstored in texts. This suggests that very large models of language alone are\nable to overcome certain types of reporting bias that are characterized by\nlocal co-occurrences.", "published": "2022-09-26 15:45:23", "link": "http://arxiv.org/abs/2209.12786v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Word to Sentence Visual Semantic Similarity for Caption Generation:\n  Lessons Learned", "abstract": "This paper focuses on enhancing the captions generated by image-caption\ngeneration systems. We propose an approach for improving caption generation\nsystems by choosing the most closely related output to the image rather than\nthe most likely output produced by the model. Our model revises the language\ngeneration output beam search from a visual context perspective. We employ a\nvisual semantic measure in a word and sentence level manner to match the proper\ncaption to the related information in the image. The proposed approach can be\napplied to any caption system as a post-processing based method.", "published": "2022-09-26 16:24:13", "link": "http://arxiv.org/abs/2209.12817v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Towards Simple and Efficient Task-Adaptive Pre-training for Text\n  Classification", "abstract": "Language models are pre-trained using large corpora of generic data like book\ncorpus, common crawl and Wikipedia, which is essential for the model to\nunderstand the linguistic characteristics of the language. New studies suggest\nusing Domain Adaptive Pre-training (DAPT) and Task-Adaptive Pre-training (TAPT)\nas an intermediate step before the final finetuning task. This step helps cover\nthe target domain vocabulary and improves the model performance on the\ndownstream task. In this work, we study the impact of training only the\nembedding layer on the model's performance during TAPT and task-specific\nfinetuning. Based on our study, we propose a simple approach to make the\nintermediate step of TAPT for BERT-based models more efficient by performing\nselective pre-training of BERT layers. We show that training only the BERT\nembedding layer during TAPT is sufficient to adapt to the vocabulary of the\ntarget domain and achieve comparable performance. Our approach is\ncomputationally efficient, with 78\\% fewer parameters trained during TAPT. The\nproposed embedding layer finetuning approach can also be an efficient domain\nadaptation technique.", "published": "2022-09-26 18:29:12", "link": "http://arxiv.org/abs/2209.12943v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Impact of Speech Recognition Errors in Passage Retrieval for\n  Spoken Question Answering", "abstract": "Interacting with a speech interface to query a Question Answering (QA) system\nis becoming increasingly popular. Typically, QA systems rely on passage\nretrieval to select candidate contexts and reading comprehension to extract the\nfinal answer. While there has been some attention to improving the reading\ncomprehension part of QA systems against errors that automatic speech\nrecognition (ASR) models introduce, the passage retrieval part remains\nunexplored. However, such errors can affect the performance of passage\nretrieval, leading to inferior end-to-end performance. To address this gap, we\naugment two existing large-scale passage ranking and open domain QA datasets\nwith synthetic ASR noise and study the robustness of lexical and dense\nretrievers against questions with ASR noise. Furthermore, we study the\ngeneralizability of data augmentation techniques across different domains; with\neach domain being a different language dialect or accent. Finally, we create a\nnew dataset with questions voiced by human users and use their transcriptions\nto show that the retrieval performance can further degrade when dealing with\nnatural ASR noise instead of synthetic ASR noise.", "published": "2022-09-26 18:29:36", "link": "http://arxiv.org/abs/2209.12944v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Dialog Acts for Task-Driven Embodied Agents", "abstract": "Embodied agents need to be able to interact in natural language understanding\ntask descriptions and asking appropriate follow up questions to obtain\nnecessary information to be effective at successfully accomplishing tasks for a\nwide range of users. In this work, we propose a set of dialog acts for\nmodelling such dialogs and annotate the TEACh dataset that includes over 3,000\nsituated, task oriented conversations (consisting of 39.5k utterances in total)\nwith dialog acts. TEACh-DA is one of the first large scale dataset of dialog\nact annotations for embodied task completion. Furthermore, we demonstrate the\nuse of this annotated dataset in training models for tagging the dialog acts of\na given utterance, predicting the dialog act of the next response given a\ndialog history, and use the dialog acts to guide agent's non-dialog behaviour.\nIn particular, our experiments on the TEACh Execution from Dialog History task\nwhere the model predicts the sequence of low level actions to be executed in\nthe environment for embodied task completion, demonstrate that dialog acts can\nimprove end task success rate by up to 2 points compared to the system without\ndialog acts.", "published": "2022-09-26 18:41:28", "link": "http://arxiv.org/abs/2209.12953v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Towards Parameter-Efficient Integration of Pre-Trained Language Models\n  In Temporal Video Grounding", "abstract": "This paper explores the task of Temporal Video Grounding (TVG) where, given\nan untrimmed video and a natural language sentence query, the goal is to\nrecognize and determine temporal boundaries of action instances in the video\ndescribed by the query. Recent works tackled this task by improving query\ninputs with large pre-trained language models (PLM) at the cost of more\nexpensive training. However, the effects of this integration are unclear, as\nthese works also propose improvements in the visual inputs. Therefore, this\npaper studies the effects of PLMs in TVG and assesses the applicability of\nparameter-efficient training with NLP adapters. We couple popular PLMs with a\nselection of existing approaches and test different adapters to reduce the\nimpact of the additional parameters. Our results on three challenging datasets\nshow that, without changing the visual inputs, TVG models greatly benefited\nfrom the PLM integration and fine-tuning, stressing the importance of sentence\nquery representation in this task. Furthermore, NLP adapters were an effective\nalternative to full fine-tuning, even though they were not tailored to our\ntask, allowing PLM integration in larger TVG models and delivering results\ncomparable to SOTA models. Finally, our results shed light on which adapters\nwork best in different scenarios.", "published": "2022-09-26 08:11:19", "link": "http://arxiv.org/abs/2209.13359v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Two-Tailed Averaging: Anytime, Adaptive, Once-in-a-While Optimal Weight\n  Averaging for Better Generalization", "abstract": "Tail Averaging improves on Polyak averaging's non-asymptotic behaviour by\nexcluding a number of leading iterates of stochastic optimization from its\ncalculations. In practice, with a finite number of optimization steps and a\nlearning rate that cannot be annealed to zero, Tail Averaging can get much\ncloser to a local minimum point of the training loss than either the individual\niterates or the Polyak average. However, the number of leading iterates to\nignore is an important hyperparameter, and starting averaging too early or too\nlate leads to inefficient use of resources or suboptimal solutions. Our work\nfocusses on improving generalization, which makes setting this hyperparameter\neven more difficult, especially in the presence of other hyperparameters and\noverfitting. Furthermore, before averaging starts, the loss is only weakly\ninformative of the final performance, which makes early stopping unreliable. To\nalleviate these problems, we propose an anytime variant of Tail Averaging\nintended for improving generalization not pure optimization, that has no\nhyperparameters and approximates the optimal tail at all optimization steps.\nOur algorithm is based on two running averages with adaptive lengths bounded in\nterms of the optimal tail length, one of which achieves approximate optimality\nwith some regularity. Requiring only the additional storage for two sets of\nweights and periodic evaluation of the loss, the proposed Two-Tailed Averaging\nalgorithm is a practical and widely applicable method for improving\ngeneralization.", "published": "2022-09-26 10:46:37", "link": "http://arxiv.org/abs/2209.12581v3", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Effects of language mismatch in automatic forensic voice comparison\n  using deep learning embeddings", "abstract": "In forensic voice comparison the speaker embedding has become widely popular\nin the last 10 years. Most of the pretrained speaker embeddings are trained on\nEnglish corpora, because it is easily accessible. Thus, language dependency can\nbe an important factor in automatic forensic voice comparison, especially when\nthe target language is linguistically very different. There are numerous\ncommercial systems available, but their models are mainly trained on a\ndifferent language (mostly English) than the target language. In the case of a\nlow-resource language, developing a corpus for forensic purposes containing\nenough speakers to train deep learning models is costly. This study aims to\ninvestigate whether a model pre-trained on English corpus can be used on a\ntarget low-resource language (here, Hungarian), different from the model is\ntrained on. Also, often multiple samples are not available from the offender\n(unknown speaker). Therefore, samples are compared pairwise with and without\nspeaker enrollment for suspect (known) speakers. Two corpora are applied that\nwere developed especially for forensic purposes, and a third that is meant for\ntraditional speaker verification. Two deep learning based speaker embedding\nvector extraction methods are used: the x-vector and ECAPA-TDNN. Speaker\nverification was evaluated in the likelihood-ratio framework. A comparison is\nmade between the language combinations (modeling, LR calibration, evaluation).\nThe results were evaluated by minCllr and EER metrics. It was found that the\nmodel pre-trained on a different language but on a corpus with a huge amount of\nspeakers performs well on samples with language mismatch. The effect of sample\ndurations and speaking styles were also examined. It was found that the longer\nthe duration of the sample in question the better the performance is. Also,\nthere is no real difference if various speaking styles are applied.", "published": "2022-09-26 11:49:37", "link": "http://arxiv.org/abs/2209.12602v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Fine-Dining Recipe Generation with Generative Pre-trained\n  Transformers", "abstract": "Food is essential to human survival. So much so that we have developed\ndifferent recipes to suit our taste needs. In this work, we propose a novel way\nof creating new, fine-dining recipes from scratch using Transformers,\nspecifically auto-regressive language models. Given a small dataset of food\nrecipes, we try to train models to identify cooking techniques, propose novel\nrecipes, and test the power of fine-tuning with minimal data.", "published": "2022-09-26 15:33:09", "link": "http://arxiv.org/abs/2209.12774v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fast-FNet: Accelerating Transformer Encoder Models via Efficient Fourier\n  Layers", "abstract": "Transformer-based language models utilize the attention mechanism for\nsubstantial performance improvements in almost all natural language processing\n(NLP) tasks. Similar attention structures are also extensively studied in\nseveral other areas. Although the attention mechanism enhances the model\nperformances significantly, its quadratic complexity prevents efficient\nprocessing of long sequences. Recent works focused on eliminating the\ndisadvantages of computational inefficiency and showed that transformer-based\nmodels can still reach competitive results without the attention layer. A\npioneering study proposed the FNet, which replaces the attention layer with the\nFourier Transform (FT) in the transformer encoder architecture. FNet achieves\ncompetitive performances concerning the original transformer encoder model\nwhile accelerating training process by removing the computational burden of the\nattention mechanism. However, the FNet model ignores essential properties of\nthe FT from the classical signal processing that can be leveraged to increase\nmodel efficiency further. We propose different methods to deploy FT efficiently\nin transformer encoder models. Our proposed architectures have smaller number\nof model parameters, shorter training times, less memory usage, and some\nadditional performance improvements. We demonstrate these improvements through\nextensive experiments on common benchmarks.", "published": "2022-09-26 16:23:02", "link": "http://arxiv.org/abs/2209.12816v2", "categories": ["cs.CL", "cs.AI", "cs.GL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Adaptation of Autoencoder for Sparsity Reduction From Clinical Notes\n  Representation Learning", "abstract": "When dealing with clinical text classification on a small dataset recent\nstudies have confirmed that a well-tuned multilayer perceptron outperforms\nother generative classifiers, including deep learning ones. To increase the\nperformance of the neural network classifier, feature selection for the\nlearning representation can effectively be used. However, most feature\nselection methods only estimate the degree of linear dependency between\nvariables and select the best features based on univariate statistical tests.\nFurthermore, the sparsity of the feature space involved in the learning\nrepresentation is ignored. Goal: Our aim is therefore to access an alternative\napproach to tackle the sparsity by compressing the clinical representation\nfeature space, where limited French clinical notes can also be dealt with\neffectively. Methods: This study proposed an autoencoder learning algorithm to\ntake advantage of sparsity reduction in clinical note representation. The\nmotivation was to determine how to compress sparse, high-dimensional data by\nreducing the dimension of the clinical note representation feature space. The\nclassification performance of the classifiers was then evaluated in the trained\nand compressed feature space. Results: The proposed approach provided overall\nperformance gains of up to 3% for each evaluation. Finally, the classifier\nachieved a 92% accuracy, 91% recall, 91% precision, and 91% f1-score in\ndetecting the patient's condition. Furthermore, the compression working\nmechanism and the autoencoder prediction process were demonstrated by applying\nthe theoretic information bottleneck framework.", "published": "2022-09-26 16:37:37", "link": "http://arxiv.org/abs/2209.12831v1", "categories": ["cs.LG", "cs.CL", "eess.SP"], "primary_category": "cs.LG"}
{"title": "The Efficacy of Self-Supervised Speech Models for Audio Representations", "abstract": "Self-supervised learning (SSL) speech models, which can serve as powerful\nupstream models to extract meaningful speech representations, have achieved\nunprecedented success in speech representation learning. However, their\neffectiveness on non-speech datasets is relatively less explored. In this work,\nwe propose an ensemble framework, with a combination of ensemble techniques, to\nfuse SSL speech models' embeddings. Extensive experiments on speech and\nnon-speech audio datasets are conducted to investigate the representation\nabilities of our ensemble method and its single constituent model. Ablation\nstudies are carried out to evaluate the performances of different ensemble\ntechniques, such as feature averaging and concatenation. All experiments are\nconducted during NeurIPS 2021 HEAR Challenge as a standard evaluation pipeline\nprovided by competition officials. Results demonstrate SSL speech models'\nstrong abilities on various non-speech tasks, while we also note that they fail\nto deal with fine-grained music tasks, such as pitch classification and note\nonset detection. In addition, feature ensemble is shown to have great potential\non producing more holistic representations, as our proposed framework generally\nsurpasses state-of-the-art SSL speech/audio models and has superior performance\non various datasets compared with other teams in HEAR Challenge. Our code is\navailable at https://github.com/tony10101105/HEAR-2021-NeurIPS-Challenge --\nNTU-GURA.", "published": "2022-09-26 15:21:06", "link": "http://arxiv.org/abs/2209.12900v3", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Cross-lingual Dysarthria Severity Classification for English, Korean,\n  and Tamil", "abstract": "This paper proposes a cross-lingual classification method for English,\nKorean, and Tamil, which employs both language-independent features and\nlanguage-unique features. First, we extract thirty-nine features from diverse\nspeech dimensions such as voice quality, pronunciation, and prosody. Second,\nfeature selections are applied to identify the optimal feature set for each\nlanguage. A set of shared features and a set of distinctive features are\ndistinguished by comparing the feature selection results of the three\nlanguages. Lastly, automatic severity classification is performed, utilizing\nthe two feature sets. Notably, the proposed method removes different features\nby languages to prevent the negative effect of unique features for other\nlanguages. Accordingly, eXtreme Gradient Boosting (XGBoost) algorithm is\nemployed for classification, due to its strength in imputing missing data. In\norder to validate the effectiveness of our proposed method, two baseline\nexperiments are conducted: experiments using the intersection set of\nmono-lingual feature sets (Intersection) and experiments using the union set of\nmono-lingual feature sets (Union). According to the experimental results, our\nmethod achieves better performance with a 67.14% F1 score, compared to 64.52%\nfor the Intersection experiment and 66.74% for the Union experiment. Further,\nthe proposed method attains better performances than mono-lingual\nclassifications for all three languages, achieving 17.67%, 2.28%, 7.79%\nrelative percentage increases for English, Korean, and Tamil, respectively. The\nresult specifies that commonly shared features and language-specific features\nmust be considered separately for cross-language dysarthria severity\nclassification.", "published": "2022-09-26 18:28:15", "link": "http://arxiv.org/abs/2209.12942v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "TaskMix: Data Augmentation for Meta-Learning of Spoken Intent\n  Understanding", "abstract": "Meta-Learning has emerged as a research direction to better transfer\nknowledge from related tasks to unseen but related tasks. However,\nMeta-Learning requires many training tasks to learn representations that\ntransfer well to unseen tasks; otherwise, it leads to overfitting, and the\nperformance degenerates to worse than Multi-task Learning. We show that a\nstate-of-the-art data augmentation method worsens this problem of overfitting\nwhen the task diversity is low. We propose a simple method, TaskMix, which\nsynthesizes new tasks by linearly interpolating existing tasks. We compare\nTaskMix against many baselines on an in-house multilingual intent\nclassification dataset of N-Best ASR hypotheses derived from real-life\nhuman-machine telephony utterances and two datasets derived from MTOP. We show\nthat TaskMix outperforms baselines, alleviates overfitting when task diversity\nis low, and does not degrade performance even when it is high.", "published": "2022-09-26 00:37:40", "link": "http://arxiv.org/abs/2210.06341v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Liquid Structural State-Space Models", "abstract": "A proper parametrization of state transition matrices of linear state-space\nmodels (SSMs) followed by standard nonlinearities enables them to efficiently\nlearn representations from sequential data, establishing the state-of-the-art\non a large series of long-range sequence modeling benchmarks. In this paper, we\nshow that we can improve further when the structural SSM such as S4 is given by\na linear liquid time-constant (LTC) state-space model. LTC neural networks are\ncausal continuous-time neural networks with an input-dependent state transition\nmodule, which makes them learn to adapt to incoming inputs at inference. We\nshow that by using a diagonal plus low-rank decomposition of the state\ntransition matrix introduced in S4, and a few simplifications, the LTC-based\nstructural state-space model, dubbed Liquid-S4, achieves the new\nstate-of-the-art generalization across sequence modeling tasks with long-term\ndependencies such as image, text, audio, and medical time-series, with an\naverage performance of 87.32% on the Long-Range Arena benchmark. On the full\nraw Speech Command recognition, dataset Liquid-S4 achieves 96.78% accuracy with\na 30% reduction in parameter counts compared to S4. The additional gain in\nperformance is the direct result of the Liquid-S4's kernel structure that takes\ninto account the similarities of the input sequence samples during training and\ninference.", "published": "2022-09-26 18:37:13", "link": "http://arxiv.org/abs/2209.12951v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.NE"], "primary_category": "cs.LG"}
{"title": "End-to-End Lyrics Recognition with Self-supervised Learning", "abstract": "Lyrics recognition is an important task in music processing. Despite\ntraditional algorithms such as the hybrid HMM- TDNN model achieving good\nperformance, studies on applying end-to-end models and self-supervised learning\n(SSL) are limited. In this paper, we first establish an end-to-end baseline for\nlyrics recognition and then explore the performance of SSL models on lyrics\nrecognition task. We evaluate a variety of upstream SSL models with different\ntraining methods (masked reconstruction, masked prediction, autoregressive\nreconstruction, and contrastive learning). Our end-to-end self-supervised\nmodels, evaluated on the DAMP music dataset, outperform the previous\nstate-of-the-art (SOTA) system by 5.23% for the dev set and 2.4% for the test\nset even without a language model trained by a large corpus. Moreover, we\ninvestigate the effect of background music on the performance of\nself-supervised learning models and conclude that the SSL models cannot extract\nfeatures efficiently in the presence of background music. Finally, we study the\nout-of-domain generalization ability of the SSL features considering that those\nmodels were not trained on music datasets.", "published": "2022-09-26 13:51:56", "link": "http://arxiv.org/abs/2209.12702v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Impact of temporal resolution on convolutional recurrent networks for\n  audio tagging and sound event detection", "abstract": "Many state-of-the-art systems for audio tagging and sound event detection\nemploy convolutional recurrent neural architectures. Typically, they are\ntrained in a mean teacher setting to deal with the heterogeneous annotation of\nthe available data.\n  In this work, we present a thorough analysis of how changing the temporal\nresolution of these convolutional recurrent neural networks - which can be done\nby simply adapting their pooling operations - impacts their performance. By\nusing a variety of evaluation metrics, we investigate the effects of adapting\nthis design parameter under several sound recognition scenarios involving\ndifferent needs in terms of temporal localization.", "published": "2022-09-26 16:49:12", "link": "http://arxiv.org/abs/2209.12843v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-Task Adversarial Training Algorithm for Multi-Speaker Neural\n  Text-to-Speech", "abstract": "We propose a novel training algorithm for a multi-speaker neural\ntext-to-speech (TTS) model based on multi-task adversarial training. A\nconventional generative adversarial network (GAN)-based training algorithm\nsignificantly improves the quality of synthetic speech by reducing the\nstatistical difference between natural and synthetic speech. However, the\nalgorithm does not guarantee the generalization performance of the trained TTS\nmodel in synthesizing voices of unseen speakers who are not included in the\ntraining data. Our algorithm alternatively trains two deep neural networks:\nmulti-task discriminator and multi-speaker neural TTS model (i.e., generator of\nGANs). The discriminator is trained not only to distinguish between natural and\nsynthetic speech but also to verify the speaker of input speech is existent or\nnon-existent (i.e., newly generated by interpolating seen speakers' embedding\nvectors). Meanwhile, the generator is trained to minimize the weighted sum of\nthe speech reconstruction loss and adversarial loss for fooling the\ndiscriminator, which achieves high-quality multi-speaker TTS even if the target\nspeaker is unseen. Experimental evaluation shows that our algorithm improves\nthe quality of synthetic speech better than a conventional GANSpeech algorithm.", "published": "2022-09-26 10:10:40", "link": "http://arxiv.org/abs/2209.12549v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-encoder attention-based architectures for sound recognition with\n  partial visual assistance", "abstract": "Large-scale sound recognition data sets typically consist of acoustic\nrecordings obtained from multimedia libraries. As a consequence, modalities\nother than audio can often be exploited to improve the outputs of models\ndesigned for associated tasks. Frequently, however, not all contents are\navailable for all samples of such a collection: For example, the original\nmaterial may have been removed from the source platform at some point, and\ntherefore, non-auditory features can no longer be acquired.\n  We demonstrate that a multi-encoder framework can be employed to deal with\nthis issue by applying this method to attention-based deep learning systems,\nwhich are currently part of the state of the art in the domain of sound\nrecognition. More specifically, we show that the proposed model extension can\nsuccessfully be utilized to incorporate partially available visual information\ninto the operational procedures of such networks, which normally only use\nauditory features during training and inference. Experimentally, we verify that\nthe considered approach leads to improved predictions in a number of evaluation\nscenarios pertaining to audio tagging and sound event detection. Additionally,\nwe scrutinize some properties and limitations of the presented technique.", "published": "2022-09-26 16:32:33", "link": "http://arxiv.org/abs/2209.12826v1", "categories": ["eess.AS", "cs.CV", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Text Independent Speaker Identification System for Access Control", "abstract": "Even human intelligence system fails to offer 100% accuracy in identifying\nspeeches from a specific individual. Machine intelligence is trying to mimic\nhumans in speaker identification problems through various approaches to speech\nfeature extraction and speech modeling techniques. This paper presents a\ntext-independent speaker identification system that employs Mel Frequency\nCepstral Coefficients (MFCC) for feature extraction and k-Nearest Neighbor\n(kNN) for classification. The maximum cross-validation accuracy obtained was\n60%. This will be improved upon in subsequent research.", "published": "2022-09-26 14:42:18", "link": "http://arxiv.org/abs/2209.14335v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Faked Speech Detection with Zero Prior Knowledge", "abstract": "Audio is one of the most used ways of human communication, but at the same\ntime it can be easily misused to trick people. With the revolution of AI, the\nrelated technologies are now accessible to almost everyone, thus making it\nsimple for the criminals to commit crimes and forgeries. In this work, we\nintroduce a neural network method to develop a classifier that will blindly\nclassify an input audio as real or mimicked; the word 'blindly' refers to the\nability to detect mimicked audio without references or real sources. We propose\na deep neural network following a sequential model that comprises three hidden\nlayers, with alternating dense and drop out layers. The proposed model was\ntrained on a set of 26 important features extracted from a large dataset of\naudios to get a classifier that was tested on the same set of features from\ndifferent audios. The data was extracted from two raw datasets, especially\ncomposed for this work; an all English dataset and a mixed dataset (Arabic plus\nEnglish) (The dataset can be provided, in raw form, by writing an email to the\nfirst author). For the purpose of comparison, the audios were also classified\nthrough human inspection with the subjects being the native speakers. The\nensued results were interesting and exhibited formidable accuracy, as we were\nable to get at least 94% correct classification of the test cases, as against\nthe 85% accuracy in the case of human observers.", "published": "2022-09-26 10:38:39", "link": "http://arxiv.org/abs/2209.12573v6", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "cs.NE", "eess.AS", "68T05, 68T07, 68T10", "I.2; I.5; I.m"], "primary_category": "cs.SD"}
