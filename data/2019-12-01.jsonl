{"title": "Machines Getting with the Program: Understanding Intent Arguments of\n  Non-Canonical Directives", "abstract": "Modern dialog managers face the challenge of having to fulfill human-level\nconversational skills as part of common user expectations, including but not\nlimited to discourse with no clear objective. Along with these requirements,\nagents are expected to extrapolate intent from the user's dialogue even when\nsubjected to non-canonical forms of speech. This depends on the agent's\ncomprehension of paraphrased forms of such utterances. Especially in\nlow-resource languages, the lack of data is a bottleneck that prevents\nadvancements of the comprehension performance for these types of agents. In\nthis regard, here we demonstrate the necessity of extracting the intent\nargument of non-canonical directives in a natural language format, which may\nyield more accurate parsing, and suggest guidelines for building a parallel\ncorpus for this purpose. Following the guidelines, we construct a Korean corpus\nof 50K instances of question/command-intent pairs, including the labels for\nclassification of the utterance type. We also propose a method for mitigating\nclass imbalance, demonstrating the potential applications of the corpus\ngeneration method and its multilingual extensibility.", "published": "2019-12-01 07:08:19", "link": "http://arxiv.org/abs/1912.00342v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HSCJN: A Holistic Semantic Constraint Joint Network for Diverse Response\n  Generation", "abstract": "The sequence-to-sequence (Seq2Seq) model generates target words iteratively\ngiven the previously observed words during decoding process, which results in\nthe loss of the holistic semantics in the target response and the complete\nsemantic relationship between responses and dialogue histories. In this paper,\nwe propose a generic diversity-promoting joint network, called Holistic\nSemantic Constraint Joint Network (HSCJN), enhancing the global sentence\ninformation, and then regularizing the objective function with penalizing the\nlow entropy output. Our network introduces more target information to improve\ndiversity, and captures direct semantic information to better constrain the\nrelevance simultaneously. Moreover, the proposed method can be easily applied\nto any Seq2Seq structure. Extensive experiments on several dialogue corpuses\nshow that our method effectively improves both semantic consistency and\ndiversity of generated responses, and achieves better performance than other\ncompetitive methods.", "published": "2019-12-01 10:41:42", "link": "http://arxiv.org/abs/1912.00380v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-supervised Visual Feature Integration for Pre-trained Language\n  Models", "abstract": "Integrating visual features has been proved useful for natural language\nunderstanding tasks. Nevertheless, in most existing multimodal language models,\nthe alignment of visual and textual data is expensive. In this paper, we\npropose a novel semi-supervised visual integration framework for pre-trained\nlanguage models. In the framework, the visual features are obtained through a\nvisualization and fusion mechanism. The uniqueness includes: 1) the integration\nis conducted via a semi-supervised approach, which does not require aligned\nimages for every sentences 2) the visual features are integrated as an external\ncomponent and can be directly used by pre-trained language models. To verify\nthe efficacy of the proposed framework, we conduct the experiments on both\nnatural language inference and reading comprehension tasks. The results\ndemonstrate that our mechanism brings improvement to two strong baseline\nmodels. Considering that our framework only requires an image database, and no\nnot requires further alignments, it provides an efficient and feasible way for\nmultimodal language learning.", "published": "2019-12-01 06:53:23", "link": "http://arxiv.org/abs/1912.00336v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Semantic Enrichment of Streaming Healthcare Data", "abstract": "In the past decade, the healthcare industry has made significant advances in\nthe digitization of patient information. However, a lack of interoperability\namong healthcare systems still imposes a high cost to patients, hospitals, and\ninsurers. Currently, most systems pass messages using idiosyncratic messaging\nstandards that require specialized knowledge to interpret. This increases the\ncost of systems integration and often puts more advanced uses of data out of\nreach. In this project, we demonstrate how two open standards, FHIR and RDF,\ncan be combined both to integrate data from disparate sources in real-time and\nmake that data queryable and susceptible to automated inference. To validate\nthe effectiveness of the semantic engine, we perform simulations of real-time\ndata feeds and demonstrate how they can be combined and used by client-side\napplications with no knowledge of the underlying sources.", "published": "2019-12-01 15:06:44", "link": "http://arxiv.org/abs/1912.00423v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Generalizable prediction of academic performance from short texts on\n  social media", "abstract": "It has already been established that digital traces can be used to predict\nvarious human attributes. In most cases, however, predictive models rely on\nfeatures that are specific to a particular source of digital trace data. In\ncontrast, short texts written by users $-$ tweets, posts, or comments $-$ are\nubiquitous across multiple platforms. In this paper, we explore the predictive\npower of short texts with respect to the academic performance of their authors.\nWe use data from a representative panel of Russian students that includes\ninformation about their educational outcomes and activity on a popular\nnetworking site, VK. We build a model to predict academic performance from\nusers' posts on VK and then apply it to a different context. In particular, we\nshow that the model could reproduce rankings of schools and universities from\nthe posts of their students on social media. We also find that the same model\ncould predict academic performance from tweets as well as from VK posts. The\ngeneralizability of a model trained on a relatively small data set could be\nexplained by the use of continuous word representations trained on a much\nlarger corpus of social media posts. This also allows for greater\ninterpretability of model predictions.", "published": "2019-12-01 18:16:18", "link": "http://arxiv.org/abs/1912.00463v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Speeding up Word Mover's Distance and its variants via properties of\n  distances between embeddings", "abstract": "The Word Mover's Distance (WMD) proposed by Kusner et al. is a distance\nbetween documents that takes advantage of semantic relations among words that\nare captured by their embeddings. This distance proved to be quite effective,\nobtaining state-of-art error rates for classification tasks, but is also\nimpracticable for large collections/documents due to its computational\ncomplexity. For circumventing this problem, variants of WMD have been proposed.\nAmong them, Relaxed Word Mover's Distance (RWMD) is one of the most successful\ndue to its simplicity, effectiveness, and also because of its fast\nimplementations.\n  Relying on assumptions that are supported by empirical properties of the\ndistances between embeddings, we propose an approach to speed up both WMD and\nRWMD. Experiments over 10 datasets suggest that our approach leads to a\nsignificant speed-up in document classification tasks while maintaining the\nsame error rates.", "published": "2019-12-01 22:08:32", "link": "http://arxiv.org/abs/1912.00509v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning to Relate from Captions and Bounding Boxes", "abstract": "In this work, we propose a novel approach that predicts the relationships\nbetween various entities in an image in a weakly supervised manner by relying\non image captions and object bounding box annotations as the sole source of\nsupervision. Our proposed approach uses a top-down attention mechanism to align\nentities in captions to objects in the image, and then leverage the syntactic\nstructure of the captions to align the relations. We use these alignments to\ntrain a relation classification network, thereby obtaining both grounded\ncaptions and dense relationships. We demonstrate the effectiveness of our model\non the Visual Genome dataset by achieving a recall@50 of 15% and recall@100 of\n25% on the relationships present in the image. We also show that the model\nsuccessfully predicts relations that are not present in the corresponding\ncaptions.", "published": "2019-12-01 03:30:00", "link": "http://arxiv.org/abs/1912.00311v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Topic-aware chatbot using Recurrent Neural Networks and Nonnegative\n  Matrix Factorization", "abstract": "We propose a novel model for a topic-aware chatbot by combining the\ntraditional Recurrent Neural Network (RNN) encoder-decoder model with a topic\nattention layer based on Nonnegative Matrix Factorization (NMF). After learning\ntopic vectors from an auxiliary text corpus via NMF, the decoder is trained so\nthat it is more likely to sample response words from the most correlated topic\nvectors. One of the main advantages in our architecture is that the user can\neasily switch the NMF-learned topic vectors so that the chatbot obtains desired\ntopic-awareness. We demonstrate our model by training on a single\nconversational data set which is then augmented with topic matrices learned\nfrom different auxiliary data sets. We show that our topic-aware chatbot not\nonly outperforms the non-topic counterpart, but also that each topic-aware\nmodel qualitatively and contextually gives the most relevant answer depending\non the topic of question.", "published": "2019-12-01 04:22:51", "link": "http://arxiv.org/abs/1912.00315v2", "categories": ["cs.CL", "cs.LG", "math.OC", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Not All Attention Is Needed: Gated Attention Network for Sequence Data", "abstract": "Although deep neural networks generally have fixed network structures, the\nconcept of dynamic mechanism has drawn more and more attention in recent years.\nAttention mechanisms compute input-dependent dynamic attention weights for\naggregating a sequence of hidden states. Dynamic network configuration in\nconvolutional neural networks (CNNs) selectively activates only part of the\nnetwork at a time for different inputs. In this paper, we combine the two\ndynamic mechanisms for text classification tasks. Traditional attention\nmechanisms attend to the whole sequence of hidden states for an input sentence,\nwhile in most cases not all attention is needed especially for long sequences.\nWe propose a novel method called Gated Attention Network (GA-Net) to\ndynamically select a subset of elements to attend to using an auxiliary\nnetwork, and compute attention weights to aggregate the selected elements. It\navoids a significant amount of unnecessary computation on unattended elements,\nand allows the model to pay attention to important parts of the sequence.\nExperiments in various datasets show that the proposed method achieves better\nperformance compared with all baseline models with global or local attention\nwhile requiring less computation and achieving better interpretability. It is\nalso promising to extend the idea to more complex attention-based models, such\nas transformers and seq-to-seq models.", "published": "2019-12-01 07:57:41", "link": "http://arxiv.org/abs/1912.00349v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Deep Human Answer Understanding for Natural Reverse QA", "abstract": "This study focuses on a reverse question answering (QA) procedure, in which\nmachines proactively raise questions and humans supply the answers. This\nprocedure exists in many real human-machine interaction applications. However,\na crucial problem in human-machine interaction is answer understanding. The\nexisting solutions have relied on mandatory option term selection to avoid\nautomatic answer understanding. However, these solutions have led to unnatural\nhuman-computer interaction and negatively affected user experience. To this\nend, the current study proposes a novel deep answer understanding network,\ncalled AntNet, for reverse QA. The network consists of three new modules,\nnamely, skeleton attention for questions, relevance-aware representation of\nanswers, and multi-hop based fusion. As answer understanding for reverse QA has\nnot been explored, a new data corpus is compiled in this study. Experimental\nresults indicate that our proposed network is significantly better than\nexisting methods and those modified from classical natural language processing\ndeep models. The effectiveness of the three new modules is also verified.", "published": "2019-12-01 13:03:03", "link": "http://arxiv.org/abs/1912.00398v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Interpreting Context of Images using Scene Graphs", "abstract": "Understanding a visual scene incorporates objects, relationships, and\ncontext. Traditional methods working on an image mostly focus on object\ndetection and fail to capture the relationship between the objects.\nRelationships can give rich semantic information about the objects in a scene.\nThe context can be conducive to comprehending an image since it will help us to\nperceive the relation between the objects and thus, give us a deeper insight\ninto the image. Through this idea, our project delivers a model that focuses on\nfinding the context present in an image by representing the image as a graph,\nwhere the nodes will the objects and edges will be the relation between them.\nThe context is found using the visual and semantic cues which are further\nconcatenated and given to the Support Vector Machines (SVM) to detect the\nrelation between two objects. This presents us with the context of the image\nwhich can be further used in applications such as similar image retrieval,\nimage captioning, or story generation.", "published": "2019-12-01 21:32:11", "link": "http://arxiv.org/abs/1912.00501v1", "categories": ["cs.CV", "cs.CL", "cs.LG", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Knowledge Infused Learning (K-IL): Towards Deep Incorporation of\n  Knowledge in Deep Learning", "abstract": "Learning the underlying patterns in data goes beyond instance-based\ngeneralization to external knowledge represented in structured graphs or\nnetworks. Deep learning that primarily constitutes neural computing stream in\nAI has shown significant advances in probabilistically learning latent patterns\nusing a multi-layered network of computational nodes (i.e., neurons/hidden\nunits). Structured knowledge that underlies symbolic computing approaches and\noften supports reasoning, has also seen significant growth in recent years, in\nthe form of broad-based (e.g., DBPedia, Yago) and domain, industry or\napplication specific knowledge graphs. A common substrate with careful\nintegration of the two will raise opportunities to develop neuro-symbolic\nlearning approaches for AI, where conceptual and probabilistic representations\nare combined. As the incorporation of external knowledge will aid in\nsupervising the learning of features for the model, deep infusion of\nrepresentational knowledge from knowledge graphs within hidden layers will\nfurther enhance the learning process. Although much work remains, we believe\nthat knowledge graphs will play an increasing role in developing hybrid\nneuro-symbolic intelligent systems (bottom-up deep learning with top-down\nsymbolic computing) as well as in building explainable AI systems for which\nknowledge graphs will provide scaffolding for punctuating neural computing. In\nthis position paper, we describe our motivation for such a neuro-symbolic\napproach and framework that combines knowledge graph and neural networks.", "published": "2019-12-01 22:36:14", "link": "http://arxiv.org/abs/1912.00512v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "V-Shaped Sparse Arrays For 2-D DOA Estimation", "abstract": "This paper proposes a new sparse array geometry for 2-D (azimuth and\nelevation) DOA (direction-of-arrival) estimation. The proposed array geometry\nis V-shaped sparse array and it is composed of two linear portions which are\ncrossing each other. The degrees of freedom of the sparse array is enhanced by\nsparse sampling property. In this respect, V-shaped coprime (VCA) and V-shaped\nnested array (VNA) structures are developed. VCA can resolve both azimuth and\nelevation angles up to MN sources with 2M + N -1 sensors in each portion and\nthe total number of sensors is 4M+2N-3. VNA can resolve O(N^2) sources with 2N\nsensors. Instead of 2-D grid search, the proposed method computes 1-D search\nfor azimuth and elevation angle estimation in a computational efficient way. In\norder to solve the pairing problem in 2-D scenario, the cross-covariance matrix\nof two portion is utilized and 2-D paired DOA estimation is performed. The\nperformance of the proposed method is evaluated with numerical simulations and\nit is shown that the proposed array geometries VCA and VNA can provide much\nless sensors as compared to the conventional coprime planar arrays.", "published": "2019-12-01 09:12:55", "link": "http://arxiv.org/abs/1912.00364v1", "categories": ["eess.SP", "eess.AS"], "primary_category": "eess.SP"}
