{"title": "Corpus-level Fine-grained Entity Typing Using Contextual Information", "abstract": "This paper addresses the problem of corpus-level entity typing, i.e.,\ninferring from a large corpus that an entity is a member of a class such as\n\"food\" or \"artist\". The application of entity typing we are interested in is\nknowledge base completion, specifically, to learn which classes an entity is a\nmember of. We propose FIGMENT to tackle this problem. FIGMENT is\nembedding-based and combines (i) a global model that scores based on aggregated\ncontextual information of an entity and (ii) a context model that first scores\nthe individual occurrences of an entity and then aggregates the scores. In our\nevaluation, FIGMENT strongly outperforms an approach to entity typing that\nrelies on relations obtained by an open information extraction system.", "published": "2016-06-25 12:22:05", "link": "http://arxiv.org/abs/1606.07901v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Intrinsic Subspace Evaluation of Word Embedding Representations", "abstract": "We introduce a new methodology for intrinsic evaluation of word\nrepresentations. Specifically, we identify four fundamental criteria based on\nthe characteristics of natural language that pose difficulties to NLP systems;\nand develop tests that directly show whether or not representations contain the\nsubspaces necessary to satisfy these criteria. Current intrinsic evaluations\nare mostly based on the overall similarity or full-space similarity of words\nand thus view vector representations as points. We show the limits of these\npoint-based intrinsic evaluations. We apply our evaluation methodology to the\ncomparison of a count vector model and several neural network models and\ndemonstrate important properties of these models.", "published": "2016-06-25 12:27:17", "link": "http://arxiv.org/abs/1606.07902v1", "categories": ["cs.CL", "68T50"], "primary_category": "cs.CL"}
{"title": "Word sense disambiguation: a complex network approach", "abstract": "In recent years, concepts and methods of complex networks have been employed\nto tackle the word sense disambiguation (WSD) task by representing words as\nnodes, which are connected if they are semantically similar. Despite the\nincreasingly number of studies carried out with such models, most of them use\nnetworks just to represent the data, while the pattern recognition performed on\nthe attribute space is performed using traditional learning techniques. In\nother words, the structural relationship between words have not been explicitly\nused in the pattern recognition process. In addition, only a few investigations\nhave probed the suitability of representations based on bipartite networks and\ngraphs (bigraphs) for the problem, as many approaches consider all possible\nlinks between words. In this context, we assess the relevance of a bipartite\nnetwork model representing both feature words (i.e. the words characterizing\nthe context) and target (ambiguous) words to solve ambiguities in written\ntexts. Here, we focus on the semantical relationships between these two type of\nwords, disregarding the relationships between feature words. In special, the\nproposed method not only serves to represent texts as graphs, but also\nconstructs a structure on which the discrimination of senses is accomplished.\nOur results revealed that the proposed learning algorithm in such bipartite\nnetworks provides excellent results mostly when topical features are employed\nto characterize the context. Surprisingly, our method even outperformed the\nsupport vector machine algorithm in particular cases, with the advantage of\nbeing robust even if a small training dataset is available. Taken together, the\nresults obtained here show that the proposed representation/classification\nmethod might be useful to improve the semantical characterization of written\ntexts.", "published": "2016-06-25 19:08:19", "link": "http://arxiv.org/abs/1606.07950v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Summarizing Decisions in Spoken Meetings", "abstract": "This paper addresses the problem of summarizing decisions in spoken meetings:\nour goal is to produce a concise {\\it decision abstract} for each meeting\ndecision. We explore and compare token-level and dialogue act-level automatic\nsummarization methods using both unsupervised and supervised learning\nframeworks. In the supervised summarization setting, and given true clusterings\nof decision-related utterances, we find that token-level summaries that employ\ndiscourse context can approach an upper bound for decision abstracts derived\ndirectly from dialogue acts. In the unsupervised summarization setting,we find\nthat summaries based on unsupervised partitioning of decision-related\nutterances perform comparably to those based on partitions generated using\nsupervised techniques (0.22 ROUGE-F1 using LDA-based topic models vs. 0.23\nusing SVMs).", "published": "2016-06-25 20:45:14", "link": "http://arxiv.org/abs/1606.07965v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Semantic Web Search and Browse Sessions for Multi-Turn Spoken\n  Dialog Systems", "abstract": "Training statistical dialog models in spoken dialog systems (SDS) requires\nlarge amounts of annotated data. The lack of scalable methods for data mining\nand annotation poses a significant hurdle for state-of-the-art statistical\ndialog managers. This paper presents an approach that directly leverage\nbillions of web search and browse sessions to overcome this hurdle. The key\ninsight is that task completion through web search and browse sessions is (a)\npredictable and (b) generalizes to spoken dialog task completion. The new\nmethod automatically mines behavioral search and browse patterns from web logs\nand translates them into spoken dialog models. We experiment with naturally\noccurring spoken dialogs and large scale web logs. Our session-based models\noutperform the state-of-the-art method for entity extraction task in SDS. We\nalso achieve better performance for both entity and relation extraction on web\nsearch queries when compared with nontrivial baselines.", "published": "2016-06-25 20:54:49", "link": "http://arxiv.org/abs/1606.07967v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "X575: writing rengas with web services", "abstract": "Our software system simulates the classical collaborative Japanese poetry\nform, renga, made of linked haikus. We used NLP methods wrapped up as web\nservices. Our experiments were only a partial success, since results fail to\nsatisfy classical constraints. To gather ideas for future work, we examine\nrelated research in semiotics, linguistics, and computing.", "published": "2016-06-25 20:04:42", "link": "http://arxiv.org/abs/1606.07955v1", "categories": ["cs.AI", "cs.CL", "I.2.1; J.5"], "primary_category": "cs.AI"}
{"title": "Sequence-Level Knowledge Distillation", "abstract": "Neural machine translation (NMT) offers a novel alternative formulation of\ntranslation that is potentially simpler than statistical approaches. However to\nreach competitive performance, NMT models need to be exceedingly large. In this\npaper we consider applying knowledge distillation approaches (Bucila et al.,\n2006; Hinton et al., 2015) that have proven successful for reducing the size of\nneural models in other domains to the problem of NMT. We demonstrate that\nstandard knowledge distillation applied to word-level prediction can be\neffective for NMT, and also introduce two novel sequence-level versions of\nknowledge distillation that further improve performance, and somewhat\nsurprisingly, seem to eliminate the need for beam search (even when applied on\nthe original teacher model). Our best student model runs 10 times faster than\nits state-of-the-art teacher with little loss in performance. It is also\nsignificantly better than a baseline model trained without knowledge\ndistillation: by 4.2/1.7 BLEU with greedy decoding/beam search. Applying weight\npruning on top of knowledge distillation results in a student model that has 13\ntimes fewer parameters than the original teacher model, with a decrease of 0.4\nBLEU.", "published": "2016-06-25 18:16:39", "link": "http://arxiv.org/abs/1606.07947v4", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Bidirectional Recurrent Neural Networks for Medical Event Detection in\n  Electronic Health Records", "abstract": "Sequence labeling for extraction of medical events and their attributes from\nunstructured text in Electronic Health Record (EHR) notes is a key step towards\nsemantic understanding of EHRs. It has important applications in health\ninformatics including pharmacovigilance and drug surveillance. The state of the\nart supervised machine learning models in this domain are based on Conditional\nRandom Fields (CRFs) with features calculated from fixed context windows. In\nthis application, we explored various recurrent neural network frameworks and\nshow that they significantly outperformed the CRF models.", "published": "2016-06-25 19:46:28", "link": "http://arxiv.org/abs/1606.07953v2", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
