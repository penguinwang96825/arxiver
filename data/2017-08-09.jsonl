{"title": "Recent Trends in Deep Learning Based Natural Language Processing", "abstract": "Deep learning methods employ multiple processing layers to learn hierarchical\nrepresentations of data and have produced state-of-the-art results in many\ndomains. Recently, a variety of model designs and methods have blossomed in the\ncontext of natural language processing (NLP). In this paper, we review\nsignificant deep learning related models and methods that have been employed\nfor numerous NLP tasks and provide a walk-through of their evolution. We also\nsummarize, compare and contrast the various models and put forward a detailed\nunderstanding of the past, present and future of deep learning in NLP.", "published": "2017-08-09 04:02:17", "link": "http://arxiv.org/abs/1708.02709v8", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying Reference Spans: Topic Modeling and Word Embeddings help IR", "abstract": "The CL-SciSumm 2016 shared task introduced an interesting problem: given a\ndocument D and a piece of text that cites D, how do we identify the text spans\nof D being referenced by the piece of text? The shared task provided the first\nannotated dataset for studying this problem. We present an analysis of our\ncontinued work in improving our system's performance on this task. We\ndemonstrate how topic models and word embeddings can be used to surpass the\npreviously best performing system.", "published": "2017-08-09 19:58:55", "link": "http://arxiv.org/abs/1708.02989v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Vector Spaces for Unsupervised Information Retrieval", "abstract": "We propose the Neural Vector Space Model (NVSM), a method that learns\nrepresentations of documents in an unsupervised manner for news article\nretrieval. In the NVSM paradigm, we learn low-dimensional representations of\nwords and documents from scratch using gradient descent and rank documents\naccording to their similarity with query representations that are composed from\nword representations. We show that NVSM performs better at document ranking\nthan existing latent semantic vector space methods. The addition of NVSM to a\nmixture of lexical language models and a state-of-the-art baseline vector space\nmodel yields a statistically significant increase in retrieval effectiveness.\nConsequently, NVSM adds a complementary relevance signal. Next to semantic\nmatching, we find that NVSM performs well in cases where lexical matching is\nneeded.\n  NVSM learns a notion of term specificity directly from the document\ncollection without feature engineering. We also show that NVSM learns\nregularities related to Luhn significance. Finally, we give advice on how to\ndeploy NVSM in situations where model selection (e.g., cross-validation) is\ninfeasible. We find that an unsupervised ensemble of multiple models trained\nwith different hyperparameter values performs better than a single\ncross-validated model. Therefore, NVSM can safely be used for ranking documents\nwithout supervised relevance judgments.", "published": "2017-08-09 03:21:20", "link": "http://arxiv.org/abs/1708.02702v4", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Tips and Tricks for Visual Question Answering: Learnings from the 2017\n  Challenge", "abstract": "This paper presents a state-of-the-art model for visual question answering\n(VQA), which won the first place in the 2017 VQA Challenge. VQA is a task of\nsignificant importance for research in artificial intelligence, given its\nmultimodal nature, clear evaluation protocol, and potential real-world\napplications. The performance of deep neural networks for VQA is very dependent\non choices of architectures and hyperparameters. To help further research in\nthe area, we describe in detail our high-performing, though relatively simple\nmodel. Through a massive exploration of architectures and hyperparameters\nrepresenting more than 3,000 GPU-hours, we identified tips and tricks that lead\nto its success, namely: sigmoid outputs, soft training targets, image features\nfrom bottom-up attention, gated tanh activations, output embeddings initialized\nusing GloVe and Google Images, large mini-batches, and smart shuffling of\ntraining data. We provide a detailed analysis of their impact on performance to\nassist others in making an appropriate selection.", "published": "2017-08-09 04:19:42", "link": "http://arxiv.org/abs/1708.02711v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "KeyXtract Twitter Model - An Essential Keywords Extraction Model for\n  Twitter Designed using NLP Tools", "abstract": "Since a tweet is limited to 140 characters, it is ambiguous and difficult for\ntraditional Natural Language Processing (NLP) tools to analyse. This research\npresents KeyXtract which enhances the machine learning based Stanford CoreNLP\nPart-of-Speech (POS) tagger with the Twitter model to extract essential\nkeywords from a tweet. The system was developed using rule-based parsers and\ntwo corpora. The data for the research was obtained from a Twitter profile of a\ntelecommunication company. The system development consisted of two stages. At\nthe initial stage, a domain specific corpus was compiled after analysing the\ntweets. The POS tagger extracted the Noun Phrases and Verb Phrases while the\nparsers removed noise and extracted any other keywords missed by the POS\ntagger. The system was evaluated using the Turing Test. After it was tested and\ncompared against Stanford CoreNLP, the second stage of the system was developed\naddressing the shortcomings of the first stage. It was enhanced using Named\nEntity Recognition and Lemmatization. The second stage was also tested using\nthe Turing test and its pass rate increased from 50.00% to 83.33%. The\nperformance of the final system output was measured using the F1 score.\nStanford CoreNLP with the Twitter model had an average F1 of 0.69 while the\nimproved system had a F1 of 0.77. The accuracy of the system could be improved\nby using a complete domain specific corpus. Since the system used linguistic\nfeatures of a sentence, it could be applied to other NLP tools.", "published": "2017-08-09 17:04:34", "link": "http://arxiv.org/abs/1708.02912v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Hierarchically-Attentive RNN for Album Summarization and Storytelling", "abstract": "We address the problem of end-to-end visual storytelling. Given a photo\nalbum, our model first selects the most representative (summary) photos, and\nthen composes a natural language story for the album. For this task, we make\nuse of the Visual Storytelling dataset and a model composed of three\nhierarchically-attentive Recurrent Neural Nets (RNNs) to: encode the album\nphotos, select representative (summary) photos, and compose the story.\nAutomatic and human evaluations show our model achieves better performance on\nselection, generation, and retrieval than baselines.", "published": "2017-08-09 19:26:47", "link": "http://arxiv.org/abs/1708.02977v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
