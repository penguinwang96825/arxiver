{"title": "Link Prediction for Wikipedia Articles as a Natural Language Inference\n  Task", "abstract": "Link prediction task is vital to automatically understanding the structure of\nlarge knowledge bases. In this paper, we present our system to solve this task\nat the Data Science and Advanced Analytics 2023 Competition \"Efficient and\nEffective Link Prediction\" (DSAA-2023 Competition) with a corpus containing\n948,233 training and 238,265 for public testing. This paper introduces an\napproach to link prediction in Wikipedia articles by formulating it as a\nnatural language inference (NLI) task. Drawing inspiration from recent\nadvancements in natural language processing and understanding, we cast link\nprediction as an NLI task, wherein the presence of a link between two articles\nis treated as a premise, and the task is to determine whether this premise\nholds based on the information presented in the articles. We implemented our\nsystem based on the Sentence Pair Classification for Link Prediction for the\nWikipedia Articles task. Our system achieved 0.99996 Macro F1-score and 1.00000\nMacro F1-score for the public and private test sets, respectively. Our team\nUIT-NLP ranked 3rd in performance on the private test set, equal to the scores\nof the first and second places. Our code is publicly for research purposes.", "published": "2023-08-31 05:25:04", "link": "http://arxiv.org/abs/2308.16469v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Thesis Distillation: Investigating The Impact of Bias in NLP Models on\n  Hate Speech Detection", "abstract": "This paper is a summary of the work done in my PhD thesis. Where I\ninvestigate the impact of bias in NLP models on the task of hate speech\ndetection from three perspectives: explainability, offensive stereotyping bias,\nand fairness. Then, I discuss the main takeaways from my thesis and how they\ncan benefit the broader NLP community. Finally, I discuss important future\nresearch directions. The findings of my thesis suggest that the bias in NLP\nmodels impacts the task of hate speech detection from all three perspectives.\nAnd that unless we start incorporating social sciences in studying bias in NLP\nmodels, we will not effectively overcome the current limitations of measuring\nand mitigating bias in NLP models.", "published": "2023-08-31 08:40:41", "link": "http://arxiv.org/abs/2308.16549v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Text Style Transfer with Deep Generative Models", "abstract": "We present a general framework for unsupervised text style transfer with deep\ngenerative models. The framework models each sentence-label pair in the\nnon-parallel corpus as partially observed from a complete quadruplet which\nadditionally contains two latent codes representing the content and style,\nrespectively. These codes are learned by exploiting dependencies inside the\nobserved data. Then a sentence is transferred by manipulating them. Our\nframework is able to unify previous embedding and prototype methods as two\nspecial forms. It also provides a principled perspective to explain previously\nproposed techniques in the field such as aligned encoder and adversarial\ntraining. We further conduct experiments on three benchmarks. Both automatic\nand human evaluation results show that our methods achieve better or\ncompetitive results compared to several strong baselines.", "published": "2023-08-31 09:29:35", "link": "http://arxiv.org/abs/2308.16584v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interpreting Sentiment Composition with Latent Semantic Tree", "abstract": "As the key to sentiment analysis, sentiment composition considers the\nclassification of a constituent via classifications of its contained\nsub-constituents and rules operated on them. Such compositionality has been\nwidely studied previously in the form of hierarchical trees including untagged\nand sentiment ones, which are intrinsically suboptimal in our view. To address\nthis, we propose semantic tree, a new tree form capable of interpreting the\nsentiment composition in a principled way. Semantic tree is a derivation of a\ncontext-free grammar (CFG) describing the specific composition rules on\ndifference semantic roles, which is designed carefully following previous\nlinguistic conclusions. However, semantic tree is a latent variable since there\nis no its annotation in regular datasets. Thus, in our method, it is\nmarginalized out via inside algorithm and learned to optimize the\nclassification performance. Quantitative and qualitative results demonstrate\nthat our method not only achieves better or competitive results compared to\nbaselines in the setting of regular and domain adaptation classification, and\nalso generates plausible tree explanations.", "published": "2023-08-31 09:35:52", "link": "http://arxiv.org/abs/2308.16588v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DictaBERT: A State-of-the-Art BERT Suite for Modern Hebrew", "abstract": "We present DictaBERT, a new state-of-the-art pre-trained BERT model for\nmodern Hebrew, outperforming existing models on most benchmarks. Additionally,\nwe release three fine-tuned versions of the model, designed to perform three\nspecific foundational tasks in the analysis of Hebrew texts: prefix\nsegmentation, morphological tagging and question answering. These fine-tuned\nmodels allow any developer to perform prefix segmentation, morphological\ntagging and question answering of a Hebrew input with a single call to a\nHuggingFace model, without the need to integrate any additional libraries or\ncode. In this paper we describe the details of the training as well and the\nresults on the different benchmarks. We release the models to the community,\nalong with sample code demonstrating their use. We release these models as part\nof our goal to help further research and development in Hebrew NLP.", "published": "2023-08-31 12:43:18", "link": "http://arxiv.org/abs/2308.16687v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing PLM Performance on Labour Market Tasks via Instruction-based\n  Finetuning and Prompt-tuning with Rules", "abstract": "The increased digitization of the labour market has given researchers,\neducators, and companies the means to analyze and better understand the labour\nmarket. However, labour market resources, although available in high volumes,\ntend to be unstructured, and as such, research towards methodologies for the\nidentification, linking, and extraction of entities becomes more and more\nimportant. Against the backdrop of this quest for better labour market\nrepresentations, resource constraints and the unavailability of large-scale\nannotated data cause a reliance on human domain experts. We demonstrate the\neffectiveness of prompt-based tuning of pre-trained language models (PLM) in\nlabour market specific applications. Our results indicate that cost-efficient\nmethods such as PTR and instruction tuning without exemplars can significantly\nincrease the performance of PLMs on downstream labour market applications\nwithout introducing additional model layers, manual annotations, and data\naugmentation.", "published": "2023-08-31 14:47:00", "link": "http://arxiv.org/abs/2308.16770v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Multilingual Automatic Dialogue Evaluation", "abstract": "The main limiting factor in the development of robust multilingual dialogue\nevaluation metrics is the lack of multilingual data and the limited\navailability of open sourced multilingual dialogue systems. In this work, we\npropose a workaround for this lack of data by leveraging a strong multilingual\npretrained LLM and augmenting existing English dialogue data using Machine\nTranslation. We empirically show that the naive approach of finetuning a\npretrained multilingual encoder model with translated data is insufficient to\noutperform the strong baseline of finetuning a multilingual model with only\nsource data. Instead, the best approach consists in the careful curation of\ntranslated data using MT Quality Estimation metrics, excluding low quality\ntranslations that hinder its performance.", "published": "2023-08-31 15:15:26", "link": "http://arxiv.org/abs/2308.16795v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simple LLM Prompting is State-of-the-Art for Robust and Multilingual\n  Dialogue Evaluation", "abstract": "Despite significant research effort in the development of automatic dialogue\nevaluation metrics, little thought is given to evaluating dialogues other than\nin English. At the same time, ensuring metrics are invariant to semantically\nsimilar responses is also an overlooked topic. In order to achieve the desired\nproperties of robustness and multilinguality for dialogue evaluation metrics,\nwe propose a novel framework that takes advantage of the strengths of current\nevaluation models with the newly-established paradigm of prompting Large\nLanguage Models (LLMs). Empirical results show our framework achieves state of\nthe art results in terms of mean Spearman correlation scores across several\nbenchmarks and ranks first place on both the Robust and Multilingual tasks of\nthe DSTC11 Track 4 \"Automatic Evaluation Metrics for Open-Domain Dialogue\nSystems\", proving the evaluation capabilities of prompted LLMs.", "published": "2023-08-31 15:19:28", "link": "http://arxiv.org/abs/2308.16797v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Distillation from Non-streaming to Streaming ASR Encoder using\n  Auxiliary Non-streaming Layer", "abstract": "Streaming automatic speech recognition (ASR) models are restricted from\naccessing future context, which results in worse performance compared to the\nnon-streaming models. To improve the performance of streaming ASR, knowledge\ndistillation (KD) from the non-streaming to streaming model has been studied,\nmainly focusing on aligning the output token probabilities. In this paper, we\npropose a layer-to-layer KD from the teacher encoder to the student encoder. To\nensure that features are extracted using the same context, we insert auxiliary\nnon-streaming branches to the student and perform KD from the non-streaming\nteacher layer to the non-streaming auxiliary layer. We design a special KD loss\nthat leverages the autoregressive predictive coding (APC) mechanism to\nencourage the streaming model to predict unseen future contexts. Experimental\nresults show that the proposed method can significantly reduce the word error\nrate compared to previous token probability distillation methods.", "published": "2023-08-31 02:58:33", "link": "http://arxiv.org/abs/2308.16415v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Sparkles: Unlocking Chats Across Multiple Images for Multimodal\n  Instruction-Following Models", "abstract": "Large language models exhibit enhanced zero-shot performance on various tasks\nwhen fine-tuned with instruction-following data. Multimodal\ninstruction-following models extend these capabilities by integrating both text\nand images. However, existing models such as MiniGPT-4 and LLaVA face\nchallenges in maintaining dialogue coherence in scenarios involving multiple\nimages. A primary reason is the lack of a specialized dataset for this critical\napplication. To bridge these gaps, we introduce SparklesDialogue, the first\nmachine-generated dialogue dataset tailored for word-level interleaved\nmulti-image and text interactions. Furthermore, we construct SparklesEval, a\nGPT-assisted benchmark for quantitatively assessing a model's conversational\ncompetence across multiple images and dialogue turns. We then present\nSparklesChat, a multimodal instruction-following model for open-ended dialogues\nacross multiple images. Our experiments validate the effectiveness of training\nSparklesChat with SparklesDialogue based on MiniGPT-4 and LLaVA-v1.5, which\nenhances comprehension across multiple images and dialogue turns, and does not\ncompromise single-image understanding capabilities. Qualitative evaluations\nfurther demonstrate SparklesChat's generality in handling real-world\napplications. All resources related to this study are publicly available at\nhttps://github.com/HYPJUDY/Sparkles.", "published": "2023-08-31 05:15:27", "link": "http://arxiv.org/abs/2308.16463v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Enhancing Subtask Performance of Multi-modal Large Language Model", "abstract": "Multi-modal Large Language Model (MLLM) refers to a model expanded from a\nLarge Language Model (LLM) that possesses the capability to handle and infer\nmulti-modal data. Current MLLMs typically begin by using LLMs to decompose\ntasks into multiple subtasks, then employing individual pre-trained models to\ncomplete specific subtasks, and ultimately utilizing LLMs to integrate the\nresults of each subtasks to obtain the results of the task. In real-world\nscenarios, when dealing with large projects, it is common practice to break\ndown the project into smaller sub-projects, with different teams providing\ncorresponding solutions or results. The project owner then decides which\nsolution or result to use, ensuring the best possible outcome for each subtask\nand, consequently, for the entire project. Inspired by this, this study\nconsiders selecting multiple pre-trained models to complete the same subtask.\nBy combining the results from multiple pre-trained models, the optimal subtask\nresult is obtained, enhancing the performance of the MLLM. Specifically, this\nstudy first selects multiple pre-trained models focused on the same subtask\nbased on distinct evaluation approaches, and then invokes these models in\nparallel to process input data and generate corresponding subtask results.\nFinally, the results from multiple pre-trained models for the same subtask are\ncompared using the LLM, and the best result is chosen as the outcome for that\nsubtask. Extensive experiments are conducted in this study using GPT-4\nannotated datasets and human-annotated datasets. The results of various\nevaluation metrics adequately demonstrate the effectiveness of the proposed\napproach in this paper.", "published": "2023-08-31 05:37:21", "link": "http://arxiv.org/abs/2308.16474v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "$\\rm SP^3$: Enhancing Structured Pruning via PCA Projection", "abstract": "Structured pruning is a widely used technique for reducing the size of\npre-trained language models (PLMs), but current methods often overlook the\npotential of compressing the hidden dimension (d) in PLMs, a dimension critical\nto model size and efficiency. This paper introduces a novel structured pruning\napproach, Structured Pruning with PCA Projection (SP3), targeting the effective\nreduction of d by projecting features into a space defined by principal\ncomponents before masking. Extensive experiments on benchmarks (GLUE and SQuAD)\nshow that SP3 can reduce d by 70%, compress 94% of the BERTbase model, maintain\nover 96% accuracy, and outperform other methods that compress d by 6% in\naccuracy at the same compression ratio. SP3 has also proven effective with\nother models, including OPT and Llama. Our data and code are available at an\nanonymous repo.", "published": "2023-08-31 05:40:14", "link": "http://arxiv.org/abs/2308.16475v3", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "The Smart Data Extractor, a Clinician Friendly Solution to Accelerate\n  and Improve the Data Collection During Clinical Trials", "abstract": "In medical research, the traditional way to collect data, i.e. browsing\npatient files, has been proven to induce bias, errors, human labor and costs.\nWe propose a semi-automated system able to extract every type of data,\nincluding notes. The Smart Data Extractor pre-populates clinic research forms\nby following rules. We performed a cross-testing experiment to compare\nsemi-automated to manual data collection. 20 target items had to be collected\nfor 79 patients. The average time to complete one form was 6'81'' for manual\ndata collection and 3'22'' with the Smart Data Extractor. There were also more\nmistakes during manual data collection (163 for the whole cohort) than with the\nSmart Data Extractor (46 for the whole cohort). We present an easy to use,\nunderstandable and agile solution to fill out clinical research forms. It\nreduces human effort and provides higher quality data, avoiding data re-entry\nand fatigue induced errors.", "published": "2023-08-31 08:28:11", "link": "http://arxiv.org/abs/2308.16537v1", "categories": ["q-bio.QM", "cs.CL"], "primary_category": "q-bio.QM"}
{"title": "Using Large Language Models to Automate Category and Trend Analysis of\n  Scientific Articles: An Application in Ophthalmology", "abstract": "Purpose: In this paper, we present an automated method for article\nclassification, leveraging the power of Large Language Models (LLM). The\nprimary focus is on the field of ophthalmology, but the model is extendable to\nother fields. Methods: We have developed a model based on Natural Language\nProcessing (NLP) techniques, including advanced LLMs, to process and analyze\nthe textual content of scientific papers. Specifically, we have employed\nzero-shot learning (ZSL) LLM models and compared against Bidirectional and\nAuto-Regressive Transformers (BART) and its variants, and Bidirectional Encoder\nRepresentations from Transformers (BERT), and its variant such as distilBERT,\nSciBERT, PubmedBERT, BioBERT. Results: The classification results demonstrate\nthe effectiveness of LLMs in categorizing large number of ophthalmology papers\nwithout human intervention. Results: To evalute the LLMs, we compiled a dataset\n(RenD) of 1000 ocular disease-related articles, which were expertly annotated\nby a panel of six specialists into 15 distinct categories. The model achieved\nmean accuracy of 0.86 and mean F1 of 0.85 based on the RenD dataset.\nConclusion: The proposed framework achieves notable improvements in both\naccuracy and efficiency. Its application in the domain of ophthalmology\nshowcases its potential for knowledge organization and retrieval in other\ndomains too. We performed trend analysis that enables the researchers and\nclinicians to easily categorize and retrieve relevant papers, saving time and\neffort in literature review and information gathering as well as identification\nof emerging scientific trends within different disciplines. Moreover, the\nextendibility of the model to other scientific fields broadens its impact in\nfacilitating research and trend analysis across diverse disciplines.", "published": "2023-08-31 12:45:53", "link": "http://arxiv.org/abs/2308.16688v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Cross-Cultural Differences in English Hate Speech Annotations:\n  From Dataset Construction to Analysis", "abstract": "Warning: this paper contains content that may be offensive or upsetting.\n  Most hate speech datasets neglect the cultural diversity within a single\nlanguage, resulting in a critical shortcoming in hate speech detection. To\naddress this, we introduce CREHate, a CRoss-cultural English Hate speech\ndataset. To construct CREHate, we follow a two-step procedure: 1) cultural post\ncollection and 2) cross-cultural annotation. We sample posts from the SBIC\ndataset, which predominantly represents North America, and collect posts from\nfour geographically diverse English-speaking countries (Australia, United\nKingdom, Singapore, and South Africa) using culturally hateful keywords we\nretrieve from our survey. Annotations are collected from the four countries\nplus the United States to establish representative labels for each country. Our\nanalysis highlights statistically significant disparities across countries in\nhate speech annotations. Only 56.2% of the posts in CREHate achieve consensus\namong all countries, with the highest pairwise label difference rate of 26%.\nQualitative analysis shows that label disagreement occurs mostly due to\ndifferent interpretations of sarcasm and the personal bias of annotators on\ndivisive topics. Lastly, we evaluate large language models (LLMs) under a\nzero-shot setting and show that current LLMs tend to show higher accuracies on\nAnglosphere country labels in CREHate. Our dataset and codes are available at:\nhttps://github.com/nlee0212/CREHate", "published": "2023-08-31 13:14:47", "link": "http://arxiv.org/abs/2308.16705v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Ladder-of-Thought: Using Knowledge as Steps to Elevate Stance Detection", "abstract": "Stance detection aims to identify the attitude expressed in a document\ntowards a given target. Techniques such as Chain-of-Thought (CoT) prompting\nhave advanced this task, enhancing a model's reasoning capabilities through the\nderivation of intermediate rationales. However, CoT relies primarily on a\nmodel's pre-trained internal knowledge during reasoning, thereby neglecting the\nvaluable external information that is previously unknown to the model. This\nomission, especially within the unsupervised reasoning process, can affect the\nmodel's overall performance. Moreover, while CoT enhances Large Language Models\n(LLMs), smaller LMs, though efficient operationally, face challenges in\ndelivering nuanced reasoning. In response to these identified gaps, we\nintroduce the Ladder-of-Thought (LoT) for the stance detection task.\nConstructed through a dual-phase Progressive Optimization Framework, LoT\ndirects the small LMs to assimilate high-quality external knowledge, refining\nthe intermediate rationales produced. These bolstered rationales subsequently\nserve as the foundation for more precise predictions - akin to how a ladder\nfacilitates reaching elevated goals. LoT achieves a balance between efficiency\nand performance. Our empirical evaluations underscore LoT's efficacy, marking a\n16% improvement over GPT-3.5 and a 10% enhancement compared to GPT-3.5 with CoT\non stance detection task.", "published": "2023-08-31 14:31:48", "link": "http://arxiv.org/abs/2308.16763v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Gender-GAP Pipeline: A Gender-Aware Polyglot Pipeline for Gender\n  Characterisation in 55 Languages", "abstract": "Gender biases in language generation systems are challenging to mitigate. One\npossible source for these biases is gender representation disparities in the\ntraining and evaluation data. Despite recent progress in documenting this\nproblem and many attempts at mitigating it, we still lack shared methodology\nand tooling to report gender representation in large datasets. Such\nquantitative reporting will enable further mitigation, e.g., via data\naugmentation. This paper describes the Gender-GAP Pipeline (for Gender-Aware\nPolyglot Pipeline), an automatic pipeline to characterize gender representation\nin large-scale datasets for 55 languages. The pipeline uses a multilingual\nlexicon of gendered person-nouns to quantify the gender representation in text.\nWe showcase it to report gender representation in WMT training data and\ndevelopment data for the News task, confirming that current data is skewed\ntowards masculine representation. Having unbalanced datasets may indirectly\noptimize our systems towards outperforming one gender over the others. We\nsuggest introducing our gender quantification pipeline in current datasets and,\nideally, modifying them toward a balanced representation.", "published": "2023-08-31 17:20:50", "link": "http://arxiv.org/abs/2308.16871v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TouchStone: Evaluating Vision-Language Models by Language Models", "abstract": "Large vision-language models (LVLMs) have recently witnessed rapid\nadvancements, exhibiting a remarkable capacity for perceiving, understanding,\nand processing visual information by connecting visual receptor with large\nlanguage models (LLMs). However, current assessments mainly focus on\nrecognizing and reasoning abilities, lacking direct evaluation of\nconversational skills and neglecting visual storytelling abilities. In this\npaper, we propose an evaluation method that uses strong LLMs as judges to\ncomprehensively evaluate the various abilities of LVLMs. Firstly, we construct\na comprehensive visual dialogue dataset TouchStone, consisting of open-world\nimages and questions, covering five major categories of abilities and 27\nsubtasks. This dataset not only covers fundamental recognition and\ncomprehension but also extends to literary creation. Secondly, by integrating\ndetailed image annotations we effectively transform the multimodal input\ncontent into a form understandable by LLMs. This enables us to employ advanced\nLLMs for directly evaluating the quality of the multimodal dialogue without\nrequiring human intervention. Through validation, we demonstrate that powerful\nLVLMs, such as GPT-4, can effectively score dialogue quality by leveraging\ntheir textual capabilities alone, aligning with human preferences. We hope our\nwork can serve as a touchstone for LVLMs' evaluation and pave the way for\nbuilding stronger LVLMs. The evaluation code is available at\nhttps://github.com/OFA-Sys/TouchStone.", "published": "2023-08-31 17:52:04", "link": "http://arxiv.org/abs/2308.16890v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Construction Grammar and Artificial Intelligence", "abstract": "In this chapter, we argue that it is highly beneficial for the contemporary\nconstruction grammarian to have a thorough understanding of the strong\nrelationship between the research fields of construction grammar and artificial\nintelligence. We start by unravelling the historical links between the two\nfields, showing that their relationship is rooted in a common attitude towards\nhuman communication and language. We then discuss the first direction of\ninfluence, focussing in particular on how insights and techniques from the\nfield of artificial intelligence play an important role in operationalising,\nvalidating and scaling constructionist approaches to language. We then proceed\nto the second direction of influence, highlighting the relevance of\nconstruction grammar insights and analyses to the artificial intelligence\nendeavour of building truly intelligent agents. We support our case with a\nvariety of illustrative examples and conclude that the further elaboration of\nthis relationship will play a key role in shaping the future of the field of\nconstruction grammar.", "published": "2023-08-31 21:15:06", "link": "http://arxiv.org/abs/2309.00135v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Can humans help BERT gain \"confidence\"?", "abstract": "The advancements in artificial intelligence over the last decade have opened\na multitude of avenues for interdisciplinary research. Since the idea of\nartificial intelligence was inspired by the working of neurons in the brain, it\nseems pretty practical to combine the two fields and take the help of cognitive\ndata to train AI models. Not only it will help to get a deeper understanding of\nthe technology, but of the brain as well. In this thesis, I conduct novel\nexperiments to integrate cognitive features from the Zurich Cognitive Corpus\n(ZuCo) (Hollenstein et al., 2018) with a transformer-based encoder model called\nBERT. I show how EEG and eye-tracking features from ZuCo can help to increase\nthe performance of the NLP model. I confirm the performance increase with the\nhelp of a robustness-checking pipeline and derive a word-EEG lexicon to use in\nbenchmarking on an external dataset that does not have any cognitive features\nassociated with it. Further, I analyze the internal working mechanism of BERT\nand explore a potential method for model explainability by correlating it with\na popular model-agnostic explainability framework called LIME (Ribeiro et al.,\n2016). Finally, I discuss the possible directions to take this research\nforward.", "published": "2023-08-31 13:12:28", "link": "http://arxiv.org/abs/2309.06580v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BioCoder: A Benchmark for Bioinformatics Code Generation with Large\n  Language Models", "abstract": "Pre-trained large language models (LLMs) have significantly improved code\ngeneration. As these models scale up, there is an increasing need for the\noutput to handle more intricate tasks and to be appropriately specialized to\nparticular domains. Here, we target bioinformatics due to the amount of domain\nknowledge, algorithms, and data operations this discipline requires. We present\nBioCoder, a benchmark developed to evaluate LLMs in generating\nbioinformatics-specific code. BioCoder spans much of the field, covering\ncross-file dependencies, class declarations, and global variables. It\nincorporates 1,026 Python functions and 1,243 Java methods extracted from\nGitHub, along with 253 examples from the Rosalind Project, all pertaining to\nbioinformatics. Using topic modeling, we show that the overall coverage of the\nincluded code is representative of the full spectrum of bioinformatics\ncalculations. BioCoder incorporates a fuzz-testing framework for evaluation. We\nhave applied it to evaluate various models including InCoder, CodeGen,\nCodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT5+, GPT-3.5, and GPT-\n4. Furthermore, we fine-tuned one model (StarCoder), demonstrating that our\ntraining dataset can enhance the performance on our testing benchmark (by >15%\nin terms of Pass@K under certain prompt configurations and always >3%). The\nresults highlight two key aspects of successful models: (1) Successful models\naccommodate a long prompt (> 2,600 tokens) with full context, including\nfunctional dependencies. (2) They contain domain-specific knowledge of\nbioinformatics, beyond just general coding capability. This is evident from the\nperformance gain of GPT-3.5/4 compared to the smaller models on our benchmark\n(50% vs. up to 25%). Availability and implementation: Code is available at:\nhttps://github.com/gersteinlab/biocoder and https://biocoder-benchmark.\ngithub.io/.", "published": "2023-08-31 04:52:58", "link": "http://arxiv.org/abs/2308.16458v5", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Generalised Winograd Schema and its Contextuality", "abstract": "Ambiguities in natural language give rise to probability distributions over\ninterpretations. The distributions are often over multiple ambiguous words at a\ntime; a multiplicity which makes them a suitable topic for sheaf-theoretic\nmodels of quantum contextuality. Previous research showed that different\nquantitative measures of contextuality correlate well with Psycholinguistic\nresearch on lexical ambiguities. In this work, we focus on coreference\nambiguities and investigate the Winograd Schema Challenge (WSC), a test\nproposed by Levesque in 2011 to evaluate the intelligence of machines. The WSC\nconsists of a collection of multiple-choice questions that require\ndisambiguating pronouns in sentences structured according to the Winograd\nschema, in a way that makes it difficult for machines to determine the correct\nreferents but remains intuitive for human comprehension. In this study, we\npropose an approach that analogously models the Winograd schema as an\nexperiment in quantum physics. However, we argue that the original Winograd\nSchema is inherently too simplistic to facilitate contextuality. We introduce a\nnovel mechanism for generalising the schema, rendering it analogous to a\nBell-CHSH measurement scenario. We report an instance of this generalised\nschema, complemented by the human judgements we gathered via a crowdsourcing\nplatform. The resulting model violates the Bell-CHSH inequality by 0.192, thus\nexhibiting contextuality in a coreference resolution setting.", "published": "2023-08-31 07:00:21", "link": "http://arxiv.org/abs/2308.16498v1", "categories": ["cs.CL", "cs.AI", "quant-ph"], "primary_category": "cs.CL"}
{"title": "Time-Varying Quasi-Closed-Phase Analysis for Accurate Formant Tracking\n  in Speech Signals", "abstract": "In this paper, we propose a new method for the accurate estimation and\ntracking of formants in speech signals using time-varying quasi-closed-phase\n(TVQCP) analysis. Conventional formant tracking methods typically adopt a\ntwo-stage estimate-and-track strategy wherein an initial set of formant\ncandidates are estimated using short-time analysis (e.g., 10--50 ms), followed\nby a tracking stage based on dynamic programming or a linear state-space model.\nOne of the main disadvantages of these approaches is that the tracking stage,\nhowever good it may be, cannot improve upon the formant estimation accuracy of\nthe first stage. The proposed TVQCP method provides a single-stage formant\ntracking that combines the estimation and tracking stages into one. TVQCP\nanalysis combines three approaches to improve formant estimation and tracking:\n(1) it uses temporally weighted quasi-closed-phase analysis to derive\nclosed-phase estimates of the vocal tract with reduced interference from the\nexcitation source, (2) it increases the residual sparsity by using the $L_1$\noptimization and (3) it uses time-varying linear prediction analysis over long\ntime windows (e.g., 100--200 ms) to impose a continuity constraint on the vocal\ntract model and hence on the formant trajectories. Formant tracking experiments\nwith a wide variety of synthetic and natural speech signals show that the\nproposed TVQCP method performs better than conventional and popular formant\ntracking tools, such as Wavesurfer and Praat (based on dynamic programming),\nthe KARMA algorithm (based on Kalman filtering), and DeepFormants (based on\ndeep neural networks trained in a supervised manner). Matlab scripts for the\nproposed method can be found at: https://github.com/njaygowda/ftrack", "published": "2023-08-31 08:30:20", "link": "http://arxiv.org/abs/2308.16540v1", "categories": ["eess.AS", "cs.CL", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Improving Mandarin Prosodic Structure Prediction with Multi-level\n  Contextual Information", "abstract": "For text-to-speech (TTS) synthesis, prosodic structure prediction (PSP) plays\nan important role in producing natural and intelligible speech. Although\ninter-utterance linguistic information can influence the speech interpretation\nof the target utterance, previous works on PSP mainly focus on utilizing\nintrautterance linguistic information of the current utterance only. This work\nproposes to use inter-utterance linguistic information to improve the\nperformance of PSP. Multi-level contextual information, which includes both\ninter-utterance and intrautterance linguistic information, is extracted by a\nhierarchical encoder from character level, utterance level and discourse level\nof the input text. Then a multi-task learning (MTL) decoder predicts prosodic\nboundaries from multi-level contextual information. Objective evaluation\nresults on two datasets show that our method achieves better F1 scores in\npredicting prosodic word (PW), prosodic phrase (PPH) and intonational phrase\n(IPH). It demonstrates the effectiveness of using multi-level contextual\ninformation for PSP. Subjective preference tests also indicate the naturalness\nof synthesized speeches are improved.", "published": "2023-08-31 09:19:15", "link": "http://arxiv.org/abs/2308.16577v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Spontaneous Style Modeling with Semi-supervised Pre-training for\n  Conversational Text-to-Speech Synthesis", "abstract": "The spontaneous behavior that often occurs in conversations makes speech more\nhuman-like compared to reading-style. However, synthesizing spontaneous-style\nspeech is challenging due to the lack of high-quality spontaneous datasets and\nthe high cost of labeling spontaneous behavior. In this paper, we propose a\nsemi-supervised pre-training method to increase the amount of spontaneous-style\nspeech and spontaneous behavioral labels. In the process of semi-supervised\nlearning, both text and speech information are considered for detecting\nspontaneous behaviors labels in speech. Moreover, a linguistic-aware encoder is\nused to model the relationship between each sentence in the conversation.\nExperimental results indicate that our proposed method achieves superior\nexpressive speech synthesis performance with the ability to model spontaneous\nbehavior in spontaneous-style speech and predict reasonable spontaneous\nbehavior from text.", "published": "2023-08-31 09:50:33", "link": "http://arxiv.org/abs/2308.16593v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Developing a Scalable Benchmark for Assessing Large Language Models in\n  Knowledge Graph Engineering", "abstract": "As the field of Large Language Models (LLMs) evolves at an accelerated pace,\nthe critical need to assess and monitor their performance emerges. We introduce\na benchmarking framework focused on knowledge graph engineering (KGE)\naccompanied by three challenges addressing syntax and error correction, facts\nextraction and dataset generation. We show that while being a useful tool, LLMs\nare yet unfit to assist in knowledge graph generation with zero-shot prompting.\nConsequently, our LLM-KG-Bench framework provides automatic evaluation and\nstorage of LLM responses as well as statistical data and visualization tools to\nsupport tracking of prompt engineering and model performance.", "published": "2023-08-31 10:31:19", "link": "http://arxiv.org/abs/2308.16622v1", "categories": ["cs.AI", "cs.CL", "cs.DB"], "primary_category": "cs.AI"}
{"title": "SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language\n  Models", "abstract": "Current speech large language models build upon discrete speech\nrepresentations, which can be categorized into semantic tokens and acoustic\ntokens. However, existing speech tokens are not specifically designed for\nspeech language modeling. To assess the suitability of speech tokens for\nbuilding speech language models, we established the first benchmark,\nSLMTokBench. Our results indicate that neither semantic nor acoustic tokens are\nideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech\ntokenizer for speech large language models. SpeechTokenizer adopts the\nEncoder-Decoder architecture with residual vector quantization (RVQ). Unifying\nsemantic and acoustic tokens, SpeechTokenizer disentangles different aspects of\nspeech information hierarchically across different RVQ layers. Furthermore, We\nconstruct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer.\nExperiments show that SpeechTokenizer performs comparably to EnCodec in speech\nreconstruction and demonstrates strong performance on the SLMTokBench\nbenchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks.\nCode and models are available at\nhttps://github.com/ZhangXInFD/SpeechTokenizer/.", "published": "2023-08-31 12:53:09", "link": "http://arxiv.org/abs/2308.16692v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Can Programming Languages Boost Each Other via Instruction Tuning?", "abstract": "When human programmers have mastered a programming language, it would be\neasier when they learn a new programming language. In this report, we focus on\nexploring whether programming languages can boost each other during the\ninstruction fine-tuning phase of code large language models. We conduct\nextensive experiments of 8 popular programming languages (Python, JavaScript,\nTypeScript, C, C++, Java, Go, HTML) on StarCoder. Results demonstrate that\nprogramming languages can significantly improve each other. For example,\nCodeM-Python 15B trained on Python is able to increase Java by an absolute\n17.95% pass@1 on HumanEval-X. More surprisingly, we found that CodeM-HTML 7B\ntrained on the HTML corpus can improve Java by an absolute 15.24% pass@1. Our\ntraining data is released at https://github.com/NL2Code/CodeM.", "published": "2023-08-31 15:53:51", "link": "http://arxiv.org/abs/2308.16824v2", "categories": ["cs.CL", "cs.AI", "cs.PL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122\n  Language Variants", "abstract": "We present Belebele, a multiple-choice machine reading comprehension (MRC)\ndataset spanning 122 language variants. Significantly expanding the language\ncoverage of natural language understanding (NLU) benchmarks, this dataset\nenables the evaluation of text models in high-, medium-, and low-resource\nlanguages. Each question is based on a short passage from the Flores-200\ndataset and has four multiple-choice answers. The questions were carefully\ncurated to discriminate between models with different levels of general\nlanguage comprehension. The English dataset on its own proves difficult enough\nto challenge state-of-the-art language models. Being fully parallel, this\ndataset enables direct comparison of model performance across all languages. We\nuse this dataset to evaluate the capabilities of multilingual masked language\nmodels (MLMs) and large language models (LLMs). We present extensive results\nand find that despite significant cross-lingual transfer in English-centric\nLLMs, much smaller MLMs pretrained on balanced multilingual data still\nunderstand far more languages. We also observe that larger vocabulary size and\nconscious vocabulary construction correlate with better performance on\nlow-resource languages. Overall, Belebele opens up new avenues for evaluating\nand analyzing the multilingual capabilities of NLP systems.", "published": "2023-08-31 17:43:08", "link": "http://arxiv.org/abs/2308.16884v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Transformers as Support Vector Machines", "abstract": "Since its inception in \"Attention Is All You Need\", transformer architecture\nhas led to revolutionary advancements in NLP. The attention layer within the\ntransformer admits a sequence of input tokens $X$ and makes them interact\nthrough pairwise similarities computed as softmax$(XQK^\\top X^\\top)$, where\n$(K,Q)$ are the trainable key-query parameters. In this work, we establish a\nformal equivalence between the optimization geometry of self-attention and a\nhard-margin SVM problem that separates optimal input tokens from non-optimal\ntokens using linear constraints on the outer-products of token pairs. This\nformalism allows us to characterize the implicit bias of 1-layer transformers\noptimized with gradient descent: (1) Optimizing the attention layer with\nvanishing regularization, parameterized by $(K,Q)$, converges in direction to\nan SVM solution minimizing the nuclear norm of the combined parameter\n$W=KQ^\\top$. Instead, directly parameterizing by $W$ minimizes a Frobenius norm\nobjective. We characterize this convergence, highlighting that it can occur\ntoward locally-optimal directions rather than global ones. (2) Complementing\nthis, we prove the local/global directional convergence of gradient descent\nunder suitable geometric conditions. Importantly, we show that\nover-parameterization catalyzes global convergence by ensuring the feasibility\nof the SVM problem and by guaranteeing a benign optimization landscape devoid\nof stationary points. (3) While our theory applies primarily to linear\nprediction heads, we propose a more general SVM equivalence that predicts the\nimplicit bias with nonlinear heads. Our findings are applicable to arbitrary\ndatasets and their validity is verified via experiments. We also introduce\nseveral open problems and research directions. We believe these findings\ninspire the interpretation of transformers as a hierarchy of SVMs that\nseparates and selects optimal tokens.", "published": "2023-08-31 17:57:50", "link": "http://arxiv.org/abs/2308.16898v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "math.OC"], "primary_category": "cs.LG"}
{"title": "PointLLM: Empowering Large Language Models to Understand Point Clouds", "abstract": "The unprecedented advancements in Large Language Models (LLMs) have shown a\nprofound impact on natural language processing but are yet to fully embrace the\nrealm of 3D understanding. This paper introduces PointLLM, a preliminary effort\nto fill this gap, enabling LLMs to understand point clouds and offering a new\navenue beyond 2D visual data. PointLLM understands colored object point clouds\nwith human instructions and generates contextually appropriate responses,\nillustrating its grasp of point clouds and common sense. Specifically, it\nleverages a point cloud encoder with a powerful LLM to effectively fuse\ngeometric, appearance, and linguistic information. We collect a novel dataset\ncomprising 660K simple and 70K complex point-text instruction pairs to enable a\ntwo-stage training strategy: aligning latent spaces and subsequently\ninstruction-tuning the unified model. To rigorously evaluate the perceptual and\ngeneralization capabilities of PointLLM, we establish two benchmarks:\nGenerative 3D Object Classification and 3D Object Captioning, assessed through\nthree different methods, including human evaluation, GPT-4/ChatGPT evaluation,\nand traditional metrics. Experimental results reveal PointLLM's superior\nperformance over existing 2D and 3D baselines, with a notable achievement in\nhuman-evaluated object captioning tasks where it surpasses human annotators in\nover 50% of the samples. Codes, datasets, and benchmarks are available at\nhttps://github.com/OpenRobotLab/PointLLM .", "published": "2023-08-31 17:59:46", "link": "http://arxiv.org/abs/2308.16911v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "YaRN: Efficient Context Window Extension of Large Language Models", "abstract": "Rotary Position Embeddings (RoPE) have been shown to effectively encode\npositional information in transformer-based language models. However, these\nmodels fail to generalize past the sequence length they were trained on. We\npresent YaRN (Yet another RoPE extensioN method), a compute-efficient method to\nextend the context window of such models, requiring 10x less tokens and 2.5x\nless training steps than previous methods. Using YaRN, we show that LLaMA\nmodels can effectively utilize and extrapolate to context lengths much longer\nthan their original pre-training would allow, while also surpassing previous\nthe state-of-the-art at context window extension. In addition, we demonstrate\nthat YaRN exhibits the capability to extrapolate beyond the limited context of\na fine-tuning dataset. The models fine-tuned using YaRN has been made available\nand reproduced online up to 128k context length at\nhttps://github.com/jquesnelle/yarn", "published": "2023-08-31 18:18:07", "link": "http://arxiv.org/abs/2309.00071v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large language models in medicine: the potentials and pitfalls", "abstract": "Large language models (LLMs) have been applied to tasks in healthcare,\nranging from medical exam questions to responding to patient questions. With\nincreasing institutional partnerships between companies producing LLMs and\nhealthcare systems, real world clinical application is coming closer to\nreality. As these models gain traction, it is essential for healthcare\npractitioners to understand what LLMs are, their development, their current and\npotential applications, and the associated pitfalls when utilized in medicine.\nThis review and accompanying tutorial aim to give an overview of these topics\nto aid healthcare practitioners in understanding the rapidly changing landscape\nof LLMs as applied to medicine.", "published": "2023-08-31 19:06:39", "link": "http://arxiv.org/abs/2309.00087v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "QS-TTS: Towards Semi-Supervised Text-to-Speech Synthesis via\n  Vector-Quantized Self-Supervised Speech Representation Learning", "abstract": "This paper proposes a novel semi-supervised TTS framework, QS-TTS, to improve\nTTS quality with lower supervised data requirements via Vector-Quantized\nSelf-Supervised Speech Representation Learning (VQ-S3RL) utilizing more\nunlabeled speech audio. This framework comprises two VQ-S3R learners: first,\nthe principal learner aims to provide a generative Multi-Stage Multi-Codebook\n(MSMC) VQ-S3R via the MSMC-VQ-GAN combined with the contrastive S3RL, while\ndecoding it back to the high-quality audio; then, the associate learner further\nabstracts the MSMC representation into a highly-compact VQ representation\nthrough a VQ-VAE. These two generative VQ-S3R learners provide profitable\nspeech representations and pre-trained models for TTS, significantly improving\nsynthesis quality with the lower requirement for supervised data. QS-TTS is\nevaluated comprehensively under various scenarios via subjective and objective\ntests in experiments. The results powerfully demonstrate the superior\nperformance of QS-TTS, winning the highest MOS over supervised or\nsemi-supervised baseline TTS approaches, especially in low-resource scenarios.\nMoreover, comparing various speech representations and transfer learning\nmethods in TTS further validates the notable improvement of the proposed\nVQ-S3RL to TTS, showing the best audio quality and intelligibility metrics. The\ntrend of slower decay in the synthesis quality of QS-TTS with decreasing\nsupervised data further highlights its lower requirements for supervised data,\nindicating its great potential in low-resource scenarios.", "published": "2023-08-31 20:25:44", "link": "http://arxiv.org/abs/2309.00126v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LLM in the Shell: Generative Honeypots", "abstract": "Honeypots are essential tools in cybersecurity for early detection, threat\nintelligence gathering, and analysis of attacker's behavior. However, most of\nthem lack the required realism to engage and fool human attackers long-term.\nBeing easy to distinguish honeypots strongly hinders their effectiveness. This\ncan happen because they are too deterministic, lack adaptability, or lack\ndeepness. This work introduces shelLM, a dynamic and realistic software\nhoneypot based on Large Language Models that generates Linux-like shell output.\nWe designed and implemented shelLM using cloud-based LLMs. We evaluated if\nshelLM can generate output as expected from a real Linux shell. The evaluation\nwas done by asking cybersecurity researchers to use the honeypot and give\nfeedback if each answer from the honeypot was the expected one from a Linux\nshell. Results indicate that shelLM can create credible and dynamic answers\ncapable of addressing the limitations of current honeypots. ShelLM reached a\nTNR of 0.90, convincing humans it was consistent with a real Linux shell. The\nsource code and prompts for replicating the experiments have been publicly\navailable.", "published": "2023-08-31 22:05:46", "link": "http://arxiv.org/abs/2309.00155v3", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Companion Animal Disease Diagnostics based on Literal-aware Medical\n  Knowledge Graph Representation Learning", "abstract": "Knowledge graph (KG) embedding has been used to benefit the diagnosis of\nanimal diseases by analyzing electronic medical records (EMRs), such as notes\nand veterinary records. However, learning representations to capture entities\nand relations with literal information in KGs is challenging as the KGs show\nheterogeneous properties and various types of literal information. Meanwhile,\nthe existing methods mostly aim to preserve graph structures surrounding target\nnodes without considering different types of literals, which could also carry\nsignificant information. In this paper, we propose a knowledge graph embedding\nmodel for the efficient diagnosis of animal diseases, which could learn various\ntypes of literal information and graph structure and fuse them into unified\nrepresentations, namely LiteralKG. Specifically, we construct a knowledge graph\nthat is built from EMRs along with literal information collected from various\nanimal hospitals. We then fuse different types of entities and node feature\ninformation into unified vector representations through gate networks. Finally,\nwe propose a self-supervised learning task to learn graph structure in pretext\ntasks and then towards various downstream tasks. Experimental results on link\nprediction tasks demonstrate that our model outperforms the baselines that\nconsist of state-of-the-art models. The source code is available at\nhttps://github.com/NSLab-CUK/LiteralKG.", "published": "2023-08-31 08:14:07", "link": "http://arxiv.org/abs/2309.03219v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "GPT has become financially literate: Insights from financial literacy\n  tests of GPT and a preliminary test of how people use it as a source of\n  advice", "abstract": "We assess the ability of GPT -- a large language model -- to serve as a\nfinancial robo-advisor for the masses, by using a financial literacy test.\nDavinci and ChatGPT based on GPT-3.5 score 66% and 65% on the financial\nliteracy test, respectively, compared to a baseline of 33%. However, ChatGPT\nbased on GPT-4 achieves a near-perfect 99% score, pointing to financial\nliteracy becoming an emergent ability of state-of-the-art models. We use the\nJudge-Advisor System and a savings dilemma to illustrate how researchers might\nassess advice-utilization from large language models. We also present a number\nof directions for future research.", "published": "2023-08-31 12:53:52", "link": "http://arxiv.org/abs/2309.00649v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "econ.GN", "q-fin.EC", "I.2.7; J.4; H.3.3"], "primary_category": "cs.CL"}
{"title": "Sequential Pitch Distributions for Raga Detection", "abstract": "Raga is a fundamental melodic concept in Indian Art Music (IAM). It is\ncharacterized by complex patterns. All performances and compositions are based\non the raga framework. Raga and tonic detection have been a long-standing\nresearch problem in the field of Music Information Retrieval. In this paper, we\nattempt to detect the raga using a novel feature to extract sequential or\ntemporal information from an audio sample. We call these Sequential Pitch\nDistributions (SPD), which are distributions taken over pitch values between\ntwo given pitch values over time. We also achieve state-of-the-art results on\nboth Hindustani and Carnatic music raga data sets with an accuracy of 99% and\n88.13%, respectively. SPD gives a great boost in accuracy over a standard pitch\ndistribution. The main goal of this paper, however, is to present an\nalternative approach to modeling the temporal aspects of the melody and thereby\ndeducing the raga.", "published": "2023-08-31 03:15:26", "link": "http://arxiv.org/abs/2308.16421v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Supervised Contrastive Learning with Nearest Neighbor Search for Speech\n  Emotion Recognition", "abstract": "Speech Emotion Recognition (SER) is a challenging task due to limited data\nand blurred boundaries of certain emotions. In this paper, we present a\ncomprehensive approach to improve the SER performance throughout the model\nlifecycle, including pre-training, fine-tuning, and inference stages. To\naddress the data scarcity issue, we utilize a pre-trained model, wav2vec2.0.\nDuring fine-tuning, we propose a novel loss function that combines\ncross-entropy loss with supervised contrastive learning loss to improve the\nmodel's discriminative ability. This approach increases the inter-class\ndistances and decreases the intra-class distances, mitigating the issue of\nblurred boundaries. Finally, to leverage the improved distances, we propose an\ninterpolation method at the inference stage that combines the model prediction\nwith the output from a k-nearest neighbors model. Our experiments on IEMOCAP\ndemonstrate that our proposed methods outperform current state-of-the-art\nresults.", "published": "2023-08-31 06:45:23", "link": "http://arxiv.org/abs/2308.16485v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "RAMP: Retrieval-Augmented MOS Prediction via Confidence-based Dynamic\n  Weighting", "abstract": "Automatic Mean Opinion Score (MOS) prediction is crucial to evaluate the\nperceptual quality of the synthetic speech. While recent approaches using\npre-trained self-supervised learning (SSL) models have shown promising results,\nthey only partly address the data scarcity issue for the feature extractor.\nThis leaves the data scarcity issue for the decoder unresolved and leading to\nsuboptimal performance. To address this challenge, we propose a\nretrieval-augmented MOS prediction method, dubbed {\\bf RAMP}, to enhance the\ndecoder's ability against the data scarcity issue. A fusing network is also\nproposed to dynamically adjust the retrieval scope for each instance and the\nfusion weights based on the predictive confidence. Experimental results show\nthat our proposed method outperforms the existing methods in multiple\nscenarios.", "published": "2023-08-31 06:48:01", "link": "http://arxiv.org/abs/2308.16488v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "LightGrad: Lightweight Diffusion Probabilistic Model for Text-to-Speech", "abstract": "Recent advances in neural text-to-speech (TTS) models bring thousands of TTS\napplications into daily life, where models are deployed in cloud to provide\nservices for customs. Among these models are diffusion probabilistic models\n(DPMs), which can be stably trained and are more parameter-efficient compared\nwith other generative models. As transmitting data between customs and the\ncloud introduces high latency and the risk of exposing private data, deploying\nTTS models on edge devices is preferred. When implementing DPMs onto edge\ndevices, there are two practical problems. First, current DPMs are not\nlightweight enough for resource-constrained devices. Second, DPMs require many\ndenoising steps in inference, which increases latency. In this work, we present\nLightGrad, a lightweight DPM for TTS. LightGrad is equipped with a lightweight\nU-Net diffusion decoder and a training-free fast sampling technique, reducing\nboth model parameters and inference latency. Streaming inference is also\nimplemented in LightGrad to reduce latency further. Compared with Grad-TTS,\nLightGrad achieves 62.2% reduction in paramters, 65.7% reduction in latency,\nwhile preserving comparable speech quality on both Chinese Mandarin and English\nin 4 denoising steps.", "published": "2023-08-31 09:05:04", "link": "http://arxiv.org/abs/2308.16569v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Biased Journey of MSD_AUDIO.ZIP", "abstract": "The equitable distribution of academic data is crucial for ensuring equal\nresearch opportunities, and ultimately further progress. Yet, due to the\ncomplexity of using the API for audio data that corresponds to the Million Song\nDataset along with its misreporting (before 2016) and the discontinuation of\nthis API (after 2016), access to this data has become restricted to those\nwithin certain affiliations that are connected peer-to-peer. In this paper, we\ndelve into this issue, drawing insights from the experiences of 22 individuals\nwho either attempted to access the data or played a role in its creation. With\nthis, we hope to initiate more critical dialogue and more thoughtful\nconsideration with regard to access privilege in the MIR community.", "published": "2023-08-31 01:42:31", "link": "http://arxiv.org/abs/2308.16389v3", "categories": ["cs.SD", "cs.CY", "eess.AS"], "primary_category": "cs.SD"}
{"title": "PhonMatchNet: Phoneme-Guided Zero-Shot Keyword Spotting for User-Defined\n  Keywords", "abstract": "This study presents a novel zero-shot user-defined keyword spotting model\nthat utilizes the audio-phoneme relationship of the keyword to improve\nperformance. Unlike the previous approach that estimates at utterance level, we\nuse both utterance and phoneme level information. Our proposed method comprises\na two-stream speech encoder architecture, self-attention-based pattern\nextractor, and phoneme-level detection loss for high performance in various\npronunciation environments. Based on experimental results, our proposed model\noutperforms the baseline model and achieves competitive performance compared\nwith full-shot keyword spotting models. Our proposed model significantly\nimproves the EER and AUC across all datasets, including familiar words, proper\nnouns, and indistinguishable pronunciations, with an average relative\nimprovement of 67% and 80%, respectively. The implementation code of our\nproposed model is available at https://github.com/ncsoft/PhonMatchNet.", "published": "2023-08-31 07:48:24", "link": "http://arxiv.org/abs/2308.16511v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Dynamic nsNet2: Efficient Deep Noise Suppression with Early Exiting", "abstract": "Although deep learning has made strides in the field of deep noise\nsuppression, leveraging deep architectures on resource-constrained devices\nstill proved challenging. Therefore, we present an early-exiting model based on\nnsNet2 that provides several levels of accuracy and resource savings by halting\ncomputations at different stages. Moreover, we adapt the original architecture\nby splitting the information flow to take into account the injected dynamism.\nWe show the trade-offs between performance and computational complexity based\non established metrics.", "published": "2023-08-31 12:29:24", "link": "http://arxiv.org/abs/2308.16678v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Improving the Expressiveness of Singing Voice Synthesis with\n  BERT Derived Semantic Information", "abstract": "This paper presents an end-to-end high-quality singing voice synthesis (SVS)\nsystem that uses bidirectional encoder representation from Transformers (BERT)\nderived semantic embeddings to improve the expressiveness of the synthesized\nsinging voice. Based on the main architecture of recently proposed VISinger, we\nput forward several specific designs for expressive singing voice synthesis.\nFirst, different from the previous SVS models, we use text representation of\nlyrics extracted from pre-trained BERT as additional input to the model. The\nrepresentation contains information about semantics of the lyrics, which could\nhelp SVS system produce more expressive and natural voice. Second, we further\nintroduce an energy predictor to stabilize the synthesized voice and model the\nwider range of energy variations that also contribute to the expressiveness of\nsinging voice. Last but not the least, to attenuate the off-key issues, the\npitch predictor is re-designed to predict the real to note pitch ratio. Both\nobjective and subjective experimental results indicate that the proposed SVS\nsystem can produce singing voice with higher-quality outperforming VISinger.", "published": "2023-08-31 16:12:01", "link": "http://arxiv.org/abs/2308.16836v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ReZero: Region-customizable Sound Extraction", "abstract": "We introduce region-customizable sound extraction (ReZero), a general and\nflexible framework for the multi-channel region-wise sound extraction (R-SE)\ntask. R-SE task aims at extracting all active target sounds (e.g., human\nspeech) within a specific, user-defined spatial region, which is different from\nconventional and existing tasks where a blind separation or a fixed, predefined\nspatial region are typically assumed. The spatial region can be defined as an\nangular window, a sphere, a cone, or other geometric patterns. Being a solution\nto the R-SE task, the proposed ReZero framework includes (1) definitions of\ndifferent types of spatial regions, (2) methods for region feature extraction\nand aggregation, and (3) a multi-channel extension of the band-split RNN\n(BSRNN) model specified for the R-SE task. We design experiments for different\nmicrophone array geometries, different types of spatial regions, and\ncomprehensive ablation studies on different system configurations. Experimental\nresults on both simulated and real-recorded data demonstrate the effectiveness\nof ReZero. Demos are available at https://innerselfm.github.io/rezero/.", "published": "2023-08-31 17:53:31", "link": "http://arxiv.org/abs/2308.16892v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving vision-inspired keyword spotting using dynamic module skipping\n  in streaming conformer encoder", "abstract": "Using a vision-inspired keyword spotting framework, we propose an\narchitecture with input-dependent dynamic depth capable of processing streaming\naudio. Specifically, we extend a conformer encoder with trainable binary gates\nthat allow us to dynamically skip network modules according to the input audio.\nOur approach improves detection and localization accuracy on continuous speech\nusing Librispeech top-1000 most frequent words while maintaining a small memory\nfootprint. The inclusion of gates also reduces the average amount of processing\nwithout affecting the overall performance. These benefits are shown to be even\nmore pronounced using the Google speech commands dataset placed over background\nnoise where up to 97% of the processing is skipped on non-speech inputs,\ntherefore making our method particularly interesting for an always-on keyword\nspotter.", "published": "2023-08-31 21:25:57", "link": "http://arxiv.org/abs/2309.00140v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "RepCodec: A Speech Representation Codec for Speech Tokenization", "abstract": "With recent rapid growth of large language models (LLMs), discrete speech\ntokenization has played an important role for injecting speech into LLMs.\nHowever, this discretization gives rise to a loss of information, consequently\nimpairing overall performance. To improve the performance of these discrete\nspeech tokens, we present RepCodec, a novel speech representation codec for\nsemantic speech tokenization. In contrast to audio codecs which reconstruct the\nraw audio, RepCodec learns a vector quantization codebook through\nreconstructing speech representations from speech encoders like HuBERT or\ndata2vec. Together, the speech encoder, the codec encoder and the vector\nquantization codebook form a pipeline for converting speech waveforms into\nsemantic tokens. The extensive experiments illustrate that RepCodec, by virtue\nof its enhanced information retention capacity, significantly outperforms the\nwidely used k-means clustering approach in both speech understanding and\ngeneration. Furthermore, this superiority extends across various speech\nencoders and languages, affirming the robustness of RepCodec. We believe our\nmethod can facilitate large language modeling research on speech processing.", "published": "2023-08-31 23:26:10", "link": "http://arxiv.org/abs/2309.00169v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Small Footprint Few-shot Keyword Spotting with Supervision on\n  Auxiliary Data", "abstract": "Few-shot keyword spotting (FS-KWS) models usually require large-scale\nannotated datasets to generalize to unseen target keywords. However, existing\nKWS datasets are limited in scale and gathering keyword-like labeled data is\ncostly undertaking. To mitigate this issue, we propose a framework that uses\neasily collectible, unlabeled reading speech data as an auxiliary source.\nSelf-supervised learning has been widely adopted for learning representations\nfrom unlabeled data; however, it is known to be suitable for large models with\nenough capacity and is not practical for training a small footprint FS-KWS\nmodel. Instead, we automatically annotate and filter the data to construct a\nkeyword-like dataset, LibriWord, enabling supervision on auxiliary data. We\nthen adopt multi-task learning that helps the model to enhance the\nrepresentation power from out-of-domain auxiliary data. Our method notably\nimproves the performance over competitive methods in the FS-KWS benchmark.", "published": "2023-08-31 07:29:42", "link": "http://arxiv.org/abs/2309.00647v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
