{"title": "Topic Identification for Speech without ASR", "abstract": "Modern topic identification (topic ID) systems for speech use automatic\nspeech recognition (ASR) to produce speech transcripts, and perform supervised\nclassification on such ASR outputs. However, under resource-limited conditions,\nthe manually transcribed speech required to develop standard ASR systems can be\nseverely limited or unavailable. In this paper, we investigate alternative\nunsupervised solutions to obtaining tokenizations of speech in terms of a\nvocabulary of automatically discovered word-like or phoneme-like units, without\ndepending on the supervised training of ASR systems. Moreover, using automatic\nphoneme-like tokenizations, we demonstrate that a convolutional neural network\nbased framework for learning spoken document representations provides\ncompetitive performance compared to a standard bag-of-words representation, as\nevidenced by comprehensive topic ID evaluations on both single-label and\nmulti-label classification tasks.", "published": "2017-03-22 00:37:33", "link": "http://arxiv.org/abs/1703.07476v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical RNN with Static Sentence-Level Attention for Text-Based\n  Speaker Change Detection", "abstract": "Speaker change detection (SCD) is an important task in dialog modeling. Our\npaper addresses the problem of text-based SCD, which differs from existing\naudio-based studies and is useful in various scenarios, for example, processing\ndialog transcripts where speaker identities are missing (e.g., OpenSubtitle),\nand enhancing audio SCD with textual information. We formulate text-based SCD\nas a matching problem of utterances before and after a certain decision point;\nwe propose a hierarchical recurrent neural network (RNN) with static\nsentence-level attention. Experimental results show that neural networks\nconsistently achieve better performance than feature-based approaches, and that\nour attention-based model significantly outperforms non-attention neural\nnetworks.", "published": "2017-03-22 15:42:28", "link": "http://arxiv.org/abs/1703.07713v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Supervised Typing of Big Graphs using Semantic Embeddings", "abstract": "We propose a supervised algorithm for generating type embeddings in the same\nsemantic vector space as a given set of entity embeddings. The algorithm is\nagnostic to the derivation of the underlying entity embeddings. It does not\nrequire any manual feature engineering, generalizes well to hundreds of types\nand achieves near-linear scaling on Big Graphs containing many millions of\ntriples and instances by virtue of an incremental execution. We demonstrate the\nutility of the embeddings on a type recommendation task, outperforming a\nnon-parametric feature-agnostic baseline while achieving 15x speedup and\nnear-constant memory usage on a full partition of DBpedia. Using\nstate-of-the-art visualization, we illustrate the agreement of our\nextensionally derived DBpedia type embeddings with the manually curated domain\nontology. Finally, we use the embeddings to probabilistically cluster about 4\nmillion DBpedia instances into 415 types in the DBpedia ontology.", "published": "2017-03-22 18:20:07", "link": "http://arxiv.org/abs/1703.07805v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Gate Activation Signal Analysis for Gated Recurrent Neural Networks and\n  Its Correlation with Phoneme Boundaries", "abstract": "In this paper we analyze the gate activation signals inside the gated\nrecurrent neural networks, and find the temporal structure of such signals is\nhighly correlated with the phoneme boundaries. This correlation is further\nverified by a set of experiments for phoneme segmentation, in which better\nresults compared to standard approaches were obtained.", "published": "2017-03-22 10:08:51", "link": "http://arxiv.org/abs/1703.07588v2", "categories": ["cs.SD", "cs.CL", "cs.LG"], "primary_category": "cs.SD"}
{"title": "Direct Acoustics-to-Word Models for English Conversational Speech\n  Recognition", "abstract": "Recent work on end-to-end automatic speech recognition (ASR) has shown that\nthe connectionist temporal classification (CTC) loss can be used to convert\nacoustics to phone or character sequences. Such systems are used with a\ndictionary and separately-trained Language Model (LM) to produce word\nsequences. However, they are not truly end-to-end in the sense of mapping\nacoustics directly to words without an intermediate phone representation. In\nthis paper, we present the first results employing direct acoustics-to-word CTC\nmodels on two well-known public benchmark tasks: Switchboard and CallHome.\nThese models do not require an LM or even a decoder at run-time and hence\nrecognize speech with minimal complexity. However, due to the large number of\nword output units, CTC word models require orders of magnitude more data to\ntrain reliably compared to traditional systems. We present some techniques to\nmitigate this issue. Our CTC word model achieves a word error rate of\n13.0%/18.8% on the Hub5-2000 Switchboard/CallHome test sets without any LM or\ndecoder compared with 9.6%/16.0% for phone-based CTC with a 4-gram LM. We also\npresent rescoring results on CTC word model lattices to quantify the\nperformance benefits of a LM, and contrast the performance of word and phone\nCTC models.", "published": "2017-03-22 17:17:16", "link": "http://arxiv.org/abs/1703.07754v1", "categories": ["cs.CL", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Recognizing Multi-talker Speech with Permutation Invariant Training", "abstract": "In this paper, we propose a novel technique for direct recognition of\nmultiple speech streams given the single channel of mixed speech, without first\nseparating them. Our technique is based on permutation invariant training (PIT)\nfor automatic speech recognition (ASR). In PIT-ASR, we compute the average\ncross entropy (CE) over all frames in the whole utterance for each possible\noutput-target assignment, pick the one with the minimum CE, and optimize for\nthat assignment. PIT-ASR forces all the frames of the same speaker to be\naligned with the same output layer. This strategy elegantly solves the label\npermutation problem and speaker tracing problem in one shot. Our experiments on\nartificially mixed AMI data showed that the proposed approach is very\npromising.", "published": "2017-03-22 08:39:32", "link": "http://arxiv.org/abs/1704.01985v4", "categories": ["cs.SD", "cs.LG", "eess.AS", "I.2.7"], "primary_category": "cs.SD"}
