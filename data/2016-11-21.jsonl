{"title": "Text Classification Improved by Integrating Bidirectional LSTM with\n  Two-dimensional Max Pooling", "abstract": "Recurrent Neural Network (RNN) is one of the most popular architectures used\nin Natural Language Processsing (NLP) tasks because its recurrent structure is\nvery suitable to process variable-length text. RNN can utilize distributed\nrepresentations of words by first converting the tokens comprising each text\ninto vectors, which form a matrix. And this matrix includes two dimensions: the\ntime-step dimension and the feature vector dimension. Then most existing models\nusually utilize one-dimensional (1D) max pooling operation or attention-based\noperation only on the time-step dimension to obtain a fixed-length vector.\nHowever, the features on the feature vector dimension are not mutually\nindependent, and simply applying 1D pooling operation over the time-step\ndimension independently may destroy the structure of the feature\nrepresentation. On the other hand, applying two-dimensional (2D) pooling\noperation over the two dimensions may sample more meaningful features for\nsequence modeling tasks. To integrate the features on both dimensions of the\nmatrix, this paper explores applying 2D max pooling operation to obtain a\nfixed-length representation of the text. This paper also utilizes 2D\nconvolution to sample more meaningful information of the matrix. Experiments\nare conducted on six text classification tasks, including sentiment analysis,\nquestion classification, subjectivity classification and newsgroup\nclassification. Compared with the state-of-the-art models, the proposed models\nachieve excellent performance on 4 out of 6 tasks. Specifically, one of the\nproposed models achieves highest accuracy on Stanford Sentiment Treebank binary\nclassification and fine-grained classification tasks.", "published": "2016-11-21 03:26:29", "link": "http://arxiv.org/abs/1611.06639v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "False-Friend Detection and Entity Matching via Unsupervised\n  Transliteration", "abstract": "Transliterations play an important role in multilingual entity reference\nresolution, because proper names increasingly travel between languages in news\nand social media. Previous work associated with machine translation targets\ntransliteration only single between language pairs, focuses on specific classes\nof entities (such as cities and celebrities) and relies on manual curation,\nwhich limits the expression power of transliteration in multilingual\nenvironment.\n  By contrast, we present an unsupervised transliteration model covering 69\nmajor languages that can generate good transliterations for arbitrary strings\nbetween any language pair. Our model yields top-(1, 20, 100) averages of\n(32.85%, 60.44%, 83.20%) in matching gold standard transliteration compared to\nresults from a recently-published system of (26.71%, 50.27%, 72.79%). We also\nshow the quality of our model in detecting true and false friends from\nWikipedia high frequency lexicons. Our method indicates a strong signal of\npronunciation similarity and boosts the probability of finding true friends in\n68 out of 69 languages.", "published": "2016-11-21 11:07:11", "link": "http://arxiv.org/abs/1611.06722v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bidirectional Tree-Structured LSTM with Head Lexicalization", "abstract": "Sequential LSTM has been extended to model tree structures, giving\ncompetitive results for a number of tasks. Existing methods model constituent\ntrees by bottom-up combinations of constituent nodes, making direct use of\ninput word information only for leaf nodes. This is different from sequential\nLSTMs, which contain reference to input words for each node. In this paper, we\npropose a method for automatic head-lexicalization for tree-structure LSTMs,\npropagating head words from leaf nodes to every constituent node. In addition,\nenabled by head lexicalization, we build a tree LSTM in the top-down direction,\nwhich corresponds to bidirectional sequential LSTM structurally. Experiments\nshow that both extensions give better representations of tree structures. Our\nfinal model gives the best results on the Standford Sentiment Treebank and\nhighly competitive results on the TREC question type classification task.", "published": "2016-11-21 14:01:53", "link": "http://arxiv.org/abs/1611.06788v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ontology Driven Disease Incidence Detection on Twitter", "abstract": "In this work we address the issue of generic automated disease incidence\nmonitoring on twitter. We employ an ontology of disease related concepts and\nuse it to obtain a conceptual representation of tweets. Unlike previous key\nword based systems and topic modeling approaches, our ontological approach\nallows us to apply more stringent criteria for determining which messages are\nrelevant such as spatial and temporal characteristics whilst giving a stronger\nguarantee that the resulting models will perform well on new data that may be\nlexically divergent. We achieve this by training learners on concepts rather\nthan individual words. For training we use a dataset containing mentions of\ninfluenza and Listeria and use the learned models to classify datasets\ncontaining mentions of an arbitrary selection of other diseases. We show that\nour ontological approach achieves good performance on this task using a variety\nof Natural Language Processing Techniques. We also show that word vectors can\nbe learned directly from our concepts to achieve even better results.", "published": "2016-11-21 07:32:56", "link": "http://arxiv.org/abs/1611.06671v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Coherent Dialogue with Attention-based Language Models", "abstract": "We model coherent conversation continuation via RNN-based dialogue models\nequipped with a dynamic attention mechanism. Our attention-RNN language model\ndynamically increases the scope of attention on the history as the conversation\ncontinues, as opposed to standard attention (or alignment) models with a fixed\ninput scope in a sequence-to-sequence model. This allows each generated word to\nbe associated with the most relevant words in its corresponding conversation\nhistory. We evaluate the model on two popular dialogue datasets, the\nopen-domain MovieTriples dataset and the closed-domain Ubuntu Troubleshoot\ndataset, and achieve significant improvements over the state-of-the-art and\nbaselines on several metrics, including complementary diversity-based metrics,\nhuman evaluation, and qualitative visualizations. We also show that a vanilla\nRNN with dynamic attention outperforms more complex memory models (e.g., LSTM\nand GRU) by allowing for flexible, long-distance memory. We promote further\ncoherence via topic modeling-based reranking.", "published": "2016-11-21 20:25:19", "link": "http://arxiv.org/abs/1611.06997v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Learning for Lexicon-Based Classification", "abstract": "In lexicon-based classification, documents are assigned labels by comparing\nthe number of words that appear from two opposed lexicons, such as positive and\nnegative sentiment. Creating such words lists is often easier than labeling\ninstances, and they can be debugged by non-experts if classification\nperformance is unsatisfactory. However, there is little analysis or\njustification of this classification heuristic. This paper describes a set of\nassumptions that can be used to derive a probabilistic justification for\nlexicon-based classification, as well as an analysis of its expected accuracy.\nOne key assumption behind lexicon-based classification is that all words in\neach lexicon are equally predictive. This is rarely true in practice, which is\nwhy lexicon-based approaches are usually outperformed by supervised classifiers\nthat learn distinct weights on each word from labeled instances. This paper\nshows that it is possible to learn such weights without labeled data, by\nleveraging co-occurrence statistics across the lexicons. This offers the best\nof both worlds: light supervision in the form of lexicons, and data-driven\nclassification with higher accuracy than traditional word-counting heuristics.", "published": "2016-11-21 18:30:17", "link": "http://arxiv.org/abs/1611.06933v1", "categories": ["cs.LG", "cs.CL", "stat.ML", "I.2.6; I.2.7"], "primary_category": "cs.LG"}
{"title": "Statistical Learning for OCR Text Correction", "abstract": "The accuracy of Optical Character Recognition (OCR) is crucial to the success\nof subsequent applications used in text analyzing pipeline. Recent models of\nOCR post-processing significantly improve the quality of OCR-generated text,\nbut are still prone to suggest correction candidates from limited observations\nwhile insufficiently accounting for the characteristics of OCR errors. In this\npaper, we show how to enlarge candidate suggestion space by using external\ncorpus and integrating OCR-specific features in a regression approach to\ncorrect OCR-generated errors. The evaluation results show that our model can\ncorrect 61.5% of the OCR-errors (considering the top 1 suggestion) and 71.5% of\nthe OCR-errors (considering the top 3 suggestions), for cases where the\ntheoretical correction upper-bound is 78%.", "published": "2016-11-21 19:00:32", "link": "http://arxiv.org/abs/1611.06950v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Robust end-to-end deep audiovisual speech recognition", "abstract": "Speech is one of the most effective ways of communication among humans. Even\nthough audio is the most common way of transmitting speech, very important\ninformation can be found in other modalities, such as vision. Vision is\nparticularly useful when the acoustic signal is corrupted. Multi-modal speech\nrecognition however has not yet found wide-spread use, mostly because the\ntemporal alignment and fusion of the different information sources is\nchallenging.\n  This paper presents an end-to-end audiovisual speech recognizer (AVSR), based\non recurrent neural networks (RNN) with a connectionist temporal classification\n(CTC) loss function. CTC creates sparse \"peaky\" output activations, and we\nanalyze the differences in the alignments of output targets (phonemes or\nvisemes) between audio-only, video-only, and audio-visual feature\nrepresentations. We present the first such experiments on the large vocabulary\nIBM ViaVoice database, which outperform previously published approaches on\nphone accuracy in clean and noisy conditions.", "published": "2016-11-21 20:08:51", "link": "http://arxiv.org/abs/1611.06986v1", "categories": ["cs.CL", "cs.LG", "cs.SD"], "primary_category": "cs.CL"}
