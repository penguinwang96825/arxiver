{"title": "LLM-based Medical Assistant Personalization with Short- and Long-Term\n  Memory Coordination", "abstract": "Large Language Models (LLMs), such as GPT3.5, have exhibited remarkable\nproficiency in comprehending and generating natural language. On the other\nhand, medical assistants hold the potential to offer substantial benefits for\nindividuals. However, the exploration of LLM-based personalized medical\nassistant remains relatively scarce. Typically, patients converse differently\nbased on their background and preferences which necessitates the task of\nenhancing user-oriented medical assistant. While one can fully train an LLM for\nthis objective, the resource consumption is unaffordable. Prior research has\nexplored memory-based methods to enhance the response with aware of previous\nmistakes for new queries during a dialogue session. We contend that a mere\nmemory module is inadequate and fully training an LLM can be excessively\ncostly. In this study, we propose a novel computational bionic memory\nmechanism, equipped with a parameter-efficient fine-tuning (PEFT) schema, to\npersonalize medical assistants.", "published": "2023-09-21 00:34:33", "link": "http://arxiv.org/abs/2309.11696v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Embedding with Neural Probabilistic Prior", "abstract": "To improve word representation learning, we propose a probabilistic prior\nwhich can be seamlessly integrated with word embedding models. Different from\nprevious methods, word embedding is taken as a probabilistic generative model,\nand it enables us to impose a prior regularizing word representation learning.\nThe proposed prior not only enhances the representation of embedding vectors\nbut also improves the model's robustness and stability. The structure of the\nproposed prior is simple and effective, and it can be easily implemented and\nflexibly plugged in most existing word embedding models. Extensive experiments\nshow the proposed method improves word representation on various tasks.", "published": "2023-09-21 06:54:32", "link": "http://arxiv.org/abs/2309.11824v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Goal-Oriented Prompt Attack and Safety Evaluation for LLMs", "abstract": "Large Language Models (LLMs) presents significant priority in text\nunderstanding and generation. However, LLMs suffer from the risk of generating\nharmful contents especially while being employed to applications. There are\nseveral black-box attack methods, such as Prompt Attack, which can change the\nbehaviour of LLMs and induce LLMs to generate unexpected answers with harmful\ncontents. Researchers are interested in Prompt Attack and Defense with LLMs,\nwhile there is no publicly available dataset with high successful attacking\nrate to evaluate the abilities of defending prompt attack. In this paper, we\nintroduce a pipeline to construct high-quality prompt attack samples, along\nwith a Chinese prompt attack dataset called CPAD. Our prompts aim to induce\nLLMs to generate unexpected outputs with several carefully designed prompt\nattack templates and widely concerned attacking contents. Different from\nprevious datasets involving safety estimation, we construct the prompts\nconsidering three dimensions: contents, attacking methods and goals.\nEspecially, the attacking goals indicate the behaviour expected after\nsuccessfully attacking the LLMs, thus the responses can be easily evaluated and\nanalysed. We run several popular Chinese LLMs on our dataset, and the results\nshow that our prompts are significantly harmful to LLMs, with around 70% attack\nsuccess rate to GPT-3.5. CPAD is publicly available at\nhttps://github.com/liuchengyuan123/CPAD.", "published": "2023-09-21 07:07:49", "link": "http://arxiv.org/abs/2309.11830v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Sanitization of Large Language Models", "abstract": "We explore a knowledge sanitization approach to mitigate the privacy concerns\nassociated with large language models (LLMs). LLMs trained on a large corpus of\nWeb data can memorize and potentially reveal sensitive or confidential\ninformation, raising critical security concerns. Our technique efficiently\nfine-tunes these models using the Low-Rank Adaptation (LoRA) method, prompting\nthem to generate harmless responses such as ``I don't know'' when queried about\nspecific information. Experimental results in a closed-book question-answering\ntask show that our straightforward method not only minimizes particular\nknowledge leakage but also preserves the overall performance of LLMs. These two\nadvantages strengthen the defense against extraction attacks and reduces the\nemission of harmful content such as hallucinations.", "published": "2023-09-21 07:49:55", "link": "http://arxiv.org/abs/2309.11852v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntactic Variation Across the Grammar: Modelling a Complex Adaptive\n  System", "abstract": "While language is a complex adaptive system, most work on syntactic variation\nobserves a few individual constructions in isolation from the rest of the\ngrammar. This means that the grammar, a network which connects thousands of\nstructures at different levels of abstraction, is reduced to a few disconnected\nvariables. This paper quantifies the impact of such reductions by\nsystematically modelling dialectal variation across 49 local populations of\nEnglish speakers in 16 countries. We perform dialect classification with both\nan entire grammar as well as with isolated nodes within the grammar in order to\ncharacterize the syntactic differences between these dialects. The results\nshow, first, that many individual nodes within the grammar are subject to\nvariation but, in isolation, none perform as well as the grammar as a whole.\nThis indicates that an important part of syntactic variation consists of\ninteractions between different parts of the grammar. Second, the results show\nthat the similarity between dialects depends heavily on the sub-set of the\ngrammar being observed: for example, New Zealand English could be more similar\nto Australian English in phrasal verbs but at the same time more similar to UK\nEnglish in dative phrases.", "published": "2023-09-21 08:14:34", "link": "http://arxiv.org/abs/2309.11869v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "High-order Joint Constituency and Dependency Parsing", "abstract": "This work revisits the topic of jointly parsing constituency and dependency\ntrees, i.e., to produce compatible constituency and dependency trees\nsimultaneously for input sentences, which is attractive considering that the\ntwo types of trees are complementary in representing syntax. The original work\nof Zhou and Zhao (2019) performs joint parsing only at the inference phase.\nThey train two separate parsers under the multi-task learning framework (i.e.,\none shared encoder and two independent decoders). They design an ad-hoc dynamic\nprogramming-based decoding algorithm of $O(n^5)$ time complexity for finding\noptimal compatible tree pairs. Compared to their work, we make progress in\nthree aspects: (1) adopting a much more efficient decoding algorithm of\n$O(n^4)$ time complexity, (2) exploring joint modeling at the training phase,\ninstead of only at the inference phase, (3) proposing high-order scoring\ncomponents to promote constituent-dependency interaction. We conduct\nexperiments and analysis on seven languages, covering both rich-resource and\nlow-resource scenarios. Results and analysis show that joint modeling leads to\na modest overall performance boost over separate modeling, but substantially\nimproves the complete matching ratio of whole trees, thanks to the explicit\nmodeling of tree compatibility.", "published": "2023-09-21 08:45:41", "link": "http://arxiv.org/abs/2309.11888v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "InstructERC: Reforming Emotion Recognition in Conversation with\n  Multi-task Retrieval-Augmented Large Language Models", "abstract": "The field of emotion recognition of conversation (ERC) has been focusing on\nseparating sentence feature encoding and context modeling, lacking exploration\nin generative paradigms based on unified designs. In this study, we propose a\nnovel approach, InstructERC, to reformulate the ERC task from a discriminative\nframework to a generative framework based on Large Language Models (LLMs).\nInstructERC makes three significant contributions: (1) it introduces a simple\nyet effective retrieval template module, which helps the model explicitly\nintegrate multi-granularity dialogue supervision information. (2) We introduce\ntwo additional emotion alignment tasks, namely speaker identification and\nemotion prediction tasks, to implicitly model the dialogue role relationships\nand future emotional tendencies in conversations. (3) Pioneeringly, we unify\nemotion labels across benchmarks through the feeling wheel to fit real\napplication scenarios. InstructERC still perform impressively on this unified\ndataset. Our LLM-based plugin framework significantly outperforms all previous\nmodels and achieves comprehensive SOTA on three commonly used ERC datasets.\nExtensive analysis of parameter-efficient and data-scaling experiments provides\nempirical guidance for applying it in practical scenarios.", "published": "2023-09-21 09:22:07", "link": "http://arxiv.org/abs/2309.11911v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scaling up COMETKIWI: Unbabel-IST 2023 Submission for the Quality\n  Estimation Shared Task", "abstract": "We present the joint contribution of Unbabel and Instituto Superior T\\'ecnico\nto the WMT 2023 Shared Task on Quality Estimation (QE). Our team participated\non all tasks: sentence- and word-level quality prediction (task 1) and\nfine-grained error span detection (task 2). For all tasks, we build on the\nCOMETKIWI-22 model (Rei et al., 2022b). Our multilingual approaches are ranked\nfirst for all tasks, reaching state-of-the-art performance for quality\nestimation at word-, span- and sentence-level granularity. Compared to the\nprevious state-of-the-art COMETKIWI-22, we show large improvements in\ncorrelation with human judgements (up to 10 Spearman points). Moreover, we\nsurpass the second-best multilingual submission to the shared-task with up to\n3.8 absolute points.", "published": "2023-09-21 09:38:56", "link": "http://arxiv.org/abs/2309.11925v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Striking Gold in Advertising: Standardization and Exploration of Ad Text\n  Generation", "abstract": "In response to the limitations of manual ad creation, significant research\nhas been conducted in the field of automatic ad text generation (ATG). However,\nthe lack of comprehensive benchmarks and well-defined problem sets has made\ncomparing different methods challenging. To tackle these challenges, we\nstandardize the task of ATG and propose a first benchmark dataset, CAMERA,\ncarefully designed and enabling the utilization of multi-modal information and\nfacilitating industry-wise evaluations. Our extensive experiments with a\nvariety of nine baselines, from classical methods to state-of-the-art models\nincluding large language models (LLMs), show the current state and the\nremaining challenges. We also explore how existing metrics in ATG and an\nLLM-based evaluator align with human evaluations.", "published": "2023-09-21 12:51:24", "link": "http://arxiv.org/abs/2309.12030v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AceGPT, Localizing Large Language Models in Arabic", "abstract": "This paper is devoted to the development of a localized Large Language Model\n(LLM) specifically for Arabic, a language imbued with unique cultural\ncharacteristics inadequately addressed by current mainstream models.\nSignificant concerns emerge when addressing cultural sensitivity and local\nvalues. To address this, the paper proposes a comprehensive solution that\nincludes further pre-training with Arabic texts, Supervised Fine-Tuning (SFT)\nutilizing native Arabic instructions, and GPT-4 responses in Arabic, alongside\nReinforcement Learning with AI Feedback (RLAIF) employing a reward model\nattuned to local culture and values. The goal is to cultivate culturally\ncognizant and value-aligned Arabic LLMs capable of accommodating the diverse,\napplication-specific needs of Arabic-speaking communities.\n  Comprehensive evaluations reveal that the resulting model, dubbed `AceGPT',\nsets the state-of-the-art standard for open Arabic LLMs across various\nbenchmarks. Codes, data, and models are in\nhttps://github.com/FreedomIntelligence/AceGPT.", "published": "2023-09-21 13:20:13", "link": "http://arxiv.org/abs/2309.12053v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SemEval-2022 Task 7: Identifying Plausible Clarifications of Implicit\n  and Underspecified Phrases in Instructional Texts", "abstract": "We describe SemEval-2022 Task 7, a shared task on rating the plausibility of\nclarifications in instructional texts. The dataset for this task consists of\nmanually clarified how-to guides for which we generated alternative\nclarifications and collected human plausibility judgements. The task of\nparticipating systems was to automatically determine the plausibility of a\nclarification in the respective context. In total, 21 participants took part in\nthis task, with the best system achieving an accuracy of 68.9%. This report\nsummarizes the results and findings from 8 teams and their system descriptions.\nFinally, we show in an additional evaluation that predictions by the top\nparticipating team make it possible to identify contexts with multiple\nplausible clarifications with an accuracy of 75.2%.", "published": "2023-09-21 14:19:04", "link": "http://arxiv.org/abs/2309.12102v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Computational Analysis of Vagueness in Revisions of Instructional\n  Texts", "abstract": "WikiHow is an open-domain repository of instructional articles for a variety\nof tasks, which can be revised by users. In this paper, we extract pairwise\nversions of an instruction before and after a revision was made. Starting from\na noisy dataset of revision histories, we specifically extract and analyze\nedits that involve cases of vagueness in instructions. We further investigate\nthe ability of a neural model to distinguish between two versions of an\ninstruction in our data by adopting a pairwise ranking task from previous work\nand showing improvements over existing baselines.", "published": "2023-09-21 14:26:04", "link": "http://arxiv.org/abs/2309.12107v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How-to Guides for Specific Audiences: A Corpus and Initial Findings", "abstract": "Instructional texts for specific target groups should ideally take into\naccount the prior knowledge and needs of the readers in order to guide them\nefficiently to their desired goals. However, targeting specific groups also\ncarries the risk of reflecting disparate social norms and subtle stereotypes.\nIn this paper, we investigate the extent to which how-to guides from one\nparticular platform, wikiHow, differ in practice depending on the intended\naudience. We conduct two case studies in which we examine qualitative features\nof texts written for specific audiences. In a generalization study, we\ninvestigate which differences can also be systematically demonstrated using\ncomputational methods. The results of our studies show that guides from\nwikiHow, like other text genres, are subject to subtle biases. We aim to raise\nawareness of these inequalities as a first step to addressing them in future\nwork.", "published": "2023-09-21 14:35:42", "link": "http://arxiv.org/abs/2309.12117v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Code Soliloquies for Accurate Calculations in Large Language Models", "abstract": "High-quality conversational datasets are crucial for the successful\ndevelopment of Intelligent Tutoring Systems (ITS) that utilize a Large Language\nModel (LLM) backend. Synthetic student-teacher dialogues, generated using\nadvanced GPT-4 models, are a common strategy for creating these datasets.\nHowever, subjects like physics that entail complex calculations pose a\nchallenge. While GPT-4 presents impressive language processing capabilities,\nits limitations in fundamental mathematical reasoning curtail its efficacy for\nsuch subjects. To tackle this limitation, we introduce in this paper an\ninnovative stateful prompt design. Our design orchestrates a mock conversation\nwhere both student and tutorbot roles are simulated by GPT-4. Each student\nresponse triggers an internal monologue, or `code soliloquy' in the\nGPT-tutorbot, which assesses whether its subsequent response would necessitate\ncalculations. If a calculation is deemed necessary, it scripts the relevant\nPython code and uses the Python output to construct a response to the student.\nOur approach notably enhances the quality of synthetic conversation datasets,\nespecially for subjects that are calculation-intensive. Our preliminary Subject\nMatter Expert evaluations reveal that our Higgs model, a fine-tuned LLaMA\nmodel, effectively uses Python for computations, which significantly enhances\nthe accuracy and computational reliability of Higgs' responses. Code, models,\nand datasets is available at https://github.com/luffycodes/Tutorbot-Spock-Phys.", "published": "2023-09-21 15:16:58", "link": "http://arxiv.org/abs/2309.12161v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Answering Health-related Questions from Medical Videos: Datasets\n  and Approaches", "abstract": "The increase in the availability of online videos has transformed the way we\naccess information and knowledge. A growing number of individuals now prefer\ninstructional videos as they offer a series of step-by-step procedures to\naccomplish particular tasks. The instructional videos from the medical domain\nmay provide the best possible visual answers to first aid, medical emergency,\nand medical education questions. Toward this, this paper is focused on\nanswering health-related questions asked by the public by providing visual\nanswers from medical videos. The scarcity of large-scale datasets in the\nmedical domain is a key challenge that hinders the development of applications\nthat can help the public with their health-related questions. To address this\nissue, we first proposed a pipelined approach to create two large-scale\ndatasets: HealthVidQA-CRF and HealthVidQA-Prompt. Later, we proposed monomodal\nand multimodal approaches that can effectively provide visual answers from\nmedical videos to natural language questions. We conducted a comprehensive\nanalysis of the results, focusing on the impact of the created datasets on\nmodel training and the significance of visual features in enhancing the\nperformance of the monomodal and multi-modal approaches. Our findings suggest\nthat these datasets have the potential to enhance the performance of medical\nvisual answer localization tasks and provide a promising future direction to\nfurther enhance the performance by using pre-trained language-vision models.", "published": "2023-09-21 16:21:28", "link": "http://arxiv.org/abs/2309.12224v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Relationship between Skill Neurons and Robustness in Prompt\n  Tuning", "abstract": "Prompt Tuning is a popular parameter-efficient finetuning method for\npre-trained large language models (PLMs). Based on experiments with RoBERTa, it\nhas been suggested that Prompt Tuning activates specific neurons in the\ntransformer's feed-forward networks, that are highly predictive and selective\nfor the given task. In this paper, we study the robustness of Prompt Tuning in\nrelation to these \"skill neurons\", using RoBERTa and T5. We show that prompts\ntuned for a specific task are transferable to tasks of the same type but are\nnot very robust to adversarial data. While prompts tuned for RoBERTa yield\nbelow-chance performance on adversarial data, prompts tuned for T5 are slightly\nmore robust and retain above-chance performance in two out of three cases. At\nthe same time, we replicate the finding that skill neurons exist in RoBERTa and\nfurther show that skill neurons also exist in T5. Interestingly, the skill\nneurons of T5 determined on non-adversarial data are also among the most\npredictive neurons on the adversarial data, which is not the case for RoBERTa.\nWe conclude that higher adversarial robustness may be related to a model's\nability to consistently activate the relevant skill neurons on adversarial\ndata.", "published": "2023-09-21 17:13:21", "link": "http://arxiv.org/abs/2309.12263v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inspire the Large Language Model by External Knowledge on BioMedical\n  Named Entity Recognition", "abstract": "Large language models (LLMs) have demonstrated dominating performance in many\nNLP tasks, especially on generative tasks. However, they often fall short in\nsome information extraction tasks, particularly those requiring domain-specific\nknowledge, such as Biomedical Named Entity Recognition (NER). In this paper,\ninspired by Chain-of-thought, we leverage the LLM to solve the Biomedical NER\nstep-by-step: break down the NER task into entity span extraction and entity\ntype determination. Additionally, for entity type determination, we inject\nentity knowledge to address the problem that LLM's lack of domain knowledge\nwhen predicting entity category. Experimental results show a significant\nimprovement in our two-step BioNER approach compared to previous few-shot LLM\nbaseline. Additionally, the incorporation of external knowledge significantly\nenhances entity category determination performance.", "published": "2023-09-21 17:39:53", "link": "http://arxiv.org/abs/2309.12278v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reranking for Natural Language Generation from Logical Forms: A Study\n  based on Large Language Models", "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language generation. However, their output quality can be inconsistent,\nposing challenges for generating natural language from logical forms (LFs).\nThis task requires the generated outputs to embody the exact semantics of LFs,\nwithout missing any LF semantics or creating any hallucinations. In this work,\nwe tackle this issue by proposing a novel generate-and-rerank approach. Our\napproach involves initially generating a set of candidate outputs by prompting\nan LLM and subsequently reranking them using a task-specific reranker model. In\naddition, we curate a manually collected dataset to evaluate the alignment\nbetween different ranking metrics and human judgements. The chosen ranking\nmetrics are utilized to enhance the training and evaluation of the reranker\nmodel. By conducting extensive experiments on three diverse datasets, we\ndemonstrate that the candidates selected by our reranker outperform those\nselected by baseline methods in terms of semantic consistency and fluency, as\nmeasured by three comprehensive metrics. Our findings provide strong evidence\nfor the effectiveness of our approach in improving the quality of generated\noutputs.", "published": "2023-09-21 17:54:58", "link": "http://arxiv.org/abs/2309.12294v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Foundation Metrics for Evaluating Effectiveness of Healthcare\n  Conversations Powered by Generative AI", "abstract": "Generative Artificial Intelligence is set to revolutionize healthcare\ndelivery by transforming traditional patient care into a more personalized,\nefficient, and proactive process. Chatbots, serving as interactive\nconversational models, will probably drive this patient-centered transformation\nin healthcare. Through the provision of various services, including diagnosis,\npersonalized lifestyle recommendations, and mental health support, the\nobjective is to substantially augment patient health outcomes, all the while\nmitigating the workload burden on healthcare providers. The life-critical\nnature of healthcare applications necessitates establishing a unified and\ncomprehensive set of evaluation metrics for conversational models. Existing\nevaluation metrics proposed for various generic large language models (LLMs)\ndemonstrate a lack of comprehension regarding medical and health concepts and\ntheir significance in promoting patients' well-being. Moreover, these metrics\nneglect pivotal user-centered aspects, including trust-building, ethics,\npersonalization, empathy, user comprehension, and emotional support. The\npurpose of this paper is to explore state-of-the-art LLM-based evaluation\nmetrics that are specifically applicable to the assessment of interactive\nconversational models in healthcare. Subsequently, we present an comprehensive\nset of evaluation metrics designed to thoroughly assess the performance of\nhealthcare chatbots from an end-user perspective. These metrics encompass an\nevaluation of language processing abilities, impact on real-world clinical\ntasks, and effectiveness in user-interactive conversations. Finally, we engage\nin a discussion concerning the challenges associated with defining and\nimplementing these metrics, with particular emphasis on confounding factors\nsuch as the target audience, evaluation methods, and prompt techniques involved\nin the evaluation process.", "published": "2023-09-21 19:36:48", "link": "http://arxiv.org/abs/2309.12444v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ContextRef: Evaluating Referenceless Metrics For Image Description\n  Generation", "abstract": "Referenceless metrics (e.g., CLIPScore) use pretrained vision--language\nmodels to assess image descriptions directly without costly ground-truth\nreference texts. Such methods can facilitate rapid progress, but only if they\ntruly align with human preference judgments. In this paper, we introduce\nContextRef, a benchmark for assessing referenceless metrics for such alignment.\nContextRef has two components: human ratings along a variety of established\nquality dimensions, and ten diverse robustness checks designed to uncover\nfundamental weaknesses. A crucial aspect of ContextRef is that images and\ndescriptions are presented in context, reflecting prior work showing that\ncontext is important for description quality. Using ContextRef, we assess a\nvariety of pretrained models, scoring functions, and techniques for\nincorporating context. None of the methods is successful with ContextRef, but\nwe show that careful fine-tuning yields substantial improvements. ContextRef\nremains a challenging benchmark though, in large part due to the challenge of\ncontext dependence.", "published": "2023-09-21 01:17:33", "link": "http://arxiv.org/abs/2309.11710v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "SLHCat: Mapping Wikipedia Categories and Lists to DBpedia by Leveraging\n  Semantic, Lexical, and Hierarchical Features", "abstract": "Wikipedia articles are hierarchically organized through categories and lists,\nproviding one of the most comprehensive and universal taxonomy, but its open\ncreation is causing redundancies and inconsistencies. Assigning DBPedia classes\nto Wikipedia categories and lists can alleviate the problem, realizing a large\nknowledge graph which is essential for categorizing digital contents through\nentity linking and typing. However, the existing approach of CaLiGraph is\nproducing incomplete and non-fine grained mappings. In this paper, we tackle\nthe problem as ontology alignment, where structural information of knowledge\ngraphs and lexical and semantic features of ontology class names are utilized\nto discover confident mappings, which are in turn utilized for finetuing\npretrained language models in a distant supervision fashion. Our method SLHCat\nconsists of two main parts: 1) Automatically generating training data by\nleveraging knowledge graph structure, semantic similarities, and named entity\ntyping. 2) Finetuning and prompt-tuning of the pre-trained language model BERT\nare carried out over the training data, to capture semantic and syntactic\nproperties of class names. Our model SLHCat is evaluated over a benchmark\ndataset constructed by annotating 3000 fine-grained CaLiGraph-DBpedia mapping\npairs. SLHCat is outperforming the baseline model by a large margin of 25% in\naccuracy, offering a practical solution for large-scale ontology mapping.", "published": "2023-09-21 05:38:14", "link": "http://arxiv.org/abs/2309.11791v2", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "Evaluating Large Language Models for Document-grounded Response\n  Generation in Information-Seeking Dialogues", "abstract": "In this paper, we investigate the use of large language models (LLMs) like\nChatGPT for document-grounded response generation in the context of\ninformation-seeking dialogues. For evaluation, we use the MultiDoc2Dial corpus\nof task-oriented dialogues in four social service domains previously used in\nthe DialDoc 2022 Shared Task. Information-seeking dialogue turns are grounded\nin multiple documents providing relevant information. We generate dialogue\ncompletion responses by prompting a ChatGPT model, using two methods:\nChat-Completion and LlamaIndex. ChatCompletion uses knowledge from ChatGPT\nmodel pretraining while LlamaIndex also extracts relevant information from\ndocuments. Observing that document-grounded response generation via LLMs cannot\nbe adequately assessed by automatic evaluation metrics as they are\nsignificantly more verbose, we perform a human evaluation where annotators rate\nthe output of the shared task winning system, the two Chat-GPT variants\noutputs, and human responses. While both ChatGPT variants are more likely to\ninclude information not present in the relevant segments, possibly including a\npresence of hallucinations, they are rated higher than both the shared task\nwinning system and human responses.", "published": "2023-09-21 07:28:03", "link": "http://arxiv.org/abs/2309.11838v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "BitCoin: Bidirectional Tagging and Supervised Contrastive Learning based\n  Joint Relational Triple Extraction Framework", "abstract": "Relation triple extraction (RTE) is an essential task in information\nextraction and knowledge graph construction. Despite recent advancements,\nexisting methods still exhibit certain limitations. They just employ\ngeneralized pre-trained models and do not consider the specificity of RTE\ntasks. Moreover, existing tagging-based approaches typically decompose the RTE\ntask into two subtasks, initially identifying subjects and subsequently\nidentifying objects and relations. They solely focus on extracting relational\ntriples from subject to object, neglecting that once the extraction of a\nsubject fails, it fails in extracting all triples associated with that subject.\nTo address these issues, we propose BitCoin, an innovative Bidirectional\ntagging and supervised Contrastive learning based joint relational triple\nextraction framework. Specifically, we design a supervised contrastive learning\nmethod that considers multiple positives per anchor rather than restricting it\nto just one positive. Furthermore, a penalty term is introduced to prevent\nexcessive similarity between the subject and object. Our framework implements\ntaggers in two directions, enabling triples extraction from subject to object\nand object to subject. Experimental results show that BitCoin achieves\nstate-of-the-art results on the benchmark datasets and significantly improves\nthe F1 score on Normal, SEO, EPO, and multiple relation extraction tasks.", "published": "2023-09-21 07:55:54", "link": "http://arxiv.org/abs/2309.11853v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Focal Inferential Infusion Coupled with Tractable Density Discrimination\n  for Implicit Hate Detection", "abstract": "Although pretrained large language models (PLMs) have achieved\nstate-of-the-art on many natural language processing (NLP) tasks, they lack an\nunderstanding of subtle expressions of implicit hate speech. Various attempts\nhave been made to enhance the detection of implicit hate by augmenting external\ncontext or enforcing label separation via distance-based metrics. Combining\nthese two approaches, we introduce FiADD, a novel Focused Inferential Adaptive\nDensity Discrimination framework. FiADD enhances the PLM finetuning pipeline by\nbringing the surface form/meaning of an implicit hate speech closer to its\nimplied form while increasing the inter-cluster distance among various labels.\nWe test FiADD on three implicit hate datasets and observe significant\nimprovement in the two-way and three-way hate classification tasks. We further\nexperiment on the generalizability of FiADD on three other tasks, detecting\nsarcasm, irony, and stance, in which surface and implied forms differ, and\nobserve similar performance improvements. Consequently, we analyze the\ngenerated latent space to understand its evolution under FiADD, which\ncorroborates the advantage of employing FiADD for implicit hate speech\ndetection.", "published": "2023-09-21 08:59:24", "link": "http://arxiv.org/abs/2309.11896v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Rethinking the Evaluating Framework for Natural Language Understanding\n  in AI Systems: Language Acquisition as a Core for Future Metrics", "abstract": "In the burgeoning field of artificial intelligence (AI), the unprecedented\nprogress of large language models (LLMs) in natural language processing (NLP)\noffers an opportunity to revisit the entire approach of traditional metrics of\nmachine intelligence, both in form and content. As the realm of machine\ncognitive evaluation has already reached Imitation, the next step is an\nefficient Language Acquisition and Understanding. Our paper proposes a paradigm\nshift from the established Turing Test towards an all-embracing framework that\nhinges on language acquisition, taking inspiration from the recent advancements\nin LLMs. The present contribution is deeply tributary of the excellent work\nfrom various disciplines, point out the need to keep interdisciplinary bridges\nopen, and delineates a more robust and sustainable approach.", "published": "2023-09-21 11:34:52", "link": "http://arxiv.org/abs/2309.11981v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset", "abstract": "Studying how people interact with large language models (LLMs) in real-world\nscenarios is increasingly important due to their widespread use in various\napplications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset\ncontaining one million real-world conversations with 25 state-of-the-art LLMs.\nThis dataset is collected from 210K unique IP addresses in the wild on our\nVicuna demo and Chatbot Arena website. We offer an overview of the dataset's\ncontent, including its curation process, basic statistics, and topic\ndistribution, highlighting its diversity, originality, and scale. We\ndemonstrate its versatility through four use cases: developing content\nmoderation models that perform similarly to GPT-4, building a safety benchmark,\ntraining instruction-following models that perform similarly to Vicuna, and\ncreating challenging benchmark questions. We believe that this dataset will\nserve as a valuable resource for understanding and advancing LLM capabilities.\nThe dataset is publicly available at\nhttps://huggingface.co/datasets/lmsys/lmsys-chat-1m.", "published": "2023-09-21 12:13:55", "link": "http://arxiv.org/abs/2309.11998v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Benchmarking quantized LLaMa-based models on the Brazilian Secondary\n  School Exam", "abstract": "Although Large Language Models (LLMs) represent a revolution in the way we\ninteract with computers, allowing the construction of complex questions and the\nability to reason over a sequence of statements, their use is restricted due to\nthe need for dedicated hardware for execution. In this study, we evaluate the\nperformance of LLMs based on the 7 and 13 billion LLaMA models, subjected to a\nquantization process and run on home hardware. The models considered were\nAlpaca, Koala, and Vicuna. To evaluate the effectiveness of these models, we\ndeveloped a database containing 1,006 questions from the ENEM (Brazilian\nNational Secondary School Exam). Our analysis revealed that the best performing\nmodels achieved an accuracy of approximately 46% for the original texts of the\nPortuguese questions and 49% on their English translations. In addition, we\nevaluated the computational efficiency of the models by measuring the time\nrequired for execution. On average, the 7 and 13 billion LLMs took\napproximately 20 and 50 seconds, respectively, to process the queries on a\nmachine equipped with an AMD Ryzen 5 3600x processor", "published": "2023-09-21 13:39:54", "link": "http://arxiv.org/abs/2309.12071v1", "categories": ["cs.AI", "cs.CL", "53-04", "I.2.7; I.2.0"], "primary_category": "cs.AI"}
{"title": "Prompt Tuned Embedding Classification for Multi-Label Industry Sector\n  Allocation", "abstract": "Prompt Tuning is emerging as a scalable and cost-effective method to\nfine-tune Pretrained Language Models (PLMs), which are often referred to as\nLarge Language Models (LLMs). This study benchmarks the performance and\ncomputational efficiency of Prompt Tuning and baselines for multi-label text\nclassification. This is applied to the challenging task of classifying\ncompanies into an investment firm's proprietary industry taxonomy, supporting\ntheir thematic investment strategy. Text-to-text classification is frequently\nreported to outperform task-specific classification heads, but has several\nlimitations when applied to a multi-label classification problem where each\nlabel consists of multiple tokens: (a) Generated labels may not match any label\nin the label taxonomy; (b) The fine-tuning process lacks permutation invariance\nand is sensitive to the order of the provided labels; (c) The model provides\nbinary decisions rather than appropriate confidence scores. Limitation (a) is\naddressed by applying constrained decoding using Trie Search, which slightly\nimproves classification performance. All limitations (a), (b), and (c) are\naddressed by replacing the PLM's language head with a classification head,\nwhich is referred to as Prompt Tuned Embedding Classification (PTEC). This\nimproves performance significantly, while also reducing computational costs\nduring inference. In our industrial application, the training data is skewed\ntowards well-known companies. We confirm that the model's performance is\nconsistent across both well-known and less-known companies. Our overall results\nindicate the continuing need to adapt state-of-the-art methods to\ndomain-specific tasks, even in the era of PLMs with strong generalization\nabilities. We release our codebase and a benchmarking dataset at\nhttps://github.com/EQTPartners/PTEC.", "published": "2023-09-21 13:45:32", "link": "http://arxiv.org/abs/2309.12075v3", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7; I.2.0"], "primary_category": "cs.CL"}
{"title": "PEFTT: Parameter-Efficient Fine-Tuning for low-resource Tibetan\n  pre-trained language models", "abstract": "In this era of large language models (LLMs), the traditional training of\nmodels has become increasingly unimaginable for regular users and institutions.\nThe exploration of efficient fine-tuning for high-resource languages on these\nmodels is an undeniable trend that is gradually gaining popularity. However,\nthere has been very little exploration for various low-resource languages, such\nas Tibetan. Research in Tibetan NLP is inherently scarce and limited. While\nthere is currently no existing large language model for Tibetan due to its\nlow-resource nature, that day will undoubtedly arrive. Therefore, research on\nefficient fine-tuning for low-resource language models like Tibetan is highly\nnecessary. Our research can serve as a reference to fill this crucial gap.\nEfficient fine-tuning strategies for pre-trained language models (PLMs) in\nTibetan have seen minimal exploration. We conducted three types of efficient\nfine-tuning experiments on the publicly available TNCC-title dataset:\n\"prompt-tuning,\" \"Adapter lightweight fine-tuning,\" and \"prompt-tuning +\nAdapter fine-tuning.\" The experimental results demonstrate significant\nimprovements using these methods, providing valuable insights for advancing\nTibetan language applications in the context of pre-trained models.", "published": "2023-09-21 14:29:23", "link": "http://arxiv.org/abs/2309.12109v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "OSN-MDAD: Machine Translation Dataset for Arabic Multi-Dialectal\n  Conversations on Online Social Media", "abstract": "While resources for English language are fairly sufficient to understand\ncontent on social media, similar resources in Arabic are still immature. The\nmain reason that the resources in Arabic are insufficient is that Arabic has\nmany dialects in addition to the standard version (MSA). Arabs do not use MSA\nin their daily communications; rather, they use dialectal versions.\nUnfortunately, social users transfer this phenomenon into their use of social\nmedia platforms, which in turn has raised an urgent need for building suitable\nAI models for language-dependent applications. Existing machine translation\n(MT) systems designed for MSA fail to work well with Arabic dialects. In light\nof this, it is necessary to adapt to the informal nature of communication on\nsocial networks by developing MT systems that can effectively handle the\nvarious dialects of Arabic. Unlike for MSA that shows advanced progress in MT\nsystems, little effort has been exerted to utilize Arabic dialects for MT\nsystems. While few attempts have been made to build translation datasets for\ndialectal Arabic, they are domain dependent and are not OSN cultural-language\nfriendly. In this work, we attempt to alleviate these limitations by proposing\nan online social network-based multidialect Arabic dataset that is crafted by\ncontextually translating English tweets into four Arabic dialects: Gulf,\nYemeni, Iraqi, and Levantine. To perform the translation, we followed our\nproposed guideline framework for content translation, which could be\nuniversally applicable for translation between foreign languages and local\ndialects. We validated the authenticity of our proposed dataset by developing\nneural MT models for four Arabic dialects. Our results have shown a superior\nperformance of our NMT models trained using our dataset. We believe that our\ndataset can reliably serve as an Arabic multidialectal translation dataset for\ninformal MT tasks.", "published": "2023-09-21 14:58:50", "link": "http://arxiv.org/abs/2309.12137v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Bridging the Gaps of Both Modality and Language: Synchronous Bilingual\n  CTC for Speech Translation and Speech Recognition", "abstract": "In this study, we present synchronous bilingual Connectionist Temporal\nClassification (CTC), an innovative framework that leverages dual CTC to bridge\nthe gaps of both modality and language in the speech translation (ST) task.\nUtilizing transcript and translation as concurrent objectives for CTC, our\nmodel bridges the gap between audio and text as well as between source and\ntarget languages. Building upon the recent advances in CTC application, we\ndevelop an enhanced variant, BiL-CTC+, that establishes new state-of-the-art\nperformances on the MuST-C ST benchmarks under resource-constrained scenarios.\nIntriguingly, our method also yields significant improvements in speech\nrecognition performance, revealing the effect of cross-lingual learning on\ntranscription and demonstrating its broad applicability. The source code is\navailable at https://github.com/xuchennlp/S2T.", "published": "2023-09-21 16:28:42", "link": "http://arxiv.org/abs/2309.12234v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "SQUARE: Automatic Question Answering Evaluation using Multiple Positive\n  and Negative References", "abstract": "Evaluation of QA systems is very challenging and expensive, with the most\nreliable approach being human annotations of correctness of answers for\nquestions. Recent works (AVA, BEM) have shown that transformer LM encoder based\nsimilarity metrics transfer well for QA evaluation, but they are limited by the\nusage of a single correct reference answer. We propose a new evaluation metric:\nSQuArE (Sentence-level QUestion AnsweRing Evaluation), using multiple reference\nanswers (combining multiple correct and incorrect references) for sentence-form\nQA. We evaluate SQuArE on both sentence-level extractive (Answer Selection) and\ngenerative (GenQA) QA systems, across multiple academic and industrial\ndatasets, and show that it outperforms previous baselines and obtains the\nhighest correlation with human annotations.", "published": "2023-09-21 16:51:30", "link": "http://arxiv.org/abs/2309.12250v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language\n  Models", "abstract": "Large language models (LLMs) have pushed the limits of natural language\nunderstanding and exhibited excellent problem-solving ability. Despite the\ngreat success, most existing open-source LLMs (e.g., LLaMA-2) are still far\naway from satisfactory for solving mathematical problem due to the complex\nreasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned\nlanguage model that specializes in mathematical reasoning. Specifically, we\nstart by bootstrapping mathematical questions by rewriting the question from\nmultiple perspectives without extra knowledge, which results in a new dataset\ncalled MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA.\nExperimental results on two popular benchmarks (i.e., GSM8K and MATH) for\nmathematical reasoning demonstrate that MetaMath outperforms a suite of\nopen-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4%\non GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the same\nsize by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of\n82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release all the\nMetaMathQA dataset, the MetaMath models with different model sizes and the\ntraining code for public use.", "published": "2023-09-21 17:45:42", "link": "http://arxiv.org/abs/2309.12284v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Constraints First: A New MDD-based Model to Generate Sentences Under\n  Constraints", "abstract": "This paper introduces a new approach to generating strongly constrained\ntexts. We consider standardized sentence generation for the typical application\nof vision screening. To solve this problem, we formalize it as a discrete\ncombinatorial optimization problem and utilize multivalued decision diagrams\n(MDD), a well-known data structure to deal with constraints. In our context,\none key strength of MDD is to compute an exhaustive set of solutions without\nperforming any search. Once the sentences are obtained, we apply a language\nmodel (GPT-2) to keep the best ones. We detail this for English and also for\nFrench where the agreement and conjugation rules are known to be more complex.\nFinally, with the help of GPT-2, we get hundreds of bona-fide candidate\nsentences. When compared with the few dozen sentences usually available in the\nwell-known vision screening test (MNREAD), this brings a major breakthrough in\nthe field of standardized sentence generation. Also, as it can be easily\nadapted for other languages, it has the potential to make the MNREAD test even\nmore valuable and usable. More generally, this paper highlights MDD as a\nconvincing alternative for constrained text generation, especially when the\nconstraints are hard to satisfy, but also for many other prospects.", "published": "2023-09-21 18:29:52", "link": "http://arxiv.org/abs/2309.12415v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Can LLMs Augment Low-Resource Reading Comprehension Datasets?\n  Opportunities and Challenges", "abstract": "Large Language Models (LLMs) have demonstrated impressive zero shot\nperformance on a wide range of NLP tasks, demonstrating the ability to reason\nand apply commonsense. A relevant application is to use them for creating high\nquality synthetic datasets for downstream tasks. In this work, we probe whether\nGPT-4 can be used to augment existing extractive reading comprehension\ndatasets. Automating data annotation processes has the potential to save large\namounts of time, money and effort that goes into manually labelling datasets.\nIn this paper, we evaluate the performance of GPT-4 as a replacement for human\nannotators for low resource reading comprehension tasks, by comparing\nperformance after fine tuning, and the cost associated with annotation. This\nwork serves to be the first analysis of LLMs as synthetic data augmenters for\nQA systems, highlighting the unique opportunities and challenges. Additionally,\nwe release augmented versions of low resource datasets, that will allow the\nresearch community to create further benchmarks for evaluation of generated\ndatasets.", "published": "2023-09-21 18:48:02", "link": "http://arxiv.org/abs/2309.12426v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Active Learning for Multilingual Fingerspelling Corpora", "abstract": "We apply active learning to help with data scarcity problems in sign\nlanguages. In particular, we perform a novel analysis of the effect of\npre-training. Since many sign languages are linguistic descendants of French\nsign language, they share hand configurations, which pre-training can hopefully\nexploit. We test this hypothesis on American, Chinese, German, and Irish\nfingerspelling corpora. We do observe a benefit from pre-training, but this may\nbe due to visual rather than linguistic similarities", "published": "2023-09-21 19:36:22", "link": "http://arxiv.org/abs/2309.12443v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "HANS, are you clever? Clever Hans Effect Analysis of Neural Systems", "abstract": "Instruction-tuned Large Language Models (It-LLMs) have been exhibiting\noutstanding abilities to reason around cognitive states, intentions, and\nreactions of all people involved, letting humans guide and comprehend\nday-to-day social interactions effectively. In fact, several multiple-choice\nquestions (MCQ) benchmarks have been proposed to construct solid assessments of\nthe models' abilities. However, earlier works are demonstrating the presence of\ninherent \"order bias\" in It-LLMs, posing challenges to the appropriate\nevaluation. In this paper, we investigate It-LLMs' resilience abilities towards\na series of probing tests using four MCQ benchmarks. Introducing adversarial\nexamples, we show a significant performance gap, mainly when varying the order\nof the choices, which reveals a selection bias and brings into discussion\nreasoning abilities. Following a correlation between first positions and model\nchoices due to positional bias, we hypothesized the presence of structural\nheuristics in the decision-making process of the It-LLMs, strengthened by\nincluding significant examples in few-shot scenarios. Finally, by using the\nChain-of-Thought (CoT) technique, we elicit the model to reason and mitigate\nthe bias by obtaining more robust models.", "published": "2023-09-21 20:52:18", "link": "http://arxiv.org/abs/2309.12481v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring the Impact of Training Data Distribution and Subword\n  Tokenization on Gender Bias in Machine Translation", "abstract": "We study the effect of tokenization on gender bias in machine translation, an\naspect that has been largely overlooked in previous works. Specifically, we\nfocus on the interactions between the frequency of gendered profession names in\ntraining data, their representation in the subword tokenizer's vocabulary, and\ngender bias. We observe that female and non-stereotypical gender inflections of\nprofession names (e.g., Spanish \"doctora\" for \"female doctor\") tend to be split\ninto multiple subword tokens. Our results indicate that the imbalance of gender\nforms in the model's training corpus is a major factor contributing to gender\nbias and has a greater impact than subword splitting. We show that analyzing\nsubword splits provides good estimates of gender-form imbalance in the training\ndata and can be used even when the corpus is not publicly available. We also\ndemonstrate that fine-tuning just the token embedding layer can decrease the\ngap in gender prediction accuracy between female and male forms without\nimpairing the translation quality.", "published": "2023-09-21 21:21:55", "link": "http://arxiv.org/abs/2309.12491v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MiChao-HuaFen 1.0: A Specialized Pre-trained Corpus Dataset for\n  Domain-specific Large Models", "abstract": "With the advancement of deep learning technologies, general-purpose large\nmodels such as GPT-4 have demonstrated exceptional capabilities across various\ndomains. Nevertheless, there remains a demand for high-quality, domain-specific\noutputs in areas like healthcare, law, and finance. This paper first evaluates\nthe existing large models for specialized domains and discusses their\nlimitations. To cater to the specific needs of certain domains, we introduce\nthe ``MiChao-HuaFen 1.0'' pre-trained corpus dataset, tailored for the news and\ngovernmental sectors. The dataset, sourced from publicly available internet\ndata from 2022, underwent multiple rounds of cleansing and processing to ensure\nhigh quality and reliable origins, with provisions for consistent and stable\nupdates. This dataset not only supports the pre-training of large models for\nChinese vertical domains but also aids in propelling deep learning research and\napplications in related fields.", "published": "2023-09-21 09:02:28", "link": "http://arxiv.org/abs/2309.13079v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SPICED: News Similarity Detection Dataset with Multiple Topics and\n  Complexity Levels", "abstract": "The proliferation of news media outlets has increased the demand for\nintelligent systems capable of detecting redundant information in news articles\nin order to enhance user experience. However, the heterogeneous nature of news\ncan lead to spurious findings in these systems: Simple heuristics such as\nwhether a pair of news are both about politics can provide strong but deceptive\ndownstream performance. Segmenting news similarity datasets into topics\nimproves the training of these models by forcing them to learn how to\ndistinguish salient characteristics under more narrow domains. However, this\nrequires the existence of topic-specific datasets, which are currently lacking.\nIn this article, we propose a novel dataset of similar news, SPICED, which\nincludes seven topics: Crime & Law, Culture & Entertainment, Disasters &\nAccidents, Economy & Business, Politics & Conflicts, Science & Technology, and\nSports. Futhermore, we present four different levels of complexity,\nspecifically designed for news similarity detection task. We benchmarked the\ncreated datasets using MinHash, BERT, SBERT, and SimCSE models.", "published": "2023-09-21 10:55:26", "link": "http://arxiv.org/abs/2309.13080v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Discourse-level Multi-scale Prosodic Model for Fine-grained Emotion\n  Analysis", "abstract": "This paper explores predicting suitable prosodic features for fine-grained\nemotion analysis from the discourse-level text. To obtain fine-grained\nemotional prosodic features as predictive values for our model, we extract a\nphoneme-level Local Prosody Embedding sequence (LPEs) and a Global Style\nEmbedding as prosodic speech features from the speech with the help of a style\ntransfer model. We propose a Discourse-level Multi-scale text Prosodic Model\n(D-MPM) that exploits multi-scale text to predict these two prosodic features.\nThe proposed model can be used to analyze better emotional prosodic features\nand thus guide the speech synthesis model to synthesize more expressive speech.\nTo quantitatively evaluate the proposed model, we contribute a new and\nlarge-scale Discourse-level Chinese Audiobook (DCA) dataset with more than\n13,000 utterances annotated sequences to evaluate the proposed model.\nExperimental results on the DCA dataset show that the multi-scale text\ninformation effectively helps to predict prosodic features, and the\ndiscourse-level text improves both the overall coherence and the user\nexperience. More interestingly, although we aim at the synthesis effect of the\nstyle transfer model, the synthesized speech by the proposed text prosodic\nanalysis model is even better than the style transfer from the original speech\nin some user evaluation indicators.", "published": "2023-09-21 07:45:44", "link": "http://arxiv.org/abs/2309.11849v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio Contrastive based Fine-tuning", "abstract": "Audio classification plays a crucial role in speech and sound processing\ntasks with a wide range of applications. There still remains a challenge of\nstriking the right balance between fitting the model to the training data\n(avoiding overfitting) and enabling it to generalise well to a new domain.\nLeveraging the transferability of contrastive learning, we introduce Audio\nContrastive-based Fine-tuning (AudioConFit), an efficient approach\ncharacterised by robust generalisability. Empirical experiments on a variety of\naudio classification tasks demonstrate the effectiveness and robustness of our\napproach, which achieves state-of-the-art results in various settings.", "published": "2023-09-21 08:59:13", "link": "http://arxiv.org/abs/2309.11895v3", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Stock Market Sentiment Classification and Backtesting via Fine-tuned\n  BERT", "abstract": "With the rapid development of big data and computing devices, low-latency\nautomatic trading platforms based on real-time information acquisition have\nbecome the main components of the stock trading market, so the topic of\nquantitative trading has received widespread attention. And for non-strongly\nefficient trading markets, human emotions and expectations always dominate\nmarket trends and trading decisions. Therefore, this paper starts from the\ntheory of emotion, taking East Money as an example, crawling user comment\ntitles data from its corresponding stock bar and performing data cleaning.\nSubsequently, a natural language processing model BERT was constructed, and the\nBERT model was fine-tuned using existing annotated data sets. The experimental\nresults show that the fine-tuned model has different degrees of performance\nimprovement compared to the original model and the baseline model.\nSubsequently, based on the above model, the user comment data crawled is\nlabeled with emotional polarity, and the obtained label information is combined\nwith the Alpha191 model to participate in regression, and significant\nregression results are obtained. Subsequently, the regression model is used to\npredict the average price change for the next five days, and use it as a signal\nto guide automatic trading. The experimental results show that the\nincorporation of emotional factors increased the return rate by 73.8\\% compared\nto the baseline during the trading period, and by 32.41\\% compared to the\noriginal alpha191 model. Finally, we discuss the advantages and disadvantages\nof incorporating emotional factors into quantitative trading, and give possible\ndirections for further research in the future.", "published": "2023-09-21 11:26:36", "link": "http://arxiv.org/abs/2309.11979v1", "categories": ["q-fin.CP", "cs.CL", "cs.LG"], "primary_category": "q-fin.CP"}
{"title": "BELT:Bootstrapping Electroencephalography-to-Language Decoding and\n  Zero-Shot Sentiment Classification by Natural Language Supervision", "abstract": "This paper presents BELT, a novel model and learning framework for the\npivotal topic of brain-to-language translation research. The translation from\nnoninvasive brain signals into readable natural language has the potential to\npromote the application scenario as well as the development of brain-computer\ninterfaces (BCI) as a whole. The critical problem in brain signal decoding or\nbrain-to-language translation is the acquisition of semantically appropriate\nand discriminative EEG representation from a dataset of limited scale and\nquality. The proposed BELT method is a generic and efficient framework that\nbootstraps EEG representation learning using off-the-shelf large-scale\npretrained language models (LMs). With a large LM's capacity for understanding\nsemantic information and zero-shot generalization, BELT utilizes large LMs\ntrained on Internet-scale datasets to bring significant improvements to the\nunderstanding of EEG signals.\n  In particular, the BELT model is composed of a deep conformer encoder and a\nvector quantization encoder. Semantical EEG representation is achieved by a\ncontrastive learning step that provides natural language supervision. We\nachieve state-of-the-art results on two featuring brain decoding tasks\nincluding the brain-to-language translation and zero-shot sentiment\nclassification. Specifically, our model surpasses the baseline model on both\ntasks by 5.45% and over 10% and archives a 42.31% BLEU-1 score and 67.32%\nprecision on the main evaluation metrics for translation and zero-shot\nsentiment classification respectively.", "published": "2023-09-21 13:24:01", "link": "http://arxiv.org/abs/2309.12056v2", "categories": ["cs.AI", "cs.CL", "eess.SP"], "primary_category": "cs.AI"}
{"title": "ChaCha: Leveraging Large Language Models to Prompt Children to Share\n  Their Emotions about Personal Events", "abstract": "Children typically learn to identify and express emotions through sharing\ntheir stories and feelings with others, particularly their family. However, it\nis challenging for parents or siblings to have emotional communication with\nchildren since children are still developing their communication skills. We\npresent ChaCha, a chatbot that encourages and guides children to share personal\nevents and associated emotions. ChaCha combines a state machine and large\nlanguage models (LLMs) to keep the dialogue on track while carrying on\nfree-form conversations. Through an exploratory study with 20 children (aged\n8-12), we examine how ChaCha prompts children to share personal events and\nguides them to describe associated emotions. Participants perceived ChaCha as a\nclose friend and shared their stories on various topics, such as family trips\nand personal achievements. Based on the findings, we discuss opportunities for\nleveraging LLMs to design child-friendly chatbots to support children in\nsharing emotions.", "published": "2023-09-21 16:43:17", "link": "http://arxiv.org/abs/2309.12244v4", "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.2; I.2.7"], "primary_category": "cs.HC"}
{"title": "Bad Actor, Good Advisor: Exploring the Role of Large Language Models in\n  Fake News Detection", "abstract": "Detecting fake news requires both a delicate sense of diverse clues and a\nprofound understanding of the real-world background, which remains challenging\nfor detectors based on small language models (SLMs) due to their knowledge and\ncapability limitations. Recent advances in large language models (LLMs) have\nshown remarkable performance in various tasks, but whether and how LLMs could\nhelp with fake news detection remains underexplored. In this paper, we\ninvestigate the potential of LLMs in fake news detection. First, we conduct an\nempirical study and find that a sophisticated LLM such as GPT 3.5 could\ngenerally expose fake news and provide desirable multi-perspective rationales\nbut still underperforms the basic SLM, fine-tuned BERT. Our subsequent analysis\nattributes such a gap to the LLM's inability to select and integrate rationales\nproperly to conclude. Based on these findings, we propose that current LLMs may\nnot substitute fine-tuned SLMs in fake news detection but can be a good advisor\nfor SLMs by providing multi-perspective instructive rationales. To instantiate\nthis proposal, we design an adaptive rationale guidance network for fake news\ndetection (ARG), in which SLMs selectively acquire insights on news analysis\nfrom the LLMs' rationales. We further derive a rationale-free version of ARG by\ndistillation, namely ARG-D, which services cost-sensitive scenarios without\nquerying LLMs. Experiments on two real-world datasets demonstrate that ARG and\nARG-D outperform three types of baseline methods, including SLM-based,\nLLM-based, and combinations of small and large language models.", "published": "2023-09-21 16:47:30", "link": "http://arxiv.org/abs/2309.12247v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "The Cambridge Law Corpus: A Dataset for Legal AI Research", "abstract": "We introduce the Cambridge Law Corpus (CLC), a dataset for legal AI research.\nIt consists of over 250 000 court cases from the UK. Most cases are from the\n21st century, but the corpus includes cases as old as the 16th century. This\npaper presents the first release of the corpus, containing the raw text and\nmeta-data. Together with the corpus, we provide annotations on case outcomes\nfor 638 cases, done by legal experts. Using our annotated data, we have trained\nand evaluated case outcome extraction with GPT-3, GPT-4 and RoBERTa models to\nprovide benchmarks. We include an extensive legal and ethical discussion to\naddress the potentially sensitive nature of this material. As a consequence,\nthe corpus will only be released for research purposes under certain\nrestrictions.", "published": "2023-09-21 17:24:40", "link": "http://arxiv.org/abs/2309.12269v4", "categories": ["cs.CL", "cs.CY", "stat.AP"], "primary_category": "cs.CL"}
{"title": "Improving VTE Identification through Adaptive NLP Model Selection and\n  Clinical Expert Rule-based Classifier from Radiology Reports", "abstract": "Rapid and accurate identification of Venous thromboembolism (VTE), a severe\ncardiovascular condition including deep vein thrombosis (DVT) and pulmonary\nembolism (PE), is important for effective treatment. Leveraging Natural\nLanguage Processing (NLP) on radiology reports, automated methods have shown\npromising advancements in identifying VTE events from retrospective data\ncohorts or aiding clinical experts in identifying VTE events from radiology\nreports. However, effectively training Deep Learning (DL) and the NLP models is\nchallenging due to limited labeled medical text data, the complexity and\nheterogeneity of radiology reports, and data imbalance. This study proposes\nnovel method combinations of DL methods, along with data augmentation, adaptive\npre-trained NLP model selection, and a clinical expert NLP rule-based\nclassifier, to improve the accuracy of VTE identification in unstructured\n(free-text) radiology reports. Our experimental results demonstrate the model's\nefficacy, achieving an impressive 97\\% accuracy and 97\\% F1 score in predicting\nDVT, and an outstanding 98.3\\% accuracy and 98.4\\% F1 score in predicting PE.\nThese findings emphasize the model's robustness and its potential to\nsignificantly contribute to VTE research.", "published": "2023-09-21 17:29:37", "link": "http://arxiv.org/abs/2309.12273v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLMR: Real-time Prompting of Interactive Worlds using Large Language\n  Models", "abstract": "We present Large Language Model for Mixed Reality (LLMR), a framework for the\nreal-time creation and modification of interactive Mixed Reality experiences\nusing LLMs. LLMR leverages novel strategies to tackle difficult cases where\nideal training data is scarce, or where the design goal requires the synthesis\nof internal dynamics, intuitive analysis, or advanced interactivity. Our\nframework relies on text interaction and the Unity game engine. By\nincorporating techniques for scene understanding, task planning,\nself-debugging, and memory management, LLMR outperforms the standard GPT-4 by\n4x in average error rate. We demonstrate LLMR's cross-platform interoperability\nwith several example worlds, and evaluate it on a variety of creation and\nmodification tasks to show that it can produce and edit diverse objects, tools,\nand scenes. Finally, we conducted a usability study (N=11) with a diverse set\nthat revealed participants had positive experiences with the system and would\nuse it again.", "published": "2023-09-21 17:37:01", "link": "http://arxiv.org/abs/2309.12276v3", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.ET"], "primary_category": "cs.HC"}
{"title": "The Reversal Curse: LLMs trained on \"A is B\" fail to learn \"B is A\"", "abstract": "We expose a surprising failure of generalization in auto-regressive large\nlanguage models (LLMs). If a model is trained on a sentence of the form \"A is\nB\", it will not automatically generalize to the reverse direction \"B is A\".\nThis is the Reversal Curse. For instance, if a model is trained on \"Valentina\nTereshkova was the first woman to travel to space\", it will not automatically\nbe able to answer the question, \"Who was the first woman to travel to space?\".\nMoreover, the likelihood of the correct answer (\"Valentina Tershkova\") will not\nbe higher than for a random name. Thus, models do not generalize a prevalent\npattern in their training set: if \"A is B\" occurs, \"B is A\" is more likely to\noccur. It is worth noting, however, that if \"A is B\" appears in-context, models\ncan deduce the reverse relationship. We provide evidence for the Reversal Curse\nby finetuning GPT-3 and Llama-1 on fictitious statements such as \"Uriah\nHawthorne is the composer of Abyssal Melodies\" and showing that they fail to\ncorrectly answer \"Who composed Abyssal Melodies?\". The Reversal Curse is robust\nacross model sizes and model families and is not alleviated by data\naugmentation. We also evaluate ChatGPT (GPT-3.5 and GPT-4) on questions about\nreal-world celebrities, such as \"Who is Tom Cruise's mother? [A: Mary Lee\nPfeiffer]\" and the reverse \"Who is Mary Lee Pfeiffer's son?\". GPT-4 correctly\nanswers questions like the former 79% of the time, compared to 33% for the\nlatter.\n  Code available at: https://github.com/lukasberglund/reversal_curse.", "published": "2023-09-21 17:52:19", "link": "http://arxiv.org/abs/2309.12288v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models", "abstract": "We present LongLoRA, an efficient fine-tuning approach that extends the\ncontext sizes of pre-trained large language models (LLMs), with limited\ncomputation cost. Typically, training LLMs with long context sizes is\ncomputationally expensive, requiring extensive training hours and GPU\nresources. For example, training on the context length of 8192 needs 16x\ncomputational costs in self-attention layers as that of 2048. In this paper, we\nspeed up the context extension of LLMs in two aspects. On the one hand,\nalthough dense global attention is needed during inference, fine-tuning the\nmodel can be effectively and efficiently done by sparse local attention. The\nproposed shifted sparse attention effectively enables context extension,\nleading to non-trivial computation saving with similar performance to\nfine-tuning with vanilla attention. Particularly, it can be implemented with\nonly two lines of code in training, while being optional in inference. On the\nother hand, we revisit the parameter-efficient fine-tuning regime for context\nexpansion. Notably, we find that LoRA for context extension works well under\nthe premise of trainable embedding and normalization. LongLoRA combines this\nimproved LoRA with S^2-Attn. LongLoRA demonstrates strong empirical results on\nvarious tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends Llama2 7B\nfrom 4k context to 100k, or Llama2 70B to 32k on a single 8x A100 machine.\nLongLoRA extends models' context while retaining their original architectures,\nand is compatible with most existing techniques, like Flash-Attention2. In\naddition, we further conduct supervised fine-tuning with LongLoRA and our long\ninstruction-following LongAlpaca dataset.", "published": "2023-09-21 17:59:11", "link": "http://arxiv.org/abs/2309.12307v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Rehearsal: Simulating Conflict to Teach Conflict Resolution", "abstract": "Interpersonal conflict is an uncomfortable but unavoidable fact of life.\nNavigating conflict successfully is a skill -- one that can be learned through\ndeliberate practice -- but few have access to effective training or feedback.\nTo expand this access, we introduce Rehearsal, a system that allows users to\nrehearse conflicts with a believable simulated interlocutor, explore\ncounterfactual \"what if?\" scenarios to identify alternative conversational\npaths, and learn through feedback on how and when to apply specific conflict\nstrategies. Users can utilize Rehearsal to practice handling a variety of\npredefined conflict scenarios, from office disputes to relationship issues, or\nthey can choose to create their own setting. To enable Rehearsal, we develop\nIRP prompting, a method of conditioning output of a large language model on the\ninfluential Interest-Rights-Power (IRP) theory from conflict resolution.\nRehearsal uses IRP to generate utterances grounded in conflict resolution\ntheory, guiding users towards counterfactual conflict resolution strategies\nthat help de-escalate difficult conversations. In a between-subjects\nevaluation, 40 participants engaged in an actual conflict with a confederate\nafter training. Compared to a control group with lecture material covering the\nsame IRP theory, participants with simulated training from Rehearsal\nsignificantly improved their performance in the unaided conflict: they reduced\ntheir use of escalating competitive strategies by an average of 67%, while\ndoubling their use of cooperative strategies. Overall, Rehearsal highlights the\npotential effectiveness of language models as tools for learning and practicing\ninterpersonal skills.", "published": "2023-09-21 17:59:20", "link": "http://arxiv.org/abs/2309.12309v2", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "LongDocFACTScore: Evaluating the Factuality of Long Document Abstractive\n  Summarisation", "abstract": "Maintaining factual consistency is a critical issue in abstractive text\nsummarisation, however, it cannot be assessed by traditional automatic metrics\nused for evaluating text summarisation, such as ROUGE scoring. Recent efforts\nhave been devoted to developing improved metrics for measuring factual\nconsistency using pre-trained language models, but these metrics have\nrestrictive token limits, and are therefore not suitable for evaluating long\ndocument text summarisation. Moreover, there is limited research and resources\navailable for evaluating whether existing automatic evaluation metrics are fit\nfor purpose when applied in long document settings. In this work, we evaluate\nthe efficacy of automatic metrics for assessing the factual consistency of long\ndocument text summarisation. We create a human-annotated data set for\nevaluating automatic factuality metrics, LongSciVerify, which contains\nfine-grained factual consistency annotations for long document summaries from\nthe scientific domain. We also propose a new evaluation framework,\nLongDocFACTScore, which is suitable for evaluating long document summarisation.\nThis framework allows metrics to be efficiently extended to any length document\nand outperforms existing state-of-the-art metrics in its ability to correlate\nwith human measures of factuality when used to evaluate long document\nsummarisation data sets. We make our code and LongSciVerify data set publicly\navailable: https://github.com/jbshp/LongDocFACTScore.", "published": "2023-09-21 19:54:54", "link": "http://arxiv.org/abs/2309.12455v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Studying and improving reasoning in humans and machines", "abstract": "In the present study, we investigate and compare reasoning in large language\nmodels (LLM) and humans using a selection of cognitive psychology tools\ntraditionally dedicated to the study of (bounded) rationality. To do so, we\npresented to human participants and an array of pretrained LLMs new variants of\nclassical cognitive experiments, and cross-compared their performances. Our\nresults showed that most of the included models presented reasoning errors akin\nto those frequently ascribed to error-prone, heuristic-based human reasoning.\nNotwithstanding this superficial similarity, an in-depth comparison between\nhumans and LLMs indicated important differences with human-like reasoning, with\nmodels limitations disappearing almost entirely in more recent LLMs releases.\nMoreover, we show that while it is possible to devise strategies to induce\nbetter performance, humans and machines are not equally-responsive to the same\nprompting schemes. We conclude by discussing the epistemological implications\nand challenges of comparing human and machine behavior for both artificial\nintelligence and cognitive psychology.", "published": "2023-09-21 21:02:05", "link": "http://arxiv.org/abs/2309.12485v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph Embedding: An Overview", "abstract": "Many mathematical models have been leveraged to design embeddings for\nrepresenting Knowledge Graph (KG) entities and relations for link prediction\nand many downstream tasks. These mathematically-inspired models are not only\nhighly scalable for inference in large KGs, but also have many explainable\nadvantages in modeling different relation patterns that can be validated\nthrough both formal proofs and empirical results. In this paper, we make a\ncomprehensive overview of the current state of research in KG completion. In\nparticular, we focus on two main branches of KG embedding (KGE) design: 1)\ndistance-based methods and 2) semantic matching-based methods. We discover the\nconnections between recently proposed models and present an underlying trend\nthat might help researchers invent novel and more effective models. Next, we\ndelve into CompoundE and CompoundE3D, which draw inspiration from 2D and 3D\naffine operations, respectively. They encompass a broad spectrum of techniques\nincluding distance-based and semantic-based methods. We will also discuss an\nemerging approach for KG completion which leverages pre-trained language models\n(PLMs) and textual descriptions of entities and relations and offer insights\ninto the integration of KGE embedding methods with PLMs for KG completion.", "published": "2023-09-21 21:52:42", "link": "http://arxiv.org/abs/2309.12501v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Towards Lexical Analysis of Dog Vocalizations via Online Videos", "abstract": "Deciphering the semantics of animal language has been a grand challenge. This\nstudy presents a data-driven investigation into the semantics of dog\nvocalizations via correlating different sound types with consistent semantics.\nWe first present a new dataset of Shiba Inu sounds, along with contextual\ninformation such as location and activity, collected from YouTube with a\nwell-constructed pipeline. The framework is also applicable to other animal\nspecies. Based on the analysis of conditioned probability between dog\nvocalizations and corresponding location and activity, we discover supporting\nevidence for previous heuristic research on the semantic meaning of various dog\nsounds. For instance, growls can signify interactions. Furthermore, our study\nyields new insights that existing word types can be subdivided into\nfiner-grained subtypes and minimal semantic unit for Shiba Inu is word-related.\nFor example, whimper can be subdivided into two types, attention-seeking and\ndiscomfort.", "published": "2023-09-21 23:53:14", "link": "http://arxiv.org/abs/2309.13086v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "How Prevalent is Gender Bias in ChatGPT? -- Exploring German and English\n  ChatGPT Responses", "abstract": "With the introduction of ChatGPT, OpenAI made large language models (LLM)\naccessible to users with limited IT expertise. However, users with no\nbackground in natural language processing (NLP) might lack a proper\nunderstanding of LLMs. Thus the awareness of their inherent limitations, and\ntherefore will take the systems' output at face value. In this paper, we\nsystematically analyse prompts and the generated responses to identify possible\nproblematic issues with a special focus on gender biases, which users need to\nbe aware of when processing the system's output. We explore how ChatGPT reacts\nin English and German if prompted to answer from a female, male, or neutral\nperspective. In an in-depth investigation, we examine selected prompts and\nanalyse to what extent responses differ if the system is prompted several times\nin an identical way. On this basis, we show that ChatGPT is indeed useful for\nhelping non-IT users draft texts for their daily work. However, it is\nabsolutely crucial to thoroughly check the system's responses for biases as\nwell as for syntactic and grammatical mistakes.", "published": "2023-09-21 07:54:25", "link": "http://arxiv.org/abs/2310.03031v3", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language\n  Model as an Agent", "abstract": "3D visual grounding is a critical skill for household robots, enabling them\nto navigate, manipulate objects, and answer questions based on their\nenvironment. While existing approaches often rely on extensive labeled data or\nexhibit limitations in handling complex language queries, we propose\nLLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model\n(LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to\ndecompose complex natural language queries into semantic constituents and\nemploys a visual grounding tool, such as OpenScene or LERF, to identify objects\nin a 3D scene. The LLM then evaluates the spatial and commonsense relations\namong the proposed objects to make a final grounding decision. Our method does\nnot require any labeled training data and can generalize to novel 3D scenes and\narbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and\ndemonstrate state-of-the-art zero-shot grounding accuracy. Our findings\nindicate that LLMs significantly improve the grounding capability, especially\nfor complex language queries, making LLM-Grounder an effective approach for 3D\nvision-language tasks in robotics. Videos and interactive demos can be found on\nthe project website https://chat-with-nerf.github.io/ .", "published": "2023-09-21 17:59:45", "link": "http://arxiv.org/abs/2309.12311v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Multimodal Deep Learning for Scientific Imaging Interpretation", "abstract": "In the domain of scientific imaging, interpreting visual data often demands\nan intricate combination of human expertise and deep comprehension of the\nsubject materials. This study presents a novel methodology to linguistically\nemulate and subsequently evaluate human-like interactions with Scanning\nElectron Microscopy (SEM) images, specifically of glass materials. Leveraging a\nmultimodal deep learning framework, our approach distills insights from both\ntextual and visual data harvested from peer-reviewed articles, further\naugmented by the capabilities of GPT-4 for refined data synthesis and\nevaluation. Despite inherent challenges--such as nuanced interpretations and\nthe limited availability of specialized datasets--our model (GlassLLaVA) excels\nin crafting accurate interpretations, identifying key features, and detecting\ndefects in previously unseen SEM images. Moreover, we introduce versatile\nevaluation metrics, suitable for an array of scientific imaging applications,\nwhich allows for benchmarking against research-grounded answers. Benefiting\nfrom the robustness of contemporary Large Language Models, our model adeptly\naligns with insights from research papers. This advancement not only\nunderscores considerable progress in bridging the gap between human and machine\ninterpretation in scientific imaging, but also hints at expansive avenues for\nfuture research and broader application.", "published": "2023-09-21 20:09:22", "link": "http://arxiv.org/abs/2309.12460v2", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Leveraging In-the-Wild Data for Effective Self-Supervised Pretraining in\n  Speaker Recognition", "abstract": "Current speaker recognition systems primarily rely on supervised approaches,\nconstrained by the scale of labeled datasets. To boost the system performance,\nresearchers leverage large pretrained models such as WavLM to transfer learned\nhigh-level features to the downstream speaker recognition task. However, this\napproach introduces extra parameters as the pretrained model remains in the\ninference stage. Another group of researchers directly apply self-supervised\nmethods such as DINO to speaker embedding learning, yet they have not explored\nits potential on large-scale in-the-wild datasets. In this paper, we present\nthe effectiveness of DINO training on the large-scale WenetSpeech dataset and\nits transferability in enhancing the supervised system performance on the\nCNCeleb dataset. Additionally, we introduce a confidence-based data filtering\nalgorithm to remove unreliable data from the pretraining dataset, leading to\nbetter performance with less training data. The associated pretrained models,\nconfidence files, pretraining and finetuning scripts will be made available in\nthe Wespeaker toolkit.", "published": "2023-09-21 02:03:26", "link": "http://arxiv.org/abs/2309.11730v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Sparsely Shared LoRA on Whisper for Child Speech Recognition", "abstract": "Whisper is a powerful automatic speech recognition (ASR) model. Nevertheless,\nits zero-shot performance on low-resource speech requires further improvement.\nChild speech, as a representative type of low-resource speech, is leveraged for\nadaptation. Recently, parameter-efficient fine-tuning (PEFT) in NLP was shown\nto be comparable and even better than full fine-tuning, while only needing to\ntune a small set of trainable parameters. However, current PEFT methods have\nnot been well examined for their effectiveness on Whisper. In this paper, only\nparameter composition types of PEFT approaches such as LoRA and Bitfit are\ninvestigated as they do not bring extra inference costs. Different popular PEFT\nmethods are examined. Particularly, we compare LoRA and AdaLoRA and figure out\nthe learnable rank coefficient is a good design. Inspired by the sparse rank\ndistribution allocated by AdaLoRA, a novel PEFT approach Sparsely Shared LoRA\n(S2-LoRA) is proposed. The two low-rank decomposed matrices are globally\nshared. Each weight matrix only has to maintain its specific rank coefficients\nthat are constrained to be sparse. Experiments on low-resource Chinese child\nspeech show that with much fewer trainable parameters, S2-LoRA can achieve\ncomparable in-domain adaptation performance to AdaLoRA and exhibit better\ngeneralization ability on out-of-domain data. In addition, the rank\ndistribution automatically learned by S2-LoRA is found to have similar patterns\nto AdaLoRA's allocation.", "published": "2023-09-21 03:35:49", "link": "http://arxiv.org/abs/2309.11756v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CoMFLP: Correlation Measure based Fast Search on ASR Layer Pruning", "abstract": "Transformer-based speech recognition (ASR) model with deep layers exhibited\nsignificant performance improvement. However, the model is inefficient for\ndeployment on resource-constrained devices. Layer pruning (LP) is a commonly\nused compression method to remove redundant layers. Previous studies on LP\nusually identify the redundant layers according to a task-specific evaluation\nmetric. They are time-consuming for models with a large number of layers, even\nin a greedy search manner. To address this problem, we propose CoMFLP, a fast\nsearch LP algorithm based on correlation measure. The correlation between\nlayers is computed to generate a correlation matrix, which identifies the\nredundancy among layers. The search process is carried out in two steps: (1)\ncoarse search: to determine top $K$ candidates by pruning the most redundant\nlayers based on the correlation matrix; (2) fine search: to select the best\npruning proposal among $K$ candidates using a task-specific evaluation metric.\nExperiments on an ASR task show that the pruning proposal determined by CoMFLP\noutperforms existing LP methods while only requiring constant time complexity.\nThe code is publicly available at https://github.com/louislau1129/CoMFLP.", "published": "2023-09-21 04:02:00", "link": "http://arxiv.org/abs/2309.11768v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Impact of Silence on Speech Anti-Spoofing", "abstract": "The current speech anti-spoofing countermeasures (CMs) show excellent\nperformance on specific datasets. However, removing the silence of test speech\nthrough Voice Activity Detection (VAD) can severely degrade performance. In\nthis paper, the impact of silence on speech anti-spoofing is analyzed. First,\nthe reasons for the impact are explored, including the proportion of silence\nduration and the content of silence. The proportion of silence duration in\nspoof speech generated by text-to-speech (TTS) algorithms is lower than that in\nbonafide speech. And the content of silence generated by different waveform\ngenerators varies compared to bonafide speech. Then the impact of silence on\nmodel prediction is explored. Even after retraining, the spoof speech generated\nby neural network based end-to-end TTS algorithms suffers a significant rise in\nerror rates when the silence is removed. To demonstrate the reasons for the\nimpact of silence on CMs, the attention distribution of a CM is visualized\nthrough class activation mapping (CAM). Furthermore, the implementation and\nanalysis of the experiments masking silence or non-silence demonstrates the\nsignificance of the proportion of silence duration for detecting TTS and the\nimportance of silence content for detecting voice conversion (VC). Based on the\nexperimental results, improving the robustness of CMs against unknown spoofing\nattacks by masking silence is also proposed. Finally, the attacks on\nanti-spoofing CMs through concatenating silence, and the mitigation of VAD and\nsilence attack through low-pass filtering are introduced.", "published": "2023-09-21 06:59:22", "link": "http://arxiv.org/abs/2309.11827v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cluster-based pruning techniques for audio data", "abstract": "Deep learning models have become widely adopted in various domains, but their\nperformance heavily relies on a vast amount of data. Datasets often contain a\nlarge number of irrelevant or redundant samples, which can lead to\ncomputational inefficiencies during the training. In this work, we introduce,\nfor the first time in the context of the audio domain, the k-means clustering\nas a method for efficient data pruning. K-means clustering provides a way to\ngroup similar samples together, allowing the reduction of the size of the\ndataset while preserving its representative characteristics. As an example, we\nperform clustering analysis on the keyword spotting (KWS) dataset. We discuss\nhow k-means clustering can significantly reduce the size of audio datasets\nwhile maintaining the classification performance across neural networks (NNs)\nwith different architectures. We further comment on the role of scaling\nanalysis in identifying the optimal pruning strategies for a large number of\nsamples. Our studies serve as a proof-of-principle, demonstrating the potential\nof data selection with distance-based clustering algorithms for the audio\ndomain and highlighting promising research avenues.", "published": "2023-09-21 09:33:41", "link": "http://arxiv.org/abs/2309.11922v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-Channel MOSRA: Mean Opinion Score and Room Acoustics Estimation\n  Using Simulated Data and a Teacher Model", "abstract": "Previous methods for predicting room acoustic parameters and speech quality\nmetrics have focused on the single-channel case, where room acoustics and Mean\nOpinion Score (MOS) are predicted for a single recording device. However,\nquality-based device selection for rooms with multiple recording devices may\nbenefit from a multi-channel approach where the descriptive metrics are\npredicted for multiple devices in parallel. Following our hypothesis that a\nmodel may benefit from multi-channel training, we develop a multi-channel model\nfor joint MOS and room acoustics prediction (MOSRA) for five channels in\nparallel. The lack of multi-channel audio data with ground truth labels\nnecessitated the creation of simulated data using an acoustic simulator with\nroom acoustic labels extracted from the generated impulse responses and labels\nfor MOS generated in a student-teacher setup using a wav2vec2-based MOS\nprediction model. Our experiments show that the multi-channel model improves\nthe prediction of the direct-to-reverberation ratio, clarity, and speech\ntransmission index over the single-channel model with roughly 5$\\times$ less\ncomputation while suffering minimal losses in the performance of the other\nmetrics.", "published": "2023-09-21 11:21:52", "link": "http://arxiv.org/abs/2309.11976v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with\n  Multi-Scale Acoustic Prompts", "abstract": "Zero-shot text-to-speech (TTS) synthesis aims to clone any unseen speaker's\nvoice without adaptation parameters. By quantizing speech waveform into\ndiscrete acoustic tokens and modeling these tokens with the language model,\nrecent language model-based TTS models show zero-shot speaker adaptation\ncapabilities with only a 3-second acoustic prompt of an unseen speaker.\nHowever, they are limited by the length of the acoustic prompt, which makes it\ndifficult to clone personal speaking style. In this paper, we propose a novel\nzero-shot TTS model with the multi-scale acoustic prompts based on a neural\ncodec language model VALL-E. A speaker-aware text encoder is proposed to learn\nthe personal speaking style at the phoneme-level from the style prompt\nconsisting of multiple sentences. Following that, a VALL-E based acoustic\ndecoder is utilized to model the timbre from the timbre prompt at the\nframe-level and generate speech. The experimental results show that our\nproposed method outperforms baselines in terms of naturalness and speaker\nsimilarity, and can achieve better performance by scaling out to a longer style\nprompt.", "published": "2023-09-21 11:22:22", "link": "http://arxiv.org/abs/2309.11977v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Multiscale Autoencoder (MSAE) Framework for End-to-End Neural Network\n  Speech Enhancement", "abstract": "Neural network approaches to single-channel speech enhancement have received\nmuch recent attention. In particular, mask-based architectures have achieved\nsignificant performance improvements over conventional methods. This paper\nproposes a multiscale autoencoder (MSAE) for mask-based end-to-end neural\nnetwork speech enhancement. The MSAE performs spectral decomposition of an\ninput waveform within separate band-limited branches, each operating with a\ndifferent rate and scale, to extract a sequence of multiscale embeddings. The\nproposed framework features intuitive parameterization of the autoencoder,\nincluding a flexible spectral band design based on the Constant-Q transform.\nAdditionally, the MSAE is constructed entirely of differentiable operators,\nallowing it to be implemented within an end-to-end neural network, and be\ndiscriminatively trained. The MSAE draws motivation both from recent multiscale\nnetwork topologies and from traditional multiresolution transforms in speech\nprocessing. Experimental results show the MSAE to provide clear performance\nbenefits relative to conventional single-branch autoencoders. Additionally, the\nproposed framework is shown to outperform a variety of state-of-the-art\nenhancement systems, both in terms of objective speech quality metrics and\nautomatic speech recognition accuracy.", "published": "2023-09-21 14:41:54", "link": "http://arxiv.org/abs/2309.12121v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Profile-Error-Tolerant Target-Speaker Voice Activity Detection", "abstract": "Target-Speaker Voice Activity Detection (TS-VAD) utilizes a set of speaker\nprofiles alongside an input audio signal to perform speaker diarization. While\nits superiority over conventional methods has been demonstrated, the method can\nsuffer from errors in speaker profiles, as those profiles are typically\nobtained by running a traditional clustering-based diarization method over the\ninput signal. This paper proposes an extension to TS-VAD, called\nProfile-Error-Tolerant TS-VAD (PET-TSVAD), which is robust to such speaker\nprofile errors. This is achieved by employing transformer-based TS-VAD that can\nhandle a variable number of speakers and further introducing a set of\nadditional pseudo-speaker profiles to handle speakers undetected during the\nfirst pass diarization. During training, we use speaker profiles estimated by\nmultiple different clustering algorithms to reduce the mismatch between the\ntraining and testing conditions regarding speaker profiles. Experimental\nresults show that PET-TSVAD consistently outperforms the existing TS-VAD method\non both the VoxConverse and DIHARD-I datasets.", "published": "2023-09-21 22:49:38", "link": "http://arxiv.org/abs/2309.12521v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FluentEditor: Text-based Speech Editing by Considering Acoustic and\n  Prosody Consistency", "abstract": "Text-based speech editing (TSE) techniques are designed to enable users to\nedit the output audio by modifying the input text transcript instead of the\naudio itself. Despite much progress in neural network-based TSE techniques, the\ncurrent techniques have focused on reducing the difference between the\ngenerated speech segment and the reference target in the editing region,\nignoring its local and global fluency in the context and original utterance. To\nmaintain the speech fluency, we propose a fluency speech editing model, termed\n\\textit{FluentEditor}, by considering fluency-aware training criterion in the\nTSE training. Specifically, the \\textit{acoustic consistency constraint} aims\nto smooth the transition between the edited region and its neighboring acoustic\nsegments consistent with the ground truth, while the \\textit{prosody\nconsistency constraint} seeks to ensure that the prosody attributes within the\nedited regions remain consistent with the overall style of the original\nutterance. The subjective and objective experimental results on VCTK\ndemonstrate that our \\textit{FluentEditor} outperforms all advanced baselines\nin terms of naturalness and fluency. The audio samples and code are available\nat \\url{https://github.com/Ai-S2-Lab/FluentEditor}.", "published": "2023-09-21 01:58:01", "link": "http://arxiv.org/abs/2309.11725v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Frame Pairwise Distance Loss for Weakly-supervised Sound Event Detection", "abstract": "Weakly-supervised learning has emerged as a promising approach to leverage\nlimited labeled data in various domains by bridging the gap between fully\nsupervised methods and unsupervised techniques. Acquisition of strong\nannotations for detecting sound events is prohibitively expensive, making\nweakly supervised learning a more cost-effective and broadly applicable\nalternative. In order to enhance the recognition rate of the learning of\ndetection of weakly-supervised sound events, we introduce a Frame Pairwise\nDistance (FPD) loss branch, complemented with a minimal amount of synthesized\ndata. The corresponding sampling and label processing strategies are also\nproposed. Two distinct distance metrics are employed to evaluate the proposed\napproach. Finally, the method is validated on the DCASE 2023 task4 dataset. The\nobtained experimental results corroborated the efficacy of this approach.", "published": "2023-09-21 05:14:58", "link": "http://arxiv.org/abs/2309.11783v2", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "TMac: Temporal Multi-Modal Graph Learning for Acoustic Event\n  Classification", "abstract": "Audiovisual data is everywhere in this digital age, which raises higher\nrequirements for the deep learning models developed on them. To well handle the\ninformation of the multi-modal data is the key to a better audiovisual modal.\nWe observe that these audiovisual data naturally have temporal attributes, such\nas the time information for each frame in the video. More concretely, such data\nis inherently multi-modal according to both audio and visual cues, which\nproceed in a strict chronological order. It indicates that temporal information\nis important in multi-modal acoustic event modeling for both intra- and\ninter-modal. However, existing methods deal with each modal feature\nindependently and simply fuse them together, which neglects the mining of\ntemporal relation and thus leads to sub-optimal performance. With this\nmotivation, we propose a Temporal Multi-modal graph learning method for\nAcoustic event Classification, called TMac, by modeling such temporal\ninformation via graph learning techniques. In particular, we construct a\ntemporal graph for each acoustic event, dividing its audio data and video data\ninto multiple segments. Each segment can be considered as a node, and the\ntemporal relationships between nodes can be considered as timestamps on their\nedges. In this case, we can smoothly capture the dynamic information in\nintra-modal and inter-modal. Several experiments are conducted to demonstrate\nTMac outperforms other SOTA models in performance. Our code is available at\nhttps://github.com/MGitHubL/TMac.", "published": "2023-09-21 07:39:08", "link": "http://arxiv.org/abs/2309.11845v2", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Is the Ideal Ratio Mask Really the Best? -- Exploring the Best\n  Extraction Performance and Optimal Mask of Mask-based Beamformers", "abstract": "This study investigates mask-based beamformers (BFs), which estimate filters\nto extract target speech using time-frequency masks. Although several BF\nmethods have been proposed, the following aspects are yet to be comprehensively\ninvestigated. 1) Which BF can provide the best extraction performance in terms\nof the closeness of the BF output to the target speech? 2) Is the optimal mask\nfor the best performance common for all BFs? 3) Is the ideal ratio mask (IRM)\nidentical to the optimal mask? Accordingly, we investigate these issues\nconsidering four mask-based BFs: the maximum signal-to-noise ratio BF, two\nvariants of this, and the multichannel Wiener filter (MWF) BF. To obtain the\noptimal mask corresponding to the peak performance for each BF, we employ an\napproach that minimizes the mean square error between the BF output and target\nspeech for each utterance. Via the experiments with the CHiME-3 dataset, we\nverify that the four BFs have the same peak performance as the upper bound\nprovided by the ideal MWF BF, whereas the optimal mask depends on the adopted\nBF and differs from the IRM. These observations differ from the conventional\nidea that the optimal mask is common for all BFs and that peak performance\ndiffers for each BF. Hence, this study contributes to the design of mask-based\nBFs.", "published": "2023-09-21 13:35:20", "link": "http://arxiv.org/abs/2309.12065v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Passage Summarization with Recurrent Models for Audio-Sheet Music\n  Retrieval", "abstract": "Many applications of cross-modal music retrieval are related to connecting\nsheet music images to audio recordings. A typical and recent approach to this\nis to learn, via deep neural networks, a joint embedding space that correlates\nshort fixed-size snippets of audio and sheet music by means of an appropriate\nsimilarity structure. However, two challenges that arise out of this strategy\nare the requirement of strongly aligned data to train the networks, and the\ninherent discrepancies of musical content between audio and sheet music\nsnippets caused by local and global tempo differences. In this paper, we\naddress these two shortcomings by designing a cross-modal recurrent network\nthat learns joint embeddings that can summarize longer passages of\ncorresponding audio and sheet music. The benefits of our method are that it\nonly requires weakly aligned audio-sheet music pairs, as well as that the\nrecurrent network handles the non-linearities caused by tempo variations\nbetween audio and sheet music. We conduct a number of experiments on synthetic\nand real piano data and scores, showing that our proposed recurrent method\nleads to more accurate retrieval in all possible configurations.", "published": "2023-09-21 14:30:02", "link": "http://arxiv.org/abs/2309.12111v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-Supervised Contrastive Learning for Robust Audio-Sheet Music\n  Retrieval Systems", "abstract": "Linking sheet music images to audio recordings remains a key problem for the\ndevelopment of efficient cross-modal music retrieval systems. One of the\nfundamental approaches toward this task is to learn a cross-modal embedding\nspace via deep neural networks that is able to connect short snippets of audio\nand sheet music. However, the scarcity of annotated data from real musical\ncontent affects the capability of such methods to generalize to real retrieval\nscenarios. In this work, we investigate whether we can mitigate this limitation\nwith self-supervised contrastive learning, by exposing a network to a large\namount of real music data as a pre-training step, by contrasting randomly\naugmented views of snippets of both modalities, namely audio and sheet images.\nThrough a number of experiments on synthetic and real piano data, we show that\npre-trained models are able to retrieve snippets with better precision in all\nscenarios and pre-training configurations. Encouraged by these results, we\nemploy the snippet embeddings in the higher-level task of cross-modal piece\nidentification and conduct more experiments on several retrieval\nconfigurations. In this task, we observe that the retrieval quality improves\nfrom 30% up to 100% when real music data is present. We then conclude by\narguing for the potential of self-supervised contrastive learning for\nalleviating the annotated data scarcity in multi-modal music retrieval models.", "published": "2023-09-21 14:54:48", "link": "http://arxiv.org/abs/2309.12134v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Robust and Truly Large-Scale Audio-Sheet Music Retrieval", "abstract": "A range of applications of multi-modal music information retrieval is centred\naround the problem of connecting large collections of sheet music (images) to\ncorresponding audio recordings, that is, identifying pairs of audio and score\nexcerpts that refer to the same musical content. One of the typical and most\nrecent approaches to this task employs cross-modal deep learning architectures\nto learn joint embedding spaces that link the two distinct modalities - audio\nand sheet music images. While there has been steady improvement on this front\nover the past years, a number of open problems still prevent large-scale\nemployment of this methodology. In this article we attempt to provide an\ninsightful examination of the current developments on audio-sheet music\nretrieval via deep learning methods. We first identify a set of main challenges\non the road towards robust and large-scale cross-modal music retrieval in real\nscenarios. We then highlight the steps we have taken so far to address some of\nthese challenges, documenting step-by-step improvement along several\ndimensions. We conclude by analysing the remaining challenges and present ideas\nfor solving these, in order to pave the way to a unified and robust methodology\nfor cross-modal music retrieval.", "published": "2023-09-21 15:11:16", "link": "http://arxiv.org/abs/2309.12158v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Weakly-supervised Automated Audio Captioning via text only training", "abstract": "In recent years, datasets of paired audio and captions have enabled\nremarkable success in automatically generating descriptions for audio clips,\nnamely Automated Audio Captioning (AAC). However, it is labor-intensive and\ntime-consuming to collect a sufficient number of paired audio and captions.\nMotivated by the recent advances in Contrastive Language-Audio Pretraining\n(CLAP), we propose a weakly-supervised approach to train an AAC model assuming\nonly text data and a pre-trained CLAP model, alleviating the need for paired\ntarget data. Our approach leverages the similarity between audio and text\nembeddings in CLAP. During training, we learn to reconstruct the text from the\nCLAP text embedding, and during inference, we decode using the audio\nembeddings. To mitigate the modality gap between the audio and text embeddings\nwe employ strategies to bridge the gap during training and inference stages. We\nevaluate our proposed method on Clotho and AudioCaps datasets demonstrating its\nability to achieve a relative performance of up to ~$83\\%$ compared to fully\nsupervised approaches trained with paired target data.", "published": "2023-09-21 16:40:46", "link": "http://arxiv.org/abs/2309.12242v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Performance Conditioning for Diffusion-Based Multi-Instrument Music\n  Synthesis", "abstract": "Generating multi-instrument music from symbolic music representations is an\nimportant task in Music Information Retrieval (MIR). A central but still\nlargely unsolved problem in this context is musically and acoustically informed\ncontrol in the generation process. As the main contribution of this work, we\npropose enhancing control of multi-instrument synthesis by conditioning a\ngenerative model on a specific performance and recording environment, thus\nallowing for better guidance of timbre and style. Building on state-of-the-art\ndiffusion-based music generative models, we introduce performance conditioning\n- a simple tool indicating the generative model to synthesize music with style\nand timbre of specific instruments taken from specific performances. Our\nprototype is evaluated using uncurated performances with diverse\ninstrumentation and achieves state-of-the-art FAD realism scores while allowing\nnovel timbre and style control. Our project page, including samples and\ndemonstrations, is available at benadar293.github.io/midipm", "published": "2023-09-21 17:44:57", "link": "http://arxiv.org/abs/2309.12283v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TalkNCE: Improving Active Speaker Detection with Talk-Aware Contrastive\n  Learning", "abstract": "The goal of this work is Active Speaker Detection (ASD), a task to determine\nwhether a person is speaking or not in a series of video frames. Previous works\nhave dealt with the task by exploring network architectures while learning\neffective representations has been less explored. In this work, we propose\nTalkNCE, a novel talk-aware contrastive loss. The loss is only applied to part\nof the full segments where a person on the screen is actually speaking. This\nencourages the model to learn effective representations through the natural\ncorrespondence of speech and facial movements. Our loss can be jointly\noptimized with the existing objectives for training ASD models without the need\nfor additional supervision or training data. The experiments demonstrate that\nour loss can be easily integrated into the existing ASD frameworks, improving\ntheir performance. Our method achieves state-of-the-art performances on\nAVA-ActiveSpeaker and ASW datasets.", "published": "2023-09-21 17:59:11", "link": "http://arxiv.org/abs/2309.12306v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Does My Dog ''Speak'' Like Me? The Acoustic Correlation between Pet Dogs\n  and Their Human Owners", "abstract": "How hosts language influence their pets' vocalization is an interesting yet\nunderexplored problem. This paper presents a preliminary investigation into the\npossible correlation between domestic dog vocal expressions and their human\nhost's language environment. We first present a new dataset of Shiba Inu dog\nvocals from YouTube, which provides 7500 clean sound clips, including their\ncontextual information of these vocals and their owner's speech clips with a\ncarefully-designed data processing pipeline. The contextual information\nincludes the scene category in which the vocal was recorded, the dog's location\nand activity. With a classification task and prominent factor analysis, we\ndiscover significant acoustic differences in the dog vocals from the two\nlanguage environments. We further identify some acoustic features from dog\nvocalizations that are potentially correlated to their host language patterns.", "published": "2023-09-21 23:49:21", "link": "http://arxiv.org/abs/2309.13085v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Variational Quantum Harmonizer: Generating Chord Progressions and Other\n  Sonification Methods with the VQE Algorithm", "abstract": "This work investigates a case study of using physical-based sonification of\nQuadratic Unconstrained Binary Optimization (QUBO) problems, optimized by the\nVariational Quantum Eigensolver (VQE) algorithm. The VQE approximates the\nsolution of the problem by using an iterative loop between the quantum computer\nand a classical optimization routine. This work explores the intermediary\nstatevectors found in each VQE iteration as the means of sonifying the\noptimization process itself. The implementation was realised in the form of a\nmusical interface prototype named Variational Quantum Harmonizer (VQH),\nproviding potential design strategies for musical applications, focusing on\nchords, chord progressions, and arpeggios. The VQH can be used both to enhance\ndata visualization or to create artistic pieces. The methodology is also\nrelevant in terms of how an artist would gain intuition towards achieving a\ndesired musical sound by carefully designing QUBO cost functions. Flexible\nmapping strategies could supply a broad portfolio of sounds for QUBO and\nquantum-inspired musical compositions, as demonstrated in a case study\ncomposition, \"Dependent Origination\" by Peter Thomas and Paulo Itaborai.", "published": "2023-09-21 16:58:35", "link": "http://arxiv.org/abs/2309.12254v1", "categories": ["cs.ET", "cs.HC", "cs.SD", "eess.AS", "quant-ph"], "primary_category": "cs.ET"}
{"title": "t-EER: Parameter-Free Tandem Evaluation of Countermeasures and Biometric\n  Comparators", "abstract": "Presentation attack (spoofing) detection (PAD) typically operates alongside\nbiometric verification to improve reliablity in the face of spoofing attacks.\nEven though the two sub-systems operate in tandem to solve the single task of\nreliable biometric verification, they address different detection tasks and are\nhence typically evaluated separately. Evidence shows that this approach is\nsuboptimal. We introduce a new metric for the joint evaluation of PAD solutions\noperating in situ with biometric verification. In contrast to the tandem\ndetection cost function proposed recently, the new tandem equal error rate\n(t-EER) is parameter free. The combination of two classifiers nonetheless leads\nto a \\emph{set} of operating points at which false alarm and miss rates are\nequal and also dependent upon the prevalence of attacks. We therefore introduce\nthe \\emph{concurrent} t-EER, a unique operating point which is invariable to\nthe prevalence of attacks. Using both modality (and even application) agnostic\nsimulated scores, as well as real scores for a voice biometrics application, we\ndemonstrate application of the t-EER to a wide range of biometric system\nevaluations under attack. The proposed approach is a strong candidate metric\nfor the tandem evaluation of PAD systems and biometric comparators.", "published": "2023-09-21 16:30:40", "link": "http://arxiv.org/abs/2309.12237v1", "categories": ["cs.CR", "cs.LG", "cs.SD", "eess.AS", "eess.IV", "stat.CO"], "primary_category": "cs.CR"}
