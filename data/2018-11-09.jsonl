{"title": "Incorporating Relevant Knowledge in Context Modeling and Response\n  Generation", "abstract": "To sustain engaging conversation, it is critical for chatbots to make good\nuse of relevant knowledge. Equipped with a knowledge base, chatbots are able to\nextract conversation-related attributes and entities to facilitate context\nmodeling and response generation. In this work, we distinguish the uses of\nattribute and entity and incorporate them into the encoder-decoder architecture\nin different manners. Based on the augmented architecture, our chatbot, namely\nMike, is able to generate responses by referring to proper entities from the\ncollected knowledge. To validate the proposed approach, we build a movie\nconversation corpus on which the proposed approach significantly outperforms\nother four knowledge-grounded models.", "published": "2018-11-09 01:18:27", "link": "http://arxiv.org/abs/1811.03729v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural sequence labeling for Vietnamese POS Tagging and NER", "abstract": "This paper presents a neural architecture for Vietnamese sequence labeling\ntasks including part-of-speech (POS) tagging and named entity recognition\n(NER). We applied the model described in \\cite{lample-EtAl:2016:N16-1} that is\na combination of bidirectional Long-Short Term Memory and Conditional Random\nFields, which rely on two sources of information about words: character-based\nword representations learned from the supervised corpus and pre-trained word\nembeddings learned from other unannotated corpora. Experiments on benchmark\ndatasets show that this work achieves state-of-the-art performances on both\ntasks - 93.52\\% accuracy for POS tagging and 94.88\\% F1 for NER. Our sourcecode\nis available at here.", "published": "2018-11-09 03:15:23", "link": "http://arxiv.org/abs/1811.03754v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Encoding Implicit Relation Requirements for Relation Extraction: A Joint\n  Inference Approach", "abstract": "Relation extraction is the task of identifying predefined relationship\nbetween entities, and plays an essential role in information extraction,\nknowledge base construction, question answering and so on. Most existing\nrelation extractors make predictions for each entity pair locally and\nindividually, while ignoring implicit global clues available across different\nentity pairs and in the knowledge base, which often leads to conflicts among\nlocal predictions from different entity pairs. This paper proposes a joint\ninference framework that employs such global clues to resolve disagreements\namong local predictions. We exploit two kinds of clues to generate constraints\nwhich can capture the implicit type and cardinality requirements of a relation.\nThose constraints can be examined in either hard style or soft style, both of\nwhich can be effectively explored in an integer linear program formulation.\nExperimental results on both English and Chinese datasets show that our\nproposed framework can effectively utilize those two categories of global clues\nand resolve the disagreements among local predictions, thus improve various\nrelation extractors when such clues are applicable to the datasets. Our\nexperiments also indicate that the clues learnt automatically from existing\nknowledge bases perform comparably to or better than those refined by human.", "published": "2018-11-09 07:06:46", "link": "http://arxiv.org/abs/1811.03796v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multimodal Grounding for Sequence-to-Sequence Speech Recognition", "abstract": "Humans are capable of processing speech by making use of multiple sensory\nmodalities. For example, the environment where a conversation takes place\ngenerally provides semantic and/or acoustic context that helps us to resolve\nambiguities or to recall named entities. Motivated by this, there have been\nmany works studying the integration of visual information into the speech\nrecognition pipeline. Specifically, in our previous work, we propose a\nmultistep visual adaptive training approach which improves the accuracy of an\naudio-based Automatic Speech Recognition (ASR) system. This approach, however,\nis not end-to-end as it requires fine-tuning the whole model with an adaptation\nlayer. In this paper, we propose novel end-to-end multimodal ASR systems and\ncompare them to the adaptive approach by using a range of visual\nrepresentations obtained from state-of-the-art convolutional neural networks.\nWe show that adaptive training is effective for S2S models leading to an\nabsolute improvement of 1.4% in word error rate. As for the end-to-end systems,\nalthough they perform better than baseline, the improvements are slightly less\nthan adaptive training, 0.8 absolute WER reduction in single-best models. Using\nensemble decoding, end-to-end models reach a WER of 15% which is the lowest\nscore among all systems.", "published": "2018-11-09 11:30:11", "link": "http://arxiv.org/abs/1811.03865v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Long Short-Term Memory with Dynamic Skip Connections", "abstract": "In recent years, long short-term memory (LSTM) has been successfully used to\nmodel sequential data of variable length. However, LSTM can still experience\ndifficulty in capturing long-term dependencies. In this work, we tried to\nalleviate this problem by introducing a dynamic skip connection, which can\nlearn to directly connect two dependent words. Since there is no dependency\ninformation in the training data, we propose a novel reinforcement\nlearning-based method to model the dependency relationship and connect\ndependent words. The proposed model computes the recurrent transition functions\nbased on the skip connections, which provides a dynamic skipping advantage over\nRNNs that always tackle entire sentences sequentially. Our experimental results\non three natural language processing tasks demonstrate that the proposed method\ncan achieve better performance than existing methods. In the number prediction\nexperiment, the proposed model outperformed LSTM with respect to accuracy by\nnearly 20%.", "published": "2018-11-09 12:11:22", "link": "http://arxiv.org/abs/1811.03873v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-shot Neural Transfer for Cross-lingual Entity Linking", "abstract": "Cross-lingual entity linking maps an entity mention in a source language to\nits corresponding entry in a structured knowledge base that is in a different\n(target) language. While previous work relies heavily on bilingual lexical\nresources to bridge the gap between the source and the target languages, these\nresources are scarce or unavailable for many low-resource languages. To address\nthis problem, we investigate zero-shot cross-lingual entity linking, in which\nwe assume no bilingual lexical resources are available in the source\nlow-resource language. Specifically, we propose pivot-based entity linking,\nwhich leverages information from a high-resource \"pivot\" language to train\ncharacter-level neural entity linking models that are transferred to the source\nlow-resource language in a zero-shot manner. With experiments on 9 low-resource\nlanguages and transfer through a total of 54 languages, we show that our\nproposed pivot-based framework improves entity linking accuracy 17% (absolute)\non average over the baseline systems, for the zero-shot scenario. Further, we\nalso investigate the use of language-universal phonological representations\nwhich improves average accuracy (absolute) by 36% when transferring between\nlanguages that use different scripts.", "published": "2018-11-09 22:50:50", "link": "http://arxiv.org/abs/1811.04154v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Semantic Representations for Novel Words: Leveraging Both Form\n  and Context", "abstract": "Word embeddings are a key component of high-performing natural language\nprocessing (NLP) systems, but it remains a challenge to learn good\nrepresentations for novel words on the fly, i.e., for words that did not occur\nin the training data. The general problem setting is that word embeddings are\ninduced on an unlabeled training corpus and then a model is trained that embeds\nnovel words into this induced embedding space. Currently, two approaches for\nlearning embeddings of novel words exist: (i) learning an embedding from the\nnovel word's surface-form (e.g., subword n-grams) and (ii) learning an\nembedding from the context in which it occurs. In this paper, we propose an\narchitecture that leverages both sources of information - surface-form and\ncontext - and show that it results in large increases in embedding quality. Our\narchitecture obtains state-of-the-art results on the Definitional Nonce and\nContextual Rare Words datasets. As input, we only require an embedding set and\nan unlabeled corpus for training our architecture to produce embeddings\nappropriate for the induced embedding space. Thus, our model can easily be\nintegrated into any existing NLP system and enhance its capability to handle\nnovel words.", "published": "2018-11-09 11:44:05", "link": "http://arxiv.org/abs/1811.03866v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Hierarchical Framework for Relation Extraction with Reinforcement\n  Learning", "abstract": "Most existing methods determine relation types only after all the entities\nhave been recognized, thus the interaction between relation types and entity\nmentions is not fully modeled. This paper presents a novel paradigm to deal\nwith relation extraction by regarding the related entities as the arguments of\na relation. We apply a hierarchical reinforcement learning (HRL) framework in\nthis paradigm to enhance the interaction between entity mentions and relation\ntypes. The whole extraction process is decomposed into a hierarchy of two-level\nRL policies for relation detection and entity extraction respectively, so that\nit is more feasible and natural to deal with overlapping relations. Our model\nwas evaluated on public datasets collected via distant supervision, and results\nshow that it gains better performance than existing methods and is more\npowerful for extracting overlapping relations.", "published": "2018-11-09 14:33:29", "link": "http://arxiv.org/abs/1811.03925v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Multilingual and Unsupervised Subword Modeling for Zero-Resource\n  Languages", "abstract": "Subword modeling for zero-resource languages aims to learn low-level\nrepresentations of speech audio without using transcriptions or other resources\nfrom the target language (such as text corpora or pronunciation dictionaries).\nA good representation should capture phonetic content and abstract away from\nother types of variability, such as speaker differences and channel noise.\nPrevious work in this area has primarily focused unsupervised learning from\ntarget language data only, and has been evaluated only intrinsically. Here we\ndirectly compare multiple methods, including some that use only target language\nspeech data and some that use transcribed speech from other (non-target)\nlanguages, and we evaluate using two intrinsic measures as well as on a\ndownstream unsupervised word segmentation and clustering task. We find that\ncombining two existing target-language-only methods yields better features than\neither method alone. Nevertheless, even better results are obtained by\nextracting target language bottleneck features using a model trained on other\nlanguages. Cross-lingual training using just one other language is enough to\nprovide this benefit, but multilingual training helps even more. In addition to\nthese results, which hold across both intrinsic measures and the extrinsic\ntask, we discuss the qualitative differences between the different types of\nlearned features.", "published": "2018-11-09 11:04:40", "link": "http://arxiv.org/abs/1811.04791v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Can We Use Speaker Recognition Technology to Attack Itself? Enhancing\n  Mimicry Attacks Using Automatic Target Speaker Selection", "abstract": "We consider technology-assisted mimicry attacks in the context of automatic\nspeaker verification (ASV). We use ASV itself to select targeted speakers to be\nattacked by human-based mimicry. We recorded 6 naive mimics for whom we select\ntarget celebrities from VoxCeleb1 and VoxCeleb2 corpora (7,365 potential\ntargets) using an i-vector system. The attacker attempts to mimic the selected\ntarget, with the utterances subjected to ASV tests using an independently\ndeveloped x-vector system. Our main finding is negative: even if some of the\nattacker scores against the target speakers were slightly increased, our mimics\ndid not succeed in spoofing the x-vector system. Interestingly, however, the\nrelative ordering of the selected targets (closest, furthest, median) are\nconsistent between the systems, which suggests some level of transferability\nbetween the systems.", "published": "2018-11-09 06:15:08", "link": "http://arxiv.org/abs/1811.03790v1", "categories": ["eess.AS", "cs.CL", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Multimodal One-Shot Learning of Speech and Images", "abstract": "Imagine a robot is shown new concepts visually together with spoken tags,\ne.g. \"milk\", \"eggs\", \"butter\". After seeing one paired audio-visual example per\nclass, it is shown a new set of unseen instances of these objects, and asked to\npick the \"milk\". Without receiving any hard labels, could it learn to match the\nnew continuous speech input to the correct visual instance? Although unimodal\none-shot learning has been studied, where one labelled example in a single\nmodality is given per class, this example motivates multimodal one-shot\nlearning. Our main contribution is to formally define this task, and to propose\nseveral baseline and advanced models. We use a dataset of paired spoken and\nvisual digits to specifically investigate recent advances in Siamese\nconvolutional neural networks. Our best Siamese model achieves twice the\naccuracy of a nearest neighbour model using pixel-distance over images and\ndynamic time warping over speech in 11-way cross-modal matching.", "published": "2018-11-09 12:14:20", "link": "http://arxiv.org/abs/1811.03875v2", "categories": ["cs.CL", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Native Language Identification using i-vector", "abstract": "The task of determining a speaker's native language based only on his\nspeeches in a second language is known as Native Language Identification or\nNLI. Due to its increasing applications in various domains of speech signal\nprocessing, this has emerged as an important research area in recent times. In\nthis paper we have proposed an i-vector based approach to develop an automatic\nNLI system using MFCC and GFCC features. For evaluation of our approach, we\nhave tested our framework on the 2016 ComParE Native language sub-challenge\ndataset which has English language speakers from 11 different native language\nbackgrounds. Our proposed method outperforms the baseline system with an\nimprovement in accuracy by 21.95% for the MFCC feature based i-vector framework\nand 22.81% for the GFCC feature based i-vector framework.", "published": "2018-11-09 17:12:47", "link": "http://arxiv.org/abs/1811.05540v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Joint Acoustic and Class Inference for Weakly Supervised Sound Event\n  Detection", "abstract": "Sound event detection is a challenging task, especially for scenes with\nmultiple simultaneous events. While event classification methods tend to be\nfairly accurate, event localization presents additional challenges, especially\nwhen large amounts of labeled data are not available. Task4 of the 2018 DCASE\nchallenge presents an event detection task that requires accuracy in both\nsegmentation and recognition of events while providing only weakly labeled\ntraining data. Supervised methods can produce accurate event labels but are\nlimited in event segmentation when training data lacks event timestamps. On the\nother hand, unsupervised methods that model the acoustic properties of the\naudio can produce accurate event boundaries but are not guided by the\ncharacteristics of event classes and sound categories. We present a hybrid\napproach that combines an acoustic-driven event boundary detection and a\nsupervised label inference using a deep neural network. This framework\nleverages benefits of both unsupervised and supervised methodologies and takes\nadvantage of large amounts of unlabeled data, making it ideal for large-scale\nweakly labeled event detection. Compared to a baseline system, the proposed\napproach delivers a 15% absolute improvement in F-score, demonstrating the\nbenefits of the hybrid bottom-up, top-down approach.", "published": "2018-11-09 18:06:21", "link": "http://arxiv.org/abs/1811.04048v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio Spectrogram Factorization for Classification of Telephony Signals\n  below the Auditory Threshold", "abstract": "Traffic Pumping attacks are a form of high-volume SPAM that target telephone\nnetworks, defraud customers and squander telephony resources. One type of call\nin these attacks is characterized by very low-amplitude signal levels, notably\nbelow the auditory threshold. We propose a technique to classify so-called\n\"dead air\" or \"silent\" SPAM calls based on features derived from factorizing\nthe caller audio spectrogram. We describe the algorithms for feature extraction\nand classification as well as our data collection methods and production\nperformance on millions of calls per week.", "published": "2018-11-09 21:24:47", "link": "http://arxiv.org/abs/1811.04139v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AttS2S-VC: Sequence-to-Sequence Voice Conversion with Attention and\n  Context Preservation Mechanisms", "abstract": "This paper describes a method based on a sequence-to-sequence learning\n(Seq2Seq) with attention and context preservation mechanism for voice\nconversion (VC) tasks. Seq2Seq has been outstanding at numerous tasks involving\nsequence modeling such as speech synthesis and recognition, machine\ntranslation, and image captioning. In contrast to current VC techniques, our\nmethod 1) stabilizes and accelerates the training procedure by considering\nguided attention and proposed context preservation losses, 2) allows not only\nspectral envelopes but also fundamental frequency contours and durations of\nspeech to be converted, 3) requires no context information such as phoneme\nlabels, and 4) requires no time-aligned source and target speech data in\nadvance. In our experiment, the proposed VC framework can be trained in only\none day, using only one GPU of an NVIDIA Tesla K80, while the quality of the\nsynthesized speech is higher than that of speech converted by Gaussian mixture\nmodel-based VC and is comparable to that of speech generated by recurrent\nneural network-based text-to-speech synthesis, which can be regarded as an\nupper limit on VC performance.", "published": "2018-11-09 05:19:43", "link": "http://arxiv.org/abs/1811.04076v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Integrating Recurrence Dynamics for Speech Emotion Recognition", "abstract": "We investigate the performance of features that can capture nonlinear\nrecurrence dynamics embedded in the speech signal for the task of Speech\nEmotion Recognition (SER). Reconstruction of the phase space of each speech\nframe and the computation of its respective Recurrence Plot (RP) reveals\ncomplex structures which can be measured by performing Recurrence\nQuantification Analysis (RQA). These measures are aggregated by using\nstatistical functionals over segment and utterance periods. We report SER\nresults for the proposed feature set on three databases using different\nclassification methods. When fusing the proposed features with traditional\nfeature sets, we show an improvement in unweighted accuracy of up to 5.7% and\n10.7% on Speaker-Dependent (SD) and Speaker-Independent (SI) SER tasks,\nrespectively, over the baseline. Following a segment-based approach we\ndemonstrate state-of-the-art performance on IEMOCAP using a Bidirectional\nRecurrent Neural Network.", "published": "2018-11-09 21:02:52", "link": "http://arxiv.org/abs/1811.04133v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "ExcitNet vocoder: A neural excitation model for parametric speech\n  synthesis systems", "abstract": "This paper proposes a WaveNet-based neural excitation model (ExcitNet) for\nstatistical parametric speech synthesis systems. Conventional WaveNet-based\nneural vocoding systems significantly improve the perceptual quality of\nsynthesized speech by statistically generating a time sequence of speech\nwaveforms through an auto-regressive framework. However, they often suffer from\nnoisy outputs because of the difficulties in capturing the complicated\ntime-varying nature of speech signals. To improve modeling efficiency, the\nproposed ExcitNet vocoder employs an adaptive inverse filter to decouple\nspectral components from the speech signal. The residual component, i.e.\nexcitation signal, is then trained and generated within the WaveNet framework.\nIn this way, the quality of the synthesized speech signal can be further\nimproved since the spectral component is well represented by a deep learning\nframework and, moreover, the residual component is efficiently generated by the\nWaveNet framework. Experimental results show that the proposed ExcitNet\nvocoder, trained both speaker-dependently and speaker-independently,\noutperforms traditional linear prediction vocoders and similarly configured\nconventional WaveNet vocoders.", "published": "2018-11-09 09:02:55", "link": "http://arxiv.org/abs/1811.04769v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
