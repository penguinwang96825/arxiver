{"title": "Native Language Identification with Large Language Models", "abstract": "We present the first experiments on Native Language Identification (NLI)\nusing LLMs such as GPT-4. NLI is the task of predicting a writer's first\nlanguage by analyzing their writings in a second language, and is used in\nsecond language acquisition and forensic linguistics. Our results show that GPT\nmodels are proficient at NLI classification, with GPT-4 setting a new\nperformance record of 91.7% on the benchmark TOEFL11 test set in a zero-shot\nsetting. We also show that unlike previous fully-supervised settings, LLMs can\nperform NLI without being limited to a set of known classes, which has\npractical implications for real-world applications. Finally, we also show that\nLLMs can provide justification for their choices, providing reasoning based on\nspelling errors, syntactic patterns, and usage of directly translated\nlinguistic patterns.", "published": "2023-12-13 00:52:15", "link": "http://arxiv.org/abs/2312.07819v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Deep Learning-Based System for Automatic Case Summarization", "abstract": "This paper presents a deep learning-based system for efficient automatic case\nsummarization. Leveraging state-of-the-art natural language processing\ntechniques, the system offers both supervised and unsupervised methods to\ngenerate concise and relevant summaries of lengthy legal case documents. The\nuser-friendly interface allows users to browse the system's database of legal\ncase documents, select their desired case, and choose their preferred\nsummarization method. The system generates comprehensive summaries for each\nsubsection of the legal text as well as an overall summary. This demo\nstreamlines legal case document analysis, potentially benefiting legal\nprofessionals by reducing workload and increasing efficiency. Future work will\nfocus on refining summarization techniques and exploring the application of our\nmethods to other types of legal texts.", "published": "2023-12-13 01:18:10", "link": "http://arxiv.org/abs/2312.07824v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph vs. Sequence: An Empirical Study on Knowledge Forms for\n  Knowledge-Grounded Dialogue", "abstract": "Knowledge-grounded dialogue is a task of generating an informative response\nbased on both the dialogue history and external knowledge source. In general,\nthere are two forms of knowledge: manually annotated knowledge graphs and\nknowledge text from website. From various evaluation viewpoints, each type of\nknowledge has advantages and downsides. To further distinguish the principles\nand determinants from the intricate factors, we conduct a thorough experiment\nand study on the task to answer three essential questions. The questions\ninvolve the choice of appropriate knowledge form, the degree of mutual effects\nbetween knowledge and the model selection, and the few-shot performance of\nknowledge. Supported by statistical shreds of evidence, we offer conclusive\nsolutions and sensible suggestions for directions and standards of future\nresearch.", "published": "2023-12-13 03:16:33", "link": "http://arxiv.org/abs/2312.07868v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey of Text Watermarking in the Era of Large Language Models", "abstract": "Text watermarking algorithms are crucial for protecting the copyright of\ntextual content. Historically, their capabilities and application scenarios\nwere limited. However, recent advancements in large language models (LLMs) have\nrevolutionized these techniques. LLMs not only enhance text watermarking\nalgorithms with their advanced abilities but also create a need for employing\nthese algorithms to protect their own copyrights or prevent potential misuse.\nThis paper conducts a comprehensive survey of the current state of text\nwatermarking technology, covering four main aspects: (1) an overview and\ncomparison of different text watermarking techniques; (2) evaluation methods\nfor text watermarking algorithms, including their detectability, impact on text\nor LLM quality, robustness under target or untargeted attacks; (3) potential\napplication scenarios for text watermarking technology; (4) current challenges\nand future directions for text watermarking. This survey aims to provide\nresearchers with a thorough understanding of text watermarking technology in\nthe era of LLM, thereby promoting its further advancement.", "published": "2023-12-13 06:11:42", "link": "http://arxiv.org/abs/2312.07913v6", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "CoRTEx: Contrastive Learning for Representing Terms via Explanations\n  with Applications on Constructing Biomedical Knowledge Graphs", "abstract": "Objective: Biomedical Knowledge Graphs play a pivotal role in various\nbiomedical research domains. Concurrently, term clustering emerges as a crucial\nstep in constructing these knowledge graphs, aiming to identify synonymous\nterms. Due to a lack of knowledge, previous contrastive learning models trained\nwith Unified Medical Language System (UMLS) synonyms struggle at clustering\ndifficult terms and do not generalize well beyond UMLS terms. In this work, we\nleverage the world knowledge from Large Language Models (LLMs) and propose\nContrastive Learning for Representing Terms via Explanations (CoRTEx) to\nenhance term representation and significantly improves term clustering.\nMaterials and Methods: The model training involves generating explanations for\na cleaned subset of UMLS terms using ChatGPT. We employ contrastive learning,\nconsidering term and explanation embeddings simultaneously, and progressively\nintroduce hard negative samples. Additionally, a ChatGPT-assisted BIRCH\nalgorithm is designed for efficient clustering of a new ontology. Results: We\nestablished a clustering test set and a hard negative test set, where our model\nconsistently achieves the highest F1 score. With CoRTEx embeddings and the\nmodified BIRCH algorithm, we grouped 35,580,932 terms from the Biomedical\nInformatics Ontology System (BIOS) into 22,104,559 clusters with O(N) queries\nto ChatGPT. Case studies highlight the model's efficacy in handling challenging\nsamples, aided by information from explanations. Conclusion: By aligning terms\nto their explanations, CoRTEx demonstrates superior accuracy over benchmark\nmodels and robustness beyond its training set, and it is suitable for\nclustering terms for large-scale biomedical ontologies.", "published": "2023-12-13 10:29:34", "link": "http://arxiv.org/abs/2312.08036v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Abusive Span Detection for Vietnamese Narrative Texts", "abstract": "Abuse in its various forms, including physical, psychological, verbal,\nsexual, financial, and cultural, has a negative impact on mental health.\nHowever, there are limited studies on applying natural language processing\n(NLP) in this field in Vietnam. Therefore, we aim to contribute by building a\nhuman-annotated Vietnamese dataset for detecting abusive content in Vietnamese\nnarrative texts. We sourced these texts from VnExpress, Vietnam's popular\nonline newspaper, where readers often share stories containing abusive content.\nIdentifying and categorizing abusive spans in these texts posed significant\nchallenges during dataset creation, but it also motivated our research. We\nexperimented with lightweight baseline models by freezing PhoBERT and\nXLM-RoBERTa and using their hidden states in a BiLSTM to assess the complexity\nof the dataset. According to our experimental results, PhoBERT outperforms\nother models in both labeled and unlabeled abusive span detection tasks. These\nresults indicate that it has the potential for future improvements.", "published": "2023-12-13 01:36:18", "link": "http://arxiv.org/abs/2312.07831v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Finetuning an LLM on Contextual Knowledge of Classics for Q&A", "abstract": "The open-source publishing of large language models (LLMs) has created many\npossibilities for how anyone who understands language and has access to a\ncomputer can interact with significant tools of artificial intelligence,\nparticularly in the context of learning and knowledge dissemination. However,\nthe utility of these models in specialized fields like Classics is still\nlargely unexplored. This project is an attempt to merge the knowledge of\nClassics with the capabilities of artificial intelligence by finetuning an LLM\nto cater to the specific needs of learners and professionals. The goal of this\nproject is to develop an LLM that not only reproduces contextual knowledge\naccurately but also exhibits a consistent \"personality\" - and, indeed, has\nconsistent propriety - to appeal to a diverse audience who possess differing\nlevels of knowledge. A significant portion of this project was dedicated to\nrefining the dataset, following the principle of \"garbage in, garbage out,\" to\nensure the model generates relevant, useful, and creative responses when given\na prompt (a statement, question, or single word). After training and\nevaluation, my model's ability to handle a vast array of different types of\ninputs and prompting exceeded expectations for a 355M parameter model, though\nits occasional hallucinations (especially when set with a high temperature),\nparticularly in its assertions about historical events or its own identity,\nmake it seem somewhat capricious and more work in the form of continuous\nfinetuning will be undertaken.", "published": "2023-12-13 02:32:01", "link": "http://arxiv.org/abs/2312.07848v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BESTMVQA: A Benchmark Evaluation System for Medical Visual Question\n  Answering", "abstract": "Medical Visual Question Answering (Med-VQA) is a very important task in\nhealthcare industry, which answers a natural language question with a medical\nimage. Existing VQA techniques in information systems can be directly applied\nto solving the task. However, they often suffer from (i) the data insufficient\nproblem, which makes it difficult to train the state of the arts (SOTAs) for\nthe domain-specific task, and (ii) the reproducibility problem, that many\nexisting models have not been thoroughly evaluated in a unified experimental\nsetup. To address these issues, this paper develops a Benchmark Evaluation\nSysTem for Medical Visual Question Answering, denoted by BESTMVQA. Given\nself-collected clinical data, our system provides a useful tool for users to\nautomatically build Med-VQA datasets, which helps overcoming the data\ninsufficient problem. Users also can conveniently select a wide spectrum of\nSOTA models from our model library to perform a comprehensive empirical study.\nWith simple configurations, our system automatically trains and evaluates the\nselected models over a benchmark dataset, and reports the comprehensive results\nfor users to develop new techniques or perform medical practice. Limitations of\nexisting work are overcome (i) by the data generation tool, which automatically\nconstructs new datasets from unstructured clinical data, and (ii) by evaluating\nSOTAs on benchmark datasets in a unified experimental setup. The demonstration\nvideo of our system can be found at https://youtu.be/QkEeFlu1x4A. Our code and\ndata will be available soon.", "published": "2023-12-13 03:08:48", "link": "http://arxiv.org/abs/2312.07867v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Modality Plug-and-Play: Elastic Modality Adaptation in Multimodal LLMs\n  for Embodied AI", "abstract": "Large Language Models (LLMs) are capable of reasoning over diverse input data\nmodalities through pre-trained encoders. However, the growing diversity of\ninput data modalities prevents incorporating all modalities into LLMs,\nespecially when LLMs are deployed on resource-constrained edge devices for\nembodied AI applications. Instead, a better option is to adaptively involve\nonly the useful modalities at runtime, depending on the current environmental\ncontexts and task requirements. For such modality adaptation, existing work\nadopts fixed connections between encoders and the LLM's input layer, leading to\nhigh training cost at runtime and ineffective cross-modal interaction. In this\npaper, we address these limitations by presenting mPnP-LLM, a new technique\nthat allows fully elastic, automated and prompt runtime modality adaptation, by\nconnecting unimodal encoders to a flexible set of last LLM blocks and making\nsuch latent connections fully trainable at runtime. Experiments over the\nnuScenes-QA dataset show that mPnP-LLM can achieve up to 3.7x FLOPs reduction\nand 30% GPU memory usage reduction, while retaining on-par accuracy with the\nexisting schemes. Under the same compute budget, mPnP-LLM improves the task\naccuracy by up to 4% compared to the best existing scheme.", "published": "2023-12-13 04:08:59", "link": "http://arxiv.org/abs/2312.07886v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Learn or Recall? Revisiting Incremental Learning with Pre-trained\n  Language Models", "abstract": "Incremental Learning (IL) has been a long-standing problem in both vision and\nNatural Language Processing (NLP) communities. In recent years, as Pre-trained\nLanguage Models (PLMs) have achieved remarkable progress in various NLP\ndownstream tasks, utilizing PLMs as backbones has become a common practice in\nrecent research of IL in NLP. Most assume that catastrophic forgetting is the\nbiggest obstacle to achieving superior IL performance and propose various\ntechniques to overcome this issue. However, we find that this assumption is\nproblematic. Specifically, we revisit more than 20 methods on four\nclassification tasks (Text Classification, Intent Classification, Relation\nExtraction, and Named Entity Recognition) under the two most popular IL\nsettings (Class-Incremental and Task-Incremental) and reveal that most of them\nseverely underestimate the inherent anti-forgetting ability of PLMs. Based on\nthe observation, we propose a frustratingly easy method called SEQ* for IL with\nPLMs. The results show that SEQ* has competitive or superior performance\ncompared to state-of-the-art (SOTA) IL methods and requires considerably less\ntrainable parameters and training time. These findings urge us to revisit the\nIL with PLMs and encourage future studies to have a fundamental understanding\nof the catastrophic forgetting in PLMs. The data, code and scripts are publicly\navailable at\nhttps://github.com/zzz47zzz/codebase-for-incremental-learning-with-llm.", "published": "2023-12-13 04:14:22", "link": "http://arxiv.org/abs/2312.07887v5", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CBQ: Cross-Block Quantization for Large Language Models", "abstract": "Post-training quantization (PTQ) has played a key role in compressing large\nlanguage models (LLMs) with ultra-low costs. However, existing PTQ methods only\nfocus on handling the outliers within one layer or one block, which ignores the\ndependency of blocks and leads to severe performance degradation in low-bit\nsettings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ\nmethod for LLMs. CBQ employs a cross-block dependency using a homologous\nreconstruction scheme, establishing long-range dependencies across multiple\nblocks to minimize error accumulation. Furthermore, CBQ incorporates a\ncoarse-to-fine preprocessing (CFP) strategy for suppressing weight and\nactivation outliers, coupled with an adaptive LoRA-Rounding technique for\nprecise weight quantization. These innovations enable CBQ to not only handle\nextreme outliers effectively but also improve overall quantization accuracy.\nExtensive experiments show that CBQ achieves superior low-bit quantization\n(W4A4, W4A8, W2A16) and outperforms existing state-of-the-art methods across\nvarious LLMs and datasets. Notably, CBQ quantizes the 4-bit LLAMA1-65B model\nwithin only 4.3 hours on a single GPU, achieving a commendable tradeoff between\nperformance and quantization efficiency.", "published": "2023-12-13 07:56:27", "link": "http://arxiv.org/abs/2312.07950v5", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Robust Few-Shot Named Entity Recognition with Boundary Discrimination\n  and Correlation Purification", "abstract": "Few-shot named entity recognition (NER) aims to recognize novel named\nentities in low-resource domains utilizing existing knowledge. However, the\npresent few-shot NER models assume that the labeled data are all clean without\nnoise or outliers, and there are few works focusing on the robustness of the\ncross-domain transfer learning ability to textual adversarial attacks in\nFew-shot NER. In this work, we comprehensively explore and assess the\nrobustness of few-shot NER models under textual adversarial attack scenario,\nand found the vulnerability of existing few-shot NER models. Furthermore, we\npropose a robust two-stage few-shot NER method with Boundary Discrimination and\nCorrelation Purification (BDCP). Specifically, in the span detection stage, the\nentity boundary discriminative module is introduced to provide a highly\ndistinguishing boundary representation space to detect entity spans. In the\nentity typing stage, the correlations between entities and contexts are\npurified by minimizing the interference information and facilitating\ncorrelation generalization to alleviate the perturbations caused by textual\nadversarial attacks. In addition, we construct adversarial examples for\nfew-shot NER based on public datasets Few-NERD and Cross-Dataset. Comprehensive\nevaluations on those two groups of few-shot NER datasets containing adversarial\nexamples demonstrate the robustness and superiority of the proposed method.", "published": "2023-12-13 08:17:00", "link": "http://arxiv.org/abs/2312.07961v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SLJP: Semantic Extraction based Legal Judgment Prediction", "abstract": "Legal Judgment Prediction (LJP) is a judicial assistance system that\nrecommends the legal components such as applicable statues, prison term and\npenalty term by analyzing the given input case document. Indian legal system is\nin the need of technical assistance such as artificial intelligence to solve\nthe crores of pending cases in various courts for years and its being increased\nday to day. Most of the existing Indian models did not adequately concentrate\non the semantics embedded in the fact description (FD) that impacts the\ndecision. The proposed semantic extraction based LJP (SLJP) model provides the\nadvantages of pretrained transformers for complex unstructured legal case\ndocument understanding and to generate embeddings. The model draws the in-depth\nsemantics of the given FD at multiple levels i.e., chunk and case document\nlevel by following the divide and conquer approach. It creates the concise view\nof the given fact description using the extracted semantics as per the original\ncourt case document structure and predicts judgment using attention mechanism.\nWe tested the model performance on two available Indian datasets Indian Legal\nDocuments corpus (ILDC) and Indian Legal Statue Identification (ILSI) and got\npromising results. Also shown the highest performance and less performance\ndegradation for increased epochs than base models on ILDC dataset.", "published": "2023-12-13 08:50:02", "link": "http://arxiv.org/abs/2312.07979v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Helping Language Models Learn More: Multi-dimensional Task Prompt for\n  Few-shot Tuning", "abstract": "Large language models (LLMs) can be used as accessible and intelligent\nchatbots by constructing natural language queries and directly inputting the\nprompt into the large language model. However, different prompt' constructions\noften lead to uncertainty in the answers and thus make it hard to utilize the\nspecific knowledge of LLMs (like ChatGPT). To alleviate this, we use an\ninterpretable structure to explain the prompt learning principle in LLMs, which\ncertificates that the effectiveness of language models is determined by\nposition changes of the task's related tokens. Therefore, we propose MTPrompt,\na multi-dimensional task prompt learning method consisting based on\ntask-related object, summary, and task description information. By\nautomatically building and searching for appropriate prompts, our proposed\nMTPrompt achieves the best results on few-shot samples setting and five\ndifferent datasets. In addition, we demonstrate the effectiveness and stability\nof our method in different experimental settings and ablation experiments. In\ninteraction with large language models, embedding more task-related information\ninto prompts will make it easier to stimulate knowledge embedded in large\nlanguage models.", "published": "2023-12-13 10:00:44", "link": "http://arxiv.org/abs/2312.08027v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Model-Based Data Acquisition for Subjective Multi-Task NLP\n  Problems", "abstract": "Data annotated by humans is a source of knowledge by describing the\npeculiarities of the problem and therefore fueling the decision process of the\ntrained model. Unfortunately, the annotation process for subjective natural\nlanguage processing (NLP) problems like offensiveness or emotion detection is\noften very expensive and time-consuming. One of the inevitable risks is to\nspend some of the funds and annotator effort on annotations that do not provide\nany additional knowledge about the specific task. To minimize these costs, we\npropose a new model-based approach that allows the selection of tasks annotated\nindividually for each text in a multi-task scenario. The experiments carried\nout on three datasets, dozens of NLP tasks, and thousands of annotations show\nthat our method allows up to 40% reduction in the number of annotations with\nnegligible loss of knowledge. The results also emphasize the need to collect a\ndiverse amount of data required to efficiently train a model, depending on the\nsubjectivity of the annotation task. We also focused on measuring the relation\nbetween subjective tasks by evaluating the model in single-task and multi-task\nscenarios. Moreover, for some datasets, training only on the labels predicted\nby our model improved the efficiency of task selection as a self-supervised\nlearning regularization technique.", "published": "2023-12-13 15:03:27", "link": "http://arxiv.org/abs/2312.08198v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "High-throughput Biomedical Relation Extraction for Semi-Structured Web\n  Articles Empowered by Large Language Models", "abstract": "Objective: To develop a high-throughput biomedical relation extraction system\nthat takes advantage of the large language models'(LLMs) reading comprehension\nability and biomedical world knowledge in a scalable and evidential manner.\nMethods: We formulate the relation extraction task as binary classifications\nfor large language models. Specifically, LLMs make the decision based on the\nexternal corpus and its world knowledge, giving the reason for the judgment for\nfactual verification. This method is tailored for semi-structured web articles,\nwherein we designate the main title as the tail entity and explicitly\nincorporate it into the context, and the potential head entities are matched\nbased on a biomedical thesaurus. Moreover, lengthy contents are sliced into\ntext chunks, embedded, and retrieved with additional embedding models. Results:\nUsing an open-source LLM, we extracted 248659 relation triplets of three\ndistinct relation types from three reputable biomedical websites. To assess the\nefficacy of the basic pipeline employed for biomedical relation extraction, we\ncurated a benchmark dataset annotated by a medical expert. Evaluation results\nindicate that the pipeline exhibits performance comparable to that of GPT-4.\nCase studies further illuminate challenges faced by contemporary LLMs in the\ncontext of biomedical relation extraction for semi-structured web articles.\nConclusion: The proposed method has demonstrated its effectiveness in\nleveraging the strengths of LLMs for high-throughput biomedical relation\nextraction. Its adaptability is evident, as it can be seamlessly extended to\ndiverse semi-structured biomedical websites, facilitating the extraction of\nvarious types of biomedical relations with ease.", "published": "2023-12-13 16:43:41", "link": "http://arxiv.org/abs/2312.08274v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Prompting LLMs with content plans to enhance the summarization of\n  scientific articles", "abstract": "This paper presents novel prompting techniques to improve the performance of\nautomatic summarization systems for scientific articles. Scientific article\nsummarization is highly challenging due to the length and complexity of these\ndocuments. We conceive, implement, and evaluate prompting techniques that\nprovide additional contextual information to guide summarization systems.\nSpecifically, we feed summarizers with lists of key terms extracted from\narticles, such as author keywords or automatically generated keywords. Our\ntechniques are tested with various summarization models and input texts.\nResults show performance gains, especially for smaller models summarizing\nsections separately. This evidences that prompting is a promising approach to\novercoming the limitations of less powerful systems. Our findings introduce a\nnew research direction of using prompts to aid smaller models.", "published": "2023-12-13 16:57:31", "link": "http://arxiv.org/abs/2312.08282v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficient Toxic Content Detection by Bootstrapping and Distilling Large\n  Language Models", "abstract": "Toxic content detection is crucial for online services to remove\ninappropriate content that violates community standards. To automate the\ndetection process, prior works have proposed varieties of machine learning (ML)\napproaches to train Language Models (LMs) for toxic content detection. However,\nboth their accuracy and transferability across datasets are limited. Recently,\nLarge Language Models (LLMs) have shown promise in toxic content detection due\nto their superior zero-shot and few-shot in-context learning ability as well as\nbroad transferability on ML tasks. However, efficiently designing prompts for\nLLMs remains challenging. Moreover, the high run-time cost of LLMs may hinder\ntheir deployments in production. To address these challenges, in this work, we\npropose BD-LLM, a novel and efficient approach to Bootstrapping and Distilling\nLLMs for toxic content detection. Specifically, we design a novel prompting\nmethod named Decision-Tree-of-Thought (DToT) to bootstrap LLMs' detection\nperformance and extract high-quality rationales. DToT can automatically select\nmore fine-grained context to re-prompt LLMs when their responses lack\nconfidence. Additionally, we use the rationales extracted via DToT to fine-tune\nstudent LMs. Our experimental results on various datasets demonstrate that DToT\ncan improve the accuracy of LLMs by up to 4.6%. Furthermore, student LMs\nfine-tuned with rationales extracted via DToT outperform baselines on all\ndatasets with up to 16.9\\% accuracy improvement, while being more than 60x\nsmaller than conventional LLMs. Finally, we observe that student LMs fine-tuned\nwith rationales exhibit better cross-dataset transferability.", "published": "2023-12-13 17:22:19", "link": "http://arxiv.org/abs/2312.08303v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Beyond English: Evaluating LLMs for Arabic Grammatical Error Correction", "abstract": "Large language models (LLMs) finetuned to follow human instruction have\nrecently exhibited significant capabilities in various English NLP tasks.\nHowever, their performance in grammatical error correction (GEC), especially on\nlanguages other than English, remains significantly unexplored. In this work,\nwe evaluate the abilities of instruction finetuned LLMs in Arabic GEC, a\ncomplex task due to Arabic's rich morphology. Our findings suggest that various\nprompting methods, coupled with (in-context) few-shot learning, demonstrate\nconsiderable effectiveness, with GPT-4 achieving up to $65.49$ F$_{1}$ score\nunder expert prompting (approximately $5$ points higher than our established\nbaseline). Despite these positive results, we find that instruction finetuned\nmodels, regardless of their size, are still outperformed by fully finetuned\nones, even if they are significantly smaller in size. This disparity highlights\nsubstantial room for improvements for LLMs. Inspired by methods used in\nlow-resource machine translation, we also develop a method exploiting synthetic\ndata that significantly outperforms previous models on two standard Arabic\nbenchmarks. Our best model achieves a new SOTA on Arabic GEC, with $73.29$ and\n$73.26$ F$_{1}$ on the 2014 and 2015 QALB datasets, respectively, compared to\npeer-reviewed published baselines.", "published": "2023-12-13 05:33:25", "link": "http://arxiv.org/abs/2312.08400v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models are Complex Table Parsers", "abstract": "With the Generative Pre-trained Transformer 3.5 (GPT-3.5) exhibiting\nremarkable reasoning and comprehension abilities in Natural Language Processing\n(NLP), most Question Answering (QA) research has primarily centered around\ngeneral QA tasks based on GPT, neglecting the specific challenges posed by\nComplex Table QA. In this paper, we propose to incorporate GPT-3.5 to address\nsuch challenges, in which complex tables are reconstructed into tuples and\nspecific prompt designs are employed for dialogues. Specifically, we encode\neach cell's hierarchical structure, position information, and content as a\ntuple. By enhancing the prompt template with an explanatory description of the\nmeaning of each tuple and the logical reasoning process of the task, we\neffectively improve the hierarchical structure awareness capability of GPT-3.5\nto better parse the complex tables. Extensive experiments and results on\nComplex Table QA datasets, i.e., the open-domain dataset HiTAB and the aviation\ndomain dataset AIT-QA show that our approach significantly outperforms previous\nwork on both datasets, leading to state-of-the-art (SOTA) performance.", "published": "2023-12-13 01:34:42", "link": "http://arxiv.org/abs/2312.11521v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ToViLaG: Your Visual-Language Generative Model is Also An Evildoer", "abstract": "Warning: this paper includes model outputs showing offensive content. Recent\nlarge-scale Visual-Language Generative Models (VLGMs) have achieved\nunprecedented improvement in multimodal image/text generation. However, these\nmodels might also generate toxic content, e.g., offensive text and pornography\nimages, raising significant ethical risks. Despite exhaustive studies on toxic\ndegeneration of language models, this problem remains largely unexplored within\nthe context of visual-language generation. This work delves into the propensity\nfor toxicity generation and susceptibility to toxic data across various VLGMs.\nFor this purpose, we built ToViLaG, a dataset comprising 32K\nco-toxic/mono-toxic text-image pairs and 1K innocuous but evocative text that\ntends to stimulate toxicity. Furthermore, we propose WInToRe, a novel toxicity\nmetric tailored to visual-language generation, which theoretically reflects\ndifferent aspects of toxicity considering both input and output. On such a\nbasis, we benchmarked the toxicity of a diverse spectrum of VLGMs and\ndiscovered that some models do more evil than expected while some are more\nvulnerable to infection, underscoring the necessity of VLGMs detoxification.\nTherefore, we develop an innovative bottleneck-based detoxification method. Our\nmethod could reduce toxicity while maintaining comparable generation quality,\nproviding a promising initial solution to this line of research.", "published": "2023-12-13 08:25:07", "link": "http://arxiv.org/abs/2312.11523v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PromptBench: A Unified Library for Evaluation of Large Language Models", "abstract": "The evaluation of large language models (LLMs) is crucial to assess their\nperformance and mitigate potential security risks. In this paper, we introduce\nPromptBench, a unified library to evaluate LLMs. It consists of several key\ncomponents that are easily used and extended by researchers: prompt\nconstruction, prompt engineering, dataset and model loading, adversarial prompt\nattack, dynamic evaluation protocols, and analysis tools. PromptBench is\ndesigned to be an open, general, and flexible codebase for research purposes\nthat can facilitate original study in creating new benchmarks, deploying\ndownstream applications, and designing new evaluation protocols. The code is\navailable at: https://github.com/microsoft/promptbench and will be continuously\nsupported.", "published": "2023-12-13 05:58:34", "link": "http://arxiv.org/abs/2312.07910v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention", "abstract": "Despite many recent works on Mixture of Experts (MoEs) for resource-efficient\nTransformer language models, existing methods mostly focus on MoEs for\nfeedforward layers. Previous attempts at extending MoE to the self-attention\nlayer fail to match the performance of the parameter-matched baseline. Our\nnovel SwitchHead is an effective MoE method for the attention layer that\nsuccessfully reduces both the compute and memory requirements, achieving\nwall-clock speedup, while matching the language modeling performance of the\nbaseline Transformer. Our novel MoE mechanism allows SwitchHead to compute up\nto 8 times fewer attention matrices than the standard Transformer. SwitchHead\ncan also be combined with MoE feedforward layers, resulting in fully-MoE\n\"SwitchAll\" Transformers. For our 262M parameter model trained on C4,\nSwitchHead matches the perplexity of standard models with only 44% compute and\n27% memory usage. Zero-shot experiments on downstream tasks confirm the\nperformance of SwitchHead, e.g., achieving more than 3.5% absolute improvements\non BliMP compared to the baseline with an equal compute resource.", "published": "2023-12-13 09:00:21", "link": "http://arxiv.org/abs/2312.07987v3", "categories": ["cs.LG", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Estimation of Concept Explanations Should be Uncertainty Aware", "abstract": "Model explanations can be valuable for interpreting and debugging predictive\nmodels. We study a specific kind called Concept Explanations, where the goal is\nto interpret a model using human-understandable concepts. Although popular for\ntheir easy interpretation, concept explanations are known to be noisy. We begin\nour work by identifying various sources of uncertainty in the estimation\npipeline that lead to such noise. We then propose an uncertainty-aware Bayesian\nestimation method to address these issues, which readily improved the quality\nof explanations. We demonstrate with theoretical analysis and empirical\nevaluation that explanations computed by our method are robust to train-time\nchoices while also being label-efficient. Further, our method proved capable of\nrecovering relevant concepts amongst a bank of thousands, in an evaluation with\nreal-datasets and off-the-shelf models, demonstrating its scalability. We\nbelieve the improved quality of uncertainty-aware concept explanations make\nthem a strong candidate for more reliable model interpretation. We release our\ncode at https://github.com/vps-anonconfs/uace.", "published": "2023-12-13 11:17:27", "link": "http://arxiv.org/abs/2312.08063v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Fine-Grained Image-Text Alignment in Medical Imaging Enables Explainable\n  Cyclic Image-Report Generation", "abstract": "To address these issues, we propose a novel Adaptive patch-word Matching\n(AdaMatch) model to correlate chest X-ray (CXR) image regions with words in\nmedical reports and apply it to CXR-report generation to provide explainability\nfor the generation process. AdaMatch exploits the fine-grained relation between\nadaptive patches and words to provide explanations of specific image regions\nwith corresponding words. To capture the abnormal regions of varying sizes and\npositions, we introduce the Adaptive Patch extraction (AdaPatch) module to\nacquire the adaptive patches for these regions adaptively. In order to provide\nexplicit explainability for CXR-report generation task, we propose an\nAdaMatch-based bidirectional large language model for Cyclic CXR-report\ngeneration (AdaMatch-Cyclic). It employs the AdaMatch to obtain the keywords\nfor CXR images and `keypatches' for medical reports as hints to guide\nCXR-report generation. Extensive experiments on two publicly available CXR\ndatasets prove the effectiveness of our method and its superior performance to\nexisting methods.", "published": "2023-12-13 11:47:28", "link": "http://arxiv.org/abs/2312.08078v5", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Extending Whisper with prompt tuning to target-speaker ASR", "abstract": "Target-speaker automatic speech recognition (ASR) aims to transcribe the\ndesired speech of a target speaker from multi-talker overlapped utterances.\nMost of the existing target-speaker ASR (TS-ASR) methods involve either\ntraining from scratch or fully fine-tuning a pre-trained model, leading to\nsignificant training costs and becoming inapplicable to large foundation\nmodels. This work leverages prompt tuning, a parameter-efficient fine-tuning\napproach, to extend Whisper, a large-scale single-talker ASR model, to TS-ASR.\nVariants of prompt tuning approaches along with their configurations are\nexplored and optimized for TS-ASR.Experimental results show that prompt tuning\ncan achieve performance comparable to state-of-the-art full training approaches\nwhile only requiring about 1\\% of task-specific model parameters. Notably, the\noriginal Whisper's features, such as inverse text normalization and timestamp\ntagging, are retained in target-speaker ASR, keeping the generated\ntranscriptions natural and informative.", "published": "2023-12-13 11:49:16", "link": "http://arxiv.org/abs/2312.08079v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Conceptualizing Suicidal Behavior: Utilizing Explanations of Predicted\n  Outcomes to Analyze Longitudinal Social Media Data", "abstract": "The COVID-19 pandemic has escalated mental health crises worldwide, with\nsocial isolation and economic instability contributing to a rise in suicidal\nbehavior. Suicide can result from social factors such as shame, abuse,\nabandonment, and mental health conditions like depression, Post-Traumatic\nStress Disorder (PTSD), Attention-Deficit/Hyperactivity Disorder (ADHD),\nanxiety disorders, and bipolar disorders. As these conditions develop, signs of\nsuicidal ideation may manifest in social media interactions. Analyzing social\nmedia data using artificial intelligence (AI) techniques can help identify\npatterns of suicidal behavior, providing invaluable insights for suicide\nprevention agencies, professionals, and broader community awareness\ninitiatives. Machine learning algorithms for this purpose require large volumes\nof accurately labeled data. Previous research has not fully explored the\npotential of incorporating explanations in analyzing and labeling longitudinal\nsocial media data. In this study, we employed a model explanation method, Layer\nIntegrated Gradients, on top of a fine-tuned state-of-the-art language model,\nto assign each token from Reddit users' posts an attribution score for\npredicting suicidal ideation. By extracting and analyzing attributions of\ntokens from the data, we propose a methodology for preliminary screening of\nsocial media posts for suicidal ideation without using large language models\nduring inference.", "published": "2023-12-13 17:15:12", "link": "http://arxiv.org/abs/2312.08299v2", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Beyond Accuracy: Automated De-Identification of Large Real-World\n  Clinical Text Datasets", "abstract": "Recent research advances achieve human-level accuracy for de-identifying\nfree-text clinical notes on research datasets, but gaps remain in reproducing\nthis in large real-world settings. This paper summarizes lessons learned from\nbuilding a system used to de-identify over one billion real clinical notes, in\na fully automated way, that was independently certified by multiple\norganizations for production use. A fully automated solution requires a very\nhigh level of accuracy that does not require manual review. A hybrid\ncontext-based model architecture is described, which outperforms a Named Entity\nRecogniton (NER) - only model by 10% on the i2b2-2014 benchmark. The proposed\nsystem makes 50%, 475%, and 575% fewer errors than the comparable AWS, Azure,\nand GCP services respectively while also outperforming ChatGPT by 33%. It\nexceeds 98% coverage of sensitive data across 7 European languages, without a\nneed for fine tuning. A second set of described models enable data obfuscation\n-- replacing sensitive data with random surrogates -- while retaining name,\ndate, gender, clinical, and format consistency. Both the practical need and the\nsolution architecture that provides for reliable & linked anonymized documents\nare described.", "published": "2023-12-13 20:15:29", "link": "http://arxiv.org/abs/2312.08495v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning adaptive planning representations with natural language\n  guidance", "abstract": "Effective planning in the real world requires not only world knowledge, but\nthe ability to leverage that knowledge to build the right representation of the\ntask at hand. Decades of hierarchical planning techniques have used\ndomain-specific temporal action abstractions to support efficient and accurate\nplanning, almost always relying on human priors and domain knowledge to\ndecompose hard tasks into smaller subproblems appropriate for a goal or set of\ngoals. This paper describes Ada (Action Domain Acquisition), a framework for\nautomatically constructing task-specific planning representations using\ntask-general background knowledge from language models (LMs). Starting with a\ngeneral-purpose hierarchical planner and a low-level goal-conditioned policy,\nAda interactively learns a library of planner-compatible high-level action\nabstractions and low-level controllers adapted to a particular domain of\nplanning tasks. On two language-guided interactive planning benchmarks (Mini\nMinecraft and ALFRED Household Tasks), Ada strongly outperforms other\napproaches that use LMs for sequential decision-making, offering more accurate\nplans and better generalization to complex tasks.", "published": "2023-12-13 23:35:31", "link": "http://arxiv.org/abs/2312.08566v1", "categories": ["cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Look Before You Leap: A Universal Emergent Decomposition of Retrieval\n  Tasks in Language Models", "abstract": "When solving challenging problems, language models (LMs) are able to identify\nrelevant information from long and complicated contexts. To study how LMs solve\nretrieval tasks in diverse situations, we introduce ORION, a collection of\nstructured retrieval tasks spanning six domains, from text understanding to\ncoding. Each task in ORION can be represented abstractly by a request (e.g. a\nquestion) that retrieves an attribute (e.g. the character name) from a context\n(e.g. a story). We apply causal analysis on 18 open-source language models with\nsizes ranging from 125 million to 70 billion parameters. We find that LMs\ninternally decompose retrieval tasks in a modular way: middle layers at the\nlast token position process the request, while late layers retrieve the correct\nentity from the context. After causally enforcing this decomposition, models\nare still able to solve the original task, preserving 70% of the original\ncorrect token probability in 98 of the 106 studied model-task pairs. We connect\nour macroscopic decomposition with a microscopic description by performing a\nfine-grained case study of a question-answering task on Pythia-2.8b. Building\non our high-level understanding, we demonstrate a proof of concept application\nfor scalable internal oversight of LMs to mitigate prompt-injection while\nrequiring human supervision on only a single input. Our solution improves\naccuracy drastically (from 15.5% to 97.5% on Pythia-12b). This work presents\nevidence of a universal emergent modular processing of tasks across varied\ndomains and models and is a pioneering effort in applying interpretability for\nscalable internal oversight of LMs.", "published": "2023-12-13 18:36:43", "link": "http://arxiv.org/abs/2312.10091v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Assessing GPT4-V on Structured Reasoning Tasks", "abstract": "Multi-modality promises to unlock further uses for large language models.\nRecently, the state-of-the-art language model GPT-4 was enhanced with vision\ncapabilities. We carry out a prompting evaluation of GPT-4V and five other\nbaselines on structured reasoning tasks, such as mathematical reasoning, visual\ndata analysis, and code generation. We show that visual Chain-of-Thought, an\nextension of Chain-of-Thought to multi-modal LLMs, yields significant\nimprovements over the vanilla model. We also present a categorized analysis of\nscenarios where these models perform well and where they struggle, highlighting\nchallenges associated with coherent multimodal reasoning.", "published": "2023-12-13 08:54:49", "link": "http://arxiv.org/abs/2312.11524v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Towards Optimal Statistical Watermarking", "abstract": "We study statistical watermarking by formulating it as a hypothesis testing\nproblem, a general framework which subsumes all previous statistical\nwatermarking methods. Key to our formulation is a coupling of the output tokens\nand the rejection region, realized by pseudo-random generators in practice,\nthat allows non-trivial trade-offs between the Type I error and Type II error.\nWe characterize the Uniformly Most Powerful (UMP) watermark in the general\nhypothesis testing setting and the minimax Type II error in the model-agnostic\nsetting. In the common scenario where the output is a sequence of $n$ tokens,\nwe establish nearly matching upper and lower bounds on the number of i.i.d.\ntokens required to guarantee small Type I and Type II errors. Our rate of\n$\\Theta(h^{-1} \\log (1/h))$ with respect to the average entropy per token $h$\nhighlights potentials for improvement from the rate of $h^{-2}$ in the previous\nworks. Moreover, we formulate the robust watermarking problem where the user is\nallowed to perform a class of perturbations on the generated texts, and\ncharacterize the optimal Type II error of robust UMP tests via a linear\nprogramming problem. To the best of our knowledge, this is the first systematic\nstatistical treatment on the watermarking problem with near-optimal rates in\nthe i.i.d. setting, which might be of interest for future works.", "published": "2023-12-13 06:57:00", "link": "http://arxiv.org/abs/2312.07930v3", "categories": ["cs.LG", "cs.CL", "cs.CR", "cs.IT", "math.IT", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Audio Deepfake Detection with Self-Supervised WavLM and Multi-Fusion\n  Attentive Classifier", "abstract": "With the rapid development of speech synthesis and voice conversion\ntechnologies, Audio Deepfake has become a serious threat to the Automatic\nSpeaker Verification (ASV) system. Numerous countermeasures are proposed to\ndetect this type of attack. In this paper, we report our efforts to combine the\nself-supervised WavLM model and Multi-Fusion Attentive classifier for audio\ndeepfake detection. Our method exploits the WavLM model to extract features\nthat are more conducive to spoofing detection for the first time. Then, we\npropose a novel Multi-Fusion Attentive (MFA) classifier based on the Attentive\nStatistics Pooling (ASP) layer. The MFA captures the complementary information\nof audio features at both time and layer levels. Experiments demonstrate that\nour methods achieve state-of-the-art results on the ASVspoof 2021 DF set and\nprovide competitive results on the ASVspoof 2019 and 2021 LA set.", "published": "2023-12-13 12:09:15", "link": "http://arxiv.org/abs/2312.08089v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Metrological support of acoustic measuring installations mid-frequency\n  devices", "abstract": "The article discusses methods for measuring the speed of sound, scattering,\nattenuation and absorption of sound in liquids, on the basis of which the\nstructural diagrams of modern devices have been developed. In real conditions,\nacoustic measurement schemes are not ideal and depending on specific structure\nmay give different results. The technical and metrological characteristics of\nthe measuring channels of modern acoustic devices are presented.\nRecommendations are given for the use of GOST in acoustic measurements and it\nis stated that the metrological characteristics of newly created measuring\ninstruments operating in in situ conditions are at the level of State primary\nstandards.", "published": "2023-12-13 20:16:09", "link": "http://arxiv.org/abs/2312.08496v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "USM-Lite: Quantization and Sparsity Aware Fine-tuning for Speech\n  Recognition with Universal Speech Models", "abstract": "End-to-end automatic speech recognition (ASR) models have seen revolutionary\nquality gains with the recent development of large-scale universal speech\nmodels (USM). However, deploying these massive USMs is extremely expensive due\nto the enormous memory usage and computational cost. Therefore, model\ncompression is an important research topic to fit USM-based ASR under budget in\nreal-world scenarios. In this study, we propose a USM fine-tuning approach for\nASR, with a low-bit quantization and N:M structured sparsity aware paradigm on\nthe model weights, reducing the model complexity from parameter precision and\nmatrix topology perspectives. We conducted extensive experiments with a\n2-billion parameter USM on a large-scale voice search dataset to evaluate our\nproposed method. A series of ablation studies validate the effectiveness of up\nto int4 quantization and 2:4 sparsity. However, a single compression technique\nfails to recover the performance well under extreme setups including int2\nquantization and 1:4 sparsity. By contrast, our proposed method can compress\nthe model to have 9.4% of the size, at the cost of only 7.3% relative word\nerror rate (WER) regressions. We also provided in-depth analyses on the results\nand discussions on the limitations and potential solutions, which would be\nvaluable for future studies.", "published": "2023-12-13 22:53:54", "link": "http://arxiv.org/abs/2312.08553v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Ultra Low Complexity Deep Learning Based Noise Suppression", "abstract": "This paper introduces an innovative method for reducing the computational\ncomplexity of deep neural networks in real-time speech enhancement on\nresource-constrained devices. The proposed approach utilizes a two-stage\nprocessing framework, employing channelwise feature reorientation to reduce the\ncomputational load of convolutional operations. By combining this with a\nmodified power law compression technique for enhanced perceptual quality, this\napproach achieves noise suppression performance comparable to state-of-the-art\nmethods with significantly less computational requirements. Notably, our\nalgorithm exhibits 3 to 4 times less computational complexity and memory usage\nthan prior state-of-the-art approaches.", "published": "2023-12-13 13:34:15", "link": "http://arxiv.org/abs/2312.08132v1", "categories": ["eess.AS", "cs.LG", "eess.SP"], "primary_category": "eess.AS"}
{"title": "EmbAu: A Novel Technique to Embed Audio Data Using Shuffled Frog Leaping\n  Algorithm", "abstract": "The aim of steganographic algorithms is to identify the appropriate pixel\npositions in the host or cover image, where bits of sensitive information can\nbe concealed for data encryption. Work is being done to improve the capacity to\nintegrate sensitive information and to maintain the visual appearance of the\nsteganographic image. Consequently, steganography is a challenging research\narea. In our currently proposed image steganographic technique, we used the\nShuffled Frog Leaping Algorithm (SFLA) to determine the order of pixels by\nwhich sensitive information can be placed in the cover image. To achieve\ngreater embedding capacity, pixels from the spatial domain of the cover image\nare carefully chosen and used for placing the sensitive data. Bolstered via\nimage steganography, the final image after embedding is resistant to\nsteganalytic attacks. The SFLA algorithm serves in the optimal pixels selection\nof any colored (RGB) cover image for secret bit embedding. Using the fitness\nfunction, the SFLA benefits by reaching a minimum cost value in an acceptable\namount of time. The pixels for embedding are meticulously chosen to minimize\nthe host image's distortion upon embedding. Moreover, an effort has been taken\nto make the detection of embedded data in the steganographic image a formidable\nchallenge. Due to the enormous need for audio data encryption in the current\nworld, we feel that our suggested method has significant potential in\nreal-world applications. In this paper, we propose and compare our strategy to\nexisting steganographic methods.", "published": "2023-12-13 17:34:08", "link": "http://arxiv.org/abs/2312.08417v1", "categories": ["cs.CR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "PerMod: Perceptually Grounded Voice Modification with Latent Diffusion\n  Models", "abstract": "Perceptual modification of voice is an elusive goal. While non-experts can\nmodify an image or sentence perceptually with available tools, it is not clear\nhow to similarly modify speech along perceptual axes. Voice conversion does\nmake it possible to convert one voice to another, but these modifications are\nhandled by black box models, and the specifics of what perceptual qualities to\nmodify and how to modify them are unclear. Towards allowing greater perceptual\ncontrol over voice, we introduce PerMod, a conditional latent diffusion model\nthat takes in an input voice and a perceptual qualities vector, and produces a\nvoice with the matching perceptual qualities. Unlike prior work, PerMod\ngenerates a new voice corresponding to specific perceptual modifications.\nEvaluating perceptual quality vectors with RMSE from both human and predicted\nlabels, we demonstrate that PerMod produces voices with the desired perceptual\nqualities for typical voices, but performs poorly on atypical voices.", "published": "2023-12-13 20:14:27", "link": "http://arxiv.org/abs/2312.08494v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "PhasePerturbation: Speech Data Augmentation via Phase Perturbation for\n  Automatic Speech Recognition", "abstract": "Most of the current speech data augmentation methods operate on either the\nraw waveform or the amplitude spectrum of speech. In this paper, we propose a\nnovel speech data augmentation method called PhasePerturbation that operates\ndynamically on the phase spectrum of speech. Instead of statically rotating a\nphase by a constant degree, PhasePerturbation utilizes three dynamic phase\nspectrum operations, i.e., a randomization operation, a frequency masking\noperation, and a temporal masking operation, to enhance the diversity of speech\ndata. We conduct experiments on wav2vec2.0 pre-trained ASR models by\nfine-tuning them with the PhasePerturbation augmented TIMIT corpus. The\nexperimental results demonstrate 10.9\\% relative reduction in the word error\nrate (WER) compared with the baseline model fine-tuned without any augmentation\noperation. Furthermore, the proposed method achieves additional improvements\n(12.9\\% and 15.9\\%) in WER by complementing the Vocal Tract Length Perturbation\n(VTLP) and the SpecAug, which are both amplitude spectrum-based augmentation\nmethods. The results highlight the capability of PhasePerturbation to improve\nthe current amplitude spectrum-based augmentation methods.", "published": "2023-12-13 23:46:26", "link": "http://arxiv.org/abs/2312.08571v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "N-Gram Unsupervised Compoundation and Feature Injection for Better\n  Symbolic Music Understanding", "abstract": "The first step to apply deep learning techniques for symbolic music\nunderstanding is to transform musical pieces (mainly in MIDI format) into\nsequences of predefined tokens like note pitch, note velocity, and chords.\nSubsequently, the sequences are fed into a neural sequence model to accomplish\nspecific tasks. Music sequences exhibit strong correlations between adjacent\nelements, making them prime candidates for N-gram techniques from Natural\nLanguage Processing (NLP). Consider classical piano music: specific melodies\nmight recur throughout a piece, with subtle variations each time. In this\npaper, we propose a novel method, NG-Midiformer, for understanding symbolic\nmusic sequences that leverages the N-gram approach. Our method involves first\nprocessing music pieces into word-like sequences with our proposed unsupervised\ncompoundation, followed by using our N-gram Transformer encoder, which can\neffectively incorporate N-gram information to enhance the primary encoder part\nfor better understanding of music sequences. The pre-training process on\nlarge-scale music datasets enables the model to thoroughly learn the N-gram\ninformation contained within music sequences, and subsequently apply this\ninformation for making inferences during the fine-tuning stage. Experiment on\nvarious datasets demonstrate the effectiveness of our method and achieved\nstate-of-the-art performance on a series of music understanding downstream\ntasks. The code and model weights will be released at\nhttps://github.com/CinqueOrigin/NG-Midiformer.", "published": "2023-12-13 06:08:37", "link": "http://arxiv.org/abs/2312.08931v2", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS", "68T07", "I.2.7"], "primary_category": "cs.SD"}
{"title": "Revisiting the Entropy Semiring for Neural Speech Recognition", "abstract": "In streaming settings, speech recognition models have to map sub-sequences of\nspeech to text before the full audio stream becomes available. However, since\nalignment information between speech and text is rarely available during\ntraining, models need to learn it in a completely self-supervised way. In\npractice, the exponential number of possible alignments makes this extremely\nchallenging, with models often learning peaky or sub-optimal alignments. Prima\nfacie, the exponential nature of the alignment space makes it difficult to even\nquantify the uncertainty of a model's alignment distribution. Fortunately, it\nhas been known for decades that the entropy of a probabilistic finite state\ntransducer can be computed in time linear to the size of the transducer via a\ndynamic programming reduction based on semirings. In this work, we revisit the\nentropy semiring for neural speech recognition models, and show how alignment\nentropy can be used to supervise models through regularization or distillation.\nWe also contribute an open-source implementation of CTC and RNN-T in the\nsemiring framework that includes numerically stable and highly parallel\nvariants of the entropy semiring. Empirically, we observe that the addition of\nalignment distillation improves the accuracy and latency of an already\nwell-optimized teacher-student distillation model, achieving state-of-the-art\nperformance on the Librispeech dataset in the streaming scenario.", "published": "2023-12-13 05:22:45", "link": "http://arxiv.org/abs/2312.10087v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On Robustness to Missing Video for Audiovisual Speech Recognition", "abstract": "It has been shown that learning audiovisual features can lead to improved\nspeech recognition performance over audio-only features, especially for noisy\nspeech. However, in many common applications, the visual features are partially\nor entirely missing, e.g.~the speaker might move off screen. Multi-modal models\nneed to be robust: missing video frames should not degrade the performance of\nan audiovisual model to be worse than that of a single-modality audio-only\nmodel. While there have been many attempts at building robust models, there is\nlittle consensus on how robustness should be evaluated. To address this, we\nintroduce a framework that allows claims about robustness to be evaluated in a\nprecise and testable way. We also conduct a systematic empirical study of the\nrobustness of common audiovisual speech recognition architectures on a range of\nacoustic noise conditions and test suites. Finally, we show that an\narchitecture-agnostic solution based on cascades can consistently achieve\nrobustness to missing video, even in settings where existing techniques for\nrobustness like dropout fall short.", "published": "2023-12-13 05:32:52", "link": "http://arxiv.org/abs/2312.10088v2", "categories": ["eess.AS", "cs.CV", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head\n  Models", "abstract": "We introduce FaceTalk, a novel generative approach designed for synthesizing\nhigh-fidelity 3D motion sequences of talking human heads from input audio\nsignal. To capture the expressive, detailed nature of human heads, including\nhair, ears, and finer-scale eye movements, we propose to couple speech signal\nwith the latent space of neural parametric head models to create high-fidelity,\ntemporally coherent motion sequences. We propose a new latent diffusion model\nfor this task, operating in the expression space of neural parametric head\nmodels, to synthesize audio-driven realistic head sequences. In the absence of\na dataset with corresponding NPHM expressions to audio, we optimize for these\ncorrespondences to produce a dataset of temporally-optimized NPHM expressions\nfit to audio-video recordings of people talking. To the best of our knowledge,\nthis is the first work to propose a generative approach for realistic and\nhigh-quality motion synthesis of volumetric human heads, representing a\nsignificant advancement in the field of audio-driven 3D animation. Notably, our\napproach stands out in its ability to generate plausible motion sequences that\ncan produce high-fidelity head animation coupled with the NPHM shape space. Our\nexperimental results substantiate the effectiveness of FaceTalk, consistently\nachieving superior and visually natural motion, encompassing diverse facial\nexpressions and styles, outperforming existing methods by 75% in perceptual\nuser study evaluation.", "published": "2023-12-13 19:01:07", "link": "http://arxiv.org/abs/2312.08459v2", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
