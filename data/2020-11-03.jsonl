{"title": "Layer-Wise Multi-View Learning for Neural Machine Translation", "abstract": "Traditional neural machine translation is limited to the topmost encoder\nlayer's context representation and cannot directly perceive the lower encoder\nlayers. Existing solutions usually rely on the adjustment of network\narchitecture, making the calculation more complicated or introducing additional\nstructural restrictions. In this work, we propose layer-wise multi-view\nlearning to solve this problem, circumventing the necessity to change the model\nstructure. We regard each encoder layer's off-the-shelf output, a by-product in\nlayer-by-layer encoding, as the redundant view for the input sentence. In this\nway, in addition to the topmost encoder layer (referred to as the primary\nview), we also incorporate an intermediate encoder layer as the auxiliary view.\nWe feed the two views to a partially shared decoder to maintain independent\nprediction. Consistency regularization based on KL divergence is used to\nencourage the two views to learn from each other. Extensive experimental\nresults on five translation tasks show that our approach yields stable\nimprovements over multiple strong baselines. As another bonus, our method is\nagnostic to network architectures and can maintain the same inference speed as\nthe original model.", "published": "2020-11-03 05:06:37", "link": "http://arxiv.org/abs/2011.01482v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CharBERT: Character-aware Pre-trained Language Model", "abstract": "Most pre-trained language models (PLMs) construct word representations at\nsubword level with Byte-Pair Encoding (BPE) or its variations, by which OOV\n(out-of-vocab) words are almost avoidable. However, those methods split a word\ninto subword units and make the representation incomplete and fragile. In this\npaper, we propose a character-aware pre-trained language model named CharBERT\nimproving on the previous methods (such as BERT, RoBERTa) to tackle these\nproblems. We first construct the contextual word embedding for each token from\nthe sequential character representations, then fuse the representations of\ncharacters and the subword representations by a novel heterogeneous interaction\nmodule. We also propose a new pre-training task named NLM (Noisy LM) for\nunsupervised character representation learning. We evaluate our method on\nquestion answering, sequence labeling, and text classification tasks, both on\nthe original datasets and adversarial misspelling test sets. The experimental\nresults show that our method can significantly improve the performance and\nrobustness of PLMs simultaneously. Pretrained models, evaluation sets, and code\nare available at https://github.com/wtma/CharBERT", "published": "2020-11-03 07:13:06", "link": "http://arxiv.org/abs/2011.01513v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AraWEAT: Multidimensional Analysis of Biases in Arabic Word Embeddings", "abstract": "Recent work has shown that distributional word vector spaces often encode\nhuman biases like sexism or racism. In this work, we conduct an extensive\nanalysis of biases in Arabic word embeddings by applying a range of recently\nintroduced bias tests on a variety of embedding spaces induced from corpora in\nArabic. We measure the presence of biases across several dimensions, namely:\nembedding models (Skip-Gram, CBOW, and FastText) and vector sizes, types of\ntext (encyclopedic text, and news vs. user-generated content), dialects\n(Egyptian Arabic vs. Modern Standard Arabic), and time (diachronic analyses\nover corpora from different time periods). Our analysis yields several\ninteresting findings, e.g., that implicit gender bias in embeddings trained on\nArabic news corpora steadily increases over time (between 2007 and 2017). We\nmake the Arabic bias specifications (AraWEAT) publicly available.", "published": "2020-11-03 09:02:57", "link": "http://arxiv.org/abs/2011.01575v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Creating a Domain-diverse Corpus for Theory-based Argument Quality\n  Assessment", "abstract": "Computational models of argument quality (AQ) have focused primarily on\nassessing the overall quality or just one specific characteristic of an\nargument, such as its convincingness or its clarity. However, previous work has\nclaimed that assessment based on theoretical dimensions of argumentation could\nbenefit writers, but developing such models has been limited by the lack of\nannotated data. In this work, we describe GAQCorpus, the first large,\ndomain-diverse annotated corpus of theory-based AQ. We discuss how we designed\nthe annotation task to reliably collect a large number of judgments with\ncrowdsourcing, formulating theory-based guidelines that helped make subjective\njudgments of AQ more objective. We demonstrate how to identify arguments and\nadapt the annotation task for three diverse domains. Our work will inform\nresearch on theory-based argumentation annotation and enable the creation of\nmore diverse corpora to support computational AQ assessment.", "published": "2020-11-03 09:40:25", "link": "http://arxiv.org/abs/2011.01589v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Experiencers, Stimuli, or Targets: Which Semantic Roles Enable Machine\n  Learning to Infer the Emotions?", "abstract": "Emotion recognition is predominantly formulated as text classification in\nwhich textual units are assigned to an emotion from a predefined inventory\n(e.g., fear, joy, anger, disgust, sadness, surprise, trust, anticipation). More\nrecently, semantic role labeling approaches have been developed to extract\nstructures from the text to answer questions like: \"who is described to feel\nthe emotion?\" (experiencer), \"what causes this emotion?\" (stimulus), and at\nwhich entity is it directed?\" (target). Though it has been shown that jointly\nmodeling stimulus and emotion category prediction is beneficial for both\nsubtasks, it remains unclear which of these semantic roles enables a classifier\nto infer the emotion. Is it the experiencer, because the identity of a person\nis biased towards a particular emotion (X is always happy)? Is it a particular\ntarget (everybody loves X) or a stimulus (doing X makes everybody sad)? We\nanswer these questions by training emotion classification models on five\navailable datasets annotated with at least one semantic role by masking the\nfillers of these roles in the text in a controlled manner and find that across\nmultiple corpora, stimuli and targets carry emotion information, while the\nexperiencer might be considered a confounder. Further, we analyze if informing\nthe model about the position of the role improves the classification decision.\nParticularly on literature corpora we find that the role information improves\nthe emotion classification.", "published": "2020-11-03 10:01:44", "link": "http://arxiv.org/abs/2011.01599v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XED: A Multilingual Dataset for Sentiment Analysis and Emotion Detection", "abstract": "We introduce XED, a multilingual fine-grained emotion dataset. The dataset\nconsists of human-annotated Finnish (25k) and English sentences (30k), as well\nas projected annotations for 30 additional languages, providing new resources\nfor many low-resource languages. We use Plutchik's core emotions to annotate\nthe dataset with the addition of neutral to create a multilabel multiclass\ndataset. The dataset is carefully evaluated using language-specific BERT models\nand SVMs to show that XED performs on par with other similar datasets and is\ntherefore a useful tool for sentiment analysis and emotion detection.", "published": "2020-11-03 10:43:22", "link": "http://arxiv.org/abs/2011.01612v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Benchmark of Rule-Based and Neural Coreference Resolution in Dutch\n  Novels and News", "abstract": "We evaluate a rule-based (Lee et al., 2013) and neural (Lee et al., 2018)\ncoreference system on Dutch datasets of two domains: literary novels and\nnews/Wikipedia text. The results provide insight into the relative strengths of\ndata-driven and knowledge-driven systems, as well as the influence of domain,\ndocument length, and annotation schemes. The neural system performs best on\nnews/Wikipedia text, while the rule-based system performs best on literature.\nThe neural system shows weaknesses with limited training data and long\ndocuments, while the rule-based system is affected by annotation differences.\nThe code and models used in this paper are available at\nhttps://github.com/andreasvc/crac2020", "published": "2020-11-03 10:52:00", "link": "http://arxiv.org/abs/2011.01615v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Results of a Single Blind Literary Taste Test with Short Anonymized\n  Novel Fragments", "abstract": "It is an open question to what extent perceptions of literary quality are\nderived from text-intrinsic versus social factors. While supervised models can\npredict literary quality ratings from textual factors quite successfully, as\nshown in the Riddle of Literary Quality project (Koolen et al., 2020), this\ndoes not prove that social factors are not important, nor can we assume that\nreaders make judgments on literary quality in the same way and based on the\nsame information as machine learning models. We report the results of a pilot\nstudy to gauge the effect of textual features on literary ratings of\nDutch-language novels by participants in a controlled experiment with 48\nparticipants. In an exploratory analysis, we compare the ratings to those from\nthe large reader survey of the Riddle in which social factors were not\nexcluded, and to machine learning predictions of those literary ratings. We\nfind moderate to strong correlations of questionnaire ratings with the survey\nratings, but the predictions are closer to the survey ratings. Code and data:\nhttps://github.com/andreasvc/litquest", "published": "2020-11-03 11:10:17", "link": "http://arxiv.org/abs/2011.01624v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Entity and Relation Extraction with Set Prediction Networks", "abstract": "The joint entity and relation extraction task aims to extract all relational\ntriples from a sentence. In essence, the relational triples contained in a\nsentence are unordered. However, previous seq2seq based models require to\nconvert the set of triples into a sequence in the training phase. To break this\nbottleneck, we treat joint entity and relation extraction as a direct set\nprediction problem, so that the extraction model can get rid of the burden of\npredicting the order of multiple triples. To solve this set prediction problem,\nwe propose networks featured by transformers with non-autoregressive parallel\ndecoding. Unlike autoregressive approaches that generate triples one by one in\na certain order, the proposed networks directly output the final set of triples\nin one shot. Furthermore, we also design a set-based loss that forces unique\npredictions via bipartite matching. Compared with cross-entropy loss that\nhighly penalizes small shifts in triple order, the proposed bipartite matching\nloss is invariant to any permutation of predictions; thus, it can provide the\nproposed networks with a more accurate training signal by ignoring triple order\nand focusing on relation types and entities. Experiments on two benchmark\ndatasets show that our proposed model significantly outperforms current\nstate-of-the-art methods. Training code and trained models will be available at\nhttp://github.com/DianboWork/SPN4RE.", "published": "2020-11-03 13:04:31", "link": "http://arxiv.org/abs/2011.01675v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Word Embeddings beyond Zero-shot Machine Translation", "abstract": "We explore the transferability of a multilingual neural machine translation\nmodel to unseen languages when the transfer is grounded solely on the\ncross-lingual word embeddings. Our experimental results show that the\ntranslation knowledge can transfer weakly to other languages and that the\ndegree of transferability depends on the languages' relatedness. We also\ndiscuss the limiting aspects of the multilingual architectures that cause weak\ntranslation transfer and suggest how to mitigate the limitations.", "published": "2020-11-03 13:18:16", "link": "http://arxiv.org/abs/2011.01682v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data-to-Text Generation with Iterative Text Editing", "abstract": "We present a novel approach to data-to-text generation based on iterative\ntext editing. Our approach maximizes the completeness and semantic accuracy of\nthe output text while leveraging the abilities of recent pre-trained models for\ntext editing (LaserTagger) and language modeling (GPT-2) to improve the text\nfluency. To this end, we first transform data items to text using trivial\ntemplates, and then we iteratively improve the resulting text by a neural model\ntrained for the sentence fusion task. The output of the model is filtered by a\nsimple heuristic and reranked with an off-the-shelf pre-trained language model.\nWe evaluate our approach on two major data-to-text datasets (WebNLG, Cleaned\nE2E) and analyze its caveats and benefits. Furthermore, we show that our\nformulation of data-to-text generation opens up the possibility for zero-shot\ndomain adaptation using a general-domain dataset for sentence fusion.", "published": "2020-11-03 13:32:38", "link": "http://arxiv.org/abs/2011.01694v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Towards Automated Anamnesis Summarization: BERT-based Models for Symptom\n  Extraction", "abstract": "Professionals in modern healthcare systems are increasingly burdened by\ndocumentation workloads. Documentation of the initial patient anamnesis is\nparticularly relevant, forming the basis of successful further diagnostic\nmeasures. However, manually prepared notes are inherently unstructured and\noften incomplete. In this paper, we investigate the potential of modern NLP\ntechniques to support doctors in this matter. We present a dataset of German\npatient monologues, and formulate a well-defined information extraction task\nunder the constraints of real-world utility and practicality. In addition, we\npropose BERT-based models in order to solve said task. We can demonstrate\npromising performance of the models in both symptom identification and symptom\nattribute extraction, significantly outperforming simpler baselines.", "published": "2020-11-03 13:34:36", "link": "http://arxiv.org/abs/2011.01696v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Subword Segmentation and a Single Bridge Language Affect Zero-Shot\n  Neural Machine Translation", "abstract": "Zero-shot neural machine translation is an attractive goal because of the\nhigh cost of obtaining data and building translation systems for new\ntranslation directions. However, previous papers have reported mixed success in\nzero-shot translation. It is hard to predict in which settings it will be\neffective, and what limits performance compared to a fully supervised system.\nIn this paper, we investigate zero-shot performance of a multilingual\nEN$\\leftrightarrow${FR,CS,DE,FI} system trained on WMT data. We find that\nzero-shot performance is highly unstable and can vary by more than 6 BLEU\nbetween training runs, making it difficult to reliably track improvements. We\nobserve a bias towards copying the source in zero-shot translation, and\ninvestigate how the choice of subword segmentation affects this bias. We find\nthat language-specific subword segmentation results in less subword copying at\ntraining time, and leads to better zero-shot performance compared to jointly\ntrained segmentation. A recent trend in multilingual models is to not train on\nparallel data between all language pairs, but have a single bridge language,\ne.g. English. We find that this negatively affects zero-shot translation and\nleads to a failure mode where the model ignores the language tag and instead\nproduces English output in zero-shot directions. We show that this bias towards\nEnglish can be effectively reduced with even a small amount of parallel data in\nsome of the non-English pairs.", "published": "2020-11-03 13:45:54", "link": "http://arxiv.org/abs/2011.01703v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Event Salience in Narratives via Barthes' Cardinal Functions", "abstract": "Events in a narrative differ in salience: some are more important to the\nstory than others. Estimating event salience is useful for tasks such as story\ngeneration, and as a tool for text analysis in narratology and folkloristics.\nTo compute event salience without any annotations, we adopt Barthes' definition\nof event salience and propose several unsupervised methods that require only a\npre-trained language model. Evaluating the proposed methods on folktales with\nevent salience annotation, we show that the proposed methods outperform\nbaseline methods and find fine-tuning a language model on narrative texts is a\nkey factor in improving the proposed methods.", "published": "2020-11-03 15:28:07", "link": "http://arxiv.org/abs/2011.01785v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-Supervised Cleansing of Web Argument Corpora", "abstract": "Debate portals and similar web platforms constitute one of the main text\nsources in computational argumentation research and its applications. While the\ncorpora built upon these sources are rich of argumentatively relevant content\nand structure, they also include text that is irrelevant, or even detrimental,\nto their purpose. In this paper, we present a precision-oriented approach to\ndetecting such irrelevant text in a semi-supervised way. Given a few seed\nexamples, the approach automatically learns basic lexical patterns of relevance\nand irrelevance and then incrementally bootstraps new patterns from sentences\nmatching the patterns. In the existing args.me corpus with 400k argumentative\ntexts, our approach detects almost 87k irrelevant sentences, at a precision of\n0.97 according to manual evaluation. With low effort, the approach can be\nadapted to other web argument corpora, providing a generic way to improve\ncorpus quality.", "published": "2020-11-03 15:45:42", "link": "http://arxiv.org/abs/2011.01798v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Gap on GAP: Tackling the Problem of Differing Data Distributions in\n  Bias-Measuring Datasets", "abstract": "Diagnostic datasets that can detect biased models are an important\nprerequisite for bias reduction within natural language processing. However,\nundesired patterns in the collected data can make such tests incorrect. For\nexample, if the feminine subset of a gender-bias-measuring coreference\nresolution dataset contains sentences with a longer average distance between\nthe pronoun and the correct candidate, an RNN-based model may perform worse on\nthis subset due to long-term dependencies. In this work, we introduce a\ntheoretically grounded method for weighting test samples to cope with such\npatterns in the test data. We demonstrate the method on the GAP dataset for\ncoreference resolution. We annotate GAP with spans of all personal names and\nshow that examples in the female subset contain more personal names and a\nlonger distance between pronouns and their referents, potentially affecting the\nbias score in an undesired way. Using our weighting method, we find the set of\nweights on the test instances that should be used for coping with these\ncorrelations, and we re-evaluate 16 recently released coreference models.", "published": "2020-11-03 16:50:13", "link": "http://arxiv.org/abs/2011.01837v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Word Sense Disambiguation Biases in Machine Translation for\n  Model-Agnostic Adversarial Attacks", "abstract": "Word sense disambiguation is a well-known source of translation errors in\nNMT. We posit that some of the incorrect disambiguation choices are due to\nmodels' over-reliance on dataset artifacts found in training data, specifically\nsuperficial word co-occurrences, rather than a deeper understanding of the\nsource text. We introduce a method for the prediction of disambiguation errors\nbased on statistical data properties, demonstrating its effectiveness across\nseveral domains and model types. Moreover, we develop a simple adversarial\nattack strategy that minimally perturbs sentences in order to elicit\ndisambiguation errors to further probe the robustness of translation models.\nOur findings indicate that disambiguation robustness varies substantially\nbetween domains and that different models trained on the same data are\nvulnerable to different attacks.", "published": "2020-11-03 17:01:44", "link": "http://arxiv.org/abs/2011.01846v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Finding Friends and Flipping Frenemies: Automatic Paraphrase Dataset\n  Augmentation Using Graph Theory", "abstract": "Most NLP datasets are manually labeled, so suffer from inconsistent labeling\nor limited size. We propose methods for automatically improving datasets by\nviewing them as graphs with expected semantic properties. We construct a\nparaphrase graph from the provided sentence pair labels, and create an\naugmented dataset by directly inferring labels from the original sentence pairs\nusing a transitivity property. We use structural balance theory to identify\nlikely mislabelings in the graph, and flip their labels. We evaluate our\nmethods on paraphrase models trained using these datasets starting from a\npretrained BERT model, and find that the automatically-enhanced training sets\nresult in more accurate models.", "published": "2020-11-03 17:18:03", "link": "http://arxiv.org/abs/2011.01856v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decoupling entrainment from consistency using deep neural networks", "abstract": "Human interlocutors tend to engage in adaptive behavior known as entrainment\nto become more similar to each other. Isolating the effect of consistency,\ni.e., speakers adhering to their individual styles, is a critical part of the\nanalysis of entrainment. We propose to treat speakers' initial vocal features\nas confounds for the prediction of subsequent outputs. Using two existing\nneural approaches to deconfounding, we define new measures of entrainment that\ncontrol for consistency. These successfully discriminate real interactions from\nfake ones. Interestingly, our stricter methods correlate with social variables\nin opposite direction from previous measures that do not account for\nconsistency. These results demonstrate the advantages of using neural networks\nto model entrainment, and raise questions regarding how to interpret prior\nassociations of conversation quality with entrainment measures that do not\naccount for consistency.", "published": "2020-11-03 17:30:05", "link": "http://arxiv.org/abs/2011.01860v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Code-switched Classification Exploiting Constituent Language\n  Resources", "abstract": "Code-switching is a commonly observed communicative phenomenon denoting a\nshift from one language to another within the same speech exchange. The\nanalysis of code-switched data often becomes an assiduous task, owing to the\nlimited availability of data. We propose converting code-switched data into its\nconstituent high resource languages for exploiting both monolingual and\ncross-lingual settings in this work. This conversion allows us to utilize the\nhigher resource availability for its constituent languages for multiple\ndownstream tasks.\n  We perform experiments for two downstream tasks, sarcasm detection and hate\nspeech detection, in the English-Hindi code-switched setting. These experiments\nshow an increase in 22% and 42.5% in F1-score for sarcasm detection and hate\nspeech detection, respectively, compared to the state-of-the-art.", "published": "2020-11-03 18:43:19", "link": "http://arxiv.org/abs/2011.01913v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SimulMT to SimulST: Adapting Simultaneous Text Translation to End-to-End\n  Simultaneous Speech Translation", "abstract": "Simultaneous text translation and end-to-end speech translation have recently\nmade great progress but little work has combined these tasks together. We\ninvestigate how to adapt simultaneous text translation methods such as wait-k\nand monotonic multihead attention to end-to-end simultaneous speech translation\nby introducing a pre-decision module. A detailed analysis is provided on the\nlatency-quality trade-offs of combining fixed and flexible pre-decision with\nfixed and flexible policies. We also design a novel computation-aware latency\nmetric, adapted from Average Lagging.", "published": "2020-11-03 22:47:58", "link": "http://arxiv.org/abs/2011.02048v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Synthetic Data for Task-Oriented Semantic Parsing with\n  Hierarchical Representations", "abstract": "Modern conversational AI systems support natural language understanding for a\nwide variety of capabilities. While a majority of these tasks can be\naccomplished using a simple and flat representation of intents and slots, more\nsophisticated capabilities require complex hierarchical representations\nsupported by semantic parsing. State-of-the-art semantic parsers are trained\nusing supervised learning with data labeled according to a hierarchical schema\nwhich might be costly to obtain or not readily available for a new domain. In\nthis work, we explore the possibility of generating synthetic data for neural\nsemantic parsing using a pretrained denoising sequence-to-sequence model (i.e.,\nBART). Specifically, we first extract masked templates from the existing\nlabeled utterances, and then fine-tune BART to generate synthetic utterances\nconditioning on the extracted templates. Finally, we use an auxiliary parser\n(AP) to filter the generated utterances. The AP guarantees the quality of the\ngenerated data. We show the potential of our approach when evaluating on the\nFacebook TOP dataset for navigation domain.", "published": "2020-11-03 22:55:40", "link": "http://arxiv.org/abs/2011.02050v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Treebanking User-Generated Content: a UD Based Overview of Guidelines,\n  Corpora and Unified Recommendations", "abstract": "This article presents a discussion on the main linguistic phenomena which\ncause difficulties in the analysis of user-generated texts found on the web and\nin social media, and proposes a set of annotation guidelines for their\ntreatment within the Universal Dependencies (UD) framework of syntactic\nanalysis. Given on the one hand the increasing number of treebanks featuring\nuser-generated content, and its somewhat inconsistent treatment in these\nresources on the other, the aim of this article is twofold: (1) to provide a\ncondensed, though comprehensive, overview of such treebanks -- based on\navailable literature -- along with their main features and a comparative\nanalysis of their annotation criteria, and (2) to propose a set of tentative\nUD-based annotation guidelines, to promote consistent treatment of the\nparticular phenomena found in these types of texts. The overarching goal of\nthis article is to provide a common framework for researchers interested in\ndeveloping similar resources in UD, thus promoting cross-linguistic\nconsistency, which is a principle that has always been central to the spirit of\nUD.", "published": "2020-11-03 23:34:42", "link": "http://arxiv.org/abs/2011.02063v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Streaming Attention-Based Models with Augmented Memory for End-to-End\n  Speech Recognition", "abstract": "Attention-based models have been gaining popularity recently for their strong\nperformance demonstrated in fields such as machine translation and automatic\nspeech recognition. One major challenge of attention-based models is the need\nof access to the full sequence and the quadratically growing computational cost\nconcerning the sequence length. These characteristics pose challenges,\nespecially for low-latency scenarios, where the system is often required to be\nstreaming. In this paper, we build a compact and streaming speech recognition\nsystem on top of the end-to-end neural transducer architecture with\nattention-based modules augmented with convolution. The proposed system equips\nthe end-to-end models with the streaming capability and reduces the large\nfootprint from the streaming attention-based model using augmented memory. On\nthe LibriSpeech dataset, our proposed system achieves word error rates 2.7% on\ntest-clean and 5.8% on test-other, to our best knowledge the lowest among\nstreaming approaches reported so far.", "published": "2020-11-03 00:43:58", "link": "http://arxiv.org/abs/2011.07120v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Supervised Contrastive Learning for Pre-trained Language Model\n  Fine-tuning", "abstract": "State-of-the-art natural language understanding classification models follow\ntwo-stages: pre-training a large language model on an auxiliary task, and then\nfine-tuning the model on a task-specific labeled dataset using cross-entropy\nloss. However, the cross-entropy loss has several shortcomings that can lead to\nsub-optimal generalization and instability. Driven by the intuition that good\ngeneralization requires capturing the similarity between examples in one class\nand contrasting them with examples in other classes, we propose a supervised\ncontrastive learning (SCL) objective for the fine-tuning stage. Combined with\ncross-entropy, our proposed SCL loss obtains significant improvements over a\nstrong RoBERTa-Large baseline on multiple datasets of the GLUE benchmark in\nfew-shot learning settings, without requiring specialized architecture, data\naugmentations, memory banks, or additional unsupervised data. Our proposed\nfine-tuning objective leads to models that are more robust to different levels\nof noise in the fine-tuning training data, and can generalize better to related\ntasks with limited labeled data.", "published": "2020-11-03 01:10:39", "link": "http://arxiv.org/abs/2011.01403v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "WSL-DS: Weakly Supervised Learning with Distant Supervision for Query\n  Focused Multi-Document Abstractive Summarization", "abstract": "In the Query Focused Multi-Document Summarization (QF-MDS) task, a set of\ndocuments and a query are given where the goal is to generate a summary from\nthese documents based on the given query. However, one major challenge for this\ntask is the lack of availability of labeled training datasets. To overcome this\nissue, in this paper, we propose a novel weakly supervised learning approach\nvia utilizing distant supervision. In particular, we use datasets similar to\nthe target dataset as the training data where we leverage pre-trained sentence\nsimilarity models to generate the weak reference summary of each individual\ndocument in a document set from the multi-document gold reference summaries.\nThen, we iteratively train our summarization model on each single-document to\nalleviate the computational complexity issue that occurs while training neural\nsummarization models in multiple documents (i.e., long sequences) at once.\nExperimental results in Document Understanding Conferences (DUC) datasets show\nthat our proposed approach sets a new state-of-the-art result in terms of\nvarious evaluation metrics.", "published": "2020-11-03 02:02:55", "link": "http://arxiv.org/abs/2011.01421v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Weakly- and Semi-supervised Evidence Extraction", "abstract": "For many prediction tasks, stakeholders desire not only predictions but also\nsupporting evidence that a human can use to verify its correctness. However, in\npractice, additional annotations marking supporting evidence may only be\navailable for a minority of training examples (if available at all). In this\npaper, we propose new methods to combine few evidence annotations (strong\nsemi-supervision) with abundant document-level labels (weak supervision) for\nthe task of evidence extraction. Evaluating on two classification tasks that\nfeature evidence annotations, we find that our methods outperform baselines\nadapted from the interpretability literature to our task. Our approach yields\nsubstantial gains with as few as hundred evidence annotations. Code and\ndatasets to reproduce our work are available at\nhttps://github.com/danishpruthi/evidence-extraction.", "published": "2020-11-03 04:05:00", "link": "http://arxiv.org/abs/2011.01459v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BioNerFlair: biomedical named entity recognition using flair embedding\n  and sequence tagger", "abstract": "Motivation: The proliferation of Biomedical research articles has made the\ntask of information retrieval more important than ever. Scientists and\nResearchers are having difficulty in finding articles that contain information\nrelevant to them. Proper extraction of biomedical entities like Disease,\nDrug/chem, Species, Gene/protein, can considerably improve the filtering of\narticles resulting in better extraction of relevant information. Performance on\nBioNer benchmarks has progressively improved because of progression in\ntransformers-based models like BERT, XLNet, OpenAI, GPT2, etc. These models\ngive excellent results; however, they are computationally expensive and we can\nachieve better scores for domain-specific tasks using other contextual\nstring-based models and LSTM-CRF based sequence tagger. Results: We introduce\nBioNerFlair, a method to train models for biomedical named entity recognition\nusing Flair plus GloVe embeddings and Bidirectional LSTM-CRF based sequence\ntagger. With almost the same generic architecture widely used for named entity\nrecognition, BioNerFlair outperforms previous state-of-the-art models. I\nperformed experiments on 8 benchmarks datasets for biomedical named entity\nrecognition. Compared to current state-of-the-art models, BioNerFlair achieves\nthe best F1-score of 90.17 beyond 84.72 on the BioCreative II gene mention\n(BC2GM) corpus, best F1-score of 94.03 beyond 92.36 on the BioCreative IV\nchemical and drug (BC4CHEMD) corpus, best F1-score of 88.73 beyond 78.58 on the\nJNLPBA corpus, best F1-score of 91.1 beyond 89.71 on the NCBI disease corpus,\nbest F1-score of 85.48 beyond 78.98 on the Species-800 corpus, while near best\nresults was observed on BC5CDR-chem, BC3CDR-disease, and LINNAEUS corpus.", "published": "2020-11-03 06:46:45", "link": "http://arxiv.org/abs/2011.01504v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "DAGA: Data Augmentation with a Generation Approach for Low-resource\n  Tagging Tasks", "abstract": "Data augmentation techniques have been widely used to improve machine\nlearning performance as they enhance the generalization capability of models.\nIn this work, to generate high quality synthetic data for low-resource tagging\ntasks, we propose a novel augmentation method with language models trained on\nthe linearized labeled sentences. Our method is applicable to both supervised\nand semi-supervised settings. For the supervised settings, we conduct extensive\nexperiments on named entity recognition (NER), part of speech (POS) tagging and\nend-to-end target based sentiment analysis (E2E-TBSA) tasks. For the\nsemi-supervised settings, we evaluate our method on the NER task under the\nconditions of given unlabeled data only and unlabeled data plus a knowledge\nbase. The results show that our method can consistently outperform the\nbaselines, particularly when the given gold training data are less.", "published": "2020-11-03 07:49:15", "link": "http://arxiv.org/abs/2011.01549v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cross-Media Keyphrase Prediction: A Unified Framework with\n  Multi-Modality Multi-Head Attention and Image Wordings", "abstract": "Social media produces large amounts of contents every day. To help users\nquickly capture what they need, keyphrase prediction is receiving a growing\nattention. Nevertheless, most prior efforts focus on text modeling, largely\nignoring the rich features embedded in the matching images. In this work, we\nexplore the joint effects of texts and images in predicting the keyphrases for\na multimedia post. To better align social media style texts and images, we\npropose: (1) a novel Multi-Modality Multi-Head Attention (M3H-Att) to capture\nthe intricate cross-media interactions; (2) image wordings, in forms of optical\ncharacters and image attributes, to bridge the two modalities. Moreover, we\ndesign a unified framework to leverage the outputs of keyphrase classification\nand generation and couple their advantages. Extensive experiments on a\nlarge-scale dataset newly collected from Twitter show that our model\nsignificantly outperforms the previous state of the art based on traditional\nattention networks. Further analyses show that our multi-head attention is able\nto attend information from various aspects and boost classification or\ngeneration in diverse scenarios.", "published": "2020-11-03 08:44:18", "link": "http://arxiv.org/abs/2011.01565v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "CMT in TREC-COVID Round 2: Mitigating the Generalization Gaps from Web\n  to Special Domain Search", "abstract": "Neural rankers based on deep pretrained language models (LMs) have been shown\nto improve many information retrieval benchmarks. However, these methods are\naffected by their the correlation between pretraining domain and target domain\nand rely on massive fine-tuning relevance labels. Directly applying pretraining\nmethods to specific domains may result in suboptimal search quality because\nspecific domains may have domain adaption problems, such as the COVID domain.\nThis paper presents a search system to alleviate the special domain adaption\nproblem. The system utilizes the domain-adaptive pretraining and few-shot\nlearning technologies to help neural rankers mitigate the domain discrepancy\nand label scarcity problems. Besides, we also integrate dense retrieval to\nalleviate traditional sparse retrieval's vocabulary mismatch obstacle. Our\nsystem performs the best among the non-manual runs in Round 2 of the TREC-COVID\ntask, which aims to retrieve useful information from scientific literature\nrelated to COVID-19. Our code is publicly available at\nhttps://github.com/thunlp/OpenMatch.", "published": "2020-11-03 09:10:48", "link": "http://arxiv.org/abs/2011.01580v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "DeL-haTE: A Deep Learning Tunable Ensemble for Hate Speech Detection", "abstract": "Online hate speech on social media has become a fast-growing problem in\nrecent times. Nefarious groups have developed large content delivery networks\nacross several main-stream (Twitter and Facebook) and fringe (Gab, 4chan,\n8chan, etc.) outlets to deliver cascades of hate messages directed both at\nindividuals and communities. Thus addressing these issues has become a top\npriority for large-scale social media outlets. Three key challenges in\nautomated detection and classification of hateful content are the lack of\nclearly labeled data, evolving vocabulary and lexicon - hashtags, emojis, etc.\n- and the lack of baseline models for fringe outlets such as Gab. In this work,\nwe propose a novel framework with three major contributions. (a) We engineer an\nensemble of deep learning models that combines the strengths of\nstate-of-the-art approaches, (b) we incorporate a tuning factor into this\nframework that leverages transfer learning to conduct automated hate speech\nclassification on unlabeled datasets, like Gab, and (c) we develop a weak\nsupervised learning methodology that allows our framework to train on unlabeled\ndata. Our ensemble models achieve an 83% hate recall on the HON dataset,\nsurpassing the performance of the state-of-the-art deep models. We demonstrate\nthat weak supervised training in combination with classifier tuning\nsignificantly increases model performance on unlabeled data from Gab, achieving\na hate recall of 67%.", "published": "2020-11-03 17:32:50", "link": "http://arxiv.org/abs/2011.01861v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Warped Language Models for Noise Robust Language Understanding", "abstract": "Masked Language Models (MLM) are self-supervised neural networks trained to\nfill in the blanks in a given sentence with masked tokens. Despite the\ntremendous success of MLMs for various text based tasks, they are not robust\nfor spoken language understanding, especially for spontaneous conversational\nspeech recognition noise. In this work we introduce Warped Language Models\n(WLM) in which input sentences at training time go through the same\nmodifications as in MLM, plus two additional modifications, namely inserting\nand dropping random tokens. These two modifications extend and contract the\nsentence in addition to the modifications in MLMs, hence the word \"warped\" in\nthe name. The insertion and drop modification of the input text during training\nof WLM resemble the types of noise due to Automatic Speech Recognition (ASR)\nerrors, and as a result WLMs are likely to be more robust to ASR noise. Through\ncomputational results we show that natural language understanding systems built\non top of WLMs perform better compared to those built based on MLMs, especially\nin the presence of ASR errors.", "published": "2020-11-03 18:26:28", "link": "http://arxiv.org/abs/2011.01900v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sound Natural: Content Rephrasing in Dialog Systems", "abstract": "We introduce a new task of rephrasing for a more natural virtual assistant.\nCurrently, virtual assistants work in the paradigm of intent slot tagging and\nthe slot values are directly passed as-is to the execution engine. However,\nthis setup fails in some scenarios such as messaging when the query given by\nthe user needs to be changed before repeating it or sending it to another user.\nFor example, for queries like 'ask my wife if she can pick up the kids' or\n'remind me to take my pills', we need to rephrase the content to 'can you pick\nup the kids' and 'take your pills' In this paper, we study the problem of\nrephrasing with messaging as a use case and release a dataset of 3000 pairs of\noriginal query and rephrased query. We show that BART, a pre-trained\ntransformers-based masked language model with auto-regressive decoding, is a\nstrong baseline for the task, and show improvements by adding a copy-pointer\nand copy loss to it. We analyze different tradeoffs of BART-based and\nLSTM-based seq2seq models, and propose a distilled LSTM-based seq2seq as the\nbest practical model.", "published": "2020-11-03 20:15:46", "link": "http://arxiv.org/abs/2011.01993v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exhaustive Entity Recognition for Coptic: Challenges and Solutions", "abstract": "Entity recognition provides semantic access to ancient materials in the\nDigital Humanities: itexposes people and places of interest in texts that\ncannot be read exhaustively, facilitates linkingresources and can provide a\nwindow into text contents, even for texts with no translations. Inthis paper we\npresent entity recognition for Coptic, the language of Hellenistic era Egypt.\nWeevaluate NLP approaches to the task and lay out difficulties in applying them\nto a low-resource,morphologically complex language. We present solutions for\nnamed and non-named nested en-tity recognition and semi-automatic entity\nlinking to Wikipedia, relying on robust dependencyparsing, feature-based CRF\nmodels, and hand-crafted knowledge base resources, enabling highaccuracy NER\nwith orders of magnitude less data than those used for high resource\nlanguages.The results suggest avenues for research on other languages in\nsimilar settings.", "published": "2020-11-03 23:49:42", "link": "http://arxiv.org/abs/2011.02068v1", "categories": ["cs.CL", "cs.DL", "68-06, 68-04"], "primary_category": "cs.CL"}
{"title": "Analyzing Sustainability Reports Using Natural Language Processing", "abstract": "Climate change is a far-reaching, global phenomenon that will impact many\naspects of our society, including the global stock market\n\\cite{dietz2016climate}. In recent years, companies have increasingly been\naiming to both mitigate their environmental impact and adapt to the changing\nclimate context. This is reported via increasingly exhaustive reports, which\ncover many types of climate risks and exposures under the umbrella of\nEnvironmental, Social, and Governance (ESG). However, given this abundance of\ndata, sustainability analysts are obliged to comb through hundreds of pages of\nreports in order to find relevant information. We leveraged recent progress in\nNatural Language Processing (NLP) to create a custom model, ClimateQA, which\nallows the analysis of financial reports in order to identify climate-relevant\nsections based on a question answering approach. We present this tool and the\nmethodology that we used to develop it in the present article.", "published": "2020-11-03 21:22:42", "link": "http://arxiv.org/abs/2011.08073v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Meta-Learning for Natural Language Understanding under Continual\n  Learning Framework", "abstract": "Neural network has been recognized with its accomplishments on tackling\nvarious natural language understanding (NLU) tasks. Methods have been developed\nto train a robust model to handle multiple tasks to gain a general\nrepresentation of text. In this paper, we implement the model-agnostic\nmeta-learning (MAML) and Online aware Meta-learning (OML) meta-objective under\nthe continual framework for NLU tasks. We validate our methods on selected\nSuperGLUE and GLUE benchmark.", "published": "2020-11-03 03:41:10", "link": "http://arxiv.org/abs/2011.01452v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Explicit Prosody Models and Deep Speaker Embeddings for\n  Atypical Voice Conversion", "abstract": "Though significant progress has been made for the voice conversion (VC) of\ntypical speech, VC for atypical speech, e.g., dysarthric and second-language\n(L2) speech, remains a challenge, since it involves correcting for atypical\nprosody while maintaining speaker identity. To address this issue, we propose a\nVC system with explicit prosodic modelling and deep speaker embedding (DSE)\nlearning. First, a speech-encoder strives to extract robust phoneme embeddings\nfrom atypical speech. Second, a prosody corrector takes in phoneme embeddings\nto infer typical phoneme duration and pitch values. Third, a conversion model\ntakes phoneme embeddings and typical prosody features as inputs to generate the\nconverted speech, conditioned on the target DSE that is learned via speaker\nencoder or speaker adaptation. Extensive experiments demonstrate that speaker\nadaptation can achieve higher speaker similarity, and the speaker encoder based\nconversion model can greatly reduce dysarthric and non-native pronunciation\npatterns with improved speech intelligibility. A comparison of speech\nrecognition results between the original dysarthric speech and converted speech\nshow that absolute reduction of 47.6% character error rate (CER) and 29.3% word\nerror rate (WER) can be achieved.", "published": "2020-11-03 13:08:53", "link": "http://arxiv.org/abs/2011.01678v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Detecting Early Onset of Depression from Social Media Text using Learned\n  Confidence Scores", "abstract": "Computational research on mental health disorders from written texts covers\nan interdisciplinary area between natural language processing and psychology. A\ncrucial aspect of this problem is prevention and early diagnosis, as suicide\nresulted from depression being the second leading cause of death for young\nadults. In this work, we focus on methods for detecting the early onset of\ndepression from social media texts, in particular from Reddit. To that end, we\nexplore the eRisk 2018 dataset and achieve good results with regard to the\nstate of the art by leveraging topic analysis and learned confidence scores to\nguide the decision process.", "published": "2020-11-03 13:34:04", "link": "http://arxiv.org/abs/2011.01695v1", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Small footprint Text-Independent Speaker Verification for Embedded\n  Systems", "abstract": "Deep neural network approaches to speaker verification have proven\nsuccessful, but typical computational requirements of State-Of-The-Art (SOTA)\nsystems make them unsuited for embedded applications. In this work, we present\na two-stage model architecture orders of magnitude smaller than common\nsolutions (237.5K learning parameters, 11.5MFLOPS) reaching a competitive\nresult of 3.31% Equal Error Rate (EER) on the well established VoxCeleb1\nverification test set. We demonstrate the possibility of running our solution\non small devices typical of IoT systems such as the Raspberry Pi 3B with a\nlatency smaller than 200ms on a 5s long utterance. Additionally, we evaluate\nour model on the acoustically challenging VOiCES corpus. We report a limited\nincrease in EER of 2.6 percentage points with respect to the best scoring model\nof the 2019 VOiCES from a Distance Challenge, against a reduction of 25.6 times\nin the number of learning parameters.", "published": "2020-11-03 13:53:05", "link": "http://arxiv.org/abs/2011.01709v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Unsupervised Pattern Discovery from Thematic Speech Archives Based on\n  Multilingual Bottleneck Features", "abstract": "The present study tackles the problem of automatically discovering spoken\nkeywords from untranscribed audio archives without requiring word-by-word\nspeech transcription by automatic speech recognition (ASR) technology. The\nproblem is of practical significance in many applications of speech analytics,\nincluding those concerning low-resource languages, and large amount of\nmultilingual and multi-genre data. We propose a two-stage approach, which\ncomprises unsupervised acoustic modeling and decoding, followed by pattern\nmining in acoustic unit sequences. The whole process starts by deriving and\nmodeling a set of subword-level speech units with untranscribed data. With the\nunsupervisedly trained acoustic models, a given audio archive is represented by\na pseudo transcription, from which spoken keywords can be discovered by string\nmining algorithms. For unsupervised acoustic modeling, a deep neural network\ntrained by multilingual speech corpora is used to generate speech segmentation\nand compute bottleneck features for segment clustering. Experimental results\nshow that the proposed system is able to effectively extract topic-related\nwords and phrases from the lecture recordings on MIT OpenCourseWare.", "published": "2020-11-03 20:06:48", "link": "http://arxiv.org/abs/2011.01986v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Internal Language Model Estimation for Domain-Adaptive End-to-End Speech\n  Recognition", "abstract": "The external language models (LM) integration remains a challenging task for\nend-to-end (E2E) automatic speech recognition (ASR) which has no clear division\nbetween acoustic and language models. In this work, we propose an internal LM\nestimation (ILME) method to facilitate a more effective integration of the\nexternal LM with all pre-existing E2E models with no additional model training,\nincluding the most popular recurrent neural network transducer (RNN-T) and\nattention-based encoder-decoder (AED) models. Trained with audio-transcript\npairs, an E2E model implicitly learns an internal LM that characterizes the\ntraining data in the source domain. With ILME, the internal LM scores of an E2E\nmodel are estimated and subtracted from the log-linear interpolation between\nthe scores of the E2E model and the external LM. The internal LM scores are\napproximated as the output of an E2E model when eliminating its acoustic\ncomponents. ILME can alleviate the domain mismatch between training and\ntesting, or improve the multi-domain E2E ASR. Experimented with 30K-hour\ntrained RNN-T and AED models, ILME achieves up to 15.5% and 6.8% relative word\nerror rate reductions from Shallow Fusion on out-of-domain LibriSpeech and\nin-domain Microsoft production test sets, respectively.", "published": "2020-11-03 20:11:04", "link": "http://arxiv.org/abs/2011.01991v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Conditioned Text Generation with Transfer for Closed-Domain Dialogue\n  Systems", "abstract": "Scarcity of training data for task-oriented dialogue systems is a well known\nproblem that is usually tackled with costly and time-consuming manual data\nannotation. An alternative solution is to rely on automatic text generation\nwhich, although less accurate than human supervision, has the advantage of\nbeing cheap and fast. Our contribution is twofold. First we show how to\noptimally train and control the generation of intent-specific sentences using a\nconditional variational autoencoder. Then we introduce a new protocol called\nquery transfer that allows to leverage a large unlabelled dataset, possibly\ncontaining irrelevant queries, to extract relevant information. Comparison with\ntwo different baselines shows that this method, in the appropriate regime,\nconsistently improves the diversity of the generated queries without\ncompromising their quality. We also demonstrate the effectiveness of our\ngeneration method as a data augmentation technique for language modelling\ntasks.", "published": "2020-11-03 14:06:10", "link": "http://arxiv.org/abs/2011.02143v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "VAW-GAN for Disentanglement and Recomposition of Emotional Elements in\n  Speech", "abstract": "Emotional voice conversion (EVC) aims to convert the emotion of speech from\none state to another while preserving the linguistic content and speaker\nidentity. In this paper, we study the disentanglement and recomposition of\nemotional elements in speech through variational autoencoding Wasserstein\ngenerative adversarial network (VAW-GAN). We propose a speaker-dependent EVC\nframework based on VAW-GAN, that includes two VAW-GAN pipelines, one for\nspectrum conversion, and another for prosody conversion. We train a spectral\nencoder that disentangles emotion and prosody (F0) information from spectral\nfeatures; we also train a prosodic encoder that disentangles emotion modulation\nof prosody (affective prosody) from linguistic prosody. At run-time, the\ndecoder of spectral VAW-GAN is conditioned on the output of prosodic VAW-GAN.\nThe vocoder takes the converted spectral and prosodic features to generate the\ntarget emotional speech. Experiments validate the effectiveness of our proposed\nmethod in both objective and subjective evaluations.", "published": "2020-11-03 08:49:33", "link": "http://arxiv.org/abs/2011.02314v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Minimum Bayes Risk Training for End-to-End Speaker-Attributed ASR", "abstract": "Recently, an end-to-end speaker-attributed automatic speech recognition (E2E\nSA-ASR) model was proposed as a joint model of speaker counting, speech\nrecognition and speaker identification for monaural overlapped speech. In the\nprevious study, the model parameters were trained based on the\nspeaker-attributed maximum mutual information (SA-MMI) criterion, with which\nthe joint posterior probability for multi-talker transcription and speaker\nidentification are maximized over training data. Although SA-MMI training\nshowed promising results for overlapped speech consisting of various numbers of\nspeakers, the training criterion was not directly linked to the final\nevaluation metric, i.e., speaker-attributed word error rate (SA-WER). In this\npaper, we propose a speaker-attributed minimum Bayes risk (SA-MBR) training\nmethod where the parameters are trained to directly minimize the expected\nSA-WER over the training data. Experiments using the LibriSpeech corpus show\nthat the proposed SA-MBR training reduces the SA-WER by 9.0 % relative compared\nwith the SA-MMI-trained model.", "published": "2020-11-03 22:28:57", "link": "http://arxiv.org/abs/2011.02921v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Topic-Centric Unsupervised Multi-Document Summarization of Scientific\n  and News Articles", "abstract": "Recent advances in natural language processing have enabled automation of a\nwide range of tasks, including machine translation, named entity recognition,\nand sentiment analysis. Automated summarization of documents, or groups of\ndocuments, however, has remained elusive, with many efforts limited to\nextraction of keywords, key phrases, or key sentences. Accurate abstractive\nsummarization has yet to be achieved due to the inherent difficulty of the\nproblem, and limited availability of training data. In this paper, we propose a\ntopic-centric unsupervised multi-document summarization framework to generate\nextractive and abstractive summaries for groups of scientific articles across\n20 Fields of Study (FoS) in Microsoft Academic Graph (MAG) and news articles\nfrom DUC-2004 Task 2. The proposed algorithm generates an abstractive summary\nby developing salient language unit selection and text generation techniques.\nOur approach matches the state-of-the-art when evaluated on automated\nextractive evaluation metrics and performs better for abstractive summarization\non five human evaluation metrics (entailment, coherence, conciseness,\nreadability, and grammar). We achieve a kappa score of 0.68 between two\nco-author linguists who evaluated our results. We plan to publicly share\nMAG-20, a human-validated gold standard dataset of topic-clustered research\narticles and their summaries to promote research in abstractive summarization.", "published": "2020-11-03 04:04:21", "link": "http://arxiv.org/abs/2011.08072v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improved End-to-End Dysarthric Speech Recognition via Meta-learning\n  Based Model Re-initialization", "abstract": "Dysarthric speech recognition is a challenging task as dysarthric data is\nlimited and its acoustics deviate significantly from normal speech. Model-based\nspeaker adaptation is a promising method by using the limited dysarthric speech\nto fine-tune a base model that has been pre-trained from large amounts of\nnormal speech to obtain speaker-dependent models. However, statistic\ndistribution mismatches between the normal and dysarthric speech data limit the\nadaptation performance of the base model. To address this problem, we propose\nto re-initialize the base model via meta-learning to obtain a better model\ninitialization. Specifically, we focus on end-to-end models and extend the\nmodel-agnostic meta learning (MAML) and Reptile algorithms to meta update the\nbase model by repeatedly simulating adaptation to different dysarthric\nspeakers. As a result, the re-initialized model acquires dysarthric speech\nknowledge and learns how to perform fast adaptation to unseen dysarthric\nspeakers with improved performance. Experimental results on UASpeech dataset\nshow that the best model with proposed methods achieves 54.2% and 7.6% relative\nword error rate reduction compared with the base model without finetuning and\nthe model directly fine-tuned from the base model, respectively, and it is\ncomparable with the state-of-the-art hybrid DNN-HMM model.", "published": "2020-11-03 13:21:51", "link": "http://arxiv.org/abs/2011.01686v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A Study of Incorporating Articulatory Movement Information in Speech\n  Enhancement", "abstract": "Although deep learning algorithms are widely used for improving speech\nenhancement (SE) performance, the performance remains limited under highly\nchallenging conditions, such as unseen noise or noise signals having low\nsignal-to-noise ratios (SNRs). This study provides a pilot investigation on a\nnovel multimodal audio-articulatory-movement SE (AAMSE) model to enhance SE\nperformance under such challenging conditions. Articulatory movement features\nand acoustic signals were used as inputs to waveform-mapping-based and\nspectral-mapping-based SE systems with three fusion strategies. In addition, an\nablation study was conducted to evaluate SE performance using a limited number\nof articulatory movement sensors. Experimental results confirm that, by\ncombining the modalities, the AAMSE model notably improves the SE performance\nin terms of speech quality and intelligibility, as compared to conventional\naudio-only SE baselines.", "published": "2020-11-03 13:29:42", "link": "http://arxiv.org/abs/2011.01691v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Short-time deep-learning based source separation for speech enhancement\n  in reverberant environments with beamforming", "abstract": "The source separation-based speech enhancement problem with multiple\nbeamforming in reverberant indoor environments is addressed in this paper. We\npropose that more generic solutions should cope with time-varying dynamic\nscenarios with moving microphone array or sources such as those found in\nvoice-based human-robot interaction or smart speaker applications. The\neffectiveness of ordinary source separation methods based on statistical models\nsuch as ICA and NMF depends on the analysis window size and cannot handle\nreverberation environments. To address these limitations, a short-term source\nseparation method based on a temporal convolutional network in combination with\ncompact bilinear pooling is presented. The proposed scheme is virtually\nindependent of the analysis window size and does not lose effectiveness when\nthe analysis window is shortened to 1.6s, which in turn is very interesting to\ntackle the source separation problem in time-varying scenarios. Also,\nimprovements in WER as high as 80% were obtained when compared to ICA and NMF\nwith multi-condition reverberant training and testing, and with time-varying\nSNR experiments to simulate a moving target speech source. Finally, the\nexperiment with the estimation of the clean signal employing the proposed\nscheme and a clean trained ASR provided a WER 13% lower than the one obtained\nwith the corrupted signal and a multi-condition trained ASR. This surprising\nresult contradicts the widely adopted practice of using multi-condition trained\nASR systems and reinforce the use of speech enhancement methods for user\nprofiling in HRI environments.", "published": "2020-11-03 19:18:53", "link": "http://arxiv.org/abs/2011.01965v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Two Heads Are Better Than One: A Two-Stage Approach for Monaural Noise\n  Reduction in the Complex Domain", "abstract": "In low signal-to-noise ratio conditions, it is difficult to effectively\nrecover the magnitude and phase information simultaneously. To address this\nproblem, this paper proposes a two-stage algorithm to decouple the joint\noptimization problem w.r.t. magnitude and phase into two sub-tasks. In the\nfirst stage, only magnitude is optimized, which incorporates noisy phase to\nobtain a coarse complex clean speech spectrum estimation. In the second stage,\nboth the magnitude and phase components are refined. The experiments are\nconducted on the WSJ0-SI84 corpus, and the results show that the proposed\napproach significantly outperforms previous baselines in terms of PESQ, ESTOI,\nand SDR.", "published": "2020-11-03 08:34:12", "link": "http://arxiv.org/abs/2011.01561v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Dynamic latency speech recognition with asynchronous revision", "abstract": "In this work we propose an inference technique, asynchronous revision, to\nunify streaming and non-streaming speech recognition models. Specifically, we\nachieve dynamic latency with only one model by using arbitrary right context\nduring inference. The model is composed of a stack of convolutional layers for\naudio encoding. In inference stage, the history states of encoder and decoder\ncan be asynchronously revised to trade off between the latency and the accuracy\nof the model. To alleviate training and inference mismatch, we propose a\ntraining technique, segment cropping, which randomly splits input utterances\ninto several segments with forward connections. This allows us to have dynamic\nlatency speech recognition results with large improvements in accuracy.\nExperiments show that our dynamic latency model with asynchronous revision\ngives 8\\%-14\\% relative improvements over the streaming models.", "published": "2020-11-03 08:50:43", "link": "http://arxiv.org/abs/2011.01570v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving RNN transducer with normalized jointer network", "abstract": "Recurrent neural transducer (RNN-T) is a promising end-to-end (E2E) model in\nautomatic speech recognition (ASR). It has shown superior performance compared\nto traditional hybrid ASR systems. However, training RNN-T from scratch is\nstill challenging. We observe a huge gradient variance during RNN-T training\nand suspect it hurts the performance. In this work, we analyze the cause of the\nhuge gradient variance in RNN-T training and proposed a new \\textit{normalized\njointer network} to overcome it. We also propose to enhance the RNN-T network\nwith a modified conformer encoder network and transformer-XL predictor networks\nto achieve the best performance. Experiments are conducted on the open 170-hour\nAISHELL-1 and industrial-level 30000-hour mandarin speech dataset. On the\nAISHELL-1 dataset, our RNN-T system gets state-of-the-art results on\nAISHELL-1's streaming and non-streaming benchmark with CER 6.15\\% and 5.37\\%\nrespectively. We further compare our RNN-T system with our well trained\ncommercial hybrid system on 30000-hour-industry audio data and get 9\\% relative\nimprovement without pre-training or external language model.", "published": "2020-11-03 09:03:59", "link": "http://arxiv.org/abs/2011.01576v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DOVER-Lap: A Method for Combining Overlap-aware Diarization Outputs", "abstract": "Several advances have been made recently towards handling overlapping speech\nfor speaker diarization. Since speech and natural language tasks often benefit\nfrom ensemble techniques, we propose an algorithm for combining outputs from\nsuch diarization systems through majority voting. Our method, DOVER-Lap, is\ninspired from the recently proposed DOVER algorithm, but is designed to handle\noverlapping segments in diarization outputs. We also modify the pair-wise\nincremental label mapping strategy used in DOVER, and propose an approximation\nalgorithm based on weighted k-partite graph matching, which performs this\nmapping using a global cost tensor. We demonstrate the strength of our method\nby combining outputs from diverse systems -- clustering-based, region proposal\nnetworks, and target-speaker voice activity detection -- on AMI and LibriCSS\ndatasets, where it consistently outperforms the single best system.\nAdditionally, we show that DOVER-Lap can be used for late fusion in\nmultichannel diarization, and compares favorably with early fusion methods like\nbeamforming.", "published": "2020-11-03 20:29:15", "link": "http://arxiv.org/abs/2011.01997v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Complex ratio masking for singing voice separation", "abstract": "Music source separation is important for applications such as karaoke and\nremixing. Much of previous research focuses on estimating short-time Fourier\ntransform (STFT) magnitude and discarding phase information. We observe that,\nfor singing voice separation, phase can make considerable improvement in\nseparation quality. This paper proposes a complex ratio masking method for\nvoice and accompaniment separation. The proposed method employs DenseUNet with\nself attention to estimate the real and imaginary components of STFT for each\nsound source. A simple ensemble technique is introduced to further improve\nseparation performance. Evaluation results demonstrate that the proposed method\noutperforms recent state-of-the-art models for both separated voice and\naccompaniment.", "published": "2020-11-03 21:19:36", "link": "http://arxiv.org/abs/2011.02008v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Integration of speech separation, diarization, and recognition for\n  multi-speaker meetings: System description, comparison, and analysis", "abstract": "Multi-speaker speech recognition of unsegmented recordings has diverse\napplications such as meeting transcription and automatic subtitle generation.\nWith technical advances in systems dealing with speech separation, speaker\ndiarization, and automatic speech recognition (ASR) in the last decade, it has\nbecome possible to build pipelines that achieve reasonable error rates on this\ntask. In this paper, we propose an end-to-end modular system for the LibriCSS\nmeeting data, which combines independently trained separation, diarization, and\nrecognition components, in that order. We study the effect of different\nstate-of-the-art methods at each stage of the pipeline, and report results\nusing task-specific metrics like SDR and DER, as well as downstream WER.\nExperiments indicate that the problem of overlapping speech for diarization and\nASR can be effectively mitigated with the presence of a well-trained separation\nmodule. Our best system achieves a speaker-attributed WER of 12.7%, which is\nclose to that of a non-overlapping ASR.", "published": "2020-11-03 21:33:29", "link": "http://arxiv.org/abs/2011.02014v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Training Wake Word Detection with Synthesized Speech Data on Confusion\n  Words", "abstract": "Confusing-words are commonly encountered in real-life keyword spotting\napplications, which causes severe degradation of performance due to complex\nspoken terms and various kinds of words that sound similar to the predefined\nkeywords. To enhance the wake word detection system's robustness on such\nscenarios, we investigate two data augmentation setups for training end-to-end\nKWS systems. One is involving the synthesized data from a multi-speaker speech\nsynthesis system, and the other augmentation is performed by adding random\nnoise to the acoustic feature. Experimental results show that augmentations\nhelp improve the system's robustness. Moreover, by augmenting the training set\nwith the synthetic data generated by the multi-speaker text-to-speech system,\nwe achieve a significant improvement regarding confusing words scenario.", "published": "2020-11-03 04:06:04", "link": "http://arxiv.org/abs/2011.01460v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "ShaneRun System Description to VoxCeleb Speaker Recognition Challenge\n  2020", "abstract": "In this report, we describe the submission of ShaneRun's team to the VoxCeleb\nSpeaker Recognition Challenge (VoxSRC) 2020. We use ResNet-34 as encoder to\nextract the speaker embeddings, which is referenced from the open-source\nvoxceleb-trainer. We also provide a simple method to implement optimum fusion\nusing t-SNE normalized distance of testing utterance pairs instead of original\nnegative Euclidean distance from the encoder. The final submitted system got\n0.3098 minDCF and 5.076 % ERR for Fixed data track, which outperformed the\nbaseline by 1.3 % minDCF and 2.2 % ERR respectively.", "published": "2020-11-03 07:26:21", "link": "http://arxiv.org/abs/2011.01518v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "StyleMelGAN: An Efficient High-Fidelity Adversarial Vocoder with\n  Temporal Adaptive Normalization", "abstract": "In recent years, neural vocoders have surpassed classical speech generation\napproaches in naturalness and perceptual quality of the synthesized speech.\nComputationally heavy models like WaveNet and WaveGlow achieve best results,\nwhile lightweight GAN models, e.g. MelGAN and Parallel WaveGAN, remain inferior\nin terms of perceptual quality. We therefore propose StyleMelGAN, a lightweight\nneural vocoder allowing synthesis of high-fidelity speech with low\ncomputational complexity. StyleMelGAN employs temporal adaptive normalization\nto style a low-dimensional noise vector with the acoustic features of the\ntarget speech. For efficient training, multiple random-window discriminators\nadversarially evaluate the speech signal analyzed by a filter bank, with\nregularization provided by a multi-scale spectral reconstruction loss. The\nhighly parallelizable speech generation is several times faster than real-time\non CPUs and GPUs. MUSHRA and P.800 listening tests show that StyleMelGAN\noutperforms prior neural vocoders in copy-synthesis and Text-to-Speech\nscenarios.", "published": "2020-11-03 08:28:47", "link": "http://arxiv.org/abs/2011.01557v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Problems using deep generative models for probabilistic audio source\n  separation", "abstract": "Recent advancements in deep generative modeling make it possible to learn\nprior distributions from complex data that subsequently can be used for\nBayesian inference. However, we find that distributions learned by deep\ngenerative models for audio signals do not exhibit the right properties that\nare necessary for tasks like audio source separation using a probabilistic\napproach. We observe that the learned prior distributions are either\ndiscriminative and extremely peaked or smooth and non-discriminative. We\nquantify this behavior for two types of deep generative models on two audio\ndatasets.", "published": "2020-11-03 15:01:47", "link": "http://arxiv.org/abs/2011.01761v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "A Two-Stage Approach to Device-Robust Acoustic Scene Classification", "abstract": "To improve device robustness, a highly desirable key feature of a competitive\ndata-driven acoustic scene classification (ASC) system, a novel two-stage\nsystem based on fully convolutional neural networks (CNNs) is proposed. Our\ntwo-stage system leverages on an ad-hoc score combination based on two CNN\nclassifiers: (i) the first CNN classifies acoustic inputs into one of three\nbroad classes, and (ii) the second CNN classifies the same inputs into one of\nten finer-grained classes. Three different CNN architectures are explored to\nimplement the two-stage classifiers, and a frequency sub-sampling scheme is\ninvestigated. Moreover, novel data augmentation schemes for ASC are also\ninvestigated. Evaluated on DCASE 2020 Task 1a, our results show that the\nproposed ASC system attains a state-of-the-art accuracy on the development set,\nwhere our best system, a two-stage fusion of CNN ensembles, delivers a 81.9%\naverage accuracy among multi-device test data, and it obtains a significant\nimprovement on unseen devices. Finally, neural saliency analysis with class\nactivation mapping (CAM) gives new insights on the patterns learnt by our\nmodels.", "published": "2020-11-03 03:27:18", "link": "http://arxiv.org/abs/2011.01447v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
