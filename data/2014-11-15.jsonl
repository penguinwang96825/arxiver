{"title": "Retrofitting Word Vectors to Semantic Lexicons", "abstract": "Vector space word representations are learned from distributional information\nof words in large corpora. Although such statistics are semantically\ninformative, they disregard the valuable information that is contained in\nsemantic lexicons such as WordNet, FrameNet, and the Paraphrase Database. This\npaper proposes a method for refining vector space representations using\nrelational information from semantic lexicons by encouraging linked words to\nhave similar vector representations, and it makes no assumptions about how the\ninput vectors were constructed. Evaluated on a battery of standard lexical\nsemantic evaluation tasks in several languages, we obtain substantial\nimprovements starting with a variety of word vector models. Our refinement\nmethod outperforms prior techniques for incorporating semantic lexicons into\nthe word vector training algorithms.", "published": "2014-11-15 17:34:20", "link": "http://arxiv.org/abs/1411.4166v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Resolution of Difficult Pronouns Using the ROSS Method", "abstract": "A new natural language understanding method for disambiguation of difficult\npronouns is described. Difficult pronouns are those pronouns for which a level\nof world or domain knowledge is needed in order to perform anaphoral or other\ntypes of resolution. Resolution of difficult pronouns may in some cases require\na prior step involving the application of inference to a situation that is\nrepresented by the natural language text. A general method is described: it\nperforms entity resolution and pronoun resolution. An extension to the general\npronoun resolution method performs inference as an embedded commonsense\nreasoning method. The general method and the embedded method utilize features\nof the ROSS representational scheme; in particular the methods use ROSS\nontology classes and the ROSS situation model. The overall method is a working\nsolution that solves the following Winograd schemas: a) trophy and suitcase, b)\nperson lifts person, c) person pays detective, and d) councilmen and\ndemonstrators.", "published": "2014-11-15 03:43:01", "link": "http://arxiv.org/abs/1411.4109v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ROSS User's Guide and Reference Manual (Version 1.0)", "abstract": "The ROSS method is a new approach in the area of knowledge representation\nthat is useful for many artificial intelligence and natural language\nunderstanding representation and reasoning tasks. (ROSS stands for\n\"Representation\", \"Ontology\", \"Structure\", \"Star\" language). ROSS is a physical\nsymbol-based representational scheme. ROSS provides a complex model for the\ndeclarative representation of physical structure and for the representation of\nprocesses and causality. From the metaphysical perspective, the ROSS view of\nexternal reality involves a 4D model, wherein discrete single-time-point\nunit-sized locations with states are the basis for all objects, processes and\naspects that can be modeled. ROSS includes a language called \"Star\" for the\nspecification of ontology classes. The ROSS method also includes a formal\nscheme called the \"instance model\". Instance models are used in the area of\nnatural language meaning representation to represent situations. This document\nis an in-depth specification of the ROSS method.", "published": "2014-11-15 22:47:35", "link": "http://arxiv.org/abs/1411.4194v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Definition of Visual Speech Element and Research on a Method of\n  Extracting Feature Vector for Korean Lip-Reading", "abstract": "In this paper, we defined the viseme (visual speech element) and described\nabout the method of extracting visual feature vector. We defined the 10 visemes\nbased on vowel by analyzing of Korean utterance and proposed the method of\nextracting the 20-dimensional visual feature vector, combination of static\nfeatures and dynamic features. Lastly, we took an experiment in recognizing\nwords based on 3-viseme HMM and evaluated the efficiency.", "published": "2014-11-15 05:44:10", "link": "http://arxiv.org/abs/1411.4114v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Investigating the Role of Prior Disambiguation in Deep-learning\n  Compositional Models of Meaning", "abstract": "This paper aims to explore the effect of prior disambiguation on neural\nnetwork- based compositional models, with the hope that better semantic\nrepresentations for text compounds can be produced. We disambiguate the input\nword vectors before they are fed into a compositional deep net. A series of\nevaluations shows the positive effect of prior disambiguation for such deep\nmodels.", "published": "2014-11-15 06:32:49", "link": "http://arxiv.org/abs/1411.4116v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
