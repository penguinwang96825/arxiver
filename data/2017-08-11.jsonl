{"title": "N-gram and Neural Language Models for Discriminating Similar Languages", "abstract": "This paper describes our submission (named clac) to the 2016 Discriminating\nSimilar Languages (DSL) shared task. We participated in the closed Sub-task 1\n(Set A) with two separate machine learning techniques. The first approach is a\ncharacter based Convolution Neural Network with a bidirectional long short term\nmemory (BiLSTM) layer (CLSTM), which achieved an accuracy of 78.45% with\nminimal tuning. The second approach is a character-based n-gram model. This\nlast approach achieved an accuracy of 88.45% which is close to the accuracy of\n89.38% achieved by the best submission, and allowed us to rank #7 overall.", "published": "2017-08-11 02:27:26", "link": "http://arxiv.org/abs/1708.03421v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Argument Labeling of Explicit Discourse Relations using LSTM Neural\n  Networks", "abstract": "Argument labeling of explicit discourse relations is a challenging task. The\nstate of the art systems achieve slightly above 55% F-measure but require\nhand-crafted features. In this paper, we propose a Long Short Term Memory\n(LSTM) based model for argument labeling. We experimented with multiple\nconfigurations of our model. Using the PDTB dataset, our best model achieved an\nF1 measure of 23.05% without any feature engineering. This is significantly\nhigher than the 20.52% achieved by the state of the art RNN approach, but\nsignificantly lower than the feature based state of the art systems. On the\nother hand, because our approach learns only from the raw dataset, it is more\nwidely applicable to multiple textual genres and languages.", "published": "2017-08-11 03:46:31", "link": "http://arxiv.org/abs/1708.03425v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What matters in a transferable neural network model for relation\n  classification in the biomedical domain?", "abstract": "Lack of sufficient labeled data often limits the applicability of advanced\nmachine learning algorithms to real life problems. However efficient use of\nTransfer Learning (TL) has been shown to be very useful across domains. TL\nutilizes valuable knowledge learned in one task (source task), where sufficient\ndata is available, to the task of interest (target task). In biomedical and\nclinical domain, it is quite common that lack of sufficient training data do\nnot allow to fully exploit machine learning models. In this work, we present\ntwo unified recurrent neural models leading to three transfer learning\nframeworks for relation classification tasks. We systematically investigate\neffectiveness of the proposed frameworks in transferring the knowledge under\nmultiple aspects related to source and target tasks, such as, similarity or\nrelatedness between source and target tasks, and size of training data for\nsource task. Our empirical results show that the proposed frameworks in general\nimprove the model performance, however these improvements do depend on aspects\nrelated to source and target tasks. This dependence then finally determine the\nchoice of a particular TL framework.", "published": "2017-08-11 06:21:22", "link": "http://arxiv.org/abs/1708.03446v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unified Neural Architecture for Drug, Disease and Clinical Entity\n  Recognition", "abstract": "Most existing methods for biomedical entity recognition task rely on explicit\nfeature engineering where many features either are specific to a particular\ntask or depends on output of other existing NLP tools. Neural architectures\nhave been shown across various domains that efforts for explicit feature design\ncan be reduced. In this work we propose an unified framework using\nbi-directional long short term memory network (BLSTM) for named entity\nrecognition (NER) tasks in biomedical and clinical domains. Three important\ncharacteristics of the framework are as follows - (1) model learns contextual\nas well as morphological features using two different BLSTM in hierarchy, (2)\nmodel uses first order linear conditional random field (CRF) in its output\nlayer in cascade of BLSTM to infer label or tag sequence, (3) model does not\nuse any domain specific features or dictionary, i.e., in another words, same\nset of features are used in the three NER tasks, namely, disease name\nrecognition (Disease NER), drug name recognition (Drug NER) and clinical entity\nrecognition (Clinical NER). We compare performance of the proposed model with\nexisting state-of-the-art models on the standard benchmark datasets of the\nthree tasks. We show empirically that the proposed framework outperforms all\nexisting models. Further our analysis of CRF layer and word-embedding obtained\nusing character based embedding show their importance.", "published": "2017-08-11 06:30:23", "link": "http://arxiv.org/abs/1708.03447v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Break it Down for Me: A Study in Automated Lyric Annotation", "abstract": "Comprehending lyrics, as found in songs and poems, can pose a challenge to\nhuman and machine readers alike. This motivates the need for systems that can\nunderstand the ambiguity and jargon found in such creative texts, and provide\ncommentary to aid readers in reaching the correct interpretation. We introduce\nthe task of automated lyric annotation (ALA). Like text simplification, a goal\nof ALA is to rephrase the original text in a more easily understandable manner.\nHowever, in ALA the system must often include additional information to clarify\nniche terminology and abstract concepts. To stimulate research on this task, we\nrelease a large collection of crowdsourced annotations for song lyrics. We\nanalyze the performance of translation and retrieval models on this task,\nmeasuring performance with both automated and human evaluation. We find that\neach model captures a unique type of information important to the task.", "published": "2017-08-11 09:58:39", "link": "http://arxiv.org/abs/1708.03492v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Identification of AltLexes using Monolingual Parallel Corpora", "abstract": "The automatic identification of discourse relations is still a challenging\ntask in natural language processing. Discourse connectives, such as \"since\" or\n\"but\", are the most informative cues to identify explicit relations; however\ndiscourse parsers typically use a closed inventory of such connectives. As a\nresult, discourse relations signaled by markers outside these inventories (i.e.\nAltLexes) are not detected as effectively. In this paper, we propose a novel\nmethod to leverage parallel corpora in text simplification and lexical\nresources to automatically identify alternative lexicalizations that signal\ndiscourse relation. When applied to the Simple Wikipedia and Newsela corpora\nalong with WordNet and the PPDB, the method allowed the automatic discovery of\n91 AltLexes.", "published": "2017-08-11 13:45:48", "link": "http://arxiv.org/abs/1708.03541v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simple and Effective Dimensionality Reduction for Word Embeddings", "abstract": "Word embeddings have become the basic building blocks for several natural\nlanguage processing and information retrieval tasks. Pre-trained word\nembeddings are used in several downstream applications as well as for\nconstructing representations for sentences, paragraphs and documents. Recently,\nthere has been an emphasis on further improving the pre-trained word vectors\nthrough post-processing algorithms. One such area of improvement is the\ndimensionality reduction of the word embeddings. Reducing the size of word\nembeddings through dimensionality reduction can improve their utility in memory\nconstrained devices, benefiting several real-world applications. In this work,\nwe present a novel algorithm that effectively combines PCA based dimensionality\nreduction with a recently proposed post-processing algorithm, to construct word\nembeddings of lower dimensions. Empirical evaluations on 12 standard word\nsimilarity benchmarks show that our algorithm reduces the embedding\ndimensionality by 50%, while achieving similar or (more often) better\nperformance than the higher dimension embeddings.", "published": "2017-08-11 17:52:36", "link": "http://arxiv.org/abs/1708.03629v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emotion Intensities in Tweets", "abstract": "This paper examines the task of detecting intensity of emotion from text. We\ncreate the first datasets of tweets annotated for anger, fear, joy, and sadness\nintensities. We use a technique called best--worst scaling (BWS) that improves\nannotation consistency and obtains reliable fine-grained scores. We show that\nemotion-word hashtags often impact emotion intensity, usually conveying a more\nintense emotion. Finally, we create a benchmark regression system and conduct\nexperiments to determine: which features are useful for detecting emotion\nintensity, and, the extent to which two emotions are similar in terms of how\nthey manifest in language.", "published": "2017-08-11 20:33:02", "link": "http://arxiv.org/abs/1708.03696v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improved Abusive Comment Moderation with User Embeddings", "abstract": "Experimenting with a dataset of approximately 1.6M user comments from a Greek\nnews sports portal, we explore how a state of the art RNN-based moderation\nmethod can be improved by adding user embeddings, user type embeddings, user\nbiases, or user type biases. We observe improvements in all cases, with user\nembeddings leading to the biggest performance gains.", "published": "2017-08-11 20:37:27", "link": "http://arxiv.org/abs/1708.03699v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WASSA-2017 Shared Task on Emotion Intensity", "abstract": "We present the first shared task on detecting the intensity of emotion felt\nby the speaker of a tweet. We create the first datasets of tweets annotated for\nanger, fear, joy, and sadness intensities using a technique called best--worst\nscaling (BWS). We show that the annotations lead to reliable fine-grained\nintensity scores (rankings of tweets by intensity). The data was partitioned\ninto training, development, and test sets for the competition. Twenty-two teams\nparticipated in the shared task, with the best system obtaining a Pearson\ncorrelation of 0.747 with the gold intensity scores. We summarize the machine\nlearning setups, resources, and tools used by the participating teams, with a\nfocus on the techniques and resources that are particularly useful for the\ntask. The emotion intensity dataset and the shared task are helping improve our\nunderstanding of how we convey more or less intense emotions through language.", "published": "2017-08-11 20:40:07", "link": "http://arxiv.org/abs/1708.03700v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Word Clouds with Background Corpus Normalization and\n  t-distributed Stochastic Neighbor Embedding", "abstract": "Many word clouds provide no semantics to the word placement, but use a random\nlayout optimized solely for aesthetic purposes. We propose a novel approach to\nmodel word significance and word affinity within a document, and in comparison\nto a large background corpus. We demonstrate its usefulness for generating more\nmeaningful word clouds as a visual summary of a given document. We then select\nkeywords based on their significance and construct the word cloud based on the\nderived affinity. Based on a modified t-distributed stochastic neighbor\nembedding (t-SNE), we generate a semantic word placement. For words that\ncooccur significantly, we include edges, and cluster the words according to\ntheir cooccurrence. For this we designed a scalable and memory-efficient\nsketch-based approach usable on commodity hardware to aggregate the required\ncorpus statistics needed for normalization, and for identifying keywords as\nwell as significant cooccurences. We empirically validate our approch using a\nlarge Wikipedia corpus.", "published": "2017-08-11 15:19:53", "link": "http://arxiv.org/abs/1708.03569v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Learning to Attend, Copy, and Generate for Session-Based Query\n  Suggestion", "abstract": "Users try to articulate their complex information needs during search\nsessions by reformulating their queries. To make this process more effective,\nsearch engines provide related queries to help users in specifying the\ninformation need in their search process. In this paper, we propose a\ncustomized sequence-to-sequence model for session-based query suggestion. In\nour model, we employ a query-aware attention mechanism to capture the structure\nof the session context. is enables us to control the scope of the session from\nwhich we infer the suggested next query, which helps not only handle the noisy\ndata but also automatically detect session boundaries. Furthermore, we observe\nthat, based on the user query reformulation behavior, within a single session a\nlarge portion of query terms is retained from the previously submitted queries\nand consists of mostly infrequent or unseen terms that are usually not included\nin the vocabulary. We therefore empower the decoder of our model to access the\nsource words from the session context during decoding by incorporating a copy\nmechanism. Moreover, we propose evaluation metrics to assess the quality of the\ngenerative models for query suggestion. We conduct an extensive set of\nexperiments and analysis. e results suggest that our model outperforms the\nbaselines both in terms of the generating queries and scoring candidate queries\nfor the task of query suggestion.", "published": "2017-08-11 00:55:57", "link": "http://arxiv.org/abs/1708.03418v4", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
