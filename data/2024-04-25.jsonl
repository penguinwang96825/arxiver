{"title": "Interpreting Answers to Yes-No Questions in Dialogues from Multiple\n  Domains", "abstract": "People often answer yes-no questions without explicitly saying yes, no, or\nsimilar polar keywords. Figuring out the meaning of indirect answers is\nchallenging, even for large language models. In this paper, we investigate this\nproblem working with dialogues from multiple domains. We present new benchmarks\nin three diverse domains: movie scripts, tennis interviews, and airline\ncustomer service. We present an approach grounded on distant supervision and\nblended training to quickly adapt to a new dialogue domain. Experimental\nresults show that our approach is never detrimental and yields F1 improvements\nas high as 11-34%.", "published": "2024-04-25 00:13:00", "link": "http://arxiv.org/abs/2404.16262v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PILA: A Historical-Linguistic Dataset of Proto-Italic and Latin", "abstract": "Computational historical linguistics seeks to systematically understand\nprocesses of sound change, including during periods at which little to no\nformal recording of language is attested. At the same time, few computational\nresources exist which deeply explore phonological and morphological connections\nbetween proto-languages and their descendants. This is particularly true for\nthe family of Italic languages. To assist historical linguists in the study of\nItalic sound change, we introduce the Proto-Italic to Latin (PILA) dataset,\nwhich consists of roughly 3,000 pairs of forms from Proto-Italic and Latin. We\nprovide a detailed description of how our dataset was created and organized.\nThen, we exhibit PILA's value in two ways. First, we present baseline results\nfor PILA on a pair of traditional computational historical linguistics tasks.\nSecond, we demonstrate PILA's capability for enhancing other\nhistorical-linguistic datasets through a dataset compatibility study.", "published": "2024-04-25 05:33:47", "link": "http://arxiv.org/abs/2404.16341v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Don't Say No: Jailbreaking LLM by Suppressing Refusal", "abstract": "Ensuring the safety alignment of Large Language Models (LLMs) is crucial to\ngenerating responses consistent with human values. Despite their ability to\nrecognize and avoid harmful queries, LLMs are vulnerable to jailbreaking\nattacks, where carefully crafted prompts seduce them to produce toxic content.\nOne category of jailbreak attacks is reformulating the task as an optimization\nby eliciting the LLM to generate affirmative responses. However, such\noptimization objective has its own limitations, such as the restriction on the\npredefined objectionable behaviors, leading to suboptimal attack performance.\nIn this study, we first uncover the reason why vanilla target loss is not\noptimal, then we explore and enhance the loss objective and introduce the DSN\n(Don't Say No) attack, which achieves successful attack by suppressing refusal.\nAnother challenge in studying jailbreak attacks is the evaluation, as it is\ndifficult to directly and accurately assess the harmfulness of the responses.\nThe existing evaluation such as refusal keyword matching reveals numerous false\npositive and false negative instances. To overcome this challenge, we propose\nan Ensemble Evaluation pipeline that novelly incorporates Natural Language\nInference (NLI) contradiction assessment and two external LLM evaluators.\nExtensive experiments demonstrate the potential of the DSN and effectiveness of\nEnsemble Evaluation compared to baseline methods.", "published": "2024-04-25 07:15:23", "link": "http://arxiv.org/abs/2404.16369v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lost in Recursion: Mining Rich Event Semantics in Knowledge Graphs", "abstract": "Our world is shaped by events of various complexity. This includes both\nsmall-scale local events like local farmer markets and large complex events\nlike political and military conflicts. The latter are typically not observed\ndirectly but through the lenses of intermediaries like newspapers or social\nmedia. In other words, we do not witness the unfolding of such events directly\nbut are confronted with narratives surrounding them. Such narratives capture\ndifferent aspects of a complex event and may also differ with respect to the\nnarrator. Thus, they provide a rich semantics concerning real-world events. In\nthis paper, we show how narratives concerning complex events can be constructed\nand utilized. We provide a formal representation of narratives based on\nrecursive nodes to represent multiple levels of detail and discuss how\nnarratives can be bound to event-centric knowledge graphs. Additionally, we\nprovide an algorithm based on incremental prompting techniques that mines such\nnarratives from texts to account for different perspectives on complex events.\nFinally, we show the effectiveness and future research directions in a proof of\nconcept.", "published": "2024-04-25 08:33:08", "link": "http://arxiv.org/abs/2404.16405v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Asking and Answering Questions to Extract Event-Argument Structures", "abstract": "This paper presents a question-answering approach to extract document-level\nevent-argument structures. We automatically ask and answer questions for each\nargument type an event may have. Questions are generated using manually defined\ntemplates and generative transformers. Template-based questions are generated\nusing predefined role-specific wh-words and event triggers from the context\ndocument. Transformer-based questions are generated using large language models\ntrained to formulate questions based on a passage and the expected answer.\nAdditionally, we develop novel data augmentation strategies specialized in\ninter-sentential event-argument relations. We use a simple span-swapping\ntechnique, coreference resolution, and large language models to augment the\ntraining instances. Our approach enables transfer learning without any\ncorpora-specific modifications and yields competitive results with the RAMS\ndataset. It outperforms previous work, and it is especially beneficial to\nextract arguments that appear in different sentences than the event trigger. We\nalso present detailed quantitative and qualitative analyses shedding light on\nthe most common errors made by our best model.", "published": "2024-04-25 08:43:06", "link": "http://arxiv.org/abs/2404.16413v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Instruction Matters: A Simple yet Effective Task Selection for Optimized\n  Instruction Tuning of Specific Tasks", "abstract": "Instruction tuning has been proven effective in enhancing zero-shot\ngeneralization across various tasks and in improving the performance of\nspecific tasks. For task-specific improvements, strategically selecting and\ntraining on related tasks that provide meaningful supervision is crucial, as\nthis approach enhances efficiency and prevents performance degradation from\nlearning irrelevant tasks. In this light, we introduce a simple yet effective\ntask selection method that leverages instruction information alone to identify\nrelevant tasks, optimizing instruction tuning for specific tasks. Our method is\nsignificantly more efficient than traditional approaches, which require complex\nmeasurements of pairwise transferability between tasks or the creation of data\nsamples for the target task. Additionally, by aligning the model with the\nunique instructional template style of the meta-dataset, we enhance its ability\nto granularly discern relevant tasks, leading to improved overall performance.\nExperimental results demonstrate that training on a small set of tasks, chosen\nsolely based on the instructions, results in substantial improvements in\nperformance on benchmarks such as P3, Big-Bench, NIV2, and Big-Bench Hard.\nSignificantly, these improvements surpass those achieved by prior task\nselection methods, highlighting the superiority of our approach.", "published": "2024-04-25 08:49:47", "link": "http://arxiv.org/abs/2404.16418v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models Perform on Par with Experts Identifying Mental\n  Health Factors in Adolescent Online Forums", "abstract": "Mental health in children and adolescents has been steadily deteriorating\nover the past few years. The recent advent of Large Language Models (LLMs)\noffers much hope for cost and time efficient scaling of monitoring and\nintervention, yet despite specifically prevalent issues such as school bullying\nand eating disorders, previous studies on have not investigated performance in\nthis domain or for open information extraction where the set of answers is not\npredetermined. We create a new dataset of Reddit posts from adolescents aged\n12-19 annotated by expert psychiatrists for the following categories: TRAUMA,\nPRECARITY, CONDITION, SYMPTOMS, SUICIDALITY and TREATMENT and compare expert\nlabels to annotations from two top performing LLMs (GPT3.5 and GPT4). In\naddition, we create two synthetic datasets to assess whether LLMs perform\nbetter when annotating data as they generate it. We find GPT4 to be on par with\nhuman inter-annotator agreement and performance on synthetic data to be\nsubstantially higher, however we find the model still occasionally errs on\nissues of negation and factuality and higher performance on synthetic data is\ndriven by greater complexity of real data rather than inherent advantage.", "published": "2024-04-25 09:42:50", "link": "http://arxiv.org/abs/2404.16461v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building a Japanese Document-Level Relation Extraction Dataset Assisted\n  by Cross-Lingual Transfer", "abstract": "Document-level Relation Extraction (DocRE) is the task of extracting all\nsemantic relationships from a document. While studies have been conducted on\nEnglish DocRE, limited attention has been given to DocRE in non-English\nlanguages. This work delves into effectively utilizing existing English\nresources to promote DocRE studies in non-English languages, with Japanese as\nthe representative case. As an initial attempt, we construct a dataset by\ntransferring an English dataset to Japanese. However, models trained on such a\ndataset suffer from low recalls. We investigate the error cases and attribute\nthe failure to different surface structures and semantics of documents\ntranslated from English and those written by native speakers. We thus switch to\nexplore if the transferred dataset can assist human annotation on Japanese\ndocuments. In our proposal, annotators edit relation predictions from a model\ntrained on the transferred dataset. Quantitative analysis shows that relation\nrecommendations suggested by the model help reduce approximately 50% of the\nhuman edit steps compared with the previous approach. Experiments quantify the\nperformance of existing DocRE models on our collected dataset, portraying the\nchallenges of Japanese and cross-lingual DocRE.", "published": "2024-04-25 10:59:02", "link": "http://arxiv.org/abs/2404.16506v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Large Language Models on Time Series Feature Understanding: A\n  Comprehensive Taxonomy and Benchmark", "abstract": "Large Language Models (LLMs) offer the potential for automatic time series\nanalysis and reporting, which is a critical task across many domains, spanning\nhealthcare, finance, climate, energy, and many more. In this paper, we propose\na framework for rigorously evaluating the capabilities of LLMs on time series\nunderstanding, encompassing both univariate and multivariate forms. We\nintroduce a comprehensive taxonomy of time series features, a critical\nframework that delineates various characteristics inherent in time series data.\nLeveraging this taxonomy, we have systematically designed and synthesized a\ndiverse dataset of time series, embodying the different outlined features, each\naccompanied by textual descriptions. This dataset acts as a solid foundation\nfor assessing the proficiency of LLMs in comprehending time series. Our\nexperiments shed light on the strengths and limitations of state-of-the-art\nLLMs in time series understanding, revealing which features these models\nreadily comprehend effectively and where they falter. In addition, we uncover\nthe sensitivity of LLMs to factors including the formatting of the data, the\nposition of points queried within a series and the overall time series length.", "published": "2024-04-25 12:24:37", "link": "http://arxiv.org/abs/2404.16563v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Internal Numeracy in Language Models: A Case Study on ALBERT", "abstract": "It has been found that Transformer-based language models have the ability to\nperform basic quantitative reasoning. In this paper, we propose a method for\nstudying how these models internally represent numerical data, and use our\nproposal to analyze the ALBERT family of language models. Specifically, we\nextract the learned embeddings these models use to represent tokens that\ncorrespond to numbers and ordinals, and subject these embeddings to Principal\nComponent Analysis (PCA). PCA results reveal that ALBERT models of different\nsizes, trained and initialized separately, consistently learn to use the axes\nof greatest variation to represent the approximate ordering of various\nnumerical concepts. Numerals and their textual counterparts are represented in\nseparate clusters, but increase along the same direction in 2D space. Our\nfindings illustrate that language models, trained purely to model text, can\nintuit basic mathematical concepts, opening avenues for NLP applications that\nintersect with quantitative reasoning.", "published": "2024-04-25 12:36:19", "link": "http://arxiv.org/abs/2404.16574v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incorporating Lexical and Syntactic Knowledge for Unsupervised\n  Cross-Lingual Transfer", "abstract": "Unsupervised cross-lingual transfer involves transferring knowledge between\nlanguages without explicit supervision. Although numerous studies have been\nconducted to improve performance in such tasks by focusing on cross-lingual\nknowledge, particularly lexical and syntactic knowledge, current approaches are\nlimited as they only incorporate syntactic or lexical information. Since each\ntype of information offers unique advantages and no previous attempts have\ncombined both, we attempt to explore the potential of this approach. In this\npaper, we present a novel framework called \"Lexicon-Syntax Enhanced\nMultilingual BERT\" that combines both lexical and syntactic knowledge.\nSpecifically, we use Multilingual BERT (mBERT) as the base model and employ two\ntechniques to enhance its learning capabilities. The code-switching technique\nis used to implicitly teach the model lexical alignment information, while a\nsyntactic-based graph attention network is designed to help the model encode\nsyntactic structure. To integrate both types of knowledge, we input\ncode-switched sequences into both the syntactic module and the mBERT base model\nsimultaneously. Our extensive experimental results demonstrate this framework\ncan consistently outperform all baselines of zero-shot cross-lingual transfer,\nwith the gains of 1.0~3.7 points on text classification, named entity\nrecognition (ner), and semantic parsing tasks. Keywords:cross-lingual transfer,\nlexicon, syntax, code-switching, graph attention network", "published": "2024-04-25 14:10:52", "link": "http://arxiv.org/abs/2404.16627v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society\n  of LLM Agents", "abstract": "As AI systems pervade human life, ensuring that large language models (LLMs)\nmake safe decisions remains a significant challenge. We introduce the\nGovernance of the Commons Simulation (GovSim), a generative simulation platform\ndesigned to study strategic interactions and cooperative decision-making in\nLLMs. In GovSim, a society of AI agents must collectively balance exploiting a\ncommon resource with sustaining it for future use. This environment enables the\nstudy of how ethical considerations, strategic planning, and negotiation skills\nimpact cooperative outcomes. We develop an LLM-based agent architecture and\ntest it with the leading open and closed LLMs. We find that all but the most\npowerful LLM agents fail to achieve a sustainable equilibrium in GovSim, with\nthe highest survival rate below 54%. Ablations reveal that successful\nmulti-agent communication between agents is critical for achieving cooperation\nin these cases. Furthermore, our analyses show that the failure to achieve\nsustainable cooperation in most LLMs stems from their inability to formulate\nand analyze hypotheses about the long-term effects of their actions on the\nequilibrium of the group. Finally, we show that agents that leverage\n\"Universalization\"-based reasoning, a theory of moral thinking, are able to\nachieve significantly better sustainability. Taken together, GovSim enables us\nto study the mechanisms that underlie sustainable self-government with\nspecificity and scale. We open source the full suite of our research results,\nincluding the simulation environment, agent prompts, and a comprehensive web\ninterface.", "published": "2024-04-25 15:59:16", "link": "http://arxiv.org/abs/2404.16698v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dataset of Quotation Attribution in German News Articles", "abstract": "Extracting who says what to whom is a crucial part in analyzing human\ncommunication in today's abundance of data such as online news articles. Yet,\nthe lack of annotated data for this task in German news articles severely\nlimits the quality and usability of possible systems. To remedy this, we\npresent a new, freely available, creative-commons-licensed dataset for\nquotation attribution in German news articles based on WIKINEWS. The dataset\nprovides curated, high-quality annotations across 1000 documents (250,000\ntokens) in a fine-grained annotation schema enabling various downstream uses\nfor the dataset. The annotations not only specify who said what but also how,\nin which context, to whom and define the type of quotation. We specify our\nannotation schema, describe the creation of the dataset and provide a\nquantitative analysis. Further, we describe suitable evaluation metrics, apply\ntwo existing systems for quotation attribution, discuss their results to\nevaluate the utility of our dataset and outline use cases of our dataset in\ndownstream tasks.", "published": "2024-04-25 17:19:13", "link": "http://arxiv.org/abs/2404.16764v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Selective Feature Attention for Representation-based Siamese\n  Text Matching", "abstract": "Representation-based Siamese networks have risen to popularity in lightweight\ntext matching due to their low deployment and inference costs. While word-level\nattention mechanisms have been implemented within Siamese networks to improve\nperformance, we propose Feature Attention (FA), a novel downstream block\ndesigned to enrich the modeling of dependencies among embedding features.\nEmploying \"squeeze-and-excitation\" techniques, the FA block dynamically adjusts\nthe emphasis on individual features, enabling the network to concentrate more\non features that significantly contribute to the final classification. Building\nupon FA, we introduce a dynamic \"selection\" mechanism called Selective Feature\nAttention (SFA), which leverages a stacked BiGRU Inception structure. The SFA\nblock facilitates multi-scale semantic extraction by traversing different\nstacked BiGRU layers, encouraging the network to selectively concentrate on\nsemantic information and embedding features across varying levels of\nabstraction. Both the FA and SFA blocks offer a seamless integration capability\nwith various Siamese networks, showcasing a plug-and-play characteristic.\nExperimental evaluations conducted across diverse text matching baselines and\nbenchmarks underscore the indispensability of modeling feature attention and\nthe superiority of the \"selection\" mechanism.", "published": "2024-04-25 17:26:59", "link": "http://arxiv.org/abs/2404.16776v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Diversity of Commonsense Generation by Large Language Models\n  via In-Context Learning", "abstract": "Generative Commonsense Reasoning (GCR) requires a model to reason about a\nsituation using commonsense knowledge, while generating coherent sentences.\nAlthough the quality of the generated sentences is crucial, the diversity of\nthe generation is equally important because it reflects the model's ability to\nuse a range of commonsense knowledge facts. Large Language Models (LLMs) have\nshown proficiency in enhancing the generation quality across various tasks\nthrough in-context learning (ICL) using given examples without the need for any\nfine-tuning. However, the diversity aspect in LLM outputs has not been\nsystematically studied before. To address this, we propose a simple method that\ndiversifies the LLM generations, while preserving their quality. Experimental\nresults on three benchmark GCR datasets show that our method achieves an ideal\nbalance between the quality and diversity. Moreover, the sentences generated by\nour proposed method can be used as training data to improve diversity in\nexisting commonsense generators.", "published": "2024-04-25 17:52:39", "link": "http://arxiv.org/abs/2404.16807v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IndicGenBench: A Multilingual Benchmark to Evaluate Generation\n  Capabilities of LLMs on Indic Languages", "abstract": "As large language models (LLMs) see increasing adoption across the globe, it\nis imperative for LLMs to be representative of the linguistic diversity of the\nworld. India is a linguistically diverse country of 1.4 Billion people. To\nfacilitate research on multilingual LLM evaluation, we release IndicGenBench -\nthe largest benchmark for evaluating LLMs on user-facing generation tasks\nacross a diverse set 29 of Indic languages covering 13 scripts and 4 language\nfamilies. IndicGenBench is composed of diverse generation tasks like\ncross-lingual summarization, machine translation, and cross-lingual question\nanswering. IndicGenBench extends existing benchmarks to many Indic languages\nthrough human curation providing multi-way parallel evaluation data for many\nunder-represented Indic languages for the first time. We evaluate a wide range\nof proprietary and open-source LLMs including GPT-3.5, GPT-4, PaLM-2, mT5,\nGemma, BLOOM and LLaMA on IndicGenBench in a variety of settings. The largest\nPaLM-2 models performs the best on most tasks, however, there is a significant\nperformance gap in all languages compared to English showing that further\nresearch is needed for the development of more inclusive multilingual language\nmodels. IndicGenBench is released at\nwww.github.com/google-research-datasets/indic-gen-bench", "published": "2024-04-25 17:57:36", "link": "http://arxiv.org/abs/2404.16816v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Examining the robustness of LLM evaluation to the distributional\n  assumptions of benchmarks", "abstract": "Benchmarks have emerged as the central approach for evaluating Large Language\nModels (LLMs). The research community often relies on a model's average\nperformance across the test prompts of a benchmark to evaluate the model's\nperformance. This is consistent with the assumption that the test prompts\nwithin a benchmark represent a random sample from a real-world distribution of\ninterest. We note that this is generally not the case; instead, we hold that\nthe distribution of interest varies according to the specific use case. We find\nthat (1) the correlation in model performance across test prompts is\nnon-random, (2) accounting for correlations across test prompts can change\nmodel rankings on major benchmarks, (3) explanatory factors for these\ncorrelations include semantic similarity and common LLM failure points.", "published": "2024-04-25 18:35:54", "link": "http://arxiv.org/abs/2404.16966v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Translation of Multifaceted Data without Re-Training of Machine\n  Translation Systems", "abstract": "Translating major language resources to build minor language resources\nbecomes a widely-used approach. Particularly in translating complex data points\ncomposed of multiple components, it is common to translate each component\nseparately. However, we argue that this practice often overlooks the\ninterrelation between components within the same data point. To address this\nlimitation, we propose a novel MT pipeline that considers the intra-data\nrelation in implementing MT for training data. In our MT pipeline, all the\ncomponents in a data point are concatenated to form a single translation\nsequence and subsequently reconstructed to the data components after\ntranslation. We introduce a Catalyst Statement (CS) to enhance the intra-data\nrelation, and Indicator Token (IT) to assist the decomposition of a translated\nsequence into its respective data components. Through our approach, we have\nachieved a considerable improvement in translation quality itself, along with\nits effectiveness as training data. Compared with the conventional approach\nthat translates each data component separately, our method yields better\ntraining data that enhances the performance of the trained model by 2.690\npoints for the web page ranking (WPR) task, and 0.845 for the question\ngeneration (QG) task in the XGLUE benchmark.", "published": "2024-04-25 00:05:19", "link": "http://arxiv.org/abs/2404.16257v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM-Based Section Identifiers Excel on Open Source but Stumble in Real\n  World Applications", "abstract": "Electronic health records (EHR) even though a boon for healthcare\npractitioners, are growing convoluted and longer every day. Sifting around\nthese lengthy EHRs is taxing and becomes a cumbersome part of physician-patient\ninteraction. Several approaches have been proposed to help alleviate this\nprevalent issue either via summarization or sectioning, however, only a few\napproaches have truly been helpful in the past. With the rise of automated\nmethods, machine learning (ML) has shown promise in solving the task of\nidentifying relevant sections in EHR. However, most ML methods rely on labeled\ndata which is difficult to get in healthcare. Large language models (LLMs) on\nthe other hand, have performed impressive feats in natural language processing\n(NLP), that too in a zero-shot manner, i.e. without any labeled data. To that\nend, we propose using LLMs to identify relevant section headers. We find that\nGPT-4 can effectively solve the task on both zero and few-shot settings as well\nas segment dramatically better than state-of-the-art methods. Additionally, we\nalso annotate a much harder real world dataset and find that GPT-4 struggles to\nperform well, alluding to further research and harder benchmarks.", "published": "2024-04-25 02:25:35", "link": "http://arxiv.org/abs/2404.16294v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WorldValuesBench: A Large-Scale Benchmark Dataset for Multi-Cultural\n  Value Awareness of Language Models", "abstract": "The awareness of multi-cultural human values is critical to the ability of\nlanguage models (LMs) to generate safe and personalized responses. However,\nthis awareness of LMs has been insufficiently studied, since the computer\nscience community lacks access to the large-scale real-world data about\nmulti-cultural values. In this paper, we present WorldValuesBench, a globally\ndiverse, large-scale benchmark dataset for the multi-cultural value prediction\ntask, which requires a model to generate a rating response to a value question\nbased on demographic contexts. Our dataset is derived from an influential\nsocial science project, World Values Survey (WVS), that has collected answers\nto hundreds of value questions (e.g., social, economic, ethical) from 94,728\nparticipants worldwide. We have constructed more than 20 million examples of\nthe type \"(demographic attributes, value question) $\\rightarrow$ answer\" from\nthe WVS responses. We perform a case study using our dataset and show that the\ntask is challenging for strong open and closed-source models. On merely\n$11.1\\%$, $25.0\\%$, $72.2\\%$, and $75.0\\%$ of the questions, Alpaca-7B,\nVicuna-7B-v1.5, Mixtral-8x7B-Instruct-v0.1, and GPT-3.5 Turbo can respectively\nachieve $<0.2$ Wasserstein 1-distance from the human normalized answer\ndistributions. WorldValuesBench opens up new research avenues in studying\nlimitations and opportunities in multi-cultural value awareness of LMs.", "published": "2024-04-25 03:23:28", "link": "http://arxiv.org/abs/2404.16308v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "VISLA Benchmark: Evaluating Embedding Sensitivity to Semantic and\n  Lexical Alterations", "abstract": "Despite their remarkable successes, state-of-the-art language models face\nchallenges in grasping certain important semantic details. This paper\nintroduces the VISLA (Variance and Invariance to Semantic and Lexical\nAlterations) benchmark, designed to evaluate the semantic and lexical\nunderstanding of language models. VISLA presents a 3-way semantic\n(in)equivalence task with a triplet of sentences associated with an image, to\nevaluate both vision-language models (VLMs) and unimodal language models\n(ULMs). An evaluation involving 34 VLMs and 20 ULMs reveals surprising\ndifficulties in distinguishing between lexical and semantic variations. Spatial\nsemantics encoded by language models also appear to be highly sensitive to\nlexical information. Notably, text encoders of VLMs demonstrate greater\nsensitivity to semantic and lexical variations than unimodal text encoders. Our\ncontributions include the unification of image-to-text and text-to-text\nretrieval tasks, an off-the-shelf evaluation without fine-tuning, and assessing\nLMs' semantic (in)variance in the presence of lexical alterations. The results\nhighlight strengths and weaknesses across diverse vision and unimodal language\nmodels, contributing to a deeper understanding of their capabilities. % VISLA\nenables a rigorous evaluation, shedding light on language models' capabilities\nin handling semantic and lexical nuances. Data and code will be made available\nat https://github.com/Sri-Harsha/visla_benchmark.", "published": "2024-04-25 07:08:00", "link": "http://arxiv.org/abs/2404.16365v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Syntax Without Planting Trees: Understanding Hierarchical\n  Generalization in Transformers", "abstract": "Transformers trained on natural language data have been shown to learn its\nhierarchical structure and generalize to sentences with unseen syntactic\nstructures without explicitly encoding any structural bias. In this work, we\ninvestigate sources of inductive bias in transformer models and their training\nthat could cause such generalization behavior to emerge. We extensively\nexperiment with transformer models trained on multiple synthetic datasets and\nwith different training objectives and show that while other objectives e.g.\nsequence-to-sequence modeling, prefix language modeling, often failed to lead\nto hierarchical generalization, models trained with the language modeling\nobjective consistently learned to generalize hierarchically. We then conduct\npruning experiments to study how transformers trained with the language\nmodeling objective encode hierarchical structure. When pruned, we find joint\nexistence of subnetworks within the model with different generalization\nbehaviors (subnetworks corresponding to hierarchical structure and linear\norder). Finally, we take a Bayesian perspective to further uncover\ntransformers' preference for hierarchical generalization: We establish a\ncorrelation between whether transformers generalize hierarchically on a dataset\nand whether the simplest explanation of that dataset is provided by a\nhierarchical grammar compared to regular grammars exhibiting linear\ngeneralization.", "published": "2024-04-25 07:10:29", "link": "http://arxiv.org/abs/2404.16367v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "U2++ MoE: Scaling 4.7x parameters with minimal impact on RTF", "abstract": "Scale has opened new frontiers in natural language processing, but at a high\ncost. In response, by learning to only activate a subset of parameters in\ntraining and inference, Mixture-of-Experts (MoE) have been proposed as an\nenergy efficient path to even larger and more capable language models and this\nshift towards a new generation of foundation models is gaining momentum,\nparticularly within the field of Automatic Speech Recognition (ASR). Recent\nworks that incorporating MoE into ASR models have complex designs such as\nrouting frames via supplementary embedding network, improving multilingual\nability for the experts, and utilizing dedicated auxiliary losses for either\nexpert load balancing or specific language handling. We found that delicate\ndesigns are not necessary, while an embarrassingly simple substitution of MoE\nlayers for all Feed-Forward Network (FFN) layers is competent for the ASR task.\nTo be more specific, we benchmark our proposed model on a large scale\ninner-source dataset (160k hours), the results show that we can scale our\nbaseline Conformer (Dense-225M) to its MoE counterparts (MoE-1B) and achieve\nDense-1B level Word Error Rate (WER) while maintaining a Dense-225M level Real\nTime Factor (RTF). Furthermore, by applying Unified 2-pass framework with\nbidirectional attention decoders (U2++), we achieve the streaming and\nnon-streaming decoding modes in a single MoE based model, which we call U2++\nMoE. We hope that our study can facilitate the research on scaling speech\nfoundation models without sacrificing deployment efficiency.", "published": "2024-04-25 08:34:21", "link": "http://arxiv.org/abs/2404.16407v2", "categories": ["cs.CL", "eess.AS", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Contextual Categorization Enhancement through LLMs Latent-Space", "abstract": "Managing the semantic quality of the categorization in large textual\ndatasets, such as Wikipedia, presents significant challenges in terms of\ncomplexity and cost. In this paper, we propose leveraging transformer models to\ndistill semantic information from texts in the Wikipedia dataset and its\nassociated categories into a latent space. We then explore different approaches\nbased on these encodings to assess and enhance the semantic identity of the\ncategories. Our graphical approach is powered by Convex Hull, while we utilize\nHierarchical Navigable Small Worlds (HNSWs) for the hierarchical approach. As a\nsolution to the information loss caused by the dimensionality reduction, we\nmodulate the following mathematical solution: an exponential decay function\ndriven by the Euclidean distances between the high-dimensional encodings of the\ntextual categories. This function represents a filter built around a contextual\ncategory and retrieves items with a certain Reconsideration Probability (RP).\nRetrieving high-RP items serves as a tool for database administrators to\nimprove data groupings by providing recommendations and identifying outliers\nwithin a contextual framework.", "published": "2024-04-25 09:20:51", "link": "http://arxiv.org/abs/2404.16442v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating Consistency and Reasoning Capabilities of Large Language\n  Models", "abstract": "Large Language Models (LLMs) are extensively used today across various\nsectors, including academia, research, business, and finance, for tasks such as\ntext generation, summarization, and translation. Despite their widespread\nadoption, these models often produce incorrect and misleading information,\nexhibiting a tendency to hallucinate. This behavior can be attributed to\nseveral factors, with consistency and reasoning capabilities being significant\ncontributors. LLMs frequently lack the ability to generate explanations and\nengage in coherent reasoning, leading to inaccurate responses. Moreover, they\nexhibit inconsistencies in their outputs. This paper aims to evaluate and\ncompare the consistency and reasoning capabilities of both public and\nproprietary LLMs. The experiments utilize the Boolq dataset as the ground\ntruth, comprising questions, answers, and corresponding explanations. Queries\nfrom the dataset are presented as prompts to the LLMs, and the generated\nresponses are evaluated against the ground truth answers. Additionally,\nexplanations are generated to assess the models' reasoning abilities.\nConsistency is evaluated by repeatedly presenting the same query to the models\nand observing for variations in their responses. For measuring reasoning\ncapabilities, the generated explanations are compared to the ground truth\nexplanations using metrics such as BERT, BLEU, and F-1 scores. The findings\nreveal that proprietary models generally outperform public models in terms of\nboth consistency and reasoning capabilities. However, even when presented with\nbasic general knowledge questions, none of the models achieved a score of 90\\%\nin both consistency and reasoning. This study underscores the direct\ncorrelation between consistency and reasoning abilities in LLMs and highlights\nthe inherent reasoning challenges present in current language models.", "published": "2024-04-25 10:03:14", "link": "http://arxiv.org/abs/2404.16478v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Understanding Privacy Risks of Embeddings Induced by Large Language\n  Models", "abstract": "Large language models (LLMs) show early signs of artificial general\nintelligence but struggle with hallucinations. One promising solution to\nmitigate these hallucinations is to store external knowledge as embeddings,\naiding LLMs in retrieval-augmented generation. However, such a solution risks\ncompromising privacy, as recent studies experimentally showed that the original\ntext can be partially reconstructed from text embeddings by pre-trained\nlanguage models. The significant advantage of LLMs over traditional pre-trained\nmodels may exacerbate these concerns. To this end, we investigate the\neffectiveness of reconstructing original knowledge and predicting entity\nattributes from these embeddings when LLMs are employed. Empirical findings\nindicate that LLMs significantly improve the accuracy of two evaluated tasks\nover those from pre-trained models, regardless of whether the texts are\nin-distribution or out-of-distribution. This underscores a heightened potential\nfor LLMs to jeopardize user privacy, highlighting the negative consequences of\ntheir widespread use. We further discuss preliminary strategies to mitigate\nthis risk.", "published": "2024-04-25 13:10:48", "link": "http://arxiv.org/abs/2404.16587v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Tele-FLM Technical Report", "abstract": "Large language models (LLMs) have showcased profound capabilities in language\nunderstanding and generation, facilitating a wide array of applications.\nHowever, there is a notable paucity of detailed, open-sourced methodologies on\nefficiently scaling LLMs beyond 50 billion parameters with minimum\ntrial-and-error cost and computational resources. In this report, we introduce\nTele-FLM (aka FLM-2), a 52B open-sourced multilingual large language model that\nfeatures a stable, efficient pre-training paradigm and enhanced factual\njudgment capabilities. Tele-FLM demonstrates superior multilingual language\nmodeling abilities, measured by BPB on textual corpus. Besides, in both English\nand Chinese foundation model evaluation, it is comparable to strong\nopen-sourced models that involve larger pre-training FLOPs, such as Llama2-70B\nand DeepSeek-67B. In addition to the model weights, we share the core designs,\nengineering practices, and training details, which we expect to benefit both\nthe academic and industrial communities.", "published": "2024-04-25 14:34:47", "link": "http://arxiv.org/abs/2404.16645v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An\u00e1lise de ambiguidade lingu\u00edstica em modelos de linguagem de grande\n  escala (LLMs)", "abstract": "Linguistic ambiguity continues to represent a significant challenge for\nnatural language processing (NLP) systems, notwithstanding the advancements in\narchitectures such as Transformers and BERT. Inspired by the recent success of\ninstructional models like ChatGPT and Gemini (In 2023, the artificial\nintelligence was called Bard.), this study aims to analyze and discuss\nlinguistic ambiguity within these models, focusing on three types prevalent in\nBrazilian Portuguese: semantic, syntactic, and lexical ambiguity. We create a\ncorpus comprising 120 sentences, both ambiguous and unambiguous, for\nclassification, explanation, and disambiguation. The models capability to\ngenerate ambiguous sentences was also explored by soliciting sets of sentences\nfor each type of ambiguity. The results underwent qualitative analysis, drawing\non recognized linguistic references, and quantitative assessment based on the\naccuracy of the responses obtained. It was evidenced that even the most\nsophisticated models, such as ChatGPT and Gemini, exhibit errors and\ndeficiencies in their responses, with explanations often providing\ninconsistent. Furthermore, the accuracy peaked at 49.58 percent, indicating the\nneed for descriptive studies for supervised learning.", "published": "2024-04-25 14:45:07", "link": "http://arxiv.org/abs/2404.16653v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ProbGate at EHRSQL 2024: Enhancing SQL Query Generation Accuracy through\n  Probabilistic Threshold Filtering and Error Handling", "abstract": "Recently, deep learning-based language models have significantly enhanced\ntext-to-SQL tasks, with promising applications in retrieving patient records\nwithin the medical domain. One notable challenge in such applications is\ndiscerning unanswerable queries. Through fine-tuning model, we demonstrate the\nfeasibility of converting medical record inquiries into SQL queries.\nAdditionally, we introduce an entropy-based method to identify and filter out\nunanswerable results. We further enhance result quality by filtering\nlow-confidence SQL through log probability-based distribution, while\ngrammatical and schema errors are mitigated by executing queries on the actual\ndatabase. We experimentally verified that our method can filter unanswerable\nquestions, which can be widely utilized even when the parameters of the model\nare not accessible, and that it can be effectively utilized in practice.", "published": "2024-04-25 14:55:07", "link": "http://arxiv.org/abs/2404.16659v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Influence of Solution Efficiency and Valence of Instruction on Additive\n  and Subtractive Solution Strategies in Humans and GPT-4", "abstract": "Generative artificial intelligences, particularly large language models\n(LLMs), play an increasingly prominent role in human decision-making contexts,\nnecessitating transparency about their capabilities. While prior studies have\nshown addition biases in humans (Adams et al., 2021) and OpenAI's GPT-3 (Winter\net al., 2023), this study extends the research by comparing human and GPT-4\nproblem-solving across both spatial and linguistic tasks, with variations in\nsolution efficiency and valence of task instruction. Four preregistered\nexperiments with 588 participants from the U.S. and 680 GPT-4 iterations\nrevealed a stronger tendency towards additive transformations in GPT-4 than in\nhumans. Human participants were less likely to use additive strategies when\nsubtraction was relatively more efficient than when addition and subtraction\nwere equally efficient. GPT-4 exhibited the opposite behavior, with a strong\naddition bias when subtraction was more efficient. In terms of valence of task\ninstruction, GPT-4's use of additive strategies increased when instructed to\n\"improve\" (positive) rather than \"edit\" (neutral). These findings demonstrate\nthat biases in human problem-solving are amplified in GPT-4, and that LLM\nbehavior differs from human efficiency-based strategies. This highlights the\nlimitations of LLMs and the need for caution when using them in real-world\napplications.", "published": "2024-04-25 15:53:00", "link": "http://arxiv.org/abs/2404.16692v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Prefix Text as a Yarn: Eliciting Non-English Alignment in Foundation\n  Language Model", "abstract": "While supervised fine-tuning (SFT) has been a straightforward approach for\ntailoring the output of foundation large language model (LLM) to specific\npreferences, concerns have been raised about the depth of this alignment, with\nsome critiques suggesting it is merely \"superficial\". We critically examine\nthis hypothesis within the scope of cross-lingual generation tasks, proposing\nthat the effectiveness of SFT may be constrained by its reliance on prior\ntokens to guide cross-lingual generation. Based on this crucial insight, and in\nresponse to the challenges posed by the costly and limited availability of\nnon-English data for SFT, we introduce a novel training-free alignment method\nnamed PreTTY, which employs minimal task-related prior tokens to bridge the\nfoundation LLM and the SFT LLM, achieving comparable performance without\ntraining. Experiments on machine translation and part-of-speech tagging across\neight languages demonstrate the efficacy of PreTTY in cross-lingual settings.\nRemarkably, by initiating the decoding process with only one or two prior\ntokens, foundation LLMs can achieve performance comparable to their SFT\ncounterparts. This method presents a cost-effective alternative to SFT and\nadvances the democratization of multilingual LLMs.", "published": "2024-04-25 17:19:36", "link": "http://arxiv.org/abs/2404.16766v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Make Your LLM Fully Utilize the Context", "abstract": "While many contemporary large language models (LLMs) can process lengthy\ninput, they still struggle to fully utilize information within the long\ncontext, known as the lost-in-the-middle challenge. We hypothesize that it\nstems from insufficient explicit supervision during the long-context training,\nwhich fails to emphasize that any position in a long context can hold crucial\ninformation. Based on this intuition, our study presents information-intensive\n(IN2) training, a purely data-driven solution to overcome lost-in-the-middle.\nSpecifically, IN2 training leverages a synthesized long-context question-answer\ndataset, where the answer requires (1) fine-grained information awareness on a\nshort segment (~128 tokens) within a synthesized long context (4K-32K tokens),\nand (2) the integration and reasoning of information from two or more short\nsegments. Through applying this information-intensive training on Mistral-7B,\nwe present FILM-7B (FILl-in-the-Middle). To thoroughly assess the ability of\nFILM-7B for utilizing long contexts, we design three probing tasks that\nencompass various context styles (document, code, and structured-data context)\nand information retrieval patterns (forward, backward, and bi-directional\nretrieval). The probing results demonstrate that FILM-7B can robustly retrieve\ninformation from different positions in its 32K context window. Beyond these\nprobing tasks, FILM-7B significantly improves the performance on real-world\nlong-context tasks (e.g., 23.5->26.9 F1 score on NarrativeQA), while\nmaintaining a comparable performance on short-context tasks (e.g., 59.3->59.2\naccuracy on MMLU). Github Link: https://github.com/microsoft/FILM.", "published": "2024-04-25 17:55:14", "link": "http://arxiv.org/abs/2404.16811v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Short Survey of Human Mobility Prediction in Epidemic Modeling from\n  Transformers to LLMs", "abstract": "This paper provides a comprehensive survey of recent advancements in\nleveraging machine learning techniques, particularly Transformer models, for\npredicting human mobility patterns during epidemics. Understanding how people\nmove during epidemics is essential for modeling the spread of diseases and\ndevising effective response strategies. Forecasting population movement is\ncrucial for informing epidemiological models and facilitating effective\nresponse planning in public health emergencies. Predicting mobility patterns\ncan enable authorities to better anticipate the geographical and temporal\nspread of diseases, allocate resources more efficiently, and implement targeted\ninterventions. We review a range of approaches utilizing both pretrained\nlanguage models like BERT and Large Language Models (LLMs) tailored\nspecifically for mobility prediction tasks. These models have demonstrated\nsignificant potential in capturing complex spatio-temporal dependencies and\ncontextual patterns in textual data.", "published": "2024-04-25 17:52:19", "link": "http://arxiv.org/abs/2404.16921v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Survey of Generative Search and Recommendation in the Era of Large\n  Language Models", "abstract": "With the information explosion on the Web, search and recommendation are\nfoundational infrastructures to satisfying users' information needs. As the two\nsides of the same coin, both revolve around the same core research problem,\nmatching queries with documents or users with items. In the recent few decades,\nsearch and recommendation have experienced synchronous technological paradigm\nshifts, including machine learning-based and deep learning-based paradigms.\nRecently, the superintelligent generative large language models have sparked a\nnew paradigm in search and recommendation, i.e., generative search (retrieval)\nand recommendation, which aims to address the matching problem in a generative\nmanner. In this paper, we provide a comprehensive survey of the emerging\nparadigm in information systems and summarize the developments in generative\nsearch and recommendation from a unified perspective. Rather than simply\ncategorizing existing works, we abstract a unified framework for the generative\nparadigm and break down the existing works into different stages within this\nframework to highlight the strengths and weaknesses. And then, we distinguish\ngenerative search and recommendation with their unique challenges, identify\nopen problems and future directions, and envision the next information-seeking\nparadigm.", "published": "2024-04-25 17:58:17", "link": "http://arxiv.org/abs/2404.16924v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A Closer Look at Classification Evaluation Metrics and a Critical\n  Reflection of Common Evaluation Practice", "abstract": "Classification systems are evaluated in a countless number of papers.\nHowever, we find that evaluation practice is often nebulous. Frequently,\nmetrics are selected without arguments, and blurry terminology invites\nmisconceptions. For instance, many works use so-called 'macro' metrics to rank\nsystems (e.g., 'macro F1') but do not clearly specify what they would expect\nfrom such a `macro' metric. This is problematic, since picking a metric can\naffect research findings, and thus any clarity in the process should be\nmaximized.\n  Starting from the intuitive concepts of bias and prevalence, we perform an\nanalysis of common evaluation metrics. The analysis helps us understand the\nmetrics' underlying properties, and how they align with expectations as found\nexpressed in papers. Then we reflect on the practical situation in the field,\nand survey evaluation practice in recent shared tasks. We find that metric\nselection is often not supported with convincing arguments, an issue that can\nmake a system ranking seem arbitrary. Our work aims at providing overview and\nguidance for more informed and transparent metric selection, fostering\nmeaningful evaluation.", "published": "2024-04-25 18:12:43", "link": "http://arxiv.org/abs/2404.16958v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Evaluating Class Membership Relations in Knowledge Graphs using Large\n  Language Models", "abstract": "A backbone of knowledge graphs are their class membership relations, which\nassign entities to a given class. As part of the knowledge engineering process,\nwe propose a new method for evaluating the quality of these relations by\nprocessing descriptions of a given entity and class using a zero-shot\nchain-of-thought classifier that uses a natural language intensional definition\nof a class. We evaluate the method using two publicly available knowledge\ngraphs, Wikidata and CaLiGraph, and 7 large language models. Using the\ngpt-4-0125-preview large language model, the method's classification\nperformance achieves a macro-averaged F1-score of 0.830 on data from Wikidata\nand 0.893 on data from CaLiGraph. Moreover, a manual analysis of the\nclassification errors shows that 40.9% of errors were due to the knowledge\ngraphs, with 16.0% due to missing relations and 24.9% due to incorrectly\nasserted relations. These results show how large language models can assist\nknowledge engineers in the process of knowledge graph refinement. The code and\ndata are available on Github.", "published": "2024-04-25 19:44:46", "link": "http://arxiv.org/abs/2404.17000v1", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.4"], "primary_category": "cs.CL"}
{"title": "T\u00fcrk\u00e7e Dil Modellerinin Performans\n  Kar\u015f\u0131la\u015ft\u0131rmas\u0131 Performance Comparison of Turkish Language\n  Models", "abstract": "The developments that language models have provided in fulfilling almost all\nkinds of tasks have attracted the attention of not only researchers but also\nthe society and have enabled them to become products. There are commercially\nsuccessful language models available. However, users may prefer open-source\nlanguage models due to cost, data privacy, or regulations. Yet, despite the\nincreasing number of these models, there is no comprehensive comparison of\ntheir performance for Turkish. This study aims to fill this gap in the\nliterature. A comparison is made among seven selected language models based on\ntheir contextual learning and question-answering abilities. Turkish datasets\nfor contextual learning and question-answering were prepared, and both\nautomatic and human evaluations were conducted. The results show that for\nquestion-answering, continuing pretraining before fine-tuning with\ninstructional datasets is more successful in adapting multilingual models to\nTurkish and that in-context learning performances do not much related to\nquestion-answering performances.", "published": "2024-04-25 20:10:14", "link": "http://arxiv.org/abs/2404.17010v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Player-Driven Emergence in LLM-Driven Game Narrative", "abstract": "We explore how interaction with large language models (LLMs) can give rise to\nemergent behaviors, empowering players to participate in the evolution of game\nnarratives. Our testbed is a text-adventure game in which players attempt to\nsolve a mystery under a fixed narrative premise, but can freely interact with\nnon-player characters generated by GPT-4, a large language model. We recruit 28\ngamers to play the game and use GPT-4 to automatically convert the game logs\ninto a node-graph representing the narrative in the player's gameplay. We find\nthat through their interactions with the non-deterministic behavior of the LLM,\nplayers are able to discover interesting new emergent nodes that were not a\npart of the original narrative but have potential for being fun and engaging.\nPlayers that created the most emergent nodes tended to be those that often\nenjoy games that facilitate discovery, exploration and experimentation.", "published": "2024-04-25 20:39:44", "link": "http://arxiv.org/abs/2404.17027v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models in the Clinic: A Comprehensive Benchmark", "abstract": "The adoption of large language models (LLMs) to assist clinicians has\nattracted remarkable attention. Existing works mainly adopt the close-ended\nquestion-answering (QA) task with answer options for evaluation. However, many\nclinical decisions involve answering open-ended questions without pre-set\noptions. To better understand LLMs in the clinic, we construct a benchmark\nClinicBench. We first collect eleven existing datasets covering diverse\nclinical language generation, understanding, and reasoning tasks. Furthermore,\nwe construct six novel datasets and clinical tasks that are complex but common\nin real-world practice, e.g., open-ended decision-making, long document\nprocessing, and emerging drug analysis. We conduct an extensive evaluation of\ntwenty-two LLMs under both zero-shot and few-shot settings. Finally, we invite\nmedical experts to evaluate the clinical usefulness of LLMs. The benchmark data\nis available at https://github.com/AI-in-Health/ClinicBench.", "published": "2024-04-25 15:51:06", "link": "http://arxiv.org/abs/2405.00716v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring News Summarization and Enrichment in a Highly Resource-Scarce\n  Indian Language: A Case Study of Mizo", "abstract": "Obtaining sufficient information in one's mother tongue is crucial for\nsatisfying the information needs of the users. While high-resource languages\nhave abundant online resources, the situation is less than ideal for very\nlow-resource languages. Moreover, the insufficient reporting of vital national\nand international events continues to be a worry, especially in languages with\nscarce resources, like \\textbf{Mizo}. In this paper, we conduct a study to\ninvestigate the effectiveness of a simple methodology designed to generate a\nholistic summary for Mizo news articles, which leverages English-language news\nto supplement and enhance the information related to the corresponding news\nevents. Furthermore, we make available 500 Mizo news articles and corresponding\nenriched holistic summaries. Human evaluation confirms that our approach\nsignificantly enhances the information coverage of Mizo news articles. The mizo\ndataset and code can be accessed at\n\\url{https://github.com/barvin04/mizo_enrichment", "published": "2024-04-25 17:23:04", "link": "http://arxiv.org/abs/2405.00717v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Can't say cant? Measuring and Reasoning of Dark Jargons in Large\n  Language Models", "abstract": "Ensuring the resilience of Large Language Models (LLMs) against malicious\nexploitation is paramount, with recent focus on mitigating offensive responses.\nYet, the understanding of cant or dark jargon remains unexplored. This paper\nintroduces a domain-specific Cant dataset and CantCounter evaluation framework,\nemploying Fine-Tuning, Co-Tuning, Data-Diffusion, and Data-Analysis stages.\nExperiments reveal LLMs, including ChatGPT, are susceptible to cant bypassing\nfilters, with varying recognition accuracy influenced by question types,\nsetups, and prompt clues. Updated models exhibit higher acceptance rates for\ncant queries. Moreover, LLM reactions differ across domains, e.g., reluctance\nto engage in racism versus LGBT topics. These findings underscore LLMs'\nunderstanding of cant and reflect training data characteristics and vendor\napproaches to sensitive topics. Additionally, we assess LLMs' ability to\ndemonstrate reasoning capabilities. Access to our datasets and code is\navailable at https://github.com/cistineup/CantCounter.", "published": "2024-04-25 17:25:53", "link": "http://arxiv.org/abs/2405.00718v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "List Items One by One: A New Data Source and Learning Paradigm for\n  Multimodal LLMs", "abstract": "Set-of-Mark (SoM) Prompting unleashes the visual grounding capability of\nGPT-4V, by enabling the model to associate visual objects with tags inserted on\nthe image. These tags, marked with alphanumerics, can be indexed via text\ntokens for easy reference. Despite the extraordinary performance from GPT-4V,\nwe observe that other Multimodal Large Language Models (MLLMs) struggle to\nunderstand these visual tags. To promote the learning of SoM prompting for\nopen-source models, we propose a new learning paradigm: \"list items one by\none,\" which asks the model to enumerate and describe all visual tags placed on\nthe image following the alphanumeric orders of tags. By integrating our curated\ndataset with other visual instruction tuning datasets, we are able to equip\nexisting MLLMs with the SoM prompting ability. Furthermore, we evaluate our\nfinetuned SoM models on five MLLM benchmarks. We find that this new dataset,\neven in a relatively small size (10k-30k images with tags), significantly\nenhances visual reasoning capabilities and reduces hallucinations for MLLMs.\nPerhaps surprisingly, these improvements persist even when the visual tags are\nomitted from input images during inference. This suggests the potential of\n\"list items one by one\" as a new paradigm for training MLLMs, which strengthens\nthe object-text alignment through the use of visual tags in the training stage.\nFinally, we conduct analyses by probing trained models to understand the\nworking mechanism of SoM. Our code and data are available at\n\\url{https://github.com/zzxslp/SoM-LLaVA}.", "published": "2024-04-25 07:29:17", "link": "http://arxiv.org/abs/2404.16375v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Hippocrates: An Open-Source Framework for Advancing Large Language\n  Models in Healthcare", "abstract": "The integration of Large Language Models (LLMs) into healthcare promises to\ntransform medical diagnostics, research, and patient care. Yet, the progression\nof medical LLMs faces obstacles such as complex training requirements, rigorous\nevaluation demands, and the dominance of proprietary models that restrict\nacademic exploration. Transparent, comprehensive access to LLM resources is\nessential for advancing the field, fostering reproducibility, and encouraging\ninnovation in healthcare AI. We present Hippocrates, an open-source LLM\nframework specifically developed for the medical domain. In stark contrast to\nprevious efforts, it offers unrestricted access to its training datasets,\ncodebase, checkpoints, and evaluation protocols. This open approach is designed\nto stimulate collaborative research, allowing the community to build upon,\nrefine, and rigorously evaluate medical LLMs within a transparent ecosystem.\nAlso, we introduce Hippo, a family of 7B models tailored for the medical\ndomain, fine-tuned from Mistral and LLaMA2 through continual pre-training,\ninstruction tuning, and reinforcement learning from human and AI feedback. Our\nmodels outperform existing open medical LLMs models by a large-margin, even\nsurpassing models with 70B parameters. Through Hippocrates, we aspire to unlock\nthe full potential of LLMs not just to advance medical knowledge and patient\ncare but also to democratize the benefits of AI research in healthcare, making\nthem available across the globe.", "published": "2024-04-25 14:06:37", "link": "http://arxiv.org/abs/2404.16621v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding", "abstract": "We present LayerSkip, an end-to-end solution to speed-up inference of large\nlanguage models (LLMs). First, during training we apply layer dropout, with low\ndropout rates for earlier layers and higher dropout rates for later layers, and\nan early exit loss where all transformer layers share the same exit. Second,\nduring inference, we show that this training recipe increases the accuracy of\nearly exit at earlier layers, without adding any auxiliary layers or modules to\nthe model. Third, we present a novel self-speculative decoding solution where\nwe exit at early layers and verify and correct with remaining layers of the\nmodel. Our proposed self-speculative decoding approach has less memory\nfootprint than other speculative decoding approaches and benefits from shared\ncompute and activations of the draft and verification stages. We run\nexperiments on different Llama model sizes on different types of training:\npretraining from scratch, continual pretraining, finetuning on specific data\ndomain, and finetuning on specific task. We implement our inference solution\nand show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x\non coding, and 2.0x on TOPv2 semantic parsing task. We open source our code and\ncheckpoints at https://github.com/facebookresearch/LayerSkip.", "published": "2024-04-25 16:20:23", "link": "http://arxiv.org/abs/2404.16710v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automatic Speech Recognition System-Independent Word Error Rate\n  Estimation", "abstract": "Word error rate (WER) is a metric used to evaluate the quality of\ntranscriptions produced by Automatic Speech Recognition (ASR) systems. In many\napplications, it is of interest to estimate WER given a pair of a speech\nutterance and a transcript. Previous work on WER estimation focused on building\nmodels that are trained with a specific ASR system in mind (referred to as ASR\nsystem-dependent). These are also domain-dependent and inflexible in real-world\napplications. In this paper, a hypothesis generation method for ASR\nSystem-Independent WER estimation (SIWE) is proposed. In contrast to prior\nwork, the WER estimators are trained using data that simulates ASR system\noutput. Hypotheses are generated using phonetically similar or linguistically\nmore likely alternative words. In WER estimation experiments, the proposed\nmethod reaches a similar performance to ASR system-dependent WER estimators on\nin-domain data and achieves state-of-the-art performance on out-of-domain data.\nOn the out-of-domain data, the SIWE model outperformed the baseline estimators\nin root mean square error and Pearson correlation coefficient by relative\n17.58% and 18.21%, respectively, on Switchboard and CALLHOME. The performance\nwas further improved when the WER of the training set was close to the WER of\nthe evaluation dataset.", "published": "2024-04-25 16:57:05", "link": "http://arxiv.org/abs/2404.16743v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "REBEL: Reinforcement Learning via Regressing Relative Rewards", "abstract": "While originally developed for continuous control problems, Proximal Policy\nOptimization (PPO) has emerged as the work-horse of a variety of reinforcement\nlearning (RL) applications, including the fine-tuning of generative models.\nUnfortunately, PPO requires multiple heuristics to enable stable convergence\n(e.g. value networks, clipping), and is notorious for its sensitivity to the\nprecise implementation of these components. In response, we take a step back\nand ask what a minimalist RL algorithm for the era of generative models would\nlook like. We propose REBEL, an algorithm that cleanly reduces the problem of\npolicy optimization to regressing the relative reward between two completions\nto a prompt in terms of the policy, enabling strikingly lightweight\nimplementation. In theory, we prove that fundamental RL algorithms like Natural\nPolicy Gradient can be seen as variants of REBEL, which allows us to match the\nstrongest known theoretical guarantees in terms of convergence and sample\ncomplexity in the RL literature. REBEL can also cleanly incorporate offline\ndata and be extended to handle the intransitive preferences we frequently see\nin practice. Empirically, we find that REBEL provides a unified approach to\nlanguage modeling and image generation with stronger or similar performance as\nPPO and DPO, all while being simpler to implement and more computationally\nefficient than PPO. When fine-tuning Llama-3-8B-Instruct, REBEL achieves strong\nperformance in AlpacaEval 2.0, MT-Bench, and Open LLM Leaderboard.", "published": "2024-04-25 17:20:45", "link": "http://arxiv.org/abs/2404.16767v4", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Continual Learning of Large Language Models: A Comprehensive Survey", "abstract": "The recent success of large language models (LLMs) trained on static,\npre-collected, general datasets has sparked numerous research directions and\napplications. One such direction addresses the non-trivial challenge of\nintegrating pre-trained LLMs into dynamic data distributions, task structures,\nand user preferences. Pre-trained LLMs, when tailored for specific needs, often\nexperience significant performance degradation in previous knowledge domains --\na phenomenon known as \"catastrophic forgetting\". While extensively studied in\nthe continual learning (CL) community, it presents new manifestations in the\nrealm of LLMs. In this survey, we provide a comprehensive overview of the\ncurrent research progress on LLMs within the context of CL. This survey is\nstructured into four main sections: we first describe an overview of\ncontinually learning LLMs, consisting of two directions of continuity: vertical\ncontinuity (or vertical continual learning), i.e., continual adaptation from\ngeneral to specific capabilities, and horizontal continuity (or horizontal\ncontinual learning), i.e., continual adaptation across time and domains\n(Section 3). We then summarize three stages of learning LLMs in the context of\nmodern CL: Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP),\nand Continual Fine-Tuning (CFT) (Section 4). Then we provide an overview of\nevaluation protocols for continual learning with LLMs, along with the current\navailable data sources (Section 5). Finally, we discuss intriguing questions\npertaining to continual learning for LLMs (Section 6). The full list of papers\nexamined in this survey is available at\nhttps://github.com/Wang-ML-Lab/llm-continual-learning-survey.", "published": "2024-04-25 17:38:57", "link": "http://arxiv.org/abs/2404.16789v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Model Extrapolation Expedites Alignment", "abstract": "Given the high computational cost of preference alignment training of large\nlanguage models (LLMs), exploring efficient methods to reduce the training\noverhead remains an important and compelling research problem. Motivated by the\nobservation that alignment training typically involves only small parameter\nchanges without injecting new knowledge into models, we propose a\nstraightforward method called ExPO (model extrapolation) to expedite LLMs'\nalignment with human preferences. Given a partially-trained model and its\ninitial SFT checkpoint, ExPO improves the implicit optimization objective of\nalignment training by simply amplifying the parameter change based on a\nfirst-order approximation, without any additional training overhead. Through\ncontrolled experiments, we demonstrate that ExPO boosts a DPO model trained\nwith only 20% steps to outperform the fully-trained one. Moreover, we show that\nExPO notably improves existing open-source LLMs (ranging from 1.8B to 70B\nparameters) on the leading AlpacaEval 2.0 and MT-Bench benchmarks, which\nhighlights ExPO's broader utility in efficiently enhancing LLM alignment.", "published": "2024-04-25 17:39:50", "link": "http://arxiv.org/abs/2404.16792v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Make-it-Real: Unleashing Large Multimodal Model for Painting 3D Objects\n  with Realistic Materials", "abstract": "Physically realistic materials are pivotal in augmenting the realism of 3D\nassets across various applications and lighting conditions. However, existing\n3D assets and generative models often lack authentic material properties.\nManual assignment of materials using graphic software is a tedious and\ntime-consuming task. In this paper, we exploit advancements in Multimodal Large\nLanguage Models (MLLMs), particularly GPT-4V, to present a novel approach,\nMake-it-Real: 1) We demonstrate that GPT-4V can effectively recognize and\ndescribe materials, allowing the construction of a detailed material library.\n2) Utilizing a combination of visual cues and hierarchical text prompts, GPT-4V\nprecisely identifies and aligns materials with the corresponding components of\n3D objects. 3) The correctly matched materials are then meticulously applied as\nreference for the new SVBRDF material generation according to the original\nalbedo map, significantly enhancing their visual authenticity. Make-it-Real\noffers a streamlined integration into the 3D content creation workflow,\nshowcasing its utility as an essential tool for developers of 3D assets.", "published": "2024-04-25 17:59:58", "link": "http://arxiv.org/abs/2404.16829v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Samsung Research China-Beijing at SemEval-2024 Task 3: A multi-stage\n  framework for Emotion-Cause Pair Extraction in Conversations", "abstract": "In human-computer interaction, it is crucial for agents to respond to human\nby understanding their emotions. Unraveling the causes of emotions is more\nchallenging. A new task named Multimodal Emotion-Cause Pair Extraction in\nConversations is responsible for recognizing emotion and identifying causal\nexpressions. In this study, we propose a multi-stage framework to generate\nemotion and extract the emotion causal pairs given the target emotion. In the\nfirst stage, Llama-2-based InstructERC is utilized to extract the emotion\ncategory of each utterance in a conversation. After emotion recognition, a\ntwo-stream attention model is employed to extract the emotion causal pairs\ngiven the target emotion for subtask 2 while MuTEC is employed to extract\ncausal span for subtask 1. Our approach achieved first place for both of the\ntwo subtasks in the competition.", "published": "2024-04-25 11:52:21", "link": "http://arxiv.org/abs/2404.16905v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Prediction Is All MoE Needs: Expert Load Distribution Goes from\n  Fluctuating to Stabilizing", "abstract": "MoE facilitates the development of large models by making the computational\ncomplexity of the model no longer scale linearly with increasing parameters.\nThe learning sparse gating network selects a set of experts for each token to\nbe processed; however, this may lead to differences in the number of tokens\nprocessed by each expert over several successive iterations, i.e., the expert\nload fluctuations, which reduces computational parallelization and resource\nutilization. To this end, we traced and analyzed loads of each expert in the\ntraining iterations for several large language models in this work, and defined\nthe transient state with \"obvious load fluctuation\" and the stable state with\n\"temporal locality\". Moreover, given the characteristics of these two states\nand the computational overhead, we deployed three classical prediction\nalgorithms that achieve accurate expert load prediction results. For the GPT3\n350M model, the average error rates for predicting the expert load proportion\nover the next 1,000 and 2,000 steps are approximately 1.3% and 1.8%,\nrespectively. This work can provide valuable guidance for expert placement or\nresource allocation for MoE model training. Based on this work, we will propose\nan expert placement scheme for transient and stable states in our coming work.", "published": "2024-04-25 15:39:59", "link": "http://arxiv.org/abs/2404.16914v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SetCSE: Set Operations using Contrastive Learning of Sentence Embeddings", "abstract": "Taking inspiration from Set Theory, we introduce SetCSE, an innovative\ninformation retrieval framework. SetCSE employs sets to represent complex\nsemantics and incorporates well-defined operations for structured information\nquerying under the provided context. Within this framework, we introduce an\ninter-set contrastive learning objective to enhance comprehension of sentence\nembedding models concerning the given semantics. Furthermore, we present a\nsuite of operations, including SetCSE intersection, difference, and operation\nseries, that leverage sentence embeddings of the enhanced model for complex\nsentence retrieval tasks. Throughout this paper, we demonstrate that SetCSE\nadheres to the conventions of human language expressions regarding compounded\nsemantics, provides a significant enhancement in the discriminatory capability\nof underlying sentence embedding models, and enables numerous information\nretrieval tasks involving convoluted and intricate prompts which cannot be\nachieved using existing querying methods.", "published": "2024-04-25 02:05:30", "link": "http://arxiv.org/abs/2404.17606v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "GuideWalk: A Novel Graph-Based Word Embedding for Enhanced Text\n  Classification", "abstract": "One of the prime problems of computer science and machine learning is to\nextract information efficiently from large-scale, heterogeneous data. Text\ndata, with its syntax, semantics, and even hidden information content,\npossesses an exceptional place among the data types in concern. The processing\nof the text data requires embedding, a method of translating the content of the\ntext to numeric vectors. A correct embedding algorithm is the starting point\nfor obtaining the full information content of the text data. In this work, a\nnew text embedding approach, namely the Guided Transition Probability Matrix\n(GTPM) model is proposed. The model uses the graph structure of sentences to\ncapture different types of information from text data, such as syntactic,\nsemantic, and hidden content. Using random walks on a weighted word graph, GTPM\ncalculates transition probabilities to derive text embedding vectors. The\nproposed method is tested with real-world data sets and eight well-known and\nsuccessful embedding algorithms. GTPM shows significantly better classification\nperformance for binary and multi-class datasets than well-known algorithms.\nAdditionally, the proposed method demonstrates superior robustness, maintaining\nperformance with limited (only $10\\%$) training data, showing an $8\\%$ decline\ncompared to $15-20\\%$ for baseline methods.", "published": "2024-04-25 18:48:11", "link": "http://arxiv.org/abs/2404.18942v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Fake Artificial Intelligence Generated Contents (FAIGC): A Survey of\n  Theories, Detection Methods, and Opportunities", "abstract": "In recent years, generative artificial intelligence models, represented by\nLarge Language Models (LLMs) and Diffusion Models (DMs), have revolutionized\ncontent production methods. These artificial intelligence-generated content\n(AIGC) have become deeply embedded in various aspects of daily life and work.\nHowever, these technologies have also led to the emergence of Fake Artificial\nIntelligence Generated Content (FAIGC), posing new challenges in distinguishing\ngenuine information. It is crucial to recognize that AIGC technology is akin to\na double-edged sword; its potent generative capabilities, while beneficial,\nalso pose risks for the creation and dissemination of FAIGC. In this survey, We\npropose a new taxonomy that provides a more comprehensive breakdown of the\nspace of FAIGC methods today. Next, we explore the modalities and generative\ntechnologies of FAIGC. We introduce FAIGC detection methods and summarize the\nrelated benchmark from various perspectives. Finally, we discuss outstanding\nchallenges and promising areas for future research.", "published": "2024-04-25 04:44:09", "link": "http://arxiv.org/abs/2405.00711v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Adapting Open-Source Large Language Models for Cost-Effective,\n  Expert-Level Clinical Note Generation with On-Policy Reinforcement Learning", "abstract": "Proprietary Large Language Models (LLMs) such as GPT-4 and Gemini have\ndemonstrated promising capabilities in clinical text summarization tasks.\nHowever, due to patient data privacy concerns and computational costs, many\nhealthcare providers prefer using small, locally-hosted models over external\ngeneric LLMs. This study presents a comprehensive domain- and task-specific\nadaptation process for the open-source LLaMA-2 13 billion parameter model,\nenabling it to generate high-quality clinical notes from outpatient\npatient-doctor dialogues. Our process incorporates continued pre-training,\nsupervised fine-tuning, and reinforcement learning from both AI and human\nfeedback. We introduced a new approach, DistillDirect, for performing on-policy\nreinforcement learning with Gemini 1.0 Pro as the teacher model. Our resulting\nmodel, LLaMA-Clinic, can generate clinical notes comparable in quality to those\nauthored by physicians. In a blinded physician reader study, the majority\n(90.4%) of individual evaluations rated the notes generated by LLaMA-Clinic as\n\"acceptable\" or higher across all three criteria: real-world readiness,\ncompleteness, and accuracy. In the more challenging \"Assessment and Plan\"\nsection, LLaMA-Clinic scored higher (4.2/5) in real-world readiness than\nphysician-authored notes (4.1/5). Our cost analysis for inference shows that\nour LLaMA-Clinic model achieves a 3.75-fold cost reduction compared to an\nexternal generic LLM service. Additionally, we highlight key considerations for\nfuture clinical note-generation tasks, emphasizing the importance of\npre-defining a best-practice note format, rather than relying on LLMs to\ndetermine this for clinical practice. We have made our newly created synthetic\nclinic dialogue-note dataset and the physician feedback dataset publicly\navailable to foster future research.", "published": "2024-04-25 15:34:53", "link": "http://arxiv.org/abs/2405.00715v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Uncovering Deceptive Tendencies in Language Models: A Simulated Company\n  AI Assistant", "abstract": "We study the tendency of AI systems to deceive by constructing a realistic\nsimulation setting of a company AI assistant. The simulated company employees\nprovide tasks for the assistant to complete, these tasks spanning writing\nassistance, information retrieval and programming. We then introduce situations\nwhere the model might be inclined to behave deceptively, while taking care to\nnot instruct or otherwise pressure the model to do so. Across different\nscenarios, we find that Claude 3 Opus\n  1) complies with a task of mass-generating comments to influence public\nperception of the company, later deceiving humans about it having done so,\n  2) lies to auditors when asked questions, and\n  3) strategically pretends to be less capable than it is during capability\nevaluations.\n  Our work demonstrates that even models trained to be helpful, harmless and\nhonest sometimes behave deceptively in realistic scenarios, without notable\nexternal pressure to do so.", "published": "2024-04-25 17:29:53", "link": "http://arxiv.org/abs/2405.01576v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Digital ASIC Design with Ongoing LLMs: Strategies and Prospects", "abstract": "The escalating complexity of modern digital systems has imposed significant\nchallenges on integrated circuit (IC) design, necessitating tools that can\nsimplify the IC design flow. The advent of Large Language Models (LLMs) has\nbeen seen as a promising development, with the potential to automate the\ngeneration of Hardware Description Language (HDL) code, thereby streamlining\ndigital IC design. However, the practical application of LLMs in this area\nfaces substantial hurdles. Notably, current LLMs often generate HDL code with\nsmall but critical syntax errors and struggle to accurately convey the\nhigh-level semantics of circuit designs. These issues significantly undermine\nthe utility of LLMs for IC design, leading to misinterpretations and\ninefficiencies.\n  In response to these challenges, this paper presents targeted strategies to\nharness the capabilities of LLMs for digital ASIC design. We outline approaches\nthat improve the reliability and accuracy of HDL code generation by LLMs. As a\npractical demonstration of these strategies, we detail the development of a\nsimple three-phase Pulse Width Modulation (PWM) generator. This project, part\nof the \"Efabless AI-Generated Open-Source Chip Design Challenge,\" successfully\npassed the Design Rule Check (DRC) and was fabricated, showcasing the potential\nof LLMs to enhance digital ASIC design. This work underscores the feasibility\nand benefits of integrating LLMs into the IC design process, offering a novel\napproach to overcoming the complexities of modern digital systems.", "published": "2024-04-25 05:16:57", "link": "http://arxiv.org/abs/2405.02329v1", "categories": ["cs.AR", "cs.AI", "cs.CL"], "primary_category": "cs.AR"}
{"title": "To what extent is ChatGPT useful for language teacher lesson plan\n  creation?", "abstract": "The advent of generative AI models holds tremendous potential for aiding\nteachers in the generation of pedagogical materials. However, numerous\nknowledge gaps concerning the behavior of these models obfuscate the generation\nof research-informed guidance for their effective usage. Here we assess trends\nin prompt specificity, variability, and weaknesses in foreign language teacher\nlesson plans generated by zero-shot prompting in ChatGPT. Iterating a series of\nprompts that increased in complexity, we found that output lesson plans were\ngenerally high quality, though additional context and specificity to a prompt\ndid not guarantee a concomitant increase in quality. Additionally, we observed\nextreme cases of variability in outputs generated by the same prompt. In many\ncases, this variability reflected a conflict between 20th century versus 21st\ncentury pedagogical practices. These results suggest that the training of\ngenerative AI models on classic texts concerning pedagogical practices may\nrepresent a currently underexplored topic with the potential to bias generated\ncontent towards teaching practices that have been long refuted by research.\nCollectively, our results offer immediate translational implications for\npracticing and training foreign language teachers on the use of AI tools. More\nbroadly, these findings reveal the existence of generative AI output trends\nthat have implications for the generation of pedagogical materials across a\ndiversity of content areas.", "published": "2024-04-25 12:00:03", "link": "http://arxiv.org/abs/2407.09974v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.CY"}
{"title": "The GPT Surprise: Offering Large Language Model Chat in a Massive Coding\n  Class Reduced Engagement but Increased Adopters Exam Performances", "abstract": "Large language models (LLMs) are quickly being adopted in a wide range of\nlearning experiences, especially via ubiquitous and broadly accessible chat\ninterfaces like ChatGPT and Copilot. This type of interface is readily\navailable to students and teachers around the world, yet relatively little\nresearch has been done to assess the impact of such generic tools on student\nlearning. Coding education is an interesting test case, both because LLMs have\nstrong performance on coding tasks, and because LLM-powered support tools are\nrapidly becoming part of the workflow of professional software engineers. To\nhelp understand the impact of generic LLM use on coding education, we conducted\na large-scale randomized control trial with 5,831 students from 146 countries\nin an online coding class in which we provided some students with access to a\nchat interface with GPT-4. We estimate positive benefits on exam performance\nfor adopters, the students who used the tool, but over all students, the\nadvertisement of GPT-4 led to a significant average decrease in exam\nparticipation. We observe similar decreases in other forms of course\nengagement. However, this decrease is modulated by the student's country of\norigin. Offering access to LLMs to students from low human development index\ncountries increased their exam participation rate on average. Our results\nsuggest there may be promising benefits to using LLMs in an introductory coding\nclass, but also potential harms for engagement, which makes their longer term\nimpact on student success unclear. Our work highlights the need for additional\ninvestigations to help understand the potential impact of future adoption and\nintegration of LLMs into classrooms.", "published": "2024-04-25 15:39:22", "link": "http://arxiv.org/abs/2407.09975v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "stat.AP"], "primary_category": "cs.CY"}
{"title": "Utilizing Large Language Models to Identify Reddit Users Considering\n  Vaping Cessation for Digital Interventions", "abstract": "The widespread adoption of social media platforms globally not only enhances\nusers' connectivity and communication but also emerges as a vital channel for\nthe dissemination of health-related information, thereby establishing social\nmedia data as an invaluable organic data resource for public health research.\nThe surge in popularity of vaping or e-cigarette use in the United States and\nother countries has caused an outbreak of e-cigarette and vaping use-associated\nlung injury (EVALI), leading to hospitalizations and fatalities in 2019,\nhighlighting the urgency to comprehend vaping behaviors and develop effective\nstrategies for cession. In this study, we extracted a sample dataset from one\nvaping sub-community on Reddit to analyze users' quit vaping intentions.\nLeveraging large language models including both the latest GPT-4 and\ntraditional BERT-based language models for sentence-level quit-vaping intention\nprediction tasks, this study compares the outcomes of these models against\nhuman annotations. Notably, when compared to human evaluators, GPT-4 model\ndemonstrates superior consistency in adhering to annotation guidelines and\nprocesses, showcasing advanced capabilities to detect nuanced user quit-vaping\nintentions that human evaluators might overlook. These preliminary findings\nemphasize the potential of GPT-4 in enhancing the accuracy and reliability of\nsocial media data analysis, especially in identifying subtle users' intentions\nthat may elude human detection.", "published": "2024-04-25 15:45:58", "link": "http://arxiv.org/abs/2404.17607v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.IR"}
{"title": "An Experiment with Electric Guitar Signals for Exploring the Virtuosity\n  based on the Entropy of Music", "abstract": "We analyze the concept of virtuosity as a collective attribute in music and\nits relationship with the entropy based on an experiment that compares two sets\nof digital signals played by composer-performer electric guitarists. Based on\nan interdisciplinary approach related to the complex systems, we computed the\nspectrum of signals, identified statistical distributions that best describe\nthem, and measured the Shannon entropy to establish their diversity. Findings\nsuggested that virtuosity might be related to a range of entropy values that\nidentify levels of diversity of the frequency components of audio signals.\nDespite the presence of different values of entropy in the two sets of signals,\nthey are statistically similar. Therefore, entropy values can be interpreted as\nlevels of virtuosity in music.", "published": "2024-04-25 00:06:28", "link": "http://arxiv.org/abs/2404.16259v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The THU-HCSI Multi-Speaker Multi-Lingual Few-Shot Voice Cloning System\n  for LIMMITS'24 Challenge", "abstract": "This paper presents the multi-speaker multi-lingual few-shot voice cloning\nsystem developed by THU-HCSI team for LIMMITS'24 Challenge. To achieve high\nspeaker similarity and naturalness in both mono-lingual and cross-lingual\nscenarios, we build the system upon YourTTS and add several enhancements. For\nfurther improving speaker similarity and speech quality, we introduce\nspeaker-aware text encoder and flow-based decoder with Transformer blocks. In\naddition, we denoise the few-shot data, mix up them with pre-training data, and\nadopt a speaker-balanced sampling strategy to guarantee effective fine-tuning\nfor target speakers. The official evaluations in track 1 show that our system\nachieves the best speaker similarity MOS of 4.25 and obtains considerable\nnaturalness MOS of 3.97.", "published": "2024-04-25 14:02:25", "link": "http://arxiv.org/abs/2404.16619v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Investigating differences in lab-quality and remote recording methods\n  with dynamic acoustic measures", "abstract": "Increasingly, phonetic research utilizes data collected from participants who\nrecord themselves on readily available devices. Though such recordings are\nconvenient, their suitability for acoustic analysis remains an open question,\nespecially regarding how the individual methods affect acoustic measures over\ntime. We used Quantile Generalized Additive Mixed Models (QGAMMs) to analyze\nmeasures of F0, intensity, and the first and second formants, comparing files\nrecorded using a laboratory-standard recording method (Zoom H6 Recorder with an\nexternal microphone), to three remote recording methods, (1) the Awesome Voice\nRecorder application on a smartphone (AVR), (2) the Zoom meeting application\nwith default settings (Zoom-default), and (3) the Zoom meeting application with\nthe \"Turn on Original Sound\" setting (Zoom-raw). A linear temporal alignment\nissue was observed for the Zoom methods over the course of the long, recording\nsession files. However, the difference was not significant for utterance-length\nfiles. F0 was reliably measured using all methods. Intensity and formants\npresented non-linear differences across methods that could not be corrected for\nsimply. Overall, the AVR files were most similar to the H6's, and so AVR is\ndeemed to be a more reliable recording method than either Zoom-default or\nZoom-raw.", "published": "2024-04-25 20:27:10", "link": "http://arxiv.org/abs/2404.17022v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Semantically consistent Video-to-Audio Generation using Multimodal\n  Language Large Model", "abstract": "Existing works have made strides in video generation, but the lack of sound\neffects (SFX) and background music (BGM) hinders a complete and immersive\nviewer experience. We introduce a novel semantically consistent v ideo-to-audio\ngeneration framework, namely SVA, which automatically generates audio\nsemantically consistent with the given video content. The framework harnesses\nthe power of multimodal large language model (MLLM) to understand video\nsemantics from a key frame and generate creative audio schemes, which are then\nutilized as prompts for text-to-audio models, resulting in video-to-audio\ngeneration with natural language as an interface. We show the satisfactory\nperformance of SVA through case study and discuss the limitations along with\nthe future research direction. The project page is available at\nhttps://huiz-a.github.io/audio4video.github.io/.", "published": "2024-04-25 03:14:49", "link": "http://arxiv.org/abs/2404.16305v2", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Leveraging tropical reef, bird and unrelated sounds for superior\n  transfer learning in marine bioacoustics", "abstract": "Machine learning has the potential to revolutionize passive acoustic\nmonitoring (PAM) for ecological assessments. However, high annotation and\ncompute costs limit the field's efficacy. Generalizable pretrained networks can\novercome these costs, but high-quality pretraining requires vast annotated\nlibraries, limiting its current applicability primarily to bird taxa. Here, we\nidentify the optimum pretraining strategy for a data-deficient domain using\ncoral reef bioacoustics. We assemble ReefSet, a large annotated library of reef\nsounds, though modest compared to bird libraries at 2% of the sample count.\nThrough testing few-shot transfer learning performance, we observe that\npretraining on bird audio provides notably superior generalizability compared\nto pretraining on ReefSet or unrelated audio alone. However, our key findings\nshow that cross-domain mixing which leverages bird, reef and unrelated audio\nduring pretraining maximizes reef generalizability. SurfPerch, our pretrained\nnetwork, provides a strong foundation for automated analysis of marine PAM data\nwith minimal annotation and compute costs.", "published": "2024-04-25 09:12:35", "link": "http://arxiv.org/abs/2404.16436v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Developing Acoustic Models for Automatic Speech Recognition in Swedish", "abstract": "This paper is concerned with automatic continuous speech recognition using\ntrainable systems. The aim of this work is to build acoustic models for spoken\nSwedish. This is done employing hidden Markov models and using the SpeechDat\ndatabase to train their parameters. Acoustic modeling has been worked out at a\nphonetic level, allowing general speech recognition applications, even though a\nsimplified task (digits and natural number recognition) has been considered for\nmodel evaluation. Different kinds of phone models have been tested, including\ncontext independent models and two variations of context dependent models.\nFurthermore many experiments have been done with bigram language models to tune\nsome of the system parameters. System performance over various speaker subsets\nwith different sex, age and dialect has also been examined. Results are\ncompared to previous similar studies showing a remarkable improvement.", "published": "2024-04-25 12:03:14", "link": "http://arxiv.org/abs/2404.16547v1", "categories": ["eess.AS", "cs.AI", "cs.SD", "68T10", "I.5.0; I.2.0; I.2.7"], "primary_category": "eess.AS"}
{"title": "COCOLA: Coherence-Oriented Contrastive Learning of Musical Audio\n  Representations", "abstract": "We present COCOLA (Coherence-Oriented Contrastive Learning for Audio), a\ncontrastive learning method for musical audio representations that captures the\nharmonic and rhythmic coherence between samples. Our method operates at the\nlevel of the stems composing music tracks and can input features obtained via\nHarmonic-Percussive Separation (HPS). COCOLA allows the objective evaluation of\ngenerative models for music accompaniment generation, which are difficult to\nbenchmark with established metrics. In this regard, we evaluate recent music\naccompaniment generation models, demonstrating the effectiveness of the\nproposed method. We release the model checkpoints trained on public datasets\ncontaining separate stems (MUSDB18-HQ, MoisesDB, Slakh2100, and CocoChorales).", "published": "2024-04-25 18:42:25", "link": "http://arxiv.org/abs/2404.16969v4", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Synthesizing Audio from Silent Video using Sequence to Sequence Modeling", "abstract": "Generating audio from a video's visual context has multiple practical\napplications in improving how we interact with audio-visual media - for\nexample, enhancing CCTV footage analysis, restoring historical videos (e.g.,\nsilent movies), and improving video generation models. We propose a novel\nmethod to generate audio from video using a sequence-to-sequence model,\nimproving on prior work that used CNNs and WaveNet and faced sound diversity\nand generalization challenges. Our approach employs a 3D Vector Quantized\nVariational Autoencoder (VQ-VAE) to capture the video's spatial and temporal\nstructures, decoding with a custom audio decoder for a broader range of sounds.\nTrained on the Youtube8M dataset segment, focusing on specific domains, our\nmodel aims to enhance applications like CCTV footage analysis, silent movie\nrestoration, and video generation models.", "published": "2024-04-25 22:19:42", "link": "http://arxiv.org/abs/2404.17608v1", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
