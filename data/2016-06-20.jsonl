{"title": "A Nonparametric Bayesian Approach for Spoken Term detection by Example\n  Query", "abstract": "State of the art speech recognition systems use data-intensive\ncontext-dependent phonemes as acoustic units. However, these approaches do not\ntranslate well to low resourced languages where large amounts of training data\nis not available. For such languages, automatic discovery of acoustic units is\ncritical. In this paper, we demonstrate the application of nonparametric\nBayesian models to acoustic unit discovery. We show that the discovered units\nare correlated with phonemes and therefore are linguistically meaningful. We\nalso present a spoken term detection (STD) by example query algorithm based on\nthese automatically learned units. We show that our proposed system produces a\nP@N of 61.2% and an EER of 13.95% on the TIMIT dataset. The improvement in the\nEER is 5% while P@N is only slightly lower than the best reported system in the\nliterature.", "published": "2016-06-20 04:06:23", "link": "http://arxiv.org/abs/1606.05967v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Role of CNL and AMR in Scalable Abstractive Summarization for\n  Multilingual Media Monitoring", "abstract": "In the era of Big Data and Deep Learning, there is a common view that machine\nlearning approaches are the only way to cope with the robust and scalable\ninformation extraction and summarization. It has been recently proposed that\nthe CNL approach could be scaled up, building on the concept of embedded CNL\nand, thus, allowing for CNL-based information extraction from e.g. normative or\nmedical texts that are rather controlled by nature but still infringe the\nboundaries of CNL. Although it is arguable if CNL can be exploited to approach\nthe robust wide-coverage semantic parsing for use cases like media monitoring,\nits potential becomes much more obvious in the opposite direction: generation\nof story highlights from the summarized AMR graphs, which is in the focus of\nthis position paper.", "published": "2016-06-20 07:15:55", "link": "http://arxiv.org/abs/1606.05994v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Data-Driven Approach for Semantic Role Labeling from Induced Grammar\n  Structures in Language", "abstract": "Semantic roles play an important role in extracting knowledge from text.\nCurrent unsupervised approaches utilize features from grammar structures, to\ninduce semantic roles. The dependence on these grammars, however, makes it\ndifficult to adapt to noisy and new languages. In this paper we develop a\ndata-driven approach to identifying semantic roles, the approach is entirely\nunsupervised up to the point where rules need to be learned to identify the\nposition the semantic role occurs. Specifically we develop a modified-ADIOS\nalgorithm based on ADIOS Solan et al. (2005) to learn grammar structures, and\nuse these grammar structures to learn the rules for identifying the semantic\nroles based on the context in which the grammar structures appeared. The\nresults obtained are comparable with the current state-of-art models that are\ninherently dependent on human annotated data.", "published": "2016-06-20 19:53:53", "link": "http://arxiv.org/abs/1606.06274v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fast, Compact, and High Quality LSTM-RNN Based Statistical Parametric\n  Speech Synthesizers for Mobile Devices", "abstract": "Acoustic models based on long short-term memory recurrent neural networks\n(LSTM-RNNs) were applied to statistical parametric speech synthesis (SPSS) and\nshowed significant improvements in naturalness and latency over those based on\nhidden Markov models (HMMs). This paper describes further optimizations of\nLSTM-RNN-based SPSS for deployment on mobile devices; weight quantization,\nmulti-frame inference, and robust inference using an {\\epsilon}-contaminated\nGaussian loss function. Experimental results in subjective listening tests show\nthat these optimizations can make LSTM-RNN-based SPSS comparable to HMM-based\nSPSS in runtime speed while maintaining naturalness. Evaluations between\nLSTM-RNN- based SPSS and HMM-driven unit selection speech synthesis are also\npresented.", "published": "2016-06-20 10:54:51", "link": "http://arxiv.org/abs/1606.06061v2", "categories": ["cs.SD", "cs.CL"], "primary_category": "cs.SD"}
{"title": "Uncertainty in Neural Network Word Embedding: Exploration of Threshold\n  for Similarity", "abstract": "Word embedding, specially with its recent developments, promises a\nquantification of the similarity between terms. However, it is not clear to\nwhich extent this similarity value can be genuinely meaningful and useful for\nsubsequent tasks. We explore how the similarity score obtained from the models\nis really indicative of term relatedness. We first observe and quantify the\nuncertainty factor of the word embedding models regarding to the similarity\nvalue. Based on this factor, we introduce a general threshold on various\ndimensions which effectively filters the highly related terms. Our evaluation\non four information retrieval collections supports the effectiveness of our\napproach as the results of the introduced threshold are significantly better\nthan the baseline while being equal to or statistically indistinguishable from\nthe optimal results.", "published": "2016-06-20 12:31:13", "link": "http://arxiv.org/abs/1606.06086v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Introducing a Calculus of Effects and Handlers for Natural Language\n  Semantics", "abstract": "In compositional model-theoretic semantics, researchers assemble\ntruth-conditions or other kinds of denotations using the lambda calculus. It\nwas previously observed that the lambda terms and/or the denotations studied\ntend to follow the same pattern: they are instances of a monad. In this paper,\nwe present an extension of the simply-typed lambda calculus that exploits this\nuniformity using the recently discovered technique of effect handlers. We prove\nthat our calculus exhibits some of the key formal properties of the lambda\ncalculus and we use it to construct a modular semantics for a small fragment\nthat involves multiple distinct semantic phenomena.", "published": "2016-06-20 14:05:59", "link": "http://arxiv.org/abs/1606.06125v2", "categories": ["cs.CL", "cs.PL"], "primary_category": "cs.CL"}
{"title": "Pragmatic factors in image description: the case of negations", "abstract": "We provide a qualitative analysis of the descriptions containing negations\n(no, not, n't, nobody, etc) in the Flickr30K corpus, and a categorization of\nnegation uses. Based on this analysis, we provide a set of requirements that an\nimage description system should have in order to generate negation sentences.\nAs a pilot experiment, we used our categorization to manually annotate\nsentences containing negations in the Flickr30K corpus, with an agreement score\nof K=0.67. With this paper, we hope to open up a broader discussion of\nsubjective language in image descriptions.", "published": "2016-06-20 15:08:22", "link": "http://arxiv.org/abs/1606.06164v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "MOSI: Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis\n  in Online Opinion Videos", "abstract": "People are sharing their opinions, stories and reviews through online video\nsharing websites every day. Studying sentiment and subjectivity in these\nopinion videos is experiencing a growing attention from academia and industry.\nWhile sentiment analysis has been successful for text, it is an understudied\nresearch question for videos and multimedia content. The biggest setbacks for\nstudies in this direction are lack of a proper dataset, methodology, baselines\nand statistical analysis of how information from different modality sources\nrelate to each other. This paper introduces to the scientific community the\nfirst opinion-level annotated corpus of sentiment and subjectivity analysis in\nonline videos called Multimodal Opinion-level Sentiment Intensity dataset\n(MOSI). The dataset is rigorously annotated with labels for subjectivity,\nsentiment intensity, per-frame and per-opinion annotated visual features, and\nper-milliseconds annotated audio features. Furthermore, we present baselines\nfor future studies in this direction as well as a new multimodal fusion\napproach that jointly models spoken words and visual gestures.", "published": "2016-06-20 19:23:53", "link": "http://arxiv.org/abs/1606.06259v2", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "The LAMBADA dataset: Word prediction requiring a broad discourse context", "abstract": "We introduce LAMBADA, a dataset to evaluate the capabilities of computational\nmodels for text understanding by means of a word prediction task. LAMBADA is a\ncollection of narrative passages sharing the characteristic that human subjects\nare able to guess their last word if they are exposed to the whole passage, but\nnot if they only see the last sentence preceding the target word. To succeed on\nLAMBADA, computational models cannot simply rely on local context, but must be\nable to keep track of information in the broader discourse. We show that\nLAMBADA exemplifies a wide range of linguistic phenomena, and that none of\nseveral state-of-the-art language models reaches accuracy above 1% on this\nnovel benchmark. We thus propose LAMBADA as a challenging test set, meant to\nencourage the development of new models capable of genuine understanding of\nbroad context in natural language text.", "published": "2016-06-20 09:37:17", "link": "http://arxiv.org/abs/1606.06031v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Product Classification in E-Commerce using Distributional Semantics", "abstract": "Product classification is the task of automatically predicting a taxonomy\npath for a product in a predefined taxonomy hierarchy given a textual product\ndescription or title. For efficient product classification we require a\nsuitable representation for a document (the textual description of a product)\nfeature vector and efficient and fast algorithms for prediction. To address the\nabove challenges, we propose a new distributional semantics representation for\ndocument vector formation. We also develop a new two-level ensemble approach\nutilizing (with respect to the taxonomy tree) a path-wise, node-wise and\ndepth-wise classifiers for error reduction in the final product classification.\nOur experiments show the effectiveness of the distributional representation and\nthe ensemble approach on data sets from a leading e-commerce platform and\nachieve better results on various evaluation metrics compared to earlier\napproaches.", "published": "2016-06-20 12:26:21", "link": "http://arxiv.org/abs/1606.06083v2", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Quantifying and Reducing Stereotypes in Word Embeddings", "abstract": "Machine learning algorithms are optimized to model statistical properties of\nthe training data. If the input data reflects stereotypes and biases of the\nbroader society, then the output of the learning algorithm also captures these\nstereotypes. In this paper, we initiate the study of gender stereotypes in {\\em\nword embedding}, a popular framework to represent text data. As their use\nbecomes increasingly common, applications can inadvertently amplify unwanted\nstereotypes. We show across multiple datasets that the embeddings contain\nsignificant gender stereotypes, especially with regard to professions. We\ncreated a novel gender analogy task and combined it with crowdsourcing to\nsystematically quantify the gender bias in a given embedding. We developed an\nefficient algorithm that reduces gender stereotype using just a handful of\ntraining examples while preserving the useful geometric properties of the\nembedding. We evaluated our algorithm on several metrics. While we focus on\nmale/female stereotypes, our framework may be applicable to other types of\nembedding biases.", "published": "2016-06-20 13:58:45", "link": "http://arxiv.org/abs/1606.06121v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "LSTM-Based Predictions for Proactive Information Retrieval", "abstract": "We describe a method for proactive information retrieval targeted at\nretrieving relevant information during a writing task. In our method, the\ncurrent task and the needs of the user are estimated, and the potential next\nsteps are unobtrusively predicted based on the user's past actions. We focus on\nthe task of writing, in which the user is coalescing previously collected\ninformation into a text. Our proactive system automatically recommends the user\nrelevant background information. The proposed system incorporates text input\nprediction using a long short-term memory (LSTM) network. We present\nsimulations, which show that the system is able to reach higher precision\nvalues in an exploratory search setting compared to both a baseline and a\ncomparison system.", "published": "2016-06-20 14:26:33", "link": "http://arxiv.org/abs/1606.06137v1", "categories": ["cs.IR", "cs.CL", "cs.NE"], "primary_category": "cs.IR"}
{"title": "Comparing the hierarchy of keywords in on-line news portals", "abstract": "The tagging of on-line content with informative keywords is a widespread\nphenomenon from scientific article repositories through blogs to on-line news\nportals. In most of the cases, the tags on a given item are free words chosen\nby the authors independently. Therefore, relations among keywords in a\ncollection of news items is unknown. However, in most cases the topics and\nconcepts described by these keywords are forming a latent hierarchy, with the\nmore general topics and categories at the top, and more specialised ones at the\nbottom. Here we apply a recent, cooccurrence-based tag hierarchy extraction\nmethod to sets of keywords obtained from four different on-line news portals.\nThe resulting hierarchies show substantial differences not just in the topics\nrendered as important (being at the top of the hierarchy) or of less interest\n(categorised low in the hierarchy), but also in the underlying network\nstructure. This reveals discrepancies between the plausible keyword association\nframeworks in the studied news portals.", "published": "2016-06-20 14:34:55", "link": "http://arxiv.org/abs/1606.06142v1", "categories": ["physics.soc-ph", "cs.CL", "cs.SI"], "primary_category": "physics.soc-ph"}
{"title": "Visualizing textual models with in-text and word-as-pixel highlighting", "abstract": "We explore two techniques which use color to make sense of statistical text\nmodels. One method uses in-text annotations to illustrate a model's view of\nparticular tokens in particular documents. Another uses a high-level,\n\"words-as-pixels\" graphic to display an entire corpus. Together, these methods\noffer both zoomed-in and zoomed-out perspectives into a model's understanding\nof text. We show how these interconnected methods help diagnose a classifier's\npoor performance on Twitter slang, and make sense of a topic model on\nhistorical political texts.", "published": "2016-06-20 22:30:19", "link": "http://arxiv.org/abs/1606.06352v1", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "A Probabilistic Generative Grammar for Semantic Parsing", "abstract": "Domain-general semantic parsing is a long-standing goal in natural language\nprocessing, where the semantic parser is capable of robustly parsing sentences\nfrom domains outside of which it was trained. Current approaches largely rely\non additional supervision from new domains in order to generalize to those\ndomains. We present a generative model of natural language utterances and\nlogical forms and demonstrate its application to semantic parsing. Our approach\nrelies on domain-independent supervision to generalize to new domains. We\nderive and implement efficient algorithms for training, parsing, and sentence\ngeneration. The work relies on a novel application of hierarchical Dirichlet\nprocesses (HDPs) for structured prediction, which we also present in this\nmanuscript.\n  This manuscript is an excerpt of chapter 4 from the Ph.D. thesis of Saparov\n(2022), where the model plays a central role in a larger natural language\nunderstanding system.\n  This manuscript provides a new simplified and more complete presentation of\nthe work first introduced in Saparov, Saraswat, and Mitchell (2017). The\ndescription and proofs of correctness of the training algorithm, parsing\nalgorithm, and sentence generation algorithm are much simplified in this new\npresentation. We also describe the novel application of hierarchical Dirichlet\nprocesses for structured prediction. In addition, we extend the earlier work\nwith a new model of word morphology, which utilizes the comprehensive\nmorphological data from Wiktionary.", "published": "2016-06-20 23:29:55", "link": "http://arxiv.org/abs/1606.06361v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Unanimous Prediction for 100% Precision with Application to Learning\n  Semantic Mappings", "abstract": "Can we train a system that, on any new input, either says \"don't know\" or\nmakes a prediction that is guaranteed to be correct? We answer the question in\nthe affirmative provided our model family is well-specified. Specifically, we\nintroduce the unanimity principle: only predict when all models consistent with\nthe training data predict the same output. We operationalize this principle for\nsemantic parsing, the task of mapping utterances to logical forms. We develop a\nsimple, efficient method that reasons over the infinite set of all consistent\nmodels by only checking two of the models. We prove that our method obtains\n100% precision even with a modest amount of training data from a possibly\nadversarial distribution. Empirically, we demonstrate the effectiveness of our\napproach on the standard GeoQuery dataset.", "published": "2016-06-20 23:59:25", "link": "http://arxiv.org/abs/1606.06368v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7; I.2.6"], "primary_category": "cs.LG"}
