{"title": "Deep convolutional acoustic word embeddings using word-pair side\n  information", "abstract": "Recent studies have been revisiting whole words as the basic modelling unit\nin speech recognition and query applications, instead of phonetic units. Such\nwhole-word segmental systems rely on a function that maps a variable-length\nspeech segment to a vector in a fixed-dimensional space; the resulting acoustic\nword embeddings need to allow for accurate discrimination between different\nword types, directly in the embedding space. We compare several old and new\napproaches in a word discrimination task. Our best approach uses side\ninformation in the form of known word pairs to train a Siamese convolutional\nneural network (CNN): a pair of tied networks that take two speech segments as\ninput and produce their embeddings, trained with a hinge loss that separates\nsame-word pairs and different-word pairs by some margin. A word classifier CNN\nperforms similarly, but requires much stronger supervision. Both types of CNNs\nyield large improvements over the best previously published results on the word\ndiscrimination task.", "published": "2015-10-05 05:25:32", "link": "http://arxiv.org/abs/1510.01032v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stochastic model for phonemes uncovers an author-dependency of their\n  usage", "abstract": "We study rank-frequency relations for phonemes, the minimal units that still\nrelate to linguistic meaning. We show that these relations can be described by\nthe Dirichlet distribution, a direct analogue of the ideal-gas model in\nstatistical mechanics. This description allows us to demonstrate that the\nrank-frequency relations for phonemes of a text do depend on its author. The\nauthor-dependency effect is not caused by the author's vocabulary (common words\nused in different texts), and is confirmed by several alternative means. This\nsuggests that it can be directly related to phonemes. These features contrast\nto rank-frequency relations for words, which are both author and text\nindependent and are governed by the Zipf's law.", "published": "2015-10-05 09:43:25", "link": "http://arxiv.org/abs/1510.01315v2", "categories": ["cs.CL", "nlin.AO"], "primary_category": "cs.CL"}
{"title": "Calculating entropy at different scales among diverse communication\n  systems", "abstract": "We evaluated the impact of changing the observation scale over the entropy\nmeasures for text descriptions. MIDI coded Music, computer code and two human\nnatural languages were studied at the scale of characters, words, and at the\nFundamental Scale resulting from adjusting the symbols length used to interpret\neach text-description until it produced minimum entropy. The results show that\nthe Fundamental Scale method is comparable with the use of words when measuring\nentropy levels in written texts. However, this method can also be used in\ncommunication systems lacking words such as music. Measuring symbolic entropy\nat the fundamental scale allows to calculate quantitatively, relative levels of\ncomplexity for different communication systems. The results open novel vision\non differences among the structure of the communication systems studied.", "published": "2015-10-05 04:16:34", "link": "http://arxiv.org/abs/1510.01026v1", "categories": ["cs.IT", "cs.CL", "math.IT"], "primary_category": "cs.IT"}
