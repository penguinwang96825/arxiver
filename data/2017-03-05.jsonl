{"title": "Random vector generation of a semantic space", "abstract": "We show how random vectors and random projection can be implemented in the\nusual vector space model to construct a Euclidean semantic space from a French\nsynonym dictionary. We evaluate theoretically the resulting noise and show the\nexperimental distribution of the similarities of terms in a neighborhood\naccording to the choice of parameters. We also show that the Schmidt\northogonalization process is applicable and can be used to separate homonyms\nwith distinct semantic meanings. Neighboring terms are easily arranged into\nsemantically significant clusters which are well suited to the generation of\nrealistic lists of synonyms and to such applications as word selection for\nautomatic text generation. This process, applicable to any language, can easily\nbe extended to collocations, is extremely fast and can be updated in real time,\nwhenever new synonyms are proposed.", "published": "2017-03-05 15:07:10", "link": "http://arxiv.org/abs/1703.02031v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controlling for Unobserved Confounds in Classification Using\n  Correlational Constraints", "abstract": "As statistical classifiers become integrated into real-world applications, it\nis important to consider not only their accuracy but also their robustness to\nchanges in the data distribution. In this paper, we consider the case where\nthere is an unobserved confounding variable $z$ that influences both the\nfeatures $\\mathbf{x}$ and the class variable $y$. When the influence of $z$\nchanges from training to testing data, we find that the classifier accuracy can\ndegrade rapidly. In our approach, we assume that we can predict the value of\n$z$ at training time with some error. The prediction for $z$ is then fed to\nPearl's back-door adjustment to build our model. Because of the attenuation\nbias caused by measurement error in $z$, standard approaches to controlling for\n$z$ are ineffective. In response, we propose a method to properly control for\nthe influence of $z$ by first estimating its relationship with the class\nvariable $y$, then updating predictions for $z$ to match that estimated\nrelationship. By adjusting the influence of $z$, we show that we can build a\nmodel that exceeds competing baselines on accuracy as well as on robustness\nover a range of confounding relationships.", "published": "2017-03-05 21:57:25", "link": "http://arxiv.org/abs/1703.01671v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Using Graphs of Classifiers to Impose Declarative Constraints on\n  Semi-supervised Learning", "abstract": "We propose a general approach to modeling semi-supervised learning (SSL)\nalgorithms. Specifically, we present a declarative language for modeling both\ntraditional supervised classification tasks and many SSL heuristics, including\nboth well-known heuristics such as co-training and novel domain-specific\nheuristics. In addition to representing individual SSL heuristics, we show that\nmultiple heuristics can be automatically combined using Bayesian optimization\nmethods. We experiment with two classes of tasks, link-based text\nclassification and relation extraction. We show modest improvements on\nwell-studied link-based classification benchmarks, and state-of-the-art results\non relation-extraction tasks for two realistic domains.", "published": "2017-03-05 04:43:41", "link": "http://arxiv.org/abs/1703.01557v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Neural Machine Translation and Sequence-to-sequence Models: A Tutorial", "abstract": "This tutorial introduces a new and powerful set of techniques variously\ncalled \"neural machine translation\" or \"neural sequence-to-sequence models\".\nThese techniques have been used in a number of tasks regarding the handling of\nhuman language, and can be a powerful tool in the toolbox of anyone who wants\nto model sequential data of some sort. The tutorial assumes that the reader\nknows the basics of math and programming, but does not assume any particular\nexperience with neural networks or natural language processing. It attempts to\nexplain the intuition behind the various methods covered, then delves into them\nwith enough mathematical detail to understand them concretely, and culiminates\nwith a suggestion for an implementation exercise, where readers can test that\nthey understood the content in practice.", "published": "2017-03-05 16:10:11", "link": "http://arxiv.org/abs/1703.01619v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
