{"title": "SubGram: Extending Skip-gram Word Representation with Substrings", "abstract": "Skip-gram (word2vec) is a recent method for creating vector representations\nof words (\"distributed word representations\") using a neural network. The\nrepresentation gained popularity in various areas of natural language\nprocessing, because it seems to capture syntactic and semantic information\nabout words without any explicit supervision in this respect. We propose\nSubGram, a refinement of the Skip-gram model to consider also the word\nstructure during the training process, achieving large gains on the Skip-gram\noriginal test set.", "published": "2018-06-18 09:31:38", "link": "http://arxiv.org/abs/1806.06571v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Enhancing Speech Emotion Recognition using Generative Adversarial\n  Networks", "abstract": "Generative Adversarial Networks (GANs) have gained a lot of attention from\nmachine learning community due to their ability to learn and mimic an input\ndata distribution. GANs consist of a discriminator and a generator working in\ntandem playing a min-max game to learn a target underlying data distribution;\nwhen fed with data-points sampled from a simpler distribution (like uniform or\nGaussian distribution). Once trained, they allow synthetic generation of\nexamples sampled from the target distribution. We investigate the application\nof GANs to generate synthetic feature vectors used for speech emotion\nrecognition. Specifically, we investigate two set ups: (i) a vanilla GAN that\nlearns the distribution of a lower dimensional representation of the actual\nhigher dimensional feature vector and, (ii) a conditional GAN that learns the\ndistribution of the higher dimensional feature vectors conditioned on the\nlabels or the emotional class to which it belongs. As a potential practical\napplication of these synthetically generated samples, we measure any\nimprovement in a classifier's performance when the synthetic data is used along\nwith real data for training. We perform cross-validation analyses followed by a\ncross-corpus study.", "published": "2018-06-18 12:21:18", "link": "http://arxiv.org/abs/1806.06626v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combining Word Feature Vector Method with the Convolutional Neural\n  Network for Slot Filling in Spoken Language Understanding", "abstract": "Slot filling is an important problem in Spoken Language Understanding (SLU)\nand Natural Language Processing (NLP), which involves identifying a user's\nintent and assigning a semantic concept to each word in a sentence. This paper\npresents a word feature vector method and combines it into the convolutional\nneural network (CNN). We consider 18 word features and each word feature is\nconstructed by merging similar word labels. By introducing the concept of\nexternal library, we propose a feature set approach that is beneficial for\nbuilding the relationship between a word from the training dataset and the\nfeature. Computational results are reported using the ATIS dataset and\ncomparisons with traditional CNN as well as bi-directional sequential CNN are\nalso presented.", "published": "2018-06-18 18:13:21", "link": "http://arxiv.org/abs/1806.06874v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comparison of Transformer and Recurrent Neural Networks on\n  Multilingual Neural Machine Translation", "abstract": "Recently, neural machine translation (NMT) has been extended to\nmultilinguality, that is to handle more than one translation direction with a\nsingle system. Multilingual NMT showed competitive performance against pure\nbilingual systems. Notably, in low-resource settings, it proved to work\neffectively and efficiently, thanks to shared representation space that is\nforced across languages and induces a sort of transfer-learning. Furthermore,\nmultilingual NMT enables so-called zero-shot inference across language pairs\nnever seen at training time. Despite the increasing interest in this framework,\nan in-depth analysis of what a multilingual NMT model is capable of and what it\nis not is still missing. Motivated by this, our work (i) provides a\nquantitative and comparative analysis of the translations produced by\nbilingual, multilingual and zero-shot systems; (ii) investigates the\ntranslation quality of two of the currently dominant neural architectures in\nMT, which are the Recurrent and the Transformer ones; and (iii) quantitatively\nexplores how the closeness between languages influences the zero-shot\ntranslation. Our analysis leverages multiple professional post-edits of\nautomatic translations by several different systems and focuses both on\nautomatic standard metrics (BLEU and TER) and on widely used error categories,\nwhich are lexical, morphology, and word order errors.", "published": "2018-06-18 21:18:18", "link": "http://arxiv.org/abs/1806.06957v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Co-training Embeddings of Knowledge Graphs and Entity Descriptions for\n  Cross-lingual Entity Alignment", "abstract": "Multilingual knowledge graph (KG) embeddings provide latent semantic\nrepresentations of entities and structured knowledge with cross-lingual\ninferences, which benefit various knowledge-driven cross-lingual NLP tasks.\nHowever, precisely learning such cross-lingual inferences is usually hindered\nby the low coverage of entity alignment in many KGs. Since many multilingual\nKGs also provide literal descriptions of entities, in this paper, we introduce\nan embedding-based approach which leverages a weakly aligned multilingual KG\nfor semi-supervised cross-lingual learning using entity descriptions. Our\napproach performs co-training of two embedding models, i.e. a multilingual KG\nembedding model and a multilingual literal description embedding model. The\nmodels are trained on a large Wikipedia-based trilingual dataset where most\nentity alignment is unknown to training. Experimental results show that the\nperformance of the proposed approach on the entity alignment task improves at\neach iteration of co-training, and eventually reaches a stage at which it\nsignificantly surpasses previous approaches. We also show that our approach has\npromising abilities for zero-shot entity alignment, and cross-lingual KG\ncompletion.", "published": "2018-06-18 02:06:46", "link": "http://arxiv.org/abs/1806.06478v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Unsupervised Word Segmentation from Speech with Attention", "abstract": "We present a first attempt to perform attentional word segmentation directly\nfrom the speech signal, with the final goal to automatically identify lexical\nunits in a low-resource, unwritten language (UL). Our methodology assumes a\npairing between recordings in the UL with translations in a well-resourced\nlanguage. It uses Acoustic Unit Discovery (AUD) to convert speech into a\nsequence of pseudo-phones that is segmented using neural soft-alignments\nproduced by a neural machine translation model. Evaluation uses an actual Bantu\nUL, Mboshi; comparisons to monolingual and bilingual baselines illustrate the\npotential of attentional word segmentation for language documentation.", "published": "2018-06-18 14:35:14", "link": "http://arxiv.org/abs/1806.06734v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Comparative Analysis of Neural QA models on SQuAD", "abstract": "The task of Question Answering has gained prominence in the past few decades\nfor testing the ability of machines to understand natural language. Large\ndatasets for Machine Reading have led to the development of neural models that\ncater to deeper language understanding compared to information retrieval tasks.\nDifferent components in these neural architectures are intended to tackle\ndifferent challenges. As a first step towards achieving generalization across\nmultiple domains, we attempt to understand and compare the peculiarities of\nexisting end-to-end neural models on the Stanford Question Answering Dataset\n(SQuAD) by performing quantitative as well as qualitative analysis of the\nresults attained by each of them. We observed that prediction errors reflect\ncertain model-specific biases, which we further discuss in this paper.", "published": "2018-06-18 22:29:51", "link": "http://arxiv.org/abs/1806.06972v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Semi-tied Units for Efficient Gating in LSTM and Highway Networks", "abstract": "Gating is a key technique used for integrating information from multiple\nsources by long short-term memory (LSTM) models and has recently also been\napplied to other models such as the highway network. Although gating is\npowerful, it is rather expensive in terms of both computation and storage as\neach gating unit uses a separate full weight matrix. This issue can be severe\nsince several gates can be used together in e.g. an LSTM cell. This paper\nproposes a semi-tied unit (STU) approach to solve this efficiency issue, which\nuses one shared weight matrix to replace those in all the units in the same\nlayer. The approach is termed \"semi-tied\" since extra parameters are used to\nseparately scale each of the shared output values. These extra scaling factors\nare associated with the network activation functions and result in the use of\nparameterised sigmoid, hyperbolic tangent, and rectified linear unit functions.\nSpeech recognition experiments using British English multi-genre broadcast data\nshowed that using STUs can reduce the calculation and storage cost by a factor\nof three for highway networks and four for LSTMs, while giving similar word\nerror rates to the original models.", "published": "2018-06-18 06:49:00", "link": "http://arxiv.org/abs/1806.06513v1", "categories": ["cs.CL", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Nonparametric Topic Modeling with Neural Inference", "abstract": "This work focuses on combining nonparametric topic models with Auto-Encoding\nVariational Bayes (AEVB). Specifically, we first propose iTM-VAE, where the\ntopics are treated as trainable parameters and the document-specific topic\nproportions are obtained by a stick-breaking construction. The inference of\niTM-VAE is modeled by neural networks such that it can be computed in a simple\nfeed-forward manner. We also describe how to introduce a hyper-prior into\niTM-VAE so as to model the uncertainty of the prior parameter. Actually, the\nhyper-prior technique is quite general and we show that it can be applied to\nother AEVB based models to alleviate the {\\it collapse-to-prior} problem\nelegantly. Moreover, we also propose HiTM-VAE, where the document-specific\ntopic distributions are generated in a hierarchical manner. HiTM-VAE is even\nmore flexible and can generate topic distributions with better variability.\nExperimental results on 20News and Reuters RCV1-V2 datasets show that the\nproposed models outperform the state-of-the-art baselines significantly. The\nadvantages of the hyper-prior technique and the hierarchical model construction\nare also confirmed by experiments.", "published": "2018-06-18 10:22:18", "link": "http://arxiv.org/abs/1806.06583v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GroupReduce: Block-Wise Low-Rank Approximation for Neural Language Model\n  Shrinking", "abstract": "Model compression is essential for serving large deep neural nets on devices\nwith limited resources or applications that require real-time responses. As a\ncase study, a state-of-the-art neural language model usually consists of one or\nmore recurrent layers sandwiched between an embedding layer used for\nrepresenting input tokens and a softmax layer for generating output tokens. For\nproblems with a very large vocabulary size, the embedding and the softmax\nmatrices can account for more than half of the model size. For instance, the\nbigLSTM model achieves state-of- the-art performance on the One-Billion-Word\n(OBW) dataset with around 800k vocabulary, and its word embedding and softmax\nmatrices use more than 6GBytes space, and are responsible for over 90% of the\nmodel parameters. In this paper, we propose GroupReduce, a novel compression\nmethod for neural language models, based on vocabulary-partition (block) based\nlow-rank matrix approximation and the inherent frequency distribution of tokens\n(the power-law distribution of words). The experimental results show our method\ncan significantly outperform traditional compression methods such as low-rank\napproximation and pruning. On the OBW dataset, our method achieved 6.6 times\ncompression rate for the embedding and softmax matrices, and when combined with\nquantization, our method can achieve 26 times compression rate, which\ntranslates to a factor of 12.8 times compression for the entire model with very\nlittle degradation in perplexity.", "published": "2018-06-18 23:08:15", "link": "http://arxiv.org/abs/1806.06950v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A Weighted Superposition of Functional Contours Model for Modelling\n  Contextual Prominence of Elementary Prosodic Contours", "abstract": "The way speech prosody encodes linguistic, paralinguistic and non-linguistic\ninformation via multiparametric representations of the speech signals is still\nan open issue. The Superposition of Functional Contours (SFC) model proposes to\ndecompose prosody into elementary multiparametric functional contours through\nthe iterative training of neural network contour generators using\nanalysis-by-synthesis. Each generator is responsible for computing\nmultiparametric contours that encode one given linguistic, paralinguistic and\nnon-linguistic information on a variable scope of rhythmic units. The\ncontributions of all generators' outputs are then overlapped and added to\nproduce the prosody of the utterance. We propose an extension of the contour\ngenerators that allows them to model the prominence of the elementary contours\nbased on contextual information. WSFC jointly learns the patterns of the\nelementary multiparametric functional contours and their weights dependent on\nthe contours' contexts. The experimental results show that the proposed\nweighted SFC (WSFC) model can successfully capture contour prominence and thus\nimprove SFC modelling performance. The WSFC is also shown to be effective at\nmodelling the impact of attitudes on the prominence of functional contours\ncuing syntactic relations in French, and that of emphasis on the prominence of\ntone contours in Chinese.", "published": "2018-06-18 15:41:41", "link": "http://arxiv.org/abs/1806.06779v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards multi-instrument drum transcription", "abstract": "Automatic drum transcription, a subtask of the more general automatic music\ntranscription, deals with extracting drum instrument note onsets from an audio\nsource. Recently, progress in transcription performance has been made using\nnon-negative matrix factorization as well as deep learning methods. However,\nthese works primarily focus on transcribing three drum instruments only: snare\ndrum, bass drum, and hi-hat. Yet, for many applications, the ability to\ntranscribe more drum instruments which make up standard drum kits used in\nwestern popular music would be desirable. In this work, convolutional and\nconvolutional recurrent neural networks are trained to transcribe a wider range\nof drum instruments. First, the shortcomings of publicly available datasets in\nthis context are discussed. To overcome these limitations, a larger synthetic\ndataset is introduced. Then, methods to train models using the new dataset\nfocusing on generalization to real world data are investigated. Finally, the\ntrained models are evaluated on publicly available datasets and results are\ndiscussed. The contributions of this work comprise: (i.) a large-scale\nsynthetic dataset for drum transcription, (ii.) first steps towards an\nautomatic drum transcription system that supports a larger range of instruments\nby evaluating and discussing training setups and the impact of datasets in this\ncontext, and (iii.) a publicly available set of trained models for drum\ntranscription. Additional materials are available at\nhttp://ifs.tuwien.ac.at/~vogl/dafx2018", "published": "2018-06-18 13:45:48", "link": "http://arxiv.org/abs/1806.06676v2", "categories": ["cs.SD", "cs.IR", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards an efficient deep learning model for musical onset detection", "abstract": "In this paper, we propose an efficient and reproducible deep learning model\nfor musical onset detection (MOD). We first review the state-of-the-art deep\nlearning models for MOD, and identify their shortcomings and challenges: (i)\nthe lack of hyper-parameter tuning details, (ii) the non-availability of code\nfor training models on other datasets, and (iii) ignoring the network\ncapability when comparing different architectures. Taking the above issues into\naccount, we experiment with seven deep learning architectures. The most\nefficient one achieves equivalent performance to our implementation of the\nstate-of-the-art architecture. However, it has only 28.3% of the total number\nof trainable parameters compared to the state-of-the-art. Our experiments are\nconducted using two different datasets: one mainly consists of instrumental\nmusic excerpts, and another developed by ourselves includes only solo singing\nvoice excerpts. Further, inter-dataset transfer learning experiments are\nconducted. The results show that the model pre-trained on one dataset fails to\ndetect onsets on another dataset, which denotes the importance of providing the\nimplementation code to enable re-training the model for a different dataset.\nDatasets, code and a Jupyter notebook running on Google Colab are publicly\navailable to make this research understandable and easy to reproduce.", "published": "2018-06-18 15:30:35", "link": "http://arxiv.org/abs/1806.06773v2", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Frequency domain variants of velvet noise and their application to\n  speech processing and synthesis: with appendices", "abstract": "We propose a new excitation source signal for VOCODERs and an all-pass\nimpulse response for post-processing of synthetic sounds and pre-processing of\nnatural sounds for data-augmentation. The proposed signals are variants of\nvelvet noise, which is a sparse discrete signal consisting of a few non-zero (1\nor -1) elements and sounds smoother than Gaussian white noise. One of the\nproposed variants, FVN (Frequency domain Velvet Noise) applies the procedure to\ngenerate a velvet noise on the cyclic frequency domain of DFT (Discrete Fourier\nTransform). Then, by smoothing the generated signal to design the phase of an\nall-pass filter followed by inverse Fourier transform yields the proposed FVN.\nTemporally variable frequency weighted mixing of FVN generated by frozen and\nshuffled random number provides a unified excitation signal which can span from\nrandom noise to a repetitive pulse train. The other variant, which is an\nall-pass impulse response, significantly reduces \"buzzy\" impression of VOCODER\noutput by filtering. Finally, we will discuss applications of the proposed\nsignal for watermarking and psychoacoustic research.", "published": "2018-06-18 16:35:47", "link": "http://arxiv.org/abs/1806.06812v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
