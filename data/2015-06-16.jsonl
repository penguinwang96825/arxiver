{"title": "Parsing Natural Language Sentences by Semi-supervised Methods", "abstract": "We present our work on semi-supervised parsing of natural language sentences,\nfocusing on multi-source crosslingual transfer of delexicalized dependency\nparsers. We first evaluate the influence of treebank annotation styles on\nparsing performance, focusing on adposition attachment style. Then, we present\nKLcpos3, an empirical language similarity measure, designed and tuned for\nsource parser weighting in multi-source delexicalized parser transfer. And\nfinally, we introduce a novel resource combination method, based on\ninterpolation of trained parser models.", "published": "2015-06-16 09:54:27", "link": "http://arxiv.org/abs/1506.04897v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Recognize Foreign Low-Frequency Words with Similar Pairs", "abstract": "Low-frequency words place a major challenge for automatic speech recognition\n(ASR). The probabilities of these words, which are often important name\nentities, are generally under-estimated by the language model (LM) due to their\nlimited occurrences in the training data. Recently, we proposed a word-pair\napproach to deal with the problem, which borrows information of frequent words\nto enhance the probabilities of low-frequency words. This paper presents an\nextension to the word-pair method by involving multiple `predicting words' to\nproduce better estimation for low-frequency words. We also employ this approach\nto deal with out-of-language words in the task of multi-lingual speech\nrecognition.", "published": "2015-06-16 12:31:43", "link": "http://arxiv.org/abs/1506.04940v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Text and Network Context for Geolocation of Social Media\n  Users", "abstract": "Research on automatically geolocating social media users has conventionally\nbeen based on the text content of posts from a given user or the social network\nof the user, with very little crossover between the two, and no bench-marking\nof the two approaches over compara- ble datasets. We bring the two threads of\nresearch together in first proposing a text-based method based on adaptive\ngrids, followed by a hybrid network- and text-based method. Evaluating over\nthree Twitter datasets, we show that the empirical difference between text- and\nnetwork-based methods is not great, and that hybridisation of the two is\nsuperior to the component methods, especially in contexts where the user graph\nis not well connected. We achieve state-of-the-art results on all three\ndatasets.", "published": "2015-06-16 00:32:33", "link": "http://arxiv.org/abs/1506.04803v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Significance of the levels of spectral valleys with application to\n  front/back distinction of vowel sounds", "abstract": "An objective critical distance (OCD) has been defined as that spacing between\nadjacent formants, when the level of the valley between them reaches the mean\nspectral level. The measured OCD lies in the same range (viz., 3-3.5 bark) as\nthe critical distance determined by subjective experiments for similar\nexperimental conditions. The level of spectral valley serves a purpose similar\nto that of the spacing between the formants with an added advantage that it can\nbe measured from the spectral envelope without an explicit knowledge of formant\nfrequencies. Based on the relative spacing of formant frequencies, the level of\nthe spectral valley, VI (between F1 and F2) is much higher than the level of\nVII (spectral valley between F2 and F3) for back vowels and vice-versa for\nfront vowels. Classification of vowels into front/back distinction with the\ndifference (VI-VII) as an acoustic feature, tested using TIMIT, NTIMIT, Tamil\nand Kannada language databases gives, on the average, an accuracy of about 95%,\nwhich is comparable to the accuracy (90.6%) obtained using a neural network\nclassifier trained and tested using MFCC as the feature vector for TIMIT\ndatabase. The acoustic feature (VI-VII) has also been tested for its robustness\non the TIMIT database for additive white and babble noise and an accuracy of\nabout 95% has been obtained for SNRs down to 25 dB for both types of noise.", "published": "2015-06-16 04:03:06", "link": "http://arxiv.org/abs/1506.04828v2", "categories": ["cs.CL", "cs.SD"], "primary_category": "cs.CL"}
{"title": "Tree-structured composition in neural networks without tree-structured\n  architectures", "abstract": "Tree-structured neural networks encode a particular tree geometry for a\nsentence in the network design. However, these models have at best only\nslightly outperformed simpler sequence-based models. We hypothesize that neural\nsequence models like LSTMs are in fact able to discover and implicitly use\nrecursive compositional structure, at least for tasks with clear cues to that\nstructure in the data. We demonstrate this possibility using an artificial data\ntask for which recursive compositional structure is crucial, and find an\nLSTM-based sequence model can indeed learn to exploit the underlying tree\nstructure. However, its performance consistently lags behind that of tree\nmodels, even on large training sets, suggesting that tree-structured models are\nmore effective at exploiting recursive structure.", "published": "2015-06-16 05:12:52", "link": "http://arxiv.org/abs/1506.04834v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Author Identification using Multi-headed Recurrent Neural Networks", "abstract": "Recurrent neural networks (RNNs) are very good at modelling the flow of text,\nbut typically need to be trained on a far larger corpus than is available for\nthe PAN 2015 Author Identification task. This paper describes a novel approach\nwhere the output layer of a character-level RNN language model is split into\nseveral independent predictive sub-models, each representing an author, while\nthe recurrent layer is shared by all. This allows the recurrent layer to model\nthe language as a whole without over-fitting, while the outputs select aspects\nof the underlying model that reflect their author's style. The method proves\ncompetitive, ranking first in two of the four languages.", "published": "2015-06-16 09:41:55", "link": "http://arxiv.org/abs/1506.04891v2", "categories": ["cs.CL", "cs.LG", "cs.NE", "68T50"], "primary_category": "cs.CL"}
{"title": "Emotion Analysis of Songs Based on Lyrical and Audio Features", "abstract": "In this paper, a method is proposed to detect the emotion of a song based on\nits lyrical and audio features. Lyrical features are generated by segmentation\nof lyrics during the process of data extraction. ANEW and WordNet knowledge is\nthen incorporated to compute Valence and Arousal values. In addition to this,\nlinguistic association rules are applied to ensure that the issue of ambiguity\nis properly addressed. Audio features are used to supplement the lyrical ones\nand include attributes like energy, tempo, and danceability. These features are\nextracted from The Echo Nest, a widely used music intelligence platform.\nConstruction of training and test sets is done on the basis of social tags\nextracted from the last.fm website. The classification is done by applying\nfeature weighting and stepwise threshold reduction on the k-Nearest Neighbors\nalgorithm to provide fuzziness in the classification.", "published": "2015-06-16 16:04:08", "link": "http://arxiv.org/abs/1506.05012v1", "categories": ["cs.CL", "cs.AI", "cs.SD"], "primary_category": "cs.CL"}
