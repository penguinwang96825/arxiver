{"title": "LSTM based Conversation Models", "abstract": "In this paper, we present a conversational model that incorporates both\ncontext and participant role for two-party conversations. Different\narchitectures are explored for integrating participant role and context\ninformation into a Long Short-term Memory (LSTM) language model. The\nconversational model can function as a language model or a language generation\nmodel. Experiments on the Ubuntu Dialog Corpus show that our model can capture\nmultiple turn interaction between participants. The proposed method outperforms\na traditional LSTM model as measured by language model perplexity and response\nranking. Generated responses show characteristic differences between the two\nparticipant roles.", "published": "2016-03-31 05:14:10", "link": "http://arxiv.org/abs/1603.09457v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Attention Models for Sequence Classification: Analysis and\n  Application to Key Term Extraction and Dialogue Act Detection", "abstract": "Recurrent neural network architectures combining with attention mechanism, or\nneural attention model, have shown promising performance recently for the tasks\nincluding speech recognition, image caption generation, visual question\nanswering and machine translation. In this paper, neural attention model is\napplied on two sequence classification tasks, dialogue act detection and key\nterm extraction. In the sequence labeling tasks, the model input is a sequence,\nand the output is the label of the input sequence. The major difficulty of\nsequence labeling is that when the input sequence is long, it can include many\nnoisy or irrelevant part. If the information in the whole sequence is treated\nequally, the noisy or irrelevant part may degrade the classification\nperformance. The attention mechanism is helpful for sequence classification\ntask because it is capable of highlighting important part among the entire\nsequence for the classification task. The experimental results show that with\nthe attention mechanism, discernible improvements were achieved in the sequence\nlabeling task considered here. The roles of the attention mechanism in the\ntasks are further analyzed and visualized in this paper.", "published": "2016-03-31 23:17:46", "link": "http://arxiv.org/abs/1604.00077v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "System Combination for Short Utterance Speaker Recognition", "abstract": "For text-independent short-utterance speaker recognition (SUSR), the\nperformance often degrades dramatically. This paper presents a combination\napproach to the SUSR tasks with two phonetic-aware systems: one is the\nDNN-based i-vector system and the other is our recently proposed\nsubregion-based GMM-UBM system. The former employs phone posteriors to\nconstruct an i-vector model in which the shared statistics offers stronger\nrobustness against limited test data, while the latter establishes a\nphone-dependent GMM-UBM system which represents speaker characteristics with\nmore details. A score-level fusion is implemented to integrate the respective\nadvantages from the two systems. Experimental results show that for the\ntext-independent SUSR task, both the DNN-based i-vector system and the\nsubregion-based GMM-UBM system outperform their respective baselines, and the\nscore-level system combination delivers performance improvement.", "published": "2016-03-31 05:47:03", "link": "http://arxiv.org/abs/1603.09460v2", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Differentiable Pooling for Unsupervised Acoustic Model Adaptation", "abstract": "We present a deep neural network (DNN) acoustic model that includes\nparametrised and differentiable pooling operators. Unsupervised acoustic model\nadaptation is cast as the problem of updating the decision boundaries\nimplemented by each pooling operator. In particular, we experiment with two\ntypes of pooling parametrisations: learned $L_p$-norm pooling and weighted\nGaussian pooling, in which the weights of both operators are treated as\nspeaker-dependent. We perform investigations using three different large\nvocabulary speech recognition corpora: AMI meetings, TED talks and Switchboard\nconversational telephone speech. We demonstrate that differentiable pooling\noperators provide a robust and relatively low-dimensional way to adapt acoustic\nmodels, with relative word error rates reductions ranging from 5--20% with\nrespect to unadapted systems, which themselves are better than the baseline\nfully-connected DNN-based acoustic models. We also investigate how the proposed\ntechniques work under various adaptation conditions including the quality of\nadaptation data and complementarity to other feature- and model-space\nadaptation methods, as well as providing an analysis of the characteristics of\neach of the proposed approaches.", "published": "2016-03-31 15:10:40", "link": "http://arxiv.org/abs/1603.09630v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Data Collection for Interactive Learning through the Dialog", "abstract": "This paper presents a dataset collected from natural dialogs which enables to\ntest the ability of dialog systems to learn new facts from user utterances\nthroughout the dialog. This interactive learning will help with one of the most\nprevailing problems of open domain dialog system, which is the sparsity of\nfacts a dialog system can reason about. The proposed dataset, consisting of\n1900 collected dialogs, allows simulation of an interactive gaining of\ndenotations and questions explanations from users which can be used for the\ninteractive learning.", "published": "2016-03-31 15:13:51", "link": "http://arxiv.org/abs/1603.09631v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Language Correction with Character-Based Attention", "abstract": "Natural language correction has the potential to help language learners\nimprove their writing skills. While approaches with separate classifiers for\ndifferent error types have high precision, they do not flexibly handle errors\nsuch as redundancy or non-idiomatic phrasing. On the other hand, word and\nphrase-based machine translation methods are not designed to cope with\northographic errors, and have recently been outpaced by neural models.\nMotivated by these issues, we present a neural network-based approach to\nlanguage correction. The core component of our method is an encoder-decoder\nrecurrent neural network with an attention mechanism. By operating at the\ncharacter level, the network avoids the problem of out-of-vocabulary words. We\nillustrate the flexibility of our approach on dataset of noisy, user-generated\ntext collected from an English learner forum. When combined with a language\nmodel, our method achieves a state-of-the-art $F_{0.5}$-score on the CoNLL 2014\nShared Task. We further demonstrate that training the network on additional\ndata with synthesized errors can improve performance.", "published": "2016-03-31 19:16:54", "link": "http://arxiv.org/abs/1603.09727v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Multiscale Features Directly From Waveforms", "abstract": "Deep learning has dramatically improved the performance of speech recognition\nsystems through learning hierarchies of features optimized for the task at\nhand. However, true end-to-end learning, where features are learned directly\nfrom waveforms, has only recently reached the performance of hand-tailored\nrepresentations based on the Fourier transform. In this paper, we detail an\napproach to use convolutional filters to push past the inherent tradeoff of\ntemporal and frequency resolution that exists for spectral representations. At\nincreased computational cost, we show that increasing temporal resolution via\nreduced stride and increasing frequency resolution via additional filters\ndelivers significant performance improvements. Further, we find more efficient\nrepresentations by simultaneously learning at multiple scales, leading to an\noverall decrease in word error rate on a difficult internal speech test set by\n20.7% relative to networks with the same number of parameters trained on\nspectrograms.", "published": "2016-03-31 09:54:44", "link": "http://arxiv.org/abs/1603.09509v2", "categories": ["cs.CL", "cs.LG", "cs.NE", "cs.SD"], "primary_category": "cs.CL"}
{"title": "Multi-task Recurrent Model for Speech and Speaker Recognition", "abstract": "Although highly correlated, speech and speaker recognition have been regarded\nas two independent tasks and studied by two communities. This is certainly not\nthe way that people behave: we decipher both speech content and speaker traits\nat the same time. This paper presents a unified model to perform speech and\nspeaker recognition simultaneously and altogether. The model is based on a\nunified neural network where the output of one task is fed to the input of the\nother, leading to a multi-task recurrent network. Experiments show that the\njoint model outperforms the task-specific models on both the two tasks.", "published": "2016-03-31 15:37:29", "link": "http://arxiv.org/abs/1603.09643v4", "categories": ["cs.CL", "cs.LG", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
