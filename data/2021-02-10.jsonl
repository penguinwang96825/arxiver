{"title": "Biomedical Question Answering: A Survey of Approaches and Challenges", "abstract": "Automatic Question Answering (QA) has been successfully applied in various\ndomains such as search engines and chatbots. Biomedical QA (BQA), as an\nemerging QA task, enables innovative applications to effectively perceive,\naccess and understand complex biomedical knowledge. There have been tremendous\ndevelopments of BQA in the past two decades, which we classify into 5\ndistinctive approaches: classic, information retrieval, machine reading\ncomprehension, knowledge base and question entailment approaches. In this\nsurvey, we introduce available datasets and representative methods of each BQA\napproach in detail. Despite the developments, BQA systems are still immature\nand rarely used in real-life settings. We identify and characterize several key\nchallenges in BQA that might lead to this issue, and discuss some potential\nfuture directions to explore.", "published": "2021-02-10 06:16:35", "link": "http://arxiv.org/abs/2102.05281v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NUVA: A Naming Utterance Verifier for Aphasia Treatment", "abstract": "Anomia (word-finding difficulties) is the hallmark of aphasia, an acquired\nlanguage disorder most commonly caused by stroke. Assessment of speech\nperformance using picture naming tasks is a key method for both diagnosis and\nmonitoring of responses to treatment interventions by people with aphasia\n(PWA). Currently, this assessment is conducted manually by speech and language\ntherapists (SLT). Surprisingly, despite advancements in automatic speech\nrecognition (ASR) and artificial intelligence with technologies like deep\nlearning, research on developing automated systems for this task has been\nscarce. Here we present NUVA, an utterance verification system incorporating a\ndeep learning element that classifies 'correct' versus' incorrect' naming\nattempts from aphasic stroke patients. When tested on eight native\nBritish-English speaking PWA the system's performance accuracy ranged between\n83.6% to 93.6%, with a 10-fold cross-validation mean of 89.5%. This performance\nwas not only significantly better than a baseline created for this study using\none of the leading commercially available ASRs (Google speech-to-text service)\nbut also comparable in some instances with two independent SLT ratings for the\nsame dataset.", "published": "2021-02-10 13:00:29", "link": "http://arxiv.org/abs/2102.05408v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Synthetic Text Data to Evaluate Causal Inference Methods", "abstract": "Drawing causal conclusions from observational data requires making\nassumptions about the true data-generating process. Causal inference research\ntypically considers low-dimensional data, such as categorical or numerical\nfields in structured medical records. High-dimensional and unstructured data\nsuch as natural language complicates the evaluation of causal inference\nmethods; such evaluations rely on synthetic datasets with known causal effects.\nModels for natural language generation have been widely studied and perform\nwell empirically. However, existing methods not immediately applicable to\nproducing synthetic datasets for causal evaluations, as they do not allow for\nquantifying a causal effect on the text itself. In this work, we develop a\nframework for adapting existing generation models to produce synthetic text\ndatasets with known causal effects. We use this framework to perform an\nempirical comparison of four recently-proposed methods for estimating causal\neffects from text data. We release our code and synthetic datasets.", "published": "2021-02-10 18:53:11", "link": "http://arxiv.org/abs/2102.05638v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Differentiable Generative Phonology", "abstract": "The goal of generative phonology, as formulated by Chomsky and Halle (1968),\nis to specify a formal system that explains the set of attested phonological\nstrings in a language. Traditionally, a collection of rules (or constraints, in\nthe case of optimality theory) and underlying forms (UF) are posited to work in\ntandem to generate phonological strings. However, the degree of abstraction of\nUFs with respect to their concrete realizations is contentious. As the main\ncontribution of our work, we implement the phonological generative system as a\nneural model differentiable end-to-end, rather than as a set of rules or\nconstraints. Contrary to traditional phonology, in our model, UFs are\ncontinuous vectors in $\\mathbb{R}^d$, rather than discrete strings. As a\nconsequence, UFs are discovered automatically rather than posited by linguists,\nand the model can scale to the size of a realistic vocabulary. Moreover, we\ncompare several modes of the generative process, contemplating: i) the presence\nor absence of an underlying representation in between morphemes and surface\nforms (SFs); and ii) the conditional dependence or independence of UFs with\nrespect to SFs. We evaluate the ability of each mode to predict attested\nphonological strings on 2 datasets covering 5 and 28 languages, respectively.\nThe results corroborate two tenets of generative phonology, viz. the necessity\nfor UFs and their independence from SFs. In general, our neural model of\ngenerative phonology learns both UFs and SFs automatically and on a\nlarge-scale.", "published": "2021-02-10 19:50:16", "link": "http://arxiv.org/abs/2102.05717v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fused Acoustic and Text Encoding for Multimodal Bilingual Pretraining\n  and Speech Translation", "abstract": "Recently, representation learning for text and speech has successfully\nimproved many language related tasks. However, all existing methods suffer from\ntwo limitations: (a) they only learn from one input modality, while a unified\nrepresentation for both speech and text is needed by tasks such as end-to-end\nspeech translation, and as a result,(b) they can not exploit various\nlarge-scale text and speech data and their performance is limited by the\nscarcity of parallel speech translation data.To address these problems, we\npropose a Fused Acoustic and Text Masked Language Model (FAT-MLM) which jointly\nlearns a unified representation for both acoustic and text input from various\ntypes of corpora including parallel data for speech recognition and machine\ntranslation, and even pure speech and text data. Within this cross-modal\nrepresentation learning framework, we further present an end-to-end model for\nFused Acoustic and Text Speech Translation (FAT-ST). Experiments on three\ntranslation directions show that by fine-tuning from FAT-MLM, our proposed\nspeech translation models substantially improve translation quality by up to\n+5.9 BLEU.", "published": "2021-02-10 22:53:40", "link": "http://arxiv.org/abs/2102.05766v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SensPick: Sense Picking for Word Sense Disambiguation", "abstract": "Word sense disambiguation (WSD) methods identify the most suitable meaning of\na word with respect to the usage of that word in a specific context. Neural\nnetwork-based WSD approaches rely on a sense-annotated corpus since they do not\nutilize lexical resources. In this study, we utilize both context and related\ngloss information of a target word to model the semantic relationship between\nthe word and the set of glosses. We propose SensPick, a type of stacked\nbidirectional Long Short Term Memory (LSTM) network to perform the WSD task.\nThe experimental evaluation demonstrates that SensPick outperforms traditional\nand state-of-the-art models on most of the benchmark datasets with a relative\nimprovement of 3.5% in F-1 score. While the improvement is not significant,\nincorporating semantic relationships brings SensPick in the leading position\ncompared to others.", "published": "2021-02-10 04:52:42", "link": "http://arxiv.org/abs/2102.05260v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Language Models for Lexical Inference in Context", "abstract": "Lexical inference in context (LIiC) is the task of recognizing textual\nentailment between two very similar sentences, i.e., sentences that only differ\nin one expression. It can therefore be seen as a variant of the natural\nlanguage inference task that is focused on lexical semantics. We formulate and\nevaluate the first approaches based on pretrained language models (LMs) for\nthis task: (i) a few-shot NLI classifier, (ii) a relation induction approach\nbased on handcrafted patterns expressing the semantics of lexical inference,\nand (iii) a variant of (ii) with patterns that were automatically extracted\nfrom a corpus. All our approaches outperform the previous state of the art,\nshowing the potential of pretrained LMs for LIiC. In an extensive analysis, we\ninvestigate factors of success and failure of our three approaches.", "published": "2021-02-10 09:08:22", "link": "http://arxiv.org/abs/2102.05331v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-turn Dialogue Reading Comprehension with Pivot Turns and Knowledge", "abstract": "Multi-turn dialogue reading comprehension aims to teach machines to read\ndialogue contexts and solve tasks such as response selection and answering\nquestions. The major challenges involve noisy history contexts and especial\nprerequisites of commonsense knowledge that is unseen in the given material.\nExisting works mainly focus on context and response matching approaches. This\nwork thus makes the first attempt to tackle the above two challenges by\nextracting substantially important turns as pivot utterances and utilizing\nexternal knowledge to enhance the representation of context. We propose a\npivot-oriented deep selection model (PoDS) on top of the Transformer-based\nlanguage models for dialogue comprehension. In detail, our model first picks\nout the pivot utterances from the conversation history according to the\nsemantic matching with the candidate response or question, if any. Besides,\nknowledge items related to the dialogue context are extracted from a knowledge\ngraph as external knowledge. Then, the pivot utterances and the external\nknowledge are combined with a well-designed mechanism for refining predictions.\nExperimental results on four dialogue comprehension benchmark tasks show that\nour proposed model achieves great improvements on baselines. A series of\nempirical comparisons are conducted to show how our selection strategies and\nthe extra knowledge injection influence the results.", "published": "2021-02-10 15:00:12", "link": "http://arxiv.org/abs/2102.05474v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards More Fine-grained and Reliable NLP Performance Prediction", "abstract": "Performance prediction, the task of estimating a system's performance without\nperforming experiments, allows us to reduce the experimental burden caused by\nthe combinatorial explosion of different datasets, languages, tasks, and\nmodels. In this paper, we make two contributions to improving performance\nprediction for NLP tasks. First, we examine performance predictors not only for\nholistic measures of accuracy like F1 or BLEU but also fine-grained performance\nmeasures such as accuracy over individual classes of examples. Second, we\npropose methods to understand the reliability of a performance prediction model\nfrom two angles: confidence intervals and calibration. We perform an analysis\nof four types of NLP tasks, and both demonstrate the feasibility of\nfine-grained performance prediction and the necessity to perform reliability\nanalysis for performance prediction methods in the future. We make our code\npublicly available: \\url{https://github.com/neulab/Reliable-NLPPP}", "published": "2021-02-10 15:23:20", "link": "http://arxiv.org/abs/2102.05486v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Argmax Flows and Multinomial Diffusion: Learning Categorical\n  Distributions", "abstract": "Generative flows and diffusion models have been predominantly trained on\nordinal data, for example natural images. This paper introduces two extensions\nof flows and diffusion for categorical data such as language or image\nsegmentation: Argmax Flows and Multinomial Diffusion. Argmax Flows are defined\nby a composition of a continuous distribution (such as a normalizing flow), and\nan argmax function. To optimize this model, we learn a probabilistic inverse\nfor the argmax that lifts the categorical data to a continuous space.\nMultinomial Diffusion gradually adds categorical noise in a diffusion process,\nfor which the generative denoising process is learned. We demonstrate that our\nmethod outperforms existing dequantization approaches on text modelling and\nmodelling on image segmentation maps in log-likelihood.", "published": "2021-02-10 11:04:17", "link": "http://arxiv.org/abs/2102.05379v3", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Customizing Contextualized Language Models forLegal Document Reviews", "abstract": "Inspired by the inductive transfer learning on computer vision, many efforts\nhave been made to train contextualized language models that boost the\nperformance of natural language processing tasks. These models are mostly\ntrained on large general-domain corpora such as news, books, or\nWikipedia.Although these pre-trained generic language models well perceive the\nsemantic and syntactic essence of a language structure, exploiting them in a\nreal-world domain-specific scenario still needs some practical considerations\nto be taken into account such as token distribution shifts, inference time,\nmemory, and their simultaneous proficiency in multiple tasks. In this paper, we\nfocus on the legal domain and present how different language model strained on\ngeneral-domain corpora can be best customized for multiple legal document\nreviewing tasks. We compare their efficiencies with respect to task\nperformances and present practical considerations.", "published": "2021-02-10 22:14:15", "link": "http://arxiv.org/abs/2102.05757v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Privacy-Preserving Graph Convolutional Networks for Text Classification", "abstract": "Graph convolutional networks (GCNs) are a powerful architecture for\nrepresentation learning on documents that naturally occur as graphs, e.g.,\ncitation or social networks. However, sensitive personal information, such as\ndocuments with people's profiles or relationships as edges, are prone to\nprivacy leaks, as the trained model might reveal the original input. Although\ndifferential privacy (DP) offers a well-founded privacy-preserving framework,\nGCNs pose theoretical and practical challenges due to their training specifics.\nWe address these challenges by adapting differentially-private gradient-based\ntraining to GCNs and conduct experiments using two optimizers on five NLP\ndatasets in two languages. We propose a simple yet efficient method based on\nrandom graph splits that not only improves the baseline privacy bounds by a\nfactor of 2.7 while retaining competitive F1 scores, but also provides strong\nprivacy guarantees of epsilon = 1.0. We show that, under certain modeling\nchoices, privacy-preserving GCNs perform up to 90% of their non-private\nvariants, while formally guaranteeing strong privacy measures.", "published": "2021-02-10 15:27:38", "link": "http://arxiv.org/abs/2102.09604v3", "categories": ["cs.SI", "cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Low-Complexity, Real-Time Joint Neural Echo Control and Speech\n  Enhancement Based On PercepNet", "abstract": "Speech enhancement algorithms based on deep learning have greatly surpassed\ntheir traditional counterparts and are now being considered for the task of\nremoving acoustic echo from hands-free communication systems. This is a\nchallenging problem due to both real-world constraints like loudspeaker\nnon-linearities, and to limited compute capabilities in some communication\nsystems. In this work, we propose a system combining a traditional acoustic\necho canceller, and a low-complexity joint residual echo and noise suppressor\nbased on a hybrid signal processing/deep neural network (DSP/DNN) approach. We\nshow that the proposed system outperforms both traditional and other neural\napproaches, while requiring only 5.5% CPU for real-time operation. We further\nshow that the system can scale to even lower complexity levels.", "published": "2021-02-10 03:44:25", "link": "http://arxiv.org/abs/2102.05245v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "VACE-WPE: Virtual Acoustic Channel Expansion Based On Neural Networks\n  for Weighted Prediction Error-Based Speech Dereverberation", "abstract": "Speech dereverberation is an important issue for many real-world speech\nprocessing applications. Among the techniques developed, the weighted\nprediction error (WPE) algorithm has been widely adopted and advanced over the\nlast decade, which blindly cancels out the late reverberation component from\nthe reverberant mixture of microphone signals. In this study, we extend the\nneural-network-based virtual acoustic channel expansion (VACE) framework for\nthe WPE-based speech dereverberation, a variant of the WPE that we recently\nproposed to enable the use of dual-channel WPE algorithm in a single-microphone\nspeech dereverberation scenario. Based on the previous study, some ablation\nstudies are conducted regarding the constituents of the VACE-WPE in an offline\nprocessing scenario. These studies help understand the dynamics of the system,\nthereby simplifying the architecture and leading to the introduction of new\nstrategies for training the neural network for the VACE. Experimental results\nin noisy reverberant environments reveal that VACE-WPE considerably outperforms\nits single-channel counterpart in terms of objective speech quality and is\ncomplementary to the single-channel WPE when employed as the front-end for the\nfar-field automatic speech recognizer.", "published": "2021-02-10 04:40:47", "link": "http://arxiv.org/abs/2102.05259v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Exploring Automatic COVID-19 Diagnosis via voice and symptoms from\n  Crowdsourced Data", "abstract": "The development of fast and accurate screening tools, which could facilitate\ntesting and prevent more costly clinical tests, is key to the current pandemic\nof COVID-19. In this context, some initial work shows promise in detecting\ndiagnostic signals of COVID-19 from audio sounds. In this paper, we propose a\nvoice-based framework to automatically detect individuals who have tested\npositive for COVID-19. We evaluate the performance of the proposed framework on\na subset of data crowdsourced from our app, containing 828 samples from 343\nparticipants. By combining voice signals and reported symptoms, an AUC of\n$0.79$ has been attained, with a sensitivity of $0.68$ and a specificity of\n$0.82$. We hope that this study opens the door to rapid, low-cost, and\nconvenient pre-screening tools to automatically detect the disease.", "published": "2021-02-10 02:28:16", "link": "http://arxiv.org/abs/2102.05225v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Voice Cloning: a Multi-Speaker Text-to-Speech Synthesis Approach based\n  on Transfer Learning", "abstract": "Deep learning models are becoming predominant in many fields of machine\nlearning. Text-to-Speech (TTS), the process of synthesizing artificial speech\nfrom text, is no exception. To this end, a deep neural network is usually\ntrained using a corpus of several hours of recorded speech from a single\nspeaker. Trying to produce the voice of a speaker other than the one learned is\nexpensive and requires large effort since it is necessary to record a new\ndataset and retrain the model. This is the main reason why the TTS models are\nusually single speaker. The proposed approach has the goal to overcome these\nlimitations trying to obtain a system which is able to model a multi-speaker\nacoustic space. This allows the generation of speech audio similar to the voice\nof different target speakers, even if they were not observed during the\ntraining phase.", "published": "2021-02-10 18:43:56", "link": "http://arxiv.org/abs/2102.05630v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-Supervised VQ-VAE for One-Shot Music Style Transfer", "abstract": "Neural style transfer, allowing to apply the artistic style of one image to\nanother, has become one of the most widely showcased computer vision\napplications shortly after its introduction. In contrast, related tasks in the\nmusic audio domain remained, until recently, largely untackled. While several\nstyle conversion methods tailored to musical signals have been proposed, most\nlack the 'one-shot' capability of classical image style transfer algorithms. On\nthe other hand, the results of existing one-shot audio style transfer methods\non musical inputs are not as compelling. In this work, we are specifically\ninterested in the problem of one-shot timbre transfer. We present a novel\nmethod for this task, based on an extension of the vector-quantized variational\nautoencoder (VQ-VAE), along with a simple self-supervised learning strategy\ndesigned to obtain disentangled representations of timbre and pitch. We\nevaluate the method using a set of objective metrics and show that it is able\nto outperform selected baselines.", "published": "2021-02-10 21:42:49", "link": "http://arxiv.org/abs/2102.05749v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "ABSP System for The Third DIHARD Challenge", "abstract": "This report describes the speaker diarization system developed by the ABSP\nLaboratory team for the third DIHARD speech diarization challenge. Our primary\ncontribution is to develop acoustic domain identification (ADI) system for\nspeaker diarization. We investigate speaker embeddings based ADI system. We\napply a domain-dependent threshold for agglomerative hierarchical clustering.\nBesides, we optimize the parameters for PCA-based dimensionality reduction in a\ndomain-dependent way. Our method of integrating domain-based processing schemes\nin the baseline system of the challenge achieved a relative improvement of\n$9.63\\%$ and $10.64\\%$ in DER for core and full conditions, respectively, for\nTrack 1 of the DIHARD III evaluation set.", "published": "2021-02-10 19:11:01", "link": "http://arxiv.org/abs/2102.09939v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
