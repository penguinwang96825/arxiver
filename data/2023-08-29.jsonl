{"title": "TransPrompt v2: A Transferable Prompting Framework for Cross-task Text\n  Classification", "abstract": "Text classification is one of the most imperative tasks in natural language\nprocessing (NLP). Recent advances with pre-trained language models (PLMs) have\nshown remarkable success on this task. However, the satisfying results obtained\nby PLMs heavily depend on the large amounts of task-specific labeled data,\nwhich may not be feasible in many application scenarios due to data access and\nprivacy constraints. The recently-proposed prompt-based fine-tuning paradigm\nimproves the performance of PLMs for few-shot text classification with\ntask-specific templates. Yet, it is unclear how the prompting knowledge can be\ntransferred across tasks, for the purpose of mutual reinforcement. We propose\nTransPrompt v2, a novel transferable prompting framework for few-shot learning\nacross similar or distant text classification tasks. For learning across\nsimilar tasks, we employ a multi-task meta-knowledge acquisition (MMA)\nprocedure to train a meta-learner that captures the cross-task transferable\nknowledge. For learning across distant tasks, we further inject the task type\ndescriptions into the prompt, and capture the intra-type and inter-type prompt\nembeddings among multiple distant tasks. Additionally, two de-biasing\ntechniques are further designed to make the trained meta-learner more\ntask-agnostic and unbiased towards any tasks. After that, the meta-learner can\nbe adapted to each specific task with better parameters initialization.\nExtensive experiments show that TransPrompt v2 outperforms single-task and\ncross-task strong baselines over multiple NLP tasks and datasets. We further\nshow that the meta-learner can effectively improve the performance of PLMs on\npreviously unseen tasks. In addition, TransPrompt v2 also outperforms strong\nfine-tuning baselines when learning with full training sets.", "published": "2023-08-29 04:16:57", "link": "http://arxiv.org/abs/2308.15010v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models on the Chessboard: A Study on ChatGPT's Formal\n  Language Comprehension and Complex Reasoning Skills", "abstract": "While large language models have made strides in natural language processing,\ntheir proficiency in complex reasoning tasks requiring formal language\ncomprehension, such as chess, remains less investigated. This paper probes the\nperformance of ChatGPT, a sophisticated language model by OpenAI in tackling\nsuch complex reasoning tasks, using chess as a case study. Through robust\nmetrics examining both the legality and quality of moves, we assess ChatGPT's\nunderstanding of the chessboard, adherence to chess rules, and strategic\ndecision-making abilities. Our evaluation identifies limitations within\nChatGPT's attention mechanism that affect its formal language comprehension and\nuncovers the model's underdeveloped self-regulation abilities. Our study also\nreveals ChatGPT's propensity for a coherent strategy in its gameplay and a\nnoticeable uptick in decision-making assertiveness when the model is presented\nwith a greater volume of natural language or possesses a more lucid\nunderstanding of the state of the chessboard. These findings contribute to the\ngrowing exploration of language models' abilities beyond natural language\nprocessing, providing valuable information for future research towards models\ndemonstrating human-like cognitive abilities.", "published": "2023-08-29 08:36:30", "link": "http://arxiv.org/abs/2308.15118v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SpikeBERT: A Language Spikformer Learned from BERT with Knowledge\n  Distillation", "abstract": "Spiking neural networks (SNNs) offer a promising avenue to implement deep\nneural networks in a more energy-efficient way. However, the network\narchitectures of existing SNNs for language tasks are still simplistic and\nrelatively shallow, and deep architectures have not been fully explored,\nresulting in a significant performance gap compared to mainstream\ntransformer-based networks such as BERT. To this end, we improve a\nrecently-proposed spiking Transformer (i.e., Spikformer) to make it possible to\nprocess language tasks and propose a two-stage knowledge distillation method\nfor training it, which combines pre-training by distilling knowledge from BERT\nwith a large collection of unlabelled texts and fine-tuning with task-specific\ninstances via knowledge distillation again from the BERT fine-tuned on the same\ntraining examples. Through extensive experimentation, we show that the models\ntrained with our method, named SpikeBERT, outperform state-of-the-art SNNs and\neven achieve comparable results to BERTs on text classification tasks for both\nEnglish and Chinese with much less energy consumption. Our code is available at\nhttps://github.com/Lvchangze/SpikeBERT.", "published": "2023-08-29 08:41:16", "link": "http://arxiv.org/abs/2308.15122v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking the Generation of Fact Checking Explanations", "abstract": "Fighting misinformation is a challenging, yet crucial, task. Despite the\ngrowing number of experts being involved in manual fact-checking, this activity\nis time-consuming and cannot keep up with the ever-increasing amount of Fake\nNews produced daily. Hence, automating this process is necessary to help curb\nmisinformation. Thus far, researchers have mainly focused on claim veracity\nclassification. In this paper, instead, we address the generation of\njustifications (textual explanation of why a claim is classified as either true\nor false) and benchmark it with novel datasets and advanced baselines. In\nparticular, we focus on summarization approaches over unstructured knowledge\n(i.e. news articles) and we experiment with several extractive and abstractive\nstrategies. We employed two datasets with different styles and structures, in\norder to assess the generalizability of our findings. Results show that in\njustification production summarization benefits from the claim information,\nand, in particular, that a claim-driven extractive step improves abstractive\nsummarization performances. Finally, we show that although cross-dataset\nexperiments suffer from performance degradation, a unique model trained on a\ncombination of the two datasets is able to retain style information in an\nefficient manner.", "published": "2023-08-29 10:40:46", "link": "http://arxiv.org/abs/2308.15202v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Classification-Guided Approach for Adversarial Attacks against Neural\n  Machine Translation", "abstract": "Neural Machine Translation (NMT) models have been shown to be vulnerable to\nadversarial attacks, wherein carefully crafted perturbations of the input can\nmislead the target model. In this paper, we introduce ACT, a novel adversarial\nattack framework against NMT systems guided by a classifier. In our attack, the\nadversary aims to craft meaning-preserving adversarial examples whose\ntranslations in the target language by the NMT model belong to a different\nclass than the original translations. Unlike previous attacks, our new approach\nhas a more substantial effect on the translation by altering the overall\nmeaning, which then leads to a different class determined by an oracle\nclassifier. To evaluate the robustness of NMT models to our attack, we propose\nenhancements to existing black-box word-replacement-based attacks by\nincorporating output translations of the target NMT model and the output logits\nof a classifier within the attack process. Extensive experiments, including a\ncomparison with existing untargeted attacks, show that our attack is\nconsiderably more successful in altering the class of the output translation\nand has more effect on the translation. This new paradigm can reveal the\nvulnerabilities of NMT systems by focusing on the class of translation rather\nthan the mere translation quality as studied traditionally.", "published": "2023-08-29 12:12:53", "link": "http://arxiv.org/abs/2308.15246v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TaskLAMA: Probing the Complex Task Understanding of Language Models", "abstract": "Structured Complex Task Decomposition (SCTD) is the problem of breaking down\na complex real-world task (such as planning a wedding) into a directed acyclic\ngraph over individual steps that contribute to achieving the task, with edges\nspecifying temporal dependencies between them. SCTD is an important component\nof assistive planning tools, and a challenge for commonsense reasoning systems.\nWe probe how accurately SCTD can be done with the knowledge extracted from\nLarge Language Models (LLMs). We introduce a high-quality human-annotated\ndataset for this problem and novel metrics to fairly assess performance of LLMs\nagainst several baselines. Our experiments reveal that LLMs are able to\ndecompose complex tasks into individual steps effectively, with a relative\nimprovement of 15% to 280% over the best baseline. We also propose a number of\napproaches to further improve their performance, with a relative improvement of\n7% to 37% over the base model. However, we find that LLMs still struggle to\npredict pairwise temporal dependencies, which reveals a gap in their\nunderstanding of complex tasks.", "published": "2023-08-29 13:36:45", "link": "http://arxiv.org/abs/2308.15299v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking Machine Ethics -- Can LLMs Perform Moral Reasoning through\n  the Lens of Moral Theories?", "abstract": "Making moral judgments is an essential step toward developing ethical AI\nsystems. Prevalent approaches are mostly implemented in a bottom-up manner,\nwhich uses a large set of annotated data to train models based on crowd-sourced\nopinions about morality. These approaches have been criticized for\novergeneralizing the moral stances of a limited group of annotators and lacking\nexplainability. This work proposes a flexible top-down framework to steer\n(Large) Language Models (LMs) to perform moral reasoning with well-established\nmoral theories from interdisciplinary research. The theory-guided top-down\nframework can incorporate various moral theories. Our experiments demonstrate\nthe effectiveness of the proposed framework on datasets derived from moral\ntheories. Furthermore, we show the alignment between different moral theories\nand existing morality datasets. Our analysis exhibits the potential and flaws\nin existing resources (models and datasets) in developing explainable moral\njudgment-making systems.", "published": "2023-08-29 15:57:32", "link": "http://arxiv.org/abs/2308.15399v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Characterizing Learning Curves During Language Model Pre-Training:\n  Learning, Forgetting, and Stability", "abstract": "How do language models learn to make predictions during pre-training? To\nstudy this, we extract learning curves from five autoregressive English\nlanguage model pre-training runs, for 1M unseen tokens in context. We observe\nthat the language models generate short repetitive phrases before learning to\ngenerate longer and more coherent text. We also find that individual tokens\noften exhibit sudden increases or decreases in loss that are surprisingly\nconsistent across pre-training runs. To better understand these fluctuations,\nwe quantify the final surprisal, within-run variability, age of acquisition,\nforgettability, and cross-run variability of learning curves for individual\ntokens in context. More frequent tokens reach lower final surprisals, exhibit\nless variability within and across pre-training runs, are learned earlier, and\nare less likely to be \"forgotten\" during pre-training. Higher n-gram\nprobabilities further accentuate these effects. Independent of the target\ntoken, shorter and more frequent contexts correlate with marginally more stable\nand quickly acquired predictions. Based on our results, we argue for the\nexistence of sequential learning dependencies between different model\ncapabilities, and we characterize language model learning as early n-gram\nlearning before gradual refinement of tail n-gram predictions.", "published": "2023-08-29 16:24:09", "link": "http://arxiv.org/abs/2308.15419v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Vulgar Remarks Detection in Chittagonian Dialect of Bangla", "abstract": "The negative effects of online bullying and harassment are increasing with\nInternet popularity, especially in social media. One solution is using natural\nlanguage processing (NLP) and machine learning (ML) methods for the automatic\ndetection of harmful remarks, but these methods are limited in low-resource\nlanguages like the Chittagonian dialect of Bangla.This study focuses on\ndetecting vulgar remarks in social media using supervised ML and deep learning\nalgorithms.Logistic Regression achieved promising accuracy (0.91) while simple\nRNN with Word2vec and fastTex had lower accuracy (0.84-0.90), highlighting the\nissue that NN algorithms require more data.", "published": "2023-08-29 17:19:32", "link": "http://arxiv.org/abs/2308.15448v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extracting Mathematical Concepts with Large Language Models", "abstract": "We extract mathematical concepts from mathematical text using generative\nlarge language models (LLMs) like ChatGPT, contributing to the field of\nautomatic term extraction (ATE) and mathematical text processing, and also to\nthe study of LLMs themselves. Our work builds on that of others in that we aim\nfor automatic extraction of terms (keywords) in one mathematical field,\ncategory theory, using as a corpus the 755 abstracts from a snapshot of the\nonline journal \"Theory and Applications of Categories\", circa 2020. Where our\nstudy diverges from previous work is in (1) providing a more thorough analysis\nof what makes mathematical term extraction a difficult problem to begin with;\n(2) paying close attention to inter-annotator disagreements; (3) providing a\nset of guidelines which both human and machine annotators could use to\nstandardize the extraction process; (4) introducing a new annotation tool to\nhelp humans with ATE, applicable to any mathematical field and even beyond\nmathematics; (5) using prompts to ChatGPT as part of the extraction process,\nand proposing best practices for such prompts; and (6) raising the question of\nwhether ChatGPT could be used as an annotator on the same level as human\nexperts. Our overall findings are that the matter of mathematical ATE is an\ninteresting field which can benefit from participation by LLMs, but LLMs\nthemselves cannot at this time surpass human performance on it.", "published": "2023-08-29 20:54:50", "link": "http://arxiv.org/abs/2309.00642v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Radiology-Llama2: Best-in-Class Large Language Model for Radiology", "abstract": "This paper introduces Radiology-Llama2, a large language model specialized\nfor radiology through a process known as instruction tuning. Radiology-Llama2\nis based on the Llama2 architecture and further trained on a large dataset of\nradiology reports to generate coherent and clinically useful impressions from\nradiological findings. Quantitative evaluations using ROUGE metrics on the\nMIMIC-CXR and OpenI datasets demonstrate that Radiology-Llama2 achieves\nstate-of-the-art performance compared to other generative language models, with\na Rouge-1 score of 0.4834 on MIMIC-CXR and 0.4185 on OpenI. Additional\nassessments by radiology experts highlight the model's strengths in\nunderstandability, coherence, relevance, conciseness, and clinical utility. The\nwork illustrates the potential of localized language models designed and tuned\nfor specialized domains like radiology. When properly evaluated and deployed,\nsuch models can transform fields like radiology by automating rote tasks and\nenhancing human expertise.", "published": "2023-08-29 17:44:28", "link": "http://arxiv.org/abs/2309.06419v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recursively Summarizing Enables Long-Term Dialogue Memory in Large\n  Language Models", "abstract": "Recently, large language models (LLMs), such as GPT-4, stand out remarkable\nconversational abilities, enabling them to engage in dynamic and contextually\nrelevant dialogues across a wide range of topics. However, given a long\nconversation, these chatbots fail to recall past information and tend to\ngenerate inconsistent responses. To address this, we propose to recursively\ngenerate summaries/ memory using large language models (LLMs) to enhance\nlong-term memory ability. Specifically, our method first stimulates LLMs to\nmemorize small dialogue contexts and then recursively produce new memory using\nprevious memory and following contexts. Finally, the chatbot can easily\ngenerate a highly consistent response with the help of the latest memory. We\nevaluate our method on both open and closed LLMs, and the experiments on the\nwidely-used public dataset show that our method can generate more consistent\nresponses in a long-context conversation. Also, we show that our strategy could\nnicely complement both long-context (e.g., 8K and 16K) and retrieval-enhanced\nLLMs, bringing further long-term dialogue performance. Notably, our method is a\npotential solution to enable the LLM to model the extremely long context. The\ncode and scripts will be released later.", "published": "2023-08-29 04:59:53", "link": "http://arxiv.org/abs/2308.15022v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Neural Ranking Models with Traditional IR Methods", "abstract": "Neural ranking methods based on large transformer models have recently gained\nsignificant attention in the information retrieval community, and have been\nadopted by major commercial solutions. Nevertheless, they are computationally\nexpensive to create, and require a great deal of labeled data for specialized\ncorpora. In this paper, we explore a low resource alternative which is a\nbag-of-embedding model for document retrieval and find that it is competitive\nwith large transformer models fine tuned on information retrieval tasks. Our\nresults show that a simple combination of TF-IDF, a traditional keyword\nmatching method, with a shallow embedding model provides a low cost path to\ncompete well with the performance of complex neural ranking models on 3\ndatasets. Furthermore, adding TF-IDF measures improves the performance of\nlarge-scale fine tuned models on these tasks.", "published": "2023-08-29 05:18:47", "link": "http://arxiv.org/abs/2308.15027v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Large language models converge toward human-like concept organization", "abstract": "Large language models show human-like performance in knowledge extraction,\nreasoning and dialogue, but it remains controversial whether this performance\nis best explained by memorization and pattern matching, or whether it reflects\nhuman-like inferential semantics and world knowledge. Knowledge bases such as\nWikiData provide large-scale, high-quality representations of inferential\nsemantics and world knowledge. We show that large language models learn to\norganize concepts in ways that are strikingly similar to how concepts are\norganized in such knowledge bases. Knowledge bases model collective,\ninstitutional knowledge, and large language models seem to induce such\nknowledge from raw text. We show that bigger and better models exhibit more\nhuman-like concept organization, across four families of language models and\nthree knowledge graph embeddings.", "published": "2023-08-29 06:09:47", "link": "http://arxiv.org/abs/2308.15047v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Adapting Text-based Dialogue State Tracker for Spoken Dialogues", "abstract": "Although there have been remarkable advances in dialogue systems through the\ndialogue systems technology competition (DSTC), it remains one of the key\nchallenges to building a robust task-oriented dialogue system with a speech\ninterface. Most of the progress has been made for text-based dialogue systems\nsince there are abundant datasets with written corpora while those with spoken\ndialogues are very scarce. However, as can be seen from voice assistant systems\nsuch as Siri and Alexa, it is of practical importance to transfer the success\nto spoken dialogues. In this paper, we describe our engineering effort in\nbuilding a highly successful model that participated in the speech-aware\ndialogue systems technology challenge track in DSTC11. Our model consists of\nthree major modules: (1) automatic speech recognition error correction to\nbridge the gap between the spoken and the text utterances, (2) text-based\ndialogue system (D3ST) for estimating the slots and values using slot\ndescriptions, and (3) post-processing for recovering the error of the estimated\nslot value. Our experiments show that it is important to use an explicit\nautomatic speech recognition error correction module, post-processing, and data\naugmentation to adapt a text-based dialogue state tracker for spoken dialogue\ncorpora.", "published": "2023-08-29 06:27:58", "link": "http://arxiv.org/abs/2308.15053v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Taxonomic Loss for Morphological Glossing of Low-Resource Languages", "abstract": "Morpheme glossing is a critical task in automated language documentation and\ncan benefit other downstream applications greatly. While state-of-the-art\nglossing systems perform very well for languages with large amounts of existing\ndata, it is more difficult to create useful models for low-resource languages.\nIn this paper, we propose the use of a taxonomic loss function that exploits\nmorphological information to make morphological glossing more performant when\ndata is scarce. We find that while the use of this loss function does not\noutperform a standard loss function with regards to single-label prediction\naccuracy, it produces better predictions when considering the top-n predicted\nlabels. We suggest this property makes the taxonomic loss function useful in a\nhuman-in-the-loop annotation setting.", "published": "2023-08-29 06:31:21", "link": "http://arxiv.org/abs/2308.15055v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sequential annotations for naturally-occurring HRI: first insights", "abstract": "We explain the methodology we developed for improving the interactions\naccomplished by an embedded conversational agent, drawing from Conversation\nAnalytic sequential and multimodal analysis. The use case is a Pepper robot\nthat is expected to inform and orient users in a library. In order to propose\nand learn better interactive schema, we are creating a corpus of\nnaturally-occurring interactions that will be made available to the community.\nTo do so, we propose an annotation practice based on some theoretical\nunderpinnings about the use of language and multimodal resources in human-robot\ninteraction. CCS CONCEPTS $\\bullet$ Computing methodologies $\\rightarrow$\nDiscourse, dialogue and pragmatics; $\\bullet$ Human-centered computing\n$\\rightarrow$ Text input; HCI theory, concepts and models; Field studies.", "published": "2023-08-29 08:07:26", "link": "http://arxiv.org/abs/2308.15097v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "The Anatomy of Conspirators: Unveiling Traits using a Comprehensive\n  Twitter Dataset", "abstract": "The discourse around conspiracy theories is currently thriving amidst the\nrampant misinformation in online environments. Research in this field has been\nfocused on detecting conspiracy theories on social media, often relying on\nlimited datasets. In this study, we present a novel methodology for\nconstructing a Twitter dataset that encompasses accounts engaged in\nconspiracy-related activities throughout the year 2022. Our approach centers on\ndata collection that is independent of specific conspiracy theories and\ninformation operations. Additionally, our dataset includes a control group\ncomprising randomly selected users who can be fairly compared to the\nindividuals involved in conspiracy activities. This comprehensive collection\neffort yielded a total of 15K accounts and 37M tweets extracted from their\ntimelines. We conduct a comparative analysis of the two groups across three\ndimensions: topics, profiles, and behavioral characteristics. The results\nindicate that conspiracy and control users exhibit similarity in terms of their\nprofile metadata characteristics. However, they diverge significantly in terms\nof behavior and activity, particularly regarding the discussed topics, the\nterminology used, and their stance on trending subjects. In addition, we find\nno significant disparity in the presence of bot users between the two groups.\nFinally, we develop a classifier to identify conspiracy users using features\nborrowed from bot, troll and linguistic literature. The results demonstrate a\nhigh accuracy level (with an F1 score of 0.94), enabling us to uncover the most\ndiscriminating features associated with conspiracy-related accounts.", "published": "2023-08-29 09:35:23", "link": "http://arxiv.org/abs/2308.15154v2", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Enhancing Psychological Counseling with Large Language Model: A\n  Multifaceted Decision-Support System for Non-Professionals", "abstract": "In the contemporary landscape of social media, an alarming number of users\nexpress negative emotions, some of which manifest as strong suicidal\nintentions. This situation underscores a profound need for trained\npsychological counselors who can enact effective mental interventions. However,\nthe development of these professionals is often an imperative but\ntime-consuming task. Consequently, the mobilization of non-professionals or\nvolunteers in this capacity emerges as a pressing concern. Leveraging the\ncapabilities of artificial intelligence, and in particular, the recent advances\nin large language models, offers a viable solution to this challenge. This\npaper introduces a novel model constructed on the foundation of large language\nmodels to fully assist non-professionals in providing psychological\ninterventions on online user discourses. This framework makes it plausible to\nharness the power of non-professional counselors in a meaningful way. A\ncomprehensive study was conducted involving ten professional psychological\ncounselors of varying expertise, evaluating the system across five critical\ndimensions. The findings affirm that our system is capable of analyzing\npatients' issues with relative accuracy and proffering professional-level\nstrategies recommendations, thereby enhancing support for non-professionals.\nThis research serves as a compelling validation of the application of large\nlanguage models in the field of psychology and lays the groundwork for a new\nparadigm of community-based mental health support.", "published": "2023-08-29 10:20:53", "link": "http://arxiv.org/abs/2308.15192v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Shared Lexical Items as Triggers of Code Switching", "abstract": "Why do bilingual speakers code-switch (mix their two languages)? Among the\nseveral theories that attempt to explain this natural and ubiquitous\nphenomenon, the Triggering Hypothesis relates code-switching to the presence of\nlexical triggers, specifically cognates and proper names, adjacent to the\nswitch point. We provide a fuller, more nuanced and refined exploration of the\ntriggering hypothesis, based on five large datasets in three language pairs,\nreflecting both spoken and written bilingual interactions. Our results show\nthat words that are assumed to reside in a mental lexicon shared by both\nlanguages indeed trigger code-switching; that the tendency to switch depends on\nthe distance of the trigger from the switch point; and on whether the trigger\nprecedes or succeeds the switch; but not on the etymology of the trigger words.\nWe thus provide strong, robust, evidence-based confirmation to several\nhypotheses on the relationships between lexical triggers and code-switching.", "published": "2023-08-29 10:55:44", "link": "http://arxiv.org/abs/2308.15209v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-party Goal Tracking with LLMs: Comparing Pre-training,\n  Fine-tuning, and Prompt Engineering", "abstract": "This paper evaluates the extent to which current Large Language Models (LLMs)\ncan capture task-oriented multi-party conversations (MPCs). We have recorded\nand transcribed 29 MPCs between patients, their companions, and a social robot\nin a hospital. We then annotated this corpus for multi-party goal-tracking and\nintent-slot recognition. People share goals, answer each other's goals, and\nprovide other people's goals in MPCs - none of which occur in dyadic\ninteractions. To understand user goals in MPCs, we compared three methods in\nzero-shot and few-shot settings: we fine-tuned T5, created pre-training tasks\nto train DialogLM using LED, and employed prompt engineering techniques with\nGPT-3.5-turbo, to determine which approach can complete this novel task with\nlimited data. GPT-3.5-turbo significantly outperformed the others in a few-shot\nsetting. The `reasoning' style prompt, when given 7% of the corpus as example\nannotated conversations, was the best performing method. It correctly annotated\n62.32% of the goal tracking MPCs, and 69.57% of the intent-slot recognition\nMPCs. A `story' style prompt increased model hallucination, which could be\ndetrimental if deployed in safety-critical settings. We conclude that\nmulti-party conversations still challenge state-of-the-art LLMs.", "published": "2023-08-29 11:40:03", "link": "http://arxiv.org/abs/2308.15231v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "PronounFlow: A Hybrid Approach for Calibrating Pronouns in Sentences", "abstract": "Flip through any book or listen to any song lyrics, and you will come across\npronouns that, in certain cases, can hinder meaning comprehension, especially\nfor machines. As the role of having cognitive machines becomes pervasive in our\nlives, numerous systems have been developed to resolve pronouns under various\nchallenges. Commensurate with this, it is believed that having systems able to\ndisambiguate pronouns in sentences will help towards the endowment of machines\nwith commonsense and reasoning abilities like those found in humans. However,\none problem these systems face with modern English is the lack of gender\npronouns, where people try to alternate by using masculine, feminine, or plural\nto avoid the whole issue. Since humanity aims to the building of systems in the\nfull-bodied sense we usually reserve for people, what happens when pronouns in\nwritten text, like plural or epicene ones, refer to unspecified entities whose\ngender is not necessarily known? Wouldn't that put extra barriers to existing\ncoreference resolution systems? Towards answering those questions, through the\nimplementation of a neural-symbolic system that utilizes the best of both\nworlds, we are employing PronounFlow, a system that reads any English sentence\nwith pronouns and entities, identifies which of them are not tied to each\nother, and makes suggestions on which to use to avoid biases. Undertaken\nexperiments show that PronounFlow not only alternates pronouns in sentences\nbased on the collective human knowledge around us but also considerably helps\ncoreference resolution systems with the pronoun disambiguation process.", "published": "2023-08-29 11:46:27", "link": "http://arxiv.org/abs/2308.15235v1", "categories": ["cs.CL", "cs.AI", "I.2.0; I.2.3; I.2.7; I.5.1; I.5.4"], "primary_category": "cs.CL"}
{"title": "Enhancing OCR Performance through Post-OCR Models: Adopting Glyph\n  Embedding for Improved Correction", "abstract": "The study investigates the potential of post-OCR models to overcome\nlimitations in OCR models and explores the impact of incorporating glyph\nembedding on post-OCR correction performance. In this study, we have developed\nour own post-OCR correction model. The novelty of our approach lies in\nembedding the OCR output using CharBERT and our unique embedding technique,\ncapturing the visual characteristics of characters. Our findings show that\npost-OCR correction effectively addresses deficiencies in inferior OCR models,\nand glyph embedding enables the model to achieve superior results, including\nthe ability to correct individual words.", "published": "2023-08-29 12:41:50", "link": "http://arxiv.org/abs/2308.15262v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "KGConv, a Conversational Corpus grounded in Wikidata", "abstract": "We present KGConv, a large, conversational corpus of 71k conversations where\neach question-answer pair is grounded in a Wikidata fact. Conversations contain\non average 8.6 questions and for each Wikidata fact, we provide multiple\nvariants (12 on average) of the corresponding question using templates, human\nannotations, hand-crafted rules and a question rewriting neural model. We\nprovide baselines for the task of Knowledge-Based, Conversational Question\nGeneration. KGConv can further be used for other generation and analysis tasks\nsuch as single-turn question generation from Wikidata triples, question\nrewriting, question answering from conversation or from knowledge graphs and\nquiz generation.", "published": "2023-08-29 13:35:51", "link": "http://arxiv.org/abs/2308.15298v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ParaGuide: Guided Diffusion Paraphrasers for Plug-and-Play Textual Style\n  Transfer", "abstract": "Textual style transfer is the task of transforming stylistic properties of\ntext while preserving meaning. Target \"styles\" can be defined in numerous ways,\nranging from single attributes (e.g, formality) to authorship (e.g,\nShakespeare). Previous unsupervised style-transfer approaches generally rely on\nsignificant amounts of labeled data for only a fixed set of styles or require\nlarge language models. In contrast, we introduce a novel diffusion-based\nframework for general-purpose style transfer that can be flexibly adapted to\narbitrary target styles at inference time. Our parameter-efficient approach,\nParaGuide, leverages paraphrase-conditioned diffusion models alongside\ngradient-based guidance from both off-the-shelf classifiers and strong existing\nstyle embedders to transform the style of text while preserving semantic\ninformation. We validate the method on the Enron Email Corpus, with both human\nand automatic evaluations, and find that it outperforms strong baselines on\nformality, sentiment, and even authorship style transfer.", "published": "2023-08-29 17:36:02", "link": "http://arxiv.org/abs/2308.15459v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Document AI: A Comparative Study of Transformer-Based, Graph-Based\n  Models, and Convolutional Neural Networks For Document Layout Analysis", "abstract": "Document AI aims to automatically analyze documents by leveraging natural\nlanguage processing and computer vision techniques. One of the major tasks of\nDocument AI is document layout analysis, which structures document pages by\ninterpreting the content and spatial relationships of layout, image, and text.\nThis task can be image-centric, wherein the aim is to identify and label\nvarious regions such as authors and paragraphs, or text-centric, where the\nfocus is on classifying individual words in a document. Although there are\nincreasingly sophisticated methods for improving layout analysis, doubts remain\nabout the extent to which their findings can be generalized to a broader\ncontext. Specifically, prior work developed systems based on very different\narchitectures, such as transformer-based, graph-based, and CNNs. However, no\nwork has mentioned the effectiveness of these models in a comparative analysis.\nMoreover, while language-independent Document AI models capable of knowledge\ntransfer have been developed, it remains to be investigated to what degree they\ncan effectively transfer knowledge. In this study, we aim to fill these gaps by\nconducting a comparative evaluation of state-of-the-art models in document\nlayout analysis and investigating the potential of cross-lingual layout\nanalysis by utilizing machine translation techniques.", "published": "2023-08-29 16:58:03", "link": "http://arxiv.org/abs/2308.15517v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Wordification: A New Way of Teaching English Spelling Patterns", "abstract": "Literacy, or the ability to read and write, is a crucial indicator of success\nin life and greater society. It is estimated that 85% of people in juvenile\ndelinquent systems cannot adequately read or write, that more than half of\nthose with substance abuse issues have complications in reading or writing and\nthat two-thirds of those who do not complete high school lack proper literacy\nskills. Furthermore, young children who do not possess reading skills matching\ngrade level by the fourth grade are approximately 80% likely to not catch up at\nall. Many may believe that in a developed country such as the United States,\nliteracy fails to be an issue; however, this is a dangerous misunderstanding.\nGlobally an estimated 1.19 trillion dollars are lost every year due to issues\nin literacy; in the USA, the loss is an estimated 300 billion. To put it in\nmore shocking terms, one in five American adults still fail to comprehend basic\nsentences. Making matters worse, the only tools available now to correct a lack\nof reading and writing ability are found in expensive tutoring or other\nprograms that oftentimes fail to be able to reach the required audience. In\nthis paper, our team puts forward a new way of teaching English spelling and\nword recognitions to grade school students in the United States: Wordification.\nWordification is a web application designed to teach English literacy using\nprinciples of linguistics applied to the orthographic and phonological\nproperties of words in a manner not fully utilized previously in any\ncomputer-based teaching application.", "published": "2023-08-29 14:14:01", "link": "http://arxiv.org/abs/2309.12981v2", "categories": ["cs.OH", "cs.CL", "K.3"], "primary_category": "cs.OH"}
{"title": "Robust Open-Set Spoken Language Identification and the CU MultiLang\n  Dataset", "abstract": "Most state-of-the-art spoken language identification models are closed-set;\nin other words, they can only output a language label from the set of classes\nthey were trained on. Open-set spoken language identification systems, however,\ngain the ability to detect when an input exhibits none of the original\nlanguages. In this paper, we implement a novel approach to open-set spoken\nlanguage identification that uses MFCC and pitch features, a TDNN model to\nextract meaningful feature embeddings, confidence thresholding on softmax\noutputs, and LDA and pLDA for learning to classify new unknown languages. We\npresent a spoken language identification system that achieves 91.76% accuracy\non trained languages and has the capability to adapt to unknown languages on\nthe fly. To that end, we also built the CU MultiLang Dataset, a large and\ndiverse multilingual speech corpus which was used to train and evaluate our\nsystem.", "published": "2023-08-29 00:44:27", "link": "http://arxiv.org/abs/2308.14951v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Killing two birds with one stone: Can an audio captioning system also be\n  used for audio-text retrieval?", "abstract": "Automated Audio Captioning (AAC) aims to develop systems capable of\ndescribing an audio recording using a textual sentence. In contrast, Audio-Text\nRetrieval (ATR) systems seek to find the best matching audio recording(s) for a\ngiven textual query (Text-to-Audio) or vice versa (Audio-to-Text). These tasks\nrequire different types of systems: AAC employs a sequence-to-sequence model,\nwhile ATR utilizes a ranking model that compares audio and text representations\nwithin a shared projection subspace. However, this work investigates the\nrelationship between AAC and ATR by exploring the ATR capabilities of an\nunmodified AAC system, without fine-tuning for the new task. Our AAC system\nconsists of an audio encoder (ConvNeXt-Tiny) trained on AudioSet for audio\ntagging, and a transformer decoder responsible for generating sentences. For\nAAC, it achieves a high SPIDEr-FL score of 0.298 on Clotho and 0.472 on\nAudioCaps on average. For ATR, we propose using the standard Cross-Entropy loss\nvalues obtained for any audio/caption pair. Experimental results on the Clotho\nand AudioCaps datasets demonstrate decent recall values using this simple\napproach. For instance, we obtained a Text-to-Audio R@1 value of 0.382 for\nAu-dioCaps, which is above the current state-of-the-art method without external\ndata. Interestingly, we observe that normalizing the loss values was necessary\nfor Audio-to-Text retrieval.", "published": "2023-08-29 07:53:17", "link": "http://arxiv.org/abs/2308.15090v1", "categories": ["cs.CL", "cs.IR", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Evaluation and Analysis of Hallucination in Large Vision-Language Models", "abstract": "Large Vision-Language Models (LVLMs) have recently achieved remarkable\nsuccess. However, LVLMs are still plagued by the hallucination problem, which\nlimits the practicality in many scenarios. Hallucination refers to the\ninformation of LVLMs' responses that does not exist in the visual input, which\nposes potential risks of substantial consequences. There has been limited work\nstudying hallucination evaluation in LVLMs. In this paper, we propose\nHallucination Evaluation based on Large Language Models (HaELM), an LLM-based\nhallucination evaluation framework. HaELM achieves an approximate 95%\nperformance comparable to ChatGPT and has additional advantages including low\ncost, reproducibility, privacy preservation and local deployment. Leveraging\nthe HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we\nanalyze the factors contributing to hallucination in LVLMs and offer helpful\nsuggestions to mitigate the hallucination problem. Our training data and human\nannotation hallucination data will be made public soon.", "published": "2023-08-29 08:51:24", "link": "http://arxiv.org/abs/2308.15126v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "FurChat: An Embodied Conversational Agent using LLMs, Combining Open and\n  Closed-Domain Dialogue with Facial Expressions", "abstract": "We demonstrate an embodied conversational agent that can function as a\nreceptionist and generate a mixture of open and closed-domain dialogue along\nwith facial expressions, by using a large language model (LLM) to develop an\nengaging conversation. We deployed the system onto a Furhat robot, which is\nhighly expressive and capable of using both verbal and nonverbal cues during\ninteraction. The system was designed specifically for the National Robotarium\nto interact with visitors through natural conversations, providing them with\ninformation about the facilities, research, news, upcoming events, etc. The\nsystem utilises the state-of-the-art GPT-3.5 model to generate such information\nalong with domain-general conversations and facial expressions based on prompt\nengineering.", "published": "2023-08-29 11:08:40", "link": "http://arxiv.org/abs/2308.15214v2", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.RO"], "primary_category": "cs.CL"}
{"title": "CLIPTrans: Transferring Visual Knowledge with Pre-trained Models for\n  Multimodal Machine Translation", "abstract": "There has been a growing interest in developing multimodal machine\ntranslation (MMT) systems that enhance neural machine translation (NMT) with\nvisual knowledge. This problem setup involves using images as auxiliary\ninformation during training, and more recently, eliminating their use during\ninference. Towards this end, previous works face a challenge in training\npowerful MMT models from scratch due to the scarcity of annotated multilingual\nvision-language data, especially for low-resource languages. Simultaneously,\nthere has been an influx of multilingual pre-trained models for NMT and\nmultimodal pre-trained models for vision-language tasks, primarily in English,\nwhich have shown exceptional generalisation ability. However, these are not\ndirectly applicable to MMT since they do not provide aligned multimodal\nmultilingual features for generative tasks. To alleviate this issue, instead of\ndesigning complex modules for MMT, we propose CLIPTrans, which simply adapts\nthe independently pre-trained multimodal M-CLIP and the multilingual mBART. In\norder to align their embedding spaces, mBART is conditioned on the M-CLIP\nfeatures by a prefix sequence generated through a lightweight mapping network.\nWe train this in a two-stage pipeline which warms up the model with image\ncaptioning before the actual translation task. Through experiments, we\ndemonstrate the merits of this framework and consequently push forward the\nstate-of-the-art across standard benchmarks by an average of +2.67 BLEU. The\ncode can be found at www.github.com/devaansh100/CLIPTrans.", "published": "2023-08-29 11:29:43", "link": "http://arxiv.org/abs/2308.15226v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Classification-Aware Neural Topic Model Combined With Interpretable\n  Analysis -- For Conflict Classification", "abstract": "A large number of conflict events are affecting the world all the time. In\norder to analyse such conflict events effectively, this paper presents a\nClassification-Aware Neural Topic Model (CANTM-IA) for Conflict Information\nClassification and Topic Discovery. The model provides a reliable\ninterpretation of classification results and discovered topics by introducing\ninterpretability analysis. At the same time, interpretation is introduced into\nthe model architecture to improve the classification performance of the model\nand to allow interpretation to focus further on the details of the data.\nFinally, the model architecture is optimised to reduce the complexity of the\nmodel.", "published": "2023-08-29 11:40:24", "link": "http://arxiv.org/abs/2308.15232v1", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "The Responsible Development of Automated Student Feedback with\n  Generative AI", "abstract": "Providing rich, constructive feedback to students is essential for supporting\nand enhancing their learning. Recent advancements in Generative Artificial\nIntelligence (AI), particularly with large language models (LLMs), present new\nopportunities to deliver scalable, repeatable, and instant feedback,\neffectively making abundant a resource that has historically been scarce and\ncostly. From a technical perspective, this approach is now feasible due to\nbreakthroughs in AI and Natural Language Processing (NLP). While the potential\neducational benefits are compelling, implementing these technologies also\nintroduces a host of ethical considerations that must be thoughtfully\naddressed. One of the core advantages of AI systems is their ability to\nautomate routine and mundane tasks, potentially freeing up human educators for\nmore nuanced work. However, the ease of automation risks a ``tyranny of the\nmajority'', where the diverse needs of minority or unique learners are\noverlooked, as they may be harder to systematize and less straightforward to\naccommodate. Ensuring inclusivity and equity in AI-generated feedback,\ntherefore, becomes a critical aspect of responsible AI implementation in\neducation. The process of developing machine learning models that produce\nvaluable, personalized, and authentic feedback also requires significant input\nfrom human domain experts. Decisions around whose expertise is incorporated,\nhow it is captured, and when it is applied have profound implications for the\nrelevance and quality of the resulting feedback. Additionally, the maintenance\nand continuous refinement of these models are necessary to adapt feedback to\nevolving contextual, theoretical, and student-related factors. Without ongoing\nadaptation, feedback risks becoming obsolete or mismatched with the current\nneeds of diverse student populations [...]", "published": "2023-08-29 14:29:57", "link": "http://arxiv.org/abs/2308.15334v3", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Historical patterns of rice farming explain modern-day language use in\n  China and Japan more than modernization and urbanization", "abstract": "We used natural language processing to analyze a billion words to study\ncultural differences on Weibo, one of China's largest social media platforms.\nWe compared predictions from two common explanations about cultural differences\nin China (economic development and urban-rural differences) against the\nless-obvious legacy of rice versus wheat farming. Rice farmers had to\ncoordinate shared irrigation networks and exchange labor to cope with higher\nlabor requirements. In contrast, wheat relied on rainfall and required half as\nmuch labor. We test whether this legacy made southern China more\ninterdependent. Across all word categories, rice explained twice as much\nvariance as economic development and urbanization. Rice areas used more words\nreflecting tight social ties, holistic thought, and a cautious, prevention\norientation. We then used Twitter data comparing prefectures in Japan, which\nlargely replicated the results from China. This provides crucial evidence of\nthe rice theory in a different nation, language, and platform.", "published": "2023-08-29 14:47:08", "link": "http://arxiv.org/abs/2308.15352v1", "categories": ["cs.CL", "cs.SI", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation", "abstract": "Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL\ntask. However, the absence of a systematical benchmark inhibits the development\nof designing effective, efficient and economic LLM-based Text-to-SQL solutions.\nTo address this challenge, in this paper, we first conduct a systematical and\nextensive comparison over existing prompt engineering methods, including\nquestion representation, example selection and example organization, and with\nthese experimental results, we elaborate their pros and cons. Based on these\nfindings, we propose a new integrated solution, named DAIL-SQL, which refreshes\nthe Spider leaderboard with 86.6% execution accuracy and sets a new bar. To\nexplore the potential of open-source LLM, we investigate them in various\nscenarios, and further enhance their performance with supervised fine-tuning.\nOur explorations highlight open-source LLMs' potential in Text-to-SQL, as well\nas the advantages and disadvantages of the supervised fine-tuning.\nAdditionally, towards an efficient and economic LLM-based Text-to-SQL solution,\nwe emphasize the token efficiency in prompt engineering and compare the prior\nstudies under this metric. We hope that our work provides a deeper\nunderstanding of Text-to-SQL with LLMs, and inspires further investigations and\nbroad applications.", "published": "2023-08-29 14:59:54", "link": "http://arxiv.org/abs/2308.15363v4", "categories": ["cs.DB", "cs.CL", "cs.LG"], "primary_category": "cs.DB"}
{"title": "When Do Program-of-Thoughts Work for Reasoning?", "abstract": "In the realm of embodied artificial intelligence, the reasoning capabilities\nof Large Language Models (LLMs) play a pivotal role. Although there are\neffective methods like program-of-thought prompting for LLMs which uses\nprogramming language to tackle complex reasoning tasks, the specific impact of\ncode data on the improvement of reasoning capabilities remains under-explored.\nTo address this gap, we propose complexity-impacted reasoning score (CIRS),\nwhich combines structural and logical attributes, to measure the correlation\nbetween code and reasoning abilities. Specifically, we use the abstract syntax\ntree to encode the structural information and calculate logical complexity by\nconsidering the difficulty and the cyclomatic complexity. Through an empirical\nanalysis, we find not all code data of complexity can be learned or understood\nby LLMs. Optimal level of complexity is critical to the improvement of\nreasoning abilities by program-aided prompting. Then we design an\nauto-synthesizing and stratifying algorithm, and apply it to instruction\ngeneration for mathematical reasoning and code data filtering for code\ngeneration tasks. Extensive results demonstrates the effectiveness of our\nproposed approach. Code will be integrated into the EasyInstruct framework at\nhttps://github.com/zjunlp/EasyInstruct.", "published": "2023-08-29 17:22:39", "link": "http://arxiv.org/abs/2308.15452v6", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Explaining Vision and Language through Graphs of Events in Space and\n  Time", "abstract": "Artificial Intelligence makes great advances today and starts to bridge the\ngap between vision and language. However, we are still far from understanding,\nexplaining and controlling explicitly the visual content from a linguistic\nperspective, because we still lack a common explainable representation between\nthe two domains. In this work we come to address this limitation and propose\nthe Graph of Events in Space and Time (GEST), by which we can represent, create\nand explain, both visual and linguistic stories. We provide a theoretical\njustification of our model and an experimental validation, which proves that\nGEST can bring a solid complementary value along powerful deep learning models.\nIn particular, GEST can help improve at the content-level the generation of\nvideos from text, by being easily incorporated into our novel video generation\nengine. Additionally, by using efficient graph matching techniques, the GEST\ngraphs can also improve the comparisons between texts at the semantic level.", "published": "2023-08-29 07:25:06", "link": "http://arxiv.org/abs/2309.08612v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Preference-based training framework for automatic speech quality\n  assessment using deep neural network", "abstract": "One objective of Speech Quality Assessment (SQA) is to estimate the ranks of\nsynthetic speech systems. However, recent SQA models are typically trained\nusing low-precision direct scores such as mean opinion scores (MOS) as the\ntraining objective, which is not straightforward to estimate ranking. Although\nit is effective for predicting quality scores of individual sentences, this\napproach does not account for speech and system preferences when ranking\nmultiple systems. We propose a training framework of SQA models that can be\ntrained with only preference scores derived from pairs of MOS to improve\nranking prediction. Our experiment reveals conditions where our framework works\nthe best in terms of pair generation, aggregation functions to derive system\nscore from utterance preferences, and threshold functions to determine\npreference from a pair of MOS. Our results demonstrate that our proposed method\nsignificantly outperforms the baseline model in Spearman's Rank Correlation\nCoefficient.", "published": "2023-08-29 10:40:57", "link": "http://arxiv.org/abs/2308.15203v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Audio Deepfake Detection: A Survey", "abstract": "Audio deepfake detection is an emerging active topic. A growing number of\nliteratures have aimed to study deepfake detection algorithms and achieved\neffective performance, the problem of which is far from being solved. Although\nthere are some review literatures, there has been no comprehensive survey that\nprovides researchers with a systematic overview of these developments with a\nunified evaluation. Accordingly, in this survey paper, we first highlight the\nkey differences across various types of deepfake audio, then outline and\nanalyse competitions, datasets, features, classifications, and evaluation of\nstate-of-the-art approaches. For each aspect, the basic techniques, advanced\ndevelopments and major challenges are discussed. In addition, we perform a\nunified comparison of representative features and classifiers on ASVspoof 2021,\nADD 2023 and In-the-Wild datasets for audio deepfake detection, respectively.\nThe survey shows that future research should address the lack of large scale\ndatasets in the wild, poor generalization of existing detection methods to\nunknown fake attacks, as well as interpretability of detection results.", "published": "2023-08-29 01:50:01", "link": "http://arxiv.org/abs/2308.14970v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Review of Differentiable Digital Signal Processing for Music & Speech\n  Synthesis", "abstract": "The term \"differentiable digital signal processing\" describes a family of\ntechniques in which loss function gradients are backpropagated through digital\nsignal processors, facilitating their integration into neural networks. This\narticle surveys the literature on differentiable audio signal processing,\nfocusing on its use in music & speech synthesis. We catalogue applications to\ntasks including music performance rendering, sound matching, and voice\ntransformation, discussing the motivations for and implications of the use of\nthis methodology. This is accompanied by an overview of digital signal\nprocessing operations that have been implemented differentiably. Finally, we\nhighlight open challenges, including optimisation pathologies, robustness to\nreal-world conditions, and design trade-offs, and discuss directions for future\nresearch.", "published": "2023-08-29 16:29:06", "link": "http://arxiv.org/abs/2308.15422v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-Transfer Learning Techniques for Detecting Auditory Brainstem\n  Response", "abstract": "The assessment of the well-being of the peripheral auditory nerve system in\nindividuals experiencing hearing impairment is conducted through auditory\nbrainstem response (ABR) testing. Audiologists assess and document the results\nof the ABR test. They interpret the findings and assign labels to them using\nreference-based markers like peak latency, waveform morphology, amplitude, and\nother relevant factors. Inaccurate assessment of ABR tests may lead to\nincorrect judgments regarding the integrity of the auditory nerve system;\ntherefore, proper Hearing Loss (HL) diagnosis and analysis are essential. To\nidentify and assess ABR automation while decreasing the possibility of human\nerror, machine learning methods, notably deep learning, may be an appropriate\noption. To address these issues, this study proposed deep-learning models using\nthe transfer-learning (TL) approach to extract features from ABR testing and\ndiagnose HL using support vector machines (SVM). Pre-trained convolutional\nneural network (CNN) architectures like AlexNet, DenseNet, GoogleNet,\nInceptionResNetV2, InceptionV3, MobileNetV2, NASNetMobile, ResNet18, ResNet50,\nResNet101, ShuffleNet, and SqueezeNet are used to extract features from the\ncollected ABR reported images dataset in the proposed model. It has been\ndecided to use six measures accuracy, precision, recall, geometric mean (GM),\nstandard deviation (SD), and area under the ROC curve to measure the\neffectiveness of the proposed model. According to experimental findings, the\nShuffleNet and ResNet50 models' TL is effective for ABR to diagnose HL using an\nSVM classifier, with a high accuracy rate of 95% when using the 5-fold\ncross-validation method.", "published": "2023-08-29 10:40:12", "link": "http://arxiv.org/abs/2308.16203v1", "categories": ["eess.AS", "cs.NE"], "primary_category": "eess.AS"}
{"title": "Let There Be Sound: Reconstructing High Quality Speech from Silent\n  Videos", "abstract": "The goal of this work is to reconstruct high quality speech from lip motions\nalone, a task also known as lip-to-speech. A key challenge of lip-to-speech\nsystems is the one-to-many mapping caused by (1) the existence of homophenes\nand (2) multiple speech variations, resulting in a mispronounced and\nover-smoothed speech. In this paper, we propose a novel lip-to-speech system\nthat significantly improves the generation quality by alleviating the\none-to-many mapping problem from multiple perspectives. Specifically, we\nincorporate (1) self-supervised speech representations to disambiguate\nhomophenes, and (2) acoustic variance information to model diverse speech\nstyles. Additionally, to better solve the aforementioned problem, we employ a\nflow based post-net which captures and refines the details of the generated\nspeech. We perform extensive experiments on two datasets, and demonstrate that\nour method achieves the generation quality close to that of real human\nutterance, outperforming existing methods in terms of speech naturalness and\nintelligibility by a large margin. Synthesised samples are available at our\ndemo page: https://mm.kaist.ac.kr/projects/LTBS.", "published": "2023-08-29 12:30:53", "link": "http://arxiv.org/abs/2308.15256v2", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
