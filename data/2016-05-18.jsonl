{"title": "Leveraging Lexical Resources for Learning Entity Embeddings in\n  Multi-Relational Data", "abstract": "Recent work in learning vector-space embeddings for multi-relational data has\nfocused on combining relational information derived from knowledge bases with\ndistributional information derived from large text corpora. We propose a simple\napproach that leverages the descriptions of entities or phrases available in\nlexical resources, in conjunction with distributional semantics, in order to\nderive a better initialization for training relational models. Applying this\ninitialization to the TransE model results in significant new state-of-the-art\nperformances on the WordNet dataset, decreasing the mean rank from the previous\nbest of 212 to 51. It also results in faster convergence of the entity\nrepresentations. We find that there is a trade-off between improving the mean\nrank and the hits@10 with this approach. This illustrates that much remains to\nbe understood regarding performance improvements in relational models.", "published": "2016-05-18 01:45:32", "link": "http://arxiv.org/abs/1605.05416v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modelling Interaction of Sentence Pair with coupled-LSTMs", "abstract": "Recently, there is rising interest in modelling the interactions of two\nsentences with deep neural networks. However, most of the existing methods\nencode two sequences with separate encoders, in which a sentence is encoded\nwith little or no information from the other sentence. In this paper, we\npropose a deep architecture to model the strong interaction of sentence pair\nwith two coupled-LSTMs. Specifically, we introduce two coupled ways to model\nthe interdependences of two LSTMs, coupling the local contextualized\ninteractions of two sentences. We then aggregate these interactions and use a\ndynamic pooling to select the most informative features. Experiments on two\nvery large datasets demonstrate the efficacy of our proposed architecture and\nits superiority to state-of-the-art methods.", "published": "2016-05-18 13:33:21", "link": "http://arxiv.org/abs/1605.05573v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Evaluation of Dialogue Systems with Next Utterance Classification", "abstract": "An open challenge in constructing dialogue systems is developing methods for\nautomatically learning dialogue strategies from large amounts of unlabelled\ndata. Recent work has proposed Next-Utterance-Classification (NUC) as a\nsurrogate task for building dialogue systems from text data. In this paper we\ninvestigate the performance of humans on this task to validate the relevance of\nNUC as a method of evaluation. Our results show three main findings: (1) humans\nare able to correctly classify responses at a rate much better than chance,\nthus confirming that the task is feasible, (2) human performance levels vary\nacross task domains (we consider 3 datasets) and expertise levels (novice vs\nexperts), thus showing that a range of performance is possible on this type of\ntask, (3) automated dialogue systems built using state-of-the-art machine\nlearning methods have similar performance to the human novices, but worse than\nthe experts, thus confirming the utility of this class of tasks for driving\nfurther research in automated dialogue systems.", "published": "2016-05-18 01:36:29", "link": "http://arxiv.org/abs/1605.05414v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Relations such as Hypernymy: Identifying and Exploiting Hearst Patterns\n  in Distributional Vectors for Lexical Entailment", "abstract": "We consider the task of predicting lexical entailment using distributional\nvectors. We perform a novel qualitative analysis of one existing model which\nwas previously shown to only measure the prototypicality of word pairs. We find\nthat the model strongly learns to identify hypernyms using Hearst patterns,\nwhich are well known to be predictive of lexical relations. We present a novel\nmodel which exploits this behavior as a method of feature extraction in an\niterative procedure similar to Principal Component Analysis. Our model combines\nthe extracted features with the strengths of other proposed models in the\nliterature, and matches or outperforms prior work on multiple data sets.", "published": "2016-05-18 04:10:41", "link": "http://arxiv.org/abs/1605.05433v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
