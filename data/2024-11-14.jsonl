{"title": "Neural Operators Can Play Dynamic Stackelberg Games", "abstract": "Dynamic Stackelberg games are a broad class of two-player games in which the\nleader acts first, and the follower chooses a response strategy to the leader's\nstrategy. Unfortunately, only stylized Stackelberg games are explicitly\nsolvable since the follower's best-response operator (as a function of the\ncontrol of the leader) is typically analytically intractable. This paper\naddresses this issue by showing that the \\textit{follower's best-response\noperator} can be approximately implemented by an \\textit{attention-based neural\noperator}, uniformly on compact subsets of adapted open-loop controls for the\nleader. We further show that the value of the Stackelberg game where the\nfollower uses the approximate best-response operator approximates the value of\nthe original Stackelberg game. Our main result is obtained using our universal\napproximation theorem for attention-based neural operators between spaces of\nsquare-integrable adapted stochastic processes, as well as stability results\nfor a general class of Stackelberg games.", "published": "2024-11-14 18:12:06", "link": "http://arxiv.org/abs/2411.09644v1", "categories": ["math.OC", "cs.LG", "cs.NA", "math.NA", "math.PR", "q-fin.CP"], "primary_category": "math.OC"}
{"title": "Personalized Help for Optimizing Low-Skilled Users' Strategy", "abstract": "AIs can beat humans in game environments; however, how helpful those agents\nare to human remains understudied. We augment CICERO, a natural language agent\nthat demonstrates superhuman performance in Diplomacy, to generate both move\nand message advice based on player intentions. A dozen Diplomacy games with\nnovice and experienced players, with varying advice settings, show that some of\nthe generated advice is beneficial. It helps novices compete with experienced\nplayers and in some instances even surpass them. The mere presence of advice\ncan be advantageous, even if players do not follow it.", "published": "2024-11-14 00:52:45", "link": "http://arxiv.org/abs/2411.09109v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "P-MMEval: A Parallel Multilingual Multitask Benchmark for Consistent\n  Evaluation of LLMs", "abstract": "Recent advancements in large language models (LLMs) showcase varied\nmultilingual capabilities across tasks like translation, code generation, and\nreasoning. Previous assessments often limited their scope to fundamental\nnatural language processing (NLP) or isolated capability-specific tasks. To\nalleviate this drawback, we aim to present a comprehensive multilingual\nmultitask benchmark. First, we present a pipeline for selecting available and\nreasonable benchmarks from massive ones, addressing the oversight in previous\nwork regarding the utility of these benchmarks, i.e., their ability to\ndifferentiate between models being evaluated. Leveraging this pipeline, we\nintroduce P-MMEval, a large-scale benchmark covering effective fundamental and\ncapability-specialized datasets. Furthermore, P-MMEval delivers consistent\nlanguage coverage across various datasets and provides parallel samples.\nFinally, we conduct extensive experiments on representative multilingual model\nseries to compare performances across models, analyze dataset effectiveness,\nexamine prompt impacts on model performances, and explore the relationship\nbetween multilingual performances and factors such as tasks, model sizes, and\nlanguages. These insights offer valuable guidance for future research. The\ndataset is available at https://huggingface.co/datasets/Qwen/P-MMEval.", "published": "2024-11-14 01:29:36", "link": "http://arxiv.org/abs/2411.09116v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unstructured Text Enhanced Open-domain Dialogue System: A Systematic\n  Survey", "abstract": "Incorporating external knowledge into dialogue generation has been proven to\nbenefit the performance of an open-domain Dialogue System (DS), such as\ngenerating informative or stylized responses, controlling conversation topics.\nIn this article, we study the open-domain DS that uses unstructured text as\nexternal knowledge sources (\\textbf{U}nstructured \\textbf{T}ext\n\\textbf{E}nhanced \\textbf{D}ialogue \\textbf{S}ystem, \\textbf{UTEDS}). The\nexistence of unstructured text entails distinctions between UTEDS and\ntraditional data-driven DS and we aim to analyze these differences. We first\ngive the definition of the UTEDS related concepts, then summarize the recently\nreleased datasets and models. We categorize UTEDS into Retrieval and Generative\nmodels and introduce them from the perspective of model components. The\nretrieval models consist of Fusion, Matching, and Ranking modules, while the\ngenerative models comprise Dialogue and Knowledge Encoding, Knowledge\nSelection, and Response Generation modules. We further summarize the evaluation\nmethods utilized in UTEDS and analyze the current models' performance. At last,\nwe discuss the future development trends of UTEDS, hoping to inspire new\nresearch in this field.", "published": "2024-11-14 03:54:42", "link": "http://arxiv.org/abs/2411.09166v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "HateGPT: Unleashing GPT-3.5 Turbo to Combat Hate Speech on X", "abstract": "The widespread use of social media platforms like Twitter and Facebook has\nenabled people of all ages to share their thoughts and experiences, leading to\nan immense accumulation of user-generated content. However, alongside the\nbenefits, these platforms also face the challenge of managing hate speech and\noffensive content, which can undermine rational discourse and threaten\ndemocratic values. As a result, there is a growing need for automated methods\nto detect and mitigate such content, especially given the complexity of\nconversations that may require contextual analysis across multiple languages,\nincluding code-mixed languages like Hinglish, German-English, and Bangla. We\nparticipated in the English task where we have to classify English tweets into\ntwo categories namely Hate and Offensive and Non Hate-Offensive. In this work,\nwe experiment with state-of-the-art large language models like GPT-3.5 Turbo\nvia prompting to classify tweets into Hate and Offensive or Non Hate-Offensive.\nIn this study, we evaluate the performance of a classification model using\nMacro-F1 scores across three distinct runs. The Macro-F1 score, which balances\nprecision and recall across all classes, is used as the primary metric for\nmodel evaluation. The scores obtained are 0.756 for run 1, 0.751 for run 2, and\n0.754 for run 3, indicating a high level of performance with minimal variance\namong the runs. The results suggest that the model consistently performs well\nin terms of precision and recall, with run 1 showing the highest performance.\nThese findings highlight the robustness and reliability of the model across\ndifferent runs.", "published": "2024-11-14 06:20:21", "link": "http://arxiv.org/abs/2411.09214v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DAHL: Domain-specific Automated Hallucination Evaluation of Long-Form\n  Text through a Benchmark Dataset in Biomedicine", "abstract": "We introduce DAHL, a benchmark dataset and automated evaluation system\ndesigned to assess hallucination in long-form text generation, specifically\nwithin the biomedical domain. Our benchmark dataset, meticulously curated from\nbiomedical research papers, consists of 8,573 questions across 29 categories.\nDAHL evaluates fact-conflicting hallucinations in Large Language Models (LLMs)\nby deconstructing responses into atomic units, each representing a single piece\nof information. The accuracy of these responses is averaged to produce the DAHL\nScore, offering a more in-depth evaluation of hallucinations compared to\nprevious methods that rely on multiple-choice tasks. We conduct experiments\nwith 8 different models, finding that larger models tend to hallucinate less;\nhowever, beyond a model size of 7 to 8 billion parameters, further scaling does\nnot significantly improve factual accuracy. The DAHL Score holds potential as\nan efficient alternative to human-annotated preference labels, being able to be\nexpanded to other specialized domains. We release the dataset and code in\npublic.", "published": "2024-11-14 07:41:34", "link": "http://arxiv.org/abs/2411.09255v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DTELS: Towards Dynamic Granularity of Timeline Summarization", "abstract": "The rapid proliferation of online news has posed significant challenges in\ntracking the continuous development of news topics. Traditional timeline\nsummarization constructs a chronological summary of the events but often lacks\nthe flexibility to meet the diverse granularity needs. To overcome this\nlimitation, we introduce a new paradigm, Dynamic-granularity TimELine\nSummarization, (DTELS), which aims to construct adaptive timelines based on\nuser instructions or requirements. This paper establishes a comprehensive\nbenchmark for DTLES that includes: (1) an evaluation framework grounded in\njournalistic standards to assess the timeline quality across four dimensions:\nInformativeness, Granular Consistency, Factuality, and Coherence; (2) a\nlarge-scale, multi-source dataset with multiple granularity timeline\nannotations based on a consensus process to facilitate authority; (3) extensive\nexperiments and analysis with two proposed solutions based on Large Language\nModels (LLMs) and existing state-of-the-art TLS methods. The experimental\nresults demonstrate the effectiveness of LLM-based solutions. However, even the\nmost advanced LLMs struggle to consistently generate timelines that are both\ninformative and granularly consistent, highlighting the challenges of the DTELS\ntask.", "published": "2024-11-14 09:16:48", "link": "http://arxiv.org/abs/2411.09297v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DriveThru: a Document Extraction Platform and Benchmark Datasets for\n  Indonesian Local Language Archives", "abstract": "Indonesia is one of the most diverse countries linguistically. However,\ndespite this linguistic diversity, Indonesian languages remain underrepresented\nin Natural Language Processing (NLP) research and technologies. In the past two\nyears, several efforts have been conducted to construct NLP resources for\nIndonesian languages. However, most of these efforts have been focused on\ncreating manual resources thus difficult to scale to more languages. Although\nmany Indonesian languages do not have a web presence, locally there are\nresources that document these languages well in printed forms such as books,\nmagazines, and newspapers. Digitizing these existing resources will enable\nscaling of Indonesian language resource construction to many more languages. In\nthis paper, we propose an alternative method of creating datasets by digitizing\ndocuments, which have not previously been used to build digital language\nresources in Indonesia. DriveThru is a platform for extracting document content\nutilizing Optical Character Recognition (OCR) techniques in its system to\nprovide language resource building with less manual effort and cost. This paper\nalso studies the utility of current state-of-the-art LLM for post-OCR\ncorrection to show the capability of increasing the character accuracy rate\n(CAR) and word accuracy rate (WAR) compared to off-the-shelf OCR.", "published": "2024-11-14 10:00:33", "link": "http://arxiv.org/abs/2411.09318v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Everyone deserves their voice to be heard: Analyzing Predictive Gender\n  Bias in ASR Models Applied to Dutch Speech Data", "abstract": "Recent research has shown that state-of-the-art (SotA) Automatic Speech\nRecognition (ASR) systems, such as Whisper, often exhibit predictive biases\nthat disproportionately affect various demographic groups. This study focuses\non identifying the performance disparities of Whisper models on Dutch speech\ndata from the Common Voice dataset and the Dutch National Public Broadcasting\norganisation. We analyzed the word error rate, character error rate and a\nBERT-based semantic similarity across gender groups. We used the moral\nframework of Weerts et al. (2022) to assess quality of service harms and\nfairness, and to provide a nuanced discussion on the implications of these\nbiases, particularly for automatic subtitling. Our findings reveal substantial\ndisparities in word error rate (WER) among gender groups across all model\nsizes, with bias identified through statistical testing.", "published": "2024-11-14 13:29:09", "link": "http://arxiv.org/abs/2411.09431v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Use of Readability Metrics in Legal Text: A Systematic Literature\n  Review", "abstract": "Understanding the text in legal documents can be challenging due to their\ncomplex structure and the inclusion of domain-specific jargon. Laws and\nregulations are often crafted in such a manner that engagement with them\nrequires formal training, potentially leading to vastly different\ninterpretations of the same texts. Linguistic complexity is an important\ncontributor to the difficulties experienced by readers. Simplifying texts could\nenhance comprehension across a broader audience, not just among trained\nprofessionals. Various metrics have been developed to measure document\nreadability. Therefore, we adopted a systematic review approach to examine the\nlinguistic and readability metrics currently employed for legal and regulatory\ntexts. A total of 3566 initial papers were screened, with 34 relevant studies\nfound and further assessed. Our primary objective was to identify which current\nmetrics were applied for evaluating readability within the legal field. Sixteen\ndifferent metrics were identified, with the Flesch-Kincaid Grade Level being\nthe most frequently used method. The majority of studies (73.5%) were found in\nthe domain of \"informed consent forms\". From the analysis, it is clear that not\nall legal domains are well represented in terms of readability metrics and that\nthere is a further need to develop more consensus on which metrics should be\napplied for legal documents.", "published": "2024-11-14 15:04:17", "link": "http://arxiv.org/abs/2411.09497v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BabyLM Challenge: Exploring the Effect of Variation Sets on Language\n  Model Training Efficiency", "abstract": "While current large language models have achieved a remarkable success, their\ndata efficiency remains a challenge to overcome. Recently it has been suggested\nthat child-directed speech (CDS) can improve training data efficiency of modern\nlanguage models based on Transformer neural networks. However, it is not yet\nunderstood which specific properties of CDS are effective for training these\nmodels. In the context of the BabyLM Challenge, we focus on Variation Sets\n(VSs), sets of consecutive utterances expressing a similar intent with slightly\ndifferent words and structures, which are ubiquitous in CDS. To assess the\nimpact of VSs on training data efficiency, we augment CDS data with different\nproportions of artificial VSs and use these datasets to train an\nauto-regressive model, GPT-2. We find that the best proportion of VSs depends\non the evaluation benchmark: BLiMP and GLUE scores benefit from the presence of\nVSs, but EWOK scores do not. Additionally, the results vary depending on\nmultiple factors such as the number of epochs and the order of utterance\npresentation. Taken together, these findings suggest that VSs can have a\nbeneficial influence on language models, while leaving room for further\ninvestigation.", "published": "2024-11-14 16:57:46", "link": "http://arxiv.org/abs/2411.09587v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive Decoding via Latent Preference Optimization", "abstract": "During language model decoding, it is known that using higher temperature\nsampling gives more creative responses, while lower temperatures are more\nfactually accurate. However, such models are commonly applied to general\ninstruction following, which involves both creative and fact seeking tasks,\nusing a single fixed temperature across all examples and tokens. In this work,\nwe introduce Adaptive Decoding, a layer added to the model to select the\nsampling temperature dynamically at inference time, at either the token or\nexample level, in order to optimize performance. To learn its parameters we\nintroduce Latent Preference Optimization (LPO) a general approach to train\ndiscrete latent variables such as choices of temperature. Our method\noutperforms all fixed decoding temperatures across a range of tasks that\nrequire different temperatures, including UltraFeedback, Creative Story\nWriting, and GSM8K.", "published": "2024-11-14 18:31:39", "link": "http://arxiv.org/abs/2411.09661v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Squeezed Attention: Accelerating Long Context Length LLM Inference", "abstract": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models.", "published": "2024-11-14 18:54:19", "link": "http://arxiv.org/abs/2411.09688v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Bayesian Optimization Approach to Machine Translation Reranking", "abstract": "Reranking a list of candidates from a machine translation system with an\nexternal scoring model and returning the highest-scoring candidate remains a\nsimple and effective method for improving the overall output quality.\nTranslation scoring models continue to grow in size, with the best models being\ncomparable to generation models. Thus, reranking can add substantial\ncomputational cost to the translation pipeline. In this work, we pose reranking\nas a Bayesian optimization (BayesOpt) problem. By strategically selecting\ncandidates to score based on a balance of exploration and exploitation, we show\nthat it is possible to find top-scoring candidates when scoring only a fraction\nof the candidate list. For instance, our method achieves the same CometKiwi\nscore using only 70 scoring evaluations compared a baseline system using 180.\nWe present a multi-fidelity setting for BayesOpt, where the candidates are\nfirst scored with a cheaper but noisier proxy scoring model, which further\nimproves the cost-performance tradeoff when using smaller but well-trained\ndistilled proxy scorers.", "published": "2024-11-14 18:58:23", "link": "http://arxiv.org/abs/2411.09694v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Gender Bias in Large Language Models", "abstract": "Gender bias in artificial intelligence has become an important issue,\nparticularly in the context of language models used in communication-oriented\napplications. This study examines the extent to which Large Language Models\n(LLMs) exhibit gender bias in pronoun selection in occupational contexts. The\nanalysis evaluates the models GPT-4, GPT-4o, PaLM 2 Text Bison and Gemini 1.0\nPro using a self-generated dataset. The jobs considered include a range of\noccupations, from those with a significant male presence to those with a\nnotable female concentration, as well as jobs with a relatively equal gender\ndistribution. Three different sentence processing methods were used to assess\npotential gender bias: masked tokens, unmasked sentences, and sentence\ncompletion. In addition, the LLMs suggested names of individuals in specific\noccupations, which were then examined for gender distribution. The results show\na positive correlation between the models' pronoun choices and the gender\ndistribution present in U.S. labor force data. Female pronouns were more often\nassociated with female-dominated occupations, while male pronouns were more\noften associated with male-dominated occupations. Sentence completion showed\nthe strongest correlation with actual gender distribution, while name\ngeneration resulted in a more balanced 'politically correct' gender\ndistribution, albeit with notable variations in predominantly male or female\noccupations. Overall, the prompting method had a greater impact on gender\ndistribution than the model selection itself, highlighting the complexity of\naddressing gender bias in LLMs. The findings highlight the importance of\nprompting in gender mapping.", "published": "2024-11-14 22:23:13", "link": "http://arxiv.org/abs/2411.09826v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic, Orthographic, and Morphological Biases in Humans' Wordle\n  Gameplay", "abstract": "We show that human players' gameplay in the game of Wordle is influenced by\nthe semantics, orthography, and morphology of the player's previous guesses. We\ndemonstrate this influence by comparing actual human players' guesses to\nnear-optimal guesses, showing that human players' guesses are biased to be\nsimilar to previous guesses semantically, orthographically, and\nmorphologically.", "published": "2024-11-14 18:20:52", "link": "http://arxiv.org/abs/2411.18634v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DROJ: A Prompt-Driven Attack against Large Language Models", "abstract": "Large Language Models (LLMs) have demonstrated exceptional capabilities\nacross various natural language processing tasks. Due to their training on\ninternet-sourced datasets, LLMs can sometimes generate objectionable content,\nnecessitating extensive alignment with human feedback to avoid such outputs.\nDespite massive alignment efforts, LLMs remain susceptible to adversarial\njailbreak attacks, which usually are manipulated prompts designed to circumvent\nsafety mechanisms and elicit harmful responses. Here, we introduce a novel\napproach, Directed Rrepresentation Optimization Jailbreak (DROJ), which\noptimizes jailbreak prompts at the embedding level to shift the hidden\nrepresentations of harmful queries towards directions that are more likely to\nelicit affirmative responses from the model. Our evaluations on LLaMA-2-7b-chat\nmodel show that DROJ achieves a 100\\% keyword-based Attack Success Rate (ASR),\neffectively preventing direct refusals. However, the model occasionally\nproduces repetitive and non-informative responses. To mitigate this, we\nintroduce a helpfulness system prompt that enhances the utility of the model's\nresponses. Our code is available at\nhttps://github.com/Leon-Leyang/LLM-Safeguard.", "published": "2024-11-14 01:48:08", "link": "http://arxiv.org/abs/2411.09125v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Financial Domain Adaptation of Language Models via Model\n  Augmentation", "abstract": "The domain adaptation of language models, including large language models\n(LLMs), has become increasingly important as the use of such models continues\nto expand. This study demonstrates the effectiveness of Composition to Augment\nLanguage Models (CALM) in adapting to the financial domain. CALM is a model to\nextend the capabilities of existing models by introducing cross-attention\nbetween two LLMs with different functions. In our experiments, we developed a\nCALM to enhance the financial performance of an LLM with strong response\ncapabilities by leveraging a financial-specialized LLM. Notably, the CALM was\ntrained using a financial dataset different from the one used to train the\nfinancial-specialized LLM, confirming CALM's ability to adapt to various\ndatasets. The models were evaluated through quantitative Japanese financial\nbenchmarks and qualitative response comparisons, demonstrating that CALM\nenables superior responses with higher scores than the original models and\nbaselines. Additionally, comparative experiments on connection points revealed\nthat connecting the middle layers of the models is most effective in\nfacilitating adaptation to the financial domain. These findings confirm that\nCALM is a practical approach for adapting LLMs to the financial domain.", "published": "2024-11-14 07:28:09", "link": "http://arxiv.org/abs/2411.09249v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A\n  Survey", "abstract": "The rapid evolution of multimodal foundation models has led to significant\nadvancements in cross-modal understanding and generation across diverse\nmodalities, including text, images, audio, and video. However, these models\nremain susceptible to jailbreak attacks, which can bypass built-in safety\nmechanisms and induce the production of potentially harmful content.\nConsequently, understanding the methods of jailbreak attacks and existing\ndefense mechanisms is essential to ensure the safe deployment of multimodal\ngenerative models in real-world scenarios, particularly in security-sensitive\napplications. To provide comprehensive insight into this topic, this survey\nreviews jailbreak and defense in multimodal generative models. First, given the\ngeneralized lifecycle of multimodal jailbreak, we systematically explore\nattacks and corresponding defense strategies across four levels: input,\nencoder, generator, and output. Based on this analysis, we present a detailed\ntaxonomy of attack methods, defense mechanisms, and evaluation frameworks\nspecific to multimodal generative models. Additionally, we cover a wide range\nof input-output configurations, including modalities such as Any-to-Text,\nAny-to-Vision, and Any-to-Any within generative systems. Finally, we highlight\ncurrent research challenges and propose potential directions for future\nresearch. The open-source repository corresponding to this work can be found at\nhttps://github.com/liuxuannan/Awesome-Multimodal-Jailbreak.", "published": "2024-11-14 07:51:51", "link": "http://arxiv.org/abs/2411.09259v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Cross-Modal Consistency in Multimodal Large Language Models", "abstract": "Recent developments in multimodal methodologies have marked the beginning of\nan exciting era for models adept at processing diverse data types, encompassing\ntext, audio, and visual content. Models like GPT-4V, which merge computer\nvision with advanced language processing, exhibit extraordinary proficiency in\nhandling intricate tasks that require a simultaneous understanding of both\ntextual and visual information. Prior research efforts have meticulously\nevaluated the efficacy of these Vision Large Language Models (VLLMs) in various\ndomains, including object detection, image captioning, and other related\nfields. However, existing analyses have often suffered from limitations,\nprimarily centering on the isolated evaluation of each modality's performance\nwhile neglecting to explore their intricate cross-modal interactions.\nSpecifically, the question of whether these models achieve the same level of\naccuracy when confronted with identical task instances across different\nmodalities remains unanswered. In this study, we take the initiative to delve\ninto the interaction and comparison among these modalities of interest by\nintroducing a novel concept termed cross-modal consistency. Furthermore, we\npropose a quantitative evaluation framework founded on this concept. Our\nexperimental findings, drawn from a curated collection of parallel\nvision-language datasets developed by us, unveil a pronounced inconsistency\nbetween the vision and language modalities within GPT-4V, despite its portrayal\nas a unified multimodal model. Our research yields insights into the\nappropriate utilization of such models and hints at potential avenues for\nenhancing their design.", "published": "2024-11-14 08:22:42", "link": "http://arxiv.org/abs/2411.09273v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "StreamAdapter: Efficient Test Time Adaptation from Contextual Streams", "abstract": "In-context learning (ICL) allows large language models (LLMs) to adapt to new\ntasks directly from the given demonstrations without requiring gradient\nupdates. While recent advances have expanded context windows to accommodate\nmore demonstrations, this approach increases inference costs without\nnecessarily improving performance. To mitigate these issues, We propose\nStreamAdapter, a novel approach that directly updates model parameters from\ncontext at test time, eliminating the need for explicit in-context\ndemonstrations. StreamAdapter employs context mapping and weight absorption\nmechanisms to dynamically transform ICL demonstrations into parameter updates\nwith minimal additional parameters. By reducing reliance on numerous in-context\nexamples, StreamAdapter significantly reduce inference costs and allows for\nefficient inference with constant time complexity, regardless of demonstration\ncount. Extensive experiments across diverse tasks and model architectures\ndemonstrate that StreamAdapter achieves comparable or superior adaptation\ncapability to ICL while requiring significantly fewer demonstrations. The\nsuperior task adaptation and context encoding capabilities of StreamAdapter on\nboth language understanding and generation tasks provides a new perspective for\nadapting LLMs at test time using context, allowing for more efficient\nadaptation across scenarios and more cost-effective inference", "published": "2024-11-14 09:03:54", "link": "http://arxiv.org/abs/2411.09289v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MM-Eval: A Hierarchical Benchmark for Modern Mongolian Evaluation in\n  LLMs", "abstract": "Large language models (LLMs) excel in high-resource languages but face\nnotable challenges in low-resource languages like Mongolian. This paper\naddresses these challenges by categorizing capabilities into language abilities\n(syntax and semantics) and cognitive abilities (knowledge and reasoning). To\nsystematically evaluate these areas, we developed MM-Eval, a specialized\ndataset based on Modern Mongolian Language Textbook I and enriched with WebQSP\nand MGSM datasets.\n  Preliminary experiments on models including Qwen2-7B-Instruct, GLM4-9b-chat,\nLlama3.1-8B-Instruct, GPT-4, and DeepseekV2.5 revealed that: 1) all models\nperformed better on syntactic tasks than semantic tasks, highlighting a gap in\ndeeper language understanding; and 2) knowledge tasks showed a moderate\ndecline, suggesting that models can transfer general knowledge from\nhigh-resource to low-resource contexts.\n  The release of MM-Eval, comprising 569 syntax, 677 semantics, 344 knowledge,\nand 250 reasoning tasks, offers valuable insights for advancing NLP and LLMs in\nlow-resource languages like Mongolian. The dataset is available at\nhttps://github.com/joenahm/MM-Eval.", "published": "2024-11-14 14:58:38", "link": "http://arxiv.org/abs/2411.09492v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Practical Guide to Fine-tuning Language Models with Limited Data", "abstract": "Employing pre-trained Large Language Models (LLMs) has become the de facto\nstandard in Natural Language Processing (NLP) despite their extensive data\nrequirements. Motivated by the recent surge in research focused on training\nLLMs with limited data, particularly in low-resource domains and languages,\nthis paper surveys recent transfer learning approaches to optimize model\nperformance in downstream tasks where data is scarce. We first address initial\nand continued pre-training strategies to better leverage prior knowledge in\nunseen domains and languages. We then examine how to maximize the utility of\nlimited data during fine-tuning and few-shot learning. The final section takes\na task-specific perspective, reviewing models and methods suited for different\nlevels of data scarcity. Our goal is to provide practitioners with practical\nguidelines for overcoming the challenges posed by constrained data while also\nhighlighting promising directions for future research.", "published": "2024-11-14 15:55:37", "link": "http://arxiv.org/abs/2411.09539v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Piecing It All Together: Verifying Multi-Hop Multimodal Claims", "abstract": "Existing claim verification datasets often do not require systems to perform\ncomplex reasoning or effectively interpret multimodal evidence. To address\nthis, we introduce a new task: multi-hop multimodal claim verification. This\ntask challenges models to reason over multiple pieces of evidence from diverse\nsources, including text, images, and tables, and determine whether the combined\nmultimodal evidence supports or refutes a given claim. To study this task, we\nconstruct MMCV, a large-scale dataset comprising 15k multi-hop claims paired\nwith multimodal evidence, generated and refined using large language models,\nwith additional input from human feedback. We show that MMCV is challenging\neven for the latest state-of-the-art multimodal large language models,\nespecially as the number of reasoning hops increases. Additionally, we\nestablish a human performance benchmark on a subset of MMCV. We hope this\ndataset and its evaluation task will encourage future research in multimodal\nmulti-hop claim verification.", "published": "2024-11-14 16:01:33", "link": "http://arxiv.org/abs/2411.09547v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Initial Nugget Evaluation Results for the TREC 2024 RAG Track with the\n  AutoNuggetizer Framework", "abstract": "This report provides an initial look at partial results from the TREC 2024\nRetrieval-Augmented Generation (RAG) Track. We have identified RAG evaluation\nas a barrier to continued progress in information access (and more broadly,\nnatural language processing and artificial intelligence), and it is our hope\nthat we can contribute to tackling the many challenges in this space. The\ncentral hypothesis we explore in this work is that the nugget evaluation\nmethodology, originally developed for the TREC Question Answering Track in\n2003, provides a solid foundation for evaluating RAG systems. As such, our\nefforts have focused on \"refactoring\" this methodology, specifically applying\nlarge language models to both automatically create nuggets and to automatically\nassign nuggets to system answers. We call this the AutoNuggetizer framework.\nWithin the TREC setup, we are able to calibrate our fully automatic process\nagainst a manual process whereby nuggets are created by human assessors\nsemi-manually and then assigned manually to system answers. Based on initial\nresults across 21 topics from 45 runs, we observe a strong correlation between\nscores derived from a fully automatic nugget evaluation and a (mostly) manual\nnugget evaluation by human assessors. This suggests that our fully automatic\nevaluation process can be used to guide future iterations of RAG systems.", "published": "2024-11-14 17:25:43", "link": "http://arxiv.org/abs/2411.09607v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "The Moral Foundations Weibo Corpus", "abstract": "Moral sentiments expressed in natural language significantly influence both\nonline and offline environments, shaping behavioral styles and interaction\npatterns, including social media selfpresentation, cyberbullying, adherence to\nsocial norms, and ethical decision-making. To effectively measure moral\nsentiments in natural language processing texts, it is crucial to utilize\nlarge, annotated datasets that provide nuanced understanding for accurate\nanalysis and modeltraining. However, existing corpora, while valuable, often\nface linguistic limitations. To address this gap in the Chinese language\ndomain,we introduce the Moral Foundation Weibo Corpus. This corpus consists of\n25,671 Chinese comments on Weibo, encompassing six diverse topic areas. Each\ncomment is manually annotated by at least three systematically trained\nannotators based on ten moral categories derived from a grounded theory of\nmorality. To assess annotator reliability, we present the kappa testresults, a\ngold standard for measuring consistency. Additionally, we apply several the\nlatest large language models to supplement the manual annotations, conducting\nanalytical experiments to compare their performance and report baseline results\nfor moral sentiment classification.", "published": "2024-11-14 17:32:03", "link": "http://arxiv.org/abs/2411.09612v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PTR: Precision-Driven Tool Recommendation for Large Language Models", "abstract": "By augmenting Large Language Models (LLMs) with external tools, their\ncapacity to solve complex problems has been significantly enhanced. However,\ndespite ongoing advancements in the parsing capabilities of LLMs, incorporating\nall available tools simultaneously in the prompt remains impractical due to the\nvast number of external tools. Consequently, it is essential to provide LLMs\nwith a precise set of tools tailored to the specific task, considering both\nquantity and quality. Current tool retrieval methods primarily focus on\nrefining the ranking list of tools and directly packaging a fixed number of\ntop-ranked tools as the tool set. However, these approaches often fail to equip\nLLMs with the optimal set of tools prior to execution, since the optimal number\nof tools for different tasks could be different, resulting in inefficiencies\nsuch as redundant or unsuitable tools, which impede immediate access to the\nmost relevant tools. This paper addresses the challenge of recommending precise\ntoolsets for LLMs. We introduce the problem of tool recommendation, define its\nscope, and propose a novel Precision-driven Tool Recommendation (PTR) approach.\nPTR captures an initial, concise set of tools by leveraging historical tool\nbundle usage and dynamically adjusts the tool set by performing tool matching,\nculminating in a multi-view-based tool addition. Additionally, we present a new\ndataset, RecTools, and a metric, TRACC, designed to evaluate the effectiveness\nof tool recommendation for LLMs. We further validate our design choices through\ncomprehensive experiments, demonstrating promising accuracy across two open\nbenchmarks and our RecTools dataset.", "published": "2024-11-14 17:33:36", "link": "http://arxiv.org/abs/2411.09613v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM Hallucination Reasoning with Zero-shot Knowledge Test", "abstract": "LLM hallucination, where LLMs occasionally generate unfaithful text, poses\nsignificant challenges for their practical applications. Most existing\ndetection methods rely on external knowledge, LLM fine-tuning, or\nhallucination-labeled datasets, and they do not distinguish between different\ntypes of hallucinations, which are crucial for improving detection performance.\nWe introduce a new task, Hallucination Reasoning, which classifies\nLLM-generated text into one of three categories: aligned, misaligned, and\nfabricated. Our novel zero-shot method assesses whether LLM has enough\nknowledge about a given prompt and text. Our experiments conducted on new\ndatasets demonstrate the effectiveness of our method in hallucination reasoning\nand underscore its importance for enhancing detection performance.", "published": "2024-11-14 18:55:26", "link": "http://arxiv.org/abs/2411.09689v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Evaluating the Predictive Capacity of ChatGPT for Academic Peer Review\n  Outcomes Across Multiple Platforms", "abstract": "While previous studies have demonstrated that Large Language Models (LLMs)\ncan predict peer review outcomes to some extent, this paper builds on that by\nintroducing two new contexts and employing a more robust method - averaging\nmultiple ChatGPT scores. The findings that averaging 30 ChatGPT predictions,\nbased on reviewer guidelines and using only the submitted titles and abstracts,\nfailed to predict peer review outcomes for F1000Research (Spearman's rho=0.00).\nHowever, it produced mostly weak positive correlations with the quality\ndimensions of SciPost Physics (rho=0.25 for validity, rho=0.25 for originality,\nrho=0.20 for significance, and rho = 0.08 for clarity) and a moderate positive\ncorrelation for papers from the International Conference on Learning\nRepresentations (ICLR) (rho=0.38). Including the full text of articles\nsignificantly increased the correlation for ICLR (rho=0.46) and slightly\nimproved it for F1000Research (rho=0.09), while it had variable effects on the\nfour quality dimension correlations for SciPost LaTeX files. The use of\nchain-of-thought system prompts slightly increased the correlation for\nF1000Research (rho=0.10), marginally reduced it for ICLR (rho=0.37), and\nfurther decreased it for SciPost Physics (rho=0.16 for validity, rho=0.18 for\noriginality, rho=0.18 for significance, and rho=0.05 for clarity). Overall, the\nresults suggest that in some contexts, ChatGPT can produce weak pre-publication\nquality assessments. However, the effectiveness of these assessments and the\noptimal strategies for employing them vary considerably across different\nplatforms, journals, and conferences. Additionally, the most suitable inputs\nfor ChatGPT appear to differ depending on the platform.", "published": "2024-11-14 19:20:33", "link": "http://arxiv.org/abs/2411.09763v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "A Benchmark for Long-Form Medical Question Answering", "abstract": "There is a lack of benchmarks for evaluating large language models (LLMs) in\nlong-form medical question answering (QA). Most existing medical QA evaluation\nbenchmarks focus on automatic metrics and multiple-choice questions. While\nvaluable, these benchmarks fail to fully capture or assess the complexities of\nreal-world clinical applications where LLMs are being deployed. Furthermore,\nexisting studies on evaluating long-form answer generation in medical QA are\nprimarily closed-source, lacking access to human medical expert annotations,\nwhich makes it difficult to reproduce results and enhance existing baselines.\nIn this work, we introduce a new publicly available benchmark featuring\nreal-world consumer medical questions with long-form answer evaluations\nannotated by medical doctors. We performed pairwise comparisons of responses\nfrom various open and closed-source medical and general-purpose LLMs based on\ncriteria such as correctness, helpfulness, harmfulness, and bias. Additionally,\nwe performed a comprehensive LLM-as-a-judge analysis to study the alignment\nbetween human judgments and LLMs. Our preliminary results highlight the strong\npotential of open LLMs in medical QA compared to leading closed models. Code &\nData: https://github.com/lavita-ai/medical-eval-sphere", "published": "2024-11-14 22:54:38", "link": "http://arxiv.org/abs/2411.09834v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Comprehensive and Practical Evaluation of Retrieval-Augmented Generation\n  Systems for Medical Question Answering", "abstract": "Retrieval-augmented generation (RAG) has emerged as a promising approach to\nenhance the performance of large language models (LLMs) in knowledge-intensive\ntasks such as those from medical domain. However, the sensitive nature of the\nmedical domain necessitates a completely accurate and trustworthy system. While\nexisting RAG benchmarks primarily focus on the standard retrieve-answer\nsetting, they overlook many practical scenarios that measure crucial aspects of\na reliable medical system. This paper addresses this gap by providing a\ncomprehensive evaluation framework for medical question-answering (QA) systems\nin a RAG setting for these situations, including sufficiency, integration, and\nrobustness. We introduce Medical Retrieval-Augmented Generation Benchmark\n(MedRGB) that provides various supplementary elements to four medical QA\ndatasets for testing LLMs' ability to handle these specific scenarios.\nUtilizing MedRGB, we conduct extensive evaluations of both state-of-the-art\ncommercial LLMs and open-source models across multiple retrieval conditions.\nOur experimental results reveals current models' limited ability to handle\nnoise and misinformation in the retrieved documents. We further analyze the\nLLMs' reasoning processes to provides valuable insights and future directions\nfor developing RAG systems in this critical medical domain.", "published": "2024-11-14 06:19:18", "link": "http://arxiv.org/abs/2411.09213v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Re-Parameterization of Lightweight Transformer for On-Device Speech\n  Emotion Recognition", "abstract": "With the increasing implementation of machine learning models on edge or\nInternet-of-Things (IoT) devices, deploying advanced models on\nresource-constrained IoT devices remains challenging. Transformer models, a\ncurrently dominant neural architecture, have achieved great success in broad\ndomains but their complexity hinders its deployment on IoT devices with limited\ncomputation capability and storage size. Although many model compression\napproaches have been explored, they often suffer from notorious performance\ndegradation. To address this issue, we introduce a new method, namely\nTransformer Re-parameterization, to boost the performance of lightweight\nTransformer models. It consists of two processes: the High-Rank Factorization\n(HRF) process in the training stage and the deHigh-Rank Factorization (deHRF)\nprocess in the inference stage. In the former process, we insert an additional\nlinear layer before the Feed-Forward Network (FFN) of the lightweight\nTransformer. It is supposed that the inserted HRF layers can enhance the model\nlearning capability. In the later process, the auxiliary HRF layer will be\nmerged together with the following FFN layer into one linear layer and thus\nrecover the original structure of the lightweight model. To examine the\neffectiveness of the proposed method, we evaluate it on three widely used\nTransformer variants, i.e., ConvTransformer, Conformer, and SpeechFormer\nnetworks, in the application of speech emotion recognition on the IEMOCAP, M3ED\nand DAIC-WOZ datasets. Experimental results show that our proposed method\nconsistently improves the performance of lightweight Transformers, even making\nthem comparable to large models. The proposed re-parameterization approach\nenables advanced Transformer models to be deployed on resource-constrained IoT\ndevices.", "published": "2024-11-14 10:36:19", "link": "http://arxiv.org/abs/2411.09339v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Robot Tasks with Fuzzy Time Requirements from Natural Language\n  Instructions", "abstract": "Natural language allows robot programming to be accessible to everyone.\nHowever, the inherent fuzziness in natural language poses challenges for\ninflexible, traditional robot systems. We focus on instructions with fuzzy time\nrequirements (e.g., \"start in a few minutes\"). Building on previous robotics\nresearch, we introduce fuzzy skills. These define an execution by the robot\nwith so-called satisfaction functions representing vague execution time\nrequirements. Such functions express a user's satisfaction over potential\nstarting times for skill execution. When the robot handles multiple fuzzy\nskills, the satisfaction function provides a temporal tolerance window for\nexecution, thus, enabling optimal scheduling based on satisfaction. We\ngeneralized such functions based on individual user expectations with a user\nstudy. The participants rated their satisfaction with an instruction's\nexecution at various times. Our investigations reveal that trapezoidal\nfunctions best approximate the users' satisfaction. Additionally, the results\nsuggest that users are more lenient if the execution is specified further into\nthe future.", "published": "2024-11-14 13:34:16", "link": "http://arxiv.org/abs/2411.09436v1", "categories": ["cs.RO", "cs.CL", "cs.HC"], "primary_category": "cs.RO"}
{"title": "Communication Compression for Tensor Parallel LLM Inference", "abstract": "Large Language Models (LLMs) have pushed the frontier of artificial\nintelligence but are comprised of hundreds of billions of parameters and\noperations. For faster inference latency, LLMs are deployed on multiple\nhardware accelerators through various Model Parallelism strategies. Our paper\nlooks into the details on one such strategy - Tensor Parallel - and proposes to\nreduce latency by compressing inter-accelerator communication. We leverage fine\ngrained quantization techniques to compress selected activations by 3.5 - 4.5x.\nOur proposed method leads up to 2x reduction of time-to-first-token (TTFT) with\nnegligible model performance degradation.", "published": "2024-11-14 15:19:01", "link": "http://arxiv.org/abs/2411.09510v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models", "abstract": "This work explores expanding the capabilities of large language models (LLMs)\npretrained on text to generate 3D meshes within a unified model. This offers\nkey advantages of (1) leveraging spatial knowledge already embedded in LLMs,\nderived from textual sources like 3D tutorials, and (2) enabling conversational\n3D generation and mesh understanding. A primary challenge is effectively\ntokenizing 3D mesh data into discrete tokens that LLMs can process seamlessly.\nTo address this, we introduce LLaMA-Mesh, a novel approach that represents the\nvertex coordinates and face definitions of 3D meshes as plain text, allowing\ndirect integration with LLMs without expanding the vocabulary. We construct a\nsupervised fine-tuning (SFT) dataset enabling pretrained LLMs to (1) generate\n3D meshes from text prompts, (2) produce interleaved text and 3D mesh outputs\nas required, and (3) understand and interpret 3D meshes. Our work is the first\nto demonstrate that LLMs can be fine-tuned to acquire complex spatial knowledge\nfor 3D mesh generation in a text-based format, effectively unifying the 3D and\ntext modalities. LLaMA-Mesh achieves mesh generation quality on par with models\ntrained from scratch while maintaining strong text generation performance.", "published": "2024-11-14 17:08:23", "link": "http://arxiv.org/abs/2411.09595v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "68T05", "I.3.5; I.2.10; I.2.6"], "primary_category": "cs.LG"}
{"title": "Less is More: Unseen Domain Fake News Detection via Causal Propagation\n  Substructures", "abstract": "The spread of fake news on social media poses significant threats to\nindividuals and society. Text-based and graph-based models have been employed\nfor fake news detection by analysing news content and propagation networks,\nshowing promising results in specific scenarios. However, these data-driven\nmodels heavily rely on pre-existing in-distribution data for training, limiting\ntheir performance when confronted with fake news from emerging or previously\nunseen domains, known as out-of-distribution (OOD) data. Tackling OOD fake news\nis a challenging yet critical task. In this paper, we introduce the Causal\nSubgraph-oriented Domain Adaptive Fake News Detection (CSDA) model, designed to\nenhance zero-shot fake news detection by extracting causal substructures from\npropagation graphs using in-distribution data and generalising this approach to\nOOD data. The model employs a graph neural network based mask generation\nprocess to identify dominant nodes and edges within the propagation graph,\nusing these substructures for fake news detection. Additionally, the\nperformance of CSDA is further improved through contrastive learning in\nfew-shot scenarios, where a limited amount of OOD data is available for\ntraining. Extensive experiments on public social media datasets demonstrate\nthat CSDA effectively handles OOD fake news detection, achieving a 7 to 16\npercents accuracy improvement over other state-of-the-art models.", "published": "2024-11-14 12:05:35", "link": "http://arxiv.org/abs/2411.09389v1", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.SI"}
{"title": "On the Limits of Language Generation: Trade-Offs Between Hallucination\n  and Mode Collapse", "abstract": "Specifying all desirable properties of a language model is challenging, but\ncertain requirements seem essential. Given samples from an unknown language,\nthe trained model should produce valid strings not seen in training and be\nexpressive enough to capture the language's full richness. Otherwise,\noutputting invalid strings constitutes \"hallucination,\" and failing to capture\nthe full range leads to \"mode collapse.\" We ask if a language model can meet\nboth requirements.\n  We investigate this within a statistical language generation setting building\non Gold and Angluin. Here, the model receives random samples from a\ndistribution over an unknown language K, which belongs to a possibly infinite\ncollection of languages. The goal is to generate unseen strings from K. We say\nthe model generates from K with consistency and breadth if, as training size\nincreases, its output converges to all unseen strings in K.\n  Kleinberg and Mullainathan [KM24] asked if consistency and breadth in\nlanguage generation are possible. We answer this negatively: for a large class\nof language models, including next-token prediction models, this is impossible\nfor most collections of candidate languages. This contrasts with [KM24]'s\nresult, showing consistent generation without breadth is possible for any\ncountable collection of languages. Our finding highlights that generation with\nbreadth fundamentally differs from generation without breadth.\n  As a byproduct, we establish near-tight bounds on the number of samples\nneeded for generation with or without breadth.\n  Finally, our results offer hope: consistent generation with breadth is\nachievable for any countable collection of languages when negative examples\n(strings outside K) are available alongside positive ones. This suggests that\npost-training feedback, which encodes negative examples, can be crucial in\nreducing hallucinations while limiting mode collapse.", "published": "2024-11-14 18:06:55", "link": "http://arxiv.org/abs/2411.09642v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "An End-To-End Stuttering Detection Method Based On Conformer And BILSTM", "abstract": "Stuttering is a neurodevelopmental speech disorder characterized by common\nspeech symptoms such as pauses, exclamations, repetition, and prolongation.\nSpeech-language pathologists typically assess the type and severity of\nstuttering by observing these symptoms. Many effective end-to-end methods exist\nfor stuttering detection, but a commonly overlooked challenge is the uncertain\nrelationship between tasks involved in this process. Using a suitable\nmulti-task strategy could improve stuttering detection performance. This paper\npresents a novel stuttering event detection model designed to help\nspeech-language pathologists assess both the type and severity of stuttering.\nFirst, the Conformer model extracts acoustic features from stuttered speech,\nfollowed by a Long Short-Term Memory (LSTM) network to capture contextual\ninformation. Finally, we explore multi-task learning for stuttering and propose\nan effective multi-task strategy. Experimental results show that our model\noutperforms current state-of-the-art methods for stuttering detection. In the\nSLT 2024 Stuttering Speech Challenge based on the AS-70 dataset [1], our model\nimproved the mean F1 score by 24.8% compared to the baseline method and\nachieved first place. On this basis, we conducted relevant extensive\nexperiments on LSTM and multi-task learning strategies respectively. The\nresults show that our proposed method improved the mean F1 score by 39.8%\ncompared to the baseline method.", "published": "2024-11-14 14:35:40", "link": "http://arxiv.org/abs/2411.09479v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "ParaLBench: A Large-Scale Benchmark for Computational Paralinguistics\n  over Acoustic Foundation Models", "abstract": "Computational paralinguistics (ComParal) aims to develop algorithms and\nmodels to automatically detect, analyze, and interpret non-verbal information\nfrom speech communication, e. g., emotion, health state, age, and gender.\nDespite its rapid progress, it heavily depends on sophisticatedly designed\nmodels given specific paralinguistic tasks. Thus, the heterogeneity and\ndiversity of ComParal models largely prevent the realistic implementation of\nComParal models. Recently, with the advent of acoustic foundation models\nbecause of self-supervised learning, developing more generic models that can\nefficiently perceive a plethora of paralinguistic information has become an\nactive topic in speech processing. However, it lacks a unified evaluation\nframework for a fair and consistent performance comparison. To bridge this gap,\nwe conduct a large-scale benchmark, namely ParaLBench, which concentrates on\nstandardizing the evaluation process of diverse paralinguistic tasks, including\ncritical aspects of affective computing such as emotion recognition and emotion\ndimensions prediction, over different acoustic foundation models. This\nbenchmark contains ten datasets with thirteen distinct paralinguistic tasks,\ncovering short-, medium- and long-term characteristics. Each task is carried\nout on 14 acoustic foundation models under a unified evaluation framework,\nwhich allows for an unbiased methodological comparison and offers a grounded\nreference for the ComParal community. Based on the insights gained from\nParaLBench, we also point out potential research directions, i.e., the\ncross-corpus generalizability, to propel ComParal research in the future. The\ncode associated with this study will be available to foster the transparency\nand replicability of this work for succeeding researchers.", "published": "2024-11-14 10:49:17", "link": "http://arxiv.org/abs/2411.09349v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Robust AI-Synthesized Speech Detection Using Feature Decomposition\n  Learning and Synthesizer Feature Augmentation", "abstract": "AI-synthesized speech, also known as deepfake speech, has recently raised\nsignificant concerns due to the rapid advancement of speech synthesis and\nspeech conversion techniques. Previous works often rely on distinguishing\nsynthesizer artifacts to identify deepfake speech. However, excessive reliance\non these specific synthesizer artifacts may result in unsatisfactory\nperformance when addressing speech signals created by unseen synthesizers. In\nthis paper, we propose a robust deepfake speech detection method that employs\nfeature decomposition to learn synthesizer-independent content features as\ncomplementary for detection. Specifically, we propose a dual-stream feature\ndecomposition learning strategy that decomposes the learned speech\nrepresentation using a synthesizer stream and a content stream. The synthesizer\nstream specializes in learning synthesizer features through supervised training\nwith synthesizer labels. Meanwhile, the content stream focuses on learning\nsynthesizer-independent content features, enabled by a pseudo-labeling-based\nsupervised learning method. This method randomly transforms speech to generate\nspeed and compression labels for training. Additionally, we employ an\nadversarial learning technique to reduce the synthesizer-related components in\nthe content stream. The final classification is determined by concatenating the\nsynthesizer and content features. To enhance the model's robustness to\ndifferent synthesizer characteristics, we further propose a synthesizer feature\naugmentation strategy that randomly blends the characteristic styles within\nreal and fake audio features and randomly shuffles the synthesizer features\nwith the content features. This strategy effectively enhances the feature\ndiversity and simulates more feature combinations.", "published": "2024-11-14 03:57:21", "link": "http://arxiv.org/abs/2411.09167v1", "categories": ["cs.SD", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improvement and Implementation of a Speech Emotion Recognition Model\n  Based on Dual-Layer LSTM", "abstract": "This paper builds upon an existing speech emotion recognition model by adding\nan additional LSTM layer to improve the accuracy and processing efficiency of\nemotion recognition from audio data. By capturing the long-term dependencies\nwithin audio sequences through a dual-layer LSTM network, the model can\nrecognize and classify complex emotional patterns more accurately. Experiments\nconducted on the RAVDESS dataset validated this approach, showing that the\nmodified dual layer LSTM model improves accuracy by 2% compared to the\nsingle-layer LSTM while significantly reducing recognition latency, thereby\nenhancing real-time performance. These results indicate that the dual-layer\nLSTM architecture is highly suitable for handling emotional features with\nlong-term dependencies, providing a viable optimization for speech emotion\nrecognition systems. This research provides a reference for practical\napplications in fields like intelligent customer service, sentiment analysis\nand human-computer interaction.", "published": "2024-11-14 05:05:36", "link": "http://arxiv.org/abs/2411.09189v2", "categories": ["cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
{"title": "Transferable Adversarial Attacks against ASR", "abstract": "Given the extensive research and real-world applications of automatic speech\nrecognition (ASR), ensuring the robustness of ASR models against minor input\nperturbations becomes a crucial consideration for maintaining their\neffectiveness in real-time scenarios. Previous explorations into ASR model\nrobustness have predominantly revolved around evaluating accuracy on white-box\nsettings with full access to ASR models. Nevertheless, full ASR model details\nare often not available in real-world applications. Therefore, evaluating the\nrobustness of black-box ASR models is essential for a comprehensive\nunderstanding of ASR model resilience. In this regard, we thoroughly study the\nvulnerability of practical black-box attacks in cutting-edge ASR models and\npropose to employ two advanced time-domain-based transferable attacks alongside\nour differentiable feature extractor. We also propose a speech-aware gradient\noptimization approach (SAGO) for ASR, which forces mistranscription with\nminimal impact on human imperceptibility through voice activity detection rule\nand a speech-aware gradient-oriented optimizer. Our comprehensive experimental\nresults reveal performance enhancements compared to baseline approaches across\nfive models on two databases.", "published": "2024-11-14 06:32:31", "link": "http://arxiv.org/abs/2411.09220v1", "categories": ["eess.AS", "cs.AI", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Towards Unified Neural Decoding of Perceived, Spoken and Imagined Speech\n  from EEG Signals", "abstract": "Brain signals accompany various information relevant to human actions and\nmental imagery, making them crucial to interpreting and understanding human\nintentions. Brain-computer interface technology leverages this brain activity\nto generate external commands for controlling the environment, offering\ncritical advantages to individuals with paralysis or locked-in syndrome. Within\nthe brain-computer interface domain, brain-to-speech research has gained\nattention, focusing on the direct synthesis of audible speech from brain\nsignals. Most current studies decode speech from brain activity using invasive\ntechniques and emphasize spoken speech data. However, humans express various\nspeech states, and distinguishing these states through non-invasive approaches\nremains a significant yet challenging task. This research investigated the\neffectiveness of deep learning models for non-invasive-based neural signal\ndecoding, with an emphasis on distinguishing between different speech\nparadigms, including perceived, overt, whispered, and imagined speech, across\nmultiple frequency bands. The model utilizing the spatial conventional neural\nnetwork module demonstrated superior performance compared to other models,\nespecially in the gamma band. Additionally, imagined speech in the theta\nfrequency band, where deep learning also showed strong effects, exhibited\nstatistically significant differences compared to the other speech paradigms.", "published": "2024-11-14 07:20:08", "link": "http://arxiv.org/abs/2411.09243v1", "categories": ["cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
{"title": "EEG-Based Speech Decoding: A Novel Approach Using Multi-Kernel Ensemble\n  Diffusion Models", "abstract": "In this study, we propose an ensemble learning framework for\nelectroencephalogram-based overt speech classification, leveraging denoising\ndiffusion probabilistic models with varying convolutional kernel sizes. The\nensemble comprises three models with kernel sizes of 51, 101, and 201,\neffectively capturing multi-scale temporal features inherent in signals. This\napproach improves the robustness and accuracy of speech decoding by\naccommodating the rich temporal complexity of neural signals. The ensemble\nmodels work in conjunction with conditional autoencoders that refine the\nreconstructed signals and maximize the useful information for downstream\nclassification tasks. The results indicate that the proposed ensemble-based\napproach significantly outperforms individual models and existing\nstate-of-the-art techniques. These findings demonstrate the potential of\nensemble methods in advancing brain signal decoding, offering new possibilities\nfor non-verbal communication applications, particularly in brain-computer\ninterface systems aimed at aiding individuals with speech impairments.", "published": "2024-11-14 09:23:58", "link": "http://arxiv.org/abs/2411.09302v1", "categories": ["cs.SD", "cs.AI", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Local deployment of large-scale music AI models on commodity hardware", "abstract": "We present the MIDInfinite, a web application capable of generating symbolic\nmusic using a large-scale generative AI model locally on commodity hardware.\nCreating this demo involved porting the Anticipatory Music Transformer, a large\nlanguage model (LLM) pre-trained on the Lakh MIDI dataset, to the Machine\nLearning Compilation (MLC) framework. Once the model is ported, MLC facilitates\ninference on a variety of runtimes including C++, mobile, and the browser. We\nenvision that MLC has the potential to bridge the gap between the landscape of\nincreasingly capable music AI models and technology more familiar to music\nsoftware developers. As a proof of concept, we build a web application that\nallows users to generate endless streams of multi-instrumental MIDI in the\nbrowser, either from scratch or conditioned on a prompt. On commodity hardware\n(an M3 Macbook Pro), our demo can generate 51 notes per second, which is faster\nthan real-time playback for 72.9% of generations, and increases to 86.3% with 2\nseconds of upfront buffering.", "published": "2024-11-14 17:49:27", "link": "http://arxiv.org/abs/2411.09625v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
