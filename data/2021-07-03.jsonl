{"title": "Neural-Symbolic Solver for Math Word Problems with Auxiliary Tasks", "abstract": "Previous math word problem solvers following the encoder-decoder paradigm\nfail to explicitly incorporate essential math symbolic constraints, leading to\nunexplainable and unreasonable predictions. Herein, we propose Neural-Symbolic\nSolver (NS-Solver) to explicitly and seamlessly incorporate different levels of\nsymbolic constraints by auxiliary tasks. Our NS-Solver consists of a problem\nreader to encode problems, a programmer to generate symbolic equations, and a\nsymbolic executor to obtain answers. Along with target expression supervision,\nour solver is also optimized via 4 new auxiliary objectives to enforce\ndifferent symbolic reasoning: a) self-supervised number prediction task\npredicting both number quantity and number locations; b) commonsense constant\nprediction task predicting what prior knowledge (e.g. how many legs a chicken\nhas) is required; c) program consistency checker computing the semantic loss\nbetween predicted equation and target equation to ensure reasonable equation\nmapping; d) duality exploiting task exploiting the quasi duality between\nsymbolic equation generation and problem's part-of-speech generation to enhance\nthe understanding ability of a solver. Besides, to provide a more realistic and\nchallenging benchmark for developing a universal and scalable solver, we also\nconstruct a new large-scale MWP benchmark CM17K consisting of 4 kinds of MWPs\n(arithmetic, one-unknown linear, one-unknown non-linear, equation set) with\nmore than 17K samples. Extensive experiments on Math23K and our CM17k\ndemonstrate the superiority of our NS-Solver compared to state-of-the-art\nmethods.", "published": "2021-07-03 13:14:58", "link": "http://arxiv.org/abs/2107.01431v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TagRec: Automated Tagging of Questions with Hierarchical Learning\n  Taxonomy", "abstract": "Online educational platforms organize academic questions based on a\nhierarchical learning taxonomy (subject-chapter-topic). Automatically tagging\nnew questions with existing taxonomy will help organize these questions into\ndifferent classes of hierarchical taxonomy so that they can be searched based\non the facets like chapter. This task can be formulated as a flat multi-class\nclassification problem. Usually, flat classification based methods ignore the\nsemantic relatedness between the terms in the hierarchical taxonomy and the\nquestions. Some traditional methods also suffer from the class imbalance issues\nas they consider only the leaf nodes ignoring the hierarchy. Hence, we\nformulate the problem as a similarity-based retrieval task where we optimize\nthe semantic relatedness between the taxonomy and the questions. We demonstrate\nthat our method helps to handle the unseen labels and hence can be used for\ntaxonomy tagging in the wild. In this method, we augment the question with its\ncorresponding answer to capture more semantic information and then align the\nquestion-answer pair's contextualized embedding with the corresponding label\n(taxonomy) vector representations. The representations are aligned by\nfine-tuning a transformer based model with a loss function that is a\ncombination of the cosine similarity and hinge rank loss. The loss function\nmaximizes the similarity between the question-answer pair and the correct label\nrepresentations and minimizes the similarity to unrelated labels. Finally, we\nperform experiments on two real-world datasets. We show that the proposed\nlearning method outperforms representations learned using the multi-class\nclassification method and other state of the art methods by 6% as measured by\nRecall@k. We also demonstrate the performance of the proposed method on unseen\nbut related learning content like the learning objectives without re-training\nthe network.", "published": "2021-07-03 11:50:55", "link": "http://arxiv.org/abs/2107.10649v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Transformers Jump Around Right in Natural Language? Assessing\n  Performance Transfer from SCAN", "abstract": "Despite their practical success, modern seq2seq architectures are unable to\ngeneralize systematically on several SCAN tasks. Hence, it is not clear if\nSCAN-style compositional generalization is useful in realistic NLP tasks. In\nthis work, we study the benefit that such compositionality brings about to\nseveral machine translation tasks. We present several focused modifications of\nTransformer that greatly improve generalization capabilities on SCAN and select\none that remains on par with a vanilla Transformer on a standard machine\ntranslation (MT) task. Next, we study its performance in low-resource settings\nand on a newly introduced distribution-shifted English-French translation task.\nOverall, we find that improvements of a SCAN-capable model do not directly\ntransfer to the resource-rich MT setup. In contrast, in the low-resource setup,\ngeneral modifications lead to an improvement of up to 13.1% BLEU score w.r.t. a\nvanilla Transformer. Similarly, an improvement of 14% in an accuracy-based\nmetric is achieved in the introduced compositional English-French translation\ntask. This provides experimental evidence that the compositional generalization\nassessed in SCAN is particularly useful in resource-starved and domain-shifted\nscenarios.", "published": "2021-07-03 07:45:41", "link": "http://arxiv.org/abs/2107.01366v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Trans4E: Link Prediction on Scholarly Knowledge Graphs", "abstract": "The incompleteness of Knowledge Graphs (KGs) is a crucial issue affecting the\nquality of AI-based services. In the scholarly domain, KGs describing research\npublications typically lack important information, hindering our ability to\nanalyse and predict research dynamics. In recent years, link prediction\napproaches based on Knowledge Graph Embedding models became the first aid for\nthis issue. In this work, we present Trans4E, a novel embedding model that is\nparticularly fit for KGs which include N to M relations with N$\\gg$M. This is\ntypical for KGs that categorize a large number of entities (e.g., research\narticles, patents, persons) according to a relatively small set of categories.\nTrans4E was applied on two large-scale knowledge graphs, the Academia/Industry\nDynAmics (AIDA) and Microsoft Academic Graph (MAG), for completing the\ninformation about Fields of Study (e.g., 'neural networks', 'machine learning',\n'artificial intelligence'), and affiliation types (e.g., 'education',\n'company', 'government'), improving the scope and accuracy of the resulting\ndata. We evaluated our approach against alternative solutions on AIDA, MAG, and\nfour other benchmarks (FB15k, FB15k-237, WN18, and WN18RR). Trans4E outperforms\nthe other models when using low embedding dimensions and obtains competitive\nresults in high dimensions.", "published": "2021-07-03 09:34:44", "link": "http://arxiv.org/abs/2107.03297v1", "categories": ["cs.AI", "cs.CL", "cs.DL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "The HCCL Speaker Verification System for Far-Field Speaker Verification\n  Challenge", "abstract": "This paper describes the systems submitted by team HCCL to the Far-Field\nSpeaker Verification Challenge. Our previous work in the AIshell Speaker\nVerification Challenge 2019 shows that the powerful modeling abilities of\nNeural Network architectures can provide exceptional performance for this kind\nof task. Therefore, in this challenge, we focus on constructing deep Neural\nNetwork architectures based on TDNN, Resnet and Res2net blocks. Most of the\ndeveloped systems consist of Neural Network embeddings are applied with PLDA\nbackend. Firstly, the speed perturbation method is applied to augment data and\nsignificant performance improvements are achieved. Then, we explore the use of\nAMsoftmax loss function and propose to join a CE-loss branch when we train\nmodel using AMsoftmax loss. In addition, the impact of score normalization on\nperformance is also investigated. The final system, a fusion of four systems,\nachieves minDCF 0.5342, EER 5.05\\% on task1 eval set, and achieves minDCF\n0.5193, EER 5.47\\% on task3 eval set.", "published": "2021-07-03 03:04:18", "link": "http://arxiv.org/abs/2107.01329v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Lottery Ticket Hypothesis Framework for Low-Complexity Device-Robust\n  Neural Acoustic Scene Classification", "abstract": "We propose a novel neural model compression strategy combining data\naugmentation, knowledge transfer, pruning, and quantization for device-robust\nacoustic scene classification (ASC). Specifically, we tackle the ASC task in a\nlow-resource environment leveraging a recently proposed advanced neural network\npruning mechanism, namely Lottery Ticket Hypothesis (LTH), to find a\nsub-network neural model associated with a small amount non-zero model\nparameters. The effectiveness of LTH for low-complexity acoustic modeling is\nassessed by investigating various data augmentation and compression schemes,\nand we report an efficient joint framework for low-complexity multi-device ASC,\ncalled \\emph{Acoustic Lottery}. Acoustic Lottery could compress an ASC model up\nto $1/10^{4}$ and attain a superior performance (validation accuracy of 79.4%\nand Log loss of 0.64) compared to its not compressed seed model. All results\nreported in this work are based on a joint effort of four groups, namely\nGT-USTC-UKE-Tencent, aiming to address the \"Low-Complexity Acoustic Scene\nClassification (ASC) with Multiple Devices\" in the DCASE 2021 Challenge Task\n1a.", "published": "2021-07-03 16:25:24", "link": "http://arxiv.org/abs/2107.01461v4", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Development of a Conversation State Prediction System", "abstract": "With the evolution of the concept of Speaker diarization using LSTM, it is\nrelatively easier to understand the speaker identities for specific segments of\ninput audio stream data than manually tagging the data. With such a concept, it\nis highly desirable to consider the possibility of using the identified speaker\nidentities to aid in recognizing the Speaker States in a conversation. In this\nstudy, the Markov Chains are used to identify and update the Speaker States for\nthe next conversations between the same set of speakers, to enable\nidentification of their states in the most natural and long conversations. The\nmodel is based on several audio samples from natural conversations of three or\ngreater than three speakers in two datasets with overall total error\npercentages for recognized states being lesser than or equal to 12%. The\nfindings imply that the proposed extension to the Speaker diarization is\neffective to predict the states for a conversation.", "published": "2021-07-03 16:33:23", "link": "http://arxiv.org/abs/2107.01462v4", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
