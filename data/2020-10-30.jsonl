{"title": "Streaming Simultaneous Speech Translation with Augmented Memory\n  Transformer", "abstract": "Transformer-based models have achieved state-of-the-art performance on speech\ntranslation tasks. However, the model architecture is not efficient enough for\nstreaming scenarios since self-attention is computed over an entire input\nsequence and the computational cost grows quadratically with the length of the\ninput sequence. Nevertheless, most of the previous work on simultaneous speech\ntranslation, the task of generating translations from partial audio input,\nignores the time spent in generating the translation when analyzing the\nlatency. With this assumption, a system may have good latency quality\ntrade-offs but be inapplicable in real-time scenarios. In this paper, we focus\non the task of streaming simultaneous speech translation, where the systems are\nnot only capable of translating with partial input but are also able to handle\nvery long or continuous input. We propose an end-to-end transformer-based\nsequence-to-sequence model, equipped with an augmented memory transformer\nencoder, which has shown great success on the streaming automatic speech\nrecognition task with hybrid or transducer-based models. We conduct an\nempirical evaluation of the proposed model on segment, context and memory sizes\nand we compare our approach to a transformer with a unidirectional mask.", "published": "2020-10-30 18:28:42", "link": "http://arxiv.org/abs/2011.00033v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Gender Bias within Narrative Tropes", "abstract": "Popular media reflects and reinforces societal biases through the use of\ntropes, which are narrative elements, such as archetypal characters and plot\narcs, that occur frequently across media. In this paper, we specifically\ninvestigate gender bias within a large collection of tropes. To enable our\nstudy, we crawl tvtropes.org, an online user-created repository that contains\n30K tropes associated with 1.9M examples of their occurrences across film,\ntelevision, and literature. We automatically score the \"genderedness\" of each\ntrope in our TVTROPES dataset, which enables an analysis of (1) highly-gendered\ntopics within tropes, (2) the relationship between gender bias and popular\nreception, and (3) how the gender of a work's creator correlates with the types\nof tropes that they use.", "published": "2020-10-30 20:26:41", "link": "http://arxiv.org/abs/2011.00092v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CliniQG4QA: Generating Diverse Questions for Domain Adaptation of\n  Clinical Question Answering", "abstract": "Clinical question answering (QA) aims to automatically answer questions from\nmedical professionals based on clinical texts. Studies show that neural QA\nmodels trained on one corpus may not generalize well to new clinical texts from\na different institute or a different patient group, where large-scale QA pairs\nare not readily available for model retraining. To address this challenge, we\npropose a simple yet effective framework, CliniQG4QA, which leverages question\ngeneration (QG) to synthesize QA pairs on new clinical contexts and boosts QA\nmodels without requiring manual annotations. In order to generate diverse types\nof questions that are essential for training QA models, we further introduce a\nseq2seq-based question phrase prediction (QPP) module that can be used together\nwith most existing QG models to diversify the generation. Our comprehensive\nexperiment results show that the QA corpus generated by our framework can\nimprove QA models on the new contexts (up to 8% absolute gain in terms of Exact\nMatch), and that the QPP module plays a crucial role in achieving the gain.", "published": "2020-10-30 02:06:10", "link": "http://arxiv.org/abs/2010.16021v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VECO: Variable and Flexible Cross-lingual Pre-training for Language\n  Understanding and Generation", "abstract": "Existing work in multilingual pretraining has demonstrated the potential of\ncross-lingual transferability by training a unified Transformer encoder for\nmultiple languages. However, much of this work only relies on the shared\nvocabulary and bilingual contexts to encourage the correlation across\nlanguages, which is loose and implicit for aligning the contextual\nrepresentations between languages. In this paper, we plug a cross-attention\nmodule into the Transformer encoder to explicitly build the interdependence\nbetween languages. It can effectively avoid the degeneration of predicting\nmasked words only conditioned on the context in its own language. More\nimportantly, when fine-tuning on downstream tasks, the cross-attention module\ncan be plugged in or out on-demand, thus naturally benefiting a wider range of\ncross-lingual tasks, from language understanding to generation.\n  As a result, the proposed cross-lingual model delivers new state-of-the-art\nresults on various cross-lingual understanding tasks of the XTREME benchmark,\ncovering text classification, sequence labeling, question answering, and\nsentence retrieval. For cross-lingual generation tasks, it also outperforms all\nexisting cross-lingual models and state-of-the-art Transformer variants on\nWMT14 English-to-German and English-to-French translation datasets, with gains\nof up to 1~2 BLEU.", "published": "2020-10-30 03:41:38", "link": "http://arxiv.org/abs/2010.16046v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Radiology Reports via Memory-driven Transformer", "abstract": "Medical imaging is frequently used in clinical practice and trials for\ndiagnosis and treatment. Writing imaging reports is time-consuming and can be\nerror-prone for inexperienced radiologists. Therefore, automatically generating\nradiology reports is highly desired to lighten the workload of radiologists and\naccordingly promote clinical automation, which is an essential task to apply\nartificial intelligence to the medical domain. In this paper, we propose to\ngenerate radiology reports with memory-driven Transformer, where a relational\nmemory is designed to record key information of the generation process and a\nmemory-driven conditional layer normalization is applied to incorporating the\nmemory into the decoder of Transformer. Experimental results on two prevailing\nradiology report datasets, IU X-Ray and MIMIC-CXR, show that our proposed\napproach outperforms previous models with respect to both language generation\nmetrics and clinical evaluations. Particularly, this is the first work\nreporting the generation results on MIMIC-CXR to the best of our knowledge.\nFurther analyses also demonstrate that our approach is able to generate long\nreports with necessary medical terms as well as meaningful image-text attention\nmappings.", "published": "2020-10-30 04:08:03", "link": "http://arxiv.org/abs/2010.16056v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Logic-guided Semantic Representation Learning for Zero-Shot Relation\n  Classification", "abstract": "Relation classification aims to extract semantic relations between entity\npairs from the sentences. However, most existing methods can only identify seen\nrelation classes that occurred during training. To recognize unseen relations\nat test time, we explore the problem of zero-shot relation classification.\nPrevious work regards the problem as reading comprehension or textual\nentailment, which have to rely on artificial descriptive information to improve\nthe understandability of relation types. Thus, rich semantic knowledge of the\nrelation labels is ignored. In this paper, we propose a novel logic-guided\nsemantic representation learning model for zero-shot relation classification.\nOur approach builds connections between seen and unseen relations via implicit\nand explicit semantic representations with knowledge graph embeddings and logic\nrules. Extensive experimental results demonstrate that our method can\ngeneralize to unseen relation types and achieve promising improvements.", "published": "2020-10-30 04:30:09", "link": "http://arxiv.org/abs/2010.16068v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Target Word Masking for Location Metonymy Resolution", "abstract": "Existing metonymy resolution approaches rely on features extracted from\nexternal resources like dictionaries and hand-crafted lexical resources. In\nthis paper, we propose an end-to-end word-level classification approach based\nonly on BERT, without dependencies on taggers, parsers, curated dictionaries of\nplace names, or other external resources. We show that our approach achieves\nthe state-of-the-art on 5 datasets, surpassing conventional BERT models and\nbenchmarks by a large margin. We also show that our approach generalises well\nto unseen data.", "published": "2020-10-30 06:34:44", "link": "http://arxiv.org/abs/2010.16097v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Towards Accurate and Consistent Evaluation: A Dataset for\n  Distantly-Supervised Relation Extraction", "abstract": "In recent years, distantly-supervised relation extraction has achieved a\ncertain success by using deep neural networks. Distant Supervision (DS) can\nautomatically generate large-scale annotated data by aligning entity pairs from\nKnowledge Bases (KB) to sentences. However, these DS-generated datasets\ninevitably have wrong labels that result in incorrect evaluation scores during\ntesting, which may mislead the researchers. To solve this problem, we build a\nnew dataset NYTH, where we use the DS-generated data as training data and hire\nannotators to label test data. Compared with the previous datasets, NYT-H has a\nmuch larger test set and then we can perform more accurate and consistent\nevaluation. Finally, we present the experimental results of several widely used\nsystems on NYT-H. The experimental results show that the ranking lists of the\ncomparison systems on the DS-labelled test data and human-annotated test data\nare different. This indicates that our human-annotated data is necessary for\nevaluation of distantly-supervised relation extraction.", "published": "2020-10-30 13:52:52", "link": "http://arxiv.org/abs/2010.16275v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Sui Generis QA Approach using RoBERTa for Adverse Drug Event\n  Identification", "abstract": "Extraction of adverse drug events from biomedical literature and other\ntextual data is an important component to monitor drug-safety and this has\nattracted attention of many researchers in healthcare. Existing works are more\npivoted around entity-relation extraction using bidirectional long short term\nmemory networks (Bi-LSTM) which does not attain the best feature\nrepresentations. In this paper, we introduce a question answering framework\nthat exploits the robustness, masking and dynamic attention capabilities of\nRoBERTa by a technique of domain adaptation and attempt to overcome the\naforementioned limitations. Our model outperforms the prior work by 9.53%\nF1-Score.", "published": "2020-10-30 19:09:48", "link": "http://arxiv.org/abs/2011.00057v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A New Neural Search and Insights Platform for Navigating and Organizing\n  AI Research", "abstract": "To provide AI researchers with modern tools for dealing with the explosive\ngrowth of the research literature in their field, we introduce a new platform,\nAI Research Navigator, that combines classical keyword search with neural\nretrieval to discover and organize relevant literature. The system provides\nsearch at multiple levels of textual granularity, from sentences to\naggregations across documents, both in natural language and through navigation\nin a domain-specific Knowledge Graph. We give an overview of the overall\narchitecture of the system and of the components for document analysis,\nquestion answering, search, analytics, expert search, and recommendations.", "published": "2020-10-30 19:12:25", "link": "http://arxiv.org/abs/2011.00061v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Dynamic Data Selection for Curriculum Learning via Ability Estimation", "abstract": "Curriculum learning methods typically rely on heuristics to estimate the\ndifficulty of training examples or the ability of the model. In this work, we\npropose replacing difficulty heuristics with learned difficulty parameters. We\nalso propose Dynamic Data selection for Curriculum Learning via Ability\nEstimation (DDaCLAE), a strategy that probes model ability at each training\nepoch to select the best training examples at that point. We show that models\nusing learned difficulty and/or ability outperform heuristic-based curriculum\nlearning models on the GLUE classification tasks.", "published": "2020-10-30 20:01:56", "link": "http://arxiv.org/abs/2011.00080v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Structured Representations of Entity Names using Active\n  Learning and Weak Supervision", "abstract": "Structured representations of entity names are useful for many entity-related\ntasks such as entity normalization and variant generation. Learning the\nimplicit structured representations of entity names without context and\nexternal knowledge is particularly challenging. In this paper, we present a\nnovel learning framework that combines active learning and weak supervision to\nsolve this problem. Our experimental evaluation show that this framework\nenables the learning of high-quality models from merely a dozen or so labeled\nexamples.", "published": "2020-10-30 21:01:22", "link": "http://arxiv.org/abs/2011.00105v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge-Based Construction of Confusion Matrices for Multi-Label\n  Classification Algorithms using Semantic Similarity Measures", "abstract": "So far, multi-label classification algorithms have been evaluated using\nstatistical methods that do not consider the semantics of the considered\nclasses and that fully depend on abstract computations such as Bayesian\nReasoning. Currently, there are several attempts to develop ontology-based\nmethods for a better assessment of supervised classification algorithms. In\nthis research paper, we define a novel approach that aligns expected labels\nwith predicted labels in multi-label classification using ontology-driven\nfeature-based semantic similarity measures and we use it to develop a method\nfor creating precise confusion matrices for a more effective evaluation of\nmulti-label classification algorithms.", "published": "2020-10-30 21:18:19", "link": "http://arxiv.org/abs/2011.00109v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Improving Dialogue Breakdown Detection with Semi-Supervised Learning", "abstract": "Building user trust in dialogue agents requires smooth and consistent\ndialogue exchanges. However, agents can easily lose conversational context and\ngenerate irrelevant utterances. These situations are called dialogue breakdown,\nwhere agent utterances prevent users from continuing the conversation. Building\nsystems to detect dialogue breakdown allows agents to recover appropriately or\navoid breakdown entirely. In this paper we investigate the use of\nsemi-supervised learning methods to improve dialogue breakdown detection,\nincluding continued pre-training on the Reddit dataset and a manifold-based\ndata augmentation method. We demonstrate the effectiveness of these methods on\nthe Dialogue Breakdown Detection Challenge (DBDC) English shared task. Our\nsubmissions to the 2020 DBDC5 shared task place first, beating baselines and\nother submissions by over 12\\% accuracy. In ablations on DBDC4 data from 2019,\nour semi-supervised learning methods improve the performance of a baseline BERT\nmodel by 2\\% accuracy. These methods are applicable generally to any dialogue\ntask and provide a simple way to improve model performance.", "published": "2020-10-30 23:04:56", "link": "http://arxiv.org/abs/2011.00136v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-Domain Sentiment Classification with Contrastive Learning and\n  Mutual Information Maximization", "abstract": "Contrastive learning (CL) has been successful as a powerful representation\nlearning method. In this work we propose CLIM: Contrastive Learning with mutual\nInformation Maximization, to explore the potential of CL on cross-domain\nsentiment classification. To the best of our knowledge, CLIM is the first to\nadopt contrastive learning for natural language processing (NLP) tasks across\ndomains. Due to scarcity of labels on the target domain, we introduce mutual\ninformation maximization (MIM) apart from CL to exploit the features that best\nsupport the final prediction. Furthermore, MIM is able to maintain a relatively\nbalanced distribution of the model's prediction, and enlarges the margin\nbetween classes on the target domain. The larger margin increases our model's\nrobustness and enables the same classifier to be optimal across domains.\nConsequently, we achieve new state-of-the-art results on the Amazon-review\ndataset as well as the airlines dataset, showing the efficacy of our proposed\nmethod CLIM.", "published": "2020-10-30 06:12:01", "link": "http://arxiv.org/abs/2010.16088v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Comparison of Speaker Role Recognition and Speaker Enrollment Protocol\n  for conversational Clinical Interviews", "abstract": "Conversations between a clinician and a patient, in natural conditions, are\nvaluable sources of information for medical follow-up. The automatic analysis\nof these dialogues could help extract new language markers and speed-up the\nclinicians' reports. Yet, it is not clear which speech processing pipeline is\nthe most performing to detect and identify the speaker turns, especially for\nindividuals with speech and language disorders. Here, we proposed a split of\nthe data that allows conducting a comparative evaluation of speaker role\nrecognition and speaker enrollment methods to solve this task. We trained\nend-to-end neural network architectures to adapt to each task and evaluate each\napproach under the same metric. Experimental results are reported on\nnaturalistic clinical conversations between Neuropsychologist and Interviewees,\nat different stages of Huntington's disease. We found that our Speaker Role\nRecognition model gave the best performances. In addition, our study underlined\nthe importance of retraining models with in-domain data. Finally, we observed\nthat results do not depend on the demographics of the Interviewee, highlighting\nthe clinical relevance of our methods.", "published": "2020-10-30 09:07:37", "link": "http://arxiv.org/abs/2010.16131v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "HyperText: Endowing FastText with Hyperbolic Geometry", "abstract": "Natural language data exhibit tree-like hierarchical structures such as the\nhypernym-hyponym relations in WordNet. FastText, as the state-of-the-art text\nclassifier based on shallow neural network in Euclidean space, may not model\nsuch hierarchies precisely with limited representation capacity. Considering\nthat hyperbolic space is naturally suitable for modeling tree-like hierarchical\ndata, we propose a new model named HyperText for efficient text classification\nby endowing FastText with hyperbolic geometry. Empirically, we show that\nHyperText outperforms FastText on a range of text classification tasks with\nmuch reduced parameters.", "published": "2020-10-30 09:30:54", "link": "http://arxiv.org/abs/2010.16143v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SLM: Learning a Discourse Language Representation with Sentence\n  Unshuffling", "abstract": "We introduce Sentence-level Language Modeling, a new pre-training objective\nfor learning a discourse language representation in a fully self-supervised\nmanner. Recent pre-training methods in NLP focus on learning either bottom or\ntop-level language representations: contextualized word representations derived\nfrom language model objectives at one extreme and a whole sequence\nrepresentation learned by order classification of two given textual segments at\nthe other. However, these models are not directly encouraged to capture\nrepresentations of intermediate-size structures that exist in natural languages\nsuch as sentences and the relationships among them. To that end, we propose a\nnew approach to encourage learning of a contextualized sentence-level\nrepresentation by shuffling the sequence of input sentences and training a\nhierarchical transformer model to reconstruct the original ordering. Through\nexperiments on downstream tasks such as GLUE, SQuAD, and DiscoEval, we show\nthat this feature of our model improves the performance of the original BERT by\nlarge margins.", "published": "2020-10-30 13:33:41", "link": "http://arxiv.org/abs/2010.16249v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Critical Assessment of State-of-the-Art in Entity Alignment", "abstract": "In this work, we perform an extensive investigation of two state-of-the-art\n(SotA) methods for the task of Entity Alignment in Knowledge Graphs. Therefore,\nwe first carefully examine the benchmarking process and identify several\nshortcomings, which make the results reported in the original works not always\ncomparable. Furthermore, we suspect that it is a common practice in the\ncommunity to make the hyperparameter optimization directly on a test set,\nreducing the informative value of reported performance. Thus, we select a\nrepresentative sample of benchmarking datasets and describe their properties.\nWe also examine different initializations for entity representations since they\nare a decisive factor for model performance. Furthermore, we use a shared\ntrain/validation/test split for a fair evaluation setting in which we evaluate\nall methods on all datasets. In our evaluation, we make several interesting\nfindings. While we observe that most of the time SotA approaches perform better\nthan baselines, they have difficulties when the dataset contains noise, which\nis the case in most real-life applications. Moreover, we find out in our\nablation study that often different features of SotA methods are crucial for\ngood performance than previously assumed. The code is available at\nhttps://github.com/mberr/ea-sota-comparison.", "published": "2020-10-30 15:09:19", "link": "http://arxiv.org/abs/2010.16314v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Topic-Preserving Synthetic News Generation: An Adversarial Deep\n  Reinforcement Learning Approach", "abstract": "Nowadays, there exist powerful language models such as OpenAI's GPT-2 that\ncan generate readable text and can be fine-tuned to generate text for a\nspecific domain. Considering GPT-2, it cannot directly generate synthetic news\nwith respect to a given topic and the output of the language model cannot be\nexplicitly controlled. In this paper, we study the novel problem of\ntopic-preserving synthetic news generation. We propose a novel deep\nreinforcement learning-based method to control the output of GPT-2 with respect\nto a given news topic. When generating text using GPT-2, by default, the most\nprobable word is selected from the vocabulary. Instead of selecting the best\nword each time from GPT-2's output, an RL agent tries to select words that\noptimize the matching of a given topic. In addition, using a fake news detector\nas an adversary, we investigate generating realistic news using our proposed\nmethod. In this paper, we consider realistic news as news that cannot be easily\ndetected by a fake news classifier. Experimental results demonstrate the\neffectiveness of the proposed framework on generating topic-preserving news\ncontent than state-of-the-art baselines.", "published": "2020-10-30 15:29:16", "link": "http://arxiv.org/abs/2010.16324v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Domain-Specific Lexical Grounding in Noisy Visual-Textual Documents", "abstract": "Images can give us insights into the contextual meanings of words, but\ncurrent image-text grounding approaches require detailed annotations. Such\ngranular annotation is rare, expensive, and unavailable in most domain-specific\ncontexts. In contrast, unlabeled multi-image, multi-sentence documents are\nabundant. Can lexical grounding be learned from such documents, even though\nthey have significant lexical and visual overlap? Working with a case study\ndataset of real estate listings, we demonstrate the challenge of distinguishing\nhighly correlated grounded terms, such as \"kitchen\" and \"bedroom\", and\nintroduce metrics to assess this document similarity. We present a simple\nunsupervised clustering-based method that increases precision and recall beyond\nobject detection and image tagging baselines when evaluated on labeled subsets\nof the dataset. The proposed method is particularly effective for local\ncontextual meanings of a word, for example associating \"granite\" with\ncountertops in the real estate dataset and with rocky landscapes in a Wikipedia\ndataset.", "published": "2020-10-30 16:39:49", "link": "http://arxiv.org/abs/2010.16363v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Phoneme Based Neural Transducer for Large Vocabulary Speech Recognition", "abstract": "To join the advantages of classical and end-to-end approaches for speech\nrecognition, we present a simple, novel and competitive approach for\nphoneme-based neural transducer modeling. Different alignment label topologies\nare compared and word-end-based phoneme label augmentation is proposed to\nimprove performance. Utilizing the local dependency of phonemes, we adopt a\nsimplified neural network structure and a straightforward integration with the\nexternal word-level language model to preserve the consistency of seq-to-seq\nmodeling. We also present a simple, stable and efficient training procedure\nusing frame-wise cross-entropy loss. A phonetic context size of one is shown to\nbe sufficient for the best performance. A simplified scheduled sampling\napproach is applied for further improvement and different decoding approaches\nare briefly compared. The overall performance of our best model is comparable\nto state-of-the-art (SOTA) results for the TED-LIUM Release 2 and Switchboard\ncorpora.", "published": "2020-10-30 16:53:29", "link": "http://arxiv.org/abs/2010.16368v4", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Directional ASR: A New Paradigm for E2E Multi-Speaker Speech Recognition\n  with Source Localization", "abstract": "This paper proposes a new paradigm for handling far-field multi-speaker data\nin an end-to-end neural network manner, called directional automatic speech\nrecognition (D-ASR), which explicitly models source speaker locations. In\nD-ASR, the azimuth angle of the sources with respect to the microphone array is\ndefined as a latent variable. This angle controls the quality of separation,\nwhich in turn determines the ASR performance. All three functionalities of\nD-ASR: localization, separation, and recognition are connected as a single\ndifferentiable neural network and trained solely based on ASR error\nminimization objectives. The advantages of D-ASR over existing methods are\nthreefold: (1) it provides explicit speaker locations, (2) it improves the\nexplainability factor, and (3) it achieves better ASR performance as the\nprocess is more streamlined. In addition, D-ASR does not require explicit\ndirection of arrival (DOA) supervision like existing data-driven localization\nmodels, which makes it more appropriate for realistic data. For the case of two\nsource mixtures, D-ASR achieves an average DOA prediction error of less than\nthree degrees. It also outperforms a strong far-field multi-speaker end-to-end\nsystem in both separation quality and ASR performance.", "published": "2020-10-30 20:26:28", "link": "http://arxiv.org/abs/2011.00091v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Joint Masked CPC and CTC Training for ASR", "abstract": "Self-supervised learning (SSL) has shown promise in learning representations\nof audio that are useful for automatic speech recognition (ASR). But, training\nSSL models like wav2vec~2.0 requires a two-stage pipeline. In this paper we\ndemonstrate a single-stage training of ASR models that can utilize both\nunlabeled and labeled data. During training, we alternately minimize two\nlosses: an unsupervised masked Contrastive Predictive Coding (CPC) loss and the\nsupervised audio-to-text alignment loss Connectionist Temporal Classification\n(CTC). We show that this joint training method directly optimizes performance\nfor the downstream ASR task using unsupervised data while achieving similar\nword error rates to wav2vec~2.0 on the Librispeech 100-hour dataset. Finally,\nwe postulate that solving the contrastive task is a regularization for the\nsupervised CTC loss.", "published": "2020-10-30 20:28:20", "link": "http://arxiv.org/abs/2011.00093v2", "categories": ["cs.CL", "cs.LG", "cs.SD"], "primary_category": "cs.CL"}
{"title": "Biomedical Concept Relatedness -- A large EHR-based benchmark", "abstract": "A promising application of AI to healthcare is the retrieval of information\nfrom electronic health records (EHRs), e.g. to aid clinicians in finding\nrelevant information for a consultation or to recruit suitable patients for a\nstudy. This requires search capabilities far beyond simple string matching,\nincluding the retrieval of concepts (diagnoses, symptoms, medications, etc.)\nrelated to the one in question. The suitability of AI methods for such\napplications is tested by predicting the relatedness of concepts with known\nrelatedness scores. However, all existing biomedical concept relatedness\ndatasets are notoriously small and consist of hand-picked concept pairs. We\nopen-source a novel concept relatedness benchmark overcoming these issues: it\nis six times larger than existing datasets and concept pairs are chosen based\non co-occurrence in EHRs, ensuring their relevance for the application of\ninterest. We present an in-depth analysis of our new dataset and compare it to\nexisting ones, highlighting that it is not only larger but also complements\nexisting datasets in terms of the types of concepts included. Initial\nexperiments with state-of-the-art embedding methods show that our dataset is a\nchallenging new benchmark for testing concept relatedness models.", "published": "2020-10-30 12:20:18", "link": "http://arxiv.org/abs/2010.16218v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "\"Thy algorithm shalt not bear false witness\": An Evaluation of\n  Multiclass Debiasing Methods on Word Embeddings", "abstract": "With the vast development and employment of artificial intelligence\napplications, research into the fairness of these algorithms has been\nincreased. Specifically, in the natural language processing domain, it has been\nshown that social biases persist in word embeddings and are thus in danger of\namplifying these biases when used. As an example of social bias, religious\nbiases are shown to persist in word embeddings and the need for its removal is\nhighlighted. This paper investigates the state-of-the-art multiclass debiasing\ntechniques: Hard debiasing, SoftWEAT debiasing and Conceptor debiasing. It\nevaluates their performance when removing religious bias on a common basis by\nquantifying bias removal via the Word Embedding Association Test (WEAT), Mean\nAverage Cosine Similarity (MAC) and the Relative Negative Sentiment Bias\n(RNSB). By investigating the religious bias removal on three widely used word\nembeddings, namely: Word2Vec, GloVe, and ConceptNet, it is shown that the\npreferred method is ConceptorDebiasing. Specifically, this technique manages to\ndecrease the measured religious bias on average by 82,42%, 96,78% and 54,76%\nfor the three word embedding sets respectively.", "published": "2020-10-30 12:49:39", "link": "http://arxiv.org/abs/2010.16228v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Leveraging Extracted Model Adversaries for Improved Black Box Attacks", "abstract": "We present a method for adversarial input generation against black box models\nfor reading comprehension based question answering. Our approach is composed of\ntwo steps. First, we approximate a victim black box model via model extraction\n(Krishna et al., 2020). Second, we use our own white box method to generate\ninput perturbations that cause the approximate model to fail. These perturbed\ninputs are used against the victim. In experiments we find that our method\nimproves on the efficacy of the AddAny---a white box attack---performed on the\napproximate model by 25% F1, and the AddSent attack---a black box attack---by\n11% F1 (Jia and Liang, 2017).", "published": "2020-10-30 15:53:50", "link": "http://arxiv.org/abs/2010.16336v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Cross-lingual Natural Language Processing Framework for Infodemic\n  Management", "abstract": "The COVID-19 pandemic has put immense pressure on health systems which are\nfurther strained due to the misinformation surrounding it. Under such a\nsituation, providing the right information at the right time is crucial. There\nis a growing demand for the management of information spread using Artificial\nIntelligence. Hence, we have exploited the potential of Natural Language\nProcessing for identifying relevant information that needs to be disseminated\namongst the masses. In this work, we present a novel Cross-lingual Natural\nLanguage Processing framework to provide relevant information by matching daily\nnews with trusted guidelines from the World Health Organization. The proposed\npipeline deploys various techniques of NLP such as summarizers, word\nembeddings, and similarity metrics to provide users with news articles along\nwith a corresponding healthcare guideline. A total of 36 models were evaluated\nand a combination of LexRank based summarizer on Word2Vec embedding with Word\nMover distance metric outperformed all other models. This novel open-source\napproach can be used as a template for proactive dissemination of relevant\nhealthcare information in the midst of misinformation spread associated with\nepidemics.", "published": "2020-10-30 16:26:35", "link": "http://arxiv.org/abs/2010.16357v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bridging Text and Knowledge with Multi-Prototype Embedding for Few-Shot\n  Relational Triple Extraction", "abstract": "Current supervised relational triple extraction approaches require huge\namounts of labeled data and thus suffer from poor performance in few-shot\nsettings. However, people can grasp new knowledge by learning a few instances.\nTo this end, we take the first step to study the few-shot relational triple\nextraction, which has not been well understood. Unlike previous single-task\nfew-shot problems, relational triple extraction is more challenging as the\nentities and relations have implicit correlations. In this paper, We propose a\nnovel multi-prototype embedding network model to jointly extract the\ncomposition of relational triples, namely, entity pairs and corresponding\nrelations. To be specific, we design a hybrid prototypical learning mechanism\nthat bridges text and knowledge concerning both entities and relations. Thus,\nimplicit correlations between entities and relations are injected.\nAdditionally, we propose a prototype-aware regularization to learn more\nrepresentative prototypes. Experimental results demonstrate that the proposed\nmethod can improve the performance of the few-shot triple extraction.", "published": "2020-10-30 04:18:39", "link": "http://arxiv.org/abs/2010.16059v1", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Curated Dataset of Urban Scenes for Audio-Visual Scene Analysis", "abstract": "This paper introduces a curated dataset of urban scenes for audio-visual\nscene analysis which consists of carefully selected and recorded material. The\ndata was recorded in multiple European cities, using the same equipment, in\nmultiple locations for each scene, and is openly available. We also present a\ncase study for audio-visual scene recognition and show that joint modeling of\naudio and visual modalities brings significant performance gain compared to\nstate of the art uni-modal systems. Our approach obtained an 84.8% accuracy\ncompared to 75.8% for the audio-only and 68.4% for the video-only equivalent\nsystems.", "published": "2020-10-30 18:21:54", "link": "http://arxiv.org/abs/2011.00030v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Adversarial defense for deep speaker recognition using hybrid\n  adversarial training", "abstract": "Deep neural network based speaker recognition systems can easily be deceived\nby an adversary using minuscule imperceptible perturbations to the input speech\nsamples. These adversarial attacks pose serious security threats to the speaker\nrecognition systems that use speech biometric. To address this concern, in this\nwork, we propose a new defense mechanism based on a hybrid adversarial training\n(HAT) setup. In contrast to existing works on countermeasures against\nadversarial attacks in deep speaker recognition that only use class-boundary\ninformation by supervised cross-entropy (CE) loss, we propose to exploit\nadditional information from supervised and unsupervised cues to craft diverse\nand stronger perturbations for adversarial training. Specifically, we employ\nmulti-task objectives using CE, feature-scattering (FS), and margin losses to\ncreate adversarial perturbations and include them for adversarial training to\nenhance the robustness of the model. We conduct speaker recognition experiments\non the Librispeech dataset, and compare the performance with state-of-the-art\nprojected gradient descent (PGD)-based adversarial training which employs only\nCE objective. The proposed HAT improves adversarial accuracy by absolute 3.29%\nand 3.18% for PGD and Carlini-Wagner (CW) attacks respectively, while retaining\nhigh accuracy on benign examples.", "published": "2020-10-30 03:05:58", "link": "http://arxiv.org/abs/2010.16038v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Beamforming for measurements under disturbed propagation conditions\n  using numerically calculated Green's functions", "abstract": "Beamforming methods for sound source localization are usually based on\nfree-field Green's functions to model the sound propagation between source and\nmicrophone. This assumption is known to be incorrect for many industrial\napplications and the beamforming results can suffer from this inconsistency\nregarding both, accuracy of source power estimation, and accuracy of source\nlocalisation. The aim of this paper is to investigate whether the use of\nnumerically calculated Green's functions can improve the results of beamforming\nmeasurements.\n  The current test cases of numerical and experimental investigations consists\nof sources placed in a short rectangular duct. The measurement is performed\noutside the duct in a semi-anechoic chamber. A typical example for this kind of\ninstallation is a fan with a heat exchanger.\n  The Green's functions for this test case are calculated numerically using the\nboundary element method. These tailored Green's functions are used to calculate\nthe corresponding beamforming steering vectors. The weighting of the Green's\nfunctions in the steering vectors has a decisive influence on the beamforming\nresults. A generalization of the common steering vector formulations is given\nbased on two scalars. It is shown that arbitrary differentiable Green's\nfunctions can be used to find the correct source position or source power level\nby using the appropriate steering vector formulations.\n  Beamforming measurements are performed using a loudspeaker as a reference\nsource at different positions in the heat exchanger duct. The measurements are\nevaluated in the frequency domain and by means of different validation criteria\nit can be shown that the results with the numerical calculated Green's function\nare improved compared to free field beamforming especially at low frequencies.", "published": "2020-10-30 09:27:10", "link": "http://arxiv.org/abs/2010.16140v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multimodal Metric Learning for Tag-based Music Retrieval", "abstract": "Tag-based music retrieval is crucial to browse large-scale music libraries\nefficiently. Hence, automatic music tagging has been actively explored, mostly\nas a classification task, which has an inherent limitation: a fixed vocabulary.\nOn the other hand, metric learning enables flexible vocabularies by using\npretrained word embeddings as side information. Also, metric learning has\nalready proven its suitability for cross-modal retrieval tasks in other domains\n(e.g., text-to-image) by jointly learning a multimodal embedding space. In this\npaper, we investigate three ideas to successfully introduce multimodal metric\nlearning for tag-based music retrieval: elaborate triplet sampling, acoustic\nand cultural music information, and domain-specific word embeddings. Our\nexperimental results show that the proposed ideas enhance the retrieval system\nquantitatively, and qualitatively. Furthermore, we release the MSD500, a subset\nof the Million Song Dataset (MSD) containing 500 cleaned tags, 7 manually\nannotated tag categories, and user taste profiles.", "published": "2020-10-30 02:46:28", "link": "http://arxiv.org/abs/2010.16030v1", "categories": ["cs.IR", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
{"title": "Deep Speaker Vector Normalization with Maximum Gaussianality Training", "abstract": "Deep speaker embedding represents the state-of-the-art technique for speaker\nrecognition. A key problem with this approach is that the resulting deep\nspeaker vectors tend to be irregularly distributed. In previous research, we\nproposed a deep normalization approach based on a new discriminative\nnormalization flow (DNF) model, by which the distributions of individual\nspeakers are arguably transformed to homogeneous Gaussians. This normalization\nwas demonstrated to be effective, but despite this remarkable success, we\nempirically found that the latent codes produced by the DNF model are generally\nneither homogeneous nor Gaussian, although the model has assumed so. In this\npaper, we argue that this problem is largely attributed to the\nmaximum-likelihood (ML) training criterion of the DNF model, which aims to\nmaximize the likelihood of the observations but not necessarily improve the\nGaussianality of the latent codes. We therefore propose a new Maximum\nGaussianality (MG) training approach that directly maximizes the Gaussianality\nof the latent codes. Our experiments on two data sets, SITW and CNCeleb,\ndemonstrate that our new MG training approach can deliver much better\nperformance than the previous ML training, and exhibits improved domain\ngeneralizability, particularly with regard to cosine scoring.", "published": "2020-10-30 09:42:06", "link": "http://arxiv.org/abs/2010.16148v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AudVowelConsNet: A Phoneme-Level Based Deep CNN Architecture for\n  Clinical Depression Diagnosis", "abstract": "Depression is a common and serious mood disorder that negatively affects the\npatient's capacity of functioning normally in daily tasks. Speech is proven to\nbe a vigorous tool in depression diagnosis. Research in psychiatry concentrated\non performing fine-grained analysis on word-level speech components\ncontributing to the manifestation of depression in speech and revealed\nsignificant variations at the phoneme-level in depressed speech. On the other\nhand, research in Machine Learning-based automatic recognition of depression\nfrom speech focused on the exploration of various acoustic features for the\ndetection of depression and its severity level. Few have focused on\nincorporating phoneme-level speech components in automatic assessment systems.\nIn this paper, we propose an Artificial Intelligence (AI) based application for\nclinical depression recognition and assessment from speech. We investigate the\nacoustic characteristics of phoneme units, specifically vowels and consonants\nfor depression recognition via Deep Learning. We present and compare three\nspectrogram-based Deep Neural Network architectures, trained on phoneme\nconsonant and vowel units and their fusion respectively. Our experiments show\nthat the deep learned consonant-based acoustic characteristics lead to better\nrecognition results than vowel-based ones. The fusion of vowel and consonant\nspeech characteristics through a deep network significantly outperforms the\nsingle space networks as well as the state-of-art deep learning approaches on\nthe DAIC-WOZ database.", "published": "2020-10-30 11:36:26", "link": "http://arxiv.org/abs/2010.16201v2", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio Dequantization Using (Co)Sparse (Non)Convex Methods", "abstract": "The paper deals with the hitherto neglected topic of audio dequantization. It\nreviews the state-of-the-art sparsity-based approaches and proposes several new\nmethods. Convex as well as non-convex approaches are included, and all the\npresented formulations come in both the synthesis and analysis variants. In the\nexperiments the methods are evaluated using the signal-to-distortion ratio\n(SDR) and PEMO-Q, a perceptually motivated metric.", "published": "2020-10-30 17:30:17", "link": "http://arxiv.org/abs/2010.16386v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
