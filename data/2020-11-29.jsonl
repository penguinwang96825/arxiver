{"title": "Intrinsic Knowledge Evaluation on Chinese Language Models", "abstract": "Recent NLP tasks have benefited a lot from pre-trained language models (LM)\nsince they are able to encode knowledge of various aspects. However, current LM\nevaluations focus on downstream performance, hence lack to comprehensively\ninspect in which aspect and to what extent have they encoded knowledge. This\npaper addresses both queries by proposing four tasks on syntactic, semantic,\ncommonsense, and factual knowledge, aggregating to a total of $39,308$\nquestions covering both linguistic and world knowledge in Chinese. Throughout\nexperiments, our probes and knowledge data prove to be a reliable benchmark for\nevaluating pre-trained Chinese LMs. Our work is publicly available at\nhttps://github.com/ZhiruoWang/ChnEval.", "published": "2020-11-29 04:34:39", "link": "http://arxiv.org/abs/2011.14277v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inflating Topic Relevance with Ideology: A Case Study of Political\n  Ideology Bias in Social Topic Detection Models", "abstract": "We investigate the impact of political ideology biases in training data.\nThrough a set of comparison studies, we examine the propagation of biases in\nseveral widely-used NLP models and its effect on the overall retrieval\naccuracy. Our work highlights the susceptibility of large, complex models to\npropagating the biases from human-selected input, which may lead to a\ndeterioration of retrieval accuracy, and the importance of controlling for\nthese biases. Finally, as a way to mitigate the bias, we propose to learn a\ntext representation that is invariant to political ideology while still judging\ntopic relevance.", "published": "2020-11-29 05:54:03", "link": "http://arxiv.org/abs/2011.14293v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generative Pre-training for Paraphrase Generation by Representing and\n  Predicting Spans in Exemplars", "abstract": "Paraphrase generation is a long-standing problem and serves an essential role\nin many natural language processing problems. Despite some encouraging results,\nrecent methods either confront the problem of favoring generic utterance or\nneed to retrain the model from scratch for each new dataset. This paper\npresents a novel approach to paraphrasing sentences, extended from the GPT-2\nmodel. We develop a template masking technique, named first-order masking, to\nmasked out irrelevant words in exemplars utilizing POS taggers. So that, the\nparaphrasing task is changed to predicting spans in masked templates. Our\nproposed approach outperforms competitive baselines, especially in the semantic\npreservation aspect. To prevent the model from being biased towards a given\ntemplate, we introduce a technique, referred to as second-order masking, which\nutilizes Bernoulli distribution to control the visibility of the\nfirst-order-masked template's tokens. Moreover, this technique allows the model\nto provide various paraphrased sentences in testing by adjusting the\nsecond-order-masking level. For scale-up objectives, we compare the performance\nof two alternatives template-selection methods, which shows that they were\nequivalent in preserving semantic information.", "published": "2020-11-29 11:36:13", "link": "http://arxiv.org/abs/2011.14344v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improved Semantic Role Labeling using Parameterized Neighborhood Memory\n  Adaptation", "abstract": "Deep neural models achieve some of the best results for semantic role\nlabeling. Inspired by instance-based learning that utilizes nearest neighbors\nto handle low-frequency context-specific training samples, we investigate the\nuse of memory adaptation techniques in deep neural models. We propose a\nparameterized neighborhood memory adaptive (PNMA) method that uses a\nparameterized representation of the nearest neighbors of tokens in a memory of\nactivations and makes predictions based on the most similar samples in the\ntraining data. We empirically show that PNMA consistently improves the SRL\nperformance of the base model irrespective of types of word embeddings. Coupled\nwith contextualized word embeddings derived from BERT, PNMA improves over\nexisting models for both span and dependency semantic parsing datasets,\nespecially on out-of-domain text, reaching F1 scores of 80.2, and 84.97 on\nCoNLL2005, and CoNLL2009 datasets, respectively.", "published": "2020-11-29 22:51:25", "link": "http://arxiv.org/abs/2011.14459v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Boundary Regression Model for Nested Named Entity Recognition", "abstract": "Recognizing named entities (NEs) is commonly conducted as a classification\nproblem that predicts a class tag for a word or a NE candidate in a sentence.\nIn shallow structures, categorized features are weighted to support the\nprediction. Recent developments in neural networks have adopted deep structures\nthat map categorized features into continuous representations. This approach\nunfolds a dense space saturated with high-order abstract semantic information,\nwhere the prediction is based on distributed feature representations. In this\npaper, positions of NEs in a sentence are represented as continuous values.\nThen, a regression operation is introduced to regress boundaries of NEs in a\nsentence. Based on boundary regression, we design a boundary regression model\nto support nested NE recognition. It is a multiobjective learning framework,\nwhich simultaneously predicts the classification score of a NE candidate and\nrefine its spatial location in a sentence. It has the advantage to resolve\nnested NEs and support boundary regression for locating NEs in a sntence. By\nsharing parameters for predicting and locating, this model enables more potent\nnonlinear function approximators to enhance model discriminability. Experiments\ndemonstrate state-of-the-art performance for nested NE recognition\\footnote{Our\ncodes to implement the BR model are available at:\n\\url{https://github.com/wuyuefei3/BR}.}.", "published": "2020-11-29 10:04:38", "link": "http://arxiv.org/abs/2011.14330v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Latent Template Induction with Gumbel-CRFs", "abstract": "Learning to control the structure of sentences is a challenging problem in\ntext generation. Existing work either relies on simple deterministic approaches\nor RL-based hard structures. We explore the use of structured variational\nautoencoders to infer latent templates for sentence generation using a soft,\ncontinuous relaxation in order to utilize reparameterization for training.\nSpecifically, we propose a Gumbel-CRF, a continuous relaxation of the CRF\nsampling algorithm using a relaxed Forward-Filtering Backward-Sampling (FFBS)\napproach. As a reparameterized gradient estimator, the Gumbel-CRF gives more\nstable gradients than score-function based estimators. As a structured\ninference network, we show that it learns interpretable templates during\ntraining, which allows us to control the decoder during testing. We demonstrate\nthe effectiveness of our methods with experiments on data-to-text generation\nand unsupervised paraphrase generation.", "published": "2020-11-29 01:00:57", "link": "http://arxiv.org/abs/2011.14244v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Novel Sentiment Analysis Engine for Preliminary Depression Status\n  Estimation on Social Media", "abstract": "Text sentiment analysis for preliminary depression status estimation of users\non social media is a widely exercised and feasible method, However, the immense\nvariety of users accessing the social media websites and their ample mix of\nvocabularies makes it difficult for commonly applied deep learning-based\nclassifiers to perform. To add to the situation, the lack of adaptability of\ntraditional supervised machine learning could hurt at many levels. We propose a\ncloud-based smartphone application, with a deep learning-based backend to\nprimarily perform depression detection on Twitter social media. The backend\nmodel consists of a RoBERTa based siamese sentence classifier that compares a\ngiven tweet (Query) with a labeled set of tweets with known sentiment (\nStandard Corpus ). The standard corpus is varied over time with expert opinion\nso as to improve the model's reliability. A psychologist ( with the patient's\npermission ) could leverage the application to assess the patient's depression\nstatus prior to counseling, which provides better insight into the mental\nhealth status of a patient. In addition, to the same, the psychologist could be\nreferred to cases of similar characteristics, which could in turn help in more\neffective treatment. We evaluate our backend model after fine-tuning it on a\npublicly available dataset. The find tuned model is made to predict depression\non a large set of tweet samples with random noise factors. The model achieved\npinnacle results, with a testing accuracy of 87.23% and an AUC of 0.8621.", "published": "2020-11-29 04:42:53", "link": "http://arxiv.org/abs/2011.14280v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Coarse-to-Fine Memory Matching for Joint Retrieval and Classification", "abstract": "We present a novel end-to-end language model for joint retrieval and\nclassification, unifying the strengths of bi- and cross- encoders into a single\nlanguage model via a coarse-to-fine memory matching search procedure for\nlearning and inference. Evaluated on the standard blind test set of the FEVER\nfact verification dataset, classification accuracy is significantly higher than\napproaches that only rely on the language model parameters as a knowledge base,\nand approaches some recent multi-model pipeline systems, using only a single\nBERT base model augmented with memory layers. We further demonstrate how\ncoupled retrieval and classification can be leveraged to identify low\nconfidence instances, and we extend exemplar auditing to this setting for\nanalyzing and constraining the model. As a result, our approach yields a means\nof updating language model behavior through two distinct mechanisms: The\nretrieved information can be updated explicitly, and the model behavior can be\nmodified via the exemplar database.", "published": "2020-11-29 05:06:03", "link": "http://arxiv.org/abs/2012.02287v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Dank or Not? -- Analyzing and Predicting the Popularity of Memes on\n  Reddit", "abstract": "Internet memes have become an increasingly pervasive form of contemporary\nsocial communication that attracted a lot of research interest recently. In\nthis paper, we analyze the data of 129,326 memes collected from Reddit in the\nmiddle of March, 2020, when the most serious coronavirus restrictions were\nbeing introduced around the world. This article not only provides a looking\nglass into the thoughts of Internet users during the COVID-19 pandemic but we\nalso perform a content-based predictive analysis of what makes a meme go viral.\nUsing machine learning methods, we also study what incremental predictive power\nimage related attributes have over textual attributes on meme popularity. We\nfind that the success of a meme can be predicted based on its content alone\nmoderately well, our best performing machine learning model predicts viral\nmemes with AUC=0.68. We also find that both image related and textual\nattributes have significant incremental predictive power over each other.", "published": "2020-11-29 09:57:17", "link": "http://arxiv.org/abs/2011.14326v2", "categories": ["cs.SI", "cs.CL", "cs.CV", "cs.CY", "physics.soc-ph", "J.4; I.2.10"], "primary_category": "cs.SI"}
{"title": "An Features Extraction and Recognition Method for Underwater Acoustic\n  Target Based on ATCNN", "abstract": "Facing the complex marine environment, it is extremely challenging to conduct\nunderwater acoustic target recognition (UATR) using ship-radiated noise.\nInspired by neural mechanism of auditory perception, this paper provides a new\ndeep neural network trained by original underwater acoustic signals with\ndepthwise separable convolution (DWS) and time-dilated convolution neural\nnetwork, named auditory perception inspired time-dilated convolution neural\nnetwork (ATCNN), and then implements detection and classification for\nunderwater acoustic signals. The proposed ATCNN model consists of learnable\nfeatures extractor and integration layer inspired by auditory perception, and\ntime-dilated convolution inspired by language model. This paper decomposes\noriginal time-domain ship-radiated noise signals into different frequency\ncomponents with depthwise separable convolution filter, and then extracts\nsignal features based on auditory perception. The deep features are integrated\non integration layer. The time-dilated convolution is used for long-term\ncontextual modeling. As a result, like language model, intra-class and\ninter-class information can be fully used for UATR. For UATR task, the\nclassification accuracy reaches 90.9%, which is the highest in contrast\nexperiment. Experimental results show that ATCNN has great potential to improve\nthe performance of UATR classification.", "published": "2020-11-29 11:11:38", "link": "http://arxiv.org/abs/2011.14336v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio-visual Speech Separation with Adversarially Disentangled Visual\n  Representation", "abstract": "Speech separation aims to separate individual voice from an audio mixture of\nmultiple simultaneous talkers. Although audio-only approaches achieve\nsatisfactory performance, they build on a strategy to handle the predefined\nconditions, limiting their application in the complex auditory scene. Towards\nthe cocktail party problem, we propose a novel audio-visual speech separation\nmodel. In our model, we use the face detector to detect the number of speakers\nin the scene and use visual information to avoid the permutation problem. To\nimprove our model's generalization ability to unknown speakers, we extract\nspeech-related visual features from visual inputs explicitly by the\nadversarially disentangled method, and use this feature to assist speech\nseparation. Besides, the time-domain approach is adopted, which could avoid the\nphase reconstruction problem existing in the time-frequency domain models. To\ncompare our model's performance with other models, we create two benchmark\ndatasets of 2-speaker mixture from GRID and TCDTIMIT audio-visual datasets.\nThrough a series of experiments, our proposed model is shown to outperform the\nstate-of-the-art audio-only model and three audio-visual models.", "published": "2020-11-29 10:48:42", "link": "http://arxiv.org/abs/2011.14334v1", "categories": ["cs.SD", "cs.CV", "eess.AS", "I.2.m"], "primary_category": "cs.SD"}
{"title": "Audio, Speech, Language, & Signal Processing for COVID-19: A\n  Comprehensive Overview", "abstract": "The Coronavirus (COVID-19) pandemic has been the research focus world-wide in\nthe year 2020. Several efforts, from collection of COVID-19 patients' data to\nscreening them for the virus's detection are taken with rigour. A major portion\nof COVID-19 symptoms are related to the functioning of the respiratory system,\nwhich in-turn critically influences the human speech production system. This\ndrives the research focus towards identifying the markers of COVID-19 in speech\nand other human generated audio signals. In this paper, we give an overview of\nthe speech and other audio signal, language and general signal processing-based\nwork done using Artificial Intelligence techniques to screen, diagnose,\nmonitor, and spread the awareness aboutCOVID-19. We also briefly describe the\nresearch related to detect accord-ing COVID-19 symptoms carried out so far. We\naspire that this collective information will be useful in developing automated\nsystems, which can help in the context of COVID-19 using non-obtrusive and easy\nto use modalities such as audio, speech, and language.", "published": "2020-11-29 21:33:59", "link": "http://arxiv.org/abs/2011.14445v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "68-02", "A.1"], "primary_category": "cs.SD"}
