{"title": "The MGB-2 Challenge: Arabic Multi-Dialect Broadcast Media Recognition", "abstract": "This paper describes the Arabic Multi-Genre Broadcast (MGB-2) Challenge for\nSLT-2016. Unlike last year's English MGB Challenge, which focused on\nrecognition of diverse TV genres, this year, the challenge has an emphasis on\nhandling the diversity in dialect in Arabic speech. Audio data comes from 19\ndistinct programmes from the Aljazeera Arabic TV channel between March 2005 and\nDecember 2015. Programmes are split into three groups: conversations,\ninterviews, and reports. A total of 1,200 hours have been released with lightly\nsupervised transcriptions for the acoustic modelling. For language modelling,\nwe made available over 110M words crawled from Aljazeera Arabic website\nAljazeera.net for a 10 year duration 2000-2011. Two lexicons have been\nprovided, one phoneme based and one grapheme based. Finally, two tasks were\nproposed for this year's challenge: standard speech transcription, and word\nalignment. This paper describes the task data and evaluation process used in\nthe MGB challenge, and summarises the results obtained.", "published": "2016-09-19 07:57:35", "link": "http://arxiv.org/abs/1609.05625v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-view Dimensionality Reduction for Dialect Identification of Arabic\n  Broadcast Speech", "abstract": "In this work, we present a new Vector Space Model (VSM) of speech utterances\nfor the task of spoken dialect identification. Generally, DID systems are built\nusing two sets of features that are extracted from speech utterances; acoustic\nand phonetic. The acoustic and phonetic features are used to form vector\nrepresentations of speech utterances in an attempt to encode information about\nthe spoken dialects. The Phonotactic and Acoustic VSMs, thus formed, are used\nfor the task of DID. The aim of this paper is to construct a single VSM that\nencodes information about spoken dialects from both the Phonotactic and\nAcoustic VSMs. Given the two views of the data, we make use of a well known\nmulti-view dimensionality reduction technique known as Canonical Correlation\nAnalysis (CCA), to form a single vector representation for each speech\nutterance that encodes dialect specific discriminative information from both\nthe phonetic and acoustic representations. We refer to this approach as feature\nspace combination approach and show that our CCA based feature vector\nrepresentation performs better on the Arabic DID task than the phonetic and\nacoustic feature representations used alone. We also present the feature space\ncombination approach as a viable alternative to the model based combination\napproach, where two DID systems are built using the two VSMs (Phonotactic and\nAcoustic) and the final prediction score is the output score combination from\nthe two systems.", "published": "2016-09-19 09:44:54", "link": "http://arxiv.org/abs/1609.05650v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Advances in All-Neural Speech Recognition", "abstract": "This paper advances the design of CTC-based all-neural (or end-to-end) speech\nrecognizers. We propose a novel symbol inventory, and a novel iterated-CTC\nmethod in which a second system is used to transform a noisy initial output\ninto a cleaner version. We present a number of stabilization and initialization\nmethods we have found useful in training these networks. We evaluate our system\non the commonly used NIST 2000 conversational telephony test set, and\nsignificantly exceed the previously published performance of similar systems,\nboth with and without the use of an external language model and decoding\ntechnology.", "published": "2016-09-19 20:52:44", "link": "http://arxiv.org/abs/1609.05935v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph-Structured Representations for Visual Question Answering", "abstract": "This paper proposes to improve visual question answering (VQA) with\nstructured representations of both scene contents and questions. A key\nchallenge in VQA is to require joint reasoning over the visual and text\ndomains. The predominant CNN/LSTM-based approach to VQA is limited by\nmonolithic vector representations that largely ignore structure in the scene\nand in the form of the question. CNN feature vectors cannot effectively capture\nsituations as simple as multiple object instances, and LSTMs process questions\nas series of words, which does not reflect the true complexity of language\nstructure. We instead propose to build graphs over the scene objects and over\nthe question words, and we describe a deep neural network that exploits the\nstructure in these representations. This shows significant benefit over the\nsequential processing of LSTMs. The overall efficacy of our approach is\ndemonstrated by significant improvements over the state-of-the-art, from 71.2%\nto 74.4% in accuracy on the \"abstract scenes\" multiple-choice benchmark, and\nfrom 34.7% to 39.1% in accuracy over pairs of \"balanced\" scenes, i.e. images\nwith fine-grained differences and opposite yes/no answers to a same question.", "published": "2016-09-19 05:21:36", "link": "http://arxiv.org/abs/1609.05600v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
