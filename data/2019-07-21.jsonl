{"title": "The Unbearable Weight of Generating Artificial Errors for Grammatical\n  Error Correction", "abstract": "In recent years, sequence-to-sequence models have been very effective for\nend-to-end grammatical error correction (GEC). As creating human-annotated\nparallel corpus for GEC is expensive and time-consuming, there has been work on\nartificial corpus generation with the aim of creating sentences that contain\nrealistic grammatical errors from grammatically correct sentences. In this\npaper, we investigate the impact of using recent neural models for generating\nerrors to help neural models to correct errors. We conduct a battery of\nexperiments on the effect of data size, models, and comparison with a\nrule-based approach.", "published": "2019-07-21 01:20:07", "link": "http://arxiv.org/abs/1907.08889v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hindi Visual Genome: A Dataset for Multimodal English-to-Hindi Machine\n  Translation", "abstract": "Visual Genome is a dataset connecting structured image information with\nEnglish language. We present ``Hindi Visual Genome'', a multimodal dataset\nconsisting of text and images suitable for English-Hindi multimodal machine\ntranslation task and multimodal research. We have selected short English\nsegments (captions) from Visual Genome along with associated images and\nautomatically translated them to Hindi with manual post-editing which took the\nassociated images into account. We prepared a set of 31525 segments,\naccompanied by a challenge test set of 1400 segments. This challenge test set\nwas created by searching for (particularly) ambiguous English words based on\nthe embedding similarity and manually selecting those where the image helps to\nresolve the ambiguity.\n  Our dataset is the first for multimodal English-Hindi machine translation,\nfreely available for non-commercial research purposes. Our Hindi version of\nVisual Genome also allows to create Hindi image labelers or other practical\ntools.\n  Hindi Visual Genome also serves in Workshop on Asian Translation (WAT) 2019\nMulti-Modal Translation Task.", "published": "2019-07-21 10:00:28", "link": "http://arxiv.org/abs/1907.08948v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Augmenting a BiLSTM tagger with a Morphological Lexicon and a Lexical\n  Category Identification Step", "abstract": "Previous work on using BiLSTM models for PoS tagging has primarily focused on\nsmall tagsets. We evaluate BiLSTM models for tagging Icelandic, a\nmorphologically rich language, using a relatively large tagset. Our baseline\nBiLSTM model achieves higher accuracy than any previously published tagger not\ntaking advantage of a morphological lexicon. When we extend the model by\nincorporating such data, we outperform previous state-of-the-art results by a\nsignificant margin. We also report on work in progress that attempts to address\nthe problem of data sparsity inherent in morphologically detailed, fine-grained\ntagsets. We experiment with training a separate model on only the lexical\ncategory and using the coarse-grained output tag as an input for the main\nmodel. This method further increases the accuracy and reduces the tagging\nerrors by 21.3% compared to previous state-of-the-art results. Finally, we\ntrain and test our tagger on a new gold standard for Icelandic.", "published": "2019-07-21 21:27:44", "link": "http://arxiv.org/abs/1907.09038v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Word Sense Disambiguation using Diffusion Kernel PCA", "abstract": "One of the major problems in natural language processing (NLP) is the word\nsense disambiguation (WSD) problem. It is the task of computationally\nidentifying the right sense of a polysemous word based on its context.\nResolving the WSD problem boosts the accuracy of many NLP focused algorithms\nsuch as text classification and machine translation. In this paper, we\nintroduce a new supervised algorithm for WSD, that is based on Kernel PCA and\nSemantic Diffusion Kernel, which is called Diffusion Kernel PCA (DKPCA). DKPCA\ngrasps the semantic similarities within terms, and it is based on PCA. These\nproperties enable us to perform feature extraction and dimension reduction\nguided by semantic similarities and within the algorithm. Our empirical results\non SensEval data demonstrate that DKPCA achieves higher or very close accuracy\nresults compared to SVM and KPCA with various well-known kernels when the\nlabeled data ratio is meager. Considering the scarcity of labeled data, whereas\nlarge quantities of unlabeled textual data are easily accessible, these are\nhighly encouraging first results to develop DKPCA further.", "published": "2019-07-21 07:16:55", "link": "http://arxiv.org/abs/1908.01832v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Using Word Embeddings to Examine Gender Bias in Dutch Newspapers,\n  1950-1990", "abstract": "Contemporary debates on filter bubbles and polarization in public and social\nmedia raise the question to what extent news media of the past exhibited\nbiases. This paper specifically examines bias related to gender in six Dutch\nnational newspapers between 1950 and 1990. We measure bias related to gender by\ncomparing local changes in word embedding models trained on newspapers with\ndivergent ideological backgrounds. We demonstrate clear differences in gender\nbias and changes within and between newspapers over time. In relation to themes\nsuch as sexuality and leisure, we see the bias moving toward women, whereas,\ngenerally, the bias shifts in the direction of men, despite growing female\nemployment number and feminist movements. Even though Dutch society became less\nstratified ideologically (depillarization), we found an increasing divergence\nin gender bias between religious and social-democratic on the one hand and\nliberal newspapers on the other. Methodologically, this paper illustrates how\nword embeddings can be used to examine historical language change. Future work\nwill investigate how fine-tuning deep contextualized embedding models, such as\nELMO, might be used for similar tasks with greater contextual information.", "published": "2019-07-21 06:58:22", "link": "http://arxiv.org/abs/1907.08922v1", "categories": ["cs.CL", "cs.CY", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Quantifying Similarity between Relations with Fact Distribution", "abstract": "We introduce a conceptually simple and effective method to quantify the\nsimilarity between relations in knowledge bases. Specifically, our approach is\nbased on the divergence between the conditional probability distributions over\nentity pairs. In this paper, these distributions are parameterized by a very\nsimple neural network. Although computing the exact similarity is in-tractable,\nwe provide a sampling-based method to get a good approximation. We empirically\nshow the outputs of our approach significantly correlate with human judgments.\nBy applying our method to various tasks, we also find that (1) our approach\ncould effectively detect redundant relations extracted by open information\nextraction (Open IE) models, that (2) even the most competitive models for\nrelational classification still make mistakes among very similar relations, and\nthat (3) our approach could be incorporated into negative sampling and softmax\nclassification to alleviate these mistakes. The source code and experiment\ndetails of this paper can be obtained from\nhttps://github.com/thunlp/relation-similarity.", "published": "2019-07-21 09:22:50", "link": "http://arxiv.org/abs/1907.08937v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.AI"}
{"title": "Are You Convinced? Choosing the More Convincing Evidence with a Siamese\n  Network", "abstract": "With the advancement in argument detection, we suggest to pay more attention\nto the challenging task of identifying the more convincing arguments. Machines\ncapable of responding and interacting with humans in helpful ways have become\nubiquitous. We now expect them to discuss with us the more delicate questions\nin our world, and they should do so armed with effective arguments. But what\nmakes an argument more persuasive? What will convince you? In this paper, we\npresent a new data set, IBM-EviConv, of pairs of evidence labeled for\nconvincingness, designed to be more challenging than existing alternatives. We\nalso propose a Siamese neural network architecture shown to outperform several\nbaselines on both a prior convincingness data set and our own. Finally, we\nprovide insights into our experimental results and the various kinds of\nargumentative value our method is capable of detecting.", "published": "2019-07-21 13:05:45", "link": "http://arxiv.org/abs/1907.08971v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Comprehensive Analysis of Twitter Trending Topics", "abstract": "In Twitter, a name, phrase, or topic that is mentioned at a greater rate than\nothers is called a \"trending topic\" or simply \"trend\". Twitter trends list has\na powerful ability to promote public events such as natural events, political\nscandals, market changes and other types of breaking news. Nevertheless, there\nhave been very few works focused on the dynamics of these trending topics. In\nthis article, we thoroughly examined the Twitter's trending topics of 2018. To\nthis end, we automatically accessed Twitter's trends API and stored the\nresulting 50 top trending topics in a novel dataset. We propose and analyze our\ndataset according to six criteria: lexical analysis, time to reach, trend\nreoccurrence, trending time, tweets count, and language analysis. Based on our\nresults, 77.6% of the topics that reached the Top-10 list were trending with\nless than 100k tweets. More than 50% of the topics could not hold the position\nfor more than an hour. English and Arabic languages comprised close to 40% and\n20% of the first rank topics, respectively.", "published": "2019-07-21 17:07:07", "link": "http://arxiv.org/abs/1907.09007v2", "categories": ["cs.SI", "cs.CL", "cs.IR", "stat.AP", "J.1; J.4"], "primary_category": "cs.SI"}
{"title": "Statistical Voice Conversion with Quasi-Periodic WaveNet Vocoder", "abstract": "In this paper, we investigate the effectiveness of a quasi-periodic WaveNet\n(QPNet) vocoder combined with a statistical spectral conversion technique for a\nvoice conversion task. The WaveNet (WN) vocoder has been applied as the\nwaveform generation module in many different voice conversion frameworks and\nachieves significant improvement over conventional vocoders. However, because\nof the fixed dilated convolution and generic network architecture, the WN\nvocoder lacks robustness against unseen input features and often requires a\nhuge network size to achieve acceptable speech quality. Such limitations\nusually lead to performance degradation in the voice conversion task. To\novercome this problem, the QPNet vocoder is applied, which includes a\npitch-dependent dilated convolution component to enhance the pitch\ncontrollability and attain a more compact network than the WN vocoder. In the\nproposed method, input spectral features are first converted using a framewise\ndeep neural network, and then the QPNet vocoder generates converted speech\nconditioned on the linearly converted prosodic and transformed spectral\nfeatures. The experimental results confirm that the QPNet vocoder achieves\nsignificantly better performance than the same-size WN vocoder while\nmaintaining comparable speech quality to the double-size WN vocoder. Index\nTerms: WaveNet, vocoder, voice conversion, pitch-dependent dilated convolution,\npitch controllability", "published": "2019-07-21 09:29:46", "link": "http://arxiv.org/abs/1907.08940v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
