{"title": "ReviewQA: a relational aspect-based opinion reading dataset", "abstract": "Deep reading models for question-answering have demonstrated promising\nperformance over the last couple of years. However current systems tend to\nlearn how to cleverly extract a span of the source document, based on its\nsimilarity with the question, instead of seeking for the appropriate answer.\nIndeed, a reading machine should be able to detect relevant passages in a\ndocument regarding a question, but more importantly, it should be able to\nreason over the important pieces of the document in order to produce an answer\nwhen it is required. To motivate this purpose, we present ReviewQA, a\nquestion-answering dataset based on hotel reviews. The questions of this\ndataset are linked to a set of relational understanding competencies that we\nexpect a model to master. Indeed, each question comes with an associated type\nthat characterizes the required competency. With this framework, it is possible\nto benchmark the main families of models and to get an overview of what are the\nstrengths and the weaknesses of a given model on the set of tasks evaluated in\nthis dataset. Our corpus contains more than 500.000 questions in natural\nlanguage over 100.000 hotel reviews. Our setup is projective, the answer of a\nquestion does not need to be extracted from a document, like in most of the\nrecent datasets, but selected among a set of candidates that contains all the\npossible answers to the questions of the dataset. Finally, we present several\nbaselines over this dataset.", "published": "2018-10-29 15:35:18", "link": "http://arxiv.org/abs/1810.12196v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Comment Generation by Leveraging User-Generated Data", "abstract": "Existing models on open-domain comment generation are difficult to train, and\nthey produce repetitive and uninteresting responses. The problem is due to\nmultiple and contradictory responses from a single article, and by the rigidity\nof retrieval methods. To solve this problem, we propose a combined approach to\nretrieval and generation methods. We propose an attentive scorer to retrieve\ninformative and relevant comments by leveraging user-generated data. Then, we\nuse such comments, together with the article, as input for a\nsequence-to-sequence model with copy mechanism. We show the robustness of our\nmodel and how it can alleviate the aforementioned issue by using a large scale\ncomment generation dataset. The result shows that the proposed generative model\nsignificantly outperforms strong baseline such as Seq2Seq with attention and\nInformation Retrieval models by around 27 and 30 BLEU-1 points respectively.", "published": "2018-10-29 17:23:33", "link": "http://arxiv.org/abs/1810.12264v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Content Selection in Deep Learning Models of Summarization", "abstract": "We carry out experiments with deep learning models of summarization across\nthe domains of news, personal stories, meetings, and medical articles in order\nto understand how content selection is performed. We find that many\nsophisticated features of state of the art extractive summarizers do not\nimprove performance over simpler models. These results suggest that it is\neasier to create a summarizer for a new domain than previous work suggests and\nbring into question the benefit of deep learning models for summarization for\nthose domains that do have massive datasets (i.e., news). At the same time,\nthey suggest important questions for new research in summarization; namely, new\nforms of sentence representations or external knowledge sources are needed that\nare better suited to the summarization task.", "published": "2018-10-29 18:42:46", "link": "http://arxiv.org/abs/1810.12343v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-label Multi-task Deep Learning for Behavioral Coding", "abstract": "We propose a methodology for estimating human behaviors in psychotherapy\nsessions using mutli-label and multi-task learning paradigms. We discuss the\nproblem of behavioral coding in which data of human interactions is the\nannotated with labels to describe relevant human behaviors of interest. We\ndescribe two related, yet distinct, corpora consisting of therapist client\ninteractions in psychotherapy sessions. We experimentally compare the proposed\nlearning approaches for estimating behaviors of interest in these datasets.\nSpecifically, we compare single and multiple label learning approaches, single\nand multiple task learning approaches, and evaluate the performance of these\napproaches when incorporating turn context. We demonstrate the prediction\nperformance gains which can be achieved by using the proposed paradigms and\ndiscuss the insights these models provide into these complex interactions.", "published": "2018-10-29 18:57:30", "link": "http://arxiv.org/abs/1810.12349v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Pragmatic Guide to Geoparsing Evaluation", "abstract": "Empirical methods in geoparsing have thus far lacked a standard evaluation\nframework describing the task, metrics and data used to compare\nstate-of-the-art systems. Evaluation is further made inconsistent, even\nunrepresentative of real-world usage by the lack of distinction between the\ndifferent types of toponyms, which necessitates new guidelines, a consolidation\nof metrics and a detailed toponym taxonomy with implications for Named Entity\nRecognition (NER) and beyond. To address these deficiencies, our manuscript\nintroduces a new framework in three parts. Part 1) Task Definition: clarified\nvia corpus linguistic analysis proposing a fine-grained Pragmatic Taxonomy of\nToponyms. Part 2) Metrics: discussed and reviewed for a rigorous evaluation\nincluding recommendations for NER/Geoparsing practitioners. Part 3) Evaluation\nData: shared via a new dataset called GeoWebNews to provide test/train examples\nand enable immediate use of our contributions. In addition to fine-grained\nGeotagging and Toponym Resolution (Geocoding), this dataset is also suitable\nfor prototyping and evaluating machine learning NLP models.", "published": "2018-10-29 19:22:12", "link": "http://arxiv.org/abs/1810.12368v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Better Internal Structure of Words for Sequence Labeling", "abstract": "Character-based neural models have recently proven very useful for many NLP\ntasks. However, there is a gap of sophistication between methods for learning\nrepresentations of sentences and words. While most character models for\nlearning representations of sentences are deep and complex, models for learning\nrepresentations of words are shallow and simple. Also, in spite of considerable\nresearch on learning character embeddings, it is still not clear which kind of\narchitecture is the best for capturing character-to-word representations. To\naddress these questions, we first investigate the gaps between methods for\nlearning word and sentence representations. We conduct detailed experiments and\ncomparisons of different state-of-the-art convolutional models, and also\ninvestigate the advantages and disadvantages of their constituents.\nFurthermore, we propose IntNet, a funnel-shaped wide convolutional neural\narchitecture with no down-sampling for learning representations of the internal\nstructure of words by composing their characters from limited, supervised\ntraining corpora. We evaluate our proposed model on six sequence labeling\ndatasets, including named entity recognition, part-of-speech tagging, and\nsyntactic chunking. Our in-depth analysis shows that IntNet significantly\noutperforms other character embedding models and obtains new state-of-the-art\nperformance without relying on any external knowledge or resources.", "published": "2018-10-29 22:39:49", "link": "http://arxiv.org/abs/1810.12443v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The problem with probabilistic DAG automata for semantic graphs", "abstract": "Semantic representations in the form of directed acyclic graphs (DAGs) have\nbeen introduced in recent years, and to model them, we need probabilistic\nmodels of DAGs. One model that has attracted some attention is the DAG\nautomaton, but it has not been studied as a probabilistic model. We show that\nsome DAG automata cannot be made into useful probabilistic models by the nearly\nuniversal strategy of assigning weights to transitions. The problem affects\nsingle-rooted, multi-rooted, and unbounded-degree variants of DAG automata, and\nappears to be pervasive. It does not affect planar variants, but these are\nproblematic for other reasons.", "published": "2018-10-29 17:24:57", "link": "http://arxiv.org/abs/1810.12266v2", "categories": ["cs.FL", "cs.CL"], "primary_category": "cs.FL"}
{"title": "Language Modeling with Sparse Product of Sememe Experts", "abstract": "Most language modeling methods rely on large-scale data to statistically\nlearn the sequential patterns of words. In this paper, we argue that words are\natomic language units but not necessarily atomic semantic units. Inspired by\nHowNet, we use sememes, the minimum semantic units in human languages, to\nrepresent the implicit semantics behind words for language modeling, named\nSememe-Driven Language Model (SDLM). More specifically, to predict the next\nword, SDLM first estimates the sememe distribution gave textual context.\nAfterward, it regards each sememe as a distinct semantic expert, and these\nexperts jointly identify the most probable senses and the corresponding word.\nIn this way, SDLM enables language models to work beyond word-level\nmanipulation to fine-grained sememe-level semantics and offers us more powerful\ntools to fine-tune language models and improve the interpretability as well as\nthe robustness of language models. Experiments on language modeling and the\ndownstream application of headline gener- ation demonstrate the significant\neffect of SDLM. Source code and data used in the experiments can be accessed at\nhttps:// github.com/thunlp/SDLM-pytorch.", "published": "2018-10-29 20:13:05", "link": "http://arxiv.org/abs/1810.12387v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Parallel Attention Mechanisms in Neural Machine Translation", "abstract": "Recent papers in neural machine translation have proposed the strict use of\nattention mechanisms over previous standards such as recurrent and\nconvolutional neural networks (RNNs and CNNs). We propose that by running\ntraditionally stacked encoding branches from encoder-decoder attention- focused\narchitectures in parallel, that even more sequential operations can be removed\nfrom the model and thereby decrease training time. In particular, we modify the\nrecently published attention-based architecture called Transformer by Google,\nby replacing sequential attention modules with parallel ones, reducing the\namount of training time and substantially improving BLEU scores at the same\ntime. Experiments over the English to German and English to French translation\ntasks show that our model establishes a new state of the art.", "published": "2018-10-29 21:58:13", "link": "http://arxiv.org/abs/1810.12427v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "STFT spectral loss for training a neural speech waveform model", "abstract": "This paper proposes a new loss using short-time Fourier transform (STFT)\nspectra for the aim of training a high-performance neural speech waveform model\nthat predicts raw continuous speech waveform samples directly. Not only\namplitude spectra but also phase spectra obtained from generated speech\nwaveforms are used to calculate the proposed loss. We also mathematically show\nthat training of the waveform model on the basis of the proposed loss can be\ninterpreted as maximum likelihood training that assumes the amplitude and phase\nspectra of generated speech waveforms following Gaussian and von Mises\ndistributions, respectively. Furthermore, this paper presents a simple network\narchitecture as the speech waveform model, which is composed of uni-directional\nlong short-term memories (LSTMs) and an auto-regressive structure. Experimental\nresults showed that the proposed neural model synthesized high-quality speech\nwaveforms.", "published": "2018-10-29 04:05:35", "link": "http://arxiv.org/abs/1810.11945v2", "categories": ["eess.AS", "cs.CL", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Investigation of enhanced Tacotron text-to-speech synthesis systems with\n  self-attention for pitch accent language", "abstract": "End-to-end speech synthesis is a promising approach that directly converts\nraw text to speech. Although it was shown that Tacotron2 outperforms classical\npipeline systems with regards to naturalness in English, its applicability to\nother languages is still unknown. Japanese could be one of the most difficult\nlanguages for which to achieve end-to-end speech synthesis, largely due to its\ncharacter diversity and pitch accents. Therefore, state-of-the-art systems are\nstill based on a traditional pipeline framework that requires a separate text\nanalyzer and duration model. Towards end-to-end Japanese speech synthesis, we\nextend Tacotron to systems with self-attention to capture long-term\ndependencies related to pitch accents and compare their audio quality with\nclassical pipeline systems under various conditions to show their pros and\ncons. In a large-scale listening test, we investigated the impacts of the\npresence of accentual-type labels, the use of force or predicted alignments,\nand acoustic features used as local condition parameters of the Wavenet\nvocoder. Our results reveal that although the proposed systems still do not\nmatch the quality of a top-line pipeline system for Japanese, we show important\nstepping stones towards end-to-end Japanese speech synthesis.", "published": "2018-10-29 05:25:21", "link": "http://arxiv.org/abs/1810.11960v2", "categories": ["eess.AS", "cs.CL", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "On Controllable Sparse Alternatives to Softmax", "abstract": "Converting an n-dimensional vector to a probability distribution over n\nobjects is a commonly used component in many machine learning tasks like\nmulticlass classification, multilabel classification, attention mechanisms etc.\nFor this, several probability mapping functions have been proposed and employed\nin literature such as softmax, sum-normalization, spherical softmax, and\nsparsemax, but there is very little understanding in terms how they relate with\neach other. Further, none of the above formulations offer an explicit control\nover the degree of sparsity. To address this, we develop a unified framework\nthat encompasses all these formulations as special cases. This framework\nensures simple closed-form solutions and existence of sub-gradients suitable\nfor learning via backpropagation. Within this framework, we propose two novel\nsparse formulations, sparsegen-lin and sparsehourglass, that seek to provide a\ncontrol over the degree of desired sparsity. We further develop novel convex\nloss functions that help induce the behavior of aforementioned formulations in\nthe multilabel classification setting, showing improved performance. We also\ndemonstrate empirically that the proposed formulations, when used to compute\nattention weights, achieve better or comparable performance on standard seq2seq\ntasks like neural machine translation and abstractive summarization.", "published": "2018-10-29 06:46:37", "link": "http://arxiv.org/abs/1810.11975v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Cascaded CNN-resBiLSTM-CTC: An End-to-End Acoustic Model For Speech\n  Recognition", "abstract": "Automatic speech recognition (ASR) tasks are resolved by end-to-end deep\nlearning models, which benefits us by less preparation of raw data, and easier\ntransformation between languages. We propose a novel end-to-end deep learning\nmodel architecture namely cascaded CNN-resBiLSTM-CTC. In the proposed model, we\nadd residual blocks in BiLSTM layers to extract sophisticated phoneme and\nsemantic information together, and apply cascaded structure to pay more\nattention mining information of hard negative samples. By applying both simple\nFast Fourier Transform (FFT) technique and n-gram language model (LM) rescoring\nmethod, we manage to achieve word error rate (WER) of 3.41% on LibriSpeech test\nclean corpora. Furthermore, we propose a new batch-varied method to speed up\nthe training process in length-varied tasks, which result in 25% less training\ntime.", "published": "2018-10-29 08:52:31", "link": "http://arxiv.org/abs/1810.12001v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speaking style adaptation in Text-To-Speech synthesis using\n  Sequence-to-sequence models with attention", "abstract": "Currently, there are increasing interests in text-to-speech (TTS) synthesis\nto use sequence-to-sequence models with attention. These models are end-to-end\nmeaning that they learn both co-articulation and duration properties directly\nfrom text and speech. Since these models are entirely data-driven, they need\nlarge amounts of data to generate synthetic speech with good quality. However,\nin challenging speaking styles, such as Lombard speech, it is difficult to\nrecord sufficiently large speech corpora. Therefore, in this study we propose a\ntransfer learning method to adapt a sequence-to-sequence based TTS system of\nnormal speaking style to Lombard style. Moreover, we experiment with a WaveNet\nvocoder in synthesis of Lombard speech. We conducted subjective evaluations to\nassess the performance of the adapted TTS systems. The subjective evaluation\nresults indicated that an adaptation system with the WaveNet vocoder clearly\noutperformed the conventional deep neural network based TTS system in synthesis\nof Lombard speech.", "published": "2018-10-29 10:53:31", "link": "http://arxiv.org/abs/1810.12051v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Do Explanations make VQA Models more Predictable to a Human?", "abstract": "A rich line of research attempts to make deep neural networks more\ntransparent by generating human-interpretable 'explanations' of their decision\nprocess, especially for interactive tasks like Visual Question Answering (VQA).\nIn this work, we analyze if existing explanations indeed make a VQA model --\nits responses as well as failures -- more predictable to a human. Surprisingly,\nwe find that they do not. On the other hand, we find that human-in-the-loop\napproaches that treat the model as a black-box do.", "published": "2018-10-29 19:14:26", "link": "http://arxiv.org/abs/1810.12366v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Learning to Screen for Fast Softmax Inference on Large Vocabulary Neural\n  Networks", "abstract": "Neural language models have been widely used in various NLP tasks, including\nmachine translation, next word prediction and conversational agents. However,\nit is challenging to deploy these models on mobile devices due to their slow\nprediction speed, where the bottleneck is to compute top candidates in the\nsoftmax layer. In this paper, we introduce a novel softmax layer approximation\nalgorithm by exploiting the clustering structure of context vectors. Our\nalgorithm uses a light-weight screening model to predict a much smaller set of\ncandidate words based on the given context, and then conducts an exact softmax\nonly within that subset. Training such a procedure end-to-end is challenging as\ntraditional clustering methods are discrete and non-differentiable, and thus\nunable to be used with back-propagation in the training process. Using the\nGumbel softmax, we are able to train the screening model end-to-end on the\ntraining set to exploit data distribution. The algorithm achieves an order of\nmagnitude faster inference than the original softmax layer for predicting\ntop-$k$ words in various tasks such as beam search in machine translation or\nnext words prediction. For example, for machine translation task on German to\nEnglish dataset with around 25K vocabulary, we can achieve 20.4 times speed up\nwith 98.9\\% precision@1 and 99.3\\% precision@5 with the original softmax layer\nprediction, while state-of-the-art ~\\citep{MSRprediction} only achieves 6.7x\nspeedup with 98.7\\% precision@1 and 98.1\\% precision@5 for the same task.", "published": "2018-10-29 20:59:56", "link": "http://arxiv.org/abs/1810.12406v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Accelerating System Log Processing by Semi-supervised Learning: A\n  Technical Report", "abstract": "There is an increasing need for more automated system-log analysis tools for\nlarge scale online system in a timely manner. However, conventional way to\nmonitor and classify the log output based on keyword list does not scale well\nfor complex system in which codes contributed by a large group of developers,\nwith diverse ways of encoding the error messages, often with misleading pre-set\nlabels. In this paper, we propose that the design of a large scale online log\nanalysis should follow the \"Least Prior Knowledge Principle\", in which\nunsupervised or semi-supervised solution with the minimal prior knowledge of\nthe log should be encoded directly. Thereby, we report our experience in\ndesigning a two-stage machine learning based method, in which the system logs\nare regarded as the output of a quasi-natural language, pre-filtered by a\nperplexity score threshold, and then undergo a fine-grained classification\nprocedure. Tests on empirical data show that our method has obvious advantage\nregarding to the processing speed and classification accuracy.", "published": "2018-10-29 00:28:26", "link": "http://arxiv.org/abs/1811.01833v1", "categories": ["cs.SE", "cs.CL", "cs.IR"], "primary_category": "cs.SE"}
{"title": "Audiovisual speaker conversion: jointly and simultaneously transforming\n  facial expression and acoustic characteristics", "abstract": "An audiovisual speaker conversion method is presented for simultaneously\ntransforming the facial expressions and voice of a source speaker into those of\na target speaker. Transforming the facial and acoustic features together makes\nit possible for the converted voice and facial expressions to be highly\ncorrelated and for the generated target speaker to appear and sound natural. It\nuses three neural networks: a conversion network that fuses and transforms the\nfacial and acoustic features, a waveform generation network that produces the\nwaveform from both the converted facial and acoustic features, and an image\nreconstruction network that outputs an RGB facial image also based on both the\nconverted features. The results of experiments using an emotional audiovisual\ndatabase showed that the proposed method achieved significantly higher\nnaturalness compared with one that separately transformed acoustic and facial\nfeatures.", "published": "2018-10-29 15:20:32", "link": "http://arxiv.org/abs/1810.12730v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "A Proper version of Synthesis-based Sparse Audio Declipper", "abstract": "Methods based on sparse representation have found great use in the recovery\nof audio signals degraded by clipping. The state of the art in declipping has\nbeen achieved by the SPADE algorithm by Kiti\\'c et. al. (LVA/ICA2015). Our\nrecent study (LVA/ICA2018) has shown that although the original S-SPADE can be\nimproved such that it converges significantly faster than the A-SPADE, the\nrestoration quality is significantly worse. In the present paper, we propose a\nnew version of S-SPADE. Experiments show that the novel version of S-SPADE\noutperforms its old version in terms of restoration quality, and that it is\ncomparable with the A-SPADE while being even slightly faster than A-SPADE.", "published": "2018-10-29 15:47:33", "link": "http://arxiv.org/abs/1810.12204v4", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Learning How to Listen: A Temporal-Frequential Attention Model for Sound\n  Event Detection", "abstract": "In this paper, we propose a temporal-frequential attention model for sound\nevent detection (SED). Our network learns how to listen with two attention\nmodels: a temporal attention model and a frequential attention model. Proposed\nsystem learns when to listen using the temporal attention model while it learns\nwhere to listen on the frequency axis using the frequential attention model.\nWith these two models, we attempt to make our system pay more attention to\nimportant frames or segments and important frequency components for sound event\ndetection. Our proposed method is demonstrated on the task 2 of Detection and\nClassification of Acoustic Scenes and Events (DCASE) 2017 Challenge and\nachieves competitive performance.", "published": "2018-10-29 03:37:55", "link": "http://arxiv.org/abs/1810.11939v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improved multipath time delay estimation using cepstrum subtraction", "abstract": "When a motor-powered vessel travels past a fixed hydrophone in a multipath\nenvironment, a Lloyd's mirror constructive/destructive interference pattern is\nobserved in the output spectrogram. The power cepstrum detects the periodic\nstructure of the Lloyd's mirror pattern by generating a sequence of pulses\n(rahmonics) located at the fundamental quefrency (periodic time) and its\nmultiples. This sequence is referred to here as the `rahmonic component' of the\npower cepstrum. The fundamental quefrency, which is the reciprocal of the\nfrequency difference between adjacent interference fringes, equates to the\nmultipath time delay. The other component of the power cepstrum is the\nnon-rahmonic (extraneous) component, which combines with the rahmonic component\nto form the (total) power cepstrum. A data processing technique, termed\n`cepstrum subtraction', is described. This technique suppresses the extraneous\ncomponent of the power cepstrum, leaving the rahmonic component that contains\nthe desired multipath time delay information. This technique is applied to real\nacoustic recordings of motor-vessel transits in a shallow water environment,\nwhere the broadband noise radiated by the vessel arrives at the hydrophone via\na direct ray path and a time-delayed multipath. The results show that cepstrum\nsubtraction improves multipath time delay estimation by a factor of two for the\nat-sea experiment.\n  keywords - time delay estimation, underwater acoustics, cepstrum, source\nlocalization, autocorrelation", "published": "2018-10-29 07:53:40", "link": "http://arxiv.org/abs/1810.11990v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An improved hybrid CTC-Attention model for speech recognition", "abstract": "Recently, end-to-end speech recognition with a hybrid model consisting of the\nconnectionist temporal classification(CTC) and the attention encoder-decoder\nachieved state-of-the-art results. In this paper, we propose a novel CTC\ndecoder structure based on the experiments we conducted and explore the\nrelation between decoding performance and the depth of encoder. We also apply\nattention smoothing mechanism to acquire more context information for\nsubword-based decoding. Taken together, these strategies allow us to achieve a\nword error rate(WER) of 4.43% without LM and 3.34% with RNN-LM on the\ntest-clean subset of the LibriSpeech corpora, which by far are the best\nreported WERs for end-to-end ASR systems on this dataset.", "published": "2018-10-29 09:28:33", "link": "http://arxiv.org/abs/1810.12020v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Contextual Speech Recognition with Difficult Negative Training Examples", "abstract": "Improving the representation of contextual information is key to unlocking\nthe potential of end-to-end (E2E) automatic speech recognition (ASR). In this\nwork, we present a novel and simple approach for training an ASR context\nmechanism with difficult negative examples. The main idea is to focus on proper\nnouns (e.g., unique entities such as names of people and places) in the\nreference transcript, and use phonetically similar phrases as negative\nexamples, encouraging the neural model to learn more discriminative\nrepresentations. We apply our approach to an end-to-end contextual ASR model\nthat jointly learns to transcribe and select the correct context items, and\nshow that our proposed method gives up to $53.1\\%$ relative improvement in word\nerror rate (WER) across several benchmarks.", "published": "2018-10-29 14:57:58", "link": "http://arxiv.org/abs/1810.12170v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Neural source-filter-based waveform model for statistical parametric\n  speech synthesis", "abstract": "Neural waveform models such as the WaveNet are used in many recent\ntext-to-speech systems, but the original WaveNet is quite slow in waveform\ngeneration because of its autoregressive (AR) structure. Although faster non-AR\nmodels were recently reported, they may be prohibitively complicated due to the\nuse of a distilling training method and the blend of other disparate training\ncriteria. This study proposes a non-AR neural source-filter waveform model that\ncan be directly trained using spectrum-based training criteria and the\nstochastic gradient descent method. Given the input acoustic features, the\nproposed model first uses a source module to generate a sine-based excitation\nsignal and then uses a filter module to transform the excitation signal into\nthe output speech waveform. Our experiments demonstrated that the proposed\nmodel generated waveforms at least 100 times faster than the AR WaveNet and the\nquality of its synthetic speech is close to that of speech generated by the AR\nWaveNet. Ablation test results showed that both the sine-wave excitation signal\nand the spectrum-based training criteria were essential to the performance of\nthe proposed model.", "published": "2018-10-29 04:10:40", "link": "http://arxiv.org/abs/1810.11946v4", "categories": ["eess.AS", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Audio inpainting of music by means of neural networks", "abstract": "We studied the ability of deep neural networks (DNNs) to restore missing\naudio content based on its context, a process usually referred to as audio\ninpainting. We focused on gaps in the range of tens of milliseconds. The\nproposed DNN structure was trained on audio signals containing music and\nmusical instruments, separately, with 64-ms long gaps. The input to the DNN was\nthe context, i.e., the signal surrounding the gap, transformed into\ntime-frequency (TF) coefficients. Our results were compared to those obtained\nfrom a reference method based on linear predictive coding (LPC). For music, our\nDNN significantly outperformed the reference method, demonstrating a generally\ngood usability of the proposed DNN structure for inpainting complex audio\nsignals like music.", "published": "2018-10-29 14:15:30", "link": "http://arxiv.org/abs/1810.12138v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "End-to-end music source separation: is it possible in the waveform\n  domain?", "abstract": "Most of the currently successful source separation techniques use the\nmagnitude spectrogram as input, and are therefore by default omitting part of\nthe signal: the phase. To avoid omitting potentially useful information, we\nstudy the viability of using end-to-end models for music source separation ---\nwhich take into account all the information available in the raw audio signal,\nincluding the phase. Although during the last decades end-to-end music source\nseparation has been considered almost unattainable, our results confirm that\nwaveform-based models can perform similarly (if not better) than a\nspectrogram-based deep learning model. Namely: a Wavenet-based model we propose\nand Wave-U-Net can outperform DeepConvSep, a recent spectrogram-based deep\nlearning model.", "published": "2018-10-29 15:24:34", "link": "http://arxiv.org/abs/1810.12187v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Enabling Factorized Piano Music Modeling and Generation with the MAESTRO\n  Dataset", "abstract": "Generating musical audio directly with neural networks is notoriously\ndifficult because it requires coherently modeling structure at many different\ntimescales. Fortunately, most music is also highly structured and can be\nrepresented as discrete note events played on musical instruments. Herein, we\nshow that by using notes as an intermediate representation, we can train a\nsuite of models capable of transcribing, composing, and synthesizing audio\nwaveforms with coherent musical structure on timescales spanning six orders of\nmagnitude (~0.1 ms to ~100 s), a process we call Wave2Midi2Wave. This large\nadvance in the state of the art is enabled by our release of the new MAESTRO\n(MIDI and Audio Edited for Synchronous TRacks and Organization) dataset,\ncomposed of over 172 hours of virtuosic piano performances captured with fine\nalignment (~3 ms) between note labels and audio waveforms. The networks and the\ndataset together present a promising approach toward creating new expressive\nand interpretable neural models of music.", "published": "2018-10-29 16:48:53", "link": "http://arxiv.org/abs/1810.12247v5", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
