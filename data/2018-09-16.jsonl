{"title": "Cross-Domain Labeled LDA for Cross-Domain Text Classification", "abstract": "Cross-domain text classification aims at building a classifier for a target\ndomain which leverages data from both source and target domain. One promising\nidea is to minimize the feature distribution differences of the two domains.\nMost existing studies explicitly minimize such differences by an exact\nalignment mechanism (aligning features by one-to-one feature alignment,\nprojection matrix etc.). Such exact alignment, however, will restrict models'\nlearning ability and will further impair models' performance on classification\ntasks when the semantic distributions of different domains are very different.\nTo address this problem, we propose a novel group alignment which aligns the\nsemantics at group level. In addition, to help the model learn better semantic\ngroups and semantics within these groups, we also propose a partial supervision\nfor model's learning in source domain. To this end, we embed the group\nalignment and a partial supervision into a cross-domain topic model, and\npropose a Cross-Domain Labeled LDA (CDL-LDA). On the standard 20Newsgroup and\nReuters dataset, extensive quantitative (classification, perplexity etc.) and\nqualitative (topic detection) experiments are conducted to show the\neffectiveness of the proposed group alignment and partial supervision.", "published": "2018-09-16 06:02:37", "link": "http://arxiv.org/abs/1809.05820v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dual Memory Network Model for Biased Product Review Classification", "abstract": "In sentiment analysis (SA) of product reviews, both user and product\ninformation are proven to be useful. Current tasks handle user profile and\nproduct information in a unified model which may not be able to learn salient\nfeatures of users and products effectively. In this work, we propose a dual\nuser and product memory network (DUPMN) model to learn user profiles and\nproduct reviews using separate memory networks. Then, the two representations\nare used jointly for sentiment prediction. The use of separate models aims to\ncapture user profiles and product information more effectively. Compared to\nstate-of-the-art unified prediction models, the evaluations on three benchmark\ndatasets, IMDB, Yelp13, and Yelp14, show that our dual learning model gives\nperformance gain of 0.6%, 1.2%, and 0.9%, respectively. The improvements are\nalso deemed very significant measured by p-values.", "published": "2018-09-16 03:56:21", "link": "http://arxiv.org/abs/1809.05807v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Meta-Embedding as Auxiliary Task Regularization", "abstract": "Word embeddings have been shown to benefit from ensambling several word\nembedding sources, often carried out using straightforward mathematical\noperations over the set of word vectors. More recently, self-supervised\nlearning has been used to find a lower-dimensional representation, similar in\nsize to the individual word embeddings within the ensemble. However, these\nmethods do not use the available manually labeled datasets that are often used\nsolely for the purpose of evaluation. We propose to reconstruct an ensemble of\nword embeddings as an auxiliary task that regularises a main task while both\ntasks share the learned meta-embedding layer. We carry out intrinsic evaluation\n(6 word similarity datasets and 3 analogy datasets) and extrinsic evaluation (4\ndownstream tasks). For intrinsic task evaluation, supervision comes from\nvarious labeled word similarity datasets. Our experimental results show that\nthe performance is improved for all word similarity datasets when compared to\nself-supervised learning methods with a mean increase of $11.33$ in Spearman\ncorrelation. Specifically, the proposed method shows the best performance in 4\nout of 6 of word similarity datasets when using a cosine reconstruction loss\nand Brier's word similarity loss. Moreover, improvements are also made when\nperforming word meta-embedding reconstruction in sequence tagging and sentence\nmeta-embedding for sentence classification.", "published": "2018-09-16 14:36:54", "link": "http://arxiv.org/abs/1809.05886v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generating Informative and Diverse Conversational Responses via\n  Adversarial Information Maximization", "abstract": "Responses generated by neural conversational models tend to lack\ninformativeness and diversity. We present Adversarial Information Maximization\n(AIM), an adversarial learning strategy that addresses these two related but\ndistinct problems. To foster response diversity, we leverage adversarial\ntraining that allows distributional matching of synthetic and real responses.\nTo improve informativeness, our framework explicitly optimizes a variational\nlower bound on pairwise mutual information between query and response.\nEmpirical results from automatic and human evaluations demonstrate that our\nmethods significantly boost informativeness and diversity.", "published": "2018-09-16 22:45:51", "link": "http://arxiv.org/abs/1809.05972v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Development of deep learning algorithms to categorize free-text notes\n  pertaining to diabetes: convolution neural networks achieve higher accuracy\n  than support vector machines", "abstract": "Health professionals can use natural language processing (NLP) technologies\nwhen reviewing electronic health records (EHR). Machine learning free-text\nclassifiers can help them identify problems and make critical decisions. We aim\nto develop deep learning neural network algorithms that identify EHR progress\nnotes pertaining to diabetes and validate the algorithms at two institutions.\nThe data used are 2,000 EHR progress notes retrieved from patients with\ndiabetes and all notes were annotated manually as diabetic or non-diabetic.\nSeveral deep learning classifiers were developed, and their performances were\nevaluated with the area under the ROC curve (AUC). The convolutional neural\nnetwork (CNN) model with a separable convolution layer accurately identified\ndiabetes-related notes in the Brigham and Womens Hospital testing set with the\nhighest AUC of 0.975. Deep learning classifiers can be used to identify EHR\nprogress notes pertaining to diabetes. In particular, the CNN-based classifier\ncan achieve a higher AUC than an SVM-based classifier.", "published": "2018-09-16 04:21:38", "link": "http://arxiv.org/abs/1809.05814v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Curriculum-Based Neighborhood Sampling For Sequence Prediction", "abstract": "The task of multi-step ahead prediction in language models is challenging\nconsidering the discrepancy between training and testing. At test time, a\nlanguage model is required to make predictions given past predictions as input,\ninstead of the past targets that are provided during training. This difference,\nknown as exposure bias, can lead to the compounding of errors along a generated\nsequence at test time.\n  In order to improve generalization in neural language models and address\ncompounding errors, we propose a curriculum learning based method that\ngradually changes an initially deterministic teacher policy to a gradually more\nstochastic policy, which we refer to as \\textit{Nearest-Neighbor Replacement\nSampling}. A chosen input at a given timestep is replaced with a sampled\nnearest neighbor of the past target with a truncated probability proportional\nto the cosine similarity between the original word and its top $k$ most similar\nwords. This allows the teacher to explore alternatives when the teacher\nprovides a sub-optimal policy or when the initial policy is difficult for the\nlearner to model. The proposed strategy is straightforward, online and requires\nlittle additional memory requirements. We report our main findings on two\nlanguage modelling benchmarks and find that the proposed approach performs\nparticularly well when used in conjunction with scheduled sampling, that too\nattempts to mitigate compounding errors in language models.", "published": "2018-09-16 17:17:56", "link": "http://arxiv.org/abs/1809.05916v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Cocktails, but no party: multipath-enabled private audio", "abstract": "We describe a private audio messaging system that uses echoes to unscramble\nmessages at a few predetermined locations in a room. The system works by\nsplitting the audio into short chunks and emitting them from different\nloudspeakers. The chunks are filtered so that as they echo around the room,\nthey sum to noise everywhere except at a few chosen focusing spots where they\nexactly reproduce the intended messages. Unlike in the case of standard\npersonal audio zones, the proposed method renders sound outside the focusing\nspots unintelligible. Our method essentially depends on echoes: the room acts\nas a mixing system such that at given points we get the desired output.\nFinally, we only require a modest number of loudspeakers and only a few impulse\nresponse measurements at points where the messages should be delivered. We\ndemonstrate the effectiveness of the proposed method via objective quantitative\nmetrics as well as informal listening experiments in a real room.", "published": "2018-09-16 12:28:19", "link": "http://arxiv.org/abs/1809.05862v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
