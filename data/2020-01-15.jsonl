{"title": "Non-Autoregressive Machine Translation with Disentangled Context\n  Transformer", "abstract": "State-of-the-art neural machine translation models generate a translation\nfrom left to right and every step is conditioned on the previously generated\ntokens. The sequential nature of this generation process causes fundamental\nlatency in inference since we cannot generate multiple tokens in each sentence\nin parallel. We propose an attention-masking based model, called Disentangled\nContext (DisCo) transformer, that simultaneously generates all tokens given\ndifferent contexts. The DisCo transformer is trained to predict every output\ntoken given an arbitrary subset of the other reference tokens. We also develop\nthe parallel easy-first inference algorithm, which iteratively refines every\ntoken in parallel and reduces the number of required iterations. Our extensive\nexperiments on 7 translation directions with varying data sizes demonstrate\nthat our model achieves competitive, if not better, performance compared to the\nstate of the art in non-autoregressive machine translation while significantly\nreducing decoding time on average. Our code is available at\nhttps://github.com/facebookresearch/DisCo.", "published": "2020-01-15 05:32:18", "link": "http://arxiv.org/abs/2001.05136v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation", "abstract": "Story generation, namely generating a reasonable story from a leading\ncontext, is an important but challenging task. In spite of the success in\nmodeling fluency and local coherence, existing neural language generation\nmodels (e.g., GPT-2) still suffer from repetition, logic conflicts, and lack of\nlong-range coherence in generated stories. We conjecture that this is because\nof the difficulty of associating relevant commonsense knowledge, understanding\nthe causal relationships, and planning entities and events with proper temporal\norder. In this paper, we devise a knowledge-enhanced pretraining model for\ncommonsense story generation. We propose to utilize commonsense knowledge from\nexternal knowledge bases to generate reasonable stories. To further capture the\ncausal and temporal dependencies between the sentences in a reasonable story,\nwe employ multi-task learning which combines a discriminative objective to\ndistinguish true and fake stories during fine-tuning. Automatic and manual\nevaluation shows that our model can generate more reasonable stories than\nstate-of-the-art baselines, particularly in terms of logic and global\ncoherence.", "published": "2020-01-15 05:42:27", "link": "http://arxiv.org/abs/2001.05139v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AvgOut: A Simple Output-Probability Measure to Eliminate Dull Responses", "abstract": "Many sequence-to-sequence dialogue models tend to generate safe,\nuninformative responses. There have been various useful efforts on trying to\neliminate them. However, these approaches either improve decoding algorithms\nduring inference, rely on hand-crafted features, or employ complex models. In\nour work, we build dialogue models that are dynamically aware of what\nutterances or tokens are dull without any feature-engineering. Specifically, we\nstart with a simple yet effective automatic metric, AvgOut, which calculates\nthe average output probability distribution of all time steps on the decoder\nside during training. This metric directly estimates which tokens are more\nlikely to be generated, thus making it a faithful evaluation of the model\ndiversity (i.e., for diverse models, the token probabilities should be more\nevenly distributed rather than peaked at a few dull tokens). We then leverage\nthis novel metric to propose three models that promote diversity without losing\nrelevance. The first model, MinAvgOut, directly maximizes the diversity score\nthrough the output distributions of each batch; the second model, Label\nFine-Tuning (LFT), prepends to the source sequence a label continuously scaled\nby the diversity score to control the diversity level; the third model, RL,\nadopts Reinforcement Learning and treats the diversity score as a reward\nsignal. Moreover, we experiment with a hybrid model by combining the loss terms\nof MinAvgOut and RL. All four models outperform their base LSTM-RNN model on\nboth diversity and relevance by a large margin, and are comparable to or better\nthan competitive baselines (also verified via human evaluation). Moreover, our\napproaches are orthogonal to the base model, making them applicable as an\nadd-on to other emerging better dialogue models in the future.", "published": "2020-01-15 18:32:06", "link": "http://arxiv.org/abs/2001.05467v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Unified System for Aggression Identification in English Code-Mixed and\n  Uni-Lingual Texts", "abstract": "Wide usage of social media platforms has increased the risk of aggression,\nwhich results in mental stress and affects the lives of people negatively like\npsychological agony, fighting behavior, and disrespect to others. Majority of\nsuch conversations contains code-mixed languages[28]. Additionally, the way\nused to express thought or communication style also changes from one social\nmedia plat-form to another platform (e.g., communication styles are different\nin twitter and Facebook). These all have increased the complexity of the\nproblem. To solve these problems, we have introduced a unified and robust\nmulti-modal deep learning architecture which works for English code-mixed\ndataset and uni-lingual English dataset both.The devised system, uses\npsycho-linguistic features and very ba-sic linguistic features. Our multi-modal\ndeep learning architecture contains, Deep Pyramid CNN, Pooled BiLSTM, and\nDisconnected RNN(with Glove and FastText embedding, both). Finally, the system\ntakes the decision based on model averaging. We evaluated our system on English\nCode-Mixed TRAC 2018 dataset and uni-lingual English dataset obtained from\nKaggle. Experimental results show that our proposed system outperforms all the\nprevious approaches on English code-mixed dataset and uni-lingual English\ndataset.", "published": "2020-01-15 17:06:29", "link": "http://arxiv.org/abs/2001.05493v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Teddy: A System for Interactive Review Analysis", "abstract": "Reviews are integral to e-commerce services and products. They contain a\nwealth of information about the opinions and experiences of users, which can\nhelp better understand consumer decisions and improve user experience with\nproducts and services. Today, data scientists analyze reviews by developing\nrules and models to extract, aggregate, and understand information embedded in\nthe review text. However, working with thousands of reviews, which are\ntypically noisy incomplete text, can be daunting without proper tools. Here we\nfirst contribute results from an interview study that we conducted with fifteen\ndata scientists who work with review text, providing insights into their\npractices and challenges. Results suggest data scientists need interactive\nsystems for many review analysis tasks. In response we introduce Teddy, an\ninteractive system that enables data scientists to quickly obtain insights from\nreviews and improve their extraction and modeling pipelines.", "published": "2020-01-15 08:19:01", "link": "http://arxiv.org/abs/2001.05171v1", "categories": ["cs.HC", "cs.CL", "cs.LG"], "primary_category": "cs.HC"}
{"title": "FGN: Fusion Glyph Network for Chinese Named Entity Recognition", "abstract": "Chinese NER is a challenging task. As pictographs, Chinese characters contain\nlatent glyph information, which is often overlooked. In this paper, we propose\nthe FGN, Fusion Glyph Network for Chinese NER. Except for adding glyph\ninformation, this method may also add extra interactive information with the\nfusion mechanism. The major innovations of FGN include: (1) a novel CNN\nstructure called CGS-CNN is proposed to capture both glyph information and\ninteractive information between glyphs from neighboring characters. (2) we\nprovide a method with sliding window and Slice-Attention to fuse the BERT\nrepresentation and glyph representation for a character, which may capture\npotential interactive knowledge between context and glyph. Experiments are\nconducted on four NER datasets, showing that FGN with LSTM-CRF as tagger\nachieves new state-of-the-arts performance for Chinese NER. Further, more\nexperiments are conducted to investigate the influences of various components\nand settings in FGN.", "published": "2020-01-15 12:39:20", "link": "http://arxiv.org/abs/2001.05272v6", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A Tree Adjoining Grammar Representation for Models Of Stochastic\n  Dynamical Systems", "abstract": "Model structure and complexity selection remains a challenging problem in\nsystem identification, especially for parametric non-linear models. Many\nEvolutionary Algorithm (EA) based methods have been proposed in the literature\nfor estimating model structure and complexity. In most cases, the proposed\nmethods are devised for estimating structure and complexity within a specified\nmodel class and hence these methods do not extend to other model structures\nwithout significant changes. In this paper, we propose a Tree Adjoining Grammar\n(TAG) for stochastic parametric models. TAGs can be used to generate models in\nan EA framework while imposing desirable structural constraints and\nincorporating prior knowledge. In this paper, we propose a TAG that can\nsystematically generate models ranging from FIRs to polynomial NARMAX models.\nFurthermore, we demonstrate that TAGs can be easily extended to more general\nmodel classes, such as the non-linear Box-Jenkins model class, enabling the\nrealization of flexible and automatic model structure and complexity selection\nvia EA.", "published": "2020-01-15 13:35:19", "link": "http://arxiv.org/abs/2001.05320v2", "categories": ["eess.SY", "cs.CL", "cs.NE", "cs.SY"], "primary_category": "eess.SY"}
{"title": "Stereotypical Bias Removal for Hate Speech Detection Task using\n  Knowledge-based Generalizations", "abstract": "With the ever-increasing cases of hate spread on social media platforms, it\nis critical to design abuse detection mechanisms to proactively avoid and\ncontrol such incidents. While there exist methods for hate speech detection,\nthey stereotype words and hence suffer from inherently biased training. Bias\nremoval has been traditionally studied for structured datasets, but we aim at\nbias mitigation from unstructured text data. In this paper, we make two\nimportant contributions. First, we systematically design methods to quantify\nthe bias for any model and propose algorithms for identifying the set of words\nwhich the model stereotypes. Second, we propose novel methods leveraging\nknowledge-based generalizations for bias-free learning. Knowledge-based\ngeneralization provides an effective way to encode knowledge because the\nabstraction they provide not only generalizes content but also facilitates\nretraction of information from the hate speech detection classifier, thereby\nreducing the imbalance. We experiment with multiple knowledge generalization\npolicies and analyze their effect on general performance and in mitigating\nbias. Our experiments with two real-world datasets, a Wikipedia Talk Pages\ndataset (WikiDetox) of size ~96k and a Twitter dataset of size ~24k, show that\nthe use of knowledge-based generalizations results in better performance by\nforcing the classifier to learn from generalized content. Our methods utilize\nexisting knowledge-bases and can easily be extended to other tasks", "published": "2020-01-15 18:17:36", "link": "http://arxiv.org/abs/2001.05495v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Insertion-Deletion Transformer", "abstract": "We propose the Insertion-Deletion Transformer, a novel transformer-based\nneural architecture and training method for sequence generation. The model\nconsists of two phases that are executed iteratively, 1) an insertion phase and\n2) a deletion phase. The insertion phase parameterizes a distribution of\ninsertions on the current output hypothesis, while the deletion phase\nparameterizes a distribution of deletions over the current output hypothesis.\nThe training method is a principled and simple algorithm, where the deletion\nmodel obtains its signal directly on-policy from the insertion model output. We\ndemonstrate the effectiveness of our Insertion-Deletion Transformer on\nsynthetic translation tasks, obtaining significant BLEU score improvement over\nan insertion-only model.", "published": "2020-01-15 20:26:48", "link": "http://arxiv.org/abs/2001.05540v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Ensemble based discriminative models for Visual Dialog Challenge 2018", "abstract": "This manuscript describes our approach for the Visual Dialog Challenge 2018.\nWe use an ensemble of three discriminative models with different encoders and\ndecoders for our final submission. Our best performing model on 'test-std'\nsplit achieves the NDCG score of 55.46 and the MRR value of 63.77, securing\nthird position in the challenge.", "published": "2020-01-15 08:20:54", "link": "http://arxiv.org/abs/2001.05865v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Show, Recall, and Tell: Image Captioning with Recall Mechanism", "abstract": "Generating natural and accurate descriptions in image cap-tioning has always\nbeen a challenge. In this paper, we pro-pose a novel recall mechanism to\nimitate the way human con-duct captioning. There are three parts in our recall\nmecha-nism : recall unit, semantic guide (SG) and recalled-wordslot (RWS).\nRecall unit is a text-retrieval module designedto retrieve recalled words for\nimages. SG and RWS are de-signed for the best use of recalled words. SG branch\ncangenerate a recalled context, which can guide the process ofgenerating\ncaption. RWS branch is responsible for copyingrecalled words to the caption.\nInspired by pointing mecha-nism in text summarization, we adopt a soft switch\nto balancethe generated-word probabilities between SG and RWS. Inthe CIDEr\noptimization step, we also introduce an individualrecalled-word reward (WR) to\nboost training. Our proposedmethods (SG+RWS+WR) achieve BLEU-4 / CIDEr /\nSPICEscores of 36.6 / 116.9 / 21.3 with cross-entropy loss and 38.7 /129.1 /\n22.4 with CIDEr optimization on MSCOCO Karpathytest split, which surpass the\nresults of other state-of-the-artmethods.", "published": "2020-01-15 16:32:51", "link": "http://arxiv.org/abs/2001.05876v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A Memory Augmented Architecture for Continuous Speaker Identification in\n  Meetings", "abstract": "We introduce and analyze a novel approach to the problem of speaker\nidentification in multi-party recorded meetings. Given a speech segment and a\nset of available candidate profiles, we propose a novel data-driven way to\nmodel the distance relations between them, aiming at identifying the speaker\nlabel corresponding to that segment. To achieve this we employ a recurrent,\nmemory-based architecture, since this class of neural networks has been shown\nto yield advanced performance in problems requiring relational reasoning. The\nproposed encoding of distance relations is shown to outperform traditional\ndistance metrics, such as the cosine distance. Additional improvements are\nreported when the temporal continuity of the audio signals and the speaker\nchanges is modeled in. In this paper, we have evaluated our method in two\ndifferent tasks, i.e. scripted and real-world business meeting scenarios, where\nwe report a relative reduction in speaker error rate of 39.28% and 51.84%,\nrespectively, compared to the baseline.", "published": "2020-01-15 03:32:18", "link": "http://arxiv.org/abs/2001.05118v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Deep Learning for MIR Tutorial", "abstract": "Deep Learning has become state of the art in visual computing and\ncontinuously emerges into the Music Information Retrieval (MIR) and audio\nretrieval domain. In order to bring attention to this topic we propose an\nintroductory tutorial on deep learning for MIR. Besides a general introduction\nto neural networks, the proposed tutorial covers a wide range of MIR relevant\ndeep learning approaches. \\textbf{Convolutional Neural Networks} are currently\na de-facto standard for deep learning based audio retrieval. \\textbf{Recurrent\nNeural Networks} have proven to be effective in onset detection tasks such as\nbeat or audio-event detection. \\textbf{Siamese Networks} have been shown\neffective in learning audio representations and distance functions specific for\nmusic similarity retrieval. We will incorporate both academic and industrial\npoints of view into the tutorial. Accompanying the tutorial, we will create a\nGithub repository for the content presented at the tutorial as well as\nreferences to state of the art work and literature for further reading. This\nrepository will remain public after the conference.", "published": "2020-01-15 12:23:17", "link": "http://arxiv.org/abs/2001.05266v1", "categories": ["cs.IR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
{"title": "Improving GANs for Speech Enhancement", "abstract": "Generative adversarial networks (GAN) have recently been shown to be\nefficient for speech enhancement. However, most, if not all, existing speech\nenhancement GANs (SEGAN) make use of a single generator to perform one-stage\nenhancement mapping. In this work, we propose to use multiple generators that\nare chained to perform multi-stage enhancement mapping, which gradually refines\nthe noisy input signals in a stage-wise fashion. Furthermore, we study two\nscenarios: (1) the generators share their parameters and (2) the generators'\nparameters are independent. The former constrains the generators to learn a\ncommon mapping that is iteratively applied at all enhancement stages and\nresults in a small model footprint. On the contrary, the latter allows the\ngenerators to flexibly learn different enhancement mappings at different stages\nof the network at the cost of an increased model size. We demonstrate that the\nproposed multi-stage enhancement approach outperforms the one-stage SEGAN\nbaseline, where the independent generators lead to more favorable results than\nthe tied generators. The source code is available at\nhttp://github.com/pquochuy/idsegan.", "published": "2020-01-15 19:57:03", "link": "http://arxiv.org/abs/2001.05532v3", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Transformer-based Online CTC/attention End-to-End Speech Recognition\n  Architecture", "abstract": "Recently, Transformer has gained success in automatic speech recognition\n(ASR) field. However, it is challenging to deploy a Transformer-based\nend-to-end (E2E) model for online speech recognition. In this paper, we propose\nthe Transformer-based online CTC/attention E2E ASR architecture, which contains\nthe chunk self-attention encoder (chunk-SAE) and the monotonic truncated\nattention (MTA) based self-attention decoder (SAD). Firstly, the chunk-SAE\nsplits the speech into isolated chunks. To reduce the computational cost and\nimprove the performance, we propose the state reuse chunk-SAE. Sencondly, the\nMTA based SAD truncates the speech features monotonically and performs\nattention on the truncated features. To support the online recognition, we\nintegrate the state reuse chunk-SAE and the MTA based SAD into online\nCTC/attention architecture. We evaluate the proposed online models on the HKUST\nMandarin ASR benchmark and achieve a 23.66% character error rate (CER) with a\n320 ms latency. Our online model yields as little as 0.19% absolute CER\ndegradation compared with the offline baseline, and achieves significant\nimprovement over our prior work on Long Short-Term Memory (LSTM) based online\nE2E models.", "published": "2020-01-15 14:36:19", "link": "http://arxiv.org/abs/2001.08290v2", "categories": ["eess.AS", "cs.LG", "cs.NE", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
