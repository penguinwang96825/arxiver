{"title": "Densely Connected Bidirectional LSTM with Applications to Sentence\n  Classification", "abstract": "Deep neural networks have recently been shown to achieve highly competitive\nperformance in many computer vision tasks due to their abilities of exploring\nin a much larger hypothesis space. However, since most deep architectures like\nstacked RNNs tend to suffer from the vanishing-gradient and overfitting\nproblems, their effects are still understudied in many NLP tasks. Inspired by\nthis, we propose a novel multi-layer RNN model called densely connected\nbidirectional long short-term memory (DC-Bi-LSTM) in this paper, which\nessentially represents each layer by the concatenation of its hidden state and\nall preceding layers' hidden states, followed by recursively passing each\nlayer's representation to all subsequent layers. We evaluate our proposed model\non five benchmark datasets of sentence classification. DC-Bi-LSTM with depth up\nto 20 can be successfully trained and obtain significant improvements over the\ntraditional Bi-LSTM with the same or even less parameters. Moreover, our model\nhas promising performance compared with the state-of-the-art approaches.", "published": "2018-02-03 01:40:20", "link": "http://arxiv.org/abs/1802.00889v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Left-Center-Right Separated Neural Network for Aspect-based Sentiment\n  Analysis with Rotatory Attention", "abstract": "Deep learning techniques have achieved success in aspect-based sentiment\nanalysis in recent years. However, there are two important issues that still\nremain to be further studied, i.e., 1) how to efficiently represent the target\nespecially when the target contains multiple words; 2) how to utilize the\ninteraction between target and left/right contexts to capture the most\nimportant words in them. In this paper, we propose an approach, called\nleft-center-right separated neural network with rotatory attention (LCR-Rot),\nto better address the two problems. Our approach has two characteristics: 1) it\nhas three separated LSTMs, i.e., left, center and right LSTMs, corresponding to\nthree parts of a review (left context, target phrase and right context); 2) it\nhas a rotatory attention mechanism which models the relation between target and\nleft/right contexts. The target2context attention is used to capture the most\nindicative sentiment words in left/right contexts. Subsequently, the\ncontext2target attention is used to capture the most important word in the\ntarget. This leads to a two-side representation of the target: left-aware\ntarget and right-aware target. We compare our approach on three benchmark\ndatasets with ten related methods proposed recently. The results show that our\napproach significantly outperforms the state-of-the-art techniques.", "published": "2018-02-03 01:43:37", "link": "http://arxiv.org/abs/1802.00892v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DeepType: Multilingual Entity Linking by Neural Type System Evolution", "abstract": "The wealth of structured (e.g. Wikidata) and unstructured data about the\nworld available today presents an incredible opportunity for tomorrow's\nArtificial Intelligence. So far, integration of these two different modalities\nis a difficult process, involving many decisions concerning how best to\nrepresent the information so that it will be captured or useful, and\nhand-labeling large amounts of data. DeepType overcomes this challenge by\nexplicitly integrating symbolic information into the reasoning process of a\nneural network with a type system. First we construct a type system, and\nsecond, we use it to constrain the outputs of a neural network to respect the\nsymbolic structure. We achieve this by reformulating the design problem into a\nmixed integer problem: create a type system and subsequently train a neural\nnetwork with it. In this reformulation discrete variables select which\nparent-child relations from an ontology are types within the type system, while\ncontinuous variables control a classifier fit to the type system. The original\nproblem cannot be solved exactly, so we propose a 2-step algorithm: 1)\nheuristic search or stochastic optimization over discrete variables that define\na type system informed by an Oracle and a Learnability heuristic, 2) gradient\ndescent to fit classifier parameters. We apply DeepType to the problem of\nEntity Linking on three standard datasets (i.e. WikiDisamb30, CoNLL (YAGO), TAC\nKBP 2010) and find that it outperforms all existing solutions by a wide margin,\nincluding approaches that rely on a human-designed type system or recent deep\nlearning-based entity embeddings, while explicitly using symbolic information\nlets it integrate new entities without retraining.", "published": "2018-02-03 20:13:42", "link": "http://arxiv.org/abs/1802.01021v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Content based Weighted Consensus Summarization", "abstract": "Multi-document summarization has received a great deal of attention in the\npast couple of decades. Several approaches have been proposed, many of which\nperform equally well and it is becoming in- creasingly difficult to choose one\nparticular system over another. An ensemble of such systems that is able to\nleverage the strengths of each individual systems can build a better and more\nrobust summary. Despite this, few attempts have been made in this direction. In\nthis paper, we describe a category of ensemble systems which use consensus\nbetween the candidate systems to build a better meta-summary. We highlight two\nmajor shortcomings of such systems: the inability to take into account relative\nperformance of individual systems and overlooking content of candidate\nsummaries in favour of the sentence rankings. We propose an alternate method,\ncontent-based weighted consensus summarization, which address these concerns.\nWe use pseudo-relevant summaries to estimate the performance of individual\ncandidate systems, and then use this information to generate a better aggregate\nranking. Experiments on DUC 2003 and DUC 2004 datasets show that the proposed\nsystem outperforms existing consensus-based techniques by a large margin.", "published": "2018-02-03 09:56:54", "link": "http://arxiv.org/abs/1802.00946v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Multi-attention Recurrent Network for Human Communication Comprehension", "abstract": "Human face-to-face communication is a complex multimodal signal. We use words\n(language modality), gestures (vision modality) and changes in tone (acoustic\nmodality) to convey our intentions. Humans easily process and understand\nface-to-face communication, however, comprehending this form of communication\nremains a significant challenge for Artificial Intelligence (AI). AI must\nunderstand each modality and the interactions between them that shape human\ncommunication. In this paper, we present a novel neural architecture for\nunderstanding human communication called the Multi-attention Recurrent Network\n(MARN). The main strength of our model comes from discovering interactions\nbetween modalities through time using a neural component called the\nMulti-attention Block (MAB) and storing them in the hybrid memory of a\nrecurrent component called the Long-short Term Hybrid Memory (LSTHM). We\nperform extensive comparisons on six publicly available datasets for multimodal\nsentiment analysis, speaker trait recognition and emotion recognition. MARN\nshows state-of-the-art performance on all the datasets.", "published": "2018-02-03 06:29:17", "link": "http://arxiv.org/abs/1802.00923v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Multimodal Sentiment Analysis with Word-Level Fusion and Reinforcement\n  Learning", "abstract": "With the increasing popularity of video sharing websites such as YouTube and\nFacebook, multimodal sentiment analysis has received increasing attention from\nthe scientific community. Contrary to previous works in multimodal sentiment\nanalysis which focus on holistic information in speech segments such as bag of\nwords representations and average facial expression intensity, we develop a\nnovel deep architecture for multimodal sentiment analysis that performs\nmodality fusion at the word level. In this paper, we propose the Gated\nMultimodal Embedding LSTM with Temporal Attention (GME-LSTM(A)) model that is\ncomposed of 2 modules. The Gated Multimodal Embedding alleviates the\ndifficulties of fusion when there are noisy modalities. The LSTM with Temporal\nAttention performs word level fusion at a finer fusion resolution between input\nmodalities and attends to the most important time steps. As a result, the\nGME-LSTM(A) is able to better model the multimodal structure of speech through\ntime and perform better sentiment comprehension. We demonstrate the\neffectiveness of this approach on the publicly-available Multimodal Corpus of\nSentiment Intensity and Subjectivity Analysis (CMU-MOSI) dataset by achieving\nstate-of-the-art sentiment classification and regression results. Qualitative\nanalysis on our model emphasizes the importance of the Temporal Attention Layer\nin sentiment prediction because the additional acoustic and visual modalities\nare noisy. We also demonstrate the effectiveness of the Gated Multimodal\nEmbedding in selectively filtering these noisy modalities out. Our results and\nanalysis open new areas in the study of sentiment analysis in human\ncommunication and provide new models for multimodal fusion.", "published": "2018-02-03 06:30:09", "link": "http://arxiv.org/abs/1802.00924v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
