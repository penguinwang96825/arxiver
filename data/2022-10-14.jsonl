{"title": "InterFair: Debiasing with Natural Language Feedback for Fair\n  Interpretable Predictions", "abstract": "Debiasing methods in NLP models traditionally focus on isolating information\nrelated to a sensitive attribute (e.g., gender or race). We instead argue that\na favorable debiasing method should use sensitive information 'fairly,' with\nexplanations, rather than blindly eliminating it. This fair balance is often\nsubjective and can be challenging to achieve algorithmically. We explore two\ninteractive setups with a frozen predictive model and show that users able to\nprovide feedback can achieve a better and fairer balance between task\nperformance and bias mitigation. In one setup, users, by interacting with test\nexamples, further decreased bias in the explanations (5-8%) while maintaining\nthe same prediction accuracy. In the other setup, human feedback was able to\ndisentangle associated bias and predictive information from the input leading\nto superior bias mitigation and improved task performance (4-5%)\nsimultaneously.", "published": "2022-10-14 00:54:12", "link": "http://arxiv.org/abs/2210.07440v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Word Sense Disambiguation with Unified Sense Representation", "abstract": "As a key natural language processing (NLP) task, word sense disambiguation\n(WSD) evaluates how well NLP models can understand the lexical semantics of\nwords under specific contexts. Benefited from the large-scale annotation,\ncurrent WSD systems have achieved impressive performances in English by\ncombining supervised learning with lexical knowledge. However, such success is\nhard to be replicated in other languages, where we only have limited\nannotations.In this paper, based on the multilingual lexicon BabelNet\ndescribing the same set of concepts across languages, we propose building\nknowledge and supervised-based Multilingual Word Sense Disambiguation (MWSD)\nsystems. We build unified sense representations for multiple languages and\naddress the annotation scarcity problem for MWSD by transferring annotations\nfrom rich-sourced languages to poorer ones. With the unified sense\nrepresentations, annotations from multiple languages can be jointly trained to\nbenefit the MWSD tasks. Evaluations of SemEval-13 and SemEval-15 datasets\ndemonstrate the effectiveness of our methodology.", "published": "2022-10-14 01:24:03", "link": "http://arxiv.org/abs/2210.07447v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controlling Bias Exposure for Fair Interpretable Predictions", "abstract": "Recent work on reducing bias in NLP models usually focuses on protecting or\nisolating information related to a sensitive attribute (like gender or race).\nHowever, when sensitive information is semantically entangled with the task\ninformation of the input, e.g., gender information is predictive for a\nprofession, a fair trade-off between task performance and bias mitigation is\ndifficult to achieve. Existing approaches perform this trade-off by eliminating\nbias information from the latent space, lacking control over how much bias is\nnecessarily required to be removed. We argue that a favorable debiasing method\nshould use sensitive information 'fairly', rather than blindly eliminating it\n(Caliskan et al., 2017; Sun et al., 2019; Bogen et al., 2020). In this work, we\nprovide a novel debiasing algorithm by adjusting the predictive model's belief\nto (1) ignore the sensitive information if it is not useful for the task; (2)\nuse sensitive information minimally as necessary for the prediction (while also\nincurring a penalty). Experimental results on two text classification tasks\n(influenced by gender) and an open-ended generation task (influenced by race)\nindicate that our model achieves a desirable trade-off between debiasing and\ntask performance along with producing debiased rationales as evidence.", "published": "2022-10-14 01:49:01", "link": "http://arxiv.org/abs/2210.07455v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Query Rewriting for Effective Misinformation Discovery", "abstract": "We propose a novel system to help fact-checkers formulate search queries for\nknown misinformation claims and effectively search across multiple social media\nplatforms. We introduce an adaptable rewriting strategy, where editing actions\nfor queries containing claims (e.g., swap a word with its synonym; change verb\ntense into present simple) are automatically learned through offline\nreinforcement learning. Our model uses a decision transformer to learn a\nsequence of editing actions that maximizes query retrieval metrics such as mean\naverage precision. We conduct a series of experiments showing that our query\nrewriting system achieves a relative increase in the effectiveness of the\nqueries of up to 42%, while producing editing action sequences that are human\ninterpretable.", "published": "2022-10-14 02:34:12", "link": "http://arxiv.org/abs/2210.07467v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transparency Helps Reveal When Language Models Learn Meaning", "abstract": "Many current NLP systems are built from language models trained to optimize\nunsupervised objectives on large amounts of raw text. Under what conditions\nmight such a procedure acquire meaning? Our systematic experiments with\nsynthetic data reveal that, with languages where all expressions have\ncontext-independent denotations (i.e., languages with strong transparency),\nboth autoregressive and masked language models successfully learn to emulate\nsemantic relations between expressions. However, when denotations are changed\nto be context-dependent with the language otherwise unmodified, this ability\ndegrades. Turning to natural language, our experiments with a specific\nphenomenon -- referential opacity -- add to the growing body of evidence that\ncurrent language models do not represent natural language semantics well. We\nshow this failure relates to the context-dependent nature of natural language\nform-meaning mappings.", "published": "2022-10-14 02:35:19", "link": "http://arxiv.org/abs/2210.07468v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "StyLEx: Explaining Style Using Human Lexical Annotations", "abstract": "Large pre-trained language models have achieved impressive results on various\nstyle classification tasks, but they often learn spurious domain-specific words\nto make predictions (Hayati et al., 2021). While human explanation highlights\nstylistic tokens as important features for this task, we observe that model\nexplanations often do not align with them. To tackle this issue, we introduce\nStyLEx, a model that learns from human-annotated explanations of stylistic\nfeatures and jointly learns to perform the task and predict these features as\nmodel explanations. Our experiments show that StyLEx can provide human-like\nstylistic lexical explanations without sacrificing the performance of\nsentence-level style prediction on both in-domain and out-of-domain datasets.\nExplanations from StyLEx show significant improvements in explanation metrics\n(sufficiency, plausibility) and when evaluated with human annotations. They are\nalso more understandable by human judges compared to the widely-used\nsaliency-based explanation baseline.", "published": "2022-10-14 02:35:47", "link": "http://arxiv.org/abs/2210.07469v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"John is 50 years old, can his son be 65?\" Evaluating NLP Models'\n  Understanding of Feasibility", "abstract": "In current NLP research, large-scale language models and their abilities are\nwidely being discussed. Some recent works have also found notable failures of\nthese models. Often these failure examples involve complex reasoning abilities.\nThis work focuses on a simple commonsense ability, reasoning about when an\naction (or its effect) is feasible. To this end, we introduce FeasibilityQA, a\nquestion-answering dataset involving binary classification (BCQ) and\nmulti-choice multi-correct questions (MCQ) that test understanding of\nfeasibility. We show that even state-of-the-art models such as GPT-3, GPT-2,\nand T5 struggle to answer the feasibility questions correctly. Specifically, on\nMCQ and BCQ questions, GPT-3 achieves an accuracy of just (19%, 62%) and (25%,\n64%) in zero-shot and few-shot settings, respectively. We also evaluate models\nby providing relevant knowledge statements required to answer the question. We\nfind that the additional knowledge leads to a 7% gain in performance, but the\noverall performance still remains low. These results make one wonder how much\ncommonsense knowledge about action feasibility is encoded in state-of-the-art\nmodels and how well they can reason about it.", "published": "2022-10-14 02:46:06", "link": "http://arxiv.org/abs/2210.07471v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MetaFill: Text Infilling for Meta-Path Generation on Heterogeneous\n  Information Networks", "abstract": "Heterogeneous Information Network (HIN) is essential to study complicated\nnetworks containing multiple edge types and node types. Meta-path, a sequence\nof node types and edge types, is the core technique to embed HINs. Since\nmanually curating meta-paths is time-consuming, there is a pressing need to\ndevelop automated meta-path generation approaches. Existing meta-path\ngeneration approaches cannot fully exploit the rich textual information in\nHINs, such as node names and edge type names. To address this problem, we\npropose MetaFill, a text-infilling-based approach for meta-path generation. The\nkey idea of MetaFill is to formulate meta-path identification problem as a word\nsequence infilling problem, which can be advanced by Pretrained Language Models\n(PLMs). We observed the superior performance of MetaFill against existing\nmeta-path generation methods and graph embedding methods that do not leverage\nmeta-paths in both link prediction and node classification on two real-world\nHIN datasets. We further demonstrated how MetaFill can accurately classify\nedges in the zero-shot setting, where existing approaches cannot generate any\nmeta-paths. MetaFill exploits PLMs to generate meta-paths for graph embedding,\nopening up new avenues for language model applications in graph analysis.", "published": "2022-10-14 03:34:09", "link": "http://arxiv.org/abs/2210.07488v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Adaptive Named Entity Recognition by Retrieving Unstructured\n  Knowledge", "abstract": "Although named entity recognition (NER) helps us to extract domain-specific\nentities from text (e.g., artists in the music domain), it is costly to create\na large amount of training data or a structured knowledge base to perform\naccurate NER in the target domain. Here, we propose self-adaptive NER, which\nretrieves external knowledge from unstructured text to learn the usages of\nentities that have not been learned well. To retrieve useful knowledge for NER,\nwe design an effective two-stage model that retrieves unstructured knowledge\nusing uncertain entities as queries. Our model predicts the entities in the\ninput and then finds those of which the prediction is not confident. Then, it\nretrieves knowledge by using these uncertain entities as queries and\nconcatenates the retrieved text to the original input to revise the prediction.\nExperiments on CrossNER datasets demonstrated that our model outperforms strong\nbaselines by 2.35 points in F1 metric.", "published": "2022-10-14 05:10:53", "link": "http://arxiv.org/abs/2210.07523v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The User-Aware Arabic Gender Rewriter", "abstract": "We introduce the User-Aware Arabic Gender Rewriter, a user-centric web-based\nsystem for Arabic gender rewriting in contexts involving two users. The system\ntakes either Arabic or English sentences as input, and provides users with the\nability to specify their desired first and/or second person target genders. The\nsystem outputs gender rewritten alternatives of the Arabic input sentences (or\ntheir Arabic translations in case of English input) to match the target users'\ngender preferences.", "published": "2022-10-14 05:34:57", "link": "http://arxiv.org/abs/2210.07538v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Q-TOD: A Query-driven Task-oriented Dialogue System", "abstract": "Existing pipelined task-oriented dialogue systems usually have difficulties\nadapting to unseen domains, whereas end-to-end systems are plagued by\nlarge-scale knowledge bases in practice. In this paper, we introduce a novel\nquery-driven task-oriented dialogue system, namely Q-TOD. The essential\ninformation from the dialogue context is extracted into a query, which is\nfurther employed to retrieve relevant knowledge records for response\ngeneration. Firstly, as the query is in the form of natural language and not\nconfined to the schema of the knowledge base, the issue of domain adaption is\nalleviated remarkably in Q-TOD. Secondly, as the query enables the decoupling\nof knowledge retrieval from the generation, Q-TOD gets rid of the issue of\nknowledge base scalability. To evaluate the effectiveness of the proposed\nQ-TOD, we collect query annotations for three publicly available task-oriented\ndialogue datasets. Comprehensive experiments verify that Q-TOD outperforms\nstrong baselines and establishes a new state-of-the-art performance on these\ndatasets.", "published": "2022-10-14 06:38:19", "link": "http://arxiv.org/abs/2210.07564v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multitask Pre-training of Modular Prompt for Chinese Few-Shot Learning", "abstract": "Prompt tuning is a parameter-efficient approach to adapting pre-trained\nlanguage models to downstream tasks. Although prompt tuning has been shown to\nmatch the performance of full model tuning when training data is sufficient, it\ntends to struggle in few-shot learning settings. In this paper, we present\nMulti-task Pre-trained Modular Prompt (MP2) to boost prompt tuning for few-shot\nlearning. MP2 is a set of combinable prompts pre-trained on 38 Chinese tasks.\nOn downstream tasks, the pre-trained prompts are selectively activated and\ncombined, leading to strong compositional generalization to unseen tasks. To\nbridge the gap between pre-training and fine-tuning, we formulate upstream and\ndownstream tasks into a unified machine reading comprehension task. Extensive\nexperiments under two learning paradigms, i.e., gradient descent and black-box\ntuning, show that MP2 significantly outperforms prompt tuning, full model\ntuning, and prior prompt pre-training methods in few-shot settings. In\naddition, we demonstrate that MP2 can achieve surprisingly fast and strong\nadaptation to downstream tasks by merely learning 8 parameters to combine the\npre-trained modular prompts.", "published": "2022-10-14 06:43:42", "link": "http://arxiv.org/abs/2210.07565v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MICO: A Multi-alternative Contrastive Learning Framework for Commonsense\n  Knowledge Representation", "abstract": "Commonsense reasoning tasks such as commonsense knowledge graph completion\nand commonsense question answering require powerful representation learning. In\nthis paper, we propose to learn commonsense knowledge representation by MICO, a\nMulti-alternative contrastve learning framework on COmmonsense knowledge graphs\n(MICO). MICO generates the commonsense knowledge representation by contextual\ninteraction between entity nodes and relations with multi-alternative\ncontrastive learning. In MICO, the head and tail entities in an $(h,r,t)$\nknowledge triple are converted to two relation-aware sequence pairs (a premise\nand an alternative) in the form of natural language. Semantic representations\ngenerated by MICO can benefit the following two tasks by simply comparing the\ndistance score between the representations: 1) zero-shot commonsense question\nanswering task; 2) inductive commonsense knowledge graph completion task.\nExtensive experiments show the effectiveness of our method.", "published": "2022-10-14 06:51:21", "link": "http://arxiv.org/abs/2210.07570v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Creation of Named Entity Recognition Datasets by Querying\n  Phrase Representations", "abstract": "Most weakly supervised named entity recognition (NER) models rely on\ndomain-specific dictionaries provided by experts. This approach is infeasible\nin many domains where dictionaries do not exist. While a phrase retrieval model\nwas used to construct pseudo-dictionaries with entities retrieved from\nWikipedia automatically in a recent study, these dictionaries often have\nlimited coverage because the retriever is likely to retrieve popular entities\nrather than rare ones. In this study, we present a novel framework, HighGEN,\nthat generates NER datasets with high-coverage pseudo-dictionaries.\nSpecifically, we create entity-rich dictionaries with a novel search method,\ncalled phrase embedding search, which encourages the retriever to search a\nspace densely populated with various entities. In addition, we use a new\nverification process based on the embedding distance between candidate entity\nmentions and entity types to reduce the false-positive noise in weak labels\ngenerated by high-coverage dictionaries. We demonstrate that HighGEN\noutperforms the previous best model by an average F1 score of 4.7 across five\nNER benchmark datasets.", "published": "2022-10-14 07:36:44", "link": "http://arxiv.org/abs/2210.07586v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ConEntail: An Entailment-based Framework for Universal Zero and Few Shot\n  Classification with Supervised Contrastive Pretraining", "abstract": "A universal classification model aims to generalize to diverse classification\ntasks in both zero and few shot settings. A promising way toward universal\nclassification is to cast heterogeneous data formats into a dataset-agnostic\n\"meta-task\" (e.g., textual entailment, question answering) then pretrain a\nmodel on the combined meta dataset. The existing work is either pretrained on\nspecific subsets of classification tasks, or pretrained on both classification\nand generation data but the model could not fulfill its potential in\nuniversality and reliability. These also leave a massive amount of annotated\ndata under-exploited. To fill these gaps, we propose ConEntail, a new framework\nfor universal zero and few shot classification with supervised contrastive\npretraining. Our unified meta-task for classification is based on nested\nentailment. It can be interpreted as \"Does sentence a entails [sentence b\nentails label c]\". This formulation enables us to make better use of 57\nannotated classification datasets for supervised contrastive pretraining and\nuniversal evaluation. In this way, ConEntail helps the model (1) absorb\nknowledge from different datasets, and (2) gain consistent performance gain\nwith more pretraining data. In experiments, we compare our model with\ndiscriminative and generative models pretrained on the same dataset. The\nresults confirm that our framework effectively exploits existing annotated data\nand consistently outperforms baselines in both zero (9.4% average improvement)\nand few shot settings (3.5% average improvement).", "published": "2022-10-14 07:37:27", "link": "http://arxiv.org/abs/2210.07587v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The State of Profanity Obfuscation in Natural Language Processing", "abstract": "Work on hate speech has made the consideration of rude and harmful examples\nin scientific publications inevitable. This raises various problems, such as\nwhether or not to obscure profanities. While science must accurately disclose\nwhat it does, the unwarranted spread of hate speech is harmful to readers, and\nincreases its internet frequency. While maintaining publications' professional\nappearance, obfuscating profanities makes it challenging to evaluate the\ncontent, especially for non-native speakers. Surveying 150 ACL papers, we\ndiscovered that obfuscation is usually employed for English but not other\nlanguages, and even so quite uneven. We discuss the problems with obfuscation\nand suggest a multilingual community resource called PrOf that has a Python\nmodule to standardize profanity obfuscation processes. We believe PrOf can help\nscientific publication policies to make hate speech work accessible and\ncomparable, irrespective of language.", "published": "2022-10-14 07:45:36", "link": "http://arxiv.org/abs/2210.07595v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mention Annotations Alone Enable Efficient Domain Adaptation for\n  Coreference Resolution", "abstract": "Although recent neural models for coreference resolution have led to\nsubstantial improvements on benchmark datasets, transferring these models to\nnew target domains containing out-of-vocabulary spans and requiring differing\nannotation schemes remains challenging. Typical approaches involve continued\ntraining on annotated target-domain data, but obtaining annotations is costly\nand time-consuming. We show that annotating mentions alone is nearly twice as\nfast as annotating full coreference chains. Accordingly, we propose a method\nfor efficiently adapting coreference models, which includes a high-precision\nmention detection objective and requires annotating only mentions in the target\ndomain. Extensive evaluation across three English coreference datasets:\nCoNLL-2012 (news/conversation), i2b2/VA (medical notes), and previously\nunstudied child welfare notes, reveals that our approach facilitates\nannotation-efficient transfer and results in a 7-14% improvement in average F1\nwithout increasing annotator time.", "published": "2022-10-14 07:57:27", "link": "http://arxiv.org/abs/2210.07602v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dense-ATOMIC: Towards Densely-connected ATOMIC with High Knowledge\n  Coverage and Massive Multi-hop Paths", "abstract": "ATOMIC is a large-scale commonsense knowledge graph (CSKG) containing\neveryday if-then knowledge triplets, i.e., {head event, relation, tail event}.\nThe one-hop annotation manner made ATOMIC a set of independent bipartite\ngraphs, which ignored the numerous links between events in different bipartite\ngraphs and consequently caused shortages in knowledge coverage and multi-hop\npaths. In this work, we aim to construct Dense-ATOMIC with high knowledge\ncoverage and massive multi-hop paths. The events in ATOMIC are normalized to a\nconsistent pattern at first. We then propose a CSKG completion method called\nRel-CSKGC to predict the relation given the head event and the tail event of a\ntriplet, and train a CSKG completion model based on existing triplets in\nATOMIC. We finally utilize the model to complete the missing links in ATOMIC\nand accordingly construct Dense-ATOMIC. Both automatic and human evaluation on\nan annotated subgraph of ATOMIC demonstrate the advantage of Rel-CSKGC over\nstrong baselines. We further conduct extensive evaluations on Dense-ATOMIC in\nterms of statistics, human evaluation, and simple downstream tasks, all proving\nDense-ATOMIC's advantages in Knowledge Coverage and Multi-hop Paths. Both the\nsource code of Rel-CSKGC and Dense-ATOMIC are publicly available on\nhttps://github.com/NUSTM/Dense-ATOMIC.", "published": "2022-10-14 08:17:11", "link": "http://arxiv.org/abs/2210.07621v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Generation Models Can Cause Harm: So What Can We Do About It?\n  An Actionable Survey", "abstract": "Recent advances in the capacity of large language models to generate\nhuman-like text have resulted in their increased adoption in user-facing\nsettings. In parallel, these improvements have prompted a heated discourse\naround the risks of societal harms they introduce, whether inadvertent or\nmalicious. Several studies have explored these harms and called for their\nmitigation via development of safer, fairer models. Going beyond enumerating\nthe risks of harms, this work provides a survey of practical methods for\naddressing potential threats and societal harms from language generation\nmodels. We draw on several prior works' taxonomies of language model risks to\npresent a structured overview of strategies for detecting and ameliorating\ndifferent kinds of risks/harms of language generators. Bridging diverse strands\nof research, this survey aims to serve as a practical guide for both LM\nresearchers and practitioners, with explanations of different mitigation\nstrategies' motivations, their limitations, and open problems for future\nresearch.", "published": "2022-10-14 10:43:39", "link": "http://arxiv.org/abs/2210.07700v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Confidence estimation of classification based on the distribution of the\n  neural network output layer", "abstract": "One of the most common problems preventing the application of prediction\nmodels in the real world is lack of generalization: The accuracy of models,\nmeasured in the benchmark does repeat itself on future data, e.g. in the\nsettings of real business. There is relatively little methods exist that\nestimate the confidence of prediction models. In this paper, we propose novel\nmethods that, given a neural network classification model, estimate uncertainty\nof particular predictions generated by this model. Furthermore, we propose a\nmethod that, given a model and a confidence level, calculates a threshold that\nseparates prediction generated by this model into two subsets, one of them\nmeets the given confidence level. In contrast to other methods, the proposed\nmethods do not require any changes on existing neural networks, because they\nsimply build on the output logit layer of a common neural network. In\nparticular, the methods infer the confidence of a particular prediction based\non the distribution of the logit values corresponding to this prediction. The\nproposed methods constitute a tool that is recommended for filtering\npredictions in the process of knowledge extraction, e.g. based on web\nscrapping, where predictions subsets are identified that maximize the precision\non cost of the recall, which is less important due to the availability of data.\nThe method has been tested on different tasks including relation extraction,\nnamed entity recognition and image classification to show the significant\nincrease of accuracy achieved.", "published": "2022-10-14 12:32:50", "link": "http://arxiv.org/abs/2210.07745v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robust Preference Learning for Storytelling via Contrastive\n  Reinforcement Learning", "abstract": "Controlled automated story generation seeks to generate natural language\nstories satisfying constraints from natural language critiques or preferences.\nExisting methods to control for story preference utilize prompt engineering\nwhich is labor intensive and often inconsistent. They may also use\nlogit-manipulation methods which require annotated datasets to exist for the\ndesired attributes. To address these issues, we first train a contrastive\nbi-encoder model to align stories with corresponding human critiques, named\nCARP, building a general purpose preference model. This is subsequently used as\na reward function to fine-tune a generative language model via reinforcement\nlearning. However, simply fine-tuning a generative language model with a\ncontrastive reward model does not always reliably result in a story generation\nsystem capable of generating stories that meet user preferences. To increase\nstory generation robustness we further fine-tune the contrastive reward model\nusing a prompt-learning technique. A human participant study is then conducted\ncomparing generations from our full system, ablations, and two baselines. We\nshow that the full fine-tuning pipeline results in a story generator preferred\nover a LLM 20x as large as well as logit-based methods. This motivates the use\nof contrastive learning for general purpose human preference modeling.", "published": "2022-10-14 13:21:33", "link": "http://arxiv.org/abs/2210.07792v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Second Wave of UD Hebrew Treebanking and Cross-Domain Parsing", "abstract": "Foundational Hebrew NLP tasks such as segmentation, tagging and parsing, have\nrelied to date on various versions of the Hebrew Treebank (HTB, Sima'an et al.\n2001). However, the data in HTB, a single-source newswire corpus, is now over\n30 years old, and does not cover many aspects of contemporary Hebrew on the\nweb. This paper presents a new, freely available UD treebank of Hebrew\nstratified from a range of topics selected from Hebrew Wikipedia. In addition\nto introducing the corpus and evaluating the quality of its annotations, we\ndeploy automatic validation tools based on grew (Guillaume, 2021), and conduct\nthe first cross domain parsing experiments in Hebrew. We obtain new\nstate-of-the-art (SOTA) results on UD NLP tasks, using a combination of the\nlatest language modelling and some incremental improvements to existing\ntransformer based approaches. We also release a new version of the UD HTB\nmatching annotation scheme updates from our new corpus.", "published": "2022-10-14 14:52:07", "link": "http://arxiv.org/abs/2210.07873v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HashFormers: Towards Vocabulary-independent Pre-trained Transformers", "abstract": "Transformer-based pre-trained language models are vocabulary-dependent,\nmapping by default each token to its corresponding embedding. This one-to-one\nmapping results into embedding matrices that occupy a lot of memory (i.e.\nmillions of parameters) and grow linearly with the size of the vocabulary.\nPrevious work on on-device transformers dynamically generate token embeddings\non-the-fly without embedding matrices using locality-sensitive hashing over\nmorphological information. These embeddings are subsequently fed into\ntransformer layers for text classification. However, these methods are not\npre-trained. Inspired by this line of work, we propose HashFormers, a new\nfamily of vocabulary-independent pre-trained transformers that support an\nunlimited vocabulary (i.e. all possible tokens in a corpus) given a\nsubstantially smaller fixed-sized embedding matrix. We achieve this by first\nintroducing computationally cheap hashing functions that bucket together\nindividual tokens to embeddings. We also propose three variants that do not\nrequire an embedding matrix at all, further reducing the memory requirements.\nWe empirically demonstrate that HashFormers are more memory efficient compared\nto standard pre-trained transformers while achieving comparable predictive\nperformance when fine-tuned on multiple text classification tasks. For example,\nour most efficient HashFormer variant has a negligible performance degradation\n(0.4\\% on GLUE) using only 99.1K parameters for representing the embeddings\ncompared to 12.3-38M parameters of state-of-the-art models.", "published": "2022-10-14 15:39:34", "link": "http://arxiv.org/abs/2210.07904v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Style Transfer as Data Augmentation: A Case Study on Named Entity\n  Recognition", "abstract": "In this work, we take the named entity recognition task in the English\nlanguage as a case study and explore style transfer as a data augmentation\nmethod to increase the size and diversity of training data in low-resource\nscenarios. We propose a new method to effectively transform the text from a\nhigh-resource domain to a low-resource domain by changing its style-related\nattributes to generate synthetic data for training. Moreover, we design a\nconstrained decoding algorithm along with a set of key ingredients for data\nselection to guarantee the generation of valid and coherent data. Experiments\nand analysis on five different domain pairs under different data regimes\ndemonstrate that our approach can significantly improve results compared to\ncurrent state-of-the-art data augmentation methods. Our approach is a practical\nsolution to data scarcity, and we expect it to be applicable to other NLP\ntasks.", "published": "2022-10-14 16:02:03", "link": "http://arxiv.org/abs/2210.07916v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MiQA: A Benchmark for Inference on Metaphorical Questions", "abstract": "We propose a benchmark to assess the capability of large language models to\nreason with conventional metaphors. Our benchmark combines the previously\nisolated topics of metaphor detection and commonsense reasoning into a single\ntask that requires a model to make inferences by accurately selecting between\nthe literal and metaphorical register. We examine the performance of\nstate-of-the-art pre-trained models on binary-choice tasks and find a large\ndiscrepancy between the performance of small and very large models, going from\nchance to near-human level. We also analyse the largest model in a generative\nsetting and find that although human performance is approached, careful\nmultiple-shot prompting is required.", "published": "2022-10-14 17:46:05", "link": "http://arxiv.org/abs/2210.07993v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Repetition in Abstractive Neural Summarizers", "abstract": "We provide a quantitative and qualitative analysis of self-repetition in the\noutput of neural summarizers. We measure self-repetition as the number of\nn-grams of length four or longer that appear in multiple outputs of the same\nsystem. We analyze the behavior of three popular architectures (BART, T5, and\nPegasus), fine-tuned on five datasets. In a regression analysis, we find that\nthe three architectures have different propensities for repeating content\nacross output summaries for inputs, with BART being particularly prone to\nself-repetition. Fine-tuning on more abstractive data, and on data featuring\nformulaic language, is associated with a higher rate of self-repetition. In\nqualitative analysis we find systems produce artefacts such as ads and\ndisclaimers unrelated to the content being summarized, as well as formulaic\nphrases common in the fine-tuning domain. Our approach to corpus-level analysis\nof self-repetition may help practitioners clean up training data for\nsummarizers and ultimately support methods for minimizing the amount of\nself-repetition.", "published": "2022-10-14 23:50:42", "link": "http://arxiv.org/abs/2210.08145v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PCFG-based Natural Language Interface Improves Generalization for\n  Controlled Text Generation", "abstract": "Existing work on controlled text generation (CTG) assumes a control interface\nof categorical attributes. In this work, we propose a natural language (NL)\ninterface, where we craft a PCFG to embed the control attributes into natural\nlanguage commands, and propose variants of existing CTG models that take\ncommands as input. In our experiments, we design tailored setups to test\nmodel's generalization abilities. We find our PCFG-based command generation\napproach is effective for handling unseen commands compared to fix-set\ntemplates; our proposed NL models can effectively generalize to unseen\nattributes, a new ability enabled by the NL interface, as well as unseen\nattribute combinations. Interestingly, we discover that the simple conditional\ngeneration approach, enhanced with our proposed NL interface, is a strong\nbaseline in those challenging settings.", "published": "2022-10-14 00:20:23", "link": "http://arxiv.org/abs/2210.07431v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating Out-of-Distribution Performance on Document Image Classifiers", "abstract": "The ability of a document classifier to handle inputs that are drawn from a\ndistribution different from the training distribution is crucial for robust\ndeployment and generalizability. The RVL-CDIP corpus is the de facto standard\nbenchmark for document classification, yet to our knowledge all studies that\nuse this corpus do not include evaluation on out-of-distribution documents. In\nthis paper, we curate and release a new out-of-distribution benchmark for\nevaluating out-of-distribution performance for document classifiers. Our new\nout-of-distribution benchmark consists of two types of documents: those that\nare not part of any of the 16 in-domain RVL-CDIP categories (RVL-CDIP-O), and\nthose that are one of the 16 in-domain categories yet are drawn from a\ndistribution different from that of the original RVL-CDIP dataset (RVL-CDIP-N).\nWhile prior work on document classification for in-domain RVL-CDIP documents\nreports high accuracy scores, we find that these models exhibit accuracy drops\nof between roughly 15-30% on our new out-of-domain RVL-CDIP-N benchmark, and\nfurther struggle to distinguish between in-domain RVL-CDIP-N and out-of-domain\nRVL-CDIP-O inputs. Our new benchmark provides researchers with a valuable new\nresource for analyzing out-of-distribution performance on document classifiers.\nOur new out-of-distribution data can be found at\nhttps://github.com/gxlarson/rvl-cdip-ood.", "published": "2022-10-14 01:24:21", "link": "http://arxiv.org/abs/2210.07448v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Holistic Sentence Embeddings for Better Out-of-Distribution Detection", "abstract": "Detecting out-of-distribution (OOD) instances is significant for the safe\ndeployment of NLP models. Among recent textual OOD detection works based on\npretrained language models (PLMs), distance-based methods have shown superior\nperformance. However, they estimate sample distance scores in the last-layer\nCLS embedding space and thus do not make full use of linguistic information\nunderlying in PLMs. To address the issue, we propose to boost OOD detection by\nderiving more holistic sentence embeddings. On the basis of the observations\nthat token averaging and layer combination contribute to improving OOD\ndetection, we propose a simple embedding approach named Avg-Avg, which averages\nall token representations from each intermediate layer as the sentence\nembedding and significantly surpasses the state-of-the-art on a comprehensive\nsuite of benchmarks by a 9.33% FAR95 margin. Furthermore, our analysis\ndemonstrates that it indeed helps preserve general linguistic knowledge in\nfine-tuned PLMs and substantially benefits detecting background shifts. The\nsimple yet effective embedding method can be applied to fine-tuned PLMs with\nnegligible extra costs, providing a free gain in OOD detection. Our code is\navailable at https://github.com/lancopku/Avg-Avg.", "published": "2022-10-14 03:22:58", "link": "http://arxiv.org/abs/2210.07485v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Psychology-guided Controllable Story Generation", "abstract": "Controllable story generation is a challenging task in the field of NLP,\nwhich has attracted increasing research interest in recent years. However, most\nexisting works generate a whole story conditioned on the appointed keywords or\nemotions, ignoring the psychological changes of the protagonist. Inspired by\npsychology theories, we introduce global psychological state chains, which\ninclude the needs and emotions of the protagonists, to help a story generation\nsystem create more controllable and well-planned stories. In this paper, we\npropose a Psychology-guIded Controllable Story Generation System (PICS) to\ngenerate stories that adhere to the given leading context and desired\npsychological state chains for the protagonist. Specifically, psychological\nstate trackers are employed to memorize the protagonist's local psychological\nstates to capture their inner temporal relationships. In addition,\npsychological state planners are adopted to gain the protagonist's global\npsychological states for story planning. Eventually, a psychology controller is\ndesigned to integrate the local and global psychological states into the story\ncontext representation for composing psychology-guided stories. Automatic and\nmanual evaluations demonstrate that PICS outperforms baselines, and each part\nof PICS shows effectiveness for writing stories with more consistent\npsychological changes.", "published": "2022-10-14 03:40:53", "link": "http://arxiv.org/abs/2210.07493v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can Language Representation Models Think in Bets?", "abstract": "In recent years, transformer-based language representation models (LRMs) have\nachieved state-of-the-art results on difficult natural language understanding\nproblems, such as question answering and text summarization. As these models\nare integrated into real-world applications, evaluating their ability to make\nrational decisions is an important research agenda, with practical\nramifications. This article investigates LRMs' rational decision-making ability\nthrough a carefully designed set of decision-making benchmarks and experiments.\nInspired by classic work in cognitive science, we model the decision-making\nproblem as a bet. We then investigate an LRM's ability to choose outcomes that\nhave optimal, or at minimum, positive expected gain. Through a robust body of\nexperiments on four established LRMs, we show that a model is only able to\n`think in bets' if it is first fine-tuned on bet questions with an identical\nstructure. Modifying the bet question's structure, while still retaining its\nfundamental characteristics, decreases an LRM's performance by more than 25\\%,\non average, although absolute performance remains well above random. LRMs are\nalso found to be more rational when selecting outcomes with non-negative\nexpected gain, rather than optimal or strictly positive expected gain. Our\nresults suggest that LRMs could potentially be applied to tasks that rely on\ncognitive decision-making skills, but that more research is necessary before\nthey can robustly make rational decisions.", "published": "2022-10-14 05:01:04", "link": "http://arxiv.org/abs/2210.07519v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AutoMoE: Heterogeneous Mixture-of-Experts with Adaptive Computation for\n  Efficient Neural Machine Translation", "abstract": "Mixture-of-Expert (MoE) models have obtained state-of-the-art performance in\nNeural Machine Translation (NMT) tasks. Existing works in MoE mostly consider a\nhomogeneous design where the same number of experts of the same size are placed\nuniformly throughout the network. Furthermore, existing MoE works do not\nconsider computational constraints (e.g., FLOPs, latency) to guide their\ndesign. To this end, we develop AutoMoE -- a framework for designing\nheterogeneous MoE's under computational constraints. AutoMoE leverages Neural\nArchitecture Search (NAS) to obtain efficient sparse MoE sub-transformers with\n4x inference speedup (CPU) and FLOPs reduction over manually designed\nTransformers, with parity in BLEU score over dense Transformer and within 1\nBLEU point of MoE SwitchTransformer, on aggregate over benchmark datasets for\nNMT. Heterogeneous search space with dense and sparsely activated Transformer\nmodules (e.g., how many experts? where to place them? what should be their\nsizes?) allows for adaptive compute -- where different amounts of computations\nare used for different tokens in the input. Adaptivity comes naturally from\nrouting decisions which send tokens to experts of different sizes. AutoMoE\ncode, data, and trained models are available at https://aka.ms/AutoMoE.", "published": "2022-10-14 05:32:17", "link": "http://arxiv.org/abs/2210.07535v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Watermarking Pre-trained Language Models with Backdooring", "abstract": "Large pre-trained language models (PLMs) have proven to be a crucial\ncomponent of modern natural language processing systems. PLMs typically need to\nbe fine-tuned on task-specific downstream datasets, which makes it hard to\nclaim the ownership of PLMs and protect the developer's intellectual property\ndue to the catastrophic forgetting phenomenon. We show that PLMs can be\nwatermarked with a multi-task learning framework by embedding backdoors\ntriggered by specific inputs defined by the owners, and those watermarks are\nhard to remove even though the watermarked PLMs are fine-tuned on multiple\ndownstream tasks. In addition to using some rare words as triggers, we also\nshow that the combination of common words can be used as backdoor triggers to\navoid them being easily detected. Extensive experiments on multiple datasets\ndemonstrate that the embedded watermarks can be robustly extracted with a high\nsuccess rate and less influenced by the follow-up fine-tuning.", "published": "2022-10-14 05:42:39", "link": "http://arxiv.org/abs/2210.07543v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Legal Case Document Summarization: Extractive and Abstractive Methods\n  and their Evaluation", "abstract": "Summarization of legal case judgement documents is a challenging problem in\nLegal NLP. However, not much analyses exist on how different families of\nsummarization models (e.g., extractive vs. abstractive) perform when applied to\nlegal case documents. This question is particularly important since many recent\ntransformer-based abstractive summarization models have restrictions on the\nnumber of input tokens, and legal documents are known to be very long. Also, it\nis an open question on how best to evaluate legal case document summarization\nsystems. In this paper, we carry out extensive experiments with several\nextractive and abstractive summarization methods (both supervised and\nunsupervised) over three legal summarization datasets that we have developed.\nOur analyses, that includes evaluation by law practitioners, lead to several\ninteresting insights on legal summarization in specific and long document\nsummarization in general.", "published": "2022-10-14 05:43:08", "link": "http://arxiv.org/abs/2210.07544v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Kernel-Whitening: Overcome Dataset Bias with Isotropic Sentence\n  Embedding", "abstract": "Dataset bias has attracted increasing attention recently for its detrimental\neffect on the generalization ability of fine-tuned models. The current\nmainstream solution is designing an additional shallow model to pre-identify\nbiased instances. However, such two-stage methods scale up the computational\ncomplexity of training process and obstruct valid feature information while\nmitigating bias. To address this issue, we utilize the representation\nnormalization method which aims at disentangling the correlations between\nfeatures of encoded sentences. We find it also promising in eliminating the\nbias problem by providing isotropic data distribution. We further propose\nKernel-Whitening, a Nystrom kernel approximation method to achieve more\nthorough debiasing on nonlinear spurious correlations. Our framework is\nend-to-end with similar time consumption to fine-tuning. Experiments show that\nKernel-Whitening significantly improves the performance of BERT on\nout-of-distribution datasets while maintaining in-distribution accuracy.", "published": "2022-10-14 05:56:38", "link": "http://arxiv.org/abs/2210.07547v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic\n  Search-Free Low-Rank Adaptation", "abstract": "With the ever-growing size of pretrained models (PMs), fine-tuning them has\nbecome more expensive and resource-hungry. As a remedy, low-rank adapters\n(LoRA) keep the main pretrained weights of the model frozen and just introduce\nsome learnable truncated SVD modules (so-called LoRA blocks) to the model.\nWhile LoRA blocks are parameter-efficient, they suffer from two major problems:\nfirst, the size of these blocks is fixed and cannot be modified after training\n(for example, if we need to change the rank of LoRA blocks, then we need to\nre-train them from scratch); second, optimizing their rank requires an\nexhaustive search and effort. In this work, we introduce a dynamic low-rank\nadaptation (DyLoRA) technique to address these two problems together. Our\nDyLoRA method trains LoRA blocks for a range of ranks instead of a single rank\nby sorting the representation learned by the adapter module at different ranks\nduring training. We evaluate our solution on different natural language\nunderstanding (GLUE benchmark) and language generation tasks (E2E, DART and\nWebNLG) using different pretrained models such as RoBERTa and GPT with\ndifferent sizes. Our results show that we can train dynamic search-free models\nwith DyLoRA at least 4 to 7 times (depending to the task) faster than LoRA\nwithout significantly compromising performance. Moreover, our models can\nperform consistently well on a much larger range of ranks compared to LoRA.", "published": "2022-10-14 06:29:22", "link": "http://arxiv.org/abs/2210.07558v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Survey of Parameters Associated with the Quality of Benchmarks in NLP", "abstract": "Several benchmarks have been built with heavy investment in resources to\ntrack our progress in NLP. Thousands of papers published in response to those\nbenchmarks have competed to top leaderboards, with models often surpassing\nhuman performance. However, recent studies have shown that models triumph over\nseveral popular benchmarks just by overfitting on spurious biases, without\ntruly learning the desired task. Despite this finding, benchmarking, while\ntrying to tackle bias, still relies on workarounds, which do not fully utilize\nthe resources invested in benchmark creation, due to the discarding of low\nquality data, and cover limited sets of bias. A potential solution to these\nissues -- a metric quantifying quality -- remains underexplored. Inspired by\nsuccessful quality indices in several domains such as power, food, and water,\nwe take the first step towards a metric by identifying certain language\nproperties that can represent various possible interactions leading to biases\nin a benchmark. We look for bias related parameters which can potentially help\npave our way towards the metric. We survey existing works and identify\nparameters capturing various properties of bias, their origins, types and\nimpact on performance, generalization, and robustness. Our analysis spans over\ndatasets and a hierarchy of tasks ranging from NLI to Summarization, ensuring\nthat our parameters are generic and are not overfitted towards a specific task\nor dataset. We also develop certain parameters in this process.", "published": "2022-10-14 06:44:14", "link": "http://arxiv.org/abs/2210.07566v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "BERTScore is Unfair: On Social Bias in Language Model-Based Metrics for\n  Text Generation", "abstract": "Automatic evaluation metrics are crucial to the development of generative\nsystems. In recent years, pre-trained language model (PLM) based metrics, such\nas BERTScore, have been commonly adopted in various generation tasks. However,\nit has been demonstrated that PLMs encode a range of stereotypical societal\nbiases, leading to a concern on the fairness of PLMs as metrics. To that end,\nthis work presents the first systematic study on the social bias in PLM-based\nmetrics. We demonstrate that popular PLM-based metrics exhibit significantly\nhigher social bias than traditional metrics on 6 sensitive attributes, namely\nrace, gender, religion, physical appearance, age, and socioeconomic status.\nIn-depth analysis suggests that choosing paradigms (matching, regression, or\ngeneration) of the metric has a greater impact on fairness than choosing PLMs.\nIn addition, we develop debiasing adapters that are injected into PLM layers,\nmitigating bias in PLM-based metrics while retaining high performance for\nevaluating text generation.", "published": "2022-10-14 08:24:11", "link": "http://arxiv.org/abs/2210.07626v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hardness of Samples Need to be Quantified for a Reliable Evaluation\n  System: Exploring Potential Opportunities with a New Task", "abstract": "Evaluation of models on benchmarks is unreliable without knowing the degree\nof sample hardness; this subsequently overestimates the capability of AI\nsystems and limits their adoption in real world applications. We propose a Data\nScoring task that requires assignment of each unannotated sample in a benchmark\na score between 0 to 1, where 0 signifies easy and 1 signifies hard. Use of\nunannotated samples in our task design is inspired from humans who can\ndetermine a question difficulty without knowing its correct answer. This also\nrules out the use of methods involving model based supervision (since they\nrequire sample annotations to get trained), eliminating potential biases\nassociated with models in deciding sample difficulty. We propose a method based\non Semantic Textual Similarity (STS) for this task; we validate our method by\nshowing that existing models are more accurate with respect to the easier\nsample-chunks than with respect to the harder sample-chunks. Finally we\ndemonstrate five novel applications.", "published": "2022-10-14 08:26:32", "link": "http://arxiv.org/abs/2210.07631v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Enabling Classifiers to Make Judgements Explicitly Aligned with Human\n  Values", "abstract": "Many NLP classification tasks, such as sexism/racism detection or toxicity\ndetection, are based on human values. Yet, human values can vary under diverse\ncultural conditions. Therefore, we introduce a framework for value-aligned\nclassification that performs prediction based on explicitly written human\nvalues in the command. Along with the task, we propose a practical approach\nthat distills value-aligned knowledge from large-scale language models (LLMs)\nto construct value-aligned classifiers in two steps. First, we generate\nvalue-aligned training data from LLMs by prompt-based few-shot learning. Next,\nwe fine-tune smaller classification models with the generated data for the\ntask. Empirical results show that our VA-Models surpass multiple baselines by\nat least 15.56% on the F1-score, including few-shot learning with OPT-175B and\nexisting text augmentation methods. We suggest that using classifiers with\nexplicit human value input improves both inclusivity & explainability in AI.", "published": "2022-10-14 09:10:49", "link": "http://arxiv.org/abs/2210.07652v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pretrained Transformers Do not Always Improve Robustness", "abstract": "Pretrained Transformers (PT) have been shown to improve Out of Distribution\n(OOD) robustness than traditional models such as Bag of Words (BOW), LSTMs,\nConvolutional Neural Networks (CNN) powered by Word2Vec and Glove embeddings.\nHow does the robustness comparison hold in a real world setting where some part\nof the dataset can be noisy? Do PT also provide more robust representation than\ntraditional models on exposure to noisy data? We perform a comparative study on\n10 models and find an empirical evidence that PT provide less robust\nrepresentation than traditional models on exposure to noisy data. We\ninvestigate further and augment PT with an adversarial filtering (AF) mechanism\nthat has been shown to improve OOD generalization. However, increase in\ngeneralization does not necessarily increase robustness, as we find that noisy\ndata fools the AF method powered by PT.", "published": "2022-10-14 09:30:36", "link": "http://arxiv.org/abs/2210.07663v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Plausible May Not Be Faithful: Probing Object Hallucination in\n  Vision-Language Pre-training", "abstract": "Large-scale vision-language pre-trained (VLP) models are prone to hallucinate\nnon-existent visual objects when generating text based on visual information.\nIn this paper, we systematically study the object hallucination problem from\nthree aspects. First, we examine recent state-of-the-art VLP models, showing\nthat they still hallucinate frequently, and models achieving better scores on\nstandard metrics (e.g., CIDEr) could be more unfaithful. Second, we investigate\nhow different types of image encoding in VLP influence hallucination, including\nregion-based, grid-based, and patch-based. Surprisingly, we find that\npatch-based features perform the best and smaller patch resolution yields a\nnon-trivial reduction in object hallucination. Third, we decouple various VLP\nobjectives and demonstrate that token-level image-text alignment and controlled\ngeneration are crucial to reducing hallucination. Based on that, we propose a\nsimple yet effective VLP loss named ObjMLM to further mitigate object\nhallucination. Results show that it reduces object hallucination by up to 17.4%\nwhen tested on two benchmarks (COCO Caption for in-domain and NoCaps for\nout-of-domain evaluation).", "published": "2022-10-14 10:27:22", "link": "http://arxiv.org/abs/2210.07688v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Extracting Cultural Commonsense Knowledge at Scale", "abstract": "Structured knowledge is important for many AI applications. Commonsense\nknowledge, which is crucial for robust human-centric AI, is covered by a small\nnumber of structured knowledge projects. However, they lack knowledge about\nhuman traits and behaviors conditioned on socio-cultural contexts, which is\ncrucial for situative AI. This paper presents CANDLE, an end-to-end methodology\nfor extracting high-quality cultural commonsense knowledge (CCSK) at scale.\nCANDLE extracts CCSK assertions from a huge web corpus and organizes them into\ncoherent clusters, for 3 domains of subjects (geography, religion, occupation)\nand several cultural facets (food, drinks, clothing, traditions, rituals,\nbehaviors). CANDLE includes judicious techniques for classification-based\nfiltering and scoring of interestingness. Experimental evaluations show the\nsuperiority of the CANDLE CCSK collection over prior works, and an extrinsic\nuse case demonstrates the benefits of CCSK for the GPT-3 language model. Code\nand data can be accessed at https://candle.mpi-inf.mpg.de/.", "published": "2022-10-14 12:53:57", "link": "http://arxiv.org/abs/2210.07763v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LEATHER: A Framework for Learning to Generate Human-like Text in\n  Dialogue", "abstract": "Algorithms for text-generation in dialogue can be misguided. For example, in\ntask-oriented settings, reinforcement learning that optimizes only task-success\ncan lead to abysmal lexical diversity. We hypothesize this is due to poor\ntheoretical understanding of the objectives in text-generation and their\nrelation to the learning process (i.e., model training). To this end, we\npropose a new theoretical framework for learning to generate text in dialogue.\nCompared to existing theories of learning, our framework allows for analysis of\nthe multi-faceted goals inherent to text-generation. We use our framework to\ndevelop theoretical guarantees for learners that adapt to unseen data. As an\nexample, we apply our theory to study data-shift within a cooperative learning\nalgorithm proposed for the GuessWhat?! visual dialogue game. From this insight,\nwe propose a new algorithm, and empirically, we demonstrate our proposal\nimproves both task-success and human-likeness of the generated text. Finally,\nwe show statistics from our theory are empirically predictive of multiple\nqualities of the generated dialogue, suggesting our theory is useful for\nmodel-selection when human evaluations are not available.", "published": "2022-10-14 13:05:11", "link": "http://arxiv.org/abs/2210.07777v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EfficientVLM: Fast and Accurate Vision-Language Models via Knowledge\n  Distillation and Modal-adaptive Pruning", "abstract": "Pre-trained vision-language models (VLMs) have achieved impressive results in\na range of vision-language tasks. However, popular VLMs usually consist of\nhundreds of millions of parameters which brings challenges for fine-tuning and\ndeployment in real-world applications due to space, memory, and latency\nconstraints. In this work, we introduce a distilling then pruning framework to\ncompress large vision-language models into smaller, faster, and more accurate\nones. We first shrink the size of a pre-trained large VLM and apply knowledge\ndistillation in the vision-language pre-training stage to obtain a\ntask-agnostic compact VLM. Then we propose a modal-adaptive pruning algorithm\nto automatically infer the importance of vision and language modalities for\ndifferent downstream tasks and adaptively remove redundant structures and\nneurons in different encoders with controllable target sparsity. We apply our\nframework to train EfficientVLM, a fast and accurate vision-language model\nconsisting of 6 vision layers, 3 text layers, and 3 cross-modal fusion layers,\naccounting for only 93 million parameters in total, which is 44.3% of the\nteacher model. EfficientVLM retains 98.4% performance of the teacher model and\naccelerates its inference speed by 2.2x. EfficientVLM achieves a large absolute\nimprovement over previous SoTA efficient VLMs of similar sizes by a large\nmargin on various vision-language tasks, including VQAv2 (+4.9%), NLVR2\n(+5.6%), ITR (R@1 on TR +17.2%, on IR + 15.6% ) and COCO caption generation\n(CIDEr +6.5), demonstrating a large potential on training lightweight VLMs.", "published": "2022-10-14 13:26:41", "link": "http://arxiv.org/abs/2210.07795v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "One Graph to Rule them All: Using NLP and Graph Neural Networks to\n  analyse Tolkien's Legendarium", "abstract": "Natural Language Processing and Machine Learning have considerably advanced\nComputational Literary Studies. Similarly, the construction of co-occurrence\nnetworks of literary characters, and their analysis using methods from social\nnetwork analysis and network science, have provided insights into the micro-\nand macro-level structure of literary texts. Combining these perspectives, in\nthis work we study character networks extracted from a text corpus of J.R.R.\nTolkien's Legendarium. We show that this perspective helps us to analyse and\nvisualise the narrative style that characterises Tolkien's works. Addressing\ncharacter classification, embedding and co-occurrence prediction, we further\ninvestigate the advantages of state-of-the-art Graph Neural Networks over a\npopular word embedding method. Our results highlight the large potential of\ngraph learning in Computational Literary Studies.", "published": "2022-10-14 14:47:56", "link": "http://arxiv.org/abs/2210.07871v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Expose Backdoors on the Way: A Feature-Based Efficient Defense against\n  Textual Backdoor Attacks", "abstract": "Natural language processing (NLP) models are known to be vulnerable to\nbackdoor attacks, which poses a newly arisen threat to NLP models. Prior online\nbackdoor defense methods for NLP models only focus on the anomalies at either\nthe input or output level, still suffering from fragility to adaptive attacks\nand high computational cost. In this work, we take the first step to\ninvestigate the unconcealment of textual poisoned samples at the\nintermediate-feature level and propose a feature-based efficient online defense\nmethod. Through extensive experiments on existing attacking methods, we find\nthat the poisoned samples are far away from clean samples in the intermediate\nfeature space of a poisoned NLP model. Motivated by this observation, we devise\na distance-based anomaly score (DAN) to distinguish poisoned samples from clean\nsamples at the feature level. Experiments on sentiment analysis and offense\ndetection tasks demonstrate the superiority of DAN, as it substantially\nsurpasses existing online defense methods in terms of defending performance and\nenjoys lower inference costs. Moreover, we show that DAN is also resistant to\nadaptive attacks based on feature-level regularization. Our code is available\nat https://github.com/lancopku/DAN.", "published": "2022-10-14 15:44:28", "link": "http://arxiv.org/abs/2210.07907v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PseudoReasoner: Leveraging Pseudo Labels for Commonsense Knowledge Base\n  Population", "abstract": "Commonsense Knowledge Base (CSKB) Population aims at reasoning over unseen\nentities and assertions on CSKBs, and is an important yet hard commonsense\nreasoning task. One challenge is that it requires out-of-domain generalization\nability as the source CSKB for training is of a relatively smaller scale (1M)\nwhile the whole candidate space for population is way larger (200M). We propose\nPseudoReasoner, a semi-supervised learning framework for CSKB population that\nuses a teacher model pre-trained on CSKBs to provide pseudo labels on the\nunlabeled candidate dataset for a student model to learn from. The teacher can\nbe a generative model rather than restricted to discriminative models as\nprevious works. In addition, we design a new filtering procedure for pseudo\nlabels based on influence function and the student model's prediction to\nfurther improve the performance. The framework can improve the backbone model\nKG-BERT (RoBERTa-large) by 3.3 points on the overall performance and\nespecially, 5.3 points on the out-of-domain performance, and achieves the\nstate-of-the-art. Codes and data are available at\nhttps://github.com/HKUST-KnowComp/PseudoReasoner.", "published": "2022-10-14 17:37:30", "link": "http://arxiv.org/abs/2210.07988v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Robust Candidate Generation for Entity Linking on Short Social Media\n  Texts", "abstract": "Entity Linking (EL) is the gateway into Knowledge Bases. Recent advances in\nEL utilize dense retrieval approaches for Candidate Generation, which addresses\nsome of the shortcomings of the Lookup based approach of matching NER mentions\nagainst pre-computed dictionaries. In this work, we show that in the domain of\nTweets, such methods suffer as users often include informal spelling, limited\ncontext, and lack of specificity, among other issues. We investigate these\nchallenges on a large and recent Tweets benchmark for EL, empirically evaluate\nlookup and dense retrieval approaches, and demonstrate a hybrid solution using\nlong contextual representation from Wikipedia is necessary to achieve\nconsiderable gains over previous work, achieving 0.93 recall.", "published": "2022-10-14 02:47:31", "link": "http://arxiv.org/abs/2210.07472v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "68T50, 68T07", "I.2.7"], "primary_category": "cs.CL"}
{"title": "SQA3D: Situated Question Answering in 3D Scenes", "abstract": "We propose a new task to benchmark scene understanding of embodied agents:\nSituated Question Answering in 3D Scenes (SQA3D). Given a scene context (e.g.,\n3D scan), SQA3D requires the tested agent to first understand its situation\n(position, orientation, etc.) in the 3D scene as described by text, then reason\nabout its surrounding environment and answer a question under that situation.\nBased upon 650 scenes from ScanNet, we provide a dataset centered around 6.8k\nunique situations, along with 20.4k descriptions and 33.4k diverse reasoning\nquestions for these situations. These questions examine a wide spectrum of\nreasoning capabilities for an intelligent agent, ranging from spatial relation\ncomprehension to commonsense understanding, navigation, and multi-hop\nreasoning. SQA3D imposes a significant challenge to current multi-modal\nespecially 3D reasoning models. We evaluate various state-of-the-art approaches\nand find that the best one only achieves an overall score of 47.20%, while\namateur human participants can reach 90.06%. We believe SQA3D could facilitate\nfuture embodied AI research with stronger situation understanding and reasoning\ncapability.", "published": "2022-10-14 02:52:26", "link": "http://arxiv.org/abs/2210.07474v5", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Bayes risk CTC: Controllable CTC alignment in Sequence-to-Sequence tasks", "abstract": "Sequence-to-Sequence (seq2seq) tasks transcribe the input sequence to a\ntarget sequence. The Connectionist Temporal Classification (CTC) criterion is\nwidely used in multiple seq2seq tasks. Besides predicting the target sequence,\na side product of CTC is to predict the alignment, which is the most probable\ninput-long sequence that specifies a hard aligning relationship between the\ninput and target units. As there are multiple potential aligning sequences\n(called paths) that are equally considered in CTC formulation, the choice of\nwhich path will be most probable and become the predicted alignment is always\nuncertain. In addition, it is usually observed that the alignment predicted by\nvanilla CTC will drift compared with its reference and rarely provides\npractical functionalities. Thus, the motivation of this work is to make the CTC\nalignment prediction controllable and thus equip CTC with extra\nfunctionalities. The Bayes risk CTC (BRCTC) criterion is then proposed in this\nwork, in which a customizable Bayes risk function is adopted to enforce the\ndesired characteristics of the predicted alignment. With the risk function, the\nBRCTC is a general framework to adopt some customizable preference over the\npaths in order to concentrate the posterior into a particular subset of the\npaths. In applications, we explore one particular preference which yields\nmodels with the down-sampling ability and reduced inference costs. By using\nBRCTC with another preference for early emissions, we obtain an improved\nperformance-latency trade-off for online models. Experimentally, the proposed\nBRCTC reduces the inference cost of offline models by up to 47% without\nperformance degradation and cuts down the overall latency of online systems to\nan unseen level.", "published": "2022-10-14 03:55:36", "link": "http://arxiv.org/abs/2210.07499v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Empirical Study Incorporating Linguistic Knowledge on Filled Pauses for\n  Personalized Spontaneous Speech Synthesis", "abstract": "We present a comprehensive empirical study for personalized spontaneous\nspeech synthesis on the basis of linguistic knowledge. With the advent of voice\ncloning for reading-style speech synthesis, a new voice cloning paradigm for\nhuman-like and spontaneous speech synthesis is required. We, therefore, focus\non personalized spontaneous speech synthesis that can clone both the\nindividual's voice timbre and speech disfluency. Specifically, we deal with\nfilled pauses, a major source of speech disfluency, which is known to play an\nimportant role in speech generation and communication in psychology and\nlinguistics. To comparatively evaluate personalized filled pause insertion and\nnon-personalized filled pause prediction methods, we developed a speech\nsynthesis method with a non-personalized external filled pause predictor\ntrained with a multi-speaker corpus. The results clarify the position-word\nentanglement of filled pauses, i.e., the necessity of precisely predicting\npositions for naturalness and the necessity of precisely predicting words for\nindividuality on the evaluation of synthesized speech.", "published": "2022-10-14 06:29:33", "link": "http://arxiv.org/abs/2210.07559v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Fine-grained Category Discovery under Coarse-grained supervision with\n  Hierarchical Weighted Self-contrastive Learning", "abstract": "Novel category discovery aims at adapting models trained on known categories\nto novel categories. Previous works only focus on the scenario where known and\nnovel categories are of the same granularity. In this paper, we investigate a\nnew practical scenario called Fine-grained Category Discovery under\nCoarse-grained supervision (FCDC). FCDC aims at discovering fine-grained\ncategories with only coarse-grained labeled data, which can adapt models to\ncategories of different granularity from known ones and reduce significant\nlabeling cost. It is also a challenging task since supervised training on\ncoarse-grained categories tends to focus on inter-class distance (distance\nbetween coarse-grained classes) but ignore intra-class distance (distance\nbetween fine-grained sub-classes) which is essential for separating\nfine-grained categories. Considering most current methods cannot transfer\nknowledge from coarse-grained level to fine-grained level, we propose a\nhierarchical weighted self-contrastive network by building a novel weighted\nself-contrastive module and combining it with supervised learning in a\nhierarchical manner. Extensive experiments on public datasets show both\neffectiveness and efficiency of our model over compared methods. Code and data\nare available at https://github.com/Lackel/Hierarchical_Weighted_SCL.", "published": "2022-10-14 12:06:23", "link": "http://arxiv.org/abs/2210.07733v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning to Jointly Transcribe and Subtitle for End-to-End Spontaneous\n  Speech Recognition", "abstract": "TV subtitles are a rich source of transcriptions of many types of speech,\nranging from read speech in news reports to conversational and spontaneous\nspeech in talk shows and soaps. However, subtitles are not verbatim (i.e.\nexact) transcriptions of speech, so they cannot be used directly to improve an\nAutomatic Speech Recognition (ASR) model. We propose a multitask dual-decoder\nTransformer model that jointly performs ASR and automatic subtitling. The ASR\ndecoder (possibly pre-trained) predicts the verbatim output and the subtitle\ndecoder generates a subtitle, while sharing the encoder. The two decoders can\nbe independent or connected. The model is trained to perform both tasks\njointly, and is able to effectively use subtitle data. We show improvements on\nregular ASR and on spontaneous and conversational ASR by incorporating the\nadditional subtitle decoder. The method does not require preprocessing\n(aligning, filtering, pseudo-labeling, ...) of the subtitles.", "published": "2022-10-14 13:01:00", "link": "http://arxiv.org/abs/2210.07771v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Prompt Conditioned VAE: Enhancing Generative Replay for Lifelong\n  Learning in Task-Oriented Dialogue", "abstract": "Lifelong learning (LL) is vital for advanced task-oriented dialogue (ToD)\nsystems. To address the catastrophic forgetting issue of LL, generative replay\nmethods are widely employed to consolidate past knowledge with generated pseudo\nsamples. However, most existing generative replay methods use only a single\ntask-specific token to control their models. This scheme is usually not strong\nenough to constrain the generative model due to insufficient information\ninvolved. In this paper, we propose a novel method, prompt conditioned VAE for\nlifelong learning (PCLL), to enhance generative replay by incorporating tasks'\nstatistics. PCLL captures task-specific distributions with a conditional\nvariational autoencoder, conditioned on natural language prompts to guide the\npseudo-sample generation. Moreover, it leverages a distillation process to\nfurther consolidate past knowledge by alleviating the noise in pseudo samples.\nExperiments on natural language understanding tasks of ToD systems demonstrate\nthat PCLL significantly outperforms competitive baselines in building LL\nmodels.", "published": "2022-10-14 13:12:14", "link": "http://arxiv.org/abs/2210.07783v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bringing NURC/SP to Digital Life: the Role of Open-source Automatic\n  Speech Recognition Models", "abstract": "The NURC Project that started in 1969 to study the cultured linguistic urban\nnorm spoken in five Brazilian capitals, was responsible for compiling a large\ncorpus for each capital. The digitized NURC/SP comprises 375 inquiries in 334\nhours of recordings taken in S\\~ao Paulo capital. Although 47 inquiries have\ntranscripts, there was no alignment between the audio-transcription, and 328\ninquiries were not transcribed. This article presents an evaluation and error\nanalysis of three automatic speech recognition models trained with spontaneous\nspeech in Portuguese and one model trained with prepared speech. The evaluation\nallowed us to choose the best model, using WER and CER metrics, in a manually\naligned sample of NURC/SP, to automatically transcribe 284 hours.", "published": "2022-10-14 14:24:30", "link": "http://arxiv.org/abs/2210.07852v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "AVLEN: Audio-Visual-Language Embodied Navigation in 3D Environments", "abstract": "Recent years have seen embodied visual navigation advance in two distinct\ndirections: (i) in equipping the AI agent to follow natural language\ninstructions, and (ii) in making the navigable world multimodal, e.g.,\naudio-visual navigation. However, the real world is not only multimodal, but\nalso often complex, and thus in spite of these advances, agents still need to\nunderstand the uncertainty in their actions and seek instructions to navigate.\nTo this end, we present AVLEN~ -- an interactive agent for\nAudio-Visual-Language Embodied Navigation. Similar to audio-visual navigation\ntasks, the goal of our embodied agent is to localize an audio event via\nnavigating the 3D visual world; however, the agent may also seek help from a\nhuman (oracle), where the assistance is provided in free-form natural language.\nTo realize these abilities, AVLEN uses a multimodal hierarchical reinforcement\nlearning backbone that learns: (a) high-level policies to choose either\naudio-cues for navigation or to query the oracle, and (b) lower-level policies\nto select navigation actions based on its audio-visual and language inputs. The\npolicies are trained via rewarding for the success on the navigation task while\nminimizing the number of queries to the oracle. To empirically evaluate AVLEN,\nwe present experiments on the SoundSpaces framework for semantic audio-visual\nnavigation tasks. Our results show that equipping the agent to ask for help\nleads to a clear improvement in performance, especially in challenging cases,\ne.g., when the sound is unheard during training or in the presence of\ndistractor sounds.", "published": "2022-10-14 16:35:06", "link": "http://arxiv.org/abs/2210.07940v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Improving generalizability of distilled self-supervised speech\n  processing models under distorted settings", "abstract": "Self-supervised learned (SSL) speech pre-trained models perform well across\nvarious speech processing tasks. Distilled versions of SSL models have been\ndeveloped to match the needs of on-device speech applications. Though having\nsimilar performance as original SSL models, distilled counterparts suffer from\nperformance degradation even more than their original versions in distorted\nenvironments. This paper proposes to apply Cross-Distortion Mapping and Domain\nAdversarial Training to SSL models during knowledge distillation to alleviate\nthe performance gap caused by the domain mismatch problem. Results show\nconsistent performance improvements under both in- and out-of-domain distorted\nsetups for different downstream tasks while keeping efficient model size.", "published": "2022-10-14 17:17:45", "link": "http://arxiv.org/abs/2210.07978v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TestAug: A Framework for Augmenting Capability-based NLP Tests", "abstract": "The recently proposed capability-based NLP testing allows model developers to\ntest the functional capabilities of NLP models, revealing functional failures\nthat cannot be detected by the traditional heldout mechanism. However, existing\nwork on capability-based testing requires extensive manual efforts and domain\nexpertise in creating the test cases. In this paper, we investigate a low-cost\napproach for the test case generation by leveraging the GPT-3 engine. We\nfurther propose to use a classifier to remove the invalid outputs from GPT-3\nand expand the outputs into templates to generate more test cases. Our\nexperiments show that TestAug has three advantages over the existing work on\nbehavioral testing: (1) TestAug can find more bugs than existing work; (2) The\ntest cases in TestAug are more diverse; and (3) TestAug largely saves the\nmanual efforts in creating the test suites. The code and data for TestAug can\nbe found at our project website (https://guanqun-yang.github.io/testaug/) and\nGitHub (https://github.com/guanqun-yang/testaug).", "published": "2022-10-14 20:42:16", "link": "http://arxiv.org/abs/2210.08097v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "TweetNERD -- End to End Entity Linking Benchmark for Tweets", "abstract": "Named Entity Recognition and Disambiguation (NERD) systems are foundational\nfor information retrieval, question answering, event detection, and other\nnatural language processing (NLP) applications. We introduce TweetNERD, a\ndataset of 340K+ Tweets across 2010-2021, for benchmarking NERD systems on\nTweets. This is the largest and most temporally diverse open sourced dataset\nbenchmark for NERD on Tweets and can be used to facilitate research in this\narea. We describe evaluation setup with TweetNERD for three NERD tasks: Named\nEntity Recognition (NER), Entity Linking with True Spans (EL), and End to End\nEntity Linking (End2End); and provide performance of existing publicly\navailable methods on specific TweetNERD splits. TweetNERD is available at:\nhttps://doi.org/10.5281/zenodo.6617192 under Creative Commons Attribution 4.0\nInternational (CC BY 4.0) license. Check out more details at\nhttps://github.com/twitter-research/TweetNERD.", "published": "2022-10-14 21:55:07", "link": "http://arxiv.org/abs/2210.08129v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "68T50, 68T07", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Hybrid Reinforced Medical Report Generation with M-Linear Attention and\n  Repetition Penalty", "abstract": "To reduce doctors' workload, deep-learning-based automatic medical report\ngeneration has recently attracted more and more research efforts, where deep\nconvolutional neural networks (CNNs) are employed to encode the input images,\nand recurrent neural networks (RNNs) are used to decode the visual features\ninto medical reports automatically. However, these state-of-the-art methods\nmainly suffer from three shortcomings: (i) incomprehensive optimization, (ii)\nlow-order and unidimensional attention mechanisms, and (iii) repeated\ngeneration. In this article, we propose a hybrid reinforced medical report\ngeneration method with m-linear attention and repetition penalty mechanism\n(HReMRG-MR) to overcome these problems. Specifically, a hybrid reward with\ndifferent weights is employed to remedy the limitations of single-metric-based\nrewards. We also propose a search algorithm with linear complexity to\napproximate the best weight combination. Furthermore, we use m-linear attention\nmodules to explore high-order feature interactions and to achieve multi-modal\nreasoning, while a repetition penalty applies penalties to repeated terms\nduring the model's training process. Extensive experimental studies on two\npublic datasets show that HReMRG-MR greatly outperforms the state-of-the-art\nbaselines in terms of all metrics. We also conducted a series of ablation\nexperiments to prove the effectiveness of all our proposed components. We also\nperformed a reward search toy experiment to give evidence that our proposed\nsearch approach can significantly reduce the search time while approximating\nthe best performance.", "published": "2022-10-14 15:27:34", "link": "http://arxiv.org/abs/2210.13729v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "COFFEE: Counterfactual Fairness for Personalized Text Generation in\n  Explainable Recommendation", "abstract": "As language models become increasingly integrated into our digital lives,\nPersonalized Text Generation (PTG) has emerged as a pivotal component with a\nwide range of applications. However, the bias inherent in user written text,\noften used for PTG model training, can inadvertently associate different levels\nof linguistic quality with users' protected attributes. The model can inherit\nthe bias and perpetuate inequality in generating text w.r.t. users' protected\nattributes, leading to unfair treatment when serving users. In this work, we\ninvestigate fairness of PTG in the context of personalized explanation\ngeneration for recommendations. We first discuss the biases in generated\nexplanations and their fairness implications. To promote fairness, we introduce\na general framework to achieve measure-specific counterfactual fairness in\nexplanation generation. Extensive experiments and human evaluations demonstrate\nthe effectiveness of our method.", "published": "2022-10-14 02:29:10", "link": "http://arxiv.org/abs/2210.15500v2", "categories": ["cs.CL", "cs.CY", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Training speech emotion classifier without categorical annotations", "abstract": "There are two paradigms of emotion representation, categorical labeling and\ndimensional description in continuous space. Therefore, the emotion recognition\ntask can be treated as a classification or regression. The main aim of this\nstudy is to investigate the relation between these two representations and\npropose a classification pipeline that uses only dimensional annotation. The\nproposed approach contains a regressor model which is trained to predict a\nvector of continuous values in dimensional representation for given speech\naudio. The output of this model can be interpreted as an emotional category\nusing a mapping algorithm. We investigated the performances of a combination of\nthree feature extractors, three neural network architectures, and three mapping\nalgorithms on two different corpora. Our study shows the advantages and\nlimitations of the classification via regression approach.", "published": "2022-10-14 08:47:41", "link": "http://arxiv.org/abs/2210.07642v1", "categories": ["cs.SD", "cs.CL", "cs.HC", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Full-Stack Bioacoustics: Field Kit to AI to Action (Workshop report)", "abstract": "Acoustic data (sound recordings) are a vital source of evidence for\ndetecting, counting, and distinguishing wildlife. This domain of \"bioacoustics\"\nhas grown in the past decade due to the massive advances in signal processing\nand machine learning, recording devices, and the capacity of data processing\nand storage. Numerous research papers describe the use of Raspberry Pi or\nsimilar devices for acoustic monitoring, and other research papers describe\nautomatic classification of animal sounds by machine learning. But for most\necologists, zoologists, conservationists, the pieces of the puzzle do not come\ntogether: the domain is fragmented. In this Lorentz workshop we bridge this gap\nby bringing together leading exponents of open hardware and open-source\nsoftware for bioacoustic monitoring and machine learning, as well as ecologists\nand other field researchers. We share skills while also building a vision for\nthe future development of \"bioacoustic AI\".\n  This report contains an overview of the workshop aims and structure, as well\nas reports from the six groups.", "published": "2022-10-14 10:21:56", "link": "http://arxiv.org/abs/2210.07685v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LeVoice ASR Systems for the ISCSLP 2022 Intelligent Cockpit Speech\n  Recognition Challenge", "abstract": "This paper describes LeVoice automatic speech recognition systems to track2\nof intelligent cockpit speech recognition challenge 2022. Track2 is a speech\nrecognition task without limits on the scope of model size. Our main points\ninclude deep learning based speech enhancement, text-to-speech based speech\ngeneration, training data augmentation via various techniques and speech\nrecognition model fusion. We compared and fused the hybrid architecture and two\nkinds of end-to-end architecture. For end-to-end modeling, we used models based\non connectionist temporal classification/attention-based encoder-decoder\narchitecture and recurrent neural network transducer/attention-based\nencoder-decoder architecture. The performance of these models is evaluated with\nan additional language model to improve word error rates. As a result, our\nsystem achieved 10.2\\% character error rate on the challenge test set data and\nranked third place among the submitted systems in the challenge.", "published": "2022-10-14 12:35:25", "link": "http://arxiv.org/abs/2210.07749v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Description and analysis of novelties introduced in DCASE Task 4 2022 on\n  the baseline system", "abstract": "The aim of the Detection and Classification of Acoustic Scenes and Events\nChallenge Task 4 is to evaluate systems for the detection of sound events in\ndomestic environments using an heterogeneous dataset. The systems need to be\nable to correctly detect the sound events present in a recorded audio clip, as\nwell as localize the events in time. This year's task is a follow-up of DCASE\n2021 Task 4, with some important novelties. The goal of this paper is to\ndescribe and motivate these new additions, and report an analysis of their\nimpact on the baseline system. We introduced three main novelties: the use of\nexternal datasets, including recently released strongly annotated clips from\nAudioset, the possibility of leveraging pre-trained models, and a new energy\nconsumption metric to raise awareness about the ecological impact of training\nsound events detectors. The results on the baseline system show that leveraging\nopen-source pretrained on AudioSet improves the results significantly in terms\nof event classification but not in terms of event segmentation.", "published": "2022-10-14 14:29:16", "link": "http://arxiv.org/abs/2210.07856v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Hierarchical Diffusion Models for Singing Voice Neural Vocoder", "abstract": "Recent progress in deep generative models has improved the quality of neural\nvocoders in speech domain. However, generating a high-quality singing voice\nremains challenging due to a wider variety of musical expressions in pitch,\nloudness, and pronunciations. In this work, we propose a hierarchical diffusion\nmodel for singing voice neural vocoders. The proposed method consists of\nmultiple diffusion models operating in different sampling rates; the model at\nthe lowest sampling rate focuses on generating accurate low-frequency\ncomponents such as pitch, and other models progressively generate the waveform\nat higher sampling rates on the basis of the data at the lower sampling rate\nand acoustic features. Experimental results show that the proposed method\nproduces high-quality singing voices for multiple singers, outperforming\nstate-of-the-art neural vocoders with a similar range of computational costs.", "published": "2022-10-14 04:30:09", "link": "http://arxiv.org/abs/2210.07508v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Transformer-Based Speech Synthesizer Attribution in an Open Set Scenario", "abstract": "Speech synthesis methods can create realistic-sounding speech, which may be\nused for fraud, spoofing, and misinformation campaigns. Forensic methods that\ndetect synthesized speech are important for protection against such attacks.\nForensic attribution methods provide even more information about the nature of\nsynthesized speech signals because they identify the specific speech synthesis\nmethod (i.e., speech synthesizer) used to create a speech signal. Due to the\nincreasing number of realistic-sounding speech synthesizers, we propose a\nspeech attribution method that generalizes to new synthesizers not seen during\ntraining. To do so, we investigate speech synthesizer attribution in both a\nclosed set scenario and an open set scenario. In other words, we consider some\nspeech synthesizers to be \"known\" synthesizers (i.e., part of the closed set)\nand others to be \"unknown\" synthesizers (i.e., part of the open set). We\nrepresent speech signals as spectrograms and train our proposed method, known\nas compact attribution transformer (CAT), on the closed set for multi-class\nclassification. Then, we extend our analysis to the open set to attribute\nsynthesized speech signals to both known and unknown synthesizers. We utilize a\nt-distributed stochastic neighbor embedding (tSNE) on the latent space of the\ntrained CAT to differentiate between each unknown synthesizer. Additionally, we\nexplore poly-1 loss formulations to improve attribution results. Our proposed\napproach successfully attributes synthesized speech signals to their respective\nspeech synthesizers in both closed and open set scenarios.", "published": "2022-10-14 05:55:21", "link": "http://arxiv.org/abs/2210.07546v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TransFusion: Transcribing Speech with Multinomial Diffusion", "abstract": "Diffusion models have shown exceptional scaling properties in the image\nsynthesis domain, and initial attempts have shown similar benefits for applying\ndiffusion to unconditional text synthesis. Denoising diffusion models attempt\nto iteratively refine a sampled noise signal until it resembles a coherent\nsignal (such as an image or written sentence). In this work we aim to see\nwhether the benefits of diffusion models can also be realized for speech\nrecognition. To this end, we propose a new way to perform speech recognition\nusing a diffusion model conditioned on pretrained speech features.\nSpecifically, we propose TransFusion: a transcribing diffusion model which\niteratively denoises a random character sequence into coherent text\ncorresponding to the transcript of a conditioning utterance. We demonstrate\ncomparable performance to existing high-performing contrastive models on the\nLibriSpeech speech recognition benchmark. To the best of our knowledge, we are\nthe first to apply denoising diffusion to speech recognition. We also propose\nnew techniques for effectively sampling and decoding multinomial diffusion\nmodels. These are required because traditional methods of sampling from\nacoustic models are not possible with our new discrete diffusion approach. Code\nand trained models are available: https://github.com/RF5/transfusion-asr", "published": "2022-10-14 10:01:43", "link": "http://arxiv.org/abs/2210.07677v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Intel Labs at Ego4D Challenge 2022: A Better Baseline for Audio-Visual\n  Diarization", "abstract": "This report describes our approach for the Audio-Visual Diarization (AVD)\ntask of the Ego4D Challenge 2022. Specifically, we present multiple technical\nimprovements over the official baselines. First, we improve the detection\nperformance of the camera wearer's voice activity by modifying the training\nscheme of its model. Second, we discover that an off-the-shelf voice activity\ndetection model can effectively remove false positives when it is applied\nsolely to the camera wearer's voice activities. Lastly, we show that better\nactive speaker detection leads to a better AVD outcome. Our final method\nobtains 65.9% DER on the test set of Ego4D, which significantly outperforms all\nthe baselines. Our submission achieved 1st place in the Ego4D Challenge 2022.", "published": "2022-10-14 12:54:03", "link": "http://arxiv.org/abs/2210.07764v3", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Accelerating RNN-based Speech Enhancement on a Multi-Core MCU with Mixed\n  FP16-INT8 Post-Training Quantization", "abstract": "This paper presents an optimized methodology to design and deploy Speech\nEnhancement (SE) algorithms based on Recurrent Neural Networks (RNNs) on a\nstate-of-the-art MicroController Unit (MCU), with 1+8 general-purpose RISC-V\ncores. To achieve low-latency execution, we propose an optimized software\npipeline interleaving parallel computation of LSTM or GRU recurrent blocks,\nfeaturing vectorized 8-bit integer (INT8) and 16-bit floating-point (FP16)\ncompute units, with manually-managed memory transfers of model parameters. To\nensure minimal accuracy degradation with respect to the full-precision models,\nwe propose a novel FP16-INT8 Mixed-Precision Post-Training Quantization (PTQ)\nscheme that compresses the recurrent layers to 8-bit while the bit precision of\nremaining layers is kept to FP16. Experiments are conducted on multiple LSTM\nand GRU based SE models trained on the Valentini dataset, featuring up to 1.24M\nparameters. Thanks to the proposed approaches, we speed-up the computation by\nup to 4x with respect to the lossless FP16 baselines. Differently from a\nuniform 8-bit quantization that degrades the PESQ score by 0.3 on average, the\nMixed-Precision PTQ scheme leads to a low-degradation of only 0.06, while\nachieving a 1.4-1.7x memory saving. Thanks to this compression, we cut the\npower cost of the external memory by fitting the large models on the limited\non-chip non-volatile memory and we gain a MCU power saving of up to 2.5x by\nreducing the supply voltage from 0.8V to 0.65V while still matching the\nreal-time constraints. Our design results 10x more energy efficient than\nstate-of-the-art SE solutions deployed on single-core MCUs that make use of\nsmaller models and quantization-aware training.", "published": "2022-10-14 10:32:05", "link": "http://arxiv.org/abs/2210.07692v1", "categories": ["cs.SD", "cs.LG", "cs.SY", "eess.AS", "eess.SY"], "primary_category": "cs.SD"}
