{"title": "Using Syntactic Features for Phishing Detection", "abstract": "This paper reports on the comparison of the subject and object of verbs in\ntheir usage between phishing emails and legitimate emails. The purpose of this\nresearch is to explore whether the syntactic structures and subjects and\nobjects of verbs can be distinguishable features for phishing detection. To\nachieve the objective, we have conducted two series of experiments: the\nsyntactic similarity for sentences, and the subject and object of verb\ncomparison. The results of the experiments indicated that both features can be\nused for some verbs, but more work has to be done for others.", "published": "2015-05-29 21:51:04", "link": "http://arxiv.org/abs/1506.00037v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Supervised Fine Tuning for Word Embedding with Integrated Knowledge", "abstract": "Learning vector representation for words is an important research field which\nmay benefit many natural language processing tasks. Two limitations exist in\nnearly all available models, which are the bias caused by the context\ndefinition and the lack of knowledge utilization. They are difficult to tackle\nbecause these algorithms are essentially unsupervised learning approaches.\nInspired by deep learning, the authors propose a supervised framework for\nlearning vector representation of words to provide additional supervised fine\ntuning after unsupervised learning. The framework is knowledge rich approacher\nand compatible with any numerical vectors word representation. The authors\nperform both intrinsic evaluation like attributional and relational similarity\nprediction and extrinsic evaluations like the sentence completion and sentiment\nanalysis. Experiments results on 6 embeddings and 4 tasks with 10 datasets show\nthat the proposed fine tuning framework may significantly improve the quality\nof the vector representation of words.", "published": "2015-05-29 06:11:00", "link": "http://arxiv.org/abs/1505.07931v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling meaning: computational interpreting and understanding of\n  natural language fragments", "abstract": "In this introductory article we present the basics of an approach to\nimplementing computational interpreting of natural language aiming to model the\nmeanings of words and phrases. Unlike other approaches, we attempt to define\nthe meanings of text fragments in a composable and computer interpretable way.\nWe discuss models and ideas for detecting different types of semantic\nincomprehension and choosing the interpretation that makes most sense in a\ngiven context. Knowledge representation is designed for handling\ncontext-sensitive and uncertain / imprecise knowledge, and for easy\naccommodation of new information. It stores quantitative information capturing\nthe essence of the concepts, because it is crucial for working with natural\nlanguage understanding and reasoning. Still, the representation is general\nenough to allow for new knowledge to be learned, and even generated by the\nsystem. The article concludes by discussing some reasoning-related topics:\npossible approaches to generation of new abstract concepts, and describing\nsituations and concepts in words (e.g. for specifying interpretation\ndifficulties).", "published": "2015-05-29 19:06:42", "link": "http://arxiv.org/abs/1505.08149v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Solving Verbal Comprehension Questions in IQ Test by Knowledge-Powered\n  Word Embedding", "abstract": "Intelligence Quotient (IQ) Test is a set of standardized questions designed\nto evaluate human intelligence. Verbal comprehension questions appear very\nfrequently in IQ tests, which measure human's verbal ability including the\nunderstanding of the words with multiple senses, the synonyms and antonyms, and\nthe analogies among words. In this work, we explore whether such tests can be\nsolved automatically by artificial intelligence technologies, especially the\ndeep learning technologies that are recently developed and successfully applied\nin a number of fields. However, we found that the task was quite challenging,\nand simply applying existing technologies (e.g., word embedding) could not\nachieve a good performance, mainly due to the multiple senses of words and the\ncomplex relations among words. To tackle these challenges, we propose a novel\nframework consisting of three components. First, we build a classifier to\nrecognize the specific type of a verbal question (e.g., analogy,\nclassification, synonym, or antonym). Second, we obtain distributed\nrepresentations of words and relations by leveraging a novel word embedding\nmethod that considers the multi-sense nature of words and the relational\nknowledge among words (or their senses) contained in dictionaries. Third, for\neach type of questions, we propose a specific solver based on the obtained\ndistributed word representations and relation representations. Experimental\nresults have shown that the proposed framework can not only outperform existing\nmethods for solving verbal comprehension questions but also exceed the average\nperformance of the Amazon Mechanical Turk workers involved in the study. The\nresults indicate that with appropriate uses of the deep learning technologies\nwe might be a further step closer to the human intelligence.", "published": "2015-05-29 02:46:44", "link": "http://arxiv.org/abs/1505.07909v4", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transition-Based Dependency Parsing with Stack Long Short-Term Memory", "abstract": "We propose a technique for learning representations of parser states in\ntransition-based dependency parsers. Our primary innovation is a new control\nstructure for sequence-to-sequence neural networks---the stack LSTM. Like the\nconventional stack data structures used in transition-based parsing, elements\ncan be pushed to or popped from the top of the stack in constant time, but, in\naddition, an LSTM maintains a continuous space embedding of the stack contents.\nThis lets us formulate an efficient parsing model that captures three facets of\na parser's state: (i) unbounded look-ahead into the buffer of incoming words,\n(ii) the complete history of actions taken by the parser, and (iii) the\ncomplete contents of the stack of partially built tree fragments, including\ntheir internal structures. Standard backpropagation techniques are used for\ntraining and yield state-of-the-art parsing performance.", "published": "2015-05-29 14:58:12", "link": "http://arxiv.org/abs/1505.08075v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
