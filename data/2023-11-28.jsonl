{"title": "Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case\n  Study in Medicine", "abstract": "Generalist foundation models such as GPT-4 have displayed surprising\ncapabilities in a wide variety of domains and tasks. Yet, there is a prevalent\nassumption that they cannot match specialist capabilities of fine-tuned models.\nFor example, most explorations to date on medical competency benchmarks have\nleveraged domain-specific training, as exemplified by efforts on BioGPT and\nMed-PaLM. We build on a prior study of GPT-4's capabilities on medical\nchallenge benchmarks in the absence of special training. Rather than using\nsimple prompting to highlight the model's out-of-the-box capabilities, we\nperform a systematic exploration of prompt engineering. We find that prompting\ninnovation can unlock deeper specialist capabilities and show that GPT-4 easily\ntops prior leading results for medical benchmarks. The prompting methods we\nexplore are general purpose, and make no specific use of domain expertise,\nremoving the need for expert-curated content. Our experimental design carefully\ncontrols for overfitting during the prompt engineering process. We introduce\nMedprompt, based on a composition of several prompting strategies. With\nMedprompt, GPT-4 achieves state-of-the-art results on all nine of the benchmark\ndatasets in the MultiMedQA suite. The method outperforms leading specialist\nmodels such as Med-PaLM 2 by a significant margin with an order of magnitude\nfewer calls to the model. Steering GPT-4 with Medprompt achieves a 27%\nreduction in error rate on the MedQA dataset over the best methods to date\nachieved with specialist models and surpasses a score of 90% for the first\ntime. Beyond medical problems, we show the power of Medprompt to generalize to\nother domains and provide evidence for the broad applicability of the approach\nvia studies of the strategy on exams in electrical engineering, machine\nlearning, philosophy, accounting, law, nursing, and clinical psychology.", "published": "2023-11-28 03:16:12", "link": "http://arxiv.org/abs/2311.16452v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Recognizing Conditional Causal Relationships about Emotions and Their\n  Corresponding Conditions", "abstract": "The study of causal relationships between emotions and causes in texts has\nrecently received much attention. Most works focus on extracting causally\nrelated clauses from documents. However, none of these works has considered\nthat the causal relationships among the extracted emotion and cause clauses can\nonly be valid under some specific context clauses. To highlight the context in\nsuch special causal relationships, we propose a new task to determine whether\nor not an input pair of emotion and cause has a valid causal relationship under\ndifferent contexts and extract the specific context clauses that participate in\nthe causal relationship. Since the task is new for which no existing dataset is\navailable, we conduct manual annotation on a benchmark dataset to obtain the\nlabels for our tasks and the annotations of each context clause's type that can\nalso be used in some other applications. We adopt negative sampling to\nconstruct the final dataset to balance the number of documents with and without\ncausal relationships. Based on the constructed dataset, we propose an\nend-to-end multi-task framework, where we design two novel and general modules\nto handle the two goals of our task. Specifically, we propose a context masking\nmodule to extract the context clauses participating in the causal\nrelationships. We propose a prediction aggregation module to fine-tune the\nprediction results according to whether the input emotion and causes depend on\nspecific context clauses. Results of extensive comparative experiments and\nablation studies demonstrate the effectiveness and generality of our proposed\nframework.", "published": "2023-11-28 07:47:25", "link": "http://arxiv.org/abs/2311.16579v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ascle: A Python Natural Language Processing Toolkit for Medical Text\n  Generation", "abstract": "This study introduces Ascle, a pioneering natural language processing (NLP)\ntoolkit designed for medical text generation. Ascle is tailored for biomedical\nresearchers and healthcare professionals with an easy-to-use, all-in-one\nsolution that requires minimal programming expertise. For the first time, Ascle\nevaluates and provides interfaces for the latest pre-trained language models,\nencompassing four advanced and challenging generative functions:\nquestion-answering, text summarization, text simplification, and machine\ntranslation. In addition, Ascle integrates 12 essential NLP functions, along\nwith query and search capabilities for clinical databases. The toolkit, its\nmodels, and associated data are publicly available via\nhttps://github.com/Yale-LILY/MedGen.", "published": "2023-11-28 08:13:29", "link": "http://arxiv.org/abs/2311.16588v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Positioning Political Texts with Large Language Models by Asking and\n  Averaging", "abstract": "We use instruction-tuned Large Language Models (LLMs) like GPT-4, Llama 3,\nMiXtral, or Aya to position political texts within policy and ideological\nspaces. We ask an LLM where a tweet or a sentence of a political text stands on\nthe focal dimension and take the average of the LLM responses to position\npolitical actors such as US Senators, or longer texts such as UK party\nmanifestos or EU policy speeches given in 10 different languages. The\ncorrelations between the position estimates obtained with the best LLMs and\nbenchmarks based on text coding by experts, crowdworkers, or roll call votes\nexceed .90. This approach is generally more accurate than the positions\nobtained with supervised classifiers trained on large amounts of research data.\nUsing instruction-tuned LLMs to position texts in policy and ideological spaces\nis fast, cost-efficient, reliable, and reproducible (in the case of open LLMs)\neven if the texts are short and written in different languages. We conclude\nwith cautionary notes about the need for empirical validation.", "published": "2023-11-28 09:45:02", "link": "http://arxiv.org/abs/2311.16639v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text2Tree: Aligning Text Representation to the Label Tree Hierarchy for\n  Imbalanced Medical Classification", "abstract": "Deep learning approaches exhibit promising performances on various text\ntasks. However, they are still struggling on medical text classification since\nsamples are often extremely imbalanced and scarce. Different from existing\nmainstream approaches that focus on supplementary semantics with external\nmedical information, this paper aims to rethink the data challenges in medical\ntexts and present a novel framework-agnostic algorithm called Text2Tree that\nonly utilizes internal label hierarchy in training deep learning models. We\nembed the ICD code tree structure of labels into cascade attention modules for\nlearning hierarchy-aware label representations. Two new learning schemes,\nSimilarity Surrogate Learning (SSL) and Dissimilarity Mixup Learning (DML), are\ndevised to boost text classification by reusing and distinguishing samples of\nother labels following the label representation hierarchy, respectively.\nExperiments on authoritative public datasets and real-world medical records\nshow that our approach stably achieves superior performances over classical and\nadvanced imbalanced classification methods.", "published": "2023-11-28 10:02:08", "link": "http://arxiv.org/abs/2311.16650v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Distribution-Based Threshold for Determining Sentence Similarity", "abstract": "We hereby present a solution to a semantic textual similarity (STS) problem\nin which it is necessary to match two sentences containing, as the only\ndistinguishing factor, highly specific information (such as names, addresses,\nidentification codes), and from which we need to derive a definition for when\nthey are similar and when they are not. The solution revolves around the use of\na neural network, based on the siamese architecture, to create the\ndistributions of the distances between similar and dissimilar pairs of\nsentences. The goal of these distributions is to find a discriminating factor,\nthat we call \"threshold\", which represents a well-defined quantity that can be\nused to distinguish vector distances of similar pairs from vector distances of\ndissimilar pairs in new predictions and later analyses. In addition, we\ndeveloped a way to score the predictions by combining attributes from both the\ndistributions' features and the way the distance function works. Finally, we\ngeneralize the results showing that they can be transferred to a wider range of\ndomains by applying the system discussed to a well-known and widely used\nbenchmark dataset for STS problems.", "published": "2023-11-28 10:42:35", "link": "http://arxiv.org/abs/2311.16675v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entity-Aspect-Opinion-Sentiment Quadruple Extraction for Fine-grained\n  Sentiment Analysis", "abstract": "Product reviews often contain a large number of implicit aspects and\nobject-attribute co-existence cases. Unfortunately, many existing studies in\nAspect-Based Sentiment Analysis (ABSA) have overlooked this issue, which can\nmake it difficult to extract opinions comprehensively and fairly. In this\npaper, we propose a new task called Entity-Aspect-Opinion-Sentiment Quadruple\nExtraction (EASQE), which aims to hierarchically decompose aspect terms into\nentities and aspects to avoid information loss, non-exclusive annotations, and\nopinion misunderstandings in ABSA tasks. To facilitate research in this new\ntask, we have constructed four datasets (Res14-EASQE, Res15-EASQE, Res16-EASQE,\nand Lap14-EASQE) based on the SemEval Restaurant and Laptop datasets. We have\nalso proposed a novel two-stage sequence-tagging based Trigger-Opinion\nframework as the baseline for the EASQE task. Empirical evaluations show that\nour Trigger-Opinion framework can generate satisfactory EASQE results and can\nalso be applied to other ABSA tasks, significantly outperforming\nstate-of-the-art methods. We have made the four datasets and source code of\nTrigger-Opinion publicly available to facilitate further research in this area.", "published": "2023-11-28 10:50:00", "link": "http://arxiv.org/abs/2311.16678v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Radiology-Aware Model-Based Evaluation Metric for Report Generation", "abstract": "We propose a new automated evaluation metric for machine-generated radiology\nreports using the successful COMET architecture adapted for the radiology\ndomain. We train and publish four medically-oriented model checkpoints,\nincluding one trained on RadGraph, a radiology knowledge graph. Our results\nshow that our metric correlates moderately to high with established metrics\nsuch as BERTscore, BLEU, and CheXbert scores. Furthermore, we demonstrate that\none of our checkpoints exhibits a high correlation with human judgment, as\nassessed using the publicly available annotations of six board-certified\nradiologists, using a set of 200 reports. We also performed our own analysis\ngathering annotations with two radiologists on a collection of 100 reports. The\nresults indicate the potential effectiveness of our method as a\nradiology-specific evaluation metric. The code, data, and model checkpoints to\nreproduce our findings will be publicly available.", "published": "2023-11-28 13:08:26", "link": "http://arxiv.org/abs/2311.16764v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Optimal Reference Translations", "abstract": "The overall translation quality reached by current machine translation (MT)\nsystems for high-resourced language pairs is remarkably good. Standard methods\nof evaluation are not suitable nor intended to uncover the many translation\nerrors and quality deficiencies that still persist. Furthermore, the quality of\nstandard reference translations is commonly questioned and comparable quality\nlevels have been reached by MT alone in several language pairs. Navigating\nfurther research in these high-resource settings is thus difficult. In this\narticle, we propose a methodology for creating more reliable document-level\nhuman reference translations, called \"optimal reference translations,\" with the\nsimple aim to raise the bar of what should be deemed \"human translation\nquality.\" We evaluate the obtained document-level optimal reference\ntranslations in comparison with \"standard\" ones, confirming a significant\nquality increase and also documenting the relationship between evaluation and\ntranslation editing.", "published": "2023-11-28 13:50:50", "link": "http://arxiv.org/abs/2311.16787v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Benchmark for Evaluating Machine Translation Metrics on Dialects\n  Without Standard Orthography", "abstract": "For sensible progress in natural language processing, it is important that we\nare aware of the limitations of the evaluation metrics we use. In this work, we\nevaluate how robust metrics are to non-standardized dialects, i.e. spelling\ndifferences in language varieties that do not have a standard orthography. To\ninvestigate this, we collect a dataset of human translations and human\njudgments for automatic machine translations from English to two Swiss German\ndialects. We further create a challenge set for dialect variation and benchmark\nexisting metrics' performances. Our results show that existing metrics cannot\nreliably evaluate Swiss German text generation outputs, especially on segment\nlevel. We propose initial design adaptations that increase robustness in the\nface of non-standardized dialects, although there remains much room for further\nimprovement. The dataset, code, and models are available here:\nhttps://github.com/textshuttle/dialect_eval", "published": "2023-11-28 15:12:11", "link": "http://arxiv.org/abs/2311.16865v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Natural Language Processing Through Transfer Learning: A Case Study on\n  Sentiment Analysis", "abstract": "Artificial intelligence and machine learning have significantly bolstered the\ntechnological world. This paper explores the potential of transfer learning in\nnatural language processing focusing mainly on sentiment analysis. The models\ntrained on the big data can also be used where data are scarce. The claim is\nthat, compared to training models from scratch, transfer learning, using\npre-trained BERT models, can increase sentiment classification accuracy. The\nstudy adopts a sophisticated experimental design that uses the IMDb dataset of\nsentimentally labelled movie reviews. Pre-processing includes tokenization and\nencoding of text data, making it suitable for NLP models. The dataset is used\non a BERT based model, measuring its performance using accuracy. The result\ncomes out to be 100 per cent accurate. Although the complete accuracy could\nappear impressive, it might be the result of overfitting or a lack of\ngeneralization. Further analysis is required to ensure the model's ability to\nhandle diverse and unseen data. The findings underscore the effectiveness of\ntransfer learning in NLP, showcasing its potential to excel in sentiment\nanalysis tasks. However, the research calls for a cautious interpretation of\nperfect accuracy and emphasizes the need for additional measures to validate\nthe model's generalization.", "published": "2023-11-28 17:12:06", "link": "http://arxiv.org/abs/2311.16965v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing the influence of attractor-verb distance on grammatical\n  agreement in humans and language models", "abstract": "Subject-verb agreement in the presence of an attractor noun located between\nthe main noun and the verb elicits complex behavior: judgments of\ngrammaticality are modulated by the grammatical features of the attractor. For\nexample, in the sentence \"The girl near the boys likes climbing\", the attractor\n(boys) disagrees in grammatical number with the verb (likes), creating a\nlocally implausible transition probability. Here, we parametrically modulate\nthe distance between the attractor and the verb while keeping the length of the\nsentence equal. We evaluate the performance of both humans and two artificial\nneural network models: both make more mistakes when the attractor is closer to\nthe verb, but neural networks get close to the chance level while humans are\nmostly able to overcome the attractor interference. Additionally, we report a\nlinear effect of attractor distance on reaction times. We hypothesize that a\npossible reason for the proximity effect is the calculation of transition\nprobabilities between adjacent words. Nevertheless, classical models of\nattraction such as the cue-based model might suffice to explain this\nphenomenon, thus paving the way for new research. Data and analyses available\nat https://osf.io/d4g6k", "published": "2023-11-28 17:25:34", "link": "http://arxiv.org/abs/2311.16978v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChatGPT's One-year Anniversary: Are Open-Source Large Language Models\n  Catching up?", "abstract": "Upon its release in late 2022, ChatGPT has brought a seismic shift in the\nentire landscape of AI, both in research and commerce. Through\ninstruction-tuning a large language model (LLM) with supervised fine-tuning and\nreinforcement learning from human feedback, it showed that a model could answer\nhuman questions and follow instructions on a broad panel of tasks. Following\nthis success, interests in LLMs have intensified, with new LLMs flourishing at\nfrequent interval across academia and industry, including many start-ups\nfocused on LLMs. While closed-source LLMs (e.g., OpenAI's GPT, Anthropic's\nClaude) generally outperform their open-source counterparts, the progress on\nthe latter has been rapid with claims of achieving parity or even better on\ncertain tasks. This has crucial implications not only on research but also on\nbusiness. In this work, on the first anniversary of ChatGPT, we provide an\nexhaustive overview of this success, surveying all tasks where an open-source\nLLM has claimed to be on par or better than ChatGPT.", "published": "2023-11-28 17:44:51", "link": "http://arxiv.org/abs/2311.16989v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RETSim: Resilient and Efficient Text Similarity", "abstract": "This paper introduces RETSim (Resilient and Efficient Text Similarity), a\nlightweight, multilingual deep learning model trained to produce robust metric\nembeddings for near-duplicate text retrieval, clustering, and dataset\ndeduplication tasks. We demonstrate that RETSim is significantly more robust\nand accurate than MinHash and neural text embeddings, achieving new\nstate-of-the-art performance on dataset deduplication, adversarial text\nretrieval benchmarks, and spam clustering tasks. We also introduce the W4NT3D\nbenchmark (Wiki-40B 4dversarial Near-T3xt Dataset) for evaluating multilingual,\nnear-duplicate text retrieval capabilities under adversarial settings. RETSim\nand the W4NT3D benchmark are open-sourced under the MIT License at\nhttps://github.com/google/unisim.", "published": "2023-11-28 22:54:33", "link": "http://arxiv.org/abs/2311.17264v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntactic Fusion: Enhancing Aspect-Level Sentiment Analysis Through\n  Multi-Tree Graph Integration", "abstract": "Recent progress in aspect-level sentiment classification has been propelled\nby the incorporation of graph neural networks (GNNs) leveraging syntactic\nstructures, particularly dependency trees. Nevertheless, the performance of\nthese models is often hampered by the innate inaccuracies of parsing\nalgorithms. To mitigate this challenge, we introduce SynthFusion, an innovative\ngraph ensemble method that amalgamates predictions from multiple parsers. This\nstrategy blends diverse dependency relations prior to the application of GNNs,\nenhancing robustness against parsing errors while avoiding extra computational\nburdens. SynthFusion circumvents the pitfalls of overparameterization and\ndiminishes the risk of overfitting, prevalent in models with stacked GNN\nlayers, by optimizing graph connectivity. Our empirical evaluations on the\nSemEval14 and Twitter14 datasets affirm that SynthFusion not only outshines\nmodels reliant on single dependency trees but also eclipses alternative\nensemble techniques, achieving this without an escalation in model complexity.", "published": "2023-11-28 15:28:22", "link": "http://arxiv.org/abs/2312.03738v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntax-Informed Interactive Model for Comprehensive Aspect-Based\n  Sentiment Analysis", "abstract": "Aspect-based sentiment analysis (ABSA), a nuanced task in text analysis,\nseeks to discern sentiment orientation linked to specific aspect terms in text.\nTraditional approaches often overlook or inadequately model the explicit\nsyntactic structures of sentences, crucial for effective aspect term\nidentification and sentiment determination. Addressing this gap, we introduce\nan innovative model: Syntactic Dependency Enhanced Multi-Task Interaction\nArchitecture (SDEMTIA) for comprehensive ABSA. Our approach innovatively\nexploits syntactic knowledge (dependency relations and types) using a\nspecialized Syntactic Dependency Embedded Interactive Network (SDEIN). We also\nincorporate a novel and efficient message-passing mechanism within a multi-task\nlearning framework to bolster learning efficacy. Our extensive experiments on\nbenchmark datasets showcase our model's superiority, significantly surpassing\nexisting methods. Additionally, incorporating BERT as an auxiliary feature\nextractor further enhances our model's performance.", "published": "2023-11-28 16:03:22", "link": "http://arxiv.org/abs/2312.03739v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MANTIS at #SMM4H 2023: Leveraging Hybrid and Ensemble Models for\n  Detection of Social Anxiety Disorder on Reddit", "abstract": "This paper presents our system employed for the Social Media Mining for\nHealth 2023 Shared Task 4: Binary classification of English Reddit posts\nself-reporting a social anxiety disorder diagnosis. We systematically\ninvestigate and contrast the efficacy of hybrid and ensemble models that\nharness specialized medical domain-adapted transformers in conjunction with\nBiLSTM neural networks. The evaluation results outline that our best performing\nmodel obtained 89.31% F1 on the validation set and 83.76% F1 on the test set.", "published": "2023-11-28 09:33:41", "link": "http://arxiv.org/abs/2312.09451v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CDEval: A Benchmark for Measuring the Cultural Dimensions of Large\n  Language Models", "abstract": "As the scaling of Large Language Models (LLMs) has dramatically enhanced\ntheir capabilities, there has been a growing focus on the alignment problem to\nensure their responsible and ethical use. While existing alignment efforts\npredominantly concentrate on universal values such as the HHH principle, the\naspect of culture, which is inherently pluralistic and diverse, has not\nreceived adequate attention. This work introduces a new benchmark, CDEval,\naimed at evaluating the cultural dimensions of LLMs. CDEval is constructed by\nincorporating both GPT-4's automated generation and human verification,\ncovering six cultural dimensions across seven domains. Our comprehensive\nexperiments provide intriguing insights into the culture of mainstream LLMs,\nhighlighting both consistencies and variations across different dimensions and\ndomains. The findings underscore the importance of integrating cultural\nconsiderations in LLM development, particularly for applications in diverse\ncultural settings. Through CDEval, we aim to broaden the horizon of LLM\nalignment research by including cultural dimensions, thus providing a more\nholistic framework for the future development and evaluation of LLMs. This\nbenchmark serves as a valuable resource for cultural studies in LLMs, paving\nthe way for more culturally aware and sensitive models.", "published": "2023-11-28 02:01:25", "link": "http://arxiv.org/abs/2311.16421v3", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Exo2EgoDVC: Dense Video Captioning of Egocentric Procedural Activities\n  Using Web Instructional Videos", "abstract": "We propose a novel benchmark for cross-view knowledge transfer of dense video\ncaptioning, adapting models from web instructional videos with exocentric views\nto an egocentric view. While dense video captioning (predicting time segments\nand their captions) is primarily studied with exocentric videos (e.g.,\nYouCook2), benchmarks with egocentric videos are restricted due to data\nscarcity. To overcome the limited video availability, transferring knowledge\nfrom abundant exocentric web videos is demanded as a practical approach.\nHowever, learning the correspondence between exocentric and egocentric views is\ndifficult due to their dynamic view changes. The web videos contain shots\nshowing either full-body or hand regions, while the egocentric view is\nconstantly shifting. This necessitates the in-depth study of cross-view\ntransfer under complex view changes. To this end, we first create a real-life\negocentric dataset (EgoYC2) whose captions follow the definition of YouCook2\ncaptions, enabling transfer learning between these datasets with access to\ntheir ground-truth. To bridge the view gaps, we propose a view-invariant\nlearning method using adversarial training, which consists of pre-training and\nfine-tuning stages. Our experiments confirm the effectiveness of overcoming the\nview change problem and knowledge transfer to egocentric views. Our benchmark\npushes the study of cross-view transfer into a new task domain of dense video\ncaptioning and envisions methodologies that describe egocentric videos in\nnatural language.", "published": "2023-11-28 02:51:13", "link": "http://arxiv.org/abs/2311.16444v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "On the Long Range Abilities of Transformers", "abstract": "Despite their dominance in modern DL and, especially, NLP domains,\ntransformer architectures exhibit sub-optimal performance on long-range tasks\ncompared to recent layers that are specifically designed for this purpose. In\nthis work, drawing inspiration from key attributes of long-range layers, such\nas state-space layers, linear RNN layers, and global convolution layers, we\ndemonstrate that minimal modifications to the transformer architecture can\nsignificantly enhance performance on the Long Range Arena (LRA) benchmark, thus\nnarrowing the gap with these specialized layers. We identify that two key\nprinciples for long-range tasks are (i) incorporating an inductive bias towards\nsmoothness, and (ii) locality. As we show, integrating these ideas into the\nattention mechanism improves results with a negligible amount of additional\ncomputation and without any additional trainable parameters. Our theory and\nexperiments also shed light on the reasons for the inferior performance of\ntransformers on long-range tasks and identify critical properties that are\nessential for successfully capturing long-range dependencies.", "published": "2023-11-28 09:21:48", "link": "http://arxiv.org/abs/2311.16620v1", "categories": ["cs.LG", "cs.CL", "F.2.2; I.2.7"], "primary_category": "cs.LG"}
{"title": "A Survey of the Evolution of Language Model-Based Dialogue Systems", "abstract": "Dialogue systems, including task-oriented_dialogue_system (TOD) and\nopen-domain_dialogue_system (ODD), have undergone significant transformations,\nwith language_models (LM) playing a central role. This survey delves into the\nhistorical trajectory of dialogue systems, elucidating their intricate\nrelationship with advancements in language models by categorizing this\nevolution into four distinct stages, each marked by pivotal LM breakthroughs:\n1) Early_Stage: characterized by statistical LMs, resulting in rule-based or\nmachine-learning-driven dialogue_systems; 2) Independent development of TOD and\nODD based on neural_language_models (NLM; e.g., LSTM and GRU), since NLMs lack\nintrinsic knowledge in their parameters; 3) fusion between different types of\ndialogue systems with the advert of pre-trained_language_models (PLMs),\nstarting from the fusion between four_sub-tasks_within_TOD, and then\nTOD_with_ODD; and 4) current LLM-based_dialogue_system, wherein LLMs can be\nused to conduct TOD and ODD seamlessly. Thus, our survey provides a\nchronological perspective aligned with LM breakthroughs, offering a\ncomprehensive review of state-of-the-art research outcomes. What's more, we\nfocus on emerging topics and discuss open challenges, providing valuable\ninsights into future directions for LLM-based_dialogue_systems. Through this\nexploration, we pave the way for a deeper_comprehension of the evolution,\nguiding future developments in LM-based dialogue_systems.", "published": "2023-11-28 13:51:32", "link": "http://arxiv.org/abs/2311.16789v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Polarized Online Discourse on Abortion: Frames and Hostile Expressions\n  among Liberals and Conservatives", "abstract": "Abortion has been one of the most divisive issues in the United States. Yet,\nmissing is comprehensive longitudinal evidence on how political divides on\nabortion are reflected in public discourse over time, on a national scale, and\nin response to key events before and after the overturn of Roe v Wade. We\nanalyze a corpus of over 3.5M tweets related to abortion over the span of one\nyear (January 2022 to January 2023) from over 1.1M users. We estimate users'\nideology and rely on state-of-the-art transformer-based classifiers to identify\nexpressions of hostility and extract five prominent frames surrounding\nabortion. We use those data to examine (a) how prevalent were expressions of\nhostility (i.e., anger, toxic speech, insults, obscenities, and hate speech),\n(b) what frames liberals and conservatives used to articulate their positions\non abortion, and (c) the prevalence of hostile expressions in liberals and\nconservative discussions of these frames. We show that liberals and\nconservatives largely mirrored each other's use of hostile expressions: as\nliberals used more hostile rhetoric, so did conservatives, especially in\nresponse to key events. In addition, the two groups used distinct frames and\ndiscussed them in vastly distinct contexts, suggesting that liberals and\nconservatives have differing perspectives on abortion. Lastly, frames favored\nby one side provoked hostile reactions from the other: liberals use more\nhostile expressions when addressing religion, fetal personhood, and exceptions\nto abortion bans, whereas conservatives use more hostile language when\naddressing bodily autonomy and women's health. This signals disrespect and\nderogation, which may further preclude understanding and exacerbate\npolarization.", "published": "2023-11-28 14:49:17", "link": "http://arxiv.org/abs/2311.16831v2", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "CharacterGLM: Customizing Chinese Conversational AI Characters with\n  Large Language Models", "abstract": "In this paper, we present CharacterGLM, a series of models built upon\nChatGLM, with model sizes ranging from 6B to 66B parameters. Our CharacterGLM\nis designed for generating Character-based Dialogues (CharacterDial), which\naims to equip a conversational AI system with character customization for\nsatisfying people's inherent social desires and emotional needs. On top of\nCharacterGLM, we can customize various AI characters or social agents by\nconfiguring their attributes (identities, interests, viewpoints, experiences,\nachievements, social relationships, etc.) and behaviors (linguistic features,\nemotional expressions, interaction patterns, etc.). Our model outperforms most\nmainstream close-source large langauge models, including the GPT series,\nespecially in terms of consistency, human-likeness, and engagement according to\nmanual evaluations. We will release our 6B version of CharacterGLM and a subset\nof training data to facilitate further research development in the direction of\ncharacter-based dialogue generation.", "published": "2023-11-28 14:49:23", "link": "http://arxiv.org/abs/2311.16832v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware\n  Direct Preference Optimization", "abstract": "Multimodal large language models have made significant advancements in recent\nyears, yet they still suffer from a common issue known as the \"hallucination\nproblem\", in which the models generate textual descriptions that inaccurately\ndepict or entirely fabricate content from associated images. This paper\nintroduces a novel solution, Hallucination-Aware Direct Preference Optimization\n(HA-DPO), which reframes the hallucination problem as a preference selection\ntask. The model is trained to favor the non-hallucinating response when\npresented with two responses of the same image (one accurate and one\nhallucinatory). Furthermore, this paper proposes an efficient pipeline for\nconstructing positive~(non-hallucinatory) and negative~(hallucinatory) sample\npairs, ensuring a high-quality, style-consistent dataset for robust preference\nlearning. When applied to three mainstream multimodal models, HA-DPO\nsignificantly reduced hallucination issues and amplified the models'\ngeneralization capabilities. Notably, the MiniGPT-4 model, when enhanced with\nHA-DPO, demonstrated a substantial improvement: POPE accuracy rose from 51.13%\nto 86.13% (an absolute improvement of 35%), and the MME score surged from\n932.00 to 1326.46 (a relative improvement of 42.32%). The codes, models, and\ndatasets are made accessible at https://opendatalab.github.io/HA-DPO.", "published": "2023-11-28 14:54:37", "link": "http://arxiv.org/abs/2311.16839v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "The Claire French Dialogue Dataset", "abstract": "We present the Claire French Dialogue Dataset (CFDD), a resource created by\nmembers of LINAGORA Labs in the context of the OpenLLM France initiative. CFDD\nis a corpus containing roughly 160 million words from transcripts and stage\nplays in French that we have assembled and publicly released in an effort to\nfurther the development of multilingual, open source language models. This\npaper describes the 24 individual corpora of which CFDD is composed and\nprovides links and citations to their original sources. It also provides our\nproposed breakdown of the full CFDD dataset into eight categories of subcorpora\nand describes the process we followed to standardize the format of the final\ndataset. We conclude with a discussion of similar work and future directions.", "published": "2023-11-28 14:55:22", "link": "http://arxiv.org/abs/2311.16840v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RELIC: Investigating Large Language Model Responses using\n  Self-Consistency", "abstract": "Large Language Models (LLMs) are notorious for blending fact with fiction and\ngenerating non-factual content, known as hallucinations. To address this\nchallenge, we propose an interactive system that helps users gain insight into\nthe reliability of the generated text. Our approach is based on the idea that\nthe self-consistency of multiple samples generated by the same LLM relates to\nits confidence in individual claims in the generated texts. Using this idea, we\ndesign RELIC, an interactive system that enables users to investigate and\nverify semantic-level variations in multiple long-form responses. This allows\nusers to recognize potentially inaccurate information in the generated text and\nmake necessary corrections. From a user study with ten participants, we\ndemonstrate that our approach helps users better verify the reliability of the\ngenerated text. We further summarize the design implications and lessons\nlearned from this research for future studies of reliable human-LLM\ninteractions.", "published": "2023-11-28 14:55:52", "link": "http://arxiv.org/abs/2311.16842v2", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "The Falcon Series of Open Language Models", "abstract": "We introduce the Falcon series: 7B, 40B, and 180B parameters causal\ndecoder-only models trained on a diverse high-quality corpora predominantly\nassembled from web data. The largest model, Falcon-180B, has been trained on\nover 3.5 trillion tokens of text--the largest openly documented pretraining\nrun. Falcon-180B significantly outperforms models such as PaLM or Chinchilla,\nand improves upon concurrently developed models such as LLaMA 2 or\nInflection-1. It nears the performance of PaLM-2-Large at a reduced pretraining\nand inference cost, making it, to our knowledge, one of the three best language\nmodels in the world along with GPT-4 and PaLM-2-Large. We report detailed\nevaluations, as well as a deep dive into the methods and custom tooling\nemployed to pretrain Falcon. Notably, we report on our custom distributed\ntraining codebase, allowing us to efficiently pretrain these models on up to\n4,096 A100s on cloud AWS infrastructure with limited interconnect. We release a\n600B tokens extract of our web dataset, as well as the Falcon-7/40/180B models\nunder a permissive license to foster open-science and accelerate the\ndevelopment of an open ecosystem of large language models.", "published": "2023-11-28 15:12:47", "link": "http://arxiv.org/abs/2311.16867v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models", "abstract": "In this work, we present a novel method to tackle the token generation\nchallenge in Vision Language Models (VLMs) for video and image understanding,\ncalled LLaMA-VID. Current VLMs, while proficient in tasks like image captioning\nand visual question answering, face computational burdens when processing long\nvideos due to the excessive visual tokens. LLaMA-VID addresses this issue by\nrepresenting each frame with two distinct tokens, namely context token and\ncontent token. The context token encodes the overall image context based on\nuser input, whereas the content token encapsulates visual cues in each frame.\nThis dual-token strategy significantly reduces the overload of long videos\nwhile preserving critical information. Generally, LLaMA-VID empowers existing\nframeworks to support hour-long videos and pushes their upper limit with an\nextra context token. It is proved to surpass previous methods on most of video-\nor image-based benchmarks. Code is available\nhttps://github.com/dvlab-research/LLaMA-VID}{https://github.com/dvlab-research/LLaMA-VID", "published": "2023-11-28 18:53:43", "link": "http://arxiv.org/abs/2311.17043v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "PEA-Diffusion: Parameter-Efficient Adapter with Knowledge Distillation\n  in non-English Text-to-Image Generation", "abstract": "Text-to-image diffusion models are well-known for their ability to generate\nrealistic images based on textual prompts. However, the existing works have\npredominantly focused on English, lacking support for non-English text-to-image\nmodels. The most commonly used translation methods cannot solve the generation\nproblem related to language culture, while training from scratch on a specific\nlanguage dataset is prohibitively expensive. In this paper, we are inspired to\npropose a simple plug-and-play language transfer method based on knowledge\ndistillation. All we need to do is train a lightweight MLP-like\nparameter-efficient adapter (PEA) with only 6M parameters under teacher\nknowledge distillation along with a small parallel data corpus. We are\nsurprised to find that freezing the parameters of UNet can still achieve\nremarkable performance on the language-specific prompt evaluation set,\ndemonstrating that PEA can stimulate the potential generation ability of the\noriginal UNet. Additionally, it closely approaches the performance of the\nEnglish text-to-image model on a general prompt evaluation set. Furthermore,\nour adapter can be used as a plugin to achieve significant results in\ndownstream tasks in cross-lingual text-to-image generation. Code will be\navailable at: https://github.com/OPPO-Mente-Lab/PEA-Diffusion", "published": "2023-11-28 02:31:52", "link": "http://arxiv.org/abs/2311.17086v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Reason out Your Layout: Evoking the Layout Master from Large Language\n  Models for Text-to-Image Synthesis", "abstract": "Recent advancements in text-to-image (T2I) generative models have shown\nremarkable capabilities in producing diverse and imaginative visuals based on\ntext prompts. Despite the advancement, these diffusion models sometimes\nstruggle to translate the semantic content from the text into images entirely.\nWhile conditioning on the layout has shown to be effective in improving the\ncompositional ability of T2I diffusion models, they typically require manual\nlayout input. In this work, we introduce a novel approach to improving T2I\ndiffusion models using Large Language Models (LLMs) as layout generators. Our\nmethod leverages the Chain-of-Thought prompting of LLMs to interpret text and\ngenerate spatially reasonable object layouts. The generated layout is then used\nto enhance the generated images' composition and spatial accuracy. Moreover, we\npropose an efficient adapter based on a cross-attention mechanism, which\nexplicitly integrates the layout information into the stable diffusion models.\nOur experiments demonstrate significant improvements in image quality and\nlayout accuracy, showcasing the potential of LLMs in augmenting generative\nimage models.", "published": "2023-11-28 14:51:13", "link": "http://arxiv.org/abs/2311.17126v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "General-Purpose vs. Domain-Adapted Large Language Models for Extraction\n  of Structured Data from Chest Radiology Reports", "abstract": "Radiologists produce unstructured data that can be valuable for clinical care\nwhen consumed by information systems. However, variability in style limits\nusage. Study compares system using domain-adapted language model (RadLing) and\ngeneral-purpose LLM (GPT-4) in extracting relevant features from chest\nradiology reports and standardizing them to common data elements (CDEs). Three\nradiologists annotated a retrospective dataset of 1399 chest XR reports (900\ntraining, 499 test) and mapped to 44 pre-selected relevant CDEs. GPT-4 system\nwas prompted with report, feature set, value set, and dynamic few-shots to\nextract values and map to CDEs. Output key:value pairs were compared to\nreference standard at both stages and an identical match was considered TP. F1\nscore for extraction was 97% for RadLing-based system and 78% for GPT-4 system.\nF1 score for mapping was 98% for RadLing and 94% for GPT-4; difference was\nstatistically significant (P<.001). RadLing's domain-adapted embeddings were\nbetter in feature extraction and its light-weight mapper had better f1 score in\nCDE assignment. RadLing system also demonstrated higher capabilities in\ndifferentiating between absent (99% vs 64%) and unspecified (99% vs 89%).\nRadLing system's domain-adapted embeddings helped improve performance of GPT-4\nsystem to 92% by giving more relevant few-shot prompts. RadLing system offers\noperational advantages including local deployment and reduced runtime costs.", "published": "2023-11-28 20:34:40", "link": "http://arxiv.org/abs/2311.17213v3", "categories": ["cs.CL", "eess.IV"], "primary_category": "cs.CL"}
{"title": "Does VLN Pretraining Work with Nonsensical or Irrelevant Instructions?", "abstract": "Data augmentation via back-translation is common when pretraining\nVision-and-Language Navigation (VLN) models, even though the generated\ninstructions are noisy. But: does that noise matter? We find that nonsensical\nor irrelevant language instructions during pretraining can have little effect\non downstream performance for both HAMT and VLN-BERT on R2R, and is still\nbetter than only using clean, human data. To underscore these results, we\nconcoct an efficient augmentation method, Unigram + Object, which generates\nnonsensical instructions that nonetheless improve downstream performance. Our\nfindings suggest that what matters for VLN R2R pretraining is the quantity of\nvisual trajectories, not the quality of instructions.", "published": "2023-11-28 23:40:13", "link": "http://arxiv.org/abs/2311.17280v4", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs", "abstract": "Graphs can inherently model interconnected objects on the Web, thereby\nfacilitating a series of Web applications, such as web analyzing and content\nrecommendation. Recently, Graph Neural Networks (GNNs) have emerged as a\nmainstream technique for graph representation learning. However, their efficacy\nwithin an end-to-end supervised framework is significantly tied to the\navailabilityof task-specific labels. To mitigate labeling costs and enhance\nrobustness in few-shot settings, pre-training on self-supervised tasks has\nemerged as a promising method, while prompting has been proposed to further\nnarrow the objective gap between pretext and downstream tasks. Although there\nhas been some initial exploration of prompt-based learning on graphs, they\nprimarily leverage a single pretext task, resulting in a limited subset of\ngeneral knowledge that could be learned from the pre-training data. Hence, in\nthis paper, we propose MultiGPrompt, a novel multi-task pre-training and\nprompting framework to exploit multiple pretext tasks for more comprehensive\npre-trained knowledge. First, in pre-training, we design a set of pretext\ntokens to synergize multiple pretext tasks. Second, we propose a dual-prompt\nmechanism consisting of composed and open prompts to leverage task-specific and\nglobal pre-training knowledge, to guide downstream tasks in few-shot settings.\nFinally, we conduct extensive experiments on six public datasets to evaluate\nand analyze MultiGPrompt.", "published": "2023-11-28 02:36:53", "link": "http://arxiv.org/abs/2312.03731v7", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA", "abstract": "As large language models (LLMs) have become increasingly compute and memory\nintensive, parameter-efficient fine-tuning (PEFT) methods are now a common\nstrategy to fine-tune LLMs. A popular PEFT method is Low-Rank Adapters (LoRA),\nwhich adds trainable low-rank \"adapters\" to selected layers. Each adapter\nconsists of a low-rank matrix product, multiplicatively scaled by a\nrank-dependent factor. This scaling factor, which divides adapters by a factor\nof the rank, results in slowed learning and stunted performance for LoRA with\nhigher-rank adapters. Consequently, the use of LoRA in practice has generally\nbeen limited to very low ranks. In this work, we study the impact of the\nscaling factor on the learning process and prove that LoRA adapters should be\ndivided by a factor of the square root of the rank. Modifying LoRA with the\nappropriate scaling factor, which we call the rank-stabilized LoRA (rsLoRA)\nmethod, easily provides for a fine-tuning compute/performance trade-off, where\nlarger ranks can be used to trade off increased computational resources during\ntraining for better fine-tuning performance, with no change in inference\ncomputing cost.", "published": "2023-11-28 03:23:20", "link": "http://arxiv.org/abs/2312.03732v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Methods to Estimate Large Language Model Confidence", "abstract": "Large Language Models have difficulty communicating uncertainty, which is a\nsignificant obstacle to applying LLMs to complex medical tasks. This study\nevaluates methods to measure LLM confidence when suggesting a diagnosis for\nchallenging clinical vignettes. GPT4 was asked a series of challenging case\nquestions using Chain of Thought and Self Consistency prompting. Multiple\nmethods were investigated to assess model confidence and evaluated on their\nability to predict the models observed accuracy. The methods evaluated were\nIntrinsic Confidence, SC Agreement Frequency and CoT Response Length. SC\nAgreement Frequency correlated with observed accuracy, yielding a higher Area\nunder the Receiver Operating Characteristic Curve compared to Intrinsic\nConfidence and CoT Length analysis. SC agreement is the most useful proxy for\nmodel confidence, especially for medical diagnosis. Model Intrinsic Confidence\nand CoT Response Length exhibit a weaker ability to differentiate between\ncorrect and incorrect answers, preventing them from being reliable and\ninterpretable markers for model confidence. We conclude GPT4 has a limited\nability to assess its own diagnostic accuracy. SC Agreement Frequency is the\nmost useful method to measure GPT4 confidence.", "published": "2023-11-28 05:44:06", "link": "http://arxiv.org/abs/2312.03733v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Conditional Prompt Tuning for Multimodal Fusion", "abstract": "We show that the representation of one modality can effectively guide the\nprompting of another modality for parameter-efficient multimodal fusion.\nSpecifically, we first encode one modality and use its representation as a\nprior to conditionally prompt all frozen layers of the other modality. This is\nachieved by disentangling the vanilla prompt vectors into three types of\nspecialized prompts that adaptively capture global-level and instance-level\nfeatures. To better produce the instance-wise prompt, we introduce the mixture\nof prompt experts (MoPE) to dynamically route each instance to the most\nsuitable prompt experts for encoding. We further study a regularization term to\navoid degenerated prompt expert routing. Thanks to our design, our method can\neffectively transfer the pretrained knowledge in unimodal encoders for\ndownstream multimodal tasks. Compared with vanilla prompting, we show that our\nMoPE-based conditional prompting is more expressive, thereby scales better with\ntraining data and the total number of prompts. We also demonstrate that our\nprompt tuning is architecture-agnostic, thereby offering high modularity.\nExtensive experiments over three multimodal datasets demonstrate\nstate-of-the-art results, matching or surpassing the performance achieved\nthrough fine-tuning, while only necessitating 0.7% of the trainable parameters.\nCode will be released: https://github.com/songrise/ConditionalPrompt.", "published": "2023-11-28 11:05:20", "link": "http://arxiv.org/abs/2312.03734v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Advancing State of the Art in Language Modeling", "abstract": "Generalization is arguably the most important goal of statistical language\nmodeling research. Publicly available benchmarks and papers published with an\nopen-source code have been critical to advancing the field. However, it is\noften very difficult, and sometimes even impossible, to reproduce the results\nfully as reported in publications. In this paper, we propose a simple framework\nthat should help advance the state of the art in language modeling in terms of\ngeneralization. We propose to publish not just the code, but also probabilities\non dev and test sets with future publications so that one can easily add the\nnew model into an ensemble. This has crucial advantages: it is much easier to\ndetermine whether a newly proposed model is actually complementary to the\ncurrent baseline. Therefore, instead of inventing new names for the old tricks,\nthe scientific community can advance faster. Finally, this approach promotes\ndiversity of ideas: one does not need to create an individual model that is the\nnew state of the art to attract attention; it will be sufficient to develop a\nnew model that learns patterns which other models do not. Thus, even a\nsuboptimal model can be found to have value. Remarkably, our approach has\nyielded new state-of-the-art results across various language modeling\nbenchmarks up to 10%.", "published": "2023-11-28 12:30:43", "link": "http://arxiv.org/abs/2312.03735v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Generic NLI approach for Classification of Sentiment Associated with\n  Therapies", "abstract": "This paper describes our system for addressing SMM4H 2023 Shared Task 2 on\n\"Classification of sentiment associated with therapies (aspect-oriented)\". In\nour work, we adopt an approach based on Natural language inference (NLI) to\nformulate this task as a sentence pair classification problem, and train\ntransformer models to predict sentiment associated with a therapy on a given\ntext. Our best model achieved 75.22\\% F1-score which was 11\\% (4\\%) more than\nthe mean (median) score of all teams' submissions.", "published": "2023-11-28 13:27:21", "link": "http://arxiv.org/abs/2312.03737v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Survey on Prompting Techniques in LLMs", "abstract": "Autoregressive Large Language Models have transformed the landscape of\nNatural Language Processing. Pre-train and prompt paradigm has replaced the\nconventional approach of pre-training and fine-tuning for many downstream NLP\ntasks. This shift has been possible largely due to LLMs and innovative\nprompting techniques. LLMs have shown great promise for a variety of downstream\ntasks owing to their vast parameters and huge datasets that they are\npre-trained on. However, in order to fully realize their potential, their\noutputs must be guided towards the desired outcomes. Prompting, in which a\nspecific input or instruction is provided to guide the LLMs toward the intended\noutput, has become a tool for achieving this goal. In this paper, we discuss\nthe various prompting techniques that have been applied to fully harness the\npower of LLMs. We present a taxonomy of existing literature on prompting\ntechniques and provide a concise survey based on this taxonomy. Further, we\nidentify some open problems in the realm of prompting in autoregressive LLMs\nwhich could serve as a direction for future research.", "published": "2023-11-28 17:56:34", "link": "http://arxiv.org/abs/2312.03740v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Adoption and Efficacy of Large Language Models: Evidence From\n  Consumer Complaints in the Financial Industry", "abstract": "Large Language Models (LLMs) are reshaping consumer decision-making,\nparticularly in communication with firms, yet our understanding of their impact\nremains limited. This research explores the effect of LLMs on consumer\ncomplaints submitted to the Consumer Financial Protection Bureau from 2015 to\n2024, documenting the adoption of LLMs for drafting complaints and evaluating\nthe likelihood of obtaining relief from financial firms. We analyzed over 1\nmillion complaints and identified a significant increase in LLM usage following\nthe release of ChatGPT. We find that LLM usage is associated with an increased\nlikelihood of obtaining relief from financial firms. To investigate this\nrelationship, we employ an instrumental variable approach to mitigate\nendogeneity concerns around LLM adoption. Although instrumental variables\nsuggest a potential causal link, they cannot fully capture all unobserved\nheterogeneity. To further establish this causal relationship, we conducted\ncontrolled experiments, which support that LLMs can enhance the clarity and\npersuasiveness of consumer narratives, thereby increasing the likelihood of\nobtaining relief. Our findings suggest that facilitating access to LLMs can\nhelp firms better understand consumer concerns and level the playing field\namong consumers. This underscores the importance of policies promoting\ntechnological accessibility, enabling all consumers to effectively voice their\nconcerns.", "published": "2023-11-28 04:07:34", "link": "http://arxiv.org/abs/2311.16466v4", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "StyleCap: Automatic Speaking-Style Captioning from Speech Based on\n  Speech and Language Self-supervised Learning Models", "abstract": "We propose StyleCap, a method to generate natural language descriptions of\nspeaking styles appearing in speech. Although most of conventional techniques\nfor para-/non-linguistic information recognition focus on the category\nclassification or the intensity estimation of pre-defined labels, they cannot\nprovide the reasoning of the recognition result in an interpretable manner.\nStyleCap is a first step towards an end-to-end method for generating\nspeaking-style prompts from speech, i.e., automatic speaking-style captioning.\nStyleCap is trained with paired data of speech and natural language\ndescriptions. We train neural networks that convert a speech representation\nvector into prefix vectors that are fed into a large language model (LLM)-based\ntext decoder. We explore an appropriate text decoder and speech feature\nrepresentation suitable for this new task. The experimental results demonstrate\nthat our StyleCap leveraging richer LLMs for the text decoder, speech\nself-supervised learning (SSL) features, and sentence rephrasing augmentation\nimproves the accuracy and diversity of generated speaking-style captions.\nSamples of speaking-style captions generated by our StyleCap are publicly\navailable.", "published": "2023-11-28 04:49:17", "link": "http://arxiv.org/abs/2311.16509v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Dynamic Fault Characteristics Evaluation in Power Grid", "abstract": "To enhance the intelligence degree in operation and maintenance, a novel\nmethod for fault detection in power grids is proposed. The proposed GNN-based\napproach first identifies fault nodes through a specialized feature extraction\nmethod coupled with a knowledge graph. By incorporating temporal data, the\nmethod leverages the status of nodes from preceding and subsequent time periods\nto help current fault detection. To validate the effectiveness of the node\nfeatures, a correlation analysis of the output features from each node was\nconducted. The results from experiments show that this method can accurately\nlocate fault nodes in simulation scenarios with a remarkable accuracy.\nAdditionally, the graph neural network based feature modeling allows for a\nqualitative examination of how faults spread across nodes, which provides\nvaluable insights for analyzing fault nodes.", "published": "2023-11-28 05:00:27", "link": "http://arxiv.org/abs/2311.16522v4", "categories": ["cs.LG", "cs.CL", "eess.SP"], "primary_category": "cs.LG"}
{"title": "LLMs for Science: Usage for Code Generation and Data Analysis", "abstract": "Large language models (LLMs) have been touted to enable increased\nproductivity in many areas of today's work life. Scientific research as an area\nof work is no exception: the potential of LLM-based tools to assist in the\ndaily work of scientists has become a highly discussed topic across\ndisciplines. However, we are only at the very onset of this subject of study.\nIt is still unclear how the potential of LLMs will materialise in research\npractice. With this study, we give first empirical evidence on the use of LLMs\nin the research process. We have investigated a set of use cases for LLM-based\ntools in scientific research, and conducted a first study to assess to which\ndegree current tools are helpful. In this paper we report specifically on use\ncases related to software engineering, such as generating application code and\ndeveloping scripts for data analytics. While we studied seemingly simple use\ncases, results across tools differ significantly. Our results highlight the\npromise of LLM-based tools in general, yet we also observe various issues,\nparticularly regarding the integrity of the output these tools provide.", "published": "2023-11-28 12:29:33", "link": "http://arxiv.org/abs/2311.16733v4", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Large Language Models Suffer From Their Own Output: An Analysis of the\n  Self-Consuming Training Loop", "abstract": "Large Language Models (LLM) are already widely used to generate content for a\nvariety of online platforms. As we are not able to safely distinguish\nLLM-generated content from human-produced content, LLM-generated content is\nused to train the next generation of LLMs, giving rise to a self-consuming\ntraining loop. From the image generation domain we know that such a\nself-consuming training loop reduces both quality and diversity of images\nfinally ending in a model collapse. However, it is unclear whether this\nalarming effect can also be observed for LLMs. Therefore, we present the first\nstudy investigating the self-consuming training loop for LLMs. Further, we\npropose a novel method based on logic expressions that allows us to\nunambiguously verify the correctness of LLM-generated content, which is\ndifficult for natural language text. We find that the self-consuming training\nloop produces correct outputs, however, the output declines in its diversity\ndepending on the proportion of the used generated data. Fresh data can slow\ndown this decline, but not stop it. Given these concerning results, we\nencourage researchers to study methods to negate this process.", "published": "2023-11-28 14:36:43", "link": "http://arxiv.org/abs/2311.16822v2", "categories": ["cs.LG", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Optimisation-Based Multi-Modal Semantic Image Editing", "abstract": "Image editing affords increased control over the aesthetics and content of\ngenerated images. Pre-existing works focus predominantly on text-based\ninstructions to achieve desired image modifications, which limit edit precision\nand accuracy. In this work, we propose an inference-time editing optimisation,\ndesigned to extend beyond textual edits to accommodate multiple editing\ninstruction types (e.g. spatial layout-based; pose, scribbles, edge maps). We\npropose to disentangle the editing task into two competing subtasks: successful\nlocal image modifications and global content consistency preservation, where\nsubtasks are guided through two dedicated loss functions. By allowing to adjust\nthe influence of each loss function, we build a flexible editing solution that\ncan be adjusted to user preferences. We evaluate our method using text, pose\nand scribble edit conditions, and highlight our ability to achieve complex\nedits, through both qualitative and quantitative experiments.", "published": "2023-11-28 15:31:11", "link": "http://arxiv.org/abs/2311.16882v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Mitigating Object Hallucinations in Large Vision-Language Models through\n  Visual Contrastive Decoding", "abstract": "Large Vision-Language Models (LVLMs) have advanced considerably, intertwining\nvisual recognition and language understanding to generate content that is not\nonly coherent but also contextually attuned. Despite their success, LVLMs still\nsuffer from the issue of object hallucinations, where models generate plausible\nyet incorrect outputs that include objects that do not exist in the images. To\nmitigate this issue, we introduce Visual Contrastive Decoding (VCD), a simple\nand training-free method that contrasts output distributions derived from\noriginal and distorted visual inputs. The proposed VCD effectively reduces the\nover-reliance on statistical bias and unimodal priors, two essential causes of\nobject hallucinations. This adjustment ensures the generated content is closely\ngrounded to visual inputs, resulting in contextually accurate outputs. Our\nexperiments show that VCD, without either additional training or the usage of\nexternal tools, significantly mitigates the object hallucination issue across\ndifferent LVLM families. Beyond mitigating object hallucinations, VCD also\nexcels in general LVLM benchmarks, highlighting its wide-ranging applicability.", "published": "2023-11-28 16:26:35", "link": "http://arxiv.org/abs/2311.16922v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Is This the Subspace You Are Looking for? An Interpretability Illusion\n  for Subspace Activation Patching", "abstract": "Mechanistic interpretability aims to understand model behaviors in terms of\nspecific, interpretable features, often hypothesized to manifest as\nlow-dimensional subspaces of activations. Specifically, recent studies have\nexplored subspace interventions (such as activation patching) as a way to\nsimultaneously manipulate model behavior and attribute the features behind it\nto given subspaces.\n  In this work, we demonstrate that these two aims diverge, potentially leading\nto an illusory sense of interpretability. Counterintuitively, even if a\nsubspace intervention makes the model's output behave as if the value of a\nfeature was changed, this effect may be achieved by activating a dormant\nparallel pathway leveraging another subspace that is causally disconnected from\nmodel outputs. We demonstrate this phenomenon in a distilled mathematical\nexample, in two real-world domains (the indirect object identification task and\nfactual recall), and present evidence for its prevalence in practice. In the\ncontext of factual recall, we further show a link to rank-1 fact editing,\nproviding a mechanistic explanation for previous work observing an\ninconsistency between fact editing performance and fact localization.\n  However, this does not imply that activation patching of subspaces is\nintrinsically unfit for interpretability. To contextualize our findings, we\nalso show what a success case looks like in a task (indirect object\nidentification) where prior manual circuit analysis informs an understanding of\nthe location of a feature. We explore the additional evidence needed to argue\nthat a patched subspace is faithful.", "published": "2023-11-28 18:32:19", "link": "http://arxiv.org/abs/2311.17030v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Scalable Extraction of Training Data from (Production) Language Models", "abstract": "This paper studies extractable memorization: training data that an adversary\ncan efficiently extract by querying a machine learning model without prior\nknowledge of the training dataset. We show an adversary can extract gigabytes\nof training data from open-source language models like Pythia or GPT-Neo,\nsemi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing\ntechniques from the literature suffice to attack unaligned models; in order to\nattack the aligned ChatGPT, we develop a new divergence attack that causes the\nmodel to diverge from its chatbot-style generations and emit training data at a\nrate 150x higher than when behaving properly. Our methods show practical\nattacks can recover far more data than previously thought, and reveal that\ncurrent alignment techniques do not eliminate memorization.", "published": "2023-11-28 18:47:03", "link": "http://arxiv.org/abs/2311.17035v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Eliciting In-Context Learning in Vision-Language Models for Videos\n  Through Curated Data Distributional Properties", "abstract": "A major reason behind the recent success of large language models (LLMs) is\ntheir \\textit{in-context learning} capability, which makes it possible to\nrapidly adapt them to downstream text-based tasks by prompting them with a\nsmall number of relevant demonstrations. While large vision-language models\n(VLMs) have recently been developed for tasks requiring both text and images,\nthey largely lack in-context learning over visual information, especially in\nunderstanding and generating text about videos. In this work, we implement\n\\textbf{E}mergent \\textbf{I}n-context \\textbf{Le}arning on \\textbf{V}ideos\n(\\eilev{}), a novel training paradigm that induces in-context learning over\nvideo and text by capturing key properties of pre-training data found by prior\nwork to be essential for in-context learning in transformers. In our\nexperiments, we show that \\eilev-trained models outperform other off-the-shelf\nVLMs in few-shot video narration for novel, rare actions. Furthermore, we\ndemonstrate that these key properties of bursty distributions, skewed marginal\ndistributions, and dynamic meaning each contribute to varying degrees to VLMs'\nin-context learning capability in narrating procedural videos. Our results,\nanalysis, and \\eilev{}-trained models yield numerous insights about the\nemergence of in-context learning over video and text, creating a foundation for\nfuture work to optimize and scale VLMs for open-domain video understanding and\nreasoning. Our code and demo are available at\n\\url{https://github.com/yukw777/EILEV}.", "published": "2023-11-28 18:53:06", "link": "http://arxiv.org/abs/2311.17041v4", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced\n  Training", "abstract": "Contrastive pretraining of image-text foundation models, such as CLIP,\ndemonstrated excellent zero-shot performance and improved robustness on a wide\nrange of downstream tasks. However, these models utilize large\ntransformer-based encoders with significant memory and latency overhead which\npose challenges for deployment on mobile devices. In this work, we introduce\nMobileCLIP -- a new family of efficient image-text models optimized for runtime\nperformance along with a novel and efficient training approach, namely\nmulti-modal reinforced training. The proposed training approach leverages\nknowledge transfer from an image captioning model and an ensemble of strong\nCLIP encoders to improve the accuracy of efficient models. Our approach avoids\ntrain-time compute overhead by storing the additional knowledge in a reinforced\ndataset. MobileCLIP sets a new state-of-the-art latency-accuracy tradeoff for\nzero-shot classification and retrieval tasks on several datasets. Our\nMobileCLIP-S2 variant is 2.3$\\times$ faster while more accurate compared to\nprevious best CLIP model based on ViT-B/16. We further demonstrate the\neffectiveness of our multi-modal reinforced training by training a CLIP model\nbased on ViT-B/16 image backbone and achieving +2.9% average performance\nimprovement on 38 evaluation benchmarks compared to the previous best.\nMoreover, we show that the proposed approach achieves 10$\\times$-1000$\\times$\nimproved learning efficiency when compared with non-reinforced CLIP training.\nCode and models are available at https://github.com/apple/ml-mobileclip .", "published": "2023-11-28 18:55:42", "link": "http://arxiv.org/abs/2311.17049v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "UniIR: Training and Benchmarking Universal Multimodal Information\n  Retrievers", "abstract": "Existing information retrieval (IR) models often assume a homogeneous format,\nlimiting their applicability to diverse user needs, such as searching for\nimages with text descriptions, searching for a news article with a headline\nimage, or finding a similar photo with a query image. To approach such\ndifferent information-seeking demands, we introduce UniIR, a unified\ninstruction-guided multimodal retriever capable of handling eight distinct\nretrieval tasks across modalities. UniIR, a single retrieval system jointly\ntrained on ten diverse multimodal-IR datasets, interprets user instructions to\nexecute various retrieval tasks, demonstrating robust performance across\nexisting datasets and zero-shot generalization to new tasks. Our experiments\nhighlight that multi-task training and instruction tuning are keys to UniIR's\ngeneralization ability. Additionally, we construct the M-BEIR, a multimodal\nretrieval benchmark with comprehensive results, to standardize the evaluation\nof universal multimodal information retrieval.", "published": "2023-11-28 18:55:52", "link": "http://arxiv.org/abs/2311.17136v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.CV"}
{"title": "Pragmatic Radiology Report Generation", "abstract": "When pneumonia is not found on a chest X-ray, should the report describe this\nnegative observation or omit it? We argue that this question cannot be answered\nfrom the X-ray alone and requires a pragmatic perspective, which captures the\ncommunicative goal that radiology reports serve between radiologists and\npatients. However, the standard image-to-text formulation for radiology report\ngeneration fails to incorporate such pragmatic intents. Following this\npragmatic perspective, we demonstrate that the indication, which describes why\na patient comes for an X-ray, drives the mentions of negative observations and\nintroduce indications as additional input to report generation. With respect to\nthe output, we develop a framework to identify uninferable information from the\nimage as a source of model hallucinations, and limit them by cleaning\ngroundtruth reports. Finally, we use indications and cleaned groundtruth\nreports to develop pragmatic models, and show that they outperform existing\nmethods not only in new pragmatics-inspired metrics (+4.3 Negative F1) but also\nin standard metrics (+6.3 Positive F1 and +11.0 BLEU-2).", "published": "2023-11-28 19:00:03", "link": "http://arxiv.org/abs/2311.17154v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "War and Peace (WarAgent): Large Language Model-based Multi-Agent\n  Simulation of World Wars", "abstract": "Can we avoid wars at the crossroads of history? This question has been\npursued by individuals, scholars, policymakers, and organizations throughout\nhuman history. In this research, we attempt to answer the question based on the\nrecent advances of Artificial Intelligence (AI) and Large Language Models\n(LLMs). We propose \\textbf{WarAgent}, an LLM-powered multi-agent AI system, to\nsimulate the participating countries, their decisions, and the consequences, in\nhistorical international conflicts, including the World War I (WWI), the World\nWar II (WWII), and the Warring States Period (WSP) in Ancient China. By\nevaluating the simulation effectiveness, we examine the advancements and\nlimitations of cutting-edge AI systems' abilities in studying complex\ncollective human behaviors such as international conflicts under diverse\nsettings. In these simulations, the emergent interactions among agents also\noffer a novel perspective for examining the triggers and conditions that lead\nto war. Our findings offer data-driven and AI-augmented insights that can\nredefine how we approach conflict resolution and peacekeeping strategies. The\nimplications stretch beyond historical analysis, offering a blueprint for using\nAI to understand human history and possibly prevent future international\nconflicts. Code and data are available at\n\\url{https://github.com/agiresearch/WarAgent}.", "published": "2023-11-28 20:59:49", "link": "http://arxiv.org/abs/2311.17227v2", "categories": ["cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.AI"}
{"title": "Training Chain-of-Thought via Latent-Variable Inference", "abstract": "Large language models (LLMs) solve problems more accurately and interpretably\nwhen instructed to work out the answer step by step using a\n``chain-of-thought'' (CoT) prompt. One can also improve LLMs' performance on a\nspecific task by supervised fine-tuning, i.e., by using gradient ascent on some\ntunable parameters to maximize the average log-likelihood of correct answers\nfrom a labeled training set. Naively combining CoT with supervised tuning\nrequires supervision not just of the correct answers, but also of detailed\nrationales that lead to those answers; these rationales are expensive to\nproduce by hand. Instead, we propose a fine-tuning strategy that tries to\nmaximize the \\emph{marginal} log-likelihood of generating a correct answer\nusing CoT prompting, approximately averaging over all possible rationales. The\ncore challenge is sampling from the posterior over rationales conditioned on\nthe correct answer; we address it using a simple Markov-chain Monte Carlo\n(MCMC) expectation-maximization (EM) algorithm inspired by the self-taught\nreasoner (STaR), memoized wake-sleep, Markovian score climbing, and persistent\ncontrastive divergence. This algorithm also admits a novel control-variate\ntechnique that drives the variance of our gradient estimates to zero as the\nmodel improves. Applying our technique to GSM8K and the tasks in BIG-Bench\nHard, we find that this MCMC-EM fine-tuning technique typically improves the\nmodel's accuracy on held-out examples more than STaR or prompt-tuning with or\nwithout CoT.", "published": "2023-11-28 17:47:32", "link": "http://arxiv.org/abs/2312.02179v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Comparing Generative Chatbots Based on Process Requirements", "abstract": "Business processes are commonly represented by modelling languages, such as\nEvent-driven Process Chain (EPC), Yet Another Workflow Language (YAWL), and the\nmost popular standard notation for modelling business processes, the Business\nProcess Model and Notation (BPMN). Most recently, chatbots, programs that allow\nusers to interact with a machine using natural language, have been increasingly\nused for business process execution support. A recent category of chatbots\nworth mentioning is generative-based chatbots, powered by Large Language Models\n(LLMs) such as OpenAI's Generative Pre-Trained Transformer (GPT) model and\nGoogle's Pathways Language Model (PaLM), which are trained on billions of\nparameters and support conversational intelligence. However, it is not clear\nwhether generative-based chatbots are able to understand and meet the\nrequirements of constructs such as those provided by BPMN for process execution\nsupport. This paper presents a case study to compare the performance of\nprominent generative models, GPT and PaLM, in the context of process execution\nsupport. The research sheds light into the challenging problem of using\nconversational approaches supported by generative chatbots as a means to\nunderstand process-aware modelling notations and support users to execute their\ntasks.", "published": "2023-11-28 18:25:22", "link": "http://arxiv.org/abs/2312.03741v1", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Debiasing Multimodal Models via Causal Information Minimization", "abstract": "Most existing debiasing methods for multimodal models, including causal\nintervention and inference methods, utilize approximate heuristics to represent\nthe biases, such as shallow features from early stages of training or unimodal\nfeatures for multimodal tasks like VQA, etc., which may not be accurate. In\nthis paper, we study bias arising from confounders in a causal graph for\nmultimodal data and examine a novel approach that leverages causally-motivated\ninformation minimization to learn the confounder representations. Robust\npredictive features contain diverse information that helps a model generalize\nto out-of-distribution data. Hence, minimizing the information content of\nfeatures obtained from a pretrained biased model helps learn the simplest\npredictive features that capture the underlying data distribution. We treat\nthese features as confounder representations and use them via methods motivated\nby causal theory to remove bias from models. We find that the learned\nconfounder representations indeed capture dataset biases, and the proposed\ndebiasing methods improve out-of-distribution (OOD) performance on multiple\nmultimodal datasets without sacrificing in-distribution performance.\nAdditionally, we introduce a novel metric to quantify the sufficiency of\nspurious features in models' predictions that further demonstrates the\neffectiveness of our proposed methods. Our code is available at:\nhttps://github.com/Vaidehi99/CausalInfoMin", "published": "2023-11-28 16:46:14", "link": "http://arxiv.org/abs/2311.16941v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "stat.ME"], "primary_category": "cs.LG"}
{"title": "ClimateX: Do LLMs Accurately Assess Human Expert Confidence in Climate\n  Statements?", "abstract": "Evaluating the accuracy of outputs generated by Large Language Models (LLMs)\nis especially important in the climate science and policy domain. We introduce\nthe Expert Confidence in Climate Statements (ClimateX) dataset, a novel,\ncurated, expert-labeled dataset consisting of 8094 climate statements collected\nfrom the latest Intergovernmental Panel on Climate Change (IPCC) reports,\nlabeled with their associated confidence levels. Using this dataset, we show\nthat recent LLMs can classify human expert confidence in climate-related\nstatements, especially in a few-shot learning setting, but with limited (up to\n47%) accuracy. Overall, models exhibit consistent and significant\nover-confidence on low and medium confidence statements. We highlight\nimplications of our results for climate communication, LLMs evaluation\nstrategies, and the use of LLMs in information retrieval systems.", "published": "2023-11-28 10:26:57", "link": "http://arxiv.org/abs/2311.17107v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Quantifying the redundancy between prosody and text", "abstract": "Prosody -- the suprasegmental component of speech, including pitch, loudness,\nand tempo -- carries critical aspects of meaning. However, the relationship\nbetween the information conveyed by prosody vs. by the words themselves remains\npoorly understood. We use large language models (LLMs) to estimate how much\ninformation is redundant between prosody and the words themselves. Using a\nlarge spoken corpus of English audiobooks, we extract prosodic features aligned\nto individual words and test how well they can be predicted from LLM\nembeddings, compared to non-contextual word embeddings. We find a high degree\nof redundancy between the information carried by the words and prosodic\ninformation across several prosodic features, including intensity, duration,\npauses, and pitch contours. Furthermore, a word's prosodic information is\nredundant with both the word itself and the context preceding as well as\nfollowing it. Still, we observe that prosodic features can not be fully\npredicted from text, suggesting that prosody carries information above and\nbeyond the words. Along with this paper, we release a general-purpose data\nprocessing pipeline for quantifying the relationship between linguistic\ninformation and extra-linguistic features.", "published": "2023-11-28 21:15:24", "link": "http://arxiv.org/abs/2311.17233v1", "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.LG", "math.IT"], "primary_category": "cs.CL"}
{"title": "De-identification of clinical free text using natural language\n  processing: A systematic review of current approaches", "abstract": "Background: Electronic health records (EHRs) are a valuable resource for\ndata-driven medical research. However, the presence of protected health\ninformation (PHI) makes EHRs unsuitable to be shared for research purposes.\nDe-identification, i.e. the process of removing PHI is a critical step in\nmaking EHR data accessible. Natural language processing has repeatedly\ndemonstrated its feasibility in automating the de-identification process.\nObjectives: Our study aims to provide systematic evidence on how the\nde-identification of clinical free text has evolved in the last thirteen years,\nand to report on the performances and limitations of the current\nstate-of-the-art systems. In addition, we aim to identify challenges and\npotential research opportunities in this field. Methods: A systematic search in\nPubMed, Web of Science and the DBLP was conducted for studies published between\nJanuary 2010 and February 2023. Titles and abstracts were examined to identify\nthe relevant studies. Selected studies were then analysed in-depth, and\ninformation was collected on de-identification methodologies, data sources, and\nmeasured performance. Results: A total of 2125 publications were identified for\nthe title and abstract screening. 69 studies were found to be relevant. Machine\nlearning (37 studies) and hybrid (26 studies) approaches are predominant, while\nsix studies relied only on rules. Majority of the approaches were trained and\nevaluated on public corpora. The 2014 i2b2/UTHealth corpus is the most\nfrequently used (36 studies), followed by the 2006 i2b2 (18 studies) and 2016\nCEGS N-GRID (10 studies) corpora.", "published": "2023-11-28 13:20:41", "link": "http://arxiv.org/abs/2312.03736v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.DL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Study of speaker localization under dynamic and reverberant environments", "abstract": "Speaker localization in a reverberant environment is a fundamental problem in\naudio signal processing. Many solutions have been developed to tackle this\nproblem. However, previous algorithms typically assume a stationary environment\nin which both the microphone array and the sound sources are not moving. With\nthe emergence of wearable microphone arrays, acoustic scenes have become\ndynamic with moving sources and arrays. This calls for algorithms that perform\nwell in dynamic environments. In this article, we study the performance of a\nspeaker localization algorithm in such an environment. The study is based on\nthe recently published EasyCom speech dataset recorded in reverberant and noisy\nenvironments using a wearable array on glasses. Although the localization\nalgorithm performs well in static environments, its performance degraded\nsubstantially when used on the EasyCom dataset. The paper presents performance\nanalysis and proposes methods for improvement.", "published": "2023-11-28 16:31:36", "link": "http://arxiv.org/abs/2311.16927v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "LC4SV: A Denoising Framework Learning to Compensate for Unseen Speaker\n  Verification Models", "abstract": "The performance of speaker verification (SV) models may drop dramatically in\nnoisy environments. A speech enhancement (SE) module can be used as a front-end\nstrategy. However, existing SE methods may fail to bring performance\nimprovements to downstream SV systems due to artifacts in the predicted signals\nof SE models. To compensate for artifacts, we propose a generic denoising\nframework named LC4SV, which can serve as a pre-processor for various unknown\ndownstream SV models. In LC4SV, we employ a learning-based interpolation agent\nto automatically generate the appropriate coefficients between the enhanced\nsignal and its noisy input to improve SV performance in noisy environments. Our\nexperimental results demonstrate that LC4SV consistently improves the\nperformance of various unseen SV systems. To the best of our knowledge, this\nwork is the first attempt to develop a learning-based interpolation scheme\naiming at improving SV performance in noisy environments.", "published": "2023-11-28 08:44:04", "link": "http://arxiv.org/abs/2311.16604v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "iMagLS: Interaural Level Difference with Magnitude Least-Squares Loss\n  for Optimized First-Order Head-Related Transfer Function", "abstract": "Binaural reproduction for headphone-based listening is an active research\narea due to its widespread use in evolving technologies such as augmented and\nvirtual reality (AR and VR). On the one hand, these applications demand high\nquality spatial audio perception to preserve the sense of immersion. On the\nother hand, recording devices may only have a few microphones, leading to\nlow-order representations such as first-order Ambisonics (FOA). However,\nfirst-order Ambisonics leads to limited externalization and spatial resolution.\nIn this paper, a novel head-related transfer function (HRTF) preprocessing\noptimization loss is proposed, and is minimized using nonlinear programming.\nThe new method, denoted iMagLS, involves the introduction of an interaural\nlevel difference (ILD) error term to the now widely used MagLS optimization\nloss for the lateral plane angles. Results indicate that the ILD error could be\nsubstantially reduced, while the HRTF magnitude error remains similar to that\nobtained with MagLS. These results could prove beneficial to the overall\nspatial quality of first-order Ambisonics, while other reproduction methods\ncould also benefit from considering this modified loss.", "published": "2023-11-28 11:25:12", "link": "http://arxiv.org/abs/2311.16702v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DiffusionTalker: Personalization and Acceleration for Speech-Driven 3D\n  Face Diffuser", "abstract": "Speech-driven 3D facial animation has been an attractive task in both\nacademia and industry. Traditional methods mostly focus on learning a\ndeterministic mapping from speech to animation. Recent approaches start to\nconsider the non-deterministic fact of speech-driven 3D face animation and\nemploy the diffusion model for the task. However, personalizing facial\nanimation and accelerating animation generation are still two major limitations\nof existing diffusion-based methods. To address the above limitations, we\npropose DiffusionTalker, a diffusion-based method that utilizes contrastive\nlearning to personalize 3D facial animation and knowledge distillation to\naccelerate 3D animation generation. Specifically, to enable personalization, we\nintroduce a learnable talking identity to aggregate knowledge in audio\nsequences. The proposed identity embeddings extract customized facial cues\nacross different people in a contrastive learning manner. During inference,\nusers can obtain personalized facial animation based on input audio, reflecting\na specific talking style. With a trained diffusion model with hundreds of\nsteps, we distill it into a lightweight model with 8 steps for acceleration.\nExtensive experiments are conducted to demonstrate that our method outperforms\nstate-of-the-art methods. The code will be released.", "published": "2023-11-28 07:13:20", "link": "http://arxiv.org/abs/2311.16565v2", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "D4AM: A General Denoising Framework for Downstream Acoustic Models", "abstract": "The performance of acoustic models degrades notably in noisy environments.\nSpeech enhancement (SE) can be used as a front-end strategy to aid automatic\nspeech recognition (ASR) systems. However, existing training objectives of SE\nmethods are not fully effective at integrating speech-text and noisy-clean\npaired data for training toward unseen ASR systems. In this study, we propose a\ngeneral denoising framework, D4AM, for various downstream acoustic models. Our\nframework fine-tunes the SE model with the backward gradient according to a\nspecific acoustic model and the corresponding classification objective. In\naddition, our method aims to consider the regression objective as an auxiliary\nloss to make the SE model generalize to other unseen acoustic models. To\njointly train an SE unit with regression and classification objectives, D4AM\nuses an adjustment scheme to directly estimate suitable weighting coefficients\nrather than undergoing a grid search process with additional training costs.\nThe adjustment scheme consists of two parts: gradient calibration and\nregression objective weighting. The experimental results show that D4AM can\nconsistently and effectively provide improvements to various unseen acoustic\nmodels and outperforms other combination setups. Specifically, when evaluated\non the Google ASR API with real noisy data completely unseen during SE\ntraining, D4AM achieves a relative WER reduction of 24.65% compared with the\ndirect feeding of noisy input. To our knowledge, this is the first work that\ndeploys an effective combination scheme of regression (denoising) and\nclassification (ASR) objectives to derive a general pre-processor applicable to\nvarious unseen ASR systems. Our code is available at\nhttps://github.com/ChangLee0903/D4AM.", "published": "2023-11-28 08:27:27", "link": "http://arxiv.org/abs/2311.16595v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Introducing STRAUSS: A flexible sonification Python package", "abstract": "We introduce STRAUSS (Sonification Tools and Resources for Analysis Using\nSound Synthesis) a modular, self-contained and flexible Python sonification\npackage, operating in a free and open source (FOSS) capacity. STRAUSS is\nintended to be a flexible tool suitable for both scientific data exploration\nand analysis as well as for producing sonifications that are suitable for\npublic outreach and artistic contexts. We explain the motivations behind\nSTRAUSS, and how these lead to our design choices. We also describe the basic\ncode structure and concepts. We then present output sonification examples,\nspecifically: (1) multiple representations of univariate data (i.e., single\ndata series) for data exploration; (2) how multi-variate data can be mapped\nonto sound to help interpret how those data variables are related and; (3) a\nfull spatial audio example for immersive Virtual Reality. We summarise,\nalluding to some of the future functionality as STRAUSS development\naccelerates.", "published": "2023-11-28 14:58:53", "link": "http://arxiv.org/abs/2311.16847v1", "categories": ["cs.SD", "astro-ph.IM", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
