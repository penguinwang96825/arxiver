{"title": "Voice based self help System: User Experience Vs Accuracy", "abstract": "In general, self help systems are being increasingly deployed by service\nbased industries because they are capable of delivering better customer service\nand increasingly the switch is to voice based self help systems because they\nprovide a natural interface for a human to interact with a machine. A speech\nbased self help system ideally needs a speech recognition engine to convert\nspoken speech to text and in addition a language processing engine to take care\nof any misrecognitions by the speech recognition engine. Any off-the-shelf\nspeech recognition engine is generally a combination of acoustic processing and\nspeech grammar. While this is the norm, we believe that ideally a speech\nrecognition application should have in addition to a speech recognition engine\na separate language processing engine to give the system better performance. In\nthis paper, we discuss ways in which the speech recognition engine and the\nlanguage processing engine can be combined to give a better user experience.", "published": "2015-04-07 07:02:38", "link": "http://arxiv.org/abs/1504.01496v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Jointly Embedding Relations and Mentions for Knowledge Population", "abstract": "This paper contributes a joint embedding model for predicting relations\nbetween a pair of entities in the scenario of relation inference. It differs\nfrom most stand-alone approaches which separately operate on either knowledge\nbases or free texts. The proposed model simultaneously learns low-dimensional\nvector representations for both triplets in knowledge repositories and the\nmentions of relations in free texts, so that we can leverage the evidence both\nresources to make more accurate predictions. We use NELL to evaluate the\nperformance of our approach, compared with cutting-edge methods. Results of\nextensive experiments show that our model achieves significant improvement on\nrelation extraction.", "published": "2015-04-07 17:44:30", "link": "http://arxiv.org/abs/1504.01683v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Margin Nearest Neighbor Embedding for Knowledge Representation", "abstract": "Traditional way of storing facts in triplets ({\\it head\\_entity, relation,\ntail\\_entity}), abbreviated as ({\\it h, r, t}), makes the knowledge intuitively\ndisplayed and easily acquired by mankind, but hardly computed or even reasoned\nby AI machines. Inspired by the success in applying {\\it Distributed\nRepresentations} to AI-related fields, recent studies expect to represent each\nentity and relation with a unique low-dimensional embedding, which is different\nfrom the symbolic and atomic framework of displaying knowledge in triplets. In\nthis way, the knowledge computing and reasoning can be essentially facilitated\nby means of a simple {\\it vector calculation}, i.e. ${\\bf h} + {\\bf r} \\approx\n{\\bf t}$. We thus contribute an effective model to learn better embeddings\nsatisfying the formula by pulling the positive tail entities ${\\bf t^{+}}$ to\nget together and close to {\\bf h} + {\\bf r} ({\\it Nearest Neighbor}), and\nsimultaneously pushing the negatives ${\\bf t^{-}}$ away from the positives\n${\\bf t^{+}}$ via keeping a {\\it Large Margin}. We also design a corresponding\nlearning algorithm to efficiently find the optimal solution based on {\\it\nStochastic Gradient Descent} in iterative fashion. Quantitative experiments\nillustrate that our approach can achieve the state-of-the-art performance,\ncompared with several latest methods on some benchmark datasets for two\nclassical applications, i.e. {\\it Link prediction} and {\\it Triplet\nclassification}. Moreover, we analyze the parameter complexities among all the\nevaluated models, and analytical results indicate that our model needs fewer\ncomputational resources on outperforming the other methods.", "published": "2015-04-07 17:50:31", "link": "http://arxiv.org/abs/1504.01684v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Deep Recurrent Neural Networks for Acoustic Modelling", "abstract": "We present a novel deep Recurrent Neural Network (RNN) model for acoustic\nmodelling in Automatic Speech Recognition (ASR). We term our contribution as a\nTC-DNN-BLSTM-DNN model, the model combines a Deep Neural Network (DNN) with\nTime Convolution (TC), followed by a Bidirectional Long Short-Term Memory\n(BLSTM), and a final DNN. The first DNN acts as a feature processor to our\nmodel, the BLSTM then generates a context from the sequence acoustic signal,\nand the final DNN takes the context and models the posterior probabilities of\nthe acoustic states. We achieve a 3.47 WER on the Wall Street Journal (WSJ)\neval92 task or more than 8% relative improvement over the baseline DNN models.", "published": "2015-04-07 06:12:14", "link": "http://arxiv.org/abs/1504.01482v1", "categories": ["cs.LG", "cs.CL", "cs.NE", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Transferring Knowledge from a RNN to a DNN", "abstract": "Deep Neural Network (DNN) acoustic models have yielded many state-of-the-art\nresults in Automatic Speech Recognition (ASR) tasks. More recently, Recurrent\nNeural Network (RNN) models have been shown to outperform DNNs counterparts.\nHowever, state-of-the-art DNN and RNN models tend to be impractical to deploy\non embedded systems with limited computational capacity. Traditionally, the\napproach for embedded platforms is to either train a small DNN directly, or to\ntrain a small DNN that learns the output distribution of a large DNN. In this\npaper, we utilize a state-of-the-art RNN to transfer knowledge to small DNN. We\nuse the RNN model to generate soft alignments and minimize the Kullback-Leibler\ndivergence against the small DNN. The small DNN trained on the soft RNN\nalignments achieved a 3.93 WER on the Wall Street Journal (WSJ) eval92 task\ncompared to a baseline 4.54 WER or more than 13% relative improvement.", "published": "2015-04-07 06:15:44", "link": "http://arxiv.org/abs/1504.01483v1", "categories": ["cs.LG", "cs.CL", "cs.NE", "stat.ML"], "primary_category": "cs.LG"}
