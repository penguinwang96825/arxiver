{"title": "Reasoning Over History: Context Aware Visual Dialog", "abstract": "While neural models have been shown to exhibit strong performance on\nsingle-turn visual question answering (VQA) tasks, extending VQA to a\nmulti-turn, conversational setting remains a challenge. One way to address this\nchallenge is to augment existing strong neural VQA models with the mechanisms\nthat allow them to retain information from previous dialog turns. One strong\nVQA model is the MAC network, which decomposes a task into a series of\nattention-based reasoning steps. However, since the MAC network is designed for\nsingle-turn question answering, it is not capable of referring to past dialog\nturns. More specifically, it struggles with tasks that require reasoning over\nthe dialog history, particularly coreference resolution. We extend the MAC\nnetwork architecture with Context-aware Attention and Memory (CAM), which\nattends over control states in past dialog turns to determine the necessary\nreasoning operations for the current question. MAC nets with CAM achieve up to\n98.25% accuracy on the CLEVR-Dialog dataset, beating the existing\nstate-of-the-art by 30% (absolute). Our error analysis indicates that with CAM,\nthe model's performance particularly improved on questions that required\ncoreference resolution.", "published": "2020-11-02 01:25:13", "link": "http://arxiv.org/abs/2011.00669v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IndoLEM and IndoBERT: A Benchmark Dataset and Pre-trained Language Model\n  for Indonesian NLP", "abstract": "Although the Indonesian language is spoken by almost 200 million people and\nthe 10th most spoken language in the world, it is under-represented in NLP\nresearch. Previous work on Indonesian has been hampered by a lack of annotated\ndatasets, a sparsity of language resources, and a lack of resource\nstandardization. In this work, we release the IndoLEM dataset comprising seven\ntasks for the Indonesian language, spanning morpho-syntax, semantics, and\ndiscourse. We additionally release IndoBERT, a new pre-trained language model\nfor Indonesian, and evaluate it over IndoLEM, in addition to benchmarking it\nagainst existing resources. Our experiments show that IndoBERT achieves\nstate-of-the-art performance over most of the tasks in IndoLEM.", "published": "2020-11-02 01:54:56", "link": "http://arxiv.org/abs/2011.00677v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Liputan6: A Large-scale Indonesian Dataset for Text Summarization", "abstract": "In this paper, we introduce a large-scale Indonesian summarization dataset.\nWe harvest articles from Liputan6.com, an online news portal, and obtain\n215,827 document-summary pairs. We leverage pre-trained language models to\ndevelop benchmark extractive and abstractive summarization methods over the\ndataset with multilingual and monolingual BERT-based models. We include a\nthorough error analysis by examining machine-generated summaries that have low\nROUGE scores, and expose both issues with ROUGE it-self, as well as with\nextractive and abstractive summarization models.", "published": "2020-11-02 02:01:12", "link": "http://arxiv.org/abs/2011.00679v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sequence-to-Sequence Networks Learn the Meaning of Reflexive Anaphora", "abstract": "Reflexive anaphora present a challenge for semantic interpretation: their\nmeaning varies depending on context in a way that appears to require abstract\nvariables. Past work has raised doubts about the ability of recurrent networks\nto meet this challenge. In this paper, we explore this question in the context\nof a fragment of English that incorporates the relevant sort of contextual\nvariability. We consider sequence-to-sequence architectures with recurrent\nunits and show that such networks are capable of learning semantic\ninterpretations for reflexive anaphora which generalize to novel antecedents.\nWe explore the effect of attention mechanisms and different recurrent unit\ntypes on the type of training data that is needed for success as measured in\ntwo ways: how much lexical support is needed to induce an abstract reflexive\nmeaning (i.e., how many distinct reflexive antecedents must occur during\ntraining) and what contexts must a noun phrase occur in to support\ngeneralization of reflexive interpretation to this noun phrase?", "published": "2020-11-02 02:06:33", "link": "http://arxiv.org/abs/2011.00682v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Domain Terminology Affects Meeting Summarization Performance", "abstract": "Meetings are essential to modern organizations. Numerous meetings are held\nand recorded daily, more than can ever be comprehended. A meeting summarization\nsystem that identifies salient utterances from the transcripts to automatically\ngenerate meeting minutes can help. It empowers users to rapidly search and sift\nthrough large meeting collections. To date, the impact of domain terminology on\nthe performance of meeting summarization remains understudied, despite that\nmeetings are rich with domain knowledge. In this paper, we create gold-standard\nannotations for domain terminology on a sizable meeting corpus; they are known\nas jargon terms. We then analyze the performance of a meeting summarization\nsystem with and without jargon terms. Our findings reveal that domain\nterminology can have a substantial impact on summarization performance. We\npublicly release all domain terminology to advance research in meeting\nsummarization.", "published": "2020-11-02 02:33:59", "link": "http://arxiv.org/abs/2011.00692v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Influence Patterns for Explaining Information Flow in BERT", "abstract": "While attention is all you need may be proving true, we do not know why:\nattention-based transformer models such as BERT are superior but how\ninformation flows from input tokens to output predictions are unclear. We\nintroduce influence patterns, abstractions of sets of paths through a\ntransformer model. Patterns quantify and localize the flow of information to\npaths passing through a sequence of model nodes. Experimentally, we find that\nsignificant portion of information flow in BERT goes through skip connections\ninstead of attention heads. We further show that consistency of patterns across\ninstances is an indicator of BERT's performance. Finally, We demonstrate that\npatterns account for far more model performance than previous attention-based\nand layer-based methods.", "published": "2020-11-02 04:28:16", "link": "http://arxiv.org/abs/2011.00740v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\u00daFAL at MRP 2020: Permutation-invariant Semantic Parsing in PERIN", "abstract": "We present PERIN, a novel permutation-invariant approach to sentence-to-graph\nsemantic parsing. PERIN is a versatile, cross-framework and language\nindependent architecture for universal modeling of semantic structures. Our\nsystem participated in the CoNLL 2020 shared task, Cross-Framework Meaning\nRepresentation Parsing (MRP 2020), where it was evaluated on five different\nframeworks (AMR, DRG, EDS, PTG and UCCA) across four languages. PERIN was one\nof the winners of the shared task. The source code and pretrained models are\navailable at https://github.com/ufal/perin.", "published": "2020-11-02 05:47:08", "link": "http://arxiv.org/abs/2011.00758v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reducing Confusion in Active Learning for Part-Of-Speech Tagging", "abstract": "Active learning (AL) uses a data selection algorithm to select useful\ntraining samples to minimize annotation cost. This is now an essential tool for\nbuilding low-resource syntactic analyzers such as part-of-speech (POS) taggers.\nExisting AL heuristics are generally designed on the principle of selecting\nuncertain yet representative training instances, where annotating these\ninstances may reduce a large number of errors. However, in an empirical study\nacross six typologically diverse languages (German, Swedish, Galician, North\nSami, Persian, and Ukrainian), we found the surprising result that even in an\noracle scenario where we know the true uncertainty of predictions, these\ncurrent heuristics are far from optimal. Based on this analysis, we pose the\nproblem of AL as selecting instances which maximally reduce the confusion\nbetween particular pairs of output tags. Extensive experimentation on the\naforementioned languages shows that our proposed AL strategy outperforms other\nAL strategies by a significant margin. We also present auxiliary results\ndemonstrating the importance of proper calibration of models, which we ensure\nthrough cross-view training, and analysis demonstrating how our proposed\nstrategy selects examples that more closely follow the oracle data\ndistribution.", "published": "2020-11-02 06:24:58", "link": "http://arxiv.org/abs/2011.00767v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-Aware Cross-Attention for Non-Autoregressive Translation", "abstract": "Non-autoregressive translation (NAT) significantly accelerates the inference\nprocess by predicting the entire target sequence. However, due to the lack of\ntarget dependency modelling in the decoder, the conditional generation process\nheavily depends on the cross-attention. In this paper, we reveal a localness\nperception problem in NAT cross-attention, for which it is difficult to\nadequately capture source context. To alleviate this problem, we propose to\nenhance signals of neighbour source tokens into conventional cross-attention.\nExperimental results on several representative datasets show that our approach\ncan consistently improve translation quality over strong NAT baselines.\nExtensive analyses demonstrate that the enhanced cross-attention achieves\nbetter exploitation of source contexts by leveraging both local and global\ninformation.", "published": "2020-11-02 06:34:33", "link": "http://arxiv.org/abs/2011.00770v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COSMO: Conditional SEQ2SEQ-based Mixture Model for Zero-Shot Commonsense\n  Question Answering", "abstract": "Commonsense reasoning refers to the ability of evaluating a social situation\nand acting accordingly. Identification of the implicit causes and effects of a\nsocial context is the driving capability which can enable machines to perform\ncommonsense reasoning. The dynamic world of social interactions requires\ncontext-dependent on-demand systems to infer such underlying information.\nHowever, current approaches in this realm lack the ability to perform\ncommonsense reasoning upon facing an unseen situation, mostly due to\nincapability of identifying a diverse range of implicit social relations. Hence\nthey fail to estimate the correct reasoning path. In this paper, we present\nConditional SEQ2SEQ-based Mixture model (COSMO), which provides us with the\ncapabilities of dynamic and diverse content generation. We use COSMO to\ngenerate context-dependent clauses, which form a dynamic Knowledge Graph (KG)\non-the-fly for commonsense reasoning. To show the adaptability of our model to\ncontext-dependant knowledge generation, we address the task of zero-shot\ncommonsense question answering. The empirical results indicate an improvement\nof up to +5.2% over the state-of-the-art models.", "published": "2020-11-02 07:08:19", "link": "http://arxiv.org/abs/2011.00777v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context Dependent Semantic Parsing: A Survey", "abstract": "Semantic parsing is the task of translating natural language utterances into\nmachine-readable meaning representations. Currently, most semantic parsing\nmethods are not able to utilize contextual information (e.g. dialogue and\ncomments history), which has a great potential to boost semantic parsing\nperformance. To address this issue, context dependent semantic parsing has\nrecently drawn a lot of attention. In this survey, we investigate progress on\nthe methods for the context dependent semantic parsing, together with the\ncurrent datasets and tasks. We then point out open problems and challenges for\nfuture research in this area. The collected resources for this topic are\navailable\nat:https://github.com/zhuang-li/Contextual-Semantic-Parsing-Paper-List.", "published": "2020-11-02 07:51:05", "link": "http://arxiv.org/abs/2011.00797v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparison by Conversion: Reverse-Engineering UCCA from Syntax and\n  Lexical Semantics", "abstract": "Building robust natural language understanding systems will require a clear\ncharacterization of whether and how various linguistic meaning representations\ncomplement each other. To perform a systematic comparative analysis, we\nevaluate the mapping between meaning representations from different frameworks\nusing two complementary methods: (i) a rule-based converter, and (ii) a\nsupervised delexicalized parser that parses to one framework using only\ninformation from the other as features. We apply these methods to convert the\nSTREUSLE corpus (with syntactic and lexical semantic annotations) to UCCA (a\ngraph-structured full-sentence meaning representation). Both methods yield\nsurprisingly accurate target representations, close to fully supervised UCCA\nparser quality---indicating that UCCA annotations are partially redundant with\nSTREUSLE annotations. Despite this substantial convergence between frameworks,\nwe find several important areas of divergence.", "published": "2020-11-02 09:03:46", "link": "http://arxiv.org/abs/2011.00834v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Far Does BERT Look At:Distance-based Clustering and Analysis of\n  BERT$'$s Attention", "abstract": "Recent research on the multi-head attention mechanism, especially that in\npre-trained models such as BERT, has shown us heuristics and clues in analyzing\nvarious aspects of the mechanism. As most of the research focus on probing\ntasks or hidden states, previous works have found some primitive patterns of\nattention head behavior by heuristic analytical methods, but a more systematic\nanalysis specific on the attention patterns still remains primitive. In this\nwork, we clearly cluster the attention heatmaps into significantly different\npatterns through unsupervised clustering on top of a set of proposed features,\nwhich corroborates with previous observations. We further study their\ncorresponding functions through analytical study. In addition, our proposed\nfeatures can be used to explain and calibrate different attention heads in\nTransformer models.", "published": "2020-11-02 12:52:31", "link": "http://arxiv.org/abs/2011.00943v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Study of Contextual Data Augmentation for Japanese Zero\n  Anaphora Resolution", "abstract": "One critical issue of zero anaphora resolution (ZAR) is the scarcity of\nlabeled data. This study explores how effectively this problem can be\nalleviated by data augmentation. We adopt a state-of-the-art data augmentation\nmethod, called the contextual data augmentation (CDA), that generates labeled\ntraining instances using a pretrained language model. The CDA has been reported\nto work well for several other natural language processing tasks, including\ntext classification and machine translation. This study addresses two\nunderexplored issues on CDA, that is, how to reduce the computational cost of\ndata augmentation and how to ensure the quality of the generated data. We also\npropose two methods to adapt CDA to ZAR: [MASK]-based augmentation and\nlinguistically-controlled masking. Consequently, the experimental results on\nJapanese ZAR show that our methods contribute to both the accuracy gain and the\ncomputation cost reduction. Our closer analysis reveals that the proposed\nmethod can improve the quality of the augmented training data when compared to\nthe conventional CDA.", "published": "2020-11-02 13:05:00", "link": "http://arxiv.org/abs/2011.00948v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Closer Look at Linguistic Knowledge in Masked Language Models: The\n  Case of Relative Clauses in American English", "abstract": "Transformer-based language models achieve high performance on various tasks,\nbut we still lack understanding of the kind of linguistic knowledge they learn\nand rely on. We evaluate three models (BERT, RoBERTa, and ALBERT), testing\ntheir grammatical and semantic knowledge by sentence-level probing, diagnostic\ncases, and masked prediction tasks. We focus on relative clauses (in American\nEnglish) as a complex phenomenon needing contextual information and antecedent\nidentification to be resolved. Based on a naturalistic dataset, probing shows\nthat all three models indeed capture linguistic knowledge about grammaticality,\nachieving high performance. Evaluation on diagnostic cases and masked\nprediction tasks considering fine-grained linguistic knowledge, however, shows\npronounced model-specific weaknesses especially on semantic knowledge, strongly\nimpacting models' performance. Our results highlight the importance of (a)model\ncomparison in evaluation task and (b) building up claims of model performance\nand the linguistic knowledge they capture beyond purely probing-based\nevaluations.", "published": "2020-11-02 13:25:39", "link": "http://arxiv.org/abs/2011.00960v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combining Event Semantics and Degree Semantics for Natural Language\n  Inference", "abstract": "In formal semantics, there are two well-developed semantic frameworks: event\nsemantics, which treats verbs and adverbial modifiers using the notion of\nevent, and degree semantics, which analyzes adjectives and comparatives using\nthe notion of degree. However, it is not obvious whether these frameworks can\nbe combined to handle cases in which the phenomena in question are interacting\nwith each other. Here, we study this issue by focusing on natural language\ninference (NLI). We implement a logic-based NLI system that combines event\nsemantics and degree semantics and their interaction with lexical knowledge. We\nevaluate the system on various NLI datasets containing linguistically\nchallenging problems. The results show that the system achieves high accuracies\non these datasets in comparison with previous logic-based systems and\ndeep-learning-based systems. This suggests that the two semantic frameworks can\nbe combined consistently to handle various combinations of linguistic phenomena\nwithout compromising the advantage of either framework.", "published": "2020-11-02 13:27:21", "link": "http://arxiv.org/abs/2011.00961v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DNN-Based Semantic Model for Rescoring N-best Speech Recognition List", "abstract": "The word error rate (WER) of an automatic speech recognition (ASR) system\nincreases when a mismatch occurs between the training and the testing\nconditions due to the noise, etc. In this case, the acoustic information can be\nless reliable. This work aims to improve ASR by modeling long-term semantic\nrelations to compensate for distorted acoustic features. We propose to perform\nthis through rescoring of the ASR N-best hypotheses list. To achieve this, we\ntrain a deep neural network (DNN). Our DNN rescoring model is aimed at\nselecting hypotheses that have better semantic consistency and therefore lower\nWER. We investigate two types of representations as part of input features to\nour DNN model: static word embeddings (from word2vec) and dynamic contextual\nembeddings (from BERT). Acoustic and linguistic features are also included. We\nperform experiments on the publicly available dataset TED-LIUM mixed with real\nnoise. The proposed rescoring approaches give significant improvement of the\nWER over the ASR system without rescoring models in two noisy conditions and\nwith n-gram and RNNLM.", "published": "2020-11-02 13:50:59", "link": "http://arxiv.org/abs/2011.00975v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Biased TextRank: Unsupervised Graph-Based Content Extraction", "abstract": "We introduce Biased TextRank, a graph-based content extraction method\ninspired by the popular TextRank algorithm that ranks text spans according to\ntheir importance for language processing tasks and according to their relevance\nto an input \"focus.\" Biased TextRank enables focused content extraction for\ntext by modifying the random restarts in the execution of TextRank. The random\nrestart probabilities are assigned based on the relevance of the graph nodes to\nthe focus of the task. We present two applications of Biased TextRank: focused\nsummarization and explanation extraction, and show that our algorithm leads to\nimproved performance on two different datasets by significant ROUGE-N score\nmargins. Much like its predecessor, Biased TextRank is unsupervised, easy to\nimplement and orders of magnitude faster and lighter than current\nstate-of-the-art Natural Language Processing methods for similar tasks.", "published": "2020-11-02 15:17:44", "link": "http://arxiv.org/abs/2011.01026v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of\n  Reasoning Steps", "abstract": "A multi-hop question answering (QA) dataset aims to test reasoning and\ninference skills by requiring a model to read multiple paragraphs to answer a\ngiven question. However, current datasets do not provide a complete explanation\nfor the reasoning process from the question to the answer. Further, previous\nstudies revealed that many examples in existing multi-hop datasets do not\nrequire multi-hop reasoning to answer a question. In this study, we present a\nnew multi-hop QA dataset, called 2WikiMultiHopQA, which uses structured and\nunstructured data. In our dataset, we introduce the evidence information\ncontaining a reasoning path for multi-hop questions. The evidence information\nhas two benefits: (i) providing a comprehensive explanation for predictions and\n(ii) evaluating the reasoning skills of a model. We carefully design a pipeline\nand a set of templates when generating a question-answer pair that guarantees\nthe multi-hop steps and the quality of the questions. We also exploit the\nstructured format in Wikidata and use logical rules to create questions that\nare natural but still require multi-hop reasoning. Through experiments, we\ndemonstrate that our dataset is challenging for multi-hop models and it ensures\nthat multi-hop reasoning is required.", "published": "2020-11-02 15:42:40", "link": "http://arxiv.org/abs/2011.01060v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enabling Zero-shot Multilingual Spoken Language Translation with\n  Language-Specific Encoders and Decoders", "abstract": "Current end-to-end approaches to Spoken Language Translation (SLT) rely on\nlimited training resources, especially for multilingual settings. On the other\nhand, Multilingual Neural Machine Translation (MultiNMT) approaches rely on\nhigher-quality and more massive data sets. Our proposed method extends a\nMultiNMT architecture based on language-specific encoders-decoders to the task\nof Multilingual SLT (MultiSLT). Our method entirely eliminates the dependency\nfrom MultiSLT data and it is able to translate while training only on ASR and\nMultiNMT data.\n  Our experiments on four different languages show that coupling the speech\nencoder to the MultiNMT architecture produces similar quality translations\ncompared to a bilingual baseline ($\\pm 0.2$ BLEU) while effectively allowing\nfor zero-shot MultiSLT. Additionally, we propose using an Adapter module for\ncoupling the speech inputs. This Adapter module produces consistent\nimprovements up to +6 BLEU points on the proposed architecture and +1 BLEU\npoint on the end-to-end baseline.", "published": "2020-11-02 16:31:14", "link": "http://arxiv.org/abs/2011.01097v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Automated Transcription of Non-Latin Script Periodicals: A Case Study in\n  the Ottoman Turkish Print Archive", "abstract": "Our study utilizes deep learning methods for the automated transcription of\nlate nineteenth- and early twentieth-century periodicals written in Arabic\nscript Ottoman Turkish (OT) using the Transkribus platform. We discuss the\nhistorical situation of OT text collections and how they were excluded for the\nmost part from the late twentieth century corpora digitization that took place\nin many Latin script languages. This exclusion has two basic reasons: the\ntechnical challenges of OCR for Arabic script languages, and the rapid\nabandonment of that very script in the Turkish historical context. In the\nspecific case of OT, opening periodical collections to digital tools require\ntraining HTR models to generate transcriptions in the Latin writing system of\ncontemporary readers of Turkish, and not, as some may expect, in right-to-left\nArabic script text. In the paper we discuss the challenges of training such\nmodels where one-to-one correspondence between the writing systems do not\nexist, and we report results based on our HTR experiments with two OT\nperiodicals from the early twentieth century. Finally, we reflect on potential\ndomain bias of HTR models in historical languages exhibiting spatio-temporal\nvariance as well as the significance of working between writing systems for\nlanguage communities that have experienced language reform and script change.", "published": "2020-11-02 17:28:36", "link": "http://arxiv.org/abs/2011.01139v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "QMUL-SDS @ SardiStance: Leveraging Network Interactions to Boost\n  Performance on Stance Detection using Knowledge Graphs", "abstract": "This paper presents our submission to the SardiStance 2020 shared task,\ndescribing the architecture used for Task A and Task B. While our submission\nfor Task A did not exceed the baseline, retraining our model using all the\ntraining tweets, showed promising results leading to (f-avg 0.601) using\nbidirectional LSTM with BERT multilingual embedding for Task A. For our\nsubmission for Task B, we ranked 6th (f-avg 0.709). With further investigation,\nour best experimented settings increased performance from (f-avg 0.573) to\n(f-avg 0.733) with same architecture and parameter settings and after only\nincorporating social interaction features -- highlighting the impact of social\ninteraction on the model's performance.", "published": "2020-11-02 18:17:51", "link": "http://arxiv.org/abs/2011.01181v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Devil is in the Details: Evaluating Limitations of Transformer-based\n  Methods for Granular Tasks", "abstract": "Contextual embeddings derived from transformer-based neural language models\nhave shown state-of-the-art performance for various tasks such as question\nanswering, sentiment analysis, and textual similarity in recent years.\nExtensive work shows how accurately such models can represent abstract,\nsemantic information present in text. In this expository work, we explore a\ntangent direction and analyze such models' performance on tasks that require a\nmore granular level of representation. We focus on the problem of textual\nsimilarity from two perspectives: matching documents on a granular level\n(requiring embeddings to capture fine-grained attributes in the text), and an\nabstract level (requiring embeddings to capture overall textual semantics). We\nempirically demonstrate, across two datasets from different domains, that\ndespite high performance in abstract document matching as expected, contextual\nembeddings are consistently (and at times, vastly) outperformed by simple\nbaselines like TF-IDF for more granular tasks. We then propose a simple but\neffective method to incorporate TF-IDF into models that use contextual\nembeddings, achieving relative improvements of up to 36% on granular tasks.", "published": "2020-11-02 18:41:32", "link": "http://arxiv.org/abs/2011.01196v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Targeted Attack on Black-Box Neural Machine Translation with Parallel\n  Data Poisoning", "abstract": "As modern neural machine translation (NMT) systems have been widely deployed,\ntheir security vulnerabilities require close scrutiny. Most recently, NMT\nsystems have been found vulnerable to targeted attacks which cause them to\nproduce specific, unsolicited, and even harmful translations. These attacks are\nusually exploited in a white-box setting, where adversarial inputs causing\ntargeted translations are discovered for a known target system. However, this\napproach is less viable when the target system is black-box and unknown to the\nadversary (e.g., secured commercial systems). In this paper, we show that\ntargeted attacks on black-box NMT systems are feasible, based on poisoning a\nsmall fraction of their parallel training data. We show that this attack can be\nrealised practically via targeted corruption of web documents crawled to form\nthe system's training data. We then analyse the effectiveness of the targeted\npoisoning in two common NMT training scenarios: the from-scratch training and\nthe pre-train & fine-tune paradigm. Our results are alarming: even on the\nstate-of-the-art systems trained with massive parallel data (tens of millions),\nthe attacks are still successful (over 50% success rate) under surprisingly low\npoisoning budgets (e.g., 0.006%). Lastly, we discuss potential defences to\ncounter such attacks.", "published": "2020-11-02 01:52:46", "link": "http://arxiv.org/abs/2011.00675v2", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Investigating Catastrophic Forgetting During Continual Training for\n  Neural Machine Translation", "abstract": "Neural machine translation (NMT) models usually suffer from catastrophic\nforgetting during continual training where the models tend to gradually forget\npreviously learned knowledge and swing to fit the newly added data which may\nhave a different distribution, e.g. a different domain. Although many methods\nhave been proposed to solve this problem, we cannot get to know what causes\nthis phenomenon yet. Under the background of domain adaptation, we investigate\nthe cause of catastrophic forgetting from the perspectives of modules and\nparameters (neurons). The investigation on the modules of the NMT model shows\nthat some modules have tight relation with the general-domain knowledge while\nsome other modules are more essential in the domain adaptation. And the\ninvestigation on the parameters shows that some parameters are important for\nboth the general-domain and in-domain translation and the great change of them\nduring continual training brings about the performance decline in\ngeneral-domain. We conduct experiments across different language pairs and\ndomains to ensure the validity and reliability of our findings.", "published": "2020-11-02 01:55:06", "link": "http://arxiv.org/abs/2011.00678v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Event-Related Bias Removal for Real-time Disaster Events", "abstract": "Social media has become an important tool to share information about crisis\nevents such as natural disasters and mass attacks. Detecting actionable posts\nthat contain useful information requires rapid analysis of huge volume of data\nin real-time. This poses a complex problem due to the large amount of posts\nthat do not contain any actionable information. Furthermore, the classification\nof information in real-time systems requires training on out-of-domain data, as\nwe do not have any data from a new emerging crisis. Prior work focuses on\nmodels pre-trained on similar event types. However, those models capture\nunnecessary event-specific biases, like the location of the event, which affect\nthe generalizability and performance of the classifiers on new unseen data from\nan emerging new event. In our work, we train an adversarial neural model to\nremove latent event-specific biases and improve the performance on tweet\nimportance classification.", "published": "2020-11-02 02:03:07", "link": "http://arxiv.org/abs/2011.00681v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ABNIRML: Analyzing the Behavior of Neural IR Models", "abstract": "Pretrained contextualized language models such as BERT and T5 have\nestablished a new state-of-the-art for ad-hoc search. However, it is not yet\nwell-understood why these methods are so effective, what makes some variants\nmore effective than others, and what pitfalls they may have. We present a new\ncomprehensive framework for Analyzing the Behavior of Neural IR ModeLs\n(ABNIRML), which includes new types of diagnostic probes that allow us to test\nseveral characteristics -- such as writing styles, factuality, sensitivity to\nparaphrasing and word order -- that are not addressed by previous techniques.\nTo demonstrate the value of the framework, we conduct an extensive empirical\nstudy that yields insights into the factors that contribute to the neural\nmodel's gains, and identify potential unintended biases the models exhibit.\nSome of our results confirm conventional wisdom, like that recent neural\nranking models rely less on exact term overlap with the query, and instead\nleverage richer linguistic information, evidenced by their higher sensitivity\nto word and sentence order. Other results are more surprising, such as that\nsome models (e.g., T5 and ColBERT) are biased towards factually correct (rather\nthan simply relevant) texts. Further, some characteristics vary even for the\nsame base language model, and other characteristics can appear due to random\nvariations during model training.", "published": "2020-11-02 03:07:38", "link": "http://arxiv.org/abs/2011.00696v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Semi-supervised Autoencoding Projective Dependency Parsing", "abstract": "We describe two end-to-end autoencoding models for semi-supervised\ngraph-based projective dependency parsing. The first model is a Locally\nAutoencoding Parser (LAP) encoding the input using continuous latent variables\nin a sequential manner; The second model is a Globally Autoencoding Parser\n(GAP) encoding the input into dependency trees as latent variables, with exact\ninference. Both models consist of two parts: an encoder enhanced by deep neural\nnetworks (DNN) that can utilize the contextual information to encode the input\ninto latent variables, and a decoder which is a generative model able to\nreconstruct the input. Both LAP and GAP admit a unified structure with\ndifferent loss functions for labeled and unlabeled data with shared parameters.\nWe conducted experiments on WSJ and UD dependency parsing data sets, showing\nthat our models can exploit the unlabeled data to improve the performance given\na limited amount of labeled data, and outperform a previously proposed\nsemi-supervised model.", "published": "2020-11-02 03:21:39", "link": "http://arxiv.org/abs/2011.00704v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "I Know What You Asked: Graph Path Learning using AMR for Commonsense\n  Reasoning", "abstract": "CommonsenseQA is a task in which a correct answer is predicted through\ncommonsense reasoning with pre-defined knowledge. Most previous works have\naimed to improve the performance with distributed representation without\nconsidering the process of predicting the answer from the semantic\nrepresentation of the question. To shed light upon the semantic interpretation\nof the question, we propose an AMR-ConceptNet-Pruned (ACP) graph. The ACP graph\nis pruned from a full integrated graph encompassing Abstract Meaning\nRepresentation (AMR) graph generated from input questions and an external\ncommonsense knowledge graph, ConceptNet (CN). Then the ACP graph is exploited\nto interpret the reasoning path as well as to predict the correct answer on the\nCommonsenseQA task. This paper presents the manner in which the commonsense\nreasoning process can be interpreted with the relations and concepts provided\nby the ACP graph. Moreover, ACP-based models are shown to outperform the\nbaselines.", "published": "2020-11-02 06:22:01", "link": "http://arxiv.org/abs/2011.00766v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adapting Pretrained Transformer to Lattices for Spoken Language\n  Understanding", "abstract": "Lattices are compact representations that encode multiple hypotheses, such as\nspeech recognition results or different word segmentations. It is shown that\nencoding lattices as opposed to 1-best results generated by automatic speech\nrecognizer (ASR) boosts the performance of spoken language understanding (SLU).\nRecently, pretrained language models with the transformer architecture have\nachieved the state-of-the-art results on natural language understanding, but\ntheir ability of encoding lattices has not been explored. Therefore, this paper\naims at adapting pretrained transformers to lattice inputs in order to perform\nunderstanding tasks specifically for spoken language. Our experiments on the\nbenchmark ATIS dataset show that fine-tuning pretrained transformers with\nlattice inputs yields clear improvement over fine-tuning with 1-best results.\nFurther evaluation demonstrates the effectiveness of our methods under\ndifferent acoustic conditions. Our code is available at\nhttps://github.com/MiuLab/Lattice-SLU", "published": "2020-11-02 07:14:34", "link": "http://arxiv.org/abs/2011.00780v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hierarchical Bi-Directional Self-Attention Networks for Paper Review\n  Rating Recommendation", "abstract": "Review rating prediction of text reviews is a rapidly growing technology with\na wide range of applications in natural language processing. However, most\nexisting methods either use hand-crafted features or learn features using deep\nlearning with simple text corpus as input for review rating prediction,\nignoring the hierarchies among data. In this paper, we propose a Hierarchical\nbi-directional self-attention Network framework (HabNet) for paper review\nrating prediction and recommendation, which can serve as an effective\ndecision-making tool for the academic paper review process. Specifically, we\nleverage the hierarchical structure of the paper reviews with three levels of\nencoders: sentence encoder (level one), intra-review encoder (level two) and\ninter-review encoder (level three). Each encoder first derives contextual\nrepresentation of each level, then generates a higher-level representation, and\nafter the learning process, we are able to identify useful predictors to make\nthe final acceptance decision, as well as to help discover the inconsistency\nbetween numerical review ratings and text sentiment conveyed by reviewers.\nFurthermore, we introduce two new metrics to evaluate models in data imbalance\nsituations. Extensive experiments on a publicly available dataset (PeerRead)\nand our own collected dataset (OpenReview) demonstrate the superiority of the\nproposed approach compared with state-of-the-art methods.", "published": "2020-11-02 08:07:50", "link": "http://arxiv.org/abs/2011.00802v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning from Non-Binary Constituency Trees via Tensor Decomposition", "abstract": "Processing sentence constituency trees in binarised form is a common and\npopular approach in literature. However, constituency trees are non-binary by\nnature. The binarisation procedure changes deeply the structure, furthering\nconstituents that instead are close. In this work, we introduce a new approach\nto deal with non-binary constituency trees which leverages tensor-based models.\nIn particular, we show how a powerful composition function based on the\ncanonical tensor decomposition can exploit such a rich structure. A key point\nof our approach is the weight sharing constraint imposed on the factor\nmatrices, which allows limiting the number of model parameters. Finally, we\nintroduce a Tree-LSTM model which takes advantage of this composition function\nand we experimentally assess its performance on different NLP tasks.", "published": "2020-11-02 10:06:59", "link": "http://arxiv.org/abs/2011.00860v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Advanced Semantics for Commonsense Knowledge Extraction", "abstract": "Commonsense knowledge (CSK) about concepts and their properties is useful for\nAI applications such as robust chatbots. Prior works like ConceptNet, TupleKB\nand others compiled large CSK collections, but are restricted in their\nexpressiveness to subject-predicate-object (SPO) triples with simple concepts\nfor S and monolithic strings for P and O. Also, these projects have either\nprioritized precision or recall, but hardly reconcile these complementary\ngoals. This paper presents a methodology, called Ascent, to automatically build\na large-scale knowledge base (KB) of CSK assertions, with advanced\nexpressiveness and both better precision and recall than prior works. Ascent\ngoes beyond triples by capturing composite concepts with subgroups and aspects,\nand by refining assertions with semantic facets. The latter are important to\nexpress temporal and spatial validity of assertions and further qualifiers.\nAscent combines open information extraction with judicious cleaning using\nlanguage models. Intrinsic evaluation shows the superior size and quality of\nthe Ascent KB, and an extrinsic evaluation for QA-support tasks underlines the\nbenefits of Ascent. A web interface, data and code can be found at\nhttps://ascent.mpi-inf.mpg.de/.", "published": "2020-11-02 11:37:17", "link": "http://arxiv.org/abs/2011.00905v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "The 2020s Political Economy of Machine Translation", "abstract": "This paper explores the hypothesis that the diversity of human languages,\nright now a barrier to interoperability in communication and trade, will become\nsignificantly less of a barrier as machine translation technologies are\ndeployed over the next several years.But this new boundary-breaking technology\ndoes not reduce all boundaries equally, and it creates new challenges for the\ndistribution of ideas and thus for innovation and economic growth.", "published": "2020-11-02 14:28:16", "link": "http://arxiv.org/abs/2011.01007v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Exploring Question-Specific Rewards for Generating Deep Questions", "abstract": "Recent question generation (QG) approaches often utilize the\nsequence-to-sequence framework (Seq2Seq) to optimize the log-likelihood of\nground-truth questions using teacher forcing. However, this training objective\nis inconsistent with actual question quality, which is often reflected by\ncertain global properties such as whether the question can be answered by the\ndocument. As such, we directly optimize for QG-specific objectives via\nreinforcement learning to improve question quality. We design three different\nrewards that target to improve the fluency, relevance, and answerability of\ngenerated questions. We conduct both automatic and human evaluations in\naddition to a thorough analysis to explore the effect of each QG-specific\nreward. We find that optimizing question-specific rewards generally leads to\nbetter performance in automatic evaluation metrics. However, only the rewards\nthat correlate well with human judgement (e.g., relevance) lead to real\nimprovement in question quality. Optimizing for the others, especially\nanswerability, introduces incorrect bias to the model, resulting in poor\nquestion quality. Our code is publicly available at\nhttps://github.com/YuxiXie/RL-for-Question-Generation.", "published": "2020-11-02 16:37:30", "link": "http://arxiv.org/abs/2011.01102v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Speaker anonymisation using the McAdams coefficient", "abstract": "Anonymisation has the goal of manipulating speech signals in order to degrade\nthe reliability of automatic approaches to speaker recognition, while\npreserving other aspects of speech, such as those relating to intelligibility\nand naturalness. This paper reports an approach to anonymisation that, unlike\nother current approaches, requires no training data, is based upon well-known\nsignal processing techniques and is both efficient and effective. The proposed\nsolution uses the McAdams coefficient to transform the spectral envelope of\nspeech signals. Results derived using common VoicePrivacy 2020 databases and\nprotocols show that random, optimised transformations can outperform competing\nsolutions in terms of anonymisation while causing only modest, additional\ndegradations to intelligibility, even in the case of a semi-informed privacy\nadversary.", "published": "2020-11-02 17:07:17", "link": "http://arxiv.org/abs/2011.01130v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Improving Variational Autoencoder for Text Modelling with Timestep-Wise\n  Regularisation", "abstract": "The Variational Autoencoder (VAE) is a popular and powerful model applied to\ntext modelling to generate diverse sentences. However, an issue known as\nposterior collapse (or KL loss vanishing) happens when the VAE is used in text\nmodelling, where the approximate posterior collapses to the prior, and the\nmodel will totally ignore the latent variables and be degraded to a plain\nlanguage model during text generation. Such an issue is particularly prevalent\nwhen RNN-based VAE models are employed for text modelling. In this paper, we\npropose a simple, generic architecture called Timestep-Wise Regularisation VAE\n(TWR-VAE), which can effectively avoid posterior collapse and can be applied to\nany RNN-based VAE models. The effectiveness and versatility of our model are\ndemonstrated in different tasks, including language modelling and dialogue\nresponse generation.", "published": "2020-11-02 17:20:56", "link": "http://arxiv.org/abs/2011.01136v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Introducing various Semantic Models for Amharic: Experimentation and\n  Evaluation with multiple Tasks and Datasets", "abstract": "The availability of different pre-trained semantic models enabled the quick\ndevelopment of machine learning components for downstream applications. Despite\nthe availability of abundant text data for low resource languages, only a few\nsemantic models are publicly available. Publicly available pre-trained models\nare usually built as a multilingual version of semantic models that can not fit\nwell for each language due to context variations. In this work, we introduce\ndifferent semantic models for Amharic. After we experiment with the existing\npre-trained semantic models, we trained and fine-tuned nine new different\nmodels using a monolingual text corpus. The models are build using word2Vec\nembeddings, distributional thesaurus (DT), contextual embeddings, and DT\nembeddings obtained via network embedding algorithms. Moreover, we employ these\nmodels for different NLP tasks and investigate their impact. We find that newly\ntrained models perform better than pre-trained multilingual models.\nFurthermore, models based on contextual embeddings from RoBERTA perform better\nthan the word2Vec models.", "published": "2020-11-02 17:48:25", "link": "http://arxiv.org/abs/2011.01154v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exemplar Guided Active Learning", "abstract": "We consider the problem of wisely using a limited budget to label a small\nsubset of a large unlabeled dataset. We are motivated by the NLP problem of\nword sense disambiguation. For any word, we have a set of candidate labels from\na knowledge base, but the label set is not necessarily representative of what\noccurs in the data: there may exist labels in the knowledge base that very\nrarely occur in the corpus because the sense is rare in modern English; and\nconversely there may exist true labels that do not exist in our knowledge base.\nOur aim is to obtain a classifier that performs as well as possible on examples\nof each \"common class\" that occurs with frequency above a given threshold in\nthe unlabeled set while annotating as few examples as possible from \"rare\nclasses\" whose labels occur with less than this frequency. The challenge is\nthat we are not informed which labels are common and which are rare, and the\ntrue label distribution may exhibit extreme skew. We describe an active\nlearning approach that (1) explicitly searches for rare classes by leveraging\nthe contextual embedding spaces provided by modern language models, and (2)\nincorporates a stopping rule that ignores classes once we prove that they occur\nbelow our target threshold with high probability. We prove that our algorithm\nonly costs logarithmically more than a hypothetical approach that knows all\ntrue label frequencies and show experimentally that incorporating automated\nsearch can significantly reduce the number of samples needed to reach target\naccuracy levels.", "published": "2020-11-02 20:01:39", "link": "http://arxiv.org/abs/2011.01285v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Automatic Detection of Machine Generated Text: A Critical Survey", "abstract": "Text generative models (TGMs) excel in producing text that matches the style\nof human language reasonably well. Such TGMs can be misused by adversaries,\ne.g., by automatically generating fake news and fake product reviews that can\nlook authentic and fool humans. Detectors that can distinguish text generated\nby TGM from human written text play a vital role in mitigating such misuse of\nTGMs. Recently, there has been a flurry of works from both natural language\nprocessing (NLP) and machine learning (ML) communities to build accurate\ndetectors for English. Despite the importance of this problem, there is\ncurrently no work that surveys this fast-growing literature and introduces\nnewcomers to important research challenges. In this work, we fill this void by\nproviding a critical survey and review of this literature to facilitate a\ncomprehensive understanding of this problem. We conduct an in-depth error\nanalysis of the state-of-the-art detector and discuss research directions to\nguide future work in this exciting area.", "published": "2020-11-02 20:59:26", "link": "http://arxiv.org/abs/2011.01314v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Sentence Embeddings from Pre-trained Language Models", "abstract": "Pre-trained contextual representations like BERT have achieved great success\nin natural language processing. However, the sentence embeddings from the\npre-trained language models without fine-tuning have been found to poorly\ncapture semantic meaning of sentences. In this paper, we argue that the\nsemantic information in the BERT embeddings is not fully exploited. We first\nreveal the theoretical connection between the masked language model\npre-training objective and the semantic similarity task theoretically, and then\nanalyze the BERT sentence embeddings empirically. We find that BERT always\ninduces a non-smooth anisotropic semantic space of sentences, which harms its\nperformance of semantic similarity. To address this issue, we propose to\ntransform the anisotropic sentence embedding distribution to a smooth and\nisotropic Gaussian distribution through normalizing flows that are learned with\nan unsupervised objective. Experimental results show that our proposed\nBERT-flow method obtains significant performance gains over the\nstate-of-the-art sentence embeddings on a variety of semantic textual\nsimilarity tasks. The code is available at\nhttps://github.com/bohanli/BERT-flow.", "published": "2020-11-02 13:14:57", "link": "http://arxiv.org/abs/2011.05864v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dual-decoder Transformer for Joint Automatic Speech Recognition and\n  Multilingual Speech Translation", "abstract": "We introduce dual-decoder Transformer, a new model architecture that jointly\nperforms automatic speech recognition (ASR) and multilingual speech translation\n(ST). Our models are based on the original Transformer architecture (Vaswani et\nal., 2017) but consist of two decoders, each responsible for one task (ASR or\nST). Our major contribution lies in how these decoders interact with each\nother: one decoder can attend to different information sources from the other\nvia a dual-attention mechanism. We propose two variants of these architectures\ncorresponding to two different levels of dependencies between the decoders,\ncalled the parallel and cross dual-decoder Transformers, respectively.\nExtensive experiments on the MuST-C dataset show that our models outperform the\npreviously-reported highest translation performance in the multilingual\nsettings, and outperform as well bilingual one-to-one results. Furthermore, our\nparallel models demonstrate no trade-off between ASR and ST compared to the\nvanilla multi-task architecture. Our code and pre-trained models are available\nat https://github.com/formiel/speech-translation.", "published": "2020-11-02 04:59:50", "link": "http://arxiv.org/abs/2011.00747v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Emergent Communication Pretraining for Few-Shot Machine Translation", "abstract": "While state-of-the-art models that rely upon massively multilingual\npretrained encoders achieve sample efficiency in downstream applications, they\nstill require abundant amounts of unlabelled text. Nevertheless, most of the\nworld's languages lack such resources. Hence, we investigate a more radical\nform of unsupervised knowledge transfer in the absence of linguistic data. In\nparticular, for the first time we pretrain neural networks via emergent\ncommunication from referential games. Our key assumption is that grounding\ncommunication on images---as a crude approximation of real-world\nenvironments---inductively biases the model towards learning natural languages.\nOn the one hand, we show that this substantially benefits machine translation\nin few-shot settings. On the other hand, this also provides an extrinsic\nevaluation protocol to probe the properties of emergent languages ex vitro.\nIntuitively, the closer they are to natural languages, the higher the gains\nfrom pretraining on them should be. For instance, in this work we measure the\ninfluence of communication success and maximum sequence length on downstream\nperformances. Finally, we introduce a customised adapter layer and annealing\nstrategies for the regulariser of maximum-a-posteriori inference during\nfine-tuning. These turn out to be crucial to facilitate knowledge transfer and\nprevent catastrophic forgetting. Compared to a recurrent baseline, our method\nyields gains of $59.0\\%$$\\sim$$147.6\\%$ in BLEU score with only $500$ NMT\ntraining instances and $65.1\\%$$\\sim$$196.7\\%$ with $1,000$ NMT training\ninstances across four language pairs. These proof-of-concept results reveal the\npotential of emergent communication pretraining for both natural language\nprocessing tasks in resource-poor settings and extrinsic evaluation of\nartificial languages.", "published": "2020-11-02 10:57:53", "link": "http://arxiv.org/abs/2011.00890v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transformer-based Arabic Dialect Identification", "abstract": "This paper presents a dialect identification (DID) system based on the\ntransformer neural network architecture. The conventional convolutional neural\nnetwork (CNN)-based systems use the shorter receptive fields. We believe that\nlong range information is equally important for language and DID, and\nself-attention mechanism in transformer captures the long range dependencies.\nIn addition, to reduce the computational complexity, self-attention with\ndownsampling is used to process the acoustic features. This process extracts\nsparse, yet informative features. Our experimental results show that\ntransformer outperforms CNN-based networks on the Arabic dialect identification\n(ADI) dataset. We also report that the score-level fusion of CNN and\ntransformer-based systems obtains an overall accuracy of 86.29% on the ADI17\ndatabase.", "published": "2020-11-02 03:10:34", "link": "http://arxiv.org/abs/2011.00699v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "End-to-end anti-spoofing with RawNet2", "abstract": "Spoofing countermeasures aim to protect automatic speaker verification\nsystems from attempts to manipulate their reliability with the use of spoofed\nspeech signals. While results from the most recent ASVspoof 2019 evaluation\nshow great potential to detect most forms of attack, some continue to evade\ndetection. This paper reports the first application of RawNet2 to\nanti-spoofing. RawNet2 ingests raw audio and has potential to learn cues that\nare not detectable using more traditional countermeasure solutions. We describe\nmodifications made to the original RawNet2 architecture so that it can be\napplied to anti-spoofing. For A17 attacks, our RawNet2 systems results are the\nsecond-best reported, while the fusion of RawNet2 and baseline countermeasures\ngives the second-best results reported for the full ASVspoof 2019 logical\naccess condition. Our results are reproducible with open source software.", "published": "2020-11-02 16:40:52", "link": "http://arxiv.org/abs/2011.01108v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "CAMP: a Two-Stage Approach to Modelling Prosody in Context", "abstract": "Prosody is an integral part of communication, but remains an open problem in\nstate-of-the-art speech synthesis. There are two major issues faced when\nmodelling prosody: (1) prosody varies at a slower rate compared with other\ncontent in the acoustic signal (e.g. segmental information and background\nnoise); (2) determining appropriate prosody without sufficient context is an\nill-posed problem. In this paper, we propose solutions to both these issues. To\nmitigate the challenge of modelling a slow-varying signal, we learn to\ndisentangle prosodic information using a word level representation. To\nalleviate the ill-posed nature of prosody modelling, we use syntactic and\nsemantic information derived from text to learn a context-dependent prior over\nour prosodic space. Our Context-Aware Model of Prosody (CAMP) outperforms the\nstate-of-the-art technique, closing the gap with natural speech by 26%. We also\nfind that replacing attention with a jointly-trained duration model improves\nprosody significantly.", "published": "2020-11-02 18:14:57", "link": "http://arxiv.org/abs/2011.01175v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Learning generic feature representation with synthetic data for\n  weakly-supervised sound event detection by inter-frame distance loss", "abstract": "Due to the limitation of strong-labeled sound event detection data set, using\nsynthetic data to improve the sound event detection system performance has been\na new research focus. In this paper, we try to exploit the usage of synthetic\ndata to improve the feature representation. Based on metric learning, we\nproposed inter-frame distance loss function for domain adaptation, and prove\nthe effectiveness of it on sound event detection. We also applied multi-task\nlearning with synthetic data. We find the the best performance can be achieved\nwhen the two methods being used together. The experiment on DCASE 2018 task 4\ntest set and DCASE 2019 task 4 synthetic set both show competitive results.", "published": "2020-11-02 03:05:33", "link": "http://arxiv.org/abs/2011.00695v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CVC: Contrastive Learning for Non-parallel Voice Conversion", "abstract": "Cycle consistent generative adversarial network (CycleGAN) and variational\nautoencoder (VAE) based models have gained popularity in non-parallel voice\nconversion recently. However, they often suffer from difficult training process\nand unsatisfactory results. In this paper, we propose CVC, a contrastive\nlearning-based adversarial approach for voice conversion. Compared to previous\nCycleGAN-based methods, CVC only requires an efficient one-way GAN training by\ntaking the advantage of contrastive learning. When it comes to non-parallel\none-to-one voice conversion, CVC is on par or better than CycleGAN and VAE\nwhile effectively reducing training time. CVC further demonstrates superior\nperformance in many-to-one voice conversion, enabling the conversion from\nunseen speakers.", "published": "2020-11-02 07:17:00", "link": "http://arxiv.org/abs/2011.00782v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sound Event Detection and Separation: a Benchmark on Desed Synthetic\n  Soundscapes", "abstract": "We propose a benchmark of state-of-the-art sound event detection systems\n(SED). We designed synthetic evaluation sets to focus on specific sound event\ndetection challenges. We analyze the performance of the submissions to DCASE\n2021 task 4 depending on time related modifications (time position of an event\nand length of clips) and we study the impact of non-target sound events and\nreverberation. We show that the localization in time of sound events is still a\nproblem for SED systems. We also show that reverberation and non-target sound\nevents are severely degrading the performance of the SED systems. In the latter\ncase, sound separation seems like a promising solution.", "published": "2020-11-02 08:05:01", "link": "http://arxiv.org/abs/2011.00801v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "What's All the FUSS About Free Universal Sound Separation Data?", "abstract": "We introduce the Free Universal Sound Separation (FUSS) dataset, a new corpus\nfor experiments in separating mixtures of an unknown number of sounds from an\nopen domain of sound types. The dataset consists of 23 hours of single-source\naudio data drawn from 357 classes, which are used to create mixtures of one to\nfour sources. To simulate reverberation, an acoustic room simulator is used to\ngenerate impulse responses of box shaped rooms with frequency-dependent\nreflective walls. Additional open-source data augmentation tools are also\nprovided to produce new mixtures with different combinations of sources and\nroom simulations. Finally, we introduce an open-source baseline separation\nmodel, based on an improved time-domain convolutional network (TDCN++), that\ncan separate a variable number of sources in a mixture. This model achieves 9.8\ndB of scale-invariant signal-to-noise ratio improvement (SI-SNRi) on mixtures\nwith two to four sources, while reconstructing single-source inputs with 35.5\ndB absolute SI-SNR. We hope this dataset will lower the barrier to new research\nand allow for fast iteration and application of novel techniques from other\nmachine learning domains to the sound separation challenge.", "published": "2020-11-02 08:09:34", "link": "http://arxiv.org/abs/2011.00803v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FeatherTTS: Robust and Efficient attention based Neural TTS", "abstract": "Attention based neural TTS is elegant speech synthesis pipeline and has shown\na powerful ability to generate natural speech. However, it is still not robust\nenough to meet the stability requirements for industrial products. Besides, it\nsuffers from slow inference speed owning to the autoregressive generation\nprocess. In this work, we propose FeatherTTS, a robust and efficient\nattention-based neural TTS system. Firstly, we propose a novel Gaussian\nattention which utilizes interpretability of Gaussian attention and the strict\nmonotonic property in TTS. By this method, we replace the commonly used stop\ntoken prediction architecture with attentive stop prediction. Secondly, we\napply block sparsity on the autoregressive decoder to speed up speech\nsynthesis. The experimental results show that our proposed FeatherTTS not only\nnearly eliminates the problem of word skipping, repeating in particularly hard\ntexts and keep the naturalness of generated speech, but also speeds up acoustic\nfeature generation by 3.5 times over Tacotron. Overall, the proposed FeatherTTS\ncan be $35$x faster than real-time on a single CPU.", "published": "2020-11-02 12:33:11", "link": "http://arxiv.org/abs/2011.00935v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Focus on the present: a regularization method for the ASR source-target\n  attention layer", "abstract": "This paper introduces a novel method to diagnose the source-target attention\nin state-of-the-art end-to-end speech recognition models with joint\nconnectionist temporal classification (CTC) and attention training. Our method\nis based on the fact that both, CTC and source-target attention, are acting on\nthe same encoder representations. To understand the functionality of the\nattention, CTC is applied to compute the token posteriors given the attention\noutputs. We found that the source-target attention heads are able to predict\nseveral tokens ahead of the current one. Inspired by the observation, a new\nregularization method is proposed which leverages CTC to make source-target\nattention more focused on the frames corresponding to the output token being\npredicted by the decoder. Experiments reveal stable improvements up to 7\\% and\n13\\% relatively with the proposed regularization on TED-LIUM 2 and LibriSpeech.", "published": "2020-11-02 18:56:33", "link": "http://arxiv.org/abs/2011.01210v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Multitask Learning and Joint Optimization for Transformer-RNN-Transducer\n  Speech Recognition", "abstract": "Recently, several types of end-to-end speech recognition methods named\ntransformer-transducer were introduced. According to those kinds of methods,\ntranscription networks are generally modeled by transformer-based neural\nnetworks, while prediction networks could be modeled by either transformers or\nrecurrent neural networks (RNN). This paper explores multitask learning, joint\noptimization, and joint decoding methods for transformer-RNN-transducer\nsystems. Our proposed methods have the main advantage in that the model can\nmaintain information on the large text corpus. We prove their effectiveness by\nperforming experiments utilizing the well-known ESPNET toolkit for the widely\nused Librispeech datasets. We also show that the proposed methods can reduce\nword error rate (WER) by 16.6 % and 13.3 % for test-clean and test-other\ndatasets, respectively, without changing the overall model structure nor\nexploiting an external LM.", "published": "2020-11-02 06:38:06", "link": "http://arxiv.org/abs/2011.00771v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Using a Bi-directional LSTM Model with Attention Mechanism trained on\n  MIDI Data for Generating Unique Music", "abstract": "Generating music is an interesting and challenging problem in the field of\nmachine learning. Mimicking human creativity has been popular in recent years,\nespecially in the field of computer vision and image processing. With the\nadvent of GANs, it is possible to generate new similar images, based on trained\ndata. But this cannot be done for music similarly, as music has an extra\ntemporal dimension. So it is necessary to understand how music is represented\nin digital form. When building models that perform this generative task, the\nlearning and generation part is done in some high-level representation such as\nMIDI (Musical Instrument Digital Interface) or scores. This paper proposes a\nbi-directional LSTM (Long short-term memory) model with attention mechanism\ncapable of generating similar type of music based on MIDI data. The music\ngenerated by the model follows the theme/style of the music the model is\ntrained on. Also, due to the nature of MIDI, the tempo, instrument, and other\nparameters can be defined, and changed, post generation.", "published": "2020-11-02 06:43:28", "link": "http://arxiv.org/abs/2011.00773v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Into the Wild with AudioScope: Unsupervised Audio-Visual Separation of\n  On-Screen Sounds", "abstract": "Recent progress in deep learning has enabled many advances in sound\nseparation and visual scene understanding. However, extracting sound sources\nwhich are apparent in natural videos remains an open problem. In this work, we\npresent AudioScope, a novel audio-visual sound separation framework that can be\ntrained without supervision to isolate on-screen sound sources from real\nin-the-wild videos. Prior audio-visual separation work assumed artificial\nlimitations on the domain of sound classes (e.g., to speech or music),\nconstrained the number of sources, and required strong sound separation or\nvisual segmentation labels. AudioScope overcomes these limitations, operating\non an open domain of sounds, with variable numbers of sources, and without\nlabels or prior visual segmentation. The training procedure for AudioScope uses\nmixture invariant training (MixIT) to separate synthetic mixtures of mixtures\n(MoMs) into individual sources, where noisy labels for mixtures are provided by\nan unsupervised audio-visual coincidence model. Using the noisy labels, along\nwith attention between video and audio features, AudioScope learns to identify\naudio-visual similarity and to suppress off-screen sounds. We demonstrate the\neffectiveness of our approach using a dataset of video clips extracted from\nopen-domain YFCC100m video data. This dataset contains a wide diversity of\nsound classes recorded in unconstrained conditions, making the application of\nprevious methods unsuitable. For evaluation and semi-supervised experiments, we\ncollected human labels for presence of on-screen and off-screen sounds on a\nsmall subset of clips.", "published": "2020-11-02 17:36:13", "link": "http://arxiv.org/abs/2011.01143v2", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Optimize what matters: Training DNN-HMM Keyword Spotting Model Using End\n  Metric", "abstract": "Deep Neural Network--Hidden Markov Model (DNN-HMM) based methods have been\nsuccessfully used for many always-on keyword spotting algorithms that detect a\nwake word to trigger a device. The DNN predicts the state probabilities of a\ngiven speech frame, while HMM decoder combines the DNN predictions of multiple\nspeech frames to compute the keyword detection score. The DNN, in prior\nmethods, is trained independent of the HMM parameters to minimize the\ncross-entropy loss between the predicted and the ground-truth state\nprobabilities. The mis-match between the DNN training loss (cross-entropy) and\nthe end metric (detection score) is the main source of sub-optimal performance\nfor the keyword spotting task. We address this loss-metric mismatch with a\nnovel end-to-end training strategy that learns the DNN parameters by optimizing\nfor the detection score. To this end, we make the HMM decoder (dynamic\nprogramming) differentiable and back-propagate through it to maximize the score\nfor the keyword and minimize the scores for non-keyword speech segments. Our\nmethod does not require any change in the model architecture or the inference\nframework; therefore, there is no overhead in run-time memory or compute\nrequirements. Moreover, we show significant reduction in false rejection rate\n(FRR) at the same false trigger experience (> 70% over independent DNN\ntraining).", "published": "2020-11-02 17:47:21", "link": "http://arxiv.org/abs/2011.01151v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning to Maximize Speech Quality Directly Using MOS Prediction for\n  Neural Text-to-Speech", "abstract": "Although recent neural text-to-speech (TTS) systems have achieved\nhigh-quality speech synthesis, there are cases where a TTS system generates\nlow-quality speech, mainly caused by limited training data or information loss\nduring knowledge distillation. Therefore, we propose a novel method to improve\nspeech quality by training a TTS model under the supervision of perceptual\nloss, which measures the distance between the maximum possible speech quality\nscore and the predicted one. We first pre-train a mean opinion score (MOS)\nprediction model and then train a TTS model to maximize the MOS of synthesized\nspeech using the pre-trained MOS prediction model. The proposed method can be\napplied independently regardless of the TTS model architecture or the cause of\nspeech quality degradation and efficiently without increasing the inference\ntime or model complexity. The evaluation results for the MOS and phone error\nrate demonstrate that our proposed approach improves previous models in terms\nof both naturalness and intelligibility.", "published": "2020-11-02 18:13:48", "link": "http://arxiv.org/abs/2011.01174v5", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
