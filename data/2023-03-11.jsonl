{"title": "An Interactive UI to Support Sensemaking over Collections of Parallel\n  Texts", "abstract": "Scientists and science journalists, among others, often need to make sense of\na large number of papers and how they compare with each other in scope, focus,\nfindings, or any other important factors. However, with a large corpus of\npapers, it's cognitively demanding to pairwise compare and contrast them all\nwith each other. Fully automating this review process would be infeasible,\nbecause it often requires domain-specific knowledge, as well as understanding\nwhat the context and motivations for the review are. While there are existing\ntools to help with the process of organizing and annotating papers for\nliterature reviews, at the core they still rely on people to serially read\nthrough papers and manually make sense of relevant information.\n  We present AVTALER, which combines peoples' unique skills, contextual\nawareness, and knowledge, together with the strength of automation. Given a set\nof comparable text excerpts from a paper corpus, it supports users in\nsensemaking and contrasting paper attributes by interactively aligning text\nexcerpts in a table so that comparable details are presented in a shared\ncolumn. AVTALER is based on a core alignment algorithm that makes use of modern\nNLP tools. Furthermore, AVTALER is a mixed-initiative system: users can\ninteractively give the system constraints which are integrated into the\nalignment construction process.", "published": "2023-03-11 01:04:25", "link": "http://arxiv.org/abs/2303.06264v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Consistency Analysis of ChatGPT", "abstract": "ChatGPT has gained a huge popularity since its introduction. Its positive\naspects have been reported through many media platforms, and some analyses even\nshowed that ChatGPT achieved a decent grade in professional exams, adding extra\nsupport to the claim that AI can now assist and even replace humans in\nindustrial fields. Others, however, doubt its reliability and trustworthiness.\nThis paper investigates the trustworthiness of ChatGPT and GPT-4 regarding\nlogically consistent behaviour, focusing specifically on semantic consistency\nand the properties of negation, symmetric, and transitive consistency. Our\nfindings suggest that while both models appear to show an enhanced language\nunderstanding and reasoning ability, they still frequently fall short of\ngenerating logically consistent predictions. We also ascertain via experiments\nthat prompt designing, few-shot learning and employing larger large language\nmodels (LLMs) are unlikely to be the ultimate solution to resolve the\ninconsistency issue of LLMs.", "published": "2023-03-11 01:19:01", "link": "http://arxiv.org/abs/2303.06273v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transcription free filler word detection with Neural semi-CRFs", "abstract": "Non-linguistic filler words, such as \"uh\" or \"um\", are prevalent in\nspontaneous speech and serve as indicators for expressing hesitation or\nuncertainty. Previous works for detecting certain non-linguistic filler words\nare highly dependent on transcriptions from a well-established commercial\nautomatic speech recognition (ASR) system. However, certain ASR systems are not\nuniversally accessible from many aspects, e.g., budget, target languages, and\ncomputational power. In this work, we investigate filler word detection system\nthat does not depend on ASR systems. We show that, by using the structured\nstate space sequence model (S4) and neural semi-Markov conditional random\nfields (semi-CRFs), we achieve an absolute F1 improvement of 6.4% (segment\nlevel) and 3.1% (event level) on the PodcastFillers dataset. We also conduct a\nqualitative analysis on the detected results to analyze the limitations of our\nproposed system.", "published": "2023-03-11 18:17:03", "link": "http://arxiv.org/abs/2303.06475v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Verbal behavior without syntactic structures: beyond Skinner and Chomsky", "abstract": "What does it mean to know language? Since the Chomskian revolution, one\npopular answer to this question has been: to possess a generative grammar that\nexclusively licenses certain syntactic structures. Decades later, not even an\napproximation to such a grammar, for any language, has been formulated; the\nidea that grammar is universal and innately specified has proved barren; and\nattempts to show how it could be learned from experience invariably come up\nshort. To move on from this impasse, we must rediscover the extent to which\nlanguage is like any other human behavior: dynamic, social, multimodal,\npatterned, and purposive, its purpose being to promote desirable actions (or\nthoughts) in others and self. Recent psychological, computational,\nneurobiological, and evolutionary insights into the shaping and structure of\nbehavior may then point us toward a new, viable account of language.", "published": "2023-03-11 00:01:21", "link": "http://arxiv.org/abs/2303.08080v1", "categories": ["cs.CL", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Parachute: Evaluating Interactive Human-LM Co-writing Systems", "abstract": "A surge of advances in language models (LMs) has led to significant interest\nin using LMs to build co-writing systems, in which humans and LMs interactively\ncontribute to a shared writing artifact. However, there is a lack of studies\nassessing co-writing systems in interactive settings. We propose a\nhuman-centered evaluation framework, Parachute, for interactive co-writing\nsystems. Parachute showcases an integrative view of interaction evaluation,\nwhere each evaluation aspect consists of categorized practical metrics.\nFurthermore, we present Parachute with a use case to demonstrate how to\nevaluate and compare co-writing systems using Parachute.", "published": "2023-03-11 07:30:52", "link": "http://arxiv.org/abs/2303.06333v2", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and\n  Multilingual Natural Language Generation", "abstract": "Natural Language Generation (NLG) accepts input data in the form of images,\nvideos, or text and generates corresponding natural language text as output.\nExisting NLG methods mainly adopt a supervised approach and rely heavily on\ncoupled data-to-text pairs. However, for many targeted scenarios and for\nnon-English languages, sufficient quantities of labeled data are often not\navailable. To relax the dependency on labeled data of downstream tasks, we\npropose an intuitive and effective zero-shot learning framework, ZeroNLG, which\ncan deal with multiple NLG tasks, including image-to-text (image captioning),\nvideo-to-text (video captioning), and text-to-text (neural machine\ntranslation), across English, Chinese, German, and French within a unified\nframework. ZeroNLG does not require any labeled downstream pairs for training.\nDuring training, ZeroNLG (i) projects different domains (across modalities and\nlanguages) to corresponding coordinates in a shared common latent space; (ii)\nbridges different domains by aligning their corresponding coordinates in this\nspace; and (iii) builds an unsupervised multilingual auto-encoder to learn to\ngenerate text by reconstructing the input text given its coordinate in shared\nlatent space. Consequently, during inference, based on the data-to-text\npipeline, ZeroNLG can generate target sentences across different languages\ngiven the coordinate of input data in the common space. Within this unified\nframework, given visual (imaging or video) data as input, ZeroNLG can perform\nzero-shot visual captioning; given textual sentences as input, ZeroNLG can\nperform zero-shot machine translation. We present the results of extensive\nexperiments on twelve NLG tasks, showing that, without using any labeled\ndownstream pairs for training, ZeroNLG generates high-quality and believable\noutputs and significantly outperforms existing zero-shot methods.", "published": "2023-03-11 17:14:33", "link": "http://arxiv.org/abs/2303.06458v3", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Stabilizing Transformer Training by Preventing Attention Entropy\n  Collapse", "abstract": "Training stability is of great importance to Transformers. In this work, we\ninvestigate the training dynamics of Transformers by examining the evolution of\nthe attention layers. In particular, we track the attention entropy for each\nattention head during the course of training, which is a proxy for model\nsharpness. We identify a common pattern across different architectures and\ntasks, where low attention entropy is accompanied by high training instability,\nwhich can take the form of oscillating loss or divergence. We denote the\npathologically low attention entropy, corresponding to highly concentrated\nattention scores, as $\\textit{entropy collapse}$. As a remedy, we propose\n$\\sigma$Reparam, a simple and efficient solution where we reparametrize all\nlinear layers with spectral normalization and an additional learned scalar. We\ndemonstrate that $\\sigma$Reparam successfully prevents entropy collapse in the\nattention layers, promoting more stable training. Additionally, we prove a\ntight lower bound of the attention entropy, which decreases exponentially fast\nwith the spectral norm of the attention logits, providing additional motivation\nfor our approach. We conduct experiments with $\\sigma$Reparam on image\nclassification, image self-supervised learning, machine translation, speech\nrecognition, and language modeling tasks. We show that $\\sigma$Reparam provides\nstability and robustness with respect to the choice of hyperparameters, going\nso far as enabling training (a) a Vision Transformer {to competitive\nperformance} without warmup, weight decay, layer normalization or adaptive\noptimizers; (b) deep architectures in machine translation and (c) speech\nrecognition to competitive performance without warmup and adaptive optimizers.\nCode is available at \\url{https://github.com/apple/ml-sigma-reparam}.", "published": "2023-03-11 03:30:47", "link": "http://arxiv.org/abs/2303.06296v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "The NPU-ASLP System for Audio-Visual Speech Recognition in MISP 2022\n  Challenge", "abstract": "This paper describes our NPU-ASLP system for the Audio-Visual Diarization and\nRecognition (AVDR) task in the Multi-modal Information based Speech Processing\n(MISP) 2022 Challenge. Specifically, the weighted prediction error (WPE) and\nguided source separation (GSS) techniques are used to reduce reverberation and\ngenerate clean signals for each single speaker first. Then, we explore the\neffectiveness of Branchformer and E-Branchformer based ASR systems. To better\nmake use of the visual modality, a cross-attention based multi-modal fusion\nmodule is proposed, which explicitly learns the contextual relationship between\ndifferent modalities. Experiments show that our system achieves a concatenated\nminimum-permutation character error rate (cpCER) of 28.13\\% and 31.21\\% on the\nDev and Eval set, and obtains second place in the challenge.", "published": "2023-03-11 08:10:31", "link": "http://arxiv.org/abs/2303.06341v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Multi-Task Sub-Band Network For Deep Residual Echo Suppression", "abstract": "This paper introduces the SWANT team entry to the ICASSP 2023 AEC Challenge.\nWe submit a system that cascades a linear filter with a neural post-filter.\nParticularly, we adopt sub-band processing to handle full-band signals and\nshape the network with multi-task learning, where dual signal voice activity\ndetection (DSVAD) and echo estimation are adopted as auxiliary tasks. Moreover,\nwe particularly improve the time frequency convolution module (TFCM) to\nincrease the receptive field using small convolution kernels. Finally, our\nsystem has ranked 4th in ICASSP 2023 AEC Challenge Non-personalized track.", "published": "2023-03-11 12:51:12", "link": "http://arxiv.org/abs/2303.06404v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Relating EEG recordings to speech using envelope tracking and the\n  speech-FFR", "abstract": "During speech perception, a listener's electroencephalogram (EEG) reflects\nacoustic-level processing as well as higher-level cognitive factors such as\nspeech comprehension and attention. However, decoding speech from EEG\nrecordings is challenging due to the low signal-to-noise ratios of EEG signals.\nWe report on an approach developed for the ICASSP 2023 'Auditory EEG Decoding'\nSignal Processing Grand Challenge. A simple ensembling method is shown to\nconsiderably improve upon the baseline decoder performance. Even higher\nclassification rates are achieved by jointly decoding the speech-evoked\nfrequency-following response and responses to the temporal envelope of speech,\nas well as by fine-tuning the decoders to individual subjects. Our results\ncould have applications in the diagnosis of hearing disorders or in cognitively\nsteered hearing aids.", "published": "2023-03-11 16:12:11", "link": "http://arxiv.org/abs/2303.06435v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "TaylorAECNet: A Taylor Style Neural Network for Full-Band Echo\n  Cancellation", "abstract": "This paper describes aecX team's entry to the ICASSP 2023 acoustic echo\ncancellation (AEC) challenge. Our system consists of an adaptive filter and a\nproposed full-band Taylor-style acoustic echo cancellation neural network\n(TaylorAECNet) as a post-filter. Specifically, we leverage the recent advances\nin Taylor expansion based decoupling-style interpretable speech enhancement and\nexplore its feasibility in the AEC task. Our TaylorAECNet based approach\nachieves an overall mean opinion score (MOS) of 4.241, a word accuracy (WAcc)\nratio of 0.767, and ranks 5th in the non-personalized track (track 1).", "published": "2023-03-11 11:12:49", "link": "http://arxiv.org/abs/2303.06379v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
