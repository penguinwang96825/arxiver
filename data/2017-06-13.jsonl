{"title": "Modelling prosodic structure using Artificial Neural Networks", "abstract": "The ability to accurately perceive whether a speaker is asking a question or\nis making a statement is crucial for any successful interaction. However,\nlearning and classifying tonal patterns has been a challenging task for\nautomatic speech recognition and for models of tonal representation, as tonal\ncontours are characterized by significant variation. This paper provides a\nclassification model of Cypriot Greek questions and statements. We evaluate two\nstate-of-the-art network architectures: a Long Short-Term Memory (LSTM) network\nand a convolutional network (ConvNet). The ConvNet outperforms the LSTM in the\nclassification task and exhibited an excellent performance with 95%\nclassification accuracy.", "published": "2017-06-13 08:28:39", "link": "http://arxiv.org/abs/1706.03952v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Exploration of Neural Sequence-to-Sequence Architectures for\n  Automatic Post-Editing", "abstract": "In this work, we explore multiple neural architectures adapted for the task\nof automatic post-editing of machine translation output. We focus on neural\nend-to-end models that combine both inputs $mt$ (raw MT output) and $src$\n(source language input) in a single neural architecture, modeling $\\{mt, src\\}\n\\rightarrow pe$ directly. Apart from that, we investigate the influence of\nhard-attention models which seem to be well-suited for monolingual tasks, as\nwell as combinations of both ideas. We report results on data sets provided\nduring the WMT-2016 shared task on automatic post-editing and can demonstrate\nthat dual-attention models that incorporate all available data in the APE\nscenario in a single model improve on the best shared task system and on all\nother published results after the shared task. Dual-attention models that are\ncombined with hard attention remain competitive despite applying fewer changes\nto the input.", "published": "2017-06-13 15:55:02", "link": "http://arxiv.org/abs/1706.04138v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying Condition-Action Statements in Medical Guidelines Using\n  Domain-Independent Features", "abstract": "This paper advances the state of the art in text understanding of medical\nguidelines by releasing two new annotated clinical guidelines datasets, and\nestablishing baselines for using machine learning to extract condition-action\npairs. In contrast to prior work that relies on manually created rules, we\nreport experiment with several supervised machine learning techniques to\nclassify sentences as to whether they express conditions and actions. We show\nthe limitations and possible extensions of this work on text mining of medical\nguidelines.", "published": "2017-06-13 18:02:27", "link": "http://arxiv.org/abs/1706.04206v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Plan, Attend, Generate: Character-level Neural Machine Translation with\n  Planning in the Decoder", "abstract": "We investigate the integration of a planning mechanism into an\nencoder-decoder architecture with an explicit alignment for character-level\nmachine translation. We develop a model that plans ahead when it computes\nalignments between the source and target sequences, constructing a matrix of\nproposed future alignments and a commitment vector that governs whether to\nfollow or recompute the plan. This mechanism is inspired by the strategic\nattentive reader and writer (STRAW) model. Our proposed model is end-to-end\ntrainable with fully differentiable operations. We show that it outperforms a\nstrong baseline on three character-level decoder neural machine translation on\nWMT'15 corpus. Our analysis demonstrates that our model can compute\nqualitatively intuitive alignments and achieves superior performance with fewer\nparameters.", "published": "2017-06-13 23:11:04", "link": "http://arxiv.org/abs/1706.05087v2", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Relation Extraction via Reading Comprehension", "abstract": "We show that relation extraction can be reduced to answering simple reading\ncomprehension questions, by associating one or more natural-language questions\nwith each relation slot. This reduction has several advantages: we can (1)\nlearn relation-extraction models by extending recent neural\nreading-comprehension techniques, (2) build very large training sets for those\nmodels by combining relation-specific crowd-sourced questions with distant\nsupervision, and even (3) do zero-shot learning by extracting new relation\ntypes that are only specified at test-time, for which we have no labeled\ntraining examples. Experiments on a Wikipedia slot-filling task demonstrate\nthat the approach can generalize to new questions for known relation types with\nhigh accuracy, and that zero-shot generalization to unseen relation types is\npossible, at lower accuracy levels, setting the bar for future work on this\ntask.", "published": "2017-06-13 15:17:42", "link": "http://arxiv.org/abs/1706.04115v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adversarially Regularized Autoencoders", "abstract": "Deep latent variable models, trained using variational autoencoders or\ngenerative adversarial networks, are now a key technique for representation\nlearning of continuous structures. However, applying similar methods to\ndiscrete structures, such as text sequences or discretized images, has proven\nto be more challenging. In this work, we propose a flexible method for training\ndeep latent variable models of discrete structures. Our approach is based on\nthe recently-proposed Wasserstein autoencoder (WAE) which formalizes the\nadversarial autoencoder (AAE) as an optimal transport problem. We first extend\nthis framework to model discrete sequences, and then further explore different\nlearned priors targeting a controllable representation. This adversarially\nregularized autoencoder (ARAE) allows us to generate natural textual outputs as\nwell as perform manipulations in the latent space to induce change in the\noutput space. Finally we show that the latent representation can be trained to\nperform unaligned textual style transfer, giving improvements both in\nautomatic/human evaluation compared to existing methods.", "published": "2017-06-13 19:00:53", "link": "http://arxiv.org/abs/1706.04223v3", "categories": ["cs.LG", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "A Supervised Approach to Extractive Summarisation of Scientific Papers", "abstract": "Automatic summarisation is a popular approach to reduce a document to its\nmain arguments. Recent research in the area has focused on neural approaches to\nsummarisation, which can be very data-hungry. However, few large datasets exist\nand none for the traditionally popular domain of scientific publications, which\nopens up challenging research avenues centered on encoding large, complex\ndocuments. In this paper, we introduce a new dataset for summarisation of\ncomputer science publications by exploiting a large resource of author provided\nsummaries and show straightforward ways of extending it further. We develop\nmodels on the dataset making use of both neural sentence encoding and\ntraditionally used summarisation features and show that models which encode\nsentences as well as their local and global context perform best, significantly\noutperforming well-established baseline methods.", "published": "2017-06-13 08:15:25", "link": "http://arxiv.org/abs/1706.03946v1", "categories": ["cs.CL", "cs.AI", "cs.NE", "stat.AP", "stat.ML"], "primary_category": "cs.CL"}
