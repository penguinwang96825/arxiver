{"title": "Sequence-Level Knowledge Distillation for Model Compression of\n  Attention-based Sequence-to-Sequence Speech Recognition", "abstract": "We investigate the feasibility of sequence-level knowledge distillation of\nSequence-to-Sequence (Seq2Seq) models for Large Vocabulary Continuous Speech\nRecognition (LVSCR). We first use a pre-trained larger teacher model to\ngenerate multiple hypotheses per utterance with beam search. With the same\ninput, we then train the student model using these hypotheses generated from\nthe teacher as pseudo labels in place of the original ground truth labels. We\nevaluate our proposed method using Wall Street Journal (WSJ) corpus. It\nachieved up to $ 9.8 \\times$ parameter reduction with accuracy loss of up to\n7.0\\% word-error rate (WER) increase", "published": "2018-11-12 02:55:55", "link": "http://arxiv.org/abs/1811.04531v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Forecasting People's Needs in Hurricane Events from Social Network", "abstract": "Social networks can serve as a valuable communication channel for calls for\nhelp, offering assistance, and coordinating rescue activities in disaster.\nSocial networks such as Twitter allow users to continuously update relevant\ninformation, which is especially useful during a crisis, where the rapidly\nchanging conditions make it crucial to be able to access accurate information\npromptly. Social media helps those directly affected to inform others of\nconditions on the ground in real time and thus enables rescue workers to\ncoordinate their efforts more effectively, better meeting the survivors' need.\nThis paper presents a new sequence to sequence based framework for forecasting\npeople's needs during disasters using social media and weather data. It\nconsists of two Long Short-Term Memory (LSTM) models, one of which encodes\ninput sequences of weather information and the other plays as a conditional\ndecoder that decodes the encoded vector and forecasts the survivors' needs.\nCase studies utilizing data collected during Hurricane Sandy in 2012, Hurricane\nHarvey and Hurricane Irma in 2017 were analyzed and the results compared with\nthose obtained using a statistical language model n-gram and an LSTM generative\nmodel. Our proposed sequence to sequence method forecast people's needs more\nsuccessfully than either of the other models. This new approach shows great\npromise for enhancing disaster management activities such as evacuation\nplanning and commodity flow management.", "published": "2018-11-12 06:37:20", "link": "http://arxiv.org/abs/1811.04577v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Cinderella Complex: Word Embeddings Reveal Gender Stereotypes in\n  Movies and Books", "abstract": "Our analysis of thousands of movies and books reveals how these cultural\nproducts weave stereotypical gender roles into morality tales and perpetuate\ngender inequality through storytelling. Using the word embedding techniques, we\nreveal the constructed emotional dependency of female characters on male\ncharacters in stories.", "published": "2018-11-12 08:26:57", "link": "http://arxiv.org/abs/1811.04599v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Personalized End-to-End Goal-Oriented Dialog", "abstract": "Most existing works on dialog systems only consider conversation content\nwhile neglecting the personality of the user the bot is interacting with, which\nbegets several unsolved issues. In this paper, we present a personalized\nend-to-end model in an attempt to leverage personalization in goal-oriented\ndialogs. We first introduce a Profile Model which encodes user profiles into\ndistributed embeddings and refers to conversation history from other similar\nusers. Then a Preference Model captures user preferences over knowledge base\nentities to handle the ambiguity in user requests. The two models are combined\ninto the Personalized MemN2N. Experiments show that the proposed model achieves\nqualitative performance improvements over state-of-the-art methods. As for\nhuman evaluation, it also outperforms other approaches in terms of task\ncompletion rate and user satisfaction.", "published": "2018-11-12 08:45:25", "link": "http://arxiv.org/abs/1811.04604v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-tuning of Language Models with Discriminator", "abstract": "Cross-entropy loss is a common choice when it comes to multiclass\nclassification tasks and language modeling in particular. Minimizing this loss\nresults in language models of very good quality. We show that it is possible to\nfine-tune these models and make them perform even better if they are fine-tuned\nwith sum of cross-entropy loss and reverse Kullback-Leibler divergence. The\nlatter is estimated using discriminator network that we train in advance.\nDuring fine-tuning probabilities of rare words that are usually underestimated\nby language models become bigger. The novel approach that we propose allows us\nto reach state-of-the-art quality on Penn Treebank: perplexity decreases from\n52.4 to 52.1. Our fine-tuning algorithm is rather fast, scales well to\ndifferent architectures and datasets and requires almost no hyperparameter\ntuning: the only hyperparameter that needs to be tuned is learning rate.", "published": "2018-11-12 09:43:24", "link": "http://arxiv.org/abs/1811.04623v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Deep Ensemble Framework for Fake News Detection and Classification", "abstract": "Fake news, rumor, incorrect information, and misinformation detection are\nnowadays crucial issues as these might have serious consequences for our social\nfabrics. The rate of such information is increasing rapidly due to the\navailability of enormous web information sources including social media feeds,\nnews blogs, online newspapers etc.\n  In this paper, we develop various deep learning models for detecting fake\nnews and classifying them into the pre-defined fine-grained categories.\n  At first, we develop models based on Convolutional Neural Network (CNN) and\nBi-directional Long Short Term Memory (Bi-LSTM) networks. The representations\nobtained from these two models are fed into a Multi-layer Perceptron Model\n(MLP) for the final classification. Our experiments on a benchmark dataset show\npromising results with an overall accuracy of 44.87\\%, which outperforms the\ncurrent state of the art.", "published": "2018-11-12 11:40:09", "link": "http://arxiv.org/abs/1811.04670v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Classifying Patent Applications with Ensemble Methods", "abstract": "We present methods for the automatic classification of patent applications\nusing an annotated dataset provided by the organizers of the ALTA 2018 shared\ntask - Classifying Patent Applications. The goal of the task is to use\ncomputational methods to categorize patent applications according to a\ncoarse-grained taxonomy of eight classes based on the International Patent\nClassification (IPC). We tested a variety of approaches for this task and the\nbest results, 0.778 micro-averaged F1-Score, were achieved by SVM ensembles\nusing a combination of words and characters as features. Our team, BMZ, was\nranked first among 14 teams in the competition.", "published": "2018-11-12 12:49:42", "link": "http://arxiv.org/abs/1811.04695v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CUNI System for the WMT18 Multimodal Translation Task", "abstract": "We present our submission to the WMT18 Multimodal Translation Task. The main\nfeature of our submission is applying a self-attentive network instead of a\nrecurrent neural network. We evaluate two methods of incorporating the visual\nfeatures in the model: first, we include the image representation as another\ninput to the network; second, we train the model to predict the visual features\nand use it as an auxiliary objective. For our submission, we acquired both\ntextual and multimodal additional data. Both of the proposed methods yield\nsignificant improvements over recurrent networks and self-attentive textual\nbaselines.", "published": "2018-11-12 12:52:03", "link": "http://arxiv.org/abs/1811.04697v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing deep CNN-based utterance embeddings for acoustic model\n  adaptation", "abstract": "We explore why deep convolutional neural networks (CNNs) with small\ntwo-dimensional kernels, primarily used for modeling spatial relations in\nimages, are also effective in speech recognition. We analyze the\nrepresentations learned by deep CNNs and compare them with deep neural network\n(DNN) representations and i-vectors, in the context of acoustic model\nadaptation. To explore whether interpretable information can be decoded from\nthe learned representations we evaluate their ability to discriminate between\nspeakers, acoustic conditions, noise type, and gender using the Aurora-4\ndataset. We extract both whole model embeddings (to capture the information\nlearned across the whole network) and layer-specific embeddings which enable\nunderstanding of the flow of information across the network. We also use\nlearned representations as the additional input for a time-delay neural network\n(TDNN) for the Aurora-4 and MGB-3 English datasets. We find that deep CNN\nembeddings outperform DNN embeddings for acoustic model adaptation and\nauxiliary features based on deep CNN embeddings result in similar word error\nrates to i-vectors.", "published": "2018-11-12 13:10:25", "link": "http://arxiv.org/abs/1811.04708v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Input Combination Strategies for Multi-Source Transformer Decoder", "abstract": "In multi-source sequence-to-sequence tasks, the attention mechanism can be\nmodeled in several ways. This topic has been thoroughly studied on recurrent\narchitectures. In this paper, we extend the previous work to the\nencoder-decoder attention in the Transformer architecture. We propose four\ndifferent input combination strategies for the encoder-decoder attention:\nserial, parallel, flat, and hierarchical. We evaluate our methods on tasks of\nmultimodal translation and translation with multiple source languages. The\nexperiments show that the models are able to use multiple sources and improve\nover single source baselines.", "published": "2018-11-12 13:33:35", "link": "http://arxiv.org/abs/1811.04716v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Non-Autoregressive Neural Machine Translation with\n  Connectionist Temporal Classification", "abstract": "Autoregressive decoding is the only part of sequence-to-sequence models that\nprevents them from massive parallelization at inference time.\nNon-autoregressive models enable the decoder to generate all output symbols\nindependently in parallel. We present a novel non-autoregressive architecture\nbased on connectionist temporal classification and evaluate it on the task of\nneural machine translation. Unlike other non-autoregressive methods which\noperate in several steps, our model can be trained end-to-end. We conduct\nexperiments on the WMT English-Romanian and English-German datasets. Our models\nachieve a significant speedup over the autoregressive models, keeping the\ntranslation quality comparable to other non-autoregressive models.", "published": "2018-11-12 13:40:04", "link": "http://arxiv.org/abs/1811.04719v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntax Helps ELMo Understand Semantics: Is Syntax Still Relevant in a\n  Deep Neural Architecture for SRL?", "abstract": "Do unsupervised methods for learning rich, contextualized token\nrepresentations obviate the need for explicit modeling of linguistic structure\nin neural network models for semantic role labeling (SRL)? We address this\nquestion by incorporating the massively successful ELMo embeddings (Peters et\nal., 2018) into LISA (Strubell et al., 2018), a strong, linguistically-informed\nneural network architecture for SRL. In experiments on the CoNLL-2005 shared\ntask we find that though ELMo out-performs typical word embeddings, beginning\nto close the gap in F1 between LISA with predicted and gold syntactic parses,\nsyntactically-informed models still out-perform syntax-free models when both\nuse ELMo, especially on out-of-domain data. Our results suggest that linguistic\nstructures are indeed still relevant in this golden age of deep learning for\nNLP.", "published": "2018-11-12 15:16:12", "link": "http://arxiv.org/abs/1811.04773v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CQASUMM: Building References for Community Question Answering\n  Summarization Corpora", "abstract": "Community Question Answering forums such as Quora, Stackoverflow are rich\nknowledge resources, often catering to information on topics overlooked by\nmajor search engines. Answers submitted to these forums are often elaborated,\ncontain spam, are marred by slurs and business promotions. It is difficult for\na reader to go through numerous such answers to gauge community opinion. As a\nresult summarization becomes a prioritized task for CQA forums. While a number\nof efforts have been made to summarize factoid CQA, little work exists in\nsummarizing non-factoid CQA. We believe this is due to the lack of a\nconsiderably large, annotated dataset for CQA summarization. We create CQASUMM,\nthe first huge annotated CQA summarization dataset by filtering the 4.4 million\nYahoo! Answers L6 dataset. We sample threads where the best answer can double\nup as a reference summary and build hundred word summaries from them. We treat\nother answers as candidates documents for summarization. We provide a script to\ngenerate the dataset and introduce the new task of Community Question Answering\nSummarization. Multi document summarization has been widely studied with news\narticle datasets, especially in the DUC and TAC challenges using news corpora.\nHowever documents in CQA have higher variance, contradicting opinion and lesser\namount of overlap. We compare the popular multi document summarization\ntechniques and evaluate their performance on our CQA corpora. We look into the\nstate-of-the-art and understand the cases where existing multi document\nsummarizers (MDS) fail. We find that most MDS workflows are built for the\nentirely factual news corpora, whereas our corpus has a fair share of opinion\nbased instances too. We therefore introduce OpinioSumm, a new MDS which\noutperforms the best baseline by 4.6% w.r.t ROUGE-1 score.", "published": "2018-11-12 18:03:45", "link": "http://arxiv.org/abs/1811.04884v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-encoder multi-resolution framework for end-to-end speech\n  recognition", "abstract": "Attention-based methods and Connectionist Temporal Classification (CTC)\nnetwork have been promising research directions for end-to-end Automatic Speech\nRecognition (ASR). The joint CTC/Attention model has achieved great success by\nutilizing both architectures during multi-task training and joint decoding. In\nthis work, we present a novel Multi-Encoder Multi-Resolution (MEMR) framework\nbased on the joint CTC/Attention model. Two heterogeneous encoders with\ndifferent architectures, temporal resolutions and separate CTC networks work in\nparallel to extract complimentary acoustic information. A hierarchical\nattention mechanism is then used to combine the encoder-level information. To\ndemonstrate the effectiveness of the proposed model, experiments are conducted\non Wall Street Journal (WSJ) and CHiME-4, resulting in relative Word Error Rate\n(WER) reduction of 18.0-32.1%. Moreover, the proposed MEMR model achieves 3.6%\nWER in the WSJ eval92 test set, which is the best WER reported for an\nend-to-end system on this benchmark.", "published": "2018-11-12 18:33:21", "link": "http://arxiv.org/abs/1811.04897v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improved Dynamic Memory Network for Dialogue Act Classification with\n  Adversarial Training", "abstract": "Dialogue Act (DA) classification is a challenging problem in dialogue\ninterpretation, which aims to attach semantic labels to utterances and\ncharacterize the speaker's intention. Currently, many existing approaches\nformulate the DA classification problem ranging from multi-classification to\nstructured prediction, which suffer from two limitations: a) these methods are\neither handcrafted feature-based or have limited memories. b) adversarial\nexamples can't be correctly classified by traditional training methods. To\naddress these issues, in this paper we first cast the problem into a question\nand answering problem and proposed an improved dynamic memory networks with\nhierarchical pyramidal utterance encoder. Moreover, we apply adversarial\ntraining to train our proposed model. We evaluate our model on two public\ndatasets, i.e., Switchboard dialogue act corpus and the MapTask corpus.\nExtensive experiments show that our proposed model is not only robust, but also\nachieves better performance when compared with some state-of-the-art baselines.", "published": "2018-11-12 22:05:25", "link": "http://arxiv.org/abs/1811.05021v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Differentiating Concepts and Instances for Knowledge Graph Embedding", "abstract": "Concepts, which represent a group of different instances sharing common\nproperties, are essential information in knowledge representation. Most\nconventional knowledge embedding methods encode both entities (concepts and\ninstances) and relations as vectors in a low dimensional semantic space\nequally, ignoring the difference between concepts and instances. In this paper,\nwe propose a novel knowledge graph embedding model named TransC by\ndifferentiating concepts and instances. Specifically, TransC encodes each\nconcept in knowledge graph as a sphere and each instance as a vector in the\nsame semantic space. We use the relative positions to model the relations\nbetween concepts and instances (i.e., instanceOf), and the relations between\nconcepts and sub-concepts (i.e., subClassOf). We evaluate our model on both\nlink prediction and triple classification tasks on the dataset based on YAGO.\nExperimental results show that TransC outperforms state-of-the-art methods, and\ncaptures the semantic transitivity for instanceOf and subClassOf relation. Our\ncodes and datasets can be obtained from https:// github.com/davidlvxin/TransC.", "published": "2018-11-12 07:09:36", "link": "http://arxiv.org/abs/1811.04588v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Not Just Depressed: Bipolar Disorder Prediction on Reddit", "abstract": "Bipolar disorder, an illness characterized by manic and depressive episodes,\naffects more than 60 million people worldwide. We present a preliminary study\non bipolar disorder prediction from user-generated text on Reddit, which relies\non users' self-reported labels. Our benchmark classifiers for bipolar disorder\nprediction outperform the baselines and reach accuracy and F1-scores of above\n86%. Feature analysis shows interesting differences in language use between\nusers with bipolar disorders and the control group, including differences in\nthe use of emotion-expressive words.", "published": "2018-11-12 10:55:11", "link": "http://arxiv.org/abs/1811.04655v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Vectorization of hypotheses and speech for faster beam search in encoder\n  decoder-based speech recognition", "abstract": "Attention-based encoder decoder network uses a left-to-right beam search\nalgorithm in the inference step. The current beam search expands hypotheses and\ntraverses the expanded hypotheses at the next time step. This traversal is\nimplemented using a for-loop program in general, and it leads to speed down of\nthe recognition process. In this paper, we propose a parallelism technique for\nbeam search, which accelerates the search process by vectorizing multiple\nhypotheses to eliminate the for-loop program. We also propose a technique to\nbatch multiple speech utterances for off-line recognition use, which reduces\nthe for-loop program with regard to the traverse of multiple utterances. This\nextension is not trivial during beam search unlike during training due to\nseveral pruning and thresholding techniques for efficient decoding. In\naddition, our method can combine scores of external modules, RNNLM and CTC, in\na batch as shallow fusion. We achieved 3.7 x speedup compared with the original\nbeam search algorithm by vectoring hypotheses, and achieved 10.5 x speedup by\nfurther changing processing unit to GPU.", "published": "2018-11-12 06:02:19", "link": "http://arxiv.org/abs/1811.04568v1", "categories": ["cs.SD", "cs.CL", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Bio-YODIE: A Named Entity Linking System for Biomedical Text", "abstract": "Ever-expanding volumes of biomedical text require automated semantic\nannotation techniques to curate and put to best use. An established field of\nresearch seeks to link mentions in text to knowledge bases such as those\nincluded in the UMLS (Unified Medical Language System), in order to enable a\nmore sophisticated understanding. This work has yielded good results for tasks\nsuch as curating literature, but increasingly, annotation systems are more\nbroadly applied. Medical vocabularies are expanding in size, and with them the\nextent of term ambiguity. Document collections are increasing in size and\ncomplexity, creating a greater need for speed and robustness. Furthermore, as\nthe technologies are turned to new tasks, requirements change; for example\ngreater coverage of expressions may be required in order to annotate patient\nrecords, and greater accuracy may be needed for applications that affect\npatients. This places new demands on the approaches currently in use. In this\nwork, we present a new system, Bio-YODIE, and compare it to two other popular\nsystems in order to give guidance about suitable approaches in different\nscenarios and how systems might be designed to accommodate future needs.", "published": "2018-11-12 17:06:53", "link": "http://arxiv.org/abs/1811.04860v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "When do Words Matter? Understanding the Impact of Lexical Choice on\n  Audience Perception using Individual Treatment Effect Estimation", "abstract": "Studies across many disciplines have shown that lexical choice can affect\naudience perception. For example, how users describe themselves in a social\nmedia profile can affect their perceived socio-economic status. However, we\nlack general methods for estimating the causal effect of lexical choice on the\nperception of a specific sentence. While randomized controlled trials may\nprovide good estimates, they do not scale to the potentially millions of\ncomparisons necessary to consider all lexical choices. Instead, in this paper,\nwe first offer two classes of methods to estimate the effect on perception of\nchanging one word to another in a given sentence. The first class of algorithms\nbuilds upon quasi-experimental designs to estimate individual treatment effects\nfrom observational data. The second class treats treatment effect estimation as\na classification problem. We conduct experiments with three data sources (Yelp,\nTwitter, and Airbnb), finding that the algorithmic estimates align well with\nthose produced by randomized-control trials. Additionally, we find that it is\npossible to transfer treatment effect classifiers across domains and still\nmaintain high accuracy.", "published": "2018-11-12 18:13:40", "link": "http://arxiv.org/abs/1811.04890v4", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Stream attention-based multi-array end-to-end speech recognition", "abstract": "Automatic Speech Recognition (ASR) using multiple microphone arrays has\nachieved great success in the far-field robustness. Taking advantage of all the\ninformation that each array shares and contributes is crucial in this task.\nMotivated by the advances of joint Connectionist Temporal Classification\n(CTC)/attention mechanism in the End-to-End (E2E) ASR, a stream attention-based\nmulti-array framework is proposed in this work. Microphone arrays, acting as\ninformation streams, are activated by separate encoders and decoded under the\ninstruction of both CTC and attention networks. In terms of attention, a\nhierarchical structure is adopted. On top of the regular attention networks,\nstream attention is introduced to steer the decoder toward the most informative\nencoders. Experiments have been conducted on AMI and DIRHA multi-array corpora\nusing the encoder-decoder architecture. Compared with the best single-array\nresults, the proposed framework has achieved relative Word Error Rates (WERs)\nreduction of 3.7% and 9.7% in the two datasets, respectively, which is better\nthan conventional strategies as well.", "published": "2018-11-12 18:42:33", "link": "http://arxiv.org/abs/1811.04903v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Unseen Word Representation by Aligning Heterogeneous Lexical Semantic\n  Spaces", "abstract": "Word embedding techniques heavily rely on the abundance of training data for\nindividual words. Given the Zipfian distribution of words in natural language\ntexts, a large number of words do not usually appear frequently or at all in\nthe training data. In this paper we put forward a technique that exploits the\nknowledge encoded in lexical resources, such as WordNet, to induce embeddings\nfor unseen words. Our approach adapts graph embedding and cross-lingual vector\nspace transformation techniques in order to merge lexical knowledge encoded in\nontologies with that derived from corpus statistics. We show that the approach\ncan provide consistent performance improvements across multiple evaluation\nbenchmarks: in-vitro, on multiple rare word similarity datasets, and in-vivo,\nin two downstream text classification tasks.", "published": "2018-11-12 20:02:00", "link": "http://arxiv.org/abs/1811.04983v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Blindfold Baselines for Embodied QA", "abstract": "We explore blindfold (question-only) baselines for Embodied Question\nAnswering. The EmbodiedQA task requires an agent to answer a question by\nintelligently navigating in a simulated environment, gathering necessary visual\ninformation only through first-person vision before finally answering.\nConsequently, a blindfold baseline which ignores the environment and visual\ninformation is a degenerate solution, yet we show through our experiments on\nthe EQAv1 dataset that a simple question-only baseline achieves\nstate-of-the-art results on the EmbodiedQA task in all cases except when the\nagent is spawned extremely close to the object.", "published": "2018-11-12 21:45:41", "link": "http://arxiv.org/abs/1811.05013v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "An Introductory Survey on Attention Mechanisms in NLP Problems", "abstract": "First derived from human intuition, later adapted to machine translation for\nautomatic token alignment, attention mechanism, a simple method that can be\nused for encoding sequence data based on the importance score each element is\nassigned, has been widely applied to and attained significant improvement in\nvarious tasks in natural language processing, including sentiment\nclassification, text summarization, question answering, dependency parsing,\netc. In this paper, we survey through recent works and conduct an introductory\nsummary of the attention mechanism in different NLP problems, aiming to provide\nour readers with basic knowledge on this widely used method, discuss its\ndifferent variants for different tasks, explore its association with other\ntechniques in machine learning, and examine methods for evaluating its\nperformance.", "published": "2018-11-12 16:19:22", "link": "http://arxiv.org/abs/1811.05544v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
