{"title": "Deep Neural Networks for Czech Multi-label Document Classification", "abstract": "This paper is focused on automatic multi-label document classification of\nCzech text documents. The current approaches usually use some pre-processing\nwhich can have negative impact (loss of information, additional implementation\nwork, etc). Therefore, we would like to omit it and use deep neural networks\nthat learn from simple features. This choice was motivated by their successful\nusage in many other machine learning fields. Two different networks are\ncompared: the first one is a standard multi-layer perceptron, while the second\none is a popular convolutional network. The experiments on a Czech newspaper\ncorpus show that both networks significantly outperform baseline method which\nuses a rich set of features with maximum entropy classifier. We have also shown\nthat convolutional network gives the best results.", "published": "2017-01-13 23:23:12", "link": "http://arxiv.org/abs/1701.03849v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Transfer Learning Schemes for Personalized Language Modeling\n  using Recurrent Neural Network", "abstract": "In this paper, we propose an efficient transfer leaning methods for training\na personalized language model using a recurrent neural network with long\nshort-term memory architecture. With our proposed fast transfer learning\nschemes, a general language model is updated to a personalized language model\nwith a small amount of user data and a limited computing resource. These\nmethods are especially useful for a mobile device environment while the data is\nprevented from transferring out of the device for privacy purposes. Through\nexperiments on dialogue data in a drama, it is verified that our transfer\nlearning methods have successfully generated the personalized language model,\nwhose output is more similar to the personal language style in both qualitative\nand quantitative aspects.", "published": "2017-01-13 07:26:00", "link": "http://arxiv.org/abs/1701.03578v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LIDE: Language Identification from Text Documents", "abstract": "The increase in the use of microblogging came along with the rapid growth on\nshort linguistic data. On the other hand deep learning is considered to be the\nnew frontier to extract meaningful information out of large amount of raw data\nin an automated manner. In this study, we engaged these two emerging fields to\ncome up with a robust language identifier on demand, namely Language\nIdentification Engine (LIDE). As a result, we achieved 95.12% accuracy in\nDiscriminating between Similar Languages (DSL) Shared Task 2015 dataset, which\nis comparable to the maximum reported accuracy of 95.54% achieved so far.", "published": "2017-01-13 14:20:06", "link": "http://arxiv.org/abs/1701.03682v1", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Kernel Approximation Methods for Speech Recognition", "abstract": "We study large-scale kernel methods for acoustic modeling in speech\nrecognition and compare their performance to deep neural networks (DNNs). We\nperform experiments on four speech recognition datasets, including the TIMIT\nand Broadcast News benchmark tasks, and compare these two types of models on\nframe-level performance metrics (accuracy, cross-entropy), as well as on\nrecognition metrics (word/character error rate). In order to scale kernel\nmethods to these large datasets, we use the random Fourier feature method of\nRahimi and Recht (2007). We propose two novel techniques for improving the\nperformance of kernel acoustic models. First, in order to reduce the number of\nrandom features required by kernel models, we propose a simple but effective\nmethod for feature selection. The method is able to explore a large number of\nnon-linear features while maintaining a compact model more efficiently than\nexisting approaches. Second, we present a number of frame-level metrics which\ncorrelate very strongly with recognition performance when computed on the\nheldout set; we take advantage of these correlations by monitoring these\nmetrics during training in order to decide when to stop learning. This\ntechnique can noticeably improve the recognition performance of both DNN and\nkernel models, while narrowing the gap between them. Additionally, we show that\nthe linear bottleneck method of Sainath et al. (2013) improves the performance\nof our kernel models significantly, in addition to speeding up training and\nmaking the models more compact. Together, these three methods dramatically\nimprove the performance of kernel acoustic models, making their performance\ncomparable to DNNs on the tasks we explored.", "published": "2017-01-13 07:24:18", "link": "http://arxiv.org/abs/1701.03577v1", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "End-to-End ASR-free Keyword Search from Speech", "abstract": "End-to-end (E2E) systems have achieved competitive results compared to\nconventional hybrid hidden Markov model (HMM)-deep neural network based\nautomatic speech recognition (ASR) systems. Such E2E systems are attractive due\nto the lack of dependence on alignments between input acoustic and output\ngrapheme or HMM state sequence during training. This paper explores the design\nof an ASR-free end-to-end system for text query-based keyword search (KWS) from\nspeech trained with minimal supervision. Our E2E KWS system consists of three\nsub-systems. The first sub-system is a recurrent neural network (RNN)-based\nacoustic auto-encoder trained to reconstruct the audio through a\nfinite-dimensional representation. The second sub-system is a character-level\nRNN language model using embeddings learned from a convolutional neural\nnetwork. Since the acoustic and text query embeddings occupy different\nrepresentation spaces, they are input to a third feed-forward neural network\nthat predicts whether the query occurs in the acoustic utterance or not. This\nE2E ASR-free KWS system performs respectably despite lacking a conventional ASR\nsystem and trains much faster.", "published": "2017-01-13 15:05:39", "link": "http://arxiv.org/abs/1701.04313v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
