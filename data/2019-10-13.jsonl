{"title": "VATEX Captioning Challenge 2019: Multi-modal Information Fusion and\n  Multi-stage Training Strategy for Video Captioning", "abstract": "Multi-modal information is essential to describe what has happened in a\nvideo. In this work, we represent videos by various appearance, motion and\naudio information guided with video topic. By following multi-stage training\nstrategy, our experiments show steady and significant improvement on the VATEX\nbenchmark. This report presents an overview and comparative analysis of our\nsystem designed for both Chinese and English tracks on VATEX Captioning\nChallenge 2019.", "published": "2019-10-13 13:54:31", "link": "http://arxiv.org/abs/1910.05752v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Progress Notes Classification and Keyword Extraction using\n  Attention-based Deep Learning Models with BERT", "abstract": "Various deep learning algorithms have been developed to analyze different\ntypes of clinical data including clinical text classification and extracting\ninformation from 'free text' and so on. However, automate the keyword\nextraction from the clinical notes is still challenging. The challenges include\ndealing with noisy clinical notes which contain various abbreviations, possible\ntypos, and unstructured sentences. The objective of this research is to\ninvestigate the attention-based deep learning models to classify the\nde-identified clinical progress notes extracted from a real-world EHR system.\nThe attention-based deep learning models can be used to interpret the models\nand understand the critical words that drive the correct or incorrect\nclassification of the clinical progress notes. The attention-based models in\nthis research are capable of presenting the human interpretable text\nclassification models. The results show that the fine-tuned BERT with the\nattention layer can achieve a high classification accuracy of 97.6%, which is\nhigher than the baseline fine-tuned BERT classification model. In this\nresearch, we also demonstrate that the attention-based models can identify\nrelevant keywords that are strongly related to the clinical progress note\ncategories.", "published": "2019-10-13 16:54:21", "link": "http://arxiv.org/abs/1910.05786v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking Exposure Bias In Language Modeling", "abstract": "Exposure bias describes the phenomenon that a language model trained under\nthe teacher forcing schema may perform poorly at the inference stage when its\npredictions are conditioned on its previous predictions unseen from the\ntraining corpus. Recently, several generative adversarial networks (GANs) and\nreinforcement learning (RL) methods have been introduced to alleviate this\nproblem. Nonetheless, a common issue in RL and GANs training is the sparsity of\nreward signals. In this paper, we adopt two simple strategies, multi-range\nreinforcing, and multi-entropy sampling, to amplify and denoise the reward\nsignal. Our model produces an improvement over competing models with regards to\nBLEU scores and road exam, a new metric we designed to measure the robustness\nagainst exposure bias in language models.", "published": "2019-10-13 23:34:04", "link": "http://arxiv.org/abs/1910.11235v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Granular Multimodal Attention Networks for Visual Dialog", "abstract": "Vision and language tasks have benefited from attention. There have been a\nnumber of different attention models proposed. However, the scale at which\nattention needs to be applied has not been well examined. Particularly, in this\nwork, we propose a new method Granular Multi-modal Attention, where we aim to\nparticularly address the question of the right granularity at which one needs\nto attend while solving the Visual Dialog task. The proposed method shows\nimprovement in both image and text attention networks. We then propose a\ngranular Multi-modal Attention network that jointly attends on the image and\ntext granules and shows the best performance. With this work, we observe that\nobtaining granular attention and doing exhaustive Multi-modal Attention appears\nto be the best way to attend while solving visual dialog.", "published": "2019-10-13 10:49:41", "link": "http://arxiv.org/abs/1910.05728v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "T-GSA: Transformer with Gaussian-weighted self-attention for speech\n  enhancement", "abstract": "Transformer neural networks (TNN) demonstrated state-of-art performance on\nmany natural language processing (NLP) tasks, replacing recurrent neural\nnetworks (RNNs), such as LSTMs or GRUs. However, TNNs did not perform well in\nspeech enhancement, whose contextual nature is different than NLP tasks, like\nmachine translation. Self-attention is a core building block of the\nTransformer, which not only enables parallelization of sequence computation,\nbut also provides the constant path length between symbols that is essential to\nlearning long-range dependencies. In this paper, we propose a Transformer with\nGaussian-weighted self-attention (T-GSA), whose attention weights are\nattenuated according to the distance between target and context symbols. The\nexperimental results show that the proposed T-GSA has significantly improved\nspeech-enhancement performance, compared to the Transformer and RNNs.", "published": "2019-10-13 23:28:07", "link": "http://arxiv.org/abs/1910.06762v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
