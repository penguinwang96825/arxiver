{"title": "Pragmatic Neural Language Modelling in Machine Translation", "abstract": "This paper presents an in-depth investigation on integrating neural language\nmodels in translation systems. Scaling neural language models is a difficult\ntask, but crucial for real-world applications. This paper evaluates the impact\non end-to-end MT quality of both new and existing scaling techniques. We show\nwhen explicitly normalising neural models is necessary and what optimisation\ntricks one should use in such scenarios. We also focus on scalable training\nalgorithms and investigate noise contrastive estimation and diagonal contexts\nas sources for further speed improvements. We explore the trade-offs between\nneural models and back-off n-gram models and find that neural models make\nstrong candidates for natural language applications in memory constrained\nenvironments, yet still lag behind traditional models in raw translation\nquality. We conclude with a set of recommendations one should follow to build a\nscalable neural language model for MT.", "published": "2014-12-22 20:08:06", "link": "http://arxiv.org/abs/1412.7119v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tailoring Word Embeddings for Bilexical Predictions: An Experimental\n  Comparison", "abstract": "We investigate the problem of inducing word embeddings that are tailored for\na particular bilexical relation. Our learning algorithm takes an existing\nlexical vector space and compresses it such that the resulting word embeddings\nare good predictors for a target bilexical relation. In experiments we show\nthat task-specific embeddings can benefit both the quality and efficiency in\nlexical prediction tasks.", "published": "2014-12-22 14:49:19", "link": "http://arxiv.org/abs/1412.7004v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Recognition using Random Indexing", "abstract": "Random Indexing is a simple implementation of Random Projections with a wide\nrange of applications. It can solve a variety of problems with good accuracy\nwithout introducing much complexity. Here we use it for identifying the\nlanguage of text samples. We present a novel method of generating language\nrepresentation vectors using letter blocks. Further, we show that the method is\neasily implemented and requires little computational power and space.\nExperiments on a number of model parameters illustrate certain properties about\nhigh dimensional sparse vector representations of data. Proof of statistically\nrelevant language vectors are shown through the extremely high success of\nvarious language recognition tasks. On a difficult data set of 21,000 short\nsentences from 21 different languages, our model performs a language\nrecognition task and achieves 97.8% accuracy, comparable to state-of-the-art\nmethods.", "published": "2014-12-22 15:34:43", "link": "http://arxiv.org/abs/1412.7026v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bayesian Optimisation for Machine Translation", "abstract": "This paper presents novel Bayesian optimisation algorithms for minimum error\nrate training of statistical machine translation systems. We explore two\nclasses of algorithms for efficiently exploring the translation space, with the\nfirst based on N-best lists and the second based on a hypergraph representation\nthat compactly represents an exponential number of translation options. Our\nalgorithms exhibit faster convergence and are capable of obtaining lower error\nrates than the existing translation model specific approaches, all within a\ngeneric Bayesian optimisation framework. Further more, we also introduce a\nrandom embedding algorithm to scale our approach to sparse high dimensional\nfeature sets.", "published": "2014-12-22 21:44:00", "link": "http://arxiv.org/abs/1412.7180v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "On Learning Vector Representations in Hierarchical Label Spaces", "abstract": "An important problem in multi-label classification is to capture label\npatterns or underlying structures that have an impact on such patterns. This\npaper addresses one such problem, namely how to exploit hierarchical structures\nover labels. We present a novel method to learn vector representations of a\nlabel space given a hierarchy of labels and label co-occurrence patterns. Our\nexperimental results demonstrate qualitatively that the proposed method is able\nto learn regularities among labels by exploiting a label hierarchy as well as\nlabel co-occurrences. It highlights the importance of the hierarchical\ninformation in order to obtain regularities which facilitate analogical\nreasoning over a label space. We also experimentally illustrate the dependency\nof the learned representations on the label hierarchy.", "published": "2014-12-22 06:12:06", "link": "http://arxiv.org/abs/1412.6881v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Joint RNN-Based Greedy Parsing and Word Composition", "abstract": "This paper introduces a greedy parser based on neural networks, which\nleverages a new compositional sub-tree representation. The greedy parser and\nthe compositional procedure are jointly trained, and tightly depends on\neach-other. The composition procedure outputs a vector representation which\nsummarizes syntactically (parsing tags) and semantically (words) sub-trees.\nComposition and tagging is achieved over continuous (word or tag)\nrepresentations, and recurrent neural networks. We reach F1 performance on par\nwith well-known existing parsers, while having the advantage of speed, thanks\nto the greedy nature of the parser. We provide a fully functional\nimplementation of the method described in this paper.", "published": "2014-12-22 15:40:31", "link": "http://arxiv.org/abs/1412.7028v4", "categories": ["cs.LG", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Diverse Embedding Neural Network Language Models", "abstract": "We propose Diverse Embedding Neural Network (DENN), a novel architecture for\nlanguage models (LMs). A DENNLM projects the input word history vector onto\nmultiple diverse low-dimensional sub-spaces instead of a single\nhigher-dimensional sub-space as in conventional feed-forward neural network\nLMs. We encourage these sub-spaces to be diverse during network training\nthrough an augmented loss function. Our language modeling experiments on the\nPenn Treebank data set show the performance benefit of using a DENNLM.", "published": "2014-12-22 17:19:56", "link": "http://arxiv.org/abs/1412.7063v5", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Efficient Exact Gradient Update for training Deep Networks with Very\n  Large Sparse Targets", "abstract": "An important class of problems involves training deep neural networks with\nsparse prediction targets of very high dimension D. These occur naturally in\ne.g. neural language models or the learning of word-embeddings, often posed as\npredicting the probability of next words among a vocabulary of size D (e.g. 200\n000). Computing the equally large, but typically non-sparse D-dimensional\noutput vector from a last hidden layer of reasonable dimension d (e.g. 500)\nincurs a prohibitive O(Dd) computational cost for each example, as does\nupdating the D x d output weight matrix and computing the gradient needed for\nbackpropagation to previous layers. While efficient handling of large sparse\nnetwork inputs is trivial, the case of large sparse targets is not, and has\nthus so far been sidestepped with approximate alternatives such as hierarchical\nsoftmax or sampling-based approximations during training. In this work we\ndevelop an original algorithmic approach which, for a family of loss functions\nthat includes squared error and spherical softmax, can compute the exact loss,\ngradient update for the output weights, and gradient for backpropagation, all\nin O(d^2) per example instead of O(Dd), remarkably without ever computing the\nD-dimensional output. The proposed algorithm yields a speedup of D/4d , i.e.\ntwo orders of magnitude for typical sizes, for that critical part of the\ncomputations that often dominates the training time in this kind of network\narchitecture.", "published": "2014-12-22 18:51:08", "link": "http://arxiv.org/abs/1412.7091v3", "categories": ["cs.NE", "cs.CL", "cs.LG"], "primary_category": "cs.NE"}
{"title": "Learning linearly separable features for speech recognition using\n  convolutional neural networks", "abstract": "Automatic speech recognition systems usually rely on spectral-based features,\nsuch as MFCC of PLP. These features are extracted based on prior knowledge such\nas, speech perception or/and speech production. Recently, convolutional neural\nnetworks have been shown to be able to estimate phoneme conditional\nprobabilities in a completely data-driven manner, i.e. using directly temporal\nraw speech signal as input. This system was shown to yield similar or better\nperformance than HMM/ANN based system on phoneme recognition task and on large\nscale continuous speech recognition task, using less parameters. Motivated by\nthese studies, we investigate the use of simple linear classifier in the\nCNN-based framework. Thus, the network learns linearly separable features from\nraw speech. We show that such system yields similar or better performance than\nMLP based system using cepstral-based features as input.", "published": "2014-12-22 19:46:01", "link": "http://arxiv.org/abs/1412.7110v6", "categories": ["cs.LG", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Reply to the commentary \"Be careful when assuming the obvious\", by P.\n  Alday", "abstract": "Here we respond to some comments by Alday concerning headedness in linguistic\ntheory and the validity of the assumptions of a mathematical model for word\norder. For brevity, we focus only on two assumptions: the unit of measurement\nof dependency length and the monotonicity of the cost of a dependency as a\nfunction of its length. We also revise the implicit psychological bias in\nAlday's comments. Notwithstanding, Alday is indicating the path for linguistic\nresearch with his unusual concerns about parsimony from multiple dimensions.", "published": "2014-12-22 22:05:06", "link": "http://arxiv.org/abs/1412.7186v2", "categories": ["cs.CL", "physics.data-an", "physics.soc-ph"], "primary_category": "cs.CL"}
