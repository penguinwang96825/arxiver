{"title": "Contextualized Rewriting for Text Summarization", "abstract": "Extractive summarization suffers from irrelevance, redundancy and\nincoherence. Existing work shows that abstractive rewriting for extractive\nsummaries can improve the conciseness and readability. These rewriting systems\nconsider extracted summaries as the only input, which is relatively focused but\ncan lose important background knowledge. In this paper, we investigate\ncontextualized rewriting, which ingests the entire original document. We\nformalize contextualized rewriting as a seq2seq problem with group alignments,\nintroducing group tag as a solution to model the alignments, identifying\nextracted summaries through content-based addressing. Results show that our\napproach significantly outperforms non-contextualized rewriting systems without\nrequiring reinforcement learning, achieving strong improvements on ROUGE scores\nupon multiple extractive summarizers.", "published": "2021-01-31 05:35:57", "link": "http://arxiv.org/abs/2102.00385v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BNLP: Natural language processing toolkit for Bengali language", "abstract": "BNLP is an open source language processing toolkit for Bengali language\nconsisting with tokenization, word embedding, POS tagging, NER tagging\nfacilities. BNLP provides pre-trained model with high accuracy to do model\nbased tokenization, embedding, POS tagging, NER tagging task for Bengali\nlanguage. BNLP pre-trained model achieves significant results in Bengali text\ntokenization, word embedding, POS tagging and NER tagging task. BNLP is using\nwidely in the Bengali research communities with 16K downloads, 119 stars and 31\nforks. BNLP is available at https://github.com/sagorbrur/bnlp.", "published": "2021-01-31 07:56:08", "link": "http://arxiv.org/abs/2102.00405v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Introduction of a novel word embedding approach based on technology\n  labels extracted from patent data", "abstract": "Diversity in patent language is growing and makes finding synonyms for\nconducting patent searches more and more challenging. In addition to that, most\napproaches for dealing with diverse patent language are based on manual search\nand human intuition. In this paper, a word embedding approach using statistical\nanalysis of human labeled data to produce accurate and language independent\nword vectors for technical terms is introduced. This paper focuses on the\nexplanation of the idea behind the statistical analysis and shows first\nqualitative results. The resulting algorithm is a development of the former\nEQMania UG (eqmania.com) and can be tested under eqalice.com until April 2021.", "published": "2021-01-31 10:37:38", "link": "http://arxiv.org/abs/2102.00425v1", "categories": ["cs.CL", "68T50", "E.1"], "primary_category": "cs.CL"}
{"title": "Extending Neural Keyword Extraction with TF-IDF tagset matching", "abstract": "Keyword extraction is the task of identifying words (or multi-word\nexpressions) that best describe a given document and serve in news portals to\nlink articles of similar topics. In this work we develop and evaluate our\nmethods on four novel data sets covering less represented, morphologically-rich\nlanguages in European news media industry (Croatian, Estonian, Latvian and\nRussian). First, we perform evaluation of two supervised neural\ntransformer-based methods (TNT-KID and BERT+BiLSTM CRF) and compare them to a\nbaseline TF-IDF based unsupervised approach. Next, we show that by combining\nthe keywords retrieved by both neural transformer based methods and extending\nthe final set of keywords with an unsupervised TF-IDF based technique, we can\ndrastically improve the recall of the system, making it appropriate to be used\nas a recommendation system in the media house environment.", "published": "2021-01-31 15:39:17", "link": "http://arxiv.org/abs/2102.00472v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Short Text Clustering with Transformers", "abstract": "Recent techniques for the task of short text clustering often rely on word\nembeddings as a transfer learning component. This paper shows that sentence\nvector representations from Transformers in conjunction with different\nclustering methods can be successfully applied to address the task.\nFurthermore, we demonstrate that the algorithm of enhancement of clustering via\niterative classification can further improve initial clustering performance\nwith different classifiers, including those based on pre-trained Transformer\nlanguage models.", "published": "2021-01-31 21:31:11", "link": "http://arxiv.org/abs/2102.00541v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Unsupervised Language-Independent Entity Disambiguation Method and\n  its Evaluation on the English and Persian Languages", "abstract": "Entity Linking is one of the essential tasks of information extraction and\nnatural language understanding. Entity linking mainly consists of two tasks:\nrecognition and disambiguation of named entities. Most studies address these\ntwo tasks separately or focus only on one of them. Moreover, most of the\nstate-of-the -art entity linking algorithms are either supervised, which have\npoor performance in the absence of annotated corpora or language-dependent,\nwhich are not appropriate for multi-lingual applications. In this paper, we\nintroduce an Unsupervised Language-Independent Entity Disambiguation (ULIED),\nwhich utilizes a novel approach to disambiguate and link named entities.\nEvaluation of ULIED on different English entity linking datasets as well as the\nonly available Persian dataset illustrates that ULIED in most of the cases\noutperforms the state-of-the-art unsupervised multi-lingual approaches.", "published": "2021-01-31 06:41:55", "link": "http://arxiv.org/abs/2102.00395v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Graph Neural Networks to Predict Customer Satisfaction Following\n  Interactions with a Corporate Call Center", "abstract": "Customer satisfaction is an important factor in creating and maintaining\nlong-term relationships with customers. Near real-time identification of\npotentially dissatisfied customers following phone calls can provide\norganizations the opportunity to take meaningful interventions and to foster\nongoing customer satisfaction and loyalty. This work describes a fully\noperational system we have developed at a large US company for predicting\ncustomer satisfaction following incoming phone calls. The system takes as an\ninput speech-to-text transcriptions of calls and predicts call satisfaction\nreported by customers on post-call surveys (scale from 1 to 10). Because of its\nordinal, subjective, and often highly-skewed nature, predicting survey scores\nis not a trivial task and presents several modeling challenges. We introduce a\ngraph neural network (GNN) approach that takes into account the comparative\nnature of the problem by considering the relative scores among batches, instead\nof only pairs of calls when training. This approach produces more accurate\npredictions than previous approaches including standard regression and\nclassification models that directly fit the survey scores with call data. Our\nproposed approach can be easily generalized to other customer satisfaction\nprediction problems.", "published": "2021-01-31 10:13:57", "link": "http://arxiv.org/abs/2102.00420v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Adversarial Contrastive Pre-training for Protein Sequences", "abstract": "Recent developments in Natural Language Processing (NLP) demonstrate that\nlarge-scale, self-supervised pre-training can be extremely beneficial for\ndownstream tasks. These ideas have been adapted to other domains, including the\nanalysis of the amino acid sequences of proteins. However, to date most\nattempts on protein sequences rely on direct masked language model style\npre-training. In this work, we design a new, adversarial pre-training method\nfor proteins, extending and specializing similar advances in NLP. We show\ncompelling results in comparison to traditional MLM pre-training, though\nfurther development is needed to ensure the gains are worth the significant\ncomputational cost.", "published": "2021-01-31 15:06:27", "link": "http://arxiv.org/abs/2102.00466v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mixup Regularized Adversarial Networks for Multi-Domain Text\n  Classification", "abstract": "Using the shared-private paradigm and adversarial training has significantly\nimproved the performances of multi-domain text classification (MDTC) models.\nHowever, there are two issues for the existing methods. First, instances from\nthe multiple domains are not sufficient for domain-invariant feature\nextraction. Second, aligning on the marginal distributions may lead to fatal\nmismatching. In this paper, we propose a mixup regularized adversarial network\n(MRAN) to address these two issues. More specifically, the domain and category\nmixup regularizations are introduced to enrich the intrinsic features in the\nshared latent space and enforce consistent predictions in-between training\ninstances such that the learned features can be more domain-invariant and\ndiscriminative. We conduct experiments on two benchmarks: The Amazon review\ndataset and the FDU-MTL dataset. Our approach on these two datasets yields\naverage accuracies of 87.64\\% and 89.0\\% respectively, outperforming all\nrelevant baselines.", "published": "2021-01-31 15:24:05", "link": "http://arxiv.org/abs/2102.00467v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TruthBot: An Automated Conversational Tool for Intent Learning, Curated\n  Information Presenting, and Fake News Alerting", "abstract": "We present TruthBot, an all-in-one multilingual conversational chatbot\ndesigned for seeking truth (trustworthy and verified information) on specific\ntopics. It helps users to obtain information specific to certain topics,\nfact-check information, and get recent news. The chatbot learns the intent of a\nquery by training a deep neural network from the data of the previous intents\nand responds appropriately when it classifies the intent in one of the classes\nabove. Each class is implemented as a separate module that uses either its own\ncurated knowledge-base or searches the web to obtain the correct information.\nThe topic of the chatbot is currently set to COVID-19. However, the bot can be\neasily customized to any topic-specific responses. Our experimental results\nshow that each module performs significantly better than its closest\ncompetitor, which is verified both quantitatively and through several\nuser-based surveys in multiple languages. TruthBot has been deployed in June\n2020 and is currently running.", "published": "2021-01-31 18:23:05", "link": "http://arxiv.org/abs/2102.00509v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Decoupling the Role of Data, Attention, and Losses in Multimodal\n  Transformers", "abstract": "Recently multimodal transformer models have gained popularity because their\nperformance on language and vision tasks suggest they learn rich\nvisual-linguistic representations. Focusing on zero-shot image retrieval tasks,\nwe study three important factors which can impact the quality of learned\nrepresentations: pretraining data, the attention mechanism, and loss functions.\nBy pretraining models on six datasets, we observe that dataset noise and\nlanguage similarity to our downstream task are important indicators of model\nperformance. Through architectural analysis, we learn that models with a\nmultimodal attention mechanism can outperform deeper models with modality\nspecific attention mechanisms. Finally, we show that successful contrastive\nlosses used in the self-supervised learning literature do not yield similar\nperformance gains when used in multimodal transformers", "published": "2021-01-31 20:36:41", "link": "http://arxiv.org/abs/2102.00529v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "An Empirical Study on the Generalization Power of Neural Representations\n  Learned via Visual Guessing Games", "abstract": "Guessing games are a prototypical instance of the \"learning by interacting\"\nparadigm. This work investigates how well an artificial agent can benefit from\nplaying guessing games when later asked to perform on novel NLP downstream\ntasks such as Visual Question Answering (VQA). We propose two ways to exploit\nplaying guessing games: 1) a supervised learning scenario in which the agent\nlearns to mimic successful guessing games and 2) a novel way for an agent to\nplay by itself, called Self-play via Iterated Experience Learning (SPIEL).\n  We evaluate the ability of both procedures to generalize: an in-domain\nevaluation shows an increased accuracy (+7.79) compared with competitors on the\nevaluation suite CompGuessWhat?!; a transfer evaluation shows improved\nperformance for VQA on the TDIUC dataset in terms of harmonic average accuracy\n(+5.31) thanks to more fine-grained object representations learned via SPIEL.", "published": "2021-01-31 10:30:48", "link": "http://arxiv.org/abs/2102.00424v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multilingual Email Zoning", "abstract": "The segmentation of emails into functional zones (also dubbed email zoning)\nis a relevant preprocessing step for most NLP tasks that deal with emails.\nHowever, despite the multilingual character of emails and their applications,\nprevious literature regarding email zoning corpora and systems was developed\nessentially for English.\n  In this paper, we analyse the existing email zoning corpora and propose a new\nmultilingual benchmark composed of 625 emails in Portuguese, Spanish and\nFrench. Moreover, we introduce OKAPI, the first multilingual email segmentation\nmodel based on a language agnostic sentence encoder. Besides generalizing well\nfor unseen languages, our model is competitive with current English benchmarks,\nand reached new state-of-the-art performances for domain adaptation tasks in\nEnglish.", "published": "2021-01-31 14:32:20", "link": "http://arxiv.org/abs/2102.00461v2", "categories": ["cs.CL", "stat.AP", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Structure-Aware Audio-to-Score Alignment using Progressively Dilated\n  Convolutional Neural Networks", "abstract": "The identification of structural differences between a music performance and\nthe score is a challenging yet integral step of audio-to-score alignment, an\nimportant subtask of music information retrieval. We present a novel method to\ndetect such differences between the score and performance for a given piece of\nmusic using progressively dilated convolutional neural networks. Our method\nincorporates varying dilation rates at different layers to capture both\nshort-term and long-term context, and can be employed successfully in the\npresence of limited annotated data. We conduct experiments on audio recordings\nof real performances that differ structurally from the score, and our results\ndemonstrate that our models outperform standard methods for structure-aware\naudio-to-score alignment.", "published": "2021-01-31 05:14:58", "link": "http://arxiv.org/abs/2102.00382v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "High Fidelity Speech Regeneration with Application to Speech Enhancement", "abstract": "Speech enhancement has seen great improvement in recent years mainly through\ncontributions in denoising, speaker separation, and dereverberation methods\nthat mostly deal with environmental effects on vocal audio. To enhance speech\nbeyond the limitations of the original signal, we take a regeneration approach,\nin which we recreate the speech from its essence, including the semi-recognized\nspeech, prosody features, and identity. We propose a wav-to-wav generative\nmodel for speech that can generate 24khz speech in a real-time manner and which\nutilizes a compact speech representation, composed of ASR and identity\nfeatures, to achieve a higher level of intelligibility. Inspired by voice\nconversion methods, we train to augment the speech characteristics while\npreserving the identity of the source using an auxiliary identity network.\nPerceptual acoustic metrics and subjective tests show that the method obtains\nvaluable improvements over recent baselines.", "published": "2021-01-31 10:54:27", "link": "http://arxiv.org/abs/2102.00429v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Boosting the Predictive Accurary of Singer Identification Using Discrete\n  Wavelet Transform For Feature Extraction", "abstract": "Facing the diversity and growth of the musical field nowadays, the search for\nprecise songs becomes more and more complex. The identity of the singer\nfacilitates this search. In this project, we focus on the problem of\nidentifying the singer by using different methods for feature extraction.\nParticularly, we introduce the Discrete Wavelet Transform (DWT) for this\npurpose. To the best of our knowledge, DWT has never been used this way before\nin the context of singer identification. This process consists of three crucial\nparts. First, the vocal signal is separated from the background music by using\nthe Robust Principal Component Analysis (RPCA). Second, features from the\nobtained vocal signal are extracted. Here, the goal is to study the performance\nof the Discrete Wavelet Transform (DWT) in comparison to the Mel Frequency\nCepstral Coefficient (MFCC) which is the most used technique in audio signals.\nFinally, we proceed with the identification of the singer where two methods\nhave experimented: the Support Vector Machine (SVM), and the Gaussian Mixture\nModel (GMM). We conclude that, for a dataset of 4 singers and 200 songs, the\nbest identification system consists of the DWT (db4) feature extraction\nintroduced in this work combined with a linear support vector machine for\nidentification resulting in a mean accuracy of 83.96%.", "published": "2021-01-31 21:58:55", "link": "http://arxiv.org/abs/2102.00550v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Infant Cry Classification with Graph Convolutional Networks", "abstract": "We propose an approach of graph convolutional networks for robust infant cry\nclassification. We construct non-fully connected graphs based on the\nsimilarities among the relevant nodes in both supervised and semi-supervised\nnode classification with convolutional neural networks to consider the\nshort-term and long-term effects of infant cry signals related to inner-class\nand inter-class messages. The approach captures the diversity of variations\nwithin infant cries, especially for limited training samples. The effectiveness\nof this approach is evaluated on Baby Chillanto Database and Baby2020 database.\nWith as limited as 20% of labeled training data, our model outperforms that of\nCNN model with 80% labeled training data and the accuracy stably improves as\nthe number of labeled training samples increases. The best results give\nsignificant improvements of 7.36% and 3.59% compared with the results of the\nCNN models on Baby Chillanto database and Baby2020 database respectively.", "published": "2021-01-31 19:23:19", "link": "http://arxiv.org/abs/2102.02909v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
