{"title": "Alpha Mining and Enhancing via Warm Start Genetic Programming for Quantitative Investment", "abstract": "Traditional genetic programming (GP) often struggles in stock alpha factor\ndiscovery due to its vast search space, overwhelming computational burden, and\nsporadic effective alphas. We find that GP performs better when focusing on\npromising regions rather than random searching. This paper proposes a new GP\nframework with carefully chosen initialization and structural constraints to\nenhance search performance and improve the interpretability of the alpha\nfactors. This approach is motivated by and mimics the alpha searching practice\nand aims to boost the efficiency of such a process. Analysis of 2020-2024\nChinese stock market data shows that our method yields superior out-of-sample\nprediction results and higher portfolio returns than the benchmark.", "published": "2024-12-01 17:13:54", "link": "http://arxiv.org/abs/2412.00896v1", "categories": ["q-fin.ST", "cs.NE", "q-fin.CP"], "primary_category": "q-fin.ST"}
{"title": "A model of strategic sustainable investment", "abstract": "We study a problem of optimal irreversible investment and emission reduction\nformulated as a nonzero-sum dynamic game between an investor with environmental\npreferences and a firm. The game is set in continuous time on an infinite-time\nhorizon. The firm generates profits with a stochastic dynamics and may spend\npart of its revenues towards emission reduction (e.g., renovating the\ninfrastructure). The firm's objective is to maximize the discounted expectation\nof a function of its profits. The investor participates in the profits and may\ndecide to invest to support the firm's production capacity. The investor uses a\nprofit function which accounts for both financial and environmental factors.\nNash equilibria of the game are obtained via a system of variational\ninequalities. We formulate a general verification theorem for this system in a\ndiffusive setup and construct an explicit solution in the zero-noise limit. Our\nexplicit results and numerical approximations show that both the investor's and\nthe firm's optimal actions are triggered by moving boundaries that increase\nwith the total amount of emission abatement.", "published": "2024-12-01 22:27:00", "link": "http://arxiv.org/abs/2412.00986v1", "categories": ["q-fin.MF", "math.OC", "93E20, 91A15, 49N90, 65K15"], "primary_category": "q-fin.MF"}
{"title": "Probabilistic Predictions of Option Prices Using Multiple Sources of Data", "abstract": "A new modular approximate Bayesian inferential framework is proposed that\nenables fast calculation of probabilistic predictions of future option prices.\nWe exploit multiple information sources, including daily spot returns,\nhigh-frequency spot data and option prices. A benefit of this modular Bayesian\napproach is that it allows us to work with the theoretical option pricing\nmodel, without needing to specify an arbitrary statistical model that links the\ntheoretical prices to their observed counterparts. We show that our approach\nproduces accurate probabilistic predictions of option prices in realistic\nscenarios and, despite not explicitly modelling pricing errors, the method is\nshown to be robust to their presence. Predictive accuracy based on the Heston\nstochastic volatility model, with predictions produced via rapid real-time\nupdates, is illustrated empirically for short-maturity options.", "published": "2024-12-01 03:40:38", "link": "http://arxiv.org/abs/2412.00658v1", "categories": ["q-fin.ST", "stat.CO", "stat.ME"], "primary_category": "q-fin.ST"}
{"title": "Multi-View Incongruity Learning for Multimodal Sarcasm Detection", "abstract": "Multimodal sarcasm detection (MSD) is essential for various downstream tasks.\nExisting MSD methods tend to rely on spurious correlations. These methods often\nmistakenly prioritize non-essential features yet still make correct\npredictions, demonstrating poor generalizability beyond training environments.\nRegarding this phenomenon, this paper undertakes several initiatives. Firstly,\nwe identify two primary causes that lead to the reliance of spurious\ncorrelations. Secondly, we address these challenges by proposing a novel method\nthat integrate Multimodal Incongruities via Contrastive Learning (MICL) for\nmultimodal sarcasm detection. Specifically, we first leverage incongruity to\ndrive multi-view learning from three views: token-patch, entity-object, and\nsentiment. Then, we introduce extensive data augmentation to mitigate the\nbiased learning of the textual modality. Additionally, we construct a test set,\nSPMSD, which consists potential spurious correlations to evaluate the the\nmodel's generalizability. Experimental results demonstrate the superiority of\nMICL on benchmark datasets, along with the analyses showcasing MICL's\nadvancement in mitigating the effect of spurious correlation.", "published": "2024-12-01 10:29:36", "link": "http://arxiv.org/abs/2412.00756v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "K-UD: Revising Korean Universal Dependencies Guidelines", "abstract": "Critique has surfaced concerning the existing linguistic annotation framework\nfor Korean Universal Dependencies (UDs), particularly in relation to syntactic\nrelationships. In this paper, our primary objective is to refine the definition\nof syntactic dependency of UDs within the context of analyzing the Korean\nlanguage. Our aim is not only to achieve a consensus within UDs but also to\ngarner agreement beyond the UD framework for analyzing Korean sentences using\ndependency structure, by establishing a linguistic consensus model.", "published": "2024-12-01 15:41:05", "link": "http://arxiv.org/abs/2412.00856v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uhura: A Benchmark for Evaluating Scientific Question Answering and\n  Truthfulness in Low-Resource African Languages", "abstract": "Evaluations of Large Language Models (LLMs) on knowledge-intensive tasks and\nfactual accuracy often focus on high-resource languages primarily because\ndatasets for low-resource languages (LRLs) are scarce. In this paper, we\npresent Uhura -- a new benchmark that focuses on two tasks in six\ntypologically-diverse African languages, created via human translation of\nexisting English benchmarks. The first dataset, Uhura-ARC-Easy, is composed of\nmultiple-choice science questions. The second, Uhura-TruthfulQA, is a safety\nbenchmark testing the truthfulness of models on topics including health, law,\nfinance, and politics. We highlight the challenges creating benchmarks with\nhighly technical content for LRLs and outline mitigation strategies. Our\nevaluation reveals a significant performance gap between proprietary models\nsuch as GPT-4o and o1-preview, and Claude models, and open-source models like\nMeta's LLaMA and Google's Gemma. Additionally, all models perform better in\nEnglish than in African languages. These results indicate that LMs struggle\nwith answering scientific questions and are more prone to generating false\nclaims in low-resource African languages. Our findings underscore the necessity\nfor continuous improvement of multilingual LM capabilities in LRL settings to\nensure safe and reliable use in real-world contexts. We open-source the Uhura\nBenchmark and Uhura Platform to foster further research and development in NLP\nfor LRLs.", "published": "2024-12-01 19:46:40", "link": "http://arxiv.org/abs/2412.00948v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Agent Collaboration in Incident Response with Large Language\n  Models", "abstract": "Incident response (IR) is a critical aspect of cybersecurity, requiring rapid\ndecision-making and coordinated efforts to address cyberattacks effectively.\nLeveraging large language models (LLMs) as intelligent agents offers a novel\napproach to enhancing collaboration and efficiency in IR scenarios. This paper\nexplores the application of LLM-based multi-agent collaboration using the\nBackdoors & Breaches framework, a tabletop game designed for cybersecurity\ntraining. We simulate real-world IR dynamics through various team structures,\nincluding centralized, decentralized, and hybrid configurations. By analyzing\nagent interactions and performance across these setups, we provide insights\ninto optimizing multi-agent collaboration for incident response. Our findings\nhighlight the potential of LLMs to enhance decision-making, improve\nadaptability, and streamline IR processes, paving the way for more effective\nand coordinated responses to cyber threats.", "published": "2024-12-01 03:12:26", "link": "http://arxiv.org/abs/2412.00652v2", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Towards Adaptive Mechanism Activation in Language Agent", "abstract": "Language Agent could be endowed with different mechanisms for autonomous task\naccomplishment. Current agents typically rely on fixed mechanisms or a set of\nmechanisms activated in a predefined order, limiting their adaptation to varied\npotential task solution structures. To this end, this paper proposes\n\\textbf{A}daptive \\textbf{L}anguage \\textbf{A}gent \\textbf{M}echanism\n\\textbf{A}ctivation Learning with Self-Exploration (\\textbf{ALAMA}), which\nfocuses on optimizing mechanism activation adaptability without reliance on\nexpert models. Initially, it builds a harmonized agent framework\n(\\textbf{UniAct}) to \\textbf{Uni}fy different mechanisms via \\textbf{Act}ions.\nThen it leverages a training-efficient optimization method based on\nself-exploration to enable the UniAct to adaptively activate the appropriate\nmechanisms according to the potential characteristics of the task. Experimental\nresults demonstrate significant improvements in downstream agent tasks,\naffirming the effectiveness of our approach in facilitating more dynamic and\ncontext-sensitive mechanism activation.", "published": "2024-12-01 08:10:04", "link": "http://arxiv.org/abs/2412.00722v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PGSO: Prompt-based Generative Sequence Optimization Network for\n  Aspect-based Sentiment Analysis", "abstract": "Recently, generative pre-training based models have demonstrated remarkable\nresults on Aspect-based Sentiment Analysis (ABSA) task. However, previous works\noveremphasize crafting various templates to paraphrase training targets for\nenhanced decoding, ignoring the internal optimizations on generative models.\nDespite notable results achieved by these target-oriented optimization methods,\nthey struggle with the complicated long texts since the implicit long-distance\nrelation, e.g., aspect-opinion relation, is difficult to extract under the\nposition embedding mechanism in generative models. Thus, in this paper, we\nfirst clarify the causes of the problem and introduce two sequence optimization\nstrategies: the rule-based static optimization and the score-based dynamic\noptimization. The rule-based approach relies on handcraft priority of\ndependency relation to reorder the context, while the score-based algorithm\ndynamically regulates the contextual sequence by calculating word position\nscores using neural network. Based on the dynamic optimization structure, we\nfurther propose a unified Prompt-based Generative Sequence Optimization network\n(named PGSO), which jointly optimizes the training target as well as the\ngenerative model. Specifically, PGSO contains two components, namely, prompt\nconstruction and sequence regulator. The former constructs a task-specific\nprompt based on unsupervised training objects to fully utilize the pre-trained\nmodel. The latter jointly leverages semantic, syntactic and original-sequence\ninformation to dynamically regulate contextual sequence. Our experiments\nconducted on four ABSA tasks across multiple benchmarks indicate that PGSO\noutperforms state-of-the-art methods, with an average improvement of 3.52% in\nF1 score.", "published": "2024-12-01 10:49:55", "link": "http://arxiv.org/abs/2412.00763v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SelfPrompt: Autonomously Evaluating LLM Robustness via\n  Domain-Constrained Knowledge Guidelines and Refined Adversarial Prompts", "abstract": "Traditional methods for evaluating the robustness of large language models\n(LLMs) often rely on standardized benchmarks, which can escalate costs and\nlimit evaluations across varied domains. This paper introduces a novel\nframework designed to autonomously evaluate the robustness of LLMs by\nincorporating refined adversarial prompts and domain-constrained knowledge\nguidelines in the form of knowledge graphs. Our method systematically generates\ndescriptive sentences from domain-constrained knowledge graph triplets to\nformulate adversarial prompts, enhancing the relevance and challenge of the\nevaluation. These prompts, generated by the LLM itself and tailored to evaluate\nits own robustness, undergo a rigorous filtering and refinement process,\nensuring that only those with high textual fluency and semantic fidelity are\nused. This self-evaluation mechanism allows the LLM to evaluate its robustness\nwithout the need for external benchmarks. We assess the effectiveness of our\nframework through extensive testing on both proprietary models like ChatGPT and\nopen-source models such as Llama-3.1, Phi-3, and Mistral. Results confirm that\nour approach not only reduces dependency on conventional data but also provides\na targeted and efficient means of evaluating LLM robustness in constrained\ndomains.", "published": "2024-12-01 10:58:53", "link": "http://arxiv.org/abs/2412.00765v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Examining Identity Drift in Conversations of LLM Agents", "abstract": "Large Language Models (LLMs) show impressive conversational abilities but\nsometimes show identity drift problems, where their interaction patterns or\nstyles change over time. As the problem has not been thoroughly examined yet,\nthis study examines identity consistency across nine LLMs. Specifically, we (1)\ninvestigate whether LLMs could maintain consistent patterns (or identity) and\n(2) analyze the effect of the model family, parameter sizes, and provided\npersona types. Our experiments involve multi-turn conversations on personal\nthemes, analyzed in qualitative and quantitative ways. Experimental results\nindicate three findings. (1) Larger models experience greater identity drift.\n(2) Model differences exist, but their effect is not stronger than parameter\nsizes. (3) Assigning a persona may not help to maintain identity. We hope these\nthree findings can help to improve persona stability in AI-driven dialogue\nsystems, particularly in long-term conversations.", "published": "2024-12-01 13:19:32", "link": "http://arxiv.org/abs/2412.00804v2", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "KnowledgePrompts: Exploring the Abilities of Large Language Models to\n  Solve Proportional Analogies via Knowledge-Enhanced Prompting", "abstract": "Making analogies is fundamental to cognition. Proportional analogies, which\nconsist of four terms, are often used to assess linguistic and cognitive\nabilities. For instance, completing analogies like \"Oxygen is to Gas as <blank>\nis to <blank>\" requires identifying the semantic relationship (e.g., \"type of\")\nbetween the first pair of terms (\"Oxygen\" and \"Gas\") and finding a second pair\nthat shares the same relationship (e.g., \"Aluminum\" and \"Metal\"). In this work,\nwe introduce a 15K Multiple-Choice Question Answering (MCQA) dataset for\nproportional analogy completion and evaluate the performance of contemporary\nLarge Language Models (LLMs) in various knowledge-enhanced prompt settings.\nSpecifically, we augment prompts with three types of knowledge: exemplar,\nstructured, and targeted. Our results show that despite extensive training\ndata, solving proportional analogies remains challenging for current LLMs, with\nthe best model achieving an accuracy of 55%. Notably, we find that providing\ntargeted knowledge can better assist models in completing proportional\nanalogies compared to providing exemplars or collections of structured\nknowledge. Our code and data are available at:\nhttps://github.com/Thiliniiw/KnowledgePrompts/", "published": "2024-12-01 16:15:14", "link": "http://arxiv.org/abs/2412.00869v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Lightweight Contenders: Navigating Semi-Supervised Text Mining through\n  Peer Collaboration and Self Transcendence", "abstract": "The semi-supervised learning (SSL) strategy in lightweight models requires\nreducing annotated samples and facilitating cost-effective inference. However,\nthe constraint on model parameters, imposed by the scarcity of training labels,\nlimits the SSL performance. In this paper, we introduce PS-NET, a novel\nframework tailored for semi-supervised text mining with lightweight models.\nPS-NET incorporates online distillation to train lightweight student models by\nimitating the Teacher model. It also integrates an ensemble of student peers\nthat collaboratively instruct each other. Additionally, PS-NET implements a\nconstant adversarial perturbation schema to further self-augmentation by\nprogressive generalizing. Our PS-NET, equipped with a 2-layer distilled BERT,\nexhibits notable performance enhancements over SOTA lightweight SSL frameworks\nof FLiText and DisCo in SSL text classification with extremely rare labelled\ndata.", "published": "2024-12-01 16:44:27", "link": "http://arxiv.org/abs/2412.00883v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "QABISAR: Query-Article Bipartite Interactions for Statutory Article\n  Retrieval", "abstract": "In this paper, we introduce QABISAR, a novel framework for statutory article\nretrieval, to overcome the semantic mismatch problem when modeling each\nquery-article pair in isolation, making it hard to learn representation that\ncan effectively capture multi-faceted information. QABISAR leverages bipartite\ninteractions between queries and articles to capture diverse aspects inherent\nin them. Further, we employ knowledge distillation to transfer enriched query\nrepresentations from the graph network into the query bi-encoder, to capture\nthe rich semantics present in the graph representations, despite absence of\ngraph-based supervision for unseen queries during inference. Our experiments on\na real-world expert-annotated dataset demonstrate its effectiveness.", "published": "2024-12-01 18:58:17", "link": "http://arxiv.org/abs/2412.00934v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "VisOnlyQA: Large Vision Language Models Still Struggle with Visual\n  Perception of Geometric Information", "abstract": "Large Vision Language Models (LVLMs) have achieved remarkable performance in\nvarious vision-language tasks. However, it is still unclear how accurately\nLVLMs can perceive visual information in images. In particular, the capability\nof LVLMs to perceive geometric information, such as shape, angle, and size,\nremains insufficiently analyzed, although the perception of these properties is\ncrucial for tasks that require a detailed visual understanding. In this work,\nwe introduce VisOnlyQA, a dataset for evaluating the geometric perception of\nLVLMs, and reveal that LVLMs often cannot accurately perceive basic geometric\ninformation in images, while human performance is nearly perfect. VisOnlyQA\nconsists of 12 tasks that directly ask about geometric information in geometric\nshapes, charts, chemical structures, and 3D shapes. Our experiments highlight\nthe following findings: (i) State-of-the-art LVLMs struggle with basic\ngeometric perception -- 20 LVLMs we evaluate, including GPT-4o and Gemini 1.5\nPro, work poorly on VisOnlyQA. (ii) Additional training data does not resolve\nthis issue -- fine-tuning on the training set of VisOnlyQA is not always\neffective, even for in-distribution tasks. (iii) Bottleneck in the architecture\n-- LVLMs using stronger LLMs exhibit better geometric perception on VisOnlyQA,\nwhile it does not require complex reasoning, suggesting that the way LVLMs\nprocess information from visual encoders is a bottleneck. The datasets, code,\nand model responses are provided at https://github.com/psunlpgroup/VisOnlyQA.", "published": "2024-12-01 19:46:22", "link": "http://arxiv.org/abs/2412.00947v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "From Priest to Doctor: Domain Adaptation for Low-Resource Neural Machine\n  Translation", "abstract": "Many of the world's languages have insufficient data to train high-performing\ngeneral neural machine translation (NMT) models, let alone domain-specific\nmodels, and often the only available parallel data are small amounts of\nreligious texts. Hence, domain adaptation (DA) is a crucial issue faced by\ncontemporary NMT and has, so far, been underexplored for low-resource\nlanguages. In this paper, we evaluate a set of methods from both low-resource\nNMT and DA in a realistic setting, in which we aim to translate between a\nhigh-resource and a low-resource language with access to only: a) parallel\nBible data, b) a bilingual dictionary, and c) a monolingual target-domain\ncorpus in the high-resource language. Our results show that the effectiveness\nof the tested methods varies, with the simplest one, DALI, being most\neffective. We follow up with a small human evaluation of DALI, which shows that\nthere is still a need for more careful investigation of how to accomplish DA\nfor low-resource NMT.", "published": "2024-12-01 21:06:08", "link": "http://arxiv.org/abs/2412.00966v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Competition Dynamics Shape Algorithmic Phases of In-Context Learning", "abstract": "In-Context Learning (ICL) has significantly expanded the general-purpose\nnature of large language models, allowing them to adapt to novel tasks using\nmerely the inputted context. This has motivated a series of papers that analyze\ntractable synthetic domains and postulate precise mechanisms that may underlie\nICL. However, the use of relatively distinct setups that often lack a sequence\nmodeling nature to them makes it unclear how general the reported insights from\nsuch studies are. Motivated by this, we propose a synthetic sequence modeling\ntask that involves learning to simulate a finite mixture of Markov chains. As\nwe show, models trained on this task reproduce most well-known results on ICL,\nhence offering a unified setting for studying the concept. Building on this\nsetup, we demonstrate we can explain a model's behavior by decomposing it into\nfour broad algorithms that combine a fuzzy retrieval vs. inference approach\nwith either unigram or bigram statistics of the context. These algorithms\nengage in a competition dynamics to dominate model behavior, with the precise\nexperimental conditions dictating which algorithm ends up superseding others:\ne.g., we find merely varying context size or amount of training yields (at\ntimes sharp) transitions between which algorithm dictates the model behavior,\nrevealing a mechanism that explains the transient nature of ICL. In this sense,\nwe argue ICL is best thought of as a mixture of different algorithms, each with\nits own peculiarities, instead of a monolithic capability. This also implies\nthat making general claims about ICL that hold universally across all settings\nmay be infeasible.", "published": "2024-12-01 23:35:53", "link": "http://arxiv.org/abs/2412.01003v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CoRNStack: High-Quality Contrastive Data for Better Code Retrieval and\n  Reranking", "abstract": "Effective code retrieval plays a crucial role in advancing code generation,\nbug fixing, and software maintenance, particularly as software systems increase\nin complexity. While current code embedding models have demonstrated promise in\nretrieving code snippets for small-scale, well-defined tasks, they often\nunderperform in more demanding real-world applications such as bug localization\nwithin GitHub repositories. We hypothesize that a key issue is their reliance\non noisy and inconsistent datasets for training, which impedes their ability to\ngeneralize to more complex retrieval scenarios. To address these limitations,\nwe introduce CoRNStack, a large-scale, high-quality contrastive training\ndataset for code that spans multiple programming languages. This dataset is\ncurated using consistency filtering to eliminate noisy positives and is further\nenriched with mined hard negatives, thereby facilitating more effective\nlearning. We demonstrate that contrastive training of embedding models using\nCoRNStack leads to state-of-the-art performance across a variety of code\nretrieval tasks. Furthermore, the dataset can be leveraged for training code\nreranking models, a largely underexplored area compared to text reranking. Our\nfinetuned code reranking model significantly improves the ranking quality over\nthe retrieved results. Finally, by employing our code retriever and reranker\ntogether, we demonstrate significant improvements in function localization for\nGitHub issues, an important component of real-world software development.", "published": "2024-12-01 23:54:12", "link": "http://arxiv.org/abs/2412.01007v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Text Is Not All You Need: Multimodal Prompting Helps LLMs Understand\n  Humor", "abstract": "While Large Language Models (LLMs) have demonstrated impressive natural\nlanguage understanding capabilities across various text-based tasks,\nunderstanding humor has remained a persistent challenge. Humor is frequently\nmultimodal, relying on phonetic ambiguity, rhythm and timing to convey meaning.\nIn this study, we explore a simple multimodal prompting approach to humor\nunderstanding and explanation. We present an LLM with both the text and the\nspoken form of a joke, generated using an off-the-shelf text-to-speech (TTS)\nsystem. Using multimodal cues improves the explanations of humor compared to\ntextual prompts across all tested datasets.", "published": "2024-12-01 06:49:31", "link": "http://arxiv.org/abs/2412.05315v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "ROSE: A Reward-Oriented Data Selection Framework for LLM Task-Specific\n  Instruction Tuning", "abstract": "Instruction tuning has underscored the significant potential of large\nlanguage models (LLMs) in producing more human-controllable and effective\noutputs in various domains. In this work, we focus on the data selection\nproblem for task-specific instruction tuning of LLMs. Prevailing methods\nprimarily rely on the crafted similarity metrics to select training data that\naligns with the test data distribution. The goal is to minimize instruction\ntuning loss on the test data, ultimately improving performance on the target\ntask. However, it has been widely observed that instruction tuning loss (i.e.,\ncross-entropy loss for next token prediction) in LLMs often fails to exhibit a\nmonotonic relationship with actual task performance. This misalignment\nundermines the effectiveness of current data selection methods for\ntask-specific instruction tuning. To address this issue, we introduce ROSE, a\nnovel Reward-Oriented inStruction data sElection method which leverages\npairwise preference loss as a reward signal to optimize data selection for\ntask-specific instruction tuning. Specifically, ROSE adapts an influence\nformulation to approximate the influence of training data points relative to a\nfew-shot preference validation set to select the most task-related training\ndata points. Experimental results show that by selecting just 5% of the\ntraining data using ROSE, our approach can achieve competitive results compared\nto fine-tuning with the full training dataset, and it surpasses other\nstate-of-the-art data selection methods for task-specific instruction tuning.\nOur qualitative analysis further confirms the robust generalizability of our\nmethod across multiple benchmark datasets and diverse model architectures.", "published": "2024-12-01 01:01:09", "link": "http://arxiv.org/abs/2412.00631v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Comparative Study of LLM-based ASR and Whisper in Low Resource and\n  Code Switching Scenario", "abstract": "Large Language Models (LLMs) have showcased exceptional performance across\ndiverse NLP tasks, and their integration with speech encoder is rapidly\nemerging as a dominant trend in the Automatic Speech Recognition (ASR) field.\nPrevious works mainly concentrated on leveraging LLMs for speech recognition in\nEnglish and Chinese. However, their potential for addressing speech recognition\nchallenges in low resource settings remains underexplored. Hence, in this work,\nwe aim to explore the capability of LLMs in low resource ASR and\nMandarin-English code switching ASR. We also evaluate and compare the\nrecognition performance of LLM-based ASR systems against Whisper model.\nExtensive experiments demonstrate that LLM-based ASR yields a relative gain of\n12.8\\% over the Whisper model in low resource ASR while Whisper performs better\nin Mandarin-English code switching ASR. We hope that this study could shed\nlight on ASR for low resource scenarios.", "published": "2024-12-01 08:07:01", "link": "http://arxiv.org/abs/2412.00721v2", "categories": ["cs.AI", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
{"title": "Prompt as Free Lunch: Enhancing Diversity in Source-Free Cross-domain\n  Few-shot Learning through Semantic-Guided Prompting", "abstract": "The source-free cross-domain few-shot learning (CD-FSL) task aims to transfer\npretrained models to target domains utilizing minimal samples, eliminating the\nneed for source domain data. Addressing this issue requires models to have\nrobust generalization abilities and strong feature representation, aligning\nwith the characteristics of large-scale pretrained models. However, large-scale\nmodels tend to lose representational ability in cross-domain scenarios due to\nlimited sample diversity. \\zlh{Given the abundant diversity provided by\nsemantic modality, this paper leverages textual modality to enhance training\nsample diversity with CLP model}, meanwhile improving model transfer\nefficiency. Specifically, we propose the SeGD-VPT framework, which is divided\ninto two phases. The first step aims to increase feature diversity by adding\ndiversity prompts to each support sample, thereby generating varying input and\nenhancing sample diversity. Furthermore, we use diversity descriptions of\nclasses to guide semantically meaningful learning of diversity prompts,\nproposing random combinations and selections of texts to increase textual\ndiversity. Additionally, deep prompt tuning is introduced to enhance the\nmodel's transfer capability. After training of the first step, support samples\nwith different diversity prompts are input into the CLIP backbone to generate\nenhanced features. After generation, the second phase trains classifiers using\nthe generated features. Extensive experimental results across several\nbenchmarks verify our method is comparable to SOTA source-utilized models and\nattain the best performance under the source-free CD-FSL setting.", "published": "2024-12-01 11:00:38", "link": "http://arxiv.org/abs/2412.00767v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification", "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .", "published": "2024-12-01 16:32:31", "link": "http://arxiv.org/abs/2412.00876v4", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Large Language Models as Mirrors of Societal Moral Standards", "abstract": "Prior research has demonstrated that language models can, to a limited\nextent, represent moral norms in a variety of cultural contexts. This research\naims to replicate these findings and further explore their validity,\nconcentrating on issues like 'homosexuality' and 'divorce'. This study\nevaluates the effectiveness of these models using information from two surveys,\nthe WVS and the PEW, that encompass moral perspectives from over 40 countries.\nThe results show that biases exist in both monolingual and multilingual models,\nand they typically fall short of accurately capturing the moral intricacies of\ndiverse cultures. However, the BLOOM model shows the best performance,\nexhibiting some positive correlations, but still does not achieve a\ncomprehensive moral understanding. This research underscores the limitations of\ncurrent PLMs in processing cross-cultural differences in values and highlights\nthe importance of developing culturally aware AI systems that better align with\nuniversal human values.", "published": "2024-12-01 20:20:35", "link": "http://arxiv.org/abs/2412.00956v1", "categories": ["cs.AI", "cs.CL", "cs.SC"], "primary_category": "cs.AI"}
{"title": "LLMs as mirrors of societal moral standards: reflection of cultural\n  divergence and agreement across ethical topics", "abstract": "Large language models (LLMs) have become increasingly pivotal in various\ndomains due the recent advancements in their performance capabilities. However,\nconcerns persist regarding biases in LLMs, including gender, racial, and\ncultural biases derived from their training data. These biases raise critical\nquestions about the ethical deployment and societal impact of LLMs.\nAcknowledging these concerns, this study investigates whether LLMs accurately\nreflect cross-cultural variations and similarities in moral perspectives. In\nassessing whether the chosen LLMs capture patterns of divergence and agreement\non moral topics across cultures, three main methods are employed: (1)\ncomparison of model-generated and survey-based moral score variances, (2)\ncluster alignment analysis to evaluate the correspondence between country\nclusters derived from model-generated moral scores and those derived from\nsurvey data, and (3) probing LLMs with direct comparative prompts. All three\nmethods involve the use of systematic prompts and token pairs designed to\nassess how well LLMs understand and reflect cultural variations in moral\nattitudes. The findings of this study indicate overall variable and low\nperformance in reflecting cross-cultural differences and similarities in moral\nvalues across the models tested, highlighting the necessity for improving\nmodels' accuracy in capturing these nuances effectively. The insights gained\nfrom this study aim to inform discussions on the ethical development and\ndeployment of LLMs in global contexts, emphasizing the importance of mitigating\nbiases and promoting fair representation across diverse cultural perspectives.", "published": "2024-12-01 20:39:42", "link": "http://arxiv.org/abs/2412.00962v1", "categories": ["cs.AI", "cs.CL", "cs.SC"], "primary_category": "cs.AI"}
{"title": "Large Language Models in Politics and Democracy: A Comprehensive Survey", "abstract": "The advancement of generative AI, particularly large language models (LLMs),\nhas a significant impact on politics and democracy, offering potential across\nvarious domains, including policymaking, political communication, analysis, and\ngovernance. This paper surveys the recent and potential applications of LLMs in\npolitics, examining both their promises and the associated challenges. This\npaper examines the ways in which LLMs are being employed in legislative\nprocesses, political communication, and political analysis. Moreover, we\ninvestigate the potential of LLMs in diplomatic and national security contexts,\neconomic and social modeling, and legal applications. While LLMs offer\nopportunities to enhance efficiency, inclusivity, and decision-making in\npolitical processes, they also present challenges related to bias,\ntransparency, and accountability. The paper underscores the necessity for\nresponsible development, ethical considerations, and governance frameworks to\nensure that the integration of LLMs into politics aligns with democratic values\nand promotes a more just and equitable society.", "published": "2024-12-01 15:23:34", "link": "http://arxiv.org/abs/2412.04498v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Voice Biomarker Analysis and Automated Severity Classification of\n  Dysarthric Speech in a Multilingual Context", "abstract": "Dysarthria, a motor speech disorder, severely impacts voice quality,\npronunciation, and prosody, leading to diminished speech intelligibility and\nreduced quality of life. Accurate assessment is crucial for effective\ntreatment, but traditional perceptual assessments are limited by their\nsubjectivity and resource intensity. To mitigate the limitations, automatic\ndysarthric speech assessment methods have been proposed to support clinicians\non their decision-making. While these methods have shown promising results,\nmost research has focused on monolingual environments. However, multilingual\napproaches are necessary to address the global burden of dysarthria and ensure\nequitable access to accurate diagnosis. This thesis proposes a novel\nmultilingual dysarthria severity classification method, by analyzing three\nlanguages: English, Korean, and Tamil.", "published": "2024-12-01 00:05:00", "link": "http://arxiv.org/abs/2412.12111v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automating Feedback Analysis in Surgical Training: Detection,\n  Categorization, and Assessment", "abstract": "This work introduces the first framework for reconstructing surgical dialogue\nfrom unstructured real-world recordings, which is crucial for characterizing\nteaching tasks. In surgical training, the formative verbal feedback that\ntrainers provide to trainees during live surgeries is crucial for ensuring\nsafety, correcting behavior immediately, and facilitating long-term skill\nacquisition. However, analyzing and quantifying this feedback is challenging\ndue to its unstructured and specialized nature. Automated systems are essential\nto manage these complexities at scale, allowing for the creation of structured\ndatasets that enhance feedback analysis and improve surgical education. Our\nframework integrates voice activity detection, speaker diarization, and\nautomated speech recaognition, with a novel enhancement that 1) removes\nhallucinations (non-existent utterances generated during speech recognition\nfueled by noise in the operating room) and 2) separates speech from trainers\nand trainees using few-shot voice samples. These aspects are vital for\nreconstructing accurate surgical dialogues and understanding the roles of\noperating room participants. Using data from 33 real-world surgeries, we\ndemonstrated the system's capability to reconstruct surgical teaching dialogues\nand detect feedback instances effectively (F1 score of 0.79+/-0.07). Moreover,\nour hallucination removal step improves feedback detection performance by ~14%.\nEvaluation on downstream clinically relevant tasks of predicting Behavioral\nAdjustment of trainees and classifying Technical feedback, showed performances\ncomparable to manual annotations with F1 scores of 0.82+/0.03 and 0.81+/0.03\nrespectively. These results highlight the effectiveness of our framework in\nsupporting clinically relevant tasks and improving over manual methods.", "published": "2024-12-01 10:35:12", "link": "http://arxiv.org/abs/2412.00760v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.ET", "cs.LG", "68T50, 68U99, 68T99", "I.2; I.2.7; I.5.4; J.3; K.3.1"], "primary_category": "eess.AS"}
{"title": "Quantifying perturbation impacts for large language models", "abstract": "We consider the problem of quantifying how an input perturbation impacts the\noutputs of large language models (LLMs), a fundamental task for model\nreliability and post-hoc interpretability. A key obstacle in this domain is\ndisentangling the meaningful changes in model responses from the intrinsic\nstochasticity of LLM outputs. To overcome this, we introduce Distribution-Based\nPerturbation Analysis (DBPA), a framework that reformulates LLM perturbation\nanalysis as a frequentist hypothesis testing problem. DBPA constructs empirical\nnull and alternative output distributions within a low-dimensional semantic\nsimilarity space via Monte Carlo sampling. Comparisons of Monte Carlo estimates\nin the reduced dimensionality space enables tractable frequentist inference\nwithout relying on restrictive distributional assumptions. The framework is\nmodel-agnostic, supports the evaluation of arbitrary input perturbations on any\nblack-box LLM, yields interpretable p-values, supports multiple perturbation\ntesting via controlled error rates, and provides scalar effect sizes for any\nchosen similarity or distance metric. We demonstrate the effectiveness of DBPA\nin evaluating perturbation impacts, showing its versatility for perturbation\nanalysis.", "published": "2024-12-01 16:13:09", "link": "http://arxiv.org/abs/2412.00868v1", "categories": ["cs.LG", "cs.CL", "math.ST", "stat.ML", "stat.TH"], "primary_category": "cs.LG"}
{"title": "Complexity boosted adaptive training for better low resource ASR\n  performance", "abstract": "During the entire training process of the ASR model, the intensity of data\naugmentation and the approach of calculating training loss are applied in a\nregulated manner based on preset parameters. For example, SpecAugment employs a\npredefined strength of augmentation to mask parts of the time-frequency domain\nspectrum. Similarly, in CTC-based multi-layer models, the loss is generally\ndetermined based on the output of the encoder's final layer during the training\nprocess. However, ignoring dynamic characteristics may suboptimally train\nmodels. To address the issue, we present a two-stage training method, known as\ncomplexity-boosted adaptive (CBA) training. It involves making dynamic\nadjustments to data augmentation strategies and CTC loss propagation based on\nthe complexity of the training samples. In the first stage, we train the model\nwith intermediate-CTC-based regularization and data augmentation without any\nadaptive policy. In the second stage, we propose a novel adaptive policy,\ncalled MinMax-IBF, which calculates the complexity of samples. We combine the\nMinMax-IBF policy to data augmentation and intermediate CTC loss regularization\nto continue training. The proposed CBA training approach shows considerable\nimprovements, up to 13.4% and 14.1% relative reduction in WER on the\nLibriSpeech 100h test-clean and test-other dataset and also up to 6.3% relative\nreduction on AISHELL-1 test set, over the Conformer architecture in Wenet.", "published": "2024-12-01 16:32:49", "link": "http://arxiv.org/abs/2412.00877v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Late fusion ensembles for speech recognition on diverse input audio\n  representations", "abstract": "We explore diverse representations of speech audio, and their effect on a\nperformance of late fusion ensemble of E-Branchformer models, applied to\nAutomatic Speech Recognition (ASR) task. Although it is generally known that\nensemble methods often improve the performance of the system even for speech\nrecognition, it is very interesting to explore how ensembles of complex\nstate-of-the-art models, such as medium-sized and large E-Branchformers, cope\nin this setting when their base models are trained on diverse representations\nof the input speech audio. The results are evaluated on four widely-used\nbenchmark datasets: \\textit{Librispeech, Aishell, Gigaspeech},\n\\textit{TEDLIUMv2} and show that improvements of $1\\% - 14\\%$ can still be\nachieved over the state-of-the-art models trained using comparable techniques\non these datasets. A noteworthy observation is that such ensemble offers\nimprovements even with the use of language models, although the gap is closing.", "published": "2024-12-01 10:19:24", "link": "http://arxiv.org/abs/2412.01861v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
