{"title": "Word2Vec vs DBnary: Augmenting METEOR using Vector Representations or\n  Lexical Resources?", "abstract": "This paper presents an approach combining lexico-semantic resources and\ndistributed representations of words applied to the evaluation in machine\ntranslation (MT). This study is made through the enrichment of a well-known MT\nevaluation metric: METEOR. This metric enables an approximate match (synonymy\nor morphological similarity) between an automatic and a reference translation.\nOur experiments are made in the framework of the Metrics task of WMT 2014. We\nshow that distributed representations are a good alternative to lexico-semantic\nresources for MT evaluation and they can even bring interesting additional\ninformation. The augmented versions of METEOR, using vector representations,\nare made available on our Github page.", "published": "2016-10-05 07:18:42", "link": "http://arxiv.org/abs/1610.01291v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A tentative model for dimensionless phoneme distance from binary\n  distinctive features", "abstract": "This work proposes a tentative model for the calculation of dimensionless\ndistances between phonemes; sounds are described with binary distinctive\nfeatures and distances show linear consistency in terms of such features. The\nmodel can be used as a scoring function for local and global pairwise alignment\nof phoneme sequences, and the distances can be used as prior probabilities for\nBayesian analyses on the phylogenetic relationship between languages,\nparticularly for cognate identification in cases where no empirical prior\nprobability is available.", "published": "2016-10-05 15:48:08", "link": "http://arxiv.org/abs/1610.01486v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VoxML: A Visualization Modeling Language", "abstract": "We present the specification for a modeling language, VoxML, which encodes\nsemantic knowledge of real-world objects represented as three-dimensional\nmodels, and of events and attributes related to and enacted over these objects.\nVoxML is intended to overcome the limitations of existing 3D visual markup\nlanguages by allowing for the encoding of a broad range of semantic knowledge\nthat can be exploited by a variety of systems and platforms, leading to\nmultimodal simulations of real-world scenarios using conceptual objects that\nrepresent their semantic values.", "published": "2016-10-05 16:27:48", "link": "http://arxiv.org/abs/1610.01508v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Structural Correspondence Learning for Domain Adaptation", "abstract": "Domain adaptation, adapting models from domains rich in labeled training data\nto domains poor in such data, is a fundamental NLP challenge. We introduce a\nneural network model that marries together ideas from two prominent strands of\nresearch on domain adaptation through representation learning: structural\ncorrespondence learning (SCL, (Blitzer et al., 2006)) and autoencoder neural\nnetworks. Particularly, our model is a three-layer neural network that learns\nto encode the nonpivot features of an input example into a low-dimensional\nrepresentation, so that the existence of pivot features (features that are\nprominent in both domains and convey useful information for the NLP task) in\nthe example can be decoded from that representation. The low-dimensional\nrepresentation is then employed in a learning algorithm for the task. Moreover,\nwe show how to inject pre-trained word embeddings into our model in order to\nimprove generalization across examples with similar pivot features. On the task\nof cross-domain product sentiment classification (Blitzer et al., 2007),\nconsisting of 12 domain pairs, our model outperforms both the SCL and the\nmarginalized stacked denoising autoencoder (MSDA, (Chen et al., 2012)) methods\nby 3.77% and 2.17% respectively, on average across domain pairs.", "published": "2016-10-05 19:57:21", "link": "http://arxiv.org/abs/1610.01588v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ECAT: Event Capture Annotation Tool", "abstract": "This paper introduces the Event Capture Annotation Tool (ECAT), a\nuser-friendly, open-source interface tool for annotating events and their\nparticipants in video, capable of extracting the 3D positions and orientations\nof objects in video captured by Microsoft's Kinect(R) hardware. The modeling\nlanguage VoxML (Pustejovsky and Krishnaswamy, 2016) underlies ECAT's object,\nprogram, and attribute representations, although ECAT uses its own spec for\nexplicit labeling of motion instances. The demonstration will show the tool's\nworkflow and the options available for capturing event-participant relations\nand browsing visual data. Mapping ECAT's output to VoxML will also be\naddressed.", "published": "2016-10-05 01:24:42", "link": "http://arxiv.org/abs/1610.01247v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Monaural Multi-Talker Speech Recognition using Factorial Speech\n  Processing Models", "abstract": "A Pascal challenge entitled monaural multi-talker speech recognition was\ndeveloped, targeting the problem of robust automatic speech recognition against\nspeech like noises which significantly degrades the performance of automatic\nspeech recognition systems. In this challenge, two competing speakers say a\nsimple command simultaneously and the objective is to recognize speech of the\ntarget speaker. Surprisingly during the challenge, a team from IBM research,\ncould achieve a performance better than human listeners on this task. The\nproposed method of the IBM team, consist of an intermediate speech separation\nand then a single-talker speech recognition. This paper reconsiders the task of\nthis challenge based on gain adapted factorial speech processing models. It\ndevelops a joint-token passing algorithm for direct utterance decoding of both\ntarget and masker speakers, simultaneously. Comparing it to the challenge\nwinner, it uses maximum uncertainty during the decoding which cannot be used in\nthe past two-phased method. It provides detailed derivation of inference on\nthese models based on general inference procedures of probabilistic graphical\nmodels. As another improvement, it uses deep neural networks for joint-speaker\nidentification and gain estimation which makes these two steps easier than\nbefore producing competitive results for these steps. The proposed method of\nthis work outperforms past super-human results and even the results were\nachieved recently by Microsoft research, using deep neural networks. It\nachieved 5.5% absolute task performance improvement compared to the first\nsuper-human system and 2.7% absolute task performance improvement compared to\nits recent competitor.", "published": "2016-10-05 11:34:36", "link": "http://arxiv.org/abs/1610.01367v1", "categories": ["cs.CL", "cs.SD"], "primary_category": "cs.CL"}
{"title": "Divide-and-Conquer based Ensemble to Spot Emotions in Speech using MFCC\n  and Random Forest", "abstract": "Besides spoken words, speech signals also carry information about speaker\ngender, age, and emotional state which can be used in a variety of speech\nanalysis applications. In this paper, a divide and conquer strategy for\nensemble classification has been proposed to recognize emotions in speech.\nIntrinsic hierarchy in emotions has been utilized to construct an emotions\ntree, which assisted in breaking down the emotion recognition task into smaller\nsub tasks. The proposed framework generates predictions in three phases.\nFirstly, emotions are detected in the input speech signal by classifying it as\nneutral or emotional. If the speech is classified as emotional, then in the\nsecond phase, it is further classified into positive and negative classes.\nFinally, individual positive or negative emotions are identified based on the\noutcomes of the previous stages. Several experiments have been performed on a\nwidely used benchmark dataset. The proposed method was able to achieve improved\nrecognition rates as compared to several other approaches.", "published": "2016-10-05 12:16:35", "link": "http://arxiv.org/abs/1610.01382v1", "categories": ["cs.SD", "cs.CL"], "primary_category": "cs.SD"}
{"title": "Comparative study of LSA vs Word2vec embeddings in small corpora: a case\n  study in dreams database", "abstract": "Word embeddings have been extensively studied in large text datasets.\nHowever, only a few studies analyze semantic representations of small corpora,\nparticularly relevant in single-person text production studies. In the present\npaper, we compare Skip-gram and LSA capabilities in this scenario, and we test\nboth techniques to extract relevant semantic patterns in single-series dreams\nreports. LSA showed better performance than Skip-gram in small size training\ncorpus in two semantic tests. As a study case, we show that LSA can capture\nrelevant words associations in dream reports series, even in cases of small\nnumber of dreams or low-frequency words. We propose that LSA can be used to\nexplore words associations in dreams reports, which could bring new insight\ninto this classic research area of psychology", "published": "2016-10-05 16:47:17", "link": "http://arxiv.org/abs/1610.01520v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Summarizing Situational and Topical Information During Crises", "abstract": "The use of microblogging platforms such as Twitter during crises has become\nwidespread. More importantly, information disseminated by affected people\ncontains useful information like reports of missing and found people, requests\nfor urgent needs etc. For rapid crisis response, humanitarian organizations\nlook for situational awareness information to understand and assess the\nseverity of the crisis. In this paper, we present a novel framework (i) to\ngenerate abstractive summaries useful for situational awareness, and (ii) to\ncapture sub-topics and present a short informative summary for each of these\ntopics. A summary is generated using a two stage framework that first extracts\na set of important tweets from the whole set of information through an\nInteger-linear programming (ILP) based optimization technique and then follows\na word graph and concept event based abstractive summarization technique to\nproduce the final summary. High accuracies obtained for all the tasks show the\neffectiveness of the proposed framework.", "published": "2016-10-05 18:39:53", "link": "http://arxiv.org/abs/1610.01561v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Visual Question Answering: Datasets, Algorithms, and Future Challenges", "abstract": "Visual Question Answering (VQA) is a recent problem in computer vision and\nnatural language processing that has garnered a large amount of interest from\nthe deep learning, computer vision, and natural language processing\ncommunities. In VQA, an algorithm needs to answer text-based questions about\nimages. Since the release of the first VQA dataset in 2014, additional datasets\nhave been released and many algorithms have been proposed. In this review, we\ncritically examine the current state of VQA in terms of problem formulation,\nexisting datasets, evaluation metrics, and algorithms. In particular, we\ndiscuss the limitations of current datasets with regard to their ability to\nproperly train and assess VQA algorithms. We then exhaustively review existing\nalgorithms for VQA. Finally, we discuss possible future directions for VQA and\nimage understanding research.", "published": "2016-10-05 14:58:36", "link": "http://arxiv.org/abs/1610.01465v4", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
