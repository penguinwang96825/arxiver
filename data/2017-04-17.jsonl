{"title": "A Neural Architecture for Generating Natural Language Descriptions from\n  Source Code Changes", "abstract": "We propose a model to automatically describe changes introduced in the source\ncode of a program using natural language. Our method receives as input a set of\ncode commits, which contains both the modifications and message introduced by\nan user. These two modalities are used to train an encoder-decoder\narchitecture. We evaluated our approach on twelve real world open source\nprojects from four different programming languages. Quantitative and\nqualitative results showed that the proposed approach can generate feasible and\nsemantically sound descriptions not only in standard in-project settings, but\nalso in a cross-project setting.", "published": "2017-04-17 03:20:07", "link": "http://arxiv.org/abs/1704.04856v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Character-level Compositionality with Visual Features", "abstract": "Previous work has modeled the compositionality of words by creating\ncharacter-level models of meaning, reducing problems of sparsity for rare\nwords. However, in many writing systems compositionality has an effect even on\nthe character-level: the meaning of a character is derived by the sum of its\nparts. In this paper, we model this effect by creating embeddings for\ncharacters based on their visual characteristics, creating an image for the\ncharacter and running it through a convolutional neural network to produce a\nvisual character embedding. Experiments on a text classification task\ndemonstrate that such model allows for better processing of instances with rare\ncharacters in languages such as Chinese, Japanese, and Korean. Additionally,\nqualitative analyses demonstrate that our proposed model learns to focus on the\nparts of characters that carry semantic content, resulting in embeddings that\nare coherent in visual space.", "published": "2017-04-17 03:30:30", "link": "http://arxiv.org/abs/1704.04859v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Joint Entity Disambiguation with Local Neural Attention", "abstract": "We propose a novel deep learning model for joint document-level entity\ndisambiguation, which leverages learned neural representations. Key components\nare entity embeddings, a neural attention mechanism over local context windows,\nand a differentiable joint inference stage for disambiguation. Our approach\nthereby combines benefits of deep learning with more traditional approaches\nsuch as graphical models and probabilistic mention-entity maps. Extensive\nexperiments show that we are able to obtain competitive or state-of-the-art\naccuracy at moderate computational costs.", "published": "2017-04-17 10:18:32", "link": "http://arxiv.org/abs/1704.04920v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FEUP at SemEval-2017 Task 5: Predicting Sentiment Polarity and Intensity\n  with Financial Word Embeddings", "abstract": "This paper presents the approach developed at the Faculty of Engineering of\nUniversity of Porto, to participate in SemEval 2017, Task 5: Fine-grained\nSentiment Analysis on Financial Microblogs and News. The task consisted in\npredicting a real continuous variable from -1.0 to +1.0 representing the\npolarity and intensity of sentiment concerning companies/stocks mentioned in\nshort texts. We modeled the task as a regression analysis problem and combined\ntraditional techniques such as pre-processing short texts, bag-of-words\nrepresentations and lexical-based features with enhanced financial specific\nbag-of-embeddings. We used an external collection of tweets and news headlines\nmentioning companies/stocks from S\\&P 500 to create financial word embeddings\nwhich are able to capture domain-specific syntactic and semantic similarities.\nThe resulting approach obtained a cosine similarity score of 0.69 in sub-task\n5.1 - Microblogs and 0.68 in sub-task 5.2 - News Headlines.", "published": "2017-04-17 18:48:00", "link": "http://arxiv.org/abs/1704.05091v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Exploring Sparsity in Recurrent Neural Networks", "abstract": "Recurrent Neural Networks (RNN) are widely used to solve a variety of\nproblems and as the quantity of data and the amount of available compute have\nincreased, so have model sizes. The number of parameters in recent\nstate-of-the-art networks makes them hard to deploy, especially on mobile\nphones and embedded devices. The challenge is due to both the size of the model\nand the time it takes to evaluate it. In order to deploy these RNNs\nefficiently, we propose a technique to reduce the parameters of a network by\npruning weights during the initial training of the network. At the end of\ntraining, the parameters of the network are sparse while accuracy is still\nclose to the original dense neural network. The network size is reduced by 8x\nand the time required to train the model remains constant. Additionally, we can\nprune a larger dense network to achieve better than baseline performance while\nstill reducing the total number of parameters significantly. Pruning RNNs\nreduces the size of the model and can also help achieve significant inference\ntime speed-up using sparse matrix multiply. Benchmarks show that using our\ntechnique model size can be reduced by 90% and speed-up is around 2x to 7x.", "published": "2017-04-17 20:42:05", "link": "http://arxiv.org/abs/1704.05119v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Sparse Communication for Distributed Gradient Descent", "abstract": "We make distributed stochastic gradient descent faster by exchanging sparse\nupdates instead of dense updates. Gradient updates are positively skewed as\nmost updates are near zero, so we map the 99% smallest updates (by absolute\nvalue) to zero then exchange sparse matrices. This method can be combined with\nquantization to further improve the compression. We explore different\nconfigurations and apply them to neural machine translation and MNIST image\nclassification tasks. Most configurations work on MNIST, whereas different\nconfigurations reduce convergence rate on the more complex translation task.\nOur experiments show that we can achieve up to 49% speed up on MNIST and 22% on\nNMT without damaging the final accuracy or BLEU.", "published": "2017-04-17 16:32:02", "link": "http://arxiv.org/abs/1704.05021v2", "categories": ["cs.CL", "cs.DC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Does Neural Machine Translation Benefit from Larger Context?", "abstract": "We propose a neural machine translation architecture that models the\nsurrounding text in addition to the source sentence. These models lead to\nbetter performance, both in terms of general translation quality and pronoun\nprediction, when trained on small corpora, although this improvement largely\ndisappears when trained with a larger corpus. We also discover that\nattention-based neural machine translation is well suited for pronoun\nprediction and compares favorably with other approaches that were specifically\ndesigned for this task.", "published": "2017-04-17 21:42:19", "link": "http://arxiv.org/abs/1704.05135v1", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
