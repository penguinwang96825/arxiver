{"title": "T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT", "abstract": "Recent advancements in large language models have demonstrated how\nchain-of-thought (CoT) and reinforcement learning (RL) can improve performance.\nHowever, applying such reasoning strategies to the visual generation domain\nremains largely unexplored. In this paper, we present T2I-R1, a novel\nreasoning-enhanced text-to-image generation model, powered by RL with a\nbi-level CoT reasoning process. Specifically, we identify two levels of CoT\nthat can be utilized to enhance different stages of generation: (1) the\nsemantic-level CoT for high-level planning of the prompt and (2) the\ntoken-level CoT for low-level pixel processing during patch-by-patch\ngeneration. To better coordinate these two levels of CoT, we introduce\nBiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes\nboth generation CoTs within the same training step. By applying our reasoning\nstrategies to the baseline model, Janus-Pro, we achieve superior performance\nwith 13% improvement on T2I-CompBench and 19% improvement on the WISE\nbenchmark, even surpassing the state-of-the-art model FLUX.1. Code is available\nat: https://github.com/CaraJ7/T2I-R1", "published": "2025-05-01 17:59:46", "link": "http://arxiv.org/abs/2505.00703v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Steering Large Language Models with Register Analysis for Arbitrary Style Transfer", "abstract": "Large Language Models (LLMs) have demonstrated strong capabilities in\nrewriting text across various styles. However, effectively leveraging this\nability for example-based arbitrary style transfer, where an input text is\nrewritten to match the style of a given exemplar, remains an open challenge. A\nkey question is how to describe the style of the exemplar to guide LLMs toward\nhigh-quality rewrites. In this work, we propose a prompting method based on\nregister analysis to guide LLMs to perform this task. Empirical evaluations\nacross multiple style transfer tasks show that our prompting approach enhances\nstyle transfer strength while preserving meaning more effectively than existing\nprompting strategies.", "published": "2025-05-01 17:39:02", "link": "http://arxiv.org/abs/2505.00679v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions", "abstract": "Memory is a fundamental component of AI systems, underpinning large language\nmodels (LLMs) based agents. While prior surveys have focused on memory\napplications with LLMs, they often overlook the atomic operations that underlie\nmemory dynamics. In this survey, we first categorize memory representations\ninto parametric, contextual structured, and contextual unstructured and then\nintroduce six fundamental memory operations: Consolidation, Updating, Indexing,\nForgetting, Retrieval, and Compression. We systematically map these operations\nto the most relevant research topics across long-term, long-context, parametric\nmodification, and multi-source memory. By reframing memory systems through the\nlens of atomic operations and representation types, this survey provides a\nstructured and dynamic perspective on research, benchmark datasets, and tools\nrelated to memory in AI, clarifying the functional interplay in LLMs based\nagents while outlining promising directions for future research\\footnote{The\npaper list, datasets, methods and tools are available at\n\\href{https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}{https://github.com/Elvin-Yiming-Du/Survey\\_Memory\\_in\\_AI}.}.", "published": "2025-05-01 17:31:33", "link": "http://arxiv.org/abs/2505.00675v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DeepCritic: Deliberate Critique with Large Language Models", "abstract": "As Large Language Models (LLMs) are rapidly evolving, providing accurate\nfeedback and scalable oversight on their outputs becomes an urgent and critical\nproblem. Leveraging LLMs as critique models to achieve automated supervision is\na promising solution. In this work, we focus on studying and enhancing the math\ncritique ability of LLMs. Current LLM critics provide critiques that are too\nshallow and superficial on each step, leading to low judgment accuracy and\nstruggling to offer sufficient feedback for the LLM generator to correct\nmistakes. To tackle this issue, we propose a novel and effective two-stage\nframework to develop LLM critics that are capable of deliberately critiquing on\neach reasoning step of math solutions. In the first stage, we utilize\nQwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for\nsupervised fine-tuning. Each seed critique consists of deliberate step-wise\ncritiques that includes multi-perspective verifications as well as in-depth\ncritiques of initial critiques for each reasoning step. Then, we perform\nreinforcement learning on the fine-tuned model with either existing\nhuman-labeled data from PRM800K or our automatically annotated data obtained\nvia Monte Carlo sampling-based correctness estimation, to further incentivize\nits critique ability. Our developed critique model built on Qwen2.5-7B-Instruct\nnot only significantly outperforms existing LLM critics (including the\nsame-sized DeepSeek-R1-distill models and GPT-4o) on various error\nidentification benchmarks, but also more effectively helps the LLM generator\nrefine erroneous steps through more detailed feedback.", "published": "2025-05-01 17:03:17", "link": "http://arxiv.org/abs/2505.00662v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the generalization of language models from in-context learning and finetuning: a controlled study", "abstract": "Large language models exhibit exciting capabilities, yet can show\nsurprisingly narrow generalization from finetuning -- from failing to\ngeneralize to simple reversals of relations they are trained on, to missing\nlogical deductions that can be made from trained information. These failures to\ngeneralize from fine-tuning can hinder practical application of these models.\nHowever, language models' in-context learning shows different inductive biases,\nand can generalize better in some of these cases. Here, we explore these\ndifferences in generalization between in-context- and fine-tuning-based\nlearning. To do so, we constructed several novel datasets to evaluate and\nimprove models' ability to generalize from finetuning data. The datasets are\nconstructed to isolate the knowledge in the dataset from that in pretraining,\nto create clean tests of generalization. We expose pretrained large models to\ncontrolled subsets of the information in these datasets -- either in context,\nor through fine-tuning -- and evaluate their performance on test sets that\nrequire various types of generalization. We find overall that in data-matched\nsettings, in-context learning can generalize more flexibly than fine-tuning\n(though we also find some qualifications of prior findings, such as cases when\nfine-tuning can generalize to reversals embedded in a larger structure of\nknowledge). We build on these findings to propose a method to enable improved\ngeneralization from fine-tuning: adding in-context inferences to finetuning\ndata. We show that this method improves generalization across various splits of\nour datasets and other benchmarks. Our results have implications for\nunderstanding the inductive biases of different modes of learning in language\nmodels, and practically improving their performance.", "published": "2025-05-01 17:02:27", "link": "http://arxiv.org/abs/2505.00661v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language Models Understanding: an Inherent Ambiguity Barrier", "abstract": "A lively ongoing debate is taking place, since the extraordinary emergence of\nLarge Language Models (LLMs) with regards to their capability to understand the\nworld and capture the meaning of the dialogues in which they are involved.\nArguments and counter-arguments have been proposed based upon thought\nexperiments, anecdotal conversations between LLMs and humans, statistical\nlinguistic analysis, philosophical considerations, and more. In this brief\npaper we present a counter-argument based upon a thought experiment and\nsemi-formal considerations leading to an inherent ambiguity barrier which\nprevents LLMs from having any understanding of what their amazingly fluent\ndialogues mean.", "published": "2025-05-01 16:55:44", "link": "http://arxiv.org/abs/2505.00654v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Investigating Task Arithmetic for Zero-Shot Information Retrieval", "abstract": "Large Language Models (LLMs) have shown impressive zero-shot performance\nacross a variety of Natural Language Processing tasks, including document\nre-ranking. However, their effectiveness degrades on unseen tasks and domains,\nlargely due to shifts in vocabulary and word distributions. In this paper, we\ninvestigate Task Arithmetic, a technique that combines the weights of LLMs\npre-trained on different tasks or domains via simple mathematical operations,\nsuch as addition or subtraction, to adapt retrieval models without requiring\nadditional fine-tuning. Our method is able to synthesize diverse tasks and\ndomain knowledge into a single model, enabling effective zero-shot adaptation\nin different retrieval contexts. Extensive experiments on publicly available\nscientific, biomedical, and multilingual datasets show that our method improves\nstate-of-the-art re-ranking performance by up to 18% in NDCG@10 and 15% in\nP@10. In addition to these empirical gains, our analysis provides insights into\nthe strengths and limitations of Task Arithmetic as a practical strategy for\nzero-shot learning and model adaptation. We make our code publicly available at\nhttps://github.com/DetectiveMB/Task-Arithmetic-for-ZS-IR.", "published": "2025-05-01 16:48:37", "link": "http://arxiv.org/abs/2505.00649v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)", "abstract": "Large language models (LLMs) that integrate multiple input roles (e.g.,\nsystem instructions, user queries, external tool outputs) are increasingly\nprevalent in practice. Ensuring that the model accurately distinguishes\nmessages from each role -- a concept we call \\emph{role separation} -- is\ncrucial for consistent multi-role behavior. Although recent work often targets\nstate-of-the-art prompt injection defenses, it remains unclear whether such\nmethods truly teach LLMs to differentiate roles or merely memorize known\ntriggers. In this paper, we examine \\emph{role-separation learning}: the\nprocess of teaching LLMs to robustly distinguish system and user tokens.\nThrough a \\emph{simple, controlled experimental framework}, we find that\nfine-tuned models often rely on two proxies for role identification: (1) task\ntype exploitation, and (2) proximity to begin-of-text. Although data\naugmentation can partially mitigate these shortcuts, it generally leads to\niterative patching rather than a deeper fix. To address this, we propose\nreinforcing \\emph{invariant signals} that mark role boundaries by adjusting\ntoken-wise cues in the model's input encoding. In particular, manipulating\nposition IDs helps the model learn clearer distinctions and reduces reliance on\nsuperficial proxies. By focusing on this mechanism-centered perspective, our\nwork illuminates how LLMs can more reliably maintain consistent multi-role\nbehavior without merely memorizing known prompts or triggers.", "published": "2025-05-01 16:06:16", "link": "http://arxiv.org/abs/2505.00626v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2"], "primary_category": "cs.CL"}
{"title": "FineScope : Precision Pruning for Domain-Specialized Large Language Models Using SAE-Guided Self-Data Cultivation", "abstract": "Training large language models (LLMs) from scratch requires significant\ncomputational resources, driving interest in developing smaller,\ndomain-specific LLMs that maintain both efficiency and strong task performance.\nMedium-sized models such as LLaMA, llama} have served as starting points for\ndomain-specific adaptation, but they often suffer from accuracy degradation\nwhen tested on specialized datasets. We introduce FineScope, a framework for\nderiving compact, domain-optimized LLMs from larger pretrained models.\nFineScope leverages the Sparse Autoencoder (SAE) framework, inspired by its\nability to produce interpretable feature representations, to extract\ndomain-specific subsets from large datasets. We apply structured pruning with\ndomain-specific constraints, ensuring that the resulting pruned models retain\nessential knowledge for the target domain. To further enhance performance,\nthese pruned models undergo self-data distillation, leveraging SAE-curated\ndatasets to restore key domain-specific information lost during pruning.\nExtensive experiments and ablation studies demonstrate that FineScope achieves\nhighly competitive performance, outperforming several large-scale\nstate-of-the-art LLMs in domain-specific tasks. Additionally, our results show\nthat FineScope enables pruned models to regain a substantial portion of their\noriginal performance when fine-tuned with SAE-curated datasets. Furthermore,\napplying these datasets to fine-tune pretrained LLMs without pruning also\nimproves their domain-specific accuracy, highlighting the robustness of our\napproach. The code will be released.", "published": "2025-05-01 16:05:08", "link": "http://arxiv.org/abs/2505.00624v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Block Circulant Adapter for Large Language Models", "abstract": "Fine-tuning large language models (LLMs) is difficult due to their huge model\nsize. Recent Fourier domain-based methods show potential for reducing\nfine-tuning costs. We propose a block circulant matrix-based fine-tuning method\nwith a stable training heuristic to leverage the properties of circulant\nmatrices and one-dimensional Fourier transforms to reduce storage and\ncomputation costs. Experiments show that our method uses $14\\times$ less number\nof parameters than VeRA, $16\\times$ smaller than LoRA and $32\\times$ less FLOPs\nthan FourierFT, while maintaining close or better task performance. Our\napproach presents a promising way in frequency domain to fine-tune large models\non downstream tasks.", "published": "2025-05-01 15:14:32", "link": "http://arxiv.org/abs/2505.00582v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension", "abstract": "Extending the context window in large language models (LLMs) is essential for\napplications involving long-form content generation. However, the linear\nincrease in key-value (KV) cache memory requirements and the quadratic\ncomplexity of self-attention with respect to sequence length present\nsignificant challenges during fine-tuning and inference. Existing methods\nsuffer from performance degradation when extending to longer contexts. In this\nwork, we introduce a novel context extension method that optimizes both\nfine-tuning and inference efficiency. Our method exploits a key observation: in\nthe frequency domain, the energy distribution of the KV cache is primarily\nconcentrated in low-frequency components. By filtering out the high-frequency\ncomponents, the KV cache can be effectively compressed with minimal information\nloss. Building on this insight, we propose an efficient compression technique,\nFreqKV, that iteratively compresses the increasing KV cache to a fixed size in\nthe frequency domain, applicable to both fine-tuning and inference. FreqKV\nintroduces no additional parameters or architectural modifications. With\nminimal fine-tuning, LLMs can learn to leverage the limited cache that is\ncompressed in the frequency domain and extend the context window efficiently.\nExperiments on various long context language modeling and understanding tasks\ndemonstrate the efficiency and efficacy of the proposed method.", "published": "2025-05-01 14:53:12", "link": "http://arxiv.org/abs/2505.00570v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Triggering Hallucinations in LLMs: A Quantitative Study of Prompt-Induced Hallucination in Large Language Models", "abstract": "Hallucinations in large language models (LLMs) present a growing challenge\nacross real-world applications, from healthcare to law, where factual\nreliability is essential. Despite advances in alignment and instruction tuning,\nLLMs can still generate outputs that are fluent yet fundamentally untrue.\nUnderstanding the cognitive dynamics that underlie these hallucinations remains\nan open problem. In this study, we propose a prompt-based framework to\nsystematically trigger and quantify hallucination: a Hallucination-Inducing\nPrompt (HIP), which synthetically fuses semantically distant concepts (e.g.,\nperiodic table of elements and tarot divination) in a misleading way, and a\nHallucination Quantifying Prompt (HQP), which scores the plausibility,\nconfidence, and coherence of the output. Controlled experiments across multiple\nLLMs revealed that HIPs consistently produced less coherent and more\nhallucinated responses than their null-fusion controls. These effects varied\nacross models, with reasoning-oriented LLMs showing distinct profiles from\ngeneral-purpose ones. Our framework provides a reproducible testbed for\nstudying hallucination vulnerability, and opens the door to developing safer,\nmore introspective LLMs that can detect and self-regulate the onset of\nconceptual instability.", "published": "2025-05-01 14:33:47", "link": "http://arxiv.org/abs/2505.00557v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "100 Days After DeepSeek-R1: A Survey on Replication Studies and More Directions for Reasoning Language Models", "abstract": "The recent development of reasoning language models (RLMs) represents a novel\nevolution in large language models. In particular, the recent release of\nDeepSeek-R1 has generated widespread social impact and sparked enthusiasm in\nthe research community for exploring the explicit reasoning paradigm of\nlanguage models. However, the implementation details of the released models\nhave not been fully open-sourced by DeepSeek, including DeepSeek-R1-Zero,\nDeepSeek-R1, and the distilled small models. As a result, many replication\nstudies have emerged aiming to reproduce the strong performance achieved by\nDeepSeek-R1, reaching comparable performance through similar training\nprocedures and fully open-source data resources. These works have investigated\nfeasible strategies for supervised fine-tuning (SFT) and reinforcement learning\nfrom verifiable rewards (RLVR), focusing on data preparation and method design,\nyielding various valuable insights. In this report, we provide a summary of\nrecent replication studies to inspire future research. We primarily focus on\nSFT and RLVR as two main directions, introducing the details for data\nconstruction, method design and training procedure of current replication\nstudies. Moreover, we conclude key findings from the implementation details and\nexperimental results reported by these studies, anticipating to inspire future\nresearch. We also discuss additional techniques of enhancing RLMs, highlighting\nthe potential of expanding the application scope of these models, and\ndiscussing the challenges in development. By this survey, we aim to help\nresearchers and developers of RLMs stay updated with the latest advancements,\nand seek to inspire new ideas to further enhance RLMs.", "published": "2025-05-01 14:28:35", "link": "http://arxiv.org/abs/2505.00551v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HalluMix: A Task-Agnostic, Multi-Domain Benchmark for Real-World Hallucination Detection", "abstract": "As large language models (LLMs) are increasingly deployed in high-stakes\ndomains, detecting hallucinated content$\\unicode{x2013}$text that is not\ngrounded in supporting evidence$\\unicode{x2013}$has become a critical\nchallenge. Existing benchmarks for hallucination detection are often\nsynthetically generated, narrowly focused on extractive question answering, and\nfail to capture the complexity of real-world scenarios involving multi-document\ncontexts and full-sentence outputs. We introduce the HalluMix Benchmark, a\ndiverse, task-agnostic dataset that includes examples from a range of domains\nand formats. Using this benchmark, we evaluate seven hallucination detection\nsystems$\\unicode{x2013}$both open and closed\nsource$\\unicode{x2013}$highlighting differences in performance across tasks,\ndocument lengths, and input representations. Our analysis highlights\nsubstantial performance disparities between short and long contexts, with\ncritical implications for real-world Retrieval Augmented Generation (RAG)\nimplementations. Quotient Detections achieves the best overall performance,\nwith an accuracy of 0.82 and an F1 score of 0.84.", "published": "2025-05-01 13:22:45", "link": "http://arxiv.org/abs/2505.00506v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Computational Identification of Regulatory Statements in EU Legislation", "abstract": "Identifying regulatory statements in legislation is useful for developing\nmetrics to measure the regulatory density and strictness of legislation. A\ncomputational method is valuable for scaling the identification of such\nstatements from a growing body of EU legislation, constituting approximately\n180,000 published legal acts between 1952 and 2023. Past work on extraction of\nthese statements varies in the permissiveness of their definitions for what\nconstitutes a regulatory statement. In this work, we provide a specific\ndefinition for our purposes based on the institutional grammar tool. We develop\nand compare two contrasting approaches for automatically identifying such\nstatements in EU legislation, one based on dependency parsing, and the other on\na transformer-based machine learning model. We found both approaches performed\nsimilarly well with accuracies of 80% and 84% respectively and a K alpha of\n0.58. The high accuracies and not exceedingly high agreement suggests potential\nfor combining strengths of both approaches.", "published": "2025-05-01 12:11:32", "link": "http://arxiv.org/abs/2505.00479v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Red Teaming Large Language Models for Healthcare", "abstract": "We present the design process and findings of the pre-conference workshop at\nthe Machine Learning for Healthcare Conference (2024) entitled Red Teaming\nLarge Language Models for Healthcare, which took place on August 15, 2024.\nConference participants, comprising a mix of computational and clinical\nexpertise, attempted to discover vulnerabilities -- realistic clinical prompts\nfor which a large language model (LLM) outputs a response that could cause\nclinical harm. Red-teaming with clinicians enables the identification of LLM\nvulnerabilities that may not be recognised by LLM developers lacking clinical\nexpertise. We report the vulnerabilities found, categorise them, and present\nthe results of a replication study assessing the vulnerabilities across all\nLLMs provided.", "published": "2025-05-01 11:43:27", "link": "http://arxiv.org/abs/2505.00467v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Toward Automated Regulatory Decision-Making: Trustworthy Medical Device Risk Classification with Multimodal Transformers and Self-Training", "abstract": "Accurate classification of medical device risk levels is essential for\nregulatory oversight and clinical safety. We present a Transformer-based\nmultimodal framework that integrates textual descriptions and visual\ninformation to predict device regulatory classification. The model incorporates\na cross-attention mechanism to capture intermodal dependencies and employs a\nself-training strategy for improved generalization under limited supervision.\nExperiments on a real-world regulatory dataset demonstrate that our approach\nachieves up to 90.4% accuracy and 97.9% AUROC, significantly outperforming\ntext-only (77.2%) and image-only (54.8%) baselines. Compared to standard\nmultimodal fusion, the self-training mechanism improved SVM performance by 3.3\npercentage points in accuracy (from 87.1% to 90.4%) and 1.4 points in macro-F1,\nsuggesting that pseudo-labeling can effectively enhance generalization under\nlimited supervision. Ablation studies further confirm the complementary\nbenefits of both cross-modal attention and self-training.", "published": "2025-05-01 09:41:41", "link": "http://arxiv.org/abs/2505.00422v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CSE-SFP: Enabling Unsupervised Sentence Representation Learning via a Single Forward Pass", "abstract": "As a fundamental task in Information Retrieval and Computational Linguistics,\nsentence representation has profound implications for a wide range of practical\napplications such as text clustering, content analysis, question-answering\nsystems, and web search. Recent advances in pre-trained language models (PLMs)\nhave driven remarkable progress in this field, particularly through\nunsupervised embedding derivation methods centered on discriminative PLMs like\nBERT. However, due to time and computational constraints, few efforts have\nattempted to integrate unsupervised sentence representation with generative\nPLMs, which typically possess much larger parameter sizes. Given that\nstate-of-the-art models in both academia and industry are predominantly based\non generative architectures, there is a pressing need for an efficient\nunsupervised text representation framework tailored to decoder-only PLMs. To\naddress this concern, we propose CSE-SFP, an innovative method that exploits\nthe structural characteristics of generative models. Compared to existing\nstrategies, CSE-SFP requires only a single forward pass to perform effective\nunsupervised contrastive learning. Rigorous experimentation demonstrates that\nCSE-SFP not only produces higher-quality embeddings but also significantly\nreduces both training time and memory consumption. Furthermore, we introduce\ntwo ratio metrics that jointly assess alignment and uniformity, thereby\nproviding a more robust means for evaluating the semantic spatial properties of\nencoding models.", "published": "2025-05-01 08:27:14", "link": "http://arxiv.org/abs/2505.00389v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KoACD: The First Korean Adolescent Dataset for Cognitive Distortion Analysis", "abstract": "Cognitive distortion refers to negative thinking patterns that can lead to\nmental health issues like depression and anxiety in adolescents. Previous\nstudies using natural language processing (NLP) have focused mainly on\nsmall-scale adult datasets, with limited research on adolescents. This study\nintroduces KoACD, the first large-scale dataset of cognitive distortions in\nKorean adolescents, containing 108,717 instances. We applied a multi-Large\nLanguage Model (LLM) negotiation method to refine distortion classification and\ngenerate synthetic data using two approaches: cognitive clarification for\ntextual clarity and cognitive balancing for diverse distortion representation.\nValidation through LLMs and expert evaluations showed that while LLMs\nclassified distortions with explicit markers, they struggled with\ncontext-dependent reasoning, where human evaluators demonstrated higher\naccuracy. KoACD aims to enhance future research on cognitive distortion\ndetection.", "published": "2025-05-01 07:37:18", "link": "http://arxiv.org/abs/2505.00367v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "R&B: Domain Regrouping and Data Mixture Balancing for Efficient Foundation Model Training", "abstract": "Data mixing strategies have successfully reduced the costs involved in\ntraining language models. While promising, such methods suffer from two flaws.\nFirst, they rely on predetermined data domains (e.g., data sources, task\ntypes), which may fail to capture critical semantic nuances, leaving\nperformance on the table. Second, these methods scale with the number of\ndomains in a computationally prohibitive way. We address these challenges via\nR&B, a framework that re-partitions training data based on semantic similarity\n(Regroup) to create finer-grained domains, and efficiently optimizes the data\ncomposition (Balance) by leveraging a Gram matrix induced by domain gradients\nobtained throughout training. Unlike prior works, it removes the need for\nadditional compute to obtain evaluation information such as losses or\ngradients. We analyze this technique under standard regularity conditions and\nprovide theoretical insights that justify R&B's effectiveness compared to\nnon-adaptive mixing approaches. Empirically, we demonstrate the effectiveness\nof R&B on five diverse datasets ranging from natural language to reasoning and\nmultimodal tasks. With as little as 0.01% additional compute overhead, R&B\nmatches or exceeds the performance of state-of-the-art data mixing strategies.", "published": "2025-05-01 07:08:19", "link": "http://arxiv.org/abs/2505.00358v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Enhancing AI-Driven Education: Integrating Cognitive Frameworks, Linguistic Feedback Analysis, and Ethical Considerations for Improved Content Generation", "abstract": "Artificial intelligence (AI) is rapidly transforming education, presenting\nunprecedented opportunities for personalized learning and streamlined content\ncreation. However, realizing the full potential of AI in educational settings\nnecessitates careful consideration of the quality, cognitive depth, and ethical\nimplications of AI-generated materials. This paper synthesizes insights from\nfour related studies to propose a comprehensive framework for enhancing\nAI-driven educational tools. We integrate cognitive assessment frameworks\n(Bloom's Taxonomy and SOLO Taxonomy), linguistic analysis of AI-generated\nfeedback, and ethical design principles to guide the development of effective\nand responsible AI tools. We outline a structured three-phase approach\nencompassing cognitive alignment, linguistic feedback integration, and ethical\nsafeguards. The practical application of this framework is demonstrated through\nits integration into OneClickQuiz, an AI-powered Moodle plugin for quiz\ngeneration. This work contributes a comprehensive and actionable guide for\neducators, researchers, and developers aiming to harness AI's potential while\nupholding pedagogical and ethical standards in educational content generation.", "published": "2025-05-01 06:36:21", "link": "http://arxiv.org/abs/2505.00339v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "T2VPhysBench: A First-Principles Benchmark for Physical Consistency in Text-to-Video Generation", "abstract": "Text-to-video generative models have made significant strides in recent\nyears, producing high-quality videos that excel in both aesthetic appeal and\naccurate instruction following, and have become central to digital art creation\nand user engagement online. Yet, despite these advancements, their ability to\nrespect fundamental physical laws remains largely untested: many outputs still\nviolate basic constraints such as rigid-body collisions, energy conservation,\nand gravitational dynamics, resulting in unrealistic or even misleading\ncontent. Existing physical-evaluation benchmarks typically rely on automatic,\npixel-level metrics applied to simplistic, life-scenario prompts, and thus\noverlook both human judgment and first-principles physics. To fill this gap, we\nintroduce \\textbf{T2VPhysBench}, a first-principled benchmark that\nsystematically evaluates whether state-of-the-art text-to-video systems, both\nopen-source and commercial, obey twelve core physical laws including Newtonian\nmechanics, conservation principles, and phenomenological effects. Our benchmark\nemploys a rigorous human evaluation protocol and includes three targeted\nstudies: (1) an overall compliance assessment showing that all models score\nbelow 0.60 on average in each law category; (2) a prompt-hint ablation\nrevealing that even detailed, law-specific hints fail to remedy physics\nviolations; and (3) a counterfactual robustness test demonstrating that models\noften generate videos that explicitly break physical rules when so instructed.\nThe results expose persistent limitations in current architectures and offer\nconcrete insights for guiding future research toward truly physics-aware video\ngeneration.", "published": "2025-05-01 06:34:55", "link": "http://arxiv.org/abs/2505.00337v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing", "abstract": "Recent advances in large language models highlighted the excessive quadratic\ncost of self-attention. Despite the significant research efforts, subquadratic\nattention methods still suffer from inferior performance in practice. We\nhypothesize that dynamic, learned content-based sparsity can lead to more\nefficient attention mechanisms. We present Mixture of Sparse Attention (MoSA),\na novel approach inspired by Mixture of Experts (MoE) with expert choice\nrouting. MoSA dynamically selects tokens for each attention head, allowing\narbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of\nlength $T$, MoSA reduces the computational complexity of each attention head\nfrom $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same\ncomputational budget, allowing higher specialization. We show that among the\ntested sparse attention variants, MoSA is the only one that can outperform the\ndense baseline, sometimes with up to 27% better perplexity for an identical\ncompute budget. MoSA can also reduce the resource usage compared to dense\nself-attention. Despite using torch implementation without an optimized kernel,\nperplexity-matched MoSA models are simultaneously faster in wall-clock time,\nrequire less memory for training, and drastically reduce the size of the\nKV-cache compared to the dense transformer baselines.", "published": "2025-05-01 05:22:11", "link": "http://arxiv.org/abs/2505.00315v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Consistency in Language Models: Current Landscape, Challenges, and Future Directions", "abstract": "The hallmark of effective language use lies in consistency -- expressing\nsimilar meanings in similar contexts and avoiding contradictions. While human\ncommunication naturally demonstrates this principle, state-of-the-art language\nmodels struggle to maintain reliable consistency across different scenarios.\nThis paper examines the landscape of consistency research in AI language\nsystems, exploring both formal consistency (including logical rule adherence)\nand informal consistency (such as moral and factual coherence). We analyze\ncurrent approaches to measure aspects of consistency, identify critical\nresearch gaps in standardization of definitions, multilingual assessment, and\nmethods to improve consistency. Our findings point to an urgent need for robust\nbenchmarks to measure and interdisciplinary approaches to ensure consistency in\nthe application of language models on domain-specific tasks while preserving\nthe utility and adaptability.", "published": "2025-05-01 03:25:25", "link": "http://arxiv.org/abs/2505.00268v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EnronQA: Towards Personalized RAG over Private Documents", "abstract": "Retrieval Augmented Generation (RAG) has become one of the most popular\nmethods for bringing knowledge-intensive context to large language models (LLM)\nbecause of its ability to bring local context at inference time without the\ncost or data leakage risks associated with fine-tuning. A clear separation of\nprivate information from the LLM training has made RAG the basis for many\nenterprise LLM workloads as it allows the company to augment LLM's\nunderstanding using customers' private documents. Despite its popularity for\nprivate documents in enterprise deployments, current RAG benchmarks for\nvalidating and optimizing RAG pipelines draw their corpora from public data\nsuch as Wikipedia or generic web pages and offer little to no personal context.\nSeeking to empower more personal and private RAG we release the EnronQA\nbenchmark, a dataset of 103,638 emails with 528,304 question-answer pairs\nacross 150 different user inboxes. EnronQA enables better benchmarking of RAG\npipelines over private data and allows for experimentation on the introduction\nof personalized retrieval settings over realistic data. Finally, we use EnronQA\nto explore the tradeoff in memorization and retrieval when reasoning over\nprivate documents.", "published": "2025-05-01 03:07:30", "link": "http://arxiv.org/abs/2505.00263v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Enriching the Korean Learner Corpus with Multi-reference Annotations and Rubric-Based Scoring", "abstract": "Despite growing global interest in Korean language education, there remains a\nsignificant lack of learner corpora tailored to Korean L2 writing. To address\nthis gap, we enhance the KoLLA Korean learner corpus by adding multiple\ngrammatical error correction (GEC) references, thereby enabling more nuanced\nand flexible evaluation of GEC systems, and reflects the variability of human\nlanguage. Additionally, we enrich the corpus with rubric-based scores aligned\nwith guidelines from the Korean National Language Institute, capturing\ngrammatical accuracy, coherence, and lexical diversity. These enhancements make\nKoLLA a robust and standardized resource for research in Korean L2 education,\nsupporting advancements in language learning, assessment, and automated error\ncorrection.", "published": "2025-05-01 03:04:07", "link": "http://arxiv.org/abs/2505.00261v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks", "abstract": "Many methods for improving Large Language Model (LLM) agents for sequential\ndecision-making tasks depend on task-specific knowledge engineering--such as\nprompt tuning, curated in-context examples, or customized observation and\naction spaces. Using these approaches, agent performance improves with the\nquality or amount of knowledge engineering invested. Instead, we investigate\nhow LLM agents can automatically improve their performance by learning\nin-context from their own successful experiences on similar tasks. Rather than\nrelying on task-specific knowledge engineering, we focus on constructing and\nrefining a database of self-generated examples. We demonstrate that even a\nnaive accumulation of successful trajectories across training tasks boosts test\nperformance on three benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%),\nand InterCode-SQL (75% to 79%)--matching the performance the initial agent\nachieves if allowed two to three attempts per task. We then introduce two\nextensions: (1) database-level selection through population-based training to\nidentify high-performing example collections, and (2) exemplar-level selection\nthat retains individual trajectories based on their empirical utility as\nin-context examples. These extensions further enhance performance, achieving\n91% on ALFWorld--matching more complex approaches that employ task-specific\ncomponents and prompts. Our results demonstrate that automatic trajectory\ndatabase construction offers a compelling alternative to labor-intensive\nknowledge engineering.", "published": "2025-05-01 00:48:12", "link": "http://arxiv.org/abs/2505.00234v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Robotic Visual Instruction", "abstract": "Recently, natural language has been the primary medium for human-robot\ninteraction. However, its inherent lack of spatial precision for robotic\ncontrol introduces challenges such as ambiguity and verbosity. To address these\nlimitations, we introduce the Robotic Visual Instruction (RoVI), a novel\nparadigm to guide robotic tasks through an object-centric, hand-drawn symbolic\nrepresentation. RoVI effectively encodes spatial-temporal information into\nhuman-interpretable visual instructions through 2D sketches, utilizing arrows,\ncircles, colors, and numbers to direct 3D robotic manipulation. To enable\nrobots to understand RoVI better and generate precise actions based on RoVI, we\npresent Visual Instruction Embodied Workflow (VIEW), a pipeline formulated for\nRoVI-conditioned policies. This approach leverages Vision-Language Models\n(VLMs) to interpret RoVI inputs, decode spatial and temporal constraints from\n2D pixel space via keypoint extraction, and then transform them into executable\n3D action sequences. We additionally curate a specialized dataset of 15K\ninstances to fine-tune small VLMs for edge deployment, enabling them to\neffectively learn RoVI capabilities. Our approach is rigorously validated\nacross 11 novel tasks in both real and simulated environments, demonstrating\nsignificant generalization capability. Notably, VIEW achieves an 87.5% success\nrate in real-world scenarios involving unseen tasks that feature multi-step\nactions, with disturbances, and trajectory-following requirements. Code and\nDatasets in this paper will be released soon.", "published": "2025-05-01 17:55:05", "link": "http://arxiv.org/abs/2505.00693v1", "categories": ["cs.RO", "cs.AI", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Towards Autonomous Micromobility through Scalable Urban Simulation", "abstract": "Micromobility, which utilizes lightweight mobile machines moving in urban\npublic spaces, such as delivery robots and mobility scooters, emerges as a\npromising alternative to vehicular mobility. Current micromobility depends\nmostly on human manual operation (in-person or remote control), which raises\nsafety and efficiency concerns when navigating busy urban environments full of\nunpredictable obstacles and pedestrians. Assisting humans with AI agents in\nmaneuvering micromobility devices presents a viable solution for enhancing\nsafety and efficiency. In this work, we present a scalable urban simulation\nsolution to advance autonomous micromobility. First, we build URBAN-SIM - a\nhigh-performance robot learning platform for large-scale training of embodied\nagents in interactive urban scenes. URBAN-SIM contains three critical modules:\nHierarchical Urban Generation pipeline, Interactive Dynamics Generation\nstrategy, and Asynchronous Scene Sampling scheme, to improve the diversity,\nrealism, and efficiency of robot learning in simulation. Then, we propose\nURBAN-BENCH - a suite of essential tasks and benchmarks to gauge various\ncapabilities of the AI agents in achieving autonomous micromobility.\nURBAN-BENCH includes eight tasks based on three core skills of the agents:\nUrban Locomotion, Urban Navigation, and Urban Traverse. We evaluate four robots\nwith heterogeneous embodiments, such as the wheeled and legged robots, across\nthese tasks. Experiments on diverse terrains and urban structures reveal each\nrobot's strengths and limitations.", "published": "2025-05-01 17:52:29", "link": "http://arxiv.org/abs/2505.00690v1", "categories": ["cs.CV", "cs.AI", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Visual Test-time Scaling for GUI Agent Grounding", "abstract": "We introduce RegionFocus, a visual test-time scaling approach for Vision\nLanguage Model Agents. Understanding webpages is challenging due to the visual\ncomplexity of GUI images and the large number of interface elements, making\naccurate action selection difficult. Our approach dynamically zooms in on\nrelevant regions, reducing background clutter and improving grounding accuracy.\nTo support this process, we propose an image-as-map mechanism that visualizes\nkey landmarks at each step, providing a transparent action record and enables\nthe agent to effectively choose among action candidates. Even with a simple\nregion selection strategy, we observe significant performance gains of 28+\\% on\nScreenspot-pro and 24+\\% on WebVoyager benchmarks on top of two\nstate-of-the-art open vision language model agents, UI-TARS and Qwen2.5-VL,\nhighlighting the effectiveness of visual test-time scaling in interactive\nsettings. We achieve a new state-of-the-art grounding performance of 61.6\\% on\nthe ScreenSpot-Pro benchmark by applying RegionFocus to a Qwen2.5-VL-72B model.\nOur code will be released publicly at https://github.com/tiangeluo/RegionFocus.", "published": "2025-05-01 17:45:59", "link": "http://arxiv.org/abs/2505.00684v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Deep Reinforcement Learning for Urban Air Quality Management: Multi-Objective Optimization of Pollution Mitigation Booth Placement in Metropolitan Environments", "abstract": "Urban air pollution remains a pressing global concern, particularly in\ndensely populated and traffic-intensive metropolitan areas like Delhi, where\nexposure to harmful pollutants severely impacts public health. Delhi, being one\nof the most polluted cities globally, experiences chronic air quality issues\ndue to vehicular emissions, industrial activities, and construction dust, which\nexacerbate its already fragile atmospheric conditions. Traditional pollution\nmitigation strategies, such as static air purifying installations, often fail\nto maximize their impact due to suboptimal placement and limited adaptability\nto dynamic urban environments. This study presents a novel deep reinforcement\nlearning (DRL) framework to optimize the placement of air purification booths\nto improve the air quality index (AQI) in the city of Delhi. We employ Proximal\nPolicy Optimization (PPO), a state-of-the-art reinforcement learning algorithm,\nto iteratively learn and identify high-impact locations based on multiple\nspatial and environmental factors, including population density, traffic\npatterns, industrial influence, and green space constraints. Our approach is\nbenchmarked against conventional placement strategies, including random and\ngreedy AQI-based methods, using multi-dimensional performance evaluation\nmetrics such as AQI improvement, spatial coverage, population and traffic\nimpact, and spatial entropy. Experimental results demonstrate that the RL-based\napproach outperforms baseline methods by achieving a balanced and effective\ndistribution of air purification infrastructure. Notably, the DRL framework\nachieves an optimal trade-off between AQI reduction and high-coverage\ndeployment, ensuring equitable environmental benefits across urban regions. The\nfindings underscore the potential of AI-driven spatial optimization in\nadvancing smart city initiatives and data-driven urban air quality management.", "published": "2025-05-01 17:19:48", "link": "http://arxiv.org/abs/2505.00668v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Wasserstein Policy Optimization", "abstract": "We introduce Wasserstein Policy Optimization (WPO), an actor-critic algorithm\nfor reinforcement learning in continuous action spaces. WPO can be derived as\nan approximation to Wasserstein gradient flow over the space of all policies\nprojected into a finite-dimensional parameter space (e.g., the weights of a\nneural network), leading to a simple and completely general closed-form update.\nThe resulting algorithm combines many properties of deterministic and classic\npolicy gradient methods. Like deterministic policy gradients, it exploits\nknowledge of the gradient of the action-value function with respect to the\naction. Like classic policy gradients, it can be applied to stochastic policies\nwith arbitrary distributions over actions -- without using the\nreparameterization trick. We show results on the DeepMind Control Suite and a\nmagnetic confinement fusion task which compare favorably with state-of-the-art\ncontinuous control methods.", "published": "2025-05-01 17:07:01", "link": "http://arxiv.org/abs/2505.00663v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Open-Source LLM-Driven Federated Transformer for Predictive IoV Management", "abstract": "The proliferation of connected vehicles within the Internet of Vehicles (IoV)\necosystem presents critical challenges in ensuring scalable, real-time, and\nprivacy-preserving traffic management. Existing centralized IoV solutions often\nsuffer from high latency, limited scalability, and reliance on proprietary\nArtificial Intelligence (AI) models, creating significant barriers to\nwidespread deployment, particularly in dynamic and privacy-sensitive\nenvironments. Meanwhile, integrating Large Language Models (LLMs) in vehicular\nsystems remains underexplored, especially concerning prompt optimization and\neffective utilization in federated contexts. To address these challenges, we\npropose the Federated Prompt-Optimized Traffic Transformer (FPoTT), a novel\nframework that leverages open-source LLMs for predictive IoV management. FPoTT\nintroduces a dynamic prompt optimization mechanism that iteratively refines\ntextual prompts to enhance trajectory prediction. The architecture employs a\ndual-layer federated learning paradigm, combining lightweight edge models for\nreal-time inference with cloud-based LLMs to retain global intelligence. A\nTransformer-driven synthetic data generator is incorporated to augment training\nwith diverse, high-fidelity traffic scenarios in the Next Generation Simulation\n(NGSIM) format. Extensive evaluations demonstrate that FPoTT, utilizing\nEleutherAI Pythia-1B, achieves 99.86% prediction accuracy on real-world data\nwhile maintaining high performance on synthetic datasets. These results\nunderscore the potential of open-source LLMs in enabling secure, adaptive, and\nscalable IoV management, offering a promising alternative to proprietary\nsolutions in smart mobility ecosystems.", "published": "2025-05-01 16:54:21", "link": "http://arxiv.org/abs/2505.00651v1", "categories": ["cs.AI", "cs.ET", "cs.LG"], "primary_category": "cs.AI"}
{"title": "OmicsCL: Unsupervised Contrastive Learning for Cancer Subtype Discovery and Survival Stratification", "abstract": "Unsupervised learning of disease subtypes from multi-omics data presents a\nsignificant opportunity for advancing personalized medicine. We introduce\nOmicsCL, a modular contrastive learning framework that jointly embeds\nheterogeneous omics modalities-such as gene expression, DNA methylation, and\nmiRNA expression-into a unified latent space. Our method incorporates a\nsurvival-aware contrastive loss that encourages the model to learn\nrepresentations aligned with survival-related patterns, without relying on\nlabeled outcomes. Evaluated on the TCGA BRCA dataset, OmicsCL uncovers\nclinically meaningful clusters and achieves strong unsupervised concordance\nwith patient survival. The framework demonstrates robustness across\nhyperparameter configurations and can be tuned to prioritize either subtype\ncoherence or survival stratification. Ablation studies confirm that integrating\nsurvival-aware loss significantly enhances the predictive power of learned\nembeddings. These results highlight the promise of contrastive objectives for\nbiological insight discovery in high-dimensional, heterogeneous omics data.", "published": "2025-05-01 16:51:48", "link": "http://arxiv.org/abs/2505.00650v1", "categories": ["cs.LG", "cs.AI", "q-bio.GN", "q-bio.QM"], "primary_category": "cs.LG"}
{"title": "Deep Learning Assisted Outer Volume Removal for Highly-Accelerated Real-Time Dynamic MRI", "abstract": "Real-time (RT) dynamic MRI plays a vital role in capturing rapid\nphysiological processes, offering unique insights into organ motion and\nfunction. Among these applications, RT cine MRI is particularly important for\nfunctional assessment of the heart with high temporal resolution. RT imaging\nenables free-breathing, ungated imaging of cardiac motion, making it a crucial\nalternative for patients who cannot tolerate conventional breath-hold,\nECG-gated acquisitions. However, achieving high acceleration rates in RT cine\nMRI is challenging due to aliasing artifacts from extra-cardiac tissues,\nparticularly at high undersampling factors. In this study, we propose a novel\nouter volume removal (OVR) method to address this challenge by eliminating\naliasing contributions from non-cardiac regions in a post-processing framework.\nOur approach estimates the outer volume signal for each timeframe using\ncomposite temporal images from time-interleaved undersampling patterns, which\ninherently contain pseudo-periodic ghosting artifacts. A deep learning (DL)\nmodel is trained to identify and remove these artifacts, producing a clean\nouter volume estimate that is subsequently subtracted from the corresponding\nk-space data. The final reconstruction is performed with a physics-driven DL\n(PD-DL) method trained using an OVR-specific loss function to restore high\nspatio-temporal resolution images. Experimental results show that the proposed\nmethod at high accelerations achieves image quality that is visually comparable\nto clinical baseline images, while outperforming conventional reconstruction\ntechniques, both qualitatively and quantitatively. The proposed approach\nprovides a practical and effective solution for artifact reduction in RT cine\nMRI without requiring acquisition modifications, offering a pathway to higher\nacceleration rates while preserving diagnostic quality.", "published": "2025-05-01 16:31:52", "link": "http://arxiv.org/abs/2505.00643v1", "categories": ["eess.IV", "cs.AI", "cs.CV", "physics.med-ph"], "primary_category": "eess.IV"}
{"title": "Neural Network Verification for Gliding Drone Control: A Case Study", "abstract": "As machine learning is increasingly deployed in autonomous systems,\nverification of neural network controllers is becoming an active research\ndomain. Existing tools and annual verification competitions suggest that soon\nthis technology will become effective for real-world applications. Our\napplication comes from the emerging field of microflyers that are passively\ntransported by the wind, which may have various uses in weather or pollution\nmonitoring. Specifically, we investigate centimetre-scale bio-inspired gliding\ndrones that resemble Alsomitra macrocarpa diaspores. In this paper, we propose\na new case study on verifying Alsomitra-inspired drones with neural network\ncontrollers, with the aim of adhering closely to a target trajectory. We show\nthat our system differs substantially from existing VNN and ARCH competition\nbenchmarks, and show that a combination of tools holds promise for verifying\nsuch systems in the future, if certain shortcomings can be overcome. We propose\na novel method for robust training of regression networks, and investigate\nformalisations of this case study in Vehicle and CORA. Our verification results\nsuggest that the investigated training methods do improve performance and\nrobustness of neural network controllers in this application, but are limited\nin scope and usefulness. This is due to systematic limitations of both Vehicle\nand CORA, and the complexity of our system reducing the scale of reachability,\nwhich we investigate in detail. If these limitations can be overcome, it will\nenable engineers to develop safe and robust technologies that improve people's\nlives and reduce our impact on the environment.", "published": "2025-05-01 16:03:38", "link": "http://arxiv.org/abs/2505.00622v1", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "primary_category": "cs.RO"}
{"title": "Pixel3DMM: Versatile Screen-Space Priors for Single-Image 3D Face Reconstruction", "abstract": "We address the 3D reconstruction of human faces from a single RGB image. To\nthis end, we propose Pixel3DMM, a set of highly-generalized vision transformers\nwhich predict per-pixel geometric cues in order to constrain the optimization\nof a 3D morphable face model (3DMM). We exploit the latent features of the DINO\nfoundation model, and introduce a tailored surface normal and uv-coordinate\nprediction head. We train our model by registering three high-quality 3D face\ndatasets against the FLAME mesh topology, which results in a total of over\n1,000 identities and 976K images. For 3D face reconstruction, we propose a\nFLAME fitting opitmization that solves for the 3DMM parameters from the\nuv-coordinate and normal estimates. To evaluate our method, we introduce a new\nbenchmark for single-image face reconstruction, which features high diversity\nfacial expressions, viewing angles, and ethnicities. Crucially, our benchmark\nis the first to evaluate both posed and neutral facial geometry. Ultimately,\nour method outperforms the most competitive baselines by over 15% in terms of\ngeometric accuracy for posed facial expressions.", "published": "2025-05-01 15:47:03", "link": "http://arxiv.org/abs/2505.00615v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Position: AI Competitions Provide the Gold Standard for Empirical Rigor in GenAI Evaluation", "abstract": "In this position paper, we observe that empirical evaluation in Generative AI\nis at a crisis point since traditional ML evaluation and benchmarking\nstrategies are insufficient to meet the needs of evaluating modern GenAI models\nand systems. There are many reasons for this, including the fact that these\nmodels typically have nearly unbounded input and output spaces, typically do\nnot have a well defined ground truth target, and typically exhibit strong\nfeedback loops and prediction dependence based on context of previous model\noutputs. On top of these critical issues, we argue that the problems of {\\em\nleakage} and {\\em contamination} are in fact the most important and difficult\nissues to address for GenAI evaluations. Interestingly, the field of AI\nCompetitions has developed effective measures and practices to combat leakage\nfor the purpose of counteracting cheating by bad actors within a competition\nsetting. This makes AI Competitions an especially valuable (but underutilized)\nresource. Now is time for the field to view AI Competitions as the gold\nstandard for empirical rigor in GenAI evaluation, and to harness and harvest\ntheir results with according value.", "published": "2025-05-01 15:43:51", "link": "http://arxiv.org/abs/2505.00612v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Combining LLMs with Logic-Based Framework to Explain MCTS", "abstract": "In response to the lack of trust in Artificial Intelligence (AI) for\nsequential planning, we design a Computational Tree Logic-guided large language\nmodel (LLM)-based natural language explanation framework designed for the Monte\nCarlo Tree Search (MCTS) algorithm. MCTS is often considered challenging to\ninterpret due to the complexity of its search trees, but our framework is\nflexible enough to handle a wide range of free-form post-hoc queries and\nknowledge-based inquiries centered around MCTS and the Markov Decision Process\n(MDP) of the application domain. By transforming user queries into logic and\nvariable statements, our framework ensures that the evidence obtained from the\nsearch tree remains factually consistent with the underlying environmental\ndynamics and any constraints in the actual stochastic control process. We\nevaluate the framework rigorously through quantitative assessments, where it\ndemonstrates strong performance in terms of accuracy and factual consistency.", "published": "2025-05-01 15:40:58", "link": "http://arxiv.org/abs/2505.00610v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Can LLMs Help Improve Analogical Reasoning For Strategic Decisions? Experimental Evidence from Humans and GPT-4", "abstract": "This study investigates whether large language models, specifically GPT4, can\nmatch human capabilities in analogical reasoning within strategic decision\nmaking contexts. Using a novel experimental design involving source to target\nmatching, we find that GPT4 achieves high recall by retrieving all plausible\nanalogies but suffers from low precision, frequently applying incorrect\nanalogies based on superficial similarities. In contrast, human participants\nexhibit high precision but low recall, selecting fewer analogies yet with\nstronger causal alignment. These findings advance theory by identifying\nmatching, the evaluative phase of analogical reasoning, as a distinct step that\nrequires accurate causal mapping beyond simple retrieval. While current LLMs\nare proficient in generating candidate analogies, humans maintain a comparative\nadvantage in recognizing deep structural similarities across domains. Error\nanalysis reveals that AI errors arise from surface level matching, whereas\nhuman errors stem from misinterpretations of causal structure. Taken together,\nthe results suggest a productive division of labor in AI assisted\norganizational decision making where LLMs may serve as broad analogy\ngenerators, while humans act as critical evaluators, applying the most\ncontextually appropriate analogies to strategic problems.", "published": "2025-05-01 15:35:01", "link": "http://arxiv.org/abs/2505.00603v1", "categories": ["cs.AI", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Fast and Low-Cost Genomic Foundation Models via Outlier Removal", "abstract": "We propose the first unified adversarial attack benchmark for Genomic\nFoundation Models (GFMs), named GERM. Unlike existing GFM benchmarks, GERM\noffers the first comprehensive evaluation framework to systematically assess\nthe vulnerability of GFMs to adversarial attacks. Methodologically, we evaluate\nthe adversarial robustness of five state-of-the-art GFMs using four widely\nadopted attack algorithms and three defense strategies. Importantly, our\nbenchmark provides an accessible and comprehensive framework to analyze GFM\nvulnerabilities with respect to model architecture, quantization schemes, and\ntraining datasets. Empirically, transformer-based models exhibit greater\nrobustness to adversarial perturbations compared to HyenaDNA, highlighting the\nimpact of architectural design on vulnerability. Moreover, adversarial attacks\nfrequently target biologically significant genomic regions, suggesting that\nthese models effectively capture meaningful sequence features.", "published": "2025-05-01 15:31:09", "link": "http://arxiv.org/abs/2505.00598v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "A Finite-State Controller Based Offline Solver for Deterministic POMDPs", "abstract": "Deterministic partially observable Markov decision processes (DetPOMDPs)\noften arise in planning problems where the agent is uncertain about its\nenvironmental state but can act and observe deterministically. In this paper,\nwe propose DetMCVI, an adaptation of the Monte Carlo Value Iteration (MCVI)\nalgorithm for DetPOMDPs, which builds policies in the form of finite-state\ncontrollers (FSCs). DetMCVI solves large problems with a high success rate,\noutperforming existing baselines for DetPOMDPs. We also verify the performance\nof the algorithm in a real-world mobile robot forest mapping scenario.", "published": "2025-05-01 15:30:26", "link": "http://arxiv.org/abs/2505.00596v1", "categories": ["cs.RO", "cs.AI", "cs.LG", "I.2.8; I.2.9"], "primary_category": "cs.RO"}
{"title": "Synthesizing and Identifying Noise Levels in Autonomous Vehicle Camera Radar Datasets", "abstract": "Detecting and tracking objects is a crucial component of any autonomous\nnavigation method. For the past decades, object detection has yielded promising\nresults using neural networks on various datasets. While many methods focus on\nperformance metrics, few projects focus on improving the robustness of these\ndetection and tracking pipelines, notably to sensor failures. In this paper we\nattempt to address this issue by creating a realistic synthetic data\naugmentation pipeline for camera-radar Autonomous Vehicle (AV) datasets. Our\ngoal is to accurately simulate sensor failures and data deterioration due to\nreal-world interferences. We also present our results of a baseline lightweight\nNoise Recognition neural network trained and tested on our augmented dataset,\nreaching an overall recognition accuracy of 54.4\\% on 11 categories across\n10086 images and 2145 radar point-clouds.", "published": "2025-05-01 15:15:50", "link": "http://arxiv.org/abs/2505.00584v1", "categories": ["cs.CV", "cs.AI", "eess.IV", "eess.SP"], "primary_category": "cs.CV"}
{"title": "Voice Cloning: Comprehensive Survey", "abstract": "Voice Cloning has rapidly advanced in today's digital world, with many\nresearchers and corporations working to improve these algorithms for various\napplications. This article aims to establish a standardized terminology for\nvoice cloning and explore its different variations. It will cover speaker\nadaptation as the fundamental concept and then delve deeper into topics such as\nfew-shot, zero-shot, and multilingual TTS within that context. Finally, we will\nexplore the evaluation metrics commonly used in voice cloning research and\nrelated datasets. This survey compiles the available voice cloning algorithms\nto encourage research toward its generation and detection to limit its misuse.", "published": "2025-05-01 15:10:29", "link": "http://arxiv.org/abs/2505.00579v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multimodal Masked Autoencoder Pre-training for 3D MRI-Based Brain Tumor Analysis with Missing Modalities", "abstract": "Multimodal magnetic resonance imaging (MRI) constitutes the first line of\ninvestigation for clinicians in the care of brain tumors, providing crucial\ninsights for surgery planning, treatment monitoring, and biomarker\nidentification. Pre-training on large datasets have been shown to help models\nlearn transferable representations and adapt with minimal labeled data. This\nbehavior is especially valuable in medical imaging, where annotations are often\nscarce. However, applying this paradigm to multimodal medical data introduces a\nchallenge: most existing approaches assume that all imaging modalities are\navailable during both pre-training and fine-tuning. In practice, missing\nmodalities often occur due to acquisition issues, specialist unavailability, or\nspecific experimental designs on small in-house datasets. Consequently, a\ncommon approach involves training a separate model for each desired modality\ncombination, making the process both resource-intensive and impractical for\nclinical use. Therefore, we introduce BM-MAE, a masked image modeling\npre-training strategy tailored for multimodal MRI data. The same pre-trained\nmodel seamlessly adapts to any combination of available modalities, extracting\nrich representations that capture both intra- and inter-modal information. This\nallows fine-tuning on any subset of modalities without requiring architectural\nchanges, while still benefiting from a model pre-trained on the full set of\nmodalities. Extensive experiments show that the proposed pre-training strategy\noutperforms or remains competitive with baselines that require separate\npre-training for each modality subset, while substantially surpassing training\nfrom scratch on several downstream tasks. Additionally, it can quickly and\nefficiently reconstruct missing modalities, highlighting its practical value.\nCode and trained models are available at: https://github.com/Lucas-rbnt/bmmae", "published": "2025-05-01 14:51:30", "link": "http://arxiv.org/abs/2505.00568v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching", "abstract": "Learning to solve complex tasks with signal temporal logic (STL)\nspecifications is crucial to many real-world applications. However, most\nprevious works only consider fixed or parametrized STL specifications due to\nthe lack of a diverse STL dataset and encoders to effectively extract temporal\nlogic information for downstream tasks. In this paper, we propose TeLoGraF,\nTemporal Logic Graph-encoded Flow, which utilizes Graph Neural Networks (GNN)\nencoder and flow-matching to learn solutions for general STL specifications. We\nidentify four commonly used STL templates and collect a total of 200K\nspecifications with paired demonstrations. We conduct extensive experiments in\nfive simulation environments ranging from simple dynamical models in the 2D\nspace to high-dimensional 7DoF Franka Panda robot arm and Ant quadruped\nnavigation. Results show that our method outperforms other baselines in the STL\nsatisfaction rate. Compared to classical STL planning algorithms, our approach\nis 10-100X faster in inference and can work on any system dynamics. Besides, we\nshow our graph-encoding method's capability to solve complex STLs and\nrobustness to out-distribution STL specifications. Code is available at\nhttps://github.com/mengyuest/TeLoGraF", "published": "2025-05-01 14:40:07", "link": "http://arxiv.org/abs/2505.00562v1", "categories": ["cs.RO", "cs.AI", "cs.FL", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Learning to Learn with Quantum Optimization via Quantum Neural Networks", "abstract": "Quantum Approximate Optimization Algorithms (QAOA) promise efficient\nsolutions to classically intractable combinatorial optimization problems by\nharnessing shallow-depth quantum circuits. Yet, their performance and\nscalability often hinge on effective parameter optimization, which remains\nnontrivial due to rugged energy landscapes and hardware noise. In this work, we\nintroduce a quantum meta-learning framework that combines quantum neural\nnetworks, specifically Quantum Long Short-Term Memory (QLSTM) architectures,\nwith QAOA. By training the QLSTM optimizer on smaller graph instances, our\napproach rapidly generalizes to larger, more complex problems, substantially\nreducing the number of iterations required for convergence. Through\ncomprehensive benchmarks on Max-Cut and Sherrington-Kirkpatrick model\ninstances, we demonstrate that QLSTM-based optimizers converge faster and\nachieve higher approximation ratios compared to classical baselines, thereby\noffering a robust pathway toward scalable quantum optimization in the NISQ era.", "published": "2025-05-01 14:39:26", "link": "http://arxiv.org/abs/2505.00561v1", "categories": ["quant-ph", "cs.AI"], "primary_category": "quant-ph"}
{"title": "On the Mechanistic Interpretability of Neural Networks for Causality in Bio-statistics", "abstract": "Interpretable insights from predictive models remain critical in\nbio-statistics, particularly when assessing causality, where classical\nstatistical and machine learning methods often provide inherent clarity. While\nNeural Networks (NNs) offer powerful capabilities for modeling complex\nbiological data, their traditional \"black-box\" nature presents challenges for\nvalidation and trust in high-stakes health applications. Recent advances in\nMechanistic Interpretability (MI) aim to decipher the internal computations\nlearned by these networks. This work investigates the application of MI\ntechniques to NNs within the context of causal inference for bio-statistics.\n  We demonstrate that MI tools can be leveraged to: (1) probe and validate the\ninternal representations learned by NNs, such as those estimating nuisance\nfunctions in frameworks like Targeted Minimum Loss-based Estimation (TMLE); (2)\ndiscover and visualize the distinct computational pathways employed by the\nnetwork to process different types of inputs, potentially revealing how\nconfounders and treatments are handled; and (3) provide methodologies for\ncomparing the learned mechanisms and extracted insights across statistical,\nmachine learning, and NN models, fostering a deeper understanding of their\nrespective strengths and weaknesses for causal bio-statistical analysis.", "published": "2025-05-01 14:30:34", "link": "http://arxiv.org/abs/2505.00555v1", "categories": ["stat.AP", "cs.AI"], "primary_category": "stat.AP"}
{"title": "Test-time Correlation Alignment", "abstract": "Deep neural networks often experience performance drops due to distribution\nshifts between training and test data. Although domain adaptation offers a\nsolution, privacy concerns restrict access to training data in many real-world\nscenarios. This restriction has spurred interest in Test-Time Adaptation (TTA),\nwhich adapts models using only unlabeled test data. However, current TTA\nmethods still face practical challenges: (1) a primary focus on instance-wise\nalignment, overlooking CORrelation ALignment (CORAL) due to missing source\ncorrelations; (2) complex backpropagation operations for model updating,\nresulting in overhead computation and (3) domain forgetting.\n  To address these challenges, we provide a theoretical analysis to investigate\nthe feasibility of Test-time Correlation Alignment (TCA), demonstrating that\ncorrelation alignment between high-certainty instances and test instances can\nenhance test performances with a theoretical guarantee. Based on this, we\npropose two simple yet effective algorithms: LinearTCA and LinearTCA+.\nLinearTCA applies a simple linear transformation to achieve both instance and\ncorrelation alignment without additional model updates, while LinearTCA+ serves\nas a plug-and-play module that can easily boost existing TTA methods. Extensive\nexperiments validate our theoretical insights and show that TCA methods\nsignificantly outperforms baselines across various tasks, benchmarks and\nbackbones. Notably, LinearTCA improves adaptation accuracy by 5.88% on\nOfficeHome dataset, while using only 4% maximum GPU memory usage and 0.6%\ncomputation time compared to the best baseline TTA method.", "published": "2025-05-01 13:59:13", "link": "http://arxiv.org/abs/2505.00533v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Safety-Critical Traffic Simulation with Guided Latent Diffusion Model", "abstract": "Safety-critical traffic simulation plays a crucial role in evaluating\nautonomous driving systems under rare and challenging scenarios. However,\nexisting approaches often generate unrealistic scenarios due to insufficient\nconsideration of physical plausibility and suffer from low generation\nefficiency. To address these limitations, we propose a guided latent diffusion\nmodel (LDM) capable of generating physically realistic and adversarial\nsafety-critical traffic scenarios. Specifically, our model employs a\ngraph-based variational autoencoder (VAE) to learn a compact latent space that\ncaptures complex multi-agent interactions while improving computational\nefficiency. Within this latent space, the diffusion model performs the\ndenoising process to produce realistic trajectories. To enable controllable and\nadversarial scenario generation, we introduce novel guidance objectives that\ndrive the diffusion process toward producing adversarial and behaviorally\nrealistic driving behaviors. Furthermore, we develop a sample selection module\nbased on physical feasibility checks to further enhance the physical\nplausibility of the generated scenarios. Extensive experiments on the nuScenes\ndataset demonstrate that our method achieves superior adversarial effectiveness\nand generation efficiency compared to existing baselines while maintaining a\nhigh level of realism. Our work provides an effective tool for realistic\nsafety-critical scenario simulation, paving the way for more robust evaluation\nof autonomous driving systems.", "published": "2025-05-01 13:33:34", "link": "http://arxiv.org/abs/2505.00515v1", "categories": ["cs.RO", "cs.AI", "cs.MA"], "primary_category": "cs.RO"}
{"title": "Variational OOD State Correction for Offline Reinforcement Learning", "abstract": "The performance of Offline reinforcement learning is significantly impacted\nby the issue of state distributional shift, and out-of-distribution (OOD) state\ncorrection is a popular approach to address this problem. In this paper, we\npropose a novel method named Density-Aware Safety Perception (DASP) for OOD\nstate correction. Specifically, our method encourages the agent to prioritize\nactions that lead to outcomes with higher data density, thereby promoting its\noperation within or the return to in-distribution (safe) regions. To achieve\nthis, we optimize the objective within a variational framework that\nconcurrently considers both the potential outcomes of decision-making and their\ndensity, thus providing crucial contextual information for safe\ndecision-making. Finally, we validate the effectiveness and feasibility of our\nproposed method through extensive experimental evaluations on the offline\nMuJoCo and AntMaze suites.", "published": "2025-05-01 13:14:07", "link": "http://arxiv.org/abs/2505.00503v1", "categories": ["cs.LG", "cs.AI", "cs.RO"], "primary_category": "cs.LG"}
{"title": "Optimal Interactive Learning on the Job via Facility Location Planning", "abstract": "Collaborative robots must continually adapt to novel tasks and user\npreferences without overburdening the user. While prior interactive robot\nlearning methods aim to reduce human effort, they are typically limited to\nsingle-task scenarios and are not well-suited for sustained, multi-task\ncollaboration. We propose COIL (Cost-Optimal Interactive Learning) -- a\nmulti-task interaction planner that minimizes human effort across a sequence of\ntasks by strategically selecting among three query types (skill, preference,\nand help). When user preferences are known, we formulate COIL as an\nuncapacitated facility location (UFL) problem, which enables bounded-suboptimal\nplanning in polynomial time using off-the-shelf approximation algorithms. We\nextend our formulation to handle uncertainty in user preferences by\nincorporating one-step belief space planning, which uses these approximation\nalgorithms as subroutines to maintain polynomial-time performance. Simulated\nand physical experiments on manipulation tasks show that our framework\nsignificantly reduces the amount of work allocated to the human while\nmaintaining successful task completion.", "published": "2025-05-01 12:45:09", "link": "http://arxiv.org/abs/2505.00490v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO"}
{"title": "MULE: Multi-terrain and Unknown Load Adaptation for Effective Quadrupedal Locomotion", "abstract": "Quadrupedal robots are increasingly deployed for load-carrying tasks across\ndiverse terrains. While Model Predictive Control (MPC)-based methods can\naccount for payload variations, they often depend on predefined gait schedules\nor trajectory generators, limiting their adaptability in unstructured\nenvironments. To address these limitations, we propose an Adaptive\nReinforcement Learning (RL) framework that enables quadrupedal robots to\ndynamically adapt to both varying payloads and diverse terrains. The framework\nconsists of a nominal policy responsible for baseline locomotion and an\nadaptive policy that learns corrective actions to preserve stability and\nimprove command tracking under payload variations. We validate the proposed\napproach through large-scale simulation experiments in Isaac Gym and real-world\nhardware deployment on a Unitree Go1 quadruped. The controller was tested on\nflat ground, slopes, and stairs under both static and dynamic payload changes.\nAcross all settings, our adaptive controller consistently outperformed the\ncontroller in tracking body height and velocity commands, demonstrating\nenhanced robustness and adaptability without requiring explicit gait design or\nmanual tuning.", "published": "2025-05-01 12:41:35", "link": "http://arxiv.org/abs/2505.00488v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO"}
{"title": "Analysis of the vulnerability of machine learning regression models to adversarial attacks using data from 5G wireless networks", "abstract": "This article describes the process of creating a script and conducting an\nanalytical study of a dataset using the DeepMIMO emulator. An advertorial\nattack was carried out using the FGSM method to maximize the gradient. A\ncomparison is made of the effectiveness of binary classifiers in the task of\ndetecting distorted data. The dynamics of changes in the quality indicators of\nthe regression model were analyzed in conditions without adversarial attacks,\nduring an adversarial attack and when the distorted data was isolated. It is\nshown that an adversarial FGSM attack with gradient maximization leads to an\nincrease in the value of the MSE metric by 33% and a decrease in the R2\nindicator by 10% on average. The LightGBM binary classifier effectively\nidentifies data with adversarial anomalies with 98% accuracy. Regression\nmachine learning models are susceptible to adversarial attacks, but rapid\nanalysis of network traffic and data transmitted over the network makes it\npossible to identify malicious activity", "published": "2025-05-01 12:36:05", "link": "http://arxiv.org/abs/2505.00487v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR"}
{"title": "JointDiT: Enhancing RGB-Depth Joint Modeling with Diffusion Transformers", "abstract": "We present JointDiT, a diffusion transformer that models the joint\ndistribution of RGB and depth. By leveraging the architectural benefit and\noutstanding image prior of the state-of-the-art diffusion transformer, JointDiT\nnot only generates high-fidelity images but also produces geometrically\nplausible and accurate depth maps. This solid joint distribution modeling is\nachieved through two simple yet effective techniques that we propose, i.e.,\nadaptive scheduling weights, which depend on the noise levels of each modality,\nand the unbalanced timestep sampling strategy. With these techniques, we train\nour model across all noise levels for each modality, enabling JointDiT to\nnaturally handle various combinatorial generation tasks, including joint\ngeneration, depth estimation, and depth-conditioned image generation by simply\ncontrolling the timestep of each branch. JointDiT demonstrates outstanding\njoint generation performance. Furthermore, it achieves comparable results in\ndepth estimation and depth-conditioned image generation, suggesting that joint\ndistribution modeling can serve as a replaceable alternative to conditional\ngeneration. The project page is available at\nhttps://byungki-k.github.io/JointDiT/.", "published": "2025-05-01 12:21:23", "link": "http://arxiv.org/abs/2505.00482v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Rule-based Classifier Models", "abstract": "We extend the formal framework of classifier models used in the legal domain.\nWhile the existing classifier framework characterises cases solely through the\nfacts involved, legal reasoning fundamentally relies on both facts and rules,\nparticularly the ratio decidendi. This paper presents an initial approach to\nincorporating sets of rules within a classifier. Our work is built on the work\nof Canavotto et al. (2023), which has developed the rule-based reason model of\nprecedential constraint within a hierarchy of factors. We demonstrate how\ndecisions for new cases can be inferred using this enriched rule-based\nclassifier framework. Additionally, we provide an example of how the time\nelement and the hierarchy of courts can be used in the new classifier\nframework.", "published": "2025-05-01 11:59:16", "link": "http://arxiv.org/abs/2505.00474v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "UserCentrix: An Agentic Memory-augmented AI Framework for Smart Spaces", "abstract": "Agentic AI, with its autonomous and proactive decision-making, has\ntransformed smart environments. By integrating Generative AI (GenAI) and\nmulti-agent systems, modern AI frameworks can dynamically adapt to user\npreferences, optimize data management, and improve resource allocation. This\npaper introduces UserCentrix, an agentic memory-augmented AI framework designed\nto enhance smart spaces through dynamic, context-aware decision-making. This\nframework integrates personalized Large Language Model (LLM) agents that\nleverage user preferences and LLM memory management to deliver proactive and\nadaptive assistance. Furthermore, it incorporates a hybrid hierarchical control\nsystem, balancing centralized and distributed processing to optimize real-time\nresponsiveness while maintaining global situational awareness. UserCentrix\nachieves resource-efficient AI interactions by embedding memory-augmented\nreasoning, cooperative agent negotiation, and adaptive orchestration\nstrategies. Our key contributions include (i) a self-organizing framework with\nproactive scaling based on task urgency, (ii) a Value of Information\n(VoI)-driven decision-making process, (iii) a meta-reasoning personal LLM\nagent, and (iv) an intelligent multi-agent coordination system for seamless\nenvironment adaptation. Experimental results across various models confirm the\neffectiveness of our approach in enhancing response accuracy, system\nefficiency, and computational resource management in real-world application.", "published": "2025-05-01 11:54:49", "link": "http://arxiv.org/abs/2505.00472v1", "categories": ["cs.AI", "cs.DC", "cs.MA", "cs.NI"], "primary_category": "cs.AI"}
{"title": "Data Therapist: Eliciting Domain Knowledge from Subject Matter Experts Using Large Language Models", "abstract": "Effective data visualization requires not only technical proficiency but also\na deep understanding of the domain-specific context in which data exists. This\ncontext often includes tacit knowledge about data provenance, quality, and\nintended use, which is rarely explicit in the dataset itself. We present the\nData Therapist, a web-based tool that helps domain experts externalize this\nimplicit knowledge through a mixed-initiative process combining iterative Q&A\nwith interactive annotation. Powered by a large language model, the system\nanalyzes user-supplied datasets, prompts users with targeted questions, and\nallows annotation at varying levels of granularity. The resulting structured\nknowledge base can inform both human and automated visualization design. We\nevaluated the tool in a qualitative study involving expert pairs from Molecular\nBiology, Accounting, Political Science, and Usable Security. The study revealed\nrecurring patterns in how experts reason about their data and highlights areas\nwhere AI support can improve visualization design.", "published": "2025-05-01 11:10:17", "link": "http://arxiv.org/abs/2505.00455v1", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC"}
{"title": "Per-Domain Generalizing Policies: On Validation Instances and Scaling Behavior", "abstract": "Recent work has shown that successful per-domain generalizing action policies\ncan be learned. Scaling behavior, from small training instances to large test\ninstances, is the key objective; and the use of validation instances larger\nthan training instances is one key to achieve it. Prior work has used fixed\nvalidation sets. Here, we introduce a method generating the validation set\ndynamically, on the fly, increasing instance size so long as informative and\nfeasible.We also introduce refined methodology for evaluating scaling behavior,\ngenerating test instances systematically to guarantee a given confidence in\ncoverage performance for each instance size. In experiments, dynamic validation\nimproves scaling behavior of GNN policies in all 9 domains used.", "published": "2025-05-01 10:32:02", "link": "http://arxiv.org/abs/2505.00439v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "ScaleTrack: Scaling and back-tracking Automated GUI Agents", "abstract": "Automated GUI agents aims to facilitate user interaction by automatically\nperforming complex tasks in digital environments, such as web, mobile, desktop\ndevices. It receives textual task instruction and GUI description to generate\nexecutable actions (\\emph{e.g.}, click) and operation boxes step by step.\nTraining a GUI agent mainly involves grounding and planning stages, in which\nthe GUI grounding focuses on finding the execution coordinates according to the\ntask, while the planning stage aims to predict the next action based on\nhistorical actions. However, previous work suffers from the limitations of\ninsufficient training data for GUI grounding, as well as the ignorance of\nbacktracking historical behaviors for GUI planning. To handle the above\nchallenges, we propose ScaleTrack, a training framework by scaling grounding\nand backtracking planning for automated GUI agents. We carefully collected GUI\nsamples of different synthesis criterions from a wide range of sources, and\nunified them into the same template for training GUI grounding models.\nMoreover, we design a novel training strategy that predicts the next action\nfrom the current GUI image, while also backtracking the historical actions that\nled to the GUI image. In this way, ScaleTrack explains the correspondence\nbetween GUI images and actions, which effectively describes the evolution rules\nof the GUI environment. Extensive experimental results demonstrate the\neffectiveness of ScaleTrack. Data and code will be available at url.", "published": "2025-05-01 09:27:13", "link": "http://arxiv.org/abs/2505.00416v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Perceptual Implications of Automatic Anonymization in Pathological Speech", "abstract": "Automatic anonymization techniques are essential for ethical sharing of\npathological speech data, yet their perceptual consequences remain\nunderstudied. This study presents the first comprehensive human-centered\nanalysis of anonymized pathological speech, using a structured perceptual\nprotocol involving ten native and non-native German listeners with diverse\nlinguistic, clinical, and technical backgrounds. Listeners evaluated\nanonymized-original utterance pairs from 180 speakers spanning Cleft Lip and\nPalate, Dysarthria, Dysglossia, Dysphonia, and age-matched healthy controls.\nSpeech was anonymized using state-of-the-art automatic methods (equal error\nrates in the range of 30-40%). Listeners completed Turing-style discrimination\nand quality rating tasks under zero-shot (single-exposure) and few-shot\n(repeated-exposure) conditions. Discrimination accuracy was high overall (91%\nzero-shot; 93% few-shot), but varied by disorder (repeated-measures ANOVA:\np=0.007), ranging from 96% (Dysarthria) to 86% (Dysphonia). Anonymization\nconsistently reduced perceived quality (from 83% to 59%, p<0.001), with\npathology-specific degradation patterns (one-way ANOVA: p=0.005). Native\nlisteners rated original speech slightly higher than non-native listeners\n(Delta=4%, p=0.199), but this difference nearly disappeared after anonymization\n(Delta=1%, p=0.724). No significant gender-based bias was observed. Critically,\nhuman perceptual outcomes did not correlate with automatic privacy or clinical\nutility metrics. These results underscore the need for listener-informed,\ndisorder- and context-specific anonymization strategies that preserve privacy\nwhile maintaining interpretability, communicative functions, and diagnostic\nutility, especially for vulnerable populations such as children.", "published": "2025-05-01 09:03:03", "link": "http://arxiv.org/abs/2505.00409v1", "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "eess.AS"}
{"title": "DeepSTA: A Spatial-Temporal Attention Network for Logistics Delivery Timely Rate Prediction in Anomaly Conditions", "abstract": "Prediction of couriers' delivery timely rates in advance is essential to the\nlogistics industry, enabling companies to take preemptive measures to ensure\nthe normal operation of delivery services. This becomes even more critical\nduring anomaly conditions like the epidemic outbreak, during which couriers'\ndelivery timely rate will decline markedly and fluctuates significantly.\nExisting studies pay less attention to the logistics scenario. Moreover, many\nworks focusing on prediction tasks in anomaly scenarios fail to explicitly\nmodel abnormal events, e.g., treating external factors equally with other\nfeatures, resulting in great information loss. Further, since some anomalous\nevents occur infrequently, traditional data-driven methods perform poorly in\nthese scenarios. To deal with them, we propose a deep spatial-temporal\nattention model, named DeepSTA. To be specific, to avoid information loss, we\ndesign an anomaly spatio-temporal learning module that employs a recurrent\nneural network to model incident information. Additionally, we utilize Node2vec\nto model correlations between road districts, and adopt graph neural networks\nand long short-term memory to capture the spatial-temporal dependencies of\ncouriers. To tackle the issue of insufficient training data in abnormal\ncircumstances, we propose an anomaly pattern attention module that adopts a\nmemory network for couriers' anomaly feature patterns storage via attention\nmechanisms. The experiments on real-world logistics datasets during the\nCOVID-19 outbreak in 2022 show the model outperforms the best baselines by\n12.11% in MAE and 13.71% in MSE, demonstrating its superior performance over\nmultiple competitive baselines.", "published": "2025-05-01 08:48:45", "link": "http://arxiv.org/abs/2505.00402v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Learning to Estimate Package Delivery Time in Mixed Imbalanced Delivery and Pickup Logistics Services", "abstract": "Accurately estimating package delivery time is essential to the logistics\nindustry, which enables reasonable work allocation and on-time service\nguarantee. This becomes even more necessary in mixed logistics scenarios where\ncouriers handle a high volume of delivery and a smaller number of pickup\nsimultaneously. However, most of the related works treat the pickup and\ndelivery patterns on couriers' decision behavior equally, neglecting that the\npickup has a greater impact on couriers' decision-making compared to the\ndelivery due to its tighter time constraints. In such context, we have three\nmain challenges: 1) multiple spatiotemporal factors are intricately\ninterconnected, significantly affecting couriers' delivery behavior; 2) pickups\nhave stricter time requirements but are limited in number, making it\nchallenging to model their effects on couriers' delivery process; 3) couriers'\nspatial mobility patterns are critical determinants of their delivery behavior,\nbut have been insufficiently explored. To deal with these, we propose TransPDT,\na Transformer-based multi-task package delivery time prediction model. We first\nemploy the Transformer encoder architecture to capture the spatio-temporal\ndependencies of couriers' historical travel routes and pending package sets.\nThen we design the pattern memory to learn the patterns of pickup in the\nimbalanced dataset via attention mechanism. We also set the route prediction as\nan auxiliary task of delivery time prediction, and incorporate the prior\ncourier spatial movement regularities in prediction. Extensive experiments on\nreal industry-scale datasets demonstrate the superiority of our method. A\nsystem based on TransPDT is deployed internally in JD Logistics to track more\nthan 2000 couriers handling hundreds of thousands of packages per day in\nBeijing.", "published": "2025-05-01 08:00:22", "link": "http://arxiv.org/abs/2505.00375v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Urban Air Mobility as a System of Systems: An LLM-Enhanced Holonic Approach", "abstract": "Urban Air Mobility (UAM) is an emerging System of System (SoS) that faces\nchallenges in system architecture, planning, task management, and execution.\nTraditional architectural approaches struggle with scalability, adaptability,\nand seamless resource integration within dynamic and complex environments. This\npaper presents an intelligent holonic architecture that incorporates Large\nLanguage Model (LLM) to manage the complexities of UAM. Holons function semi\nautonomously, allowing for real time coordination among air taxis, ground\ntransport, and vertiports. LLMs process natural language inputs, generate\nadaptive plans, and manage disruptions such as weather changes or airspace\nclosures.Through a case study of multimodal transportation with electric\nscooters and air taxis, we demonstrate how this architecture enables dynamic\nresource allocation, real time replanning, and autonomous adaptation without\ncentralized control, creating more resilient and efficient urban transportation\nnetworks. By advancing decentralized control and AI driven adaptability, this\nwork lays the groundwork for resilient, human centric UAM ecosystems, with\nfuture efforts targeting hybrid AI integration and real world validation.", "published": "2025-05-01 07:39:11", "link": "http://arxiv.org/abs/2505.00368v1", "categories": ["cs.AI", "cs.ET", "cs.MA", "cs.RO"], "primary_category": "cs.AI"}
{"title": "SacFL: Self-Adaptive Federated Continual Learning for Resource-Constrained End Devices", "abstract": "The proliferation of end devices has led to a distributed computing paradigm,\nwherein on-device machine learning models continuously process diverse data\ngenerated by these devices. The dynamic nature of this data, characterized by\ncontinuous changes or data drift, poses significant challenges for on-device\nmodels. To address this issue, continual learning (CL) is proposed, enabling\nmachine learning models to incrementally update their knowledge and mitigate\ncatastrophic forgetting. However, the traditional centralized approach to CL is\nunsuitable for end devices due to privacy and data volume concerns. In this\ncontext, federated continual learning (FCL) emerges as a promising solution,\npreserving user data locally while enhancing models through collaborative\nupdates. Aiming at the challenges of limited storage resources for CL, poor\nautonomy in task shift detection, and difficulty in coping with new adversarial\ntasks in FCL scenario, we propose a novel FCL framework named SacFL. SacFL\nemploys an Encoder-Decoder architecture to separate task-robust and\ntask-sensitive components, significantly reducing storage demands by retaining\nlightweight task-sensitive components for resource-constrained end devices.\nMoreover, $\\rm{SacFL}$ leverages contrastive learning to introduce an\nautonomous data shift detection mechanism, enabling it to discern whether a new\ntask has emerged and whether it is a benign task. This capability ultimately\nallows the device to autonomously trigger CL or attack defense strategy without\nadditional information, which is more practical for end devices. Comprehensive\nexperiments conducted on multiple text and image datasets, such as Cifar100 and\nTHUCNews, have validated the effectiveness of $\\rm{SacFL}$ in both\nclass-incremental and domain-incremental scenarios. Furthermore, a demo system\nhas been developed to verify its practicality.", "published": "2025-05-01 07:26:35", "link": "http://arxiv.org/abs/2505.00365v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "TNStream: Applying Tightest Neighbors to Micro-Clusters to Define Multi-Density Clusters in Streaming Data", "abstract": "In data stream clustering, systematic theory of stream clustering algorithms\nremains relatively scarce. Recently, density-based methods have gained\nattention. However, existing algorithms struggle to simultaneously handle\narbitrarily shaped, multi-density, high-dimensional data while maintaining\nstrong outlier resistance. Clustering quality significantly deteriorates when\ndata density varies complexly. This paper proposes a clustering algorithm based\non the novel concept of Tightest Neighbors and introduces a data stream\nclustering theory based on the Skeleton Set. Based on these theories, this\npaper develops a new method, TNStream, a fully online algorithm. The algorithm\nadaptively determines the clustering radius based on local similarity,\nsummarizing the evolution of multi-density data streams in micro-clusters. It\nthen applies a Tightest Neighbors-based clustering algorithm to form final\nclusters. To improve efficiency in high-dimensional cases, Locality-Sensitive\nHashing (LSH) is employed to structure micro-clusters, addressing the challenge\nof storing k-nearest neighbors. TNStream is evaluated on various synthetic and\nreal-world datasets using different clustering metrics. Experimental results\ndemonstrate its effectiveness in improving clustering quality for multi-density\ndata and validate the proposed data stream clustering theory.", "published": "2025-05-01 07:15:20", "link": "http://arxiv.org/abs/2505.00359v1", "categories": ["cs.LG", "cs.AI", "cs.NE", "68T05, 68W20", "H.2.8; I.5.3"], "primary_category": "cs.LG"}
{"title": "Optimizing Deep Neural Networks using Safety-Guided Self Compression", "abstract": "The deployment of deep neural networks on resource-constrained devices\nnecessitates effective model com- pression strategies that judiciously balance\nthe reduction of model size with the preservation of performance. This study\nintroduces a novel safety-driven quantization framework that leverages\npreservation sets to systematically prune and quantize neural network weights,\nthereby optimizing model complexity without compromising accuracy. The proposed\nmethodology is rigorously evaluated on both a convolutional neural network\n(CNN) and an attention-based language model, demonstrating its applicability\nacross diverse architectural paradigms. Experimental results reveal that our\nframework achieves up to a 2.5% enhancement in test accuracy relative to the\noriginal unquantized models while maintaining 60% of the initial model size. In\ncomparison to conventional quantization techniques, our approach not only\naugments generalization by eliminating parameter noise and retaining essential\nweights but also reduces variance, thereby ensuring the retention of critical\nmodel features. These findings underscore the efficacy of safety-driven\nquantization as a robust and reliable strategy for the efficient optimization\nof deep learn- ing models. The implementation and comprehensive experimental\nevaluations of our framework are publicly accessible at GitHub.", "published": "2025-05-01 06:50:30", "link": "http://arxiv.org/abs/2505.00350v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Pushing the Limits of Low-Bit Optimizers: A Focus on EMA Dynamics", "abstract": "The explosion in model sizes leads to continued growth in prohibitive\ntraining/fine-tuning costs, particularly for stateful optimizers which maintain\nauxiliary information of even 2x the model size to achieve optimal convergence.\nWe therefore present in this work a novel type of optimizer that carries with\nextremely lightweight state overloads, achieved through ultra-low-precision\nquantization. While previous efforts have achieved certain success with 8-bit\nor 4-bit quantization, our approach enables optimizers to operate at precision\nas low as 3 bits, or even 2 bits per state element. This is accomplished by\nidentifying and addressing two critical challenges: the signal swamping problem\nin unsigned quantization that results in unchanged state dynamics, and the\nrapidly increased gradient variance in signed quantization that leads to\nincorrect descent directions. The theoretical analysis suggests a tailored\nlogarithmic quantization for the former and a precision-specific momentum value\nfor the latter. Consequently, the proposed SOLO achieves substantial memory\nsavings (approximately 45 GB when training a 7B model) with minimal accuracy\nloss. We hope that SOLO can contribute to overcoming the bottleneck in\ncomputational resources, thereby promoting greater accessibility in fundamental\nresearch.", "published": "2025-05-01 06:47:45", "link": "http://arxiv.org/abs/2505.00347v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Efficient Neural Video Representation with Temporally Coherent Modulation", "abstract": "Implicit neural representations (INR) has found successful applications\nacross diverse domains. To employ INR in real-life, it is important to speed up\ntraining. In the field of INR for video applications, the state-of-the-art\napproach employs grid-type parametric encoding and successfully achieves a\nfaster encoding speed in comparison to its predecessors. However, the grid\nusage, which does not consider the video's dynamic nature, leads to redundant\nuse of trainable parameters. As a result, it has significantly lower parameter\nefficiency and higher bitrate compared to NeRV-style methods that do not use a\nparametric encoding. To address the problem, we propose Neural Video\nrepresentation with Temporally coherent Modulation (NVTM), a novel framework\nthat can capture dynamic characteristics of video. By decomposing the\nspatio-temporal 3D video data into a set of 2D grids with flow information,\nNVTM enables learning video representation rapidly and uses parameter\nefficiently. Our framework enables to process temporally corresponding pixels\nat once, resulting in the fastest encoding speed for a reasonable video\nquality, especially when compared to the NeRV-style method, with a speed\nincrease of over 3 times. Also, it remarks an average of 1.54dB/0.019\nimprovements in PSNR/LPIPS on UVG (Dynamic) (even with 10% fewer parameters)\nand an average of 1.84dB/0.013 improvements in PSNR/LPIPS on MCL-JCV (Dynamic),\ncompared to previous grid-type works. By expanding this to compression tasks,\nwe demonstrate comparable performance to video compression standards (H.264,\nHEVC) and recent INR approaches for video compression. Additionally, we perform\nextensive experiments demonstrating the superior performance of our algorithm\nacross diverse tasks, encompassing super resolution, frame interpolation and\nvideo inpainting. Project page is https://sujiikim.github.io/NVTM/.", "published": "2025-05-01 06:20:42", "link": "http://arxiv.org/abs/2505.00335v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "CognitionNet: A Collaborative Neural Network for Play Style Discovery in Online Skill Gaming Platform", "abstract": "Games are one of the safest source of realizing self-esteem and relaxation at\nthe same time. An online gaming platform typically has massive data coming in,\ne.g., in-game actions, player moves, clickstreams, transactions etc. It is\nrather interesting, as something as simple as data on gaming moves can help\ncreate a psychological imprint of the user at that moment, based on her\nimpulsive reactions and response to a situation in the game. Mining this\nknowledge can: (a) immediately help better explain observed and predicted\nplayer behavior; and (b) consequently propel deeper understanding towards\nplayers' experience, growth and protection. To this effect, we focus on\ndiscovery of the \"game behaviours\" as micro-patterns formed by continuous\nsequence of games and the persistent \"play styles\" of the players' as a\nsequence of such sequences on an online skill gaming platform for Rummy. We\npropose a two stage deep neural network, CognitionNet. The first stage focuses\non mining game behaviours as cluster representations in a latent space while\nthe second aggregates over these micro patterns to discover play styles via a\nsupervised classification objective around player engagement. The dual\nobjective allows CognitionNet to reveal several player psychology inspired\ndecision making and tactics. To our knowledge, this is the first and\none-of-its-kind research to fully automate the discovery of: (i) player\npsychology and game tactics from telemetry data; and (ii) relevant diagnostic\nexplanations to players' engagement predictions. The collaborative training of\nthe two networks with differential input dimensions is enabled using a novel\nformulation of \"bridge loss\". The network plays pivotal role in obtaining\nhomogeneous and consistent play style definitions and significantly outperforms\nthe SOTA baselines wherever applicable.", "published": "2025-05-01 05:51:19", "link": "http://arxiv.org/abs/2505.00325v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI"}
{"title": "AI2-Active Safety: AI-enabled Interaction-aware Active Safety Analysis with Vehicle Dynamics", "abstract": "This paper introduces an AI-enabled, interaction-aware active safety analysis\nframework that accounts for groupwise vehicle interactions. Specifically, the\nframework employs a bicycle model-augmented with road gradient\nconsiderations-to accurately capture vehicle dynamics. In parallel, a\nhypergraph-based AI model is developed to predict probabilistic trajectories of\nambient traffic. By integrating these two components, the framework derives\nvehicle intra-spacing over a 3D road surface as the solution of a stochastic\nordinary differential equation, yielding high-fidelity surrogate safety\nmeasures such as time-to-collision (TTC). To demonstrate its effectiveness, the\nframework is analyzed using stochastic numerical methods comprising 4th-order\nRunge-Kutta integration and AI inference, generating probability-weighted\nhigh-fidelity TTC (HF-TTC) distributions that reflect complex multi-agent\nmaneuvers and behavioral uncertainties. Evaluated with HF-TTC against\ntraditional constant-velocity TTC and non-interaction-aware approaches on\nhighway datasets, the proposed framework offers a systematic methodology for\nactive safety analysis with enhanced potential for improving safety perception\nin complex traffic environments.", "published": "2025-05-01 05:46:34", "link": "http://arxiv.org/abs/2505.00322v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO"}
{"title": "Surrogate modeling of Cellular-Potts Agent-Based Models as a segmentation task using the U-Net neural network architecture", "abstract": "The Cellular-Potts model is a powerful and ubiquitous framework for\ndeveloping computational models for simulating complex multicellular biological\nsystems. Cellular-Potts models (CPMs) are often computationally expensive due\nto the explicit modeling of interactions among large numbers of individual\nmodel agents and diffusive fields described by partial differential equations\n(PDEs). In this work, we develop a convolutional neural network (CNN) surrogate\nmodel using a U-Net architecture that accounts for periodic boundary\nconditions. We use this model to accelerate the evaluation of a mechanistic CPM\npreviously used to investigate \\textit{in vitro} vasculogenesis. The surrogate\nmodel was trained to predict 100 computational steps ahead (Monte-Carlo steps,\nMCS), accelerating simulation evaluations by a factor of 590 times compared to\nCPM code execution. Over multiple recursive evaluations, our model effectively\ncaptures the emergent behaviors demonstrated by the original Cellular-Potts\nmodel of such as vessel sprouting, extension and anastomosis, and contraction\nof vascular lacunae. This approach demonstrates the potential for deep learning\nto serve as efficient surrogate models for CPM simulations, enabling faster\nevaluation of computationally expensive CPM of biological processes at greater\nspatial and temporal scales.", "published": "2025-05-01 05:30:38", "link": "http://arxiv.org/abs/2505.00316v1", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "primary_category": "cs.LG"}
{"title": "AI-Assisted Decision-Making for Clinical Assessment of Auto-Segmented Contour Quality", "abstract": "Purpose: This study presents a Deep Learning (DL)-based quality assessment\n(QA) approach for evaluating auto-generated contours (auto-contours) in\nradiotherapy, with emphasis on Online Adaptive Radiotherapy (OART). Leveraging\nBayesian Ordinal Classification (BOC) and calibrated uncertainty thresholds,\nthe method enables confident QA predictions without relying on ground truth\ncontours or extensive manual labeling. Methods: We developed a BOC model to\nclassify auto-contour quality and quantify prediction uncertainty. A\ncalibration step was used to optimize uncertainty thresholds that meet clinical\naccuracy needs. The method was validated under three data scenarios: no manual\nlabels, limited labels, and extensive labels. For rectum contours in prostate\ncancer, we applied geometric surrogate labels when manual labels were absent,\ntransfer learning when limited, and direct supervision when ample labels were\navailable. Results: The BOC model delivered robust performance across all\nscenarios. Fine-tuning with just 30 manual labels and calibrating with 34\nsubjects yielded over 90% accuracy on test data. Using the calibrated\nthreshold, over 93% of the auto-contours' qualities were accurately predicted\nin over 98% of cases, reducing unnecessary manual reviews and highlighting\ncases needing correction. Conclusion: The proposed QA model enhances contouring\nefficiency in OART by reducing manual workload and enabling fast, informed\nclinical decisions. Through uncertainty quantification, it ensures safer, more\nreliable radiotherapy workflows.", "published": "2025-05-01 05:05:35", "link": "http://arxiv.org/abs/2505.00308v1", "categories": ["cs.CV", "cs.AI", "stat.AP"], "primary_category": "cs.CV"}
{"title": "Fine-grained spatial-temporal perception for gas leak segmentation", "abstract": "Gas leaks pose significant risks to human health and the environment. Despite\nlong-standing concerns, there are limited methods that can efficiently and\naccurately detect and segment leaks due to their concealed appearance and\nrandom shapes. In this paper, we propose a Fine-grained Spatial-Temporal\nPerception (FGSTP) algorithm for gas leak segmentation. FGSTP captures critical\nmotion clues across frames and integrates them with refined object features in\nan end-to-end network. Specifically, we first construct a correlation volume to\ncapture motion information between consecutive frames. Then, the fine-grained\nperception progressively refines the object-level features using previous\noutputs. Finally, a decoder is employed to optimize boundary segmentation.\nBecause there is no highly precise labeled dataset for gas leak segmentation,\nwe manually label a gas leak video dataset, GasVid. Experimental results on\nGasVid demonstrate that our model excels in segmenting non-rigid objects such\nas gas leaks, generating the most accurate mask compared to other\nstate-of-the-art (SOTA) models.", "published": "2025-05-01 04:35:57", "link": "http://arxiv.org/abs/2505.00295v1", "categories": ["cs.CV", "cs.AI", "68T45 (Primary), 68T07 (Secondary)", "I.2.10; I.4.6"], "primary_category": "cs.CV"}
{"title": "Multi-Hierarchical Fine-Grained Feature Mapping Driven by Feature Contribution for Molecular Odor Prediction", "abstract": "Molecular odor prediction is the process of using a molecule's structure to\npredict its smell. While accurate prediction remains challenging, AI models can\nsuggest potential odors. Existing methods, however, often rely on basic\ndescriptors or handcrafted fingerprints, which lack expressive power and hinder\neffective learning. Furthermore, these methods suffer from severe class\nimbalance, limiting the training effectiveness of AI models. To address these\nchallenges, we propose a Feature Contribution-driven Hierarchical Multi-Feature\nMapping Network (HMFNet). Specifically, we introduce a fine-grained, Local\nMulti-Hierarchy Feature Extraction module (LMFE) that performs deep feature\nextraction at the atomic level, capturing detailed features crucial for odor\nprediction. To enhance the extraction of discriminative atomic features, we\nintegrate a Harmonic Modulated Feature Mapping (HMFM). This module dynamically\nlearns feature importance and frequency modulation, improving the model's\ncapability to capture relevant patterns. Additionally, a Global Multi-Hierarchy\nFeature Extraction module (GMFE) is designed to learn global features from the\nmolecular graph topology, enabling the model to fully leverage global\ninformation and enhance its discriminative power for odor prediction. To\nfurther mitigate the issue of class imbalance, we propose a Chemically-Informed\nLoss (CIL). Experimental results demonstrate that our approach significantly\nimproves performance across various deep learning models, highlighting its\npotential to advance molecular structure representation and accelerate the\ndevelopment of AI-driven technologies.", "published": "2025-05-01 04:26:31", "link": "http://arxiv.org/abs/2505.00290v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving", "abstract": "Vision-Language Models (VLMs) have demonstrated significant potential for\nend-to-end autonomous driving. However, fully exploiting their capabilities for\nsafe and reliable vehicle control remains an open research challenge. To\nsystematically examine advances and limitations of VLMs in driving tasks, we\nintroduce LightEMMA, a Lightweight End-to-End Multimodal Model for Autonomous\ndriving. LightEMMA provides a unified, VLM-based autonomous driving framework\nwithout ad hoc customizations, enabling easy integration and evaluation of\nevolving state-of-the-art commercial and open-source models. We construct\ntwelve autonomous driving agents using various VLMs and evaluate their\nperformance on the nuScenes prediction task, comprehensively assessing metrics\nsuch as inference time, computational cost, and predictive accuracy.\nIllustrative examples highlight that, despite their strong scenario\ninterpretation capabilities, VLMs' practical performance in autonomous driving\ntasks remains concerning, emphasizing the need for further improvements. The\ncode is available at https://github.com/michigan-traffic-lab/LightEMMA.", "published": "2025-05-01 04:12:41", "link": "http://arxiv.org/abs/2505.00284v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO"}
{"title": "DeCo: Defect-Aware Modeling with Contrasting Matching for Optimizing Task Assignment in Online IC Testing", "abstract": "In the semiconductor industry, integrated circuit (IC) processes play a vital\nrole, as the rising complexity and market expectations necessitate improvements\nin yield. Identifying IC defects and assigning IC testing tasks to the right\nengineers improves efficiency and reduces losses. While current studies\nemphasize fault localization or defect classification, they overlook the\nintegration of defect characteristics, historical failures, and the insights\nfrom engineer expertise, which restrains their effectiveness in improving IC\nhandling. To leverage AI for these challenges, we propose DeCo, an innovative\napproach for optimizing task assignment in IC testing. DeCo constructs a novel\ndefect-aware graph from IC testing reports, capturing co-failure relationships\nto enhance defect differentiation, even with scarce defect data. Additionally,\nit formulates defect-aware representations for engineers and tasks, reinforced\nby local and global structure modeling on the defect-aware graph. Finally, a\ncontrasting-based assignment mechanism pairs testing tasks with QA engineers by\nconsidering their skill level and current workload, thus promoting an equitable\nand efficient job dispatch. Experiments on a real-world dataset demonstrate\nthat DeCo achieves the highest task-handling success rates in different\nscenarios, exceeding 80\\%, while also maintaining balanced workloads on both\nscarce or expanded defect data. Moreover, case studies reveal that DeCo can\nassign tasks to potentially capable engineers, even for their unfamiliar\ndefects, highlighting its potential as an AI-driven solution for the real-world\nIC failure analysis and task handling.", "published": "2025-05-01 04:01:14", "link": "http://arxiv.org/abs/2505.00278v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Pack-PTQ: Advancing Post-training Quantization of Neural Networks by Pack-wise Reconstruction", "abstract": "Post-training quantization (PTQ) has evolved as a prominent solution for\ncompressing complex models, which advocates a small calibration dataset and\navoids end-to-end retraining. However, most existing PTQ methods employ\nblock-wise reconstruction, which neglects cross-block dependency and exhibits a\nnotable accuracy drop in low-bit cases. To address these limitations, this\npaper presents a novel PTQ method, dubbed Pack-PTQ. First, we design a\nHessian-guided adaptive packing mechanism to partition blocks into\nnon-overlapping packs, which serve as the base unit for reconstruction, thereby\npreserving the cross-block dependency and enabling accurate quantization\nparameters estimation. Second, based on the pack configuration, we propose a\nmixed-precision quantization approach to assign varied bit-widths to packs\naccording to their distinct sensitivities, thereby further enhancing\nperformance. Extensive experiments on 2D image and 3D point cloud\nclassification tasks, using various network architectures, demonstrate the\nsuperiority of our method over the state-of-the-art PTQ methods.", "published": "2025-05-01 02:53:46", "link": "http://arxiv.org/abs/2505.00259v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Empowering Agentic Video Analytics Systems with Video Language Models", "abstract": "AI-driven video analytics has become increasingly pivotal across diverse\ndomains. However, existing systems are often constrained to specific,\npredefined tasks, limiting their adaptability in open-ended analytical\nscenarios. The recent emergence of Video-Language Models (VLMs) as\ntransformative technologies offers significant potential for enabling\nopen-ended video understanding, reasoning, and analytics. Nevertheless, their\nlimited context windows present challenges when processing ultra-long video\ncontent, which is prevalent in real-world applications. To address this, we\nintroduce AVA, a VLM-powered system designed for open-ended, advanced video\nanalytics. AVA incorporates two key innovations: (1) the near real-time\nconstruction of Event Knowledge Graphs (EKGs) for efficient indexing of long or\ncontinuous video streams, and (2) an agentic retrieval-generation mechanism\nthat leverages EKGs to handle complex and diverse queries. Comprehensive\nevaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate that\nAVA achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy,\nrespectively, significantly surpassing existing VLM and video\nRetrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate video\nanalytics in ultra-long and open-world video scenarios, we introduce a new\nbenchmark, AVA-100. This benchmark comprises 8 videos, each exceeding 10 hours\nin duration, along with 120 manually annotated, diverse, and complex\nquestion-answer pairs. On AVA-100, AVA achieves top-tier performance with an\naccuracy of 75.8%.", "published": "2025-05-01 02:40:23", "link": "http://arxiv.org/abs/2505.00254v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "LLM-Based Threat Detection and Prevention Framework for IoT Ecosystems", "abstract": "The increasing complexity and scale of the Internet of Things (IoT) have made\nsecurity a critical concern. This paper presents a novel Large Language Model\n(LLM)-based framework for comprehensive threat detection and prevention in IoT\nenvironments. The system integrates lightweight LLMs fine-tuned on IoT-specific\ndatasets (IoT-23, TON_IoT) for real-time anomaly detection and automated,\ncontext-aware mitigation strategies optimized for resource-constrained devices.\nA modular Docker-based deployment enables scalable and reproducible evaluation\nacross diverse network conditions. Experimental results in simulated IoT\nenvironments demonstrate significant improvements in detection accuracy,\nresponse latency, and resource efficiency over traditional security methods.\nThe proposed framework highlights the potential of LLM-driven, autonomous\nsecurity solutions for future IoT ecosystems.", "published": "2025-05-01 01:18:54", "link": "http://arxiv.org/abs/2505.00240v1", "categories": ["cs.CR", "cs.AI", "cs.ET", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Scaling On-Device GPU Inference for Large Generative Models", "abstract": "Driven by the advancements in generative AI, large machine learning models\nhave revolutionized domains such as image processing, audio synthesis, and\nspeech recognition. While server-based deployments remain the locus of peak\nperformance, the imperative for on-device inference, necessitated by privacy\nand efficiency considerations, persists. Recognizing GPUs as the on-device ML\naccelerator with the widest reach, we present ML Drift--an optimized framework\nthat extends the capabilities of state-of-the-art GPU-accelerated inference\nengines. ML Drift enables on-device execution of generative AI workloads which\ncontain 10 to 100x more parameters than existing on-device generative AI\nmodels. ML Drift addresses intricate engineering challenges associated with\ncross-GPU API development, and ensures broad compatibility across mobile and\ndesktop/laptop platforms, thereby facilitating the deployment of significantly\nmore complex models on resource-constrained devices. Our GPU-accelerated ML/AI\ninference engine achieves an order-of-magnitude performance improvement\nrelative to existing open-source GPU inference engines.", "published": "2025-05-01 00:44:13", "link": "http://arxiv.org/abs/2505.00232v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Predicting Estimated Times of Restoration for Electrical Outages Using Longitudinal Tabular Transformers", "abstract": "As climate variability increases, the ability of utility providers to deliver\nprecise Estimated Times of Restoration (ETR) during natural disasters has\nbecome increasingly critical. Accurate and timely ETRs are essential for\nenabling customer preparedness during extended power outages, where informed\ndecision-making can be crucial, particularly in severe weather conditions.\nNonetheless, prevailing utility practices predominantly depend on manual\nassessments or traditional statistical methods, which often fail to achieve the\nlevel of precision required for reliable and actionable predictions. To address\nthese limitations, we propose a Longitudinal Tabular Transformer (LTT) model\nthat leverages historical outage event data along with sequential updates of\nthese events to improve the accuracy of ETR predictions. The model's\nperformance was evaluated over 34,000 storm-related outage events from three\nmajor utility companies, collectively serving over 3 million customers over a\n2-year period. Results demonstrate that the LTT model improves the Customer\nSatisfaction Impact (CSI) metric by an average of 19.08% (p > 0.001) compared\nto existing methods. Additionally, we introduce customer-informed regression\nmetrics that align model evaluation with real-world satisfaction, ensuring the\noutcomes resonate with customer expectations. Furthermore, we employ\ninterpretability techniques to analyze the temporal significance of\nincorporating sequential updates in modeling outage events and to identify the\ncontributions of predictive features to a given ETR. This comprehensive\napproach not only improves predictive accuracy but also enhances transparency,\nfostering greater trust in the model's capabilities.", "published": "2025-05-01 00:25:43", "link": "http://arxiv.org/abs/2505.00225v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Controllable Weather Synthesis and Removal with Video Diffusion Models", "abstract": "Generating realistic and controllable weather effects in videos is valuable\nfor many applications. Physics-based weather simulation requires precise\nreconstructions that are hard to scale to in-the-wild videos, while current\nvideo editing often lacks realism and control. In this work, we introduce\nWeatherWeaver, a video diffusion model that synthesizes diverse weather effects\n-- including rain, snow, fog, and clouds -- directly into any input video\nwithout the need for 3D modeling. Our model provides precise control over\nweather effect intensity and supports blending various weather types, ensuring\nboth realism and adaptability. To overcome the scarcity of paired training\ndata, we propose a novel data strategy combining synthetic videos, generative\nimage editing, and auto-labeled real-world videos. Extensive evaluations show\nthat our method outperforms state-of-the-art methods in weather simulation and\nremoval, providing high-quality, physically plausible, and\nscene-identity-preserving results over various real-world videos.", "published": "2025-05-01 17:59:57", "link": "http://arxiv.org/abs/2505.00704v1", "categories": ["cs.GR", "cs.CV"], "primary_category": "cs.GR"}
{"title": "RayZer: A Self-supervised Large View Synthesis Model", "abstract": "We present RayZer, a self-supervised multi-view 3D Vision model trained\nwithout any 3D supervision, i.e., camera poses and scene geometry, while\nexhibiting emerging 3D awareness. Concretely, RayZer takes unposed and\nuncalibrated images as input, recovers camera parameters, reconstructs a scene\nrepresentation, and synthesizes novel views. During training, RayZer relies\nsolely on its self-predicted camera poses to render target views, eliminating\nthe need for any ground-truth camera annotations and allowing RayZer to be\ntrained with 2D image supervision. The emerging 3D awareness of RayZer is\nattributed to two key factors. First, we design a self-supervised framework,\nwhich achieves 3D-aware auto-encoding of input images by disentangling camera\nand scene representations. Second, we design a transformer-based model in which\nthe only 3D prior is the ray structure, connecting camera, pixel, and scene\nsimultaneously. RayZer demonstrates comparable or even superior novel view\nsynthesis performance than ``oracle'' methods that rely on pose annotations in\nboth training and testing. Project: https://hwjiang1510.github.io/RayZer/", "published": "2025-05-01 17:59:34", "link": "http://arxiv.org/abs/2505.00702v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "GuideSR: Rethinking Guidance for One-Step High-Fidelity Diffusion-Based Super-Resolution", "abstract": "In this paper, we propose GuideSR, a novel single-step diffusion-based image\nsuper-resolution (SR) model specifically designed to enhance image fidelity.\nExisting diffusion-based SR approaches typically adapt pre-trained generative\nmodels to image restoration tasks by adding extra conditioning on a\nVAE-downsampled representation of the degraded input, which often compromises\nstructural fidelity. GuideSR addresses this limitation by introducing a\ndual-branch architecture comprising: (1) a Guidance Branch that preserves\nhigh-fidelity structures from the original-resolution degraded input, and (2) a\nDiffusion Branch, which a pre-trained latent diffusion model to enhance\nperceptual quality. Unlike conventional conditioning mechanisms, our Guidance\nBranch features a tailored structure for image restoration tasks, combining\nFull Resolution Blocks (FRBs) with channel attention and an Image Guidance\nNetwork (IGN) with guided attention. By embedding detailed structural\ninformation directly into the restoration pipeline, GuideSR produces sharper\nand more visually consistent results. Extensive experiments on benchmark\ndatasets demonstrate that GuideSR achieves state-of-the-art performance while\nmaintaining the low computational cost of single-step approaches, with up to\n1.39dB PSNR gain on challenging real-world datasets. Our approach consistently\noutperforms existing methods across various reference-based metrics including\nPSNR, SSIM, LPIPS, DISTS and FID, further representing a practical advancement\nfor real-world image restoration.", "published": "2025-05-01 17:48:25", "link": "http://arxiv.org/abs/2505.00687v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "MINERVA: Evaluating Complex Video Reasoning", "abstract": "Multimodal LLMs are turning their focus to video benchmarks, however most\nvideo benchmarks only provide outcome supervision, with no intermediate or\ninterpretable reasoning steps. This makes it challenging to assess if models\nare truly able to combine perceptual and temporal information to reason about\nvideos, or simply get the correct answer by chance or by exploiting linguistic\nbiases. To remedy this, we provide a new video reasoning dataset called MINERVA\nfor modern multimodal models. Each question in the dataset comes with 5 answer\nchoices, as well as detailed, hand-crafted reasoning traces. Our dataset is\nmultimodal, diverse in terms of video domain and length, and consists of\ncomplex multi-step questions. Extensive benchmarking shows that our dataset\nprovides a challenge for frontier open-source and proprietary models. We\nperform fine-grained error analysis to identify common failure modes across\nvarious models, and create a taxonomy of reasoning errors. We use this to\nexplore both human and LLM-as-a-judge methods for scoring video reasoning\ntraces, and find that failure modes are primarily related to temporal\nlocalization, followed by visual perception errors, as opposed to logical or\ncompleteness errors. The dataset, along with questions, answer candidates and\nreasoning traces will be publicly available under\nhttps://github.com/google-deepmind/neptune?tab=readme-ov-file\\#minerva.", "published": "2025-05-01 17:41:49", "link": "http://arxiv.org/abs/2505.00681v1", "categories": ["cs.LG", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Vision Mamba in Remote Sensing: A Comprehensive Survey of Techniques, Applications and Outlook", "abstract": "Deep learning has profoundly transformed remote sensing, yet prevailing\narchitectures like Convolutional Neural Networks (CNNs) and Vision Transformers\n(ViTs) remain constrained by critical trade-offs: CNNs suffer from limited\nreceptive fields, while ViTs grapple with quadratic computational complexity,\nhindering their scalability for high-resolution remote sensing data. State\nSpace Models (SSMs), particularly the recently proposed Mamba architecture,\nhave emerged as a paradigm-shifting solution, combining linear computational\nscaling with global context modeling. This survey presents a comprehensive\nreview of Mamba-based methodologies in remote sensing, systematically analyzing\nabout 120 studies to construct a holistic taxonomy of innovations and\napplications. Our contributions are structured across five dimensions: (i)\nfoundational principles of vision Mamba architectures, (ii) micro-architectural\nadvancements such as adaptive scan strategies and hybrid SSM formulations,\n(iii) macro-architectural integrations, including CNN-Transformer-Mamba hybrids\nand frequency-domain adaptations, (iv) rigorous benchmarking against\nstate-of-the-art methods in multiple application tasks, such as object\ndetection, semantic segmentation, change detection, etc. and (v) critical\nanalysis of unresolved challenges with actionable future directions. By\nbridging the gap between SSM theory and remote sensing practice, this survey\nestablishes Mamba as a transformative framework for remote sensing analysis. To\nour knowledge, this paper is the first systematic review of Mamba architectures\nin remote sensing. Our work provides a structured foundation for advancing\nresearch in remote sensing systems through SSM-based methods. We curate an\nopen-source repository\n(https://github.com/BaoBao0926/Awesome-Mamba-in-Remote-Sensing) to foster\ncommunity-driven advancements.", "published": "2025-05-01 16:07:51", "link": "http://arxiv.org/abs/2505.00630v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Brain Foundation Models with Hypergraph Dynamic Adapter for Brain Disease Analysis", "abstract": "Brain diseases, such as Alzheimer's disease and brain tumors, present\nprofound challenges due to their complexity and societal impact. Recent\nadvancements in brain foundation models have shown significant promise in\naddressing a range of brain-related tasks. However, current brain foundation\nmodels are limited by task and data homogeneity, restricted generalization\nbeyond segmentation or classification, and inefficient adaptation to diverse\nclinical tasks. In this work, we propose SAM-Brain3D, a brain-specific\nfoundation model trained on over 66,000 brain image-label pairs across 14 MRI\nsub-modalities, and Hypergraph Dynamic Adapter (HyDA), a lightweight adapter\nfor efficient and effective downstream adaptation. SAM-Brain3D captures\ndetailed brain-specific anatomical and modality priors for segmenting diverse\nbrain targets and broader downstream tasks. HyDA leverages hypergraphs to fuse\ncomplementary multi-modal data and dynamically generate patient-specific\nconvolutional kernels for multi-scale feature fusion and personalized\npatient-wise adaptation. Together, our framework excels across a broad spectrum\nof brain disease segmentation and classification tasks. Extensive experiments\ndemonstrate that our method consistently outperforms existing state-of-the-art\napproaches, offering a new paradigm for brain disease analysis through\nmulti-modal, multi-scale, and dynamic foundation modeling.", "published": "2025-05-01 16:06:17", "link": "http://arxiv.org/abs/2505.00627v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Diverse Semantics-Guided Feature Alignment and Decoupling for Visible-Infrared Person Re-Identification", "abstract": "Visible-Infrared Person Re-Identification (VI-ReID) is a challenging task due\nto the large modality discrepancy between visible and infrared images, which\ncomplicates the alignment of their features into a suitable common space.\nMoreover, style noise, such as illumination and color contrast, reduces the\nidentity discriminability and modality invariance of features. To address these\nchallenges, we propose a novel Diverse Semantics-guided Feature Alignment and\nDecoupling (DSFAD) network to align identity-relevant features from different\nmodalities into a textual embedding space and disentangle identity-irrelevant\nfeatures within each modality. Specifically, we develop a Diverse\nSemantics-guided Feature Alignment (DSFA) module, which generates pedestrian\ndescriptions with diverse sentence structures to guide the cross-modality\nalignment of visual features. Furthermore, to filter out style information, we\npropose a Semantic Margin-guided Feature Decoupling (SMFD) module, which\ndecomposes visual features into pedestrian-related and style-related\ncomponents, and then constrains the similarity between the former and the\ntextual embeddings to be at least a margin higher than that between the latter\nand the textual embeddings. Additionally, to prevent the loss of pedestrian\nsemantics during feature decoupling, we design a Semantic Consistency-guided\nFeature Restitution (SCFR) module, which further excavates useful information\nfor identification from the style-related features and restores it back into\nthe pedestrian-related features, and then constrains the similarity between the\nfeatures after restitution and the textual embeddings to be consistent with\nthat between the features before decoupling and the textual embeddings.\nExtensive experiments on three VI-ReID datasets demonstrate the superiority of\nour DSFAD.", "published": "2025-05-01 15:55:38", "link": "http://arxiv.org/abs/2505.00619v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Dietary Intake Estimation via Continuous 3D Reconstruction of Food", "abstract": "Monitoring dietary habits is crucial for preventing health risks associated\nwith overeating and undereating, including obesity, diabetes, and\ncardiovascular diseases. Traditional methods for tracking food intake rely on\nself-reported data before or after the eating, which are prone to inaccuracies.\nThis study proposes an approach to accurately monitor ingest behaviours by\nleveraging 3D food models constructed from monocular 2D video. Using COLMAP and\npose estimation algorithms, we generate detailed 3D representations of food,\nallowing us to observe changes in food volume as it is consumed. Experiments\nwith toy models and real food items demonstrate the approach's potential.\nMeanwhile, we have proposed a new methodology for automated state recognition\nchallenges to accurately detect state changes and maintain model fidelity. The\n3D reconstruction approach shows promise in capturing comprehensive dietary\nbehaviour insights, ultimately contributing to the development of automated and\naccurate dietary monitoring tools.", "published": "2025-05-01 15:35:42", "link": "http://arxiv.org/abs/2505.00606v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Visual Trajectory Prediction of Vessels for Inland Navigation", "abstract": "The future of inland navigation increasingly relies on autonomous systems and\nremote operations, emphasizing the need for accurate vessel trajectory\nprediction. This study addresses the challenges of video-based vessel tracking\nand prediction by integrating advanced object detection methods, Kalman\nfilters, and spline-based interpolation. However, existing detection systems\noften misclassify objects in inland waterways due to complex surroundings. A\ncomparative evaluation of tracking algorithms, including BoT-SORT, Deep\nOC-SORT, and ByeTrack, highlights the robustness of the Kalman filter in\nproviding smoothed trajectories. Experimental results from diverse scenarios\ndemonstrate improved accuracy in predicting vessel movements, which is\nessential for collision avoidance and situational awareness. The findings\nunderline the necessity of customized datasets and models for inland\nnavigation. Future work will expand the datasets and incorporate vessel\nclassification to refine predictions, supporting both autonomous systems and\nhuman operators in complex environments.", "published": "2025-05-01 15:31:15", "link": "http://arxiv.org/abs/2505.00599v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Uncertainty-Aware Multi-Expert Knowledge Distillation for Imbalanced Disease Grading", "abstract": "Automatic disease image grading is a significant application of artificial\nintelligence for healthcare, enabling faster and more accurate patient\nassessments. However, domain shifts, which are exacerbated by data imbalance,\nintroduce bias into the model, posing deployment difficulties in clinical\napplications. To address the problem, we propose a novel\n\\textbf{U}ncertainty-aware \\textbf{M}ulti-experts \\textbf{K}nowledge\n\\textbf{D}istillation (UMKD) framework to transfer knowledge from multiple\nexpert models to a single student model. Specifically, to extract\ndiscriminative features, UMKD decouples task-agnostic and task-specific\nfeatures with shallow and compact feature alignment in the feature space. At\nthe output space, an uncertainty-aware decoupled distillation (UDD) mechanism\ndynamically adjusts knowledge transfer weights based on expert model\nuncertainties, ensuring robust and reliable distillation. Additionally, UMKD\nalso tackles the problems of model architecture heterogeneity and distribution\ndiscrepancies between source and target domains, which are inadequately tackled\nby previous KD approaches. Extensive experiments on histology prostate grading\n(\\textit{SICAPv2}) and fundus image grading (\\textit{APTOS}) demonstrate that\nUMKD achieves a new state-of-the-art in both source-imbalanced and\ntarget-imbalanced scenarios, offering a robust and practical solution for\nreal-world disease image grading.", "published": "2025-05-01 15:26:23", "link": "http://arxiv.org/abs/2505.00592v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "X-ray illicit object detection using hybrid CNN-transformer neural network architectures", "abstract": "In the field of X-ray security applications, even the smallest details can\nsignificantly impact outcomes. Objects that are heavily occluded or\nintentionally concealed pose a great challenge for detection, whether by human\nobservation or through advanced technological applications. While certain Deep\nLearning (DL) architectures demonstrate strong performance in processing local\ninformation, such as Convolutional Neural Networks (CNNs), others excel in\nhandling distant information, e.g., transformers. In X-ray security imaging the\nliterature has been dominated by the use of CNN-based methods, while the\nintegration of the two aforementioned leading architectures has not been\nsufficiently explored. In this paper, various hybrid CNN-transformer\narchitectures are evaluated against a common CNN object detection baseline,\nnamely YOLOv8. In particular, a CNN (HGNetV2) and a hybrid CNN-transformer\n(Next-ViT-S) backbone are combined with different CNN/transformer detection\nheads (YOLOv8 and RT-DETR). The resulting architectures are comparatively\nevaluated on three challenging public X-ray inspection datasets, namely EDS,\nHiXray, and PIDray. Interestingly, while the YOLOv8 detector with its default\nbackbone (CSP-DarkNet53) is generally shown to be advantageous on the HiXray\nand PIDray datasets, when a domain distribution shift is incorporated in the\nX-ray images (as happens in the EDS datasets), hybrid CNN-transformer\narchitectures exhibit increased robustness. Detailed comparative evaluation\nresults, including object-level detection performance and object-size error\nanalysis, demonstrate the strengths and weaknesses of each architectural\ncombination and suggest guidelines for future research. The source code and\nnetwork weights of the models employed in this study are available at\nhttps://github.com/jgenc/xray-comparative-evaluation.", "published": "2025-05-01 14:40:38", "link": "http://arxiv.org/abs/2505.00564v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "A Robust Deep Networks based Multi-Object MultiCamera Tracking System for City Scale Traffic", "abstract": "Vision sensors are becoming more important in Intelligent Transportation\nSystems (ITS) for traffic monitoring, management, and optimization as the\nnumber of network cameras continues to rise. However, manual object tracking\nand matching across multiple non-overlapping cameras pose significant\nchallenges in city-scale urban traffic scenarios. These challenges include\nhandling diverse vehicle attributes, occlusions, illumination variations,\nshadows, and varying video resolutions. To address these issues, we propose an\nefficient and cost-effective deep learning-based framework for Multi-Object\nMulti-Camera Tracking (MO-MCT). The proposed framework utilizes Mask R-CNN for\nobject detection and employs Non-Maximum Suppression (NMS) to select target\nobjects from overlapping detections. Transfer learning is employed for\nre-identification, enabling the association and generation of vehicle tracklets\nacross multiple cameras. Moreover, we leverage appropriate loss functions and\ndistance measures to handle occlusion, illumination, and shadow challenges. The\nfinal solution identification module performs feature extraction using\nResNet-152 coupled with Deep SORT based vehicle tracking. The proposed\nframework is evaluated on the 5th AI City Challenge dataset (Track 3),\ncomprising 46 camera feeds. Among these 46 camera streams, 40 are used for\nmodel training and validation, while the remaining six are utilized for model\ntesting. The proposed framework achieves competitive performance with an IDF1\nscore of 0.8289, and precision and recall scores of 0.9026 and 0.8527\nrespectively, demonstrating its effectiveness in robust and accurate vehicle\ntracking.", "published": "2025-05-01 14:00:25", "link": "http://arxiv.org/abs/2505.00534v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "A Methodological and Structural Review of Parkinsons Disease Detection Across Diverse Data Modalities", "abstract": "Parkinsons Disease (PD) is a progressive neurological disorder that primarily\naffects motor functions and can lead to mild cognitive impairment (MCI) and\ndementia in its advanced stages. With approximately 10 million people diagnosed\nglobally 1 to 1.8 per 1,000 individuals, according to reports by the Japan\nTimes and the Parkinson Foundation early and accurate diagnosis of PD is\ncrucial for improving patient outcomes. While numerous studies have utilized\nmachine learning (ML) and deep learning (DL) techniques for PD recognition,\nexisting surveys are limited in scope, often focusing on single data modalities\nand failing to capture the potential of multimodal approaches. To address these\ngaps, this study presents a comprehensive review of PD recognition systems\nacross diverse data modalities, including Magnetic Resonance Imaging (MRI),\ngait-based pose analysis, gait sensory data, handwriting analysis, speech test\ndata, Electroencephalography (EEG), and multimodal fusion techniques. Based on\nover 347 articles from leading scientific databases, this review examines key\naspects such as data collection methods, settings, feature representations, and\nsystem performance, with a focus on recognition accuracy and robustness. This\nsurvey aims to serve as a comprehensive resource for researchers, providing\nactionable guidance for the development of next generation PD recognition\nsystems. By leveraging diverse data modalities and cutting-edge machine\nlearning paradigms, this work contributes to advancing the state of PD\ndiagnostics and improving patient care through innovative, multimodal\napproaches.", "published": "2025-05-01 13:47:45", "link": "http://arxiv.org/abs/2505.00525v1", "categories": ["eess.IV", "cs.CV", "cs.LG"], "primary_category": "eess.IV"}
{"title": "InterLoc: LiDAR-based Intersection Localization using Road Segmentation with Automated Evaluation Method", "abstract": "Intersections are geometric and functional key points in every road network.\nThey offer strong landmarks to correct GNSS dropouts and anchor new sensor data\nin up-to-date maps. Despite that importance, intersection detectors either\nignore the rich semantic information already computed onboard or depend on\nscarce, hand-labeled intersection datasets. To close that gap, this paper\npresents a LiDAR-based method for intersection detection that (i) fuses\nsemantic road segmentation with vehicle localization to detect intersection\ncandidates in a bird's eye view (BEV) representation and (ii) refines those\ncandidates by analyzing branch topology with a least squares formulation. To\nevaluate our method, we introduce an automated benchmarking pipeline that pairs\ndetections with OpenStreetMap (OSM) intersection nodes using precise GNSS/INS\nground-truth poses. Tested on eight SemanticKITTI sequences, the approach\nachieves a mean localization error of 1.9 m, 89% precision, and 77% recall at a\n5 m tolerance, outperforming the latest learning-based baseline. Moreover, the\nmethod is robust to segmentation errors higher than those of the benchmark\nmodel, demonstrating its applicability in the real world.", "published": "2025-05-01 13:30:28", "link": "http://arxiv.org/abs/2505.00512v1", "categories": ["cs.CV", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Inconsistency-based Active Learning for LiDAR Object Detection", "abstract": "Deep learning models for object detection in autonomous driving have recently\nachieved impressive performance gains and are already being deployed in\nvehicles worldwide. However, current models require increasingly large datasets\nfor training. Acquiring and labeling such data is costly, necessitating the\ndevelopment of new strategies to optimize this process. Active learning is a\npromising approach that has been extensively researched in the image domain. In\nour work, we extend this concept to the LiDAR domain by developing several\ninconsistency-based sample selection strategies and evaluate their\neffectiveness in various settings. Our results show that using a naive\ninconsistency approach based on the number of detected boxes, we achieve the\nsame mAP as the random sampling strategy with 50% of the labeled data.", "published": "2025-05-01 13:29:56", "link": "http://arxiv.org/abs/2505.00511v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "HeAL3D: Heuristical-enhanced Active Learning for 3D Object Detection", "abstract": "Active Learning has proved to be a relevant approach to perform sample\nselection for training models for Autonomous Driving. Particularly, previous\nworks on active learning for 3D object detection have shown that selection of\nsamples in uncontrolled scenarios is challenging. Furthermore, current\napproaches focus exclusively on the theoretical aspects of the sample selection\nproblem but neglect the practical insights that can be obtained from the\nextensive literature and application of 3D detection models. In this paper, we\nintroduce HeAL (Heuristical-enhanced Active Learning for 3D Object Detection)\nwhich integrates those heuristical features together with Localization and\nClassification to deliver the most contributing samples to the model's\ntraining. In contrast to previous works, our approach integrates heuristical\nfeatures such as object distance and point-quantity to estimate the\nuncertainty, which enhance the usefulness of selected samples to train\ndetection models. Our quantitative evaluation on KITTI shows that HeAL presents\ncompetitive mAP with respect to the State-of-the-Art, and achieves the same mAP\nas the full-supervised baseline with only 24% of the samples.", "published": "2025-05-01 13:24:55", "link": "http://arxiv.org/abs/2505.00507v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Towards Scalable Human-aligned Benchmark for Text-guided Image Editing", "abstract": "A variety of text-guided image editing models have been proposed recently.\nHowever, there is no widely-accepted standard evaluation method mainly due to\nthe subjective nature of the task, letting researchers rely on manual user\nstudy. To address this, we introduce a novel Human-Aligned benchmark for\nText-guided Image Editing (HATIE). Providing a large-scale benchmark set\ncovering a wide range of editing tasks, it allows reliable evaluation, not\nlimited to specific easy-to-evaluate cases. Also, HATIE provides a\nfully-automated and omnidirectional evaluation pipeline. Particularly, we\ncombine multiple scores measuring various aspects of editing so as to align\nwith human perception. We empirically verify that the evaluation of HATIE is\nindeed human-aligned in various aspects, and provide benchmark results on\nseveral state-of-the-art models to provide deeper insights on their\nperformance.", "published": "2025-05-01 13:06:05", "link": "http://arxiv.org/abs/2505.00502v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "KeySync: A Robust Approach for Leakage-free Lip Synchronization in High Resolution", "abstract": "Lip synchronization, known as the task of aligning lip movements in an\nexisting video with new input audio, is typically framed as a simpler variant\nof audio-driven facial animation. However, as well as suffering from the usual\nissues in talking head generation (e.g., temporal consistency), lip\nsynchronization presents significant new challenges such as expression leakage\nfrom the input video and facial occlusions, which can severely impact\nreal-world applications like automated dubbing, but are often neglected in\nexisting works. To address these shortcomings, we present KeySync, a two-stage\nframework that succeeds in solving the issue of temporal consistency, while\nalso incorporating solutions for leakage and occlusions using a carefully\ndesigned masking strategy. We show that KeySync achieves state-of-the-art\nresults in lip reconstruction and cross-synchronization, improving visual\nquality and reducing expression leakage according to LipLeak, our novel leakage\nmetric. Furthermore, we demonstrate the effectiveness of our new masking\napproach in handling occlusions and validate our architectural choices through\nseveral ablation studies. Code and model weights can be found at\nhttps://antonibigata.github.io/KeySync.", "published": "2025-05-01 12:56:17", "link": "http://arxiv.org/abs/2505.00497v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "CORSTITCH - A free, open source software for stitching and georeferencing underwater coral reef videos", "abstract": "CorStitch is an open-source software developed to automate the creation of\naccurate georeferenced reef mosaics from video transects obtained through\nAutomated Rapid Reef Assessment System surveys. We utilized a Fourier-based\nimage correlation algorithm to stitch sequential video frames, aligning them\nwith synchronized GNSS timestamps. The resulting compressed Keyhole Markup\nLanguage files, compatible with geographic information systems such as Google\nEarth, enable detailed spatial analysis. Validation through comparative\nanalysis of mosaics from two temporally distinct surveys of the same reef\ndemonstrated the software's consistent and reliable performance.", "published": "2025-05-01 11:29:45", "link": "http://arxiv.org/abs/2505.00462v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "ClearLines - Camera Calibration from Straight Lines", "abstract": "The problem of calibration from straight lines is fundamental in geometric\ncomputer vision, with well-established theoretical foundations. However, its\npractical applicability remains limited, particularly in real-world outdoor\nscenarios. These environments pose significant challenges due to diverse and\ncluttered scenes, interrupted reprojections of straight 3D lines, and varying\nlighting conditions, making the task notoriously difficult. Furthermore, the\nfield lacks a dedicated dataset encouraging the development of respective\ndetection algorithms. In this study, we present a small dataset named\n\"ClearLines\", and by detailing its creation process, provide practical insights\nthat can serve as a guide for developing and refining straight 3D line\ndetection algorithms.", "published": "2025-05-01 10:55:36", "link": "http://arxiv.org/abs/2505.00452v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Leveraging Pretrained Diffusion Models for Zero-Shot Part Assembly", "abstract": "3D part assembly aims to understand part relationships and predict their\n6-DoF poses to construct realistic 3D shapes, addressing the growing demand for\nautonomous assembly, which is crucial for robots. Existing methods mainly\nestimate the transformation of each part by training neural networks under\nsupervision, which requires a substantial quantity of manually labeled data.\nHowever, the high cost of data collection and the immense variability of\nreal-world shapes and parts make traditional methods impractical for\nlarge-scale applications. In this paper, we propose first a zero-shot part\nassembly method that utilizes pre-trained point cloud diffusion models as\ndiscriminators in the assembly process, guiding the manipulation of parts to\nform realistic shapes. Specifically, we theoretically demonstrate that\nutilizing a diffusion model for zero-shot part assembly can be transformed into\nan Iterative Closest Point (ICP) process. Then, we propose a novel pushing-away\nstrategy to address the overlap parts, thereby further enhancing the robustness\nof the method. To verify our work, we conduct extensive experiments and\nquantitative comparisons to several strong baseline methods, demonstrating the\neffectiveness of the proposed approach, which even surpasses the supervised\nlearning method. The code has been released on\nhttps://github.com/Ruiyuan-Zhang/Zero-Shot-Assembly.", "published": "2025-05-01 09:54:12", "link": "http://arxiv.org/abs/2505.00426v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Real-Time Animatable 2DGS-Avatars with Detail Enhancement from Monocular Videos", "abstract": "High-quality, animatable 3D human avatar reconstruction from monocular videos\noffers significant potential for reducing reliance on complex hardware, making\nit highly practical for applications in game development, augmented reality,\nand social media. However, existing methods still face substantial challenges\nin capturing fine geometric details and maintaining animation stability,\nparticularly under dynamic or complex poses. To address these issues, we\npropose a novel real-time framework for animatable human avatar reconstruction\nbased on 2D Gaussian Splatting (2DGS). By leveraging 2DGS and global SMPL pose\nparameters, our framework not only aligns positional and rotational\ndiscrepancies but also enables robust and natural pose-driven animation of the\nreconstructed avatars. Furthermore, we introduce a Rotation Compensation\nNetwork (RCN) that learns rotation residuals by integrating local geometric\nfeatures with global pose parameters. This network significantly improves the\nhandling of non-rigid deformations and ensures smooth, artifact-free pose\ntransitions during animation. Experimental results demonstrate that our method\nsuccessfully reconstructs realistic and highly animatable human avatars from\nmonocular videos, effectively preserving fine-grained details while ensuring\nstable and natural pose variation. Our approach surpasses current\nstate-of-the-art methods in both reconstruction quality and animation\nrobustness on public benchmarks.", "published": "2025-05-01 09:41:28", "link": "http://arxiv.org/abs/2505.00421v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "SOTA: Spike-Navigated Optimal TrAnsport Saliency Region Detection in Composite-bias Videos", "abstract": "Existing saliency detection methods struggle in real-world scenarios due to\nmotion blur and occlusions. In contrast, spike cameras, with their high\ntemporal resolution, significantly enhance visual saliency maps. However, the\ncomposite noise inherent to spike camera imaging introduces discontinuities in\nsaliency detection. Low-quality samples further distort model predictions,\nleading to saliency bias. To address these challenges, we propose\nSpike-navigated Optimal TrAnsport Saliency Region Detection (SOTA), a framework\nthat leverages the strengths of spike cameras while mitigating biases in both\nspatial and temporal dimensions. Our method introduces Spike-based Micro-debias\n(SM) to capture subtle frame-to-frame variations and preserve critical details,\neven under minimal scene or lighting changes. Additionally, Spike-based\nGlobal-debias (SG) refines predictions by reducing inconsistencies across\ndiverse conditions. Extensive experiments on real and synthetic datasets\ndemonstrate that SOTA outperforms existing methods by eliminating composite\nnoise bias. Our code and dataset will be released at\nhttps://github.com/lwxfight/sota.", "published": "2025-05-01 08:30:40", "link": "http://arxiv.org/abs/2505.00394v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "The Invisible Threat: Evaluating the Vulnerability of Cross-Spectral Face Recognition to Presentation Attacks", "abstract": "Cross-spectral face recognition systems are designed to enhance the\nperformance of facial recognition systems by enabling cross-modal matching\nunder challenging operational conditions. A particularly relevant application\nis the matching of near-infrared (NIR) images to visible-spectrum (VIS) images,\nenabling the verification of individuals by comparing NIR facial captures\nacquired with VIS reference images. The use of NIR imaging offers several\nadvantages, including greater robustness to illumination variations, better\nvisibility through glasses and glare, and greater resistance to presentation\nattacks. Despite these claimed benefits, the robustness of NIR-based systems\nagainst presentation attacks has not been systematically studied in the\nliterature. In this work, we conduct a comprehensive evaluation into the\nvulnerability of NIR-VIS cross-spectral face recognition systems to\npresentation attacks. Our empirical findings indicate that, although these\nsystems exhibit a certain degree of reliability, they remain vulnerable to\nspecific attacks, emphasizing the need for further research in this area.", "published": "2025-05-01 08:15:48", "link": "http://arxiv.org/abs/2505.00380v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Cues3D: Unleashing the Power of Sole NeRF for Consistent and Unique Instances in Open-Vocabulary 3D Panoptic Segmentation", "abstract": "Open-vocabulary 3D panoptic segmentation has recently emerged as a\nsignificant trend. Top-performing methods currently integrate 2D segmentation\nwith geometry-aware 3D primitives. However, the advantage would be lost without\nhigh-fidelity 3D point clouds, such as methods based on Neural Radiance Field\n(NeRF). These methods are limited by the insufficient capacity to maintain\nconsistency across partial observations. To address this, recent works have\nutilized contrastive loss or cross-view association pre-processing for view\nconsensus. In contrast to them, we present Cues3D, a compact approach that\nrelies solely on NeRF instead of pre-associations. The core idea is that NeRF's\nimplicit 3D field inherently establishes a globally consistent geometry,\nenabling effective object distinction without explicit cross-view supervision.\nWe propose a three-phase training framework for NeRF,\ninitialization-disambiguation-refinement, whereby the instance IDs are\ncorrected using the initially-learned knowledge. Additionally, an instance\ndisambiguation method is proposed to match NeRF-rendered 3D masks and ensure\nglobally unique 3D instance identities. With the aid of Cues3D, we obtain\nhighly consistent and unique 3D instance ID for each object across views with a\nbalanced version of NeRF. Our experiments are conducted on ScanNet v2,\nScanNet200, ScanNet++, and Replica datasets for 3D instance, panoptic, and\nsemantic segmentation tasks. Cues3D outperforms other 2D image-based methods\nand competes with the latest 2D-3D merging based methods, while even surpassing\nthem when using additional 3D point clouds. The code link could be found in the\nappendix and will be released on\n\\href{https://github.com/mRobotit/Cues3D}{github}", "published": "2025-05-01 08:12:03", "link": "http://arxiv.org/abs/2505.00378v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Towards Lightweight Hyperspectral Image Super-Resolution with Depthwise Separable Dilated Convolutional Network", "abstract": "Deep neural networks have demonstrated highly competitive performance in\nsuper-resolution (SR) for natural images by learning mappings from\nlow-resolution (LR) to high-resolution (HR) images. However, hyperspectral\nsuper-resolution remains an ill-posed problem due to the high spectral\ndimensionality of the data and the scarcity of available training samples.\nMoreover, existing methods often rely on large models with a high number of\nparameters or require the fusion with panchromatic or RGB images, both of which\nare often impractical in real-world scenarios. Inspired by the MobileNet\narchitecture, we introduce a lightweight depthwise separable dilated\nconvolutional network (DSDCN) to address the aforementioned challenges.\nSpecifically, our model leverages multiple depthwise separable convolutions,\nsimilar to the MobileNet architecture, and further incorporates a dilated\nconvolution fusion block to make the model more flexible for the extraction of\nboth spatial and spectral features. In addition, we propose a custom loss\nfunction that combines mean squared error (MSE), an L2 norm\nregularization-based constraint, and a spectral angle-based loss, ensuring the\npreservation of both spectral and spatial details. The proposed model achieves\nvery competitive performance on two publicly available hyperspectral datasets,\nmaking it well-suited for hyperspectral image super-resolution tasks. The\nsource codes are publicly available at:\n\\href{https://github.com/Usman1021/lightweight}{https://github.com/Usman1021/lightweight}.", "published": "2025-05-01 07:57:23", "link": "http://arxiv.org/abs/2505.00374v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Automated segmenta-on of pediatric neuroblastoma on multi-modal MRI: Results of the SPPIN challenge at MICCAI 2023", "abstract": "Surgery plays an important role within the treatment for neuroblastoma, a\ncommon pediatric cancer. This requires careful planning, often via magnetic\nresonance imaging (MRI)-based anatomical 3D models. However, creating these\nmodels is often time-consuming and user dependent. We organized the Surgical\nPlanning in Pediatric Neuroblastoma (SPPIN) challenge, to stimulate\ndevelopments on this topic, and set a benchmark for fully automatic\nsegmentation of neuroblastoma on multi-model MRI. The challenge started with a\ntraining phase, where teams received 78 sets of MRI scans from 34 patients,\nconsisting of both diagnostic and post-chemotherapy MRI scans. The final test\nphase, consisting of 18 MRI sets from 9 patients, determined the ranking of the\nteams. Ranking was based on the Dice similarity coefficient (Dice score), the\n95th percentile of the Hausdorff distance (HD95) and the volumetric similarity\n(VS). The SPPIN challenge was hosted at MICCAI 2023. The final leaderboard\nconsisted of 9 teams. The highest-ranking team achieved a median Dice score\n0.82, a median HD95 of 7.69 mm and a VS of 0.91, utilizing a large, pretrained\nnetwork called STU-Net. A significant difference for the segmentation results\nbetween diagnostic and post-chemotherapy MRI scans was observed (Dice = 0.89 vs\nDice = 0.59, P = 0.01) for the highest-ranking team. SPPIN is the first medical\nsegmentation challenge in extracranial pediatric oncology. The highest-ranking\nteam used a large pre-trained network, suggesting that pretraining can be of\nuse in small, heterogenous datasets. Although the results of the\nhighest-ranking team were high for most patients, segmentation especially in\nsmall, pre-treated tumors were insufficient. Therefore, more reliable\nsegmentation methods are needed to create clinically applicable models to aid\nsurgical planning in pediatric neuroblastoma.", "published": "2025-05-01 07:46:03", "link": "http://arxiv.org/abs/2505.00369v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Quaternion Wavelet-Conditioned Diffusion Models for Image Super-Resolution", "abstract": "Image Super-Resolution is a fundamental problem in computer vision with broad\napplications spacing from medical imaging to satellite analysis. The ability to\nreconstruct high-resolution images from low-resolution inputs is crucial for\nenhancing downstream tasks such as object detection and segmentation. While\ndeep learning has significantly advanced SR, achieving high-quality\nreconstructions with fine-grained details and realistic textures remains\nchallenging, particularly at high upscaling factors. Recent approaches\nleveraging diffusion models have demonstrated promising results, yet they often\nstruggle to balance perceptual quality with structural fidelity. In this work,\nwe introduce ResQu a novel SR framework that integrates a quaternion wavelet\npreprocessing framework with latent diffusion models, incorporating a new\nquaternion wavelet- and time-aware encoder. Unlike prior methods that simply\napply wavelet transforms within diffusion models, our approach enhances the\nconditioning process by exploiting quaternion wavelet embeddings, which are\ndynamically integrated at different stages of denoising. Furthermore, we also\nleverage the generative priors of foundation models such as Stable Diffusion.\nExtensive experiments on domain-specific datasets demonstrate that our method\nachieves outstanding SR results, outperforming in many cases existing\napproaches in perceptual quality and standard evaluation metrics. The code will\nbe available after the revision process.", "published": "2025-05-01 06:17:33", "link": "http://arxiv.org/abs/2505.00334v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "AWARE-NET: Adaptive Weighted Averaging for Robust Ensemble Network in Deepfake Detection", "abstract": "Deepfake detection has become increasingly important due to the rise of\nsynthetic media, which poses significant risks to digital identity and cyber\npresence for security and trust. While multiple approaches have improved\ndetection accuracy, challenges remain in achieving consistent performance\nacross diverse datasets and manipulation types. In response, we propose a novel\ntwo-tier ensemble framework for deepfake detection based on deep learning that\nhierarchically combines multiple instances of three state-of-the-art\narchitectures: Xception, Res2Net101, and EfficientNet-B7. Our framework employs\na unique approach where each architecture is instantiated three times with\ndifferent initializations to enhance model diversity, followed by a learnable\nweighting mechanism that dynamically combines their predictions. Unlike\ntraditional fixed-weight ensembles, our first-tier averages predictions within\neach architecture family to reduce model variance, while the second tier learns\noptimal contribution weights through backpropagation, automatically adjusting\neach architecture's influence based on their detection reliability. Our\nexperiments achieved state-of-the-art intra-dataset performance with AUC scores\nof 99.22% (FF++) and 100.00% (CelebDF-v2), and F1 scores of 98.06% (FF++) and\n99.94% (CelebDF-v2) without augmentation. With augmentation, we achieve AUC\nscores of 99.47% (FF++) and 100.00% (CelebDF-v2), and F1 scores of 98.43%\n(FF++) and 99.95% (CelebDF-v2). The framework demonstrates robust cross-dataset\ngeneralization, achieving AUC scores of 88.20% and 72.52%, and F1 scores of\n93.16% and 80.62% in cross-dataset evaluations.", "published": "2025-05-01 05:14:50", "link": "http://arxiv.org/abs/2505.00312v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "AdCare-VLM: Leveraging Large Vision Language Model (LVLM) to Monitor Long-Term Medication Adherence and Care", "abstract": "Chronic diseases, including diabetes, hypertension, asthma, HIV-AIDS,\nepilepsy, and tuberculosis, necessitate rigorous adherence to medication to\navert disease progression, manage symptoms, and decrease mortality rates.\nAdherence is frequently undermined by factors including patient behavior,\ncaregiver support, elevated medical costs, and insufficient healthcare\ninfrastructure. We propose AdCare-VLM, a specialized Video-LLaVA-based\nmultimodal large vision language model (LVLM) aimed at visual question\nanswering (VQA) concerning medication adherence through patient videos. We\nemploy a private dataset comprising 806 custom-annotated tuberculosis (TB)\nmedication monitoring videos, which have been labeled by clinical experts, to\nfine-tune the model for adherence pattern detection. We present LLM-TB-VQA, a\ndetailed medical adherence VQA dataset that encompasses positive, negative, and\nambiguous adherence cases. Our method identifies correlations between visual\nfeatures, such as the clear visibility of the patient's face, medication, water\nintake, and the act of ingestion, and their associated medical concepts in\ncaptions. This facilitates the integration of aligned visual-linguistic\nrepresentations and improves multimodal interactions. Experimental results\nindicate that our method surpasses parameter-efficient fine-tuning (PEFT)\nenabled VLM models, such as LLaVA-V1.5 and Chat-UniVi, with absolute\nimprovements ranging from 3.1% to 3.54% across pre-trained, regular, and\nlow-rank adaptation (LoRA) configurations. Comprehensive ablation studies and\nattention map visualizations substantiate our approach, enhancing\ninterpretability.", "published": "2025-05-01 03:48:12", "link": "http://arxiv.org/abs/2505.00275v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "ReXGradient-160K: A Large-Scale Publicly Available Dataset of Chest Radiographs with Free-text Reports", "abstract": "We present ReXGradient-160K, representing the largest publicly available\nchest X-ray dataset to date in terms of the number of patients. This dataset\ncontains 160,000 chest X-ray studies with paired radiological reports from\n109,487 unique patients across 3 U.S. health systems (79 medical sites). This\ncomprehensive dataset includes multiple images per study and detailed radiology\nreports, making it particularly valuable for the development and evaluation of\nAI systems for medical imaging and automated report generation models. The\ndataset is divided into training (140,000 studies), validation (10,000\nstudies), and public test (10,000 studies) sets, with an additional private\ntest set (10,000 studies) reserved for model evaluation on the ReXrank\nbenchmark. By providing this extensive dataset, we aim to accelerate research\nin medical imaging AI and advance the state-of-the-art in automated\nradiological analysis. Our dataset will be open-sourced at\nhttps://huggingface.co/datasets/rajpurkarlab/ReXGradient-160K.", "published": "2025-05-01 00:29:50", "link": "http://arxiv.org/abs/2505.00228v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Maximum list $r$-colorable induced subgraphs in $kP_3$-free graphs", "abstract": "We show that, for every fixed positive integers $r$ and $k$,\n\\textsc{Max-Weight List $r$-Colorable Induced Subgraph} admits a\npolynomial-time algorithm on $kP_3$-free graphs. This problem is a common\ngeneralization of \\textsc{Max-Weight Independent Set}, \\textsc{Odd Cycle\nTransversal} and \\textsc{List $r$-Coloring}, among others. Our result has\nseveral consequences.\n  First, it implies that, for every fixed $r \\geq 5$, assuming $\\mathsf{P}\\neq\n\\mathsf{NP}$, \\textsc{Max-Weight List $r$-Colorable Induced Subgraph} is\npolynomial-time solvable on $H$-free graphs if and only if $H$ is an induced\nsubgraph of either $kP_3$ or $P_5+kP_1$, for some $k \\geq 1$. Second, it makes\nconsiderable progress toward a complexity dichotomy for \\textsc{Odd Cycle\nTransversal} on $H$-free graphs, allowing to answer a question of Agrawal,\nLima, Lokshtanov, Rz{\\k{a}}{\\.z}ewski, Saurabh, and Sharma [TALG 2024]. Third,\nit gives a short and self-contained proof of the known result of Chudnovsky,\nHajebi, and Spirkl [Combinatorica 2024] that \\textsc{List $r$-Coloring} on\n$kP_3$-free graphs is polynomial-time solvable for every fixed $r$ and $k$.\n  We also consider two natural distance-$d$ generalizations of\n\\textsc{Max-Weight Independent Set} and \\textsc{List $r$-Coloring} and provide\npolynomial-time algorithms on $kP_3$-free graphs for every fixed integers $r$,\n$k$, and $d \\geq 6$.", "published": "2025-05-01 09:15:53", "link": "http://arxiv.org/abs/2505.00412v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "Efficient Recommendation with Millions of Items by Dynamic Pruning of Sub-Item Embeddings", "abstract": "A large item catalogue is a major challenge for deploying modern sequential\nrecommender models, since it makes the memory footprint of the model large and\nincreases inference latency. One promising approach to address this is RecJPQ,\nwhich replaces item embeddings with sub-item embeddings. However, slow\ninference remains problematic because finding the top highest-scored items\nusually requires scoring all items in the catalogue, which may not be feasible\nfor large catalogues. By adapting dynamic pruning concepts from document\nretrieval, we propose the RecJPQPrune dynamic pruning algorithm to efficiently\nfind the top highest-scored items without computing the scores of all items in\nthe catalogue. Our RecJPQPrune algorithm is safe-up-to-rank K since it\ntheoretically guarantees that no potentially high-scored item is excluded from\nthe final top K recommendation list, thereby ensuring no impact on\neffectiveness. Our experiments on two large datasets and three recommendation\nmodels demonstrate the efficiency achievable using RecJPQPrune: for instance,\non the Tmall dataset with 2.2M items, we can reduce the median model scoring\ntime by 64 times compared to the Transformer Default baseline, and 5.3 times\ncompared to a recent scoring approach called PQTopK. Overall, this paper\ndemonstrates the effective and efficient inference of Transformer-based\nrecommendation models at catalogue scales not previously reported in the\nliterature. Indeed, our RecJPQPrune algorithm can score 2 million items in\nunder 10 milliseconds without GPUs, and without relying on Approximate Nearest\nNeighbour (ANN) techniques.", "published": "2025-05-01 14:36:33", "link": "http://arxiv.org/abs/2505.00560v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Graph Spectral Filtering with Chebyshev Interpolation for Recommendation", "abstract": "Graph convolutional networks have recently gained prominence in collaborative\nfiltering (CF) for recommendations. However, we identify potential bottlenecks\nin two foundational components. First, the embedding layer leads to a latent\nspace with limited capacity, overlooking locally observed but potentially\nvaluable preference patterns. Also, the widely-used neighborhood aggregation is\nlimited in its ability to leverage diverse preference patterns in a\nfine-grained manner. Building on spectral graph theory, we reveal that these\nlimitations stem from graph filtering with a cut-off in the frequency spectrum\nand a restricted linear form. To address these issues, we introduce ChebyCF, a\nCF framework based on graph spectral filtering. Instead of a learned embedding,\nit takes a user's raw interaction history to utilize the full spectrum of\nsignals contained in it. Also, it adopts Chebyshev interpolation to effectively\napproximate a flexible non-linear graph filter, and further enhances it by\nusing an additional ideal pass filter and degree-based normalization. Through\nextensive experiments, we verify that ChebyCF overcomes the aforementioned\nbottlenecks and achieves state-of-the-art performance across multiple\nbenchmarks and reasonably fast inference. Our code is available at\nhttps://github.com/chanwoo0806/ChebyCF.", "published": "2025-05-01 14:28:44", "link": "http://arxiv.org/abs/2505.00552v1", "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Key exchange protocol based on circulant matrix action over congruence-simple semiring", "abstract": "We present a new key exchange protocol based on circulant matrices acting on\nmatrices over a congruence-simple semiring. We describe how to compute matrices\nwith the necessary properties for the implementation of the protocol.\nAdditionally, we provide an analysis of its computational cost and its security\nagainst known attacks.", "published": "2025-05-01 17:07:11", "link": "http://arxiv.org/abs/2505.00664v1", "categories": ["math.AC", "cs.CR", "cs.IT", "math.IT"], "primary_category": "math.AC"}
{"title": "AI-based CSI Feedback with Digital Twins: Real-World Validation and Insights", "abstract": "Deep learning (DL) has shown great potential for enhancing channel state\ninformation (CSI) feedback in multiple-input multiple-output (MIMO)\ncommunication systems, a subject currently under study by the 3GPP standards\nbody. Digital twins (DTs) have emerged as an effective means to generate\nsite-specific datasets for training DL-based CSI feedback models. However, most\nexisting studies rely solely on simulations, leaving the effectiveness of DTs\nin reducing DL training costs yet to be validated through realistic\nexperimental setups. This paper addresses this gap by establishing a real-world\n(RW) environment and corresponding virtual channels using ray tracing with\nreplicated 3D models and accurate antenna properties. We evaluate whether\nmodels trained in DT environments can effectively operate in RW scenarios and\nquantify the benefits of online learning (OL) for performance enhancement.\nResults show that a dedicated DT remains essential even with OL to achieve\nsatisfactory performance in RW scenarios.", "published": "2025-05-01 17:02:22", "link": "http://arxiv.org/abs/2505.00660v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "RIS Partitioning and User Clustering for Resilient Non-Orthogonal Multiple Access UAV Networks", "abstract": "The integration of reconfigurable intelligent surfaces (RISs) and unmanned\naerial vehicles (UAVs) has emerged as a promising solution for enhancing\nconnectivity in future wireless networks. This paper designs well-connected and\nresilient UAV networks by deploying and virtually partitioning multiple RISs to\ncreate multiple RIS-aided links, focusing on a link-layer perspective. The\nRIS-aided links are created to connect user equipment (UE) to blocked and\nreliable UAVs, where multiple UEs can transmit to same UAV via RIS using\nnon-orthogonal multiple access (NOMA), granting access to UEs and maximizing\nnetwork connectivity. We first derive exact and approximated closed-form\nexpressions for signal-to-interference plus noise ratio (SINR) based on aligned\nand non-aligned RIS-aided beams. Then, we propose to formulate the problem of\nmaximizing network connectivity that jointly considers (i) UE NOMA clustering,\n(ii) RIS-aided link selection, and (ii) virtual RIS partitioning. This problem\nis a computationally expensive combinatorial optimization. To tackle this\nproblem, a two-step iterative approach, called RIS-aided NOMA, is proposed. In\nthe first step, the UEs are clustered to the RISs according to their channel\ngains, while UAVs are associated to those generated clusters based on their\nreliability, which measures the criticality of UAVs. The second step optimally\npartitions the RISs to support each of the cluster members. In this step, we\nderive the closed-form equations for the optimal partitioning of RISs within\nthe clusters. Simulation results demonstrate that the proposed RIS-aided NOMA\nyields a gain of 30% to 40%, respectively, compared to UAV traditional scheme.\nThe finding emphasizes the potential of integrating RIS with UAV communications\nas a robust and reliable connectivity solution for future wireless\ncommunication systems.", "published": "2025-05-01 17:00:50", "link": "http://arxiv.org/abs/2505.00658v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Error Exponents for Oblivious Relaying and Connections to Source Coding with a Helper", "abstract": "The information bottleneck channel, also known as oblivious relaying, is a\ntwo-hop channel where a transmitter sends messages to a remote receiver via an\nintermediate relay node. A codeword sent by the transmitter passes through a\ndiscrete memoryless channel to reach the relay, and then the relay processes\nthe noisy channel output and forwards it to the receiver through a noiseless\nrate-limited link. The relay is oblivious, in the sense that it has no\nknowledge of the channel codebook used in transmission. Past works on oblivious\nrelaying are focused on characterizing achievable rates. In this work, we study\nerror exponents and explore connections to loseless source coding with a\nhelper, also known as the Wyner-Ahlswede-K\\\"orner (WAK) problem. We first\nestablish an achievable error exponent for oblivious relaying under constant\ncompositions codes. A key feature of our analysis is the use of the type\ncovering lemma to design the relay's compress-forward scheme. We then show that\nemploying constant composition code ensembles does not improve the rates\nachieved with their IID counterparts. We also derive a sphere packing upper\nbound for the error exponent. In the second part of this paper, we establish a\nconnection between the information bottleneck channel and the WAK problem. We\nshow that good codes for the latter can be produced through permuting codes\ndesigned for the former. This is accomplished by revisiting Ahlswede's covering\nlemma, and extending it to achieve simultaneous covering of a type class by\nseveral distinct sets using the same sequence of permutations. We then apply\nour approach to attain the best known achievable error exponent for the WAK\nproblem, previously established by Kelly and Wagner. As a byproduct of our\nderivations, we also establish error exponents and achievable rates under\nmismatched decoding rules.", "published": "2025-05-01 14:50:51", "link": "http://arxiv.org/abs/2505.00567v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Exponentially Consistent Low Complexity Tests for Outlier Hypothesis Testing with Distribution Uncertainty", "abstract": "We revisit the outlier hypothesis testing (OHT) problem of Li et al. (TIT\n2024) and propose exponentially consistent tests when there is distribution\nuncertainty for both nominal samples and outliers. In original OHT, one is\ngiven a list of sequences, most of which are generated i.i.d. from a\ndistribution called the nominal distribution while the rest are generated\ni.i.d. from another distribution named the anomalous distribution. The task of\nOHT is to identify outliers when both the nominal and anomalous distributions\nare unknown. Motivated by the study for classification with distribution\nuncertainty by Hsu and Wang (ISIT 2020), we consider OHT with distribution\nuncertainty, where each nominal sample is generated from a distribution\ncentered around the unknown nominal distribution and each outlier is generated\nfrom a distribution centered around the unknown anomalous distribution. With a\nfurther step towards practical applications, in the spirit of Bu et al. (TSP\n2019), we propose low-complexity tests when the number of outliers is known and\nunknown, and show that our proposed tests are exponentially consistent.\nFurthermore, we demonstrate that there is a penalty for not knowing the number\nof outliers in the error exponent when outliers exist. Our results strengthen\nBu et al. in three aspects: i) our tests allow distribution uncertainty and\nreveal the impact of distribution uncertainty on the performance of\nlow-complexity tests; ii) when the number of outliers is known and there is no\ndistribution uncertainty, our test achieves the same asymptotic performance\nwith lower complexity; and iii) when the number of outliers is unknown, we\ncharacterize the tradeoff among the three error probabilities, while two of\nthese error probabilities were not analyzed by Bu et al. even when there is no\ndistribution uncertainty. Finally, we illustrate our theoretical results using\nnumerical examples.", "published": "2025-05-01 14:33:56", "link": "http://arxiv.org/abs/2505.00558v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Sum Rate Maximization for NOMA-Assisted Uplink Pinching-Antenna Systems", "abstract": "In this paper, we investigate an uplink communication scenario in which\nmultiple users communicate with an access point (AP) employing non-orthogonal\nmultiple access (NOMA). A pinching antenna, which can be activated at an\narbitrary point along a dielectric waveguide, is deployed at the AP to\ndynamically reconfigure user channels. The objective is to maximize the system\nsum rate by jointly optimizing the pinching-antenna's position and the users'\ntransmit powers. The formulated optimization problem is non-convex, and\naddressed using the particle swarm optimization (PSO) algorithm. For\nperformance benchmarking, two time division multiple access (TDMA) schemes are\nconsidered: one based on the pinching antenna individually activated for each\nuser, and the other based on the single-pinching-antenna configuration serving\nall users. Numerical results demonstrate that the use of the pinching antenna\nsignificantly enhances the system sum rate compared to conventional antenna\narchitectures. Moreover, the NOMA-based scheme outperforms the TDMA-based\nscheme with a single pinching antenna but is outperformed by the TDMA-based\napproach when the pinching antenna is adaptively configured for each user.\nFinally, the proposed PSO-based method is shown to achieve near-optimal\nperformance for both NOMA and TDMA with a common pinching-antenna\nconfiguration.", "published": "2025-05-01 14:27:02", "link": "http://arxiv.org/abs/2505.00549v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "GAN-based Generator of Adversarial Attack on Intelligent End-to-End Autoencoder-based Communication System", "abstract": "Deep neural networks have been applied in wireless communications system to\nintelligently adapt to dynamically changing channel conditions, while the users\nare still under the threat of the malicious attacks due to the broadcasting\nproperty of wireless channels. However, most attack models require the\nknowledge of the target details, which is difficult to be implemented in real\nsystems. Our objective is to develop an attack model with no requirement for\nthe target information, while enhancing the block error rate. In our design, we\npropose a novel Generative Adversarial Networks(GANs) based attack\narchitecture, which exploits the property of deep learning models being\nvulnerable to perturbations induced by dynamically changing channel conditions.\nIn the proposed generator, the attack network is composed of convolution layer,\nconvolution transpose layer and linear layer. Then we present the training\nstrategy and the details of the training algorithm. Subsequently, we propose\nthe validation strategy to evaluate the performance of the generator.\nSimulations are conducted and the results show that our proposed adversarial\nattack generator achieve better block error rate attack performance than that\nof benchmark schemes over Additive White Gaussian Noise (AWGN) channel,\nRayleigh channel and High-Speed Railway channel.", "published": "2025-05-01 08:31:54", "link": "http://arxiv.org/abs/2505.00395v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "On the Importance of Gaussianizing Representations", "abstract": "The normal distribution plays a central role in information theory - it is at\nthe same time the best-case signal and worst-case noise distribution, has the\ngreatest representational capacity of any distribution, and offers an\nequivalence between uncorrelatedness and independence for joint distributions.\nAccounting for the mean and variance of activations throughout the layers of\ndeep neural networks has had a significant effect on facilitating their\neffective training, but seldom has a prescription for precisely what\ndistribution these activations should take, and how this might be achieved,\nbeen offered. Motivated by the information-theoretic properties of the normal\ndistribution, we address this question and concurrently present normality\nnormalization: a novel normalization layer which encourages normality in the\nfeature representations of neural networks using the power transform and\nemploys additive Gaussian noise during training. Our experiments\ncomprehensively demonstrate the effectiveness of normality normalization, in\nregards to its generalization performance on an array of widely used model and\ndataset combinations, its strong performance across various common factors of\nvariation such as model width, depth, and training minibatch size, its\nsuitability for usage wherever existing normalization layers are conventionally\nused, and as a means to improving model robustness to random perturbations.", "published": "2025-05-01 17:47:44", "link": "http://arxiv.org/abs/2505.00685v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Bayes-Optimal Fair Classification with Multiple Sensitive Features", "abstract": "Existing theoretical work on Bayes-optimal fair classifiers usually considers\na single (binary) sensitive feature. In practice, individuals are often defined\nby multiple sensitive features. In this paper, we characterize the\nBayes-optimal fair classifier for multiple sensitive features under general\napproximate fairness measures, including mean difference and mean ratio. We\nshow that these approximate measures for existing group fairness notions,\nincluding Demographic Parity, Equal Opportunity, Predictive Equality, and\nAccuracy Parity, are linear transformations of selection rates for specific\ngroups defined by both labels and sensitive features. We then characterize that\nBayes-optimal fair classifiers for multiple sensitive features become\ninstance-dependent thresholding rules that rely on a weighted sum of these\ngroup membership probabilities. Our framework applies to both attribute-aware\nand attribute-blind settings and can accommodate composite fairness notions\nlike Equalized Odds. Building on this, we propose two practical algorithms for\nBayes-optimal fair classification via in-processing and post-processing. We\nshow empirically that our methods compare favorably to existing methods.", "published": "2025-05-01 16:12:12", "link": "http://arxiv.org/abs/2505.00631v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "SA-GAT-SR: Self-Adaptable Graph Attention Networks with Symbolic Regression for high-fidelity material property prediction", "abstract": "Recent advances in machine learning have demonstrated an enormous utility of\ndeep learning approaches, particularly Graph Neural Networks (GNNs) for\nmaterials science. These methods have emerged as powerful tools for\nhigh-throughput prediction of material properties, offering a compelling\nenhancement and alternative to traditional first-principles calculations. While\nthe community has predominantly focused on developing increasingly complex and\nuniversal models to enhance predictive accuracy, such approaches often lack\nphysical interpretability and insights into materials behavior. Here, we\nintroduce a novel computational paradigm, Self-Adaptable Graph Attention\nNetworks integrated with Symbolic Regression (SA-GAT-SR), that synergistically\ncombines the predictive capability of GNNs with the interpretative power of\nsymbolic regression. Our framework employs a self-adaptable encoding algorithm\nthat automatically identifies and adjust attention weights so as to screen\ncritical features from an expansive 180-dimensional feature space while\nmaintaining O(n) computational scaling. The integrated SR module subsequently\ndistills these features into compact analytical expressions that explicitly\nreveal quantum-mechanically meaningful relationships, achieving 23 times\nacceleration compared to conventional SR implementations that heavily rely on\nfirst principle calculations-derived features as input. This work suggests a\nnew framework in computational materials science, bridging the gap between\npredictive accuracy and physical interpretability, offering valuable physical\ninsights into material behavior.", "published": "2025-05-01 16:05:10", "link": "http://arxiv.org/abs/2505.00625v1", "categories": ["physics.comp-ph", "cond-mat.mtrl-sci", "cs.LG"], "primary_category": "physics.comp-ph"}
{"title": "Explainable AI in Spatial Analysis", "abstract": "This chapter discusses the opportunities of eXplainable Artificial\nIntelligence (XAI) within the realm of spatial analysis. A key objective in\nspatial analysis is to model spatial relationships and infer spatial processes\nto generate knowledge from spatial data, which has been largely based on\nspatial statistical methods. More recently, machine learning offers scalable\nand flexible approaches that complement traditional methods and has been\nincreasingly applied in spatial data science. Despite its advantages, machine\nlearning is often criticized for being a black box, which limits our\nunderstanding of model behavior and output. Recognizing this limitation, XAI\nhas emerged as a pivotal field in AI that provides methods to explain the\noutput of machine learning models to enhance transparency and understanding.\nThese methods are crucial for model diagnosis, bias detection, and ensuring the\nreliability of results obtained from machine learning models. This chapter\nintroduces key concepts and methods in XAI with a focus on Shapley value-based\napproaches, which is arguably the most popular XAI method, and their\nintegration with spatial analysis. An empirical example of county-level voting\nbehaviors in the 2020 Presidential election is presented to demonstrate the use\nof Shapley values and spatial analysis with a comparison to multi-scale\ngeographically weighted regression. The chapter concludes with a discussion on\nthe challenges and limitations of current XAI techniques and proposes new\ndirections.", "published": "2025-05-01 15:25:23", "link": "http://arxiv.org/abs/2505.00591v1", "categories": ["cs.LG", "econ.EM"], "primary_category": "cs.LG"}
{"title": "Unlocking the Potential of Linear Networks for Irregular Multivariate Time Series Forecasting", "abstract": "Time series forecasting holds significant importance across various\nindustries, including finance, transportation, energy, healthcare, and climate.\nDespite the widespread use of linear networks due to their low computational\ncost and effectiveness in modeling temporal dependencies, most existing\nresearch has concentrated on regularly sampled and fully observed multivariate\ntime series. However, in practice, we frequently encounter irregular\nmultivariate time series characterized by variable sampling intervals and\nmissing values. The inherent intra-series inconsistency and inter-series\nasynchrony in such data hinder effective modeling and forecasting with\ntraditional linear networks relying on static weights. To tackle these\nchallenges, this paper introduces a novel model named AiT. AiT utilizes an\nadaptive linear network capable of dynamically adjusting weights according to\nobservation time points to address intra-series inconsistency, thereby\nenhancing the accuracy of temporal dependencies modeling. Furthermore, by\nincorporating the Transformer module on variable semantics embeddings, AiT\nefficiently captures variable correlations, avoiding the challenge of\ninter-series asynchrony. Comprehensive experiments across four benchmark\ndatasets demonstrate the superiority of AiT, improving prediction accuracy by\n11% and decreasing runtime by 52% compared to existing state-of-the-art\nmethods.", "published": "2025-05-01 15:24:48", "link": "http://arxiv.org/abs/2505.00590v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "ParkDiffusion: Heterogeneous Multi-Agent Multi-Modal Trajectory Prediction for Automated Parking using Diffusion Models", "abstract": "Automated parking is a critical feature of Advanced Driver Assistance Systems\n(ADAS), where accurate trajectory prediction is essential to bridge perception\nand planning modules. Despite its significance, research in this domain remains\nrelatively limited, with most existing studies concentrating on single-modal\ntrajectory prediction of vehicles. In this work, we propose ParkDiffusion, a\nnovel approach that predicts the trajectories of both vehicles and pedestrians\nin automated parking scenarios. ParkDiffusion employs diffusion models to\ncapture the inherent uncertainty and multi-modality of future trajectories,\nincorporating several key innovations. First, we propose a dual map encoder\nthat processes soft semantic cues and hard geometric constraints using a\ntwo-step cross-attention mechanism. Second, we introduce an adaptive agent type\nembedding module, which dynamically conditions the prediction process on the\ndistinct characteristics of vehicles and pedestrians. Third, to ensure\nkinematic feasibility, our model outputs control signals that are subsequently\nused within a kinematic framework to generate physically feasible trajectories.\nWe evaluate ParkDiffusion on the Dragon Lake Parking (DLP) dataset and the\nIntersections Drone (inD) dataset. Our work establishes a new baseline for\nheterogeneous trajectory prediction in parking scenarios, outperforming\nexisting methods by a considerable margin.", "published": "2025-05-01 15:16:59", "link": "http://arxiv.org/abs/2505.00586v1", "categories": ["cs.RO", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Parameter-Efficient Fine-Tuning with Circulant and Diagonal Vectors", "abstract": "Foundation models have achieved tremendous success in different domains.\nHowever, their huge computation and storage complexity make these models\ndifficult to fine-tune and also less applicable in practice. Recent study shows\ntraining in Fourier domain can be an effective fine-tuning method in terms of\nboth model performance and number of training parameters. In this work, we\npropose to further reduce the complexity by the factorization through the\nproduct of interleaved circulant and diagonal matrices. In addition, we address\nthe case of non-square fine-tuning weights by partitioning the circulant matrix\ninto blocks. Our method avoids the construction of weight change matrix and\nutilizes 1D fast Fourier transform (FFT) instead of 2D FFT. Experimental\nresults show that our method achieves similar or better performance across\nvarious tasks with much less floating-point operations (FLOPs) and the number\nof trainable parameters.", "published": "2025-05-01 15:11:46", "link": "http://arxiv.org/abs/2505.00580v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Transition States Energies from Machine Learning: An Application to Reverse Water-Gas Shift on Single-Atom Alloys", "abstract": "Obtaining accurate transition state (TS) energies is a bottleneck in\ncomputational screening of complex materials and reaction networks due to the\nhigh cost of TS search methods and first-principles methods such as density\nfunctional theory (DFT). Here we propose a machine learning (ML) model for\npredicting TS energies based on Gaussian process regression with the\nWasserstein Weisfeiler-Lehman graph kernel (WWL-GPR). Applying the model to\npredict adsorption and TS energies for the reverse water-gas shift (RWGS)\nreaction on single-atom alloy (SAA) catalysts, we show that it can\nsignificantly improve the accuracy compared to traditional approaches based on\nscaling relations or ML models without a graph representation. Further\nbenefitting from the low cost of model training, we train an ensemble of\nWWL-GPR models to obtain uncertainties through subsampling of the training data\nand show how these uncertainties propagate to turnover frequency (TOF)\npredictions through the construction of an ensemble of microkinetic models.\nComparing the errors in model-based vs DFT-based TOF predictions, we show that\nthe WWL-GPR model reduces errors by almost an order of magnitude compared to\nscaling relations. This demonstrates the critical impact of accurate energy\npredictions on catalytic activity estimation. Finally, we apply our model to\nscreen new materials, identifying promising catalysts for RWGS. This work\nhighlights the power of combining advanced ML techniques with DFT and\nmicrokinetic modeling for screening catalysts for complex reactions like RWGS,\nproviding a robust framework for future catalyst design.", "published": "2025-05-01 15:01:02", "link": "http://arxiv.org/abs/2505.00574v1", "categories": ["cond-mat.mtrl-sci", "cs.LG"], "primary_category": "cond-mat.mtrl-sci"}
{"title": "Hypothesis-free discovery from epidemiological data by automatic detection and local inference for tree-based nonlinearities and interactions", "abstract": "In epidemiological settings, Machine Learning (ML) is gaining popularity for\nhypothesis-free discovery of risk (or protective) factors. Although ML is\nstrong at discovering non-linearities and interactions, this power is currently\ncompromised by a lack of reliable inference. Although local measures of feature\neffect can be combined with tree ensembles, uncertainty quantifications for\nthese measures remain only partially available and oftentimes unsatisfactory.\nWe propose RuleSHAP, a framework for using rule-based, hypothesis-free\ndiscovery that combines sparse Bayesian regression, tree ensembles and Shapley\nvalues in a one-step procedure that both detects and tests complex patterns at\nthe individual level. To ease computation, we derive a formula that computes\nmarginal Shapley values more efficiently for our setting. We demonstrate the\nvalidity of our framework on simulated data. To illustrate, we apply our\nmachinery to data from an epidemiological cohort to detect and infer several\neffects for high cholesterol and blood pressure, such as nonlinear interaction\neffects between features like age, sex, ethnicity, BMI and glucose level.", "published": "2025-05-01 14:55:22", "link": "http://arxiv.org/abs/2505.00571v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Directly Forecasting Belief for Reinforcement Learning with Delays", "abstract": "Reinforcement learning (RL) with delays is challenging as sensory perceptions\nlag behind the actual events: the RL agent needs to estimate the real state of\nits environment based on past observations. State-of-the-art (SOTA) methods\ntypically employ recursive, step-by-step forecasting of states. This can cause\nthe accumulation of compounding errors. To tackle this problem, our novel\nbelief estimation method, named Directly Forecasting Belief Transformer (DFBT),\ndirectly forecasts states from observations without incrementally estimating\nintermediate states step-by-step. We theoretically demonstrate that DFBT\ngreatly reduces compounding errors of existing recursively forecasting methods,\nyielding stronger performance guarantees. In experiments with D4RL offline\ndatasets, DFBT reduces compounding errors with remarkable prediction accuracy.\nDFBT's capability to forecast state sequences also facilitates multi-step\nbootstrapping, thus greatly improving learning efficiency. On the MuJoCo\nbenchmark, our DFBT-based method substantially outperforms SOTA baselines.", "published": "2025-05-01 14:20:48", "link": "http://arxiv.org/abs/2505.00546v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "KnowEEG: Explainable Knowledge Driven EEG Classification", "abstract": "Electroencephalography (EEG) is a method of recording brain activity that\nshows significant promise in applications ranging from disease classification\nto emotion detection and brain-computer interfaces. Recent advances in deep\nlearning have improved EEG classification performance yet model explainability\nremains an issue. To address this key limitation of explainability we introduce\nKnowEEG; a novel explainable machine learning approach for EEG classification.\nKnowEEG extracts a comprehensive set of per-electrode features, filters them\nusing statistical tests, and integrates between-electrode connectivity\nstatistics. These features are then input to our modified Random Forest model\n(Fusion Forest) that balances per electrode statistics with between electrode\nconnectivity features in growing the trees of the forest. By incorporating\nknowledge from both the generalized time-series and EEG-specific domains,\nKnowEEG achieves performance comparable to or exceeding state-of-the-art deep\nlearning models across five different classification tasks: emotion detection,\nmental workload classification, eyes open/closed detection, abnormal EEG\nclassification, and event detection. In addition to high performance, KnowEEG\nprovides inherent explainability through feature importance scores for\nunderstandable features. We demonstrate by example on the eyes closed/open\nclassification task that this explainability can be used to discover knowledge\nabout the classes. This discovered knowledge for eyes open/closed\nclassification was proven to be correct by current neuroscience literature.\nTherefore, the impact of KnowEEG will be significant for domains where EEG\nexplainability is critical such as healthcare.", "published": "2025-05-01 14:05:55", "link": "http://arxiv.org/abs/2505.00541v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Emergence of Roles in Robotic Teams with Model Sharing and Limited Communication", "abstract": "We present a reinforcement learning strategy for use in multi-agent foraging\nsystems in which the learning is centralised to a single agent and its model is\nperiodically disseminated among the population of non-learning agents. In a\ndomain where multi-agent reinforcement learning (MARL) is the common approach,\nthis approach aims to significantly reduce the computational and energy demands\ncompared to approaches such as MARL and centralised learning models. By\ndeveloping high performing foraging agents, these approaches can be translated\ninto real-world applications such as logistics, environmental monitoring, and\nautonomous exploration. A reward function was incorporated into this approach\nthat promotes role development among agents, without explicit directives. This\nled to the differentiation of behaviours among the agents. The implicit\nencouragement of role differentiation allows for dynamic actions in which\nagents can alter roles dependent on their interactions with the environment\nwithout the need for explicit communication between agents.", "published": "2025-05-01 14:05:46", "link": "http://arxiv.org/abs/2505.00540v1", "categories": ["cs.MA", "cs.LG", "cs.RO", "cs.SY", "eess.SY"], "primary_category": "cs.MA"}
{"title": "Leveraging Partial SMILES Validation Scheme for Enhanced Drug Design in Reinforcement Learning Frameworks", "abstract": "SMILES-based molecule generation has emerged as a powerful approach in drug\ndiscovery. Deep reinforcement learning (RL) using large language model (LLM)\nhas been incorporated into the molecule generation process to achieve high\nmatching score in term of likelihood of desired molecule candidates. However, a\ncritical challenge in this approach is catastrophic forgetting during the RL\nphase, where knowledge such as molecule validity, which often exceeds 99\\%\nduring pretraining, significantly deteriorates. Current RL algorithms applied\nin drug discovery, such as REINVENT, use prior models as anchors to retian\npretraining knowledge, but these methods lack robust exploration mechanisms. To\naddress these issues, we propose Partial SMILES Validation-PPO (PSV-PPO), a\nnovel RL algorithm that incorporates real-time partial SMILES validation to\nprevent catastrophic forgetting while encouraging exploration. Unlike\ntraditional RL approaches that validate molecule structures only after\ngenerating entire sequences, PSV-PPO performs stepwise validation at each\nauto-regressive step, evaluating not only the selected token candidate but also\nall potential branches stemming from the prior partial sequence. This enables\nearly detection of invalid partial SMILES across all potential paths. As a\nresult, PSV-PPO maintains high validity rates even during aggressive\nexploration of the vast chemical space. Our experiments on the PMO and GuacaMol\nbenchmark datasets demonstrate that PSV-PPO significantly reduces the number of\ninvalid generated structures while maintaining competitive exploration and\noptimization performance. While our work primarily focuses on maintaining\nvalidity, the framework of PSV-PPO can be extended in future research to\nincorporate additional forms of valuable domain knowledge, further enhancing\nreinforcement learning applications in drug discovery.", "published": "2025-05-01 13:57:20", "link": "http://arxiv.org/abs/2505.00530v1", "categories": ["cs.LG", "cs.CE", "q-bio.BM"], "primary_category": "cs.LG"}
{"title": "Pre-Training Estimators for Structural Models: Application to Consumer Search", "abstract": "We explore pretraining estimators for structural econometric models. The\nestimator is \"pretrained\" in the sense that the bulk of the computational cost\nand researcher effort occur during the construction of the estimator.\nSubsequent applications of the estimator to different datasets require little\ncomputational cost or researcher effort. The estimation leverages a neural net\nto recognize the structural model's parameter from data patterns. As an initial\ntrial, this paper builds a pretrained estimator for a sequential search model\nthat is known to be difficult to estimate. We evaluate the pretrained estimator\non 14 real datasets. The estimation takes seconds to run and shows high\naccuracy. We provide the estimator at pnnehome.github.io. More generally,\npretrained, off-the-shelf estimators can make structural models more accessible\nto researchers and practitioners.", "published": "2025-05-01 13:51:01", "link": "http://arxiv.org/abs/2505.00526v1", "categories": ["econ.EM", "cs.LG", "stat.CO", "G.3; J.4; I.2"], "primary_category": "econ.EM"}
{"title": "Self-Ablating Transformers: More Interpretability, Less Sparsity", "abstract": "A growing intuition in machine learning suggests a link between sparsity and\ninterpretability. We introduce a novel self-ablation mechanism to investigate\nthis connection ante-hoc in the context of language transformers. Our approach\ndynamically enforces a k-winner-takes-all constraint, forcing the model to\ndemonstrate selective activation across neuron and attention units. Unlike\npost-hoc methods that analyze already-trained models, our approach integrates\ninterpretability directly into model training, promoting feature localization\nfrom inception. Training small models on the TinyStories dataset and employing\ninterpretability tests, we find that self-ablation leads to more localized\ncircuits, concentrated feature representations, and increased neuron\nspecialization without compromising language modelling performance.\nSurprisingly, our method also decreased overall sparsity, indicating that\nself-ablation promotes specialization rather than widespread inactivity. This\nreveals a complex interplay between sparsity and interpretability, where\ndecreased global sparsity can coexist with increased local specialization,\nleading to enhanced interpretability. To facilitate reproducibility, we make\nour code available at\nhttps://github.com/keenanpepper/self-ablating-transformers.", "published": "2025-05-01 13:25:37", "link": "http://arxiv.org/abs/2505.00509v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Implicit Neural-Representation Learning for Elastic Deformable-Object Manipulations", "abstract": "We aim to solve the problem of manipulating deformable objects, particularly\nelastic bands, in real-world scenarios. However, deformable object manipulation\n(DOM) requires a policy that works on a large state space due to the unlimited\ndegree of freedom (DoF) of deformable objects. Further, their dense but partial\nobservations (e.g., images or point clouds) may increase the sampling\ncomplexity and uncertainty in policy learning. To figure it out, we propose a\nnovel implicit neural-representation (INR) learning for elastic DOMs, called\nINR-DOM. Our method learns consistent state representations associated with\npartially observable elastic objects reconstructing a complete and implicit\nsurface represented as a signed distance function. Furthermore, we perform\nexploratory representation fine-tuning through reinforcement learning (RL) that\nenables RL algorithms to effectively learn exploitable representations while\nefficiently obtaining a DOM policy. We perform quantitative and qualitative\nanalyses building three simulated environments and real-world manipulation\nstudies with a Franka Emika Panda arm. Videos are available at\nhttp://inr-dom.github.io.", "published": "2025-05-01 13:00:56", "link": "http://arxiv.org/abs/2505.00500v1", "categories": ["cs.RO", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Enhancing Tropical Cyclone Path Forecasting with an Improved Transformer Network", "abstract": "A storm is a type of extreme weather. Therefore, forecasting the path of a\nstorm is extremely important for protecting human life and property. However,\nstorm forecasting is very challenging because storm trajectories frequently\nchange. In this study, we propose an improved deep learning method using a\nTransformer network to predict the movement trajectory of a storm over the next\n6 hours. The storm data used to train the model was obtained from the National\nOceanic and Atmospheric Administration (NOAA) [1]. Simulation results show that\nthe proposed method is more accurate than traditional methods. Moreover, the\nproposed method is faster and more cost-effective", "published": "2025-05-01 12:48:34", "link": "http://arxiv.org/abs/2505.00495v1", "categories": ["cs.LG", "cs.PF"], "primary_category": "cs.LG"}
{"title": "Interpretable Spatial-Temporal Fusion Transformers: Multi-Output Prediction for Parametric Dynamical Systems with Time-Varying Inputs", "abstract": "We explore the promising performance of a transformer model in predicting\noutputs of parametric dynamical systems with external time-varying input\nsignals. The outputs of such systems vary not only with physical parameters but\nalso with external time-varying input signals. Accurately catching the dynamics\nof such systems is challenging. We have adapted and extended an existing\ntransformer model for single output prediction to a multiple-output transformer\nthat is able to predict multiple output responses of these systems. The\nmultiple-output transformer generalizes the interpretability of the original\ntransformer. The generalized interpretable attention weight matrix explores not\nonly the temporal correlations in the sequence, but also the interactions\nbetween the multiple outputs, providing explanation for the spatial correlation\nin the output domain. This multiple-output transformer accurately predicts the\nsequence of multiple outputs, regardless of the nonlinearity of the system and\nthe dimensionality of the parameter space.", "published": "2025-05-01 11:55:42", "link": "http://arxiv.org/abs/2505.00473v1", "categories": ["cs.LG", "cs.NA", "math.NA"], "primary_category": "cs.LG"}
{"title": "A Generalised Framework for Property-Driven Machine Learning", "abstract": "Neural networks have been shown to frequently fail to satisfy critical safety\nand correctness properties after training, highlighting the pressing need for\ntraining methods that incorporate such properties directly. While adversarial\ntraining can be used to improve robustness to small perturbations within\n$\\epsilon$-cubes, domains other than computer vision -- such as control systems\nand natural language processing -- may require more flexible input region\nspecifications via generalised hyper-rectangles. Meanwhile, differentiable\nlogics offer a way to encode arbitrary logical constraints as additional loss\nterms that guide the learning process towards satisfying these constraints. In\nthis paper, we investigate how these two complementary approaches can be\nunified within a single framework for property-driven machine learning. We show\nthat well-known properties from the literature are subcases of this general\napproach, and we demonstrate its practical effectiveness on a case study\ninvolving a neural network controller for a drone system. Our framework is\npublicly available at https://github.com/tflinkow/property-driven-ml.", "published": "2025-05-01 11:33:38", "link": "http://arxiv.org/abs/2505.00466v1", "categories": ["cs.LG", "cs.LO"], "primary_category": "cs.LG"}
{"title": "Subspace-Distance-Enabled Active Learning for Efficient Data-Driven Model Reduction of Parametric Dynamical Systems", "abstract": "In situations where the solution of a high-fidelity dynamical system needs to\nbe evaluated repeatedly, over a vast pool of parametric configurations and in\nabsence of access to the underlying governing equations, data-driven model\nreduction techniques are preferable. We propose a novel active learning\napproach to build a parametric data-driven reduced-order model (ROM) by\ngreedily picking the most important parameter samples from the parameter\ndomain. As a result, during the ROM construction phase, the number of\nhigh-fidelity solutions dynamically grow in a principled fashion. The\nhigh-fidelity solution snapshots are expressed in several parameter-specific\nlinear subspaces, with the help of proper orthogonal decomposition (POD), and\nthe relative distance between these subspaces is used as a guiding mechanism to\nperform active learning. For successfully achieving this, we provide a distance\nmeasure to evaluate the similarity between pairs of linear subspaces with\ndifferent dimensions, and also show that this distance measure is a metric. The\nusability of the proposed subspace-distance-enabled active learning (SDE-AL)\nframework is demonstrated by augmenting two existing non-intrusive\nreduced-order modeling approaches, and providing their active-learning-driven\n(ActLearn) extensions, namely, SDE-ActLearn-POD-KSNN, and SDE-ActLearn-POD-NN.\nFurthermore, we report positive results for two parametric physical models,\nhighlighting the efficiency of the proposed SDE-AL approach.", "published": "2025-05-01 11:28:18", "link": "http://arxiv.org/abs/2505.00460v1", "categories": ["math.NA", "cs.CE", "cs.LG", "cs.NA", "math.DS", "physics.comp-ph"], "primary_category": "math.NA"}
{"title": "Over-the-Air Inference over Multi-hop MIMO Networks", "abstract": "A novel over-the-air machine learning framework over multi-hop multiple-input\nand multiple-output (MIMO) networks is proposed. The core idea is to imitate\nfully connected (FC) neural network layers using multiple MIMO channels by\ncarefully designing the precoding matrices at the transmitting nodes. A neural\nnetwork dubbed PrototypeNet is employed consisting of multiple FC layers, with\nthe number of neurons of each layer equal to the number of antennas of the\ncorresponding terminal. To achieve satisfactory performance, we train\nPrototypeNet based on a customized loss function consisting of classification\nerror and the power of latent vectors to satisfy transmit power constraints,\nwith noise injection during training. Precoding matrices for each hop are then\nobtained by solving an optimization problem. We also propose a multiple-block\nextension when the number of antennas is limited. Numerical results verify that\nthe proposed over-the-air transmission scheme can achieve satisfactory\nclassification accuracy under a power constraint. The results also show that\nhigher classification accuracy can be achieved with an increasing number of\nhops at a modest signal-to-noise ratio (SNR).", "published": "2025-05-01 09:59:32", "link": "http://arxiv.org/abs/2505.00430v1", "categories": ["eess.SP", "cs.LG"], "primary_category": "eess.SP"}
{"title": "CICADA: Cross-Domain Interpretable Coding for Anomaly Detection and Adaptation in Multivariate Time Series", "abstract": "Unsupervised Time series anomaly detection plays a crucial role in\napplications across industries. However, existing methods face significant\nchallenges due to data distributional shifts across different domains, which\nare exacerbated by the non-stationarity of time series over time. Existing\nmodels fail to generalize under multiple heterogeneous source domains and\nemerging unseen new target domains. To fill the research gap, we introduce\nCICADA (Cross-domain Interpretable Coding for Anomaly Detection and\nAdaptation), with four key innovations: (1) a mixture of experts (MOE)\nframework that captures domain-agnostic anomaly features with high flexibility\nand interpretability; (2) a novel selective meta-learning mechanism to prevent\nnegative transfer between dissimilar domains, (3) an adaptive expansion\nalgorithm for emerging heterogeneous domain expansion, and (4) a hierarchical\nattention structure that quantifies expert contributions during fusion to\nenhance interpretability further.Extensive experiments on synthetic and\nreal-world industrial datasets demonstrate that CICADA outperforms\nstate-of-the-art methods in both cross-domain detection performance and\ninterpretability.", "published": "2025-05-01 09:26:40", "link": "http://arxiv.org/abs/2505.00415v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Machine Learning Meets Transparency in Osteoporosis Risk Assessment: A Comparative Study of ML and Explainability Analysis", "abstract": "The present research tackles the difficulty of predicting osteoporosis risk\nvia machine learning (ML) approaches, emphasizing the use of explainable\nartificial intelligence (XAI) to improve model transparency. Osteoporosis is a\nsignificant public health concern, sometimes remaining untreated owing to its\nasymptomatic characteristics, and early identification is essential to avert\nfractures. The research assesses six machine learning classifiers: Random\nForest, Logistic Regression, XGBoost, AdaBoost, LightGBM, and Gradient Boosting\nand utilizes a dataset based on clinical, demographic, and lifestyle variables.\nThe models are refined using GridSearchCV to calibrate hyperparameters, with\nthe objective of enhancing predictive efficacy. XGBoost had the greatest\naccuracy (91%) among the evaluated models, surpassing others in precision\n(0.92), recall (0.91), and F1-score (0.90). The research further integrates XAI\napproaches, such as SHAP, LIME, and Permutation Feature Importance, to\nelucidate the decision-making process of the optimal model. The study indicates\nthat age is the primary determinant in forecasting osteoporosis risk, followed\nby hormonal alterations and familial history. These results corroborate\nclinical knowledge and affirm the models' therapeutic significance. The\nresearch underscores the significance of explainability in machine learning\nmodels for healthcare applications, guaranteeing that physicians can rely on\nthe system's predictions. The report ultimately proposes directions for further\nresearch, such as validation across varied populations and the integration of\nsupplementary biomarkers for enhanced predictive accuracy.", "published": "2025-05-01 09:05:02", "link": "http://arxiv.org/abs/2505.00410v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Safety in the Face of Adversity: Achieving Zero Constraint Violation in Online Learning with Slowly Changing Constraints", "abstract": "We present the first theoretical guarantees for zero constraint violation in\nOnline Convex Optimization (OCO) across all rounds, addressing dynamic\nconstraint changes. Unlike existing approaches in constrained OCO, which allow\nfor occasional safety breaches, we provide the first approach for maintaining\nstrict safety under the assumption of gradually evolving constraints, namely\nthe constraints change at most by a small amount between consecutive rounds.\nThis is achieved through a primal-dual approach and Online Gradient Ascent in\nthe dual space. We show that employing a dichotomous learning rate enables\nensuring both safety, via zero constraint violation, and sublinear regret. Our\nframework marks a departure from previous work by providing the first provable\nguarantees for maintaining absolute safety in the face of changing constraints\nin OCO.", "published": "2025-05-01 08:41:17", "link": "http://arxiv.org/abs/2505.00398v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Approximation to Deep Q-Network by Stochastic Delay Differential Equations", "abstract": "Despite the significant breakthroughs that the Deep Q-Network (DQN) has\nbrought to reinforcement learning, its theoretical analysis remains limited. In\nthis paper, we construct a stochastic differential delay equation (SDDE) based\non the DQN algorithm and estimate the Wasserstein-1 distance between them. We\nprovide an upper bound for the distance and prove that the distance between the\ntwo converges to zero as the step size approaches zero. This result allows us\nto understand DQN's two key techniques, the experience replay and the target\nnetwork, from the perspective of continuous systems. Specifically, the delay\nterm in the equation, corresponding to the target network, contributes to the\nstability of the system. Our approach leverages a refined Lindeberg principle\nand an operator comparison to establish these results.", "published": "2025-05-01 08:19:24", "link": "http://arxiv.org/abs/2505.00382v1", "categories": ["cs.LG", "math.PR"], "primary_category": "cs.LG"}
{"title": "From GNNs to Trees: Multi-Granular Interpretability for Graph Neural Networks", "abstract": "Interpretable Graph Neural Networks (GNNs) aim to reveal the underlying\nreasoning behind model predictions, attributing their decisions to specific\nsubgraphs that are informative. However, existing subgraph-based interpretable\nmethods suffer from an overemphasis on local structure, potentially overlooking\nlong-range dependencies within the entire graphs. Although recent efforts that\nrely on graph coarsening have proven beneficial for global interpretability,\nthey inevitably reduce the graphs to a fixed granularity. Such an inflexible\nway can only capture graph connectivity at a specific level, whereas real-world\ngraph tasks often exhibit relationships at varying granularities (e.g.,\nrelevant interactions in proteins span from functional groups, to amino acids,\nand up to protein domains). In this paper, we introduce a novel Tree-like\nInterpretable Framework (TIF) for graph classification, where plain GNNs are\ntransformed into hierarchical trees, with each level featuring coarsened graphs\nof different granularity as tree nodes. Specifically, TIF iteratively adopts a\ngraph coarsening module to compress original graphs (i.e., root nodes of trees)\ninto increasingly coarser ones (i.e., child nodes of trees), while preserving\ndiversity among tree nodes within different branches through a dedicated graph\nperturbation module. Finally, we propose an adaptive routing module to identify\nthe most informative root-to-leaf paths, providing not only the final\nprediction but also the multi-granular interpretability for the decision-making\nprocess. Extensive experiments on the graph classification benchmarks with both\nsynthetic and real-world datasets demonstrate the superiority of TIF in\ninterpretability, while also delivering a competitive prediction performance\nakin to the state-of-the-art counterparts.", "published": "2025-05-01 07:22:51", "link": "http://arxiv.org/abs/2505.00364v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Validation of a 24-hour-ahead Prediction model for a Residential Electrical Load under diverse climate", "abstract": "Accurate household electrical energy demand prediction is essential for\neffectively managing sustainable Energy Communities. Integrated with the Energy\nManagement System, these communities aim to optimise operational costs.\nHowever, most existing forecasting models are region-specific and depend on\nlarge datasets, limiting their applicability across different climates and\ngeographical areas. These models often lack flexibility and may not perform\nwell in regions with limited historical data, leading to inaccurate\npredictions. This paper proposes a global model for 24-hour-ahead hourly\nelectrical energy demand prediction that is designed to perform effectively\nacross diverse climate conditions and datasets. The model's efficiency is\ndemonstrated using data from two distinct regions: Ireland, with a maritime\nclimate and Vietnam, with a tropical climate. Remarkably, the model achieves\nhigh accuracy even with a limited dataset spanning only nine months. Its\nrobustness is further validated across different seasons in Ireland (summer and\nwinter) and Vietnam (dry and wet). The proposed model is evaluated against\nstate-of-the-art machine learning and deep learning methods. Simulation results\nindicate that the model consistently outperforms benchmark models, showcasing\nits capability to provide reliable forecasts globally, regardless of varying\nclimatic conditions and data availability. This research underscores the\nmodel's potential to enhance the efficiency and sustainability of Energy\nCommunities worldwide. The proposed model achieves a Mean Absolute Percentage\nError of 8.0% and 4.0% on the full Irish and Vietnamese datasets.", "published": "2025-05-01 06:48:26", "link": "http://arxiv.org/abs/2505.00348v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Communication-Efficient Wireless Federated Fine-Tuning for Large-Scale AI Models", "abstract": "Transformer-based large language models (LLMs) have achieved remarkable\nsuccess across various tasks. Yet, fine-tuning such massive models in federated\nlearning (FL) settings poses significant challenges due to resource constraints\nand communication overhead. Low-Rank Adaptation (LoRA) addresses these issues\nby training compact, low-rank matrices instead of fully fine-tuning large\nmodels. This paper introduces a wireless federated LoRA fine-tuning framework\nthat optimizes both learning performance and communication efficiency. We\nprovide a novel convergence analysis, revealing how LoRA rank and covariance\neffects influence FL training dynamics. Leveraging these insights, we propose\nSparsified Orthogonal Fine-Tuning (\\textbf{SOFT}), an adaptive sparsification\nmethod that streamlines parameter updates without expensive matrix\nmultiplications and singular value decomposition (SVD) operations.\nAdditionally, we present a Two Stage Federated Algorithm (\\textbf{TSFA})\nalgorithm that pre-determines key parameters offline and dynamically adjusts\nbandwidth and sparsification online, ensuring efficient training under latency\nconstraints. Experiments on benchmark datasets show that our approach achieves\naccuracy comparable to ideal scenario models while significantly reducing\ncommunication overhead. Our framework thus enables scalable, resource-efficient\ndeployment of large models in real-world wireless FL scenarios.", "published": "2025-05-01 06:15:38", "link": "http://arxiv.org/abs/2505.00333v1", "categories": ["cs.LG", "eess.SP"], "primary_category": "cs.LG"}
{"title": "Optimal Vector Compressed Sensing Using James Stein Shrinkage", "abstract": "The trend in modern science and technology is to take vector measurements\nrather than scalars, ruthlessly scaling to ever higher dimensional vectors. For\nabout two decades now, traditional scalar Compressed Sensing has been\nsynonymous with a Convex Optimization based procedure called Basis Pursuit. In\nthe vector recovery case, the natural tendency is to return to a\nstraightforward vector extension of Basis Pursuit, also based on Convex\nOptimization. However, Convex Optimization is provably suboptimal, particularly\nwhen $B$ is large. In this paper, we propose SteinSense, a lightweight\niterative algorithm, which is provably optimal when $B$ is large. It does not\nhave any tuning parameter, does not need any training data, requires zero\nknowledge of sparsity, is embarrassingly simple to implement, and all of this\nmakes it easily scalable to high vector dimensions. We conduct a massive volume\nof both real and synthetic experiments that confirm the efficacy of SteinSense,\nand also provide theoretical justification based on ideas from Approximate\nMessage Passing. Fascinatingly, we discover that SteinSense is quite robust,\ndelivering the same quality of performance on real data, and even under\nsubstantial departures from conditions under which existing theory holds.", "published": "2025-05-01 05:55:01", "link": "http://arxiv.org/abs/2505.00326v1", "categories": ["cs.LG", "eess.IV", "eess.SP", "stat.CO", "stat.ME"], "primary_category": "cs.LG"}
{"title": "Edge Large AI Models: Revolutionizing 6G Networks", "abstract": "Large artificial intelligence models (LAMs) possess human-like abilities to\nsolve a wide range of real-world problems, exemplifying the potential of\nexperts in various domains and modalities. By leveraging the communication and\ncomputation capabilities of geographically dispersed edge devices, edge LAM\nemerges as an enabling technology to empower the delivery of various real-time\nintelligent services in 6G. Unlike traditional edge artificial intelligence\n(AI) that primarily supports a single task using small models, edge LAM is\nfeatured by the need of the decomposition and distributed deployment of large\nmodels, and the ability to support highly generalized and diverse tasks.\nHowever, due to limited communication, computation, and storage resources over\nwireless networks, the vast number of trainable neurons and the substantial\ncommunication overhead pose a formidable hurdle to the practical deployment of\nedge LAMs. In this paper, we investigate the opportunities and challenges of\nedge LAMs from the perspectives of model decomposition and resource management.\nSpecifically, we propose collaborative fine-tuning and full-parameter training\nframeworks, alongside a microservice-assisted inference architecture, to\nenhance the deployment of edge LAM over wireless networks. Additionally, we\ninvestigate the application of edge LAM in air-interface designs, focusing on\nchannel prediction and beamforming. These innovative frameworks and\napplications offer valuable insights and solutions for advancing 6G technology.", "published": "2025-05-01 05:44:00", "link": "http://arxiv.org/abs/2505.00321v1", "categories": ["cs.NI", "cs.LG", "eess.SP"], "primary_category": "cs.NI"}
{"title": "FedEMA: Federated Exponential Moving Averaging with Negative Entropy Regularizer in Autonomous Driving", "abstract": "Street Scene Semantic Understanding (denoted as S3U) is a crucial but complex\ntask for autonomous driving (AD) vehicles. Their inference models typically\nface poor generalization due to domain-shift. Federated Learning (FL) has\nemerged as a promising paradigm for enhancing the generalization of AD models\nthrough privacy-preserving distributed learning. However, these FL AD models\nface significant temporal catastrophic forgetting when deployed in dynamically\nevolving environments, where continuous adaptation causes abrupt erosion of\nhistorical knowledge. This paper proposes Federated Exponential Moving Average\n(FedEMA), a novel framework that addresses this challenge through two integral\ninnovations: (I) Server-side model's historical fitting capability preservation\nvia fusing current FL round's aggregation model and a proposed previous FL\nround's exponential moving average (EMA) model; (II) Vehicle-side negative\nentropy regularization to prevent FL models' possible overfitting to\nEMA-introduced temporal patterns. Above two strategies empower FedEMA a\ndual-objective optimization that balances model generalization and\nadaptability. In addition, we conduct theoretical convergence analysis for the\nproposed FedEMA. Extensive experiments both on Cityscapes dataset and Camvid\ndataset demonstrate FedEMA's superiority over existing approaches, showing\n7.12% higher mean Intersection-over-Union (mIoU).", "published": "2025-05-01 05:37:43", "link": "http://arxiv.org/abs/2505.00318v1", "categories": ["cs.RO", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Statistical Learning for Heterogeneous Treatment Effects: Pretraining, Prognosis, and Prediction", "abstract": "Robust estimation of heterogeneous treatment effects is a fundamental\nchallenge for optimal decision-making in domains ranging from personalized\nmedicine to educational policy. In recent years, predictive machine learning\nhas emerged as a valuable toolbox for causal estimation, enabling more flexible\neffect estimation. However, accurately estimating conditional average treatment\neffects (CATE) remains a major challenge, particularly in the presence of many\ncovariates. In this article, we propose pretraining strategies that leverages a\nphenomenon in real-world applications: factors that are prognostic of the\noutcome are frequently also predictive of treatment effect heterogeneity. In\nmedicine, for example, components of the same biological signaling pathways\nfrequently influence both baseline risk and treatment response. Specifically,\nwe demonstrate our approach within the R-learner framework, which estimates the\nCATE by solving individual prediction problems based on a residualized loss. We\nuse this structure to incorporate \"side information\" and develop models that\ncan exploit synergies between risk prediction and causal effect estimation. In\nsettings where these synergies are present, this cross-task learning enables\nmore accurate signal detection: yields lower estimation error, reduced false\ndiscovery rates, and higher power for detecting heterogeneity.", "published": "2025-05-01 05:12:14", "link": "http://arxiv.org/abs/2505.00310v1", "categories": ["stat.ML", "cs.LG", "stat.ME"], "primary_category": "stat.ML"}
{"title": "Gateformer: Advancing Multivariate Time Series Forecasting through Temporal and Variate-Wise Attention with Gated Representations", "abstract": "There has been a recent surge of interest in time series modeling using the\nTransformer architecture. However, forecasting multivariate time series with\nTransformer presents a unique challenge as it requires modeling both temporal\n(cross-time) and variate (cross-variate) dependencies. While Transformer-based\nmodels have gained popularity for their flexibility in capturing both\nsequential and cross-variate relationships, it is unclear how to best integrate\nthese two sources of information in the context of the Transformer architecture\nwhile optimizing for both performance and efficiency. We re-purpose the\nTransformer architecture to effectively model both cross-time and cross-variate\ndependencies. Our approach begins by embedding each variate independently into\na variate-wise representation that captures its cross-time dynamics, and then\nmodels cross-variate dependencies through attention mechanisms on these learned\nembeddings. Gating operations in both cross-time and cross-variate modeling\nphases regulate information flow, allowing the model to focus on the most\nrelevant features for accurate predictions. Our method achieves\nstate-of-the-art performance across 13 real-world datasets and can be\nseamlessly integrated into other Transformer-based and LLM-based forecasters,\ndelivering performance improvements up to 20.7\\% over original models. Code is\navailable at this repository: https://github.com/nyuolab/Gateformer.", "published": "2025-05-01 04:59:05", "link": "http://arxiv.org/abs/2505.00307v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Reinforcement Learning with Continuous Actions Under Unmeasured Confounding", "abstract": "This paper addresses the challenge of offline policy learning in\nreinforcement learning with continuous action spaces when unmeasured\nconfounders are present. While most existing research focuses on policy\nevaluation within partially observable Markov decision processes (POMDPs) and\nassumes discrete action spaces, we advance this field by establishing a novel\nidentification result to enable the nonparametric estimation of policy value\nfor a given target policy under an infinite-horizon framework. Leveraging this\nidentification, we develop a minimax estimator and introduce a\npolicy-gradient-based algorithm to identify the in-class optimal policy that\nmaximizes the estimated policy value. Furthermore, we provide theoretical\nresults regarding the consistency, finite-sample error bound, and regret bound\nof the resulting optimal policy. Extensive simulations and a real-world\napplication using the German Family Panel data demonstrate the effectiveness of\nour proposed methodology.", "published": "2025-05-01 04:55:29", "link": "http://arxiv.org/abs/2505.00304v1", "categories": ["stat.ML", "cs.LG", "stat.ME"], "primary_category": "stat.ML"}
{"title": "Temporal Attention Evolutional Graph Convolutional Network for Multivariate Time Series Forecasting", "abstract": "Multivariate time series forecasting enables the prediction of future states\nby leveraging historical data, thereby facilitating decision-making processes.\nEach data node in a multivariate time series encompasses a sequence of multiple\ndimensions. These nodes exhibit interdependent relationships, forming a graph\nstructure. While existing prediction methods often assume a fixed graph\nstructure, many real-world scenarios involve dynamic graph structures.\nMoreover, interactions among time series observed at different time scales vary\nsignificantly. To enhance prediction accuracy by capturing precise temporal and\nspatial features, this paper introduces the Temporal Attention Evolutional\nGraph Convolutional Network (TAEGCN). This novel method not only integrates\ncausal temporal convolution and a multi-head self-attention mechanism to learn\ntemporal features of nodes, but also construct the dynamic graph structure\nbased on these temporal features to keep the consistency of the changing in\nspatial feature with temporal series. TAEGCN adeptly captures temporal causal\nrelationships and hidden spatial dependencies within the data. Furthermore,\nTAEGCN incorporates a unified neural network that seamlessly integrates these\ncomponents to generate final predictions. Experimental results conducted on two\npublic transportation network datasets, METR-LA and PEMS-BAY, demonstrate the\nsuperior performance of the proposed model.", "published": "2025-05-01 04:50:00", "link": "http://arxiv.org/abs/2505.00302v1", "categories": ["cs.LG", "68T09 (Primary), 68T07 (Secondary)"], "primary_category": "cs.LG"}
{"title": "Intelligent Task Scheduling for Microservices via A3C-Based Reinforcement Learning", "abstract": "To address the challenges of high resource dynamism and intensive task\nconcurrency in microservice systems, this paper proposes an adaptive resource\nscheduling method based on the A3C reinforcement learning algorithm. The\nscheduling problem is modeled as a Markov Decision Process, where policy and\nvalue networks are jointly optimized to enable fine-grained resource allocation\nunder varying load conditions. The method incorporates an asynchronous\nmulti-threaded learning mechanism, allowing multiple agents to perform parallel\nsampling and synchronize updates to the global network parameters. This design\nimproves both policy convergence efficiency and model stability. In the\nexperimental section, a real-world dataset is used to construct a scheduling\nscenario. The proposed method is compared with several typical approaches\nacross multiple evaluation metrics, including task delay, scheduling success\nrate, resource utilization, and convergence speed. The results show that the\nproposed method delivers high scheduling performance and system stability in\nmulti-task concurrent environments. It effectively alleviates the resource\nallocation bottlenecks faced by traditional methods under heavy load,\ndemonstrating its practical value for intelligent scheduling in microservice\nsystems.", "published": "2025-05-01 04:42:48", "link": "http://arxiv.org/abs/2505.00299v1", "categories": ["cs.DC", "cs.LG"], "primary_category": "cs.DC"}
{"title": "Repetition Makes Perfect: Recurrent Sum-GNNs Match Message Passing Limit", "abstract": "We provide first tight bounds for the expressivity of Recurrent Graph Neural\nNetworks (recurrent GNNs) with finite-precision parameters. We prove that\nrecurrent GNNs, with sum aggregation and ReLU activation, can emulate any graph\nalgorithm that respects the natural message-passing invariance induced by the\ncolor refinement (or Weisfeiler-Leman) algorithm. While it is well known that\nthe expressive power of GNNs is limited by this invariance [Morris et al., AAAI\n2019; Xu et al., ICLR 2019], we establish that recurrent GNNs can actually\nreach this limit. This is in contrast to non-recurrent GNNs, which have the\npower of Weisfeiler-Leman only in a very weak, \"non-uniform\", sense where every\ngraph size requires a different GNN model to compute with. The emulation we\nconstruct introduces only a polynomial overhead in both time and space.\n  Furthermore, we show that by incorporating random initialization, recurrent\nGNNs can emulate all graph algorithms, implying in particular that any graph\nalgorithm with polynomial-time complexity can be emulated by a recurrent GNN\nwith random initialization, running in polynomial time.", "published": "2025-05-01 04:27:35", "link": "http://arxiv.org/abs/2505.00291v1", "categories": ["cs.LG", "68T05, 68T07", "I.2.6"], "primary_category": "cs.LG"}
{"title": "A Unifying Framework for Robust and Efficient Inference with Unstructured Data", "abstract": "This paper presents a general framework for conducting efficient and robust\ninference on parameters derived from unstructured data, which include text,\nimages, audio, and video. Economists have long incorporated data extracted from\ntexts and images into their analyses, a practice that has accelerated with\nadvancements in deep neural networks. However, neural networks do not\ngenerically produce unbiased predictions, potentially propagating bias to\nestimators that use their outputs. To address this challenge, we reframe\ninference with unstructured data as a missing structured data problem, where\nstructured data are imputed from unstructured inputs using deep neural\nnetworks. This perspective allows us to apply classic results from\nsemiparametric inference, yielding valid, efficient, and robust estimators\nbased on unstructured data. We formalize this approach with MARS (Missing At\nRandom Structured Data), a unifying framework that integrates and extends\nexisting methods for debiased inference using machine learning predictions,\nlinking them to a variety of older, familiar problems such as causal inference.\nWe develop robust and efficient estimators for both descriptive and causal\nestimands and address challenges such as inference using aggregated and\ntransformed predictions from unstructured data. Importantly, MARS applies to\ncommon empirical settings that have received limited attention in the existing\nliterature. Finally, we reanalyze prominent studies that use unstructured data,\ndemonstrating the practical value of MARS.", "published": "2025-05-01 04:11:25", "link": "http://arxiv.org/abs/2505.00282v1", "categories": ["econ.EM", "cs.LG"], "primary_category": "econ.EM"}
{"title": "Policies of Multiple Skill Levels for Better Strength Estimation in Games", "abstract": "Accurately estimating human skill levels is crucial for designing effective\nhuman-AI interactions so that AI can provide appropriate challenges or\nguidance. In games where AI players have beaten top human professionals,\nstrength estimation plays a key role in adapting AI behavior to match human\nskill levels. In a previous state-of-the-art study, researchers have proposed a\nstrength estimator trained using human players' match data. Given some matches,\nthe strength estimator computes strength scores and uses them to estimate\nplayer ranks (skill levels). In this paper, we focus on the observation that\nhuman players' behavior tendency varies according to their strength and aim to\nimprove the accuracy of strength estimation by taking this into account.\nSpecifically, in addition to strength scores, we obtain policies for different\nskill levels from neural networks trained using human players' match data. We\nthen combine features based on these policies with the strength scores to\nestimate strength. We conducted experiments on Go and chess. For Go, our method\nachieved an accuracy of 80% in strength estimation when given 10 matches, which\nincreased to 92% when given 20 matches. In comparison, the previous\nstate-of-the-art method had an accuracy of 71% with 10 matches and 84% with 20\nmatches, demonstrating improvements of 8-9%. We observed similar improvements\nin chess. These results contribute to developing a more accurate strength\nestimation method and to improving human-AI interaction.", "published": "2025-05-01 04:02:20", "link": "http://arxiv.org/abs/2505.00279v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Field-scale soil moisture estimated from Sentinel-1 SAR data using a knowledge-guided deep learning approach", "abstract": "Soil moisture (SM) estimation from active microwave data remains challenging\ndue to the complex interactions between radar backscatter and surface\ncharacteristics. While the water cloud model (WCM) provides a semi-physical\napproach for understanding these interactions, its empirical component often\nlimits performance across diverse agricultural landscapes. This research\npresents preliminary efforts for developing a knowledge-guided deep learning\napproach, which integrates WCM principles into a long short-term memory (LSTM)\nmodel, to estimate field SM using Sentinel-1 Synthetic Aperture Radar (SAR)\ndata. Our proposed approach leverages LSTM's capacity to capture spatiotemporal\ndependencies while maintaining physical consistency through a modified\ndual-component loss function, including a WCM-based semi-physical component and\na boundary condition regularisation. The proposed approach is built upon the\nsoil backscatter coefficients isolated from the total backscatter, together\nwith Landsat-resolution vegetation information and surface characteristics. A\nfour-fold spatial cross-validation was performed against in-situ SM data to\nassess the model performance. Results showed the proposed approach reduced SM\nretrieval uncertainties by 0.02 m$^3$/m$^3$ and achieved correlation\ncoefficients (R) of up to 0.64 in areas with varying vegetation cover and\nsurface conditions, demonstrating the potential to address the\nover-simplification in WCM.", "published": "2025-05-01 03:12:25", "link": "http://arxiv.org/abs/2505.00265v1", "categories": ["cs.LG", "eess.IV"], "primary_category": "cs.LG"}
{"title": "Graph Privacy: A Heterogeneous Federated GNN for Trans-Border Financial Data Circulation", "abstract": "The sharing of external data has become a strong demand of financial\ninstitutions, but the privacy issue has led to the difficulty of\ninterconnecting different platforms and the low degree of data openness. To\neffectively solve the privacy problem of financial data in trans-border flow\nand sharing, to ensure that the data is available but not visible, to realize\nthe joint portrait of all kinds of heterogeneous data of business organizations\nin different industries, we propose a Heterogeneous Federated Graph Neural\nNetwork (HFGNN) approach. In this method, the distribution of heterogeneous\nbusiness data of trans-border organizations is taken as subgraphs, and the\nsharing and circulation process among subgraphs is constructed as a\nstatistically heterogeneous global graph through a central server. Each\nsubgraph learns the corresponding personalized service model through local\ntraining to select and update the relevant subset of subgraphs with aggregated\nparameters, and effectively separates and combines topological and feature\ninformation among subgraphs. Finally, our simulation experimental results show\nthat the proposed method has higher accuracy performance and faster convergence\nspeed than existing methods.", "published": "2025-05-01 02:47:43", "link": "http://arxiv.org/abs/2505.00257v1", "categories": ["cs.LG", "cs.CR"], "primary_category": "cs.LG"}
{"title": "D-Tracker: Modeling Interest Diffusion in Social Activity Tensor Data Streams", "abstract": "Large quantities of social activity data, such as weekly web search volumes\nand the number of new infections with infectious diseases, reflect peoples'\ninterests and activities. It is important to discover temporal patterns from\nsuch data and to forecast future activities accurately. However, modeling and\nforecasting social activity data streams is difficult because they are\nhigh-dimensional and composed of multiple time-varying dynamics such as trends,\nseasonality, and interest diffusion. In this paper, we propose D-Tracker, a\nmethod for continuously capturing time-varying temporal patterns within social\nactivity tensor data streams and forecasting future activities. Our proposed\nmethod has the following properties: (a) Interpretable: it incorporates the\npartial differential equation into a tensor decomposition framework and\ncaptures time-varying temporal patterns such as trends, seasonality, and\ninterest diffusion between locations in an interpretable manner; (b) Automatic:\nit has no hyperparameters and continuously models tensor data streams fully\nautomatically; (c) Scalable: the computation time of D-Tracker is independent\nof the time series length. Experiments using web search volume data obtained\nfrom GoogleTrends, and COVID-19 infection data obtained from COVID-19 Open Data\nRepository show that our method can achieve higher forecasting accuracy in less\ncomputation time than existing methods while extracting the interest diffusion\nbetween locations. Our source code and datasets are available at\n{https://github.com/Higashiguchi-Shingo/D-Tracker.", "published": "2025-05-01 01:29:05", "link": "http://arxiv.org/abs/2505.00242v1", "categories": ["cs.SI", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Future-Oriented Navigation: Dynamic Obstacle Avoidance with One-Shot Energy-Based Multimodal Motion Prediction", "abstract": "This paper proposes an integrated approach for the safe and efficient control\nof mobile robots in dynamic and uncertain environments. The approach consists\nof two key steps: one-shot multimodal motion prediction to anticipate motions\nof dynamic obstacles and model predictive control to incorporate these\npredictions into the motion planning process. Motion prediction is driven by an\nenergy-based neural network that generates high-resolution, multi-step\npredictions in a single operation. The prediction outcomes are further utilized\nto create geometric shapes formulated as mathematical constraints. Instead of\ntreating each dynamic obstacle individually, predicted obstacles are grouped by\nproximity in an unsupervised way to improve performance and efficiency. The\noverall collision-free navigation is handled by model predictive control with a\nspecific design for proactive dynamic obstacle avoidance. The proposed approach\nallows mobile robots to navigate effectively in dynamic environments. Its\nperformance is accessed across various scenarios that represent typical\nwarehouse settings. The results demonstrate that the proposed approach\noutperforms other existing dynamic obstacle avoidance methods.", "published": "2025-05-01 01:13:56", "link": "http://arxiv.org/abs/2505.00237v1", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "primary_category": "cs.RO"}
{"title": "Node2Vec-DGI-EL: A Hierarchical Graph Representation Learning Model for Ingredient-Disease Association Prediction", "abstract": "Traditional Chinese medicine, as an essential component of traditional\nmedicine, contains active ingredients that serve as a crucial source for modern\ndrug development, holding immense therapeutic potential and development value.\nA multi-layered and complex network is formed from Chinese medicine to diseases\nand used to predict the potential associations between Chinese medicine\ningredients and diseases. This study proposes an ingredient-disease association\nprediction model (Node2Vec-DGI-EL) based on hierarchical graph representation\nlearning. First, the model uses the Node2Vec algorithm to extract node\nembedding vectors from the network as the initial features of the nodes. Next,\nthe network nodes are deeply represented and learned using the DGI algorithm to\nenhance the model's expressive power. To improve prediction accuracy and\nrobustness, an ensemble learning method is incorporated to achieve more\naccurate ingredient-disease association predictions. The effectiveness of the\nmodel is then evaluated through a series of theoretical verifications. The\nresults demonstrated that the proposed model significantly outperformed\nexisting methods, achieving an AUC of 0.9987 and an AUPR of 0.9545, thereby\nindicating superior predictive capability. Ablation experiments further\nrevealed the contribution and importance of each module. Additionally, case\nstudies explored potential associations, such as triptonide with hypertensive\nretinopathy and methyl ursolate with colorectal cancer. Molecular docking\nexperiments validated these findings, showing the triptonide-PGR interaction\nand the methyl ursolate-NFE2L2 interaction can bind stable. In conclusion, the\nNode2Vec-DGI-EL model focuses on TCM datasets and effectively predicts\ningredient-disease associations, overcoming the reliance on node semantic\ninformation.", "published": "2025-05-01 01:06:05", "link": "http://arxiv.org/abs/2505.00236v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Explorative Curriculum Learning for Strongly Correlated Electron Systems", "abstract": "Recent advances in neural network quantum states (NQS) have enabled\nhigh-accuracy predictions for complex quantum many-body systems such as\nstrongly correlated electron systems. However, the computational cost remains\nprohibitive, making exploration of the diverse parameters of interaction\nstrengths and other physical parameters inefficient. While transfer learning\nhas been proposed to mitigate this challenge, achieving generalization to\nlarge-scale systems and diverse parameter regimes remains difficult. To address\nthis limitation, we propose a novel curriculum learning framework based on\ntransfer learning for NQS. This facilitates efficient and stable exploration\nacross a vast parameter space of quantum many-body systems. In addition, by\ninterpreting NQS transfer learning through a perturbative lens, we demonstrate\nhow prior physical knowledge can be flexibly incorporated into the curriculum\nlearning process. We also propose Pairing-Net, an architecture to practically\nimplement this strategy for strongly correlated electron systems, and\nempirically verify its effectiveness. Our results show an approximately\n200-fold speedup in computation and a marked improvement in optimization\nstability compared to conventional methods.", "published": "2025-05-01 00:46:52", "link": "http://arxiv.org/abs/2505.00233v1", "categories": ["cond-mat.str-el", "cs.LG"], "primary_category": "cond-mat.str-el"}
{"title": "Inference for max-linear Bayesian networks with noise", "abstract": "Max-Linear Bayesian Networks (MLBNs) provide a powerful framework for causal\ninference in extreme-value settings; we consider MLBNs with noise parameters\nwith a given topology in terms of the max-plus algebra by taking its logarithm.\nThen, we show that an estimator of a parameter for each edge in a directed\nacyclic graph (DAG) is distributed normally. We end this paper with\ncomputational experiments with the expectation and maximization (EM) algorithm\nand quadratic optimization.", "published": "2025-05-01 00:31:37", "link": "http://arxiv.org/abs/2505.00229v1", "categories": ["stat.ML", "cs.LG", "math.OC", "math.ST", "stat.TH", "14T90, 62A09, 62H30, 90C20, 90C90"], "primary_category": "stat.ML"}
{"title": "The local coupling of noise technique and its application to lower error bounds for strong approximation of SDEs with irregular coefficients", "abstract": "In recent years, interest in approximation methods for stochastic\ndifferential equations (SDEs) with non-Lipschitz continuous coefficients has\nincreased. We show lower bounds for the $L^p$-error of such methods in the case\nof approximation at a single point in time or globally in time. On the one\nhand, we show that for a large class of piecewise Lipschitz continuous drifts\nand non-additive diffusions the best possible $L^p$-error rate for final time\napproximation that can be achieved by any method based on finitely many\nevaluations of the driving Brownian motion is at most $3/4$, which was\npreviously known only for additive diffusions. Moreover, we show that the best\n$L^p$-error rate for global approximation that can be achieved by any method\nbased on finitely many evaluations of the driving Brownian motion is at most\n$1/2$ when the drift is locally bounded and the diffusion is locally Lipschitz\ncontinuous.\n  For the derivation of the lower bounds we introduce a new method of proof:\nthe local coupling of noise technique. Using this technique when approximating\na solution $X$ of the SDE at the final time, a lower bound for the $L^p$-error\nof any approximation method based on evaluations of the driving Brownian motion\nat the points $t_1 < \\dots < t_n$ can be determined by the $L^p$-distances of\nsolutions of the same SDE on $[t_{i-1}, t_i]$ with initial values $X_{t_{i-1}}$\nand driving Brownian motions that are coupled at $t_{i-1}, t_i$ and\nindependent, conditioned on the values of the Brownian motion at $t_{i-1},\nt_i$.", "published": "2025-05-01 16:59:01", "link": "http://arxiv.org/abs/2505.00656v1", "categories": ["math.PR", "cs.NA", "math.NA", "65C30, 65C20 (Primary), 60H10 (Secondary)"], "primary_category": "math.PR"}
{"title": "Adaptive Nonoverlapping Preconditioners for the Helmholtz Equation", "abstract": "The Helmholtz equation poses significant computational challenges due to its\noscillatory solutions, particularly for large wavenumbers. Inspired by the\nSchur complement system for elliptic problems, this paper presents a novel\nsubstructuring approach to mitigate the potential ill-posedness of local\nDirichlet problems for the Helmholtz equation. We propose two types of\npreconditioners within the framework of nonoverlapping spectral additive\nSchwarz (NOSAS) methods. The first type of preconditioner focuses on the real\npart of the Helmholtz problem, while the second type addresses both the real\nand imaginary components, providing a comprehensive strategy to enhance\nscalability and reduce computational cost. Our approach is purely algebraic,\nwhich allows for adaptability to various discretizations and heterogeneous\nHelmholtz coefficients while maintaining theoretical convergence for thresholds\nclose to zero. Numerical experiments confirm the effectiveness of the proposed\npreconditioners, demonstrating robust convergence rates and scalability, even\nfor large wavenumbers.", "published": "2025-05-01 16:45:14", "link": "http://arxiv.org/abs/2505.00648v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Model order reduction of hemodynamics by space-time reduced basis and reduced fluid-structure interaction", "abstract": "In this work, we apply the space-time Galerkin reduced basis (ST-GRB) method\nto a reduced fluid-structure interaction model, for the numerical simulation of\nhemodynamics in arteries. In essence, ST-GRB extends the classical reduced\nbasis (RB) method, exploiting a data-driven low-dimensional linear encoding of\nthe temporal dynamics to further cut the computational costs. The current\ninvestigation brings forth two key enhancements, compared to previous works on\nthe topic. On the one side, we model blood flow through the Navier-Stokes\nequations, hence accounting for convection. In this regard, we implement a\nhyper-reduction scheme, based on approximate space-time reduced affine\ndecompositions, to deal with nonlinearities effectively. On the other side, we\nmove beyond the constraint of modelling blood vessels as rigid structures,\nacknowledging the importance of elasticity for the accurate simulation of\ncomplex blood flow patterns. To limit computational complexity, we adopt the\nCoupled Momentum model, incorporating the effect of wall compliance in the\nfluid's equations through a generalized Robin boundary condition. In\nparticular, we propose an efficient strategy for handling the spatio-temporal\nprojection of the structural displacement, which ultimately configures as a\nby-product. The performances of ST-GRB are assessed in three different\nnumerical experiments. The results confirm that the proposed approach can\noutperform the classical RB method, yielding precise approximations of\nhigh-fidelity solutions at more convenient costs. However, the computational\ngains of ST-GRB vanish if the number of retained temporal modes is too large,\nwhich occurs either when complex dynamics arise or if very precise solutions\nare sought.", "published": "2025-05-01 14:24:41", "link": "http://arxiv.org/abs/2505.00548v1", "categories": ["math.NA", "cs.NA", "65M22 (Primary) 35Q30, 76D05 (Secondary)", "G.1.8; I.6.5"], "primary_category": "math.NA"}
{"title": "Weak Random Feature Method for Solving Partial Differential Equations", "abstract": "The random feature method (RFM) has demonstrated great potential in bridging\ntraditional numerical methods and machine learning techniques for solving\npartial differential equations (PDEs). It retains the advantages of mesh-free\napproaches while achieving spectral accuracy for smooth solutions, without the\nneed for iterative procedures. However, the implementation of RFM in the\nidentification of weak solutions remains a subject of limited comprehension,\ndespite crucial role of weak solutions in addressing numerous applied problems.\nWhile the direct application of RFM to problems without strong solutions is\nfraught with potential challenges, we propose an enhancement to the original\nrandom feature method that is specifically suited for finding weak solutions\nand is termed as Weak RFM. Essentially, Weak RFM reformulates the original RFM\nby adopting the weak form of the governing equations and constructing a new\nlinear system through the use of carefully designed test functions, ensuring\nthat the resulting solution satisfies the weak form by default. To rigorously\nevaluate the performance of the proposed method, we conduct extensive\nexperiments on a variety of benchmark problems, including challenging\nthree-dimensional cases, and compare its performance with state of the art\nmachine learning-based approaches. The results demonstrate that Weak RFM\nachieves comparable or superior accuracy while significantly reducing\ncomputational time and memory consumption, highlighting its potential as a\nhighly efficient and robust tool for finding weak solutions to various PDE\nproblems.", "published": "2025-05-01 13:25:17", "link": "http://arxiv.org/abs/2505.00508v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Analysis of evolution equation with variable-exponent memory modeling multiscale viscoelasticity", "abstract": "We investigate the well-posedness and solution regularity of an evolution\nequation with non-positive type variable-exponent memory, which describes\nmultiscale viscoelasticity in materials with memory. The perturbation method is\napplied for model transformation, based on which the well-posedness is proved.\nThen the weighted solution regularity is derived, where the initial singularity\nis characterized by the initial value of variable exponent.", "published": "2025-05-01 10:42:01", "link": "http://arxiv.org/abs/2505.00446v1", "categories": ["math.AP", "cs.NA", "math.NA", "35R09"], "primary_category": "math.AP"}
{"title": "Error bounds for function approximation using generated sets", "abstract": "This paper explores the use of \"generated sets\" $\\{ \\{ k \\boldsymbol{\\zeta}\n\\} : k = 1, \\ldots, n \\}$ for function approximation in reproducing kernel\nHilbert spaces which consist of multi-dimensional functions with an absolutely\nconvergent Fourier series. The algorithm is a least squares algorithm that\nsamples the function at the points of a generated set. We show that there exist\n$\\boldsymbol{\\zeta} \\in [0,1]^d$ for which the worst-case $L_2$ error has the\noptimal order of convergence if the space has polynomially converging\napproximation numbers. In fact, this holds for a significant portion of the\ngenerators. Additionally we show that a restriction to rational generators is\npossible with a slight increase of the bound. Furthermore, we specialise the\nresults to the weighted Korobov space, where we derive a bound applicable to\nlow values of sample points, and state tractability results.", "published": "2025-05-01 10:33:19", "link": "http://arxiv.org/abs/2505.00440v1", "categories": ["math.NA", "cs.NA", "65D15, 65T40"], "primary_category": "math.NA"}
{"title": "Stability of the first-order unified gas-kinetic scheme based on a linear kinetic model", "abstract": "The unified gas-kinetic scheme (UGKS) is becoming increasingly popular for\nmultiscale simulations in all flow regimes. This paper provides the first\nanalytical study on the stability of the UGKS applied to a linear kinetic\nmodel, which is able to reproduce the one-dimensional linear scalar\nadvection-diffusion equation via the Chapman-Enskog expansion method. Adopting\nperiodic boundary conditions and neglecting the error from numerical\nintegration, this paper rigorously proves the weighted $L^2$-stability of the\nfirst-order UGKS under the Courant-Friedrichs-Lewy (CFL) conditions. It is\nshown that the time step of the method is not constrained by being less than\nthe particle collision time, nor is it limited by parabolic type CFL conditions\ntypically applied in solving diffusion equations. The novelty of the proof lies\nin that based on the ratio of the time step to the particle collision time, the\nupdate of distribution functions is viewed as a convex combinations of\nsub-methods related to various physics processes, such as the particle free\ntransport and collisions. The weighted $L^2$-stability of the sub-methods is\nobtained by considering them as discretizations to corresponding linear\nhyperbolic systems and utilizing the associated Riemann invariants. Finally,\nthe strong stability preserving property of the UGKS leads to the desired\nweighted $L^2$-stability.", "published": "2025-05-01 10:12:07", "link": "http://arxiv.org/abs/2505.00434v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Affine matrix scrambling achieves smoothness-dependent convergence rates", "abstract": "We study the convergence rate of the median estimator for affine matrix\nscrambled digital nets applied to integrands over the unit hypercube $[0,\n1]^s$. By taking the median of $(2r-1)$ independent randomized quasi-Monte\nCarlo (RQMC) samples, we demonstrate that the desired convergence rates can be\nachieved without increasing the number of randomizations $r$ as the quadrature\nsize $N$ grows for both bounded and unbounded integrands. For unbounded\nintegrands, our analysis assumes a boundary growth condition on the weak\nderivatives and also considers singularities such as kinks and jump\ndiscontinuities. Notably, when $r = 1$, the median estimator reduces to the\nstandard RQMC estimator. By applying analytical techniques developed for median\nestimators, we prove that the affine matrix scrambled estimator achieves a\nconvergence rate depending on the integrand's smoothness, and is therefore not\nlimited by the canonical rate $\\mathcal{O}(N^{-3/2})$. However, this\nsmoothness-dependent theoretical rate is not observed empirically in numerical\nexperiments when the affine matrix scrambling yields a heavy-tailed sampling\ndistribution. In contrast, the median estimator consistently reveals the\ntheoretical rates and yields smaller integration errors than mean estimators,\nfurther highlighting its advantages.", "published": "2025-05-01 09:09:58", "link": "http://arxiv.org/abs/2505.00411v1", "categories": ["math.NA", "cs.NA", "65C05"], "primary_category": "math.NA"}
{"title": "Improving the scalability of a high-order atmospheric dynamics solver based on the deal.II library", "abstract": "We present recent advances on the massively parallel performance of a\nnumerical scheme for atmosphere dynamics applications based on the deal.II\nlibrary. The implicit-explicit discontinuous finite element scheme is based on\na matrix-free approach, meaning that no global sparse matrix is built and only\nthe action of the linear operators on a vector is actually implemented.\nFollowing a profiling analysis, we focus on the performance optimization of the\nnumerical method and describe the impact of different preconditioning and\nsolving techniques in this framework. Moreover, we show how the use of the\nlatest version of the \\texttt{deal.II} library and of suitable execution flags\ncan improve the parallel performance.", "published": "2025-05-01 08:20:10", "link": "http://arxiv.org/abs/2505.00384v1", "categories": ["math.NA", "cs.DC", "cs.NA", "cs.PF"], "primary_category": "math.NA"}
{"title": "On the Schr\u00f6dingerization method for linear non-unitary dynamics with optimal dependence on matrix queries", "abstract": "The Schr\\\"odingerization method converts linear partial and ordinary\ndifferential equations with non-unitary dynamics into systems of\nSchr\\\"odinger-type equations with unitary evolution. It does so via the\nso-called warped phase transformation that maps the original equation into a\nSchr\\\"odinger-type equation in one higher dimension\n\\cite{Schrshort,JLY22SchrLong}. We show that by employing a smooth\ninitialization of the warped phase transform \\cite{JLM24SchrBackward},\nSchr\\\"odingerization can in fact achieve optimal scaling in matrix queries.\nThis paper presents the detailed implementation of three smooth initializations\nfor the Schr\\\"odingerization method: (a) the cut-off function, (b) the\nhigher-order polynomial interpolation, and (c) the Fourier transform methods,\nthat achieve optimality for (a) and near-optimality for (b) and (c). A detailed\nanalysis of key parameters affecting time complexity is conducted.", "published": "2025-05-01 07:46:50", "link": "http://arxiv.org/abs/2505.00370v1", "categories": ["math.NA", "cs.NA", "quant-ph"], "primary_category": "math.NA"}
{"title": "Integral Representations of Sobolev Spaces via ReLU$^k$ Activation Function and Optimal Error Estimates for Linearized Networks", "abstract": "This paper presents two main theoretical results concerning shallow neural\nnetworks with ReLU$^k$ activation functions. We establish a novel integral\nrepresentation for Sobolev spaces, showing that every function in\n$H^{\\frac{d+2k+1}{2}}(\\Omega)$ can be expressed as an $L^2$-weighted integral\nof ReLU$^k$ ridge functions over the unit sphere. This result mirrors the known\nrepresentation of Barron spaces and highlights a fundamental connection between\nSobolev regularity and neural network representations. Moreover, we prove that\nlinearized shallow networks -- constructed by fixed inner parameters and\noptimizing only the linear coefficients -- achieve optimal approximation rates\n$O(n^{-\\frac{1}{2}-\\frac{2k+1}{2d}})$ in Sobolev spaces.", "published": "2025-05-01 06:50:41", "link": "http://arxiv.org/abs/2505.00351v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Nystr\u00f6m Type Exponential Integrators for Strongly Magnetized Charged Particle Dynamics", "abstract": "Calculating the dynamics of charged particles in electromagnetic fields (i.e.\nthe particle pushing problem) is one of the most computationally intensive\ncomponents of particle-in-cell (PIC) methods for plasma physics simulations.\nThis task is especially challenging when the plasma is strongly magnetized,\nsince in this case the particle motion consists of a wide range of temporal\nscales from highly oscillatory fast gyromotion to slow macroscopic behavior and\nthe resulting numerical model is very stiff. Current state-of-the-art time\nintegrators used to simulate particle motion have limitations given the severe\nnumerical stiffness of the problem and more efficient methods are of interest.\nRecently, exponential integrators have been proposed as a promising new\napproach for these simulations and shown to offer computational advantages over\ncommonly used schemes. Exponential methods can solve linear problems exactly\nand are $A$-stable. In this paper, the standard exponential algorithms\nframework is extended to derive Nystr\\\"om-type exponential methods that\nintegrate the Newtonian equations of motion as a second-order differential\nequation. Specific Nystr\\\"om-type schemes of second and third orders are\nderived and applied to strongly magnetized particle pushing problems. Numerical\nexperiments are presented to demonstrate that the Nystr\\\"om-type exponential\nintegrators can provide significant improvement in computational efficiency\nover the standard exponential methods.", "published": "2025-05-01 04:22:49", "link": "http://arxiv.org/abs/2505.00288v1", "categories": ["physics.comp-ph", "cs.NA", "math.NA", "physics.plasm-ph"], "primary_category": "physics.comp-ph"}
{"title": "A polytopal discrete de Rham scheme for the exterior calculus Einstein's equations", "abstract": "In this work, based on the $3+1$ decomposition in [23, 32], we present a\nfully exterior calculus breakdown of spacetime and Einstein's equations. Links\nto the orthonormal frame approach [37] are drawn to help understand the\nvariables in this context. Two formulations are derived, discretised and tested\nusing the exterior calculus discrete de Rham complex [12], and some discrete\nquantities are shown to be conserved in one of the cases.", "published": "2025-05-01 04:18:49", "link": "http://arxiv.org/abs/2505.00286v1", "categories": ["gr-qc", "cs.NA", "math.NA"], "primary_category": "gr-qc"}
{"title": "Mixed Precision Orthogonalization-Free Projection Methods for Eigenvalue and Singular Value Problems", "abstract": "Mixed-precision arithmetic offers significant computational advantages for\nlarge-scale matrix computation tasks, yet preserving accuracy and stability in\neigenvalue problems and the singular value decomposition (SVD) remains\nchallenging. This paper introduces an approach that eliminates\northogonalization requirements in traditional Rayleigh-Ritz projection methods.\nThe proposed method employs non-orthogonal bases computed at reduced precision,\nresulting in bases computed without inner-products. A primary focus is on\nmaintaining the linear independence of the basis vectors. Through extensive\nevaluation with both synthetic test cases and real-world applications, we\ndemonstrate that the proposed approach achieves the desired accuracy while\nfully taking full advantage of mixed-precision arithmetic.", "published": "2025-05-01 04:05:58", "link": "http://arxiv.org/abs/2505.00281v1", "categories": ["math.NA", "cs.NA", "15A23, 65F25, 65Y05, 68W10"], "primary_category": "math.NA"}
{"title": "Quantile-RK and Double Quantile-RK Error Horizon Analysis", "abstract": "In solving linear systems of equations of the form $Ax=b$, corruptions\npresent in $b$ affect stochastic iterative algorithms' ability to reach the\ntrue solution $x^\\ast$ to the uncorrupted linear system. The randomized\nKaczmarz method converges in expectation to $x^\\ast$ up to an error horizon\ndependent on the conditioning of $A$ and the supremum norm of the corruption in\n$b$. To avoid this error horizon in the sparse corruption setting, previous\nworks have proposed quantile-based adaptations that make iterative methods\nrobust. Our work first establishes a new convergence rate for the\nquantile-based random Kaczmarz (qRK) and double quantile-based random Kaczmarz\n(dqRK) methods, which, under mild conditions, improves upon known bounds. We\nfurther consider the more practical setting in which the vector $b$ includes\nboth non-sparse \"noise\" and sparse \"corruption\". Error horizon bounds for qRK\nand dqRK are derived and shown to produce a smaller error horizon compared to\ntheir non-quantile-based counterparts, further demonstrating the advantages of\nquantile-based methods.", "published": "2025-05-01 02:48:15", "link": "http://arxiv.org/abs/2505.00258v1", "categories": ["math.NA", "cs.NA", "65F10, 65F20"], "primary_category": "math.NA"}
{"title": "Numerical analysis on locally risk-minimizing strategies for Barndorff-Nielsen and Shephard models", "abstract": "We develop a numerical method for locally risk-minimizing (LRM) strategies\nfor Barndorff-Nielsen and Shephard (BNS) models. Arai et al. (2017) derived a\nmathematical expression for LRM strategies in BNS models using Malliavin\ncalculus for L\\'evy processes and presented some numerical results only for the\ncase where the asset price process is a martingale. Subsequently, Arai and Imai\n(2024) developed the first Monte Carlo (MC) method available for non-martingale\nBNS models with infinite active jumps. Here, we modify the expression obtained\nby Arai et al. (2017) into a numerically tractable form, and, using the MC\nmethod developed by Arai and Imai (2024), propose a numerical method of LRM\nstrategies available for non-martingale BNS models with infinite active jumps.\nIn the final part of this paper, we will conduct some numerical experiments.", "published": "2025-05-01 02:41:11", "link": "http://arxiv.org/abs/2505.00255v1", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "Do global forecasting models require frequent retraining?", "abstract": "In an era of increasing computational capabilities and growing environmental\nconsciousness, organizations face a critical challenge in balancing the\naccuracy of forecasting models with computational efficiency and\nsustainability. Global forecasting models, lowering the computational time,\nhave gained significant attention over the years. However, the common practice\nof retraining these models with new observations raises important questions\nabout the costs of forecasting. Using ten different machine learning and deep\nlearning models, we analyzed various retraining scenarios, ranging from\ncontinuous updates to no retraining at all, across two large retail datasets.\nWe showed that less frequent retraining strategies maintain the forecast\naccuracy while reducing the computational costs, providing a more sustainable\napproach to large-scale forecasting. We also found that machine learning models\nare a marginally better choice to reduce the costs of forecasting when coupled\nwith less frequent model retraining strategies as the frequency of the data\nincreases. Our findings challenge the conventional belief that frequent\nretraining is essential for maintaining forecasting accuracy. Instead, periodic\nretraining offers a good balance between predictive performance and efficiency,\nboth in the case of point and probabilistic forecasting. These insights provide\nactionable guidelines for organizations seeking to optimize forecasting\npipelines while reducing costs and energy consumption.", "published": "2025-05-01 07:00:29", "link": "http://arxiv.org/abs/2505.00356v1", "categories": ["stat.AP", "stat.ML", "stat.OT"], "primary_category": "stat.AP"}
{"title": "Leveraging Surplus Electricity: Profitability of Bitcoin Mining as a National Strategy in South Korea", "abstract": "This study examines the feasibility and profitability of utilizing surplus\nelectricity for Bitcoin mining. Surplus electricity refers to the remaining\nelectricity after net metering, which can be repurposed for Bitcoin mining to\nimprove Korea Electric Power Corporation's (KEPCO) energy resource efficiency\nand alleviate its debt challenges. Net metering (or net energy metering) is an\nelectricity billing mechanism that allows consumers who generate some or all of\ntheir own electricity to use that electricity when they want, rather than when\nit is produced. Using the latest Bitcoin miner, the Antminer S21 XP Hyd, the\nstudy evaluates daily Bitcoin mining when operating at 30,565 and 45,439 units,\nincorporating Bitcoin network hash rates to assess profitability. To examine\nprofitability, the Random Forest Regressor and Long Short-Term Memory models\nwere used to predict the Bitcoin price. The analysis shows that the use of\nexcess electricity for Bitcoin mining not only generates economic revenue, but\nalso minimizes energy loss, reduces debt, and resolves unsettled payment issues\nfor KEPCO. This study empirically investigates and analyzes the integration of\nelectricity surplus in South Korea with bitcoin mining for the first time. The\nfindings highlight the potential to strengthen the financial stability of KEPCO\nand demonstrate the feasibility of Bitcoin mining. In addition, this research\nserves as a foundational resource for future advancements in the Bitcoin mining\nindustry and the efficient use of energy resources.", "published": "2025-05-01 04:52:33", "link": "http://arxiv.org/abs/2505.00303v1", "categories": ["stat.AP", "stat.ML", "68Q99", "D.3.4; E.1; G.3"], "primary_category": "stat.AP"}
{"title": "Physical Limits and Optimal Synthesis of Beyond Diagonal Anomalous Scatterers", "abstract": "Realizing metasurfaces for anomalous scattering is fundamental to designing\nreflector arrays, reconfigurable intelligent surfaces, and metasurface\nantennas. However, the basic cost of steering scattering into non-specular\ndirections is not fully understood. This paper derives tight physical bounds on\nanomalous scattering using antenna array systems equipped with non-local\nmatching networks. The matching networks are explicitly synthesized based on\nthe solutions of the optimization problems that define these bounds.\nFurthermore, we analyze fundamental limits for metasurface antennas implemented\nwith metallic and dielectric materials exhibiting minimal loss within a finite\ndesign region. The results reveal a typical 6dB reduction in bistatic radar\ncross section (RCS) in anomalous directions compared to the forward direction.\nNumerical examples complement the theory and illustrate the inherent cost of\nachieving anomalous scattering relative to forward or specular scattering for\ncanonical configurations.", "published": "2025-05-01 17:53:04", "link": "http://arxiv.org/abs/2505.00691v1", "categories": ["physics.optics", "cs.SY", "eess.SP", "eess.SY", "physics.class-ph"], "primary_category": "physics.optics"}
{"title": "Spreading Depolarization Detection in Electrocorticogram Spectrogram Imaging by Deep Learning: Is It Just About Delta Band?", "abstract": "Prevention of secondary brain injury is a core aim of neurocritical care,\nwith Spreading Depolarizations (SDs) recognized as a significant independent\ncause. SDs are typically monitored through invasive, high-frequency\nelectrocorticography (ECoG); however, detection remains challenging due to\nsignal artifacts that obscure critical SD-related electrophysiological changes,\nsuch as power attenuation and DC drifting. Recent studies suggest spectrogram\nanalysis could improve SD detection; however, brain injury patients often show\npower reduction across all bands except delta, causing class imbalance.\nPrevious methods focusing solely on delta mitigates imbalance but overlooks\nfeatures in other frequencies, limiting detection performance. This study\nexplores using multi-frequency spectrogram analysis, revealing that essential\nSD-related features span multiple frequency bands beyond the most active delta\nband. This study demonstrated that further integration of both alpha and delta\nbands could result in enhanced SD detection accuracy by a deep learning model.", "published": "2025-05-01 17:17:15", "link": "http://arxiv.org/abs/2505.00666v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Secure Multi-Hop Relaying in Large-Scale Space-Air-Ground-Sea Integrated Networks", "abstract": "As a key enabler of borderless and ubiquitous connectivity,\nspace-air-ground-sea integrated networks (SAGSINs) are expected to be a\ncornerstone of 6G wireless communications. However, the multi-tiered and\nglobal-scale nature of SAGSINs also amplifies the security vulnerabilities,\nparticularly due to the hidden, passive eavesdroppers distributed throughout\nthe network. In this paper, we introduce a joint optimization framework for\nmulti-hop relaying in SAGSINs that maximizes the minimum user throughput while\nensuring a minimum strictly positive secure connection (SPSC) probability. We\nfirst derive a closed-form expression for the SPSC probability and incorporate\nthis into a cross-layer optimization framework that jointly optimizes radio\nresources and relay routes. Specifically, we propose an $\\mathcal{O}(1)$\noptimal frequency allocation and power splitting strategy-dividing power levels\nof data transmission and cooperative jamming. We then introduce a Monte-Carlo\nrelay routing algorithm that closely approaches the performance of the\nnumerical upper-bound method. We validate our framework on testbeds built with\nreal-world dataset. All source code and data for reproducing the numerical\nexperiments will be open-sourced.", "published": "2025-05-01 14:59:47", "link": "http://arxiv.org/abs/2505.00573v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "A Temporal Broadening-Aware Pulse Width Adaptation Scheme for ISI Mitigation and Energy Efficiency in THz Communication", "abstract": "Terahertz (THz) communication ensures the provision of ultra-high data rates\nowing to its abundant bandwidth; however, its performance is impeded by complex\npropagation mechanisms. In particular, molecular absorption induces a temporal\nbroadening effect (TBE), which causes pulse spreading and inter-symbol\ninterference (ISI), especially in ON-OFF keying-based systems. To address this,\nwe propose an adaptive pulse-width transmission scheme that dynamically adjusts\npulse durations based on the anticipated TBE. This approach suppresses ISI by\nconfining energy within symbol durations while also exploiting TBE\nconstructively to reduce pulse transmissions in specific bit patterns, leading\nto improved energy efficiency (EE) as an additional advantage of the proposed\nscheme. Analytical derivations and simulation results confirm that the proposed\nscheme substantially improves EE and bit error rate under practical THz channel\nconditions.", "published": "2025-05-01 10:30:45", "link": "http://arxiv.org/abs/2505.00438v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Stealth Signals: Multi-Discriminator GANs for Covert Communications Against Diverse Wardens", "abstract": "Covert wireless communications are critical for concealing the existence of\nany transmission from adversarial wardens, particularly in complex environments\nwith multiple heterogeneous detectors. This paper proposes a novel adversarial\nAI framework leveraging a multi-discriminator Generative Adversarial Network\n(GAN) to design signals that evade detection by diverse wardens, while ensuring\nreliable decoding by the intended receiver. The transmitter is modeled as a\ngenerator that produces noise-like signals, while every warden is modeled as an\nindividual discriminator, suggesting varied channel conditions and detection\ntechniques. Unlike traditional methods like spread spectrum or\nsingle-discriminator GANs, our approach addresses multi-warden scenarios with\nmoving receiver and wardens, which enhances robustness in urban surveillance,\nmilitary operations, and 6G networks. Performance evaluation shows encouraging\nresults with improved detection probabilities and bit error rates (BERs), in up\nto five warden cases, compared to noise injection and single-discriminator\nbaselines. The scalability and flexibility of the system make it a potential\ncandidate for future wireless secure systems, and potential future directions\ninclude real-time optimization and synergy with 6G technologies such as\nintelligent reflecting surfaces.", "published": "2025-05-01 08:43:20", "link": "http://arxiv.org/abs/2505.00399v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Conformal changepoint localization", "abstract": "Changepoint localization is the problem of estimating the index at which a\nchange occurred in the data generating distribution of an ordered list of data,\nor declaring that no change occurred. We present the broadly applicable CONCH\n(CONformal CHangepoint localization) algorithm, which uses a matrix of\nconformal p-values to produce a confidence interval for a (single) changepoint\nunder the mild assumption that the pre-change and post-change distributions are\neach exchangeable. We exemplify the CONCH algorithm on a variety of synthetic\nand real-world datasets, including using black-box pre-trained classifiers to\ndetect changes in sequences of images or text.", "published": "2025-05-01 04:27:52", "link": "http://arxiv.org/abs/2505.00292v1", "categories": ["math.ST", "eess.SP", "stat.ME", "stat.TH"], "primary_category": "math.ST"}
{"title": "NeMo-Inspector: A Visualization Tool for LLM Generation Analysis", "abstract": "Adapting Large Language Models (LLMs) to novel tasks and enhancing their\noverall capabilities often requires large, high-quality training datasets.\nSynthetic data, generated at scale, serves a valuable alternative when\nreal-world data is scarce or difficult to obtain. However, ensuring the quality\nof synthetic datasets is challenging, as developers must manually inspect and\nrefine numerous samples to identify errors and areas for improvement. This\nprocess is time-consuming and requires specialized tools. We introduce\nNeMo-Inspector, an open-source tool designed to simplify the analysis of\nsynthetic datasets with integrated inference capabilities. We demonstrate its\neffectiveness through two real-world cases. Analysis and cleaning of the\nsynthetically generated GSM-Plus dataset with NeMo-Inspector led to a\nsignificant decrease in low-quality samples from 46.99% to 19.51%. The tool\nalso helped identify and correct generation errors in OpenMath models,\nimproving accuracy by 1.92% on the MATH dataset and by 4.17% on the GSM8K\ndataset for a Meta-Llama-3-8B model fine-tuned on synthetic data generated from\nNemotron-4-340B.", "published": "2025-05-01 22:47:06", "link": "http://arxiv.org/abs/2505.00903v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SmallPlan: Leverage Small Language Models for Sequential Path Planning with Simulation-Powered, LLM-Guided Distillation", "abstract": "Efficient path planning in robotics, particularly within large-scale, dynamic\nenvironments, remains a significant hurdle. While Large Language Models (LLMs)\noffer strong reasoning capabilities, their high computational cost and limited\nadaptability in dynamic scenarios hinder real-time deployment on edge devices.\nWe present SmallPlan -- a novel framework leveraging LLMs as teacher models to\ntrain lightweight Small Language Models (SLMs) for high-level path planning\ntasks. In SmallPlan, the SLMs provide optimal action sequences to navigate\nacross scene graphs that compactly represent full-scaled 3D scenes. The SLMs\nare trained in a simulation-powered, interleaved manner with LLM-guided\nsupervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not\nonly enables SLMs to successfully complete navigation tasks but also makes them\naware of important factors like travel distance and number of trials. Through\nexperiments, we demonstrate that the fine-tuned SLMs perform competitively with\nlarger models like GPT-4o on sequential path planning, without suffering from\nhallucination and overfitting. SmallPlan is resource-efficient, making it\nwell-suited for edge-device deployment and advancing practical autonomous\nrobotics.", "published": "2025-05-01 19:44:36", "link": "http://arxiv.org/abs/2505.00831v1", "categories": ["cs.RO", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Knowledge-augmented Pre-trained Language Models for Biomedical Relation Extraction", "abstract": "Automatic relationship extraction (RE) from biomedical literature is critical\nfor managing the vast amount of scientific knowledge produced each year. In\nrecent years, utilizing pre-trained language models (PLMs) has become the\nprevalent approach in RE. Several studies report improved performance when\nincorporating additional context information while fine-tuning PLMs for RE.\nHowever, variations in the PLMs applied, the databases used for augmentation,\nhyper-parameter optimization, and evaluation methods complicate direct\ncomparisons between studies and raise questions about the generalizability of\nthese findings. Our study addresses this research gap by evaluating PLMs\nenhanced with contextual information on five datasets spanning four relation\nscenarios within a consistent evaluation framework. We evaluate three baseline\nPLMs and first conduct extensive hyperparameter optimization. After selecting\nthe top-performing model, we enhance it with additional data, including textual\nentity descriptions, relational information from knowledge graphs, and\nmolecular structure encodings. Our findings illustrate the importance of i) the\nchoice of the underlying language model and ii) a comprehensive hyperparameter\noptimization for achieving strong extraction performance. Although inclusion of\ncontext information yield only minor overall improvements, an ablation study\nreveals substantial benefits for smaller PLMs when such external data was\nincluded during fine-tuning.", "published": "2025-05-01 19:16:18", "link": "http://arxiv.org/abs/2505.00814v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Mathematical Philosophy of Explanations in Mechanistic Interpretability -- The Strange Science Part I.i", "abstract": "Mechanistic Interpretability aims to understand neural networks through\ncausal explanations. We argue for the Explanatory View Hypothesis: that\nMechanistic Interpretability research is a principled approach to understanding\nmodels because neural networks contain implicit explanations which can be\nextracted and understood. We hence show that Explanatory Faithfulness, an\nassessment of how well an explanation fits a model, is well-defined. We propose\na definition of Mechanistic Interpretability (MI) as the practice of producing\nModel-level, Ontic, Causal-Mechanistic, and Falsifiable explanations of neural\nnetworks, allowing us to distinguish MI from other interpretability paradigms\nand detail MI's inherent limits. We formulate the Principle of Explanatory\nOptimism, a conjecture which we argue is a necessary precondition for the\nsuccess of Mechanistic Interpretability.", "published": "2025-05-01 19:08:34", "link": "http://arxiv.org/abs/2505.00808v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Reasoning Capabilities and Invariability of Large Language Models", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in\nmanipulating natural language across multiple applications, but their ability\nto handle simple reasoning tasks is often questioned. In this work, we aim to\nprovide a comprehensive analysis of LLMs' reasoning competence, specifically\nfocusing on their prompt dependency. In particular, we introduce a new\nbenchmark dataset with a series of simple reasoning questions demanding shallow\nlogical reasoning. Aligned with cognitive psychology standards, the questions\nare confined to a basic domain revolving around geometric figures, ensuring\nthat responses are independent of any pre-existing intuition about the world\nand rely solely on deduction. An empirical analysis involving zero-shot and\nfew-shot prompting across 24 LLMs of different sizes reveals that, while LLMs\nwith over 70 billion parameters perform better in the zero-shot setting, there\nis still a large room for improvement. An additional test with chain-of-thought\nprompting over 22 LLMs shows that this additional prompt can aid or damage the\nperformance of models, depending on whether the rationale is required before or\nafter the answer.", "published": "2025-05-01 18:12:30", "link": "http://arxiv.org/abs/2505.00776v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Modal Language Models as Text-to-Image Model Evaluators", "abstract": "The steady improvements of text-to-image (T2I) generative models lead to slow\ndeprecation of automatic evaluation benchmarks that rely on static datasets,\nmotivating researchers to seek alternative ways to evaluate the T2I progress.\nIn this paper, we explore the potential of multi-modal large language models\n(MLLMs) as evaluator agents that interact with a T2I model, with the objective\nof assessing prompt-generation consistency and image aesthetics. We present\nMultimodal Text-to-Image Eval (MT2IE), an evaluation framework that iteratively\ngenerates prompts for evaluation, scores generated images and matches T2I\nevaluation of existing benchmarks with a fraction of the prompts used in\nexisting static benchmarks. Moreover, we show that MT2IE's prompt-generation\nconsistency scores have higher correlation with human judgment than scores\npreviously introduced in the literature. MT2IE generates prompts that are\nefficient at probing T2I model performance, producing the same relative T2I\nmodel rankings as existing benchmarks while using only 1/80th the number of\nprompts for evaluation.", "published": "2025-05-01 17:47:55", "link": "http://arxiv.org/abs/2505.00759v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "100 Days After DeepSeek-R1: A Survey on Replication Studies and More Directions for Reasoning Language Models", "abstract": "The recent development of reasoning language models (RLMs) represents a novel\nevolution in large language models. In particular, the recent release of\nDeepSeek-R1 has generated widespread social impact and sparked enthusiasm in\nthe research community for exploring the explicit reasoning paradigm of\nlanguage models. However, the implementation details of the released models\nhave not been fully open-sourced by DeepSeek, including DeepSeek-R1-Zero,\nDeepSeek-R1, and the distilled small models. As a result, many replication\nstudies have emerged aiming to reproduce the strong performance achieved by\nDeepSeek-R1, reaching comparable performance through similar training\nprocedures and fully open-source data resources. These works have investigated\nfeasible strategies for supervised fine-tuning (SFT) and reinforcement learning\nfrom verifiable rewards (RLVR), focusing on data preparation and method design,\nyielding various valuable insights. In this report, we provide a summary of\nrecent replication studies to inspire future research. We primarily focus on\nSFT and RLVR as two main directions, introducing the details for data\nconstruction, method design and training procedure of current replication\nstudies. Moreover, we conclude key findings from the implementation details and\nexperimental results reported by these studies, anticipating to inspire future\nresearch. We also discuss additional techniques of enhancing RLMs, highlighting\nthe potential of expanding the application scope of these models, and\ndiscussing the challenges in development. By this survey, we aim to help\nresearchers and developers of RLMs stay updated with the latest advancements,\nand seek to inspire new ideas to further enhance RLMs.", "published": "2025-05-01 14:28:35", "link": "http://arxiv.org/abs/2505.00551v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Large Language Model based Human-Agent Systems", "abstract": "Recent advances in large language models (LLMs) have sparked growing interest\nin building fully autonomous agents. However, fully autonomous LLM-based agents\nstill face significant challenges, including limited reliability due to\nhallucinations, difficulty in handling complex tasks, and substantial safety\nand ethical risks, all of which limit their feasibility and trustworthiness in\nreal-world applications. To overcome these limitations, LLM-based human-agent\nsystems (LLM-HAS) incorporate human-provided information, feedback, or control\ninto the agent system to enhance system performance, reliability and safety.\nThis paper provides the first comprehensive and structured survey of LLM-HAS.\nIt clarifies fundamental concepts, systematically presents core components\nshaping these systems, including environment & profiling, human feedback,\ninteraction types, orchestration and communication, explores emerging\napplications, and discusses unique challenges and opportunities. By\nconsolidating current knowledge and offering a structured overview, we aim to\nfoster further research and innovation in this rapidly evolving\ninterdisciplinary field. Paper lists and resources are available at\nhttps://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-System-Papers.", "published": "2025-05-01 08:29:26", "link": "http://arxiv.org/abs/2505.00753v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks", "abstract": "Many methods for improving Large Language Model (LLM) agents for sequential\ndecision-making tasks depend on task-specific knowledge engineering--such as\nprompt tuning, curated in-context examples, or customized observation and\naction spaces. Using these approaches, agent performance improves with the\nquality or amount of knowledge engineering invested. Instead, we investigate\nhow LLM agents can automatically improve their performance by learning\nin-context from their own successful experiences on similar tasks. Rather than\nrelying on task-specific knowledge engineering, we focus on constructing and\nrefining a database of self-generated examples. We demonstrate that even a\nnaive accumulation of successful trajectories across training tasks boosts test\nperformance on three benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%),\nand InterCode-SQL (75% to 79%)--matching the performance the initial agent\nachieves if allowed two to three attempts per task. We then introduce two\nextensions: (1) database-level selection through population-based training to\nidentify high-performing example collections, and (2) exemplar-level selection\nthat retains individual trajectories based on their empirical utility as\nin-context examples. These extensions further enhance performance, achieving\n91% on ALFWorld--matching more complex approaches that employ task-specific\ncomponents and prompts. Our results demonstrate that automatic trajectory\ndatabase construction offers a compelling alternative to labor-intensive\nknowledge engineering.", "published": "2025-05-01 00:48:12", "link": "http://arxiv.org/abs/2505.00234v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Dynamic and Distributed Routing in IoT Networks based on Multi-Objective Q-Learning", "abstract": "The last few decades have witnessed a rapid increase in IoT devices owing to\ntheir wide range of applications, such as smart healthcare monitoring systems,\nsmart cities, and environmental monitoring. A critical task in IoT networks is\nsensing and transmitting information over the network. The IoT nodes gather\ndata by sensing the environment and then transmit this data to a destination\nnode via multi-hop communication, following some routing protocols. These\nprotocols are usually designed to optimize possibly contradictory objectives,\nsuch as maximizing packet delivery ratio and energy efficiency. While most\nliterature has focused on optimizing a static objective that remains unchanged,\nmany real-world IoT applications require adapting to rapidly shifting\npriorities. For example, in monitoring systems, some transmissions are\ntime-critical and require a high priority on low latency, while other\ntransmissions are less urgent and instead prioritize energy efficiency. To meet\nsuch dynamic demands, we propose novel dynamic and distributed routing based on\nmultiobjective Q-learning that can adapt to changes in preferences in\nreal-time. Our algorithm builds on ideas from both multi-objective optimization\nand Q-learning. We also propose a novel greedy interpolation policy scheme to\ntake near-optimal decisions for unexpected preference changes. The proposed\nscheme can approximate and utilize the Pareto-efficient solutions for dynamic\npreferences, thus utilizing past knowledge to adapt to unpredictable\npreferences quickly during runtime. Simulation results show that the proposed\nscheme outperforms state-of-the-art algorithms for various exploration\nstrategies, preference variation patterns, and important metrics like overall\nreward, energy efficiency, and packet delivery ratio.", "published": "2025-05-01 23:34:35", "link": "http://arxiv.org/abs/2505.00918v1", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.NI"], "primary_category": "cs.DC"}
{"title": "Multivariate Conformal Selection", "abstract": "Selecting high-quality candidates from large datasets is critical in\napplications such as drug discovery, precision medicine, and alignment of large\nlanguage models (LLMs). While Conformal Selection (CS) provides rigorous\nuncertainty quantification, it is limited to univariate responses and scalar\ncriteria. To address this issue, we propose Multivariate Conformal Selection\n(mCS), a generalization of CS designed for multivariate response settings. Our\nmethod introduces regional monotonicity and employs multivariate nonconformity\nscores to construct conformal p-values, enabling finite-sample False Discovery\nRate (FDR) control. We present two variants: mCS-dist, using distance-based\nscores, and mCS-learn, which learns optimal scores via differentiable\noptimization. Experiments on simulated and real-world datasets demonstrate that\nmCS significantly improves selection power while maintaining FDR control,\nestablishing it as a robust framework for multivariate selection tasks.", "published": "2025-05-01 23:33:57", "link": "http://arxiv.org/abs/2505.00917v1", "categories": ["stat.ME", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "stat.ME"}
{"title": "Fine-Tuning without Performance Degradation", "abstract": "Fine-tuning policies learned offline remains a major challenge in application\ndomains. Monotonic performance improvement during \\emph{fine-tuning} is often\nchallenging, as agents typically experience performance degradation at the\nearly fine-tuning stage. The community has identified multiple difficulties in\nfine-tuning a learned network online, however, the majority of progress has\nfocused on improving learning efficiency during fine-tuning. In practice, this\ncomes at a serious cost during fine-tuning: initially, agent performance\ndegrades as the agent explores and effectively overrides the policy learned\noffline. We show across a range of settings, many offline-to-online algorithms\nexhibit either (1) performance degradation or (2) slow learning (sometimes\neffectively no improvement) during fine-tuning. We introduce a new fine-tuning\nalgorithm, based on an algorithm called Jump Start, that gradually allows more\nexploration based on online estimates of performance. Empirically, this\napproach achieves fast fine-tuning and significantly reduces performance\ndegradations compared with existing algorithms designed to do the same.", "published": "2025-05-01 23:19:07", "link": "http://arxiv.org/abs/2505.00913v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Rethinking Time Encoding via Learnable Transformation Functions", "abstract": "Effectively modeling time information and incorporating it into applications\nor models involving chronologically occurring events is crucial. Real-world\nscenarios often involve diverse and complex time patterns, which pose\nsignificant challenges for time encoding methods. While previous methods focus\non capturing time patterns, many rely on specific inductive biases, such as\nusing trigonometric functions to model periodicity. This narrow focus on\nsingle-pattern modeling makes them less effective in handling the diversity and\ncomplexities of real-world time patterns. In this paper, we investigate to\nimprove the existing commonly used time encoding methods and introduce\nLearnable Transformation-based Generalized Time Encoding (LeTE). We propose\nusing deep function learning techniques to parameterize non-linear\ntransformations in time encoding, making them learnable and capable of modeling\ngeneralized time patterns, including diverse and complex temporal dynamics. By\nenabling learnable transformations, LeTE encompasses previous methods as\nspecific cases and allows seamless integration into a wide range of tasks.\nThrough extensive experiments across diverse domains, we demonstrate the\nversatility and effectiveness of LeTE.", "published": "2025-05-01 22:04:18", "link": "http://arxiv.org/abs/2505.00887v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Towards Explainable Temporal User Profiling with LLMs", "abstract": "Accurately modeling user preferences is vital not only for improving\nrecommendation performance but also for enhancing transparency in recommender\nsystems. Conventional user profiling methods, such as averaging item\nembeddings, often overlook the evolving, nuanced nature of user interests,\nparticularly the interplay between short-term and long-term preferences. In\nthis work, we leverage large language models (LLMs) to generate natural\nlanguage summaries of users' interaction histories, distinguishing recent\nbehaviors from more persistent tendencies. Our framework not only models\ntemporal user preferences but also produces natural language profiles that can\nbe used to explain recommendations in an interpretable manner. These textual\nprofiles are encoded via a pre-trained model, and an attention mechanism\ndynamically fuses the short-term and long-term embeddings into a comprehensive\nuser representation. Beyond boosting recommendation accuracy over multiple\nbaselines, our approach naturally supports explainability: the interpretable\ntext summaries and attention weights can be exposed to end users, offering\ninsights into why specific items are suggested. Experiments on real-world\ndatasets underscore both the performance gains and the promise of generating\nclearer, more transparent justifications for content-based recommendations.", "published": "2025-05-01 22:02:46", "link": "http://arxiv.org/abs/2505.00886v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "Car Sensors Health Monitoring by Verification Based on Autoencoder and Random Forest Regression", "abstract": "Driver assistance systems provide a wide range of crucial services, including\nclosely monitoring the condition of vehicles. This paper showcases a\ngroundbreaking sensor health monitoring system designed for the automotive\nindustry. The ingenious system leverages cutting-edge techniques to process\ndata collected from various vehicle sensors. It compares their outputs within\nthe Electronic Control Unit (ECU) to evaluate the health of each sensor. To\nunravel the intricate correlations between sensor data, an extensive\nexploration of machine learning and deep learning methodologies was conducted.\nThrough meticulous analysis, the most correlated sensor data were identified.\nThese valuable insights were then utilized to provide accurate estimations of\nsensor values. Among the diverse learning methods examined, the combination of\nautoencoders for detecting sensor failures and random forest regression for\nestimating sensor values proved to yield the most impressive outcomes. A\nstatistical model using the normal distribution has been developed to identify\npossible sensor failures proactively. By comparing the actual values of the\nsensors with their estimated values based on correlated sensors, faulty sensors\ncan be detected early. When a defective sensor is detected, both the driver and\nthe maintenance department are promptly alerted. Additionally, the system\nreplaces the value of the faulty sensor with the estimated value obtained\nthrough analysis. This proactive approach was evaluated using data from twenty\nessential sensors in the Saipa's Quick vehicle's ECU, resulting in an\nimpressive accuracy rate of 99\\%.", "published": "2025-05-01 21:37:51", "link": "http://arxiv.org/abs/2505.00876v1", "categories": ["cs.AI", "cs.LG", "68T05", "I.2.1"], "primary_category": "cs.AI"}
{"title": "Thoughts without Thinking: Reconsidering the Explanatory Value of Chain-of-Thought Reasoning in LLMs through Agentic Pipelines", "abstract": "Agentic pipelines present novel challenges and opportunities for\nhuman-centered explainability. The HCXAI community is still grappling with how\nbest to make the inner workings of LLMs transparent in actionable ways. Agentic\npipelines consist of multiple LLMs working in cooperation with minimal human\ncontrol. In this research paper, we present early findings from an agentic\npipeline implementation of a perceptive task guidance system. Through\nquantitative and qualitative analysis, we analyze how Chain-of-Thought (CoT)\nreasoning, a common vehicle for explainability in LLMs, operates within agentic\npipelines. We demonstrate that CoT reasoning alone does not lead to better\noutputs, nor does it offer explainability, as it tends to produce explanations\nwithout explainability, in that they do not improve the ability of end users to\nbetter understand systems or achieve their goals.", "published": "2025-05-01 21:37:30", "link": "http://arxiv.org/abs/2505.00875v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "IK Seed Generator for Dual-Arm Human-like Physicality Robot with Mobile Base", "abstract": "Robots are strongly expected as a means of replacing human tasks. If a robot\nhas a human-like physicality, the possibility of replacing human tasks\nincreases. In the case of household service robots, it is desirable for them to\nbe on a human-like size so that they do not become excessively large in order\nto coexist with humans in their operating environment. However, robots with\nsize limitations tend to have difficulty solving inverse kinematics (IK) due to\nmechanical limitations, such as joint angle limitations. Conversely, if the\ndifficulty coming from this limitation could be mitigated, one can expect that\nthe use of such robots becomes more valuable. In numerical IK solver, which is\ncommonly used for robots with higher degrees-of-freedom (DOF), the solvability\nof IK depends on the initial guess given to the solver. Thus, this paper\nproposes a method for generating a good initial guess for a numerical IK solver\ngiven the target hand configuration. For the purpose, we define the goodness of\nan initial guess using the scaled Jacobian matrix, which can calculate the\nmanipulability index considering the joint limits. These two factors are\nrelated to the difficulty of solving IK. We generate the initial guess by\noptimizing the goodness using the genetic algorithm (GA). To enumerate much\npossible IK solutions, we use the reachability map that represents the\nreachable area of the robot hand in the arm-base coordinate system. We conduct\nquantitative evaluation and prove that using an initial guess that is judged to\nbe better using the goodness value increases the probability that IK is solved.\nFinally, as an application of the proposed method, we show that by generating\ngood initial guesses for IK a robot actually achieves three typical scenarios.", "published": "2025-05-01 21:33:23", "link": "http://arxiv.org/abs/2505.00871v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO"}
{"title": "ICQuant: Index Coding enables Low-bit LLM Quantization", "abstract": "The rapid deployment of Large Language Models (LLMs) highlights the need for\nefficient low-bit post-training quantization (PTQ), due to their high memory\ncosts. A key challenge in weight quantization is the presence of outliers,\nwhich inflate quantization ranges and lead to large errors. While a number of\noutlier suppression techniques have been proposed, they either: fail to\neffectively shrink the quantization range, or incur (relatively) high bit\noverhead. In this paper, we present ICQuant, a novel framework that leverages\noutlier statistics to design an efficient index coding scheme for outlier-aware\nweight-only quantization. Compared to existing outlier suppression techniques\nrequiring $\\approx 1$ bit overhead to halve the quantization range, ICQuant\nrequires only $\\approx 0.3$ bits; a significant saving in extreme compression\nregimes (e.g., 2-3 bits per weight). ICQuant can be used on top of any existing\nquantizers to eliminate outliers, improving the quantization quality. Using\njust 2.3 bits per weight and simple scalar quantizers, ICQuant improves the\nzero-shot accuracy of the 2-bit Llama3-70B model by up to 130% and 150%\nrelative to QTIP and QuIP#; and it achieves comparable performance to the\nbest-known fine-tuned quantizer (PV-tuning) without fine-tuning.", "published": "2025-05-01 20:23:29", "link": "http://arxiv.org/abs/2505.00850v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "OET: Optimization-based prompt injection Evaluation Toolkit", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding and generation, enabling their widespread\nadoption across various domains. However, their susceptibility to prompt\ninjection attacks poses significant security risks, as adversarial inputs can\nmanipulate model behavior and override intended instructions. Despite numerous\ndefense strategies, a standardized framework to rigorously evaluate their\neffectiveness, especially under adaptive adversarial scenarios, is lacking. To\naddress this gap, we introduce OET, an optimization-based evaluation toolkit\nthat systematically benchmarks prompt injection attacks and defenses across\ndiverse datasets using an adaptive testing framework. Our toolkit features a\nmodular workflow that facilitates adversarial string generation, dynamic attack\nexecution, and comprehensive result analysis, offering a unified platform for\nassessing adversarial robustness. Crucially, the adaptive testing framework\nleverages optimization methods with both white-box and black-box access to\ngenerate worst-case adversarial examples, thereby enabling strict red-teaming\nevaluations. Extensive experiments underscore the limitations of current\ndefense mechanisms, with some models remaining susceptible even after\nimplementing security enhancements.", "published": "2025-05-01 20:09:48", "link": "http://arxiv.org/abs/2505.00843v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR"}
{"title": "From Texts to Shields: Convergence of Large Language Models and Cybersecurity", "abstract": "This report explores the convergence of large language models (LLMs) and\ncybersecurity, synthesizing interdisciplinary insights from network security,\nartificial intelligence, formal methods, and human-centered design. It examines\nemerging applications of LLMs in software and network security, 5G\nvulnerability analysis, and generative security engineering. The report\nhighlights the role of agentic LLMs in automating complex tasks, improving\noperational efficiency, and enabling reasoning-driven security analytics.\nSocio-technical challenges associated with the deployment of LLMs -- including\ntrust, transparency, and ethical considerations -- can be addressed through\nstrategies such as human-in-the-loop systems, role-specific training, and\nproactive robustness testing. The report further outlines critical research\nchallenges in ensuring interpretability, safety, and fairness in LLM-based\nsystems, particularly in high-stakes domains. By integrating technical advances\nwith organizational and societal considerations, this report presents a\nforward-looking research agenda for the secure and effective adoption of LLMs\nin cybersecurity.", "published": "2025-05-01 20:01:07", "link": "http://arxiv.org/abs/2505.00841v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR"}
{"title": "MIMIC-\\RNum{4}-Ext-22MCTS: A 22 Millions-Event Temporal Clinical Time-Series Dataset with Relative Timestamp for Risk Prediction", "abstract": "Clinical risk prediction based on machine learning algorithms plays a vital\nrole in modern healthcare. A crucial component in developing a reliable\nprediction model is collecting high-quality time series clinical events. In\nthis work, we release such a dataset that consists of 22,588,586 Clinical Time\nSeries events, which we term MIMIC-\\RNum{4}-Ext-22MCTS. Our source data are\ndischarge summaries selected from the well-known yet unstructured MIMIC-IV-Note\n\\cite{Johnson2023-pg}. We then extract clinical events as short text span from\nthe discharge summaries, along with the timestamps of these events as temporal\ninformation. The general-purpose MIMIC-IV-Note pose specific challenges for our\nwork: it turns out that the discharge summaries are too lengthy for typical\nnatural language models to process, and the clinical events of interest often\nare not accompanied with explicit timestamps. Therefore, we propose a new\nframework that works as follows: 1) we break each discharge summary into\nmanageably small text chunks; 2) we apply contextual BM25 and contextual\nsemantic search to retrieve chunks that have a high potential of containing\nclinical events; and 3) we carefully design prompts to teach the recently\nreleased Llama-3.1-8B \\cite{touvron2023llama} model to identify or infer\ntemporal information of the chunks. We show that the obtained dataset is so\ninformative and transparent that standard models fine-tuned on our dataset are\nachieving significant improvements in healthcare applications. In particular,\nthe BERT model fine-tuned based on our dataset achieves 10\\% improvement in\naccuracy on medical question answering task, and 3\\% improvement in clinical\ntrial matching task compared with the classic BERT. The GPT-2 model, fine-tuned\non our dataset, produces more clinically reliable results for clinical\nquestions.", "published": "2025-05-01 19:40:27", "link": "http://arxiv.org/abs/2505.00827v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from Large Language Models", "abstract": "Side-channel attacks on shared hardware resources increasingly threaten\nconfidentiality, especially with the rise of Large Language Models (LLMs). In\nthis work, we introduce Spill The Beans, a novel application of cache\nside-channels to leak tokens generated by an LLM. By co-locating an attack\nprocess on the same hardware as the victim model, we flush and reload embedding\nvectors from the embedding layer, where each token corresponds to a unique\nembedding vector. When accessed during token generation, it results in a cache\nhit detectable by our attack on shared lower-level caches.\n  A significant challenge is the massive size of LLMs, which, by nature of\ntheir compute intensive operation, quickly evicts embedding vectors from the\ncache. We address this by balancing the number of tokens monitored against the\namount of information leaked. Monitoring more tokens increases potential\nvocabulary leakage but raises the chance of missing cache hits due to eviction;\nmonitoring fewer tokens improves detection reliability but limits vocabulary\ncoverage.\n  Through extensive experimentation, we demonstrate the feasibility of leaking\ntokens from LLMs via cache side-channels. Our findings reveal a new\nvulnerability in LLM deployments, highlighting that even sophisticated models\nare susceptible to traditional side-channel attacks. We discuss the\nimplications for privacy and security in LLM-serving infrastructures and\nsuggest considerations for mitigating such threats. For proof of concept we\nconsider two concrete attack scenarios: Our experiments show that an attacker\ncan recover as much as 80%-90% of a high entropy API key with single shot\nmonitoring. As for English text we can reach a 40% recovery rate with a single\nshot. We should note that the rate highly depends on the monitored token set\nand these rates can be improved by targeting more specialized output domains.", "published": "2025-05-01 19:18:56", "link": "http://arxiv.org/abs/2505.00817v1", "categories": ["cs.CR", "cs.AI", "K.6.5"], "primary_category": "cs.CR"}
{"title": "Handling Label Noise via Instance-Level Difficulty Modeling and Dynamic Optimization", "abstract": "Recent studies indicate that deep neural networks degrade in generalization\nperformance under noisy supervision. Existing methods focus on isolating clean\nsubsets or correcting noisy labels, facing limitations such as high\ncomputational costs, heavy hyperparameter tuning process, and coarse-grained\noptimization. To address these challenges, we propose a novel two-stage noisy\nlearning framework that enables instance-level optimization through a\ndynamically weighted loss function, avoiding hyperparameter tuning. To obtain\nstable and accurate information about noise modeling, we introduce a simple yet\neffective metric, termed wrong event, which dynamically models the cleanliness\nand difficulty of individual samples while maintaining computational costs. Our\nframework first collects wrong event information and builds a strong base\nmodel. Then we perform noise-robust training on the base model, using a\nprobabilistic model to handle the wrong event information of samples.\nExperiments on five synthetic and real-world LNL benchmarks demonstrate our\nmethod surpasses state-of-the-art methods in performance, achieves a nearly 75%\nreduction in computational time and improves model scalability.", "published": "2025-05-01 19:12:58", "link": "http://arxiv.org/abs/2505.00812v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "To Repair or Not to Repair? Investigating the Importance of AB-Cycles for the State-of-the-Art TSP Heuristic EAX", "abstract": "The Edge Assembly Crossover (EAX) algorithm is the state-of-the-art heuristic\nfor solving the Traveling Salesperson Problem (TSP). It regularly outperforms\nother methods, such as the Lin-Kernighan-Helsgaun heuristic (LKH), across\ndiverse sets of TSP instances. Essentially, EAX employs a two-stage mechanism\nthat focuses on improving the current solutions, first, at the local and,\nsubsequently, at the global level. Although the second phase of the algorithm\nhas been thoroughly studied, configured, and refined in the past, in\nparticular, its first stage has hardly been examined.\n  In this paper, we thus focus on the first stage of EAX and introduce a novel\nmethod that quickly verifies whether the AB-cycles, generated during its\ninternal optimization procedure, yield valid tours -- or whether they need to\nbe repaired. Knowledge of the latter is also particularly relevant before\napplying other powerful crossover operators such as the Generalized Partition\nCrossover (GPX). Based on our insights, we propose and evaluate several\nimproved versions of EAX. According to our benchmark study across 10 000\ndifferent TSP instances, the most promising of our proposed EAX variants\ndemonstrates improved computational efficiency and solution quality on\npreviously rather difficult instances compared to the current state-of-the-art\nEAX algorithm.", "published": "2025-05-01 19:04:23", "link": "http://arxiv.org/abs/2505.00803v1", "categories": ["cs.NE", "cs.AI"], "primary_category": "cs.NE"}
{"title": "Explanations as Bias Detectors: A Critical Study of Local Post-hoc XAI Methods for Fairness Exploration", "abstract": "As Artificial Intelligence (AI) is increasingly used in areas that\nsignificantly impact human lives, concerns about fairness and transparency have\ngrown, especially regarding their impact on protected groups. Recently, the\nintersection of explainability and fairness has emerged as an important area to\npromote responsible AI systems. This paper explores how explainability methods\ncan be leveraged to detect and interpret unfairness. We propose a pipeline that\nintegrates local post-hoc explanation methods to derive fairness-related\ninsights. During the pipeline design, we identify and address critical\nquestions arising from the use of explanations as bias detectors such as the\nrelationship between distributive and procedural fairness, the effect of\nremoving the protected attribute, the consistency and quality of results across\ndifferent explanation methods, the impact of various aggregation strategies of\nlocal explanations on group fairness evaluations, and the overall\ntrustworthiness of explanations as bias detectors. Our results show the\npotential of explanation methods used for fairness while highlighting the need\nto carefully consider the aforementioned critical aspects.", "published": "2025-05-01 19:03:18", "link": "http://arxiv.org/abs/2505.00802v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Howard's Policy Iteration is Subexponential for Deterministic Markov Decision Problems with Rewards of Fixed Bit-size and Arbitrary Discount Factor", "abstract": "Howard's Policy Iteration (HPI) is a classic algorithm for solving Markov\nDecision Problems (MDPs). HPI uses a \"greedy\" switching rule to update from any\nnon-optimal policy to a dominating one, iterating until an optimal policy is\nfound. Despite its introduction over 60 years ago, the best-known upper bounds\non HPI's running time remain exponential in the number of states -- indeed even\non the restricted class of MDPs with only deterministic transitions (DMDPs).\nMeanwhile, the tightest lower bound for HPI for MDPs with a constant number of\nactions per state is only linear. In this paper, we report a significant\nimprovement: a subexponential upper bound for HPI on DMDPs, which is\nparameterised by the bit-size of the rewards, while independent of the discount\nfactor. The same upper bound also applies to DMDPs with only two possible\nrewards (which may be of arbitrary size).", "published": "2025-05-01 18:50:10", "link": "http://arxiv.org/abs/2505.00795v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Scalable Meta-Learning via Mixed-Mode Differentiation", "abstract": "Gradient-based bilevel optimisation is a powerful technique with applications\nin hyperparameter optimisation, task adaptation, algorithm discovery,\nmeta-learning more broadly, and beyond. It often requires differentiating\nthrough the gradient-based optimisation process itself, leading to\n\"gradient-of-a-gradient\" calculations with computationally expensive\nsecond-order and mixed derivatives. While modern automatic differentiation\nlibraries provide a convenient way to write programs for calculating these\nderivatives, they oftentimes cannot fully exploit the specific structure of\nthese problems out-of-the-box, leading to suboptimal performance. In this\npaper, we analyse such cases and propose Mixed-Flow Meta-Gradients, or\nMixFlow-MG -- a practical algorithm that uses mixed-mode differentiation to\nconstruct more efficient and scalable computational graphs yielding over 10x\nmemory and up to 25% wall-clock time improvements over standard implementations\nin modern meta-learning setups.", "published": "2025-05-01 18:46:44", "link": "http://arxiv.org/abs/2505.00793v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Constructing an Optimal Behavior Basis for the Option Keyboard", "abstract": "Multi-task reinforcement learning aims to quickly identify solutions for new\ntasks with minimal or no additional interaction with the environment.\nGeneralized Policy Improvement (GPI) addresses this by combining a set of base\npolicies to produce a new one that is at least as good -- though not\nnecessarily optimal -- as any individual base policy. Optimality can be\nensured, particularly in the linear-reward case, via techniques that compute a\nConvex Coverage Set (CCS). However, these are computationally expensive and do\nnot scale to complex domains. The Option Keyboard (OK) improves upon GPI by\nproducing policies that are at least as good -- and often better. It achieves\nthis through a learned meta-policy that dynamically combines base policies.\nHowever, its performance critically depends on the choice of base policies.\nThis raises a key question: is there an optimal set of base policies -- an\noptimal behavior basis -- that enables zero-shot identification of optimal\nsolutions for any linear tasks? We solve this open problem by introducing a\nnovel method that efficiently constructs such an optimal behavior basis. We\nshow that it significantly reduces the number of base policies needed to ensure\noptimality in new tasks. We also prove that it is strictly more expressive than\na CCS, enabling particular classes of non-linear tasks to be solved optimally.\nWe empirically evaluate our technique in challenging domains and show that it\noutperforms state-of-the-art approaches, increasingly so as task complexity\nincreases.", "published": "2025-05-01 18:32:21", "link": "http://arxiv.org/abs/2505.00787v1", "categories": ["cs.LG", "cs.AI", "I.2"], "primary_category": "cs.LG"}
{"title": "Fast and Low-Cost Genomic Foundation Models via Outlier Removal", "abstract": "To address the challenge of scarce computational resources in genomic\nmodeling, we introduce GERM, a genomic foundation model with strong compression\nperformance and fast adaptability. GERM improves upon models like DNABERT-2 by\neliminating outliers that hinder low-rank adaptation and post-training\nquantization, enhancing both efficiency and robustness. We replace the vanilla\nattention layer with an outlier-free mechanism inspired by associative memory\nmodels. By removing outliers during both pre-training and fine-tuning, this\napproach accelerates adaptation, reduces computational costs, and enhances\nquantization robustness within acceptable loss margins. Additionally, we\npropose GERM-T, a strategy that employs small-step continual learning within\nthe outlier-free framework, leveraging original checkpoints to avoid retraining\nfrom scratch. Empirically, GERM improves fine-tuning performance by 37.98% and\nquantization by 64.34% over the baseline model. It also reduces average\nkurtosis by 92.14% and maximum infinity norm by 82.77%. Compared to leading\nmethods, GERM consistently delivers superior performance, offering a practical\nsolution for genomic modeling in resource-constrained settings. Code is\navailable at https://github.com/MAGICS-LAB/GERM.", "published": "2025-05-01 15:31:09", "link": "http://arxiv.org/abs/2505.00598v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Are Minimal Radial Distortion Solvers Really Necessary for Relative Pose Estimation?", "abstract": "Estimating the relative pose between two cameras is a fundamental step in\nmany applications such as Structure-from-Motion. The common approach to\nrelative pose estimation is to apply a minimal solver inside a RANSAC loop.\nHighly efficient solvers exist for pinhole cameras. Yet, (nearly) all cameras\nexhibit radial distortion. Not modeling radial distortion leads to\n(significantly) worse results. However, minimal radial distortion solvers are\nsignificantly more complex than pinhole solvers, both in terms of run-time and\nimplementation efforts. This paper compares radial distortion solvers with two\nsimple-to-implement approaches that do not use minimal radial distortion\nsolvers: The first approach combines an efficient pinhole solver with sampled\nradial undistortion parameters, where the sampled parameters are used for\nundistortion prior to applying the pinhole solver. The second approach uses a\nstate-of-the-art neural network to estimate the distortion parameters rather\nthan sampling them from a set of potential values. Extensive experiments on\nmultiple datasets, and different camera setups, show that complex minimal\nradial distortion solvers are not necessary in practice. We discuss under which\nconditions a simple sampling of radial undistortion parameters is preferable\nover calibrating cameras using a learning-based prior approach. Code and newly\ncreated benchmark for relative pose estimation under radial distortion are\navailable at https://github.com/kocurvik/rdnet.", "published": "2025-05-01 21:16:54", "link": "http://arxiv.org/abs/2505.00866v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "The Comparability of Model Fusion to Measured Data in Confuser Rejection", "abstract": "Data collection has always been a major issue in the modeling and training of\nlarge deep learning networks, as no dataset can account for every slight\ndeviation we might see in live usage. Collecting samples can be especially\ncostly for Synthetic Aperture Radar (SAR), limiting the amount of unique\ntargets and operating conditions we are able to observe from. To counter this\nlack of data, simulators have been developed utilizing the shooting and\nbouncing ray method to allow for the generation of synthetic SAR data on 3D\nmodels. While effective, the synthetically generated data does not perfectly\ncorrelate to the measured data leading to issues when training models solely on\nsynthetic data. We aim to use computational power as a substitution for this\nlack of quality measured data, by ensembling many models trained on synthetic\ndata. Synthetic data is also not complete, as we do not know what targets might\nbe present in a live environment. Therefore we need to have our ensembling\ntechniques account for these unknown targets by applying confuser rejection in\nwhich our models will reject unknown targets it is presented with, and only\nclassify those it has been trained on.", "published": "2025-05-01 19:51:30", "link": "http://arxiv.org/abs/2505.00836v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Advancing Wheat Crop Analysis: A Survey of Deep Learning Approaches Using Hyperspectral Imaging", "abstract": "As one of the most widely cultivated and consumed crops, wheat is essential\nto global food security. However, wheat production is increasingly challenged\nby pests, diseases, climate change, and water scarcity, threatening yields.\nTraditional crop monitoring methods are labor-intensive and often ineffective\nfor early issue detection. Hyperspectral imaging (HSI) has emerged as a\nnon-destructive and efficient technology for remote crop health assessment.\nHowever, the high dimensionality of HSI data and limited availability of\nlabeled samples present notable challenges. In recent years, deep learning has\nshown great promise in addressing these challenges due to its ability to\nextract and analysis complex structures. Despite advancements in applying deep\nlearning methods to HSI data for wheat crop analysis, no comprehensive survey\ncurrently exists in this field. This review addresses this gap by summarizing\nbenchmark datasets, tracking advancements in deep learning methods, and\nanalyzing key applications such as variety classification, disease detection,\nand yield estimation. It also highlights the strengths, limitations, and future\nopportunities in leveraging deep learning methods for HSI-based wheat crop\nanalysis. We have listed the current state-of-the-art papers and will continue\ntracking updating them in the following\nhttps://github.com/fadi-07/Awesome-Wheat-HSI-DeepLearning.", "published": "2025-05-01 19:07:28", "link": "http://arxiv.org/abs/2505.00805v1", "categories": ["cs.CV", "eess.IV"], "primary_category": "cs.CV"}
{"title": "SpatialLLM: A Compound 3D-Informed Design towards Spatially-Intelligent Large Multimodal Models", "abstract": "Humans naturally understand 3D spatial relationships, enabling complex\nreasoning like predicting collisions of vehicles from different directions.\nCurrent large multimodal models (LMMs), however, lack of this capability of 3D\nspatial reasoning. This limitation stems from the scarcity of 3D training data\nand the bias in current model designs toward 2D data. In this paper, we\nsystematically study the impact of 3D-informed data, architecture, and training\nsetups, introducing SpatialLLM, a large multi-modal model with advanced 3D\nspatial reasoning abilities. To address data limitations, we develop two types\nof 3D-informed training datasets: (1) 3D-informed probing data focused on\nobject's 3D location and orientation, and (2) 3D-informed conversation data for\ncomplex spatial relationships. Notably, we are the first to curate VQA data\nthat incorporate 3D orientation relationships on real images. Furthermore, we\nsystematically integrate these two types of training data with the\narchitectural and training designs of LMMs, providing a roadmap for optimal\ndesign aimed at achieving superior 3D reasoning capabilities. Our SpatialLLM\nadvances machines toward highly capable 3D-informed reasoning, surpassing\nGPT-4o performance by 8.7%. Our systematic empirical design and the resulting\nfindings offer valuable insights for future research in this direction.", "published": "2025-05-01 18:36:17", "link": "http://arxiv.org/abs/2505.00788v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "AI-ready Snow Radar Echogram Dataset (SRED) for climate change monitoring", "abstract": "Tracking internal layers in radar echograms with high accuracy is essential\nfor understanding ice sheet dynamics and quantifying the impact of accelerated\nice discharge in Greenland and other polar regions due to contemporary global\nclimate warming. Deep learning algorithms have become the leading approach for\nautomating this task, but the absence of a standardized and well-annotated\nechogram dataset has hindered the ability to test and compare algorithms\nreliably, limiting the advancement of state-of-the-art methods for the radar\nechogram layer tracking problem. This study introduces the first comprehensive\n``deep learning ready'' radar echogram dataset derived from Snow Radar airborne\ndata collected during the National Aeronautics and Space Administration\nOperation Ice Bridge (OIB) mission in 2012. The dataset contains 13,717 labeled\nand 57,815 weakly-labeled echograms covering diverse snow zones (dry, ablation,\nwet) with varying along-track resolutions. To demonstrate its utility, we\nevaluated the performance of five deep learning models on the dataset. Our\nresults show that while current computer vision segmentation algorithms can\nidentify and track snow layer pixels in echogram images, advanced end-to-end\nmodels are needed to directly extract snow depth and annual accumulation from\nechograms, reducing or eliminating post-processing. The dataset and\naccompanying benchmarking framework provide a valuable resource for advancing\nradar echogram layer tracking and snow accumulation estimation, advancing our\nunderstanding of polar ice sheets response to climate warming.", "published": "2025-05-01 18:29:36", "link": "http://arxiv.org/abs/2505.00786v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Person detection and re-identification in open-world settings of retail stores and public spaces", "abstract": "Practical applications of computer vision in smart cities usually assume\nsystem integration and operation in challenging open-world environments. In the\ncase of person re-identification task the main goal is to retrieve information\nwhether the specific person has appeared in another place at a different time\ninstance of the same video, or over multiple camera feeds. This typically\nassumes collecting raw data from video surveillance cameras in different places\nand under varying illumination conditions. In the considered open-world setting\nit also requires detection and localization of the person inside the analyzed\nvideo frame before the main re-identification step. With multi-person and\nmulti-camera setups the system complexity becomes higher, requiring\nsophisticated tracking solutions and re-identification models. In this work we\nwill discuss existing challenges in system design architectures, consider\npossible solutions based on different computer vision techniques, and describe\napplications of such systems in retail stores and public spaces for improved\nmarketing analytics. In order to analyse sensitivity of person\nre-identification task under different open-world environments, a performance\nof one close to real-time solution will be demonstrated over several video\ncaptures and live camera feeds. Finally, based on conducted experiments we will\nindicate further research directions and possible system improvements.", "published": "2025-05-01 18:04:53", "link": "http://arxiv.org/abs/2505.00772v1", "categories": ["cs.CV", "68T10, 68T07, 68T45, 94A08, 94A13,", "I.4.9; I.4.8; I.5.4; I.5.5; I.2.10; C.3; J.4; J.7"], "primary_category": "cs.CV"}
{"title": "Multimodal Masked Autoencoder Pre-training for 3D MRI-Based Brain Tumor Analysis with Missing Modalities", "abstract": "Multimodal magnetic resonance imaging (MRI) constitutes the first line of\ninvestigation for clinicians in the care of brain tumors, providing crucial\ninsights for surgery planning, treatment monitoring, and biomarker\nidentification. Pre-training on large datasets have been shown to help models\nlearn transferable representations and adapt with minimal labeled data. This\nbehavior is especially valuable in medical imaging, where annotations are often\nscarce. However, applying this paradigm to multimodal medical data introduces a\nchallenge: most existing approaches assume that all imaging modalities are\navailable during both pre-training and fine-tuning. In practice, missing\nmodalities often occur due to acquisition issues, specialist unavailability, or\nspecific experimental designs on small in-house datasets. Consequently, a\ncommon approach involves training a separate model for each desired modality\ncombination, making the process both resource-intensive and impractical for\nclinical use. Therefore, we introduce BM-MAE, a masked image modeling\npre-training strategy tailored for multimodal MRI data. The same pre-trained\nmodel seamlessly adapts to any combination of available modalities, extracting\nrich representations that capture both intra- and inter-modal information. This\nallows fine-tuning on any subset of modalities without requiring architectural\nchanges, while still benefiting from a model pre-trained on the full set of\nmodalities. Extensive experiments show that the proposed pre-training strategy\noutperforms or remains competitive with baselines that require separate\npre-training for each modality subset, while substantially surpassing training\nfrom scratch on several downstream tasks. Additionally, it can quickly and\nefficiently reconstruct missing modalities, highlighting its practical value.\nCode and trained models are available at: https://github.com/Lucas-rbnt/BM-MAE", "published": "2025-05-01 14:51:30", "link": "http://arxiv.org/abs/2505.00568v2", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "InterLoc: LiDAR-based Intersection Localization using Road Segmentation with Automated Evaluation Method", "abstract": "Online localization of road intersections is beneficial for autonomous\nvehicle localization, mapping and motion planning. Intersections offer strong\nlandmarks to correct vehicle pose estimation in GNSS dropouts and anchor new\nsensor data in up-to-date maps. They are also decisive routing nodes in road\nnetwork graphs. Despite that importance, intersection localization has not been\nwidely studied, with existing methods either ignore the rich semantic\ninformation already computed onboard or depend on scarce, hand-labeled\nintersection datasets. To close that gap, this paper presents a LiDAR-based\nmethod for online vehicle-centric intersection localization. We fuse semantic\nroad segmentation with vehicle local pose to detect intersection candidates in\na bird's eye view (BEV) representation. We then refine those candidates by\nanalyzing branch topology and correcting the intersection point in a least\nsquares formulation. To evaluate our method, we introduce an automated\nbenchmarking pipeline that pairs localized intersection points with\nOpenStreetMap (OSM) intersection nodes using precise GNSS/INS ground-truth\nposes. Experiments on SemanticKITTI show that the method outperforms the latest\nlearning-based baseline in accuracy and reliability. Moreover, sensitivity\ntests demonstrate that our method is robust to challenging segmentation error\nlevels, highlighting its applicability in the real world.", "published": "2025-05-01 13:30:28", "link": "http://arxiv.org/abs/2505.00512v2", "categories": ["cs.CV", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Efficient On-Chip Implementation of 4D Radar-Based 3D Object Detection on Hailo-8L", "abstract": "4D radar has attracted attention in autonomous driving due to its ability to\nenable robust 3D object detection even under adverse weather conditions. To\npractically deploy such technologies, it is essential to achieve real-time\nprocessing within low-power embedded environments. Addressing this, we present\nthe first on-chip implementation of a 4D radar-based 3D object detection model\non the Hailo-8L AI accelerator. Although conventional 3D convolutional neural\nnetwork (CNN) architectures require 5D inputs, the Hailo-8L only supports 4D\ntensors, posing a significant challenge. To overcome this limitation, we\nintroduce a tensor transformation method that reshapes 5D inputs into 4D\nformats during the compilation process, enabling direct deployment without\naltering the model structure. The proposed system achieves 46.47% AP_3D and\n52.75% AP_BEV, maintaining comparable accuracy to GPU-based models while\nachieving an inference speed of 13.76 Hz. These results demonstrate the\napplicability of 4D radar-based perception technologies to autonomous driving\nsystems.", "published": "2025-05-01 12:10:04", "link": "http://arxiv.org/abs/2505.00757v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "P2P-Insole: Human Pose Estimation Using Foot Pressure Distribution and Motion Sensors", "abstract": "This work presents P2P-Insole, a low-cost approach for estimating and\nvisualizing 3D human skeletal data using insole-type sensors integrated with\nIMUs. Each insole, fabricated with e-textile garment techniques, costs under\nUSD 1, making it significantly cheaper than commercial alternatives and ideal\nfor large-scale production. Our approach uses foot pressure distribution,\nacceleration, and rotation data to overcome limitations, providing a\nlightweight, minimally intrusive, and privacy-aware solution. The system\nemploys a Transformer model for efficient temporal feature extraction, enriched\nby first and second derivatives in the input stream. Including multimodal\ninformation, such as accelerometers and rotational measurements, improves the\naccuracy of complex motion pattern recognition. These facts are demonstrated\nexperimentally, while error metrics show the robustness of the approach in\nvarious posture estimation tasks. This work could be the foundation for a\nlow-cost, practical application in rehabilitation, injury prevention, and\nhealth monitoring while enabling further development through sensor\noptimization and expanded datasets.", "published": "2025-05-01 09:28:29", "link": "http://arxiv.org/abs/2505.00755v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "DARTer: Dynamic Adaptive Representation Tracker for Nighttime UAV Tracking", "abstract": "Nighttime UAV tracking presents significant challenges due to extreme\nillumination variations and viewpoint changes, which severely degrade tracking\nperformance. Existing approaches either rely on light enhancers with high\ncomputational costs or introduce redundant domain adaptation mechanisms,\nfailing to fully utilize the dynamic features in varying perspectives. To\naddress these issues, we propose \\textbf{DARTer} (\\textbf{D}ynamic\n\\textbf{A}daptive \\textbf{R}epresentation \\textbf{T}racker), an end-to-end\ntracking framework designed for nighttime UAV scenarios. DARTer leverages a\nDynamic Feature Blender (DFB) to effectively fuse multi-perspective nighttime\nfeatures from static and dynamic templates, enhancing representation\nrobustness. Meanwhile, a Dynamic Feature Activator (DFA) adaptively activates\nVision Transformer layers based on extracted features, significantly improving\nefficiency by reducing redundant computations. Our model eliminates the need\nfor complex multi-task loss functions, enabling a streamlined training process.\nExtensive experiments on multiple nighttime UAV tracking benchmarks demonstrate\nthe superiority of DARTer over state-of-the-art trackers. These results confirm\nthat DARTer effectively balances tracking accuracy and efficiency, making it a\npromising solution for real-world nighttime UAV tracking applications.", "published": "2025-05-01 05:24:14", "link": "http://arxiv.org/abs/2505.00752v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "InstructAttribute: Fine-grained Object Attributes editing with Instruction", "abstract": "Text-to-image (T2I) diffusion models, renowned for their advanced generative\nabilities, are extensively utilized in image editing applications,\ndemonstrating remarkable effectiveness. However, achieving precise control over\nfine-grained attributes still presents considerable challenges. Existing image\nediting techniques either fail to modify the attributes of an object or\nstruggle to preserve its structure and maintain consistency in other areas of\nthe image. To address these challenges, we propose the Structure-Preserving and\nAttribute Amplification (SPAA), a training-free method which enables precise\ncontrol over the color and material transformations of objects by editing the\nself-attention maps and cross-attention values. Furthermore, we constructed the\nAttribute Dataset, which encompasses nearly all colors and materials associated\nwith various objects, by integrating multimodal large language models (MLLM) to\ndevelop an automated pipeline for data filtering and instruction labeling.\nTraining on this dataset, we present our InstructAttribute, an\ninstruction-based model designed to facilitate fine-grained editing of color\nand material attributes. Extensive experiments demonstrate that our method\nachieves superior performance in object-level color and material editing,\noutperforming existing instruction-based image editing approaches.", "published": "2025-05-01 03:24:28", "link": "http://arxiv.org/abs/2505.00751v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Empowering Agentic Video Analytics Systems with Video Language Models", "abstract": "AI-driven video analytics has become increasingly pivotal across diverse\ndomains. However, existing systems are often constrained to specific,\npredefined tasks, limiting their adaptability in open-ended analytical\nscenarios. The recent emergence of Video-Language Models (VLMs) as\ntransformative technologies offers significant potential for enabling\nopen-ended video understanding, reasoning, and analytics. Nevertheless, their\nlimited context windows present challenges when processing ultra-long video\ncontent, which is prevalent in real-world applications. To address this, we\nintroduce AVAS, a VLM-powered system designed for open-ended, advanced video\nanalytics. AVAS incorporates two key innovations: (1) the near real-time\nconstruction of Event Knowledge Graphs (EKGs) for efficient indexing of long or\ncontinuous video streams, and (2) an agentic retrieval-generation mechanism\nthat leverages EKGs to handle complex and diverse queries. Comprehensive\nevaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate that\nAVAS achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy,\nrespectively, significantly surpassing existing VLM and video\nRetrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate video\nanalytics in ultra-long and open-world video scenarios, we introduce a new\nbenchmark, AVAS-100. This benchmark comprises 8 videos, each exceeding 10 hours\nin duration, along with 120 manually annotated, diverse, and complex\nquestion-answer pairs. On AVAS-100, AVAS achieves top-tier performance with an\naccuracy of 75.8%.", "published": "2025-05-01 02:40:23", "link": "http://arxiv.org/abs/2505.00254v2", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Cluster deletion and clique partitioning in graphs with bounded clique number", "abstract": "The Cluster Deletion problem takes a graph $G$ as input and asks for a\nminimum size set of edges $X$ such that $G-X$ is the disjoint union of complete\ngraphs. An equivalent formulation is the Clique Partition problem, which asks\nto find a partition of $V(G)$ into cliques such that the total number of edges\nis maximized.\n  We begin by giving a much simpler proof of a theorem of Gao, Hare, and Nastos\nthat Cluster Deletion is efficiently solvable on the class of cographs. We then\ninvestigate Cluster Deletion and Clique Partition on permutation graphs, which\nare a superclass of cographs. Our findings suggest that Cluster Deletion may be\nNP-hard on permutation graphs.\n  Finally, we prove that for graphs with clique number at most $c$, there is a\n$\\frac{2\\binom{c}{2}}{\\binom{c}{2}+1}$-approximation algorithm for Clique\nPartition. This is the first polynomial time algorithm which achieves an\napproximation ratio better than 2 for graphs with bounded clique number. More\ngenerally, our algorithm runs in polynomial time on any graph class for which\nMaximum Clique can be computed in polynomial time. We also provide a class of\nexamples which shows that our approximation ratio is best possible.", "published": "2025-05-01 23:45:07", "link": "http://arxiv.org/abs/2505.00922v1", "categories": ["cs.DS", "cs.DM", "math.CO", "05C85, 05C69, 90C27", "G.2.2; F.2.0"], "primary_category": "cs.DS"}
{"title": "Rigidity of polytopes with edge length and coplanarity constraints", "abstract": "We investigate a novel setting for polytope rigidity, where a flex must\npreserve edge lengths and the planarity of faces, but is allowed to change the\nshapes of faces. For instance, the regular cube is flexible in this notion. We\npresent techniques for constructing flexible polytopes and find that\nflexibility seems to be an exceptional property. Based on this observation, we\nintroduce a notion of generic realizations for polytopes and conjecture that\nconvex polytopes are generically rigid in dimension $d\\geq 3$. We prove this\nconjecture in dimension $d=3$. Motivated by our findings we also pose several\nquestions that are intended to inspire future research into this notion of\npolytope rigidity.", "published": "2025-05-01 21:36:09", "link": "http://arxiv.org/abs/2505.00874v1", "categories": ["math.CO", "cs.DM", "math.MG", "51M20, 52C25, 52B11"], "primary_category": "math.CO"}
{"title": "Prime Integer Matrices", "abstract": "This paper introduces prime integer matrices and its properties. It provides\na simple way to construct families of pairwise co-prime integer matrices, that\nmay have applications in multidimensional co-prime sensing and multidimensional\nChinese remainder theorem.", "published": "2025-05-01 21:04:32", "link": "http://arxiv.org/abs/2505.00862v1", "categories": ["eess.SP", "cs.DM", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "Non-Adaptive Cryptanalytic Time-Space Lower Bounds via a Shearer-like Inequality for Permutations", "abstract": "The power of adaptivity in algorithms has been intensively studied in diverse\nareas of theoretical computer science. In this paper, we obtain a number of\nsharp lower bound results which show that adaptivity provides a significant\nextra power in cryptanalytic time-space tradeoffs with (possibly unlimited)\npreprocessing time.\n  Most notably, we consider the discrete logarithm (DLOG) problem in a generic\ngroup of $N$ elements. The classical `baby-step giant-step' algorithm for the\nproblem has time complexity $T=O(\\sqrt{N})$, uses $O(\\sqrt{N})$ bits of space\n(up to logarithmic factors in $N$) and achieves constant success probability.\n  We examine a generalized setting where an algorithm obtains an advice string\nof $S$ bits and is allowed to make $T$ arbitrary non-adaptive queries that\ndepend on the advice string (but not on the challenge group element).\n  We show that in this setting, the $T=O(\\sqrt{N})$ online time complexity of\nthe baby-step giant-step algorithm cannot be improved, unless the advice string\nis more than $\\Omega(\\sqrt{N})$ bits long. This lies in stark contrast with the\nclassical adaptive Pollard's rho algorithm for DLOG, which can exploit\npreprocessing to obtain the tradeoff curve $ST^2=O(N)$. We obtain similar sharp\nlower bounds for several other cryptanalytic problems.\n  To obtain our results, we present a new model that allows analyzing\nnon-adaptive preprocessing algorithms for a wide array of search and decision\nproblems in a unified way. Since previous proof techniques inherently cannot\ndistinguish between adaptive and non-adaptive algorithms for the problems in\nour model, they cannot be used to obtain our results. Consequently, our proof\nuses a variant of Shearer's lemma for this setting, due to Barthe,\nCordero-Erausquin, Ledoux, and Maurey (2011). This seems to be the first time a\nvariant of Shearer's lemma for permutations is used in an algorithmic context.", "published": "2025-05-01 22:17:11", "link": "http://arxiv.org/abs/2505.00894v1", "categories": ["cs.CR", "cs.IT", "math.IT"], "primary_category": "cs.CR"}
{"title": "The Alicki-Fannes-Winter technique in the quasi-classical settings: advanced version and its applications", "abstract": "We describe an advanced version of the AFW-technique proposed in [Lett. Math.\nPhys., 113, 121 (2023)],[Lobachevskii J. Math., 44(6), 2169 (2023)] which\nallows us to obtain lower semicontinuity bounds, continuity bounds and local\nlower bounds for characteristics of quantum systems and discrete random\nvariables.\n  We consider applications of the new version of the AFW-technique to several\nbasic characteristics of quantum systems (the von Neumann entropy, the\nenergy-type functionals, the quantum relative entropy, the conditional entropy\nand the entanglement of formation).", "published": "2025-05-01 21:57:25", "link": "http://arxiv.org/abs/2505.00882v1", "categories": ["quant-ph", "cs.IT", "math-ph", "math.IT", "math.MP"], "primary_category": "quant-ph"}
{"title": "A Single-Bit Redundancy Framework for Multi-Dimensional Parametric Constraints", "abstract": "Constrained coding plays a key role in optimizing performance and mitigating\nerrors in applications such as storage and communication, where specific\nconstraints on codewords are required. While non-parametric constraints have\nbeen well-studied, parametric constraints, which depend on sequence length,\nhave traditionally been tackled with ad hoc solutions. Recent advances have\nintroduced unified methods for parametric constrained coding. This paper\nextends these approaches to multidimensional settings, generalizing an\niterative framework to efficiently encode arrays subject to parametric\nconstraints. We demonstrate the application of the method to existing and new\nconstraints, highlighting its versatility and potential for advanced storage\nsystems.", "published": "2025-05-01 21:27:28", "link": "http://arxiv.org/abs/2505.00869v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Deep Autoencoder-Based Constellation Design in Multiple Access Channels", "abstract": "In multiple access channels (MAC), multiple users share a transmission medium\nto communicate with a common receiver. Traditional constellations like\nquadrature amplitude modulation are optimized for point-to-point systems and\nlack mechanisms to mitigate inter-user interference, leading to suboptimal\nperformance in MAC environments. To address this, we propose a novel framework\nfor constellation design in MAC that employs deep autoencoder (DAE)-based\ncommunication systems. This approach intelligently creates flexible\nconstellations aware of inter-user interference, reducing symbol error rate and\nenhancing the constellation-constrained sum capacity of the channel.\nComparisons against analytically derived constellations demonstrate that\nDAE-designed constellations consistently perform best or equal to the best\nacross various system parameters. Furthermore, we apply the DAE to scenarios\nwhere no analytical solutions have been developed, such as with more than two\nusers, demonstrating the adaptability of the model.", "published": "2025-05-01 21:24:12", "link": "http://arxiv.org/abs/2505.00868v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Improved Approximation of Sensor Network Performance for Seabed Acoustic Sensors", "abstract": "Sensor locations to detect Poisson-distributed targets, such as seabed\nsensors that detect shipping traffic, can be selected to maximize the so-called\nvoid probability, which is the probability of detecting all targets. Because\nevaluation of void probability is computationally expensive, we propose a new\napproximation of void probability that can greatly reduce the computational\ncost of selecting locations for a network of sensors. We build upon prior work\nthat approximates void probability using Jensen's inequality. Our new approach\nbetter accommodates uncertainty in the (Poisson) target model and yields a\nsharper error bound. The proposed method is evaluated using historical ship\ntraffic data from the Hampton Roads Channel, Virginia, demonstrating a\nreduction in the approximation error compared to the previous approach. The\nresults validate the effectiveness of the improved approximation for maritime\nsurveillance applications.", "published": "2025-05-01 19:05:03", "link": "http://arxiv.org/abs/2505.00804v1", "categories": ["cs.RO", "cs.IT", "math.IT"], "primary_category": "cs.RO"}
{"title": "AI-based CSI Feedback with Digital Twins: Real-World Validation and Insights", "abstract": "Deep learning (DL) has shown great potential for enhancing channel state\ninformation (CSI) feedback in multiple-input multiple-output (MIMO)\ncommunication systems, a subject currently under study by the 3GPP standards\nbody. Digital twins (DTs) have emerged as an effective means to generate\nsite-specific datasets for training DL-based CSI feedback models. However, most\nexisting studies rely solely on simulations, leaving the effectiveness of DTs\nin reducing DL training costs yet to be validated through realistic\nexperimental setups. This paper addresses this gap by establishing a real-world\n(RW) environment and corresponding virtual channels using ray tracing with\nreplicated 3D models and accurate antenna properties. We evaluate whether\nmodels trained in DT environments can effectively operate in RW scenarios and\nquantify the benefits of online learning (OL) for performance enhancement.\nResults show that a dedicated DT remains essential even with OL to achieve\nsatisfactory performance in RW scenarios.", "published": "2025-05-01 17:02:22", "link": "http://arxiv.org/abs/2505.00660v2", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Gaussian Process Policy Iteration with Additive Schwarz Acceleration for Forward and Inverse HJB and Mean Field Game Problems", "abstract": "We propose a Gaussian Process (GP)-based policy iteration framework for\naddressing both forward and inverse problems in Hamilton--Jacobi--Bellman (HJB)\nequations and mean field games (MFGs). Policy iteration is formulated as an\nalternating procedure between solving the value function under a fixed control\npolicy and updating the policy based on the resulting value function. By\nexploiting the linear structure of GPs for function approximation, each policy\nevaluation step admits an explicit closed-form solution, eliminating the need\nfor numerical optimization. To improve convergence, we incorporate the additive\nSchwarz acceleration as a preconditioning step following each policy update.\nNumerical experiments demonstrate the effectiveness of Schwarz acceleration in\nimproving computational efficiency.", "published": "2025-05-01 23:04:52", "link": "http://arxiv.org/abs/2505.00909v1", "categories": ["cs.LG", "math.OC"], "primary_category": "cs.LG"}
{"title": "Learning Neural Control Barrier Functions from Offline Data with Conservatism", "abstract": "Safety filters, particularly those based on control barrier functions, have\ngained increased interest as effective tools for safe control of dynamical\nsystems. Existing correct-by-construction synthesis algorithms, however, suffer\nfrom the curse of dimensionality. Deep learning approaches have been proposed\nin recent years to address this challenge. In this paper, we contribute to this\nline of work by proposing an algorithm for training control barrier functions\nfrom offline datasets. Our algorithm trains the filter to not only prevent the\nsystem from reaching unsafe states but also out-of-distribution ones, at which\nthe filter would be unreliable. It is inspired by Conservative Q-learning, an\noffline reinforcement learning algorithm. We call its outputs Conservative\nControl Barrier Functions (CCBFs). Our empirical results demonstrate that CCBFs\noutperform existing methods in maintaining safety and out-of-distribution\navoidance while minimally affecting task performance.", "published": "2025-05-01 23:01:03", "link": "http://arxiv.org/abs/2505.00908v1", "categories": ["cs.LG", "cs.FL", "cs.RO", "cs.SY", "eess.SY"], "primary_category": "cs.LG"}
{"title": "A stabilized march approach to adjoint-based sensitivity analysis of chaotic flows", "abstract": "Adjoint-based sensitivity analysis is of interest in computational science\ndue to its ability to compute sensitivities at a lower cost with respect to\nseveral design parameters. However, conventional sensitivity analysis methods\nfail in the presence of chaotic flows. Popular approaches to chaotic\nsensitivity analysis of flows involve the use of the shadowing trajectory. The\nstate-of-the-art approach computes the shadowing trajectory by solving a least\nsquares minimization problem, resulting in a space-time linear system of\nequations. The current paper computes the adjoint shadowing trajectory using\nthe stabilized march, by specifying the adjoint boundary conditions instead of\nsolving a minimization problem. This approach results in a space-time linear\nsystem that can be solved through a single backward substitution of order\n$\\mathcal{O}(n_u^2)$ with $n_u$ being the dimension of the unstable subspace.\nIt is proven to compute sensitivities that converge to the true sensitivity for\nlarge integration times and that the error in the sensitivity due to the\ndiscretization is of the order of the local truncation error of the scheme. The\napproach is numerically verified on the Lorentz 63 and Kuramoto-Sivasinsky\nequations.", "published": "2025-05-01 19:54:32", "link": "http://arxiv.org/abs/2505.00838v1", "categories": ["math.NA", "cs.NA", "math.OC", "34A34, 37A99, 37D20, 37D45, 37N30, 46N40, 65P99, 76F20"], "primary_category": "math.NA"}
{"title": "New Smoothness Indicator Within an Active Flux Framework", "abstract": "In this work, we introduce a new smoothness indicator (SI), which is capable\nof detecting ``rough'' parts of the solutions computed by active flux (AF)\nmethods for hyperbolic (systems of) conservation laws. The new SI is based on\nmeasuring the difference between the two sets of solutions (either cell\naverages and point values or cell averages on overlapping grids) evolved at\neach time step of AF methods. The key idea in the derivation of the new SI is\nthat in the ``rough'' parts of the evolved solutions, the difference is ${\\cal\nO}(1)$, while in the smooth areas, it is proportional to the order of the\nunderlying AF method. The performance of the new SI, that is, its ability to\nautomatically and robustly detect ``rough'' parts of the computed solutions, is\nillustrated on several numerical examples, in which the one-dimensional Euler\nequations of gas dynamics are numerically solved by a recently introduced\nsemi-discrete finite-volume AF method on overlapping grids.", "published": "2025-05-01 19:08:37", "link": "http://arxiv.org/abs/2505.00809v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "A New Semi-Discrete Finite-Volume Active Flux Method for Hyperbolic Conservation Laws", "abstract": "In this work, we introduce a new active flux (AF) method for hyperbolic\nsystems of conservation laws. Following an AF approach recently proposed in\n[{\\sc R. Abgrall}, Commun. Appl. Math. Comput., 5 (2023), pp. 370--402], we\nconsider two different formulations of the studied system (the original\nconservative formulation and a primitive one containing nonconservative\nproducts), and discretize them on overlapping staggered meshes using two\ndifferent numerical schemes. The novelty of our method is twofold. First, we\nintroduce an original paradigm making use of overlapping finite-volume (FV)\nmeshes over which cell averages of conservative and primitive variables are\nevolved using semi-discrete FV methods: The nonconservative system is\ndiscretized by a path-conservative central-upwind scheme and its solution is\nused to evaluate very simple numerical fluxes for the discretization of the\noriginal conservative system. Second, to ensure the nonlinear stability of the\nresulting AF method, we design a post-processing, which also guarantees a\nconservative coupling between the two sets of variables. We test the proposed\nsemi-discrete FV AF method on a number of benchmarks for the one- and\ntwo-dimensional Euler equations of gas dynamics.", "published": "2025-05-01 18:58:37", "link": "http://arxiv.org/abs/2505.00798v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Mixed Precision Orthogonalization-Free Projection Methods for Eigenvalue and Singular Value Problems", "abstract": "Mixed-precision arithmetic offers significant computational advantages for\nlarge-scale matrix computation tasks, yet preserving accuracy and stability in\neigenvalue problems and the singular value decomposition (SVD) remains\nchallenging. This paper introduces an approach that eliminates\northogonalization requirements in traditional Rayleigh-Ritz projection methods.\nThe proposed method employs non-orthogonal bases computed at reduced precision,\nresulting in bases computed without inner-products. A primary focus is on\nmaintaining the linear independence of the basis vectors. Through extensive\nevaluation with both synthetic test cases and real-world applications, we\ndemonstrate that the proposed approach achieves the desired accuracy while\nfully taking advantage of mixed-precision arithmetic.", "published": "2025-05-01 04:05:58", "link": "http://arxiv.org/abs/2505.00281v2", "categories": ["math.NA", "cs.NA", "15A23, 65F25, 65Y05, 68W10"], "primary_category": "math.NA"}
{"title": "On the emergence of numerical instabilities in Next Generation Reservoir Computing", "abstract": "Next Generation Reservoir Computing (NGRC) is a low-cost machine learning\nmethod for forecasting chaotic time series from data. However, ensuring the\ndynamical stability of NGRC models during autonomous prediction remains a\nchallenge. In this work, we uncover a key connection between the numerical\nconditioning of the NGRC feature matrix -- formed by polynomial evaluations on\ntime-delay coordinates -- and the long-term NGRC dynamics. Merging tools from\nnumerical linear algebra and ergodic theory of dynamical systems, we\nsystematically study how the feature matrix conditioning varies across\nhyperparameters. We demonstrate that the NGRC feature matrix tends to be\nill-conditioned for short time lags and high-degree polynomials.\nIll-conditioning amplifies sensitivity to training data perturbations, which\ncan produce unstable NGRC dynamics. We evaluate the impact of different\nnumerical algorithms (Cholesky, SVD, and LU) for solving the regularized\nleast-squares problem.", "published": "2025-05-01 20:16:44", "link": "http://arxiv.org/abs/2505.00846v1", "categories": ["stat.ML", "cs.LG", "math.DS", "physics.data-an", "37M10, 62M10, 65F22,"], "primary_category": "stat.ML"}
{"title": "Multi-site modelling and reconstruction of past extreme skew surges along the French Atlantic coast", "abstract": "Appropriate modelling of extreme skew surges is crucial, particularly for\ncoastal risk management. Our study focuses on modelling extreme skew surges\nalong the French Atlantic coast, with a particular emphasis on investigating\nthe extremal dependence structure between stations. We employ the\npeak-over-threshold framework, where a multivariate extreme event is defined\nwhenever at least one location records a large value, though not necessarily\nall stations simultaneously. A novel method for determining an appropriate\nlevel (threshold) above which observations can be classified as extreme is\nproposed. Two complementary approaches are explored. First, the multivariate\ngeneralized Pareto distribution is employed to model extremes, leveraging its\nproperties to derive a generative model that predicts extreme skew surges at\none station based on observed extremes at nearby stations. Second, a novel\nextreme regression framework is assessed for point predictions. This specific\nregression framework enables accurate point predictions using only the \"angle\"\nof input variables, i.e. input variables divided by their norms. The ultimate\nobjective is to reconstruct historical skew surge time series at stations with\nlimited data. This is achieved by integrating extreme skew surge data from\nstations with longer records, such as Brest and Saint-Nazaire, which provide\nover 150 years of observations.", "published": "2025-05-01 19:51:26", "link": "http://arxiv.org/abs/2505.00835v1", "categories": ["stat.AP", "cs.LG", "stat.ML"], "primary_category": "stat.AP"}
{"title": "Q-Learning with Clustered-SMART (cSMART) Data: Examining Moderators in the Construction of Clustered Adaptive Interventions", "abstract": "A clustered adaptive intervention (cAI) is a pre-specified sequence of\ndecision rules that guides practitioners on how best - and based on which\nmeasures - to tailor cluster-level intervention to improve outcomes at the\nlevel of individuals within the clusters. A clustered sequential multiple\nassignment randomized trial (cSMART) is a type of trial that is used to inform\nthe empirical development of a cAI. The most common type of secondary aim in a\ncSMART focuses on assessing causal effect moderation by candidate tailoring\nvariables. We introduce a clustered Q-learning framework with the M-out-of-N\nCluster Bootstrap using data from a cSMART to evaluate whether a set of\ncandidate tailoring variables may be useful in defining an optimal cAI. This\napproach could construct confidence intervals (CI) with near-nominal coverage\nto assess parameters indexing the causal effect moderation function.\nSpecifically, it allows reliable inferences concerning the utility of candidate\ntailoring variables in constructing a cAI that maximizes a mean end-of-study\noutcome even when \"non-regularity\", a well-known challenge exists. Simulations\ndemonstrate the numerical performance of the proposed method across varying\nnon-regularity conditions and investigate the impact of varying number of\nclusters and intra-cluster correlation coefficient on CI coverage. Methods are\napplied on ADEPT dataset to inform the construction of a clinic-level cAI for\nimproving evidence-based practice in treating mood disorders.", "published": "2025-05-01 19:24:39", "link": "http://arxiv.org/abs/2505.00822v1", "categories": ["stat.ME", "cs.LG", "stat.ML"], "primary_category": "stat.ME"}
{"title": "Physics-Informed Neural Network-Driven Sparse Field Discretization Method for Near-Field Acoustic Holography", "abstract": "We propose the Physics-Informed Neural Network-driven Sparse Field\nDiscretization method (PINN-SFD), a novel self-supervised, physics-informed\ndeep learning approach for addressing the Near-Field Acoustic Holography (NAH)\nproblem. Unlike existing deep learning methods for NAH, which are predominantly\nsupervised by large datasets, our approach does not require a training phase\nand it is physics-informed. The wave propagation field is discretized into\nsparse regions, a process referred to as field discretization, which includes a\nseries of set of source planes, to address the inverse problem. Our method\nemploys the discretized Kirchhoff-Helmholtz integral as the wave propagation\nmodel. By incorporating virtual planes, additional constraints are enforced\nnear the actual sound source, improving the reconstruction process.\nOptimization is carried out using Physics-Informed Neural Networks (PINNs),\nwhere physics-based constraints are integrated into the loss functions to\naccount for both direct (from equivalent source plane to hologram plane) and\nadditional (from virtual planes to hologram plane) wave propagation paths.\nAdditionally, sparsity is enforced on the velocity of the equivalent sources.\nOur comprehensive validation across various rectangular and violin top plates,\ncovering a wide range of vibrational modes, demonstrates that PINN-SFD\nconsistently outperforms the conventional Compressive-Equivalent Source Method\n(C-ESM), particularly in terms of reconstruction accuracy for complex\nvibrational patterns. Significantly, this method demonstrates reduced\nsensitivity to regularization parameters compared to C-ESM.", "published": "2025-05-01 22:20:19", "link": "http://arxiv.org/abs/2505.00897v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "SMSAT: A Multimodal Acoustic Dataset and Deep Contrastive Learning Framework for Affective and Physiological Modeling of Spiritual Meditation", "abstract": "Understanding how auditory stimuli influence emotional and physiological\nstates is fundamental to advancing affective computing and mental health\ntechnologies. In this paper, we present a multimodal evaluation of the\naffective and physiological impacts of three auditory conditions, that is,\nspiritual meditation (SM), music (M), and natural silence (NS), using a\ncomprehensive suite of biometric signal measures. To facilitate this analysis,\nwe introduce the Spiritual, Music, Silence Acoustic Time Series (SMSAT)\ndataset, a novel benchmark comprising acoustic time series (ATS) signals\nrecorded under controlled exposure protocols, with careful attention to\ndemographic diversity and experimental consistency. To model the auditory\ninduced states, we develop a contrastive learning based SMSAT audio encoder\nthat extracts highly discriminative embeddings from ATS data, achieving 99.99%\nclassification accuracy in interclass and intraclass evaluations. Furthermore,\nwe propose the Calmness Analysis Model (CAM), a deep learning framework\nintegrating 25 handcrafted and learned features for affective state\nclassification across auditory conditions, attaining robust 99.99%\nclassification accuracy. In contrast, pairwise t tests reveal significant\ndeviations in cardiac response characteristics (CRC) between SM analysis via\nANOVA inducing more significant physiological fluctuations. Compared to\nexisting state of the art methods reporting accuracies up to 90%, the proposed\nmodel demonstrates substantial performance gains (up to 99%). This work\ncontributes a validated multimodal dataset and a scalable deep learning\nframework for affective computing applications in stress monitoring, mental\nwell-being, and therapeutic audio-based interventions.", "published": "2025-05-01 19:55:50", "link": "http://arxiv.org/abs/2505.00839v1", "categories": ["cs.SD", "cs.SI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GVPT -- A software for guided visual pitch tracking", "abstract": "GVPT (Guided visual pitch tracking) is a publicly available, real-time pitch\ntracking software designed to guide and evaluate vocal pitch control using\nvisual feedback. Developed for clinical and research applications, the system\npresents various visual target pitch contour and overlays the subject's pitch\nin real-time to promote accurate vocal reproduction. GVPT supports difficulty\nmodification, session logging, and precise pitch tracking. The software enables\nvoice pitch control exercise in both experimental and therapeutic settings.", "published": "2025-05-01 00:55:52", "link": "http://arxiv.org/abs/2505.00750v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "mmSnap: Bayesian One-Shot Fusion in a Self-Calibrated mmWave Radar Network", "abstract": "We present mmSnap, a collaborative RF sensing framework using multiple radar\nnodes, and demonstrate its feasibility and efficacy using commercially\navailable mmWave MIMO radars. Collaborative fusion requires network\ncalibration, or estimates of the relative poses (positions and orientations) of\nthe sensors. We experimentally validate a self-calibration algorithm developed\nin our prior work, which estimates relative poses in closed form by least\nsquares matching of target tracks within the common field of view (FoV). We\nthen develop and demonstrate a Bayesian framework for one-shot fusion of\nmeasurements from multiple calibrated nodes, which yields instantaneous\nestimates of position and velocity vectors that match smoothed estimates from\nmulti-frame tracking. Our experiments, conducted outdoors with two radar nodes\ntracking a moving human target, validate the core assumptions required to\ndevelop a broader set of capabilities for networked sensing with\nopportunistically deployed nodes.", "published": "2025-05-01 20:43:14", "link": "http://arxiv.org/abs/2505.00857v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "SeLR: Sparsity-enhanced Lagrangian Relaxation for Computation Offloading at the Edge", "abstract": "This paper introduces a novel computational approach for offloading sensor\ndata processing tasks to servers in edge networks for better accuracy and\nmakespan. A task is assigned with one of several offloading options, each\ncomprises a server, a route for uploading data to the server, and a service\nprofile that specifies the performance and resource consumption at the server\nand in the network. This offline offloading and routing problem is formulated\nas mixed integer programming (MIP), which is non-convex and HP-hard due to the\ndiscrete decision variables associated to the offloading options. The novelty\nof our approach is to transform this non-convex problem into iterative convex\noptimization by relaxing integer decision variables into continuous space,\ncombining primal-dual optimization for penalizing constraint violations and\nreweighted $L_1$-minimization for promoting solution sparsity, which achieves\nbetter convergence through a smoother path in a continuous search space.\nCompared to existing greedy heuristics, our approach can achieve a better\nPareto frontier in accuracy and latency, scales better to larger problem\ninstances, and can achieve a 7.72--9.17$\\times$ reduction in computational\noverhead of scheduling compared to the optimal solver in hierarchically\norganized edge networks with 300 nodes and 50--100 tasks.", "published": "2025-05-01 20:20:52", "link": "http://arxiv.org/abs/2505.00848v1", "categories": ["cs.NI", "cs.SY", "eess.SP", "eess.SY", "C.2.1"], "primary_category": "cs.NI"}
{"title": "A Multi-Granularity Multimodal Retrieval Framework for Multimodal Document Tasks", "abstract": "Retrieval-augmented generation (RAG) systems have predominantly focused on\ntext-based retrieval, limiting their effectiveness in handling visually-rich\ndocuments that encompass text, images, tables, and charts. To bridge this gap,\nwe propose a unified multi-granularity multimodal retrieval framework tailored\nfor two benchmark tasks: MMDocIR and M2KR. Our approach integrates hierarchical\nencoding strategies, modality-aware retrieval mechanisms, and reranking modules\nto effectively capture and utilize the complex interdependencies between\ntextual and visual modalities. By leveraging off-the-shelf vision-language\nmodels and implementing a training-free hybridretrieval strategy, our framework\ndemonstrates robust performance without the need for task-specific fine-tuning.\nExperimental evaluations reveal that incorporating layout-aware search and\nreranking modules significantly enhances retrieval accuracy, achieving a top\nperformance score of 65.56. This work underscores the potential of scalable and\nreproducible solutions in advancing multimodal document retrieval systems.", "published": "2025-05-01 02:40:30", "link": "http://arxiv.org/abs/2505.01457v1", "categories": ["cs.IR", "cs.CV"], "primary_category": "cs.IR"}
{"title": "Affine matrix scrambling achieves smoothness-dependent convergence rates", "abstract": "We study the convergence rate of the median estimator for affine matrix\nscrambled digital nets applied to integrands over the unit hypercube $[0,\n1]^s$. By taking the median of $(2r-1)$ independent randomized quasi-Monte\nCarlo (RQMC) samples, we demonstrate that the desired convergence rates can be\nachieved without increasing the number of randomizations $r$ as the quadrature\nsize $N$ grows for both bounded and unbounded integrands. For unbounded\nintegrands, our analysis assumes a boundary growth condition on the weak\nderivatives and also considers singularities such as kinks and jump\ndiscontinuities. Notably, when $r = 1$, the median estimator reduces to the\nstandard RQMC estimator. By applying analytical techniques developed for median\nestimators, we prove that the affine matrix scrambled estimator achieves a\nconvergence rate depending on the integrand's smoothness, and is therefore not\nlimited by the canonical rate $\\mathcal{O}(N^{-3/2})$. However, this\nsmoothness-dependent theoretical rate is not observed empirically in numerical\nexperiments when the affine matrix scrambling yields a heavy-tailed sampling\ndistribution. In contrast, the median estimator consistently reveals the\ntheoretical rates and yields smaller integration errors than mean estimators,\nfurther highlighting its advantages.", "published": "2025-05-01 09:09:58", "link": "http://arxiv.org/abs/2505.00411v2", "categories": ["math.NA", "cs.NA", "65C05"], "primary_category": "math.NA"}
{"title": "Efficient Krylov methods for linear response in plane-wave electronic structure calculations", "abstract": "We propose a novel algorithm based on inexact GMRES methods for linear\nresponse calculations in density functional theory. Such calculations require\niteratively solving a nested linear problem $\\mathcal{E} \\delta\\rho = b$ to\nobtain the variation of the electron density $\\delta \\rho$. Notably each\napplication of the dielectric operator $\\mathcal{E}$ in turn requires the\niterative solution of multiple linear systems, the Sternheimer equations. We\ndevelop computable bounds to estimate the accuracy of the density variation\ngiven the tolerances to which the Sternheimer equations have been solved. Based\non this result we suggest reliable strategies for adaptively selecting the\nconvergence tolerances of the Sternheimer equations, such that each\napplications of $\\mathcal{E}$ is no more accurate than needed. Experiments on\nchallenging materials systems of practical relevance demonstrate our strategies\nto achieve superlinear convergence as well as a reduction of computational time\nby about 40% while preserving the accuracy of the returned response solution.\nOur algorithm seamlessly combines with standard preconditioning approaches\nknown from the context of self-consistent field problems making it a promising\nframework for efficient response solvers based on Krylov subspace techniques.", "published": "2025-05-01 08:30:52", "link": "http://arxiv.org/abs/2505.02319v1", "categories": ["math.NA", "cs.NA", "physics.comp-ph"], "primary_category": "math.NA"}
{"title": "A Multi-Granularity Retrieval Framework for Visually-Rich Documents", "abstract": "Retrieval-augmented generation (RAG) systems have predominantly focused on\ntext-based retrieval, limiting their effectiveness in handling visually-rich\ndocuments that encompass text, images, tables, and charts. To bridge this gap,\nwe propose a unified multi-granularity multimodal retrieval framework tailored\nfor two benchmark tasks: MMDocIR and M2KR. Our approach integrates hierarchical\nencoding strategies, modality-aware retrieval mechanisms, and vision-language\nmodel (VLM)-based candidate filtering to effectively capture and utilize the\ncomplex interdependencies between textual and visual modalities. By leveraging\noff-the-shelf vision-language models and implementing a training-free hybrid\nretrieval strategy, our framework demonstrates robust performance without the\nneed for task-specific fine-tuning. Experimental evaluations reveal that\nincorporating layout-aware search and VLM-based candidate verification\nsignificantly enhances retrieval accuracy, achieving a top performance score of\n65.56. This work underscores the potential of scalable and reproducible\nsolutions in advancing multimodal document retrieval systems.", "published": "2025-05-01 02:40:30", "link": "http://arxiv.org/abs/2505.01457v2", "categories": ["cs.IR", "cs.CV"], "primary_category": "cs.IR"}
{"title": "A new architecture of high-order deep neural networks that learn martingales", "abstract": "A new deep-learning neural network architecture based on high-order weak\napproximation algorithms for stochastic differential equations (SDEs) is\nproposed. The architecture enables the efficient learning of martingales by\ndeep learning models. The behaviour of deep neural networks based on this\narchitecture, when applied to the problem of pricing financial derivatives, is\nalso examined. The core of this new architecture lies in the high-order weak\napproximation algorithms of the explicit Runge--Kutta type, wherein the\napproximation is realised solely through iterative compositions and linear\ncombinations of vector fields of the target SDEs.", "published": "2025-05-01 04:49:53", "link": "http://arxiv.org/abs/2505.03789v1", "categories": ["cs.LG", "math.PR", "q-fin.CP", "65C30, 60H35, 91G60, 68T07"], "primary_category": "cs.LG"}
{"title": "Utilising Gradient-Based Proposals Within Sequential Monte Carlo Samplers for Training of Partial Bayesian Neural Networks", "abstract": "Partial Bayesian neural networks (pBNNs) have been shown to perform\ncompetitively with fully Bayesian neural networks while only having a subset of\nthe parameters be stochastic. Using sequential Monte Carlo (SMC) samplers as\nthe inference method for pBNNs gives a non-parametric probabilistic estimation\nof the stochastic parameters, and has shown improved performance over\nparametric methods. In this paper we introduce a new SMC-based training method\nfor pBNNs by utilising a guided proposal and incorporating gradient-based\nMarkov kernels, which gives us better scalability on high dimensional problems.\nWe show that our new method outperforms the state-of-the-art in terms of\npredictive performance and optimal loss. We also show that pBNNs scale well\nwith larger batch sizes, resulting in significantly reduced training times and\noften better performance.", "published": "2025-05-01 20:05:38", "link": "http://arxiv.org/abs/2505.03797v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
