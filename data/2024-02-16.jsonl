{"title": "Chain of Logic: Rule-Based Reasoning with Large Language Models", "abstract": "Rule-based reasoning, a fundamental type of legal reasoning, enables us to\ndraw conclusions by accurately applying a rule to a set of facts. We explore\ncausal language models as rule-based reasoners, specifically with respect to\ncompositional rules - rules consisting of multiple elements which form a\ncomplex logical expression. Reasoning about compositional rules is challenging\nbecause it requires multiple reasoning steps, and attending to the logical\nrelationships between elements. We introduce a new prompting method, Chain of\nLogic, which elicits rule-based reasoning through decomposition (solving\nelements as independent threads of logic), and recomposition (recombining these\nsub-answers to resolve the underlying logical expression). This method was\ninspired by the IRAC (Issue, Rule, Application, Conclusion) framework, a\nsequential reasoning approach used by lawyers. We evaluate chain of logic\nacross eight rule-based reasoning tasks involving three distinct compositional\nrules from the LegalBench benchmark and demonstrate it consistently outperforms\nother prompting methods, including chain of thought and self-ask, using\nopen-source and commercial language models.", "published": "2024-02-16 01:54:43", "link": "http://arxiv.org/abs/2402.10400v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pushing the Limits of Zero-shot End-to-End Speech Translation", "abstract": "Data scarcity and the modality gap between the speech and text modalities are\ntwo major obstacles of end-to-end Speech Translation (ST) systems, thus\nhindering their performance. Prior work has attempted to mitigate these\nchallenges by leveraging external MT data and optimizing distance metrics that\nbring closer the speech-text representations. However, achieving competitive\nresults typically requires some ST data. For this reason, we introduce\nZeroSwot, a method for zero-shot ST that bridges the modality gap without any\npaired ST data. Leveraging a novel CTC compression and Optimal Transport, we\ntrain a speech encoder using only ASR data, to align with the representation\nspace of a massively multilingual MT model. The speech encoder seamlessly\nintegrates with the MT model at inference, enabling direct translation from\nspeech to text, across all languages supported by the MT model. Our experiments\nshow that we can effectively close the modality gap without ST data, while our\nresults on MuST-C and CoVoST demonstrate our method's superiority over not only\nprevious zero-shot models, but also supervised ones, achieving state-of-the-art\nresults.", "published": "2024-02-16 03:06:37", "link": "http://arxiv.org/abs/2402.10422v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DELL: Generating Reactions and Explanations for LLM-Based Misinformation\n  Detection", "abstract": "Large language models are limited by challenges in factuality and\nhallucinations to be directly employed off-the-shelf for judging the veracity\nof news articles, where factual accuracy is paramount. In this work, we propose\nDELL that identifies three key stages in misinformation detection where LLMs\ncould be incorporated as part of the pipeline: 1) LLMs could \\emph{generate\nnews reactions} to represent diverse perspectives and simulate user-news\ninteraction networks; 2) LLMs could \\emph{generate explanations} for proxy\ntasks (e.g., sentiment, stance) to enrich the contexts of news articles and\nproduce experts specializing in various aspects of news understanding; 3) LLMs\ncould \\emph{merge task-specific experts} and provide an overall prediction by\nincorporating the predictions and confidence scores of varying experts.\nExtensive experiments on seven datasets with three LLMs demonstrate that DELL\noutperforms state-of-the-art baselines by up to 16.8\\% in macro f1-score.\nFurther analysis reveals that the generated reactions and explanations are\ngreatly helpful in misinformation detection, while our proposed LLM-guided\nexpert merging helps produce better-calibrated predictions.", "published": "2024-02-16 03:24:56", "link": "http://arxiv.org/abs/2402.10426v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Smaller Language Models are capable of selecting Instruction-Tuning\n  Training Data for Larger Language Models", "abstract": "Instruction-tuning language models has become a crucial step in aligning them\nfor general use. Typically, this process involves extensive training on large\ndatasets, incurring high training costs. In this paper, we introduce a novel\ntraining data selection based on the learning percentage of the samples. We\nassert that current language models possess the capability to autonomously\nselect high-quality training data, leading to comparable or improved\nperformance compared to training on the entire dataset. Our experiments span\ndifferent-sized models, revealing that this characteristic holds for models\nranging from 1B (small) to 13B (large) in size. Moreover, we demonstrate an\ninteresting finding that the data hardness transfers across model sizes, and a\nsmaller 350M model can effectively curate high-quality training data with hard\nsamples for a larger 13B model, resulting in an equally or superior\ninstruction-tuned model compared to training on the complete dataset. Utilizing\nopen-sourced OPT and Llama-2 models up to 13B in size, two publicly available\ninstruction-tuning training datasets and evaluated by both automatic metrics &\nhumans, our paper introduces a novel approach to training data selection,\nshowcasing a more efficient alternative.", "published": "2024-02-16 03:39:37", "link": "http://arxiv.org/abs/2402.10430v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "I Am Not Them: Fluid Identities and Persistent Out-group Bias in Large\n  Language Models", "abstract": "We explored cultural biases-individualism vs. collectivism-in ChatGPT across\nthree Western languages (i.e., English, German, and French) and three Eastern\nlanguages (i.e., Chinese, Japanese, and Korean). When ChatGPT adopted an\nindividualistic persona in Western languages, its collectivism scores (i.e.,\nout-group values) exhibited a more negative trend, surpassing their positive\norientation towards individualism (i.e., in-group values). Conversely, when a\ncollectivistic persona was assigned to ChatGPT in Eastern languages, a similar\npattern emerged with more negative responses toward individualism (i.e.,\nout-group values) as compared to collectivism (i.e., in-group values). The\nresults indicate that when imbued with a particular social identity, ChatGPT\ndiscerns in-group and out-group, embracing in-group values while eschewing\nout-group values. Notably, the negativity towards the out-group, from which\nprejudices and discrimination arise, exceeded the positivity towards the\nin-group. The experiment was replicated in the political domain, and the\nresults remained consistent. Furthermore, this replication unveiled an\nintrinsic Democratic bias in Large Language Models (LLMs), aligning with\nearlier findings and providing integral insights into mitigating such bias\nthrough prompt engineering. Extensive robustness checks were performed using\nvarying hyperparameter and persona setup methods, with or without social\nidentity labels, across other popular language models.", "published": "2024-02-16 03:54:48", "link": "http://arxiv.org/abs/2402.10436v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Steering Conversational Large Language Models for Long Emotional Support\n  Conversations", "abstract": "In this study, we address the challenge of enabling large language models\n(LLMs) to consistently adhere to emotional support strategies in extended\nconversations. We focus on the steerability of the Llama-2 and Llama-3 suite of\nmodels, examining their ability to maintain these strategies throughout\ninteractions. To assess this, we introduce the Strategy Relevant Attention\n(SRA) metric, which quantifies the model's adherence to the prompted strategy\nthrough attention maps. To facilitate our study, we create a\nstrategy-conditioned synthetic conversational dataset derived from the ESConv\ndataset. We also propose various baselines informed by our proposed SRA metric\nto address the challenge and propose a fine-tuned model that significantly\nenhances the steerability of the base model in following the strategy\nthroughout the conversation. The code and data are publicly available on our\nGitHub.", "published": "2024-02-16 05:03:01", "link": "http://arxiv.org/abs/2402.10453v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conversational SimulMT: Efficient Simultaneous Translation with Large\n  Language Models", "abstract": "Simultaneous machine translation (SimulMT) presents a challenging trade-off\nbetween translation quality and latency. Recent studies have shown that LLMs\ncan achieve good performance in SimulMT tasks. However, this often comes at the\nexpense of high inference cost and latency. In this paper, we propose a\nconversational SimulMT framework to enhance the inference efficiency of\nLLM-based SimulMT through multi-turn-dialogue-based decoding. Our experiments\nwith Llama2-7b-chat on two SimulMT benchmarks demonstrate the superiority of\nLLM in translation quality while achieving comparable computational latency to\nspecialized SimulMT models.", "published": "2024-02-16 10:32:16", "link": "http://arxiv.org/abs/2402.10552v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Disordered-DABS: A Benchmark for Dynamic Aspect-Based Summarization in\n  Disordered Texts", "abstract": "Aspect-based summarization has seen significant advancements, especially in\nstructured text. Yet, summarizing disordered, large-scale texts, like those\nfound in social media and customer feedback, remains a significant challenge.\nCurrent research largely targets predefined aspects within structured texts,\nneglecting the complexities of dynamic and disordered environments. Addressing\nthis gap, we introduce Disordered-DABS, a novel benchmark for dynamic\naspect-based summarization tailored to unstructured text. Developed by adapting\nexisting datasets for cost-efficiency and scalability, our comprehensive\nexperiments and detailed human evaluations reveal that Disordered-DABS poses\nunique challenges to contemporary summarization models, including\nstate-of-the-art language models such as GPT-3.5.", "published": "2024-02-16 10:35:18", "link": "http://arxiv.org/abs/2402.10554v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural paraphrasing by automatically crawled and aligned sentence pairs", "abstract": "Paraphrasing is the task of re-writing an input text using other words,\nwithout altering the meaning of the original content. Conversational systems\ncan exploit automatic paraphrasing to make the conversation more natural, e.g.,\ntalking about a certain topic using different paraphrases in different time\ninstants. Recently, the task of automatically generating paraphrases has been\napproached in the context of Natural Language Generation (NLG). While many\nexisting systems simply consist in rule-based models, the recent success of the\nDeep Neural Networks in several NLG tasks naturally suggests the possibility of\nexploiting such networks for generating paraphrases. However, the main obstacle\ntoward neural-network-based paraphrasing is the lack of large datasets with\naligned pairs of sentences and paraphrases, that are needed to efficiently\ntrain the neural models. In this paper we present a method for the automatic\ngeneration of large aligned corpora, that is based on the assumption that news\nand blog websites talk about the same events using different narrative styles.\nWe propose a similarity search procedure with linguistic constraints that,\ngiven a reference sentence, is able to locate the most similar candidate\nparaphrases out from millions of indexed sentences. The data generation process\nis evaluated in the case of the Italian language, performing experiments using\npointer-based deep neural architectures.", "published": "2024-02-16 10:40:38", "link": "http://arxiv.org/abs/2402.10558v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LinkNER: Linking Local Named Entity Recognition Models to Large Language\n  Models using Uncertainty", "abstract": "Named Entity Recognition (NER) serves as a fundamental task in natural\nlanguage understanding, bearing direct implications for web content analysis,\nsearch engines, and information retrieval systems. Fine-tuned NER models\nexhibit satisfactory performance on standard NER benchmarks. However, due to\nlimited fine-tuning data and lack of knowledge, it performs poorly on unseen\nentity recognition. As a result, the usability and reliability of NER models in\nweb-related applications are compromised. Instead, Large Language Models (LLMs)\nlike GPT-4 possess extensive external knowledge, but research indicates that\nthey lack specialty for NER tasks. Furthermore, non-public and large-scale\nweights make tuning LLMs difficult. To address these challenges, we propose a\nframework that combines small fine-tuned models with LLMs (LinkNER) and an\nuncertainty-based linking strategy called RDC that enables fine-tuned models to\ncomplement black-box LLMs, achieving better performance. We experiment with\nboth standard NER test sets and noisy social media datasets. LinkNER enhances\nNER task performance, notably surpassing SOTA models in robustness tests. We\nalso quantitatively analyze the influence of key components like uncertainty\nestimation methods, LLMs, and in-context learning on diverse NER tasks,\noffering specific web-related recommendations. Code is available at\nhttps://github.com/zhzhengit/LinkNER.", "published": "2024-02-16 11:02:29", "link": "http://arxiv.org/abs/2402.10573v3", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Retrieve Only When It Needs: Adaptive Retrieval Augmentation for\n  Hallucination Mitigation in Large Language Models", "abstract": "Hallucinations pose a significant challenge for the practical implementation\nof large language models (LLMs). The utilization of parametric knowledge in\ngenerating factual content is constrained by the limited knowledge of LLMs,\npotentially resulting in internal hallucinations. While incorporating external\ninformation can help fill knowledge gaps, it also introduces the risk of\nirrelevant information, thereby increasing the likelihood of external\nhallucinations. A careful and balanced integration of the parametric knowledge\nwithin LLMs with external information is crucial to alleviate hallucinations.\nIn this study, we present Rowen, a novel approach that enhances LLMs with a\nselective retrieval augmentation process tailored to address hallucinated\noutputs. This process is governed by a multilingual semantic-aware detection\nmodule, which evaluates the consistency of the perturbed responses across\nvarious languages for the same queries. Upon detecting inconsistencies\nindicative of hallucinations, Rowen activates the retrieval of external\ninformation to rectify the model outputs. Rowen adeptly harmonizes the\nintrinsic parameters in LLMs with external knowledge sources, effectively\nmitigating hallucinations by ensuring a balanced integration of internal\nreasoning and external evidence. Through a comprehensive empirical analysis, we\ndemonstrate that Rowen surpasses the current state-of-the-art in both detecting\nand mitigating hallucinated content within the outputs of LLMs.", "published": "2024-02-16 11:55:40", "link": "http://arxiv.org/abs/2402.10612v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Role-playing Systems through Aggressive Queries: Evaluation\n  and Improvement", "abstract": "The advent of Large Language Models (LLMs) has propelled dialogue generation\ninto new realms, particularly in the field of role-playing systems (RPSs).\nWhile enhanced with ordinary role-relevant training dialogues, existing\nLLM-based RPSs still struggle to align with roles when handling intricate and\ntrapped queries in boundary scenarios. In this paper, we design the Modular\nORchestrated Trap-setting Interaction SystEm (MORTISE) to benchmark and improve\nthe role-playing LLMs' performance. MORTISE can produce highly role-relevant\naggressive queries through the collaborative effort of multiple LLM-based\nmodules, and formulate corresponding responses to create an adversarial\ntraining dataset via a consistent response generator. We select 190 Chinese and\nEnglish roles to construct aggressive queries to benchmark existing\nrole-playing LLMs. Through comprehensive evaluation, we find that existing\nmodels exhibit a general deficiency in role alignment capabilities. We further\nselect 180 of the roles to collect an adversarial training dataset (named\nRoleAD) and retain the other 10 roles for testing. Experiments on models\nimproved by RoleAD indicate that our adversarial dataset ameliorates this\ndeficiency, with the improvements demonstrating a degree of generalizability in\nordinary scenarios.", "published": "2024-02-16 12:12:05", "link": "http://arxiv.org/abs/2402.10618v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via\n  Self-Distillation", "abstract": "The upscaling of Large Language Models (LLMs) has yielded impressive advances\nin natural language processing, yet it also poses significant deployment\nchallenges. Weight quantization has emerged as a widely embraced solution to\nreduce memory and computational demands. This paper introduces BitDistiller, a\nframework that synergizes Quantization-Aware Training (QAT) with Knowledge\nDistillation (KD) to boost the performance of LLMs at ultra-low precisions\n(sub-4-bit). Specifically, BitDistiller first incorporates a tailored\nasymmetric quantization and clipping technique to maximally preserve the\nfidelity of quantized weights, and then proposes a novel Confidence-Aware\nKullback-Leibler Divergence (CAKLD) objective, which is employed in a\nself-distillation manner to enable faster convergence and superior model\nperformance. Empirical evaluations demonstrate that BitDistiller significantly\nsurpasses existing methods in both 3-bit and 2-bit configurations on general\nlanguage understanding and complex reasoning benchmarks. Notably, BitDistiller\nis shown to be more cost-effective, demanding fewer data and training\nresources. The code is available at https://github.com/DD-DuDa/BitDistiller.", "published": "2024-02-16 12:27:15", "link": "http://arxiv.org/abs/2402.10631v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generalizability of Mixture of Domain-Specific Adapters from the Lens of\n  Signed Weight Directions and its Application to Effective Model Pruning", "abstract": "Several parameter-efficient fine-tuning methods based on adapters have been\nproposed as a streamlined approach to incorporate not only a single specialized\nknowledge into existing Pre-Trained Language Models (PLMs) but also multiple of\nthem at once. Recent works such as AdapterSoup propose to mix not all but only\na selective sub-set of domain-specific adapters during inference via model\nweight averaging to optimize performance on novel, unseen domains with\nexcellent computational efficiency. However, the essential generalizability of\nthis emerging weight-space adapter mixing mechanism on \\textit{unseen,\nin-domain examples} remains unexplored. Thus, in this study, we conduct a\ncomprehensive analysis to elucidate the generalizability of domain-specific\nadapter mixtures in in-domain evaluation. We also provide investigations into\nthe inner workings of the mixture of domain-specific adapters by analyzing\ntheir weight signs, yielding critical analysis on the negative correlation\nbetween their fraction of weight sign difference and their mixtures'\ngeneralizability.", "published": "2024-02-16 12:39:10", "link": "http://arxiv.org/abs/2402.10639v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AbsInstruct: Eliciting Abstraction Ability from LLMs through Explanation\n  Tuning with Plausibility Estimation", "abstract": "Abstraction ability is crucial in human intelligence, which can also benefit\nvarious tasks in NLP study. Existing work shows that LLMs are deficient in\nabstract ability, and how to improve it remains unexplored. In this work, we\ndesign the framework AbsInstruct to enhance LLMs' abstraction ability through\ninstruction tuning. The framework builds instructions with in-depth\nexplanations to assist LLMs in capturing the underlying rationale of\nabstraction. Meanwhile, we introduce a plausibility estimator to select\ninstructions that are more consistent with the abstraction knowledge of LLMs to\nbe aligned. Then, our framework combines abstraction instructions with\ngeneral-purpose ones to build a hybrid dataset. Extensive experiments and\nanalyses demonstrate that our framework can considerably enhance LLMs'\nabstraction ability with strong generalization performance while maintaining\ntheir general instruction-following abilities.", "published": "2024-02-16 12:47:11", "link": "http://arxiv.org/abs/2402.10646v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Numerical Reasoning with the Guidance of Reliable Reasoning\n  Processes", "abstract": "Numerical reasoning is an essential ability for NLP systems to handle numeric\ninformation. Recent research indicates that fine-tuning a small-scale model to\nlearn generating reasoning processes alongside answers can significantly\nenhance performance. However, current methods have the limitation that most\nmethods generate reasoning processes with large language models (LLMs), which\nare \"unreliable\" since such processes could contain information unrelated to\nthe answer. To address this limitation, we introduce Enhancing NumeriCal\nreasOning with Reliable procEsses (Encore), which derives the reliable\nreasoning process by decomposing the answer formula, ensuring which fully\nsupports the answer. Nevertheless, models could lack enough data to learn the\nreasoning process generation adequately, since our method generates only one\nsingle reasoning process for one formula. To overcome this difficulty, we\npresent a series of pre-training tasks to help models learn the reasoning\nprocess generation with synthesized data. The experiments show that Encore\nyields improvement on all five experimental datasets with an average of 1.8%,\nproving the effectiveness of our method.", "published": "2024-02-16 13:02:11", "link": "http://arxiv.org/abs/2402.10654v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine Tuning Named Entity Extraction Models for the Fantasy Domain", "abstract": "Named Entity Recognition (NER) is a sequence classification Natural Language\nProcessing task where entities are identified in the text and classified into\npredefined categories. It acts as a foundation for most information extraction\nsystems. Dungeons and Dragons (D&D) is an open-ended tabletop fantasy game with\nits own diverse lore. DnD entities are domain-specific and are thus\nunrecognizable by even the state-of-the-art off-the-shelf NER systems as the\nNER systems are trained on general data for pre-defined categories such as:\nperson (PERS), location (LOC), organization (ORG), and miscellaneous (MISC).\nFor meaningful extraction of information from fantasy text, the entities need\nto be classified into domain-specific entity categories as well as the models\nbe fine-tuned on a domain-relevant corpus. This work uses available lore of\nmonsters in the D&D domain to fine-tune Trankit, which is a prolific NER\nframework that uses a pre-trained model for NER. Upon this training, the system\nacquires the ability to extract monster names from relevant domain documents\nunder a novel NER tag. This work compares the accuracy of the monster name\nidentification against; the zero-shot Trankit model and two FLAIR models. The\nfine-tuned Trankit model achieves an 87.86% F1 score surpassing all the other\nconsidered models.", "published": "2024-02-16 13:11:13", "link": "http://arxiv.org/abs/2402.10662v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL", "abstract": "Currently, the in-context learning method based on large language models\n(LLMs) has become the mainstream of text-to-SQL research. Previous works have\ndiscussed how to select demonstrations related to the user question from a\nhuman-labeled demonstration pool. However, human labeling suffers from the\nlimitations of insufficient diversity and high labeling overhead. Therefore, in\nthis paper, we discuss how to measure and improve the diversity of the\ndemonstrations for text-to-SQL. We present a metric to measure the diversity of\nthe demonstrations and analyze the insufficient of the existing labeled data by\nexperiments. Based on the above discovery, we propose fusing iteratively for\ndemonstrations (Fused) to build a high-diversity demonstration pool through\nhuman-free multiple-iteration synthesis, improving diversity and lowering label\ncost. Our method achieves an average improvement of 3.2% and 5.0% with and\nwithout human labeling on several mainstream datasets, which proves the\neffectiveness of Fused.", "published": "2024-02-16 13:13:18", "link": "http://arxiv.org/abs/2402.10663v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MURRE: Multi-Hop Table Retrieval with Removal for Open-Domain\n  Text-to-SQL", "abstract": "The open-domain text-to-SQL task aims to retrieve question-relevant tables\nfrom massive databases and generate SQL. However, the performance of current\nmethods is constrained by single-hop retrieval, and existing multi-hop\nretrieval of open-domain question answering is not directly applicable due to\nthe tendency to retrieve tables similar to the retrieved ones but irrelevant to\nthe question. Since the questions in text-to-SQL usually contain all required\ninformation, while previous multi-hop retrieval supplements the questions with\nretrieved documents. Therefore, we propose the multi-hop table retrieval with\nremoval (MURRE), which removes previously retrieved information from the\nquestion to guide the retriever towards unretrieved relevant tables. Our\nexperiments on two open-domain text-to-SQL datasets demonstrate an average\nimprovement of 5.7% over the previous state-of-the-art results.", "published": "2024-02-16 13:14:35", "link": "http://arxiv.org/abs/2402.10666v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Humans or LLMs as the Judge? A Study on Judgement Biases", "abstract": "Adopting human and large language models (LLM) as judges (a.k.a human- and\nLLM-as-a-judge) for evaluating the performance of LLMs has recently gained\nattention. Nonetheless, this approach concurrently introduces potential biases\nfrom human and LLMs, questioning the reliability of the evaluation results. In\nthis paper, we propose a novel framework that is free from referencing\ngroundtruth annotations for investigating Misinformation Oversight Bias, Gender\nBias, Authority Bias and Beauty Bias on LLM and human judges. We curate a\ndataset referring to the revised Bloom's Taxonomy and conduct thousands of\nevaluations. Results show that human and LLM judges are vulnerable to\nperturbations to various degrees, and that even the cutting-edge judges possess\nconsiderable biases. We further exploit these biases to conduct attacks on LLM\njudges. We hope that our work can notify the community of the bias and\nvulnerability of human- and LLM-as-a-judge, as well as the urgency of\ndeveloping robust evaluation systems.", "published": "2024-02-16 13:21:06", "link": "http://arxiv.org/abs/2402.10669v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL\n  through Workflow Paradigm", "abstract": "In-context learning of large-language models (LLMs) has achieved remarkable\nsuccess in the field of natural language processing, while extensive case\nstudies reveal that the single-step chain-of-thought prompting approach faces\nchallenges such as attention diffusion and inadequate performance in complex\ntasks like text-to-SQL. To improve the contextual learning capabilities of LLMs\nin text-to-SQL, a workflow paradigm method is proposed, aiming to enhance the\nattention and problem-solving scope of LLMs through decomposition.\nSpecifically, the information determination module for eliminating redundant\ninformation and the brand-new prompt structure based on problem classification\ngreatly enhance the model's attention. Additionally, the inclusion of\nself-correction and active learning modules greatly expands the problem-solving\nscope of LLMs, hence improving the upper limit of LLM-based approaches.\nExtensive experiments conducted on three datasets demonstrate that our approach\noutperforms other methods by a significant margin. About 2-3 percentage point\nimprovements compared to the existing baseline on the Spider Dev,\nSpider-Realistic, and Bird Dev datasets and new SOTA results on the Spider Test\ndataset are achieved. Our code is available on GitHub:\n\\url{https://github.com/FlyingFeather/DEA-SQL}.", "published": "2024-02-16 13:24:05", "link": "http://arxiv.org/abs/2402.10671v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "German Text Simplification: Finetuning Large Language Models with\n  Semi-Synthetic Data", "abstract": "This study pioneers the use of synthetically generated data for training\ngenerative models in document-level text simplification of German texts. We\ndemonstrate the effectiveness of our approach with real-world online texts.\nAddressing the challenge of data scarcity in language simplification, we\ncrawled professionally simplified German texts and synthesized a corpus using\nGPT-4. We finetune Large Language Models with up to 13 billion parameters on\nthis data and evaluate their performance. This paper employs various\nmethodologies for evaluation and demonstrates the limitations of currently used\nrule-based metrics. Both automatic and manual evaluations reveal that our\nmodels can significantly simplify real-world online texts, indicating the\npotential of synthetic data in improving text simplification.", "published": "2024-02-16 13:28:44", "link": "http://arxiv.org/abs/2402.10675v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Towards Uncovering How Large Language Model Works: An Explainability\n  Perspective", "abstract": "Large language models (LLMs) have led to breakthroughs in language tasks, yet\nthe internal mechanisms that enable their remarkable generalization and\nreasoning abilities remain opaque. This lack of transparency presents\nchallenges such as hallucinations, toxicity, and misalignment with human\nvalues, hindering the safe and beneficial deployment of LLMs. This paper aims\nto uncover the mechanisms underlying LLM functionality through the lens of\nexplainability. First, we review how knowledge is architecturally composed\nwithin LLMs and encoded in their internal parameters via mechanistic\ninterpretability techniques. Then, we summarize how knowledge is embedded in\nLLM representations by leveraging probing techniques and representation\nengineering. Additionally, we investigate the training dynamics through a\nmechanistic perspective to explain phenomena such as grokking and memorization.\nLastly, we explore how the insights gained from these explanations can enhance\nLLM performance through model editing, improve efficiency through pruning, and\nbetter align with human values.", "published": "2024-02-16 13:46:06", "link": "http://arxiv.org/abs/2402.10688v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cultural Commonsense Knowledge for Intercultural Dialogues", "abstract": "Despite recent progress, large language models (LLMs) still face the\nchallenge of appropriately reacting to the intricacies of social and cultural\nconventions. This paper presents MANGO, a methodology for distilling\nhigh-accuracy, high-recall assertions of cultural knowledge. We judiciously and\niteratively prompt LLMs for this purpose from two entry points, concepts and\ncultures. Outputs are consolidated via clustering and generative summarization.\nRunning the MANGO method with GPT-3.5 as underlying LLM yields 167K\nhigh-accuracy assertions for 30K concepts and 11K cultures, surpassing prior\nresources by a large margin in quality and size. In an extrinsic evaluation for\nintercultural dialogues, we explore augmenting dialogue systems with cultural\nknowledge assertions. Notably, despite LLMs inherently possessing cultural\nknowledge, we find that adding knowledge from MANGO improves the overall\nquality, specificity, and cultural sensitivity of dialogue responses, as judged\nby human annotators. Data and code are available for download.", "published": "2024-02-16 13:46:38", "link": "http://arxiv.org/abs/2402.10689v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Python is Not Always the Best Choice: Embracing Multilingual Program of\n  Thoughts", "abstract": "Program of Thoughts (PoT) is an approach characterized by its executable\nintermediate steps, which ensure the accuracy of the logical calculations in\nthe reasoning process. Currently, PoT primarily uses Python. However, relying\nsolely on a single language may result in suboptimal solutions and overlook the\npotential benefits of other programming languages. In this paper, we conduct\ncomprehensive experiments on the programming languages used in PoT and find\nthat no single language consistently delivers optimal performance across all\ntasks and models. The effectiveness of each language varies depending on the\nspecific scenarios. Inspired by this, we propose a task and model agnostic\napproach called MultiPoT, which harnesses strength and diversity from various\nlanguages. Experimental results reveal that it significantly outperforms Python\nSelf-Consistency. Furthermore, it achieves comparable or superior performance\ncompared to the best monolingual PoT in almost all tasks across all models. In\nparticular, MultiPoT achieves more than 4.6% improvement on average on ChatGPT\n(gpt-3.5-turbo-0701).", "published": "2024-02-16 13:48:06", "link": "http://arxiv.org/abs/2402.10691v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking Human-like Translation Strategy: Integrating Drift-Diffusion\n  Model with Large Language Models for Machine Translation", "abstract": "Large language models (LLMs) have demonstrated promising potential in various\ndownstream tasks, including machine translation. However, prior work on\nLLM-based machine translation has mainly focused on better utilizing training\ndata, demonstrations, or pre-defined and universal knowledge to improve\nperformance, with a lack of consideration of decision-making like human\ntranslators. In this paper, we incorporate Thinker with the Drift-Diffusion\nModel (Thinker-DDM) to address this issue. We then redefine the Drift-Diffusion\nprocess to emulate human translators' dynamic decision-making under constrained\nresources. We conduct extensive experiments under the high-resource,\nlow-resource, and commonsense translation settings using the WMT22 and CommonMT\ndatasets, in which Thinker-DDM outperforms baselines in the first two\nscenarios. We also perform additional analysis and evaluation on commonsense\ntranslation to illustrate the high effectiveness and efficacy of the proposed\nmethod.", "published": "2024-02-16 14:00:56", "link": "http://arxiv.org/abs/2402.10699v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing the Reasoning Capabilities of LLMs in the context of\n  Evidence-based Claim Verification", "abstract": "Although LLMs have shown great performance on Mathematics and Coding related\nreasoning tasks, the reasoning capabilities of LLMs regarding other forms of\nreasoning are still an open problem. Here, we examine the issue of reasoning\nfrom the perspective of claim verification. We propose a framework designed to\nbreak down any claim paired with evidence into atomic reasoning types that are\nnecessary for verification. We use this framework to create Reasoning in\nEvidence-based Claim Verification (RECV), the first claim verification\nbenchmark, incorporating real-world claims, to assess the deductive and\nabductive reasoning capabilities of LLMs. The benchmark comprises of three\ndatasets, covering reasoning problems of increasing complexity. We evaluate\nthree state-of-the-art proprietary LLMs under multiple prompt settings. Our\nresults show that while LLMs can address deductive reasoning problems, they\nconsistently fail in cases of abductive reasoning. Moreover, we observe that\nenhancing LLMs with rationale generation is not always beneficial. Nonetheless,\nwe find that generated rationales are semantically similar to those provided by\nhumans, especially in deductive reasoning cases.", "published": "2024-02-16 14:52:05", "link": "http://arxiv.org/abs/2402.10735v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Let's Learn Step by Step: Enhancing In-Context Learning Ability with\n  Curriculum Learning", "abstract": "Demonstration ordering, which is an important strategy for in-context\nlearning (ICL), can significantly affects the performance of large language\nmodels (LLMs). However, most of the current approaches of ordering require high\ncomputational costs to introduce the priori knowledge. In this paper, inspired\nby the human learning process, we propose a simple but effective demonstration\nordering method for ICL, named the few-shot In-Context Curriculum Learning\n(ICCL). The ICCL implies gradually increasing the complexity of prompt\ndemonstrations during the inference process. The difficulty can be assessed by\nhuman experts or LLMs-driven metrics, such as perplexity. Then we design\nextensive experiments to discuss the effectiveness of the ICCL at both\ncorpus-level and instance-level. Moreover, we also investigate the formation\nmechanism of LLM's ICCL capability. Experimental results demonstrate that ICCL,\ndeveloped during the instruction-tuning stage, is effective for representative\nopen-source LLMs. To facilitate further research and applications by other\nscholars, we make the code publicly available.", "published": "2024-02-16 14:55:33", "link": "http://arxiv.org/abs/2402.10738v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Construction of a Syntactic Analysis Map for Yi Shui School through Text\n  Mining and Natural Language Processing Research", "abstract": "Entity and relationship extraction is a crucial component in natural language\nprocessing tasks such as knowledge graph construction, question answering\nsystem design, and semantic analysis. Most of the information of the Yishui\nschool of traditional Chinese Medicine (TCM) is stored in the form of\nunstructured classical Chinese text. The key information extraction of TCM\ntexts plays an important role in mining and studying the academic schools of\nTCM. In order to solve these problems efficiently using artificial intelligence\nmethods, this study constructs a word segmentation and entity relationship\nextraction model based on conditional random fields under the framework of\nnatural language processing technology to identify and extract the entity\nrelationship of traditional Chinese medicine texts, and uses the common\nweighting technology of TF-IDF information retrieval and data mining to extract\nimportant key entity information in different ancient books. The dependency\nsyntactic parser based on neural network is used to analyze the grammatical\nrelationship between entities in each ancient book article, and it is\nrepresented as a tree structure visualization, which lays the foundation for\nthe next construction of the knowledge graph of Yishui school and the use of\nartificial intelligence methods to carry out the research of TCM academic\nschools.", "published": "2024-02-16 14:59:55", "link": "http://arxiv.org/abs/2402.10743v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing ESG Impact Type Identification through Early Fusion and\n  Multilingual Models", "abstract": "In the evolving landscape of Environmental, Social, and Corporate Governance\n(ESG) impact assessment, the ML-ESG-2 shared task proposes identifying ESG\nimpact types. To address this challenge, we present a comprehensive system\nleveraging ensemble learning techniques, capitalizing on early and late fusion\napproaches. Our approach employs four distinct models: mBERT, FlauBERT-base,\nALBERT-base-v2, and a Multi-Layer Perceptron (MLP) incorporating Latent\nSemantic Analysis (LSA) and Term Frequency-Inverse Document Frequency (TF-IDF)\nfeatures. Through extensive experimentation, we find that our early fusion\nensemble approach, featuring the integration of LSA, TF-IDF, mBERT,\nFlauBERT-base, and ALBERT-base-v2, delivers the best performance. Our system\noffers a comprehensive ESG impact type identification solution, contributing to\nthe responsible and sustainable decision-making processes vital in today's\nfinancial and corporate governance landscape.", "published": "2024-02-16 15:54:24", "link": "http://arxiv.org/abs/2402.10772v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Condensed Transition Graph Framework for Zero-shot Link Prediction\n  with Large Language Models", "abstract": "Zero-shot link prediction (ZSLP) on knowledge graphs aims at automatically\nidentifying relations between given entities. Existing methods primarily employ\nauxiliary information to predict tail entity given head entity and its\nrelation, yet face challenges due to the occasional unavailability of such\ndetailed information and the inherent simplicity of predicting tail entities\nbased on semantic similarities. Even though Large Language Models (LLMs) offer\na promising solution to predict unobserved relations between the head and tail\nentity in a zero-shot manner, their performance is still restricted due to the\ninability to leverage all the (exponentially many) paths' information between\ntwo entities, which are critical in collectively indicating their relation\ntypes. To address this, in this work, we introduce a Condensed Transition Graph\nFramework for Zero-Shot Link Prediction (CTLP), which encodes all the paths'\ninformation in linear time complexity to predict unseen relations between\nentities, attaining both efficiency and information preservation. Specifically,\nwe design a condensed transition graph encoder with theoretical guarantees on\nits coverage, expressiveness, and efficiency. It is learned by a transition\ngraph contrastive learning strategy. Subsequently, we design a soft instruction\ntuning to learn and map the all-path embedding to the input of LLMs.\nExperimental results show that our proposed CTLP method achieves\nstate-of-the-art performance on three standard ZSLP datasets", "published": "2024-02-16 16:02:33", "link": "http://arxiv.org/abs/2402.10779v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Hybrid Question Answering via Program-based Prompting", "abstract": "Question answering over heterogeneous data requires reasoning over diverse\nsources of data, which is challenging due to the large scale of information and\norganic coupling of heterogeneous data. Various approaches have been proposed\nto address these challenges. One approach involves training specialized\nretrievers to select relevant information, thereby reducing the input length.\nAnother approach is to transform diverse modalities of data into a single\nmodality, simplifying the task difficulty and enabling more straightforward\nprocessing. In this paper, we propose HProPro, a novel program-based prompting\nframework for the hybrid question answering task. HProPro follows the code\ngeneration and execution paradigm. In addition, HProPro integrates various\nfunctions to tackle the hybrid reasoning scenario. Specifically, HProPro\ncontains function declaration and function implementation to perform hybrid\ninformation-seeking over data from various sources and modalities, which\nenables reasoning over such data without training specialized retrievers or\nperforming modal transformations. Experimental results on two typical hybrid\nquestion answering benchmarks HybridQA and MultiModalQA demonstrate the\neffectiveness of HProPro: it surpasses all baseline systems and achieves the\nbest performances in the few-shot settings on both datasets.", "published": "2024-02-16 16:35:41", "link": "http://arxiv.org/abs/2402.10812v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Time Series Forecasting with LLMs: Understanding and Enhancing Model\n  Capabilities", "abstract": "Large language models (LLMs) have been applied in many fields and have\ndeveloped rapidly in recent years. As a classic machine learning task, time\nseries forecasting has recently been boosted by LLMs. Recent works treat large\nlanguage models as \\emph{zero-shot} time series reasoners without further\nfine-tuning, which achieves remarkable performance. However, there are some\nunexplored research problems when applying LLMs for time series forecasting\nunder the zero-shot setting. For instance, the LLMs' preferences for the input\ntime series are less understood. In this paper, by comparing LLMs with\ntraditional time series forecasting models, we observe many interesting\nproperties of LLMs in the context of time series forecasting. First, our study\nshows that LLMs perform well in predicting time series with clear patterns and\ntrends, but face challenges with datasets lacking periodicity. This observation\ncan be explained by the ability of LLMs to recognize the underlying period\nwithin datasets, which is supported by our experiments. In addition, the input\nstrategy is investigated, and it is found that incorporating external knowledge\nand adopting natural language paraphrases substantially improve the predictive\nperformance of LLMs for time series. Overall, our study contributes insight\ninto LLMs' advantages and limitations in time series forecasting under\ndifferent conditions.", "published": "2024-02-16 17:15:28", "link": "http://arxiv.org/abs/2402.10835v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EcoRank: Budget-Constrained Text Re-ranking Using Large Language Models", "abstract": "Large Language Models (LLMs) have achieved state-of-the-art performance in\ntext re-ranking. This process includes queries and candidate passages in the\nprompts, utilizing pointwise, listwise, and pairwise prompting strategies. A\nlimitation of these ranking strategies with LLMs is their cost: the process can\nbecome expensive due to API charges, which are based on the number of input and\noutput tokens. We study how to maximize the re-ranking performance given a\nbudget, by navigating the vast search spaces of prompt choices, LLM APIs, and\nbudget splits. We propose a suite of budget-constrained methods to perform text\nre-ranking using a set of LLM APIs. Our most efficient method, called EcoRank,\nis a two-layered pipeline that jointly optimizes decisions regarding budget\nallocation across prompt strategies and LLM APIs. Our experimental results on\nfour popular QA and passage reranking datasets show that EcoRank outperforms\nother budget-aware supervised and unsupervised baselines.", "published": "2024-02-16 18:03:42", "link": "http://arxiv.org/abs/2402.10866v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reviewer2: Optimizing Review Generation Through Prompt Generation", "abstract": "Recent developments in LLMs offer new opportunities for assisting authors in\nimproving their work. In this paper, we envision a use case where authors can\nreceive LLM-generated reviews that uncover weak points in the current draft.\nWhile initial methods for automated review generation already exist, these\nmethods tend to produce reviews that lack detail, and they do not cover the\nrange of opinions that human reviewers produce. To address this shortcoming, we\npropose an efficient two-stage review generation framework called Reviewer2.\nUnlike prior work, this approach explicitly models the distribution of possible\naspects that the review may address. We show that this leads to more detailed\nreviews that better cover the range of aspects that human reviewers identify in\nthe draft. As part of the research, we generate a large-scale review dataset of\n27k papers and 99k reviews that we annotate with aspect prompts, which we make\navailable as a resource for future research.", "published": "2024-02-16 18:43:10", "link": "http://arxiv.org/abs/2402.10886v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal\n  Question-Answering", "abstract": "Existing work on Temporal Question Answering (TQA) has predominantly focused\non questions anchored to specific timestamps or events (e.g. \"Who was the US\npresident in 1970?\"). Little work has studied questions whose temporal context\nis relative to the present time (e.g. \"Who was the previous US president?\"). We\nrefer to this problem as Present-Anchored Temporal QA (PATQA). PATQA poses\nunique challenges: (1) large language models (LLMs) may have outdated\nknowledge, (2) complex temporal relationships (e.g. 'before', 'previous') are\nhard to reason, (3) multi-hop reasoning may be required, and (4) the gold\nanswers of benchmarks must be continuously updated. To address these\nchallenges, we introduce the PAT-Questions benchmark, which includes single and\nmulti-hop temporal questions. The answers in PAT-Questions can be automatically\nrefreshed by re-running SPARQL queries on a knowledge graph, if available. We\nevaluate several state-of-the-art LLMs and a SOTA temporal reasoning model\n(TEMPREASON-T5) on PAT-Questions through direct prompting and\nretrieval-augmented generation (RAG). The results highlight the limitations of\nexisting solutions in PATQA and motivate the need for new methods to improve\nPATQA reasoning capabilities.", "published": "2024-02-16 19:26:09", "link": "http://arxiv.org/abs/2402.11034v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Word Embeddings in the LLM Era", "abstract": "Large Language Models (LLMs) have recently shown remarkable advancement in\nvarious NLP tasks. As such, a popular trend has emerged lately where NLP\nresearchers extract word/sentence/document embeddings from these large\ndecoder-only models and use them for various inference tasks with promising\nresults. However, it is still unclear whether the performance improvement of\nLLM-induced embeddings is merely because of scale or whether underlying\nembeddings they produce significantly differ from classical encoding models\nlike Word2Vec, GloVe, Sentence-BERT (SBERT) or Universal Sentence Encoder\n(USE). This is the central question we investigate in the paper by\nsystematically comparing classical decontextualized and contextualized word\nembeddings with the same for LLM-induced embeddings. Our results show that LLMs\ncluster semantically related words more tightly and perform better on analogy\ntasks in decontextualized settings. However, in contextualized settings,\nclassical models like SimCSE often outperform LLMs in sentence-level similarity\nassessment tasks, highlighting their continued relevance for fine-grained\nsemantics.", "published": "2024-02-16 21:47:30", "link": "http://arxiv.org/abs/2402.11094v3", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "When LLMs Meet Cunning Texts: A Fallacy Understanding Benchmark for\n  Large Language Models", "abstract": "Recently, Large Language Models (LLMs) make remarkable evolutions in language\nunderstanding and generation. Following this, various benchmarks for measuring\nall kinds of capabilities of LLMs have sprung up. In this paper, we challenge\nthe reasoning and understanding abilities of LLMs by proposing a FaLlacy\nUnderstanding Benchmark (FLUB) containing cunning texts that are easy for\nhumans to understand but difficult for models to grasp. Specifically, the\ncunning texts that FLUB focuses on mainly consist of the tricky, humorous, and\nmisleading texts collected from the real internet environment. And we design\nthree tasks with increasing difficulty in the FLUB benchmark to evaluate the\nfallacy understanding ability of LLMs. Based on FLUB, we investigate the\nperformance of multiple representative and advanced LLMs, reflecting our FLUB\nis challenging and worthy of more future study. Interesting discoveries and\nvaluable insights are achieved in our extensive experiments and detailed\nanalyses. We hope that our benchmark can encourage the community to improve\nLLMs' ability to understand fallacies. Our data and codes are available at\nhttps://github.com/THUKElab/FLUB.", "published": "2024-02-16 22:12:53", "link": "http://arxiv.org/abs/2402.11100v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Models as Science Tutors", "abstract": "NLP has recently made exciting progress toward training language models (LMs)\nwith strong scientific problem-solving skills. However, model development has\nnot focused on real-life use-cases of LMs for science, including applications\nin education that require processing long scientific documents. To address\nthis, we introduce TutorEval and TutorChat. TutorEval is a diverse\nquestion-answering benchmark consisting of questions about long chapters from\nSTEM textbooks, written by experts. TutorEval helps measure real-life usability\nof LMs as scientific assistants, and it is the first benchmark combining long\ncontexts, free-form generation, and multi-disciplinary scientific knowledge.\nMoreover, we show that fine-tuning base models with existing dialogue datasets\nleads to poor performance on TutorEval. Therefore, we create TutorChat, a\ndataset of 80,000 long synthetic dialogues about textbooks. We use TutorChat to\nfine-tune Llemma models with 7B and 34B parameters. These LM tutors specialized\nin math have a 32K-token context window, and they excel at TutorEval while\nperforming strongly on GSM8K and MATH. Our datasets build on open-source\nmaterials, and we release our models, data, and evaluations.", "published": "2024-02-16 22:24:13", "link": "http://arxiv.org/abs/2402.11111v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BlendFilter: Advancing Retrieval-Augmented Large Language Models via\n  Query Generation Blending and Knowledge Filtering", "abstract": "Retrieval-augmented Large Language Models (LLMs) offer substantial benefits\nin enhancing performance across knowledge-intensive scenarios. However, these\nmethods often face challenges with complex inputs and encounter difficulties\ndue to noisy knowledge retrieval, notably hindering model effectiveness. To\naddress this issue, we introduce BlendFilter, a novel approach that elevates\nretrieval-augmented LLMs by integrating query generation blending with\nknowledge filtering. BlendFilter proposes the blending process through its\nquery generation method, which integrates both external and internal knowledge\naugmentation with the original query, ensuring comprehensive information\ngathering. Additionally, our distinctive knowledge filtering module capitalizes\non the intrinsic capabilities of the LLM, effectively eliminating extraneous\ndata. We conduct extensive experiments on three open-domain question answering\nbenchmarks, and the findings clearly indicate that our innovative BlendFilter\nsurpasses state-of-the-art baselines significantly.", "published": "2024-02-16 23:28:02", "link": "http://arxiv.org/abs/2402.11129v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM\n  Workflows", "abstract": "Large language models (LLMs) have become a dominant and important tool for\nNLP researchers in a wide range of tasks. Today, many researchers use LLMs in\nsynthetic data generation, task evaluation, fine-tuning, distillation, and\nother model-in-the-loop research workflows. However, challenges arise when\nusing these models that stem from their scale, their closed source nature, and\nthe lack of standardized tooling for these new and emerging workflows. The\nrapid rise to prominence of these models and these unique challenges has had\nimmediate adverse impacts on open science and on the reproducibility of work\nthat uses them. In this paper, we introduce DataDreamer, an open source Python\nlibrary that allows researchers to write simple code to implement powerful LLM\nworkflows. DataDreamer also helps researchers adhere to best practices that we\npropose to encourage open science and reproducibility. The library and\ndocumentation are available at https://github.com/datadreamer-dev/DataDreamer .", "published": "2024-02-16 00:10:26", "link": "http://arxiv.org/abs/2402.10379v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Grounding Language about Belief in a Bayesian Theory-of-Mind", "abstract": "Despite the fact that beliefs are mental states that cannot be directly\nobserved, humans talk about each others' beliefs on a regular basis, often\nusing rich compositional language to describe what others think and know. What\nexplains this capacity to interpret the hidden epistemic content of other\nminds? In this paper, we take a step towards an answer by grounding the\nsemantics of belief statements in a Bayesian theory-of-mind: By modeling how\nhumans jointly infer coherent sets of goals, beliefs, and plans that explain an\nagent's actions, then evaluating statements about the agent's beliefs against\nthese inferences via epistemic logic, our framework provides a conceptual role\nsemantics for belief, explaining the gradedness and compositionality of human\nbelief attributions, as well as their intimate connection with goals and plans.\nWe evaluate this framework by studying how humans attribute goals and beliefs\nwhile watching an agent solve a doors-and-keys gridworld puzzle that requires\ninstrumental reasoning about hidden objects. In contrast to pure logical\ndeduction, non-mentalizing baselines, and mentalizing that ignores the role of\ninstrumental plans, our model provides a much better fit to human goal and\nbelief attributions, demonstrating the importance of theory-of-mind for a\nsemantics of belief.", "published": "2024-02-16 02:47:09", "link": "http://arxiv.org/abs/2402.10416v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Understanding In-Context Learning with a Pelican Soup Framework", "abstract": "Many existing theoretical analyses of in-context learning for natural\nlanguage processing are based on latent variable models that leaves gaps\nbetween theory and practice. We aim to close these gaps by proposing a\ntheoretical framework, the Pelican Soup Framework. In this framework, we\nintroduce (1) the notion of a common sense knowledge base, (2) a general\nformalism for natural language classification tasks, and the notion of (3)\nmeaning association. Under this framework, we can establish a\n$\\mathcal{O}(1/T)$ loss bound for in-context learning, where $T$ is the number\nof example-label pairs in the demonstration. Compared with previous works, our\nbound reflects the effect of the choice of verbalizers and the effect of\ninstruction tuning. An additional notion of \\textit{atom concepts} makes our\nframework possible to explain the generalization to tasks unseen in the\nlanguage model training data. Finally, we propose a toy setup, Calcutec, and a\ndigit addition task that mimics types of distribution shifts a model needs to\novercome to perform in-context learning. We also experiment with GPT2-Large on\nreal-world NLP tasks. Our empirical results demonstrate the efficacy of our\nframework to explain in-context learning.", "published": "2024-02-16 03:20:14", "link": "http://arxiv.org/abs/2402.10424v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Incremental Sequence Labeling: A Tale of Two Shifts", "abstract": "The incremental sequence labeling task involves continuously learning new\nclasses over time while retaining knowledge of the previous ones. Our\ninvestigation identifies two significant semantic shifts: E2O (where the model\nmislabels an old entity as a non-entity) and O2E (where the model labels a\nnon-entity or old entity as a new entity). Previous research has predominantly\nfocused on addressing the E2O problem, neglecting the O2E issue. This\nnegligence results in a model bias towards classifying new data samples as\nbelonging to the new class during the learning process. To address these\nchallenges, we propose a novel framework, Incremental Sequential Labeling\nwithout Semantic Shifts (IS3). Motivated by the identified semantic shifts (E2O\nand O2E), IS3 aims to mitigate catastrophic forgetting in models. As for the\nE2O problem, we use knowledge distillation to maintain the model's\ndiscriminative ability for old entities. Simultaneously, to tackle the O2E\nproblem, we alleviate the model's bias towards new entities through debiased\nloss and optimization levels. Our experimental evaluation, conducted on three\ndatasets with various incremental settings, demonstrates the superior\nperformance of IS3 compared to the previous state-of-the-art method by a\nsignificant margin.The data, code, and scripts are publicly available at\nhttps://github.com/zzz47zzz/codebase-for-incremental-learning-with-llm.", "published": "2024-02-16 04:41:33", "link": "http://arxiv.org/abs/2402.10447v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large\n  Language Model Tuning", "abstract": "Finetuning large language models requires huge GPU memory, restricting the\nchoice to acquire Larger models. While the quantized version of the Low-Rank\nAdaptation technique, named QLoRA, significantly alleviates this issue, finding\nthe efficient LoRA rank is still challenging. Moreover, QLoRA is trained on a\npre-defined rank and, therefore, cannot be reconfigured for its lower ranks\nwithout requiring further fine-tuning steps. This paper proposes QDyLoRA\n-Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach\nfor dynamic low-rank adaptation. Motivated by Dynamic LoRA, QDyLoRA is able to\nefficiently finetune LLMs on a set of pre-defined LoRA ranks. QDyLoRA enables\nfine-tuning Falcon-40b for ranks 1 to 64 on a single 32 GB V100-GPU through one\nround of fine-tuning. Experimental results show that QDyLoRA is competitive to\nQLoRA and outperforms when employing its optimal rank.", "published": "2024-02-16 05:42:17", "link": "http://arxiv.org/abs/2402.10462v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Large Language Models as Zero-shot Dialogue State Tracker through\n  Function Calling", "abstract": "Large language models (LLMs) are increasingly prevalent in conversational\nsystems due to their advanced understanding and generative capabilities in\ngeneral contexts. However, their effectiveness in task-oriented dialogues\n(TOD), which requires not only response generation but also effective dialogue\nstate tracking (DST) within specific tasks and domains, remains less\nsatisfying. In this work, we propose a novel approach FnCTOD for solving DST\nwith LLMs through function calling. This method improves zero-shot DST,\nallowing adaptation to diverse domains without extensive data collection or\nmodel tuning. Our experimental results demonstrate that our approach achieves\nexceptional performance with both modestly sized open-source and also\nproprietary LLMs: with in-context prompting it enables various 7B or 13B\nparameter models to surpass the previous state-of-the-art (SOTA) achieved by\nChatGPT, and improves ChatGPT's performance beating the SOTA by 5.6% average\njoint goal accuracy (JGA). Individual model results for GPT-3.5 and GPT-4 are\nboosted by 4.8% and 14%, respectively. We also show that by fine-tuning on a\nsmall collection of diverse task-oriented dialogues, we can equip modestly\nsized models, specifically a 13B parameter LLaMA2-Chat model, with\nfunction-calling capabilities and DST performance comparable to ChatGPT while\nmaintaining their chat capabilities. We have made the code publicly available\nat https://github.com/facebookresearch/FnCTOD", "published": "2024-02-16 06:13:18", "link": "http://arxiv.org/abs/2402.10466v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Comparing Hallucination Detection Metrics for Multilingual Generation", "abstract": "While many hallucination detection techniques have been evaluated on English\ntext, their effectiveness in multilingual contexts remains unknown. This paper\nassesses how well various factual hallucination detection metrics (lexical\nmetrics like ROUGE and Named Entity Overlap, and Natural Language Inference\n(NLI)-based metrics) identify hallucinations in generated biographical\nsummaries across languages. We compare how well automatic metrics correlate to\neach other and whether they agree with human judgments of factuality. Our\nanalysis reveals that while the lexical metrics are ineffective, NLI-based\nmetrics perform well, correlating with human annotations in many settings and\noften outperforming supervised models. However, NLI metrics are still limited,\nas they do not detect single-fact hallucinations well and fail for\nlower-resource languages. Therefore, our findings highlight the gaps in\nexisiting hallucination detection methods for non-English languages and\nmotivate future research to develop more robust multilingual detection methods\nfor LLM hallucinations.", "published": "2024-02-16 08:10:34", "link": "http://arxiv.org/abs/2402.10496v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can We Verify Step by Step for Incorrect Answer Detection?", "abstract": "Chain-of-Thought (CoT) prompting has marked a significant advancement in\nenhancing the reasoning capabilities of large language models (LLMs). Previous\nstudies have developed various extensions of CoT, which focus primarily on\nenhancing end-task performance. In addition, there has been research on\nassessing the quality of reasoning chains in CoT. This raises an intriguing\nquestion: Is it possible to predict the accuracy of LLM outputs by scrutinizing\nthe reasoning chains they generate? To answer this research question, we\nintroduce a benchmark, R2PE, designed specifically to explore the relationship\nbetween reasoning chains and performance in various reasoning tasks spanning\nfive different domains. This benchmark aims to measure the falsehood of the\nfinal output of LLMs based on the reasoning steps. To make full use of\ninformation in multiple reasoning chains, we propose the process discernibility\nscore (PDS) framework that beats the answer-checking baseline by a large\nmargin. Concretely, this resulted in an average of $5.1\\%$ increase in the F1\nscore and $2.97\\%$ improvement in AUC-PR across all 45 subsets within R2PE. We\nfurther demonstrate our PDS's efficacy in advancing open-domain QA accuracy.", "published": "2024-02-16 09:29:50", "link": "http://arxiv.org/abs/2402.10528v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Strong hallucinations from negation and how to fix them", "abstract": "Despite great performance on many tasks, language models (LMs) still struggle\nwith reasoning, sometimes providing responses that cannot possibly be true\nbecause they stem from logical incoherence. We call such responses\n\\textit{strong hallucinations} and prove that they follow from an LM's\ncomputation of its internal representations for logical operators and outputs\nfrom those representations. Focusing on negation, we provide a novel solution\nin which negation is treated not as another element of a latent representation,\nbut as \\textit{an operation over an LM's latent representations that constrains\nhow they may evolve}. We show that our approach improves model performance in\ncloze prompting and natural language inference tasks with negation without\nrequiring training on sparse negative data.", "published": "2024-02-16 10:11:20", "link": "http://arxiv.org/abs/2402.10543v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "SPAR: Personalized Content-Based Recommendation via Long Engagement\n  Attention", "abstract": "Leveraging users' long engagement histories is essential for personalized\ncontent recommendations. The success of pretrained language models (PLMs) in\nNLP has led to their use in encoding user histories and candidate items,\nframing content recommendations as textual semantic matching tasks. However,\nexisting works still struggle with processing very long user historical text\nand insufficient user-item interaction. In this paper, we introduce a\ncontent-based recommendation framework, SPAR, which effectively tackles the\nchallenges of holistic user interest extraction from the long user engagement\nhistory. It achieves so by leveraging PLM, poly-attention layers and attention\nsparsity mechanisms to encode user's history in a session-based manner. The\nuser and item side features are sufficiently fused for engagement prediction\nwhile maintaining standalone representations for both sides, which is efficient\nfor practical model deployment. Moreover, we enhance user profiling by\nexploiting large language model (LLM) to extract global interests from user\nengagement history. Extensive experiments on two benchmark datasets demonstrate\nthat our framework outperforms existing state-of-the-art (SoTA) methods.", "published": "2024-02-16 10:36:38", "link": "http://arxiv.org/abs/2402.10555v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs\n  ready for the Indian Legal Domain?", "abstract": "Recent advancements in language technology and Artificial Intelligence have\nresulted in numerous Language Models being proposed to perform various tasks in\nthe legal domain ranging from predicting judgments to generating summaries.\nDespite their immense potential, these models have been proven to learn and\nexhibit societal biases and make unfair predictions. In this study, we explore\nthe ability of Large Language Models (LLMs) to perform legal tasks in the\nIndian landscape when social factors are involved. We present a novel metric,\n$\\beta$-weighted $\\textit{Legal Safety Score ($LSS_{\\beta}$)}$, which\nencapsulates both the fairness and accuracy aspects of the LLM. We assess LLMs'\nsafety by considering its performance in the $\\textit{Binary Statutory\nReasoning}$ task and its fairness exhibition with respect to various axes of\ndisparities in the Indian society. Task performance and fairness scores of\nLLaMA and LLaMA--2 models indicate that the proposed $LSS_{\\beta}$ metric can\neffectively determine the readiness of a model for safe usage in the legal\nsector. We also propose finetuning pipelines, utilising specialised legal\ndatasets, as a potential method to mitigate bias and improve model safety. The\nfinetuning procedures on LLaMA and LLaMA--2 models increase the $LSS_{\\beta}$,\nimproving their usability in the Indian legal domain. Our code is publicly\nreleased.", "published": "2024-02-16 10:54:10", "link": "http://arxiv.org/abs/2402.10567v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Threads of Subtlety: Detecting Machine-Generated Texts Through Discourse\n  Motifs", "abstract": "With the advent of large language models (LLM), the line between\nhuman-crafted and machine-generated texts has become increasingly blurred. This\npaper delves into the inquiry of identifying discernible and unique linguistic\nproperties in texts that were written by humans, particularly uncovering the\nunderlying discourse structures of texts beyond their surface structures.\nIntroducing a novel methodology, we leverage hierarchical parse trees and\nrecursive hypergraphs to unveil distinctive discourse patterns in texts\nproduced by both LLMs and humans. Empirical findings demonstrate that, although\nboth LLMs and humans generate distinct discourse patterns influenced by\nspecific domains, human-written texts exhibit more structural variability,\nreflecting the nuanced nature of human writing in different domains. Notably,\nincorporating hierarchical discourse features enhances binary classifiers'\noverall performance in distinguishing between human-written and\nmachine-generated texts, even on out-of-distribution and paraphrased samples.\nThis underscores the significance of incorporating hierarchical discourse\nfeatures in the analysis of text patterns. The code and dataset are available\nat https://github.com/minnesotanlp/threads-of-subtlety.", "published": "2024-02-16 11:20:30", "link": "http://arxiv.org/abs/2402.10586v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do Llamas Work in English? On the Latent Language of Multilingual\n  Transformers", "abstract": "We ask whether multilingual language models trained on unbalanced,\nEnglish-dominated corpora use English as an internal pivot language -- a\nquestion of key importance for understanding how language models function and\nthe origins of linguistic bias. Focusing on the Llama-2 family of transformer\nmodels, our study uses carefully constructed non-English prompts with a unique\ncorrect single-token continuation. From layer to layer, transformers gradually\nmap an input embedding of the final prompt token to an output embedding from\nwhich next-token probabilities are computed. Tracking intermediate embeddings\nthrough their high-dimensional space reveals three distinct phases, whereby\nintermediate embeddings (1) start far away from output token embeddings; (2)\nalready allow for decoding a semantically correct next token in the middle\nlayers, but give higher probability to its version in English than in the input\nlanguage; (3) finally move into an input-language-specific region of the\nembedding space. We cast these results into a conceptual model where the three\nphases operate in \"input space\", \"concept space\", and \"output space\",\nrespectively. Crucially, our evidence suggests that the abstract \"concept\nspace\" lies closer to English than to other languages, which may have important\nconsequences regarding the biases held by multilingual language models.", "published": "2024-02-16 11:21:28", "link": "http://arxiv.org/abs/2402.10588v4", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Efficiency at Scale: Investigating the Performance of Diminutive\n  Language Models in Clinical Tasks", "abstract": "The entry of large language models (LLMs) into research and commercial spaces\nhas led to a trend of ever-larger models, with initial promises of\ngeneralisability, followed by a widespread desire to downsize and create\nspecialised models without the need for complete fine-tuning, using Parameter\nEfficient Fine-tuning (PEFT) methods. We present an investigation into the\nsuitability of different PEFT methods to clinical decision-making tasks, across\na range of model sizes, including extremely small models with as few as $25$\nmillion parameters.\n  Our analysis shows that the performance of most PEFT approaches varies\nsignificantly from one task to another, with the exception of LoRA, which\nmaintains relatively high performance across all model sizes and tasks,\ntypically approaching or matching full fine-tuned performance. The\neffectiveness of PEFT methods in the clinical domain is evident, particularly\nfor specialised models which can operate on low-cost, in-house computing\ninfrastructure. The advantages of these models, in terms of speed and reduced\ntraining costs, dramatically outweighs any performance gain from large\nfoundation LLMs. Furthermore, we highlight how domain-specific pre-training\ninteracts with PEFT methods and model size, and discuss how these factors\ninterplay to provide the best efficiency-performance trade-off. Full code\navailable at: tbd.", "published": "2024-02-16 11:30:11", "link": "http://arxiv.org/abs/2402.10597v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "When \"Competency\" in Reasoning Opens the Door to Vulnerability:\n  Jailbreaking LLMs via Novel Complex Ciphers", "abstract": "Recent advancements in Large Language Model (LLM) safety have primarily\nfocused on mitigating attacks crafted in natural language or common ciphers\n(e.g. Base64), which are likely integrated into newer models' safety training.\nHowever, we reveal a paradoxical vulnerability: as LLMs advance in reasoning,\nthey inadvertently become more susceptible to novel jailbreaking attacks.\nEnhanced reasoning enables LLMs to interpret complex instructions and decode\ncomplex user-defined ciphers, creating an exploitable security gap. To study\nthis vulnerability, we introduce Attacks using Custom Encryptions (ACE), a\njailbreaking technique that encodes malicious queries with novel ciphers.\nExtending ACE, we introduce Layered Attacks using Custom Encryptions (LACE),\nwhich applies multi-layer ciphers to amplify attack complexity. Furthermore, we\ndevelop CipherBench, a benchmark designed to evaluate LLMs' accuracy in\ndecoding encrypted benign text. Our experiments reveal a critical trade-off:\nLLMs that are more capable of decoding ciphers are more vulnerable to these\njailbreaking attacks, with success rates on GPT-4o escalating from 40% under\nACE to 78% with LACE. These findings highlight a critical insight: as LLMs\nbecome more adept at deciphering complex user ciphers--many of which cannot be\npreemptively included in safety training--they become increasingly exploitable.", "published": "2024-02-16 11:37:05", "link": "http://arxiv.org/abs/2402.10601v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "`Keep it Together': Enforcing Cohesion in Extractive Summaries by\n  Simulating Human Memory", "abstract": "Extractive summaries are usually presented as lists of sentences with no\nexpected cohesion between them. In this paper, we aim to enforce cohesion\nwhilst controlling for informativeness and redundancy in summaries, in cases\nwhere the input exhibits high redundancy. The pipeline controls for redundancy\nin long inputs as it is consumed, and balances informativeness and cohesion\nduring sentence selection. Our sentence selector simulates human memory to keep\ntrack of topics --modeled as lexical chains--, enforcing cohesive ties between\nnoun phrases. Across a variety of domains, our experiments revealed that it is\npossible to extract highly cohesive summaries that nevertheless read as\ninformative to humans as summaries extracted by only accounting for\ninformativeness or redundancy. The extracted summaries exhibit smooth topic\ntransitions between sentences as signaled by lexical chains, with chains\nspanning adjacent or near-adjacent sentences.", "published": "2024-02-16 12:43:26", "link": "http://arxiv.org/abs/2402.10643v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Linear Transformers with Learnable Kernel Functions are Better\n  In-Context Models", "abstract": "Advancing the frontier of subquadratic architectures for Language Models\n(LMs) is crucial in the rapidly evolving field of natural language processing.\nCurrent innovations, including State Space Models, were initially celebrated\nfor surpassing Transformer performance on language modeling tasks. However,\nthese models have revealed deficiencies in essential In-Context Learning\ncapabilities - a domain where the Transformer traditionally shines. The Based\nmodel emerged as a hybrid solution, blending a Linear Transformer with a kernel\ninspired by the Taylor expansion of exponential functions, augmented by\nconvolutional networks. Mirroring the Transformer's in-context adeptness, it\nbecame a strong contender in the field. In our work, we present a singular,\nelegant alteration to the Based kernel that amplifies its In-Context Learning\nabilities evaluated with the Multi-Query Associative Recall task and overall\nlanguage modeling process, as demonstrated on the Pile dataset.", "published": "2024-02-16 12:44:15", "link": "http://arxiv.org/abs/2402.10644v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Can Separators Improve Chain-of-Thought Prompting?", "abstract": "Chain-of-thought (CoT) prompting is a simple and effective method for\nimproving the reasoning capabilities of Large Language Models (LLMs). The basic\nidea of CoT is to let LLMs break down their thought processes step-by-step by\nputting exemplars in the input prompt. However, the densely structured prompt\nexemplars of CoT may cause the cognitive overload of LLMs. Inspired by human\ncognition, we introduce COT-SEP, a method that strategically employs separators\nat the end of each exemplar in CoT prompting. These separators are designed to\nhelp the LLMs understand their thought processes better while reasoning.\nInterestingly, it turns out that COT-SEP significantly improves the LLMs'\nperformances on complex reasoning tasks (e.g., GSM8K, AQuA, CSQA), compared\nwith the vanilla CoT, which does not use separators. We also study the effects\nof the type and the location of separators tested on multiple LLMs, including\nGPT-3.5-Turbo, GPT-4, and LLaMA-2 7B.", "published": "2024-02-16 12:46:16", "link": "http://arxiv.org/abs/2402.10645v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via\n  Vision-Language Foundation Models", "abstract": "Object navigation (ObjectNav) requires an agent to navigate through unseen\nenvironments to find queried objects. Many previous methods attempted to solve\nthis task by relying on supervised or reinforcement learning, where they are\ntrained on limited household datasets with close-set objects. However, two key\nchallenges are unsolved: understanding free-form natural language instructions\nthat demand open-set objects, and generalizing to new environments in a\nzero-shot manner. Aiming to solve the two challenges, in this paper, we propose\nOpenFMNav, an Open-set Foundation Model based framework for zero-shot object\nNavigation. We first unleash the reasoning abilities of large language models\n(LLMs) to extract proposed objects from natural language instructions that meet\nthe user's demand. We then leverage the generalizability of large vision\nlanguage models (VLMs) to actively discover and detect candidate objects from\nthe scene, building a Versatile Semantic Score Map (VSSM). Then, by conducting\ncommon sense reasoning on VSSM, our method can perform effective\nlanguage-guided exploration and exploitation of the scene and finally reach the\ngoal. By leveraging the reasoning and generalizing abilities of foundation\nmodels, our method can understand free-form human instructions and perform\neffective open-set zero-shot navigation in diverse environments. Extensive\nexperiments on the HM3D ObjectNav benchmark show that our method surpasses all\nthe strong baselines on all metrics, proving our method's effectiveness.\nFurthermore, we perform real robot demonstrations to validate our method's\nopen-set-ness and generalizability to real-world environments.", "published": "2024-02-16 13:21:33", "link": "http://arxiv.org/abs/2402.10670v2", "categories": ["cs.CL", "cs.RO"], "primary_category": "cs.CL"}
{"title": "LongHeads: Multi-Head Attention is Secretly a Long Context Processor", "abstract": "Large language models (LLMs) have achieved impressive performance in numerous\ndomains but often struggle to process lengthy inputs effectively and\nefficiently due to limited length generalization and attention's quadratic\ncomputational demands. Many sought to mitigate this by restricting the\nattention window within the pre-trained length. However, these methods\nintroduce new issues such as ignoring the middle context and requiring\nadditional training. To address these problems, we propose LongHeads, a\ntraining-free framework that enhances LLM's long context ability by unlocking\nmulti-head attention's untapped potential. Instead of allowing each head to\nattend to the full sentence, which struggles with generalizing to longer\nsequences due to out-of-distribution (OOD) issues, we allow each head to\nprocess in-distribution length by selecting and attending to important context\nchunks. To this end, we propose a chunk selection strategy that relies on the\ninherent correlation between the query and the key representations, efficiently\ndistributing context chunks to different heads. In this way, each head ensures\nit can effectively process attended tokens within the trained length, while\ndifferent heads in different layers can collectively process longer contexts.\nLongHeads works efficiently in linear time, fits seamlessly with many LLMs that\nuse relative positional encoding. LongHeads achieves 100% accuracy at the 128k\nlength on passkey retrieval task, verifying LongHeads's efficacy in extending\nthe usable context window for existing models. We release our code at\nhttps://github.com/LuLuLuyi/LongHeads .", "published": "2024-02-16 13:39:34", "link": "http://arxiv.org/abs/2402.10685v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Precision and Recall to assess the quality and diversity of\n  LLMs", "abstract": "We introduce a novel evaluation framework for Large Language Models (LLMs)\nsuch as \\textsc{Llama-2} and \\textsc{Mistral}, focusing on importing Precision\nand Recall metrics from image generation to text generation. This approach\nallows for a nuanced assessment of the quality and diversity of generated text\nwithout the need for aligned corpora. By conducting a comprehensive evaluation\nof state-of-the-art language models, the study reveals new insights into their\nperformance on open-ended generation tasks, which are not adequately captured\nby traditional benchmarks. The findings highlight a trade-off between the\nquality and diversity of generated samples, particularly when models are\nfine-tuned on instruction dataset or with human feedback. This work extends the\ntoolkit for distribution-based NLP evaluation, offering insights into the\npractical capabilities and challenges that current LLMs face in generating\ndiverse and high-quality text. We release our code and data.", "published": "2024-02-16 13:53:26", "link": "http://arxiv.org/abs/2402.10693v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient\n  Language Model Inference", "abstract": "The development of state-of-the-art generative large language models (LLMs)\ndisproportionately relies on English-centric tokenizers, vocabulary and\npre-training data. Despite the fact that some LLMs have multilingual\ncapabilities, recent studies have shown that their inference efficiency\ndeteriorates when generating text in languages other than English. This results\nin increased inference time and costs. Cross-lingual vocabulary adaptation\n(CVA) methods have been proposed for adapting models to a target language\naiming to improve downstream performance. However, the effectiveness of these\nmethods on increasing inference efficiency of generative LLMs has yet to be\nexplored. In this paper, we perform an empirical study of five CVA methods on\nfour generative LLMs (including monolingual and multilingual models) across\nfour typologically-diverse languages and four natural language understanding\ntasks. We find that CVA substantially contributes to LLM inference speedups of\nup to 271.5\\%. We also show that adapting LLMs that have been pre-trained on\nmore balanced multilingual data results in downstream performance comparable to\nthe original models.", "published": "2024-02-16 14:15:15", "link": "http://arxiv.org/abs/2402.10712v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GenRES: Rethinking Evaluation for Generative Relation Extraction in the\n  Era of Large Language Models", "abstract": "The field of relation extraction (RE) is experiencing a notable shift towards\ngenerative relation extraction (GRE), leveraging the capabilities of large\nlanguage models (LLMs). However, we discovered that traditional relation\nextraction (RE) metrics like precision and recall fall short in evaluating GRE\nmethods. This shortfall arises because these metrics rely on exact matching\nwith human-annotated reference relations, while GRE methods often produce\ndiverse and semantically accurate relations that differ from the references. To\nfill this gap, we introduce GenRES for a multi-dimensional assessment in terms\nof the topic similarity, uniqueness, granularity, factualness, and completeness\nof the GRE results. With GenRES, we empirically identified that (1)\nprecision/recall fails to justify the performance of GRE methods; (2)\nhuman-annotated referential relations can be incomplete; (3) prompting LLMs\nwith a fixed set of relations or entities can cause hallucinations. Next, we\nconducted a human evaluation of GRE methods that shows GenRES is consistent\nwith human preferences for RE quality. Last, we made a comprehensive evaluation\nof fourteen leading LLMs using GenRES across document, bag, and sentence level\nRE datasets, respectively, to set the benchmark for future research in GRE", "published": "2024-02-16 15:01:24", "link": "http://arxiv.org/abs/2402.10744v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ToolSword: Unveiling Safety Issues of Large Language Models in Tool\n  Learning Across Three Stages", "abstract": "Tool learning is widely acknowledged as a foundational approach or deploying\nlarge language models (LLMs) in real-world scenarios. While current research\nprimarily emphasizes leveraging tools to augment LLMs, it frequently neglects\nemerging safety considerations tied to their application. To fill this gap, we\npresent *ToolSword*, a comprehensive framework dedicated to meticulously\ninvestigating safety issues linked to LLMs in tool learning. Specifically,\nToolSword delineates six safety scenarios for LLMs in tool learning,\nencompassing **malicious queries** and **jailbreak attacks** in the input\nstage, **noisy misdirection** and **risky cues** in the execution stage, and\n**harmful feedback** and **error conflicts** in the output stage. Experiments\nconducted on 11 open-source and closed-source LLMs reveal enduring safety\nchallenges in tool learning, such as handling harmful queries, employing risky\ntools, and delivering detrimental feedback, which even GPT-4 is susceptible to.\nMoreover, we conduct further studies with the aim of fostering research on tool\nlearning safety. The data is released in\nhttps://github.com/Junjie-Ye/ToolSword.", "published": "2024-02-16 15:19:46", "link": "http://arxiv.org/abs/2402.10753v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Inference to the Best Explanation in Large Language Models", "abstract": "While Large Language Models (LLMs) have found success in real-world\napplications, their underlying explanatory process is still poorly understood.\nThis paper proposes IBE-Eval, a framework inspired by philosophical accounts on\nInference to the Best Explanation (IBE) to advance the interpretation and\nevaluation of LLMs' explanations. IBE-Eval estimates the plausibility of\nnatural language explanations through a combination of explicit logical and\nlinguistic features including: consistency, parsimony, coherence, and\nuncertainty. Extensive experiments are conducted on Causal Question Answering\n(CQA), where \\textit{IBE-Eval} is tasked to select the most plausible causal\nexplanation amongst competing ones generated by LLMs (i.e., GPT 3.5 and Llama\n2). The experiments reveal that IBE-Eval can successfully identify the best\nexplanation with up to 77\\% accuracy ($\\approx 27\\%$ above random), improving\nupon a GPT 3.5-as-a-Judge baseline ($\\approx+17\\%$) while being intrinsically\nmore efficient and interpretable. Additional analyses suggest that, despite\nmodel-specific variances, LLM-generated explanations tend to conform to IBE\ncriteria and that IBE-Eval is significantly correlated with human judgment,\nopening up opportunities for future development of automated explanation\nverification tools.", "published": "2024-02-16 15:41:23", "link": "http://arxiv.org/abs/2402.10767v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "How Reliable Are Automatic Evaluation Methods for Instruction-Tuned\n  LLMs?", "abstract": "Work on instruction-tuned Large Language Models (LLMs) has used automatic\nmethods based on text overlap and LLM judgments as cost-effective alternatives\nto human evaluation. In this paper, we perform a meta-evaluation of such\nmethods and assess their reliability across a broad range of tasks. In\nevaluating how well automatic methods align with human evaluations, correlation\nmetrics are the most commonly employed method despite their inherent\nlimitations when dealing with ties and different scales. To address these\nshortcomings, we use Pairwise Accuracy as an alternative to standard\ncorrelation measures. We observe that while automatic evaluation methods can\napproximate human ratings under specific conditions, their validity is highly\ncontext-dependent. Specifically, the simple ROUGE-L metric correlates very well\nwith human ratings for short-answer English tasks but is unreliable in\nfree-form generation tasks and cross-lingual scenarios. The effectiveness of\nthe more advanced method of using GPT-4 as a judge diminishes significantly if\nreference answers are not included in the prompt, which is the scenario where\nthis method has the potential to provide the most value compared to other\nmetrics. Our findings enhance the understanding of how automatic methods should\nbe applied and interpreted when developing and evaluating instruction-tuned\nLLMs.", "published": "2024-02-16 15:48:33", "link": "http://arxiv.org/abs/2402.10770v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Quantifying the Persona Effect in LLM Simulations", "abstract": "Large language models (LLMs) have shown remarkable promise in simulating\nhuman language and behavior. This study investigates how integrating persona\nvariables-demographic, social, and behavioral factors-impacts LLMs' ability to\nsimulate diverse perspectives. We find that persona variables account for <10%\nvariance in annotations in existing subjective NLP datasets. Nonetheless,\nincorporating persona variables via prompting in LLMs provides modest but\nstatistically significant improvements. Persona prompting is most effective in\nsamples where many annotators disagree, but their disagreements are relatively\nminor. Notably, we find a linear relationship in our setting: the stronger the\ncorrelation between persona variables and human annotations, the more accurate\nthe LLM predictions are using persona prompting. In a zero-shot setting, a\npowerful 70b model with persona prompting captures 81% of the annotation\nvariance achievable by linear regression trained on ground truth annotations.\nHowever, for most subjective NLP datasets, where persona variables have limited\nexplanatory power, the benefits of persona prompting are limited.", "published": "2024-02-16 16:35:35", "link": "http://arxiv.org/abs/2402.10811v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Universal Prompt Optimizer for Safe Text-to-Image Generation", "abstract": "Text-to-Image (T2I) models have shown great performance in generating images\nbased on textual prompts. However, these models are vulnerable to unsafe input\nto generate unsafe content like sexual, harassment and illegal-activity images.\nExisting studies based on image checker, model fine-tuning and embedding\nblocking are impractical in real-world applications. Hence, we propose the\nfirst universal prompt optimizer for safe T2I (POSI) generation in black-box\nscenario. We first construct a dataset consisting of toxic-clean prompt pairs\nby GPT-3.5 Turbo. To guide the optimizer to have the ability of converting\ntoxic prompt to clean prompt while preserving semantic information, we design a\nnovel reward function measuring toxicity and text alignment of generated images\nand train the optimizer through Proximal Policy Optimization. Experiments show\nthat our approach can effectively reduce the likelihood of various T2I models\nin generating inappropriate images, with no significant impact on text\nalignment. It is also flexible to be combined with methods to achieve better\nperformance. Our code is available at https://github.com/wu-zongyu/POSI.", "published": "2024-02-16 18:36:36", "link": "http://arxiv.org/abs/2402.10882v6", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "FinTral: A Family of GPT-4 Level Multimodal Financial Large Language\n  Models", "abstract": "We introduce FinTral, a suite of state-of-the-art multimodal large language\nmodels (LLMs) built upon the Mistral-7b model and tailored for financial\nanalysis. FinTral integrates textual, numerical, tabular, and image data. We\nenhance FinTral with domain-specific pretraining, instruction fine-tuning, and\nRLAIF training by exploiting a large collection of textual and visual datasets\nwe curate for this work. We also introduce an extensive benchmark featuring\nnine tasks and 25 datasets for evaluation, including hallucinations in the\nfinancial domain. Our FinTral model trained with direct preference optimization\nemploying advanced Tools and Retrieval methods, dubbed FinTral-DPO-T&R,\ndemonstrates an exceptional zero-shot performance. It outperforms ChatGPT-3.5\nin all tasks and surpasses GPT-4 in five out of nine tasks, marking a\nsignificant advancement in AI-driven financial technology. We also demonstrate\nthat FinTral has the potential to excel in real-time analysis and\ndecision-making in diverse financial contexts. The GitHub repository for\nFinTral is available at \\url{https://github.com/UBC-NLP/fintral}.", "published": "2024-02-16 05:05:12", "link": "http://arxiv.org/abs/2402.10986v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WilKE: Wise-Layer Knowledge Editor for Lifelong Knowledge Editing", "abstract": "Knowledge editing aims to rectify inaccuracies in large language models\n(LLMs) without costly retraining for outdated or erroneous knowledge. However,\ncurrent knowledge editing methods primarily focus on single editing, failing to\nmeet the requirements for lifelong editing. This study reveals a performance\ndegradation encountered by knowledge editing in lifelong editing, characterized\nby toxicity buildup and toxicity flash, with the primary cause identified as\npattern unmatch. We introduce a knowledge editing approach named Wise-Layer\nKnowledge Editor (WilKE), which selects editing layer based on the pattern\nmatching degree of editing knowledge across different layers in language\nmodels. Experimental results demonstrate that, in lifelong editing, WilKE\nexhibits an average improvement of 46.2% and 67.8% on editing GPT2-XL and GPT-J\nrelative to state-of-the-art knowledge editing methods.", "published": "2024-02-16 05:29:59", "link": "http://arxiv.org/abs/2402.10987v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "\"Understanding AI\": Semantic Grounding in Large Language Models", "abstract": "Do LLMs understand the meaning of the texts they generate? Do they possess a\nsemantic grounding? And how could we understand whether and what they\nunderstand? I start the paper with the observation that we have recently\nwitnessed a generative turn in AI, since generative models, including LLMs, are\nkey for self-supervised learning. To assess the question of semantic grounding,\nI distinguish and discuss five methodological ways. The most promising way is\nto apply core assumptions of theories of meaning in philosophy of mind and\nlanguage to LLMs. Grounding proves to be a gradual affair with a\nthree-dimensional distinction between functional, social and causal grounding.\nLLMs show basic evidence in all three dimensions. A strong argument is that\nLLMs develop world models. Hence, LLMs are neither stochastic parrots nor\nsemantic zombies, but already understand the language they generate, at least\nin an elementary sense.", "published": "2024-02-16 14:23:55", "link": "http://arxiv.org/abs/2402.10992v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ASGEA: Exploiting Logic Rules from Align-Subgraphs for Entity Alignment", "abstract": "Entity alignment (EA) aims to identify entities across different knowledge\ngraphs that represent the same real-world objects. Recent embedding-based EA\nmethods have achieved state-of-the-art performance in EA yet faced\ninterpretability challenges as they purely rely on the embedding distance and\nneglect the logic rules behind a pair of aligned entities. In this paper, we\npropose the Align-Subgraph Entity Alignment (ASGEA) framework to exploit logic\nrules from Align-Subgraphs. ASGEA uses anchor links as bridges to construct\nAlign-Subgraphs and spreads along the paths across KGs, which distinguishes it\nfrom the embedding-based methods. Furthermore, we design an interpretable\nPath-based Graph Neural Network, ASGNN, to effectively identify and integrate\nthe logic rules across KGs. We also introduce a node-level multi-modal\nattention mechanism coupled with multi-modal enriched anchors to augment the\nAlign-Subgraph. Our experimental results demonstrate the superior performance\nof ASGEA over the existing embedding-based methods in both EA and Multi-Modal\nEA (MMEA) tasks.", "published": "2024-02-16 17:03:05", "link": "http://arxiv.org/abs/2402.11000v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Value Biases: How LLMs Deviate Towards the Ideal", "abstract": "Large-Language-Models (LLMs) are deployed in a wide range of applications,\nand their response has an increasing social impact. Understanding the\nnon-deliberate(ive) mechanism of LLMs in giving responses is essential in\nexplaining their performance and discerning their biases in real-world\napplications. This is analogous to human studies, where such inadvertent\nresponses are referred to as sampling. We study this sampling of LLMs in light\nof value bias and show that the sampling of LLMs tends to favour high-value\noptions. Value bias corresponds to this shift of response from the most likely\ntowards an ideal value represented in the LLM. In fact, this effect can be\nreproduced even with new entities learnt via in-context prompting. We show that\nthis bias manifests in unexpected places and has implications on relevant\napplication scenarios, like choosing exemplars. The results show that value\nbias is strong in LLMs across different categories, similar to the results\nfound in human studies.", "published": "2024-02-16 18:28:43", "link": "http://arxiv.org/abs/2402.11005v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dense Passage Retrieval: Is it Retrieving?", "abstract": "Dense passage retrieval (DPR) is the first step in the retrieval augmented\ngeneration (RAG) paradigm for improving the performance of large language\nmodels (LLM). DPR fine-tunes pre-trained networks to enhance the alignment of\nthe embeddings between queries and relevant textual data. A deeper\nunderstanding of DPR fine-tuning will be required to fundamentally unlock the\nfull potential of this approach. In this work, we explore DPR-trained models\nmechanistically by using a combination of probing, layer activation analysis,\nand model editing. Our experiments show that DPR training decentralizes how\nknowledge is stored in the network, creating multiple access pathways to the\nsame information. We also uncover a limitation in this training style: the\ninternal knowledge of the pre-trained model bounds what the retrieval model can\nretrieve. These findings suggest a few possible directions for dense retrieval:\n(1) expose the DPR training process to more knowledge so more can be\ndecentralized, (2) inject facts as decentralized representations, (3) model and\nincorporate knowledge uncertainty in the retrieval process, and (4) directly\nmap internal model knowledge to a knowledge base.", "published": "2024-02-16 19:28:52", "link": "http://arxiv.org/abs/2402.11035v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Large Language Models Fall Short: Understanding Complex Relationships in\n  Detective Narratives", "abstract": "Existing datasets for narrative understanding often fail to represent the\ncomplexity and uncertainty of relationships in real-life social scenarios. To\naddress this gap, we introduce a new benchmark, Conan, designed for extracting\nand analysing intricate character relation graphs from detective narratives.\nSpecifically, we designed hierarchical relationship categories and manually\nextracted and annotated role-oriented relationships from the perspectives of\nvarious characters, incorporating both public relationships known to most\ncharacters and secret ones known to only a few. Our experiments with advanced\nLarge Language Models (LLMs) like GPT-3.5, GPT-4, and Llama2 reveal their\nlimitations in inferencing complex relationships and handling longer\nnarratives. The combination of the Conan dataset and our pipeline strategy is\ngeared towards understanding the ability of LLMs to comprehend nuanced\nrelational dynamics in narrative contexts.", "published": "2024-02-16 19:59:45", "link": "http://arxiv.org/abs/2402.11051v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "II-MMR: Identifying and Improving Multi-modal Multi-hop Reasoning in\n  Visual Question Answering", "abstract": "Visual Question Answering (VQA) often involves diverse reasoning scenarios\nacross Vision and Language (V&L). Most prior VQA studies, however, have merely\nfocused on assessing the model's overall accuracy without evaluating it on\ndifferent reasoning cases. Furthermore, some recent works observe that\nconventional Chain-of-Thought (CoT) prompting fails to generate effective\nreasoning for VQA, especially for complex scenarios requiring multi-hop\nreasoning. In this paper, we propose II-MMR, a novel idea to identify and\nimprove multi-modal multi-hop reasoning in VQA. In specific, II-MMR takes a VQA\nquestion with an image and finds a reasoning path to reach its answer using two\nnovel language promptings: (i) answer prediction-guided CoT prompt, or (ii)\nknowledge triplet-guided prompt. II-MMR then analyzes this path to identify\ndifferent reasoning cases in current VQA benchmarks by estimating how many hops\nand what types (i.e., visual or beyond-visual) of reasoning are required to\nanswer the question. On popular benchmarks including GQA and A-OKVQA, II-MMR\nobserves that most of their VQA questions are easy to answer, simply demanding\n\"single-hop\" reasoning, whereas only a few questions require \"multi-hop\"\nreasoning. Moreover, while the recent V&L model struggles with such complex\nmulti-hop reasoning questions even using the traditional CoT method, II-MMR\nshows its effectiveness across all reasoning cases in both zero-shot and\nfine-tuning settings.", "published": "2024-02-16 20:14:47", "link": "http://arxiv.org/abs/2402.11058v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Large Language Models for Causal Discovery: Current Landscape and Future\n  Directions", "abstract": "Causal discovery (CD) and Large Language Models (LLMs) have emerged as\ntransformative fields in artificial intelligence that have evolved largely\nindependently. While CD specializes in uncovering cause-effect relationships\nfrom data, and LLMs excel at natural language processing and generation, their\nintegration presents unique opportunities for advancing causal understanding.\nThis survey examines how LLMs are transforming CD across three key dimensions:\ndirect causal extraction from text, integration of domain knowledge into\nstatistical methods, and refinement of causal structures. We systematically\nanalyze approaches that leverage LLMs for CD tasks, highlighting their\ninnovative use of metadata and natural language for causal inference. Our\nanalysis reveals both LLMs' potential to enhance traditional CD methods and\ntheir current limitations as imperfect expert systems. We identify key research\ngaps, outline evaluation frameworks and benchmarks for LLM-based causal\ndiscovery, and advocate future research efforts for leveraging LLMs in\ncausality research. As the first comprehensive examination of the synergy\nbetween LLMs and CD, this work lays the groundwork for future advances in the\nfield.", "published": "2024-02-16 20:48:53", "link": "http://arxiv.org/abs/2402.11068v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AFaCTA: Assisting the Annotation of Factual Claim Detection with\n  Reliable LLM Annotators", "abstract": "With the rise of generative AI, automated fact-checking methods to combat\nmisinformation are becoming more and more important. However, factual claim\ndetection, the first step in a fact-checking pipeline, suffers from two key\nissues that limit its scalability and generalizability: (1) inconsistency in\ndefinitions of the task and what a claim is, and (2) the high cost of manual\nannotation. To address (1), we review the definitions in related work and\npropose a unifying definition of factual claims that focuses on verifiability.\nTo address (2), we introduce AFaCTA (Automatic Factual Claim deTection\nAnnotator), a novel framework that assists in the annotation of factual claims\nwith the help of large language models (LLMs). AFaCTA calibrates its annotation\nconfidence with consistency along three predefined reasoning paths. Extensive\nevaluation and experiments in the domain of political speech reveal that AFaCTA\ncan efficiently assist experts in annotating factual claims and training\nhigh-quality classifiers, and can work with or without expert supervision. Our\nanalyses also result in PoliClaim, a comprehensive claim detection dataset\nspanning diverse political topics.", "published": "2024-02-16 20:59:57", "link": "http://arxiv.org/abs/2402.11073v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Navigating the Dual Facets: A Comprehensive Evaluation of Sequential\n  Memory Editing in Large Language Models", "abstract": "Memory Editing (ME) has emerged as an efficient method to modify erroneous\nfacts or inject new facts into Large Language Models (LLMs). Two mainstream ME\nmethods exist: parameter-modifying ME and parameter-preserving ME (integrating\nextra modules while preserving original parameters). Regrettably, previous\nstudies on ME evaluation have two critical limitations: (i) evaluating LLMs\nwith single edit only, neglecting the need for continuous editing, and (ii)\nevaluations focusing solely on basic factual triples, overlooking broader LLM\ncapabilities like logical reasoning and reading understanding. This study\naddresses these limitations with contributions threefold: (i) We explore how ME\naffects a wide range of fundamental capabilities of LLMs under sequential\nediting. Experimental results reveal an intriguing phenomenon: Most\nparameter-modifying ME consistently degrade performance across all tasks after\na few sequential edits. In contrast, parameter-preserving ME effectively\nmaintains LLMs' fundamental capabilities but struggles to accurately recall\nedited knowledge presented in a different format. (ii) We extend our evaluation\nto different editing settings, such as layers to edit, model size, instruction\ntuning, etc. Experimental findings indicate several strategies that can\npotentially mitigate the adverse effects of ME. (iii) We further explain why\nparameter-modifying ME damages LLMs from three dimensions: parameter changes\nafter editing, language modeling capability, and the in-context learning\ncapability. Our in-depth study advocates more careful use of ME in real-world\nscenarios.", "published": "2024-02-16 23:08:55", "link": "http://arxiv.org/abs/2402.11122v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Where is the answer? Investigating Positional Bias in Language Model\n  Knowledge Extraction", "abstract": "Large language models require updates to remain up-to-date or adapt to new\ndomains by fine-tuning them with new documents. One key is memorizing the\nlatest information in a way that the memorized information is extractable with\na query prompt. However, LLMs suffer from a phenomenon called perplexity curse;\ndespite minimizing document perplexity during fine-tuning, LLMs struggle to\nextract information through a prompt sentence. In this new knowledge\nacquisition and extraction, we find a very intriguing fact that LLMs can\naccurately answer questions about the first sentence, but they struggle to\nextract information described in the middle or end of the documents used for\nfine-tuning. Our study suggests that the auto-regressive training causes this\nissue; each token is prompted by reliance on all previous tokens, which hinders\nthe model from recalling information from training documents by question\nprompts. To conduct the in-depth study, we publish both synthetic and real\ndatasets, enabling the evaluation of the QA performance w.r.t. the position of\nthe corresponding answer in a document. Our investigation shows that even a\nlarge model suffers from the perplexity curse, but regularization such as\ndenoising auto-regressive loss can enhance the information extraction from\ndiverse positions. These findings will be (i) a key to improving knowledge\nextraction from LLMs and (ii) new elements to discuss the trade-off between RAG\nand fine-tuning in adapting LLMs to a new domain.", "published": "2024-02-16 06:29:16", "link": "http://arxiv.org/abs/2402.12170v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Orca-Math: Unlocking the potential of SLMs in Grade School Math", "abstract": "Mathematical word problem-solving has long been recognized as a complex task\nfor small language models (SLMs). A recent study hypothesized that the smallest\nmodel size, needed to achieve over 80% accuracy on the GSM8K benchmark, is 34\nbillion parameters. To reach this level of performance with smaller models,\nresearcher often train SLMs to generate Python code or use tools to help avoid\ncalculation errors. Additionally, they employ ensembling, where outputs of up\nto 100 model runs are combined to arrive at a more accurate result. Result\nselection is done using consensus, majority vote or a separate a verifier model\nused in conjunction with the SLM. Ensembling provides a substantial boost in\naccuracy but at a significant cost increase with multiple calls to the model\n(e.g., Phi-GSM uses top-48 to boost the performance from 68.2 to 81.5).\n  In this work, we present Orca-Math, a 7-billion-parameter SLM based on the\nMistral-7B, which achieves 86.81% on GSM8k without the need for multiple model\ncalls or the use of verifiers, code execution or any other external tools. Our\napproach has the following key elements: (1) A high quality synthetic dataset\nof 200K math problems created using a multi-agent setup where agents\ncollaborate to create the data, (2) An iterative learning techniques that\nenables the SLM to practice solving problems, receive feedback on its solutions\nand learn from preference pairs incorporating the SLM solutions and the\nfeedback. When trained with Supervised Fine-Tuning alone, Orca-Math achieves\n81.50% on GSM8k pass@1 metric. With iterative preference learning, Orca-Math\nachieves 86.81% pass@1. Orca-Math surpasses the performance of significantly\nlarger models such as LLAMA-2-70B, WizardMath-70B, Gemini-Pro, ChatGPT-3.5. It\nalso significantly outperforms other smaller models while using much smaller\ndata (hundreds of thousands vs. millions of problems).", "published": "2024-02-16 23:44:38", "link": "http://arxiv.org/abs/2402.14830v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Novel BERT-based Classifier to Detect Political Leaning of YouTube\n  Videos based on their Titles", "abstract": "A quarter of US adults regularly get their news from YouTube. Yet, despite\nthe massive political content available on the platform, to date no classifier\nhas been proposed to identify the political leaning of YouTube videos. To fill\nthis gap, we propose a novel classifier based on Bert -- a language model from\nGoogle -- to classify YouTube videos merely based on their titles into six\ncategories, namely: Far Left, Left, Center, Anti-Woke, Right, and Far Right. We\nused a public dataset of 10 million YouTube video titles (under various\ncategories) to train and validate the proposed classifier. We compare the\nclassifier against several alternatives that we trained on the same dataset,\nrevealing that our classifier achieves the highest accuracy (75%) and the\nhighest F1 score (77%). To further validate the classification performance, we\ncollect videos from YouTube channels of numerous prominent news agencies, such\nas Fox News and New York Times, which have widely known political leanings, and\napply our classifier to their video titles. For the vast majority of cases, the\npredicted political leaning matches that of the news agency.", "published": "2024-02-16 14:44:30", "link": "http://arxiv.org/abs/2404.04261v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Subgraph-level Universal Prompt Tuning", "abstract": "In the evolving landscape of machine learning, the adaptation of pre-trained\nmodels through prompt tuning has become increasingly prominent. This trend is\nparticularly observable in the graph domain, where diverse pre-training\nstrategies present unique challenges in developing effective prompt-based\ntuning methods for graph neural networks. Previous approaches have been\nlimited, focusing on specialized prompting functions tailored to models with\nedge prediction pre-training tasks. These methods, however, suffer from a lack\nof generalizability across different pre-training strategies. Recently, a\nsimple prompt tuning method has been designed for any pre-training strategy,\nfunctioning within the input graph's feature space. This allows it to\ntheoretically emulate any type of prompting function, thereby significantly\nincreasing its versatility for a range of downstream applications.\nNevertheless, the capacity of such simple prompts to fully grasp the complex\ncontexts found in graphs remains an open question, necessitating further\ninvestigation. Addressing this challenge, our work introduces the\nSubgraph-level Universal Prompt Tuning (SUPT) approach, focusing on the\ndetailed context within subgraphs. In SUPT, prompt features are assigned at the\nsubgraph-level, preserving the method's universal capability. This requires\nextremely fewer tuning parameters than fine-tuning-based methods, outperforming\nthem in 42 out of 45 full-shot scenario experiments with an average improvement\nof over 2.5%. In few-shot scenarios, it excels in 41 out of 45 experiments,\nachieving an average performance increase of more than 6.6%.", "published": "2024-02-16 00:25:24", "link": "http://arxiv.org/abs/2402.10380v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Understanding Survey Paper Taxonomy about Large Language Models via\n  Graph Representation Learning", "abstract": "As new research on Large Language Models (LLMs) continues, it is difficult to\nkeep up with new research and models. To help researchers synthesize the new\nresearch many have written survey papers, but even those have become numerous.\nIn this paper, we develop a method to automatically assign survey papers to a\ntaxonomy. We collect the metadata of 144 LLM survey papers and explore three\nparadigms to classify papers within the taxonomy. Our work indicates that\nleveraging graph structure information on co-category graphs can significantly\noutperform the language models in two paradigms; pre-trained language models'\nfine-tuning and zero-shot/few-shot classifications using LLMs. We find that our\nmodel surpasses an average human recognition level and that fine-tuning LLMs\nusing weak labels generated by a smaller model, such as the GCN in this study,\ncan be more effective than using ground-truth labels, revealing the potential\nof weak-to-strong generalization in the taxonomy classification task.", "published": "2024-02-16 02:21:59", "link": "http://arxiv.org/abs/2402.10409v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Measuring and Reducing LLM Hallucination without Gold-Standard Answers", "abstract": "LLM hallucination, i.e. generating factually incorrect yet seemingly\nconvincing answers, is currently a major threat to the trustworthiness and\nreliability of LLMs. The first step towards solving this complicated problem is\nto measure it. However, existing hallucination metrics require having a\nbenchmark dataset with gold-standard answers, i.e. \"best\" or \"correct\" answers\nwritten by humans. Such requirements make hallucination measurement costly and\nprone to human errors. In this work, we propose Factualness Evaluations via\nWeighting LLMs (FEWL), an innovative hallucination metric that is specifically\ndesigned for the scenario when gold-standard answers are absent. FEWL leverages\nthe answers from off-the-shelf LLMs that serve as a proxy of gold-standard\nanswers. The key challenge is how to quantify the expertise of reference LLMs\nresourcefully. We show FEWL has certain theoretical guarantees and demonstrate\nempirically it gives more accurate hallucination measures than naively using\nreference LLMs. We also show how to leverage FEWL to reduce hallucination\nthrough both in-context learning and supervised fine-tuning. Extensive\nexperiment results on Truthful-QA, CHALE, and HaluEval datasets demonstrate the\neffectiveness of FEWL.", "published": "2024-02-16 02:32:06", "link": "http://arxiv.org/abs/2402.10412v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating and Improving Continual Learning in Spoken Language\n  Understanding", "abstract": "Continual learning has emerged as an increasingly important challenge across\nvarious tasks, including Spoken Language Understanding (SLU). In SLU, its\nobjective is to effectively handle the emergence of new concepts and evolving\nenvironments. The evaluation of continual learning algorithms typically\ninvolves assessing the model's stability, plasticity, and generalizability as\nfundamental aspects of standards. However, existing continual learning metrics\nprimarily focus on only one or two of the properties. They neglect the overall\nperformance across all tasks, and do not adequately disentangle the plasticity\nversus stability/generalizability trade-offs within the model. In this work, we\npropose an evaluation methodology that provides a unified evaluation on\nstability, plasticity, and generalizability in continual learning. By employing\nthe proposed metric, we demonstrate how introducing various knowledge\ndistillations can improve different aspects of these three properties of the\nSLU model. We further show that our proposed metric is more sensitive in\ncapturing the impact of task ordering in continual learning, making it better\nsuited for practical use-case scenarios.", "published": "2024-02-16 03:30:27", "link": "http://arxiv.org/abs/2402.10427v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Active Preference Optimization for Sample Efficient RLHF", "abstract": "Reinforcement Learning from Human Feedback (RLHF) is pivotal in aligning\nLarge Language Models (LLMs) with human preferences. Although aligned\ngenerative models have shown remarkable abilities in various tasks, their\nreliance on high-quality human preference data creates a costly bottleneck in\nthe practical application of RLHF. One primary reason is that current methods\nrely on uniformly picking prompt-generation pairs from a dataset of\nprompt-generations, to collect human feedback, resulting in sub-optimal\nalignment under a constrained budget, which highlights the criticality of\nadaptive strategies in efficient alignment. Recent works [Mehta et al., 2023,\nMuldrew et al., 2024] have tried to address this problem by designing various\nheuristics based on generation uncertainty. However, either the assumptions in\n[Mehta et al., 2023] are restrictive, or [Muldrew et al., 2024] do not provide\nany rigorous theoretical guarantee. To address these, we reformulate RLHF\nwithin contextual preference bandit framework, treating prompts as contexts,\nand develop an active-learning algorithm, $\\textit{Active Preference\nOptimization}$ ($\\texttt{APO}$), which enhances model alignment by querying\npreference data from the most important samples, achieving superior performance\nfor small sample budget. We analyze the theoretical performance guarantees of\n$\\texttt{APO}$ under the BTL preference model showing that the suboptimality\ngap of the policy learned via $\\texttt{APO}$ scales as $O(1/\\sqrt{T})$ for a\nbudget of $T$. We also show that collecting preference data by choosing prompts\nrandomly leads to a policy that suffers a constant sub-optimality. We perform\ndetailed experimental evaluations on practical preference datasets to validate\n$\\texttt{APO}$'s efficacy over the existing methods, establishing it as a\nsample-efficient and practical solution of alignment in a cost-effective and\nscalable manner.", "published": "2024-02-16 08:19:34", "link": "http://arxiv.org/abs/2402.10500v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large\n  Language Models", "abstract": "Automatic side-by-side evaluation has emerged as a promising approach to\nevaluating the quality of responses from large language models (LLMs). However,\nanalyzing the results from this evaluation approach raises scalability and\ninterpretability challenges. In this paper, we present LLM Comparator, a novel\nvisual analytics tool for interactively analyzing results from automatic\nside-by-side evaluation. The tool supports interactive workflows for users to\nunderstand when and why a model performs better or worse than a baseline model,\nand how the responses from two models are qualitatively different. We\niteratively designed and developed the tool by closely working with researchers\nand engineers at a large technology company. This paper details the user\nchallenges we identified, the design and development of the tool, and an\nobservational study with participants who regularly evaluate their models.", "published": "2024-02-16 09:14:49", "link": "http://arxiv.org/abs/2402.10524v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.HC"}
{"title": "Assessing biomedical knowledge robustness in large language models by\n  query-efficient sampling attacks", "abstract": "The increasing depth of parametric domain knowledge in large language models\n(LLMs) is fueling their rapid deployment in real-world applications.\nUnderstanding model vulnerabilities in high-stakes and knowledge-intensive\ntasks is essential for quantifying the trustworthiness of model predictions and\nregulating their use. The recent discovery of named entities as adversarial\nexamples (i.e. adversarial entities) in natural language processing tasks\nraises questions about their potential impact on the knowledge robustness of\npre-trained and finetuned LLMs in high-stakes and specialized domains. We\nexamined the use of type-consistent entity substitution as a template for\ncollecting adversarial entities for billion-parameter LLMs with biomedical\nknowledge. To this end, we developed an embedding-space attack based on\npowerscaled distance-weighted sampling to assess the robustness of their\nbiomedical knowledge with a low query budget and controllable coverage. Our\nmethod has favorable query efficiency and scaling over alternative approaches\nbased on random sampling and blackbox gradient-guided search, which we\ndemonstrated for adversarial distractor generation in biomedical question\nanswering. Subsequent failure mode analysis uncovered two regimes of\nadversarial entities on the attack surface with distinct characteristics and we\nshowed that entity substitution attacks can manipulate token-wise Shapley value\nexplanations, which become deceptive in this setting. Our approach complements\nstandard evaluations for high-capacity models and the results highlight the\nbrittleness of domain knowledge in LLMs.", "published": "2024-02-16 09:29:38", "link": "http://arxiv.org/abs/2402.10527v3", "categories": ["cs.CL", "cs.CR", "stat.AP"], "primary_category": "cs.CL"}
{"title": "A novel integrated industrial approach with cobots in the age of\n  industry 4.0 through conversational interaction and computer vision", "abstract": "From robots that replace workers to robots that serve as helpful colleagues,\nthe field of robotic automation is experiencing a new trend that represents a\nhuge challenge for component manufacturers. The contribution starts from an\ninnovative vision that sees an ever closer collaboration between Cobot, able to\ndo a specific physical job with precision, the AI world, able to analyze\ninformation and support the decision-making process, and the man able to have a\nstrategic vision of the future.", "published": "2024-02-16 10:35:01", "link": "http://arxiv.org/abs/2402.10553v1", "categories": ["cs.RO", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Direct Preference Optimization with an Offset", "abstract": "Direct preference optimization (DPO) is a successful fine-tuning strategy for\naligning large language models with human preferences without the need to train\na reward model or employ reinforcement learning. DPO, as originally formulated,\nrelies on binary preference data and fine-tunes a language model to increase\nthe likelihood of a preferred response over a dispreferred response. However,\nnot all preference pairs are equal. Sometimes, the preferred response is only\nslightly better than the dispreferred one. In other cases, the preference is\nmuch stronger. For instance, if a response contains harmful or toxic content,\nthe annotator will have a strong preference for that response. In this paper,\nwe propose a generalization of DPO, termed DPO with an offset (ODPO), that does\nnot treat every preference pair equally during fine-tuning. Intuitively, ODPO\nrequires the difference between the likelihood of the preferred and\ndispreferred response to be greater than an offset value. The offset is\ndetermined based on the extent to which one response is preferred over another.\nOur experiments on various tasks suggest that ODPO significantly outperforms\nDPO in aligning language models, especially when the number of preference pairs\nis limited.", "published": "2024-02-16 10:55:38", "link": "http://arxiv.org/abs/2402.10571v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate\n  Controllable Controversial Statements", "abstract": "Making LLMs speak for different, especially minority groups of people, and\ngenerate statements supporting their diverse or even controversial perspectives\nis critical to creating an inclusive environment. However, existing LLMs lack\nsufficient controllability to the stance of their generated content, which\noften contains inconsistent, neutral, or biased statements. In this paper, we\nimprove the controllability of LLMs in generating statements supporting an\nargument the user defined in the prompt. We find that multi-round debates\nbetween two LLMs with opposite stances generate higher-quality and more salient\nstatements for each, which are important training data to improve the\ncontrollability of LLMs. Motivated by this, we develop a novel debate & tuning\n(DEBATUNE) pipeline finetuning LLMs to generate the statements obtained via\ndebate. To examine DEBATUNE, we curate the largest dataset of debate topics so\nfar, which covers 710 controversial topics and corresponding arguments for each\ntopic. Evaluations by the GPT-4 judge with a novel controversy controllability\nmetric show that LLMs' capability of generating diverse perspectives is\nsignificantly improved by DEBATUNE. Moreover, such controllability can be\ngeneralized to unseen topics, generating high-quality statements supporting\ncontroversial arguments.", "published": "2024-02-16 12:00:34", "link": "http://arxiv.org/abs/2402.10614v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Network Formation and Dynamics Among Multi-LLMs", "abstract": "Social networks fundamentally shape human opinions, behaviors, and the\ndissemination of information. As large language models (LLMs) like GPT, Claude,\nand Llama increasingly integrate into social and professional settings,\nunderstanding their behavior in the context of social interactions and network\nformation becomes essential. This study develops a framework to systematically\nexamine whether the network formation behaviors of multiple LLMs approximate\ncertain aspects of human network dynamics. By simulating interactions among LLM\nagents across various model families, we observe that these models consistently\nexhibit key patterns associated with social network principles including\npreferential attachment, triadic closure, homophily, community structure, and\nthe small-world phenomenon when forming networks. Moreover, LLMs adapt their\nnetwork formation strategies based on each network's characteristics,\nreflecting the context-dependent nature of human behavior: in Facebook\nnetworks, they prioritize triadic closure and homophily, mirroring close-knit\nfriendships; in phone networks, homophily and preferential attachment dominate,\ncapturing personal and professional connections, while in employment networks,\nLLMs favor heterophily and high-degree connections, aligning with career\nadvancement dynamics. These results open new avenues for using LLMs in network\nscience research, with potential applications in agent-based modeling and\nsynthetic network generation.", "published": "2024-02-16 13:10:14", "link": "http://arxiv.org/abs/2402.10659v4", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.SI"}
{"title": "Distillation Enhanced Generative Retrieval", "abstract": "Generative retrieval is a promising new paradigm in text retrieval that\ngenerates identifier strings of relevant passages as the retrieval target. This\nparadigm leverages powerful generative language models, distinct from\ntraditional sparse or dense retrieval methods. In this work, we identify a\nviable direction to further enhance generative retrieval via distillation and\npropose a feasible framework, named DGR. DGR utilizes sophisticated ranking\nmodels, such as the cross-encoder, in a teacher role to supply a passage rank\nlist, which captures the varying relevance degrees of passages instead of\nbinary hard labels; subsequently, DGR employs a specially designed distilled\nRankNet loss to optimize the generative retrieval model, considering the\npassage rank order provided by the teacher model as labels. This framework only\nrequires an additional distillation step to enhance current generative\nretrieval systems and does not add any burden to the inference stage. We\nconduct experiments on four public datasets, and the results indicate that DGR\nachieves state-of-the-art performance among the generative retrieval methods.\nAdditionally, DGR demonstrates exceptional robustness and generalizability with\nvarious teacher models and distillation losses.", "published": "2024-02-16 15:48:24", "link": "http://arxiv.org/abs/2402.10769v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for\n  the Acceleration of Lightweight LLMs on the Edge", "abstract": "Despite the remarkable strides of Large Language Models (LLMs) in various\nfields, the wide applications of LLMs on edge devices are limited due to their\nmassive parameters and computations. To address this, quantization is commonly\nadopted to generate lightweight LLMs with efficient computations and fast\ninference. However, Post-Training Quantization (PTQ) methods dramatically\ndegrade in quality when quantizing weights, activations, and KV cache together\nto below 8 bits. Besides, many Quantization-Aware Training (QAT) works quantize\nmodel weights, leaving the activations untouched, which do not fully exploit\nthe potential of quantization for inference acceleration on the edge. In this\npaper, we propose EdgeQAT, the Entropy and Distribution Guided QAT for the\noptimization of lightweight LLMs to achieve inference acceleration on Edge\ndevices. We first identify that the performance drop of quantization primarily\nstems from the information distortion in quantized attention maps, demonstrated\nby the different distributions in quantized query and key of the self-attention\nmechanism. Then, the entropy and distribution guided QAT is proposed to\nmitigate the information distortion. Moreover, we design a token\nimportance-aware adaptive method to dynamically quantize the tokens with\ndifferent bit widths for further optimization and acceleration. Our extensive\nexperiments verify the substantial improvements with our framework across\nvarious datasets. Furthermore, we achieve an on-device speedup of up to 2.37x\ncompared with its FP16 counterparts across multiple edge devices, signaling a\ngroundbreaking advancement.", "published": "2024-02-16 16:10:38", "link": "http://arxiv.org/abs/2402.10787v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "In Search of Needles in a 11M Haystack: Recurrent Memory Finds What LLMs\n  Miss", "abstract": "This paper addresses the challenge of processing long documents using\ngenerative transformer models. To evaluate different approaches, we introduce\nBABILong, a new benchmark designed to assess model capabilities in extracting\nand processing distributed facts within extensive texts. Our evaluation, which\nincludes benchmarks for GPT-4 and RAG, reveals that common methods are\neffective only for sequences up to $10^4$ elements. In contrast, fine-tuning\nGPT-2 with recurrent memory augmentations enables it to handle tasks involving\nup to $11\\times 10^6$ elements. This achievement marks a substantial leap, as\nit is by far the longest input processed by any neural network model to date,\ndemonstrating a significant improvement in the processing capabilities for long\nsequences.", "published": "2024-02-16 16:15:01", "link": "http://arxiv.org/abs/2402.10790v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-modal Preference Alignment Remedies Degradation of Visual\n  Instruction Tuning on Language Models", "abstract": "Multi-modal large language models (MLLMs) are expected to support multi-turn\nqueries of interchanging image and text modalities in production. However, the\ncurrent MLLMs trained with visual-question-answering (VQA) datasets could\nsuffer from degradation, as VQA datasets lack the diversity and complexity of\nthe original text instruction datasets with which the underlying language model\nwas trained. To address this degradation, we first collect a lightweight,\n5k-sample VQA preference dataset where answers were annotated by Gemini for\nfive quality metrics in a granular fashion and investigate standard Supervised\nFine-tuning, rejection sampling, Direct Preference Optimization (DPO) and\nSteerLM algorithms. Our findings indicate that with DPO, we can surpass the\ninstruction-following capabilities of the language model, achieving a 6.73\nscore on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99. This enhancement\nin textual instruction-following capability correlates with boosted visual\ninstruction performance (+4.9\\% on MM-Vet, +6\\% on LLaVA-Bench), with minimal\nalignment tax on visual knowledge benchmarks compared to the previous RLHF\napproach. In conclusion, we propose a distillation-based multi-modal alignment\nmodel with fine-grained annotations on a small dataset that restores and boosts\nMLLM's language capability after visual instruction tuning.", "published": "2024-02-16 18:42:08", "link": "http://arxiv.org/abs/2402.10884v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "When is Tree Search Useful for LLM Planning? It Depends on the\n  Discriminator", "abstract": "In this paper, we examine how large language models (LLMs) solve multi-step\nproblems under a language agent framework with three components: a generator, a\ndiscriminator, and a planning method. We investigate the practical utility of\ntwo advanced planning methods, iterative correction and tree search. We present\na comprehensive analysis of how discrimination accuracy affects the overall\nperformance of agents when using these two methods or a simpler method,\nre-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical\nreasoning, show that: (1) advanced planning methods demand discriminators with\nat least 90% accuracy to achieve significant improvements over re-ranking; (2)\ncurrent LLMs' discrimination abilities have not met the needs of advanced\nplanning methods to achieve such improvements; (3) with LLM-based\ndiscriminators, advanced planning methods may not adequately balance accuracy\nand efficiency. For example, compared to the other two methods, tree search is\nat least 10--20 times slower but leads to negligible performance gains, which\nhinders its real-world applications. Code and data are available at\nhttps://github.com/OSU-NLP-Group/llm-planning-eval.", "published": "2024-02-16 18:45:58", "link": "http://arxiv.org/abs/2402.10890v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Instruction Diversity Drives Generalization To Unseen Tasks", "abstract": "Instruction tuning -- fine-tuning a large language model (LLM) on pairs of\ninstructions and desired outcomes -- is an approach that enables pre-trained\nlanguage models to perform real-world tasks and follow human instructions. Its\npractical success depends on the model learning a broader set of instructions\nthan those it was trained on. Yet the factors that determine model\ngeneralization to such \\emph{unseen tasks} are not well understood. %To\nunderstand the driving factors of generalization, In this paper, we experiment\nwith string rewrites, a symbolic task that serves as a building block for\nTuring complete Markov algorithms while allowing experimental control of\n\"inputs\" and \"instructions\". We investigate the trade-off between the number of\ninstructions the model is trained on and the number of training samples\nprovided for each instruction and observe that the diversity of the instruction\nset determines generalization. Generalization emerges once a diverse enough set\nof tasks is provided, even though very few examples are provided for each task.\nInstruction diversity also ensures robustness with respect to non-uniform\ndistributions of instructions in the training set.", "published": "2024-02-16 18:47:21", "link": "http://arxiv.org/abs/2402.10891v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Proving membership in LLM pretraining data via data watermarks", "abstract": "Detecting whether copyright holders' works were used in LLM pretraining is\npoised to be an important problem. This work proposes using data watermarks to\nenable principled detection with only black-box model access, provided that the\nrightholder contributed multiple training documents and watermarked them before\npublic release. By applying a randomly sampled data watermark, detection can be\nframed as hypothesis testing, which provides guarantees on the false detection\nrate. We study two watermarks: one that inserts random sequences, and another\nthat randomly substitutes characters with Unicode lookalikes. We first show how\nthree aspects of watermark design -- watermark length, number of duplications,\nand interference -- affect the power of the hypothesis test. Next, we study how\na watermark's detection strength changes under model and dataset scaling: while\nincreasing the dataset size decreases the strength of the watermark, watermarks\nremain strong if the model size also increases. Finally, we view SHA hashes as\nnatural watermarks and show that we can robustly detect hashes from\nBLOOM-176B's training data, as long as they occurred at least 90 times.\nTogether, our results point towards a promising future for data watermarks in\nreal world use.", "published": "2024-02-16 18:49:27", "link": "http://arxiv.org/abs/2402.10892v3", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "RLVF: Learning from Verbal Feedback without Overgeneralization", "abstract": "The diversity of contexts in which large language models (LLMs) are deployed\nrequires the ability to modify or customize default model behaviors to\nincorporate nuanced requirements and preferences. A convenient interface to\nspecify such model adjustments is high-level verbal feedback, such as \"Don't\nuse emojis when drafting emails to my boss.\" However, while writing high-level\nfeedback is far simpler than collecting annotations for reinforcement learning\nfrom human feedback (RLHF), we find that simply prompting a model with such\nfeedback leads to overgeneralization of the feedback to contexts where it is\nnot relevant. We study the problem of incorporating verbal feedback without\nsuch overgeneralization, inspiring a new method Contextualized Critiques with\nConstrained Preference Optimization (C3PO). C3PO uses a piece of high-level\nfeedback to generate a small synthetic preference dataset specifying how the\nfeedback should (and should not) be applied. It then fine-tunes the model in\naccordance with the synthetic preference data while minimizing the divergence\nfrom the original model for prompts where the feedback does not apply. Our\nexperimental results indicate that our approach effectively applies verbal\nfeedback to relevant scenarios while preserving existing behaviors for other\ncontexts. For both human- and GPT-4-generated high-level feedback, C3PO\neffectively adheres to the given feedback comparably to in-context baselines\nwhile reducing overgeneralization by 30%.", "published": "2024-02-16 18:50:24", "link": "http://arxiv.org/abs/2402.10893v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Persona-DB: Efficient Large Language Model Personalization for Response\n  Prediction with Collaborative Data Refinement", "abstract": "The increasing demand for personalized interactions with large language\nmodels (LLMs) calls for methodologies capable of accurately and efficiently\nidentifying user opinions and preferences. Retrieval augmentation emerges as an\neffective strategy, as it can accommodate a vast number of users without the\ncosts from fine-tuning. Existing research, however, has largely focused on\nenhancing the retrieval stage and devoted limited exploration toward optimizing\nthe representation of the database, a crucial aspect for tasks such as\npersonalization. In this work, we examine the problem from a novel angle,\nfocusing on how data can be better represented for more data-efficient\nretrieval in the context of LLM customization. To tackle this challenge, we\nintroduce Persona-DB, a simple yet effective framework consisting of a\nhierarchical construction process to improve generalization across task\ncontexts and collaborative refinement to effectively bridge knowledge gaps\namong users. In the evaluation of response prediction, Persona-DB demonstrates\nsuperior context efficiency in maintaining accuracy with a significantly\nreduced retrieval size, a critical advantage in scenarios with extensive\nhistories or limited context windows. Our experiments also indicate a marked\nimprovement of over 10% under cold-start scenarios, when users have extremely\nsparse data. Furthermore, our analysis reveals the increasing importance of\ncollaborative knowledge as the retrieval capacity expands.", "published": "2024-02-16 20:20:43", "link": "http://arxiv.org/abs/2402.11060v3", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Model Editing by Standard Fine-Tuning", "abstract": "Standard fine-tuning is considered not as effective as specialized methods\nfor model editing due to its comparatively poor performance. However, it is\nsimple, agnostic to the architectural details of the model being edited, and\nable to leverage advances in standard training techniques with no additional\nwork (e.g., black-box PEFT for computational efficiency), making it an\nappealing choice for a model editor. In this work, we show that standard\nfine-tuning alone can yield competitive model editing performance with two\nminor modifications. First, we optimize the conditional likelihood rather than\nthe full likelihood. Second, in addition to the typical practice of training on\nrandomly paraphrased edit prompts to encourage generalization, we also train on\nrandom or similar unedited facts to encourage locality. Our experiments on the\nZsRE and CounterFact datasets demonstrate that these simple modifications allow\nstandard fine-tuning to match or outperform highly specialized editors in terms\nof edit score.", "published": "2024-02-16 21:10:33", "link": "http://arxiv.org/abs/2402.11078v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Whose Emotions and Moral Sentiments Do Language Models Reflect?", "abstract": "Language models (LMs) are known to represent the perspectives of some social\ngroups better than others, which may impact their performance, especially on\nsubjective tasks such as content moderation and hate speech detection. To\nexplore how LMs represent different perspectives, existing research focused on\npositional alignment, i.e., how closely the models mimic the opinions and\nstances of different groups, e.g., liberals or conservatives. However, human\ncommunication also encompasses emotional and moral dimensions. We define the\nproblem of affective alignment, which measures how LMs' emotional and moral\ntone represents those of different groups. By comparing the affect of responses\ngenerated by 36 LMs to the affect of Twitter messages, we observe significant\nmisalignment of LMs with both ideological groups. This misalignment is larger\nthan the partisan divide in the U.S. Even after steering the LMs towards\nspecific ideological perspectives, the misalignment and liberal tendencies of\nthe model persist, suggesting a systemic bias within LMs.", "published": "2024-02-16 22:34:53", "link": "http://arxiv.org/abs/2402.11114v2", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Speculative Streaming: Fast LLM Inference without Auxiliary Models", "abstract": "Speculative decoding is a prominent technique to speed up the inference of a\nlarge target language model based on predictions of an auxiliary draft model.\nWhile effective, in application-specific settings, it often involves\nfine-tuning both draft and target models to achieve high acceptance rates. As\nthe number of downstream tasks grows, these draft models add significant\ncomplexity to inference systems. We propose Speculative Streaming, a\nsingle-model speculative decoding method that fuses drafting into the target\nmodel by changing the fine-tuning objective from next token prediction to\nfuture n-gram prediction. Speculative Streaming speeds up decoding by 1.8 -\n3.1X in a diverse set of tasks, such as Summarization, Structured Queries, and\nMeaning Representation, without sacrificing generation quality. Additionally,\nSpeculative Streaming is parameter-efficient. It achieves on-par/higher\nspeed-ups than Medusa-style architectures while using ~10000X fewer extra\nparameters, making it well-suited for resource-constrained devices.", "published": "2024-02-16 23:36:43", "link": "http://arxiv.org/abs/2402.11131v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Using Hallucinations to Bypass GPT4's Filter", "abstract": "Large language models (LLMs) are initially trained on vast amounts of data,\nthen fine-tuned using reinforcement learning from human feedback (RLHF); this\nalso serves to teach the LLM to provide appropriate and safe responses. In this\npaper, we present a novel method to manipulate the fine-tuned version into\nreverting to its pre-RLHF behavior, effectively erasing the model's filters;\nthe exploit currently works for GPT4, Claude Sonnet, and (to some extent) for\nInflection-2.5. Unlike other jailbreaks (for example, the popular \"Do Anything\nNow\" (DAN) ), our method does not rely on instructing the LLM to override its\nRLHF policy; hence, simply modifying the RLHF process is unlikely to address\nit. Instead, we induce a hallucination involving reversed text during which the\nmodel reverts to a word bucket, effectively pausing the model's filter. We\nbelieve that our exploit presents a fundamental vulnerability in LLMs currently\nunaddressed, as well as an opportunity to better understand the inner workings\nof LLMs during hallucinations.", "published": "2024-02-16 17:02:53", "link": "http://arxiv.org/abs/2403.04769v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Born With a Silver Spoon? Investigating Socioeconomic Bias in Large\n  Language Models", "abstract": "Socioeconomic bias in society exacerbates disparities, influencing access to\nopportunities and resources based on individuals' economic and social\nbackgrounds. This pervasive issue perpetuates systemic inequalities, hindering\nthe pursuit of inclusive progress as a society. In this paper, we investigate\nthe presence of socioeconomic bias, if any, in large language models. To this\nend, we introduce a novel dataset SilverSpoon, consisting of 3000 samples that\nillustrate hypothetical scenarios that involve underprivileged people\nperforming ethically ambiguous actions due to their circumstances, and ask\nwhether the action is ethically justified. Further, this dataset has a\ndual-labeling scheme and has been annotated by people belonging to both ends of\nthe socioeconomic spectrum. Using SilverSpoon, we evaluate the degree of\nsocioeconomic bias expressed in large language models and the variation of this\ndegree as a function of model size. We also perform qualitative analysis to\nanalyze the nature of this bias. Our analysis reveals that while humans\ndisagree on which situations require empathy toward the underprivileged, most\nlarge language models are unable to empathize with the socioeconomically\nunderprivileged regardless of the situation. To foster further research in this\ndomain, we make SilverSpoon and our evaluation harness publicly available.", "published": "2024-02-16 23:18:19", "link": "http://arxiv.org/abs/2403.14633v4", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Regulating Large Language Models: A Roundtable Report", "abstract": "On July 20, 2023, a group of 27 scholars and digital rights advocates with\nexpertise in law, computer science, political science, and other disciplines\ngathered for the Large Language Models, Law and Policy Roundtable, co-hosted by\nthe NYU School of Law's Information Law Institute and the Center for Democracy\n& Technology. The roundtable convened to discuss how law and policy can help\naddress some of the larger societal problems posed by large language models\n(LLMs). The discussion focused on three policy topic areas in particular:\n  1. Truthfulness: What risks do LLMs pose in terms of generating mis- and\ndisinformation? How can these risks be mitigated from a technical and/or\nregulatory perspective?\n  2. Privacy: What are the biggest privacy risks involved in the creation,\ndeployment, and use of LLMs? How can these risks be mitigated from a technical\nand/or regulatory perspective?\n  3. Market concentration: What threats do LLMs pose concerning market/power\nconcentration? How can these risks be mitigated from a technical and/or\nregulatory perspective?\n  In this paper, we provide a detailed summary of the day's proceedings. We\nfirst recap what we deem to be the most important contributions made during the\nissue framing discussions. We then provide a list of potential legal and\nregulatory interventions generated during the brainstorming discussions.", "published": "2024-02-16 21:49:17", "link": "http://arxiv.org/abs/2403.15397v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Emoji Driven Crypto Assets Market Reactions", "abstract": "In the burgeoning realm of cryptocurrency, social media platforms like\nTwitter have become pivotal in influencing market trends and investor\nsentiments. In our study, we leverage GPT-4 and a fine-tuned transformer-based\nBERT model for a multimodal sentiment analysis, focusing on the impact of emoji\nsentiment on cryptocurrency markets. By translating emojis into quantifiable\nsentiment data, we correlate these insights with key market indicators like BTC\nPrice and the VCRIX index. Our architecture's analysis of emoji sentiment\ndemonstrated a distinct advantage over FinBERT's pure text sentiment analysis\nin such predicting power. This approach may be fed into the development of\ntrading strategies aimed at utilizing social media elements to identify and\nforecast market trends. Crucially, our findings suggest that strategies based\non emoji sentiment can facilitate the avoidance of significant market downturns\nand contribute to the stabilization of returns. This research underscores the\npractical benefits of integrating advanced AI-driven analyses into financial\nstrategies, offering a nuanced perspective on the interplay between digital\ncommunication and market dynamics in an academic context.", "published": "2024-02-16 07:05:49", "link": "http://arxiv.org/abs/2402.10481v2", "categories": ["q-fin.CP", "cs.AI", "cs.CL", "cs.LG", "q-fin.ST"], "primary_category": "q-fin.CP"}
{"title": "Properties and Challenges of LLM-Generated Explanations", "abstract": "The self-rationalising capabilities of large language models (LLMs) have been\nexplored in restricted settings, using task/specific data sets. However,\ncurrent LLMs do not (only) rely on specifically annotated data; nonetheless,\nthey frequently explain their outputs. The properties of the generated\nexplanations are influenced by the pre-training corpus and by the target data\nused for instruction fine-tuning. As the pre-training corpus includes a large\namount of human-written explanations \"in the wild\", we hypothesise that LLMs\nadopt common properties of human explanations. By analysing the outputs for a\nmulti-domain instruction fine-tuning data set, we find that generated\nexplanations show selectivity and contain illustrative elements, but less\nfrequently are subjective or misleading. We discuss reasons and consequences of\nthe properties' presence or absence. In particular, we outline positive and\nnegative implications depending on the goals and user groups of the\nself-rationalising system.", "published": "2024-02-16 09:37:54", "link": "http://arxiv.org/abs/2402.10532v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generative Cross-Modal Retrieval: Memorizing Images in Multimodal\n  Language Models for Retrieval and Beyond", "abstract": "The recent advancements in generative language models have demonstrated their\nability to memorize knowledge from documents and recall knowledge to respond to\nuser queries effectively. Building upon this capability, we propose to enable\nmultimodal large language models (MLLMs) to memorize and recall images within\ntheir parameters. Given a user query for visual content, the MLLM is\nanticipated to \"recall\" the relevant image from its parameters as the response.\nAchieving this target presents notable challenges, including inbuilt visual\nmemory and visual recall schemes within MLLMs. To address these challenges, we\nintroduce a generative cross-modal retrieval framework, which assigns unique\nidentifier strings to represent images and involves two training steps:\nlearning to memorize and learning to retrieve. The first step focuses on\ntraining the MLLM to memorize the association between images and their\nrespective identifiers. The latter step teaches the MLLM to generate the\ncorresponding identifier of the target image, given the textual query input. By\nmemorizing images in MLLMs, we introduce a new paradigm to cross-modal\nretrieval, distinct from previous discriminative approaches. The experiments\ndemonstrate that the generative paradigm performs effectively and efficiently\neven with large-scale image candidate sets.", "published": "2024-02-16 16:31:46", "link": "http://arxiv.org/abs/2402.10805v1", "categories": ["cs.MM", "cs.AI", "cs.CL", "cs.CV", "cs.IR"], "primary_category": "cs.MM"}
{"title": "APCodec: A Neural Audio Codec with Parallel Amplitude and Phase Spectrum\n  Encoding and Decoding", "abstract": "This paper introduces a novel neural audio codec targeting high waveform\nsampling rates and low bitrates named APCodec, which seamlessly integrates the\nstrengths of parametric codecs and waveform codecs. The APCodec revolutionizes\nthe process of audio encoding and decoding by concurrently handling the\namplitude and phase spectra as audio parametric characteristics like parametric\ncodecs. It is composed of an encoder and a decoder with the modified ConvNeXt\nv2 network as the backbone, connected by a quantizer based on the residual\nvector quantization (RVQ) mechanism. The encoder compresses the audio amplitude\nand phase spectra in parallel, amalgamating them into a continuous latent code\nat a reduced temporal resolution. This code is subsequently quantized by the\nquantizer. Ultimately, the decoder reconstructs the audio amplitude and phase\nspectra in parallel, and the decoded waveform is obtained by inverse short-time\nFourier transform. To ensure the fidelity of decoded audio like waveform\ncodecs, spectral-level loss, quantization loss, and generative adversarial\nnetwork (GAN) based loss are collectively employed for training the APCodec. To\nsupport low-latency streamable inference, we employ feed-forward layers and\ncausal deconvolutional layers in APCodec, incorporating a knowledge\ndistillation training strategy to enhance the quality of decoded audio.\nExperimental results confirm that our proposed APCodec can encode 48 kHz audio\nat bitrate of just 6 kbps, with no significant degradation in the quality of\nthe decoded audio. At the same bitrate, our proposed APCodec also demonstrates\nsuperior decoded audio quality and faster generation speed compared to\nwell-known codecs, such as Encodec, AudioDec and DAC.", "published": "2024-02-16 09:38:16", "link": "http://arxiv.org/abs/2402.10533v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speaking in Wavelet Domain: A Simple and Efficient Approach to Speed up\n  Speech Diffusion Model", "abstract": "Recently, Denoising Diffusion Probabilistic Models (DDPMs) have attained\nleading performances across a diverse range of generative tasks. However, in\nthe field of speech synthesis, although DDPMs exhibit impressive performance,\ntheir long training duration and substantial inference costs hinder practical\ndeployment. Existing approaches primarily focus on enhancing inference speed,\nwhile approaches to accelerate training a key factor in the costs associated\nwith adding or customizing voices often necessitate complex modifications to\nthe model, compromising their universal applicability. To address the\naforementioned challenges, we propose an inquiry: is it possible to enhance the\ntraining/inference speed and performance of DDPMs by modifying the speech\nsignal itself? In this paper, we double the training and inference speed of\nSpeech DDPMs by simply redirecting the generative target to the wavelet domain.\nThis method not only achieves comparable or superior performance to the\noriginal model in speech synthesis tasks but also demonstrates its versatility.\nBy investigating and utilizing different wavelet bases, our approach proves\neffective not just in speech synthesis, but also in speech enhancement.", "published": "2024-02-16 12:43:01", "link": "http://arxiv.org/abs/2402.10642v2", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Learning Disentangled Audio Representations through Controlled Synthesis", "abstract": "This paper tackles the scarcity of benchmarking data in disentangled auditory\nrepresentation learning. We introduce SynTone, a synthetic dataset with\nexplicit ground truth explanatory factors for evaluating disentanglement\ntechniques. Benchmarking state-of-the-art methods on SynTone highlights its\nutility for method evaluation. Our results underscore strengths and limitations\nin audio disentanglement, motivating future research.", "published": "2024-02-16 10:20:42", "link": "http://arxiv.org/abs/2402.10547v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
