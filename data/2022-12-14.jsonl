{"title": "Pre-trained Language Models Can be Fully Zero-Shot Learners", "abstract": "How can we extend a pre-trained model to many language understanding tasks,\nwithout labeled or additional unlabeled data? Pre-trained language models\n(PLMs) have been effective for a wide range of NLP tasks. However, existing\napproaches either require fine-tuning on downstream labeled datasets or\nmanually constructing proper prompts. In this paper, we propose nonparametric\nprompting PLM (NPPrompt) for fully zero-shot language understanding. Unlike\nprevious methods, NPPrompt uses only pre-trained language models and does not\nrequire any labeled data or additional raw corpus for further fine-tuning, nor\ndoes it rely on humans to construct a comprehensive set of prompt label words.\nWe evaluate NPPrompt against previous major few-shot and zero-shot learning\nmethods on diverse NLP tasks: including text classification, text entailment,\nsimilar text retrieval, and paraphrasing. Experimental results demonstrate that\nour NPPrompt outperforms the previous best fully zero-shot method by big\nmargins, with absolute gains of 12.8% in accuracy on text classification and\n18.9% on the GLUE benchmark.", "published": "2022-12-14 00:03:52", "link": "http://arxiv.org/abs/2212.06950v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-task Learning for Cross-Lingual Sentiment Analysis", "abstract": "This paper presents a cross-lingual sentiment analysis of news articles using\nzero-shot and few-shot learning. The study aims to classify the Croatian news\narticles with positive, negative, and neutral sentiments using the Slovene\ndataset. The system is based on a trilingual BERT-based model trained in three\nlanguages: English, Slovene, Croatian. The paper analyses different setups\nusing datasets in two languages and proposes a simple multi-task model to\nperform sentiment classification. The evaluation is performed using the\nfew-shot and zero-shot scenarios in single-task and multi-task experiments for\nCroatian and Slovene.", "published": "2022-12-14 11:29:03", "link": "http://arxiv.org/abs/2212.07160v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building and Evaluating Universal Named-Entity Recognition English\n  corpus", "abstract": "This article presents the application of the Universal Named Entity framework\nto generate automatically annotated corpora. By using a workflow that extracts\nWikipedia data and meta-data and DBpedia information, we generated an English\ndataset which is described and evaluated. Furthermore, we conducted a set of\nexperiments to improve the annotations in terms of precision, recall, and\nF1-measure. The final dataset is available and the established workflow can be\napplied to any language with existing Wikipedia and DBpedia. As part of future\nresearch, we intend to continue improving the annotation process and extend it\nto other languages.", "published": "2022-12-14 11:32:24", "link": "http://arxiv.org/abs/2212.07162v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quotations, Coreference Resolution, and Sentiment Annotations in\n  Croatian News Articles: An Exploratory Study", "abstract": "This paper presents a corpus annotated for the task of direct-speech\nextraction in Croatian. The paper focuses on the annotation of the quotation,\nco-reference resolution, and sentiment annotation in SETimes news corpus in\nCroatian and on the analysis of its language-specific differences compared to\nEnglish. From this, a list of the phenomena that require special attention when\nperforming these annotations is derived. The generated corpus with quotation\nfeatures annotations can be used for multiple tasks in the field of Natural\nLanguage Processing.", "published": "2022-12-14 11:54:12", "link": "http://arxiv.org/abs/2212.07172v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mitigating Negative Style Transfer in Hybrid Dialogue System", "abstract": "As the functionality of dialogue systems evolves, hybrid dialogue systems\nthat accomplish user-specific goals and participate in open-topic chitchat with\nusers are attracting growing attention. Existing research learns both tasks\nconcurrently utilizing a multi-task fusion technique but ignores the negative\ntransfer phenomenon induced by the unique textual style differences. Therefore,\ncontrastive learning based on the latent variable model is used to decouple the\nvarious textual genres in the latent space. We devise supervised and\nself-supervised positive and negative sample constructions for diverse\ndatasets. In addition, to capitalize on the style information contained in the\ndecoupled latent variables, we employ a style prefix that incorporates latent\nvariables further to control the generation of responses with varying styles.\nWe performed extensive experiments on three dialogue datasets, including a\nhybrid dialogue dataset and two task-oriented dialogue datasets. The\nexperimental results demonstrate that our method can mitigate the negative\nstyle transfer issue and achieves state-of-the-art performance on multiple\ndialogue datasets.", "published": "2022-12-14 12:13:34", "link": "http://arxiv.org/abs/2212.07183v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VTCC-NLP at NL4Opt competition subtask 1: An Ensemble Pre-trained\n  language models for Named Entity Recognition", "abstract": "We propose a combined three pre-trained language models (XLM-R, BART, and\nDeBERTa-V3) as an empower of contextualized embedding for named entity\nrecognition. Our model achieves a 92.9% F1 score on the test set and ranks 5th\non the leaderboard at NL4Opt competition subtask 1.", "published": "2022-12-14 13:41:36", "link": "http://arxiv.org/abs/2212.07219v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Byte and Wordpiece Level Models for Massively Multilingual\n  Semantic Parsing", "abstract": "Token free approaches have been successfully applied to a series of word and\nspan level tasks. In this work, we compare a byte-level (ByT5) and a wordpiece\nbased (mT5) sequence to sequence model on the 51 languages of the MASSIVE\nmultilingual semantic parsing dataset. We examine multiple experimental\nsettings: (i) zero-shot, (ii) full gold data and (iii) zero-shot with synthetic\ndata. By leveraging a state-of-the-art label projection method for machine\ntranslated examples, we are able to reduce the gap in exact match accuracy to\nonly 5 points with respect to a model trained on gold data from all the\nlanguages. We additionally provide insights on the cross-lingual transfer of\nByT5 and show how the model compares with respect to mT5 across all parameter\nsizes.", "published": "2022-12-14 13:48:32", "link": "http://arxiv.org/abs/2212.07223v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End\n  Language Modeling", "abstract": "Static subword tokenization algorithms have been an essential component of\nrecent works on language modeling. However, their static nature results in\nimportant flaws that degrade the models' downstream performance and robustness.\nIn this work, we propose MANTa, a Module for Adaptive Neural TokenizAtion.\nMANTa is a differentiable tokenizer trained end-to-end with the language model.\nThe resulting system offers a trade-off between the expressiveness of\nbyte-level models and the speed of models trained using subword tokenization.\nIn addition, our tokenizer is highly explainable since it produces an explicit\nsegmentation of sequences into blocks. We evaluate our pre-trained model on\nseveral English datasets from different domains as well as on synthetic noise.\nWe find that MANTa improves robustness to character perturbations and\nout-of-domain data. We then show that MANTa performs comparably to other models\non the general-domain GLUE benchmark. Finally, we show that it is considerably\nfaster than strictly byte-level models.", "published": "2022-12-14 15:33:44", "link": "http://arxiv.org/abs/2212.07284v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building Multilingual Corpora for a Complex Named Entity Recognition and\n  Classification Hierarchy using Wikipedia and DBpedia", "abstract": "With the ever-growing popularity of the field of NLP, the demand for datasets\nin low resourced-languages follows suit. Following a previously established\nframework, in this paper, we present the UNER dataset, a multilingual and\nhierarchical parallel corpus annotated for named-entities. We describe in\ndetail the developed procedure necessary to create this type of dataset in any\nlanguage available on Wikipedia with DBpedia information. The three-step\nprocedure extracts entities from Wikipedia articles, links them to DBpedia, and\nmaps the DBpedia sets of classes to the UNER labels. This is followed by a\npost-processing procedure that significantly increases the number of identified\nentities in the final results. The paper concludes with a statistical and\nqualitative analysis of the resulting dataset.", "published": "2022-12-14 11:38:48", "link": "http://arxiv.org/abs/2212.07429v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Artificial Intelligence for Health Message Generation: Theory, Method,\n  and an Empirical Study Using Prompt Engineering", "abstract": "This study introduces and examines the potential of an AI system to generate\nhealth awareness messages. The topic of folic acid, a vitamin that is critical\nduring pregnancy, served as a test case. Using prompt engineering, we generated\nmessages that could be used to raise awareness and compared them to retweeted\nhuman-generated messages via computational and human evaluation methods. The\nsystem was easy to use and prolific, and computational analyses revealed that\nthe AI-generated messages were on par with human-generated ones in terms of\nsentiment, reading ease, and semantic content. Also, the human evaluation study\nshowed that AI-generated messages ranked higher in message quality and clarity.\nWe discuss the theoretical, practical, and ethical implications of these\nresults.", "published": "2022-12-14 21:13:08", "link": "http://arxiv.org/abs/2212.07507v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Natural Language Processing to Augment Structured Social\n  Determinants of Health Data in the Electronic Health Record", "abstract": "Objective: Social determinants of health (SDOH) impact health outcomes and\nare documented in the electronic health record (EHR) through structured data\nand unstructured clinical notes. However, clinical notes often contain more\ncomprehensive SDOH information, detailing aspects such as status, severity, and\ntemporality. This work has two primary objectives: i) develop a natural\nlanguage processing (NLP) information extraction model to capture detailed SDOH\ninformation and ii) evaluate the information gain achieved by applying the SDOH\nextractor to clinical narratives and combining the extracted representations\nwith existing structured data.\n  Materials and Methods: We developed a novel SDOH extractor using a deep\nlearning entity and relation extraction architecture to characterize SDOH\nacross various dimensions. In an EHR case study, we applied the SDOH extractor\nto a large clinical data set with 225,089 patients and 430,406 notes with\nsocial history sections and compared the extracted SDOH information with\nexisting structured data.\n  Results: The SDOH extractor achieved 0.86 F1 on a withheld test set. In the\nEHR case study, we found extracted SDOH information complements existing\nstructured data with 32% of homeless patients, 19% of current tobacco users,\nand 10% of drug users only having these health risk factors documented in the\nclinical narrative.\n  Conclusions: Utilizing EHR data to identify SDOH health risk factors and\nsocial needs may improve patient care and outcomes. Semantic representations of\ntext-encoded SDOH information can augment existing structured data, and this\nmore comprehensive SDOH representation can assist health systems in identifying\nand addressing these social needs.", "published": "2022-12-14 22:51:49", "link": "http://arxiv.org/abs/2212.07538v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Build-a-Bot: Teaching Conversational AI Using a Transformer-Based Intent\n  Recognition and Question Answering Architecture", "abstract": "As artificial intelligence (AI) becomes a prominent part of modern life, AI\nliteracy is becoming important for all citizens, not just those in technology\ncareers. Previous research in AI education materials has largely focused on the\nintroduction of terminology as well as AI use cases and ethics, but few allow\nstudents to learn by creating their own machine learning models. Therefore,\nthere is a need for enriching AI educational tools with more adaptable and\nflexible platforms for interested educators with any level of technical\nexperience to utilize within their teaching material. As such, we propose the\ndevelopment of an open-source tool (Build-a-Bot) for students and teachers to\nnot only create their own transformer-based chatbots based on their own course\nmaterial, but also learn the fundamentals of AI through the model creation\nprocess. The primary concern of this paper is the creation of an interface for\nstudents to learn the principles of artificial intelligence by using a natural\nlanguage pipeline to train a customized model to answer questions based on\ntheir own school curriculums. The model uses contexts given by their\ninstructor, such as chapters of a textbook, to answer questions and is deployed\non an interactive chatbot/voice agent. The pipeline teaches students data\ncollection, data augmentation, intent recognition, and question answering by\nhaving them work through each of these processes while creating their AI agent,\ndiverging from previous chatbot work where students and teachers use the bots\nas black-boxes with no abilities for customization or the bots lack AI\ncapabilities, with the majority of dialogue scripts being rule-based. In\naddition, our tool is designed to make each step of this pipeline intuitive for\nstudents at a middle-school level. Further work primarily lies in providing our\ntool to schools and seeking student and teacher evaluations.", "published": "2022-12-14 22:57:44", "link": "http://arxiv.org/abs/2212.07542v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Find Someone Who: Visual Commonsense Understanding in Human-Centric\n  Grounding", "abstract": "From a visual scene containing multiple people, human is able to distinguish\neach individual given the context descriptions about what happened before,\ntheir mental/physical states or intentions, etc. Above ability heavily relies\non human-centric commonsense knowledge and reasoning. For example, if asked to\nidentify the \"person who needs healing\" in an image, we need to first know that\nthey usually have injuries or suffering expressions, then find the\ncorresponding visual clues before finally grounding the person. We present a\nnew commonsense task, Human-centric Commonsense Grounding, that tests the\nmodels' ability to ground individuals given the context descriptions about what\nhappened before, and their mental/physical states or intentions. We further\ncreate a benchmark, HumanCog, a dataset with 130k grounded commonsensical\ndescriptions annotated on 67k images, covering diverse types of commonsense and\nvisual scenes. We set up a context-object-aware method as a strong baseline\nthat outperforms previous pre-trained and non-pretrained models. Further\nanalysis demonstrates that rich visual commonsense and powerful integration of\nmulti-modal commonsense are essential, which sheds light on future works. Data\nand code will be available https://github.com/Hxyou/HumanCog.", "published": "2022-12-14 01:37:16", "link": "http://arxiv.org/abs/2212.06971v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "SMSMix: Sense-Maintained Sentence Mixup for Word Sense Disambiguation", "abstract": "Word Sense Disambiguation (WSD) is an NLP task aimed at determining the\ncorrect sense of a word in a sentence from discrete sense choices. Although\ncurrent systems have attained unprecedented performances for such tasks, the\nnonuniform distribution of word senses during training generally results in\nsystems performing poorly on rare senses. To this end, we consider data\naugmentation to increase the frequency of these least frequent senses (LFS) to\nreduce the distributional bias of senses during training. We propose\nSense-Maintained Sentence Mixup (SMSMix), a novel word-level mixup method that\nmaintains the sense of a target word. SMSMix smoothly blends two sentences\nusing mask prediction while preserving the relevant span determined by saliency\nscores to maintain a specific word's sense. To the best of our knowledge, this\nis the first attempt to apply mixup in NLP while preserving the meaning of a\nspecific word. With extensive experiments, we validate that our augmentation\nmethod can effectively give more information about rare senses during training\nwith maintained target sense label.", "published": "2022-12-14 07:48:42", "link": "http://arxiv.org/abs/2212.07072v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-Modal Similarity-Based Curriculum Learning for Image Captioning", "abstract": "Image captioning models require the high-level generalization ability to\ndescribe the contents of various images in words. Most existing approaches\ntreat the image-caption pairs equally in their training without considering the\ndifferences in their learning difficulties. Several image captioning approaches\nintroduce curriculum learning methods that present training data with\nincreasing levels of difficulty. However, their difficulty measurements are\neither based on domain-specific features or prior model training. In this\npaper, we propose a simple yet efficient difficulty measurement for image\ncaptioning using cross-modal similarity calculated by a pretrained\nvision-language model. Experiments on the COCO and Flickr30k datasets show that\nour proposed approach achieves superior performance and competitive convergence\nspeed to baselines without requiring heuristics or incurring additional\ntraining costs. Moreover, the higher model performance on difficult examples\nand unseen data also demonstrates the generalization ability.", "published": "2022-12-14 07:52:36", "link": "http://arxiv.org/abs/2212.07075v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Towards mapping the contemporary art world with ArtLM: an art-specific\n  NLP model", "abstract": "With an increasing amount of data in the art world, discovering artists and\nartworks suitable to collectors' tastes becomes a challenge. It is no longer\nenough to use visual information, as contextual information about the artist\nhas become just as important in contemporary art. In this work, we present a\ngeneric Natural Language Processing framework (called ArtLM) to discover the\nconnections among contemporary artists based on their biographies. In this\napproach, we first continue to pre-train the existing general English language\nmodels with a large amount of unlabelled art-related data. We then fine-tune\nthis new pre-trained model with our biography pair dataset manually annotated\nby a team of professionals in the art industry. With extensive experiments, we\ndemonstrate that our ArtLM achieves 85.6% accuracy and 84.0% F1 score and\noutperforms other baseline models. We also provide a visualisation and a\nqualitative analysis of the artist network built from ArtLM's outputs.", "published": "2022-12-14 09:26:07", "link": "http://arxiv.org/abs/2212.07127v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MIST: a Large-Scale Annotated Resource and Neural Models for Functions\n  of Modal Verbs in English Scientific Text", "abstract": "Modal verbs (e.g., \"can\", \"should\", or \"must\") occur highly frequently in\nscientific articles. Decoding their function is not straightforward: they are\noften used for hedging, but they may also denote abilities and restrictions.\nUnderstanding their meaning is important for various NLP tasks such as writing\nassistance or accurate information extraction from scientific text.\n  To foster research on the usage of modals in this genre, we introduce the\nMIST (Modals In Scientific Text) dataset, which contains 3737 modal instances\nin five scientific domains annotated for their semantic, pragmatic, or\nrhetorical function. We systematically evaluate a set of competitive neural\narchitectures on MIST. Transfer experiments reveal that leveraging\nnon-scientific data is of limited benefit for modeling the distinctions in\nMIST. Our corpus analysis provides evidence that scientific communities differ\nin their usage of modal verbs, yet, classifiers trained on scientific data\ngeneralize to some extent to unseen scientific domains.", "published": "2022-12-14 11:10:03", "link": "http://arxiv.org/abs/2212.07156v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Understanding Translationese in Cross-Lingual Summarization", "abstract": "Given a document in a source language, cross-lingual summarization (CLS) aims\nat generating a concise summary in a different target language. Unlike\nmonolingual summarization (MS), naturally occurring source-language documents\npaired with target-language summaries are rare. To collect large-scale CLS\ndata, existing datasets typically involve translation in their creation.\nHowever, the translated text is distinguished from the text originally written\nin that language, i.e., translationese. In this paper, we first confirm that\ndifferent approaches of constructing CLS datasets will lead to different\ndegrees of translationese. Then we systematically investigate how\ntranslationese affects CLS model evaluation and performance when it appears in\nsource documents or target summaries. In detail, we find that (1) the\ntranslationese in documents or summaries of test sets might lead to the\ndiscrepancy between human judgment and automatic evaluation; (2) the\ntranslationese in training sets would harm model performance in real-world\napplications; (3) though machine-translated documents involve translationese,\nthey are very useful for building CLS systems on low-resource languages under\nspecific training strategies. Lastly, we give suggestions for future CLS\nresearch including dataset and model developments. We hope that our work could\nlet researchers notice the phenomenon of translationese in CLS and take it into\naccount in the future.", "published": "2022-12-14 13:41:49", "link": "http://arxiv.org/abs/2212.07220v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "APOLLO: An Optimized Training Approach for Long-form Numerical Reasoning", "abstract": "Long-form numerical reasoning in financial analysis aims to generate a\nreasoning program to calculate the correct answer for a given question.\nPrevious work followed a retriever-generator framework, where the retriever\nselects key facts from a long-form document, and the generator generates a\nreasoning program based on retrieved facts. However, they treated all facts\nequally without considering the different contributions of facts with and\nwithout numbers. Meanwhile, the program consistency were ignored under\nsupervised training, resulting in lower training accuracy and diversity. To\nsolve these problems, we proposed APOLLO to improve the long-form numerical\nreasoning framework. For the retriever, we adopt a number-aware negative\nsampling strategy to enable the retriever to be more discriminative on key\nnumerical facts. For the generator, we design consistency-based reinforcement\nlearning and target program augmentation strategy based on the consistency of\nprogram execution results. Experimental results on the FinQA and ConvFinQA\nleaderboard verify the effectiveness of our proposed method, achieving the new\nstate-of-the-art.", "published": "2022-12-14 14:34:15", "link": "http://arxiv.org/abs/2212.07249v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Linguistically Informed Multi-Objective Pre-Training for Natural\n  Language Inference", "abstract": "We introduce a linguistically enhanced combination of pre-training methods\nfor transformers. The pre-training objectives include POS-tagging, synset\nprediction based on semantic knowledge graphs, and parent prediction based on\ndependency parse trees. Our approach achieves competitive results on the\nNatural Language Inference task, compared to the state of the art. Specifically\nfor smaller models, the method results in a significant performance boost,\nemphasizing the fact that intelligent pre-training can make up for fewer\nparameters and help building more efficient models. Combining POS-tagging and\nsynset prediction yields the overall best results.", "published": "2022-12-14 10:50:13", "link": "http://arxiv.org/abs/2212.07428v5", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Relationship Between Online Harmful Behaviors and Social Network Message\n  Writing Style", "abstract": "In this paper, we explore the relationship between an individual's writing\nstyle and the risk that they will engage in online harmful behaviors (such as\ncyberbullying). In particular, we consider whether measurable differences in\nwriting style relate to different personality types, as modeled by the Big-Five\npersonality traits and the Dark Triad traits, and can differentiate between\nusers who do or do not engage in harmful behaviors. We study messages from\nnearly 2,500 users from two online communities (Twitter and Reddit) and find\nthat we can measure significant personality differences between regular and\nharmful users from the writing style of as few as 100 tweets or 40 Reddit\nposts, aggregate these values to distinguish between healthy and harmful\ncommunities, and also use style attributes to predict which users will engage\nin harmful behaviors.", "published": "2022-12-14 22:13:55", "link": "http://arxiv.org/abs/2212.07526v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Efficient Speech Representation Learning with Low-Bit Quantization", "abstract": "With the development of hardware for machine learning, newer models often\ncome at the cost of both increased sizes and computational complexity. In\neffort to improve the efficiency for these models, we apply and investigate\nrecent quantization techniques on speech representation learning models. The\nquantization techniques were evaluated on the SUPERB benchmark. On the ASR\ntask, with aggressive quantization to 1 bit, we achieved 86.32% storage\nreduction (184.42 -> 25.23), 88% estimated runtime reduction (1.00 -> 0.12)\nwith increased word error rate (7.06 -> 15.96). In comparison with\nDistillHuBERT which also aims for model compression, the 2-bit configuration\nyielded slightly smaller storage (35.84 vs. 46.98), better word error rate\n(12.68 vs. 13.37) and more efficient estimated runtime (0.15 vs. 0.73).", "published": "2022-12-14 06:09:08", "link": "http://arxiv.org/abs/2301.00652v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Disentangling Prosody Representations with Unsupervised Speech\n  Reconstruction", "abstract": "Human speech can be characterized by different components, including semantic\ncontent, speaker identity and prosodic information. Significant progress has\nbeen made in disentangling representations for semantic content and speaker\nidentity in Automatic Speech Recognition (ASR) and speaker verification tasks\nrespectively. However, it is still an open challenging research question to\nextract prosodic information because of the intrinsic association of different\nattributes, such as timbre and rhythm, and because of the need for supervised\ntraining schemes to achieve robust large-scale and speaker-independent ASR. The\naim of this paper is to address the disentanglement of emotional prosody from\nspeech based on unsupervised reconstruction. Specifically, we identify, design,\nimplement and integrate three crucial components in our proposed speech\nreconstruction model Prosody2Vec: (1) a unit encoder that transforms speech\nsignals into discrete units for semantic content, (2) a pretrained speaker\nverification model to generate speaker identity embeddings, and (3) a trainable\nprosody encoder to learn prosody representations. We first pretrain the\nProsody2Vec representations on unlabelled emotional speech corpora, then\nfine-tune the model on specific datasets to perform Speech Emotion Recognition\n(SER) and Emotional Voice Conversion (EVC) tasks. Both objective (weighted and\nunweighted accuracies) and subjective (mean opinion score) evaluations on the\nEVC task suggest that Prosody2Vec effectively captures general prosodic\nfeatures that can be smoothly transferred to other emotional speech. In\naddition, our SER experiments on the IEMOCAP dataset reveal that the prosody\nfeatures learned by Prosody2Vec are complementary and beneficial for the\nperformance of widely used speech pretraining models and surpass the\nstate-of-the-art methods when combining Prosody2Vec with HuBERT\nrepresentations.", "published": "2022-12-14 01:37:35", "link": "http://arxiv.org/abs/2212.06972v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AsPOS: Assamese Part of Speech Tagger using Deep Learning Approach", "abstract": "Part of Speech (POS) tagging is crucial to Natural Language Processing (NLP).\nIt is a well-studied topic in several resource-rich languages. However, the\ndevelopment of computational linguistic resources is still in its infancy\ndespite the existence of numerous languages that are historically and literary\nrich. Assamese, an Indian scheduled language, spoken by more than 25 million\npeople, falls under this category. In this paper, we present a Deep Learning\n(DL)-based POS tagger for Assamese. The development process is divided into two\nstages. In the first phase, several pre-trained word embeddings are employed to\ntrain several tagging models. This allows us to evaluate the performance of the\nword embeddings in the POS tagging task. The top-performing model from the\nfirst phase is employed to annotate another set of new sentences. In the second\nphase, the model is trained further using the fresh dataset. Finally, we attain\na tagging accuracy of 86.52% in F1 score. The model may serve as a baseline for\nfurther study on DL-based Assamese POS tagging.", "published": "2022-12-14 05:36:18", "link": "http://arxiv.org/abs/2212.07043v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "DialogQAE: N-to-N Question Answer Pair Extraction from Customer Service\n  Chatlog", "abstract": "Harvesting question-answer (QA) pairs from customer service chatlog in the\nwild is an efficient way to enrich the knowledge base for customer service\nchatbots in the cold start or continuous integration scenarios. Prior work\nattempts to obtain 1-to-1 QA pairs from growing customer service chatlog, which\nfails to integrate the incomplete utterances from the dialog context for\ncomposite QA retrieval. In this paper, we propose N-to-N QA extraction task in\nwhich the derived questions and corresponding answers might be separated across\ndifferent utterances. We introduce a suite of generative/discriminative tagging\nbased methods with end-to-end and two-stage variants that perform well on 5\ncustomer service datasets and for the first time setup a benchmark for N-to-N\nDialogQAE with utterance and session level evaluation metrics. With a deep dive\ninto extracted QA pairs, we find that the relations between and inside the QA\npairs can be indicators to analyze the dialogue structure, e.g. information\nseeking, clarification, barge-in and elaboration. We also show that the\nproposed models can adapt to different domains and languages, and reduce the\nlabor cost of knowledge accumulation in the real-world product dialogue\nplatform.", "published": "2022-12-14 09:05:14", "link": "http://arxiv.org/abs/2212.07112v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Explainability of Text Processing and Retrieval Methods: A Critical\n  Survey", "abstract": "Deep Learning and Machine Learning based models have become extremely popular\nin text processing and information retrieval. However, the non-linear\nstructures present inside the networks make these models largely inscrutable. A\nsignificant body of research has focused on increasing the transparency of\nthese models. This article provides a broad overview of research on the\nexplainability and interpretability of natural language processing and\ninformation retrieval methods. More specifically, we survey approaches that\nhave been applied to explain word embeddings, sequence modeling, attention\nmodules, transformers, BERT, and document ranking. The concluding section\nsuggests some possible directions for future research on this topic.", "published": "2022-12-14 09:25:49", "link": "http://arxiv.org/abs/2212.07126v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Speech and Natural Language Processing Technologies for Pseudo-Pilot\n  Simulator", "abstract": "This paper describes a simple yet efficient repetition-based modular system\nfor speeding up air-traffic controllers (ATCos) training. E.g., a human pilot\nis still required in EUROCONTROL's ESCAPE lite simulator (see\nhttps://www.eurocontrol.int/simulator/escape) during ATCo training. However,\nthis need can be substituted by an automatic system that could act as a pilot.\nIn this paper, we aim to develop and integrate a pseudo-pilot agent into the\nATCo training pipeline by merging diverse artificial intelligence (AI) powered\nmodules. The system understands the voice communications issued by the ATCo,\nand, in turn, it generates a spoken prompt that follows the pilot's phraseology\nto the initial communication. Our system mainly relies on open-source AI tools\nand air traffic control (ATC) databases, thus, proving its simplicity and ease\nof replicability. The overall pipeline is composed of the following: (1) a\nsubmodule that receives and pre-processes the input stream of raw audio, (2) an\nautomatic speech recognition (ASR) system that transforms audio into a sequence\nof words; (3) a high-level ATC-related entity parser, which extracts relevant\ninformation from the communication, i.e., callsigns and commands, and finally,\n(4) a speech synthesizer submodule that generates responses based on the\nhigh-level ATC entities previously extracted. Overall, we show that this system\ncould pave the way toward developing a real proof-of-concept pseudo-pilot\nsystem. Hence, speeding up the training of ATCos while drastically reducing its\noverall cost.", "published": "2022-12-14 11:34:59", "link": "http://arxiv.org/abs/2212.07164v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "The Infinite Index: Information Retrieval on Generative Text-To-Image\n  Models", "abstract": "Conditional generative models such as DALL-E and Stable Diffusion generate\nimages based on a user-defined text, the prompt. Finding and refining prompts\nthat produce a desired image has become the art of prompt engineering.\nGenerative models do not provide a built-in retrieval model for a user's\ninformation need expressed through prompts. In light of an extensive literature\nreview, we reframe prompt engineering for generative models as interactive\ntext-based retrieval on a novel kind of \"infinite index\". We apply these\ninsights for the first time in a case study on image generation for game design\nwith an expert. Finally, we envision how active learning may help to guide the\nretrieval of generated images.", "published": "2022-12-14 19:50:35", "link": "http://arxiv.org/abs/2212.07476v2", "categories": ["cs.IR", "cs.CL", "cs.CV"], "primary_category": "cs.IR"}
{"title": "Efficient Self-supervised Learning with Contextualized Target\n  Representations for Vision, Speech and Language", "abstract": "Current self-supervised learning algorithms are often modality-specific and\nrequire large amounts of computational resources. To address these issues, we\nincrease the training efficiency of data2vec, a learning objective that\ngeneralizes across several modalities. We do not encode masked tokens, use a\nfast convolutional decoder and amortize the effort to build teacher\nrepresentations. data2vec 2.0 benefits from the rich contextualized target\nrepresentations introduced in data2vec which enable a fast self-supervised\nlearner. Experiments on ImageNet-1K image classification show that data2vec 2.0\nmatches the accuracy of Masked Autoencoders in 16.4x lower pre-training time,\non Librispeech speech recognition it performs as well as wav2vec 2.0 in 10.6x\nless time, and on GLUE natural language understanding it matches a retrained\nRoBERTa model in half the time. Trading some speed for accuracy results in\nImageNet-1K top-1 accuracy of 86.8\\% with a ViT-L model trained for 150 epochs.", "published": "2022-12-14 22:13:11", "link": "http://arxiv.org/abs/2212.07525v2", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Causes and Cures for Interference in Multilingual Translation", "abstract": "Multilingual machine translation models can benefit from synergy between\ndifferent language pairs, but also suffer from interference. While there is a\ngrowing number of sophisticated methods that aim to eliminate interference, our\nunderstanding of interference as a phenomenon is still limited. This work\nidentifies the main factors that contribute to interference in multilingual\nmachine translation. Through systematic experimentation, we find that\ninterference (or synergy) are primarily determined by model size, data size,\nand the proportion of each language pair within the total dataset. We observe\nthat substantial interference occurs mainly when the model is very small with\nrespect to the available training data, and that using standard transformer\nconfigurations with less than one billion parameters largely alleviates\ninterference and promotes synergy. Moreover, we show that tuning the sampling\ntemperature to control the proportion of each language pair in the data is key\nto balancing the amount of interference between low and high resource language\npairs effectively, and can lead to superior performance overall.", "published": "2022-12-14 22:30:55", "link": "http://arxiv.org/abs/2212.07530v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Detection of Contextualized Embedding Bias with Application\n  to Ideology", "abstract": "We propose a fully unsupervised method to detect bias in contextualized\nembeddings. The method leverages the assortative information latently encoded\nby social networks and combines orthogonality regularization, structured\nsparsity learning, and graph neural networks to find the embedding subspace\ncapturing this information. As a concrete example, we focus on the phenomenon\nof ideological bias: we introduce the concept of an ideological subspace, show\nhow it can be found by applying our method to online discussion forums, and\npresent techniques to probe it. Our experiments suggest that the ideological\nsubspace encodes abstract evaluative semantics and reflects changes in the\npolitical left-right spectrum during the presidency of Donald Trump.", "published": "2022-12-14 23:31:14", "link": "http://arxiv.org/abs/2212.07547v1", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "ReDDIT: Regret Detection and Domain Identification from Text", "abstract": "In this paper, we present a study of regret and its expression on social\nmedia platforms. Specifically, we present a novel dataset of Reddit texts that\nhave been classified into three classes: Regret by Action, Regret by Inaction,\nand No Regret. We then use this dataset to investigate the language used to\nexpress regret on Reddit and to identify the domains of text that are most\ncommonly associated with regret. Our findings show that Reddit users are most\nlikely to express regret for past actions, particularly in the domain of\nrelationships. We also found that deep learning models using GloVe embedding\noutperformed other models in all experiments, indicating the effectiveness of\nGloVe for representing the meaning and context of words in the domain of\nregret. Overall, our study provides valuable insights into the nature and\nprevalence of regret on social media, as well as the potential of deep learning\nand word embeddings for analyzing and understanding emotional language in\nonline text. These findings have implications for the development of natural\nlanguage processing algorithms and the design of social media platforms that\nsupport emotional expression and communication.", "published": "2022-12-14 23:41:57", "link": "http://arxiv.org/abs/2212.07549v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DopplerBAS: Binaural Audio Synthesis Addressing Doppler Effect", "abstract": "Recently, binaural audio synthesis (BAS) has emerged as a promising research\nfield for its applications in augmented and virtual realities. Binaural audio\nhelps users orient themselves and establish immersion by providing the brain\nwith interaural time differences reflecting spatial information. However,\nexisting BAS methods are limited in terms of phase estimation, which is crucial\nfor spatial hearing. In this paper, we propose the \\textbf{DopplerBAS} method\nto explicitly address the Doppler effect of the moving sound source.\nSpecifically, we calculate the radial relative velocity of the moving speaker\nin spherical coordinates, which further guides the synthesis of binaural audio.\nThis simple method introduces no additional hyper-parameters and does not\nmodify the loss functions, and is plug-and-play: it scales well to different\ntypes of backbones. DopperBAS distinctly improves the representative WarpNet\nand BinauralGrad backbones in the phase error metric and reaches a new state of\nthe art (SOTA): 0.780 (versus the current SOTA 0.807). Experiments and ablation\nstudies demonstrate the effectiveness of our method.", "published": "2022-12-14 03:18:21", "link": "http://arxiv.org/abs/2212.07000v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Probing Deep Speaker Embeddings for Speaker-related Tasks", "abstract": "Deep speaker embeddings have shown promising results in speaker recognition,\nas well as in other speaker-related tasks. However, some issues are still under\nexplored, for instance, the information encoded in these representations and\ntheir influence on downstream tasks. Four deep speaker embeddings are studied\nin this paper, namely, d-vector, x-vector, ResNetSE-34 and ECAPA-TDNN. Inspired\nby human voice mechanisms, we explored possibly encoded information from\nperspectives of identity, contents and channels; Based on this, experiments\nwere conducted on three categories of speaker-related tasks to further explore\nimpacts of different deep embeddings, including discriminative tasks (speaker\nverification and diarization), guiding tasks (target speaker detection and\nextraction) and regulating tasks (multi-speaker text-to-speech). Results show\nthat all deep embeddings encoded channel and content information in addition to\nspeaker identity, but the extent could vary and their performance on\nspeaker-related tasks can be tremendously different: ECAPA-TDNN is dominant in\ndiscriminative tasks, and d-vector leads the guiding tasks, while regulating\ntask is less sensitive to the choice of speaker representations. These may\nbenefit future research utilizing speaker embeddings.", "published": "2022-12-14 07:33:36", "link": "http://arxiv.org/abs/2212.07068v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Multi-Scale Feature Fusion Transformer Network for End-to-End Single\n  Channel Speech Separation", "abstract": "Recently studies on time-domain audio separation networks (TasNets) have made\na great stride in speech separation. One of the most representative TasNets is\na network with a dual-path segmentation approach. However, the original model\ncalled DPRNN used a fixed feature dimension and unchanged segment size\nthroughout all layers of the network. In this paper, we propose a multi-scale\nfeature fusion transformer network (MSFFT-Net) based on the conventional\ndual-path structure for single-channel speech separation. Unlike the\nconventional dual-path structure where only one processing path exists,\nadopting several iterative blocks with alternative intra-chunk and inter-chunk\noperations to capture local and global context information, the proposed\nMSFFT-Net has multiple parallel processing paths where the feature information\ncan be exchanged between multiple parallel processing paths. Experiments show\nthat our proposed networks based on multi-scale feature fusion structure have\nachieved better results than the original dual-path model on the benchmark\ndataset-WSJ0-2mix, where the SI-SNRi score of MSFFT-3P is 20.7dB (1.47%\nimprovement), and MSFFT-2P is 21.0dB (3.45% improvement), which achieves SOTA\non WSJ0-2mix without any data augmentation method.", "published": "2022-12-14 11:32:28", "link": "http://arxiv.org/abs/2212.07163v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Tackling the Cocktail Fork Problem for Separation and Transcription of\n  Real-World Soundtracks", "abstract": "Emulating the human ability to solve the cocktail party problem, i.e., focus\non a source of interest in a complex acoustic scene, is a long standing goal of\naudio source separation research. Much of this research investigates separating\nspeech from noise, speech from speech, musical instruments from each other, or\nsound events from each other. In this paper, we focus on the cocktail fork\nproblem, which takes a three-pronged approach to source separation by\nseparating an audio mixture such as a movie soundtrack or podcast into the\nthree broad categories of speech, music, and sound effects (SFX - understood to\ninclude ambient noise and natural sound events). We benchmark the performance\nof several deep learning-based source separation models on this task and\nevaluate them with respect to simple objective measures such as\nsignal-to-distortion ratio (SDR) as well as objective metrics that better\ncorrelate with human perception. Furthermore, we thoroughly evaluate how source\nseparation can influence downstream transcription tasks. First, we investigate\nthe task of activity detection on the three sources as a way to both further\nimprove source separation and perform transcription. We formulate the\ntranscription tasks as speech recognition for speech and audio tagging for\nmusic and SFX. We observe that, while the use of source separation estimates\nimproves transcription performance in comparison to the original soundtrack,\nperformance is still sub-optimal due to artifacts introduced by the separation\nprocess. Therefore, we thoroughly investigate how remixing of the three\nseparated source stems at various relative levels can reduce artifacts and\nconsequently improve the transcription performance. We find that remixing music\nand SFX interferences at a target SNR of 17.5 dB reduces speech recognition\nword error rate, and similar impact from remixing is observed for tagging music\nand SFX content.", "published": "2022-12-14 16:47:43", "link": "http://arxiv.org/abs/2212.07327v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Event-driven Spectrotemporal Feature Extraction and Classification using\n  a Silicon Cochlea Model", "abstract": "This paper presents a reconfigurable digital implementation of an event-based\nbinaural cochlear system on a Field Programmable Gate Array (FPGA). It consists\nof a pair of the Cascade of Asymmetric Resonators with Fast Acting Compression\n(CAR FAC) cochlea models and leaky integrate and fire (LIF) neurons.\nAdditionally, we propose an event-driven SpectroTemporal Receptive Field (STRF)\nFeature Extraction using Adaptive Selection Thresholds (FEAST). It is tested on\nthe TIDIGTIS benchmark and compared with current event-based auditory signal\nprocessing approaches and neural networks.", "published": "2022-12-14 09:54:01", "link": "http://arxiv.org/abs/2212.07136v1", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "CLIPSep: Learning Text-queried Sound Separation with Noisy Unlabeled\n  Videos", "abstract": "Recent years have seen progress beyond domain-specific sound separation for\nspeech or music towards universal sound separation for arbitrary sounds. Prior\nwork on universal sound separation has investigated separating a target sound\nout of an audio mixture given a text query. Such text-queried sound separation\nsystems provide a natural and scalable interface for specifying arbitrary\ntarget sounds. However, supervised text-queried sound separation systems\nrequire costly labeled audio-text pairs for training. Moreover, the audio\nprovided in existing datasets is often recorded in a controlled environment,\ncausing a considerable generalization gap to noisy audio in the wild. In this\nwork, we aim to approach text-queried universal sound separation by using only\nunlabeled data. We propose to leverage the visual modality as a bridge to learn\nthe desired audio-textual correspondence. The proposed CLIPSep model first\nencodes the input query into a query vector using the contrastive\nlanguage-image pretraining (CLIP) model, and the query vector is then used to\ncondition an audio separation model to separate out the target sound. While the\nmodel is trained on image-audio pairs extracted from unlabeled videos, at test\ntime we can instead query the model with text inputs in a zero-shot setting,\nthanks to the joint language-image embedding learned by the CLIP model.\nFurther, videos in the wild often contain off-screen sounds and background\nnoise that may hinder the model from learning the desired audio-textual\ncorrespondence. To address this problem, we further propose an approach called\nnoise invariant training for training a query-based sound separation model on\nnoisy data. Experimental results show that the proposed models successfully\nlearn text-queried universal sound separation using only noisy unlabeled\nvideos, even achieving competitive performance against a supervised model in\nsome settings.", "published": "2022-12-14 07:21:45", "link": "http://arxiv.org/abs/2212.07065v2", "categories": ["cs.SD", "cs.CV", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
