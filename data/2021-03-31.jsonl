{"title": "Domain-specific MT for Low-resource Languages: The case of\n  Bambara-French", "abstract": "Translating to and from low-resource languages is a challenge for machine\ntranslation (MT) systems due to a lack of parallel data. In this paper we\naddress the issue of domain-specific MT for Bambara, an under-resourced Mande\nlanguage spoken in Mali. We present the first domain-specific parallel dataset\nfor MT of Bambara into and from French. We discuss challenges in working with\nsmall quantities of domain-specific data for a low-resource language and we\npresent the results of machine learning experiments on this data.", "published": "2021-03-31 18:12:03", "link": "http://arxiv.org/abs/2104.00041v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Statistical Analysis of Summarization Evaluation Metrics using\n  Resampling Methods", "abstract": "The quality of a summarization evaluation metric is quantified by calculating\nthe correlation between its scores and human annotations across a large number\nof summaries. Currently, it is unclear how precise these correlation estimates\nare, nor whether differences between two metrics' correlations reflect a true\ndifference or if it is due to mere chance. In this work, we address these two\nproblems by proposing methods for calculating confidence intervals and running\nhypothesis tests for correlations using two resampling methods, bootstrapping\nand permutation. After evaluating which of the proposed methods is most\nappropriate for summarization through two simulation experiments, we analyze\nthe results of applying these methods to several different automatic evaluation\nmetrics across three sets of human annotations. We find that the confidence\nintervals are rather wide, demonstrating high uncertainty in the reliability of\nautomatic metrics. Further, although many metrics fail to show statistical\nimprovements over ROUGE, two recent works, QAEval and BERTScore, do in some\nevaluation settings.", "published": "2021-03-31 18:28:14", "link": "http://arxiv.org/abs/2104.00054v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Exploration of Data Augmentation Techniques for Improving English to\n  Tigrinya Translation", "abstract": "It has been shown that the performance of neural machine translation (NMT)\ndrops starkly in low-resource conditions, often requiring large amounts of\nauxiliary data to achieve competitive results. An effective method of\ngenerating auxiliary data is back-translation of target language sentences. In\nthis work, we present a case study of Tigrinya where we investigate several\nback-translation methods to generate synthetic source sentences. We find that\nin low-resource conditions, back-translation by pivoting through a\nhigher-resource language related to the target language proves most effective\nresulting in substantial improvements over baselines.", "published": "2021-03-31 03:31:09", "link": "http://arxiv.org/abs/2103.16789v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Supervised Euphemism Detection and Identification for Content\n  Moderation", "abstract": "Fringe groups and organizations have a long history of using\neuphemisms--ordinary-sounding words with a secret meaning--to conceal what they\nare discussing. Nowadays, one common use of euphemisms is to evade content\nmoderation policies enforced by social media platforms. Existing tools for\nenforcing policy automatically rely on keyword searches for words on a \"ban\nlist\", but these are notoriously imprecise: even when limited to swearwords,\nthey can still cause embarrassing false positives. When a commonly used\nordinary word acquires a euphemistic meaning, adding it to a keyword-based ban\nlist is hopeless: consider \"pot\" (storage container or marijuana?) or \"heater\"\n(household appliance or firearm?) The current generation of social media\ncompanies instead hire staff to check posts manually, but this is expensive,\ninhumane, and not much more effective. It is usually apparent to a human\nmoderator that a word is being used euphemistically, but they may not know what\nthe secret meaning is, and therefore whether the message violates policy. Also,\nwhen a euphemism is banned, the group that used it need only invent another\none, leaving moderators one step behind.\n  This paper will demonstrate unsupervised algorithms that, by analyzing words\nin their sentence-level context, can both detect words being used\neuphemistically, and identify the secret meaning of each word. Compared to the\nexisting state of the art, which uses context-free word embeddings, our\nalgorithm for detecting euphemisms achieves 30-400% higher detection accuracies\nof unlabeled euphemisms in a text corpus. Our algorithm for revealing\neuphemistic meanings of words is the first of its kind, as far as we are aware.\nIn the arms race between content moderators and policy evaders, our algorithms\nmay help shift the balance in the direction of the moderators.", "published": "2021-03-31 04:52:38", "link": "http://arxiv.org/abs/2103.16808v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Limited Data Emotional Voice Conversion Leveraging Text-to-Speech:\n  Two-stage Sequence-to-Sequence Training", "abstract": "Emotional voice conversion (EVC) aims to change the emotional state of an\nutterance while preserving the linguistic content and speaker identity. In this\npaper, we propose a novel 2-stage training strategy for sequence-to-sequence\nemotional voice conversion with a limited amount of emotional speech data. We\nnote that the proposed EVC framework leverages text-to-speech (TTS) as they\nshare a common goal that is to generate high-quality expressive voice. In stage\n1, we perform style initialization with a multi-speaker TTS corpus, to\ndisentangle speaking style and linguistic content. In stage 2, we perform\nemotion training with a limited amount of emotional speech data, to learn how\nto disentangle emotional style and linguistic information from the speech. The\nproposed framework can perform both spectrum and prosody conversion and\nachieves significant improvement over the state-of-the-art baselines in both\nobjective and subjective evaluation.", "published": "2021-03-31 04:56:14", "link": "http://arxiv.org/abs/2103.16809v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Few-shot learning through contextual data augmentation", "abstract": "Machine translation (MT) models used in industries with constantly changing\ntopics, such as translation or news agencies, need to adapt to new data to\nmaintain their performance over time. Our aim is to teach a pre-trained MT\nmodel to translate previously unseen words accurately, based on very few\nexamples. We propose (i) an experimental setup allowing us to simulate novel\nvocabulary appearing in human-submitted translations, and (ii) corresponding\nevaluation metrics to compare our approaches. We extend a data augmentation\napproach using a pre-trained language model to create training examples with\nsimilar contexts for novel words. We compare different fine-tuning and data\naugmentation approaches and show that adaptation on the scale of one to five\nexamples is possible. Combining data augmentation with randomly selected\ntraining sentences leads to the highest BLEU score and accuracy improvements.\nImpressively, with only 1 to 5 examples, our model reports better accuracy\nscores than a reference system trained with on average 313 parallel examples.", "published": "2021-03-31 09:05:43", "link": "http://arxiv.org/abs/2103.16911v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Neural Approaches to Relation Triplets Extraction: A Comprehensive\n  Survey", "abstract": "Recently, with the advances made in continuous representation of words (word\nembeddings) and deep neural architectures, many research works are published in\nthe area of relation extraction and it is very difficult to keep track of so\nmany papers. To help future research, we present a comprehensive review of the\nrecently published research works in relation extraction. We mostly focus on\nrelation extraction using deep neural networks which have achieved\nstate-of-the-art performance on publicly available datasets. In this survey, we\ncover sentence-level relation extraction to document-level relation extraction,\npipeline-based approaches to joint extraction approaches, annotated datasets to\ndistantly supervised datasets along with few very recent research directions\nsuch as zero-shot or few-shot relation extraction, noise mitigation in\ndistantly supervised datasets. Regarding neural architectures, we cover\nconvolutional models, recurrent network models, attention network models, and\ngraph convolutional models in this survey.", "published": "2021-03-31 09:27:15", "link": "http://arxiv.org/abs/2103.16929v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UA-GEC: Grammatical Error Correction and Fluency Corpus for the\n  Ukrainian Language", "abstract": "We present a corpus professionally annotated for grammatical error correction\n(GEC) and fluency edits in the Ukrainian language. To the best of our\nknowledge, this is the first GEC corpus for the Ukrainian language. We\ncollected texts with errors (20,715 sentences) from a diverse pool of\ncontributors, including both native and non-native speakers. The data cover a\nwide variety of writing domains, from text chats and essays to formal writing.\nProfessional proofreaders corrected and annotated the corpus for errors\nrelating to fluency, grammar, punctuation, and spelling. This corpus can be\nused for developing and evaluating GEC systems in Ukrainian. More generally, it\ncan be used for researching multilingual and low-resource NLP, morphologically\nrich languages, document-level GEC, and fluency correction. The corpus is\npublicly available at https://github.com/grammarly/ua-gec", "published": "2021-03-31 11:18:36", "link": "http://arxiv.org/abs/2103.16997v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Defx at SemEval-2020 Task 6: Joint Extraction of Concepts and Relations\n  for Definition Extraction", "abstract": "Definition Extraction systems are a valuable knowledge source for both humans\nand algorithms. In this paper we describe our submissions to the DeftEval\nshared task (SemEval-2020 Task 6), which is evaluated on an English textbook\ncorpus. We provide a detailed explanation of our system for the joint\nextraction of definition concepts and the relations among them. Furthermore we\nprovide an ablation study of our model variations and describe the results of\nan error analysis.", "published": "2021-03-31 14:01:20", "link": "http://arxiv.org/abs/2103.17090v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "No Keyword is an Island: In search of covert associations", "abstract": "This paper describes how corpus-assisted discourse analysis based on keyword\n(KW) identification and interpretation can benefit from employing Market basket\nanalysis (MBA) after KW extraction. MBA is a data mining technique used\noriginally in marketing that can reveal consistent associations between items\nin a shopping cart, but also between keywords in a corpus of many texts. By\nidentifying recurring associations between KWs we can compensate for the lack\nof wider context which is a major issue impeding the interpretation of isolated\nKWs (esp. when analyzing large data). To showcase the advantages of MBA in\n\"re-contextualizing\" keywords within the discourse, a pilot study on the topic\nof migration was conducted contrasting anti-system and center-right Czech\ninternet media. was conducted. The results show that MBA is useful in\nidentifying the dominant strategy of anti-system news portals: to weave in a\nconfounding ideological undercurrent and connect the concept of migrants to a\nmultitude of other topics (i.e., flooding the discourse).", "published": "2021-03-31 14:33:29", "link": "http://arxiv.org/abs/2103.17114v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Divide and Rule: Effective Pre-Training for Context-Aware Multi-Encoder\n  Translation Models", "abstract": "Multi-encoder models are a broad family of context-aware neural machine\ntranslation systems that aim to improve translation quality by encoding\ndocument-level contextual information alongside the current sentence. The\ncontext encoding is undertaken by contextual parameters, trained on\ndocument-level data. In this work, we discuss the difficulty of training these\nparameters effectively, due to the sparsity of the words in need of context\n(i.e., the training signal), and their relevant context. We propose to\npre-train the contextual parameters over split sentence pairs, which makes an\nefficient use of the available data for two reasons. Firstly, it increases the\ncontextual training signal by breaking intra-sentential syntactic relations,\nand thus pushing the model to search the context for disambiguating clues more\nfrequently. Secondly, it eases the retrieval of relevant context, since context\nsegments become shorter. We propose four different splitting methods, and\nevaluate our approach with BLEU and contrastive test sets. Results show that it\nconsistently improves learning of contextual parameters, both in low and high\nresource settings.", "published": "2021-03-31 15:15:32", "link": "http://arxiv.org/abs/2103.17151v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Augmenting Poetry Composition with Verse by Verse", "abstract": "We describe Verse by Verse, our experiment in augmenting the creative process\nof writing poetry with an AI. We have created a group of AI poets, styled after\nvarious American classic poets, that are able to offer as suggestions generated\nlines of verse while a user is composing a poem. In this paper, we describe the\nunderlying system to offer these suggestions. This includes a generative model,\nwhich is tasked with generating a large corpus of lines of verse offline and\nwhich are then stored in an index, and a dual-encoder model that is tasked with\nrecommending the next possible set of verses from our index given the previous\nline of verse.", "published": "2021-03-31 16:31:57", "link": "http://arxiv.org/abs/2103.17205v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Misinformation detection in Luganda-English code-mixed social media text", "abstract": "The increasing occurrence, forms, and negative effects of misinformation on\nsocial media platforms has necessitated more misinformation detection tools.\nCurrently, work is being done addressing COVID-19 misinformation however, there\nare no misinformation detection tools for any of the 40 distinct indigenous\nUgandan languages. This paper addresses this gap by presenting basic language\nresources and a misinformation detection data set based on code-mixed\nLuganda-English messages sourced from the Facebook and Twitter social media\nplatforms. Several machine learning methods are applied on the misinformation\ndetection data set to develop classification models for detecting whether a\ncode-mixed Luganda-English message contains misinformation or not. A 10-fold\ncross validation evaluation of the classification methods in an experimental\nmisinformation detection task shows that a Discriminative Multinomial Naive\nBayes (DMNB) method achieves the highest accuracy and F-measure of 78.19% and\n77.90% respectively. Also, Support Vector Machine and Bagging ensemble\nclassification models achieve comparable results. These results are promising\nsince the machine learning models are based on n-gram features from only the\nmisinformation detection dataset.", "published": "2021-03-31 21:12:29", "link": "http://arxiv.org/abs/2104.00124v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "RLAD: Time Series Anomaly Detection through Reinforcement Learning and\n  Active Learning", "abstract": "We introduce a new semi-supervised, time series anomaly detection algorithm\nthat uses deep reinforcement learning (DRL) and active learning to efficiently\nlearn and adapt to anomalies in real-world time series data. Our model - called\nRLAD - makes no assumption about the underlying mechanism that produces the\nobservation sequence and continuously adapts the detection model based on\nexperience with anomalous patterns. In addition, it requires no manual tuning\nof parameters and outperforms all state-of-art methods we compare with, both\nunsupervised and semi-supervised, across several figures of merit. More\nspecifically, we outperform the best unsupervised approach by a factor of 1.58\non the F1 score, with only 1% of labels and up to around 4.4x on another\nreal-world dataset with only 0.1% of labels. We compare RLAD with seven\ndeep-learning based algorithms across two common anomaly detection datasets\nwith up to around 3M data points and between 0.28% to 2.65% anomalies.We\noutperform all of them across several important performance metrics.", "published": "2021-03-31 15:21:15", "link": "http://arxiv.org/abs/2104.00543v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Amharic Text Clustering Using Encyclopedic Knowledge with Neural Word\n  Embedding", "abstract": "In this digital era, almost in every discipline people are using automated\nsystems that generate information represented in document format in different\nnatural languages. As a result, there is a growing interest towards better\nsolutions for finding, organizing and analyzing these documents. In this paper,\nwe propose a system that clusters Amharic text documents using Encyclopedic\nKnowledge (EK) with neural word embedding. EK enables the representation of\nrelated concepts and neural word embedding allows us to handle the contexts of\nthe relatedness. During the clustering process, all the text documents pass\nthrough preprocessing stages. Enriched text document features are extracted\nfrom each document by mapping with EK and word embedding model. TF-IDF weighted\nvector of enriched feature was generated. Finally, text documents are clustered\nusing popular spherical K-means algorithm. The proposed system is tested with\nAmharic text corpus and Amharic Wikipedia data. Test results show that the use\nof EK with word embedding for document clustering improves the average accuracy\nover the use of only EK. Furthermore, changing the size of the class has a\nsignificant effect on accuracy.", "published": "2021-03-31 05:37:33", "link": "http://arxiv.org/abs/2105.00809v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Joint Khmer Word Segmentation and Part-of-Speech Tagging Using Deep\n  Learning", "abstract": "Khmer text is written from left to right with optional space. Space is not\nserved as a word boundary but instead, it is used for readability or other\nfunctional purposes. Word segmentation is a prior step for downstream tasks\nsuch as part-of-speech (POS) tagging and thus, the robustness of POS tagging\nhighly depends on word segmentation. The conventional Khmer POS tagging is a\ntwo-stage process that begins with word segmentation and then actual tagging of\neach word, afterward. In this work, a joint word segmentation and POS tagging\napproach using a single deep learning model is proposed so that word\nsegmentation and POS tagging can be performed spontaneously. The proposed model\nwas trained and tested using the publicly available Khmer POS dataset. The\nvalidation suggested that the performance of the joint model is on par with the\nconventional two-stage POS tagging.", "published": "2021-03-31 04:26:54", "link": "http://arxiv.org/abs/2103.16801v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Neighbourhood Framework for Resource-Lean Content Flagging", "abstract": "We propose a novel framework for cross-lingual content flagging with limited\ntarget-language data, which significantly outperforms prior work in terms of\npredictive performance. The framework is based on a nearest-neighbour\narchitecture. It is a modern instantiation of the vanilla k-nearest neighbour\nmodel, as we use Transformer representations in all its components. Our\nframework can adapt to new source-language instances, without the need to be\nretrained from scratch. Unlike prior work on neighbourhood-based approaches, we\nencode the neighbourhood information based on query--neighbour interactions. We\npropose two encoding schemes and we show their effectiveness using both\nqualitative and quantitative analysis. Our evaluation results on eight\nlanguages from two different datasets for abusive language detection show\nsizable improvements of up to 9.5 F1 points absolute (for Italian) over strong\nbaselines. On average, we achieve 3.6 absolute F1 points of improvement for the\nthree languages in the Jigsaw Multilingual dataset and 2.14 points for the WUL\ndataset.", "published": "2021-03-31 13:22:51", "link": "http://arxiv.org/abs/2103.17055v3", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Modeling Users and Online Communities for Abuse Detection: A Position on\n  Ethics and Explainability", "abstract": "Abuse on the Internet is an important societal problem of our time. Millions\nof Internet users face harassment, racism, personal attacks, and other types of\nabuse across various platforms. The psychological effects of abuse on\nindividuals can be profound and lasting. Consequently, over the past few years,\nthere has been a substantial research effort towards automated abusive language\ndetection in the field of NLP. In this position paper, we discuss the role that\nmodeling of users and online communities plays in abuse detection.\nSpecifically, we review and analyze the state of the art methods that leverage\nuser or community information to enhance the understanding and detection of\nabusive language. We then explore the ethical challenges of incorporating user\nand community information, laying out considerations to guide future research.\nFinally, we address the topic of explainability in abusive language detection,\nproposing properties that an explainable method should aim to exhibit. We\ndescribe how user and community information can facilitate the realization of\nthese properties and discuss the effective operationalization of explainability\nin view of the properties.", "published": "2021-03-31 16:20:37", "link": "http://arxiv.org/abs/2103.17191v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Leveraging Neural Machine Translation for Word Alignment", "abstract": "The most common tools for word-alignment rely on a large amount of parallel\nsentences, which are then usually processed according to one of the IBM model\nalgorithms. The training data is, however, the same as for machine translation\n(MT) systems, especially for neural MT (NMT), which itself is able to produce\nword-alignments using the trained attention heads. This is convenient because\nword-alignment is theoretically a viable byproduct of any attention-based NMT,\nwhich is also able to provide decoder scores for a translated sentence pair.\n  We summarize different approaches on how word-alignment can be extracted from\nalignment scores and then explore ways in which scores can be extracted from\nNMT, focusing on inferring the word-alignment scores based on output sentence\nand token probabilities. We compare this to the extraction of alignment scores\nfrom attention. We conclude with aggregating all of the sources of alignment\nscores into a simple feed-forward network which achieves the best results when\ncombined alignment extractors are used.", "published": "2021-03-31 17:51:35", "link": "http://arxiv.org/abs/2103.17250v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Language Transfer vs Iterative Back Translation for\n  Unsupervised Machine Translation", "abstract": "This work focuses on comparing different solutions for machine translation on\nlow resource language pairs, namely, with zero-shot transfer learning and\nunsupervised machine translation. We discuss how the data size affects the\nperformance of both unsupervised MT and transfer learning. Additionally we also\nlook at how the domain of the data affects the result of unsupervised MT. The\ncode to all the experiments performed in this project are accessible on Github.", "published": "2021-03-31 20:47:19", "link": "http://arxiv.org/abs/2104.00106v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Analysis on Image Set Visual Question Answering", "abstract": "We tackle the challenge of Visual Question Answering in multi-image setting\nfor the ISVQA dataset. Traditional VQA tasks have focused on a single-image\nsetting where the target answer is generated from a single image. Image set\nVQA, however, comprises of a set of images and requires finding connection\nbetween images, relate the objects across images based on these connections and\ngenerate a unified answer. In this report, we work with 4 approaches in a bid\nto improve the performance on the task. We analyse and compare our results with\nthree baseline models - LXMERT, HME-VideoQA and VisualBERT - and show that our\napproaches can provide a slight improvement over the baselines. In specific, we\ntry to improve on the spatial awareness of the model and help the model\nidentify color using enhanced pre-training, reduce language dependence using\nadversarial regularization, and improve counting using regression loss and\ngraph based deduplication. We further delve into an in-depth analysis on the\nlanguage bias in the ISVQA dataset and show how models trained on ISVQA\nimplicitly learn to associate language more strongly with the final answer.", "published": "2021-03-31 20:47:32", "link": "http://arxiv.org/abs/2104.00107v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Multi-Encoder Learning and Stream Fusion for Transformer-Based\n  End-to-End Automatic Speech Recognition", "abstract": "Stream fusion, also known as system combination, is a common technique in\nautomatic speech recognition for traditional hybrid hidden Markov model\napproaches, yet mostly unexplored for modern deep neural network end-to-end\nmodel architectures. Here, we investigate various fusion techniques for the\nall-attention-based encoder-decoder architecture known as the transformer,\nstriving to achieve optimal fusion by investigating different fusion levels in\nan example single-microphone setting with fusion of standard magnitude and\nphase features. We introduce a novel multi-encoder learning method that\nperforms a weighted combination of two encoder-decoder multi-head attention\noutputs only during training. Employing then only the magnitude feature encoder\nin inference, we are able to show consistent improvement on Wall Street Journal\n(WSJ) with language model and on Librispeech, without increase in runtime or\nparameters. Combining two such multi-encoder trained models by a simple late\nfusion in inference, we achieve state-of-the-art performance for\ntransformer-based models on WSJ with a significant WER reduction of 19%\nrelative compared to the current benchmark approach.", "published": "2021-03-31 21:07:43", "link": "http://arxiv.org/abs/2104.00120v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Topic Scaling: A Joint Document Scaling -- Topic Model Approach To Learn\n  Time-Specific Topics", "abstract": "This paper proposes a new methodology to study sequential corpora by\nimplementing a two-stage algorithm that learns time-based topics with respect\nto a scale of document positions and introduces the concept of Topic Scaling\nwhich ranks learned topics within the same document scale. The first stage\nranks documents using Wordfish, a Poisson-based document scaling method, to\nestimate document positions that serve, in the second stage, as a dependent\nvariable to learn relevant topics via a supervised Latent Dirichlet Allocation.\nThis novelty brings two innovations in text mining as it explains document\npositions, whose scale is a latent variable, and ranks the inferred topics on\nthe document scale to match their occurrences within the corpus and track their\nevolution. Tested on the U.S. State Of The Union two-party addresses, this\ninductive approach reveals that each party dominates one end of the learned\nscale with interchangeable transitions that follow the parties' term of office.\nBesides a demonstrated high accuracy in predicting in-sample documents'\npositions from topic scores, this method reveals further hidden topics that\ndifferentiate similar documents by increasing the number of learned topics to\nunfold potential nested hierarchical topic structures. Compared to other\npopular topic models, Topic Scaling learns topics with respect to document\nsimilarities without specifying a time frequency to learn topic evolution, thus\ncapturing broader topic patterns than dynamic topic models and yielding more\ninterpretable outputs than a plain latent Dirichlet allocation.", "published": "2021-03-31 12:35:36", "link": "http://arxiv.org/abs/2104.01117v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "CloneBot: Personalized Dialogue-Response Predictions", "abstract": "Our project task was to create a model that, given a speaker ID, chat\nhistory, and an utterance query, can predict the response utterance in a\nconversation. The model is personalized for each speaker. This task can be a\nuseful tool for building speech bots that talk in a human-like manner in a live\nconversation. Further, we succeeded at using dense-vector encoding clustering\nto be able to retrieve relevant historical dialogue context, a useful strategy\nfor overcoming the input limitations of neural-based models when predictions\nrequire longer-term references from the dialogue history. In this paper, we\nhave implemented a state-of-the-art model using pre-training and fine-tuning\ntechniques built on transformer architecture and multi-headed attention blocks\nfor the Switchboard corpus. We also show how efficient vector clustering\nalgorithms can be used for real-time utterance predictions that require no\ntraining and therefore work on offline and encrypted message histories.", "published": "2021-03-31 01:15:37", "link": "http://arxiv.org/abs/2103.16750v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large-Scale Pre-Training of End-to-End Multi-Talker ASR for Meeting\n  Transcription with Single Distant Microphone", "abstract": "Transcribing meetings containing overlapped speech with only a single distant\nmicrophone (SDM) has been one of the most challenging problems for automatic\nspeech recognition (ASR). While various approaches have been proposed, all\nprevious studies on the monaural overlapped speech recognition problem were\nbased on either simulation data or small-scale real data. In this paper, we\nextensively investigate a two-step approach where we first pre-train a\nserialized output training (SOT)-based multi-talker ASR by using large-scale\nsimulation data and then fine-tune the model with a small amount of real\nmeeting data. Experiments are conducted by utilizing 75 thousand (K) hours of\nour internal single-talker recording to simulate a total of 900K hours of\nmulti-talker audio segments for supervised pre-training. With fine-tuning on\nthe 70 hours of the AMI-SDM training data, our SOT ASR model achieves a word\nerror rate (WER) of 21.2% for the AMI-SDM evaluation set while automatically\ncounting speakers in each test segment. This result is not only significantly\nbetter than the previous state-of-the-art WER of 36.4% with oracle utterance\nboundary information but also better than a result by a similarly fine-tuned\nsingle-talker ASR model applied to beamformed audio.", "published": "2021-03-31 02:43:32", "link": "http://arxiv.org/abs/2103.16776v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Integer-only Zero-shot Quantization for Efficient Speech Recognition", "abstract": "End-to-end neural network models achieve improved performance on various\nautomatic speech recognition (ASR) tasks. However, these models perform poorly\non edge hardware due to large memory and computation requirements. While\nquantizing model weights and/or activations to low-precision can be a promising\nsolution, previous research on quantizing ASR models is limited. In particular,\nthe previous approaches use floating-point arithmetic during inference and thus\nthey cannot fully exploit efficient integer processing units. Moreover, they\nrequire training and/or validation data during quantization, which may not be\navailable due to security or privacy concerns. To address these limitations, we\npropose an integer-only, zero-shot quantization scheme for ASR models. In\nparticular, we generate synthetic data whose runtime statistics resemble the\nreal data, and we use it to calibrate models during quantization. We apply our\nmethod to quantize QuartzNet, Jasper, and Conformer and show negligible WER\ndegradation as compared to the full-precision baseline models, even without\nusing any data. Moreover, we achieve up to 2.35x speedup on a T4 GPU and 4x\ncompression rate, with a modest WER degradation of <1% with INT8 quantization.", "published": "2021-03-31 06:05:40", "link": "http://arxiv.org/abs/2103.16827v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery", "abstract": "Inspired by the ability of StyleGAN to generate highly realistic images in a\nvariety of domains, much recent work has focused on understanding how to use\nthe latent spaces of StyleGAN to manipulate generated and real images. However,\ndiscovering semantically meaningful latent manipulations typically involves\npainstaking human examination of the many degrees of freedom, or an annotated\ncollection of images for each desired manipulation. In this work, we explore\nleveraging the power of recently introduced Contrastive Language-Image\nPre-training (CLIP) models in order to develop a text-based interface for\nStyleGAN image manipulation that does not require such manual effort. We first\nintroduce an optimization scheme that utilizes a CLIP-based loss to modify an\ninput latent vector in response to a user-provided text prompt. Next, we\ndescribe a latent mapper that infers a text-guided latent manipulation step for\na given input image, allowing faster and more stable text-based manipulation.\nFinally, we present a method for mapping a text prompts to input-agnostic\ndirections in StyleGAN's style space, enabling interactive text-driven image\nmanipulation. Extensive results and comparisons demonstrate the effectiveness\nof our approaches.", "published": "2021-03-31 17:51:25", "link": "http://arxiv.org/abs/2103.17249v1", "categories": ["cs.CV", "cs.CL", "cs.GR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Deep Noise Suppression With Non-Intrusive PESQNet Supervision Enabling\n  the Use of Real Training Data", "abstract": "Data-driven speech enhancement employing deep neural networks (DNNs) can\nprovide state-of-the-art performance even in the presence of non-stationary\nnoise. During the training process, most of the speech enhancement neural\nnetworks are trained in a fully supervised way with losses requiring noisy\nspeech to be synthesized by clean speech and additive noise. However, in a real\nimplementation, only the noisy speech mixture is available, which leads to the\nquestion, how such data could be advantageously employed in training. In this\nwork, we propose an end-to-end non-intrusive PESQNet DNN which estimates\nperceptual evaluation of speech quality (PESQ) scores, allowing a\nreference-free loss for real data. As a further novelty, we combine the PESQNet\nloss with denoising and dereverberation loss terms, and train a complex\nmask-based fully convolutional recurrent neural network (FCRN) in a \"weakly\"\nsupervised way, each training cycle employing some synthetic data, some real\ndata, and again synthetic data to keep the PESQNet up-to-date. In a subjective\nlistening test, our proposed framework outperforms the Interspeech 2021 Deep\nNoise Suppression (DNS) Challenge baseline overall by 0.09 MOS points and in\nparticular by 0.45 background noise MOS points.", "published": "2021-03-31 13:58:39", "link": "http://arxiv.org/abs/2103.17088v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "TS-RIR: Translated synthetic room impulse responses for speech\n  augmentation", "abstract": "We present a method for improving the quality of synthetic room impulse\nresponses for far-field speech recognition. We bridge the gap between the\nfidelity of synthetic room impulse responses (RIRs) and the real room impulse\nresponses using our novel, TS-RIRGAN architecture. Given a synthetic RIR in the\nform of raw audio, we use TS-RIRGAN to translate it into a real RIR. We also\nperform real-world sub-band room equalization on the translated synthetic RIR.\nOur overall approach improves the quality of synthetic RIRs by compensating\nlow-frequency wave effects, similar to those in real RIRs. We evaluate the\nperformance of improved synthetic RIRs on a far-field speech dataset augmented\nby convolving the LibriSpeech clean speech dataset [1] with RIRs and adding\nbackground noise. We show that far-field speech augmented using our improved\nsynthetic RIRs reduces the word error rate by up to 19.9% in Kaldi far-field\nautomatic speech recognition benchmark [2].", "published": "2021-03-31 04:45:35", "link": "http://arxiv.org/abs/2103.16804v5", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TeCANet: Temporal-Contextual Attention Network for Environment-Aware\n  Speech Dereverberation", "abstract": "In this paper, we exploit the effective way to leverage contextual\ninformation to improve the speech dereverberation performance in real-world\nreverberant environments. We propose a temporal-contextual attention approach\non the deep neural network (DNN) for environment-aware speech dereverberation,\nwhich can adaptively attend to the contextual information. More specifically, a\nFullBand based Temporal Attention approach (FTA) is proposed, which models the\ncorrelations between the fullband information of the context frames. In\naddition, considering the difference between the attenuation of high frequency\nbands and low frequency bands (high frequency bands attenuate faster than low\nfrequency bands) in the room impulse response (RIR), we also propose a SubBand\nbased Temporal Attention approach (STA). In order to guide the network to be\nmore aware of the reverberant environments, we jointly optimize the\ndereverberation network and the reverberation time (RT60) estimator in a\nmulti-task manner. Our experimental results indicate that the proposed method\noutperforms our previously proposed reverberation-time-aware DNN and the\nlearned attention weights are fully physical consistent. We also report a\npreliminary yet promising dereverberation and recognition experiment on real\ntest data.", "published": "2021-03-31 07:03:26", "link": "http://arxiv.org/abs/2103.16849v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SpecAugment++: A Hidden Space Data Augmentation Method for Acoustic\n  Scene Classification", "abstract": "In this paper, we present SpecAugment++, a novel data augmentation method for\ndeep neural networks based acoustic scene classification (ASC). Different from\nother popular data augmentation methods such as SpecAugment and mixup that only\nwork on the input space, SpecAugment++ is applied to both the input space and\nthe hidden space of the deep neural networks to enhance the input and the\nintermediate feature representations. For an intermediate hidden state, the\naugmentation techniques consist of masking blocks of frequency channels and\nmasking blocks of time frames, which improve generalization by enabling a model\nto attend not only to the most discriminative parts of the feature, but also\nthe entire parts. Apart from using zeros for masking, we also examine two\napproaches for masking based on the use of other samples within the minibatch,\nwhich helps introduce noises to the networks to make them more discriminative\nfor classification. The experimental results on the DCASE 2018 Task1 dataset\nand DCASE 2019 Task1 dataset show that our proposed method can obtain 3.6% and\n4.7% accuracy gains over a strong baseline without augmentation (i.e.\nCP-ResNet) respectively, and outperforms other previous data augmentation\nmethods.", "published": "2021-03-31 07:15:52", "link": "http://arxiv.org/abs/2103.16858v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Y$^2$-Net FCRN for Acoustic Echo and Noise Suppression", "abstract": "In recent years, deep neural networks (DNNs) were studied as an alternative\nto traditional acoustic echo cancellation (AEC) algorithms. The proposed models\nachieved remarkable performance for the separate tasks of AEC and residual echo\nsuppression (RES). A promising network topology is a fully convolutional\nrecurrent network (FCRN) structure, which has already proven its performance on\nboth noise suppression and AEC tasks, individually. However, the combination of\nAEC, postfiltering, and noise suppression to a single network typically leads\nto a noticeable decline in the quality of the near-end speech component due to\nthe lack of a separate loss for echo estimation. In this paper, we propose a\ntwo-stage model (Y$^2$-Net) which consists of two FCRNs, each with two inputs\nand one output (Y-Net). The first stage (AEC) yields an echo estimate, which -\nas a novelty for a DNN AEC model - is further used by the second stage to\nperform RES and noise suppression. While the subjective listening test of the\nInterspeech 2021 AEC Challenge mostly yielded results close to the baseline,\nthe proposed method scored an average improvement of 0.46 points over the\nbaseline on the blind testset in double-talk on the instrumental metric DECMOS,\nprovided by the challenge organizers.", "published": "2021-03-31 16:18:32", "link": "http://arxiv.org/abs/2103.17189v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "PhyAug: Physics-Directed Data Augmentation for Deep Sensing Model\n  Transfer in Cyber-Physical Systems", "abstract": "Run-time domain shifts from training-phase domains are common in sensing\nsystems designed with deep learning. The shifts can be caused by sensor\ncharacteristic variations and/or discrepancies between the design-phase model\nand the actual model of the sensed physical process. To address these issues,\nexisting transfer learning techniques require substantial target-domain data\nand thus incur high post-deployment overhead. This paper proposes to exploit\nthe first principle governing the domain shift to reduce the demand on\ntarget-domain data. Specifically, our proposed approach called PhyAug uses the\nfirst principle fitted with few labeled or unlabeled source/target-domain data\npairs to transform the existing source-domain training data into augmented data\nfor updating the deep neural networks. In two case studies of keyword spotting\nand DeepSpeech2-based automatic speech recognition, with 5-second unlabeled\ndata collected from the target microphones, PhyAug recovers the recognition\naccuracy losses due to microphone characteristic variations by 37% to 72%. In a\ncase study of seismic source localization with TDoA fngerprints, by exploiting\nthe frst principle of signal propagation in uneven media, PhyAug only requires\n3% to 8% of labeled TDoA measurements required by the vanilla fingerprinting\napproach in achieving the same localization accuracy.", "published": "2021-03-31 13:49:45", "link": "http://arxiv.org/abs/2104.01160v2", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Near field Acoustic Holography on arbitrary shapes using Convolutional\n  Neural Network", "abstract": "Near-field Acoustic Holography (NAH) is a well-known problem aimed at\nestimating the vibrational velocity field of a structure by means of acoustic\nmeasurements. In this paper, we propose a NAH technique based on Convolutional\nNeural Network (CNN). The devised CNN predicts the vibrational field on the\nsurface of arbitrary shaped plates (violin plates) with orthotropic material\nproperties from a limited number of measurements. In particular, the\narchitecture, named Super Resolution CNN (SRCNN), is able to estimate the\nvibrational field with a higher spatial resolution compared to the input\npressure. The pressure and velocity datasets have been generated through Finite\nElement Method simulations. We validate the proposed method by comparing the\nestimates with the synthesized ground truth and with a state-of-the-art\ntechnique. Moreover, we evaluate the robustness of the devised network against\nnoisy input data.", "published": "2021-03-31 09:41:11", "link": "http://arxiv.org/abs/2103.16935v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adversarial Attacks and Defenses for Speech Recognition Systems", "abstract": "The ubiquitous presence of machine learning systems in our lives necessitates\nresearch into their vulnerabilities and appropriate countermeasures. In\nparticular, we investigate the effectiveness of adversarial attacks and\ndefenses against automatic speech recognition (ASR) systems. We select two ASR\nmodels - a thoroughly studied DeepSpeech model and a more recent Espresso\nframework Transformer encoder-decoder model. We investigate two threat models:\na denial-of-service scenario where fast gradient-sign method (FGSM) or weak\nprojected gradient descent (PGD) attacks are used to degrade the model's word\nerror rate (WER); and a targeted scenario where a more potent imperceptible\nattack forces the system to recognize a specific phrase. We find that the\nattack transferability across the investigated ASR systems is limited. To\ndefend the model, we use two preprocessing defenses: randomized smoothing and\nWaveGAN-based vocoder, and find that they significantly improve the model's\nadversarial robustness. We show that a WaveGAN vocoder can be a useful\ncountermeasure to adversarial attacks on ASR systems - even when it is jointly\nattacked with the ASR, the target phrases' word error rate is high.", "published": "2021-03-31 14:44:58", "link": "http://arxiv.org/abs/2103.17122v1", "categories": ["eess.AS", "cs.CR", "cs.SD"], "primary_category": "eess.AS"}
