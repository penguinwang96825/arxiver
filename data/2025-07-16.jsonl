{"title": "Language Models Improve When Pretraining Data Matches Target Tasks", "abstract": "Every data selection method inherently has a target. In practice, these\ntargets often emerge implicitly through benchmark-driven iteration: researchers\ndevelop selection strategies, train models, measure benchmark performance, then\nrefine accordingly. This raises a natural question: what happens when we make\nthis optimization explicit? To explore this, we propose benchmark-targeted\nranking (BETR), a simple method that selects pretraining documents based on\nsimilarity to benchmark training examples. BETR embeds benchmark examples and a\nsample of pretraining documents in a shared space, scores this sample by\nsimilarity to benchmarks, then trains a lightweight classifier to predict these\nscores for the full corpus. We compare data selection methods by training over\n500 models spanning $10^{19}$ to $10^{22}$ FLOPs and fitting scaling laws to\nthem. From this, we find that simply aligning pretraining data to evaluation\nbenchmarks using BETR achieves a 2.1x compute multiplier over DCLM-Baseline\n(4.7x over unfiltered data) and improves performance on 9 out of 10 tasks\nacross all scales. BETR also generalizes well: when targeting a diverse set of\nbenchmarks disjoint from our evaluation suite, it still matches or outperforms\nbaselines. Our scaling analysis further reveals a clear trend: larger models\nrequire less aggressive filtering. Overall, our findings show that directly\nmatching pretraining data to target tasks precisely shapes model capabilities\nand highlight that optimal selection strategies must adapt to model scale.", "published": "2025-07-16 17:59:45", "link": "http://arxiv.org/abs/2507.12466v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling", "abstract": "Modeling latent representations in a hyperspherical space has proven\neffective for capturing directional similarities in high-dimensional text data,\nbenefiting topic modeling. Variational autoencoder-based neural topic models\n(VAE-NTMs) commonly adopt the von Mises-Fisher prior to encode hyperspherical\nstructure. However, VAE-NTMs often suffer from posterior collapse, where the KL\ndivergence term in the objective function highly diminishes, leading to\nineffective latent representations. To mitigate this issue while modeling\nhyperspherical structure in the latent space, we propose the Spherical Sliced\nWasserstein Autoencoder for Topic Modeling (S2WTM). S2WTM employs a prior\ndistribution supported on the unit hypersphere and leverages the Spherical\nSliced-Wasserstein distance to align the aggregated posterior distribution with\nthe prior. Experimental results demonstrate that S2WTM outperforms\nstate-of-the-art topic models, generating more coherent and diverse topics\nwhile improving performance on downstream tasks.", "published": "2025-07-16 17:47:45", "link": "http://arxiv.org/abs/2507.12451v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models", "abstract": "Open-weights reasoning language models generate long chains-of-thought (CoTs)\nbefore producing a final response, which improves performance but introduces\nadditional alignment risks, with harmful content often appearing in both the\nCoTs and the final outputs. In this work, we investigate if we can use CoTs to\npredict final response misalignment. We evaluate a range of monitoring\napproaches, including humans, highly-capable large language models, and text\nclassifiers, using either CoT text or activations. First, we find that a simple\nlinear probe trained on CoT activations can significantly outperform all\ntext-based methods in predicting whether a final response will be safe or\nunsafe. CoT texts are often unfaithful and can mislead humans and classifiers,\nwhile model latents (i.e., CoT activations) offer a more reliable predictive\nsignal. Second, the probe makes accurate predictions before reasoning\ncompletes, achieving strong performance even when applied to early CoT\nsegments. These findings generalize across model sizes, families, and safety\nbenchmarks, suggesting that lightweight probes could enable real-time safety\nmonitoring and early intervention during generation.", "published": "2025-07-16 17:16:03", "link": "http://arxiv.org/abs/2507.12428v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Advancing Retrieval-Augmented Generation for Structured Enterprise and Internal Data", "abstract": "Organizations increasingly rely on proprietary enterprise data, including HR\nrecords, structured reports, and tabular documents, for critical\ndecision-making. While Large Language Models (LLMs) have strong generative\ncapabilities, they are limited by static pretraining, short context windows,\nand challenges in processing heterogeneous data formats. Conventional\nRetrieval-Augmented Generation (RAG) frameworks address some of these gaps but\noften struggle with structured and semi-structured data.\n  This work proposes an advanced RAG framework that combines hybrid retrieval\nstrategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by\nmetadata-aware filtering with SpaCy NER and cross-encoder reranking. The\nframework applies semantic chunking to maintain textual coherence and retains\ntabular data structures to preserve row-column integrity. Quantized indexing\noptimizes retrieval efficiency, while human-in-the-loop feedback and\nconversation memory improve adaptability.\n  Experiments on enterprise datasets show notable improvements: Precision@5\nincreased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74),\nand Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative\nevaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness\n(4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale.\nThese results demonstrate the framework's effectiveness in delivering accurate,\ncomprehensive, and contextually relevant responses for enterprise tasks. Future\nwork includes extending to multimodal data and integrating agent-based\nretrieval. The source code will be released at\nhttps://github.com/CheerlaChandana/Enterprise-Chatbot", "published": "2025-07-16 17:13:06", "link": "http://arxiv.org/abs/2507.12425v1", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Probing for Arithmetic Errors in Language Models", "abstract": "We investigate whether internal activations in language models can be used to\ndetect arithmetic errors. Starting with a controlled setting of 3-digit\naddition, we show that simple probes can accurately decode both the model's\npredicted output and the correct answer from hidden states, regardless of\nwhether the model's output is correct. Building on this, we train lightweight\nerror detectors that predict model correctness with over 90% accuracy. We then\nextend our analysis to structured chain-of-thought traces on addition-only\nGSM8K problems and find that probes trained on simple arithmetic generalize\nwell to this more complex setting, revealing consistent internal\nrepresentations. Finally, we demonstrate that these probes can guide selective\nre-prompting of erroneous reasoning steps, improving task accuracy with minimal\ndisruption to correct outputs. Our findings suggest that arithmetic errors can\nbe anticipated from internal activations alone, and that simple probes offer a\nviable path toward lightweight model self-correction.", "published": "2025-07-16 16:27:50", "link": "http://arxiv.org/abs/2507.12379v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Developing Visual Augmented Q&A System using Scalable Vision Embedding Retrieval & Late Interaction Re-ranker", "abstract": "Traditional information extraction systems face challenges with text only\nlanguage models as it does not consider infographics (visual elements of\ninformation) such as tables, charts, images etc. often used to convey complex\ninformation to readers. Multimodal LLM (MLLM) face challenges of finding needle\nin the haystack problem i.e., either longer context length or substantial\nnumber of documents as search space. Late interaction mechanism over visual\nlanguage models has shown state of the art performance in retrieval-based\nvision augmented Q&A tasks. There are yet few challenges using it for RAG based\nmulti-modal Q&A. Firstly, many popular and widely adopted vector databases do\nnot support native multi-vector retrieval. Secondly, late interaction requires\ncomputation which inflates space footprint and can hinder enterprise adoption.\nLastly, the current state of late interaction mechanism does not leverage the\napproximate neighbor search indexing methods for large speed ups in retrieval\nprocess. This paper explores a pragmatic approach to make vision retrieval\nprocess scalable and efficient without compromising on performance quality. We\npropose multi-step custom implementation utilizing widely adopted hybrid search\n(metadata & embedding) and state of the art late interaction re-ranker to\nretrieve best matching pages. Finally, MLLM are prompted as reader to generate\nanswers from contextualized best matching pages. Through experiments, we\nobserve that the proposed design is scalable (significant speed up) and stable\n(without degrading performance quality), hence can be used as production\nsystems at enterprises.", "published": "2025-07-16 16:27:05", "link": "http://arxiv.org/abs/2507.12378v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Web-Browsing LLMs Can Access Social Media Profiles and Infer User Demographics", "abstract": "Large language models (LLMs) have traditionally relied on static training\ndata, limiting their knowledge to fixed snapshots. Recent advancements,\nhowever, have equipped LLMs with web browsing capabilities, enabling real time\ninformation retrieval and multi step reasoning over live web content. While\nprior studies have demonstrated LLMs ability to access and analyze websites,\ntheir capacity to directly retrieve and analyze social media data remains\nunexplored. Here, we evaluate whether web browsing LLMs can infer demographic\nattributes of social media users given only their usernames. Using a synthetic\ndataset of 48 X (Twitter) accounts and a survey dataset of 1,384 international\nparticipants, we show that these models can access social media content and\npredict user demographics with reasonable accuracy. Analysis of the synthetic\ndataset further reveals how LLMs parse and interpret social media profiles,\nwhich may introduce gender and political biases against accounts with minimal\nactivity. While this capability holds promise for computational social science\nin the post API era, it also raises risks of misuse particularly in information\noperations and targeted advertising underscoring the need for safeguards. We\nrecommend that LLM providers restrict this capability in public facing\napplications, while preserving controlled access for verified research\npurposes.", "published": "2025-07-16 16:21:01", "link": "http://arxiv.org/abs/2507.12372v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests through Debate", "abstract": "Large Language Models (LLMs) have demonstrated significant capabilities in\nunderstanding and generating human language, contributing to more natural\ninteractions with complex systems. However, they face challenges such as\nambiguity in user requests processed by LLMs. To address these challenges, this\npaper introduces and evaluates a multi-agent debate framework designed to\nenhance detection and resolution capabilities beyond single models. The\nframework consists of three LLM architectures (Llama3-8B, Gemma2-9B, and\nMistral-7B variants) and a dataset with diverse ambiguities. The debate\nframework markedly enhanced the performance of Llama3-8B and Mistral-7B\nvariants over their individual baselines, with Mistral-7B-led debates achieving\na notable 76.7% success rate and proving particularly effective for complex\nambiguities and efficient consensus. While acknowledging varying model\nresponses to collaborative strategies, these findings underscore the debate\nframework's value as a targeted method for augmenting LLM capabilities. This\nwork offers important insights for developing more robust and adaptive language\nunderstanding systems by showing how structured debates can lead to improved\nclarity in interactive systems.", "published": "2025-07-16 16:15:25", "link": "http://arxiv.org/abs/2507.12370v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Exploring Gender Bias in Alzheimer's Disease Detection: Insights from Mandarin and Greek Speech Perception", "abstract": "Gender bias has been widely observed in speech perception tasks, influenced\nby the fundamental voicing differences between genders. This study reveals a\ngender bias in the perception of Alzheimer's Disease (AD) speech. In a\nperception experiment involving 16 Chinese listeners evaluating both Chinese\nand Greek speech, we identified that male speech was more frequently identified\nas AD, with this bias being particularly pronounced in Chinese speech. Acoustic\nanalysis showed that shimmer values in male speech were significantly\nassociated with AD perception, while speech portion exhibited a significant\nnegative correlation with AD identification. Although language did not have a\nsignificant impact on AD perception, our findings underscore the critical role\nof gender bias in AD speech perception. This work highlights the necessity of\naddressing gender bias when developing AD detection models and calls for\nfurther research to validate model performance across different linguistic\ncontexts.", "published": "2025-07-16 15:56:09", "link": "http://arxiv.org/abs/2507.12356v1", "categories": ["cs.CL", "cs.HC", "cs.SD"], "primary_category": "cs.CL"}
{"title": "Nonlinear Concept Erasure: a Density Matching Approach", "abstract": "Ensuring that neural models used in real-world applications cannot infer\nsensitive information, such as demographic attributes like gender or race, from\ntext representations is a critical challenge when fairness is a concern. We\naddress this issue through concept erasure, a process that removes information\nrelated to a specific concept from distributed representations while preserving\nas much of the remaining semantic information as possible. Our approach\ninvolves learning an orthogonal projection in the embedding space, designed to\nmake the class-conditional feature distributions of the discrete concept to\nerase indistinguishable after projection. By adjusting the rank of the\nprojector, we control the extent of information removal, while its\northogonality ensures strict preservation of the local structure of the\nembeddings. Our method, termed $\\overline{\\mathrm{L}}$EOPARD, achieves\nstate-of-the-art performance in nonlinear erasure of a discrete attribute on\nclassic natural language processing benchmarks. Furthermore, we demonstrate\nthat $\\overline{\\mathrm{L}}$EOPARD effectively mitigates bias in deep nonlinear\nclassifiers, thereby promoting fairness.", "published": "2025-07-16 15:36:15", "link": "http://arxiv.org/abs/2507.12341v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization", "abstract": "Large Language Models (LLMs) have become widely used across diverse NLP tasks\nand domains, demonstrating their adaptability and effectiveness. In the realm\nof Electronic Design Automation (EDA), LLMs show promise for tasks like\nRegister-Transfer Level (RTL) code generation and summarization. However,\ndespite the proliferation of LLMs for general code-related tasks, there's a\ndearth of research focused on evaluating and refining these models for hardware\ndescription languages (HDLs), notably VHDL. In this study, we evaluate the\nperformance of existing code LLMs for VHDL code generation and summarization\nusing various metrics and two datasets -- VHDL-Eval and VHDL-Xform. The latter,\nan in-house dataset, aims to gauge LLMs' understanding of functionally\nequivalent code. Our findings reveal consistent underperformance of these\nmodels across different metrics, underscoring a significant gap in their\nsuitability for this domain. To address this challenge, we propose\nChain-of-Descriptions (CoDes), a novel approach to enhance the performance of\nLLMs for VHDL code generation and summarization tasks. CoDes involves\ngenerating a series of intermediate descriptive steps based on: (i) the problem\nstatement for code generation, and (ii) the VHDL code for summarization. These\nsteps are then integrated with the original input prompt (problem statement or\ncode) and provided as input to the LLMs to generate the final output. Our\nexperiments demonstrate that the CoDes approach significantly surpasses the\nstandard prompting strategy across various metrics on both datasets. This\nmethod not only improves the quality of VHDL code generation and summarization\nbut also serves as a framework for future research aimed at enhancing code LLMs\nfor VHDL.", "published": "2025-07-16 15:05:30", "link": "http://arxiv.org/abs/2507.12308v1", "categories": ["cs.CL", "cs.AI", "cs.AR"], "primary_category": "cs.CL"}
{"title": "Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding", "abstract": "Text anomaly detection is a critical task in natural language processing\n(NLP), with applications spanning fraud detection, misinformation\nidentification, spam detection and content moderation, etc. Despite significant\nadvances in large language models (LLMs) and anomaly detection algorithms, the\nabsence of standardized and comprehensive benchmarks for evaluating the\nexisting anomaly detection methods on text data limits rigorous comparison and\ndevelopment of innovative approaches. This work performs a comprehensive\nempirical study and introduces a benchmark for text anomaly detection,\nleveraging embeddings from diverse pre-trained language models across a wide\narray of text datasets. Our work systematically evaluates the effectiveness of\nembedding-based text anomaly detection by incorporating (1) early language\nmodels (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI\n(small, ada, large)); (3) multi-domain text datasets (news, social media,\nscientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC).\nOur experiments reveal a critical empirical insight: embedding quality\nsignificantly governs anomaly detection efficacy, and deep learning-based\napproaches demonstrate no performance advantage over conventional shallow\nalgorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived\nembeddings.In addition, we observe strongly low-rank characteristics in\ncross-model performance matrices, which enables an efficient strategy for rapid\nmodel evaluation (or embedding evaluation) and selection in practical\napplications. Furthermore, by open-sourcing our benchmark toolkit that includes\nall embeddings from different models and code at\nhttps://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work\nprovides a foundation for future research in robust and scalable text anomaly\ndetection systems.", "published": "2025-07-16 14:47:41", "link": "http://arxiv.org/abs/2507.12295v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MERA Code: A Unified Framework for Evaluating Code Generation Across Tasks", "abstract": "Advancements in LLMs have enhanced task automation in software engineering;\nhowever, current evaluations primarily focus on natural language tasks,\noverlooking code quality. Most benchmarks prioritize high-level reasoning over\nexecutable code and real-world performance, leaving gaps in understanding true\ncapabilities and risks associated with these models in production. To address\nthis issue, we propose MERA Code, a new addition to the MERA benchmark family,\nspecifically focused on evaluating code for the latest code generation LLMs in\nRussian. This benchmark includes 11 evaluation tasks that span 8 programming\nlanguages. Our proposed evaluation methodology features a taxonomy that\noutlines the practical coding skills necessary for models to complete these\ntasks. The benchmark comprises an open-source codebase for users to conduct\nMERA assessments, a scoring system compatible with various programming\nenvironments, and a platform featuring a leaderboard and submission system. We\nevaluate open LLMs and frontier API models, analyzing their limitations in\nterms of practical coding tasks in non-English languages. We are publicly\nreleasing MERA to guide future research, anticipate groundbreaking features in\nmodel development, and standardize evaluation procedures.", "published": "2025-07-16 14:31:33", "link": "http://arxiv.org/abs/2507.12284v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Infherno: End-to-end Agent-based FHIR Resource Synthesis from Free-form Clinical Notes", "abstract": "For clinical data integration and healthcare services, the HL7 FHIR standard\nhas established itself as a desirable format for interoperability between\ncomplex health data. Previous attempts at automating the translation from\nfree-form clinical notes into structured FHIR resources rely on modular,\nrule-based systems or LLMs with instruction tuning and constrained decoding.\nSince they frequently suffer from limited generalizability and structural\ninconformity, we propose an end-to-end framework powered by LLM agents, code\nexecution, and healthcare terminology database tools to address these issues.\nOur solution, called Infherno, is designed to adhere to the FHIR document\nschema and competes well with a human baseline in predicting FHIR resources\nfrom unstructured text. The implementation features a front end for custom and\nsynthetic data and both local and proprietary models, supporting clinical data\nintegration processes and interoperability across institutions.", "published": "2025-07-16 14:06:51", "link": "http://arxiv.org/abs/2507.12261v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Translationese-index: Using Likelihood Ratios for Graded and Generalizable Measurement of Translationese", "abstract": "In this paper, we propose the first quantitative measure for translationese\n-- the translationese-index (T-index) for graded and generalizable measurement\nof translationese, computed from the likelihood ratios of two contrastively\nfine-tuned language models (LMs). We use a synthesized dataset and a dataset\nwith translations in the wild to evaluate T-index's generalizability in\ncross-domain settings and its validity against human judgments. Our results\nshow that T-index is both robust and efficient. T-index scored by two 0.5B LMs\nfine-tuned on only 1-5k pairs of synthetic data can well capture translationese\nin the wild. We find that the relative differences in T-indices between\ntranslations can well predict pairwise translationese annotations obtained from\nhuman annotators; and the absolute values of T-indices correlate well with\nhuman ratings of degrees of translationese (Pearson's $r = 0.568$).\nAdditionally, the correlation between T-index and existing machine translation\n(MT) quality estimation (QE) metrics such as BLEU and COMET is low, suggesting\nthat T-index is not covered by these metrics and can serve as a complementary\nmetric in MT QE.", "published": "2025-07-16 14:06:05", "link": "http://arxiv.org/abs/2507.12260v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Contextual ASR via Multi-grained Fusion with Large Language Models", "abstract": "While end-to-end Automatic Speech Recognition (ASR) models have shown\nimpressive performance in transcribing general speech, they often struggle to\naccurately recognize contextually relevant keywords, such as proper nouns or\nuser-specific entities.\n  Previous approaches have explored leveraging keyword dictionaries in the\ntextual modality to improve keyword recognition, either through token-level\nfusion that guides token-by-token generation or phrase-level fusion that\nenables direct copying of keyword phrases.\n  However, these methods operate at different granularities and have their own\nlimitations.\n  In this paper, we propose a novel multi-grained fusion approach that jointly\nleverages the strengths of both token-level and phrase-level fusion with Large\nLanguage Models (LLMs).\n  Our approach incorporates a late-fusion strategy that elegantly combines\nASR's acoustic information with LLM's rich contextual knowledge, balancing\nfine-grained token precision with holistic phrase-level understanding.\n  Experiments on Chinese and English datasets demonstrate that our approach\nachieves state-of-the-art performance on keyword-related metrics while\npreserving high accuracy on non-keyword text.\n  Ablation studies further confirm that the token-level and phrase-level\ncomponents both contribute significantly to the performance gains,\ncomplementing each other in our joint multi-grained framework.\n  The code and models will be publicly available at https://github.com/.", "published": "2025-07-16 13:59:32", "link": "http://arxiv.org/abs/2507.12252v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards few-shot isolated word reading assessment", "abstract": "We explore an ASR-free method for isolated word reading assessment in\nlow-resource settings. Our few-shot approach compares input child speech to a\nsmall set of adult-provided reference templates. Inputs and templates are\nencoded using intermediate layers from large self-supervised learned (SSL)\nmodels. Using an Afrikaans child speech benchmark, we investigate design\noptions such as discretising SSL features and barycentre averaging of the\ntemplates. Idealised experiments show reasonable performance for adults, but a\nsubstantial drop for child speech input, even with child templates. Despite the\nsuccess of employing SSL representations in low-resource speech tasks, our work\nhighlights the limitations of SSL representations for processing child data\nwhen used in a few-shot classification system.", "published": "2025-07-16 13:20:32", "link": "http://arxiv.org/abs/2507.12217v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Toward a Behavioural Translation Style Space: Simulating the Temporal Dynamics of Affect, Behaviour, and Cognition in Human Translation Production", "abstract": "The paper introduces a Behavioural Translation Style Space (BTSS) that\ndescribes possible behavioural translation patterns. The suggested BTSS is\norganized as a hierarchical structure that entails various embedded processing\nlayers. We posit that observable translation behaviour - i.e., eye and finger\nmovements - is fundamental when executing the physical act of translation but\nit is caused and shaped by higher-order cognitive processes and affective\ntranslation states. We analyse records of keystrokes and gaze data as\nindicators of the hidden mental processing structure and organize the\nbehavioural patterns as a multi-layered embedded BTSS. The BTSS serves as the\nbasis for a computational translation agent to simulate the temporal dynamics\nof affect, automatized behaviour and cognition during human translation\nproduction.", "published": "2025-07-16 13:10:10", "link": "http://arxiv.org/abs/2507.12208v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RUMAA: Repeat-Aware Unified Music Audio Analysis for Score-Performance Alignment, Transcription, and Mistake Detection", "abstract": "This study introduces RUMAA, a transformer-based framework for music\nperformance analysis that unifies score-to-performance alignment,\nscore-informed transcription, and mistake detection in a near end-to-end\nmanner. Unlike prior methods addressing these tasks separately, RUMAA\nintegrates them using pre-trained score and audio encoders and a novel\ntri-stream decoder capturing task interdependencies through proxy tasks. It\naligns human-readable MusicXML scores with repeat symbols to full-length\nperformance audio, overcoming traditional MIDI-based methods that rely on\nmanually unfolded score-MIDI data with pre-specified repeat structures. RUMAA\nmatches state-of-the-art alignment methods on non-repeated scores and\noutperforms them on scores with repeats in a public piano music dataset, while\nalso delivering promising transcription and mistake detection results.", "published": "2025-07-16 12:13:13", "link": "http://arxiv.org/abs/2507.12175v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Overview of the Sensemaking Task at the ELOQUENT 2025 Lab: LLMs as Teachers, Students and Evaluators", "abstract": "ELOQUENT is a set of shared tasks that aims to create easily testable\nhigh-level criteria for evaluating generative language models. Sensemaking is\none such shared task.\n  In Sensemaking, we try to assess how well generative models ``make sense out\nof a given text'' in three steps inspired by exams in a classroom setting: (1)\nTeacher systems should prepare a set of questions, (2) Student systems should\nanswer these questions, and (3) Evaluator systems should score these answers,\nall adhering rather strictly to a given set of input materials.\n  We report on the 2025 edition of Sensemaking, where we had 7 sources of test\nmaterials (fact-checking analyses of statements, textbooks, transcribed\nrecordings of a lecture, and educational videos) spanning English, German,\nUkrainian, and Czech languages.\n  This year, 4 teams participated, providing us with 2 Teacher submissions, 2\nStudent submissions, and 2 Evaluator submissions. We added baselines for\nTeacher and Student using commercial large language model systems. We devised a\nfully automatic evaluation procedure, which we compare to a minimalistic manual\nevaluation.\n  We were able to make some interesting observations. For the first task, the\ncreation of questions, better evaluation strategies will still have to be\ndevised because it is difficult to discern the quality of the various candidate\nquestion sets. In the second task, question answering, the LLMs examined\noverall perform acceptably, but restricting their answers to the given input\ntexts remains problematic. In the third task, evaluation of question answers,\nour adversarial tests reveal that systems using the LLM-as-a-Judge paradigm\nerroneously rate both garbled question-answer pairs and answers to mixed-up\nquestions as acceptable.", "published": "2025-07-16 11:19:28", "link": "http://arxiv.org/abs/2507.12143v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA Optimization", "abstract": "Low-Rank Adaptation (LoRA) has become a widely adopted standard for\nparameter-efficient fine-tuning of large language models (LLMs), significantly\nreducing memory and computational demands. However, challenges remain,\nincluding finding optimal initialization strategies or mitigating\noverparametrization in low-rank matrix factorization. In this work, we propose\na novel approach that addresses both of the challenges simultaneously within a\nunified framework. Our method treats a set of fixed-rank LoRA matrices as a\nsmooth manifold. Considering adapters as elements on this manifold removes\noverparametrization, while determining the direction of the fastest loss\ndecrease along the manifold provides initialization. Special care is taken to\nobtain numerically stable and computationally efficient implementation of our\nmethod, using best practices from numerical linear algebra and Riemannian\noptimization. Experimental results on LLM and diffusion model architectures\ndemonstrate that RiemannLoRA consistently improves both convergence speed and\nfinal performance over standard LoRA and its state-of-the-art modifications.", "published": "2025-07-16 11:17:12", "link": "http://arxiv.org/abs/2507.12142v1", "categories": ["cs.LG", "cs.CL", "cs.NA", "math.DG", "math.NA", "68T07, 65F55, 53Z50"], "primary_category": "cs.LG"}
{"title": "Iterative Augmentation with Summarization Refinement (IASR) Evaluation for Unstructured Survey data Modeling and Analysis", "abstract": "Text data augmentation is a widely used strategy for mitigating data sparsity\nin natural language processing (NLP), particularly in low-resource settings\nwhere limited samples hinder effective semantic modeling. While augmentation\ncan improve input diversity and downstream interpretability, existing\ntechniques often lack mechanisms to ensure semantic preservation during\nlarge-scale or iterative generation, leading to redundancy and instability.\nThis work introduces a principled evaluation framework for large language model\n(LLM) based text augmentation, comprising two components: (1) Scalability\nAnalysis, which measures semantic consistency as augmentation volume increases,\nand (2) Iterative Augmentation with Summarization Refinement (IASR), which\nevaluates semantic drift across recursive paraphrasing cycles. Empirical\nevaluations across state-of-the-art LLMs show that GPT-3.5 Turbo achieved the\nbest balance of semantic fidelity, diversity, and generation efficiency.\nApplied to a real-world topic modeling task using BERTopic with GPT-enhanced\nfew-shot labeling, the proposed approach results in a 400% increase in topic\ngranularity and complete elimination of topic overlaps. These findings\nvalidated the utility of the proposed frameworks for structured evaluation of\nLLM-based augmentation in practical NLP pipelines.", "published": "2025-07-16 10:49:30", "link": "http://arxiv.org/abs/2507.12126v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Findings of MEGA: Maths Explanation with LLMs using the Socratic Method for Active Learning", "abstract": "This paper presents an intervention study on the effects of the combined\nmethods of (1) the Socratic method, (2) Chain of Thought (CoT) reasoning, (3)\nsimplified gamification and (4) formative feedback on university students'\nMaths learning driven by large language models (LLMs). We call our approach\nMathematics Explanations through Games by AI LLMs (MEGA). Some students\nstruggle with Maths and as a result avoid Math-related discipline or subjects\ndespite the importance of Maths across many fields, including signal\nprocessing. Oftentimes, students' Maths difficulties stem from suboptimal\npedagogy. We compared the MEGA method to the traditional step-by-step (CoT)\nmethod to ascertain which is better by using a within-group design after\nrandomly assigning questions for the participants, who are university students.\nSamples (n=60) were randomly drawn from each of the two test sets of the Grade\nSchool Math 8K (GSM8K) and Mathematics Aptitude Test of Heuristics (MATH)\ndatasets, based on the error margin of 11%, the confidence level of 90%, and a\nmanageable number of samples for the student evaluators. These samples were\nused to evaluate two capable LLMs at length (Generative Pretrained Transformer\n4o (GPT4o) and Claude 3.5 Sonnet) out of the initial six that were tested for\ncapability. The results showed that students agree in more instances that the\nMEGA method is experienced as better for learning for both datasets. It is even\nmuch better than the CoT (47.5% compared to 26.67%) in the more difficult MATH\ndataset, indicating that MEGA is better at explaining difficult Maths problems.", "published": "2025-07-16 09:39:56", "link": "http://arxiv.org/abs/2507.12079v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BOOKCOREF: Coreference Resolution at Book Scale", "abstract": "Coreference Resolution systems are typically evaluated on benchmarks\ncontaining small- to medium-scale documents. When it comes to evaluating long\ntexts, however, existing benchmarks, such as LitBank, remain limited in length\nand do not adequately assess system capabilities at the book scale, i.e., when\nco-referring mentions span hundreds of thousands of tokens. To fill this gap,\nwe first put forward a novel automatic pipeline that produces high-quality\nCoreference Resolution annotations on full narrative texts. Then, we adopt this\npipeline to create the first book-scale coreference benchmark, BOOKCOREF, with\nan average document length of more than 200,000 tokens. We carry out a series\nof experiments showing the robustness of our automatic procedure and\ndemonstrating the value of our resource, which enables current long-document\ncoreference systems to gain up to +20 CoNLL-F1 points when evaluated on full\nbooks. Moreover, we report on the new challenges introduced by this\nunprecedented book-scale setting, highlighting that current models fail to\ndeliver the same performance they achieve on smaller documents. We release our\ndata and code to encourage research and development of new book-scale\nCoreference Resolution systems at https://github.com/sapienzanlp/bookcoref.", "published": "2025-07-16 09:35:38", "link": "http://arxiv.org/abs/2507.12075v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "StylOch at PAN: Gradient-Boosted Trees with Frequency-Based Stylometric Features", "abstract": "This submission to the binary AI detection task is based on a modular\nstylometric pipeline, where: public spaCy models are used for text\npreprocessing (including tokenisation, named entity recognition, dependency\nparsing, part-of-speech tagging, and morphology annotation) and extracting\nseveral thousand features (frequencies of n-grams of the above linguistic\nannotations); light-gradient boosting machines are used as the classifier. We\ncollect a large corpus of more than 500 000 machine-generated texts for the\nclassifier's training. We explore several parameter options to increase the\nclassifier's capacity and take advantage of that training set. Our approach\nfollows the non-neural, computationally inexpensive but explainable approach\nfound effective previously.", "published": "2025-07-16 09:21:20", "link": "http://arxiv.org/abs/2507.12064v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating the Ability of Large Language Models to Reason about Cardinal Directions, Revisited", "abstract": "We investigate the abilities of 28 Large language Models (LLMs) to reason\nabout cardinal directions (CDs) using a benchmark generated from a set of\ntemplates, extensively testing an LLM's ability to determine the correct CD\ngiven a particular scenario. The templates allow for a number of degrees of\nvariation such as means of locomotion of the agent involved, and whether set in\nthe first, second or third person. Even the newer Large Reasoning Models are\nunable to reliably determine the correct CD for all questions. This paper\nsummarises and extends earlier work presented at COSIT-24.", "published": "2025-07-16 09:16:36", "link": "http://arxiv.org/abs/2507.12059v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comparative Approach to Assessing Linguistic Creativity of Large Language Models and Humans", "abstract": "The following paper introduces a general linguistic creativity test for\nhumans and Large Language Models (LLMs). The test consists of various tasks\naimed at assessing their ability to generate new original words and phrases\nbased on word formation processes (derivation and compounding) and on\nmetaphorical language use. We administered the test to 24 humans and to an\nequal number of LLMs, and we automatically evaluated their answers using OCSAI\ntool for three criteria: Originality, Elaboration, and Flexibility. The results\nshow that LLMs not only outperformed humans in all the assessed criteria, but\ndid better in six out of the eight test tasks. We then computed the uniqueness\nof the individual answers, which showed some minor differences between humans\nand LLMs. Finally, we performed a short manual analysis of the dataset, which\nrevealed that humans are more inclined towards E(extending)-creativity, while\nLLMs favor F(ixed)-creativity.", "published": "2025-07-16 08:56:19", "link": "http://arxiv.org/abs/2507.12039v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Data and Parameter Efficiency of Neural Language Models Using Representation Analysis", "abstract": "This thesis addresses challenges related to data and parameter efficiency in\nneural language models, with a focus on representation analysis and the\nintroduction of new optimization techniques. The first part examines the\nproperties and dynamics of language representations within neural models,\nemphasizing their significance in enhancing robustness and generalization. It\nproposes innovative approaches based on representation smoothness, including\nregularization strategies that utilize Jacobian and Hessian matrices to\nstabilize training and mitigate sensitivity to input perturbations. The second\npart focuses on methods to significantly enhance data and parameter efficiency\nby integrating active learning strategies with parameter-efficient fine-tuning,\nguided by insights from representation smoothness analysis. It presents\nsmoothness-informed early-stopping techniques designed to eliminate the need\nfor labeled validation sets and proposes innovative combinations of active\nlearning and parameter-efficient fine-tuning to reduce labeling efforts and\ncomputational resources. Extensive experimental evaluations across various NLP\ntasks demonstrate that these combined approaches substantially outperform\ntraditional methods in terms of performance, stability, and efficiency. The\nthird part explores weak supervision techniques enhanced by in-context learning\nto effectively utilize unlabeled data, further reducing dependence on extensive\nlabeling. It shows that using in-context learning as a mechanism for weak\nsupervision enables models to better generalize from limited labeled data by\nleveraging unlabeled examples more effectively during training. Comprehensive\nempirical evaluations confirm significant gains in model accuracy,\nadaptability, and robustness, especially in low-resource settings and dynamic\ndata environments.", "published": "2025-07-16 07:58:20", "link": "http://arxiv.org/abs/2507.12004v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simplifications are Absolutists: How Simplified Language Reduces Word Sense Awareness in LLM-Generated Definitions", "abstract": "Large Language Models (LLMs) can provide accurate word definitions and\nexplanations for any context. However, the scope of the definition changes for\ndifferent target groups, like children or language learners. This is especially\nrelevant for homonyms, words with multiple meanings, where oversimplification\nmight risk information loss by omitting key senses, potentially misleading\nusers who trust LLM outputs. We investigate how simplification impacts homonym\ndefinition quality across three target groups: Normal, Simple, and ELI5. Using\ntwo novel evaluation datasets spanning multiple languages, we test DeepSeek v3,\nLlama 4 Maverick, Qwen3-30B A3B, GPT-4o mini, and Llama 3.1 8B via LLM-as-Judge\nand human annotations. Our results show that simplification drastically\ndegrades definition completeness by neglecting polysemy, increasing the risk of\nmisunderstanding. Fine-tuning Llama 3.1 8B with Direct Preference Optimization\nsubstantially improves homonym response quality across all prompt types. These\nfindings highlight the need to balance simplicity and completeness in\neducational NLP to ensure reliable, context-aware definitions for all learners.", "published": "2025-07-16 07:25:27", "link": "http://arxiv.org/abs/2507.11981v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Value-Based Large Language Model Agent Simulation for Mutual Evaluation of Trust and Interpersonal Closeness", "abstract": "Large language models (LLMs) have emerged as powerful tools for simulating\ncomplex social phenomena using human-like agents with specific traits. In human\nsocieties, value similarity is important for building trust and close\nrelationships; however, it remains unexplored whether this principle holds true\nin artificial societies comprising LLM agents. Therefore, this study\ninvestigates the influence of value similarity on relationship-building among\nLLM agents through two experiments. First, in a preliminary experiment, we\nevaluated the controllability of values in LLMs to identify the most effective\nmodel and prompt design for controlling the values. Subsequently, in the main\nexperiment, we generated pairs of LLM agents imbued with specific values and\nanalyzed their mutual evaluations of trust and interpersonal closeness\nfollowing a dialogue. The experiments were conducted in English and Japanese to\ninvestigate language dependence. The results confirmed that pairs of agents\nwith higher value similarity exhibited greater mutual trust and interpersonal\ncloseness. Our findings demonstrate that the LLM agent simulation serves as a\nvalid testbed for social science theories, contributes to elucidating the\nmechanisms by which values influence relationship building, and provides a\nfoundation for inspiring new theories and insights into the social sciences.", "published": "2025-07-16 07:21:59", "link": "http://arxiv.org/abs/2507.11979v1", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Graph Representations for Reading Comprehension Analysis using Large Language Model and Eye-Tracking Biomarker", "abstract": "Reading comprehension is a fundamental skill in human cognitive development.\nWith the advancement of Large Language Models (LLMs), there is a growing need\nto compare how humans and LLMs understand language across different contexts\nand apply this understanding to functional tasks such as inference, emotion\ninterpretation, and information retrieval. Our previous work used LLMs and\nhuman biomarkers to study the reading comprehension process. The results showed\nthat the biomarkers corresponding to words with high and low relevance to the\ninference target, as labeled by the LLMs, exhibited distinct patterns,\nparticularly when validated using eye-tracking data. However, focusing solely\non individual words limited the depth of understanding, which made the\nconclusions somewhat simplistic despite their potential significance. This\nstudy used an LLM-based AI agent to group words from a reading passage into\nnodes and edges, forming a graph-based text representation based on semantic\nmeaning and question-oriented prompts. We then compare the distribution of eye\nfixations on important nodes and edges. Our findings indicate that LLMs exhibit\nhigh consistency in language understanding at the level of graph topological\nstructure. These results build on our previous findings and offer insights into\neffective human-AI co-learning strategies.", "published": "2025-07-16 07:15:59", "link": "http://arxiv.org/abs/2507.11972v1", "categories": ["cs.CL", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation", "abstract": "As online communication increasingly incorporates under-represented languages\nand colloquial dialects, standard translation systems often fail to preserve\nlocal slang, code-mixing, and culturally embedded markers of harmful speech.\nTranslating toxic content between low-resource language pairs poses additional\nchallenges due to scarce parallel data and safety filters that sanitize\noffensive expressions. In this work, we propose a reproducible, two-stage\nframework for toxicity-preserving translation, demonstrated on a code-mixed\nSinglish safety corpus. First, we perform human-verified few-shot prompt\nengineering: we iteratively curate and rank annotator-selected Singlish-target\nexamples to capture nuanced slang, tone, and toxicity. Second, we optimize\nmodel-prompt pairs by benchmarking several large language models using semantic\nsimilarity via direct and back-translation. Quantitative human evaluation\nconfirms the effectiveness and efficiency of our pipeline. Beyond improving\ntranslation quality, our framework contributes to the safety of multicultural\nLLMs by supporting culturally sensitive moderation and benchmarking in\nlow-resource contexts. By positioning Singlish as a testbed for inclusive NLP,\nwe underscore the importance of preserving sociolinguistic nuance in real-world\napplications such as content moderation and regional platform governance.", "published": "2025-07-16 06:58:02", "link": "http://arxiv.org/abs/2507.11966v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "PoTPTQ: A Two-step Power-of-Two Post-training for LLMs", "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious natural language processing (NLP) tasks. However, their deployment is\nchallenging due to the substantial computational resources required.\nPower-of-two (PoT) quantization is a general tool to counteract this\ndifficulty. Albeit previous works on PoT quantization can be efficiently\ndequantized on CPUs using fixed-point addition, it showed less effectiveness on\nGPUs. The reason is entanglement of the sign bit and sequential bit\nmanipulations needed for dequantization. We propose a novel POT quantization\nframework for LLM weights that (i) outperforms state-of-the-art accuracy in\nextremely low-precision number formats, and (ii) enables faster inference\nthrough more efficient dequantization. To maintain the accuracy of the\nquantized model, we introduce a two-step post-training algorithm: (i)\ninitialize the quantization scales with a robust starting point, and (ii)\nrefine these scales using a minimal calibration set. The performance of our PoT\npost-training algorithm surpasses the current state-of-the-art in integer\nquantization, particularly at low precisions such as 2- and 3-bit formats. Our\nPoT quantization accelerates the dequantization step required for the floating\npoint inference and leads to $3.67\\times$ speed up on a NVIDIA V100, and\n$1.63\\times$ on a NVIDIA RTX 4090, compared to uniform integer dequantization.", "published": "2025-07-16 06:44:14", "link": "http://arxiv.org/abs/2507.11959v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The benefits of query-based KGQA systems for complex and temporal questions in LLM era", "abstract": "Large language models excel in question-answering (QA) yet still struggle\nwith multi-hop reasoning and temporal questions. Query-based knowledge graph QA\n(KGQA) offers a modular alternative by generating executable queries instead of\ndirect answers. We explore multi-stage query-based framework for WikiData QA,\nproposing multi-stage approach that enhances performance on challenging\nmulti-hop and temporal benchmarks. Through generalization and rejection\nstudies, we evaluate robustness across multi-hop and temporal QA datasets.\nAdditionally, we introduce a novel entity linking and predicate matching method\nusing CoT reasoning. Our results demonstrate the potential of query-based\nmulti-stage KGQA framework for improving multi-hop and temporal QA with small\nlanguage models. Code and data: https://github.com/ar2max/NLDB-KGQA-System", "published": "2025-07-16 06:41:03", "link": "http://arxiv.org/abs/2507.11954v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "IAM: Efficient Inference through Attention Mapping between Different-scale LLMs", "abstract": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency.", "published": "2025-07-16 06:39:11", "link": "http://arxiv.org/abs/2507.11953v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DAC: A Dynamic Attention-aware Approach for Task-Agnostic Prompt Compression", "abstract": "Task-agnostic prompt compression leverages the redundancy in natural language\nto reduce computational overhead and enhance information density within\nprompts, especially in long-context scenarios. Existing methods predominantly\nrely on information entropy as the metric to compress lexical units, aiming to\nachieve minimal information loss. However, these approaches overlook two\ncritical aspects: (i) the importance of attention-critical tokens at the\nalgorithmic level, and (ii) shifts in information entropy during the\ncompression process. Motivated by these challenges, we propose a dynamic\nattention-aware approach for task-agnostic prompt compression (DAC). This\napproach effectively integrates entropy and attention information, dynamically\nsensing entropy shifts during compression to achieve fine-grained prompt\ncompression. Extensive experiments across various domains, including LongBench,\nGSM8K, and BBH, show that DAC consistently yields robust and substantial\nimprovements across a diverse range of tasks and LLMs, offering compelling\nevidence of its efficacy.", "published": "2025-07-16 06:16:06", "link": "http://arxiv.org/abs/2507.11942v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BlockBPE: Parallel BPE Tokenization", "abstract": "Tokenization is a critical preprocessing step in large language model\npipelines, yet widely-used implementations remain CPU-bound and suboptimal for\nbatch inference workflows on GPU. We present BlockBPE, a parallel GPU\nimplementation of byte-pair encoding (BPE) that achieves near linear-time\ncomplexity under realistic assumptions and is optimized for high-throughput,\nbatch inference. Unlike existing Rust-based tokenizers such as HuggingFace\nTokenizers or OpenAI's tiktoken-whose runtimes are dominated by Regex\npre-tokenization and exhibit $O(n \\log n)$ runtime-BlockBPE eliminates the\nRegex pre-tokenization which leads to small loss in generation quality, but\nenables highly parallelized token merges within thread blocks, reducing overall\ncomplexity to $O(nd)$ where $d \\ll n$. On high-batch inference workloads,\nBlockBPE achieves up to 2x higher throughput than tiktoken and 2.5x over\nHuggingFace Tokenizers.", "published": "2025-07-16 06:12:41", "link": "http://arxiv.org/abs/2507.11941v1", "categories": ["cs.CL", "cs.DC"], "primary_category": "cs.CL"}
{"title": "POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering", "abstract": "Charts are a universally adopted medium for interpreting and communicating\ndata. However, existing chart understanding benchmarks are predominantly\nEnglish-centric, limiting their accessibility and applicability to global\naudiences. In this paper, we present PolyChartQA, the first large-scale\nmultilingual chart question answering benchmark covering 22,606 charts and\n26,151 question-answering pairs across 10 diverse languages. PolyChartQA is\nbuilt using a decoupled pipeline that separates chart data from rendering code,\nallowing multilingual charts to be flexibly generated by simply translating the\ndata and reusing the code. We leverage state-of-the-art LLM-based translation\nand enforce rigorous quality control in the pipeline to ensure the linguistic\nand semantic consistency of the generated multilingual charts. PolyChartQA\nfacilitates systematic evaluation of multilingual chart understanding.\nExperiments on both open- and closed-source large vision-language models reveal\na significant performance gap between English and other languages, especially\nlow-resource ones with non-Latin scripts. This benchmark lays a foundation for\nadvancing globally inclusive vision-language models.", "published": "2025-07-16 06:09:02", "link": "http://arxiv.org/abs/2507.11939v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MM"], "primary_category": "cs.CL"}
{"title": "A Survey of Deep Learning for Geometry Problem Solving", "abstract": "Geometry problem solving is a key area of mathematical reasoning, which is\nwidely involved in many important fields such as education, mathematical\nability assessment of artificial intelligence, and multimodal ability\nassessment. In recent years, the rapid development of deep learning technology,\nespecially the rise of multimodal large language models, has triggered a\nwidespread research boom. This paper provides a survey of the applications of\ndeep learning in geometry problem solving, including (i) a comprehensive\nsummary of the relevant tasks in geometry problem solving; (ii) a thorough\nreview of related deep learning methods; (iii) a detailed analysis of\nevaluation metrics and methods; and (iv) a critical discussion of the current\nchallenges and future directions that can be explored. Our goal is to provide a\ncomprehensive and practical reference of deep learning for geometry problem\nsolving to promote further developments in this field. We create a continuously\nupdated list of papers on GitHub: https://github.com/majianz/dl4gps.", "published": "2025-07-16 06:03:08", "link": "http://arxiv.org/abs/2507.11936v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Marco-Bench-MIF: On Multilingual Instruction-Following Capability of Large Language Models", "abstract": "Instruction-following capability has become a major ability to be evaluated\nfor Large Language Models (LLMs). However, existing datasets, such as IFEval,\nare either predominantly monolingual and centered on English or simply machine\ntranslated to other languages, limiting their applicability in multilingual\ncontexts. In this paper, we present an carefully-curated extension of IFEval to\na localized multilingual version named Marco-Bench-MIF, covering 30 languages\nwith varying levels of localization. Our benchmark addresses linguistic\nconstraints (e.g., modifying capitalization requirements for Chinese) and\ncultural references (e.g., substituting region-specific company names in\nprompts) via a hybrid pipeline combining translation with verification. Through\ncomprehensive evaluation of 20+ LLMs on our Marco-Bench-MIF, we found that: (1)\n25-35% accuracy gap between high/low-resource languages, (2) model scales\nlargely impact performance by 45-60% yet persists script-specific challenges,\nand (3) machine-translated data underestimates accuracy by7-22% versus\nlocalized data. Our analysis identifies challenges in multilingual instruction\nfollowing, including keyword consistency preservation and compositional\nconstraint adherence across languages. Our Marco-Bench-MIF is available at\nhttps://github.com/AIDC-AI/Marco-Bench-MIF.", "published": "2025-07-16 03:49:41", "link": "http://arxiv.org/abs/2507.11882v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMs Encode Harmfulness and Refusal Separately", "abstract": "LLMs are trained to refuse harmful instructions, but do they truly understand\nharmfulness beyond just refusing? Prior work has shown that LLMs' refusal\nbehaviors can be mediated by a one-dimensional subspace, i.e., a refusal\ndirection. In this work, we identify a new dimension to analyze safety\nmechanisms in LLMs, i.e., harmfulness, which is encoded internally as a\nseparate concept from refusal. There exists a harmfulness direction that is\ndistinct from the refusal direction. As causal evidence, steering along the\nharmfulness direction can lead LLMs to interpret harmless instructions as\nharmful, but steering along the refusal direction tends to elicit refusal\nresponses directly without reversing the model's judgment on harmfulness.\nFurthermore, using our identified harmfulness concept, we find that certain\njailbreak methods work by reducing the refusal signals without reversing the\nmodel's internal belief of harmfulness. We also find that adversarially\nfinetuning models to accept harmful instructions has minimal impact on the\nmodel's internal belief of harmfulness. These insights lead to a practical\nsafety application: The model's latent harmfulness representation can serve as\nan intrinsic safeguard (Latent Guard) for detecting unsafe inputs and reducing\nover-refusals that is robust to finetuning attacks. For instance, our Latent\nGuard achieves performance comparable to or better than Llama Guard 3 8B, a\ndedicated finetuned safeguard model, across different jailbreak methods. Our\nfindings suggest that LLMs' internal understanding of harmfulness is more\nrobust than their refusal decision to diverse input instructions, offering a\nnew perspective to study AI safety", "published": "2025-07-16 03:48:03", "link": "http://arxiv.org/abs/2507.11878v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DualReward: A Dynamic Reinforcement Learning Framework for Cloze Tests Distractor Generation", "abstract": "This paper introduces DualReward, a novel reinforcement learning framework\nfor automatic distractor generation in cloze tests. Unlike conventional\napproaches that rely primarily on supervised learning or static generative\nmodels, our method employs a dual reward structure with adaptive scaling that\ndifferentiates between human-created gold standard distractors and\nmodel-generated candidates. The framework dynamically adjusts reward signal\nintensity based on model performance and confidence. We evaluate our approach\non both passage-level (CLOTH-F) and sentence-level (MCQ) cloze test datasets,\ndemonstrating consistent improvements over state-of-the-art baselines.\nExperimental results show that our adaptive reward scaling mechanism provides\nmodest but consistent benefits on homogeneous datasets (CLOTH-F) and more\nsubstantial improvements (3.48-3.86% in P@1) on diverse, cross-domain data\n(MCQ), suggesting its particular effectiveness for handling varied question\ntypes and domains. Our work offers a flexible framework that effectively\nbalances learning from reliable human examples while exploring novel,\nhigh-quality distractors for automated test generation.", "published": "2025-07-16 03:39:36", "link": "http://arxiv.org/abs/2507.11875v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COLA-GEC: A Bidirectional Framework for Enhancing Grammatical Acceptability and Error Correction", "abstract": "Grammatical Error Correction (GEC) and grammatical acceptability judgment\n(COLA) are core tasks in natural language processing, sharing foundational\ngrammatical knowledge yet typically evolving independently. This paper\nintroduces COLA-GEC, a novel bidirectional framework that enhances both tasks\nthrough mutual knowledge transfer. First, we augment grammatical acceptability\nmodels using GEC datasets, significantly improving their performance across\nmultiple languages. Second, we integrate grammatical acceptability signals into\nGEC model training via a dynamic loss function, effectively guiding corrections\ntoward grammatically acceptable outputs. Our approach achieves state-of-the-art\nresults on several multilingual benchmarks. Comprehensive error analysis\nhighlights remaining challenges, particularly in punctuation error correction,\nproviding insights for future improvements in grammatical modeling.", "published": "2025-07-16 03:29:05", "link": "http://arxiv.org/abs/2507.11867v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Domain Transfer and Few-Shot Learning for Personal Identifiable Information Recognition", "abstract": "Accurate recognition of personally identifiable information (PII) is central\nto automated text anonymization. This paper investigates the effectiveness of\ncross-domain model transfer, multi-domain data fusion, and sample-efficient\nlearning for PII recognition. Using annotated corpora from healthcare (I2B2),\nlegal (TAB), and biography (Wikipedia), we evaluate models across four\ndimensions: in-domain performance, cross-domain transferability, fusion, and\nfew-shot learning. Results show legal-domain data transfers well to\nbiographical texts, while medical domains resist incoming transfer. Fusion\nbenefits are domain-specific, and high-quality recognition is achievable with\nonly 10% of training data in low-specialization domains.", "published": "2025-07-16 03:14:36", "link": "http://arxiv.org/abs/2507.11862v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Your LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential", "abstract": "Autoregressive language models are constrained by their inherently sequential\nnature, generating one token at a time. This paradigm limits inference speed\nand parallelism, especially during later stages of generation when the\ndirection and semantics of text are relatively certain. In this work, we\npropose a novel framework that leverages the inherent knowledge of vanilla\nautoregressive language models about future tokens, combining techniques to\nrealize this potential and enable simultaneous prediction of multiple\nsubsequent tokens. Our approach introduces several key innovations: (1) a\nmasked-input formulation where multiple future tokens are jointly predicted\nfrom a common prefix; (2) a gated LoRA formulation that preserves the original\nLLM's functionality, while equipping it for multi-token prediction; (3) a\nlightweight, learnable sampler module that generates coherent sequences from\nthe predicted future tokens; (4) a set of auxiliary training losses, including\na consistency loss, to enhance the coherence and accuracy of jointly generated\ntokens; and (5) a speculative generation strategy that expands tokens\nquadratically in the future while maintaining high fidelity. Our method\nachieves significant speedups through supervised fine-tuning on pretrained\nmodels. For example, it generates code and math nearly 5x faster, and improves\ngeneral chat and knowledge tasks by almost 2.5x. These gains come without any\nloss in quality.", "published": "2025-07-16 02:31:40", "link": "http://arxiv.org/abs/2507.11851v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ILID: Native Script Language Identification for Indian Languages", "abstract": "The language identification task is a crucial fundamental step in NLP. Often\nit serves as a pre-processing step for widely used NLP applications such as\nmultilingual machine translation, information retrieval, question and\nanswering, and text summarization. The core challenge of language\nidentification lies in distinguishing languages in noisy, short, and code-mixed\nenvironments. This becomes even harder in case of diverse Indian languages that\nexhibit lexical and phonetic similarities, but have distinct differences. Many\nIndian languages share the same script making the task even more challenging.\nIn this paper, we release a dataset of 230K sentences consisting of English and\nall 22 official Indian languages labeled with their language identifiers where\ndata in most languages are newly created. We also develop and release robust\nbaseline models using state-of-the-art approaches in machine learning and deep\nlearning that can aid the research in this field. Our baseline models are\ncomparable to the state-of-the-art models for the language identification task.", "published": "2025-07-16 01:39:32", "link": "http://arxiv.org/abs/2507.11832v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tracing Facts or just Copies? A critical investigation of the Competitions of Mechanisms in Large Language Models", "abstract": "This paper presents a reproducibility study examining how Large Language\nModels (LLMs) manage competing factual and counterfactual information, focusing\non the role of attention heads in this process. We attempt to reproduce and\nreconcile findings from three recent studies by Ortu et al., Yu, Merullo, and\nPavlick and McDougall et al. that investigate the competition between\nmodel-learned facts and contradictory context information through Mechanistic\nInterpretability tools. Our study specifically examines the relationship\nbetween attention head strength and factual output ratios, evaluates competing\nhypotheses about attention heads' suppression mechanisms, and investigates the\ndomain specificity of these attention patterns. Our findings suggest that\nattention heads promoting factual output do so via general copy suppression\nrather than selective counterfactual suppression, as strengthening them can\nalso inhibit correct facts. Additionally, we show that attention head behavior\nis domain-dependent, with larger models exhibiting more specialized and\ncategory-sensitive patterns.", "published": "2025-07-16 00:08:48", "link": "http://arxiv.org/abs/2507.11809v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Interpreting Radiologist's Intention from Eye Movements in Chest X-ray Diagnosis", "abstract": "Radiologists rely on eye movements to navigate and interpret medical images.\nA trained radiologist possesses knowledge about the potential diseases that may\nbe present in the images and, when searching, follows a mental checklist to\nlocate them using their gaze. This is a key observation, yet existing models\nfail to capture the underlying intent behind each fixation. In this paper, we\nintroduce a deep learning-based approach, RadGazeIntent, designed to model this\nbehavior: having an intention to find something and actively searching for it.\nOur transformer-based architecture processes both the temporal and spatial\ndimensions of gaze data, transforming fine-grained fixation features into\ncoarse, meaningful representations of diagnostic intent to interpret\nradiologists' goals. To capture the nuances of radiologists' varied\nintention-driven behaviors, we process existing medical eye-tracking datasets\nto create three intention-labeled subsets: RadSeq (Systematic Sequential\nSearch), RadExplore (Uncertainty-driven Exploration), and RadHybrid (Hybrid\nPattern). Experimental results demonstrate RadGazeIntent's ability to predict\nwhich findings radiologists are examining at specific moments, outperforming\nbaseline methods across all intention-labeled datasets.", "published": "2025-07-16 17:58:35", "link": "http://arxiv.org/abs/2507.12461v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "LLM-Based Config Synthesis requires Disambiguation", "abstract": "Beyond hallucinations, another problem in program synthesis using LLMs is\nambiguity in user intent. We illustrate the ambiguity problem in a networking\ncontext for LLM-based incremental configuration synthesis of route-maps and\nACLs. These structures frequently overlap in header space, making the relative\npriority of actions impossible for the LLM to infer without user interaction.\nMeasurements in a large cloud identify complex ACLs with 100's of overlaps,\nshowing ambiguity is a real problem. We propose a prototype system, Clarify,\nwhich uses an LLM augmented with a new module called a Disambiguator that helps\nelicit user intent. On a small synthetic workload, Clarify incrementally\nsynthesizes routing policies after disambiguation and then verifies them. Our\ntreatment of ambiguities is useful more generally when the intent of updates\ncan be correctly synthesized by LLMs, but their integration is ambiguous and\ncan lead to different global behaviors.", "published": "2025-07-16 17:29:15", "link": "http://arxiv.org/abs/2507.12443v1", "categories": ["cs.NI", "cs.AI", "cs.HC", "cs.PL"], "primary_category": "cs.NI"}
{"title": "Characterizing State Space Model (SSM) and SSM-Transformer Hybrid Language Model Performance with Long Context Length", "abstract": "The demand for machine intelligence capable of processing continuous,\nlong-context inputs on local devices is growing rapidly. However, the quadratic\ncomplexity and memory requirements of traditional Transformer architectures\nmake them inefficient and often unusable for these tasks. This has spurred a\nparadigm shift towards new architectures like State Space Models (SSMs) and\nhybrids, which promise near-linear scaling. While most current research focuses\non the accuracy and theoretical throughput of these models, a systematic\nperformance characterization on practical consumer hardware is critically\nneeded to guide system-level optimization and unlock new applications.\n  To address this gap, we present a comprehensive, comparative benchmarking of\ncarefully selected Transformer, SSM, and hybrid models specifically for\nlong-context inference on consumer and embedded GPUs. Our analysis reveals that\nSSMs are not only viable but superior for this domain, capable of processing\nsequences up to 220K tokens on a 24GB consumer GPU-approximately 4x longer than\ncomparable Transformers. While Transformers may be up to 1.8x faster at short\nsequences, SSMs demonstrate a dramatic performance inversion, becoming up to 4x\nfaster at very long contexts (~57K tokens). Our operator-level analysis reveals\nthat custom, hardware-aware SSM kernels dominate the inference runtime,\naccounting for over 55% of latency on edge platforms, identifying them as a\nprimary target for future hardware acceleration. We also provide detailed,\ndevice-specific characterization results to guide system co-design for the\nedge. To foster further research, we will open-source our characterization\nframework.", "published": "2025-07-16 17:28:40", "link": "http://arxiv.org/abs/2507.12442v1", "categories": ["cs.AR", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "primary_category": "cs.AR"}
{"title": "EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos", "abstract": "Real robot data collection for imitation learning has led to significant\nadvancements in robotic manipulation. However, the requirement for robot\nhardware in the process fundamentally constrains the scale of the data. In this\npaper, we explore training Vision-Language-Action (VLA) models using egocentric\nhuman videos. The benefit of using human videos is not only for their scale but\nmore importantly for the richness of scenes and tasks. With a VLA trained on\nhuman video that predicts human wrist and hand actions, we can perform Inverse\nKinematics and retargeting to convert the human actions to robot actions. We\nfine-tune the model using a few robot manipulation demonstrations to obtain the\nrobot policy, namely EgoVLA. We propose a simulation benchmark called Isaac\nHumanoid Manipulation Benchmark, where we design diverse bimanual manipulation\ntasks with demonstrations. We fine-tune and evaluate EgoVLA with Isaac Humanoid\nManipulation Benchmark and show significant improvements over baselines and\nablate the importance of human data. Videos can be found on our website:\nhttps://rchalyang.github.io/EgoVLA", "published": "2025-07-16 17:27:44", "link": "http://arxiv.org/abs/2507.12440v1", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Unit-Based Histopathology Tissue Segmentation via Multi-Level Feature Representation", "abstract": "We propose UTS, a unit-based tissue segmentation framework for histopathology\nthat classifies each fixed-size 32 * 32 tile, rather than each pixel, as the\nsegmentation unit. This approach reduces annotation effort and improves\ncomputational efficiency without compromising accuracy. To implement this\napproach, we introduce a Multi-Level Vision Transformer (L-ViT), which benefits\nthe multi-level feature representation to capture both fine-grained morphology\nand global tissue context. Trained to segment breast tissue into three\ncategories (infiltrating tumor, non-neoplastic stroma, and fat), UTS supports\nclinically relevant tasks such as tumor-stroma quantification and surgical\nmargin assessment. Evaluated on 386,371 tiles from 459 H&E-stained regions, it\noutperforms U-Net variants and transformer-based baselines. Code and Dataset\nwill be available at GitHub.", "published": "2025-07-16 17:15:18", "link": "http://arxiv.org/abs/2507.12427v1", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "eess.IV"}
{"title": "Mixture of Raytraced Experts", "abstract": "We introduce a Mixture of Raytraced Experts, a stacked Mixture of Experts\n(MoE) architecture which can dynamically select sequences of experts, producing\ncomputational graphs of variable width and depth. Existing MoE architectures\ngenerally require a fixed amount of computation for a given sample. Our\napproach, in contrast, yields predictions with increasing accuracy as the\ncomputation cycles through the experts' sequence. We train our model by\niteratively sampling from a set of candidate experts, unfolding the sequence\nakin to how Recurrent Neural Networks are trained. Our method does not require\nload-balancing mechanisms, and preliminary experiments show a reduction in\ntraining epochs of 10\\% to 40\\% with a comparable/higher accuracy. These\nresults point to new research directions in the field of MoEs, allowing the\ndesign of potentially faster and more expressive models. The code is available\nat https://github.com/nutig/RayTracing", "published": "2025-07-16 17:08:46", "link": "http://arxiv.org/abs/2507.12419v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "QuRe: Query-Relevant Retrieval through Hard Negative Sampling in Composed Image Retrieval", "abstract": "Composed Image Retrieval (CIR) retrieves relevant images based on a reference\nimage and accompanying text describing desired modifications. However, existing\nCIR methods only focus on retrieving the target image and disregard the\nrelevance of other images. This limitation arises because most methods\nemploying contrastive learning-which treats the target image as positive and\nall other images in the batch as negatives-can inadvertently include false\nnegatives. This may result in retrieving irrelevant images, reducing user\nsatisfaction even when the target image is retrieved. To address this issue, we\npropose Query-Relevant Retrieval through Hard Negative Sampling (QuRe), which\noptimizes a reward model objective to reduce false negatives. Additionally, we\nintroduce a hard negative sampling strategy that selects images positioned\nbetween two steep drops in relevance scores following the target image, to\neffectively filter false negatives. In order to evaluate CIR models on their\nalignment with human satisfaction, we create Human-Preference FashionIQ\n(HP-FashionIQ), a new dataset that explicitly captures user preferences beyond\ntarget retrieval. Extensive experiments demonstrate that QuRe achieves\nstate-of-the-art performance on FashionIQ and CIRR datasets while exhibiting\nthe strongest alignment with human preferences on the HP-FashionIQ dataset. The\nsource code is available at https://github.com/jackwaky/QuRe.", "published": "2025-07-16 17:06:33", "link": "http://arxiv.org/abs/2507.12416v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "AutoVDC: Automated Vision Data Cleaning Using Vision-Language Models", "abstract": "Training of autonomous driving systems requires extensive datasets with\nprecise annotations to attain robust performance. Human annotations suffer from\nimperfections, and multiple iterations are often needed to produce high-quality\ndatasets. However, manually reviewing large datasets is laborious and\nexpensive. In this paper, we introduce AutoVDC (Automated Vision Data Cleaning)\nframework and investigate the utilization of Vision-Language Models (VLMs) to\nautomatically identify erroneous annotations in vision datasets, thereby\nenabling users to eliminate these errors and enhance data quality. We validate\nour approach using the KITTI and nuImages datasets, which contain object\ndetection benchmarks for autonomous driving. To test the effectiveness of\nAutoVDC, we create dataset variants with intentionally injected erroneous\nannotations and observe the error detection rate of our approach. Additionally,\nwe compare the detection rates using different VLMs and explore the impact of\nVLM fine-tuning on our pipeline. The results demonstrate our method's high\nperformance in error detection and data cleaning experiments, indicating its\npotential to significantly improve the reliability and accuracy of large-scale\nproduction datasets in autonomous driving.", "published": "2025-07-16 17:04:49", "link": "http://arxiv.org/abs/2507.12414v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "primary_category": "cs.CV"}
{"title": "NOCTA: Non-Greedy Objective Cost-Tradeoff Acquisition for Longitudinal Data", "abstract": "In many critical applications, resource constraints limit the amount of\ninformation that can be gathered to make predictions. For example, in\nhealthcare, patient data often spans diverse features ranging from lab tests to\nimaging studies. Each feature may carry different information and must be\nacquired at a respective cost of time, money, or risk to the patient. Moreover,\ntemporal prediction tasks, where both instance features and labels evolve over\ntime, introduce additional complexity in deciding when or what information is\nimportant. In this work, we propose NOCTA, a Non-Greedy Objective Cost-Tradeoff\nAcquisition method that sequentially acquires the most informative features at\ninference time while accounting for both temporal dynamics and acquisition\ncost. We first introduce a cohesive estimation target for our NOCTA setting,\nand then develop two complementary estimators: 1) a non-parametric method based\non nearest neighbors to guide the acquisition (NOCTA-NP), and 2) a parametric\nmethod that directly predicts the utility of potential acquisitions (NOCTA-P).\nExperiments on synthetic and real-world medical datasets demonstrate that both\nNOCTA variants outperform existing baselines.", "published": "2025-07-16 17:00:41", "link": "http://arxiv.org/abs/2507.12412v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "GitChameleon: Evaluating AI Code Generation Against Python Library Version Incompatibilities", "abstract": "The rapid evolution of software libraries poses a considerable hurdle for\ncode generation, necessitating continuous adaptation to frequent version\nupdates while preserving backward compatibility. While existing code evolution\nbenchmarks provide valuable insights, they typically lack execution-based\nevaluation for generating code compliant with specific library versions. To\naddress this, we introduce GitChameleon, a novel, meticulously curated dataset\ncomprising 328 Python code completion problems, each conditioned on specific\nlibrary versions and accompanied by executable unit tests. GitChameleon\nrigorously evaluates the capacity of contemporary large language models (LLMs),\nLLM-powered agents, code assistants, and RAG systems to perform\nversion-conditioned code generation that demonstrates functional accuracy\nthrough execution. Our extensive evaluations indicate that state-of-the-art\nsystems encounter significant challenges with this task; enterprise models\nachieving baseline success rates in the 48-51\\% range, underscoring the\nintricacy of the problem. By offering an execution-based benchmark emphasizing\nthe dynamic nature of code libraries, GitChameleon enables a clearer\nunderstanding of this challenge and helps guide the development of more\nadaptable and dependable AI code generation methods. We make the dataset and\nevaluation code publicly available at\nhttps://github.com/mrcabbage972/GitChameleonBenchmark.", "published": "2025-07-16 16:10:42", "link": "http://arxiv.org/abs/2507.12367v1", "categories": ["cs.SE", "cs.AI", "cs.PL"], "primary_category": "cs.SE"}
{"title": "FactorHD: A Hyperdimensional Computing Model for Multi-Object Multi-Class Representation and Factorization", "abstract": "Neuro-symbolic artificial intelligence (neuro-symbolic AI) excels in logical\nanalysis and reasoning. Hyperdimensional Computing (HDC), a promising\nbrain-inspired computational model, is integral to neuro-symbolic AI. Various\nHDC models have been proposed to represent class-instance and class-class\nrelations, but when representing the more complex class-subclass relation,\nwhere multiple objects associate different levels of classes and subclasses,\nthey face challenges for factorization, a crucial task for neuro-symbolic AI\nsystems. In this article, we propose FactorHD, a novel HDC model capable of\nrepresenting and factorizing the complex class-subclass relation efficiently.\nFactorHD features a symbolic encoding method that embeds an extra memorization\nclause, preserving more information for multiple objects. In addition, it\nemploys an efficient factorization algorithm that selectively eliminates\nredundant classes by identifying the memorization clause of the target class.\nSuch model significantly enhances computing efficiency and accuracy in\nrepresenting and factorizing multiple objects with class-subclass relation,\novercoming limitations of existing HDC models such as \"superposition\ncatastrophe\" and \"the problem of 2\". Evaluations show that FactorHD achieves\napproximately 5667x speedup at a representation size of 10^9 compared to\nexisting HDC models. When integrated with the ResNet-18 neural network,\nFactorHD achieves 92.48% factorization accuracy on the Cifar-10 dataset.", "published": "2025-07-16 16:09:51", "link": "http://arxiv.org/abs/2507.12366v1", "categories": ["cs.SC", "cs.AI", "cs.CV"], "primary_category": "cs.SC"}
{"title": "Cluster Contrast for Unsupervised Visual Representation Learning", "abstract": "We introduce Cluster Contrast (CueCo), a novel approach to unsupervised\nvisual representation learning that effectively combines the strengths of\ncontrastive learning and clustering methods. Inspired by recent advancements,\nCueCo is designed to simultaneously scatter and align feature representations\nwithin the feature space. This method utilizes two neural networks, a query and\na key, where the key network is updated through a slow-moving average of the\nquery outputs. CueCo employs a contrastive loss to push dissimilar features\napart, enhancing inter-class separation, and a clustering objective to pull\ntogether features of the same cluster, promoting intra-class compactness. Our\nmethod achieves 91.40% top-1 classification accuracy on CIFAR-10, 68.56% on\nCIFAR-100, and 78.65% on ImageNet-100 using linear evaluation with a ResNet-18\nbackbone. By integrating contrastive learning with clustering, CueCo sets a new\ndirection for advancing unsupervised visual representation learning.", "published": "2025-07-16 15:59:43", "link": "http://arxiv.org/abs/2507.12359v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Neural Polar Decoders for Deletion Channels", "abstract": "This paper introduces a neural polar decoder (NPD) for deletion channels with\na constant deletion rate. Existing polar decoders for deletion channels exhibit\nhigh computational complexity of $O(N^4)$, where $N$ is the block length. This\nlimits the application of polar codes for deletion channels to\nshort-to-moderate block lengths. In this work, we demonstrate that employing\nNPDs for deletion channels can reduce the computational complexity. First, we\nextend the architecture of the NPD to support deletion channels. Specifically,\nthe NPD architecture consists of four neural networks (NNs), each replicating\nfundamental successive cancellation (SC) decoder operations. To support\ndeletion channels, we change the architecture of only one. The computational\ncomplexity of the NPD is $O(AN\\log N)$, where the parameter $A$ represents a\ncomputational budget determined by the user and is independent of the channel.\nWe evaluate the new extended NPD for deletion channels with deletion rates\n$\\delta\\in\\{0.01, 0.1\\}$ and we verify the NPD with the ground truth given by\nthe trellis decoder by Tal et al. We further show that due to the reduced\ncomplexity of the NPD, we are able to incorporate list decoding and further\nimprove performance. We believe that the extended NPD presented here could have\napplications in future technologies like DNA storage.", "published": "2025-07-16 15:22:34", "link": "http://arxiv.org/abs/2507.12329v1", "categories": ["cs.IT", "cs.AI", "cs.LG", "math.IT"], "primary_category": "cs.IT"}
{"title": "Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models", "abstract": "We argue that diffusion models' success in modeling complex distributions is,\nfor the most part, coming from their input conditioning. This paper\ninvestigates the representation used to condition diffusion models from the\nperspective that ideal representations should improve sample fidelity, be easy\nto generate, and be compositional to allow out-of-training samples generation.\nWe introduce Discrete Latent Code (DLC), an image representation derived from\nSimplicial Embeddings trained with a self-supervised learning objective. DLCs\nare sequences of discrete tokens, as opposed to the standard continuous image\nembeddings. They are easy to generate and their compositionality enables\nsampling of novel images beyond the training distribution. Diffusion models\ntrained with DLCs have improved generation fidelity, establishing a new\nstate-of-the-art for unconditional image generation on ImageNet. Additionally,\nwe show that composing DLCs allows the image generator to produce\nout-of-distribution samples that coherently combine the semantics of images in\ndiverse ways. Finally, we showcase how DLCs can enable text-to-image generation\nby leveraging large-scale pretrained language models. We efficiently finetune a\ntext diffusion language model to generate DLCs that produce novel samples\noutside of the image generator training distribution.", "published": "2025-07-16 15:12:17", "link": "http://arxiv.org/abs/2507.12318v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Thought Purity: Defense Paradigm For Chain-of-Thought Attack", "abstract": "While reinforcement learning-trained Large Reasoning Models (LRMs, e.g.,\nDeepseek-R1) demonstrate advanced reasoning capabilities in the evolving Large\nLanguage Models (LLMs) domain, their susceptibility to security threats remains\na critical vulnerability. This weakness is particularly evident in\nChain-of-Thought (CoT) generation processes, where adversarial methods like\nbackdoor prompt attacks can systematically subvert the model's core reasoning\nmechanisms. The emerging Chain-of-Thought Attack (CoTA) reveals this\nvulnerability through exploiting prompt controllability, simultaneously\ndegrading both CoT safety and task performance with low-cost interventions. To\naddress this compounded security-performance vulnerability, we propose Thought\nPurity (TP): a defense paradigm that systematically strengthens resistance to\nmalicious content while preserving operational efficacy. Our solution achieves\nthis through three synergistic components: (1) a safety-optimized data\nprocessing pipeline (2) reinforcement learning-enhanced rule constraints (3)\nadaptive monitoring metrics. Our approach establishes the first comprehensive\ndefense mechanism against CoTA vulnerabilities in reinforcement\nlearning-aligned reasoning systems, significantly advancing the\nsecurity-functionality equilibrium for next-generation AI architectures.", "published": "2025-07-16 15:09:13", "link": "http://arxiv.org/abs/2507.12314v1", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.CR"], "primary_category": "cs.LG"}
{"title": "PROL : Rehearsal Free Continual Learning in Streaming Data via Prompt Online Learning", "abstract": "The data privacy constraint in online continual learning (OCL), where the\ndata can be seen only once, complicates the catastrophic forgetting problem in\nstreaming data. A common approach applied by the current SOTAs in OCL is with\nthe use of memory saving exemplars or features from previous classes to be\nreplayed in the current task. On the other hand, the prompt-based approach\nperforms excellently in continual learning but with the cost of a growing\nnumber of trainable parameters. The first approach may not be applicable in\npractice due to data openness policy, while the second approach has the issue\nof throughput associated with the streaming data. In this study, we propose a\nnovel prompt-based method for online continual learning that includes 4 main\ncomponents: (1) single light-weight prompt generator as a general knowledge,\n(2) trainable scaler-and-shifter as specific knowledge, (3) pre-trained model\n(PTM) generalization preserving, and (4) hard-soft updates mechanism. Our\nproposed method achieves significantly higher performance than the current\nSOTAs in CIFAR100, ImageNet-R, ImageNet-A, and CUB dataset. Our complexity\nanalysis shows that our method requires a relatively smaller number of\nparameters and achieves moderate training time, inference time, and throughput.\nFor further study, the source code of our method is available at\nhttps://github.com/anwarmaxsum/PROL.", "published": "2025-07-16 15:04:46", "link": "http://arxiv.org/abs/2507.12305v1", "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in Extremely Preterm Infants", "abstract": "Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of\nextremely low birth weight infants. Defined by oxygen dependence at 36 weeks\npostmenstrual age, it causes lifelong respiratory complications. However,\npreventive interventions carry severe risks, including neurodevelopmental\nimpairment, ventilator-induced lung injury, and systemic complications.\nTherefore, early BPD prognosis and prediction of BPD outcome is crucial to\navoid unnecessary toxicity in low risk infants. Admission radiographs of\nextremely preterm infants are routinely acquired within 24h of life and could\nserve as a non-invasive prognostic tool. In this work, we developed and\ninvestigated a deep learning approach using chest X-rays from 163 extremely\nlow-birth-weight infants ($\\leq$32 weeks gestation, 401-999g) obtained within\n24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult\nchest radiographs, employing progressive layer freezing with discriminative\nlearning rates to prevent overfitting and evaluated a CutMix augmentation and\nlinear probing. For moderate/severe BPD outcome prediction, our best performing\nmodel with progressive freezing, linear probing and CutMix achieved an AUROC of\n0.78 $\\pm$ 0.10, balanced accuracy of 0.69 $\\pm$ 0.10, and an F1-score of 0.67\n$\\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet\ninitialization (p = 0.031) which confirms domain-specific pretraining to be\nimportant for BPD outcome prediction. Routine IRDS grades showed limited\nprognostic value (AUROC 0.57 $\\pm$ 0.11), confirming the need of learned\nmarkers. Our approach demonstrates that domain-specific pretraining enables\naccurate BPD prediction from routine day-1 radiographs. Through progressive\nfreezing and linear probing, the method remains computationally feasible for\nsite-level implementation and future federated learning deployments.", "published": "2025-07-16 14:19:44", "link": "http://arxiv.org/abs/2507.12269v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A Framework for Nonstationary Gaussian Processes with Neural Network Parameters", "abstract": "Gaussian processes have become a popular tool for nonparametric regression\nbecause of their flexibility and uncertainty quantification. However, they\noften use stationary kernels, which limit the expressiveness of the model and\nmay be unsuitable for many datasets. We propose a framework that uses\nnonstationary kernels whose parameters vary across the feature space, modeling\nthese parameters as the output of a neural network that takes the features as\ninput. The neural network and Gaussian process are trained jointly using the\nchain rule to calculate derivatives. Our method clearly describes the behavior\nof the nonstationary parameters and is compatible with approximation methods\nfor scaling to large datasets. It is flexible and easily adapts to different\nnonstationary kernels without needing to redesign the optimization procedure.\nOur methods are implemented with the GPyTorch library and can be readily\nmodified. We test a nonstationary variance and noise variant of our method on\nseveral machine learning datasets and find that it achieves better accuracy and\nlog-score than both a stationary model and a hierarchical model approximated\nwith variational inference. Similar results are observed for a model with only\nnonstationary variance. We also demonstrate our approach's ability to recover\nthe nonstationary parameters of a spatial dataset.", "published": "2025-07-16 14:09:49", "link": "http://arxiv.org/abs/2507.12262v1", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Looking for Fairness in Recommender Systems", "abstract": "Recommender systems can be found everywhere today, shaping our everyday\nexperience whenever we're consuming content, ordering food, buying groceries\nonline, or even just reading the news. Let's imagine we're in the process of\nbuilding a recommender system to make content suggestions to users on social\nmedia. When thinking about fairness, it becomes clear there are several\nperspectives to consider: the users asking for tailored suggestions, the\ncontent creators hoping for some limelight, and society at large, navigating\nthe repercussions of algorithmic recommendations. A shared fairness concern\nacross all three is the emergence of filter bubbles, a side-effect that takes\nplace when recommender systems are almost \"too good\", making recommendations so\ntailored that users become inadvertently confined to a narrow set of\nopinions/themes and isolated from alternative ideas. From the user's\nperspective, this is akin to manipulation. From the small content creator's\nperspective, this is an obstacle preventing them access to a whole range of\npotential fans. From society's perspective, the potential consequences are\nfar-reaching, influencing collective opinions, social behavior and political\ndecisions. How can our recommender system be fine-tuned to avoid the creation\nof filter bubbles, and ensure a more inclusive and diverse content landscape?\nApproaching this problem involves defining one (or more) performance metric to\nrepresent diversity, and tweaking our recommender system's performance through\nthe lens of fairness. By incorporating this metric into our evaluation\nframework, we aim to strike a balance between personalized recommendations and\nthe broader societal goal of fostering rich and varied cultures and points of\nview.", "published": "2025-07-16 13:53:02", "link": "http://arxiv.org/abs/2507.12242v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "Xiangqi-R1: Enhancing Spatial Strategic Reasoning in LLMs for Chinese Chess via Reinforcement Learning", "abstract": "Game playing has long served as a fundamental benchmark for evaluating\nArtificial General Intelligence (AGI). While Large Language Models (LLMs) have\ndemonstrated impressive capabilities in general reasoning, their effectiveness\nin spatial strategic reasoning, which is critical for complex and fully\nobservable board games, remains insufficiently explored. In this work, we adopt\nChinese Chess (Xiangqi) as a challenging and rich testbed due to its intricate\nrules and spatial complexity. To advance LLMs' strategic competence in such\nenvironments, we propose a training framework tailored to Xiangqi, built upon a\nlarge-scale dataset of five million board-move pairs enhanced with expert\nannotations and engine evaluations. Building on this foundation, we introduce\nXiangqi-R1, a 7B-parameter model trained in multi-stage manner: (1) fine-tuning\nfor legal move prediction to capture basic spatial rules, (2) incorporating\nstrategic annotations to improve decision-making, and (3) applying\nreinforcement learning via Group Relative Policy Optimization (GRPO) with\nmulti-dimensional reward signals to enhance reasoning stability. Our\nExperimental results indicate that, despite their size and power,\ngeneral-purpose LLMs struggle to achieve satisfactory performance in these\ntasks. Compared to general-purpose LLMs, Xiangqi-R1 greatly advances with an\n18% rise in move legality and a 22% boost in analysis accuracy. Our results\npoint to a promising path for creating general strategic intelligence in\nspatially complex areas.", "published": "2025-07-16 13:19:46", "link": "http://arxiv.org/abs/2507.12215v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Draw an Ugly Person An Exploration of Generative AIs Perceptions of Ugliness", "abstract": "Generative AI does not only replicate human creativity but also reproduces\ndeep-seated cultural biases, making it crucial to critically examine how\nconcepts like ugliness are understood and expressed by these tools. This study\ninvestigates how four different generative AI models understand and express\nugliness through text and image and explores the biases embedded within these\nrepresentations. We extracted 13 adjectives associated with ugliness through\niterative prompting of a large language model and generated 624 images across\nfour AI models and three prompts. Demographic and socioeconomic attributes\nwithin the images were independently coded and thematically analyzed. Our\nfindings show that AI models disproportionately associate ugliness with old\nwhite male figures, reflecting entrenched social biases as well as paradoxical\nbiases, where efforts to avoid stereotypical depictions of marginalized groups\ninadvertently result in the disproportionate projection of negative attributes\nonto majority groups. Qualitative analysis further reveals that, despite\nsupposed attempts to frame ugliness within social contexts, conventional\nphysical markers such as asymmetry and aging persist as central visual motifs.\nThese findings demonstrate that despite attempts to create more equal\nrepresentations, generative AI continues to perpetuate inherited and\nparadoxical biases, underscoring the critical work being done to create ethical\nAI training paradigms and advance methodologies for more inclusive AI\ndevelopment.", "published": "2025-07-16 13:16:56", "link": "http://arxiv.org/abs/2507.12212v1", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC"}
{"title": "BuildEvo: Designing Building Energy Consumption Forecasting Heuristics via LLM-driven Evolution", "abstract": "Accurate building energy forecasting is essential, yet traditional heuristics\noften lack precision, while advanced models can be opaque and struggle with\ngeneralization by neglecting physical principles. This paper introduces\nBuildEvo, a novel framework that uses Large Language Models (LLMs) to\nautomatically design effective and interpretable energy prediction heuristics.\nWithin an evolutionary process, BuildEvo guides LLMs to construct and enhance\nheuristics by systematically incorporating physical insights from building\ncharacteristics and operational data (e.g., from the Building Data Genome\nProject 2). Evaluations show BuildEvo achieves state-of-the-art performance on\nbenchmarks, offering improved generalization and transparent prediction logic.\nThis work advances the automated design of robust, physically grounded\nheuristics, promoting trustworthy models for complex energy systems.", "published": "2025-07-16 13:07:24", "link": "http://arxiv.org/abs/2507.12207v1", "categories": ["cs.AI", "cs.NE"], "primary_category": "cs.AI"}
{"title": "Sparse Autoencoders for Sequential Recommendation Models: Interpretation and Flexible Control", "abstract": "Many current state-of-the-art models for sequential recommendations are based\non transformer architectures. Interpretation and explanation of such black box\nmodels is an important research question, as a better understanding of their\ninternals can help understand, influence, and control their behavior, which is\nvery important in a variety of real-world applications. Recently sparse\nautoencoders (SAE) have been shown to be a promising unsupervised approach for\nextracting interpretable features from language models. These autoencoders\nlearn to reconstruct hidden states of the transformer's internal layers from\nsparse linear combinations of directions in their activation space.\n  This paper is focused on the application of SAE to the sequential\nrecommendation domain. We show that this approach can be successfully applied\nto the transformer trained on a sequential recommendation task: learned\ndirections turn out to be more interpretable and monosemantic than the original\nhidden state dimensions. Moreover, we demonstrate that the features learned by\nSAE can be used to effectively and flexibly control the model's behavior,\nproviding end-users with a straightforward method to adjust their\nrecommendations to different custom scenarios and contexts.", "published": "2025-07-16 12:57:43", "link": "http://arxiv.org/abs/2507.12202v1", "categories": ["cs.IR", "cs.AI", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Quantize More, Lose Less: Autoregressive Generation from Residually Quantized Speech Representations", "abstract": "Text-to-speech (TTS) synthesis has seen renewed progress under the discrete\nmodeling paradigm. Existing autoregressive approaches often rely on\nsingle-codebook representations, which suffer from significant information\nloss. Even with post-hoc refinement techniques such as flow matching, these\nmethods fail to recover fine-grained details (e.g., prosodic nuances,\nspeaker-specific timbres), especially in challenging scenarios like singing\nvoice or music synthesis. We propose QTTS, a novel TTS framework built upon our\nnew audio codec, QDAC. The core innovation of QDAC lies in its end-to-end\ntraining of an ASR-based auto-regressive network with a GAN, which achieves\nsuperior semantic feature disentanglement for scalable, near-lossless\ncompression. QTTS models these discrete codes using two innovative strategies:\nthe Hierarchical Parallel architecture, which uses a dual-AR structure to model\ninter-codebook dependencies for higher-quality synthesis, and the Delay\nMultihead approach, which employs parallelized prediction with a fixed delay to\naccelerate inference speed. Our experiments demonstrate that the proposed\nframework achieves higher synthesis quality and better preserves expressive\ncontent compared to baseline. This suggests that scaling up compression via\nmulti-codebook modeling is a promising direction for high-fidelity,\ngeneral-purpose speech and audio generation.", "published": "2025-07-16 12:47:09", "link": "http://arxiv.org/abs/2507.12197v1", "categories": ["cs.SD", "cs.AI"], "primary_category": "cs.SD"}
{"title": "Revealing the Ancient Beauty: Digital Reconstruction of Temple Tiles using Computer Vision", "abstract": "Modern digitised approaches have dramatically changed the preservation and\nrestoration of cultural treasures, integrating computer scientists into\nmultidisciplinary projects with ease. Machine learning, deep learning, and\ncomputer vision techniques have revolutionised developing sectors like 3D\nreconstruction, picture inpainting,IoT-based methods, genetic algorithms, and\nimage processing with the integration of computer scientists into\nmultidisciplinary initiatives. We suggest three cutting-edge techniques in\nrecognition of the special qualities of Indian monuments, which are famous for\ntheir architectural skill and aesthetic appeal. First is the Fractal\nConvolution methodology, a segmentation method based on image processing that\nsuccessfully reveals subtle architectural patterns within these irreplaceable\ncultural buildings. The second is a revolutionary Self-Sensitive Tile Filling\n(SSTF) method created especially for West Bengal's mesmerising Bankura\nTerracotta Temples with a brand-new data augmentation method called MosaicSlice\non the third. Furthermore, we delve deeper into the Super Resolution strategy\nto upscale the images without losing significant amount of quality. Our methods\nallow for the development of seamless region-filling and highly detailed tiles\nwhile maintaining authenticity using a novel data augmentation strategy within\naffordable costs introducing automation. By providing effective solutions that\npreserve the delicate balance between tradition and innovation, this study\nimproves the subject and eventually ensures unrivalled efficiency and aesthetic\nexcellence in cultural heritage protection. The suggested approaches advance\nthe field into an era of unmatched efficiency and aesthetic quality while\ncarefully upholding the delicate equilibrium between tradition and innovation.", "published": "2025-07-16 12:46:04", "link": "http://arxiv.org/abs/2507.12195v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Selective Quantization Tuning for ONNX Models", "abstract": "Quantization is a process that reduces the precision of deep neural network\nmodels to lower model size and computational demands, often at the cost of\naccuracy. However, fully quantized models may exhibit sub-optimal performance\nbelow acceptable levels and face deployment challenges on low-end hardware\naccelerators due to practical constraints. To address these issues,\nquantization can be selectively applied to only a subset of layers, but\nselecting which layers to exclude is non-trivial. To this direction, we propose\nTuneQn, a suite enabling selective quantization, deployment and execution of\nONNX models across various CPU and GPU devices, combined with profiling and\nmulti-objective optimization. TuneQn generates selectively quantized ONNX\nmodels, deploys them on different hardware, measures performance on metrics\nlike accuracy and size, performs Pareto Front minimization to identify the best\nmodel candidate and visualizes the results. To demonstrate the effectiveness of\nTuneQn, we evaluated TuneQn on four ONNX models with two quantization settings\nacross CPU and GPU devices. As a result, we demonstrated that our utility\neffectively performs selective quantization and tuning, selecting ONNX model\ncandidates with up to a $54.14$% reduction in accuracy loss compared to the\nfully quantized model, and up to a $72.9$% model size reduction compared to the\noriginal model.", "published": "2025-07-16 12:46:04", "link": "http://arxiv.org/abs/2507.12196v1", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "primary_category": "cs.LG"}
{"title": "BenchRL-QAS: Benchmarking reinforcement learning algorithms for quantum architecture search", "abstract": "We introduce BenchRL-QAS, a unified benchmarking framework for systematically\nevaluating reinforcement learning (RL) algorithms in quantum architecture\nsearch (QAS) across diverse variational quantum algorithm tasks and system\nsizes ranging from 2- to 8-qubit. Our study benchmarks nine RL agents including\nboth value-based and policy-gradient methods on representative quantum problems\nsuch as variational quantum eigensolver, variational quantum state\ndiagonalization, quantum classification, and state preparation, spanning both\nnoiseless and realistic noisy regimes. We propose a weighted ranking metric\nthat balances accuracy, circuit depth, gate count, and computational\nefficiency, enabling fair and comprehensive comparison. Our results first\nreveal that RL-based quantum classifier outperforms baseline variational\nclassifiers. Then we conclude that no single RL algorithm is universally\noptimal when considering a set of QAS tasks; algorithmic performance is highly\ncontext-dependent, varying with task structure, qubit count, and noise. This\nempirical finding provides strong evidence for the \"no free lunch\" principle in\nRL-based quantum circuit design and highlights the necessity of tailored\nalgorithm selection and systematic benchmarking for advancing quantum circuit\nsynthesis. This work represents the most comprehensive RL-QAS benchmarking\neffort to date, and BenchRL-QAS along with all experimental data are made\npublicly available to support reproducibility and future research\nhttps://github.com/azhar-ikhtiarudin/bench-rlqas.", "published": "2025-07-16 12:43:25", "link": "http://arxiv.org/abs/2507.12189v1", "categories": ["quant-ph", "cs.AI", "cs.LG", "cs.PF"], "primary_category": "quant-ph"}
{"title": "Wavelet-based Decoupling Framework for low-light Stereo Image Enhancement", "abstract": "Low-light images suffer from complex degradation, and existing enhancement\nmethods often encode all degradation factors within a single latent space. This\nleads to highly entangled features and strong black-box characteristics, making\nthe model prone to shortcut learning. To mitigate the above issues, this paper\nproposes a wavelet-based low-light stereo image enhancement method with feature\nspace decoupling. Our insight comes from the following findings: (1) Wavelet\ntransform enables the independent processing of low-frequency and\nhigh-frequency information. (2) Illumination adjustment can be achieved by\nadjusting the low-frequency component of a low-light image, extracted through\nmulti-level wavelet decomposition. Thus, by using wavelet transform the feature\nspace is decomposed into a low-frequency branch for illumination adjustment and\nmultiple high-frequency branches for texture enhancement. Additionally, stereo\nlow-light image enhancement can extract useful cues from another view to\nimprove enhancement. To this end, we propose a novel high-frequency guided\ncross-view interaction module (HF-CIM) that operates within high-frequency\nbranches rather than across the entire feature space, effectively extracting\nvaluable image details from the other view. Furthermore, to enhance the\nhigh-frequency information, a detail and texture enhancement module (DTEM) is\nproposed based on cross-attention mechanism. The model is trained on a dataset\nconsisting of images with uniform illumination and images with non-uniform\nillumination. Experimental results on both real and synthetic images indicate\nthat our algorithm offers significant advantages in light adjustment while\neffectively recovering high-frequency information. The code and dataset are\npublicly available at: https://github.com/Cherisherr/WDCI-Net.git.", "published": "2025-07-16 12:42:27", "link": "http://arxiv.org/abs/2507.12188v1", "categories": ["cs.CV", "cs.AI", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Partially Observable Reference Policy Programming: Solving POMDPs Sans Numerical Optimisation", "abstract": "This paper proposes Partially Observable Reference Policy Programming, a\nnovel anytime online approximate POMDP solver which samples meaningful future\nhistories very deeply while simultaneously forcing a gradual policy update. We\nprovide theoretical guarantees for the algorithm's underlying scheme which say\nthat the performance loss is bounded by the average of the sampling\napproximation errors rather than the usual maximum, a crucial requirement given\nthe sampling sparsity of online planning. Empirical evaluations on two\nlarge-scale problems with dynamically evolving environments -- including a\nhelicopter emergency scenario in the Corsica region requiring approximately 150\nplanning steps -- corroborate the theoretical results and indicate that our\nsolver considerably outperforms current online benchmarks.", "published": "2025-07-16 12:33:32", "link": "http://arxiv.org/abs/2507.12186v1", "categories": ["cs.AI", "I.2.8; I.2.9"], "primary_category": "cs.AI"}
{"title": "PRISM: Distributed Inference for Foundation Models at Edge", "abstract": "Foundation models (FMs) have achieved remarkable success across a wide range\nof applications, from image classification to natural langurage processing, but\npose significant challenges for deployment at edge. This has sparked growing\ninterest in developing practical and efficient strategies for bringing\nfoundation models to edge environments. In this work, we propose PRISM, a\ncommunication-efficient and compute-aware strategy for distributed Transformer\ninference on edge devices. Our method leverages a Segment Means representation\nto approximate intermediate output features, drastically reducing inter-device\ncommunication. Additionally, we restructure the self-attention mechanism to\neliminate redundant computations caused by per-device Key/Value calculation in\nposition-wise partitioning and design a partition-aware causal masking scheme\ntailored for autoregressive models. We evaluate PRISM on ViT, BERT, and GPT-2\nacross diverse datasets, namely CIFAR-10, CIFAR-100, ImageNet-1k, GLUE, and\nCBT. Our results demonstrate substantial reductions in communication overhead\n(up to 99.2% for BERT at compression rate CR = 128) and per-device computation\n(51.24% for BERT at the same setting), with only minor accuracy degradation.\nThis method offers a scalable and practical solution for deploying foundation\nmodels in distributed resource-constrained environments.", "published": "2025-07-16 11:25:03", "link": "http://arxiv.org/abs/2507.12145v1", "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Quantum Machine Learning in Multi-Qubit Phase-Space Part I: Foundations", "abstract": "Quantum machine learning (QML) seeks to exploit the intrinsic properties of\nquantum mechanical systems, including superposition, coherence, and quantum\nentanglement for classical data processing. However, due to the exponential\ngrowth of the Hilbert space, QML faces practical limits in classical\nsimulations with the state-vector representation of quantum system. On the\nother hand, phase-space methods offer an alternative by encoding quantum states\nas quasi-probability functions. Building on prior work in qubit phase-space and\nthe Stratonovich-Weyl (SW) correspondence, we construct a closed, composable\ndynamical formalism for one- and many-qubit systems in phase-space. This\nformalism replaces the operator algebra of the Pauli group with function\ndynamics on symplectic manifolds, and recasts the curse of dimensionality in\nterms of harmonic support on a domain that scales linearly with the number of\nqubits. It opens a new route for QML based on variational modelling over\nphase-space.", "published": "2025-07-16 10:37:16", "link": "http://arxiv.org/abs/2507.12117v1", "categories": ["quant-ph", "cs.AI", "math-ph", "math.MP"], "primary_category": "quant-ph"}
{"title": "Topology Enhanced MARL for Multi-Vehicle Cooperative Decision-Making of CAVs", "abstract": "The exploration-exploitation trade-off constitutes one of the fundamental\nchallenges in reinforcement learning (RL), which is exacerbated in multi-agent\nreinforcement learning (MARL) due to the exponential growth of joint\nstate-action spaces. This paper proposes a topology-enhanced MARL (TPE-MARL)\nmethod for optimizing cooperative decision-making of connected and autonomous\nvehicles (CAVs) in mixed traffic. This work presents two primary contributions:\nFirst, we construct a game topology tensor for dynamic traffic flow,\neffectively compressing high-dimensional traffic state information and decrease\nthe search space for MARL algorithms. Second, building upon the designed game\ntopology tensor and using QMIX as the backbone RL algorithm, we establish a\ntopology-enhanced MARL framework incorporating visit counts and agent mutual\ninformation. Extensive simulations across varying traffic densities and CAV\npenetration rates demonstrate the effectiveness of TPE-MARL. Evaluations\nencompassing training dynamics, exploration patterns, macroscopic traffic\nperformance metrics, and microscopic vehicle behaviors reveal that TPE-MARL\nsuccessfully balances exploration and exploitation. Consequently, it exhibits\nsuperior performance in terms of traffic efficiency, safety, decision\nsmoothness, and task completion. Furthermore, the algorithm demonstrates\ndecision-making rationality comparable to or exceeding that of human drivers in\nboth mixed-autonomy and fully autonomous traffic scenarios. Code of our work is\navailable at\n\\href{https://github.com/leoPub/tpemarl}{https://github.com/leoPub/tpemarl}.", "published": "2025-07-16 10:27:36", "link": "http://arxiv.org/abs/2507.12110v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Multimodal Coordinated Online Behavior: Trade-offs and Strategies", "abstract": "Coordinated online behavior, which spans from beneficial collective actions\nto harmful manipulation such as disinformation campaigns, has become a key\nfocus in digital ecosystem analysis. Traditional methods often rely on\nmonomodal approaches, focusing on single types of interactions like co-retweets\nor co-hashtags, or consider multiple modalities independently of each other.\nHowever, these approaches may overlook the complex dynamics inherent in\nmultimodal coordination. This study compares different ways of operationalizing\nthe detection of multimodal coordinated behavior. It examines the trade-off\nbetween weakly and strongly integrated multimodal models, highlighting the\nbalance between capturing broader coordination patterns and identifying tightly\ncoordinated behavior. By comparing monomodal and multimodal approaches, we\nassess the unique contributions of different data modalities and explore how\nvarying implementations of multimodality impact detection outcomes. Our\nfindings reveal that not all the modalities provide distinct insights, but that\nwith a multimodal approach we can get a more comprehensive understanding of\ncoordination dynamics. This work enhances the ability to detect and analyze\ncoordinated online behavior, offering new perspectives for safeguarding the\nintegrity of digital platforms.", "published": "2025-07-16 10:25:45", "link": "http://arxiv.org/abs/2507.12108v1", "categories": ["cs.SI", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Non-Adaptive Adversarial Face Generation", "abstract": "Adversarial attacks on face recognition systems (FRSs) pose serious security\nand privacy threats, especially when these systems are used for identity\nverification. In this paper, we propose a novel method for generating\nadversarial faces-synthetic facial images that are visually distinct yet\nrecognized as a target identity by the FRS. Unlike iterative optimization-based\napproaches (e.g., gradient descent or other iterative solvers), our method\nleverages the structural characteristics of the FRS feature space. We figure\nout that individuals sharing the same attribute (e.g., gender or race) form an\nattributed subsphere. By utilizing such subspheres, our method achieves both\nnon-adaptiveness and a remarkably small number of queries. This eliminates the\nneed for relying on transferability and open-source surrogate models, which\nhave been a typical strategy when repeated adaptive queries to commercial FRSs\nare impossible. Despite requiring only a single non-adaptive query consisting\nof 100 face images, our method achieves a high success rate of over 93% against\nAWS's CompareFaces API at its default threshold. Furthermore, unlike many\nexisting attacks that perturb a given image, our method can deliberately\nproduce adversarial faces that impersonate the target identity while exhibiting\nhigh-level attributes chosen by the adversary.", "published": "2025-07-16 10:24:54", "link": "http://arxiv.org/abs/2507.12107v1", "categories": ["cs.CV", "cs.AI", "cs.CR", "I.2.6; I.5.4; D.4.6; K.6.5; I.4.8"], "primary_category": "cs.CV"}
{"title": "From Static to Intelligent: Evolving SaaS Pricing with LLMs", "abstract": "The SaaS paradigm has revolutionized software distribution by offering\nflexible pricing options to meet diverse customer needs. However, the rapid\nexpansion of the SaaS market has introduced significant complexity for DevOps\nteams, who must manually manage and evolve pricing structures, an approach that\nis both time-consuming and prone to errors. The absence of automated tools for\npricing analysis restricts the ability to efficiently evaluate, optimize, and\nscale these models. This paper proposes leveraging intelligent pricing\n(iPricing), dynamic, machine-readable pricing models, as a solution to these\nchallenges. Intelligent pricing enables competitive analysis, streamlines\noperational decision-making, and supports continuous pricing evolution in\nresponse to market dynamics, leading to improved efficiency and accuracy. We\npresent an LLM-driven approach that automates the transformation of static HTML\npricing into iPricing, significantly improving efficiency and consistency while\nminimizing human error. Our implementation, AI4Pricing2Yaml, features a basic\nInformation Extractor that uses web scraping and LLMs technologies to extract\nessential pricing components, plans, features, usage limits, and add-ons, from\nSaaS websites. Validation against a dataset of 30 distinct commercial SaaS,\nencompassing over 150 intelligent pricings, demonstrates the system's\neffectiveness in extracting the desired elements across all steps. However,\nchallenges remain in addressing hallucinations, complex structures, and dynamic\ncontent. This work highlights the potential of automating intelligent pricing\ntransformation to streamline SaaS pricing management, offering implications for\nimproved consistency and scalability in an increasingly intricate pricing\nlandscape. Future research will focus on refining extraction capabilities and\nenhancing the system's adaptability to a wider range of SaaS websites.", "published": "2025-07-16 10:20:14", "link": "http://arxiv.org/abs/2507.12104v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE"}
{"title": "InstructFLIP: Exploring Unified Vision-Language Model for Face Anti-spoofing", "abstract": "Face anti-spoofing (FAS) aims to construct a robust system that can withstand\ndiverse attacks. While recent efforts have concentrated mainly on cross-domain\ngeneralization, two significant challenges persist: limited semantic\nunderstanding of attack types and training redundancy across domains. We\naddress the first by integrating vision-language models (VLMs) to enhance the\nperception of visual input. For the second challenge, we employ a meta-domain\nstrategy to learn a unified model that generalizes well across multiple\ndomains. Our proposed InstructFLIP is a novel instruction-tuned framework that\nleverages VLMs to enhance generalization via textual guidance trained solely on\na single domain. At its core, InstructFLIP explicitly decouples instructions\ninto content and style components, where content-based instructions focus on\nthe essential semantics of spoofing, and style-based instructions consider\nvariations related to the environment and camera characteristics. Extensive\nexperiments demonstrate the effectiveness of InstructFLIP by outperforming SOTA\nmodels in accuracy and substantially reducing training redundancy across\ndiverse domains in FAS. Project website is available at\nhttps://kunkunlin1221.github.io/InstructFLIP.", "published": "2025-07-16 09:16:51", "link": "http://arxiv.org/abs/2507.12060v1", "categories": ["cs.CV", "cs.AI", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Intra-view and Inter-view Correlation Guided Multi-view Novel Class Discovery", "abstract": "In this paper, we address the problem of novel class discovery (NCD), which\naims to cluster novel classes by leveraging knowledge from disjoint known\nclasses. While recent advances have made significant progress in this area,\nexisting NCD methods face two major limitations. First, they primarily focus on\nsingle-view data (e.g., images), overlooking the increasingly common multi-view\ndata, such as multi-omics datasets used in disease diagnosis. Second, their\nreliance on pseudo-labels to supervise novel class clustering often results in\nunstable performance, as pseudo-label quality is highly sensitive to factors\nsuch as data noise and feature dimensionality. To address these challenges, we\npropose a novel framework named Intra-view and Inter-view Correlation Guided\nMulti-view Novel Class Discovery (IICMVNCD), which is the first attempt to\nexplore NCD in multi-view setting so far. Specifically, at the intra-view\nlevel, leveraging the distributional similarity between known and novel\nclasses, we employ matrix factorization to decompose features into\nview-specific shared base matrices and factor matrices. The base matrices\ncapture distributional consistency among the two datasets, while the factor\nmatrices model pairwise relationships between samples. At the inter-view level,\nwe utilize view relationships among known classes to guide the clustering of\nnovel classes. This includes generating predicted labels through the weighted\nfusion of factor matrices and dynamically adjusting view weights of known\nclasses based on the supervision loss, which are then transferred to novel\nclass learning. Experimental results validate the effectiveness of our proposed\napproach.", "published": "2025-07-16 08:42:52", "link": "http://arxiv.org/abs/2507.12029v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "SS-DC: Spatial-Spectral Decoupling and Coupling Across Visible-Infrared Gap for Domain Adaptive Object Detection", "abstract": "Unsupervised domain adaptive object detection (UDAOD) from the visible domain\nto the infrared (RGB-IR) domain is challenging. Existing methods regard the RGB\ndomain as a unified domain and neglect the multiple subdomains within it, such\nas daytime, nighttime, and foggy scenes. We argue that decoupling the\ndomain-invariant (DI) and domain-specific (DS) features across these multiple\nsubdomains is beneficial for RGB-IR domain adaptation. To this end, this paper\nproposes a new SS-DC framework based on a decoupling-coupling strategy. In\nterms of decoupling, we design a Spectral Adaptive Idempotent Decoupling (SAID)\nmodule in the aspect of spectral decomposition. Due to the style and content\ninformation being highly embedded in different frequency bands, this module can\ndecouple DI and DS components more accurately and interpretably. A novel filter\nbank-based spectral processing paradigm and a self-distillation-driven\ndecoupling loss are proposed to improve the spectral domain decoupling. In\nterms of coupling, a new spatial-spectral coupling method is proposed, which\nrealizes joint coupling through spatial and spectral DI feature pyramids.\nMeanwhile, this paper introduces DS from decoupling to reduce the domain bias.\nExtensive experiments demonstrate that our method can significantly improve the\nbaseline performance and outperform existing UDAOD methods on multiple RGB-IR\ndatasets, including a new experimental protocol proposed in this paper based on\nthe FLIR-ADAS dataset.", "published": "2025-07-16 08:21:41", "link": "http://arxiv.org/abs/2507.12017v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Identifying Signatures of Image Phenotypes to Track Treatment Response in Liver Disease", "abstract": "Quantifiable image patterns associated with disease progression and treatment\nresponse are critical tools for guiding individual treatment, and for\ndeveloping novel therapies. Here, we show that unsupervised machine learning\ncan identify a pattern vocabulary of liver tissue in magnetic resonance images\nthat quantifies treatment response in diffuse liver disease. Deep clustering\nnetworks simultaneously encode and cluster patches of medical images into a\nlow-dimensional latent space to establish a tissue vocabulary. The resulting\ntissue types capture differential tissue change and its location in the liver\nassociated with treatment response. We demonstrate the utility of the\nvocabulary on a randomized controlled trial cohort of non-alcoholic\nsteatohepatitis patients. First, we use the vocabulary to compare longitudinal\nliver change in a placebo and a treatment cohort. Results show that the method\nidentifies specific liver tissue change pathways associated with treatment, and\nenables a better separation between treatment groups than established\nnon-imaging measures. Moreover, we show that the vocabulary can predict biopsy\nderived features from non-invasive imaging data. We validate the method on a\nseparate replication cohort to demonstrate the applicability of the proposed\nmethod.", "published": "2025-07-16 08:11:48", "link": "http://arxiv.org/abs/2507.12012v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "eess.IV"}
{"title": "DUSE: A Data Expansion Framework for Low-resource Automatic Modulation Recognition based on Active Learning", "abstract": "Although deep neural networks have made remarkable achievements in the field\nof automatic modulation recognition (AMR), these models often require a large\namount of labeled data for training. However, in many practical scenarios, the\navailable target domain data is scarce and difficult to meet the needs of model\ntraining. The most direct way is to collect data manually and perform expert\nannotation, but the high time and labor costs are unbearable. Another common\nmethod is data augmentation. Although it can enrich training samples to a\ncertain extent, it does not introduce new data and therefore cannot\nfundamentally solve the problem of data scarcity. To address these challenges,\nwe introduce a data expansion framework called Dynamic Uncertainty-driven\nSample Expansion (DUSE). Specifically, DUSE uses an uncertainty scoring\nfunction to filter out useful samples from relevant AMR datasets and employs an\nactive learning strategy to continuously refine the scorer. Extensive\nexperiments demonstrate that DUSE consistently outperforms 8 coreset selection\nbaselines in both class-balance and class-imbalance settings. Besides, DUSE\nexhibits strong cross-architecture generalization for unseen models.", "published": "2025-07-16 08:09:41", "link": "http://arxiv.org/abs/2507.12011v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Dual form Complementary Masking for Domain-Adaptive Image Segmentation", "abstract": "Recent works have correlated Masked Image Modeling (MIM) with consistency\nregularization in Unsupervised Domain Adaptation (UDA). However, they merely\ntreat masking as a special form of deformation on the input images and neglect\nthe theoretical analysis, which leads to a superficial understanding of masked\nreconstruction and insufficient exploitation of its potential in enhancing\nfeature extraction and representation learning. In this paper, we reframe\nmasked reconstruction as a sparse signal reconstruction problem and\ntheoretically prove that the dual form of complementary masks possesses\nsuperior capabilities in extracting domain-agnostic image features. Based on\nthis compelling insight, we propose MaskTwins, a simple yet effective UDA\nframework that integrates masked reconstruction directly into the main training\npipeline. MaskTwins uncovers intrinsic structural patterns that persist across\ndisparate domains by enforcing consistency between predictions of images masked\nin complementary ways, enabling domain generalization in an end-to-end manner.\nExtensive experiments verify the superiority of MaskTwins over baseline methods\nin natural and biological image segmentation. These results demonstrate the\nsignificant advantages of MaskTwins in extracting domain-invariant features\nwithout the need for separate pre-training, offering a new paradigm for\ndomain-adaptive segmentation.", "published": "2025-07-16 08:05:22", "link": "http://arxiv.org/abs/2507.12008v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Frequency-Dynamic Attention Modulation for Dense Prediction", "abstract": "Vision Transformers (ViTs) have significantly advanced computer vision,\ndemonstrating strong performance across various tasks. However, the attention\nmechanism in ViTs makes each layer function as a low-pass filter, and the\nstacked-layer architecture in existing transformers suffers from frequency\nvanishing. This leads to the loss of critical details and textures. We propose\na novel, circuit-theory-inspired strategy called Frequency-Dynamic Attention\nModulation (FDAM), which can be easily plugged into ViTs. FDAM directly\nmodulates the overall frequency response of ViTs and consists of two\ntechniques: Attention Inversion (AttInv) and Frequency Dynamic Scaling\n(FreqScale). Since circuit theory uses low-pass filters as fundamental\nelements, we introduce AttInv, a method that generates complementary high-pass\nfiltering by inverting the low-pass filter in the attention matrix, and\ndynamically combining the two. We further design FreqScale to weight different\nfrequency components for fine-grained adjustments to the target response\nfunction. Through feature similarity analysis and effective rank evaluation, we\ndemonstrate that our approach avoids representation collapse, leading to\nconsistent performance improvements across various models, including SegFormer,\nDeiT, and MaskDINO. These improvements are evident in tasks such as semantic\nsegmentation, object detection, and instance segmentation. Additionally, we\napply our method to remote sensing detection, achieving state-of-the-art\nresults in single-scale settings. The code is available at\n\\href{https://github.com/Linwei-Chen/FDAM}{https://github.com/Linwei-Chen/FDAM}.", "published": "2025-07-16 07:59:54", "link": "http://arxiv.org/abs/2507.12006v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Can LLMs Find Fraudsters? Multi-level LLM Enhanced Graph Fraud Detection", "abstract": "Graph fraud detection has garnered significant attention as Graph Neural\nNetworks (GNNs) have proven effective in modeling complex relationships within\nmultimodal data. However, existing graph fraud detection methods typically use\npreprocessed node embeddings and predefined graph structures to reveal\nfraudsters, which ignore the rich semantic cues contained in raw textual\ninformation. Although Large Language Models (LLMs) exhibit powerful\ncapabilities in processing textual information, it remains a significant\nchallenge to perform multimodal fusion of processed textual embeddings with\ngraph structures. In this paper, we propose a \\textbf{M}ulti-level \\textbf{L}LM\n\\textbf{E}nhanced Graph Fraud \\textbf{D}etection framework called MLED. In\nMLED, we utilize LLMs to extract external knowledge from textual information to\nenhance graph fraud detection methods. To integrate LLMs with graph structure\ninformation and enhance the ability to distinguish fraudsters, we design a\nmulti-level LLM enhanced framework including type-level enhancer and\nrelation-level enhancer. One is to enhance the difference between the\nfraudsters and the benign entities, the other is to enhance the importance of\nthe fraudsters in different relations. The experiments on four real-world\ndatasets show that MLED achieves state-of-the-art performance in graph fraud\ndetection as a generalized framework that can be applied to existing methods.", "published": "2025-07-16 07:50:43", "link": "http://arxiv.org/abs/2507.11997v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Understanding visual attention beehind bee-inspired UAV navigation", "abstract": "Bio-inspired design is often used in autonomous UAV navigation due to the\ncapacity of biological systems for flight and obstacle avoidance despite\nlimited sensory and computational capabilities. In particular, honeybees mainly\nuse the sensory input of optic flow, the apparent motion of objects in their\nvisual field, to navigate cluttered environments. In our work, we train a\nReinforcement Learning agent to navigate a tunnel with obstacles using only\noptic flow as sensory input. We inspect the attention patterns of trained\nagents to determine the regions of optic flow on which they primarily base\ntheir motor decisions. We find that agents trained in this way pay most\nattention to regions of discontinuity in optic flow, as well as regions with\nlarge optic flow magnitude. The trained agents appear to navigate a cluttered\ntunnel by avoiding the obstacles that produce large optic flow, while\nmaintaining a centered position in their environment, which resembles the\nbehavior seen in flying insects. This pattern persists across independently\ntrained agents, which suggests that this could be a good strategy for\ndeveloping a simple explicit control law for physical UAVs.", "published": "2025-07-16 07:44:25", "link": "http://arxiv.org/abs/2507.11992v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Robust Planning for Autonomous Vehicles with Diffusion-Based Failure Samplers", "abstract": "High-risk traffic zones such as intersections are a major cause of\ncollisions. This study leverages deep generative models to enhance the safety\nof autonomous vehicles in an intersection context. We train a 1000-step\ndenoising diffusion probabilistic model to generate collision-causing sensor\nnoise sequences for an autonomous vehicle navigating a four-way intersection\nbased on the current relative position and velocity of an intruder. Using the\ngenerative adversarial architecture, the 1000-step model is distilled into a\nsingle-step denoising diffusion model which demonstrates fast inference speed\nwhile maintaining similar sampling quality. We demonstrate one possible\napplication of the single-step model in building a robust planner for the\nautonomous vehicle. The planner uses the single-step model to efficiently\nsample potential failure cases based on the currently measured traffic state to\ninform its decision-making. Through simulation experiments, the robust planner\ndemonstrates significantly lower failure rate and delay rate compared with the\nbaseline Intelligent Driver Model controller.", "published": "2025-07-16 07:43:55", "link": "http://arxiv.org/abs/2507.11991v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO"}
{"title": "Aime: Towards Fully-Autonomous Multi-Agent Framework", "abstract": "Multi-Agent Systems (MAS) powered by Large Language Models (LLMs) are\nemerging as a powerful paradigm for solving complex, multifaceted problems.\nHowever, the potential of these systems is often constrained by the prevalent\nplan-and-execute framework, which suffers from critical limitations: rigid plan\nexecution, static agent capabilities, and inefficient communication. These\nweaknesses hinder their adaptability and robustness in dynamic environments.\nThis paper introduces Aime, a novel multi-agent framework designed to overcome\nthese challenges through dynamic, reactive planning and execution. Aime\nreplaces the conventional static workflow with a fluid and adaptive\narchitecture. Its core innovations include: (1) a Dynamic Planner that\ncontinuously refines the overall strategy based on real-time execution\nfeedback; (2) an Actor Factory that implements Dynamic Actor instantiation,\nassembling specialized agents on-demand with tailored tools and knowledge; and\n(3) a centralized Progress Management Module that serves as a single source of\ntruth for coherent, system-wide state awareness. We empirically evaluated Aime\non a diverse suite of benchmarks spanning general reasoning (GAIA), software\nengineering (SWE-bench Verified), and live web navigation (WebVoyager). The\nresults demonstrate that Aime consistently outperforms even highly specialized\nstate-of-the-art agents in their respective domains. Its superior adaptability\nand task success rate establish Aime as a more resilient and effective\nfoundation for multi-agent collaboration.", "published": "2025-07-16 07:38:28", "link": "http://arxiv.org/abs/2507.11988v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Formal Verification of Neural Certificates Done Dynamically", "abstract": "Neural certificates have emerged as a powerful tool in cyber-physical systems\ncontrol, providing witnesses of correctness. These certificates, such as\nbarrier functions, often learned alongside control policies, once verified,\nserve as mathematical proofs of system safety. However, traditional formal\nverification of their defining conditions typically faces scalability\nchallenges due to exhaustive state-space exploration. To address this\nchallenge, we propose a lightweight runtime monitoring framework that\nintegrates real-time verification and does not require access to the underlying\ncontrol policy. Our monitor observes the system during deployment and performs\non-the-fly verification of the certificate over a lookahead region to ensure\nsafety within a finite prediction horizon. We instantiate this framework for\nReLU-based control barrier functions and demonstrate its practical\neffectiveness in a case study. Our approach enables timely detection of safety\nviolations and incorrect certificates with minimal overhead, providing an\neffective but lightweight alternative to the static verification of the\ncertificates.", "published": "2025-07-16 07:37:23", "link": "http://arxiv.org/abs/2507.11987v1", "categories": ["cs.SC", "cs.AI"], "primary_category": "cs.SC"}
{"title": "Online Training and Pruning of Deep Reinforcement Learning Networks", "abstract": "Scaling deep neural networks (NN) of reinforcement learning (RL) algorithms\nhas been shown to enhance performance when feature extraction networks are used\nbut the gained performance comes at the significant expense of increased\ncomputational and memory complexity. Neural network pruning methods have\nsuccessfully addressed this challenge in supervised learning. However, their\napplication to RL is underexplored. We propose an approach to integrate\nsimultaneous training and pruning within advanced RL methods, in particular to\nRL algorithms enhanced by the Online Feature Extractor Network (OFENet). Our\nnetworks (XiNet) are trained to solve stochastic optimization problems over the\nRL networks' weights and the parameters of variational Bernoulli distributions\nfor 0/1 Random Variables $\\xi$ scaling each unit in the networks. The\nstochastic problem formulation induces regularization terms that promote\nconvergence of the variational parameters to 0 when a unit contributes little\nto the performance. In this case, the corresponding structure is rendered\npermanently inactive and pruned from its network. We propose a cost-aware,\nsparsity-promoting regularization scheme, tailored to the DenseNet architecture\nof OFENets expressing the parameter complexity of involved networks in terms of\nthe parameters of the RVs in these networks. Then, when matching this cost with\nthe regularization terms, the many hyperparameters associated with them are\nautomatically selected, effectively combining the RL objectives and network\ncompression. We evaluate our method on continuous control benchmarks (MuJoCo)\nand the Soft Actor-Critic RL agent, demonstrating that OFENets can be pruned\nconsiderably with minimal loss in performance. Furthermore, our results confirm\nthat pruning large networks during training produces more efficient and higher\nperforming RL agents rather than training smaller networks from scratch.", "published": "2025-07-16 07:17:41", "link": "http://arxiv.org/abs/2507.11975v1", "categories": ["cs.LG", "cs.AI", "cs.RO"], "primary_category": "cs.LG"}
{"title": "Kevin: Multi-Turn RL for Generating CUDA Kernels", "abstract": "Writing GPU kernels is a challenging task and critical for AI systems'\nefficiency. It is also highly iterative: domain experts write code and improve\nperformance through execution feedback. Moreover, it presents verifiable\nrewards like correctness and speedup, making it a natural environment to apply\nReinforcement Learning (RL). To explicitly incorporate the iterative nature of\nthis process into training, we develop a flexible multi-turn RL recipe that\naddresses unique challenges encountered in real-world settings, such as\nlearning from long trajectories and effective reward attribution across turns.\nWe present Kevin - K(ernel D)evin, the first model trained with multi-turn RL\nfor CUDA kernel generation and optimization. In our evaluation setup, Kevin\nshows significant gains over its base model (QwQ-32B), improving correctness of\ngenerated kernels (in pure CUDA) from 56% to 82% and mean speedup from 0.53x to\n1.10x of baseline (PyTorch Eager), and surpassing frontier models like o4-mini\n(0.78x). Finally, we study its behavior across test-time scaling axes: we found\nscaling serial refinement more beneficial than parallel sampling. In\nparticular, when given more refinement turns, Kevin shows a higher rate of\nimprovement.", "published": "2025-07-16 06:33:07", "link": "http://arxiv.org/abs/2507.11948v1", "categories": ["cs.LG", "cs.AI", "cs.PF", "cs.SE"], "primary_category": "cs.LG"}
{"title": "RaDL: Relation-aware Disentangled Learning for Multi-Instance Text-to-Image Generation", "abstract": "With recent advancements in text-to-image (T2I) models, effectively\ngenerating multiple instances within a single image prompt has become a crucial\nchallenge. Existing methods, while successful in generating positions of\nindividual instances, often struggle to account for relationship discrepancy\nand multiple attributes leakage. To address these limitations, this paper\nproposes the relation-aware disentangled learning (RaDL) framework. RaDL\nenhances instance-specific attributes through learnable parameters and\ngenerates relation-aware image features via Relation Attention, utilizing\naction verbs extracted from the global prompt. Through extensive evaluations on\nbenchmarks such as COCO-Position, COCO-MIG, and DrawBench, we demonstrate that\nRaDL outperforms existing methods, showing significant improvements in\npositional accuracy, multiple attributes consideration, and the relationships\nbetween instances. Our results present RaDL as the solution for generating\nimages that consider both the relationships and multiple attributes of each\ninstance within the multi-instance image.", "published": "2025-07-16 06:28:20", "link": "http://arxiv.org/abs/2507.11947v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Effective Fine-Tuning of Vision Transformers with Low-Rank Adaptation for Privacy-Preserving Image Classification", "abstract": "We propose a low-rank adaptation method for training privacy-preserving\nvision transformer (ViT) models that efficiently freezes pre-trained ViT model\nweights. In the proposed method, trainable rank decomposition matrices are\ninjected into each layer of the ViT architecture, and moreover, the patch\nembedding layer is not frozen, unlike in the case of the conventional low-rank\nadaptation methods. The proposed method allows us not only to reduce the number\nof trainable parameters but to also maintain almost the same accuracy as that\nof full-time tuning.", "published": "2025-07-16 06:18:52", "link": "http://arxiv.org/abs/2507.11943v1", "categories": ["cs.CR", "cs.AI", "cs.CV"], "primary_category": "cs.CR"}
{"title": "Native-AI Empowered Scalable Architectures and Solutions for Future Non-Terrestrial Networks: An Overview", "abstract": "As the path toward 6G networks is being charted, the emerging applications\nhave motivated evolutions of network architectures to realize the efficient,\nreliable, and flexible wireless networks. Among the potential architectures,\nthe non-terrestrial network (NTN) and open radio access network (ORAN) have\nreceived increasing interest from both academia and industry. Although the\ndeployment of NTNs ensures coverage, enhances spectral efficiency, and improves\nthe resilience of wireless networks. The high altitude and mobility of NTN\npresent new challenges in the development and operations (DevOps) lifecycle,\nhindering intelligent and scalable network management due to the lack of native\nartificial intelligence (AI) capability. With the advantages of ORAN in\ndisaggregation, openness, virtualization, and intelligence, several works\npropose integrating ORAN principles into the NTN, focusing mainly on ORAN\ndeployment options based on transparent and regenerative systems. However, a\nholistic view of how to effectively combine ORAN and NTN throughout the DevOps\nlifecycle is still missing, especially regarding how intelligent ORAN addresses\nthe scalability challenges in NTN. Motivated by this, in this paper, we first\nprovide the background knowledge about ORAN and NTN, outline the\nstate-of-the-art research on ORAN for NTNs, and present the DevOps challenges\nthat motivate the adoption of ORAN solutions. We then propose the ORAN-based\nNTN framework, discussing its features and architectures in detail. These\ninclude the discussion about flexible fronthaul split, RAN intelligent\ncontrollers (RICs) enhancement for distributed learning, scalable deployment\narchitecture, and multi-domain service management. Finally, the future research\ndirections, including combinations of the ORAN-based NTN framework and other\nenabling technologies and schemes, as well as the candidate use cases, are\nhighlighted.", "published": "2025-07-16 05:58:45", "link": "http://arxiv.org/abs/2507.11935v1", "categories": ["cs.NI", "cs.AI", "cs.SY", "eess.SY"], "primary_category": "cs.NI"}
{"title": "A Parallel CPU-GPU Framework for Cost-Bounded DFS with Applications to IDA* and BTS", "abstract": "The rapid advancement of GPU technology has unlocked powerful parallel\nprocessing capabilities, creating new opportunities to enhance classic search\nalgorithms. A recent successful application of GPUs is in compressing large\npattern database (PDB) heuristics using neural networks while preserving\nheuristic admissibility. However, very few algorithms have been designed to\nexploit GPUs during search. Several variants of A* exist that batch GPU\ncomputations. In this paper we introduce a method for batching GPU computations\nin depth first search. In particular, we describe a new cost-bounded\ndepth-first search (CB-DFS) method that leverages the combined parallelism of\nmodern CPUs and GPUs. This is used to create algorithms like \\emph{Batch IDA*},\nan extension of the Iterative Deepening A* (IDA*) algorithm, or Batch BTS, an\nextensions of Budgeted Tree Search. Our approach builds on the general approach\nused by Asynchronous Parallel IDA* (AIDA*), while maintaining optimality\nguarantees. We evaluate the approach on the 3x3 Rubik's Cube and 4x4 sliding\ntile puzzle (STP), showing that GPU operations can be efficiently batched in\nDFS. Additionally, we conduct extensive experiments to analyze the effects of\nhyperparameters, neural network heuristic size, and hardware resources on\nperformance.", "published": "2025-07-16 05:07:33", "link": "http://arxiv.org/abs/2507.11916v1", "categories": ["cs.AI", "cs.DC"], "primary_category": "cs.AI"}
{"title": "Spatial Frequency Modulation for Semantic Segmentation", "abstract": "High spatial frequency information, including fine details like textures,\nsignificantly contributes to the accuracy of semantic segmentation. However,\naccording to the Nyquist-Shannon Sampling Theorem, high-frequency components\nare vulnerable to aliasing or distortion when propagating through downsampling\nlayers such as strided-convolution. Here, we propose a novel Spatial Frequency\nModulation (SFM) that modulates high-frequency features to a lower frequency\nbefore downsampling and then demodulates them back during upsampling.\nSpecifically, we implement modulation through adaptive resampling (ARS) and\ndesign a lightweight add-on that can densely sample the high-frequency areas to\nscale up the signal, thereby lowering its frequency in accordance with the\nFrequency Scaling Property. We also propose Multi-Scale Adaptive Upsampling\n(MSAU) to demodulate the modulated feature and recover high-frequency\ninformation through non-uniform upsampling This module further improves\nsegmentation by explicitly exploiting information interaction between densely\nand sparsely resampled areas at multiple scales. Both modules can seamlessly\nintegrate with various architectures, extending from convolutional neural\nnetworks to transformers. Feature visualization and analysis confirm that our\nmethod effectively alleviates aliasing while successfully retaining details\nafter demodulation. Finally, we validate the broad applicability and\neffectiveness of SFM by extending it to image classification, adversarial\nrobustness, instance segmentation, and panoptic segmentation tasks. The code is\navailable at\n\\href{https://github.com/Linwei-Chen/SFM}{https://github.com/Linwei-Chen/SFM}.", "published": "2025-07-16 04:15:53", "link": "http://arxiv.org/abs/2507.11893v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "From Coarse to Nuanced: Cross-Modal Alignment of Fine-Grained Linguistic Cues and Visual Salient Regions for Dynamic Emotion Recognition", "abstract": "Dynamic Facial Expression Recognition (DFER) aims to identify human emotions\nfrom temporally evolving facial movements and plays a critical role in\naffective computing. While recent vision-language approaches have introduced\nsemantic textual descriptions to guide expression recognition, existing methods\nstill face two key limitations: they often underutilize the subtle emotional\ncues embedded in generated text, and they have yet to incorporate sufficiently\neffective mechanisms for filtering out facial dynamics that are irrelevant to\nemotional expression. To address these gaps, We propose GRACE, Granular\nRepresentation Alignment for Cross-modal Emotion recognition that integrates\ndynamic motion modeling, semantic text refinement, and token-level cross-modal\nalignment to facilitate the precise localization of emotionally salient\nspatiotemporal features. Our method constructs emotion-aware textual\ndescriptions via a Coarse-to-fine Affective Text Enhancement (CATE) module and\nhighlights expression-relevant facial motion through a motion-difference\nweighting mechanism. These refined semantic and visual signals are aligned at\nthe token level using entropy-regularized optimal transport. Experiments on\nthree benchmark datasets demonstrate that our method significantly improves\nrecognition performance, particularly in challenging settings with ambiguous or\nimbalanced emotion classes, establishing new state-of-the-art (SOTA) results in\nterms of both UAR and WAR.", "published": "2025-07-16 04:15:06", "link": "http://arxiv.org/abs/2507.11892v1", "categories": ["cs.CV", "cs.AI", "cs.HC"], "primary_category": "cs.CV"}
{"title": "Interactive Hybrid Rice Breeding with Parametric Dual Projection", "abstract": "Hybrid rice breeding crossbreeds different rice lines and cultivates the\nresulting hybrids in fields to select those with desirable agronomic traits,\nsuch as higher yields. Recently, genomic selection has emerged as an efficient\nway for hybrid rice breeding. It predicts the traits of hybrids based on their\ngenes, which helps exclude many undesired hybrids, largely reducing the\nworkload of field cultivation. However, due to the limited accuracy of genomic\nprediction models, breeders still need to combine their experience with the\nmodels to identify regulatory genes that control traits and select hybrids,\nwhich remains a time-consuming process. To ease this process, in this paper, we\nproposed a visual analysis method to facilitate interactive hybrid rice\nbreeding. Regulatory gene identification and hybrid selection naturally\nensemble a dual-analysis task. Therefore, we developed a parametric dual\nprojection method with theoretical guarantees to facilitate interactive dual\nanalysis. Based on this dual projection method, we further developed a gene\nvisualization and a hybrid visualization to verify the identified regulatory\ngenes and hybrids. The effectiveness of our method is demonstrated through the\nquantitative evaluation of the parametric dual projection method, identified\nregulatory genes and desired hybrids in the case study, and positive feedback\nfrom breeders.", "published": "2025-07-16 02:25:31", "link": "http://arxiv.org/abs/2507.11848v1", "categories": ["cs.HC", "cs.AI", "q-bio.QM"], "primary_category": "cs.HC"}
{"title": "MNIST-Gen: A Modular MNIST-Style Dataset Generation Using Hierarchical Semantics, Reinforcement Learning, and Category Theory", "abstract": "Neural networks are often benchmarked using standard datasets such as MNIST,\nFashionMNIST, or other variants of MNIST, which, while accessible, are limited\nto generic classes such as digits or clothing items. For researchers working on\ndomain-specific tasks, such as classifying trees, food items, or other\nreal-world objects, these data sets are insufficient and irrelevant.\nAdditionally, creating and publishing a custom dataset can be time consuming,\nlegally constrained, or beyond the scope of individual projects. We present\nMNIST-Gen, an automated, modular, and adaptive framework for generating\nMNIST-style image datasets tailored to user-specified categories using\nhierarchical semantic categorization. The system combines CLIP-based semantic\nunderstanding with reinforcement learning and human feedback to achieve\nintelligent categorization with minimal manual intervention. Our hierarchical\napproach supports complex category structures with semantic characteristics,\nenabling fine-grained subcategorization and multiple processing modes:\nindividual review for maximum control, smart batch processing for large\ndatasets, and fast batch processing for rapid creation. Inspired by category\ntheory, MNIST-Gen models each data transformation stage as a composable\nmorphism, enhancing clarity, modularity, and extensibility. As proof of\nconcept, we generate and benchmark two novel datasets-\\textit{Tree-MNIST} and\n\\textit{Food-MNIST}-demonstrating MNIST-Gen's utility for producing\ntask-specific evaluation data while achieving 85\\% automatic categorization\naccuracy and 80\\% time savings compared to manual approaches.", "published": "2025-07-16 00:50:09", "link": "http://arxiv.org/abs/2507.11821v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.HC"], "primary_category": "cs.LG"}
{"title": "The Evolving Role of Large Language Models in Scientific Innovation: Evaluator, Collaborator, and Scientist", "abstract": "Scientific innovation is undergoing a paradigm shift driven by the rapid\nadvancement of Large Language Models (LLMs). As science faces mounting\nchallenges including information overload, disciplinary silos, and diminishing\nreturns on conventional research methods, LLMs are emerging as powerful agents\ncapable not only of enhancing scientific workflows but also of participating in\nand potentially leading the innovation process. Existing surveys mainly focus\non different perspectives, phrases, and tasks in scientific research and\ndiscovery, while they have limitations in understanding the transformative\npotential and role differentiation of LLM. This survey proposes a comprehensive\nframework to categorize the evolving roles of LLMs in scientific innovation\nacross three hierarchical levels: Evaluator, Collaborator, and Scientist. We\ndistinguish between LLMs' contributions to structured scientific research\nprocesses and open-ended scientific discovery, thereby offering a unified\ntaxonomy that clarifies capability boundaries, evaluation criteria, and\nhuman-AI interaction patterns at each level. Through an extensive analysis of\ncurrent methodologies, benchmarks, systems, and evaluation metrics, this survey\ndelivers an in-depth and systematic synthesis on LLM-driven scientific\ninnovation. We present LLMs not only as tools for automating existing\nprocesses, but also as catalysts capable of reshaping the epistemological\nfoundations of science itself. This survey offers conceptual clarity, practical\nguidance, and theoretical foundations for future research, while also\nhighlighting open challenges and ethical considerations in the pursuit of\nincreasingly autonomous AI-driven science. Resources related to this survey can\nbe accessed on GitHub at: https://github.com/haoxuan-unt2024/llm4innovation.", "published": "2025-07-16 00:11:01", "link": "http://arxiv.org/abs/2507.11810v1", "categories": ["cs.DL", "cs.AI"], "primary_category": "cs.DL"}
{"title": "CLID-MU: Cross-Layer Information Divergence Based Meta Update Strategy for Learning with Noisy Labels", "abstract": "Learning with noisy labels (LNL) is essential for training deep neural\nnetworks with imperfect data. Meta-learning approaches have achieved success by\nusing a clean unbiased labeled set to train a robust model. However, this\napproach heavily depends on the availability of a clean labeled meta-dataset,\nwhich is difficult to obtain in practice. In this work, we thus tackle the\nchallenge of meta-learning for noisy label scenarios without relying on a clean\nlabeled dataset. Our approach leverages the data itself while bypassing the\nneed for labels. Building on the insight that clean samples effectively\npreserve the consistency of related data structures across the last hidden and\nthe final layer, whereas noisy samples disrupt this consistency, we design the\nCross-layer Information Divergence-based Meta Update Strategy (CLID-MU).\nCLID-MU leverages the alignment of data structures across these diverse feature\nspaces to evaluate model performance and use this alignment to guide training.\nExperiments on benchmark datasets with varying amounts of labels under both\nsynthetic and real-world noise demonstrate that CLID-MU outperforms\nstate-of-the-art methods. The code is released at\nhttps://github.com/ruofanhu/CLID-MU.", "published": "2025-07-16 00:03:07", "link": "http://arxiv.org/abs/2507.11807v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "PhysX: Physical-Grounded 3D Asset Generation", "abstract": "3D modeling is moving from virtual to physical. Existing 3D generation\nprimarily emphasizes geometries and textures while neglecting physical-grounded\nmodeling. Consequently, despite the rapid development of 3D generative models,\nthe synthesized 3D assets often overlook rich and important physical\nproperties, hampering their real-world application in physical domains like\nsimulation and embodied AI. As an initial attempt to address this challenge, we\npropose \\textbf{PhysX}, an end-to-end paradigm for physical-grounded 3D asset\ngeneration. 1) To bridge the critical gap in physics-annotated 3D datasets, we\npresent PhysXNet - the first physics-grounded 3D dataset systematically\nannotated across five foundational dimensions: absolute scale, material,\naffordance, kinematics, and function description. In particular, we devise a\nscalable human-in-the-loop annotation pipeline based on vision-language models,\nwhich enables efficient creation of physics-first assets from raw 3D assets.2)\nFurthermore, we propose \\textbf{PhysXGen}, a feed-forward framework for\nphysics-grounded image-to-3D asset generation, injecting physical knowledge\ninto the pre-trained 3D structural space. Specifically, PhysXGen employs a\ndual-branch architecture to explicitly model the latent correlations between 3D\nstructures and physical properties, thereby producing 3D assets with plausible\nphysical predictions while preserving the native geometry quality. Extensive\nexperiments validate the superior performance and promising generalization\ncapability of our framework. All the code, data, and models will be released to\nfacilitate future research in generative physical AI.", "published": "2025-07-16 17:59:35", "link": "http://arxiv.org/abs/2507.12465v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "CytoSAE: Interpretable Cell Embeddings for Hematology", "abstract": "Sparse autoencoders (SAEs) emerged as a promising tool for mechanistic\ninterpretability of transformer-based foundation models. Very recently, SAEs\nwere also adopted for the visual domain, enabling the discovery of visual\nconcepts and their patch-wise attribution to tokens in the transformer model.\nWhile a growing number of foundation models emerged for medical imaging, tools\nfor explaining their inferences are still lacking. In this work, we show the\napplicability of SAEs for hematology. We propose CytoSAE, a sparse autoencoder\nwhich is trained on over 40,000 peripheral blood single-cell images. CytoSAE\ngeneralizes to diverse and out-of-domain datasets, including bone marrow\ncytology, where it identifies morphologically relevant concepts which we\nvalidated with medical experts. Furthermore, we demonstrate scenarios in which\nCytoSAE can generate patient-specific and disease-specific concepts, enabling\nthe detection of pathognomonic cells and localized cellular abnormalities at\nthe patch level. We quantified the effect of concepts on a patient-level AML\nsubtype classification task and show that CytoSAE concepts reach performance\ncomparable to the state-of-the-art, while offering explainability on the\nsub-cellular level. Source code and model weights are available at\nhttps://github.com/dynamical-inference/cytosae.", "published": "2025-07-16 17:59:32", "link": "http://arxiv.org/abs/2507.12464v1", "categories": ["cs.CV", "cs.LG", "q-bio.QM"], "primary_category": "cs.CV"}
{"title": "MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior Understanding", "abstract": "Humans are integral components of the transportation ecosystem, and\nunderstanding their behaviors is crucial to facilitating the development of\nsafe driving systems. Although recent progress has explored various aspects of\nhuman behavior$\\unicode{x2014}$such as motion, trajectories, and\nintention$\\unicode{x2014}$a comprehensive benchmark for evaluating human\nbehavior understanding in autonomous driving remains unavailable. In this work,\nwe propose $\\textbf{MMHU}$, a large-scale benchmark for human behavior analysis\nfeaturing rich annotations, such as human motion and trajectories, text\ndescription for human motions, human intention, and critical behavior labels\nrelevant to driving safety. Our dataset encompasses 57k human motion clips and\n1.73M frames gathered from diverse sources, including established driving\ndatasets such as Waymo, in-the-wild videos from YouTube, and self-collected\ndata. A human-in-the-loop annotation pipeline is developed to generate rich\nbehavior captions. We provide a thorough dataset analysis and benchmark\nmultiple tasks$\\unicode{x2014}$ranging from motion prediction to motion\ngeneration and human behavior question answering$\\unicode{x2014}$thereby\noffering a broad evaluation suite. Project page :\nhttps://MMHU-Benchmark.github.io.", "published": "2025-07-16 17:59:30", "link": "http://arxiv.org/abs/2507.12463v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "SpatialTrackerV2: 3D Point Tracking Made Easy", "abstract": "We present SpatialTrackerV2, a feed-forward 3D point tracking method for\nmonocular videos. Going beyond modular pipelines built on off-the-shelf\ncomponents for 3D tracking, our approach unifies the intrinsic connections\nbetween point tracking, monocular depth, and camera pose estimation into a\nhigh-performing and feedforward 3D point tracker. It decomposes world-space 3D\nmotion into scene geometry, camera ego-motion, and pixel-wise object motion,\nwith a fully differentiable and end-to-end architecture, allowing scalable\ntraining across a wide range of datasets, including synthetic sequences, posed\nRGB-D videos, and unlabeled in-the-wild footage. By learning geometry and\nmotion jointly from such heterogeneous data, SpatialTrackerV2 outperforms\nexisting 3D tracking methods by 30%, and matches the accuracy of leading\ndynamic 3D reconstruction approaches while running 50$\\times$ faster.", "published": "2025-07-16 17:59:03", "link": "http://arxiv.org/abs/2507.12462v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Mitigating Object Hallucinations via Sentence-Level Early Intervention", "abstract": "Multimodal large language models (MLLMs) have revolutionized cross-modal\nunderstanding but continue to struggle with hallucinations - fabricated content\ncontradicting visual inputs. Existing hallucination mitigation methods either\nincur prohibitive computational costs or introduce distribution mismatches\nbetween training data and model outputs. We identify a critical insight:\nhallucinations predominantly emerge at the early stages of text generation and\npropagate through subsequent outputs. To address this, we propose **SENTINEL**\n(**S**entence-level **E**arly i**N**tervention **T**hrough **IN**-domain\npr**E**ference **L**earning), a framework that eliminates dependency on human\nannotations. Specifically, we first bootstrap high-quality in-domain preference\npairs by iteratively sampling model outputs, validating object existence\nthrough cross-checking with two open-vocabulary detectors, and classifying\nsentences into hallucinated/non-hallucinated categories. Subsequently, we use\ncontext-coherent positive samples and hallucinated negative samples to build\ncontext-aware preference data iteratively. Finally, we train models using a\ncontext-aware preference loss (C-DPO) that emphasizes discriminative learning\nat the sentence level where hallucinations initially manifest. Experimental\nresults show that SENTINEL can reduce hallucinations by over 90\\% compared to\nthe original model and outperforms the previous state-of-the-art method on both\nhallucination benchmarks and general capabilities benchmarks, demonstrating its\nsuperiority and generalization ability. The models, datasets, and code are\navailable at https://github.com/pspdada/SENTINEL.", "published": "2025-07-16 17:55:43", "link": "http://arxiv.org/abs/2507.12455v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Vision-based Perception for Autonomous Vehicles in Obstacle Avoidance Scenarios", "abstract": "Obstacle avoidance is essential for ensuring the safety of autonomous\nvehicles. Accurate perception and motion planning are crucial to enabling\nvehicles to navigate complex environments while avoiding collisions. In this\npaper, we propose an efficient obstacle avoidance pipeline that leverages a\ncamera-only perception module and a Frenet-Pure Pursuit-based planning\nstrategy. By integrating advancements in computer vision, the system utilizes\nYOLOv11 for object detection and state-of-the-art monocular depth estimation\nmodels, such as Depth Anything V2, to estimate object distances. A comparative\nanalysis of these models provides valuable insights into their accuracy,\nefficiency, and robustness in real-world conditions. The system is evaluated in\ndiverse scenarios on a university campus, demonstrating its effectiveness in\nhandling various obstacles and enhancing autonomous navigation. The video\npresenting the results of the obstacle avoidance experiments is available at:\nhttps://www.youtube.com/watch?v=FoXiO5S_tA8", "published": "2025-07-16 17:41:14", "link": "http://arxiv.org/abs/2507.12449v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Describe Anything Model for Visual Question Answering on Text-rich Images", "abstract": "Recent progress has been made in region-aware vision-language modeling,\nparticularly with the emergence of the Describe Anything Model (DAM). DAM is\ncapable of generating detailed descriptions of any specific image areas or\nobjects without the need for additional localized image-text alignment\nsupervision. We hypothesize that such region-level descriptive capability is\nbeneficial for the task of Visual Question Answering (VQA), especially in\nchallenging scenarios involving images with dense text. In such settings, the\nfine-grained extraction of textual information is crucial to producing correct\nanswers. Motivated by this, we introduce DAM-QA, a framework with a tailored\nevaluation protocol, developed to investigate and harness the region-aware\ncapabilities from DAM for the text-rich VQA problem that requires reasoning\nover text-based information within images. DAM-QA incorporates a mechanism that\naggregates answers from multiple regional views of image content, enabling more\neffective identification of evidence that may be tied to text-related elements.\nExperiments on six VQA benchmarks show that our approach consistently\noutperforms the baseline DAM, with a notable 7+ point gain on DocVQA. DAM-QA\nalso achieves the best overall performance among region-aware models with fewer\nparameters, significantly narrowing the gap with strong generalist VLMs. These\nresults highlight the potential of DAM-like models for text-rich and broader\nVQA tasks when paired with efficient usage and integration strategies. Our code\nis publicly available at https://github.com/Linvyl/DAM-QA.git.", "published": "2025-07-16 17:28:19", "link": "http://arxiv.org/abs/2507.12441v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Traffic-Aware Pedestrian Intention Prediction", "abstract": "Accurate pedestrian intention estimation is crucial for the safe navigation\nof autonomous vehicles (AVs) and hence attracts a lot of research attention.\nHowever, current models often fail to adequately consider dynamic traffic\nsignals and contextual scene information, which are critical for real-world\napplications. This paper presents a Traffic-Aware Spatio-Temporal Graph\nConvolutional Network (TA-STGCN) that integrates traffic signs and their states\n(Red, Yellow, Green) into pedestrian intention prediction. Our approach\nintroduces the integration of dynamic traffic signal states and bounding box\nsize as key features, allowing the model to capture both spatial and temporal\ndependencies in complex urban environments. The model surpasses existing\nmethods in accuracy. Specifically, TA-STGCN achieves a 4.75% higher accuracy\ncompared to the baseline model on the PIE dataset, demonstrating its\neffectiveness in improving pedestrian intention prediction.", "published": "2025-07-16 17:20:36", "link": "http://arxiv.org/abs/2507.12433v1", "categories": ["cs.CV", "cs.SY", "eess.SY", "I.2.10; I.5.1"], "primary_category": "cs.CV"}
{"title": "DVFL-Net: A Lightweight Distilled Video Focal Modulation Network for Spatio-Temporal Action Recognition", "abstract": "The landscape of video recognition has evolved significantly, shifting from\ntraditional Convolutional Neural Networks (CNNs) to Transformer-based\narchitectures for improved accuracy. While 3D CNNs have been effective at\ncapturing spatiotemporal dynamics, recent Transformer models leverage\nself-attention to model long-range spatial and temporal dependencies. Despite\nachieving state-of-the-art performance on major benchmarks, Transformers remain\ncomputationally expensive, particularly with dense video data. To address this,\nwe propose a lightweight Video Focal Modulation Network, DVFL-Net, which\ndistills spatiotemporal knowledge from a large pre-trained teacher into a\ncompact nano student model, enabling efficient on-device deployment. DVFL-Net\nutilizes knowledge distillation and spatial-temporal feature modulation to\nsignificantly reduce computation while preserving high recognition performance.\nWe employ forward Kullback-Leibler (KL) divergence alongside spatio-temporal\nfocal modulation to effectively transfer both local and global context from the\nVideo-FocalNet Base (teacher) to the proposed VFL-Net (student). We evaluate\nDVFL-Net on UCF50, UCF101, HMDB51, SSV2, and Kinetics-400, benchmarking it\nagainst recent state-of-the-art methods in Human Action Recognition (HAR).\nAdditionally, we conduct a detailed ablation study analyzing the impact of\nforward KL divergence. The results confirm the superiority of DVFL-Net in\nachieving an optimal balance between performance and efficiency, demonstrating\nlower memory usage, reduced GFLOPs, and strong accuracy, making it a practical\nsolution for real-time HAR applications.", "published": "2025-07-16 17:15:06", "link": "http://arxiv.org/abs/2507.12426v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "InterpIoU: Rethinking Bounding Box Regression with Interpolation-Based IoU Optimization", "abstract": "Bounding box regression (BBR) is fundamental to object detection, where the\nregression loss is crucial for accurate localization. Existing IoU-based losses\noften incorporate handcrafted geometric penalties to address IoU's\nnon-differentiability in non-overlapping cases and enhance BBR performance.\nHowever, these penalties are sensitive to box shape, size, and distribution,\noften leading to suboptimal optimization for small objects and undesired\nbehaviors such as bounding box enlargement due to misalignment with the IoU\nobjective. To address these limitations, we propose InterpIoU, a novel loss\nfunction that replaces handcrafted geometric penalties with a term based on the\nIoU between interpolated boxes and the target. By using interpolated boxes to\nbridge the gap between predictions and ground truth, InterpIoU provides\nmeaningful gradients in non-overlapping cases and inherently avoids the box\nenlargement issue caused by misaligned penalties. Simulation results further\nshow that IoU itself serves as an ideal regression target, while existing\ngeometric penalties are both unnecessary and suboptimal. Building on InterpIoU,\nwe introduce Dynamic InterpIoU, which dynamically adjusts interpolation\ncoefficients based on IoU values, enhancing adaptability to scenarios with\ndiverse object distributions. Experiments on COCO, VisDrone, and PASCAL VOC\nshow that our methods consistently outperform state-of-the-art IoU-based losses\nacross various detection frameworks, with particularly notable improvements in\nsmall object detection, confirming their effectiveness.", "published": "2025-07-16 17:09:04", "link": "http://arxiv.org/abs/2507.12420v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Spontaneous Spatial Cognition Emerges during Egocentric Video Viewing through Non-invasive BCI", "abstract": "Humans possess a remarkable capacity for spatial cognition, allowing for\nself-localization even in novel or unfamiliar environments. While hippocampal\nneurons encoding position and orientation are well documented, the large-scale\nneural dynamics supporting spatial representation, particularly during\nnaturalistic, passive experience, remain poorly understood. Here, we\ndemonstrate for the first time that non-invasive brain-computer interfaces\n(BCIs) based on electroencephalography (EEG) can decode spontaneous,\nfine-grained egocentric 6D pose, comprising three-dimensional position and\norientation, during passive viewing of egocentric video. Despite EEG's limited\nspatial resolution and high signal noise, we find that spatially coherent\nvisual input (i.e., continuous and structured motion) reliably evokes decodable\nspatial representations, aligning with participants' subjective sense of\nspatial engagement. Decoding performance further improves when visual input is\npresented at a frame rate of 100 ms per image, suggesting alignment with\nintrinsic neural temporal dynamics. Using gradient-based backpropagation\nthrough a neural decoding model, we identify distinct EEG channels contributing\nto position -- and orientation specific -- components, revealing a distributed\nyet complementary neural encoding scheme. These findings indicate that the\nbrain's spatial systems operate spontaneously and continuously, even under\npassive conditions, challenging traditional distinctions between active and\npassive spatial cognition. Our results offer a non-invasive window into the\nautomatic construction of egocentric spatial maps and advance our understanding\nof how the human mind transforms everyday sensory experience into structured\ninternal representations.", "published": "2025-07-16 17:07:57", "link": "http://arxiv.org/abs/2507.12417v1", "categories": ["q-bio.NC", "cs.CV", "eess.SP"], "primary_category": "q-bio.NC"}
{"title": "OD-VIRAT: A Large-Scale Benchmark for Object Detection in Realistic Surveillance Environments", "abstract": "Realistic human surveillance datasets are crucial for training and evaluating\ncomputer vision models under real-world conditions, facilitating the\ndevelopment of robust algorithms for human and human-interacting object\ndetection in complex environments. These datasets need to offer diverse and\nchallenging data to enable a comprehensive assessment of model performance and\nthe creation of more reliable surveillance systems for public safety. To this\nend, we present two visual object detection benchmarks named OD-VIRAT Large and\nOD-VIRAT Tiny, aiming at advancing visual understanding tasks in surveillance\nimagery. The video sequences in both benchmarks cover 10 different scenes of\nhuman surveillance recorded from significant height and distance. The proposed\nbenchmarks offer rich annotations of bounding boxes and categories, where\nOD-VIRAT Large has 8.7 million annotated instances in 599,996 images and\nOD-VIRAT Tiny has 288,901 annotated instances in 19,860 images. This work also\nfocuses on benchmarking state-of-the-art object detection architectures,\nincluding RETMDET, YOLOX, RetinaNet, DETR, and Deformable-DETR on this object\ndetection-specific variant of VIRAT dataset. To the best of our knowledge, it\nis the first work to examine the performance of these recently published\nstate-of-the-art object detection architectures on realistic surveillance\nimagery under challenging conditions such as complex backgrounds, occluded\nobjects, and small-scale objects. The proposed benchmarking and experimental\nsettings will help in providing insights concerning the performance of selected\nobject detection models and set the base for developing more efficient and\nrobust object detection architectures.", "published": "2025-07-16 16:41:47", "link": "http://arxiv.org/abs/2507.12396v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Text-driven Multiplanar Visual Interaction for Semi-supervised Medical Image Segmentation", "abstract": "Semi-supervised medical image segmentation is a crucial technique for\nalleviating the high cost of data annotation. When labeled data is limited,\ntextual information can provide additional context to enhance visual semantic\nunderstanding. However, research exploring the use of textual data to enhance\nvisual semantic embeddings in 3D medical imaging tasks remains scarce. In this\npaper, we propose a novel text-driven multiplanar visual interaction framework\nfor semi-supervised medical image segmentation (termed Text-SemiSeg), which\nconsists of three main modules: Text-enhanced Multiplanar Representation (TMR),\nCategory-aware Semantic Alignment (CSA), and Dynamic Cognitive Augmentation\n(DCA). Specifically, TMR facilitates text-visual interaction through planar\nmapping, thereby enhancing the category awareness of visual features. CSA\nperforms cross-modal semantic alignment between the text features with\nintroduced learnable variables and the intermediate layer of visual features.\nDCA reduces the distribution discrepancy between labeled and unlabeled data\nthrough their interaction, thus improving the model's robustness. Finally,\nexperiments on three public datasets demonstrate that our model effectively\nenhances visual features with textual information and outperforms other\nmethods. Our code is available at https://github.com/taozh2017/Text-SemiSeg.", "published": "2025-07-16 16:29:30", "link": "http://arxiv.org/abs/2507.12382v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Improving Lightweight Weed Detection via Knowledge Distillation", "abstract": "Weed detection is a critical component of precision agriculture, facilitating\ntargeted herbicide application and reducing environmental impact. However,\ndeploying accurate object detection models on resource-limited platforms\nremains challenging, particularly when differentiating visually similar weed\nspecies commonly encountered in plant phenotyping applications. In this work,\nwe investigate Channel-wise Knowledge Distillation (CWD) and Masked Generative\nDistillation (MGD) to enhance the performance of lightweight models for\nreal-time smart spraying systems. Utilizing YOLO11x as the teacher model and\nYOLO11n as both reference and student, both CWD and MGD effectively transfer\nknowledge from the teacher to the student model. Our experiments, conducted on\na real-world dataset comprising sugar beet crops and four weed types (Cirsium,\nConvolvulus, Fallopia, and Echinochloa), consistently show increased AP50\nacross all classes. The distilled CWD student model achieves a notable\nimprovement of 2.5% and MGD achieves 1.9% in mAP50 over the baseline without\nincreasing model complexity. Additionally, we validate real-time deployment\nfeasibility by evaluating the student YOLO11n model on Jetson Orin Nano and\nRaspberry Pi 5 embedded devices, performing five independent runs to evaluate\nperformance stability across random seeds. These findings confirm CWD and MGD\nas an effective, efficient, and practical approach for improving deep\nlearning-based weed detection accuracy in precision agriculture and plant\nphenotyping scenarios.", "published": "2025-07-16 15:38:07", "link": "http://arxiv.org/abs/2507.12344v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Unsupervised Monocular 3D Keypoint Discovery from Multi-View Diffusion Priors", "abstract": "This paper introduces KeyDiff3D, a framework for unsupervised monocular 3D\nkeypoints estimation that accurately predicts 3D keypoints from a single image.\nWhile previous methods rely on manual annotations or calibrated multi-view\nimages, both of which are expensive to collect, our method enables monocular 3D\nkeypoints estimation using only a collection of single-view images. To achieve\nthis, we leverage powerful geometric priors embedded in a pretrained multi-view\ndiffusion model. In our framework, this model generates multi-view images from\na single image, serving as a supervision signal to provide 3D geometric cues to\nour model. We also use the diffusion model as a powerful 2D multi-view feature\nextractor and construct 3D feature volumes from its intermediate\nrepresentations. This transforms implicit 3D priors learned by the diffusion\nmodel into explicit 3D features. Beyond accurate keypoints estimation, we\nfurther introduce a pipeline that enables manipulation of 3D objects generated\nby the diffusion model. Experimental results on diverse aspects and datasets,\nincluding Human3.6M, Stanford Dogs, and several in-the-wild and out-of-domain\ndatasets, highlight the effectiveness of our method in terms of accuracy,\ngeneralization, and its ability to enable manipulation of 3D objects generated\nby the diffusion model from a single image.", "published": "2025-07-16 15:29:07", "link": "http://arxiv.org/abs/2507.12336v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "RegCL: Continual Adaptation of Segment Anything Model via Model Merging", "abstract": "To address the performance limitations of the Segment Anything Model (SAM) in\nspecific domains, existing works primarily adopt adapter-based one-step\nadaptation paradigms. However, some of these methods are specific developed for\nspecific domains. If used on other domains may lead to performance degradation.\nThis issue of catastrophic forgetting severely limits the model's scalability.\nTo address this issue, this paper proposes RegCL, a novel non-replay continual\nlearning (CL) framework designed for efficient multi-domain knowledge\nintegration through model merging. Specifically, RegCL incorporates the model\nmerging algorithm into the continual learning paradigm by merging the\nparameters of SAM's adaptation modules (e.g., LoRA modules) trained on\ndifferent domains. The merging process is guided by weight optimization, which\nminimizes prediction discrepancies between the merged model and each of the\ndomain-specific models. RegCL effectively consolidates multi-domain knowledge\nwhile maintaining parameter efficiency, i.e., the model size remains constant\nregardless of the number of tasks, and no historical data storage is required.\nExperimental results demonstrate that RegCL achieves favorable continual\nlearning performance across multiple downstream datasets, validating its\neffectiveness in dynamic scenarios.", "published": "2025-07-16 14:51:37", "link": "http://arxiv.org/abs/2507.12297v1", "categories": ["cs.LG", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Efficient Calisthenics Skills Classification through Foreground Instance Selection and Depth Estimation", "abstract": "Calisthenics skill classification is the computer vision task of inferring\nthe skill performed by an athlete from images, enabling automatic performance\nassessment and personalized analytics. Traditional methods for calisthenics\nskill recognition are based on pose estimation methods to determine the\nposition of skeletal data from images, which is later fed to a classification\nalgorithm to infer the performed skill. Despite the progress in human pose\nestimation algorithms, they still involve high computational costs, long\ninference times, and complex setups, which limit the applicability of such\napproaches in real-time applications or mobile devices. This work proposes a\ndirect approach to calisthenics skill recognition, which leverages depth\nestimation and athlete patch retrieval to avoid the computationally expensive\nhuman pose estimation module. Using Depth Anything V2 for depth estimation and\nYOLOv10 for athlete localization, we segment the subject from the background\nrather than relying on traditional pose estimation techniques. This strategy\nincreases efficiency, reduces inference time, and improves classification\naccuracy. Our approach significantly outperforms skeleton-based methods,\nachieving 38.3x faster inference with RGB image patches and improved\nclassification accuracy with depth patches (0.837 vs. 0.815). Beyond these\nperformance gains, the modular design of our pipeline allows for flexible\nreplacement of components, enabling future enhancements and adaptation to\nreal-world applications.", "published": "2025-07-16 14:44:29", "link": "http://arxiv.org/abs/2507.12292v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "FADE: Adversarial Concept Erasure in Flow Models", "abstract": "Diffusion models have demonstrated remarkable image generation capabilities,\nbut also pose risks in privacy and fairness by memorizing sensitive concepts or\nperpetuating biases. We propose a novel \\textbf{concept erasure} method for\ntext-to-image diffusion models, designed to remove specified concepts (e.g., a\nprivate individual or a harmful stereotype) from the model's generative\nrepertoire. Our method, termed \\textbf{FADE} (Fair Adversarial Diffusion\nErasure), combines a trajectory-aware fine-tuning strategy with an adversarial\nobjective to ensure the concept is reliably removed while preserving overall\nmodel fidelity. Theoretically, we prove a formal guarantee that our approach\nminimizes the mutual information between the erased concept and the model's\noutputs, ensuring privacy and fairness. Empirically, we evaluate FADE on Stable\nDiffusion and FLUX, using benchmarks from prior work (e.g., object, celebrity,\nexplicit content, and style erasure tasks from MACE). FADE achieves\nstate-of-the-art concept removal performance, surpassing recent baselines like\nESD, UCE, MACE, and ANT in terms of removal efficacy and image quality.\nNotably, FADE improves the harmonic mean of concept removal and fidelity by\n5--10\\% over the best prior method. We also conduct an ablation study to\nvalidate each component of FADE, confirming that our adversarial and\ntrajectory-preserving objectives each contribute to its superior performance.\nOur work sets a new standard for safe and fair generative modeling by\nunlearning specified concepts without retraining from scratch.", "published": "2025-07-16 14:31:21", "link": "http://arxiv.org/abs/2507.12283v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Comparative Analysis of CNN Performance in Keras, PyTorch and JAX on PathMNIST", "abstract": "Deep learning has significantly advanced the field of medical image\nclassification, particularly with the adoption of Convolutional Neural Networks\n(CNNs). Various deep learning frameworks such as Keras, PyTorch and JAX offer\nunique advantages in model development and deployment. However, their\ncomparative performance in medical imaging tasks remains underexplored. This\nstudy presents a comprehensive analysis of CNN implementations across these\nframeworks, using the PathMNIST dataset as a benchmark. We evaluate training\nefficiency, classification accuracy and inference speed to assess their\nsuitability for real-world applications. Our findings highlight the trade-offs\nbetween computational speed and model accuracy, offering valuable insights for\nresearchers and practitioners in medical image analysis.", "published": "2025-07-16 13:57:50", "link": "http://arxiv.org/abs/2507.12248v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Calisthenics Skills Temporal Video Segmentation", "abstract": "Calisthenics is a fast-growing bodyweight discipline that consists of\ndifferent categories, one of which is focused on skills. Skills in calisthenics\nencompass both static and dynamic elements performed by athletes. The\nevaluation of static skills is based on their difficulty level and the duration\nof the hold. Automated tools able to recognize isometric skills from a video by\nsegmenting them to estimate their duration would be desirable to assist\nathletes in their training and judges during competitions. Although the video\nunderstanding literature on action recognition through body pose analysis is\nrich, no previous work has specifically addressed the problem of calisthenics\nskill temporal video segmentation. This study aims to provide an initial step\ntowards the implementation of automated tools within the field of Calisthenics.\nTo advance knowledge in this context, we propose a dataset of video footage of\nstatic calisthenics skills performed by athletes. Each video is annotated with\na temporal segmentation which determines the extent of each skill. We hence\nreport the results of a baseline approach to address the problem of skill\ntemporal segmentation on the proposed dataset. The results highlight the\nfeasibility of the proposed problem, while there is still room for improvement.", "published": "2025-07-16 13:55:27", "link": "http://arxiv.org/abs/2507.12245v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Generate to Ground: Multimodal Text Conditioning Boosts Phrase Grounding in Medical Vision-Language Models", "abstract": "Phrase grounding, i.e., mapping natural language phrases to specific image\nregions, holds significant potential for disease localization in medical\nimaging through clinical reports. While current state-of-the-art methods rely\non discriminative, self-supervised contrastive models, we demonstrate that\ngenerative text-to-image diffusion models, leveraging cross-attention maps, can\nachieve superior zero-shot phrase grounding performance. Contrary to prior\nassumptions, we show that fine-tuning diffusion models with a frozen,\ndomain-specific language model, such as CXR-BERT, substantially outperforms\ndomain-agnostic counterparts. This setup achieves remarkable improvements, with\nmIoU scores doubling those of current discriminative methods. These findings\nhighlight the underexplored potential of generative models for phrase grounding\ntasks. To further enhance performance, we introduce Bimodal Bias Merging (BBM),\na novel post-processing technique that aligns text and image biases to identify\nregions of high certainty. BBM refines cross-attention maps, achieving even\ngreater localization accuracy. Our results establish generative approaches as a\nmore effective paradigm for phrase grounding in the medical imaging domain,\npaving the way for more robust and interpretable applications in clinical\npractice. The source code and model weights are available at\nhttps://github.com/Felix-012/generate_to_ground.", "published": "2025-07-16 13:48:32", "link": "http://arxiv.org/abs/2507.12236v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "MGFFD-VLM: Multi-Granularity Prompt Learning for Face Forgery Detection with VLM", "abstract": "Recent studies have utilized visual large language models (VLMs) to answer\nnot only \"Is this face a forgery?\" but also \"Why is the face a forgery?\" These\nstudies introduced forgery-related attributes, such as forgery location and\ntype, to construct deepfake VQA datasets and train VLMs, achieving high\naccuracy while providing human-understandable explanatory text descriptions.\nHowever, these methods still have limitations. For example, they do not fully\nleverage face quality-related attributes, which are often abnormal in forged\nfaces, and they lack effective training strategies for forgery-aware VLMs. In\nthis paper, we extend the VQA dataset to create DD-VQA+, which features a\nricher set of attributes and a more diverse range of samples. Furthermore, we\nintroduce a novel forgery detection framework, MGFFD-VLM, which integrates an\nAttribute-Driven Hybrid LoRA Strategy to enhance the capabilities of Visual\nLarge Language Models (VLMs). Additionally, our framework incorporates\nMulti-Granularity Prompt Learning and a Forgery-Aware Training Strategy. By\ntransforming classification and forgery segmentation results into prompts, our\nmethod not only improves forgery classification but also enhances\ninterpretability. To further boost detection performance, we design multiple\nforgery-related auxiliary losses. Experimental results demonstrate that our\napproach surpasses existing methods in both text-based forgery judgment and\nanalysis, achieving superior accuracy.", "published": "2025-07-16 13:47:13", "link": "http://arxiv.org/abs/2507.12232v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "RODS: Robust Optimization Inspired Diffusion Sampling for Detecting and Reducing Hallucination in Generative Models", "abstract": "Diffusion models have achieved state-of-the-art performance in generative\nmodeling, yet their sampling procedures remain vulnerable to hallucinations,\noften stemming from inaccuracies in score approximation. In this work, we\nreinterpret diffusion sampling through the lens of optimization and introduce\nRODS (Robust Optimization-inspired Diffusion Sampler), a novel method that\ndetects and corrects high-risk sampling steps using geometric cues from the\nloss landscape. RODS enforces smoother sampling trajectories and adaptively\nadjusts perturbations, reducing hallucinations without retraining and at\nminimal additional inference cost. Experiments on AFHQv2, FFHQ, and 11k-hands\ndemonstrate that RODS improves both sampling fidelity and robustness, detecting\nover 70% of hallucinated samples and correcting more than 25%, all while\navoiding the introduction of new artifacts.", "published": "2025-07-16 12:55:58", "link": "http://arxiv.org/abs/2507.12201v1", "categories": ["cs.CV", "math.OC"], "primary_category": "cs.CV"}
{"title": "Hybrid Ensemble Approaches: Optimal Deep Feature Fusion and Hyperparameter-Tuned Classifier Ensembling for Enhanced Brain Tumor Classification", "abstract": "Magnetic Resonance Imaging (MRI) is widely recognized as the most reliable\ntool for detecting tumors due to its capability to produce detailed images that\nreveal their presence. However, the accuracy of diagnosis can be compromised\nwhen human specialists evaluate these images. Factors such as fatigue, limited\nexpertise, and insufficient image detail can lead to errors. For example, small\ntumors might go unnoticed, or overlap with healthy brain regions could result\nin misidentification. To address these challenges and enhance diagnostic\nprecision, this study proposes a novel double ensembling framework, consisting\nof ensembled pre-trained deep learning (DL) models for feature extraction and\nensembled fine-tuned hyperparameter machine learning (ML) models to efficiently\nclassify brain tumors. Specifically, our method includes extensive\npreprocessing and augmentation, transfer learning concepts by utilizing various\npre-trained deep convolutional neural networks and vision transformer networks\nto extract deep features from brain MRI, and fine-tune hyperparameters of ML\nclassifiers. Our experiments utilized three different publicly available Kaggle\nMRI brain tumor datasets to evaluate the pre-trained DL feature extractor\nmodels, ML classifiers, and the effectiveness of an ensemble of deep features\nalong with an ensemble of ML classifiers for brain tumor classification. Our\nresults indicate that the proposed feature fusion and classifier fusion improve\nupon the state of the art, with hyperparameter fine-tuning providing a\nsignificant enhancement over the ensemble method. Additionally, we present an\nablation study to illustrate how each component contributes to accurate brain\ntumor classification.", "published": "2025-07-16 12:22:11", "link": "http://arxiv.org/abs/2507.12177v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Fine-Grained Image Recognition from Scratch with Teacher-Guided Data Augmentation", "abstract": "Fine-grained image recognition (FGIR) aims to distinguish visually similar\nsub-categories within a broader class, such as identifying bird species. While\nmost existing FGIR methods rely on backbones pretrained on large-scale datasets\nlike ImageNet, this dependence limits adaptability to resource-constrained\nenvironments and hinders the development of task-specific architectures\ntailored to the unique challenges of FGIR.\n  In this work, we challenge the conventional reliance on pretrained models by\ndemonstrating that high-performance FGIR systems can be trained entirely from\nscratch. We introduce a novel training framework, TGDA, that integrates\ndata-aware augmentation with weak supervision via a fine-grained-aware teacher\nmodel, implemented through knowledge distillation. This framework unlocks the\ndesign of task-specific and hardware-aware architectures, including LRNets for\nlow-resolution FGIR and ViTFS, a family of Vision Transformers optimized for\nefficient inference.\n  Extensive experiments across three FGIR benchmarks over diverse settings\ninvolving low-resolution and high-resolution inputs show that our method\nconsistently matches or surpasses state-of-the-art pretrained counterparts. In\nparticular, in the low-resolution setting, LRNets trained with TGDA improve\naccuracy by up to 23\\% over prior methods while requiring up to 20.6x less\nparameters, lower FLOPs, and significantly less training data. Similarly,\nViTFS-T can match the performance of a ViT B-16 pretrained on ImageNet-21k\nwhile using 15.3x fewer trainable parameters and requiring orders of magnitudes\nless data. These results highlight TGDA's potential as an adaptable alternative\nto pretraining, paving the way for more efficient fine-grained vision systems.", "published": "2025-07-16 11:37:33", "link": "http://arxiv.org/abs/2507.12157v1", "categories": ["cs.CV", "I.2; I.4"], "primary_category": "cs.CV"}
{"title": "Neural Human Pose Prior", "abstract": "We introduce a principled, data-driven approach for modeling a neural prior\nover human body poses using normalizing flows. Unlike heuristic or\nlow-expressivity alternatives, our method leverages RealNVP to learn a flexible\ndensity over poses represented in the 6D rotation format. We address the\nchallenge of modeling distributions on the manifold of valid 6D rotations by\ninverting the Gram-Schmidt process during training, enabling stable learning\nwhile preserving downstream compatibility with rotation-based frameworks. Our\narchitecture and training pipeline are framework-agnostic and easily\nreproducible. We demonstrate the effectiveness of the learned prior through\nboth qualitative and quantitative evaluations, and we analyze its impact via\nablation studies. This work provides a sound probabilistic foundation for\nintegrating pose priors into human motion capture and reconstruction pipelines.", "published": "2025-07-16 11:11:18", "link": "http://arxiv.org/abs/2507.12138v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "AD-GS: Object-Aware B-Spline Gaussian Splatting for Self-Supervised Autonomous Driving", "abstract": "Modeling and rendering dynamic urban driving scenes is crucial for\nself-driving simulation. Current high-quality methods typically rely on costly\nmanual object tracklet annotations, while self-supervised approaches fail to\ncapture dynamic object motions accurately and decompose scenes properly,\nresulting in rendering artifacts. We introduce AD-GS, a novel self-supervised\nframework for high-quality free-viewpoint rendering of driving scenes from a\nsingle log. At its core is a novel learnable motion model that integrates\nlocality-aware B-spline curves with global-aware trigonometric functions,\nenabling flexible yet precise dynamic object modeling. Rather than requiring\ncomprehensive semantic labeling, AD-GS automatically segments scenes into\nobjects and background with the simplified pseudo 2D segmentation, representing\nobjects using dynamic Gaussians and bidirectional temporal visibility masks.\nFurther, our model incorporates visibility reasoning and physically rigid\nregularization to enhance robustness. Extensive evaluations demonstrate that\nour annotation-free model significantly outperforms current state-of-the-art\nannotation-free methods and is competitive with annotation-dependent\napproaches.", "published": "2025-07-16 11:10:57", "link": "http://arxiv.org/abs/2507.12137v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Learning Pixel-adaptive Multi-layer Perceptrons for Real-time Image Enhancement", "abstract": "Deep learning-based bilateral grid processing has emerged as a promising\nsolution for image enhancement, inherently encoding spatial and intensity\ninformation while enabling efficient full-resolution processing through slicing\noperations. However, existing approaches are limited to linear affine\ntransformations, hindering their ability to model complex color relationships.\nMeanwhile, while multi-layer perceptrons (MLPs) excel at non-linear mappings,\ntraditional MLP-based methods employ globally shared parameters, which is hard\nto deal with localized variations. To overcome these dual challenges, we\npropose a Bilateral Grid-based Pixel-Adaptive Multi-layer Perceptron (BPAM)\nframework. Our approach synergizes the spatial modeling of bilateral grids with\nthe non-linear capabilities of MLPs. Specifically, we generate bilateral grids\ncontaining MLP parameters, where each pixel dynamically retrieves its unique\ntransformation parameters and obtain a distinct MLP for color mapping based on\nspatial coordinates and intensity values. In addition, we propose a novel grid\ndecomposition strategy that categorizes MLP parameters into distinct types\nstored in separate subgrids. Multi-channel guidance maps are used to extract\ncategory-specific parameters from corresponding subgrids, ensuring effective\nutilization of color information during slicing while guiding precise parameter\ngeneration. Extensive experiments on public datasets demonstrate that our\nmethod outperforms state-of-the-art methods in performance while maintaining\nreal-time processing capabilities.", "published": "2025-07-16 11:09:39", "link": "http://arxiv.org/abs/2507.12135v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "DoRF: Doppler Radiance Fields for Robust Human Activity Recognition Using Wi-Fi", "abstract": "Wi-Fi Channel State Information (CSI) has gained increasing interest for\nremote sensing applications. Recent studies show that Doppler velocity\nprojections extracted from CSI can enable human activity recognition (HAR) that\nis robust to environmental changes and generalizes to new users. However,\ndespite these advances, generalizability still remains insufficient for\npractical deployment. Inspired by neural radiance fields (NeRF), which learn a\nvolumetric representation of a 3D scene from 2D images, this work proposes a\nnovel approach to reconstruct an informative 3D latent motion representation\nfrom one-dimensional Doppler velocity projections extracted from Wi-Fi CSI. The\nresulting latent representation is then used to construct a uniform Doppler\nradiance field (DoRF) of the motion, providing a comprehensive view of the\nperformed activity and improving the robustness to environmental variability.\nThe results show that the proposed approach noticeably enhances the\ngeneralization accuracy of Wi-Fi-based HAR, highlighting the strong potential\nof DoRFs for practical sensing applications.", "published": "2025-07-16 11:00:46", "link": "http://arxiv.org/abs/2507.12132v1", "categories": ["eess.SP", "cs.CV"], "primary_category": "eess.SP"}
{"title": "Block-based Symmetric Pruning and Fusion for Efficient Vision Transformers", "abstract": "Vision Transformer (ViT) has achieved impressive results across various\nvision tasks, yet its high computational cost limits practical applications.\nRecent methods have aimed to reduce ViT's $O(n^2)$ complexity by pruning\nunimportant tokens. However, these techniques often sacrifice accuracy by\nindependently pruning query (Q) and key (K) tokens, leading to performance\ndegradation due to overlooked token interactions. To address this limitation,\nwe introduce a novel {\\bf Block-based Symmetric Pruning and Fusion} for\nefficient ViT (BSPF-ViT) that optimizes the pruning of Q/K tokens jointly.\nUnlike previous methods that consider only a single direction, our approach\nevaluates each token and its neighbors to decide which tokens to retain by\ntaking token interaction into account. The retained tokens are compressed\nthrough a similarity fusion step, preserving key information while reducing\ncomputational costs. The shared weights of Q/K tokens create a symmetric\nattention matrix, allowing pruning only the upper triangular part for speed up.\nBSPF-ViT consistently outperforms state-of-the-art ViT methods at all pruning\nlevels, increasing ImageNet classification accuracy by 1.3% on DeiT-T and 2.0%\non DeiT-S, while reducing computational overhead by 50%. It achieves 40%\nspeedup with improved accuracy across various ViTs.", "published": "2025-07-16 10:48:56", "link": "http://arxiv.org/abs/2507.12125v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Open-Vocabulary Indoor Object Grounding with 3D Hierarchical Scene Graph", "abstract": "We propose OVIGo-3DHSG method - Open-Vocabulary Indoor Grounding of objects\nusing 3D Hierarchical Scene Graph. OVIGo-3DHSG represents an extensive indoor\nenvironment over a Hierarchical Scene Graph derived from sequences of RGB-D\nframes utilizing a set of open-vocabulary foundation models and sensor data\nprocessing. The hierarchical representation explicitly models spatial relations\nacross floors, rooms, locations, and objects. To effectively address complex\nqueries involving spatial reference to other objects, we integrate the\nhierarchical scene graph with a Large Language Model for multistep reasoning.\nThis integration leverages inter-layer (e.g., room-to-object) and intra-layer\n(e.g., object-to-object) connections, enhancing spatial contextual\nunderstanding. We investigate the semantic and geometry accuracy of\nhierarchical representation on Habitat Matterport 3D Semantic multi-floor\nscenes. Our approach demonstrates efficient scene comprehension and robust\nobject grounding compared to existing methods. Overall OVIGo-3DHSG demonstrates\nstrong potential for applications requiring spatial reasoning and understanding\nof indoor environments. Related materials can be found at\nhttps://github.com/linukc/OVIGo-3DHSG.", "published": "2025-07-16 10:47:12", "link": "http://arxiv.org/abs/2507.12123v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "LidarPainter: One-Step Away From Any Lidar View To Novel Guidance", "abstract": "Dynamic driving scene reconstruction is of great importance in fields like\ndigital twin system and autonomous driving simulation. However, unacceptable\ndegradation occurs when the view deviates from the input trajectory, leading to\ncorrupted background and vehicle models. To improve reconstruction quality on\nnovel trajectory, existing methods are subject to various limitations including\ninconsistency, deformation, and time consumption. This paper proposes\nLidarPainter, a one-step diffusion model that recovers consistent driving views\nfrom sparse LiDAR condition and artifact-corrupted renderings in real-time,\nenabling high-fidelity lane shifts in driving scene reconstruction. Extensive\nexperiments show that LidarPainter outperforms state-of-the-art methods in\nspeed, quality and resource efficiency, specifically 7 x faster than\nStreetCrafter with only one fifth of GPU memory required. LidarPainter also\nsupports stylized generation using text prompts such as \"foggy\" and \"night\",\nallowing for a diverse expansion of the existing asset library.", "published": "2025-07-16 10:30:47", "link": "http://arxiv.org/abs/2507.12114v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Out-of-distribution data supervision towards biomedical semantic segmentation", "abstract": "Biomedical segmentation networks easily suffer from the unexpected\nmisclassification between foreground and background objects when learning on\nlimited and imperfect medical datasets. Inspired by the strong power of\nOut-of-Distribution (OoD) data on other visual tasks, we propose a data-centric\nframework, Med-OoD to address this issue by introducing OoD data supervision\ninto fully-supervised biomedical segmentation with none of the following needs:\n(i) external data sources, (ii) feature regularization objectives, (iii)\nadditional annotations. Our method can be seamlessly integrated into\nsegmentation networks without any modification on the architectures. Extensive\nexperiments show that Med-OoD largely prevents various segmentation networks\nfrom the pixel misclassification on medical images and achieves considerable\nperformance improvements on Lizard dataset. We also present an emerging\nlearning paradigm of training a medical segmentation network completely using\nOoD data devoid of foreground class labels, surprisingly turning out 76.1% mIoU\nas test result. We hope this learning paradigm will attract people to rethink\nthe roles of OoD data. Code is made available at\nhttps://github.com/StudioYG/Med-OoD.", "published": "2025-07-16 10:21:45", "link": "http://arxiv.org/abs/2507.12105v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "DeepShade: Enable Shade Simulation by Text-conditioned Image Generation", "abstract": "Heatwaves pose a significant threat to public health, especially as global\nwarming intensifies. However, current routing systems (e.g., online maps) fail\nto incorporate shade information due to the difficulty of estimating shades\ndirectly from noisy satellite imagery and the limited availability of training\ndata for generative models. In this paper, we address these challenges through\ntwo main contributions. First, we build an extensive dataset covering diverse\nlongitude-latitude regions, varying levels of building density, and different\nurban layouts. Leveraging Blender-based 3D simulations alongside building\noutlines, we capture building shadows under various solar zenith angles\nthroughout the year and at different times of day. These simulated shadows are\naligned with satellite images, providing a rich resource for learning shade\npatterns. Second, we propose the DeepShade, a diffusion-based model designed to\nlearn and synthesize shade variations over time. It emphasizes the nuance of\nedge features by jointly considering RGB with the Canny edge layer, and\nincorporates contrastive learning to capture the temporal change rules of\nshade. Then, by conditioning on textual descriptions of known conditions (e.g.,\ntime of day, solar angles), our framework provides improved performance in\ngenerating shade images. We demonstrate the utility of our approach by using\nour shade predictions to calculate shade ratios for real-world route planning\nin Tempe, Arizona. We believe this work will benefit society by providing a\nreference for urban planning in extreme heat weather and its potential\npractical applications in the environment.", "published": "2025-07-16 10:19:12", "link": "http://arxiv.org/abs/2507.12103v1", "categories": ["cs.CV", "cs.CY", "68T45, 68U10, 62H35", "I.2.10; I.4.8; I.5.1"], "primary_category": "cs.CV"}
{"title": "BRUM: Robust 3D Vehicle Reconstruction from 360 Sparse Images", "abstract": "Accurate 3D reconstruction of vehicles is vital for applications such as\nvehicle inspection, predictive maintenance, and urban planning. Existing\nmethods like Neural Radiance Fields and Gaussian Splatting have shown\nimpressive results but remain limited by their reliance on dense input views,\nwhich hinders real-world applicability. This paper addresses the challenge of\nreconstructing vehicles from sparse-view inputs, leveraging depth maps and a\nrobust pose estimation architecture to synthesize novel views and augment\ntraining data. Specifically, we enhance Gaussian Splatting by integrating a\nselective photometric loss, applied only to high-confidence pixels, and\nreplacing standard Structure-from-Motion pipelines with the DUSt3R architecture\nto improve camera pose estimation. Furthermore, we present a novel dataset\nfeaturing both synthetic and real-world public transportation vehicles,\nenabling extensive evaluation of our approach. Experimental results demonstrate\nstate-of-the-art performance across multiple benchmarks, showcasing the\nmethod's ability to achieve high-quality reconstructions even under constrained\ninput conditions.", "published": "2025-07-16 10:04:35", "link": "http://arxiv.org/abs/2507.12095v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Benchmarking and Explaining Deep Learning Cortical Lesion MRI Segmentation in Multiple Sclerosis", "abstract": "Cortical lesions (CLs) have emerged as valuable biomarkers in multiple\nsclerosis (MS), offering high diagnostic specificity and prognostic relevance.\nHowever, their routine clinical integration remains limited due to subtle\nmagnetic resonance imaging (MRI) appearance, challenges in expert annotation,\nand a lack of standardized automated methods. We propose a comprehensive\nmulti-centric benchmark of CL detection and segmentation in MRI. A total of 656\nMRI scans, including clinical trial and research data from four institutions,\nwere acquired at 3T and 7T using MP2RAGE and MPRAGE sequences with\nexpert-consensus annotations. We rely on the self-configuring nnU-Net\nframework, designed for medical imaging segmentation, and propose adaptations\ntailored to the improved CL detection. We evaluated model generalization\nthrough out-of-distribution testing, demonstrating strong lesion detection\ncapabilities with an F1-score of 0.64 and 0.5 in and out of the domain,\nrespectively. We also analyze internal model features and model errors for a\nbetter understanding of AI decision-making. Our study examines how data\nvariability, lesion ambiguity, and protocol differences impact model\nperformance, offering future recommendations to address these barriers to\nclinical adoption. To reinforce the reproducibility, the implementation and\nmodels will be publicly accessible and ready to use at\nhttps://github.com/Medical-Image-Analysis-Laboratory/ and\nhttps://doi.org/10.5281/zenodo.15911797.", "published": "2025-07-16 09:56:11", "link": "http://arxiv.org/abs/2507.12092v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "YOLOv8-SMOT: An Efficient and Robust Framework for Real-Time Small Object Tracking via Slice-Assisted Training and Adaptive Association", "abstract": "Tracking small, agile multi-objects (SMOT), such as birds, from an Unmanned\nAerial Vehicle (UAV) perspective is a highly challenging computer vision task.\nThe difficulty stems from three main sources: the extreme scarcity of target\nappearance features, the complex motion entanglement caused by the combined\ndynamics of the camera and the targets themselves, and the frequent occlusions\nand identity ambiguity arising from dense flocking behavior. This paper details\nour championship-winning solution in the MVA 2025 \"Finding Birds\" Small\nMulti-Object Tracking Challenge (SMOT4SB), which adopts the\ntracking-by-detection paradigm with targeted innovations at both the detection\nand association levels. On the detection side, we propose a systematic training\nenhancement framework named \\textbf{SliceTrain}. This framework, through the\nsynergy of 'deterministic full-coverage slicing' and 'slice-level stochastic\naugmentation, effectively addresses the problem of insufficient learning for\nsmall objects in high-resolution image training. On the tracking side, we\ndesigned a robust tracker that is completely independent of appearance\ninformation. By integrating a \\textbf{motion direction maintenance (EMA)}\nmechanism and an \\textbf{adaptive similarity metric} combining \\textbf{bounding\nbox expansion and distance penalty} into the OC-SORT framework, our tracker can\nstably handle irregular motion and maintain target identities. Our method\nachieves state-of-the-art performance on the SMOT4SB public test set, reaching\nan SO-HOTA score of \\textbf{55.205}, which fully validates the effectiveness\nand advancement of our framework in solving complex real-world SMOT problems.\nThe source code will be made available at\nhttps://github.com/Salvatore-Love/YOLOv8-SMOT.", "published": "2025-07-16 09:51:19", "link": "http://arxiv.org/abs/2507.12087v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Foresight in Motion: Reinforcing Trajectory Prediction with Reward Heuristics", "abstract": "Motion forecasting for on-road traffic agents presents both a significant\nchallenge and a critical necessity for ensuring safety in autonomous driving\nsystems. In contrast to most existing data-driven approaches that directly\npredict future trajectories, we rethink this task from a planning perspective,\nadvocating a \"First Reasoning, Then Forecasting\" strategy that explicitly\nincorporates behavior intentions as spatial guidance for trajectory prediction.\nTo achieve this, we introduce an interpretable, reward-driven intention\nreasoner grounded in a novel query-centric Inverse Reinforcement Learning (IRL)\nscheme. Our method first encodes traffic agents and scene elements into a\nunified vectorized representation, then aggregates contextual features through\na query-centric paradigm. This enables the derivation of a reward distribution,\na compact yet informative representation of the target agent's behavior within\nthe given scene context via IRL. Guided by this reward heuristic, we perform\npolicy rollouts to reason about multiple plausible intentions, providing\nvaluable priors for subsequent trajectory generation. Finally, we develop a\nhierarchical DETR-like decoder integrated with bidirectional selective state\nspace models to produce accurate future trajectories along with their\nassociated probabilities. Extensive experiments on the large-scale Argoverse\nand nuScenes motion forecasting datasets demonstrate that our approach\nsignificantly enhances trajectory prediction confidence, achieving highly\ncompetitive performance relative to state-of-the-art methods.", "published": "2025-07-16 09:46:17", "link": "http://arxiv.org/abs/2507.12083v1", "categories": ["cs.CV", "cs.RO"], "primary_category": "cs.CV"}
{"title": "MS-DETR: Towards Effective Video Moment Retrieval and Highlight Detection by Joint Motion-Semantic Learning", "abstract": "Video Moment Retrieval (MR) and Highlight Detection (HD) aim to pinpoint\nspecific moments and assess clip-wise relevance based on the text query. While\nDETR-based joint frameworks have made significant strides, there remains\nuntapped potential in harnessing the intricate relationships between temporal\nmotion and spatial semantics within video content. In this paper, we propose\nthe Motion-Semantics DETR (MS-DETR), a framework that captures rich\nmotion-semantics features through unified learning for MR/HD tasks. The encoder\nfirst explicitly models disentangled intra-modal correlations within motion and\nsemantics dimensions, guided by the given text queries. Subsequently, the\ndecoder utilizes the task-wise correlation across temporal motion and spatial\nsemantics dimensions to enable precise query-guided localization for MR and\nrefined highlight boundary delineation for HD. Furthermore, we observe the\ninherent sparsity dilemma within the motion and semantics dimensions of MR/HD\ndatasets. To address this issue, we enrich the corpus from both dimensions by\ngeneration strategies and propose contrastive denoising learning to ensure the\nabove components learn robustly and effectively. Extensive experiments on four\nMR/HD benchmarks demonstrate that our method outperforms existing\nstate-of-the-art models by a margin. Our code is available at\nhttps://github.com/snailma0229/MS-DETR.git.", "published": "2025-07-16 09:18:18", "link": "http://arxiv.org/abs/2507.12062v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "IDFace: Face Template Protection for Efficient and Secure Identification", "abstract": "As face recognition systems (FRS) become more widely used, user privacy\nbecomes more important. A key privacy issue in FRS is protecting the user's\nface template, as the characteristics of the user's face image can be recovered\nfrom the template. Although recent advances in cryptographic tools such as\nhomomorphic encryption (HE) have provided opportunities for securing the FRS,\nHE cannot be used directly with FRS in an efficient plug-and-play manner. In\nparticular, although HE is functionally complete for arbitrary programs, it is\nbasically designed for algebraic operations on encrypted data of predetermined\nshape, such as a polynomial ring. Thus, a non-tailored combination of HE and\nthe system can yield very inefficient performance, and many previous HE-based\nface template protection methods are hundreds of times slower than plain\nsystems without protection. In this study, we propose IDFace, a new HE-based\nsecure and efficient face identification method with template protection.\nIDFace is designed on the basis of two novel techniques for efficient searching\non a (homomorphically encrypted) biometric database with an angular metric. The\nfirst technique is a template representation transformation that sharply\nreduces the unit cost for the matching test. The second is a space-efficient\nencoding that reduces wasted space from the encryption algorithm, thus saving\nthe number of operations on encrypted templates. Through experiments, we show\nthat IDFace can identify a face template from among a database of 1M encrypted\ntemplates in 126ms, showing only 2X overhead compared to the identification\nover plaintexts.", "published": "2025-07-16 09:10:40", "link": "http://arxiv.org/abs/2507.12050v1", "categories": ["cs.CR", "cs.CV", "I.5.4; K.6.5; D.4.6; I.4.7"], "primary_category": "cs.CR"}
{"title": "MoViAD: Modular Visual Anomaly Detection", "abstract": "VAD is a critical field in machine learning focused on identifying deviations\nfrom normal patterns in images, often challenged by the scarcity of anomalous\ndata and the need for unsupervised training. To accelerate research and\ndeployment in this domain, we introduce MoViAD, a comprehensive and highly\nmodular library designed to provide fast and easy access to state-of-the-art\nVAD models, trainers, datasets, and VAD utilities. MoViAD supports a wide array\nof scenarios, including continual, semi-supervised, few-shots, noisy, and many\nmore. In addition, it addresses practical deployment challenges through\ndedicated Edge and IoT settings, offering optimized models and backbones, along\nwith quantization and compression utilities for efficient on-device execution\nand distributed inference. MoViAD integrates a selection of backbones, robust\nevaluation VAD metrics (pixel-level and image-level) and useful profiling tools\nfor efficiency analysis. The library is designed for fast, effortless\ndeployment, enabling machine learning engineers to easily use it for their\nspecific setup with custom models, datasets, and backbones. At the same time,\nit offers the flexibility and extensibility researchers need to develop and\nexperiment with new methods.", "published": "2025-07-16 09:10:38", "link": "http://arxiv.org/abs/2507.12049v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Stereo Sound Event Localization and Detection with Onscreen/offscreen Classification", "abstract": "This paper presents the objective, dataset, baseline, and metrics of Task 3\nof the DCASE2025 Challenge on sound event localization and detection (SELD). In\nprevious editions, the challenge used four-channel audio formats of first-order\nAmbisonics (FOA) and microphone array. In contrast, this year's challenge\ninvestigates SELD with stereo audio data (termed stereo SELD). This change\nshifts the focus from more specialized 360{\\deg} audio and audiovisual scene\nanalysis to more commonplace audio and media scenarios with limited\nfield-of-view (FOV). Due to inherent angular ambiguities in stereo audio data,\nthe task focuses on direction-of-arrival (DOA) estimation in the azimuth plane\n(left-right axis) along with distance estimation. The challenge remains divided\ninto two tracks: audio-only and audiovisual, with the audiovisual track\nintroducing a new sub-task of onscreen/offscreen event classification\nnecessitated by the limited FOV. This challenge introduces the DCASE2025 Task3\nStereo SELD Dataset, whose stereo audio and perspective video clips are sampled\nand converted from the STARSS23 recordings. The baseline system is designed to\nprocess stereo audio and corresponding video frames as inputs. In addition to\nthe typical SELD event classification and localization, it integrates\nonscreen/offscreen classification for the audiovisual track. The evaluation\nmetrics have been modified to introduce an onscreen/offscreen accuracy metric,\nwhich assesses the models' ability to identify which sound sources are\nonscreen. In the experimental evaluation, the baseline system performs\nreasonably well with the stereo audio data.", "published": "2025-07-16 08:59:09", "link": "http://arxiv.org/abs/2507.12042v1", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS", "eess.IV"], "primary_category": "cs.SD"}
{"title": "SGLoc: Semantic Localization System for Camera Pose Estimation from 3D Gaussian Splatting Representation", "abstract": "We propose SGLoc, a novel localization system that directly regresses camera\nposes from 3D Gaussian Splatting (3DGS) representation by leveraging semantic\ninformation. Our method utilizes the semantic relationship between 2D image and\n3D scene representation to estimate the 6DoF pose without prior pose\ninformation. In this system, we introduce a multi-level pose regression\nstrategy that progressively estimates and refines the pose of query image from\nthe global 3DGS map, without requiring initial pose priors. Moreover, we\nintroduce a semantic-based global retrieval algorithm that establishes\ncorrespondences between 2D (image) and 3D (3DGS map). By matching the extracted\nscene semantic descriptors of 2D query image and 3DGS semantic representation,\nwe align the image with the local region of the global 3DGS map, thereby\nobtaining a coarse pose estimation. Subsequently, we refine the coarse pose by\niteratively optimizing the difference between the query image and the rendered\nimage from 3DGS. Our SGLoc demonstrates superior performance over baselines on\n12scenes and 7scenes datasets, showing excellent capabilities in global\nlocalization without initial pose prior. Code will be available at\nhttps://github.com/IRMVLab/SGLoc.", "published": "2025-07-16 08:39:08", "link": "http://arxiv.org/abs/2507.12027v1", "categories": ["cs.CV", "cs.RO", "I.4.8; I.2.9"], "primary_category": "cs.CV"}
{"title": "3D-MoRe: Unified Modal-Contextual Reasoning for Embodied Question Answering", "abstract": "With the growing need for diverse and scalable data in indoor scene tasks,\nsuch as question answering and dense captioning, we propose 3D-MoRe, a novel\nparadigm designed to generate large-scale 3D-language datasets by leveraging\nthe strengths of foundational models. The framework integrates key components,\nincluding multi-modal embedding, cross-modal interaction, and a language model\ndecoder, to process natural language instructions and 3D scene data. This\napproach facilitates enhanced reasoning and response generation in complex 3D\nenvironments. Using the ScanNet 3D scene dataset, along with text annotations\nfrom ScanQA and ScanRefer, 3D-MoRe generates 62,000 question-answer (QA) pairs\nand 73,000 object descriptions across 1,513 scenes. We also employ various data\naugmentation techniques and implement semantic filtering to ensure high-quality\ndata. Experiments on ScanQA demonstrate that 3D-MoRe significantly outperforms\nstate-of-the-art baselines, with the CIDEr score improving by 2.15\\%.\nSimilarly, on ScanRefer, our approach achieves a notable increase in CIDEr@0.5\nby 1.84\\%, highlighting its effectiveness in both tasks. Our code and generated\ndatasets will be publicly released to benefit the community, and both can be\naccessed on the https://3D-MoRe.github.io.", "published": "2025-07-16 08:38:26", "link": "http://arxiv.org/abs/2507.12026v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "MVAR: MultiVariate AutoRegressive Air Pollutants Forecasting Model", "abstract": "Air pollutants pose a significant threat to the environment and human health,\nthus forecasting accurate pollutant concentrations is essential for pollution\nwarnings and policy-making. Existing studies predominantly focus on\nsingle-pollutant forecasting, neglecting the interactions among different\npollutants and their diverse spatial responses. To address the practical needs\nof forecasting multivariate air pollutants, we propose MultiVariate\nAutoRegressive air pollutants forecasting model (MVAR), which reduces the\ndependency on long-time-window inputs and boosts the data utilization\nefficiency. We also design the Multivariate Autoregressive Training Paradigm,\nenabling MVAR to achieve 120-hour long-term sequential forecasting.\nAdditionally, MVAR develops Meteorological Coupled Spatial Transformer block,\nenabling the flexible coupling of AI-based meteorological forecasts while\nlearning the interactions among pollutants and their diverse spatial responses.\nAs for the lack of standardized datasets in air pollutants forecasting, we\nconstruct a comprehensive dataset covering 6 major pollutants across 75 cities\nin North China from 2018 to 2023, including ERA5 reanalysis data and FuXi-2.0\nforecast data. Experimental results demonstrate that the proposed model\noutperforms state-of-the-art methods and validate the effectiveness of the\nproposed architecture.", "published": "2025-07-16 08:30:41", "link": "http://arxiv.org/abs/2507.12023v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Dataset Ownership Verification for Pre-trained Masked Models", "abstract": "High-quality open-source datasets have emerged as a pivotal catalyst driving\nthe swift advancement of deep learning, while facing the looming threat of\npotential exploitation. Protecting these datasets is of paramount importance\nfor the interests of their owners. The verification of dataset ownership has\nevolved into a crucial approach in this domain; however, existing verification\ntechniques are predominantly tailored to supervised models and contrastive\npre-trained models, rendering them ill-suited for direct application to the\nincreasingly prevalent masked models. In this work, we introduce the inaugural\nmethodology addressing this critical, yet unresolved challenge, termed Dataset\nOwnership Verification for Masked Modeling (DOV4MM). The central objective is\nto ascertain whether a suspicious black-box model has been pre-trained on a\nparticular unlabeled dataset, thereby assisting dataset owners in safeguarding\ntheir rights. DOV4MM is grounded in our empirical observation that when a model\nis pre-trained on the target dataset, the difficulty of reconstructing masked\ninformation within the embedding space exhibits a marked contrast to models not\npre-trained on that dataset. We validated the efficacy of DOV4MM through ten\nmasked image models on ImageNet-1K and four masked language models on\nWikiText-103. The results demonstrate that DOV4MM rejects the null hypothesis,\nwith a $p$-value considerably below 0.05, surpassing all prior approaches. Code\nis available at https://github.com/xieyc99/DOV4MM.", "published": "2025-07-16 08:30:30", "link": "http://arxiv.org/abs/2507.12022v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Deep Neural Encoder-Decoder Model to Relate fMRI Brain Activity with Naturalistic Stimuli", "abstract": "We propose an end-to-end deep neural encoder-decoder model to encode and\ndecode brain activity in response to naturalistic stimuli using functional\nmagnetic resonance imaging (fMRI) data. Leveraging temporally correlated input\nfrom consecutive film frames, we employ temporal convolutional layers in our\narchitecture, which effectively allows to bridge the temporal resolution gap\nbetween natural movie stimuli and fMRI acquisitions. Our model predicts\nactivity of voxels in and around the visual cortex and performs reconstruction\nof corresponding visual inputs from neural activity. Finally, we investigate\nbrain regions contributing to visual decoding through saliency maps. We find\nthat the most contributing regions are the middle occipital area, the fusiform\narea, and the calcarine, respectively employed in shape perception, complex\nrecognition (in particular face perception), and basic visual features such as\nedges and contrasts. These functions being strongly solicited are in line with\nthe decoder's capability to reconstruct edges, faces, and contrasts. All in\nall, this suggests the possibility to probe our understanding of visual\nprocessing in films using as a proxy the behaviour of deep learning models such\nas the one proposed in this paper.", "published": "2025-07-16 08:08:48", "link": "http://arxiv.org/abs/2507.12009v1", "categories": ["cs.CV", "cs.HC"], "primary_category": "cs.CV"}
{"title": "AU-Blendshape for Fine-grained Stylized 3D Facial Expression Manipulation", "abstract": "While 3D facial animation has made impressive progress, challenges still\nexist in realizing fine-grained stylized 3D facial expression manipulation due\nto the lack of appropriate datasets. In this paper, we introduce the\nAUBlendSet, a 3D facial dataset based on AU-Blendshape representation for\nfine-grained facial expression manipulation across identities. AUBlendSet is a\nblendshape data collection based on 32 standard facial action units (AUs)\nacross 500 identities, along with an additional set of facial postures\nannotated with detailed AUs. Based on AUBlendSet, we propose AUBlendNet to\nlearn AU-Blendshape basis vectors for different character styles. AUBlendNet\npredicts, in parallel, the AU-Blendshape basis vectors of the corresponding\nstyle for a given identity mesh, thereby achieving stylized 3D emotional facial\nmanipulation. We comprehensively validate the effectiveness of AUBlendSet and\nAUBlendNet through tasks such as stylized facial expression manipulation,\nspeech-driven emotional facial animation, and emotion recognition data\naugmentation. Through a series of qualitative and quantitative experiments, we\ndemonstrate the potential and importance of AUBlendSet and AUBlendNet in 3D\nfacial animation tasks. To the best of our knowledge, AUBlendSet is the first\ndataset, and AUBlendNet is the first network for continuous 3D facial\nexpression manipulation for any identity through facial AUs. Our source code is\navailable at https://github.com/wslh852/AUBlendNet.git.", "published": "2025-07-16 07:56:25", "link": "http://arxiv.org/abs/2507.12001v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "SAMST: A Transformer framework based on SAM pseudo label filtering for remote sensing semi-supervised semantic segmentation", "abstract": "Public remote sensing datasets often face limitations in universality due to\nresolution variability and inconsistent land cover category definitions. To\nharness the vast pool of unlabeled remote sensing data, we propose SAMST, a\nsemi-supervised semantic segmentation method. SAMST leverages the strengths of\nthe Segment Anything Model (SAM) in zero-shot generalization and boundary\ndetection. SAMST iteratively refines pseudo-labels through two main components:\nsupervised model self-training using both labeled and pseudo-labeled data, and\na SAM-based Pseudo-label Refiner. The Pseudo-label Refiner comprises three\nmodules: a Threshold Filter Module for preprocessing, a Prompt Generation\nModule for extracting connected regions and generating prompts for SAM, and a\nLabel Refinement Module for final label stitching. By integrating the\ngeneralization power of large models with the training efficiency of small\nmodels, SAMST improves pseudo-label accuracy, thereby enhancing overall model\nperformance. Experiments on the Potsdam dataset validate the effectiveness and\nfeasibility of SAMST, demonstrating its potential to address the challenges\nposed by limited labeled data in remote sensing semantic segmentation.", "published": "2025-07-16 07:47:49", "link": "http://arxiv.org/abs/2507.11994v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "ID-EA: Identity-driven Text Enhancement and Adaptation with Textual Inversion for Personalized Text-to-Image Generation", "abstract": "Recently, personalized portrait generation with a text-to-image diffusion\nmodel has significantly advanced with Textual Inversion, emerging as a\npromising approach for creating high-fidelity personalized images. Despite its\npotential, current Textual Inversion methods struggle to maintain consistent\nfacial identity due to semantic misalignments between textual and visual\nembedding spaces regarding identity. We introduce ID-EA, a novel framework that\nguides text embeddings to align with visual identity embeddings, thereby\nimproving identity preservation in a personalized generation. ID-EA comprises\ntwo key components: the ID-driven Enhancer (ID-Enhancer) and the ID-conditioned\nAdapter (ID-Adapter). First, the ID-Enhancer integrates identity embeddings\nwith a textual ID anchor, refining visual identity embeddings derived from a\nface recognition model using representative text embeddings. Then, the\nID-Adapter leverages the identity-enhanced embedding to adapt the text\ncondition, ensuring identity preservation by adjusting the cross-attention\nmodule in the pre-trained UNet model. This process encourages the text features\nto find the most related visual clues across the foreground snippets. Extensive\nquantitative and qualitative evaluations demonstrate that ID-EA substantially\noutperforms state-of-the-art methods in identity preservation metrics while\nachieving remarkable computational efficiency, generating personalized\nportraits approximately 15 times faster than existing approaches.", "published": "2025-07-16 07:42:02", "link": "http://arxiv.org/abs/2507.11990v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Style Composition within Distinct LoRA modules for Traditional Art", "abstract": "Diffusion-based text-to-image models have achieved remarkable results in\nsynthesizing diverse images from text prompts and can capture specific artistic\nstyles via style personalization. However, their entangled latent space and\nlack of smooth interpolation make it difficult to apply distinct painting\ntechniques in a controlled, regional manner, often causing one style to\ndominate. To overcome this, we propose a zero-shot diffusion pipeline that\nnaturally blends multiple styles by performing style composition on the\ndenoised latents predicted during the flow-matching denoising process of\nseparately trained, style-specialized models. We leverage the fact that\nlower-noise latents carry stronger stylistic information and fuse them across\nheterogeneous diffusion pipelines using spatial masks, enabling precise,\nregion-specific style control. This mechanism preserves the fidelity of each\nindividual style while allowing user-guided mixing. Furthermore, to ensure\nstructural coherence across different models, we incorporate depth-map\nconditioning via ControlNet into the diffusion framework. Qualitative and\nquantitative experiments demonstrate that our method successfully achieves\nregion-specific style mixing according to the given masks.", "published": "2025-07-16 07:36:07", "link": "http://arxiv.org/abs/2507.11986v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Unsupervised Part Discovery via Descriptor-Based Masked Image Restoration with Optimized Constraints", "abstract": "Part-level features are crucial for image understanding, but few studies\nfocus on them because of the lack of fine-grained labels. Although unsupervised\npart discovery can eliminate the reliance on labels, most of them cannot\nmaintain robustness across various categories and scenarios, which restricts\ntheir application range. To overcome this limitation, we present a more\neffective paradigm for unsupervised part discovery, named Masked Part\nAutoencoder (MPAE). It first learns part descriptors as well as a feature map\nfrom the inputs and produces patch features from a masked version of the\noriginal images. Then, the masked regions are filled with the learned part\ndescriptors based on the similarity between the local features and descriptors.\nBy restoring these masked patches using the part descriptors, they become\nbetter aligned with their part shapes, guided by appearance features from\nunmasked patches. Finally, MPAE robustly discovers meaningful parts that\nclosely match the actual object shapes, even in complex scenarios. Moreover,\nseveral looser yet more effective constraints are proposed to enable MPAE to\nidentify the presence of parts across various scenarios and categories in an\nunsupervised manner. This provides the foundation for addressing challenges\nposed by occlusion and for exploring part similarity across multiple\ncategories. Extensive experiments demonstrate that our method robustly\ndiscovers meaningful parts across various categories and scenarios. The code is\navailable at the project https://github.com/Jiahao-UTS/MPAE.", "published": "2025-07-16 07:33:14", "link": "http://arxiv.org/abs/2507.11985v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "EC-Diff: Fast and High-Quality Edge-Cloud Collaborative Inference for Diffusion Models", "abstract": "Diffusion Models have shown remarkable proficiency in image and video\nsynthesis. As model size and latency increase limit user experience, hybrid\nedge-cloud collaborative framework was recently proposed to realize fast\ninference and high-quality generation, where the cloud model initiates\nhigh-quality semantic planning and the edge model expedites later-stage\nrefinement. However, excessive cloud denoising prolongs inference time, while\ninsufficient steps cause semantic ambiguity, leading to inconsistency in edge\nmodel output. To address these challenges, we propose EC-Diff that accelerates\ncloud inference through gradient-based noise estimation while identifying the\noptimal point for cloud-edge handoff to maintain generation quality.\nSpecifically, we design a K-step noise approximation strategy to reduce cloud\ninference frequency by using noise gradients between steps and applying cloud\ninference periodically to adjust errors. Then we design a two-stage greedy\nsearch algorithm to efficiently find the optimal parameters for noise\napproximation and edge model switching. Extensive experiments demonstrate that\nour method significantly enhances generation quality compared to edge\ninference, while achieving up to an average $2\\times$ speedup in inference\ncompared to cloud inference. Video samples and source code are available at\nhttps://ec-diff.github.io/.", "published": "2025-07-16 07:23:14", "link": "http://arxiv.org/abs/2507.11980v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "HPR3D: Hierarchical Proxy Representation for High-Fidelity 3D Reconstruction and Controllable Editing", "abstract": "Current 3D representations like meshes, voxels, point clouds, and NeRF-based\nneural implicit fields exhibit significant limitations: they are often\ntask-specific, lacking universal applicability across reconstruction,\ngeneration, editing, and driving. While meshes offer high precision, their\ndense vertex data complicates editing; NeRFs deliver excellent rendering but\nsuffer from structural ambiguity, hindering animation and manipulation; all\nrepresentations inherently struggle with the trade-off between data complexity\nand fidelity. To overcome these issues, we introduce a novel 3D Hierarchical\nProxy Node representation. Its core innovation lies in representing an object's\nshape and texture via a sparse set of hierarchically organized\n(tree-structured) proxy nodes distributed on its surface and interior. Each\nnode stores local shape and texture information (implicitly encoded by a small\nMLP) within its neighborhood. Querying any 3D coordinate's properties involves\nefficient neural interpolation and lightweight decoding from relevant nearby\nand parent nodes. This framework yields a highly compact representation where\nnodes align with local semantics, enabling direct drag-and-edit manipulation,\nand offers scalable quality-complexity control. Extensive experiments across 3D\nreconstruction and editing demonstrate our method's expressive efficiency,\nhigh-fidelity rendering quality, and superior editability.", "published": "2025-07-16 07:09:05", "link": "http://arxiv.org/abs/2507.11971v1", "categories": ["cs.GR", "cs.CV"], "primary_category": "cs.GR"}
{"title": "GS-Bias: Global-Spatial Bias Learner for Single-Image Test-Time Adaptation of Vision-Language Models", "abstract": "Recent advances in test-time adaptation (TTA) for Vision-Language Models\n(VLMs) have garnered increasing attention, particularly through the use of\nmultiple augmented views of a single image to boost zero-shot generalization.\nUnfortunately, existing methods fail to strike a satisfactory balance between\nperformance and efficiency, either due to excessive overhead of tuning text\nprompts or unstable benefits from handcrafted, training-free visual feature\nenhancement. In this paper, we present Global-Spatial Bias Learner (GS-Bias),\nan efficient and effective TTA paradigm that incorporates two learnable biases\nduring TTA, unfolded as the global bias and spatial bias. Particularly, the\nglobal bias captures the global semantic features of a test image by learning\nconsistency across augmented views, while spatial bias learns the semantic\ncoherence between regions in the image's spatial visual representation. It is\nworth highlighting that these two sets of biases are directly added to the\nlogits outputed by the pretrained VLMs, which circumvent the full\nbackpropagation through VLM that hinders the efficiency of existing TTA\nmethods. This endows GS-Bias with extremely high efficiency while achieving\nstate-of-the-art performance on 15 benchmark datasets. For example, it achieves\na 2.23% improvement over TPT in cross-dataset generalization and a 2.72%\nimprovement in domain generalization, while requiring only 6.5% of TPT's memory\nusage on ImageNet.", "published": "2025-07-16 07:02:45", "link": "http://arxiv.org/abs/2507.11969v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Watch, Listen, Understand, Mislead: Tri-modal Adversarial Attacks on Short Videos for Content Appropriateness Evaluation", "abstract": "Multimodal Large Language Models (MLLMs) are increasingly used for content\nmoderation, yet their robustness in short-form video contexts remains\nunderexplored. Current safety evaluations often rely on unimodal attacks,\nfailing to address combined attack vulnerabilities. In this paper, we introduce\na comprehensive framework for evaluating the tri-modal safety of MLLMs. First,\nwe present the Short-Video Multimodal Adversarial (SVMA) dataset, comprising\ndiverse short-form videos with human-guided synthetic adversarial attacks.\nSecond, we propose ChimeraBreak, a novel tri-modal attack strategy that\nsimultaneously challenges visual, auditory, and semantic reasoning pathways.\nExtensive experiments on state-of-the-art MLLMs reveal significant\nvulnerabilities with high Attack Success Rates (ASR). Our findings uncover\ndistinct failure modes, showing model biases toward misclassifying benign or\npolicy-violating content. We assess results using LLM-as-a-judge, demonstrating\nattack reasoning efficacy. Our dataset and findings provide crucial insights\nfor developing more robust and safe MLLMs.", "published": "2025-07-16 07:02:15", "link": "http://arxiv.org/abs/2507.11968v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Language-Guided Contrastive Audio-Visual Masked Autoencoder with Automatically Generated Audio-Visual-Text Triplets from Videos", "abstract": "In this paper, we propose Language-Guided Contrastive Audio-Visual Masked\nAutoencoders (LG-CAV-MAE) to improve audio-visual representation learning.\nLG-CAV-MAE integrates a pretrained text encoder into contrastive audio-visual\nmasked autoencoders, enabling the model to learn across audio, visual and text\nmodalities. To train LG-CAV-MAE, we introduce an automatic method to generate\naudio-visual-text triplets from unlabeled videos. We first generate frame-level\ncaptions using an image captioning model and then apply CLAP-based filtering to\nensure strong alignment between audio and captions. This approach yields\nhigh-quality audio-visual-text triplets without requiring manual annotations.\nWe evaluate LG-CAV-MAE on audio-visual retrieval tasks, as well as an\naudio-visual classification task. Our method significantly outperforms existing\napproaches, achieving up to a 5.6% improvement in recall@10 for retrieval tasks\nand a 3.2% improvement for the classification task.", "published": "2025-07-16 06:58:14", "link": "http://arxiv.org/abs/2507.11967v1", "categories": ["cs.CV", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Prototypical Progressive Alignment and Reweighting for Generalizable Semantic Segmentation", "abstract": "Generalizable semantic segmentation aims to perform well on unseen target\ndomains, a critical challenge due to real-world applications requiring high\ngeneralizability. Class-wise prototypes, representing class centroids, serve as\ndomain-invariant cues that benefit generalization due to their stability and\nsemantic consistency. However, this approach faces three challenges. First,\nexisting methods often adopt coarse prototypical alignment strategies, which\nmay hinder performance. Second, naive prototypes computed by averaging source\nbatch features are prone to overfitting and may be negatively affected by\nunrelated source data. Third, most methods treat all source samples equally,\nignoring the fact that different features have varying adaptation difficulties.\nTo address these limitations, we propose a novel framework for generalizable\nsemantic segmentation: Prototypical Progressive Alignment and Reweighting\n(PPAR), leveraging the strong generalization ability of the CLIP model.\nSpecifically, we define two prototypes: the Original Text Prototype (OTP) and\nVisual Text Prototype (VTP), generated via CLIP to serve as a solid base for\nalignment. We then introduce a progressive alignment strategy that aligns\nfeatures in an easy-to-difficult manner, reducing domain gaps gradually.\nFurthermore, we propose a prototypical reweighting mechanism that estimates the\nreliability of source data and adjusts its contribution, mitigating the effect\nof irrelevant or harmful features (i.e., reducing negative transfer). We also\nprovide a theoretical analysis showing the alignment between our method and\ndomain generalization theory. Extensive experiments across multiple benchmarks\ndemonstrate that PPAR achieves state-of-the-art performance, validating its\neffectiveness.", "published": "2025-07-16 06:42:21", "link": "http://arxiv.org/abs/2507.11955v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "MOSPA: Human Motion Generation Driven by Spatial Audio", "abstract": "Enabling virtual humans to dynamically and realistically respond to diverse\nauditory stimuli remains a key challenge in character animation, demanding the\nintegration of perceptual modeling and motion synthesis. Despite its\nsignificance, this task remains largely unexplored. Most previous works have\nprimarily focused on mapping modalities like speech, audio, and music to\ngenerate human motion. As of yet, these models typically overlook the impact of\nspatial features encoded in spatial audio signals on human motion. To bridge\nthis gap and enable high-quality modeling of human movements in response to\nspatial audio, we introduce the first comprehensive Spatial Audio-Driven Human\nMotion (SAM) dataset, which contains diverse and high-quality spatial audio and\nmotion data. For benchmarking, we develop a simple yet effective\ndiffusion-based generative framework for human MOtion generation driven by\nSPatial Audio, termed MOSPA, which faithfully captures the relationship between\nbody motion and spatial audio through an effective fusion mechanism. Once\ntrained, MOSPA could generate diverse realistic human motions conditioned on\nvarying spatial audio inputs. We perform a thorough investigation of the\nproposed dataset and conduct extensive experiments for benchmarking, where our\nmethod achieves state-of-the-art performance on this task. Our model and\ndataset will be open-sourced upon acceptance. Please refer to our supplementary\nvideo for more details.", "published": "2025-07-16 06:33:11", "link": "http://arxiv.org/abs/2507.11949v1", "categories": ["cs.GR", "cs.CV", "cs.RO"], "primary_category": "cs.GR"}
{"title": "A Multi-Level Similarity Approach for Single-View Object Grasping: Matching, Planning, and Fine-Tuning", "abstract": "Grasping unknown objects from a single view has remained a challenging topic\nin robotics due to the uncertainty of partial observation. Recent advances in\nlarge-scale models have led to benchmark solutions such as GraspNet-1Billion.\nHowever, such learning-based approaches still face a critical limitation in\nperformance robustness for their sensitivity to sensing noise and environmental\nchanges. To address this bottleneck in achieving highly generalized grasping,\nwe abandon the traditional learning framework and introduce a new perspective:\nsimilarity matching, where similar known objects are utilized to guide the\ngrasping of unknown target objects. We newly propose a method that robustly\nachieves unknown-object grasping from a single viewpoint through three key\nsteps: 1) Leverage the visual features of the observed object to perform\nsimilarity matching with an existing database containing various object models,\nidentifying potential candidates with high similarity; 2) Use the candidate\nmodels with pre-existing grasping knowledge to plan imitative grasps for the\nunknown target object; 3) Optimize the grasp quality through a local\nfine-tuning process. To address the uncertainty caused by partial and noisy\nobservation, we propose a multi-level similarity matching framework that\nintegrates semantic, geometric, and dimensional features for comprehensive\nevaluation. Especially, we introduce a novel point cloud geometric descriptor,\nthe C-FPFH descriptor, which facilitates accurate similarity assessment between\npartial point clouds of observed objects and complete point clouds of database\nmodels. In addition, we incorporate the use of large language models, introduce\nthe semi-oriented bounding box, and develop a novel point cloud registration\napproach based on plane detection to enhance matching accuracy under\nsingle-view conditions. Videos are available at https://youtu.be/qQDIELMhQmk.", "published": "2025-07-16 06:07:57", "link": "http://arxiv.org/abs/2507.11938v1", "categories": ["cs.RO", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Hyperphantasia: A Benchmark for Evaluating the Mental Visualization Capabilities of Multimodal LLMs", "abstract": "Mental visualization, the ability to construct and manipulate visual\nrepresentations internally, is a core component of human cognition and plays a\nvital role in tasks involving reasoning, prediction, and abstraction. Despite\nthe rapid progress of Multimodal Large Language Models (MLLMs), current\nbenchmarks primarily assess passive visual perception, offering limited insight\ninto the more active capability of internally constructing visual patterns to\nsupport problem solving. Yet mental visualization is a critical cognitive skill\nin humans, supporting abilities such as spatial navigation, predicting physical\ntrajectories, and solving complex visual problems through imaginative\nsimulation. To bridge this gap, we introduce Hyperphantasia, a synthetic\nbenchmark designed to evaluate the mental visualization abilities of MLLMs\nthrough four carefully constructed puzzles. Each task is procedurally generated\nand presented at three difficulty levels, enabling controlled analysis of model\nperformance across increasing complexity. Our comprehensive evaluation of\nstate-of-the-art models reveals a substantial gap between the performance of\nhumans and MLLMs. Additionally, we explore the potential of reinforcement\nlearning to improve visual simulation capabilities. Our findings suggest that\nwhile some models exhibit partial competence in recognizing visual patterns,\nrobust mental visualization remains an open challenge for current MLLMs.", "published": "2025-07-16 05:54:37", "link": "http://arxiv.org/abs/2507.11932v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Dark-EvGS: Event Camera as an Eye for Radiance Field in the Dark", "abstract": "In low-light environments, conventional cameras often struggle to capture\nclear multi-view images of objects due to dynamic range limitations and motion\nblur caused by long exposure. Event cameras, with their high-dynamic range and\nhigh-speed properties, have the potential to mitigate these issues.\nAdditionally, 3D Gaussian Splatting (GS) enables radiance field reconstruction,\nfacilitating bright frame synthesis from multiple viewpoints in low-light\nconditions. However, naively using an event-assisted 3D GS approach still faced\nchallenges because, in low light, events are noisy, frames lack quality, and\nthe color tone may be inconsistent. To address these issues, we propose\nDark-EvGS, the first event-assisted 3D GS framework that enables the\nreconstruction of bright frames from arbitrary viewpoints along the camera\ntrajectory. Triplet-level supervision is proposed to gain holistic knowledge,\ngranular details, and sharp scene rendering. The color tone matching block is\nproposed to guarantee the color consistency of the rendered frames.\nFurthermore, we introduce the first real-captured dataset for the event-guided\nbright frame synthesis task via 3D GS-based radiance field reconstruction.\nExperiments demonstrate that our method achieves better results than existing\nmethods, conquering radiance field reconstruction under challenging low-light\nconditions. The code and sample data are included in the supplementary\nmaterial.", "published": "2025-07-16 05:54:33", "link": "http://arxiv.org/abs/2507.11931v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "SEPose: A Synthetic Event-based Human Pose Estimation Dataset for Pedestrian Monitoring", "abstract": "Event-based sensors have emerged as a promising solution for addressing\nchallenging conditions in pedestrian and traffic monitoring systems. Their\nlow-latency and high dynamic range allow for improved response time in\nsafety-critical situations caused by distracted walking or other unusual\nmovements. However, the availability of data covering such scenarios remains\nlimited. To address this gap, we present SEPose -- a comprehensive synthetic\nevent-based human pose estimation dataset for fixed pedestrian perception\ngenerated using dynamic vision sensors in the CARLA simulator. With nearly 350K\nannotated pedestrians with body pose keypoints from the perspective of fixed\ntraffic cameras, SEPose is a comprehensive synthetic multi-person pose\nestimation dataset that spans busy and light crowds and traffic across diverse\nlighting and weather conditions in 4-way intersections in urban, suburban, and\nrural environments. We train existing state-of-the-art models such as RVT and\nYOLOv8 on our dataset and evaluate them on real event-based data to demonstrate\nthe sim-to-real generalization capabilities of the proposed dataset.", "published": "2025-07-16 04:54:11", "link": "http://arxiv.org/abs/2507.11910v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "CompressedVQA-HDR: Generalized Full-reference and No-reference Quality Assessment Models for Compressed High Dynamic Range Videos", "abstract": "Video compression is a standard procedure applied to all videos to minimize\nstorage and transmission demands while preserving visual quality as much as\npossible. Therefore, evaluating the visual quality of compressed videos is\ncrucial for guiding the practical usage and further development of video\ncompression algorithms. Although numerous compressed video quality assessment\n(VQA) methods have been proposed, they often lack the generalization capability\nneeded to handle the increasing diversity of video types, particularly high\ndynamic range (HDR) content. In this paper, we introduce CompressedVQA-HDR, an\neffective VQA framework designed to address the challenges of HDR video quality\nassessment. Specifically, we adopt the Swin Transformer and SigLip 2 as the\nbackbone networks for the proposed full-reference (FR) and no-reference (NR)\nVQA models, respectively. For the FR model, we compute deep structural and\ntextural similarities between reference and distorted frames using\nintermediate-layer features extracted from the Swin Transformer as its\nquality-aware feature representation. For the NR model, we extract the global\nmean of the final-layer feature maps from SigLip 2 as its quality-aware\nrepresentation. To mitigate the issue of limited HDR training data, we\npre-train the FR model on a large-scale standard dynamic range (SDR) VQA\ndataset and fine-tune it on the HDRSDR-VQA dataset. For the NR model, we employ\nan iterative mixed-dataset training strategy across multiple compressed VQA\ndatasets, followed by fine-tuning on the HDRSDR-VQA dataset. Experimental\nresults show that our models achieve state-of-the-art performance compared to\nexisting FR and NR VQA models. Moreover, CompressedVQA-HDR-FR won first place\nin the FR track of the Generalizable HDR & SDR Video Quality Measurement Grand\nChallenge at IEEE ICME 2025. The code is available at\nhttps://github.com/sunwei925/CompressedVQA-HDR.", "published": "2025-07-16 04:33:06", "link": "http://arxiv.org/abs/2507.11900v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "A Spatial-Physics Informed Model for 3D Spiral Sample Scanned by SQUID Microscopy", "abstract": "The development of advanced packaging is essential in the semiconductor\nmanufacturing industry. However, non-destructive testing (NDT) of advanced\npackaging becomes increasingly challenging due to the depth and complexity of\nthe layers involved. In such a scenario, Magnetic field imaging (MFI) enables\nthe imaging of magnetic fields generated by currents. For MFI to be effective\nin NDT, the magnetic fields must be converted into current density. This\nconversion has typically relied solely on a Fast Fourier Transform (FFT) for\nmagnetic field inversion; however, the existing approach does not consider eddy\ncurrent effects or image misalignment in the test setup. In this paper, we\npresent a spatial-physics informed model (SPIM) designed for a 3D spiral sample\nscanned using Superconducting QUantum Interference Device (SQUID) microscopy.\nThe SPIM encompasses three key components: i) magnetic image enhancement by\naligning all the \"sharp\" wire field signals to mitigate the eddy current effect\nusing both in-phase (I-channel) and quadrature-phase (Q-channel) images; (ii)\nmagnetic image alignment that addresses skew effects caused by any misalignment\nof the scanning SQUID microscope relative to the wire segments; and (iii) an\ninversion method for converting magnetic fields to magnetic currents by\nintegrating the Biot-Savart Law with FFT. The results show that the SPIM\nimproves I-channel sharpness by 0.3% and reduces Q-channel sharpness by 25%.\nAlso, we were able to remove rotational and skew misalignments of 0.30 in a\nreal image. Overall, SPIM highlights the potential of combining spatial\nanalysis with physics-driven models in practical applications.", "published": "2025-07-16 02:34:33", "link": "http://arxiv.org/abs/2507.11853v1", "categories": ["physics.ins-det", "cs.CV"], "primary_category": "physics.ins-det"}
{"title": "Towards Autonomous Riding: A Review of Perception, Planning, and Control in Intelligent Two-Wheelers", "abstract": "The rapid adoption of micromobility solutions, particularly two-wheeled\nvehicles like e-scooters and e-bikes, has created an urgent need for reliable\nautonomous riding (AR) technologies. While autonomous driving (AD) systems have\nmatured significantly, AR presents unique challenges due to the inherent\ninstability of two-wheeled platforms, limited size, limited power, and\nunpredictable environments, which pose very serious concerns about road users'\nsafety. This review provides a comprehensive analysis of AR systems by\nsystematically examining their core components, perception, planning, and\ncontrol, through the lens of AD technologies. We identify critical gaps in\ncurrent AR research, including a lack of comprehensive perception systems for\nvarious AR tasks, limited industry and government support for such\ndevelopments, and insufficient attention from the research community. The\nreview analyses the gaps of AR from the perspective of AD to highlight\npromising research directions, such as multimodal sensor techniques for\nlightweight platforms and edge deep learning architectures. By synthesising\ninsights from AD research with the specific requirements of AR, this review\naims to accelerate the development of safe, efficient, and scalable autonomous\nriding systems for future urban mobility.", "published": "2025-07-16 02:33:54", "link": "http://arxiv.org/abs/2507.11852v1", "categories": ["cs.RO", "cs.CV", "93C85", "F.2.2; I.2.7"], "primary_category": "cs.RO"}
{"title": "ProtoConNet: Prototypical Augmentation and Alignment for Open-Set Few-Shot Image Classification", "abstract": "Open-set few-shot image classification aims to train models using a small\namount of labeled data, enabling them to achieve good generalization when\nconfronted with unknown environments. Existing methods mainly use visual\ninformation from a single image to learn class representations to distinguish\nknown from unknown categories. However, these methods often overlook the\nbenefits of integrating rich contextual information. To address this issue,\nthis paper proposes a prototypical augmentation and alignment method, termed\nProtoConNet, which incorporates background information from different samples\nto enhance the diversity of the feature space, breaking the spurious\nassociations between context and image subjects in few-shot scenarios.\nSpecifically, it consists of three main modules: the clustering-based data\nselection (CDS) module mines diverse data patterns while preserving core\nfeatures; the contextual-enhanced semantic refinement (CSR) module builds a\ncontext dictionary to integrate into image representations, which boosts the\nmodel's robustness in various scenarios; and the prototypical alignment (PA)\nmodule reduces the gap between image representations and class prototypes,\namplifying feature distances for known and unknown classes. Experimental\nresults from two datasets verified that ProtoConNet enhances the effectiveness\nof representation learning in few-shot scenarios and identifies open-set\nsamples, making it superior to existing methods.", "published": "2025-07-16 02:20:52", "link": "http://arxiv.org/abs/2507.11845v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "CorrMoE: Mixture of Experts with De-stylization Learning for Cross-Scene and Cross-Domain Correspondence Pruning", "abstract": "Establishing reliable correspondences between image pairs is a fundamental\ntask in computer vision, underpinning applications such as 3D reconstruction\nand visual localization. Although recent methods have made progress in pruning\noutliers from dense correspondence sets, they often hypothesize consistent\nvisual domains and overlook the challenges posed by diverse scene structures.\nIn this paper, we propose CorrMoE, a novel correspondence pruning framework\nthat enhances robustness under cross-domain and cross-scene variations. To\naddress domain shift, we introduce a De-stylization Dual Branch, performing\nstyle mixing on both implicit and explicit graph features to mitigate the\nadverse influence of domain-specific representations. For scene diversity, we\ndesign a Bi-Fusion Mixture of Experts module that adaptively integrates\nmulti-perspective features through linear-complexity attention and dynamic\nexpert routing. Extensive experiments on benchmark datasets demonstrate that\nCorrMoE achieves superior accuracy and generalization compared to\nstate-of-the-art methods. The code and pre-trained models are available at\nhttps://github.com/peiwenxia/CorrMoE.", "published": "2025-07-16 01:44:01", "link": "http://arxiv.org/abs/2507.11834v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Modeling Feasible Locomotion of Nanobots for Cancer Detection and Treatment", "abstract": "Deploying motile nanosized particles, also known as ``nanobots'', in the\nhuman body promises to improve selectivity in drug delivery and reduce side\neffects. We consider a swarm of nanobots locating a single cancerous region and\ntreating it by releasing an onboard payload of drugs at the site. At nanoscale,\nthe computation, communication, sensing, and locomotion capabilities of\nindividual agents are extremely limited, noisy, and/or nonexistent.\n  We present a general model to formally describe the individual and collective\nbehavior of agents in a colloidal environment, such as the bloodstream, for\ncancer detection and treatment by nanobots. This includes a feasible and\nprecise model of agent locomotion, inspired by actual nanoparticles that, in\nthe presence of an external chemical gradient, move towards areas of higher\nconcentration by means of self-propulsion. We present two variants of our\ngeneral model: The first assumes an endogenous chemical gradient that is fixed\nover time and centered at the targeted cancer site; the second is a more\nspeculative and dynamic variant in which agents themselves create and amplify a\nchemical gradient centered at the cancer site. In both settings, agents can\nsense the gradient and ascend it noisily, locating the cancer site more quickly\nthan via simple Brownian motion.\n  For the first variant of the model, we present simulation results to show the\nbehavior of agents under our locomotion model, as well as {analytical results}\nto bound the time it takes for the agents to reach the cancer site. For the\nsecond variant, simulation results highlight the collective benefit in having\nagents issue their own chemical signal. While arguably more speculative in its\nagent capability assumptions, this variant shows a significant improvement in\nruntime performance over the first variant, resulting from its chemical signal\namplification mechanism.", "published": "2025-07-16 16:44:50", "link": "http://arxiv.org/abs/2507.12400v1", "categories": ["cs.MA", "cs.DM"], "primary_category": "cs.MA"}
{"title": "A near-complete resolution of the exponential-time complexity of k-opt for the traveling salesman problem", "abstract": "The $k$-opt algorithm is one of the simplest and most widely used heuristics\nfor solving the traveling salesman problem. Starting from an arbitrary tour,\nthe $k$-opt algorithm improves the current tour in each iteration by exchanging\nup to $k$ edges. The algorithm continues until no further improvement of this\nkind is possible. For a long time, it remained an open question how many\niterations the $k$-opt algorithm might require for small values of $k$,\nassuming the use of an optimal pivot rule. In this paper, we resolve this\nquestion for the cases $k = 3$ and $k = 4$ by proving that in both these cases\nan exponential number of iterations may be needed even if an optimal pivot rule\nis used. Combined with a recent result from Heimann, Hoang, and Hougardy (ICALP\n2024), this provides a complete answer for all $k \\geq 3$ regarding the number\nof iterations the $k$-opt algorithm may require under an optimal pivot rule. In\naddition we establish an analogous exponential lower bound for the 2.5-opt\nalgorithm, a variant that generalizes 2-opt and is a restricted version of\n3-opt. All our results hold for both the general and the metric traveling\nsalesman problem.", "published": "2025-07-16 15:04:26", "link": "http://arxiv.org/abs/2507.12304v1", "categories": ["cs.DS", "cs.DM", "68W25, 68W40, 68Q25, 90C27", "F.2.2; G.2.1; G.2.2"], "primary_category": "cs.DS"}
{"title": "Matroids are Equitable", "abstract": "We show that if the ground set of a matroid can be partitioned into $k\\ge 2$\nbases, then for any given subset $S$ of the ground set, there is a partition\ninto $k$ bases such that the sizes of the intersections of the bases with $S$\nmay differ by at most one. This settles the matroid equitability conjecture by\nFekete and Szab\\'o (Electron.~J.~Comb.~2011) in the affirmative. We also\ninvestigate equitable splittings of two disjoint sets $S_1$ and $S_2$, and show\nthat there is a partition into $k$ bases such that the sizes of the\nintersections with $S_1$ may differ by at most one and the sizes of the\nintersections with $S_2$ may differ by at most two; this is the best possible\none can hope for arbitrary matroids.\n  We also derive applications of this result into matroid constrained fair\ndivision problems. We show that there exists a matroid-constrained fair\ndivision that is envy-free up to 1 item if the valuations are identical and\ntri-valued additive. We also show that for bi-valued additive valuations, there\nexists a matroid-constrained allocation that provides everyone their maximin\nshare.", "published": "2025-07-16 10:09:09", "link": "http://arxiv.org/abs/2507.12100v1", "categories": ["math.CO", "cs.DM", "cs.GT"], "primary_category": "math.CO"}
{"title": "The Directed Disjoint Paths Problem with Congestion", "abstract": "The classic result by Fortune, Hopcroft, and Wyllie [TCS~'80] states that the\ndirected disjoint paths problem is NP-complete even for two pairs of terminals.\nExtending this well-known result, we show that the directed disjoint paths\nproblem is NP-complete for any constant congestion $c \\geq 1$ and~$k \\geq 3c-1$\npairs of terminals. This refutes a conjecture by Giannopoulou et al.\n[SODA~'22], which says that the directed disjoint paths problem with congestion\ntwo is polynomial-time solvable for any constant number $k$ of terminal pairs.\nWe then consider the cases that are not covered by this hardness result. The\nfirst nontrivial case is $c=2$ and $k = 3$. Our second main result is to show\nthat this case is polynomial-time solvable.", "published": "2025-07-16 10:05:11", "link": "http://arxiv.org/abs/2507.12096v1", "categories": ["cs.DM", "math.CO", "05C83, 05C85, 05C10, 05C75, 68R10", "F.2.0; G.2.2"], "primary_category": "cs.DM"}
{"title": "Every Poset has a Large Cut", "abstract": "We prove that every finite poset has a directed cut with at least one half of\nthe poset's pairwise order relations. The bound is tight. Also, the largest\ndirected cut in a poset can be found in linear time.", "published": "2025-07-16 09:38:12", "link": "http://arxiv.org/abs/2507.12077v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "Unavoidable butterfly minors in digraphs of large cycle rank", "abstract": "Cycle rank is one of the depth parameters for digraphs introduced by Eggan in\n1963. We show that there exists a function $f:\\mathbb{N}\\to \\mathbb{N}$ such\nthat every digraph of cycle rank at least $f(k)$ contains a directed cycle\nchain, a directed ladder, or a directed tree chain of order $k$ as a butterfly\nminor. We also investigate a new connection between cycle rank and a directed\nanalogue of the weak coloring number of graphs.", "published": "2025-07-16 00:24:53", "link": "http://arxiv.org/abs/2507.11814v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "New allocation rule based on graph structures and their application to economic phenomena", "abstract": "This study introduces the \\emph{edge-based Shapley value}, a novel allocation\nrule within cooperative game theory, specifically tailored for networked\nsystems, where value is generated through interactions represented by edges.\nTraditional allocation rules, such as the Shapley and Myerson values, evaluate\nplayer contributions based on node-level characteristics, or connected\ncomponents. However, these approaches often fail to adequately capture the\nfunctional role of edges, which are crucial in systems such as supply chains\nand digital platforms, where interactions, rather than individual agents, are\nthe primary drivers of value. Our edge-based Shapley value shifts the\ncharacteristic function from node sets to edge sets, thereby enabling a more\ngranular and context-sensitive evaluation of the contributions. We establish\nits theoretical foundations, demonstrate its relationship to classical\nallocation rules, and show that it retains key properties such as fairness and\nsymmetry. To illustrate its applicability, we present two use cases: content\nplatform networks and supply chain logistics (SCL). In both cases, our method\nproduces intuitive and structurally consistent allocations, particularly in\nscenarios with overlapping routes, exclusive contracts or cost-sensitive paths.\nThis framework offers a new perspective on value attribution in cooperative\nsettings with complex interaction structures and provides practical tools for\nanalyzing real-world economic and logistical networks.", "published": "2025-07-16 00:06:53", "link": "http://arxiv.org/abs/2507.11808v1", "categories": ["cs.GT", "cs.DM", "Primary 68R10, Secondary 90B06"], "primary_category": "cs.GT"}
{"title": "An Ecosystem for Ontology Interoperability", "abstract": "Ontology interoperability is one of the complicated issues that restricts the\nuse of ontologies in knowledge graphs (KGs). Different ontologies with\nconflicting and overlapping concepts make it difficult to design, develop, and\ndeploy an interoperable ontology for downstream tasks. We propose an ecosystem\nfor ontology interoperability. The ecosystem employs three state-of-the-art\nsemantic techniques in different phases of the ontology engineering life cycle:\nontology design patterns (ODPs) in the design phase, ontology matching and\nversioning (OM\\&OV) in the develop phase, and ontology-compliant knowledge\ngraphs (OCKGs) in the deploy phase, to achieve better ontology interoperability\nin real-world applications. A case study in the building domain validates the\nusefulness of the proposed ecosystem.", "published": "2025-07-16 15:06:29", "link": "http://arxiv.org/abs/2507.12311v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "AFPM: Alignment-based Frame Patch Modeling for Cross-Dataset EEG Decoding", "abstract": "Electroencephalogram (EEG) decoding models for brain-computer interfaces\n(BCIs) struggle with cross-dataset learning and generalization due to channel\nlayout inconsistencies, non-stationary signal distributions, and limited\nneurophysiological prior integration. To address these issues, we propose a\nplug-and-play Alignment-Based Frame-Patch Modeling (AFPM) framework, which has\ntwo main components: 1) Spatial Alignment, which selects task-relevant channels\nbased on brain-region priors, aligns EEG distributions across domains, and\nremaps the selected channels to a unified layout; and, 2) Frame-Patch Encoding,\nwhich models multi-dataset signals into unified spatiotemporal patches for EEG\ndecoding. Compared to 17 state-of-the-art approaches that need dataset-specific\ntuning, the proposed calibration-free AFPM achieves performance gains of up to\n4.40% on motor imagery and 3.58% on event-related potential tasks. To our\nknowledge, this is the first calibration-free cross-dataset EEG decoding\nframework, substantially enhancing the practicalness of BCIs in real-world\napplications.", "published": "2025-07-16 04:55:09", "link": "http://arxiv.org/abs/2507.11911v1", "categories": ["cs.HC", "cs.IR", "cs.LG"], "primary_category": "cs.HC"}
{"title": "SIEVE: Effective Filtered Vector Search with Collection of Indexes", "abstract": "Many real-world tasks such as recommending videos with the kids tag can be\nreduced to finding most similar vectors associated with hard predicates. This\ntask, filtered vector search, is challenging as prior state-of-the-art\ngraph-based (unfiltered) similarity search techniques quickly degenerate when\nhard constraints are considered. That is, effective graph-based filtered\nsimilarity search relies on sufficient connectivity for reaching the most\nsimilar items within just a few hops. To consider predicates, recent works\npropose modifying graph traversal to visit only the items that may satisfy\npredicates. However, they fail to offer the just-a-few-hops property for a wide\nrange of predicates: they must restrict predicates significantly or lose\nefficiency if only a small fraction of items satisfy predicates.\n  We propose an opposite approach: instead of constraining traversal, we build\nmany indexes each serving different predicate forms. For effective\nconstruction, we devise a three-dimensional analytical model capturing\nrelationships among index size, search time, and recall, with which we follow a\nworkload-aware approach to pack as many useful indexes as possible into a\ncollection. At query time, the analytical model is employed yet again to\ndiscern the one that offers the fastest search at a given recall. We show\nsuperior performance and support on datasets with varying selectivities and\nforms: our approach achieves up to 8.06x speedup while having as low as 1%\nbuild time versus other indexes, with less than 2.15x memory of a standard HNSW\ngraph and modest knowledge of past workloads.", "published": "2025-07-16 04:46:28", "link": "http://arxiv.org/abs/2507.11907v1", "categories": ["cs.DB", "cs.IR"], "primary_category": "cs.DB"}
{"title": "Context-Aware Search and Retrieval Over Erasure Channels", "abstract": "This paper introduces and analyzes a search and retrieval model that adopts\nkey semantic communication principles from retrieval-augmented generation. We\nspecifically present an information-theoretic analysis of a remote document\nretrieval system operating over a symbol erasure channel. The proposed model\nencodes the feature vector of a query, derived from term-frequency weights of a\nlanguage corpus by using a repetition code with an adaptive rate dependent on\nthe contextual importance of the terms. At the decoder, we select between two\ndocuments based on the contextual closeness of the recovered query. By\nleveraging a jointly Gaussian approximation for both the true and reconstructed\nsimilarity scores, we derive an explicit expression for the retrieval error\nprobability, i.e., the probability under which the less similar document is\nselected. Numerical simulations on synthetic and real-world data (Google NQ)\nconfirm the validity of the analysis. They further demonstrate that assigning\ngreater redundancy to critical features effectively reduces the error rate,\nhighlighting the effectiveness of semantic-aware feature encoding in\nerror-prone communication settings.", "published": "2025-07-16 04:21:46", "link": "http://arxiv.org/abs/2507.11894v1", "categories": ["cs.IR", "cs.IT", "math.IT"], "primary_category": "cs.IR"}
{"title": "Similarity-Guided Diffusion for Contrastive Sequential Recommendation", "abstract": "In sequential recommendation systems, data augmentation and contrastive\nlearning techniques have recently been introduced using diffusion models to\nachieve robust representation learning. However, most of the existing\napproaches use random augmentation, which risk damaging the contextual\ninformation of the original sequence. Accordingly, we propose a\nSimilarity-Guided Diffusion for Contrastive Sequential Recommendation. Our\nmethod leverages the similarity between item embedding vectors to generate\nsemantically consistent noise. Moreover, we utilize high confidence score in\nthe denoising process to select our augmentation positions. This approach more\neffectively reflects contextual and structural information compared to\naugmentation at random positions. From a contrastive learning perspective, the\nproposed augmentation technique provides more discriminative positive and\nnegative samples, simultaneously improving training efficiency and\nrecommendation performance. Experimental results on five benchmark datasets\nshow that SimDiffRec outperforms the existing baseline models.", "published": "2025-07-16 03:26:24", "link": "http://arxiv.org/abs/2507.11866v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Efficient Remote Monitoring through Noisy Random Access with Retransmissions", "abstract": "We consider a rare event monitoring system consisting of a set of devices and\na base station, where devices transmit information about rare events to the\nbase station using a random multiple access scheme. We introduce a model in\nwhich the presence of noise in the multiple access channel can cause message\nloss even in the absence of transmission collisions. The occurrence of events\nis modeled by a family of independent two-state Markov chains (with states 0\nand 1). We analyze how repeated transmissions affect system performance. Two\nefficiency criteria are proposed and studied: the maximum probability that a\nmessage about an event from a fixed device is successfully delivered to the\nbase station and the maximum frequency at which the base station successfully\nreceives updates about the entire system. For each criterion, we determine the\noptimal number of retransmissions as a function of the system parameters.", "published": "2025-07-16 16:11:36", "link": "http://arxiv.org/abs/2507.12368v1", "categories": ["cs.IT", "math.IT", "math.PR"], "primary_category": "cs.IT"}
{"title": "Leveraging Bi-Directional Channel Reciprocity for Robust Ultra-Low-Rate Implicit CSI Feedback with Deep Learning", "abstract": "Deep learning-based implicit channel state information (CSI) feedback has\nbeen introduced to enhance spectral efficiency in massive MIMO systems.\nExisting methods often show performance degradation in ultra-low-rate scenarios\nand inadaptability across diverse environments. In this paper, we propose\nDual-ImRUNet, an efficient uplink-assisted deep implicit CSI feedback framework\nincorporating two novel plug-in preprocessing modules to achieve ultra-low\nfeedback rates while maintaining high environmental robustness. First, a novel\nbi-directional correlation enhancement module is proposed to strengthen the\ncorrelation between uplink and downlink CSI eigenvector matrices. This module\nprojects highly correlated uplink and downlink channel matrices into their\nrespective eigenspaces, effectively reducing redundancy for ultra-low-rate\nfeedback. Second, an innovative input format alignment module is designed to\nmaintain consistent data distributions at both encoder and decoder sides\nwithout extra transmission overhead, thereby enhancing robustness against\nenvironmental variations. Finally, we develop an efficient transformer-based\nimplicit CSI feedback network to exploit angular-delay domain sparsity and\nbi-directional correlation for ultra-low-rate CSI compression. Simulation\nresults demonstrate successful reduction of the feedback overhead by 85%\ncompared with the state-of-the-art method and robustness against unseen\nenvironments.", "published": "2025-07-16 15:00:39", "link": "http://arxiv.org/abs/2507.12301v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "Characterization and constructions of binary self-orthogonal singly-even linear codes", "abstract": "Recent research has focused extensively on constructing binary\nself-orthogonal (SO) linear codes due to their applications in quantum\ninformation theory, lattice design, and related areas. Despite significant\nactivity, the fundamental characterization remains unchanged: binary SO codes\nare necessarily even (all codeword weights even), while doubly-even codes\n(weights divisible by $4$) are automatically SO.\n  This paper advances the theory by addressing the understudied case of\nsingly-even (even but not doubly-even) SO codes. We first provide a complete\ncharacterization of binary SO linear codes, and a necessary and sufficient\ncondition for binary SO singly-even linear codes is given. Moreover, we give a\ngeneral approach to generating many binary SO linear codes from two known SO\nlinear codes, yielding three infinite classes of binary SO singly-even linear\ncodes with few weights. Note that these new codes are also minimal and violate\nthe Aschikhmin-Barg condition. Their weight distributions are determined.\nFurthermore, we give a necessary and sufficient condition for a Boolean\nfunction $f$ such that the linear code proposed from $f$ via a well-known\ngeneric construction is SO singly-even, and a general approach to constructing\nBoolean functions satisfying this condition is provided, yielding several\ninfinite classes of binary SO singly-even minimal linear codes with few\nweights. Finally, we would like to emphasize that using the methods in this\npaper, we can construct more binary linear codes that are SO, singly-even,\nminimal, violating the AB condition, and with few weights at the same time.", "published": "2025-07-16 13:50:20", "link": "http://arxiv.org/abs/2507.12240v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Lowering Error Floors for Hard Decision Decoding of OFEC Code", "abstract": "Stall patterns are known to cause an error floor in hard decision decoding of\nthe OFEC code. We propose a novel stall pattern removal algorithm that lowers\nthe error floor of state-of-the-art algorithms by an order of magnitude", "published": "2025-07-16 11:34:15", "link": "http://arxiv.org/abs/2507.12155v1", "categories": ["cs.IT", "math.IT", "94-06"], "primary_category": "cs.IT"}
{"title": "On the error correction of iterative bounded distance decoding of generalized LDPC codes", "abstract": "Consider an ensemble of regular generalized LDPC (GLDPC) codes and assume\nthat the same component code is associated with each parity check node. To\ndecode a GLDPC code from the ensemble, we use the bit flipping bounded distance\ndecoding algorithm, which is an extension of the bit flipping algorithm for\nLDPC codes. Previous work has shown conditions, under which, for a typical code\nin the ensemble with blocklength sufficiently large, a positive constant\nfraction of worst case errors can be corrected. In this work we first show that\nthese requirements can be relaxed for ensembles with small left degrees. While\nprevious work on GLDPC codes has considered expander graph arguments, our\nanalysis formulates a necessary condition that the Tanner graph needs to\nsatisfy for a failure event and then shows that the probability of this event\nvanishes for a sufficiently large blocklength. We then extend the analysis to\nrandom error correction and derive a lower bound on the fraction of random\nerrors that can be corrected asymptotically. We discuss the extension of our\nresults to non-binary GLDPC codes and present numerical examples.", "published": "2025-07-16 09:30:48", "link": "http://arxiv.org/abs/2507.12073v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "The Role of Rank in Mismatched Low-Rank Symmetric Matrix Estimation", "abstract": "We investigate the performance of a Bayesian statistician tasked with\nrecovering a rank-\\(k\\) signal matrix \\(\\bS \\bS^{\\top} \\in \\mathbb{R}^{n \\times\nn}\\), corrupted by element-wise additive Gaussian noise. This problem lies at\nthe core of numerous applications in machine learning, signal processing, and\nstatistics. We derive an analytic expression for the asymptotic mean-square\nerror (MSE) of the Bayesian estimator under mismatches in the assumed signal\nrank, signal power, and signal-to-noise ratio (SNR), considering both sphere\nand Gaussian signals. Additionally, we conduct a rigorous analysis of how rank\nmismatch influences the asymptotic MSE. Our primary technical tools include the\nspectrum of Gaussian orthogonal ensembles (GOE) with low-rank perturbations and\nasymptotic behavior of \\(k\\)-dimensional spherical integrals.", "published": "2025-07-16 08:24:44", "link": "http://arxiv.org/abs/2507.12019v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Sub-Connected Hybrid Beamfocusing Design for RSMA-Enabled Near-Field Communications with Imperfect CSI and SIC", "abstract": "Near-field spherical waves inherently encode both direction and distance\ninformation, enabling spotlight-like beam focusing for targeted interference\nmitigation. However, whether such beam focusing can fully eliminate\ninterference under perfect and imperfect channel state information (CSI),\nrendering advanced interference management schemes unnecessary, remains an open\nquestion. To address this, we investigate rate-splitting multiple access\n(RSMA)-enabled near-field communications (NFC) under imperfect SCI. Our\ntransmit scheme employs a sub-connected hybrid analog-digital (HAD)\narchitecture to reduce hardware overhead while incorporating imperfect\nsuccessive interference cancellation (SIC) for practical implementation. A\nminimum rate maximization problem is formulated by jointly optimizing the\nanalog beamfocuser, the digital beamfocuser, and the common rate allocation. To\nsolve the non-convex problem, we develop a penalty-based block coordinate\ndescent (BCD) algorithm, deriving closed-form expressions for the optimal\nanalog and digital beamfocusers solutions. Furthermore, to reduce computational\ncomplexity, we propose a low-complexity algorithm, where analog and digital\nbeamfocusers are designed in two separate stages. Simulation results underscore\nthat: 1) beamfocusing alone is insufficient to fully suppress interference even\nunder perfect CSI; 2) RSMA exhibits superior interference management over SDMA\nunder imperfect CSI and SIC conditions; 3) sub-connected HAD architecture\ndelivers near-optimal digital beamfocusing performance with fewer radio\nfrequency chains.", "published": "2025-07-16 02:37:58", "link": "http://arxiv.org/abs/2507.11854v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Cost-aware Stopping for Bayesian Optimization", "abstract": "In automated machine learning, scientific discovery, and other applications\nof Bayesian optimization, deciding when to stop evaluating expensive black-box\nfunctions is an important practical consideration. While several adaptive\nstopping rules have been proposed, in the cost-aware setting they lack\nguarantees ensuring they stop before incurring excessive function evaluation\ncosts. We propose a cost-aware stopping rule for Bayesian optimization that\nadapts to varying evaluation costs and is free of heuristic tuning. Our rule is\ngrounded in a theoretical connection to state-of-the-art cost-aware acquisition\nfunctions, namely the Pandora's Box Gittins Index (PBGI) and log expected\nimprovement per cost. We prove a theoretical guarantee bounding the expected\ncumulative evaluation cost incurred by our stopping rule when paired with these\ntwo acquisition functions. In experiments on synthetic and empirical tasks,\nincluding hyperparameter optimization and neural architecture size search, we\nshow that combining our stopping rule with the PBGI acquisition function\nconsistently matches or outperforms other acquisition-function--stopping-rule\npairs in terms of cost-adjusted simple regret, a metric capturing trade-offs\nbetween solution quality and cumulative evaluation cost.", "published": "2025-07-16 17:54:14", "link": "http://arxiv.org/abs/2507.12453v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "A Bayesian Incentive Mechanism for Poison-Resilient Federated Learning", "abstract": "Federated learning (FL) enables collaborative model training across\ndecentralized clients while preserving data privacy. However, its\nopen-participation nature exposes it to data-poisoning attacks, in which\nmalicious actors submit corrupted model updates to degrade the global model.\nExisting defenses are often reactive, relying on statistical aggregation rules\nthat can be computationally expensive and that typically assume an honest\nmajority. This paper introduces a proactive, economic defense: a lightweight\nBayesian incentive mechanism that makes malicious behavior economically\nirrational. Each training round is modeled as a Bayesian game of incomplete\ninformation in which the server, acting as the principal, uses a small, private\nvalidation dataset to verify update quality before issuing payments. The design\nsatisfies Individual Rationality (IR) for benevolent clients, ensuring their\nparticipation is profitable, and Incentive Compatibility (IC), making poisoning\nan economically dominated strategy. Extensive experiments on non-IID partitions\nof MNIST and FashionMNIST demonstrate robustness: with 50% label-flipping\nadversaries on MNIST, the mechanism maintains 96.7% accuracy, only 0.3\npercentage points lower than in a scenario with 30% label-flipping adversaries.\nThis outcome is 51.7 percentage points better than standard FedAvg, which\ncollapses under the same 50% attack. The mechanism is computationally light,\nbudget-bounded, and readily integrates into existing FL frameworks, offering a\npractical route to economically robust and sustainable FL ecosystems.", "published": "2025-07-16 17:27:25", "link": "http://arxiv.org/abs/2507.12439v1", "categories": ["cs.LG", "cs.CR", "cs.GT"], "primary_category": "cs.LG"}
{"title": "Targeted Deep Architectures: A TMLE-Based Framework for Robust Causal Inference in Neural Networks", "abstract": "Modern deep neural networks are powerful predictive tools yet often lack\nvalid inference for causal parameters, such as treatment effects or entire\nsurvival curves. While frameworks like Double Machine Learning (DML) and\nTargeted Maximum Likelihood Estimation (TMLE) can debias machine-learning fits,\nexisting neural implementations either rely on \"targeted losses\" that do not\nguarantee solving the efficient influence function equation or computationally\nexpensive post-hoc \"fluctuations\" for multi-parameter settings. We propose\nTargeted Deep Architectures (TDA), a new framework that embeds TMLE directly\ninto the network's parameter space with no restrictions on the backbone\narchitecture. Specifically, TDA partitions model parameters - freezing all but\na small \"targeting\" subset - and iteratively updates them along a targeting\ngradient, derived from projecting the influence functions onto the span of the\ngradients of the loss with respect to weights. This procedure yields plug-in\nestimates that remove first-order bias and produce asymptotically valid\nconfidence intervals. Crucially, TDA easily extends to multi-dimensional causal\nestimands (e.g., entire survival curves) by merging separate targeting\ngradients into a single universal targeting update. Theoretically, TDA inherits\nclassical TMLE properties, including double robustness and semiparametric\nefficiency. Empirically, on the benchmark IHDP dataset (average treatment\neffects) and simulated survival data with informative censoring, TDA reduces\nbias and improves coverage relative to both standard neural-network estimators\nand prior post-hoc approaches. In doing so, TDA establishes a direct, scalable\npathway toward rigorous causal inference within modern deep architectures for\ncomplex multi-parameter targets.", "published": "2025-07-16 17:24:06", "link": "http://arxiv.org/abs/2507.12435v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Neural Network-Guided Symbolic Regression for Interpretable Descriptor Discovery in Perovskite Catalysts", "abstract": "Understanding and predicting the activity of oxide perovskite catalysts for\nthe oxygen evolution reaction (OER) requires descriptors that are both accurate\nand physically interpretable. While symbolic regression (SR) offers a path to\ndiscover such formulas, its performance degrades with high-dimensional inputs\nand small datasets. We present a two-phase framework that combines neural\nnetworks (NN), feature importance analysis, and symbolic regression (SR) to\ndiscover interpretable descriptors for OER activity in oxide perovskites. In\nPhase I, using a small dataset and seven structural features, we reproduce and\nimprove the known {\\mu}/t descriptor by engineering composite features and\napplying symbolic regression, achieving training and validation MAEs of 22.8\nand 20.8 meV, respectively. In Phase II, we expand to 164 features, reduce\ndimensionality, and identify LUMO energy as a key electronic descriptor. A\nfinal formula using {\\mu}/t, {\\mu}/RA, and LUMO energy achieves improved\naccuracy (training and validation MAEs of 22.1 and 20.6 meV) with strong\nphysical interpretability. Our results demonstrate that NN-guided symbolic\nregression enables accurate, interpretable, and physically meaningful\ndescriptor discovery in data-scarce regimes, indicating interpretability need\nnot sacrifice accuracy for materials informatics.", "published": "2025-07-16 16:47:38", "link": "http://arxiv.org/abs/2507.12404v1", "categories": ["physics.data-an", "cond-mat.mtrl-sci", "cs.LG", "physics.comp-ph"], "primary_category": "physics.data-an"}
{"title": "ROC-n-reroll: How verifier imperfection affects test-time scaling", "abstract": "Test-time scaling aims to improve language model performance by leveraging\nadditional compute during inference. While many works have empirically studied\ntechniques like Best-of-N (BoN) and rejection sampling that make use of a\nverifier to enable test-time scaling, there is little theoretical understanding\nof how verifier imperfection affects performance. In this work, we address this\ngap. Specifically, we prove how instance-level accuracy of these methods is\nprecisely characterized by the geometry of the verifier's ROC curve.\nInterestingly, while scaling is determined by the local geometry of the ROC\ncurve for rejection sampling, it depends on global properties of the ROC curve\nfor BoN. As a consequence when the ROC curve is unknown, it is impossible to\nextrapolate the performance of rejection sampling based on the low-compute\nregime. Furthermore, while rejection sampling outperforms BoN for fixed\ncompute, in the infinite-compute limit both methods converge to the same level\nof accuracy, determined by the slope of the ROC curve near the origin. Our\ntheoretical results are confirmed by experiments on GSM8K using different\nversions of Llama and Qwen to generate and verify solutions.", "published": "2025-07-16 16:44:29", "link": "http://arxiv.org/abs/2507.12399v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Trustworthy Tree-based Machine Learning by $MoS_2$ Flash-based Analog CAM with Inherent Soft Boundaries", "abstract": "The rapid advancement of artificial intelligence has raised concerns\nregarding its trustworthiness, especially in terms of interpretability and\nrobustness. Tree-based models like Random Forest and XGBoost excel in\ninterpretability and accuracy for tabular data, but scaling them remains\ncomputationally expensive due to poor data locality and high data dependence.\nPrevious efforts to accelerate these models with analog content addressable\nmemory (CAM) have struggled, due to the fact that the difficult-to-implement\nsharp decision boundaries are highly susceptible to device variations, which\nleads to poor hardware performance and vulnerability to adversarial attacks.\nThis work presents a novel hardware-software co-design approach using $MoS_2$\nFlash-based analog CAM with inherent soft boundaries, enabling efficient\ninference with soft tree-based models. Our soft tree model inference\nexperiments on $MoS_2$ analog CAM arrays show this method achieves exceptional\nrobustness against device variation and adversarial attacks while achieving\nstate-of-the-art accuracy. Specifically, our fabricated analog CAM arrays\nachieve $96\\%$ accuracy on Wisconsin Diagnostic Breast Cancer (WDBC) database,\nwhile maintaining decision explainability. Our experimentally calibrated model\nvalidated only a $0.6\\%$ accuracy drop on the MNIST dataset under $10\\%$ device\nthreshold variation, compared to a $45.3\\%$ drop for traditional decision\ntrees. This work paves the way for specialized hardware that enhances AI's\ntrustworthiness and efficiency.", "published": "2025-07-16 16:31:20", "link": "http://arxiv.org/abs/2507.12384v1", "categories": ["cs.LG", "cs.ET"], "primary_category": "cs.LG"}
{"title": "Improving Reinforcement Learning Sample-Efficiency using Local Approximation", "abstract": "In this study, we derive Probably Approximately Correct (PAC) bounds on the\nasymptotic sample-complexity for RL within the infinite-horizon Markov Decision\nProcess (MDP) setting that are sharper than those in existing literature. The\npremise of our study is twofold: firstly, the further two states are from each\nother, transition-wise, the less relevant the value of the first state is when\nlearning the $\\epsilon$-optimal value of the second; secondly, the amount of\n'effort', sample-complexity-wise, expended in learning the $\\epsilon$-optimal\nvalue of a state is independent of the number of samples required to learn the\n$\\epsilon$-optimal value of a second state that is a sufficient number of\ntransitions away from the first. Inversely, states within each other's vicinity\nhave values that are dependent on each other and will require a similar number\nof samples to learn. By approximating the original MDP using smaller MDPs\nconstructed using subsets of the original's state-space, we are able to reduce\nthe sample-complexity by a logarithmic factor to $O(SA \\log A)$ timesteps,\nwhere $S$ and $A$ are the state and action space sizes. We are able to extend\nthese results to an infinite-horizon, model-free setting by constructing a\nPAC-MDP algorithm with the aforementioned sample-complexity. We conclude with\nshowing how significant the improvement is by comparing our algorithm against\nprior work in an experimental setting.", "published": "2025-07-16 16:31:17", "link": "http://arxiv.org/abs/2507.12383v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Heat Kernel Goes Topological", "abstract": "Topological neural networks have emerged as powerful successors of graph\nneural networks. However, they typically involve higher-order message passing,\nwhich incurs significant computational expense. We circumvent this issue with a\nnovel topological framework that introduces a Laplacian operator on\ncombinatorial complexes (CCs), enabling efficient computation of heat kernels\nthat serve as node descriptors. Our approach captures multiscale information\nand enables permutation-equivariant representations, allowing easy integration\ninto modern transformer-based architectures.\n  Theoretically, the proposed method is maximally expressive because it can\ndistinguish arbitrary non-isomorphic CCs. Empirically, it significantly\noutperforms existing topological methods in terms of computational efficiency.\nBesides demonstrating competitive performance with the state-of-the-art\ndescriptors on standard molecular datasets, it exhibits superior capability in\ndistinguishing complex topological structures and avoiding blind spots on\ntopological benchmarks. Overall, this work advances topological deep learning\nby providing expressive yet scalable representations, thereby opening up\nexciting avenues for molecular classification and property prediction tasks.", "published": "2025-07-16 16:28:10", "link": "http://arxiv.org/abs/2507.12380v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Robust Causal Discovery in Real-World Time Series with Power-Laws", "abstract": "Exploring causal relationships in stochastic time series is a challenging yet\ncrucial task with a vast range of applications, including finance, economics,\nneuroscience, and climate science. Many algorithms for Causal Discovery (CD)\nhave been proposed, but they often exhibit a high sensitivity to noise,\nresulting in misleading causal inferences when applied to real data. In this\npaper, we observe that the frequency spectra of typical real-world time series\nfollow a power-law distribution, notably due to an inherent self-organizing\nbehavior. Leveraging this insight, we build a robust CD method based on the\nextraction of power -law spectral features that amplify genuine causal signals.\nOur method consistently outperforms state-of-the-art alternatives on both\nsynthetic benchmarks and real-world datasets with known causal structures,\ndemonstrating its robustness and practical relevance.", "published": "2025-07-16 14:02:21", "link": "http://arxiv.org/abs/2507.12257v1", "categories": ["cs.LG", "physics.data-an", "stat.ML", "stat.OT"], "primary_category": "cs.LG"}
{"title": "Surrogate Quantum Circuit Design for the Lattice Boltzmann Collision Operator", "abstract": "Direct numerical simulation of turbulent flows at high Reynolds numbers\nremains a major challenge for traditional computational fluid dynamics (CFD)\ntools running on classical computer hardware. This has motivated growing\ninterest in quantum algorithms for CFD to enable flow simulations on quantum\ncomputers. The reason being that these computers are expected to deliver\npotential speed-ups for certain problems. One promising quantum CFD approach is\na fully quantum implementation of the lattice Boltzmann method called QLBM.\nAlthough efficient quantum routines are now available for the streaming step,\nimplementing the nonlinear, irreversible collision step with a low depth\ncircuit that avoids additional ancilla qubits, probabilistic post-selection and\nrepeated executions remains a significant challenge. In this study, we address\nthis challenge by introducing a framework for learning a surrogate quantum\ncircuit (SQC) that approximates the full Bhatnagar Gross Krook (BGK) collision\noperator for the D2Q9 lattice. The four qubit circuit is trained to respect the\nphysical properties of the BGK collision operator, including mass and momentum\nconservation, D8 equivariance and scale equivariance. When compiled to the gate\nset used by IBM Heron processor under the assumption of full qubit\nconnectivity, the 15 block SQC requires only 2,430 native gates and uses\nneither ancilla qubits nor post-selection or repeated executions. Moreover, its\ndepth is independent of the grid resolution, as collision is a local operation\nthat can exploit quantum parallelism to its full extent. We validate the SQC on\ntwo benchmark flows, the Taylor Green vortex decay and the lid driven cavity,\ndemonstrating that it accurately captures vortex dissipation and flow\nrecirculation.", "published": "2025-07-16 14:02:01", "link": "http://arxiv.org/abs/2507.12256v1", "categories": ["quant-ph", "cs.LG", "physics.comp-ph"], "primary_category": "quant-ph"}
{"title": "Universal Fourier Neural Operators for Micromechanics", "abstract": "\\noindent Solving cell problems in homogenization is hard, and available\ndeep-learning frameworks fail to match the speed and generality of traditional\ncomputational frameworks. More to the point, it is generally unclear what to\nexpect of machine-learning approaches, let alone single out which approaches\nare promising. In the work at hand, we advocate Fourier Neural Operators (FNOs)\nfor micromechanics, empowering them by insights from computational\nmicromechanics methods based on the fast Fourier transform (FFT). We construct\nan FNO surrogate mimicking the basic scheme foundational for FFT-based methods\nand show that the resulting operator predicts solutions to cell problems with\n\\emph{arbitrary} stiffness distribution only subject to a material-contrast\nconstraint up to a desired accuracy. In particular, there are no restrictions\non the material symmetry like isotropy, on the number of phases and on the\ngeometry of the interfaces between materials. Also, the provided fidelity is\nsharp and uniform, providing explicit guarantees leveraging our physical\nempowerment of FNOs. To show the desired universal approximation property, we\nconstruct an FNO explicitly that requires no training to begin with. Still, the\nobtained neural operator complies with the same memory requirements as the\nbasic scheme and comes with runtimes proportional to classical FFT solvers. In\nparticular, large-scale problems with more than 100 million voxels are readily\nhandled. The goal of this work is to underline the potential of FNOs for\nsolving micromechanical problems, linking FFT-based methods to FNOs. This\nconnection is expected to provide a fruitful exchange between both worlds.", "published": "2025-07-16 13:47:20", "link": "http://arxiv.org/abs/2507.12233v1", "categories": ["cs.CE", "cs.LG"], "primary_category": "cs.CE"}
{"title": "Optimizers Qualitatively Alter Solutions And We Should Leverage This", "abstract": "Due to the nonlinear nature of Deep Neural Networks (DNNs), one can not\nguarantee convergence to a unique global minimum of the loss when using\noptimizers relying only on local information, such as SGD. Indeed, this was a\nprimary source of skepticism regarding the feasibility of DNNs in the early\ndays of the field. The past decades of progress in deep learning have revealed\nthis skepticism to be misplaced, and a large body of empirical evidence shows\nthat sufficiently large DNNs following standard training protocols exhibit\nwell-behaved optimization dynamics that converge to performant solutions. This\nsuccess has biased the community to use convex optimization as a mental model\nfor learning, leading to a focus on training efficiency, either in terms of\nrequired iteration, FLOPs or wall-clock time, when improving optimizers. We\nargue that, while this perspective has proven extremely fruitful, another\nperspective specific to DNNs has received considerably less attention: the\noptimizer not only influences the rate of convergence, but also the qualitative\nproperties of the learned solutions. Restated, the optimizer can and will\nencode inductive biases and change the effective expressivity of a given class\nof models. Furthermore, we believe the optimizer can be an effective way of\nencoding desiderata in the learning process. We contend that the community\nshould aim at understanding the biases of already existing methods, as well as\naim to build new optimizers with the explicit intent of inducing certain\nproperties of the solution, rather than solely judging them based on their\nconvergence rates. We hope our arguments will inspire research to improve our\nunderstanding of how the learning process can impact the type of solution we\nconverge to, and lead to a greater recognition of optimizers design as a\ncritical lever that complements the roles of architecture and data in shaping\nmodel outcomes.", "published": "2025-07-16 13:33:31", "link": "http://arxiv.org/abs/2507.12224v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Physics-Informed Linear Model (PILM): Analytical Representations and Application to Crustal Strain Rate Estimation", "abstract": "Many physical systems are described by partial differential equations (PDEs),\nand solving these equations and estimating their coefficients or boundary\nconditions (BCs) from observational data play a crucial role in understanding\nthe associated phenomena. Recently, a machine learning approach known as\nphysics-informed neural network, which solves PDEs using neural networks by\nminimizing the sum of residuals from the PDEs, BCs, and data, has gained\nsignificant attention in the scientific community. In this study, we\ninvestigate a physics-informed linear model (PILM) that uses linear\ncombinations of basis functions to represent solutions, thereby enabling an\nanalytical representation of optimal solutions. The PILM was formulated and\nverified for illustrative forward and inverse problems including cases with\nuncertain BCs. Furthermore, the PILM was applied to estimate crustal strain\nrates using geodetic data. Specifically, physical regularization that enforces\nelastic equilibrium on the velocity fields was compared with mathematical\nregularization that imposes smoothness constraints. From a Bayesian\nperspective, mathematical regularization exhibited superior performance. The\nPILM provides an analytically solvable framework applicable to linear forward\nand inverse problems, underdetermined systems, and physical regularization.", "published": "2025-07-16 13:23:39", "link": "http://arxiv.org/abs/2507.12218v1", "categories": ["cs.LG", "physics.geo-ph"], "primary_category": "cs.LG"}
{"title": "Explainable Evidential Clustering", "abstract": "Unsupervised classification is a fundamental machine learning problem.\nReal-world data often contain imperfections, characterized by uncertainty and\nimprecision, which are not well handled by traditional methods. Evidential\nclustering, based on Dempster-Shafer theory, addresses these challenges. This\npaper explores the underexplored problem of explaining evidential clustering\nresults, which is crucial for high-stakes domains such as healthcare. Our\nanalysis shows that, in the general case, representativity is a necessary and\nsufficient condition for decision trees to serve as abductive explainers.\nBuilding on the concept of representativity, we generalize this idea to\naccommodate partial labeling through utility functions. These functions enable\nthe representation of \"tolerable\" mistakes, leading to the definition of\nevidential mistakeness as explanation cost and the construction of explainers\ntailored to evidential classifiers. Finally, we propose the Iterative\nEvidential Mistake Minimization (IEMM) algorithm, which provides interpretable\nand cautious decision tree explanations for evidential clustering functions. We\nvalidate the proposed algorithm on synthetic and real-world data. Taking into\naccount the decision-maker's preferences, we were able to provide an\nexplanation that was satisfactory up to 93% of the time.", "published": "2025-07-16 12:44:25", "link": "http://arxiv.org/abs/2507.12192v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "RadioDiff-3D: A 3D$\\times$3D Radio Map Dataset and Generative Diffusion Based Benchmark for 6G Environment-Aware Communication", "abstract": "Radio maps (RMs) serve as a critical foundation for enabling\nenvironment-aware wireless communication, as they provide the spatial\ndistribution of wireless channel characteristics. Despite recent progress in RM\nconstruction using data-driven approaches, most existing methods focus solely\non pathloss prediction in a fixed 2D plane, neglecting key parameters such as\ndirection of arrival (DoA), time of arrival (ToA), and vertical spatial\nvariations. Such a limitation is primarily due to the reliance on static\nlearning paradigms, which hinder generalization beyond the training data\ndistribution. To address these challenges, we propose UrbanRadio3D, a\nlarge-scale, high-resolution 3D RM dataset constructed via ray tracing in\nrealistic urban environments. UrbanRadio3D is over 37$\\times$3 larger than\nprevious datasets across a 3D space with 3 metrics as pathloss, DoA, and ToA,\nforming a novel 3D$\\times$33D dataset with 7$\\times$3 more height layers than\nprior state-of-the-art (SOTA) dataset. To benchmark 3D RM construction, a UNet\nwith 3D convolutional operators is proposed. Moreover, we further introduce\nRadioDiff-3D, a diffusion-model-based generative framework utilizing the 3D\nconvolutional architecture. RadioDiff-3D supports both radiation-aware\nscenarios with known transmitter locations and radiation-unaware settings based\non sparse spatial observations. Extensive evaluations on UrbanRadio3D validate\nthat RadioDiff-3D achieves superior performance in constructing rich,\nhigh-dimensional radio maps under diverse environmental dynamics. This work\nprovides a foundational dataset and benchmark for future research in 3D\nenvironment-aware communication. The dataset is available at\nhttps://github.com/UNIC-Lab/UrbanRadio3D.", "published": "2025-07-16 11:54:08", "link": "http://arxiv.org/abs/2507.12166v1", "categories": ["cs.LG", "cs.SY", "eess.SY"], "primary_category": "cs.LG"}
{"title": "Multi-Component VAE with Gaussian Markov Random Field", "abstract": "Multi-component datasets with intricate dependencies, like industrial\nassemblies or multi-modal imaging, challenge current generative modeling\ntechniques. Existing Multi-component Variational AutoEncoders typically rely on\nsimplified aggregation strategies, neglecting critical nuances and consequently\ncompromising structural coherence across generated components. To explicitly\naddress this gap, we introduce the Gaussian Markov Random Field Multi-Component\nVariational AutoEncoder , a novel generative framework embedding Gaussian\nMarkov Random Fields into both prior and posterior distributions. This design\nchoice explicitly models cross-component relationships, enabling richer\nrepresentation and faithful reproduction of complex interactions. Empirically,\nour GMRF MCVAE achieves state-of-the-art performance on a synthetic Copula\ndataset specifically constructed to evaluate intricate component relationships,\ndemonstrates competitive results on the PolyMNIST benchmark, and significantly\nenhances structural coherence on the real-world BIKED dataset. Our results\nindicate that the GMRF MCVAE is especially suited for practical applications\ndemanding robust and realistic modeling of multi-component coherence", "published": "2025-07-16 11:53:08", "link": "http://arxiv.org/abs/2507.12165v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "FourCastNet 3: A geometric approach to probabilistic machine-learning weather forecasting at scale", "abstract": "FourCastNet 3 advances global weather modeling by implementing a scalable,\ngeometric machine learning (ML) approach to probabilistic ensemble forecasting.\nThe approach is designed to respect spherical geometry and to accurately model\nthe spatially correlated probabilistic nature of the problem, resulting in\nstable spectra and realistic dynamics across multiple scales. FourCastNet 3\ndelivers forecasting accuracy that surpasses leading conventional ensemble\nmodels and rivals the best diffusion-based methods, while producing forecasts 8\nto 60 times faster than these approaches. In contrast to other ML approaches,\nFourCastNet 3 demonstrates excellent probabilistic calibration and retains\nrealistic spectra, even at extended lead times of up to 60 days. All of these\nadvances are realized using a purely convolutional neural network architecture\ntailored for spherical geometry. Scalable and efficient large-scale training on\n1024 GPUs and more is enabled by a novel training paradigm for combined model-\nand data-parallelism, inspired by domain decomposition methods in classical\nnumerical models. Additionally, FourCastNet 3 enables rapid inference on a\nsingle GPU, producing a 90-day global forecast at 0.25{\\deg}, 6-hourly\nresolution in under 20 seconds. Its computational efficiency, medium-range\nprobabilistic skill, spectral fidelity, and rollout stability at subseasonal\ntimescales make it a strong candidate for improving meteorological forecasting\nand early warning systems through large ensemble predictions.", "published": "2025-07-16 11:22:18", "link": "http://arxiv.org/abs/2507.12144v1", "categories": ["cs.LG", "physics.ao-ph", "86-10, 68T07", "I.2.1; I.6.5; G.3"], "primary_category": "cs.LG"}
{"title": "HyDRA: A Hybrid Dual-Mode Network for Closed- and Open-Set RFFI with Optimized VMD", "abstract": "Device recognition is vital for security in wireless communication systems,\nparticularly for applications like access control. Radio Frequency Fingerprint\nIdentification (RFFI) offers a non-cryptographic solution by exploiting\nhardware-induced signal distortions. This paper proposes HyDRA, a Hybrid\nDual-mode RF Architecture that integrates an optimized Variational Mode\nDecomposition (VMD) with a novel architecture based on the fusion of\nConvolutional Neural Networks (CNNs), Transformers, and Mamba components,\ndesigned to support both closed-set and open-set classification tasks. The\noptimized VMD enhances preprocessing efficiency and classification accuracy by\nfixing center frequencies and using closed-form solutions. HyDRA employs the\nTransformer Dynamic Sequence Encoder (TDSE) for global dependency modeling and\nthe Mamba Linear Flow Encoder (MLFE) for linear-complexity processing, adapting\nto varying conditions. Evaluation on public datasets demonstrates\nstate-of-the-art (SOTA) accuracy in closed-set scenarios and robust performance\nin our proposed open-set classification method, effectively identifying\nunauthorized devices. Deployed on NVIDIA Jetson Xavier NX, HyDRA achieves\nmillisecond-level inference speed with low power consumption, providing a\npractical solution for real-time wireless authentication in real-world\nenvironments.", "published": "2025-07-16 11:02:11", "link": "http://arxiv.org/abs/2507.12133v1", "categories": ["cs.LG", "eess.SP"], "primary_category": "cs.LG"}
{"title": "Self-Adaptive and Robust Federated Spectrum Sensing without Benign Majority for Cellular Networks", "abstract": "Advancements in wireless and mobile technologies, including 5G advanced and\nthe envisioned 6G, are driving exponential growth in wireless devices. However,\nthis rapid expansion exacerbates spectrum scarcity, posing a critical\nchallenge. Dynamic spectrum allocation (DSA)--which relies on sensing and\ndynamically sharing spectrum--has emerged as an essential solution to address\nthis issue. While machine learning (ML) models hold significant potential for\nimproving spectrum sensing, their adoption in centralized ML-based DSA systems\nis limited by privacy concerns, bandwidth constraints, and regulatory\nchallenges. To overcome these limitations, distributed ML-based approaches such\nas Federated Learning (FL) offer promising alternatives. This work addresses\ntwo key challenges in FL-based spectrum sensing (FLSS). First, the scarcity of\nlabeled data for training FL models in practical spectrum sensing scenarios is\ntackled with a semi-supervised FL approach, combined with energy detection,\nenabling model training on unlabeled datasets. Second, we examine the security\nvulnerabilities of FLSS, focusing on the impact of data poisoning attacks. Our\nanalysis highlights the shortcomings of existing majority-based defenses in\ncountering such attacks. To address these vulnerabilities, we propose a novel\ndefense mechanism inspired by vaccination, which effectively mitigates data\npoisoning attacks without relying on majority-based assumptions. Extensive\nexperiments on both synthetic and real-world datasets validate our solutions,\ndemonstrating that FLSS can achieve near-perfect accuracy on unlabeled datasets\nand maintain Byzantine robustness against both targeted and untargeted data\npoisoning attacks, even when a significant proportion of participants are\nmalicious.", "published": "2025-07-16 10:53:19", "link": "http://arxiv.org/abs/2507.12127v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "A Privacy-Preserving Framework for Advertising Personalization Incorporating Federated Learning and Differential Privacy", "abstract": "To mitigate privacy leakage and performance issues in personalized\nadvertising, this paper proposes a framework that integrates federated learning\nand differential privacy. The system combines distributed feature extraction,\ndynamic privacy budget allocation, and robust model aggregation to balance\nmodel accuracy, communication overhead, and privacy protection. Multi-party\nsecure computing and anomaly detection mechanisms further enhance system\nresilience against malicious attacks. Experimental results demonstrate that the\nframework achieves dual optimization of recommendation accuracy and system\nefficiency while ensuring privacy, providing both a practical solution and a\ntheoretical foundation for applying privacy protection technologies in\nadvertisement recommendation.", "published": "2025-07-16 10:07:19", "link": "http://arxiv.org/abs/2507.12098v1", "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Measuring Informativeness Gap of (Mis)Calibrated Predictors", "abstract": "In many applications, decision-makers must choose between multiple predictive\nmodels that may all be miscalibrated. Which model (i.e., predictor) is more\n\"useful\" in downstream decision tasks? To answer this, our first contribution\nintroduces the notion of the informativeness gap between any two predictors,\ndefined as the maximum normalized payoff advantage one predictor offers over\nthe other across all decision-making tasks. Our framework strictly generalizes\nseveral existing notions: it subsumes U-Calibration [KLST-23] and Calibration\nDecision Loss [HW-24], which compare a miscalibrated predictor to its\ncalibrated counterpart, and it recovers Blackwell informativeness [Bla-51,\nBla-53] as a special case when both predictors are perfectly calibrated. Our\nsecond contribution is a dual characterization of the informativeness gap,\nwhich gives rise to a natural informativeness measure that can be viewed as a\nrelaxed variant of the earth mover's distance (EMD) between two prediction\ndistributions. We show that this measure satisfies natural desiderata: it is\ncomplete and sound, and it can be estimated sample-efficiently in the\nprediction-only access setting. Along the way, we also obtain novel\ncombinatorial structural results when applying this measure to perfectly\ncalibrated predictors.", "published": "2025-07-16 10:01:22", "link": "http://arxiv.org/abs/2507.12094v1", "categories": ["cs.LG", "cs.GT"], "primary_category": "cs.LG"}
{"title": "Improved Analysis for Sign-based Methods with Momentum Updates", "abstract": "In this paper, we present enhanced analysis for sign-based optimization\nalgorithms with momentum updates. Traditional sign-based methods, under the\nseparable smoothness assumption, guarantee a convergence rate of\n$\\mathcal{O}(T^{-1/4})$, but they either require large batch sizes or assume\nunimodal symmetric stochastic noise. To address these limitations, we\ndemonstrate that signSGD with momentum can achieve the same convergence rate\nusing constant batch sizes without additional assumptions. Our analysis, under\nthe standard $l_2$-smoothness condition, improves upon the result of the prior\nmomentum-based signSGD method by a factor of $\\mathcal{O}(d^{1/2})$, where $d$\nis the problem dimension. Furthermore, we explore sign-based methods with\nmajority vote in distributed settings and show that the proposed momentum-based\nmethod yields convergence rates of $\\mathcal{O}\\left( d^{1/2}T^{-1/2} +\ndn^{-1/2} \\right)$ and $\\mathcal{O}\\left( \\max \\{ d^{1/4}T^{-1/4},\nd^{1/10}T^{-1/5} \\} \\right)$, which outperform the previous results of\n$\\mathcal{O}\\left( dT^{-1/4} + dn^{-1/2} \\right)$ and $\\mathcal{O}\\left(\nd^{3/8}T^{-1/8} \\right)$, respectively. Numerical experiments further validate\nthe effectiveness of the proposed methods.", "published": "2025-07-16 09:54:08", "link": "http://arxiv.org/abs/2507.12091v1", "categories": ["math.OC", "cs.LG"], "primary_category": "math.OC"}
{"title": "Emergence of Quantised Representations Isolated to Anisotropic Functions", "abstract": "This paper describes a novel methodology for determining representational\nalignment, developed upon the existing Spotlight Resonance method. Using this,\nit is found that algebraic symmetries of network primitives are a strong\npredictor for task-agnostic structure in representations. Particularly, this\nnew tool is used to gain insight into how discrete representations can form and\narrange in autoencoder models, through an ablation study where only the\nactivation function is altered. Representations are found to tend to discretise\nwhen the activation functions are defined through a discrete algebraic\npermutation-equivariant symmetry. In contrast, they remain continuous under a\ncontinuous algebraic orthogonal-equivariant definition. These findings\ncorroborate the hypothesis that functional form choices can carry unintended\ninductive biases which produce task-independent artefactual structures in\nrepresentations, particularly that contemporary forms induce discretisation of\notherwise continuous structure -- a quantisation effect. Moreover, this\nsupports a general causal model for one mode in which discrete representations\nmay form, and could constitute a prerequisite for downstream interpretability\nphenomena, including grandmother neurons, discrete coding schemes, general\nlinear features and possibly Superposition. Hence, this tool and proposed\nmechanism for the influence of functional form on representations may provide\nseveral insights into emergent interpretability research. Finally, preliminary\nresults indicate that quantisation of representations appears to correlate with\na measurable increase in reconstruction error, reinforcing previous conjectures\nthat this collapse can be detrimental.", "published": "2025-07-16 09:27:54", "link": "http://arxiv.org/abs/2507.12070v1", "categories": ["cs.LG", "I.5.1; F.1.1; I.2.6"], "primary_category": "cs.LG"}
{"title": "FloGAN: Scenario-Based Urban Mobility Flow Generation via Conditional GANs and Dynamic Region Decoupling", "abstract": "The mobility patterns of people in cities evolve alongside changes in land\nuse and population. This makes it crucial for urban planners to simulate and\nanalyze human mobility patterns for purposes such as transportation\noptimization and sustainable urban development. Existing generative models\nborrowed from machine learning rely heavily on historical trajectories and\noften overlook evolving factors like changes in population density and land\nuse. Mechanistic approaches incorporate population density and facility\ndistribution but assume static scenarios, limiting their utility for future\nprojections where historical data for calibration is unavailable. This study\nintroduces a novel, data-driven approach for generating origin-destination\nmobility flows tailored to simulated urban scenarios. Our method leverages\nadaptive factors such as dynamic region sizes and land use archetypes, and it\nutilizes conditional generative adversarial networks (cGANs) to blend\nhistorical data with these adaptive parameters. The approach facilitates rapid\nmobility flow generation with adjustable spatial granularity based on regions\nof interest, without requiring extensive calibration data or complex behavior\nmodeling. The promising performance of our approach is demonstrated by its\napplication to mobile phone data from Singapore, and by its comparison with\nexisting methods.", "published": "2025-07-16 09:12:38", "link": "http://arxiv.org/abs/2507.12053v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Information-Theoretic Generalization Bounds of Replay-based Continual Learning", "abstract": "Continual learning (CL) has emerged as a dominant paradigm for acquiring\nknowledge from sequential tasks while avoiding catastrophic forgetting.\nAlthough many CL methods have been proposed to show impressive empirical\nperformance, the theoretical understanding of their generalization behavior\nremains limited, particularly for replay-based approaches. In this paper, we\nestablish a unified theoretical framework for replay-based CL, deriving a\nseries of information-theoretic bounds that explicitly characterize how the\nmemory buffer interacts with the current task to affect generalization.\nSpecifically, our hypothesis-based bounds reveal that utilizing the limited\nexemplars of previous tasks alongside the current task data, rather than\nexhaustive replay, facilitates improved generalization while effectively\nmitigating catastrophic forgetting. Furthermore, our prediction-based bounds\nyield tighter and computationally tractable upper bounds of the generalization\ngap through the use of low-dimensional variables. Our analysis is general and\nbroadly applicable to a wide range of learning algorithms, exemplified by\nstochastic gradient Langevin dynamics (SGLD) as a representative method.\nComprehensive experimental evaluations demonstrate the effectiveness of our\nderived bounds in capturing the generalization dynamics in replay-based CL\nsettings.", "published": "2025-07-16 09:00:57", "link": "http://arxiv.org/abs/2507.12043v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Granular feedback merits sophisticated aggregation", "abstract": "Human feedback is increasingly used across diverse applications like training\nAI models, developing recommender systems, and measuring public opinion -- with\ngranular feedback often being preferred over binary feedback for its greater\ninformativeness. While it is easy to accurately estimate a population's\ndistribution of feedback given feedback from a large number of individuals,\ncost constraints typically necessitate using smaller groups. A simple method to\napproximate the population distribution is regularized averaging: compute the\nempirical distribution and regularize it toward a prior. Can we do better? As\nwe will discuss, the answer to this question depends on feedback granularity.\n  Suppose one wants to predict a population's distribution of feedback using\nfeedback from a limited number of individuals. We show that, as feedback\ngranularity increases, one can substantially improve upon predictions of\nregularized averaging by combining individuals' feedback in ways more\nsophisticated than regularized averaging.\n  Our empirical analysis using questions on social attitudes confirms this\npattern. In particular, with binary feedback, sophistication barely reduces the\nnumber of individuals required to attain a fixed level of performance. By\ncontrast, with five-point feedback, sophisticated methods match the performance\nof regularized averaging with about half as many individuals.", "published": "2025-07-16 08:58:27", "link": "http://arxiv.org/abs/2507.12041v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Incorporating Fairness Constraints into Archetypal Analysis", "abstract": "Archetypal Analysis (AA) is an unsupervised learning method that represents\ndata as convex combinations of extreme patterns called archetypes. While AA\nprovides interpretable and low-dimensional representations, it can\ninadvertently encode sensitive attributes, leading to fairness concerns. In\nthis work, we propose Fair Archetypal Analysis (FairAA), a modified formulation\nthat explicitly reduces the influence of sensitive group information in the\nlearned projections. We also introduce FairKernelAA, a nonlinear extension that\naddresses fairness in more complex data distributions. Our approach\nincorporates a fairness regularization term while preserving the structure and\ninterpretability of the archetypes. We evaluate FairAA and FairKernelAA on\nsynthetic datasets, including linear, nonlinear, and multi-group scenarios,\ndemonstrating their ability to reduce group separability -- as measured by mean\nmaximum discrepancy and linear separability -- without substantially\ncompromising explained variance. We further validate our methods on the\nreal-world ANSUR I dataset, confirming their robustness and practical utility.\nThe results show that FairAA achieves a favorable trade-off between utility and\nfairness, making it a promising tool for responsible representation learning in\nsensitive applications.", "published": "2025-07-16 08:28:01", "link": "http://arxiv.org/abs/2507.12021v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Expanding ML-Documentation Standards For Better Security", "abstract": "This article presents the current state of ML-security and of the\ndocumentation of ML-based systems, models and datasets in research and practice\nbased on an extensive review of the existing literature. It shows a generally\nlow awareness of security aspects among ML-practitioners and organizations and\nan often unstandardized approach to documentation, leading to overall low\nquality of ML-documentation. Existing standards are not regularly adopted in\npractice and IT-security aspects are often not included in documentation. Due\nto these factors, there is a clear need for improved security documentation in\nML, as one step towards addressing the existing gaps in ML-security. To achieve\nthis, we propose expanding existing documentation standards for\nML-documentation to include a security section with specific security relevant\ninformation. Implementing this, a novel expanded method of documenting security\nrequirements in ML-documentation is presented, based on the existing Model\nCards and Datasheets for Datasets standards, but with the recommendation to\nadopt these findings in all ML-documentation.", "published": "2025-07-16 07:57:57", "link": "http://arxiv.org/abs/2507.12003v1", "categories": ["cs.CR", "cs.LG", "cs.SE"], "primary_category": "cs.CR"}
{"title": "Detecting In-Person Conversations in Noisy Real-World Environments with Smartwatch Audio and Motion Sensing", "abstract": "Social interactions play a crucial role in shaping human behavior,\nrelationships, and societies. It encompasses various forms of communication,\nsuch as verbal conversation, non-verbal gestures, facial expressions, and body\nlanguage. In this work, we develop a novel computational approach to detect a\nfoundational aspect of human social interactions, in-person verbal\nconversations, by leveraging audio and inertial data captured with a commodity\nsmartwatch in acoustically-challenging scenarios. To evaluate our approach, we\nconducted a lab study with 11 participants and a semi-naturalistic study with\n24 participants. We analyzed machine learning and deep learning models with 3\ndifferent fusion methods, showing the advantages of fusing audio and inertial\ndata to consider not only verbal cues but also non-verbal gestures in\nconversations. Furthermore, we perform a comprehensive set of evaluations\nacross activities and sampling rates to demonstrate the benefits of multimodal\nsensing in specific contexts. Overall, our framework achieved 82.0$\\pm$3.0%\nmacro F1-score when detecting conversations in the lab and 77.2$\\pm$1.8% in the\nsemi-naturalistic setting.", "published": "2025-07-16 07:57:15", "link": "http://arxiv.org/abs/2507.12002v1", "categories": ["cs.LG", "I.2.0; J.4"], "primary_category": "cs.LG"}
{"title": "Dataset-Adaptive Dimensionality Reduction", "abstract": "Selecting the appropriate dimensionality reduction (DR) technique and\ndetermining its optimal hyperparameter settings that maximize the accuracy of\nthe output projections typically involves extensive trial and error, often\nresulting in unnecessary computational overhead. To address this challenge, we\npropose a dataset-adaptive approach to DR optimization guided by structural\ncomplexity metrics. These metrics quantify the intrinsic complexity of a\ndataset, predicting whether higher-dimensional spaces are necessary to\nrepresent it accurately. Since complex datasets are often inaccurately\nrepresented in two-dimensional projections, leveraging these metrics enables us\nto predict the maximum achievable accuracy of DR techniques for a given\ndataset, eliminating redundant trials in optimizing DR. We introduce the design\nand theoretical foundations of these structural complexity metrics. We\nquantitatively verify that our metrics effectively approximate the ground truth\ncomplexity of datasets and confirm their suitability for guiding\ndataset-adaptive DR workflow. Finally, we empirically show that our\ndataset-adaptive workflow significantly enhances the efficiency of DR\noptimization without compromising accuracy.", "published": "2025-07-16 07:32:08", "link": "http://arxiv.org/abs/2507.11984v1", "categories": ["cs.HC", "cs.LG"], "primary_category": "cs.HC"}
{"title": "Recent results on searches with boosted Higgs bosons at CMS", "abstract": "The study of boosted Higgs bosons at the LHC provides a unique window to\nprobe Higgs boson couplings at high energy scales and search for signs of\nphysics beyond the standard model. In these proceedings, we present recent\nresults on boosted Higgs boson searches at the CMS experiment, highlighting\ninnovative reconstruction and tagging techniques that enhance sensitivity in\nthis challenging regime.", "published": "2025-07-16 07:18:51", "link": "http://arxiv.org/abs/2507.11977v1", "categories": ["hep-ex", "cs.LG"], "primary_category": "hep-ex"}
{"title": "d-DQIVAR: Data-centric Visual Analytics and Reasoning for Data Quality Improvement", "abstract": "Approaches to enhancing data quality (DQ) are classified into two main\ncategories: data- and process-driven. However, prior research has predominantly\nutilized batch data preprocessing within the data-driven framework, which often\nproves insufficient for optimizing machine learning (ML) model performance and\nfrequently leads to distortions in data characteristics. Existing studies have\nprimarily focused on data preprocessing rather than genuine data quality\nimprovement (DQI). In this paper, we introduce d-DQIVAR, a novel visual\nanalytics system designed to facilitate DQI strategies aimed at improving ML\nmodel performance. Our system integrates visual analytics techniques that\nleverage both data-driven and process-driven approaches. Data-driven techniques\ntackle DQ issues such as imputation, outlier detection, deletion, format\nstandardization, removal of duplicate records, and feature selection.\nProcess-driven strategies encompass evaluating DQ and DQI procedures by\nconsidering DQ dimensions and ML model performance and applying the\nKolmogorov-Smirnov test. We illustrate how our system empowers users to harness\nexpert and domain knowledge effectively within a practical workflow through\ncase studies, evaluations, and user studies.", "published": "2025-07-16 06:45:08", "link": "http://arxiv.org/abs/2507.11960v1", "categories": ["cs.HC", "cs.LG"], "primary_category": "cs.HC"}
{"title": "RNAMunin: A Deep Machine Learning Model for Non-coding RNA Discovery", "abstract": "Functional annotation of microbial genomes is often biased toward\nprotein-coding genes, leaving a vast, unexplored landscape of non-coding RNAs\n(ncRNAs) that are critical for regulating bacterial and archaeal physiology,\nstress response and metabolism. Identifying ncRNAs directly from genomic\nsequence is a paramount challenge in bioinformatics and biology, essential for\nunderstanding the complete regulatory potential of an organism. This paper\npresents RNAMunin, a machine learning (ML) model that is capable of finding\nncRNAs using genomic sequence alone. It is also computationally viable for\nlarge sequence datasets such as long read metagenomic assemblies with contigs\ntotaling multiple Gbp. RNAMunin is trained on Rfam sequences extracted from\napproximately 60 Gbp of long read metagenomes from 16 San Francisco Estuary\nsamples. We know of no other model that can detect ncRNAs based solely on\ngenomic sequence at this scale. Since RNAMunin only requires genomic sequence\nas input, we do not need for an ncRNA to be transcribed to find it, i.e., we do\nnot need transcriptomics data. We wrote this manuscript in a narrative style in\norder to best convey how RNAMunin was developed and how it works in detail.\nUnlike almost all current ML models, at approximately 1M parameters, RNAMunin\nis very small and very fast.", "published": "2025-07-16 06:33:50", "link": "http://arxiv.org/abs/2507.11950v1", "categories": ["q-bio.GN", "cs.LG"], "primary_category": "q-bio.GN"}
{"title": "Accelerating RF Power Amplifier Design via Intelligent Sampling and ML-Based Parameter Tuning", "abstract": "This paper presents a machine learning-accelerated optimization framework for\nRF power amplifier design that reduces simulation requirements by 65% while\nmaintaining $\\pm0.3$ to $\\pm0.4$ dBm accuracy. The proposed method combines\nMaxMin Latin Hypercube Sampling with CatBoost gradient boosting to\nintelligently explore multidimensional parameter spaces. Instead of\nexhaustively simulating all parameter combinations to achieve target P2dB\ncompression specifications, our approach strategically selects approximately\n35% of critical simulation points. The framework processes ADS netlists,\nexecutes harmonic balance simulations on the reduced dataset, and trains a\nCatBoost model to predict P2dB performance across the entire design space.\nValidation across 15 PA operating modes yields an average $R^2$ of 0.901, with\nthe system ranking parameter combinations by their likelihood of meeting target\nspecifications. The integrated solution delivers 58.24% to 77.78% reduction in\nsimulation time through automated GUI-based workflows, enabling rapid design\niterations without compromising accuracy standards required for production RF\ncircuits.", "published": "2025-07-16 05:52:24", "link": "http://arxiv.org/abs/2507.11928v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "From Generative to Episodic: Sample-Efficient Replicable Reinforcement Learning", "abstract": "The epidemic failure of replicability across empirical science and machine\nlearning has recently motivated the formal study of replicable learning\nalgorithms [Impagliazzo et al. (2022)]. In batch settings where data comes from\na fixed i.i.d. source (e.g., hypothesis testing, supervised learning), the\ndesign of data-efficient replicable algorithms is now more or less understood.\nIn contrast, there remain significant gaps in our knowledge for control\nsettings like reinforcement learning where an agent must interact directly with\na shifting environment. Karbasi et. al show that with access to a generative\nmodel of an environment with $S$ states and $A$ actions (the RL 'batch\nsetting'), replicably learning a near-optimal policy costs only\n$\\tilde{O}(S^2A^2)$ samples. On the other hand, the best upper bound without a\ngenerative model jumps to $\\tilde{O}(S^7 A^7)$ [Eaton et al. (2024)] due to the\nsubstantial difficulty of environment exploration. This gap raises a key\nquestion in the broader theory of replicability: Is replicable exploration\ninherently more expensive than batch learning? Is sample-efficient replicable\nRL even possible?\n  In this work, we (nearly) resolve this problem (for low-horizon tabular\nMDPs): exploration is not a significant barrier to replicable learning! Our\nmain result is a replicable RL algorithm on $\\tilde{O}(S^2A)$ samples, bridging\nthe gap between the generative and episodic settings. We complement this with a\nmatching $\\tilde{\\Omega}(S^2A)$ lower bound in the generative setting (under\nthe common parallel sampling assumption) and an unconditional lower bound in\nthe episodic setting of $\\tilde{\\Omega}(S^2)$ showcasing the near-optimality of\nour algorithm with respect to the state space $S$.", "published": "2025-07-16 05:43:46", "link": "http://arxiv.org/abs/2507.11926v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Resampling strategies for imbalanced regression: a survey and empirical analysis", "abstract": "Imbalanced problems can arise in different real-world situations, and to\naddress this, certain strategies in the form of resampling or balancing\nalgorithms are proposed. This issue has largely been studied in the context of\nclassification, and yet, the same problem features in regression tasks, where\ntarget values are continuous. This work presents an extensive experimental\nstudy comprising various balancing and predictive models, and wich uses metrics\nto capture important elements for the user and to evaluate the predictive model\nin an imbalanced regression data context. It also proposes a taxonomy for\nimbalanced regression approaches based on three crucial criteria: regression\nmodel, learning process, and evaluation metrics. The study offers new insights\ninto the use of such strategies, highlighting the advantages they bring to each\nmodel's learning process, and indicating directions for further studies. The\ncode, data and further information related to the experiments performed herein\ncan be found on GitHub: https://github.com/JusciAvelino/imbalancedRegression.", "published": "2025-07-16 04:34:42", "link": "http://arxiv.org/abs/2507.11902v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Imbalanced Regression Pipeline Recommendation", "abstract": "Imbalanced problems are prevalent in various real-world scenarios and are\nextensively explored in classification tasks. However, they also present\nchallenges for regression tasks due to the rarity of certain target values. A\ncommon alternative is to employ balancing algorithms in preprocessing to\naddress dataset imbalance. However, due to the variety of resampling methods\nand learning models, determining the optimal solution requires testing many\ncombinations. Furthermore, the learning model, dataset, and evaluation metric\naffect the best strategies. This work proposes the Meta-learning for Imbalanced\nRegression (Meta-IR) framework, which diverges from existing literature by\ntraining meta-classifiers to recommend the best pipeline composed of the\nresampling strategy and learning model per task in a zero-shot fashion. The\nmeta-classifiers are trained using a set of meta-features to learn how to map\nthe meta-features to the classes indicating the best pipeline. We propose two\nformulations: Independent and Chained. Independent trains the meta-classifiers\nto separately indicate the best learning algorithm and resampling strategy.\nChained involves a sequential procedure where the output of one meta-classifier\nis used as input for another to model intrinsic relationship factors. The\nChained scenario showed superior performance, suggesting a relationship between\nthe learning algorithm and the resampling strategy per task. Compared with\nAutoML frameworks, Meta-IR obtained better results. Moreover, compared with\nbaselines of six learning algorithms and six resampling algorithms plus no\nresampling, totaling 42 (6 X 7) configurations, Meta-IR outperformed all of\nthem. The code, data, and further information of the experiments can be found\non GitHub: https://github.com/JusciAvelino/Meta-IR.", "published": "2025-07-16 04:34:02", "link": "http://arxiv.org/abs/2507.11901v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Newfluence: Boosting Model interpretability and Understanding in High Dimensions", "abstract": "The increasing complexity of machine learning (ML) and artificial\nintelligence (AI) models has created a pressing need for tools that help\nscientists, engineers, and policymakers interpret and refine model decisions\nand predictions. Influence functions, originating from robust statistics, have\nemerged as a popular approach for this purpose.\n  However, the heuristic foundations of influence functions rely on\nlow-dimensional assumptions where the number of parameters $p$ is much smaller\nthan the number of observations $n$. In contrast, modern AI models often\noperate in high-dimensional regimes with large $p$, challenging these\nassumptions.\n  In this paper, we examine the accuracy of influence functions in\nhigh-dimensional settings. Our theoretical and empirical analyses reveal that\ninfluence functions cannot reliably fulfill their intended purpose. We then\nintroduce an alternative approximation, called Newfluence, that maintains\nsimilar computational efficiency while offering significantly improved\naccuracy.\n  Newfluence is expected to provide more accurate insights than many existing\nmethods for interpreting complex AI models and diagnosing their issues.\nMoreover, the high-dimensional framework we develop in this paper can also be\napplied to analyze other popular techniques, such as Shapley values.", "published": "2025-07-16 04:22:16", "link": "http://arxiv.org/abs/2507.11895v1", "categories": ["stat.ML", "cs.LG", "stat.ME"], "primary_category": "stat.ML"}
{"title": "Choosing the Better Bandit Algorithm under Data Sharing: When Do A/B Experiments Work?", "abstract": "We study A/B experiments that are designed to compare the performance of two\nrecommendation algorithms. Prior work has shown that the standard\ndifference-in-means estimator is biased in estimating the global treatment\neffect (GTE) due to a particular form of interference between experimental\nunits. Specifically, units under the treatment and control algorithms\ncontribute to a shared pool of data that subsequently train both algorithms,\nresulting in interference between the two groups. The bias arising from this\ntype of data sharing is known as \"symbiosis bias\". In this paper, we highlight\nthat, for decision-making purposes, the sign of the GTE often matters more than\nits precise magnitude when selecting the better algorithm. We formalize this\ninsight under a multi-armed bandit framework and theoretically characterize\nwhen the sign of the expected GTE estimate under data sharing aligns with or\ncontradicts the sign of the true GTE. Our analysis identifies the level of\nexploration versus exploitation as a key determinant of how symbiosis bias\nimpacts algorithm selection.", "published": "2025-07-16 04:14:52", "link": "http://arxiv.org/abs/2507.11891v1", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "primary_category": "stat.ML"}
{"title": "A Policy-Improved Deep Deterministic Policy Gradient Framework for the Discount Order Acceptance Strategy of Ride-hailing Drivers", "abstract": "The rapid expansion of platform integration has emerged as an effective\nsolution to mitigate market fragmentation by consolidating multiple\nride-hailing platforms into a single application. To address heterogeneous\npassenger preferences, third-party integrators provide Discount Express service\ndelivered by express drivers at lower trip fares. For the individual platform,\nencouraging broader participation of drivers in Discount Express services has\nthe potential to expand the accessible demand pool and improve matching\nefficiency, but often at the cost of reduced profit margins. This study aims to\ndynamically manage drivers' acceptance of Discount Express from the perspective\nof individual platforms. The lack of historical data under the new business\nmodel necessitates online learning. However, early-stage exploration through\ntrial and error can be costly in practice, highlighting the need for reliable\nearly-stage performance in real-world deployment. To address these challenges,\nthis study formulates the decision regarding the proportion of drivers'\nacceptance behavior as a continuous control task. In response to the high\nstochasticity, the opaque matching mechanisms employed by third-party\nintegrator, and the limited availability of historical data, we propose a\npolicy-improved deep deterministic policy gradient (pi-DDPG) framework. The\nproposed framework incorporates a refiner module to boost policy performance\nduring the early training phase, leverages a convolutional long short-term\nmemory network to effectively capture complex spatiotemporal patterns, and\nadopts a prioritized experience replay mechanism to enhance learning\nefficiency. A simulator based on a real-world dataset is developed to validate\nthe effectiveness of the proposed pi-DDPG. Numerical experiments demonstrate\nthat pi-DDPG achieves superior learning efficiency and significantly reduces\nearly-stage training losses.", "published": "2025-07-16 03:24:54", "link": "http://arxiv.org/abs/2507.11865v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "OrdShap: Feature Position Importance for Sequential Black-Box Models", "abstract": "Sequential deep learning models excel in domains with temporal or sequential\ndependencies, but their complexity necessitates post-hoc feature attribution\nmethods for understanding their predictions. While existing techniques quantify\nfeature importance, they inherently assume fixed feature ordering - conflating\nthe effects of (1) feature values and (2) their positions within input\nsequences. To address this gap, we introduce OrdShap, a novel attribution\nmethod that disentangles these effects by quantifying how a model's predictions\nchange in response to permuting feature position. We establish a game-theoretic\nconnection between OrdShap and Sanchez-Berganti\\~nos values, providing a\ntheoretically grounded approach to position-sensitive attribution. Empirical\nresults from health, natural language, and synthetic datasets highlight\nOrdShap's effectiveness in capturing feature value and feature position\nattributions, and provide deeper insight into model behavior.", "published": "2025-07-16 02:40:01", "link": "http://arxiv.org/abs/2507.11855v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Generalized Linear Bandits: Almost Optimal Regret with One-Pass Update", "abstract": "We study the generalized linear bandit (GLB) problem, a contextual\nmulti-armed bandit framework that extends the classical linear model by\nincorporating a non-linear link function, thereby modeling a broad class of\nreward distributions such as Bernoulli and Poisson. While GLBs are widely\napplicable to real-world scenarios, their non-linear nature introduces\nsignificant challenges in achieving both computational and statistical\nefficiency. Existing methods typically trade off between two objectives, either\nincurring high per-round costs for optimal regret guarantees or compromising\nstatistical efficiency to enable constant-time updates. In this paper, we\npropose a jointly efficient algorithm that attains a nearly optimal regret\nbound with $\\mathcal{O}(1)$ time and space complexities per round. The core of\nour method is a tight confidence set for the online mirror descent (OMD)\nestimator, which is derived through a novel analysis that leverages the notion\nof mix loss from online prediction. The analysis shows that our OMD estimator,\neven with its one-pass updates, achieves statistical efficiency comparable to\nmaximum likelihood estimation, thereby leading to a jointly efficient\noptimistic method.", "published": "2025-07-16 02:24:21", "link": "http://arxiv.org/abs/2507.11847v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "CosmoFlow: Scale-Aware Representation Learning for Cosmology with Flow Matching", "abstract": "Generative machine learning models have been demonstrated to be able to learn\nlow dimensional representations of data that preserve information required for\ndownstream tasks. In this work, we demonstrate that flow matching based\ngenerative models can learn compact, semantically rich latent representations\nof field level cold dark matter (CDM) simulation data without supervision. Our\nmodel, CosmoFlow, learns representations 32x smaller than the raw field data,\nusable for field level reconstruction, synthetic data generation, and parameter\ninference. Our model also learns interpretable representations, in which\ndifferent latent channels correspond to features at different cosmological\nscales.", "published": "2025-07-16 02:15:31", "link": "http://arxiv.org/abs/2507.11842v1", "categories": ["astro-ph.CO", "cs.LG"], "primary_category": "astro-ph.CO"}
{"title": "Protenix-Mini: Efficient Structure Predictor via Compact Architecture, Few-Step Diffusion and Switchable pLM", "abstract": "Lightweight inference is critical for biomolecular structure prediction and\nother downstream tasks, enabling efficient real-world deployment and\ninference-time scaling for large-scale applications. In this work, we address\nthe challenge of balancing model efficiency and prediction accuracy by making\nseveral key modifications, 1) Multi-step AF3 sampler is replaced by a few-step\nODE sampler, significantly reducing computational overhead for the diffusion\nmodule part during inference; 2) In the open-source Protenix framework, a\nsubset of pairformer or diffusion transformer blocks doesn't make contributions\nto the final structure prediction, presenting opportunities for architectural\npruning and lightweight redesign; 3) A model incorporating an ESM module is\ntrained to substitute the conventional MSA module, reducing MSA preprocessing\ntime. Building on these key insights, we present Protenix-Mini, a compact and\noptimized model designed for efficient protein structure prediction. This\nstreamlined version incorporates a more efficient architectural design with a\ntwo-step Ordinary Differential Equation (ODE) sampling strategy. By eliminating\nredundant Transformer components and refining the sampling process,\nProtenix-Mini significantly reduces model complexity with slight accuracy drop.\nEvaluations on benchmark datasets demonstrate that it achieves high-fidelity\npredictions, with only a negligible 1 to 5 percent decrease in performance on\nbenchmark datasets compared to its full-scale counterpart. This makes\nProtenix-Mini an ideal choice for applications where computational resources\nare limited but accurate structure prediction remains crucial.", "published": "2025-07-16 02:08:25", "link": "http://arxiv.org/abs/2507.11839v1", "categories": ["cs.LG", "q-bio.QM"], "primary_category": "cs.LG"}
{"title": "HyperEvent:Learning Cohesive Events for Large-scale Dynamic Link Prediction", "abstract": "Dynamic link prediction in continuous-time dynamic graphs is a fundamental\ntask for modeling evolving complex systems. Existing node-centric and\nevent-centric methods focus on individual interactions or atomic states,\nfailing to capture the structural cohesion of composite hyper-events, groups of\ncausally related events. To address this, we propose HyperEvent, a framework\nreframing dynamic link prediction as hyper-event recognition. Central to\nHyperEvent is the dynamic construction of an association sequence using event\ncorrelation vectors. These vectors quantify pairwise dependencies between the\nquery event and relevant historical events, thereby characterizing the\nstructural cohesion of a potential hyper-event. The framework predicts the\noccurrence of the query event by evaluating whether it collectively forms a\nvalid hyper-event with these historical events. Notably, HyperEvent outperforms\nstate-of-the-art methods on 4 out of 5 datasets in the official leaderboard.\nFor scalability, we further introduce an efficient parallel training algorithm\nthat segments large event streams to enable concurrent training. Experiments\nvalidate HyperEvent's superior accuracy and efficiency on large-scale graphs.\nAmong which HyperEvent achieves a 6.95% improvement in Mean Reciprocal Rank\nover state-of-the-art baseline on the large-scale Flight dataset while\nutilizing only 10.17% of the training time.", "published": "2025-07-16 01:57:40", "link": "http://arxiv.org/abs/2507.11836v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Arctic Inference with Shift Parallelism: Fast and Efficient Open Source Inference System for Enterprise AI", "abstract": "Inference is now the dominant AI workload, yet existing systems force\ntrade-offs between latency, throughput, and cost. Arctic Inference, an\nopen-source vLLM plugin from Snowflake AI Research, introduces Shift\nParallelism, a dynamic parallelism strategy that adapts to real-world traffic\nwhile integrating speculative decoding, SwiftKV compute reduction, and\noptimized embedding inference. It achieves up to 3.4 times faster request\ncompletion, 1.75 times faster generation, and 1.6M tokens/sec per GPU for\nembeddings, outperforming both latency- and throughput-optimized deployments.\nAlready powering Snowflake Cortex AI, Arctic Inference delivers\nstate-of-the-art, cost-effective inference for enterprise AI and is now\navailable to the community.", "published": "2025-07-16 01:32:31", "link": "http://arxiv.org/abs/2507.11830v1", "categories": ["cs.DC", "cs.LG"], "primary_category": "cs.DC"}
{"title": "SynCoGen: Synthesizable 3D Molecule Generation via Joint Reaction and Coordinate Modeling", "abstract": "Ensuring synthesizability in generative small molecule design remains a major\nchallenge. While recent developments in synthesizable molecule generation have\ndemonstrated promising results, these efforts have been largely confined to 2D\nmolecular graph representations, limiting the ability to perform geometry-based\nconditional generation. In this work, we present SynCoGen (Synthesizable\nCo-Generation), a single framework that combines simultaneous masked graph\ndiffusion and flow matching for synthesizable 3D molecule generation. SynCoGen\nsamples from the joint distribution of molecular building blocks, chemical\nreactions, and atomic coordinates. To train the model, we curated SynSpace, a\ndataset containing over 600K synthesis-aware building block graphs and 3.3M\nconformers. SynCoGen achieves state-of-the-art performance in unconditional\nsmall molecule graph and conformer generation, and the model delivers\ncompetitive performance in zero-shot molecular linker design for protein ligand\ngeneration in drug discovery. Overall, this multimodal formulation represents a\nfoundation for future applications enabled by non-autoregressive molecular\ngeneration, including analog expansion, lead optimization, and direct structure\nconditioning.", "published": "2025-07-16 00:36:35", "link": "http://arxiv.org/abs/2507.11818v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "MOFSimBench: Evaluating Universal Machine Learning Interatomic Potentials In Metal--Organic Framework Molecular Modeling", "abstract": "Universal machine learning interatomic potentials (uMLIPs) have emerged as\npowerful tools for accelerating atomistic simulations, offering scalable and\nefficient modeling with accuracy close to quantum calculations. However, their\nreliability and effectiveness in practical, real-world applications remain an\nopen question. Metal-organic frameworks (MOFs) and related nanoporous materials\nare highly porous crystals with critical relevance in carbon capture, energy\nstorage, and catalysis applications. Modeling nanoporous materials presents\ndistinct challenges for uMLIPs due to their diverse chemistry, structural\ncomplexity, including porosity and coordination bonds, and the absence from\nexisting training datasets. Here, we introduce MOFSimBench, a benchmark to\nevaluate uMLIPs on key materials modeling tasks for nanoporous materials,\nincluding structural optimization, molecular dynamics (MD) stability, the\nprediction of bulk properties, such as bulk modulus and heat capacity, and\nguest-host interactions. Evaluating over 20 models from various architectures\non a chemically and structurally diverse materials set, we find that\ntop-performing uMLIPs consistently outperform classical force fields and\nfine-tuned machine learning potentials across all tasks, demonstrating their\nreadiness for deployment in nanoporous materials modeling. Our analysis\nhighlights that data quality, particularly the diversity of training sets and\ninclusion of out-of-equilibrium conformations, plays a more critical role than\nmodel architecture in determining performance across all evaluated uMLIPs. We\nrelease our modular and extendable benchmarking framework at\nhttps://github.com/AI4ChemS/mofsim-bench, providing an open resource to guide\nthe adoption for nanoporous materials modeling and further development of\nuMLIPs.", "published": "2025-07-16 00:00:55", "link": "http://arxiv.org/abs/2507.11806v1", "categories": ["cond-mat.mtrl-sci", "cs.LG", "physics.comp-ph"], "primary_category": "cond-mat.mtrl-sci"}
{"title": "Fast and Scalable Game-Theoretic Trajectory Planning with Intentional Uncertainties", "abstract": "Trajectory planning involving multi-agent interactions has been a\nlong-standing challenge in the field of robotics, primarily burdened by the\ninherent yet intricate interactions among agents. While game-theoretic methods\nare widely acknowledged for their effectiveness in managing multi-agent\ninteractions, significant impediments persist when it comes to accommodating\nthe intentional uncertainties of agents. In the context of intentional\nuncertainties, the heavy computational burdens associated with existing\ngame-theoretic methods are induced, leading to inefficiencies and poor\nscalability. In this paper, we propose a novel game-theoretic interactive\ntrajectory planning method to effectively address the intentional uncertainties\nof agents, and it demonstrates both high efficiency and enhanced scalability.\nAs the underpinning basis, we model the interactions between agents under\nintentional uncertainties as a general Bayesian game, and we show that its\nagent-form equivalence can be represented as a potential game under certain\nminor assumptions. The existence and attainability of the optimal interactive\ntrajectories are illustrated, as the corresponding Bayesian Nash equilibrium\ncan be attained by optimizing a unified optimization problem. Additionally, we\npresent a distributed algorithm based on the dual consensus alternating\ndirection method of multipliers (ADMM) tailored to the parallel solving of the\nproblem, thereby significantly improving the scalability. The attendant\noutcomes from simulations and experiments demonstrate that the proposed method\nis effective across a range of scenarios characterized by general forms of\nintentional uncertainties. Its scalability surpasses that of existing\ncentralized and decentralized baselines, allowing for real-time interactive\ntrajectory planning in uncertain game settings.", "published": "2025-07-16 12:12:25", "link": "http://arxiv.org/abs/2507.12174v1", "categories": ["cs.RO", "cs.MA"], "primary_category": "cs.RO"}
{"title": "CoCre-Sam (Kokkuri-san): Modeling Ouija Board as Collective Langevin Dynamics Sampling from Fused Language Models", "abstract": "Collective human activities like using an Ouija board (or Kokkuri-san) often\nproduce emergent, coherent linguistic outputs unintended by any single\nparticipant. While psychological explanations such as the ideomotor effect\nexist, a computational understanding of how decentralized, implicit linguistic\nknowledge fuses through shared physical interaction remains elusive. We\nintroduce CoCre-Sam (Collective-Creature Sampling), a framework modeling this\nphenomenon as collective Langevin dynamics sampling from implicitly fused\nlanguage models. Each participant is represented as an agent associated with an\nenergy landscape derived from an internal language model reflecting linguistic\npriors, and agents exert stochastic forces based on local energy gradients. We\ntheoretically prove that the collective motion of the shared pointer\n(planchette) corresponds to Langevin MCMC sampling from the sum of individual\nenergy landscapes, representing fused collective knowledge. Simulations\nvalidate that CoCre-Sam dynamics effectively fuse different models and generate\nmeaningful character sequences, while ablation studies confirm the essential\nroles of collective interaction and stochasticity. Altogether, CoCre-Sam\nprovides a novel computational mechanism linking individual implicit knowledge,\nembodied collective action, and emergent linguistic phenomena, grounding these\ncomplex interactions in the principles of probabilistic sampling.", "published": "2025-07-16 04:45:23", "link": "http://arxiv.org/abs/2507.11906v1", "categories": ["cs.MA", "cs.HC"], "primary_category": "cs.MA"}
{"title": "Linearization-Based Feedback Stabilization of McKean-Vlasov PDEs", "abstract": "We study the feedback stabilization of the McKean-Vlasov PDE on the torus.\nOur goal is to steer the dynamics toward a prescribed stationary distribution\nor accelerate convergence to it using a time-dependent control potential. We\nreformulate the controlled PDE in a weighted-projected space and apply the\nground-state transform to obtain a Schrodinger-type operator. The resulting\noperator framework enables spectral analysis, verification of the\ninfinite-dimensional Hautus test, and the construction of Riccati-based\nfeedback laws. We rigorously prove local exponential stabilization via maximal\nregularity arguments and nonlinear estimates. Numerical experiments on\nwell-studied models (the noisy Kuramoto model for synchronization, the O(2)\nspin model in a magnetic field, and the Gaussian/von Mises attractive\ninteraction potential) showcase the effectiveness of our control strategy,\ndemonstrating convergence speed-ups and stabilization of otherwise unstable\nequilibria.", "published": "2025-07-16 16:59:49", "link": "http://arxiv.org/abs/2507.12411v1", "categories": ["math.OC", "cs.NA", "math-ph", "math.MP", "math.NA", "49M41, 35Q84, 65M70", "G.1.6; G.1.8"], "primary_category": "math.OC"}
{"title": "Refinement of the theory and convergence of the Sinc convolution", "abstract": "The Sinc convolution is an approximate formula for indefinite convolutions\nproposed by F. Stenger. The formula was derived based on the Sinc indefinite\nintegration formula combined with the single-exponential transformation.\nAlthough its efficiency has been confirmed in variety of areas, there remain\nsome open problems in its theory. The first contribution of this study is to\nresolve those problems by refinement of the theory of the Sinc convolution.\nThis contribution includes a partial resolution of Stenger's conjecture. The\nsecond contribution of this study is to improve the convergence rate by\nreplacement of the single-exponential transformation with the\ndouble-exponential transformation. In both theoretical and numerical ways, this\nstudy also shows that the convergence rate of the new formula is improved\ncompared to Stenger's formula.", "published": "2025-07-16 16:52:02", "link": "http://arxiv.org/abs/2507.12406v1", "categories": ["math.NA", "cs.NA", "65D15, 65D30"], "primary_category": "math.NA"}
{"title": "A bound-preserving and conservative enriched Galerkin method for elliptic problems", "abstract": "We propose a locally conservative enriched Galerkin scheme that respects the\ndiscrete maximum principle of an elliptic problem. To this end, we use a\nsubstantial over-penalization of the discrete solution's jumps to obtain\noptimal convergence. To avoid the ill-conditioning issues that arise in\nover-penalized schemes, we introduce an involved splitting approach that\nseparates the system of equations for the discontinuous solution part from the\nsystem of equations for the continuous solution part, yielding well-behaved\nsubproblems. We prove the existence of discrete solutions and optimal error\nestimates, which are validated numerically.", "published": "2025-07-16 15:32:40", "link": "http://arxiv.org/abs/2507.12338v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "The iterated Golub-Kahan-Tikhonov method", "abstract": "The Golub-Kahan-Tikhonov method is a popular solution technique for large\nlinear discrete ill-posed problems. This method first applies partial\nGolub-Kahan bidiagonalization to reduce the size of the given problem and then\nuses Tikhonov regularization to compute a meaningful approximate solution of\nthe reduced problem. It is well known that iterated variants of this method\noften yield approximate solutions of higher quality than the standard\nnon-iterated method. Moreover, it produces more accurate computed solutions\nthan the Arnoldi method when the matrix that defines the linear discrete\nill-posed problem is far from symmetric.\n  This paper starts with an ill-posed operator equation in infinite-dimensional\nHilbert space, discretizes the equation, and then applies the iterated\nGolub-Kahan-Tikhonov method to the solution of the latter problem. An error\nanalysis that addresses all discretization and approximation errors is\nprovided. Additionally, a new approach for choosing the regularization\nparameter is described. This solution scheme produces more accurate approximate\nsolutions than the standard (non-iterated) Golub-Kahan-Tikhonov method and the\niterated Arnoldi-Tikhonov method.", "published": "2025-07-16 15:05:15", "link": "http://arxiv.org/abs/2507.12307v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Optimal Spectral Approximation in the Overlaps for Generalized Finite Element Methods", "abstract": "In this paper, we study a generalized finite element method for solving\nsecond-order elliptic partial differential equations with rough coefficients.\nThe method uses local approximation spaces computed by solving eigenvalue\nproblems on rings around the boundary of local subdomains. Compared to the\ncorresponding method that solves eigenvalue problems on the whole subdomains,\nthe problem size and the bandwidth of the resulting system matrices are\nsubstantially reduced, resulting in faster spectral computations. We prove a\nnearly exponential a priori decay result for the local approximation errors of\nthe proposed method, which implies the nearly exponential decay of the overall\napproximation error of the method. The proposed method can also be used as a\npreconditioner, and only a slight adaptation of our theory is necessary to\nprove the optimal convergence of the preconditioned iteration. Numerical\nexperiments are presented to support the effectiveness of the proposed method\nand to investigate its coefficient robustness.", "published": "2025-07-16 13:36:27", "link": "http://arxiv.org/abs/2507.12226v1", "categories": ["math.NA", "cs.NA", "65F10, 65N15, 65N30, 65N55"], "primary_category": "math.NA"}
{"title": "A Hybrid High-Order method for the power-law Brinkman problem robust in all regimes", "abstract": "In this work we propose and analyze a new Hybrid High-Order method for the\nBrinkman problem for fluids with power-law viscosity. The proposed method\nsupports general meshes and arbitrary approximation orders and is robust in all\nregimes, from pure (power-law) Stokes to pure Darcy. Robustness is reflected by\nerror estimates that distinguish the contributions from Stokes- and\nDarcy-dominated elements as identified by an appropriate dimensionless number,\nand that additionally account for pre-asymptotic orders of convergence.\nTheoretical results are illustrated by a complete panel of numerical\nexperiments.", "published": "2025-07-16 11:15:32", "link": "http://arxiv.org/abs/2507.12140v1", "categories": ["math.NA", "cs.NA", "65N30, 65N08, 76S05, 76D07"], "primary_category": "math.NA"}
{"title": "An augmented Lagrangian method for strongly regular minimizers in a class of convex composite optimization problems", "abstract": "In this paper, we study a class of convex composite optimization problems. We\nbegin by characterizing the equivalence between the primal/dual strong\nsecond-order sufficient condition and the dual/primal nondegeneracy condition.\nBuilding on this foundation, we derive a specific set of equivalent conditions\nfor the perturbation analysis of the problem. Furthermore, we employ the\naugmented Lagrangian method (ALM) to solve the problem and provide theoretical\nguarantees for its performance. Specifically, we establish the equivalence\nbetween the primal/dual second-order sufficient condition and the dual/primal\nstrict Robinson constraint qualification, as well as the equivalence between\nthe dual nondegeneracy condition and the nonsingularity of Clarke's generalized\nJacobian for the ALM subproblem. These theoretical results form a solid\nfoundation for designing efficient algorithms. Finally, we apply the ALM to the\nvon Neumann entropy optimization problem and present numerical experiments to\ndemonstrate the algorithm's effectiveness.", "published": "2025-07-16 08:58:05", "link": "http://arxiv.org/abs/2507.12040v1", "categories": ["math.OC", "cs.NA", "math.NA", "49J52, 49J53, 90C31, 90C22"], "primary_category": "math.OC"}
{"title": "The Arrow-Hurwicz iteration for virtual element discretizations of the incompressible Navier-Stokes equations", "abstract": "This article presents a detailed analysis of the Arrow-Hurwicz iteration\napplied to the solution of the incompressible Navier-Stokes equations,\ndiscretized by a divergence-free mixed virtual element method. Under a set of\nappropriate assumptions, it is rigorously demonstrated that the method exhibits\ngeometric convergence, with a contraction factor that remains independent of\nthe mesh sizes. A series of numerical experiments are conducted to validate the\ntheoretical findings and to assess the computational performance of the\nproposed method.", "published": "2025-07-16 08:53:22", "link": "http://arxiv.org/abs/2507.12036v1", "categories": ["math.NA", "cs.NA", "math.AP"], "primary_category": "math.NA"}
{"title": "Structured First-Layer Initialization Pre-Training Techniques to Accelerate Training Process Based on $\\varepsilon$-Rank", "abstract": "Training deep neural networks for scientific computing remains\ncomputationally expensive due to the slow formation of diverse feature\nrepresentations in early training stages. Recent studies identify a staircase\nphenomenon in training dynamics, where loss decreases are closely correlated\nwith increases in $\\varepsilon$-rank, reflecting the effective number of\nlinearly independent neuron functions. Motivated by this observation, this work\nproposes a structured first-layer initialization (SFLI) pre-training method to\nenhance the diversity of neural features at initialization by constructing\n$\\varepsilon$-linearly independent neurons in the input layer. We present\nsystematic initialization schemes compatible with various activation functions\nand integrate the strategy into multiple neural architectures, including\nmodified multi-layer perceptrons and physics-informed residual adaptive\nnetworks. Extensive numerical experiments on function approximation and PDE\nbenchmarks, demonstrate that SFLI significantly improves the initial\n$\\varepsilon$-rank, accelerates convergence, mitigates spectral bias, and\nenhances prediction accuracy. With the help of SILP, we only need to add one\nline of code to conventional existing algorithms.", "published": "2025-07-16 06:52:59", "link": "http://arxiv.org/abs/2507.11962v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Automatic reproducing kernel and regularization for learning convolution kernels", "abstract": "Learning convolution kernels in operators from data arises in numerous\napplications and represents an ill-posed inverse problem of broad interest.\nWith scant prior information, kernel methods offer a natural nonparametric\napproach with regularization. However, a major challenge is to select a proper\nreproducing kernel, especially as operators and data vary. We show that the\ninput data and convolution operator themselves induce an automatic,\ndata-adaptive RKHS (DA-RKHS), obviating manual kernel selection. In particular,\nwhen the observation data is discrete and finite, there is a finite set of\nautomatic basis functions sufficient to represent the estimators in the\nDA-RKHS, including the minimal-norm least-squares, Tikhonov, and\nconjugate-gradient estimators. We develop both Tikhonov and scalable iterative\nand hybrid algorithms using the automatic basis functions. Numerical\nexperiments on integral, nonlocal, and aggregation operators confirm that our\nautomatic RKHS regularization consistently outperforms standard ridge\nregression and Gaussian process methods with preselected kernels.", "published": "2025-07-16 06:19:13", "link": "http://arxiv.org/abs/2507.11944v1", "categories": ["math.NA", "cs.NA", "47A52, 65F22, 65J20"], "primary_category": "math.NA"}
{"title": "Analysis of a fast fully discrete finite element method for fractional viscoelastic wave propagation", "abstract": "This paper is devoted to a numerical analysis of a fractional viscoelastic\nwave propagation model that generalizes the fractional Maxwell model and the\nfractional Zener model. First, we convert the model problem into a velocity\ntype integro-differential equation and establish existence, uniqueness and\nregularity of its solution. Then we consider a conforming\nlinear/bilinear/trilinear finite element semi-discrete scheme and a fast scheme\nof backward Euler full discretization with a sum-of-exponentials (SOE)\napproximation for the convolution integral, and derive error estimates for the\nsemi-discrete and fully discrete schemes. Finally, we provide several numerical\nexamples to verify the theoretical results.", "published": "2025-07-16 00:51:12", "link": "http://arxiv.org/abs/2507.11822v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "A quasi-interpolation operator yielding fully computable error bounds", "abstract": "We design a quasi-interpolation operator from the Sobolev space\n$H^1_0(\\Omega)$ to its finite-dimensional finite element subspace formed by\npiecewise polynomials on a simplicial mesh with a computable approximation\nconstant. The operator 1) is defined on the entire $H^1_0(\\Omega)$, no\nadditional regularity is needed; 2) allows for an arbitrary polynomial degree;\n3) works in any space dimension; 4) is defined locally, in vertex patches of\nmesh elements; 5) yields optimal estimates for both the $H^1$ seminorm and the\n$L^2$ norm error; 6) gives a computable constant for both the $H^1$ seminorm\nand the $L^2$ norm error; 7) leads to the equivalence of global-best and\nlocal-best errors; 8) possesses the projection property. Its construction\nfollows the so-called potential reconstruction from a posteriori error\nanalysis. Numerical experiments illustrate that our quasi-interpolation\noperator systematically gives the correct convergence rates in both the $H^1$\nseminorm and the $L^2$ norm and its certified overestimation factor is rather\nsharp and stable in all tested situations.", "published": "2025-07-16 00:40:46", "link": "http://arxiv.org/abs/2507.11819v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Analytic estimation of parameters of stochastic volatility diffusion models with exponential-affine characteristic function for currency option pricing", "abstract": "This dissertation develops and justifies a novel method for deriving\napproximate formulas to estimate two parameters in stochastic volatility\ndiffusion models with exponentially-affine characteristic functions and single-\nor two-factor variance. These formulas aim to improve the accuracy of option\npricing and enhance the calibration process by providing reliable initial\nvalues for local minimization algorithms. The parameters relate to the\nvolatility of the stochastic factor in instantaneous variance dynamics and the\ncorrelation between stochastic factors and asset price dynamics.\n  The study comprises five chapters. Chapter one outlines the currency option\nmarket, pricing methods, and the general structure of stochastic volatility\nmodels. Chapter two derives the replication strategy dynamics and introduces a\nnew two-factor volatility model: the OUOU model. Chapter three analyzes the\ndistribution and surface dynamics of implied volatilities using principal\ncomponent and common factor analysis. Chapter four discusses calibration\nmethods for stochastic volatility models, particularly the Heston model, and\npresents the new Implied Central Moments method to estimate parameters in the\nHeston and Sch\\\"obel-Zhu models. Extensions to two-factor models, Bates and\nOUOU, are also explored. Chapter five evaluates the performance of the proposed\nformulas on the EURUSD options market, demonstrating the superior accuracy of\nthe new method.\n  The dissertation successfully meets its research objectives, expanding tools\nfor derivative pricing and risk assessment. Key contributions include faster\nand more precise parameter estimation formulas and the introduction of the OUOU\nmodel - an extension of the Sch\\\"obel-Zhu model with a semi-analytical\nvaluation formula for European options, previously unexamined in the\nliterature.", "published": "2025-07-16 03:32:23", "link": "http://arxiv.org/abs/2507.11868v1", "categories": ["q-fin.MF", "q-fin.PR", "q-fin.ST"], "primary_category": "q-fin.MF"}
{"title": "Surrogate modeling for uncertainty quantification in nonlinear dynamics", "abstract": "Predicting the behavior of complex systems in engineering often involves\nsignificant uncertainty about operating conditions, such as external loads,\nenvironmental effects, and manufacturing variability. As a result, uncertainty\nquantification (UQ) has become a critical tool in modeling-based engineering,\nproviding methods to identify, characterize, and propagate uncertainty through\ncomputational models. However, the stochastic nature of UQ typically requires\nnumerous evaluations of these models, which can be computationally expensive\nand limit the scope of feasible analyses. To address this, surrogate models,\ni.e., efficient functional approximations trained on a limited set of\nsimulations, have become central in modern UQ practice. This book chapter\npresents a concise review of surrogate modeling techniques for UQ, with a focus\non the particularly challenging task of capturing the full time-dependent\nresponse of dynamical systems. It introduces a classification of time-dependent\nproblems based on the complexity of input excitation and discusses\ncorresponding surrogate approaches, including combinations of principal\ncomponent analysis with polynomial chaos expansions, time warping techniques,\nand nonlinear autoregressive models with exogenous inputs (NARX models). Each\nmethod is illustrated with simple application examples to clarify the\nunderlying ideas and practical use.", "published": "2025-07-16 15:57:09", "link": "http://arxiv.org/abs/2507.12358v1", "categories": ["stat.CO", "stat.AP", "stat.ML"], "primary_category": "stat.CO"}
{"title": "Fast Variational Bayes for Large Spatial Data", "abstract": "Recent variational Bayes methods for geospatial regression, proposed as an\nalternative to computationally expensive Markov chain Monte Carlo (MCMC)\nsampling, have leveraged Nearest Neighbor Gaussian processes (NNGP) to achieve\nscalability. Yet, these variational methods remain inferior in accuracy and\nspeed compared to spNNGP, the state-of-the-art MCMC-based software for NNGP. We\nintroduce spVarBayes, a suite of fast variational Bayesian approaches for\nlarge-scale geospatial data analysis using NNGP. Our contributions are\nprimarily computational. We replace auto-differentiation with a combination of\ncalculus of variations, closed-form gradient updates, and linear response\ncorrections for improved variance estimation. We also accommodate covariates\n(fixed effects) in the model and offer inference on the variance parameters.\nSimulation experiments demonstrate that we achieve comparable accuracy to\nspNNGP but with reduced computational costs, and considerably outperform\nexisting variational inference methods in terms of both accuracy and speed.\nAnalysis of a large forest canopy height dataset illustrates the practical\nimplementation of proposed methods and shows that the inference results are\nconsistent with those obtained from the MCMC approach. The proposed methods are\nimplemented in publicly available Github R-package spVarBayes.", "published": "2025-07-16 13:59:27", "link": "http://arxiv.org/abs/2507.12251v1", "categories": ["stat.CO", "stat.ME", "stat.ML"], "primary_category": "stat.CO"}
{"title": "Designing Algorithms for Entropic Optimal Transport from an Optimisation Perspective", "abstract": "In this work, we develop a collection of novel methods for the\nentropic-regularised optimal transport problem, which are inspired by existing\nmirror descent interpretations of the Sinkhorn algorithm used for solving this\nproblem. These are fundamentally proposed from an optimisation perspective:\neither based on the associated semi-dual problem, or based on solving a\nnon-convex constrained problem over subset of joint distributions. This\noptimisation viewpoint results in non-asymptotic rates of convergence for the\nproposed methods under minimal assumptions on the problem structure. We also\npropose a momentum-equipped method with provable accelerated guarantees through\nthis viewpoint, akin to those in the Euclidean setting. The broader framework\nwe develop based on optimisation over the joint distributions also finds an\nanalogue in the dynamical Schr\\\"{o}dinger bridge problem.", "published": "2025-07-16 13:56:11", "link": "http://arxiv.org/abs/2507.12246v1", "categories": ["math.OC", "math.PR", "stat.ML"], "primary_category": "math.OC"}
{"title": "Enhancing Signal Proportion Estimation Through Leveraging Arbitrary Covariance Structures", "abstract": "Accurately estimating the proportion of true signals among a large number of\nvariables is crucial for enhancing the precision and reliability of scientific\nresearch. Traditional signal proportion estimators often assume independence\namong variables and specific signal sparsity conditions, limiting their\napplicability in real-world scenarios where such assumptions may not hold. This\npaper introduces a novel signal proportion estimator that leverages arbitrary\ncovariance dependence information among variables, thereby improving\nperformance across a wide range of sparsity levels and dependence structures.\nBuilding on previous work that provides lower confidence bounds for signal\nproportions, we extend this approach by incorporating the principal factor\napproximation procedure to account for variable dependence. Our theoretical\ninsights offer a deeper understanding of how signal sparsity, signal intensity,\nand covariance dependence interact. By comparing the conditions for estimation\nconsistency before and after dependence adjustment, we highlight the advantages\nof integrating dependence information across different contexts. This\ntheoretical foundation not only validates the effectiveness of the new\nestimator but also guides its practical application, ensuring reliable use in\ndiverse scenarios. Through extensive simulations, we demonstrate that our\nmethod outperforms state-of-the-art estimators in both estimation accuracy and\nthe detection of weaker signals that might otherwise go undetected.", "published": "2025-07-16 05:37:42", "link": "http://arxiv.org/abs/2507.11922v1", "categories": ["math.ST", "stat.ME", "stat.ML", "stat.TH"], "primary_category": "math.ST"}
{"title": "Room Impulse Response Generation Conditioned on Acoustic Parameters", "abstract": "The generation of room impulse responses (RIRs) using deep neural networks\nhas attracted growing research interest due to its applications in virtual and\naugmented reality, audio postproduction, and related fields. Most existing\napproaches condition generative models on physical descriptions of a room, such\nas its size, shape, and surface materials. However, this reliance on geometric\ninformation limits their usability in scenarios where the room layout is\nunknown or when perceptual realism (how a space sounds to a listener) is more\nimportant than strict physical accuracy. In this study, we propose an\nalternative strategy: conditioning RIR generation directly on a set of RIR\nacoustic parameters. These parameters include various measures of reverberation\ntime and direct sound to reverberation ratio, both broadband and bandwise. By\nspecifying how the space should sound instead of how it should look, our method\nenables more flexible and perceptually driven RIR generation. We explore both\nautoregressive and non-autoregressive generative models operating in the\nDescript Audio Codec domain, using either discrete token sequences or\ncontinuous embeddings. Specifically, we have selected four models to evaluate:\nan autoregressive transformer, the MaskGIT model, a flow matching model, and a\nclassifier-based approach. Objective and subjective evaluations are performed\nto compare these methods with state-of-the-art alternatives. Results show that\nthe proposed models match or outperform state-of-the-art alternatives, with the\nMaskGIT model achieving the best performance.", "published": "2025-07-16 11:09:50", "link": "http://arxiv.org/abs/2507.12136v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Soft-Constrained Spatially Selective Active Noise Control for Open-fitting Hearables", "abstract": "Recent advances in spatially selective active noise control (SSANC) using\nmultiple microphones have enabled hearables to suppress undesired noise while\npreserving desired speech from a specific direction. Aiming to achieve minimal\nspeech distortion, a hard constraint has been used in previous work in the\noptimization problem to compute the control filter. In this work, we propose a\nsoft-constrained SSANC system that uses a frequency-independent parameter to\ntrade off between speech distortion and noise reduction. We derive both time-\nand frequency-domain formulations, and show that conventional active noise\ncontrol and hard-constrained SSANC represent two limiting cases of the proposed\ndesign. We evaluate the system through simulations using a pair of open-fitting\nhearables in an anechoic environment with one speech source and two noise\nsources. The simulation results validate the theoretical derivations and\ndemonstrate that for a broad range of the trade-off parameter, the\nsignal-to-noise ratio and the speech quality and intelligibility in terms of\nPESQ and ESTOI can be substantially improved compared to the hard-constrained\ndesign.", "published": "2025-07-16 10:47:05", "link": "http://arxiv.org/abs/2507.12122v1", "categories": ["eess.AS", "cs.SY", "eess.SP", "eess.SY"], "primary_category": "eess.AS"}
{"title": "MambaRate: Speech Quality Assessment Across Different Sampling Rates", "abstract": "We propose MambaRate, which predicts Mean Opinion Scores (MOS) with limited\nbias regarding the sampling rate of the waveform under evaluation. It is\ndesigned for Track 3 of the AudioMOS Challenge 2025, which focuses on\npredicting MOS for speech in high sampling frequencies. Our model leverages\nself-supervised embeddings and selective state space modeling. The target\nratings are encoded in a continuous representation via Gaussian radial basis\nfunctions (RBF). The results of the challenge were based on the system-level\nSpearman's Rank Correllation Coefficient (SRCC) metric. An initial MambaRate\nversion (T16 system) outperformed the pre-trained baseline (B03) by ~14% in a\nfew-shot setting without pre-training. T16 ranked fourth out of five in the\nchallenge, differing by ~6% from the winning system. We present additional\nresults on the BVCC dataset as well as ablations with different representations\nas input, which outperform the initial T16 version.", "published": "2025-07-16 09:53:29", "link": "http://arxiv.org/abs/2507.12090v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "VoxATtack: A Multimodal Attack on Voice Anonymization Systems", "abstract": "Voice anonymization systems aim to protect speaker privacy by obscuring vocal\ntraits while preserving the linguistic content relevant for downstream\napplications. However, because these linguistic cues remain intact, they can be\nexploited to identify semantic speech patterns associated with specific\nspeakers. In this work, we present VoxATtack, a novel multimodal\nde-anonymization model that incorporates both acoustic and textual information\nto attack anonymization systems. While previous research has focused on\nrefining speaker representations extracted from speech, we show that\nincorporating textual information with a standard ECAPA-TDNN improves the\nattacker's performance. Our proposed VoxATtack model employs a dual-branch\narchitecture, with an ECAPA-TDNN processing anonymized speech and a pretrained\nBERT encoding the transcriptions. Both outputs are projected into embeddings of\nequal dimensionality and then fused based on confidence weights computed on a\nper-utterance basis. When evaluating our approach on the VoicePrivacy Attacker\nChallenge (VPAC) dataset, it outperforms the top-ranking attackers on five out\nof seven benchmarks, namely B3, B4, B5, T8-5, and T12-5. To further boost\nperformance, we leverage anonymized speech and SpecAugment as augmentation\ntechniques. This enhancement enables VoxATtack to achieve state-of-the-art on\nall VPAC benchmarks, after scoring 20.6% and 27.2% average equal error rate on\nT10-2 and T25-1, respectively. Our results demonstrate that incorporating\ntextual information and selective data augmentation reveals critical\nvulnerabilities in current voice anonymization methods and exposes potential\nweaknesses in the datasets used to evaluate them.", "published": "2025-07-16 09:40:40", "link": "http://arxiv.org/abs/2507.12081v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Self-Boosted Weight-Constrained FxLMS: A Robustness Distributed Active Noise Control Algorithm Without Internode Communication", "abstract": "Compared to the conventional centralized multichannel active noise control\n(MCANC) algorithm, which requires substantial computational resources,\ndecentralized approaches exhibit higher computational efficiency but typically\nresult in inferior noise reduction performance. To enhance performance,\ndistributed ANC methods have been introduced, enabling information exchange\namong ANC nodes; however, the resulting communication latency often compromises\nsystem stability. To overcome these limitations, we propose a self-boosted\nweight-constrained filtered-reference least mean square (SB-WCFxLMS) algorithm\nfor the distributed MCANC system without internode communication. The WCFxLMS\nalgorithm is specifically designed to mitigate divergence issues caused by the\ninternode cross-talk effect. The self-boosted strategy lets each ANC node\nindependently adapt its constraint parameters based on its local noise\nreduction performance, thus ensuring effective noise cancellation without the\nneed for inter-node communication. With the assistance of this mechanism, this\napproach significantly reduces both computational complexity and communication\noverhead. Numerical simulations employing real acoustic paths and compressor\nnoise validate the effectiveness and robustness of the proposed system. The\nresults demonstrate that our proposed method achieves satisfactory noise\ncancellation performance with minimal resource requirements.", "published": "2025-07-16 09:04:33", "link": "http://arxiv.org/abs/2507.12045v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Schr\u00f6dinger Bridge Consistency Trajectory Models for Speech Enhancement", "abstract": "Speech enhancement (SE) utilizing diffusion models is a promising technology\nthat improves speech quality in noisy speech data. Furthermore, the\nSchr\\\"odinger bridge (SB) has recently been used in diffusion-based SE to\nimprove speech quality by resolving a mismatch between the endpoint of the\nforward process and the starting point of the reverse process. However, the SB\nstill exhibits slow inference owing to the necessity of a large number of\nfunction evaluations (NFE) for inference to obtain high-quality results. While\nConsistency Models (CMs) address this issue by employing consistency training\nthat uses distillation from pretrained models in the field of image generation,\nit does not improve generation quality when the number of steps increases. As a\nsolution to this problem, Consistency Trajectory Models (CTMs) not only\naccelerate inference speed but also maintain a favorable trade-off between\nquality and speed. Furthermore, SoundCTM demonstrates the applicability of CTM\ntechniques to the field of sound generation. In this paper, we present\nSchr\\\"odinger bridge Consistency Trajectory Models (SBCTM) by applying the\nCTM's technique to the Schr\\\"odinger bridge for SE. Additionally, we introduce\na novel auxiliary loss, including a perceptual loss, into the original CTM's\ntraining framework. As a result, SBCTM achieves an approximately 16x\nimprovement in the real-time factor (RTF) compared to the conventional\nSchr\\\"odinger bridge for SE. Furthermore, the favorable trade-off between\nquality and speed in SBCTM allows for time-efficient inference by limiting\nmulti-step refinement to cases where 1-step inference is insufficient. Our\ncode, pretrained models, and audio samples are available at\nhttps://github.com/sony/sbctm/.", "published": "2025-07-16 05:43:02", "link": "http://arxiv.org/abs/2507.11925v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Multimodal Data Fusion Generative Adversarial Network for Real Time Underwater Sound Speed Field Construction", "abstract": "Sound speed profiles (SSPs) are essential parameters underwater that affects\nthe propagation mode of underwater signals and has a critical impact on the\nenergy efficiency of underwater acoustic communication and accuracy of\nunderwater acoustic positioning. Traditionally, SSPs can be obtained by\nmatching field processing (MFP), compressive sensing (CS), and deep learning\n(DL) methods. However, existing methods mainly rely on on-site underwater sonar\nobservation data, which put forward strict requirements on the deployment of\nsonar observation systems. To achieve high-precision estimation of sound\nvelocity distribution in a given sea area without on-site underwater data\nmeasurement, we propose a multi-modal data-fusion generative adversarial\nnetwork model with residual attention block (MDF-RAGAN) for SSP construction.\nTo improve the model's ability for capturing global spatial feature\ncorrelations, we embedded the attention mechanisms, and use residual modules\nfor deeply capturing small disturbances in the deep ocean sound velocity\ndistribution caused by changes of SST. Experimental results on real open\ndataset show that the proposed model outperforms other state-of-the-art\nmethods, which achieves an accuracy with an error of less than 0.3m/s.\nSpecifically, MDF-RAGAN not only outperforms convolutional neural network (CNN)\nand spatial interpolation (SITP) by nearly a factor of two, but also achieves\nabout 65.8\\% root mean square error (RMSE) reduction compared to mean profile,\nwhich fully reflects the enhancement of overall profile matching by\nmulti-source fusion and cross-modal attention.", "published": "2025-07-16 00:21:54", "link": "http://arxiv.org/abs/2507.11812v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Road Roughness Estimation via Fusion of Standard Onboard Automotive Sensors", "abstract": "Road roughness significantly affects vehicle vibrations and ride quality. We\nintroduce a Kalman filter (KF)-based method for estimating road roughness in\nterms of the international roughness index (IRI) by fusing inertial and speed\nmeasurements, offering a cost-effective solution for pavement monitoring. The\nmethod involves system identification on a physical vehicle to estimate\nrealistic model parameters, followed by KF-based reconstruction of the\nlongitudinal road profile to compute IRI values. It explores IRI estimation\nusing vertical and lateral vibrations, the latter more common in modern\nvehicles. Validation on 230 km of real-world data shows promising results, with\nIRI estimation errors ranging from 1% to 10% of the reference values. However,\naccuracy deteriorates significantly when using only lateral vibrations,\nhighlighting their limitations. These findings demonstrate the potential of\nKF-based estimation for efficient road roughness monitoring.", "published": "2025-07-16 15:10:45", "link": "http://arxiv.org/abs/2507.12317v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Frequency-responsive RCS characteristics and scaling implications for ISAC development", "abstract": "This paper presents an investigation on the Radar Cross-Section (RCS) of\nvarious targets, with the objective of analysing how RCS properties vary with\nfrequency. Targets such as an Automated Guided Vehicle (AGV), a pedestrian, and\na full-scale car were measured in the frequency bands referred to in industry\nstandards as FR2 and FR3. Measurements were taken in diverse environments,\nindoors and outdoors, to ensure comprehensive scenario coverage. The\nmethodology employed in RCS extraction performs background subtraction,\nfollowed by time-domain gating to isolate the influence of the target. This\nanalysis compares the RCS values and how the points of greatest contribution\nare distributed across different bands based on the range response of the RCS.\nAnalysis of the results demonstrated how RCS values change with frequency and\ntarget shape, providing insights into the electromagnetic behaviour of these\ntargets. Key findings highlight how much scaling RCS values based on frequency\nand geometry is complex and varies among different types of materials and\nshapes. These insights are instrumental for advancing sensing systems and\nenhancing 3GPP channel models, particularly for Integrated Sensing and\nCommunications (ISAC) techniques proposed for 6G standards.", "published": "2025-07-16 13:48:21", "link": "http://arxiv.org/abs/2507.12235v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Novel Approach to Dual-Channel Estimation in Integrated Sensing and Communications for 6G", "abstract": "Integrated Sensing and Communication (ISAC) design is crucial for 6G and\nharmonizes environmental data sensing with communication, emphasizing the need\nto understand and model these elements. This paper delves into dual-channel\nmodels for ISAC, employing channel extraction techniques to validate and\nenhance accuracy. Focusing on millimeter wave (mmWave) radars, it explores the\nextraction of the bistatic sensing channel from monostatic measurements and\nsubsequent communication channel estimation. The proposed methods involve\ninterference extraction, module and phase correlation analyses, chirp\nclustering, and auto-clutter reduction. A comprehensive set-up in an anechoic\nchamber with controlled scenarios evaluates the proposed techniques,\ndemonstrating successful channel extraction and validation through Root Mean\nSquare Delay Spread (RMS DS), Power Delay Profile (PDP), and Angle of Arrival\n(AoA) analysis. Comparison with Ray-Tracing (RT) simulations confirms the\neffectiveness of the proposed approach, presenting an innovative stride towards\nfully integrated sensing and communication in future networks.", "published": "2025-07-16 13:27:27", "link": "http://arxiv.org/abs/2507.12221v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Cell Sensing: Traffic detection", "abstract": "This work presents a passive sensing system for traffic monitoring using\nambient Long Term Evolution (LTE) signals as a non-intrusive and scalable\nalternative to traditional surveillance methods. The approach employs a\ndual-receiver architecture analyzing Channel State Information (CSI) to isolate\ndifferential Doppler shifts induced by moving targets, effectively mitigating\nhardware-induced phase impairments. Implemented with a Software Defined Radio\n(SDR) platform and srsRAN software, the system demonstrated over 90% detection\naccuracy for speeds above 6000 mm/min in controlled indoor tests, and provided\nreliable speed estimations for pedestrians and vehicles in outdoor evaluations.\nDespite challenges at low speeds, directional ambiguity, and multipath fading\nin urban settings, the results validate LTE-based passive sensing as a feasible\ntraffic monitoring method, identifying critical areas for future research such\nas angle-of-arrival (AoA) integration, machine learning, and real-time embedded\nsystem development.", "published": "2025-07-16 13:14:22", "link": "http://arxiv.org/abs/2507.12211v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "PAPR of DFT-s-OTFS with Pulse Shaping", "abstract": "Orthogonal Time Frequency Space (OTFS) suffers from high peak-to-average\npower ratio (PAPR) when the number of Doppler bins is large. To address this\nissue, a discrete Fourier transform spread OTFS (DFT-s-OTFS) scheme is employed\nby applying DFT spreading across the Doppler dimension. This paper presents a\nthorough PAPR analysis of DFT-s-OTFS in the uplink scenario using different\npulse shaping filters and resource allocation strategies. Specifically, we\nderive a PAPR upper bound of DFT-s-OTFS with interleaved and block Doppler\nresource allocation schemes. Our analysis reveals that DFT-s-OTFS with\ninterleaved allocation yields a lower PAPR than that of block allocation.\nFurthermore, we show that interleaved allocation produces a periodic\ntime-domain signal composed of repeated quadrature amplitude modulated (QAM)\nsymbols which simplifies the transmitter design. Based on our analytical\nresults, the root raised cosine (RRC) pulse generally results in a higher\nmaximum PAPR compared to the rectangular pulse. Simulation results confirm the\nvalidity of the derived PAPR upper bounds. Furthermore, we also demonstrate\nthrough BER simulation analysis that the DFT-s-OTFS gives the same performance\nas OTFS without DFT spreading.", "published": "2025-07-16 13:14:14", "link": "http://arxiv.org/abs/2507.12210v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "A Practical Analysis: Understanding Phase Noise Modelling in Time and Frequency Domain for Phase-Locked Loops", "abstract": "In MIMO systems, the presence of phase noise is a significant factor that can\ndegrade performance. For MIMO testbeds build from SDR devices, phase noise\ncannot be ignored, particular in applications that require phase\nsynchronization. This is especially relevant in MIMO systems that employ\ndigital beamforming, where precise phase alignment is crucial. Accordingly,\naccurate phase noise modelling of SDR devices is essential. However, the\ninformation provided in data sheets for different SDR models varies widely and\nis often insufficient for comprehensive characterization of their phase noise\nperformance. While numerical simulations of PLL phase noise behavior are\ndocumented in the literature, there is a lack of extensive measurements\nsupported by appropriate system modelling. In this work, we present a practical\nphase noise modeling methodology applied to an SDR from the USRP X310 series.\nBased on measurement data, we derive estimates of key PLL performance\nindicators such as cycle-to-cycle jitter, oscillator constants, and PLL\nbandwidth. Furthermore, we propose a parametric model for the phase noise PSD\nof the PLL circuit and provide corresponding parameter estimates. This model\ncan be used for further investigation into the impact of phase noise on MIMO\nsystem performance implemented by similar SDR devices.", "published": "2025-07-16 11:26:11", "link": "http://arxiv.org/abs/2507.12146v1", "categories": ["eess.SP", "cs.SY", "eess.SY"], "primary_category": "eess.SP"}
{"title": "Enhancing Situational Awareness in ISAC Networks via Drone Swarms: A Real-World Channel Sounding Data Set", "abstract": "With the upcoming capabilities of integrated sensing and communication (ISAC)\nand the incorporation of user equipment (UE) like unmanned aerial vehicles\n(UAVs) in 6G mobile networks, there is a significant opportunity to enhance\nsituational awareness through multi-static radar sensing in meshed ISAC\nnetworks. This paper presents a real-world channel sounding data set acquired\nusing a testbed with synchronized, distributed ground-based sensor nodes and\nflying sensor nodes within a swarm of up to four drones. The conducted\nmeasurement campaign is designed to sense the bi-static reflectivity of objects\nsuch as parking cars, vertical take-off and landing (VTOL) aircraft, and small\ndrones in multi-path environments. We detail the rationale behind the selection\nof the included scenarios and the configuration of the participating nodesand\npresent exemplary results to demonstrate the potential of using collaborating\ndrone swarms for multi-static radar tracking and localization in air-to-air\n(A2A) and air-to-ground (A2G) scenarios. The data sets are publicly available\nto support the development and validation of future ISAC algorithms in\nreal-world environments rather than relying solely on simulation.", "published": "2025-07-16 08:09:37", "link": "http://arxiv.org/abs/2507.12010v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "DSSD: Efficient Edge-Device Deployment and Collaborative Inference via Distributed Split Speculative Decoding", "abstract": "Large language models (LLMs) have transformed natural language processing but\nface critical deployment challenges in device-edge systems due to resource\nlimitations and communication overhead. To address these issues, collaborative\nframeworks have emerged that combine small language models (SLMs) on devices\nwith LLMs at the edge, using speculative decoding (SD) to improve efficiency.\nHowever, existing solutions often trade inference accuracy for latency or\nsuffer from high uplink transmission costs when verifying candidate tokens. In\nthis paper, we propose Distributed Split Speculative Decoding (DSSD), a novel\narchitecture that not only preserves the SLM-LLM split but also partitions the\nverification phase between the device and edge. In this way, DSSD replaces the\nuplink transmission of multiple vocabulary distributions with a single downlink\ntransmission, significantly reducing communication latency while maintaining\ninference quality. Experiments show that our solution outperforms current\nmethods, and codes are at:\nhttps://github.com/JasonNing96/DSSD-Efficient-Edge-Computing", "published": "2025-07-16 07:55:06", "link": "http://arxiv.org/abs/2507.12000v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "STFT-based Time-Frequency Mode Decomposition: A Fast and Robust Method for Multicomponent Signal Analysis", "abstract": "The decomposition of complex, multicomponent, and non-stationary signals into\ntheir constituent modes is a fundamental yet significant challenge in science\nand engineering. Existing methods often struggle with a trade-off among\naccuracy, computational cost, and the need for prior information such as the\nnumber of modes. This paper introduces time-frequency mode decomposition\n(TFMD), a novel framework for the fast, robust, and adaptive decomposition of\nsuch signals. TFMD operates on the principle that modes form contiguous\nhigh-energy regions in the time-frequency domain. Its non-iterative pipeline\nreframes signal decomposition as an image segmentation task: a signal is\ntransformed into a spectrogram, which is then smoothed to enhance the\ncontinuity of these high-energy regions. A sequence of adaptive thresholding\nand connected-component labeling with size-based filtering is then employed to\nautomatically segment the spectrogram and generate a mask for each mode. The\nmodes are finally reconstructed via the inverse short-time Fourier transform.\nValidation on diverse synthetic signals demonstrates that TFMD accurately\ndetermines the number of modes and reconstructs them with high fidelity. Its\nperformance is particularly strong in high-noise conditions. A comparative\nanalysis confirms that TFMD provides robust, competitive performance across a\nwider variety of signal types, while a theoretical complexity analysis reveals\nits superior computational efficiency stemming from its non-iterative design.\nThe method's practical utility is further demonstrated by successfully\nextracting modal responses from a real-world footbridge vibration signal. TFMD\nprovides a computationally efficient and powerful paradigm for multicomponent\nsignal analysis, offering a compelling balance of accuracy, versatility, and\nefficiency for large-scale or time-sensitive applications.", "published": "2025-07-16 05:28:47", "link": "http://arxiv.org/abs/2507.11919v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Scene Graph-Aided Probabilistic Semantic Communication for Image Transmission", "abstract": "Semantic communication emphasizes the transmission of meaning rather than raw\nsymbols. It offers a promising solution to alleviate network congestion and\nimprove transmission efficiency. In this paper, we propose a wireless image\ncommunication framework that employs probability graphs as shared semantic\nknowledge base among distributed users. High-level image semantics are\nrepresented via scene graphs, and a two-stage compression algorithm is devised\nto remove predictable components based on learned conditional and co-occurrence\nprobabilities. At the transmitter, the algorithm filters redundant relations\nand entity pairs, while at the receiver, semantic recovery leverages the same\nprobability graphs to reconstruct omitted information. For further research, we\nalso put forward a multi-round semantic compression algorithm with its\ntheoretical performance analysis. Simulation results demonstrate that our\nsemantic-aware scheme achieves superior transmission throughput and satiable\nsemantic alignment, validating the efficacy of leveraging high-level semantics\nfor image communication.", "published": "2025-07-16 04:57:05", "link": "http://arxiv.org/abs/2507.11913v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Joint UAV Placement and Transceiver Design in Multi-User Wireless Relay Networks", "abstract": "In this paper, a novel approach is proposed to improve the minimum\nsignal-to-interference-plus-noise-ratio (SINR) among users in non-orthogonal\nmulti-user wireless relay networks, by optimizing the placement of unmanned\naerial vehicle (UAV) relays, relay beamforming, and receive combining. The\ndesign is separated into two problems: beamforming-aware UAV placement\noptimization and transceiver design for minimum SINR maximization. A\nsignificant challenge in beamforming-aware UAV placement optimization is the\nlack of instantaneous channel state information (CSI) prior to deploying UAV\nrelays, making it difficult to derive the beamforming SINR in non-orthogonal\nmulti-user transmission. To address this issue, an approximation of the\nexpected beamforming SINR is derived using the narrow beam property of a\nmassive MIMO base station. Based on this, a UAV placement algorithm is proposed\nto provide UAV positions that improve the minimum expected beamforming SINR\namong users, using a difference-of-convex framework. Subsequently, after\ndeploying the UAV relays to the optimized positions, and with estimated CSI\navailable, a joint relay beamforming and receive combining (JRBC) algorithm is\nproposed to optimize the transceiver to improve the minimum beamforming SINR\namong users, using a block-coordinate descent approach. Numerical results show\nthat the UAV placement algorithm combined with the JRBC algorithm provides a\n4.6 dB SINR improvement over state-of-the-art schemes.", "published": "2025-07-16 04:55:19", "link": "http://arxiv.org/abs/2507.11912v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Directional Measurements and Analysis for FR3 Low-Altitude Channels in a Campus Environment", "abstract": "In this paper, we present detailed low-altitude channel measurements at the\nFR3 band in an outdoor campus environment. Using a time-domain channel sounder\nsystem, we conduct two types of measurements: path loss measurements by moving\nthe transmitter (Tx) at one-meter intervals along a 26-point rooftop path, and\ndirectional power angular spectrum measurements through antenna scanning at\nhalf-power beam width intervals. The path loss analysis across different Rx\nshows that the close-in model outperforms conventional 3GPP models and\nheight-corrected variants, with path loss exponents close to free space values\nindicating line-of-sight dominance. The power angular spectrum measurements\nshow that propagation behavior varies significantly with environmental\nconditions. Closer Rx exhibit stronger sensitivity to ground reflections during\ndownward Tx tilting, while obstructed links display uniform angular\ncharacteristics due to dominant scattering effects, and corridor environments\nproduce asymmetric power distributions. These results indicate that\nlow-altitude propagation is characterized by complex interactions between Tx\nheight and ground scattering mechanisms, providing fundamental insights for\nchannel modeling in emerging mid-band communication systems.", "published": "2025-07-16 02:21:21", "link": "http://arxiv.org/abs/2507.11846v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "MERA Code: A Unified Framework for Evaluating Code Generation Across Tasks", "abstract": "Advancements in LLMs have enhanced task automation in software engineering;\nhowever, current evaluations primarily focus on natural language tasks,\noverlooking code quality. Most benchmarks prioritize high-level reasoning over\nexecutable code and real-world performance, leaving gaps in understanding true\ncapabilities and risks associated with these models in production. To address\nthis issue, we propose MERA Code, a new addition to the MERA benchmark family,\nspecifically focused on evaluating code for the latest code generation LLMs in\nRussian. This benchmark includes 11 evaluation tasks that span 8 programming\nlanguages. Our proposed evaluation methodology features a taxonomy that\noutlines the practical coding skills necessary for models to complete these\ntasks. The benchmark comprises an open-source codebase for users to conduct\nMERA assessments, a scoring system compatible with various programming\nenvironments, and a platform featuring a leaderboard and submission system. We\nevaluate open LLMs and frontier API models, analyzing their limitations in\nterms of practical coding tasks in non-English languages. We are publicly\nreleasing MERA to guide future research, anticipate groundbreaking features in\nmodel development, and standardize evaluation procedures.", "published": "2025-07-16 14:31:33", "link": "http://arxiv.org/abs/2507.12284v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "A Comparative Approach to Assessing Linguistic Creativity of Large Language Models and Humans", "abstract": "The following paper introduces a general linguistic creativity test for\nhumans and Large Language Models (LLMs). The test consists of various tasks\naimed at assessing their ability to generate new original words and phrases\nbased on word formation processes (derivation and compounding) and on\nmetaphorical language use. We administered the test to 24 humans and to an\nequal number of LLMs, and we automatically evaluated their answers using OCSAI\ntool for three criteria: Originality, Elaboration, and Flexibility. The results\nshow that LLMs not only outperformed humans in all the assessed criteria, but\ndid better in six out of the eight test tasks. We then computed the uniqueness\nof the individual answers, which showed some minor differences between humans\nand LLMs. Finally, we performed a short manual analysis of the dataset, which\nrevealed that humans are more inclined towards E(extending)-creativity, while\nLLMs favor F(ixed)-creativity.", "published": "2025-07-16 08:56:19", "link": "http://arxiv.org/abs/2507.12039v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos", "abstract": "Real robot data collection for imitation learning has led to significant\nadvancements in robotic manipulation. However, the requirement for robot\nhardware in the process fundamentally constrains the scale of the data. In this\npaper, we explore training Vision-Language-Action (VLA) models using egocentric\nhuman videos. The benefit of using human videos is not only for their scale but\nmore importantly for the richness of scenes and tasks. With a VLA trained on\nhuman video that predicts human wrist and hand actions, we can perform Inverse\nKinematics and retargeting to convert the human actions to robot actions. We\nfine-tune the model using a few robot manipulation demonstrations to obtain the\nrobot policy, namely EgoVLA. We propose a simulation benchmark called Ego\nHumanoid Manipulation Benchmark, where we design diverse bimanual manipulation\ntasks with demonstrations. We fine-tune and evaluate EgoVLA with Ego Humanoid\nManipulation Benchmark and show significant improvements over baselines and\nablate the importance of human data. Videos can be found on our website:\nhttps://rchalyang.github.io/EgoVLA", "published": "2025-07-16 17:27:44", "link": "http://arxiv.org/abs/2507.12440v2", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models", "abstract": "We argue that diffusion models' success in modeling complex distributions is,\nfor the most part, coming from their input conditioning. This paper\ninvestigates the representation used to condition diffusion models from the\nperspective that ideal representations should improve sample fidelity, be easy\nto generate, and be compositional to allow out-of-training samples generation.\nWe introduce Discrete Latent Code (DLC), an image representation derived from\nSimplicial Embeddings trained with a self-supervised learning objective. DLCs\nare sequences of discrete tokens, as opposed to the standard continuous image\nembeddings. They are easy to generate and their compositionality enables\nsampling of novel images beyond the training distribution. Diffusion models\ntrained with DLCs have improved generation fidelity, establishing a new\nstate-of-the-art for unconditional image generation on ImageNet. Additionally,\nwe show that composing DLCs allows the image generator to produce\nout-of-distribution samples that coherently combine the semantics of images in\ndiverse ways. Finally, we showcase how DLCs can enable text-to-image generation\nby leveraging large-scale pretrained language models. We efficiently finetune a\ntext diffusion language model to generate DLCs that produce novel samples\noutside of the image generator training distribution.", "published": "2025-07-16 15:12:17", "link": "http://arxiv.org/abs/2507.12318v2", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in Extremely Preterm Infants", "abstract": "Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of\nextremely low birth weight infants. Defined by oxygen dependence at 36 weeks\npostmenstrual age, it causes lifelong respiratory complications. However,\npreventive interventions carry severe risks, including neurodevelopmental\nimpairment, ventilator-induced lung injury, and systemic complications.\nTherefore, early BPD prognosis and prediction of BPD outcome is crucial to\navoid unnecessary toxicity in low risk infants. Admission radiographs of\nextremely preterm infants are routinely acquired within 24h of life and could\nserve as a non-invasive prognostic tool. In this work, we developed and\ninvestigated a deep learning approach using chest X-rays from 163 extremely\nlow-birth-weight infants ($\\leq$32 weeks gestation, 401-999g) obtained within\n24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult\nchest radiographs, employing progressive layer freezing with discriminative\nlearning rates to prevent overfitting and evaluated a CutMix augmentation and\nlinear probing. For moderate/severe BPD outcome prediction, our best performing\nmodel with progressive freezing, linear probing and CutMix achieved an AUROC of\n0.78 $\\pm$ 0.10, balanced accuracy of 0.69 $\\pm$ 0.10, and an F1-score of 0.67\n$\\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet\ninitialization (p = 0.031) which confirms domain-specific pretraining to be\nimportant for BPD outcome prediction. Routine IRDS grades showed limited\nprognostic value (AUROC 0.57 $\\pm$ 0.11), confirming the need of learned\nmarkers. Our approach demonstrates that domain-specific pretraining enables\naccurate BPD prediction from routine day-1 radiographs. Through progressive\nfreezing and linear probing, the method remains computationally feasible for\nsite-level implementation and future federated learning deployments.", "published": "2025-07-16 14:19:44", "link": "http://arxiv.org/abs/2507.12269v2", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Aime: Towards Fully-Autonomous Multi-Agent Framework", "abstract": "Multi-Agent Systems (MAS) powered by Large Language Models (LLMs) are\nemerging as a powerful paradigm for solving complex, multifaceted problems.\nHowever, the potential of these systems is often constrained by the prevalent\nplan-and-execute framework, which suffers from critical limitations: rigid plan\nexecution, static agent capabilities, and inefficient communication. These\nweaknesses hinder their adaptability and robustness in dynamic environments.\nThis paper introduces Aime, a novel multi-agent framework designed to overcome\nthese challenges through dynamic, reactive planning and execution. Aime\nreplaces the conventional static workflow with a fluid and adaptive\narchitecture. Its core innovations include: (1) a Dynamic Planner that\ncontinuously refines the overall strategy based on real-time execution\nfeedback; (2) an Actor Factory that implements Dynamic Actor instantiation,\nassembling specialized agents on-demand with tailored tools and knowledge; and\n(3) a centralized Progress Management Module that serves as a single source of\ntruth for coherent, system-wide state awareness. We empirically evaluated Aime\non a diverse suite of benchmarks spanning general reasoning (GAIA), software\nengineering (SWE-bench Verified), and live web navigation (WebVoyager). The\nresults demonstrate that Aime consistently outperforms even highly specialized\nstate-of-the-art agents in their respective domains. Its superior adaptability\nand task success rate establish Aime as a more resilient and effective\nfoundation for multi-agent collaboration.", "published": "2025-07-16 07:38:28", "link": "http://arxiv.org/abs/2507.11988v2", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "PhysX: Physical-Grounded 3D Asset Generation", "abstract": "3D modeling is moving from virtual to physical. Existing 3D generation\nprimarily emphasizes geometries and textures while neglecting physical-grounded\nmodeling. Consequently, despite the rapid development of 3D generative models,\nthe synthesized 3D assets often overlook rich and important physical\nproperties, hampering their real-world application in physical domains like\nsimulation and embodied AI. As an initial attempt to address this challenge, we\npropose \\textbf{PhysX}, an end-to-end paradigm for physical-grounded 3D asset\ngeneration. 1) To bridge the critical gap in physics-annotated 3D datasets, we\npresent PhysXNet - the first physics-grounded 3D dataset systematically\nannotated across five foundational dimensions: absolute scale, material,\naffordance, kinematics, and function description. In particular, we devise a\nscalable human-in-the-loop annotation pipeline based on vision-language models,\nwhich enables efficient creation of physics-first assets from raw 3D assets.2)\nFurthermore, we propose \\textbf{PhysXGen}, a feed-forward framework for\nphysics-grounded image-to-3D asset generation, injecting physical knowledge\ninto the pre-trained 3D structural space. Specifically, PhysXGen employs a\ndual-branch architecture to explicitly model the latent correlations between 3D\nstructures and physical properties, thereby producing 3D assets with plausible\nphysical predictions while preserving the native geometry quality. Extensive\nexperiments validate the superior performance and promising generalization\ncapability of our framework. All the code, data, and models will be released to\nfacilitate future research in generative physical AI.", "published": "2025-07-16 17:59:35", "link": "http://arxiv.org/abs/2507.12465v2", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Physics-Informed Linear Model (PILM): Analytical Representations and Application to Crustal Strain Rate Estimation", "abstract": "Many physical systems are described by partial differential equations (PDEs),\nand solving these equations and estimating their coefficients or boundary\nconditions (BCs) from observational data play a crucial role in understanding\nthe associated phenomena. Recently, a machine learning approach known as\nphysics-informed neural network, which solves PDEs using neural networks by\nminimizing the sum of residuals from the PDEs, BCs, and data, has gained\nsignificant attention in the scientific community. In this study, we\ninvestigate a physics-informed linear model (PILM) that uses linear\ncombinations of basis functions to represent solutions, thereby enabling an\nanalytical representation of optimal solutions. The PILM was formulated and\nverified for illustrative forward and inverse problems including cases with\nuncertain BCs. Furthermore, the PILM was applied to estimate crustal strain\nrates using geodetic data. Specifically, physical regularization that enforces\nelastic equilibrium on the velocity fields was compared with mathematical\nregularization that imposes smoothness constraints. From a Bayesian\nperspective, mathematical regularization exhibited superior performance. The\nPILM provides an analytically solvable framework applicable to linear forward\nand inverse problems, underdetermined systems, and physical regularization.", "published": "2025-07-16 13:23:39", "link": "http://arxiv.org/abs/2507.12218v2", "categories": ["cs.LG", "physics.geo-ph"], "primary_category": "cs.LG"}
{"title": "VoxATtack: A Multimodal Attack on Voice Anonymization Systems", "abstract": "Voice anonymization systems aim to protect speaker privacy by obscuring vocal\ntraits while preserving the linguistic content relevant for downstream\napplications. However, because these linguistic cues remain intact, they can be\nexploited to identify semantic speech patterns associated with specific\nspeakers. In this work, we present VoxATtack, a novel multimodal\nde-anonymization model that incorporates both acoustic and textual information\nto attack anonymization systems. While previous research has focused on\nrefining speaker representations extracted from speech, we show that\nincorporating textual information with a standard ECAPA-TDNN improves the\nattacker's performance. Our proposed VoxATtack model employs a dual-branch\narchitecture, with an ECAPA-TDNN processing anonymized speech and a pretrained\nBERT encoding the transcriptions. Both outputs are projected into embeddings of\nequal dimensionality and then fused based on confidence weights computed on a\nper-utterance basis. When evaluating our approach on the VoicePrivacy Attacker\nChallenge (VPAC) dataset, it outperforms the top-ranking attackers on five out\nof seven benchmarks, namely B3, B4, B5, T8-5, and T12-5. To further boost\nperformance, we leverage anonymized speech and SpecAugment as augmentation\ntechniques. This enhancement enables VoxATtack to achieve state-of-the-art on\nall VPAC benchmarks, after scoring 20.6% and 27.2% average equal error rate on\nT10-2 and T25-1, respectively. Our results demonstrate that incorporating\ntextual information and selective data augmentation reveals critical\nvulnerabilities in current voice anonymization methods and exposes potential\nweaknesses in the datasets used to evaluate them.", "published": "2025-07-16 09:40:40", "link": "http://arxiv.org/abs/2507.12081v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "DSSD: Efficient Edge-Device LLM Deployment and Collaborative Inference via Distributed Split Speculative Decoding", "abstract": "Large language models (LLMs) have transformed natural language processing but\nface critical deployment challenges in device-edge systems due to resource\nlimitations and communication overhead. To address these issues, collaborative\nframeworks have emerged that combine small language models (SLMs) on devices\nwith LLMs at the edge, using speculative decoding (SD) to improve efficiency.\nHowever, existing solutions often trade inference accuracy for latency or\nsuffer from high uplink transmission costs when verifying candidate tokens. In\nthis paper, we propose Distributed Split Speculative Decoding (DSSD), a novel\narchitecture that not only preserves the SLM-LLM split but also partitions the\nverification phase between the device and edge. In this way, DSSD replaces the\nuplink transmission of multiple vocabulary distributions with a single downlink\ntransmission, significantly reducing communication latency while maintaining\ninference quality. Experiments show that our solution outperforms current\nmethods, and codes are at:\nhttps://github.com/JasonNing96/DSSD-Efficient-Edge-Computing", "published": "2025-07-16 07:55:06", "link": "http://arxiv.org/abs/2507.12000v2", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Improving Drug Identification in Overdose Death Surveillance using Large Language Models", "abstract": "The rising rate of drug-related deaths in the United States, largely driven\nby fentanyl, requires timely and accurate surveillance. However, critical\noverdose data are often buried in free-text coroner reports, leading to delays\nand information loss when coded into ICD (International Classification of\nDisease)-10 classifications. Natural language processing (NLP) models may\nautomate and enhance overdose surveillance, but prior applications have been\nlimited. A dataset of 35,433 death records from multiple U.S. jurisdictions in\n2020 was used for model training and internal testing. External validation was\nconducted using a novel separate dataset of 3,335 records from 2023-2024.\nMultiple NLP approaches were evaluated for classifying specific drug\ninvolvement from unstructured death certificate text. These included\ntraditional single- and multi-label classifiers, as well as fine-tuned\nencoder-only language models such as Bidirectional Encoder Representations from\nTransformers (BERT) and BioClinicalBERT, and contemporary decoder-only large\nlanguage models such as Qwen 3 and Llama 3. Model performance was assessed\nusing macro-averaged F1 scores, and 95% confidence intervals were calculated to\nquantify uncertainty. Fine-tuned BioClinicalBERT models achieved near-perfect\nperformance, with macro F1 scores >=0.998 on the internal test set. External\nvalidation confirmed robustness (macro F1=0.966), outperforming conventional\nmachine learning, general-domain BERT models, and various decoder-only large\nlanguage models. NLP models, particularly fine-tuned clinical variants like\nBioClinicalBERT, offer a highly accurate and scalable solution for overdose\ndeath classification from free-text reports. These methods can significantly\naccelerate surveillance workflows, overcoming the limitations of manual ICD-10\ncoding and supporting near real-time detection of emerging substance use\ntrends.", "published": "2025-07-16 23:29:19", "link": "http://arxiv.org/abs/2507.12679v1", "categories": ["cs.CL", "q-bio.QM", "I.2.7; J.3"], "primary_category": "cs.CL"}
{"title": "The first open machine translation system for the Chechen language", "abstract": "We introduce the first open-source model for translation between the\nvulnerable Chechen language and Russian, and the dataset collected to train and\nevaluate it. We explore fine-tuning capabilities for including a new language\ninto a large language model system for multilingual translation NLLB-200. The\nBLEU / ChrF++ scores for our model are 8.34 / 34.69 and 20.89 / 44.55 for\ntranslation from Russian to Chechen and reverse direction, respectively. The\nrelease of the translation models is accompanied by the distribution of\nparallel words, phrases and sentences corpora and multilingual sentence encoder\nadapted to the Chechen language.", "published": "2025-07-16 23:07:07", "link": "http://arxiv.org/abs/2507.12672v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Fuzzy Approach to Project Success: Measuring What Matters", "abstract": "This paper introduces a novel approach to project success evaluation by\nintegrating fuzzy logic into an existing construct. Traditional Likert-scale\nmeasures often overlook the context-dependent and multifaceted nature of\nproject success. The proposed hierarchical Type-1 Mamdani fuzzy system\nprioritizes sustained positive impact for end-users, reducing emphasis on\nsecondary outcomes like stakeholder satisfaction and internal project success.\nThis dynamic approach may provide a more accurate measure of project success\nand could be adaptable to complex evaluations. Future research will focus on\nempirical testing and broader applications of fuzzy logic in social science.", "published": "2025-07-16 22:05:13", "link": "http://arxiv.org/abs/2507.12653v1", "categories": ["cs.SE", "cs.CL", "H.4.m"], "primary_category": "cs.SE"}
{"title": "Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal Large Language Models", "abstract": "This paper focuses on monolithic Multimodal Large Language Models (MLLMs),\nwhich integrate visual encoding and language decoding into a single model.\nExisting structures and pre-training strategies for monolithic MLLMs often\nsuffer from unstable optimization and catastrophic forgetting. To address these\nchallenges, our key idea is to embed a new visual parameter space into a\npre-trained LLM, enabling stable learning of visual knowledge from noisy data\nvia delta tuning. Based on this principle, we first introduce Mono-InternVL, an\nadvanced monolithic MLLM that incorporates a set of visual experts through a\nmultimodal mixture-of-experts architecture. In addition, we design an\ninnovative Endogenous Visual Pre-training (EViP) for Mono-InternVL to maximize\nits visual capabilities via progressive learning. Mono-InternVL achieves\ncompetitive performance against existing MLLMs but also leads to relatively\nexpensive data cost. Therefore, we further present Mono-InternVL-1.5, a cheaper\nand stronger monolithic MLLM equipped with an improved EViP (EViP++). EViP++\nintroduces additional visual attention experts to Mono-InternVL-1.5 and\nre-organizes the pre-training process in an efficient manner. During inference,\nit includes a fused CUDA kernel to speed up its MoE operations. With these\ndesigns, Mono-InternVL-1.5 significantly reduces training and inference costs,\nwhile still maintaining competitive performance with Mono-InternVL. To evaluate\nour approach, we conduct extensive experiments across 15 benchmarks. Results\ndemonstrate that Mono-InternVL outperforms existing monolithic MLLMs on 12 out\nof 15 benchmarks, e.g., +114-point improvement over Emu3 on OCRBench. Compared\nto its modular counterpart, i.e., InternVL-1.5, Mono-InternVL-1.5 achieves\nsimilar multimodal performance while reducing first-token latency by up to 69%.\nCode and models are released at https://github.com/OpenGVLab/Mono-InternVL.", "published": "2025-07-16 18:31:23", "link": "http://arxiv.org/abs/2507.12566v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Is This Just Fantasy? Language Model Representations Reflect Human Judgments of Event Plausibility", "abstract": "Language models (LMs) are used for a diverse range of tasks, from question\nanswering to writing fantastical stories. In order to reliably accomplish these\ntasks, LMs must be able to discern the modal category of a sentence (i.e.,\nwhether it describes something that is possible, impossible, completely\nnonsensical, etc.). However, recent studies have called into question the\nability of LMs to categorize sentences according to modality (Michaelov et al.,\n2025; Kauf et al., 2023). In this work, we identify linear representations that\ndiscriminate between modal categories within a variety of LMs, or modal\ndifference vectors. Analysis of modal difference vectors reveals that LMs have\naccess to more reliable modal categorization judgments than previously\nreported. Furthermore, we find that modal difference vectors emerge in a\nconsistent order as models become more competent (i.e., through training steps,\nlayers, and parameter count). Notably, we find that modal difference vectors\nidentified within LM activations can be used to model fine-grained human\ncategorization behavior. This potentially provides a novel view into how human\nparticipants distinguish between modal categories, which we explore by\ncorrelating projections along modal difference vectors with human participants'\nratings of interpretable features. In summary, we derive new insights into LM\nmodal categorization using techniques from mechanistic interpretability, with\nthe potential to inform our understanding of modal categorization in humans.", "published": "2025-07-16 18:04:26", "link": "http://arxiv.org/abs/2507.12553v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models", "abstract": "When faced with novel situations, people are able to marshal relevant\nconsiderations from a wide range of background knowledge and put these to use\nin inferences and predictions. What permits us to draw in globally relevant\ninformation and reason over it coherently? Here, we explore the hypothesis that\npeople use a combination of distributed and symbolic representations to\nconstruct bespoke mental models tailored to novel situations. We propose a\ncomputational implementation of this idea -- a ``Model Synthesis Architecture''\n(MSA) -- using language models to implement global relevance-based retrieval\nand model synthesis and probabilistic programs to implement bespoke, coherent\nworld models. We evaluate our MSA as a model of human judgments on a novel\nreasoning dataset. The dataset -- built around a `Model Olympics` domain of\nsports vignettes -- tests models' capacity for human-like, open-ended reasoning\nby requiring (i) judgments about novel causal structures described in language;\n(ii) drawing on large bodies of background knowledge; and (iii) doing both in\nlight of observations that introduce arbitrary novel variables. Our MSA\napproach captures human judgments better than language model-only baselines,\nunder both direct and chain-of-thought generations from the LM that supports\nmodel synthesis. These results suggest that MSAs can be implemented in a way\nthat mirrors people's ability to deliver locally coherent reasoning over\nglobally relevant variables, offering a path to understanding and replicating\nhuman reasoning in open-ended domains.", "published": "2025-07-16 18:01:03", "link": "http://arxiv.org/abs/2507.12547v1", "categories": ["cs.CL", "cs.AI", "cs.PL"], "primary_category": "cs.CL"}
{"title": "Scaling Up RL: Unlocking Diverse Reasoning in LLMs via Prolonged Training", "abstract": "Recent advancements in reasoning-focused language models such as OpenAI's O1\nand DeepSeek-R1 have shown that scaling test-time computation-through\nchain-of-thought reasoning and iterative exploration-can yield substantial\nimprovements on complex tasks like mathematics and code generation. These\nbreakthroughs have been driven by large-scale reinforcement learning (RL),\nparticularly when combined with verifiable reward signals that provide\nobjective and grounded supervision. In this report, we investigate the effects\nof prolonged reinforcement learning on a small language model across a diverse\nset of reasoning domains. Our work identifies several key ingredients for\neffective training, including the use of verifiable reward tasks, enhancements\nto Group Relative Policy Optimization (GRPO), and practical techniques to\nimprove training stability and generalization. We introduce controlled KL\nregularization, clipping ratio, and periodic reference policy resets as\ncritical components for unlocking long-term performance gains. Our model\nachieves significant improvements over strong baselines, including +14.7% on\nmath, +13.9% on coding, and +54.8% on logic puzzle tasks. To facilitate\ncontinued research, we release our model publicly.", "published": "2025-07-16 17:59:24", "link": "http://arxiv.org/abs/2507.12507v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Benchmarking Deception Probes via Black-to-White Performance Boosts", "abstract": "AI assistants will occasionally respond deceptively to user queries.\nRecently, linear classifiers (called \"deception probes\") have been trained to\ndistinguish the internal activations of a language model during deceptive\nversus honest responses. However, it's unclear how effective these probes are\nat detecting deception in practice, nor whether such probes are resistant to\nsimple counter strategies from a deceptive assistant who wishes to evade\ndetection. In this paper, we compare white-box monitoring (where the monitor\nhas access to token-level probe activations) to black-box monitoring (without\nsuch access). We benchmark deception probes by the extent to which the white\nbox monitor outperforms the black-box monitor, i.e. the black-to-white\nperformance boost. We find weak but encouraging black-to-white performance\nboosts from existing deception probes.", "published": "2025-07-16 23:49:55", "link": "http://arxiv.org/abs/2507.12691v1", "categories": ["cs.AI", "cs.LG", "I.2.7; K.4.1"], "primary_category": "cs.AI"}
{"title": "Data Transformation Strategies to Remove Heterogeneity", "abstract": "Data heterogeneity is a prevalent issue, stemming from various conflicting\nfactors, making its utilization complex. This uncertainty, particularly\nresulting from disparities in data formats, frequently necessitates the\ninvolvement of experts to find resolutions. Current methodologies primarily\naddress conflicts related to data structures and schemas, often overlooking the\npivotal role played by data transformation. As the utilization of artificial\nintelligence (AI) continues to expand, there is a growing demand for a more\nstreamlined data preparation process, and data transformation becomes\nparamount. It customizes training data to enhance AI learning efficiency and\nadapts input formats to suit diverse AI models. Selecting an appropriate\ntransformation technique is paramount in preserving crucial data details.\nDespite the widespread integration of AI across various industries,\ncomprehensive reviews concerning contemporary data transformation approaches\nare scarce. This survey explores the intricacies of data heterogeneity and its\nunderlying sources. It systematically categorizes and presents strategies to\naddress heterogeneity stemming from differences in data formats, shedding light\non the inherent challenges associated with each strategy.", "published": "2025-07-16 23:27:24", "link": "http://arxiv.org/abs/2507.12677v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "FORTRESS: Function-composition Optimized Real-Time Resilient Structural Segmentation via Kolmogorov-Arnold Enhanced Spatial Attention Networks", "abstract": "Automated structural defect segmentation in civil infrastructure faces a\ncritical challenge: achieving high accuracy while maintaining computational\nefficiency for real-time deployment. This paper presents FORTRESS\n(Function-composition Optimized Real-Time Resilient Structural Segmentation), a\nnew architecture that balances accuracy and speed by using a special method\nthat combines depthwise separable convolutions with adaptive Kolmogorov-Arnold\nNetwork integration. FORTRESS incorporates three key innovations: a systematic\ndepthwise separable convolution framework achieving a 3.6x parameter reduction\nper layer, adaptive TiKAN integration that selectively applies function\ncomposition transformations only when computationally beneficial, and\nmulti-scale attention fusion combining spatial, channel, and KAN-enhanced\nfeatures across decoder levels. The architecture achieves remarkable efficiency\ngains with 91% parameter reduction (31M to 2.9M), 91% computational complexity\nreduction (13.7 to 1.17 GFLOPs), and 3x inference speed improvement while\ndelivering superior segmentation performance. Evaluation on benchmark\ninfrastructure datasets demonstrates state-of-the-art results with an F1- score\nof 0.771 and a mean IoU of 0.677, significantly outperforming existing methods\nincluding U-Net, SA-UNet, and U- KAN. The dual optimization strategy proves\nessential for optimal performance, establishing FORTRESS as a robust solution\nfor practical structural defect segmentation in resource-constrained\nenvironments where both accuracy and computational efficiency are paramount.\nComprehensive architectural specifications are provided in the Supplemental\nMaterial. Source code is available at URL:\nhttps://github.com/faeyelab/fortress-paper-code.", "published": "2025-07-16 23:17:58", "link": "http://arxiv.org/abs/2507.12675v1", "categories": ["cs.CV", "cs.AI", "eess.IV"], "primary_category": "cs.CV"}
{"title": "ParaStudent: Generating and Evaluating Realistic Student Code by Teaching LLMs to Struggle", "abstract": "Large Language Models (LLMs) have shown strong performance on programming\ntasks, but can they generate student-like code like real students - imperfect,\niterative, and stylistically diverse? We present ParaStudent, a systematic\nstudy of LLM-based \"student-like\" code generation in an introductory\nprogramming course setting. Using a dataset of timestamped student submissions\nacross multiple semesters, we design low- and high-resolution experiments to\nmodel student progress and evaluate code outputs along semantic, functional,\nand stylistic dimensions. Our results show that fine-tuning significantly\nimproves alignment with real student trajectories and captures error patterns,\nincremental improvements, and stylistic variations more faithfully. This study\nshows that modeling realistic student code requires capturing learning dynamics\nthrough context-aware generation, temporal modeling, and multi-dimensional\nevaluation. Code for experiments and evaluation is available at\n\\href{https://github.com/mmiroyan/ParaStudent}{\\texttt{github.com/mmiroyan/ParaStudent}}.", "published": "2025-07-16 23:12:14", "link": "http://arxiv.org/abs/2507.12674v1", "categories": ["cs.CY", "cs.AI", "cs.SE"], "primary_category": "cs.CY"}
{"title": "InSight: AI Mobile Screening Tool for Multiple Eye Disease Detection using Multimodal Fusion", "abstract": "Background/Objectives: Age-related macular degeneration, glaucoma, diabetic\nretinopathy (DR), diabetic macular edema, and pathological myopia affect\nhundreds of millions of people worldwide. Early screening for these diseases is\nessential, yet access to medical care remains limited in low- and middle-income\ncountries as well as in resource-limited settings. We develop InSight, an\nAI-based app that combines patient metadata with fundus images for accurate\ndiagnosis of five common eye diseases to improve accessibility of screenings.\n  Methods: InSight features a three-stage pipeline: real-time image quality\nassessment, disease diagnosis model, and a DR grading model to assess severity.\nOur disease diagnosis model incorporates three key innovations: (a) Multimodal\nfusion technique (MetaFusion) combining clinical metadata and images; (b)\nPretraining method leveraging supervised and self-supervised loss functions;\nand (c) Multitask model to simultaneously predict 5 diseases. We make use of\nBRSET (lab-captured images) and mBRSET (smartphone-captured images) datasets,\nboth of which also contain clinical metadata for model training/evaluation.\n  Results: Trained on a dataset of BRSET and mBRSET images, the image quality\nchecker achieves near-100% accuracy in filtering out low-quality fundus images.\nThe multimodal pretrained disease diagnosis model outperforms models using only\nimages by 6% in balanced accuracy for BRSET and 4% for mBRSET.\n  Conclusions: The InSight pipeline demonstrates robustness across varied image\nconditions and has high diagnostic accuracy across all five diseases,\ngeneralizing to both smartphone and lab captured images. The multitask model\ncontributes to the lightweight nature of the pipeline, making it five times\ncomputationally efficient compared to having five individual models\ncorresponding to each disease.", "published": "2025-07-16 23:00:10", "link": "http://arxiv.org/abs/2507.12669v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Fly, Fail, Fix: Iterative Game Repair with Reinforcement Learning and Large Multimodal Models", "abstract": "Game design hinges on understanding how static rules and content translate\ninto dynamic player behavior - something modern generative systems that inspect\nonly a game's code or assets struggle to capture. We present an automated\ndesign iteration framework that closes this gap by pairing a reinforcement\nlearning (RL) agent, which playtests the game, with a large multimodal model\n(LMM), which revises the game based on what the agent does. In each loop the RL\nplayer completes several episodes, producing (i) numerical play metrics and/or\n(ii) a compact image strip summarising recent video frames. The LMM designer\nreceives a gameplay goal and the current game configuration, analyses the play\ntraces, and edits the configuration to steer future behaviour toward the goal.\nWe demonstrate results that LMMs can reason over behavioral traces supplied by\nRL agents to iteratively refine game mechanics, pointing toward practical,\nscalable tools for AI-assisted game design.", "published": "2025-07-16 22:45:40", "link": "http://arxiv.org/abs/2507.12666v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Single Conversation Methodology: A Human-Centered Protocol for AI-Assisted Software Development", "abstract": "We propose the Single Conversation Methodology (SCM), a novel and pragmatic\napproach to software development using large language models (LLMs). In\ncontrast to ad hoc interactions with generative AI, SCM emphasizes a structured\nand persistent development dialogue, where all stages of a project - from\nrequirements to architecture and implementation - unfold within a single,\nlong-context conversation. The methodology is grounded on principles of\ncognitive clarity, traceability, modularity, and documentation. We define its\nphases, best practices, and philosophical stance, while arguing that SCM offers\na necessary correction to the passive reliance on LLMs prevalent in current\npractices. We aim to reassert the active role of the developer as architect and\nsupervisor of the intelligent tool.", "published": "2025-07-16 22:43:30", "link": "http://arxiv.org/abs/2507.12665v1", "categories": ["cs.SE", "cs.AI", "cs.HC"], "primary_category": "cs.SE"}
{"title": "Improving physics-informed neural network extrapolation via transfer learning and adaptive activation functions", "abstract": "Physics-Informed Neural Networks (PINNs) are deep learning models that\nincorporate the governing physical laws of a system into the learning process,\nmaking them well-suited for solving complex scientific and engineering\nproblems. Recently, PINNs have gained widespread attention as a powerful\nframework for combining physical principles with data-driven modeling to\nimprove prediction accuracy. Despite their successes, however, PINNs often\nexhibit poor extrapolation performance outside the training domain and are\nhighly sensitive to the choice of activation functions (AFs). In this paper, we\nintroduce a transfer learning (TL) method to improve the extrapolation\ncapability of PINNs. Our approach applies transfer learning (TL) within an\nextended training domain, using only a small number of carefully selected\ncollocation points. Additionally, we propose an adaptive AF that takes the form\nof a linear combination of standard AFs, which improves both the robustness and\naccuracy of the model. Through a series of experiments, we demonstrate that our\nmethod achieves an average of 40% reduction in relative L2 error and an average\nof 50% reduction in mean absolute error in the extrapolation domain, all\nwithout a significant increase in computational cost. The code is available at\nhttps://github.com/LiuzLab/PINN-extrapolation .", "published": "2025-07-16 22:19:53", "link": "http://arxiv.org/abs/2507.12659v1", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.DS", "math.NA", "stat.ML"], "primary_category": "cs.LG"}
{"title": "VLMgineer: Vision Language Models as Robotic Toolsmiths", "abstract": "Tool design and use reflect the ability to understand and manipulate the\nphysical world through creativity, planning, and foresight. As such, these\ncapabilities are often regarded as measurable indicators of intelligence across\nbiological species. While much of today's research on robotic intelligence\nfocuses on generating better controllers, inventing smarter tools offers a\ncomplementary form of physical intelligence: shifting the onus of\nproblem-solving onto the tool's design. Given the vast and impressive\ncommon-sense, reasoning, and creative capabilities of today's foundation\nmodels, we investigate whether these models can provide useful priors to\nautomatically design and effectively wield such tools? We present VLMgineer, a\nframework that harnesses the code generation abilities of vision language\nmodels (VLMs) together with evolutionary search to iteratively co-design\nphysical tools and the action plans that operate them to perform a task. We\nevaluate VLMgineer on a diverse new benchmark of everyday manipulation\nscenarios that demand creative tool design and use. Across this suite,\nVLMgineer consistently discovers tools and policies that solve tasks more\neffectively and innovatively, transforming challenging robotics problems into\nstraightforward executions. It also outperforms VLM-generated designs from\nhuman specifications and existing human-crafted tools for everyday tasks. To\nfacilitate future research on automated tool invention, we will release our\nbenchmark and code.", "published": "2025-07-16 21:30:05", "link": "http://arxiv.org/abs/2507.12644v1", "categories": ["cs.RO", "cs.AI", "cs.LG"], "primary_category": "cs.RO"}
{"title": "QSpark: Towards Reliable Qiskit Code Generation", "abstract": "Quantum circuits must be error-resilient, yet LLMs like Granite-20B-Code and\nStarCoder often output flawed Qiskit code. We fine-tuned a 32 B model with two\nRL methods, Group Relative Policy Optimization (GRPO) and Odds-Ratio Preference\nOptimization (ORPO), using a richly annotated synthetic dataset. On the Qiskit\nHumanEval benchmark, ORPO reaches 56.29\\% Pass@1 ($\\approx+10$ pp over\nGranite-8B-QK) and GRPO hits 49\\%, both beating all general-purpose baselines;\non the original HumanEval they score 65.90\\% and 63.00\\%. GRPO excels on basic\ntasks (42/54), ORPO on intermediate ones (41/68), and neither solves the five\nadvanced tasks, highlighting clear gains yet room for progress in AI-assisted\nquantum programming.", "published": "2025-07-16 21:27:31", "link": "http://arxiv.org/abs/2507.12642v1", "categories": ["cs.SE", "cs.AI", "quant-ph"], "primary_category": "cs.SE"}
{"title": "Achieving Robust Channel Estimation Neural Networks by Designed Training Data", "abstract": "Channel estimation is crucial in cognitive communications, as it enables\nintelligent spectrum sensing and adaptive transmission by providing accurate\ninformation about the current channel state. However, in many papers neural\nnetworks are frequently tested by training and testing on one example channel\nor similar channels. This is because data-driven methods often degrade on new\ndata which they are not trained on, as they cannot extrapolate their training\nknowledge. This is despite the fact physical channels are often assumed to be\ntime-variant. However, due to the low latency requirements and limited\ncomputing resources, neural networks may not have enough time and computing\nresources to execute online training to fine-tune the parameters. This\nmotivates us to design offline-trained neural networks that can perform\nrobustly over wireless channels, but without any actual channel information\nbeing known at design time. In this paper, we propose design criteria to\ngenerate synthetic training datasets for neural networks, which guarantee that\nafter training the resulting networks achieve a certain mean squared error\n(MSE) on new and previously unseen channels. Therefore, neural network\nsolutions require no prior channel information or parameters update for\nreal-world implementations. Based on the proposed design criteria, we further\npropose a benchmark design which ensures intelligent operation for different\nchannel profiles. To demonstrate general applicability, we use neural networks\nwith different levels of complexity to show that the generalization achieved\nappears to be independent of neural network architecture. From simulations,\nneural networks achieve robust generalization to wireless channels with both\nfixed channel profiles and variable delay spreads.", "published": "2025-07-16 21:04:37", "link": "http://arxiv.org/abs/2507.12630v1", "categories": ["eess.SP", "cs.AI"], "primary_category": "eess.SP"}
{"title": "BootSeer: Analyzing and Mitigating Initialization Bottlenecks in Large-Scale LLM Training", "abstract": "Large Language Models (LLMs) have become a cornerstone of modern AI, driving\nbreakthroughs in natural language processing and expanding into multimodal jobs\ninvolving images, audio, and video. As with most computational software, it is\nimportant to distinguish between ordinary runtime performance and startup\noverhead. Prior research has focused on runtime performance: improving training\nefficiency and stability. This work focuses instead on the increasingly\ncritical issue of startup overhead in training: the delay before training jobs\nbegin execution. Startup overhead is particularly important in large,\nindustrial-scale LLMs, where failures occur more frequently and multiple teams\noperate in iterative update-debug cycles. In one of our training clusters, more\nthan 3.5% of GPU time is wasted due to startup overhead alone.\n  In this work, we present the first in-depth characterization of LLM training\nstartup overhead based on real production data. We analyze the components of\nstartup cost, quantify its direct impact, and examine how it scales with job\nsize. These insights motivate the design of Bootseer, a system-level\noptimization framework that addresses three primary startup bottlenecks: (a)\ncontainer image loading, (b) runtime dependency installation, and (c) model\ncheckpoint resumption. To mitigate these bottlenecks, Bootseer introduces three\ntechniques: (a) hot block record-and-prefetch, (b) dependency snapshotting, and\n(c) striped HDFS-FUSE. Bootseer has been deployed in a production environment\nand evaluated on real LLM training workloads, demonstrating a 50% reduction in\nstartup overhead.", "published": "2025-07-16 20:32:33", "link": "http://arxiv.org/abs/2507.12619v1", "categories": ["cs.LG", "cs.AI", "cs.DC"], "primary_category": "cs.LG"}
{"title": "Learning What Matters: Probabilistic Task Selection via Mutual Information for Model Finetuning", "abstract": "The performance of finetuned large language models (LLMs) hinges critically\non the composition of the training mixture. However, selecting an optimal blend\nof task datasets remains a largely manual, heuristic driven process, with\npractitioners often relying on uniform or size based sampling strategies. We\nintroduce TASKPGM, a principled and scalable framework for mixture optimization\nthat selects continuous task proportions by minimizing an energy function over\na Markov Random Field (MRF). Task relationships are modeled using behavioral\ndivergences such as Jensen Shannon Divergence and Pointwise Mutual Information\ncomputed from the predictive distributions of single task finetuned models. Our\nmethod yields a closed form solution under simplex constraints and provably\nbalances representativeness and diversity among tasks. We provide theoretical\nguarantees, including weak submodularity for budgeted variants, and demonstrate\nconsistent empirical improvements on Llama 2 and Mistral across evaluation\nsuites such as MMLU and BIGBench. Beyond performance, TASKPGM offers\ninterpretable insights into task influence and mixture composition, making it a\npowerful tool for efficient and robust LLM finetuning.", "published": "2025-07-16 20:14:55", "link": "http://arxiv.org/abs/2507.12612v1", "categories": ["cs.LG", "cs.AI", "68T50", "I.2.7; I.2.6; I.2.4"], "primary_category": "cs.LG"}
{"title": "MS-DGCNN++: A Multi-Scale Fusion Dynamic Graph Neural Network with Biological Knowledge Integration for LiDAR Tree Species Classification", "abstract": "Tree species classification from terrestrial LiDAR point clouds is\nchallenging because of the complex multi-scale geometric structures in forest\nenvironments. Existing approaches using multi-scale dynamic graph convolutional\nneural networks (MS-DGCNN) employ parallel multi-scale processing, which fails\nto capture the semantic relationships between the hierarchical levels of the\ntree architecture. We present MS-DGCNN++, a hierarchical multiscale fusion\ndynamic graph convolutional network that uses semantically meaningful feature\nextraction at local, branch, and canopy scales with cross-scale information\npropagation. Our method employs scale-specific feature engineering, including\nstandard geometric features for the local scale, normalized relative vectors\nfor the branch scale, and distance information for the canopy scale. This\nhierarchical approach replaces uniform parallel processing with semantically\ndifferentiated representations that are aligned with the natural tree\nstructure. Under the same proposed tree species data augmentation strategy for\nall experiments, MS-DGCNN++ achieved an accuracy of 94.96 \\% on STPCTLS,\noutperforming DGCNN, MS-DGCNN, and the state-of-the-art model PPT. On\nFOR-species20K, it achieves 67.25\\% accuracy (6.1\\% improvement compared to\nMS-DGCNN). For standard 3D object recognition, our method outperformed DGCNN\nand MS-DGCNN with overall accuracies of 93.15\\% on ModelNet40 and 94.05\\% on\nModelNet10. With lower parameters and reduced complexity compared to\nstate-of-the-art transformer approaches, our method is suitable for\nresource-constrained applications while maintaining a competitive accuracy.\nBeyond tree classification, the method generalizes to standard 3D object\nrecognition, establishing it as a versatile solution for diverse point cloud\nprocessing applications. The implementation code is publicly available at\nhttps://github.com/said-ohamouddou/MS-DGCNN2.", "published": "2025-07-16 19:44:23", "link": "http://arxiv.org/abs/2507.12602v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "A Survey of Explainable Reinforcement Learning: Targets, Methods and Needs", "abstract": "The success of recent Artificial Intelligence (AI) models has been\naccompanied by the opacity of their internal mechanisms, due notably to the use\nof deep neural networks. In order to understand these internal mechanisms and\nexplain the output of these AI models, a set of methods have been proposed,\ngrouped under the domain of eXplainable AI (XAI). This paper focuses on a\nsub-domain of XAI, called eXplainable Reinforcement Learning (XRL), which aims\nto explain the actions of an agent that has learned by reinforcement learning.\nWe propose an intuitive taxonomy based on two questions \"What\" and \"How\". The\nfirst question focuses on the target that the method explains, while the second\nrelates to the way the explanation is provided. We use this taxonomy to provide\na state-of-the-art review of over 250 papers. In addition, we present a set of\ndomains close to XRL, which we believe should get attention from the community.\nFinally, we identify some needs for the field of XRL.", "published": "2025-07-16 19:41:41", "link": "http://arxiv.org/abs/2507.12599v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Assay2Mol: large language model-based drug design using BioAssay context", "abstract": "Scientific databases aggregate vast amounts of quantitative data alongside\ndescriptive text. In biochemistry, molecule screening assays evaluate the\nfunctional responses of candidate molecules against disease targets.\nUnstructured text that describes the biological mechanisms through which these\ntargets operate, experimental screening protocols, and other attributes of\nassays offer rich information for new drug discovery campaigns but has been\nuntapped because of that unstructured format. We present Assay2Mol, a large\nlanguage model-based workflow that can capitalize on the vast existing\nbiochemical screening assays for early-stage drug discovery. Assay2Mol\nretrieves existing assay records involving targets similar to the new target\nand generates candidate molecules using in-context learning with the retrieved\nassay screening data. Assay2Mol outperforms recent machine learning approaches\nthat generate candidate ligand molecules for target protein structures, while\nalso promoting more synthesizable molecule generation.", "published": "2025-07-16 18:42:18", "link": "http://arxiv.org/abs/2507.12574v1", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "primary_category": "cs.LG"}
{"title": "Safeguarding Federated Learning-based Road Condition Classification", "abstract": "Federated Learning (FL) has emerged as a promising solution for\nprivacy-preserving autonomous driving, specifically camera-based Road Condition\nClassification (RCC) systems, harnessing distributed sensing, computing, and\ncommunication resources on board vehicles without sharing sensitive image data.\nHowever, the collaborative nature of FL-RCC frameworks introduces new\nvulnerabilities: Targeted Label Flipping Attacks (TLFAs), in which malicious\nclients (vehicles) deliberately alter their training data labels to compromise\nthe learned model inference performance. Such attacks can, e.g., cause a\nvehicle to mis-classify slippery, dangerous road conditions as pristine and\nexceed recommended speed. However, TLFAs for FL-based RCC systems are largely\nmissing. We address this challenge with a threefold contribution: 1) we\ndisclose the vulnerability of existing FL-RCC systems to TLFAs; 2) we introduce\na novel label-distance-based metric to precisely quantify the safety risks\nposed by TLFAs; and 3) we propose FLARE, a defensive mechanism leveraging\nneuron-wise analysis of the output layer to mitigate TLFA effects. Extensive\nexperiments across three RCC tasks, four evaluation metrics, six baselines, and\nthree deep learning models demonstrate both the severity of TLFAs on FL-RCC\nsystems and the effectiveness of FLARE in mitigating the attack impact.", "published": "2025-07-16 18:33:29", "link": "http://arxiv.org/abs/2507.12568v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR"}
{"title": "Can Mental Imagery Improve the Thinking Capabilities of AI Systems?", "abstract": "Although existing models can interact with humans and provide satisfactory\nresponses, they lack the ability to act autonomously or engage in independent\nreasoning. Furthermore, input data in these models is typically provided as\nexplicit queries, even when some sensory data is already acquired.\n  In addition, AI agents, which are computational entities designed to perform\ntasks and make decisions autonomously based on their programming, data inputs,\nand learned knowledge, have shown significant progress. However, they struggle\nwith integrating knowledge across multiple domains, unlike humans.\n  Mental imagery plays a fundamental role in the brain's thinking process,\nwhich involves performing tasks based on internal multisensory data, planned\nactions, needs, and reasoning capabilities. In this paper, we investigate how\nto integrate mental imagery into a machine thinking framework and how this\ncould be beneficial in initiating the thinking process. Our proposed machine\nthinking framework integrates a Cognitive thinking unit supported by three\nauxiliary units: the Input Data Unit, the Needs Unit, and the Mental Imagery\nUnit. Within this framework, data is represented as natural language sentences\nor drawn sketches, serving both informative and decision-making purposes. We\nconducted validation tests for this framework, and the results are presented\nand discussed.", "published": "2025-07-16 18:06:13", "link": "http://arxiv.org/abs/2507.12555v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "MindJourney: Test-Time Scaling with World Models for Spatial Reasoning", "abstract": "Spatial reasoning in 3D space is central to human cognition and indispensable\nfor embodied tasks such as navigation and manipulation. However,\nstate-of-the-art vision-language models (VLMs) struggle frequently with tasks\nas simple as anticipating how a scene will look after an egocentric motion:\nthey perceive 2D images but lack an internal model of 3D dynamics. We therefore\npropose MindJourney, a test-time scaling framework that grants a VLM with this\nmissing capability by coupling it to a controllable world model based on video\ndiffusion. The VLM iteratively sketches a concise camera trajectory, while the\nworld model synthesizes the corresponding view at each step. The VLM then\nreasons over this multi-view evidence gathered during the interactive\nexploration. Without any fine-tuning, our MindJourney achieves over an average\n8% performance boost on the representative spatial reasoning benchmark SAT,\nshowing that pairing VLMs with world models for test-time scaling offers a\nsimple, plug-and-play route to robust 3D reasoning. Meanwhile, our method also\nimproves upon the test-time inference VLMs trained through reinforcement\nlearning, which demonstrates the potential of our method that utilizes world\nmodels for test-time scaling.", "published": "2025-07-16 17:59:36", "link": "http://arxiv.org/abs/2507.12508v1", "categories": ["cs.CV", "cs.AI", "cs.RO"], "primary_category": "cs.CV"}
{"title": "TRIQA: Image Quality Assessment by Contrastive Pretraining on Ordered Distortion Triplets", "abstract": "Image Quality Assessment (IQA) models aim to predict perceptual image quality\nin alignment with human judgments. No-Reference (NR) IQA remains particularly\nchallenging due to the absence of a reference image. While deep learning has\nsignificantly advanced this field, a major hurdle in developing NR-IQA models\nis the limited availability of subjectively labeled data. Most existing deep\nlearning-based NR-IQA approaches rely on pre-training on large-scale datasets\nbefore fine-tuning for IQA tasks. To further advance progress in this area, we\npropose a novel approach that constructs a custom dataset using a limited\nnumber of reference content images and introduces a no-reference IQA model that\nincorporates both content and quality features for perceptual quality\nprediction. Specifically, we train a quality-aware model using contrastive\ntriplet-based learning, enabling efficient training with fewer samples while\nachieving strong generalization performance across publicly available datasets.\nOur repository is available at https://github.com/rajeshsureddi/triqa.", "published": "2025-07-16 23:43:12", "link": "http://arxiv.org/abs/2507.12687v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Integrated Oculomics and Lipidomics Reveal Microvascular Metabolic Signatures Associated with Cardiovascular Health in a Healthy Cohort", "abstract": "Cardiovascular disease (CVD) remains the leading global cause of mortality,\nyet current risk stratification methods often fail to detect early, subclinical\nchanges. Previous studies have generally not integrated retinal\nmicrovasculature characteristics with comprehensive serum lipidomic profiles as\npotential indicators of CVD risk. In this study, an innovative imaging omics\nframework was introduced, combining retinal microvascular traits derived\nthrough deep learning based image processing with serum lipidomic data to\nhighlight asymptomatic biomarkers of cardiovascular risk beyond the\nconventional lipid panel. This represents the first large scale, covariate\nadjusted and stratified correlation analysis conducted in a healthy population,\nwhich is essential for identifying early indicators of disease. Retinal\nphenotypes were quantified using automated image analysis tools, while serum\nlipid profiling was performed by Ultra High Performance Liquid Chromatography\nElectrospray ionization High resolution mass spectrometry (UHPLC ESI HRMS).\nStrong, age- and sex-independent correlations were established, particularly\nbetween average artery width, vessel density, and lipid subclasses such as\ntriacylglycerols (TAGs), diacylglycerols (DAGs), and ceramides (Cers). These\nassociations suggest a converging mechanism of microvascular remodeling under\nmetabolic stress. By linking detailed\n  vascular structural phenotypes to specific lipid species, this study fills a\ncritical gap in the understanding of early CVD pathogenesis. This integration\nnot only offers a novel perspective on microvascular metabolic associations but\nalso presents a significant opportunity for the identification of robust,\nnon-invasive biomarkers. Ultimately, these findings may support improved early\ndetection, targeted prevention, and personalized approaches in cardiovascular\nhealthcare.", "published": "2025-07-16 22:40:17", "link": "http://arxiv.org/abs/2507.12663v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Reconstruct, Inpaint, Finetune: Dynamic Novel-view Synthesis from Monocular Videos", "abstract": "We explore novel-view synthesis for dynamic scenes from monocular videos.\nPrior approaches rely on costly test-time optimization of 4D representations or\ndo not preserve scene geometry when trained in a feed-forward manner. Our\napproach is based on three key insights: (1) covisible pixels (that are visible\nin both the input and target views) can be rendered by first reconstructing the\ndynamic 3D scene and rendering the reconstruction from the novel-views and (2)\nhidden pixels in novel views can be \"inpainted\" with feed-forward 2D video\ndiffusion models. Notably, our video inpainting diffusion model (CogNVS) can be\nself-supervised from 2D videos, allowing us to train it on a large corpus of\nin-the-wild videos. This in turn allows for (3) CogNVS to be applied zero-shot\nto novel test videos via test-time finetuning. We empirically verify that\nCogNVS outperforms almost all prior art for novel-view synthesis of dynamic\nscenes from monocular videos.", "published": "2025-07-16 21:40:29", "link": "http://arxiv.org/abs/2507.12646v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Funnel-HOI: Top-Down Perception for Zero-Shot HOI Detection", "abstract": "Human-object interaction detection (HOID) refers to localizing interactive\nhuman-object pairs in images and identifying the interactions. Since there\ncould be an exponential number of object-action combinations, labeled data is\nlimited - leading to a long-tail distribution problem. Recently, zero-shot\nlearning emerged as a solution, with end-to-end transformer-based object\ndetectors adapted for HOID becoming successful frameworks. However, their\nprimary focus is designing improved decoders for learning entangled or\ndisentangled interpretations of interactions. We advocate that HOI-specific\ncues must be anticipated at the encoder stage itself to obtain a stronger scene\ninterpretation. Consequently, we build a top-down framework named Funnel-HOI\ninspired by the human tendency to grasp well-defined concepts first and then\nassociate them with abstract concepts during scene understanding. We first\nprobe an image for the presence of objects (well-defined concepts) and then\nprobe for actions (abstract concepts) associated with them. A novel asymmetric\nco-attention mechanism mines these cues utilizing multimodal information\n(incorporating zero-shot capabilities) and yields stronger interaction\nrepresentations at the encoder level. Furthermore, a novel loss is devised that\nconsiders objectaction relatedness and regulates misclassification penalty\nbetter than existing loss functions for guiding the interaction classifier.\nExtensive experiments on the HICO-DET and V-COCO datasets across\nfully-supervised and six zero-shot settings reveal our state-of-the-art\nperformance, with up to 12.4% and 8.4% gains for unseen and rare HOI\ncategories, respectively.", "published": "2025-07-16 20:47:24", "link": "http://arxiv.org/abs/2507.12628v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Pathology-Guided Virtual Staining Metric for Evaluation and Training", "abstract": "Virtual staining has emerged as a powerful alternative to traditional\nhistopathological staining techniques, enabling rapid, reagent-free image\ntransformations. However, existing evaluation methods predominantly rely on\nfull-reference image quality assessment (FR-IQA) metrics such as structural\nsimilarity, which are originally designed for natural images and often fail to\ncapture pathology-relevant features. Expert pathology reviews have also been\nused, but they are inherently subjective and time-consuming.\n  In this study, we introduce PaPIS (Pathology-Aware Perceptual Image\nSimilarity), a novel FR-IQA metric specifically tailored for virtual staining\nevaluation. PaPIS leverages deep learning-based features trained on cell\nmorphology segmentation and incorporates Retinex-inspired feature decomposition\nto better reflect histological perceptual quality. Comparative experiments\ndemonstrate that PaPIS more accurately aligns with pathology-relevant visual\ncues and distinguishes subtle cellular structures that traditional and existing\nperceptual metrics tend to overlook. Furthermore, integrating PaPIS as a\nguiding loss function in a virtual staining model leads to improved\nhistological fidelity.\n  This work highlights the critical need for pathology-aware evaluation\nframeworks to advance the development and clinical readiness of virtual\nstaining technologies.", "published": "2025-07-16 20:39:55", "link": "http://arxiv.org/abs/2507.12624v1", "categories": ["eess.IV", "cs.CV", "cs.SY", "eess.SY"], "primary_category": "eess.IV"}
{"title": "Predicting Soccer Penalty Kick Direction Using Human Action Recognition", "abstract": "Action anticipation has become a prominent topic in Human Action Recognition\n(HAR). However, its application to real-world sports scenarios remains limited\nby the availability of suitable annotated datasets. This work presents a novel\ndataset of manually annotated soccer penalty kicks to predict shot direction\nbased on pre-kick player movements. We propose a deep learning classifier to\nbenchmark this dataset that integrates HAR-based feature embeddings with\ncontextual metadata. We evaluate twenty-two backbone models across seven\narchitecture families (MViTv2, MViTv1, SlowFast, Slow, X3D, I3D, C2D),\nachieving up to 63.9% accuracy in predicting shot direction (left or right),\noutperforming the real goalkeepers' decisions. These results demonstrate the\ndataset's value for anticipatory action recognition and validate our model's\npotential as a generalizable approach for sports-based predictive tasks.", "published": "2025-07-16 20:27:11", "link": "http://arxiv.org/abs/2507.12617v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Quantitative Edge Eigenvector Universality for Random Regular Graphs: Berry-Esseen Bounds with Explicit Constants", "abstract": "We establish the first quantitative Berry-Esseen bounds for edge eigenvector\nstatistics in random regular graphs. For any $d$-regular graph on $N$ vertices\nwith fixed $d \\geq 3$ and deterministic unit vector $\\mathbf{q} \\perp\n\\mathbf{e}$, we prove that the normalized overlap $\\sqrt{N}\\langle \\mathbf{q},\n\\mathbf{u}_2 \\rangle$ satisfies \\[ \\sup_{x \\in \\mathbb{R}}\n\\left|\\mathbb{P}\\left(\\sqrt{N}\\langle \\mathbf{q}, \\mathbf{u}_2 \\rangle \\leq\nx\\right) - \\Phi(x)\\right| \\leq C_d N^{-1/6+\\varepsilon} \\] where $\\mathbf{u}_2$\nis the second eigenvector and $C_d \\leq \\tilde{C}d^3\\varepsilon^{-10}$ for an\nabsolute constant $\\tilde{C}$. This provides the first explicit convergence\nrate for the recent edge eigenvector universality results of He, Huang, and Yau\n\\cite{HHY25}.\n  Our proof introduces a single-scale comparison method using constrained Dyson\nBrownian motion that preserves the degree constraint $\\tilde{H}_t\\mathbf{e} =\n0$ throughout the evolution. The key technical innovation is a sharp edge\nisotropic local law with explicit constant $C(d,\\varepsilon) \\leq\n\\tilde{C}d\\varepsilon^{-5}$, enabling precise control of eigenvector overlap\ndynamics.\n  At the critical time $t_* = N^{-1/3+\\varepsilon}$, we perform a fourth-order\ncumulant comparison with constrained GOE, achieving optimal error bounds\nthrough a single comparison rather than the traditional multi-scale approach.\nWe extend our results to joint universality for the top $K$ edge eigenvectors\nwith $K \\leq N^{1/10-\\delta}$, showing they converge to independent Gaussians.\nThrough analysis of eigenvalue spacing barriers, critical time scales, and\ncomparison across multiple proof methods, we provide evidence that the\n$N^{-1/6}$ rate is optimal for sparse regular graphs. All constants are tracked\nexplicitly throughout, enabling finite-size applications in spectral algorithms\nand network analysis.", "published": "2025-07-16 05:31:51", "link": "http://arxiv.org/abs/2507.12502v1", "categories": ["math.PR", "cs.DM", "math.CO", "math.SP", "60B20, 15B52, 05C80, 60F05", "G.2.2; G.3; F.2.1; G.1.3"], "primary_category": "math.PR"}
{"title": "Differential Communication in Channels with Mobility and Delay Spread using Zak-OTFS", "abstract": "Zak-transform based orthogonal time frequency space (Zak-OTFS) is a\ndelay-Doppler (DD) domain modulation scheme in which the signal processing is\ncarried out in the DD domain. The channel when viewed in the DD domain is\npredictable. However, even with Zak-OTFS, pilots need to be sent periodically,\nalbeit at a lower rate. In this paper, we propose a differential communication\nscheme for Zak-OTFS systems that alleviates the need for periodic pilot\ntransmission. Towards this, we analytically show that the detected data can be\nused as a pilot and that the channel estimate obtained from the detected data\ncan enable further detection enabling the \"differential\" aspect of the\ncommunication. Specifically, we leverage the prediction capability of the DD\nchannel in Zak-OTFS to use the channel estimate (obtained from detected data\nsymbols treated as pilots) in the previous instant to detect data in the next\ninstant and propagate this forward. The advantages are two fold. First, it\nallows the data symbols to enjoy higher energy since the energy that would\notherwise be required for pilot symbols can also be allocated to data symbols.\nSecond, it allows for full spectral efficiency compared to point or embedded\npilots. Comparison with the full spectral efficiency achieving spread pilot\nscheme shows that the proposed method achieves better bit-error rate at lower\ncomplexity.", "published": "2025-07-16 19:22:42", "link": "http://arxiv.org/abs/2507.12593v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "Robust Resource Allocation for Pinching-Antenna Systems under Imperfect CSI", "abstract": "Pinching-antenna technology has lately showcased its promising capability for\nreconfiguring wireless propagation environments, especially in high-frequency\ncommunication systems like millimeter-wave and terahertz bands. By dynamically\nplacing the antenna over a dielectric waveguide, line-of-sight (LoS)\nconnections can be made to significantly improve system performance. Although\nrecent research have illustrated the advantages of pinching-antenna-assisted\ndesigns, they mainly presuppose complete knowledge of user locations -- an\nimpractical assumption in real-world systems. To address this issue, the robust\nresource allocation in a multi-user pinching antenna downlink system with\nuncertain user positions is investigated, aiming to minimize total transmit\npower while satisfying individual outage probability constraints. First, we\naddress the single-user case, deriving the optimal pinching antenna position\nand obtaining the corresponding power allocation using a bisection method\ncombined with geometric analysis. We then extend this solution to the\nmulti-user case. In this case, we optimize the pinching antenna position using\na particle swarm optimization (PSO) algorithm to handle the resulting\nnon-convex and non-differentiable optimization problem. Simulation results\ndemonstrate that the proposed scheme outperforms conventional fixed-antenna\nsystems and validate the effectiveness of the PSO-based antenna placement\nstrategy under location uncertainty.", "published": "2025-07-16 18:59:57", "link": "http://arxiv.org/abs/2507.12582v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Finite-Dimensional Gaussian Approximation for Deep Neural Networks: Universality in Random Weights", "abstract": "We study the Finite-Dimensional Distributions (FDDs) of deep neural networks\nwith randomly initialized weights that have finite-order moments. Specifically,\nwe establish Gaussian approximation bounds in the Wasserstein-$1$ norm between\nthe FDDs and their Gaussian limit assuming a Lipschitz activation function and\nallowing the layer widths to grow to infinity at arbitrary relative rates. In\nthe special case where all widths are proportional to a common scale parameter\n$n$ and there are $L-1$ hidden layers, we obtain convergence rates of order\n$n^{-({1}/{6})^{L-1} + \\epsilon}$, for any $\\epsilon > 0$.", "published": "2025-07-16 23:41:09", "link": "http://arxiv.org/abs/2507.12686v1", "categories": ["stat.ML", "cs.LG", "math.PR", "math.ST", "stat.TH"], "primary_category": "stat.ML"}
{"title": "Physics constrained learning of stochastic characteristics", "abstract": "Accurate state estimation requires careful consideration of uncertainty\nsurrounding the process and measurement models; these characteristics are\nusually not well-known and need an experienced designer to select the\ncovariance matrices. An error in the selection of covariance matrices could\nimpact the accuracy of the estimation algorithm and may sometimes cause the\nfilter to diverge. Identifying noise characteristics has long been a\nchallenging problem due to uncertainty surrounding noise sources and\ndifficulties in systematic noise modeling. Most existing approaches try\nidentifying unknown covariance matrices through an optimization algorithm\ninvolving innovation sequences. In recent years, learning approaches have been\nutilized to determine the stochastic characteristics of process and measurement\nmodels. We present a learning-based methodology with different loss functions\nto identify noise characteristics and test these approaches' performance for\nreal-time vehicle state estimation", "published": "2025-07-16 22:31:29", "link": "http://arxiv.org/abs/2507.12661v1", "categories": ["stat.ML", "cs.LG", "cs.SY", "eess.SY"], "primary_category": "stat.ML"}
{"title": "Distributional Reinforcement Learning on Path-dependent Options", "abstract": "We reinterpret and propose a framework for pricing path-dependent financial\nderivatives by estimating the full distribution of payoffs using Distributional\nReinforcement Learning (DistRL). Unlike traditional methods that focus on\nexpected option value, our approach models the entire conditional distribution\nof payoffs, allowing for risk-aware pricing, tail-risk estimation, and enhanced\nuncertainty quantification. We demonstrate the efficacy of this method on Asian\noptions, using quantile-based value function approximators.", "published": "2025-07-16 22:14:54", "link": "http://arxiv.org/abs/2507.12657v1", "categories": ["q-fin.MF", "cs.LG"], "primary_category": "q-fin.MF"}
{"title": "Federated Learning in Open- and Closed-Loop EMG Decoding: A Privacy and Performance Perspective", "abstract": "Invasive and non-invasive neural interfaces hold promise as high-bandwidth\ninput devices for next-generation technologies. However, neural signals\ninherently encode sensitive information about an individual's identity and\nhealth, making data sharing for decoder training a critical privacy challenge.\nFederated learning (FL), a distributed, privacy-preserving learning framework,\npresents a promising solution, but it remains unexplored in closed-loop\nadaptive neural interfaces. Here, we introduce FL-based neural decoding and\nsystematically evaluate its performance and privacy using high-dimensional\nelectromyography signals in both open- and closed-loop scenarios. In open-loop\nsimulations, FL significantly outperformed local learning baselines,\ndemonstrating its potential for high-performance, privacy-conscious neural\ndecoding. In contrast, closed-loop user studies required adapting FL methods to\naccommodate single-user, real-time interactions, a scenario not supported by\nstandard FL. This modification resulted in local learning decoders surpassing\nthe adapted FL approach in closed-loop performance, yet local learning still\ncarried higher privacy risks. Our findings highlight a critical\nperformance-privacy tradeoff in real-time adaptive applications and indicate\nthe need for FL methods specifically designed for co-adaptive, single-user\napplications.", "published": "2025-07-16 21:59:25", "link": "http://arxiv.org/abs/2507.12652v1", "categories": ["cs.LG", "cs.CR", "cs.HC"], "primary_category": "cs.LG"}
{"title": "A Novel Data Augmentation Strategy for Robust Deep Learning Classification of Biomedical Time-Series Data: Application to ECG and EEG Analysis", "abstract": "The increasing need for accurate and unified analysis of diverse biological\nsignals, such as ECG and EEG, is paramount for comprehensive patient\nassessment, especially in synchronous monitoring. Despite advances in\nmulti-sensor fusion, a critical gap remains in developing unified architectures\nthat effectively process and extract features from fundamentally different\nphysiological signals. Another challenge is the inherent class imbalance in\nmany biomedical datasets, often causing biased performance in traditional\nmethods. This study addresses these issues by proposing a novel and unified\ndeep learning framework that achieves state-of-the-art performance across\ndifferent signal types. Our method integrates a ResNet-based CNN with an\nattention mechanism, enhanced by a novel data augmentation strategy:\ntime-domain concatenation of multiple augmented variants of each signal to\ngenerate richer representations. Unlike prior work, we scientifically increase\nsignal complexity to achieve future-reaching capabilities, which resulted in\nthe best predictions compared to the state of the art. Preprocessing steps\nincluded wavelet denoising, baseline removal, and standardization. Class\nimbalance was effectively managed through the combined use of this advanced\ndata augmentation and the Focal Loss function. Regularization techniques were\napplied during training to ensure generalization. We rigorously evaluated the\nproposed architecture on three benchmark datasets: UCI Seizure EEG, MIT-BIH\nArrhythmia, and PTB Diagnostic ECG. It achieved accuracies of 99.96%, 99.78%,\nand 100%, respectively, demonstrating robustness across diverse signal types\nand clinical contexts. Finally, the architecture requires ~130 MB of memory and\nprocesses each sample in ~10 ms, suggesting suitability for deployment on\nlow-end or wearable devices.", "published": "2025-07-16 21:38:10", "link": "http://arxiv.org/abs/2507.12645v1", "categories": ["eess.SP", "cs.LG"], "primary_category": "eess.SP"}
{"title": "NLI4VolVis: Natural Language Interaction for Volume Visualization via LLM Multi-Agents and Editable 3D Gaussian Splatting", "abstract": "Traditional volume visualization (VolVis) methods, like direct volume\nrendering, suffer from rigid transfer function designs and high computational\ncosts. Although novel view synthesis approaches enhance rendering efficiency,\nthey require additional learning effort for non-experts and lack support for\nsemantic-level interaction. To bridge this gap, we propose NLI4VolVis, an\ninteractive system that enables users to explore, query, and edit volumetric\nscenes using natural language. NLI4VolVis integrates multi-view semantic\nsegmentation and vision-language models to extract and understand semantic\ncomponents in a scene. We introduce a multi-agent large language model\narchitecture equipped with extensive function-calling tools to interpret user\nintents and execute visualization tasks. The agents leverage external tools and\ndeclarative VolVis commands to interact with the VolVis engine powered by 3D\neditable Gaussians, enabling open-vocabulary object querying, real-time scene\nediting, best-view selection, and 2D stylization. We validate our system\nthrough case studies and a user study, highlighting its improved accessibility\nand usability in volumetric data exploration. We strongly recommend readers\ncheck our case studies, demo video, and source code at\nhttps://nli4volvis.github.io/.", "published": "2025-07-16 20:35:46", "link": "http://arxiv.org/abs/2507.12621v1", "categories": ["cs.HC", "cs.GR", "cs.MA"], "primary_category": "cs.HC"}
{"title": "A Unified Framework for Efficient Kernel and Polynomial Interpolation", "abstract": "We present a unified interpolation scheme that combines compactly-supported\npositive-definite kernels and multivariate polynomials. This unified framework\ngeneralizes interpolation with compactly-supported kernels and also classical\npolynomial least squares approximation. To facilitate the efficient use of this\nunified interpolation scheme, we present specialized numerical linear algebra\nprocedures that leverage standard matrix factorizations. These procedures allow\nfor efficient computation and storage of the unified interpolant. We also\npresent a modification to the numerical linear algebra that allows us to\ngeneralize the application of the unified framework to target functions on\nmanifolds with and without boundary. Our numerical experiments on both\nEuclidean domains and manifolds indicate that the unified interpolant is\nsuperior to polynomial least", "published": "2025-07-16 20:53:30", "link": "http://arxiv.org/abs/2507.12629v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Keep the beat going: Automatic drum transcription with momentum", "abstract": "A simple, interpretable way to perform automatic drum transcription is by\nfactoring the magnitude spectrogram of a recorded musical piece using a\npartially fixed nonnegative matrix factorization. There are two natural ways to\noptimize the nonnegative matrix factorization, including a multiplicative\nupdate rule and projected gradient descent with momentum. The methods differ in\ntheir empirical accuracies and theoretical convergence guarantees. This paper\nsummarizes the methods and their time complexities, and it applies the methods\nto the ENST-Drums data set and an original recording from the author's band,\nevaluating the empirical accuracy with respect to ground-truth drum\nannotations. The results indicate that projected gradient descent with momentum\nleads to higher accuracy for a fixed runtime, and it satisfies stronger\nconvergence guarantees.", "published": "2025-07-16 19:33:22", "link": "http://arxiv.org/abs/2507.12596v1", "categories": ["math.NA", "cs.NA", "cs.SD", "eess.AS"], "primary_category": "math.NA"}
{"title": "On the factorization of matrices into products of positive definite ones", "abstract": "The present work revisits and provides a new approach on a result by Charles\nBallantine, on the factorization of a square matrix with positive determinant\ninto a product of positive definite factors. {\\em Ballantine-type}\nfactorizations, that bound the number of positive definite factors, proved\ncentral in solving a basic, yet elusive control problem--the strong\ncontrollability of a linear system via control in the form of state feedback.\nBallantine's result transcends control engineering, and highlights the little\nappreciated fact that rotations can be realized by the successive application\nof irrotational motions. Our approach is constructive and is based on the\ntheory of optimal mass transport, specifically, it relates successive rotations\nof Gaussian distributions to corresponding optimal transport maps that\nconstitute the sought factors.", "published": "2025-07-16 18:19:14", "link": "http://arxiv.org/abs/2507.12560v1", "categories": ["math.OC", "cs.NA", "cs.SY", "eess.SY", "math.DS", "math.NA", "15A23, 15B48, 49Q22, 37N35, 37J25, 93C05, 93Axx, 90C52, 65F30, 68T30"], "primary_category": "math.OC"}
{"title": "Cross-World Assumption and Refining Prediction Intervals for Individual Treatment Effects", "abstract": "While average treatment effects (ATE) and conditional average treatment\neffects (CATE) provide valuable population- and subgroup-level summaries, they\nfail to capture uncertainty at the individual level. For high-stakes\ndecision-making, individual treatment effect (ITE) estimates must be\naccompanied by valid prediction intervals that reflect heterogeneity and\nunit-specific uncertainty. However, the fundamental unidentifiability of ITEs\nlimits the ability to derive precise and reliable individual-level uncertainty\nestimates. To address this challenge, we investigate the role of a cross-world\ncorrelation parameter, $ \\rho(x) = cor(Y(1), Y(0) | X = x) $, which describes\nthe dependence between potential outcomes, given covariates, in the\nNeyman-Rubin super-population model with i.i.d. units. Although $ \\rho $ is\nfundamentally unidentifiable, we argue that in most real-world applications, it\nis possible to impose reasonable and interpretable bounds informed by\ndomain-expert knowledge. Given $\\rho$, we design prediction intervals for ITE,\nachieving more stable and accurate coverage with substantially shorter widths;\noften less than 1/3 of those from competing methods. The resulting intervals\nsatisfy coverage guarantees $P\\big(Y(1) - Y(0) \\in C_{ITE}(X)\\big) \\geq 1 -\n\\alpha$ and are asymptotically optimal under Gaussian assumptions. We provide\nstrong theoretical and empirical arguments that cross-world assumptions can\nmake individual uncertainty quantification both practically informative and\nstatistically valid.", "published": "2025-07-16 18:58:18", "link": "http://arxiv.org/abs/2507.12581v1", "categories": ["stat.ME", "math.ST", "stat.ML", "stat.TH", "62A01", "G.3"], "primary_category": "stat.ME"}
{"title": "The Serial Scaling Hypothesis", "abstract": "While machine learning has advanced through massive parallelization, we\nidentify a critical blind spot: some problems are fundamentally sequential.\nThese \"inherently serial\" problems-from mathematical reasoning to physical\nsimulations to sequential decision-making-require dependent computational steps\nthat cannot be parallelized. Drawing from complexity theory, we formalize this\ndistinction and demonstrate that current parallel-centric architectures face\nfundamental limitations on such tasks. We argue that recognizing the serial\nnature of computation holds profound implications on machine learning, model\ndesign, hardware development. As AI tackles increasingly complex reasoning,\ndeliberately scaling serial computation-not just parallel computation-is\nessential for continued progress.", "published": "2025-07-16 18:01:26", "link": "http://arxiv.org/abs/2507.12549v1", "categories": ["cs.LG", "cs.CC", "stat.ML", "68Q15, 68Q10, 68T07", "F.1.1; F.1.3; I.2.6"], "primary_category": "cs.LG"}
{"title": "Complex non-backtracking matrix for directed graphs", "abstract": "Graph representation matrices are essential tools in graph data analysis.\nRecently, Hermitian adjacency matrices have been proposed to investigate\ndirected graph structures. Previous studies have demonstrated that these\nmatrices can extract valuable information for clustering. In this paper, we\npropose the complex non-backtracking matrix that integrates the properties of\nthe Hermitian adjacency matrix and the non-backtracking matrix. The proposed\nmatrix has similar properties with the non-backtracking matrix of undirected\ngraphs. We reveal relationships between the complex non-backtracking matrix and\nthe Hermitian adjacency matrix. Also, we provide intriguing insights that this\nmatrix representation holds cluster information, particularly for sparse\ndirected graphs.", "published": "2025-07-16 05:57:11", "link": "http://arxiv.org/abs/2507.12503v1", "categories": ["math.CO", "cs.LG", "stat.ML"], "primary_category": "math.CO"}
{"title": "Enhancing In-Domain and Out-Domain EmoFake Detection via Cooperative Multilingual Speech Foundation Models", "abstract": "In this work, we address EmoFake Detection (EFD). We hypothesize that\nmultilingual speech foundation models (SFMs) will be particularly effective for\nEFD due to their pre-training across diverse languages, enabling a nuanced\nunderstanding of variations in pitch, tone, and intensity. To validate this, we\nconduct a comprehensive comparative analysis of state-of-the-art (SOTA) SFMs.\nOur results shows the superiority of multilingual SFMs for same language\n(in-domain) as well as cross-lingual (out-domain) evaluation. To our end, we\nalso propose, THAMA for fusion of foundation models (FMs) motivated by related\nresearch where combining FMs have shown improved performance. THAMA leverages\nthe complementary conjunction of tucker decomposition and hadamard product for\neffective fusion. With THAMA, synergized with cooperative multilingual SFMs\nachieves topmost performance across in-domain and out-domain settings,\noutperforming individual FMs, baseline fusion techniques, and prior SOTA\nmethods.", "published": "2025-07-16 19:30:51", "link": "http://arxiv.org/abs/2507.12595v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Evaluation of Neural Surrogates for Physical Modelling Synthesis of Nonlinear Elastic Plates", "abstract": "Physical modelling synthesis aims to generate audio from physical simulations\nof vibrating structures. Thin elastic plates are a common model for drum\nmembranes. Traditional numerical methods like finite differences and finite\nelements offer high accuracy but are computationally demanding, limiting their\nuse in real-time audio applications. This paper presents a comparative analysis\nof neural network-based approaches for solving the vibration of nonlinear\nelastic plates. We evaluate several state-of-the-art models, trained on short\nsequences, for prediction of long sequences in an autoregressive fashion. We\nshow some of the limitations of these models, and why is not enough to look at\nthe prediction error in the time domain. We discuss the implications for\nreal-time audio synthesis and propose future directions for improving neural\napproaches to model nonlinear vibration.", "published": "2025-07-16 18:25:11", "link": "http://arxiv.org/abs/2507.12563v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "EME-TTS: Unlocking the Emphasis and Emotion Link in Speech Synthesis", "abstract": "In recent years, emotional Text-to-Speech (TTS) synthesis and\nemphasis-controllable speech synthesis have advanced significantly. However,\ntheir interaction remains underexplored. We propose Emphasis Meets Emotion TTS\n(EME-TTS), a novel framework designed to address two key research questions:\n(1) how to effectively utilize emphasis to enhance the expressiveness of\nemotional speech, and (2) how to maintain the perceptual clarity and stability\nof target emphasis across different emotions. EME-TTS employs weakly supervised\nlearning with emphasis pseudo-labels and variance-based emphasis features.\nAdditionally, the proposed Emphasis Perception Enhancement (EPE) block enhances\nthe interaction between emotional signals and emphasis positions. Experimental\nresults show that EME-TTS, when combined with large language models for\nemphasis position prediction, enables more natural emotional speech synthesis\nwhile preserving stable and distinguishable target emphasis across emotions.\nSynthesized samples are available on-line.", "published": "2025-07-16 08:19:20", "link": "http://arxiv.org/abs/2507.12015v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Mapping Emotions in the Brain: A Bi-Hemispheric Neural Model with Explainable Deep Learning", "abstract": "Recent advances have shown promise in emotion recognition from\nelectroencephalogram (EEG) signals by employing bi-hemispheric neural\narchitectures that incorporate neuroscientific priors into deep learning\nmodels. However, interpretability remains a significant limitation for their\napplication in sensitive fields such as affective computing and cognitive\nmodeling. In this work, we introduce a post-hoc interpretability framework\ntailored to dual-stream EEG classifiers, extending the Local Interpretable\nModel-Agnostic Explanations (LIME) approach to accommodate structured,\nbi-hemispheric inputs. Our method adapts LIME to handle structured two-branch\ninputs corresponding to left and right-hemisphere EEG channel groups. It\ndecomposes prediction relevance into per-channel contributions across\nhemispheres and emotional classes. We apply this framework to a previously\nvalidated dual-branch recurrent neural network trained on EmoNeuroDB, a dataset\nof EEG recordings captured during a VR-based emotion elicitation task. The\nresulting explanations reveal emotion-specific hemispheric activation patterns\nconsistent with known neurophysiological phenomena, such as frontal\nlateralization in joy and posterior asymmetry in sadness. Furthermore, we\naggregate local explanations across samples to derive global channel importance\nprofiles, enabling a neurophysiologically grounded interpretation of the\nmodel's decisions. Correlation analysis between symmetric electrodes further\nhighlights the model's emotion-dependent lateralization behavior, supporting\nthe functional asymmetries reported in affective neuroscience.", "published": "2025-07-16 20:39:58", "link": "http://arxiv.org/abs/2507.12625v1", "categories": ["q-bio.NC", "cs.HC", "eess.SP"], "primary_category": "q-bio.NC"}
{"title": "Emerging Paradigms in the Energy Sector: Forecasting and System Control Optimisation", "abstract": "The energy sector is experiencing rapid transformation due to increasing\nrenewable energy integration, decentralisation of power systems, and a\nheightened focus on efficiency and sustainability. With energy demand becoming\nincreasingly dynamic and generation sources more variable, advanced forecasting\nand optimisation strategies are crucial for maintaining grid stability,\ncost-effectiveness, and environmental sustainability. This paper explores\nemerging paradigms in energy forecasting and management, emphasizing four\ncritical domains: Energy Demand Forecasting integrated with Weather Data,\nBuilding Energy Optimisation, Heat Network Optimisation, and Energy Management\nSystem (EMS) Optimisation within a System of Systems (SoS) framework.\nLeveraging machine learning techniques and Model Predictive Control (MPC), the\nstudy demonstrates substantial enhancements in energy efficiency across scales\n-- from individual buildings to complex interconnected energy networks.\nWeather-informed demand forecasting significantly improves grid resilience and\nresource allocation strategies. Smart building optimisation integrates\npredictive analytics to substantially reduce energy consumption without\ncompromising occupant comfort. Optimising CHP-based heat networks achieves cost\nand carbon savings while adhering to operational and asset constraints. At the\nsystems level, sophisticated EMS optimisation ensures coordinated control of\ndistributed resources, storage solutions, and demand-side flexibility. Through\nreal-world case studies we highlight the potential of AI-driven automation and\nintegrated control solutions in facilitating a resilient, efficient, and\nsustainable energy future.", "published": "2025-07-16 16:21:07", "link": "http://arxiv.org/abs/2507.12373v1", "categories": ["cs.ET", "eess.SP"], "primary_category": "cs.ET"}
{"title": "Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models", "abstract": "When faced with novel situations, people are able to marshal relevant\nconsiderations from a wide range of background knowledge and put these to use\nin inferences and predictions. What permits us to draw in globally relevant\ninformation and reason over it coherently? Here, we explore the hypothesis that\npeople use a combination of distributed and symbolic representations to\nconstruct bespoke mental models tailored to novel situations. We propose a\ncomputational implementation of this idea -- a ``Model Synthesis Architecture''\n(MSA) -- using language models to implement global relevance-based retrieval\nand model synthesis and probabilistic programs to implement bespoke, coherent\nworld models. We evaluate our MSA as a model of human judgments on a novel\nreasoning dataset. The dataset -- built around a `Model Olympics` domain of\nsports vignettes -- tests models' capacity for human-like, open-ended reasoning\nby requiring (i) judgments about novel causal structures described in language;\n(ii) drawing on large bodies of background knowledge; and (iii) doing both in\nlight of observations that introduce arbitrary novel variables. Our MSA\napproach captures human judgments better than language model-only baselines,\nunder both direct and chain-of-thought generations from the LM that supports\nmodel synthesis. These results suggest that MSAs can be implemented in a way\nthat mirrors people's ability to deliver locally coherent reasoning over\nglobally relevant variables, offering a path to understanding and replicating\nhuman reasoning in open-ended domains.", "published": "2025-07-16 18:01:03", "link": "http://arxiv.org/abs/2507.12547v2", "categories": ["cs.CL", "cs.AI", "cs.PL"], "primary_category": "cs.CL"}
{"title": "ParaStudent: Generating and Evaluating Realistic Student Code by Teaching LLMs to Struggle", "abstract": "Large Language Models (LLMs) have shown strong performance on programming\ntasks, but can they generate student-like code like real students - imperfect,\niterative, and stylistically diverse? We present ParaStudent, a systematic\nstudy of LLM-based \"student-like\" code generation in an introductory\nprogramming course setting. Using a dataset of timestamped student submissions\nacross multiple semesters, we design low- and high-resolution experiments to\nmodel student progress and evaluate code outputs along semantic, functional,\nand stylistic dimensions. Our results show that fine-tuning significantly\nimproves alignment with real student trajectories and captures error patterns,\nincremental improvements, and stylistic variations more faithfully. This study\nshows that modeling realistic student code requires capturing learning dynamics\nthrough context-aware generation, temporal modeling, and multi-dimensional\nevaluation. Code for experiments and evaluation is available at\nhttps://github.com/mmiroyan/ParaStudent.", "published": "2025-07-16 23:12:14", "link": "http://arxiv.org/abs/2507.12674v2", "categories": ["cs.CY", "cs.AI", "cs.SE"], "primary_category": "cs.CY"}
{"title": "Differential Communication in Channels with Mobility and Delay Spread using Zak-OTFS", "abstract": "Zak-transform based orthogonal time frequency space (Zak-OTFS) is a\ndelay-Doppler (DD) domain modulation scheme in which the signal processing is\ncarried out in the DD domain. The channel when viewed in the DD domain is\npredictable. However, even with Zak-OTFS, pilots need to be sent periodically,\nalbeit at a lower rate. In this paper, we propose a differential communication\nscheme for Zak-OTFS systems that alleviates the need for periodic pilot\ntransmission. Towards this, we analytically show that the detected data can be\nused as a pilot and that the channel estimate obtained from the detected data\ncan enable further detection enabling the \"differential\" aspect of the\ncommunication. Specifically, we leverage the prediction capability of the DD\nchannel in Zak-OTFS to use the channel estimate (obtained from detected data\nsymbols treated as pilots) in the previous instant to detect data in the next\ninstant and propagate this forward. The advantages are two fold. First, it\nallows the data symbols to enjoy higher energy since the energy that would\notherwise be required for pilot symbols can also be allocated to data symbols.\nSecond, it allows for full spectral efficiency compared to point or embedded\npilots. Comparison with the full spectral efficiency achieving spread pilot\nscheme shows that the proposed method achieves better bit-error rate at lower\ncomplexity.", "published": "2025-07-16 19:22:42", "link": "http://arxiv.org/abs/2507.12593v2", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "Complex non-backtracking matrix for directed graphs", "abstract": "Graph representation matrices are essential tools in graph data analysis.\nRecently, Hermitian adjacency matrices have been proposed to investigate\ndirected graph structures. Previous studies have demonstrated that these\nmatrices can extract valuable information for clustering. In this paper, we\npropose the complex non-backtracking matrix that integrates the properties of\nthe Hermitian adjacency matrix and the non-backtracking matrix. The proposed\nmatrix has similar properties with the non-backtracking matrix of undirected\ngraphs. We reveal relationships between the complex non-backtracking matrix and\nthe Hermitian adjacency matrix. Also, we provide intriguing insights that this\nmatrix representation holds cluster information, particularly for sparse\ndirected graphs.", "published": "2025-07-16 05:57:11", "link": "http://arxiv.org/abs/2507.12503v2", "categories": ["math.CO", "cs.LG", "stat.ML"], "primary_category": "math.CO"}
{"title": "DyG-RAG: Dynamic Graph Retrieval-Augmented Generation with Event-Centric Reasoning", "abstract": "Graph Retrieval-Augmented Generation has emerged as a powerful paradigm for\ngrounding large language models with external structured knowledge. However,\nexisting Graph RAG methods struggle with temporal reasoning, due to their\ninability to model the evolving structure and order of real-world events. In\nthis work, we introduce DyG-RAG, a novel event-centric dynamic graph\nretrieval-augmented generation framework designed to capture and reason over\ntemporal knowledge embedded in unstructured text. To eliminate temporal\nambiguity in traditional retrieval units, DyG-RAG proposes Dynamic Event Units\n(DEUs) that explicitly encode both semantic content and precise temporal\nanchors, enabling accurate and interpretable time-aware retrieval. To capture\ntemporal and causal dependencies across events, DyG-RAG constructs an event\ngraph by linking DEUs that share entities and occur close in time, supporting\nefficient and meaningful multi-hop reasoning. To ensure temporally consistent\ngeneration, DyG-RAG introduces an event timeline retrieval pipeline that\nretrieves event sequences via time-aware traversal, and proposes a Time\nChain-of-Thought strategy for temporally grounded answer generation. This\nunified pipeline enables DyG-RAG to retrieve coherent, temporally ordered event\nsequences and to answer complex, time-sensitive queries that standard RAG\nsystems cannot resolve. Extensive experiments on temporal QA benchmarks\ndemonstrate that DyG-RAG significantly improves the accuracy and recall of\nthree typical types of temporal reasoning questions, paving the way for more\nfaithful and temporal-aware generation. DyG-RAG is available at\nhttps://github.com/RingBDStack/DyG-RAG.", "published": "2025-07-16 10:22:35", "link": "http://arxiv.org/abs/2507.13396v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "SIEVE: Effective Filtered Vector Search with Collection of Indexes", "abstract": "Many real-world tasks such as recommending videos with the kids tag can be\nreduced to finding most similar vectors associated with hard predicates. This\ntask, filtered vector search, is challenging as prior state-of-the-art\ngraph-based (unfiltered) similarity search techniques quickly degenerate when\nhard constraints are considered. That is, effective graph-based filtered\nsimilarity search relies on sufficient connectivity for reaching the most\nsimilar items within just a few hops. To consider predicates, recent works\npropose modifying graph traversal to visit only the items that may satisfy\npredicates. However, they fail to offer the just-a-few-hops property for a wide\nrange of predicates: they must restrict predicates significantly or lose\nefficiency if only a small fraction of items satisfy predicates.\n  We propose an opposite approach: instead of constraining traversal, we build\nmany indexes each serving different predicate forms. For effective\nconstruction, we devise a three-dimensional analytical model capturing\nrelationships among index size, search time, and recall, with which we follow a\nworkload-aware approach to pack as many useful indexes as possible into a\ncollection. At query time, the analytical model is employed yet again to\ndiscern the one that offers the fastest search at a given recall. We show\nsuperior performance and support on datasets with varying selectivities and\nforms: our approach achieves up to 8.06x speedup while having as low as 1%\nbuild time versus other indexes, with less than 2.15x memory of a standard HNSW\ngraph and modest knowledge of past workloads.", "published": "2025-07-16 04:46:28", "link": "http://arxiv.org/abs/2507.11907v2", "categories": ["cs.DB", "cs.IR"], "primary_category": "cs.DB"}
{"title": "A Unified Framework for Efficient Kernel and Polynomial Interpolation", "abstract": "We present a unified interpolation scheme that combines compactly-supported\npositive-definite kernels and multivariate polynomials. This unified framework\ngeneralizes interpolation with compactly-supported kernels and also classical\npolynomial least squares approximation. To facilitate the efficient use of this\nunified interpolation scheme, we present specialized numerical linear algebra\nprocedures that leverage standard matrix factorizations. These procedures allow\nfor efficient computation and storage of the unified interpolant. We also\npresent a modification to the numerical linear algebra that allows us to\ngeneralize the application of the unified framework to target functions on\nmanifolds with and without boundary. Our numerical experiments on both\nEuclidean domains and manifolds indicate that the unified interpolant is\nsuperior to polynomial least squares for the interpolation of target functions\nin settings with boundaries.", "published": "2025-07-16 20:53:30", "link": "http://arxiv.org/abs/2507.12629v2", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Refinement of the theory and convergence of the Sinc convolution", "abstract": "The Sinc convolution is an approximate formula for indefinite convolutions\nproposed by Stenger. The formula was derived based on the Sinc indefinite\nintegration formula combined with the single-exponential transformation.\nAlthough its efficiency has been confirmed in various fields, several\ntheoretical issues remain unresolved. The first contribution of this study is\nto resolve those issues by refining the underlying theory of the Sinc\nconvolution. This contribution includes an essential resolution of Stenger's\nconjecture. The second contribution of this study is to improve the convergence\nrate by replacing the single-exponential transformation with the\ndouble-exponential transformation. Theoretical analysis and numerical\nexperiments confirm that the modified formula achieves superior convergence\ncompared to Stenger's original formula.", "published": "2025-07-16 16:52:02", "link": "http://arxiv.org/abs/2507.12406v2", "categories": ["math.NA", "cs.NA", "65D15, 65D30"], "primary_category": "math.NA"}
{"title": "Achieving Robust Channel Estimation Neural Networks by Designed Training Data", "abstract": "Channel estimation is crucial in wireless communications. However, in many\npapers neural networks are frequently tested by training and testing on one\nexample channel or similar channels. This is because data-driven methods often\ndegrade on new data which they are not trained on, as they cannot extrapolate\ntheir training knowledge. This is despite the fact physical channels are often\nassumed to be time-variant. However, due to the low latency requirements and\nlimited computing resources, neural networks may not have enough time and\ncomputing resources to execute online training to fine-tune the parameters.\nThis motivates us to design offline-trained neural networks that can perform\nrobustly over wireless channels, but without any actual channel information\nbeing known at design time. In this paper, we propose design criteria to\ngenerate synthetic training datasets for neural networks, which guarantee that\nafter training the resulting networks achieve a certain mean squared error\n(MSE) on new and previously unseen channels. Therefore, trained neural networks\nrequire no prior channel information or parameters update for real-world\nimplementations. Based on the proposed design criteria, we further propose a\nbenchmark design which ensures intelligent operation for different channel\nprofiles. To demonstrate general applicability, we use neural networks with\ndifferent levels of complexity to show that the generalization achieved appears\nto be independent of neural network architecture. From simulations, neural\nnetworks achieve robust generalization to wireless channels with both fixed\nchannel profiles and variable delay spreads.", "published": "2025-07-16 21:04:37", "link": "http://arxiv.org/abs/2507.12630v2", "categories": ["eess.SP", "cs.AI"], "primary_category": "eess.SP"}
{"title": "Developing an AI-Guided Assistant Device for the Deaf and Hearing Impaired", "abstract": "This study aims to develop a deep learning system for an accessibility device\nfor the deaf or hearing impaired. The device will accurately localize and\nidentify sound sources in real time. This study will fill an important gap in\ncurrent research by leveraging machine learning techniques to target the\nunderprivileged community. The system includes three main components. 1.\nJerryNet: A custom designed CNN architecture that determines the direction of\narrival (DoA) for nine possible directions. 2. Audio Classification: This model\nis based on fine-tuning the Contrastive Language-Audio Pretraining (CLAP) model\nto identify the exact sound classes only based on audio. 3. Multimodal\nintegration model: This is an accurate sound localization model that combines\naudio, visual, and text data to locate the exact sound sources in the images.\nThe part consists of two modules, one object detection using Yolov9 to generate\nall the bounding boxes of the objects, and an audio visual localization model\nto identify the optimal bounding box using complete Intersection over Union\n(CIoU). The hardware consists of a four-microphone rectangular formation and a\ncamera mounted on glasses with a wristband for displaying necessary information\nlike direction. On a custom collected data set, JerryNet achieved a precision\nof 91. 1% for the sound direction, outperforming all the baseline models. The\nCLAP model achieved 98.5% and 95% accuracy on custom and AudioSet datasets,\nrespectively. The audio-visual localization model within component 3 yielded a\ncIoU of 0.892 and an AUC of 0.658, surpassing other similar models. There are\nmany future potentials to this study, paving the way to creating a new\ngeneration of accessibility devices.", "published": "2025-07-16 05:03:33", "link": "http://arxiv.org/abs/2507.14215v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Diffusion-based translation between unpaired spontaneous premature neonatal EEG and fetal MEG", "abstract": "Background and objective: Brain activity in premature newborns has\ntraditionally been studied using electroencephalography (EEG), leading to\nsubstantial advances in our understanding of early neural development. However,\nsince brain development takes root at the fetal stage, a critical window of\nthis process remains largely unknown. The only technique capable of recording\nneural activity in the intrauterine environment is fetal magnetoencephalography\n(fMEG), but this approach presents challenges in terms of data quality and\nscarcity. Using artificial intelligence, the present research aims to transfer\nthe well-established knowledge from EEG studies to fMEG to improve\nunderstanding of prenatal brain development, laying the foundations for better\ndetection and treatment of potential pathologies. Methods: We developed an\nunpaired diffusion translation method based on dual diffusion bridges, which\nnotably includes numerical integration improvements to obtain more qualitative\nresults at a lower computational cost. Models were trained on our unpaired\ndataset of bursts of spontaneous activity from 30 high-resolution premature\nnewborns EEG recordings and 44 fMEG recordings. Results: We demonstrate that\nour method achieves significant improvement upon previous results obtained with\nGenerative Adversarial Networks (GANs), by almost 5% on the mean squared error\nin the time domain, and completely eliminating the mode collapse problem in the\nfrequency domain, thus achieving near-perfect signal fidelity. Conclusion: We\nset a new state of the art in the EEG-fMEG unpaired translation problem, as our\ndeveloped tool completely paves the way for early brain activity analysis.\nOverall, we also believe that our method could be reused for other unpaired\nsignal translation applications.", "published": "2025-07-16 15:50:07", "link": "http://arxiv.org/abs/2507.14224v1", "categories": ["eess.SP", "cs.LG"], "primary_category": "eess.SP"}
{"title": "Advanced Space Mapping Technique Integrating a Shared Coarse Model for Multistate Tuning-Driven Multiphysics Optimization of Tunable Filters", "abstract": "This article introduces an advanced space mapping (SM) technique that applies\na shared electromagnetic (EM)-based coarse model for multistate tuning-driven\nmultiphysics optimization of tunable filters. The SM method combines the\ncomputational efficiency of EM single-physics simulations with the precision of\nmultiphysics simulations. The shared coarse model is based on EM single-physics\nresponses corresponding to various nontunable design parameters values.\nConversely, the fine model is implemented to delineate the behavior of\nmultiphysics responses concerning both nontunable and tunable design parameter\nvalues. The proposed overall surrogate model comprises multiple subsurrogate\nmodels, each consisting of one shared coarse model and two distinct mapping\nneural networks. The responses from the shared coarse model in the EM\nsingle-physics filed offer a suitable approximation for the fine responses in\nthe multiphysics filed, whereas the mapping neural networks facilitate\ntransition from the EM single-physics field to the multiphysics field. Each\nsubsurrogate model maintains consistent nontunable design parameter values but\npossesses unique tunable design parameter values. By developing multiple\nsubsurrogate models, optimization can be simultaneously performed for each\ntuning state. Nontunable design parameter values are constrained by all tuning\nstates, whereas tunable design parameter values are confined to their\nrespective tuning states. This optimization technique simultaneously accounts\nfor all the tuning states to fulfill the necessary multiple tuning state\nrequirements. Multiple EM and multiphysics training samples are generated\nconcurrently to develop the surrogate model. Compared with existing direct\nmultiphysics parameterized modeling techniques, our proposed method achieves\nsuperior multiphysics modeling accuracy with fewer training samples and reduced\ncomputational costs.", "published": "2025-07-16 11:47:35", "link": "http://arxiv.org/abs/2507.14220v1", "categories": ["eess.SP", "cs.LG", "physics.acc-ph"], "primary_category": "eess.SP"}
{"title": "Distributed Machine Learning Approach for Low-Latency Localization in Cell-Free Massive MIMO Systems", "abstract": "Low-latency localization is critical in cellular networks to support\nreal-time applications requiring precise positioning. In this paper, we propose\na distributed machine learning (ML) framework for fingerprint-based\nlocalization tailored to cell-free massive multiple-input multiple-output\n(MIMO) systems, an emerging architecture for 6G networks. The proposed\nframework enables each access point (AP) to independently train a Gaussian\nprocess regression model using local angle-of-arrival and received signal\nstrength fingerprints. These models provide probabilistic position estimates\nfor the user equipment (UE), which are then fused by the UE with minimal\ncomputational overhead to derive a final location estimate. This decentralized\napproach eliminates the need for fronthaul communication between the APs and\nthe central processing unit (CPU), thereby reducing latency. Additionally,\ndistributing computational tasks across the APs alleviates the processing\nburden on the CPU compared to traditional centralized localization schemes.\nSimulation results demonstrate that the proposed distributed framework achieves\nlocalization accuracy comparable to centralized methods, despite lacking the\nbenefits of centralized data aggregation. Moreover, it effectively reduces\nuncertainty of the location estimates, as evidenced by the 95\\% covariance\nellipse. The results highlight the potential of distributed ML for enabling\nlow-latency, high-accuracy localization in future 6G networks.", "published": "2025-07-16 06:05:16", "link": "http://arxiv.org/abs/2507.14216v1", "categories": ["eess.SP", "cs.LG"], "primary_category": "eess.SP"}
{"title": "From Black Box to Biomarker: Sparse Autoencoders for Interpreting Speech Models of Parkinson's Disease", "abstract": "Speech holds promise as a cost-effective and non-invasive biomarker for\nneurological conditions such as Parkinson's disease (PD). While deep learning\nsystems trained on raw audio can find subtle signals not available from\nhand-crafted features, their black-box nature hinders clinical adoption. To\naddress this, we apply sparse autoencoders (SAEs) to uncover interpretable\ninternal representations from a speech-based PD detection system. We introduce\na novel mask-based activation for adapting SAEs to small biomedical datasets,\ncreating sparse disentangled dictionary representations. These dictionary\nentries are found to have strong associations with characteristic articulatory\ndeficits in PD speech, such as reduced spectral flux and increased spectral\nflatness in the low-energy regions highlighted by the model attention. We\nfurther show that the spectral flux is related to volumetric measurements of\nthe putamen from MRI scans, demonstrating the potential of SAEs to reveal\nclinically relevant biomarkers for disease monitoring and diagnosis.", "published": "2025-07-16 16:22:02", "link": "http://arxiv.org/abs/2507.16836v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
