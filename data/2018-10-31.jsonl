{"title": "GraphIE: A Graph-Based Framework for Information Extraction", "abstract": "Most modern Information Extraction (IE) systems are implemented as sequential\ntaggers and only model local dependencies. Non-local and non-sequential context\nis, however, a valuable source of information to improve predictions. In this\npaper, we introduce GraphIE, a framework that operates over a graph\nrepresenting a broad set of dependencies between textual units (i.e. words or\nsentences). The algorithm propagates information between connected nodes\nthrough graph convolutions, generating a richer representation that can be\nexploited to improve word-level predictions. Evaluation on three different\ntasks --- namely textual, social media and visual information extraction ---\nshows that GraphIE consistently outperforms the state-of-the-art sequence\ntagging model by a significant margin.", "published": "2018-10-31 02:52:21", "link": "http://arxiv.org/abs/1810.13083v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attentive Neural Network for Named Entity Recognition in Vietnamese", "abstract": "We propose an attentive neural network for the task of named entity\nrecognition in Vietnamese. The proposed attentive neural model makes use of\ncharacter-based language models and word embeddings to encode words as vector\nrepresentations. A neural network architecture of encoder, attention, and\ndecoder layers is then utilized to encode knowledge of input sentences and to\nlabel entity tags. The experimental results show that the proposed attentive\nneural network achieves the state-of-the-art results on the benchmark named\nentity recognition datasets in Vietnamese in comparison to both hand-crafted\nfeatures based models and neural models.", "published": "2018-10-31 04:05:05", "link": "http://arxiv.org/abs/1810.13097v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Giving Space to Your Message: Assistive Word Segmentation for the\n  Electronic Typing of Digital Minorities", "abstract": "For readability and disambiguation of the written text, appropriate word\nsegmentation is recommended for documentation, and it also holds for the\ndigitized texts. If the language is agglutinative while far from scriptio\ncontinua, for instance in the Korean language, the problem becomes more\nsignificant. However, some device users these days find it challenging to\ncommunicate via key stroking, not only for handicap but also for being\nunskilled. In this study, we propose a real-time assistive technology that\nutilizes an automatic word segmentation, designed for digital minorities who\nare not familiar with electronic typing. We propose a data-driven system\ntrained upon a spoken Korean language corpus with various non-canonical\nexpressions and dialects, guaranteeing the comprehension of contextual\ninformation. Through quantitative and qualitative comparison with other text\nprocessing toolkits, we show the reliability of the proposed system and its fit\nwith colloquial and non-normalized texts, which fulfills the aim of supportive\ntechnology.", "published": "2018-10-31 05:21:17", "link": "http://arxiv.org/abs/1810.13113v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WikiConv: A Corpus of the Complete Conversational History of a Large\n  Online Collaborative Community", "abstract": "We present a corpus that encompasses the complete history of conversations\nbetween contributors to Wikipedia, one of the largest online collaborative\ncommunities. By recording the intermediate states of conversations---including\nnot only comments and replies, but also their modifications, deletions and\nrestorations---this data offers an unprecedented view of online conversation.\nThis level of detail supports new research questions pertaining to the process\n(and challenges) of large-scale online collaboration. We illustrate the corpus'\npotential with two case studies that highlight new perspectives on earlier\nwork. First, we explore how a person's conversational behavior depends on how\nthey relate to the discussion's venue. Second, we show that community\nmoderation of toxic behavior happens at a higher rate than previously\nestimated. Finally the reconstruction framework is designed to be language\nagnostic, and we show that it can extract high quality conversational data in\nboth Chinese and English.", "published": "2018-10-31 09:46:11", "link": "http://arxiv.org/abs/1810.13181v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SURFACE: Semantically Rich Fact Validation with Explanations", "abstract": "Judging the veracity of a sentence making one or more claims is an important\nand challenging problem with many dimensions. The recent FEVER task asked\nparticipants to classify input sentences as either SUPPORTED, REFUTED or\nNotEnoughInfo using Wikipedia as a source of true facts. SURFACE does this task\nand explains its decision through a selection of sentences from the trusted\nsource. Our multi-task neural approach uses semantic lexical frames from\nFrameNet to jointly (i) find relevant evidential sentences in the trusted\nsource and (ii) use them to classify the input sentence's veracity. An\nevaluation of our efficient three-parameter model on the FEVER dataset showed\nan improvement of 90% over the state-of-the-art baseline on retrieving relevant\nsentences and a 70% relative improvement in classification.", "published": "2018-10-31 11:38:40", "link": "http://arxiv.org/abs/1810.13223v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Convolutional Self-Attention Network", "abstract": "Self-attention network (SAN) has recently attracted increasing interest due\nto its fully parallelized computation and flexibility in modeling dependencies.\nIt can be further enhanced with multi-headed attention mechanism by allowing\nthe model to jointly attend to information from different representation\nsubspaces at different positions (Vaswani et al., 2017). In this work, we\npropose a novel convolutional self-attention network (CSAN), which offers SAN\nthe abilities to 1) capture neighboring dependencies, and 2) model the\ninteraction between multiple attention heads. Experimental results on WMT14\nEnglish-to-German translation task demonstrate that the proposed approach\noutperforms both the strong Transformer baseline and other existing works on\nenhancing the locality of SAN. Comparing with previous work, our model does not\nintroduce any new parameters.", "published": "2018-10-31 14:58:30", "link": "http://arxiv.org/abs/1810.13320v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Transfer Learning for Multilingual Task Oriented Dialog", "abstract": "One of the first steps in the utterance interpretation pipeline of many\ntask-oriented conversational AI systems is to identify user intents and the\ncorresponding slots. Since data collection for machine learning models for this\ntask is time-consuming, it is desirable to make use of existing data in a\nhigh-resource language to train models in low-resource languages. However,\ndevelopment of such models has largely been hindered by the lack of\nmultilingual training data. In this paper, we present a new data set of 57k\nannotated utterances in English (43k), Spanish (8.6k) and Thai (5k) across the\ndomains weather, alarm, and reminder. We use this data set to evaluate three\ndifferent cross-lingual transfer methods: (1) translating the training data,\n(2) using cross-lingual pre-trained embeddings, and (3) a novel method of using\na multilingual machine translation encoder as contextual word representations.\nWe find that given several hundred training examples in the the target\nlanguage, the latter two methods outperform translating the training data.\nFurther, in very low-resource settings, multilingual contextual word\nrepresentations give better results than using cross-lingual static embeddings.\nWe also compare the cross-lingual methods to using monolingual resources in the\nform of contextual ELMo representations and find that given just small amounts\nof target language data, this method outperforms all cross-lingual methods,\nwhich highlights the need for more sophisticated cross-lingual methods.", "published": "2018-10-31 15:15:00", "link": "http://arxiv.org/abs/1810.13327v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Picking Apart Story Salads", "abstract": "During natural disasters and conflicts, information about what happened is\noften confusing, messy, and distributed across many sources. We would like to\nbe able to automatically identify relevant information and assemble it into\ncoherent narratives of what happened. To make this task accessible to neural\nmodels, we introduce Story Salads, mixtures of multiple documents that can be\ngenerated at scale. By exploiting the Wikipedia hierarchy, we can generate\nsalads that exhibit challenging inference problems. Story salads give rise to a\nnovel, challenging clustering task, where the objective is to group sentences\nfrom the same narratives. We demonstrate that simple bag-of-words similarity\nclustering falls short on this task and that it is necessary to take into\naccount global context and coherence.", "published": "2018-10-31 16:38:55", "link": "http://arxiv.org/abs/1810.13391v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "You May Not Need Attention", "abstract": "In NMT, how far can we get without attention and without separate encoding\nand decoding? To answer that question, we introduce a recurrent neural\ntranslation model that does not use attention and does not have a separate\nencoder and decoder. Our eager translation model is low-latency, writing target\ntokens as soon as it reads the first source token, and uses constant memory\nduring decoding. It performs on par with the standard attention-based model of\nBahdanau et al. (2014), and better on long sentences.", "published": "2018-10-31 17:09:37", "link": "http://arxiv.org/abs/1810.13409v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extracting Linguistic Resources from the Web for Concept-to-Text\n  Generation", "abstract": "Many concept-to-text generation systems require domain-specific linguistic\nresources to produce high quality texts, but manually constructing these\nresources can be tedious and costly. Focusing on NaturalOWL, a publicly\navailable state of the art natural language generator for OWL ontologies, we\npropose methods to extract from the Web sentence plans and natural language\nnames, two of the most important types of domain-specific linguistic resources\nused by the generator. Experiments show that texts generated using linguistic\nresources extracted by our methods in a semi-automatic manner, with minimal\nhuman involvement, are perceived as being almost as good as texts generated\nusing manually authored linguistic resources, and much better than texts\nproduced by using linguistic resources extracted from the relation and entity\nidentifiers of the ontology.", "published": "2018-10-31 17:23:05", "link": "http://arxiv.org/abs/1810.13414v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Machine Reading Comprehension with General Reading Strategies", "abstract": "Reading strategies have been shown to improve comprehension levels,\nespecially for readers lacking adequate prior knowledge. Just as the process of\nknowledge accumulation is time-consuming for human readers, it is\nresource-demanding to impart rich general domain knowledge into a deep language\nmodel via pre-training. Inspired by reading strategies identified in cognitive\nscience, and given limited computational resources -- just a pre-trained model\nand a fixed number of training instances -- we propose three general strategies\naimed to improve non-extractive machine reading comprehension (MRC): (i) BACK\nAND FORTH READING that considers both the original and reverse order of an\ninput sequence, (ii) HIGHLIGHTING, which adds a trainable embedding to the text\nembedding of tokens that are relevant to the question and candidate answers,\nand (iii) SELF-ASSESSMENT that generates practice questions and candidate\nanswers directly from the text in an unsupervised manner.\n  By fine-tuning a pre-trained language model (Radford et al., 2018) with our\nproposed strategies on the largest general domain multiple-choice MRC dataset\nRACE, we obtain a 5.8% absolute increase in accuracy over the previous best\nresult achieved by the same pre-trained model fine-tuned on RACE without the\nuse of strategies. We further fine-tune the resulting model on a target MRC\ntask, leading to an absolute improvement of 6.2% in average accuracy over\nprevious state-of-the-art approaches on six representative non-extractive MRC\ndatasets from different domains (i.e., ARC, OpenBookQA, MCTest, SemEval-2018\nTask 11, ROCStories, and MultiRC). These results demonstrate the effectiveness\nof our proposed strategies and the versatility and general applicability of our\nfine-tuned models that incorporate these strategies. Core code is available at\nhttps://github.com/nlpdata/strategy/.", "published": "2018-10-31 17:54:37", "link": "http://arxiv.org/abs/1810.13441v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Texts with Integer Linear Programming", "abstract": "Concept-to-text generation typically employs a pipeline architecture, which\noften leads to suboptimal texts. Content selection, for example, may greedily\nselect the most important facts, which may require, however, too many words to\nexpress, and this may be undesirable when space is limited or expensive.\nSelecting other facts, possibly only slightly less important, may allow the\nlexicalization stage to use much fewer words, or to report more facts in the\nsame space. Decisions made during content selection and lexicalization may also\nlead to more or fewer sentence aggregation opportunities, affecting the length\nand readability of the resulting texts. Building upon on a publicly available\nstate of the art natural language generator for Semantic Web ontologies, this\narticle presents an Integer Linear Programming model that, unlike pipeline\narchitectures, jointly considers choices available in content selection,\nlexicalization, and sentence aggregation to avoid greedy local decisions and\nproduce more compact texts, i.e., texts that report more facts per word.\nCompact texts are desirable, for example, when generating advertisements to be\nincluded in Web search results, or when summarizing structured information in\nlimited space. An extended version of the proposed model also considers a\nlimited form of referring expression generation and avoids redundant sentences.\nAn approximation of the two models can be used when longer texts need to be\ngenerated. Experiments with three ontologies confirm that the proposed models\nlead to more compact texts, compared to pipeline systems, with no deterioration\nor with improvements in the perceived quality of the generated texts.", "published": "2018-10-31 18:24:32", "link": "http://arxiv.org/abs/1811.00051v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aligning Very Small Parallel Corpora Using Cross-Lingual Word Embeddings\n  and a Monogamy Objective", "abstract": "Count-based word alignment methods, such as the IBM models or fast-align,\nstruggle on very small parallel corpora. We therefore present an alternative\napproach based on cross-lingual word embeddings (CLWEs), which are trained on\npurely monolingual data. Our main contribution is an unsupervised objective to\nadapt CLWEs to parallel corpora. In experiments on between 25 and 500\nsentences, our method outperforms fast-align. We also show that our fine-tuning\nobjective consistently improves a CLWE-only baseline.", "published": "2018-10-31 18:58:22", "link": "http://arxiv.org/abs/1811.00066v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effective Feature Representation for Clinical Text Concept Extraction", "abstract": "Crucial information about the practice of healthcare is recorded only in\nfree-form text, which creates an enormous opportunity for high-impact NLP.\nHowever, annotated healthcare datasets tend to be small and expensive to\nobtain, which raises the question of how to make maximally efficient uses of\nthe available data. To this end, we develop an LSTM-CRF model for combining\nunsupervised word representations and hand-built feature representations\nderived from publicly available healthcare ontologies. We show that this\ncombined model yields superior performance on five datasets of diverse kinds of\nhealthcare text (clinical, social, scientific, commercial). Each involves the\nlabeling of complex, multi-word spans that pick out different healthcare\nconcepts. We also introduce a new labeled dataset for identifying the treatment\nrelations between drugs and diseases.", "published": "2018-10-31 19:06:50", "link": "http://arxiv.org/abs/1811.00070v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A task in a suit and a tie: paraphrase generation with semantic\n  augmentation", "abstract": "Paraphrasing is rooted in semantics. We show the effectiveness of\ntransformers (Vaswani et al. 2017) for paraphrase generation and further\nimprovements by incorporating PropBank labels via a multi-encoder. Evaluating\non MSCOCO and WikiAnswers, we find that transformers are fast and effective,\nand that semantic augmentation for both transformers and LSTMs leads to sizable\n2-3 point gains in BLEU, METEOR and TER. More importantly, we find surprisingly\nlarge gains on human evaluations compared to previous models. Nevertheless,\nmanual inspection of generated paraphrases reveals ample room for improvement:\neven our best model produces human-acceptable paraphrases for only 28% of\ncaptions from the CHIA dataset (Sharma et al. 2018), and it fails spectacularly\non sentences from Wikipedia. Overall, these results point to the potential for\nincorporating semantics in the task while highlighting the need for stronger\nevaluation.", "published": "2018-10-31 21:04:04", "link": "http://arxiv.org/abs/1811.00119v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring Issue Ownership using Word Embeddings", "abstract": "Sentiment and topic analysis are common methods used for social media\nmonitoring. Essentially, these methods answers questions such as, \"what is\nbeing talked about, regarding X\", and \"what do people feel, regarding X\". In\nthis paper, we investigate another venue for social media monitoring, namely\nissue ownership and agenda setting, which are concepts from political science\nthat have been used to explain voter choice and electoral outcomes. We argue\nthat issue alignment and agenda setting can be seen as a kind of semantic\nsource similarity of the kind \"how similar is source A to issue owner P, when\ntalking about issue X\", and as such can be measured using word/document\nembedding techniques. We present work in progress towards measuring that kind\nof conditioned similarity, and introduce a new notion of similarity for\npredictive embeddings. We then test this method by measuring the similarity\nbetween politically aligned media and political parties, conditioned on\nbloc-specific issues.", "published": "2018-10-31 21:31:08", "link": "http://arxiv.org/abs/1811.00127v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning", "abstract": "We present ATOMIC, an atlas of everyday commonsense reasoning, organized\nthrough 877k textual descriptions of inferential knowledge. Compared to\nexisting resources that center around taxonomic knowledge, ATOMIC focuses on\ninferential knowledge organized as typed if-then relations with variables\n(e.g., \"if X pays Y a compliment, then Y will likely return the compliment\").\nWe propose nine if-then relation types to distinguish causes vs. effects,\nagents vs. themes, voluntary vs. involuntary events, and actions vs. mental\nstates. By generatively training on the rich inferential knowledge described in\nATOMIC, we show that neural models can acquire simple commonsense capabilities\nand reason about previously unseen events. Experimental results demonstrate\nthat multitask models that incorporate the hierarchical structure of if-then\nrelation types lead to more accurate inference compared to models trained in\nisolation, as measured by both automatic and human evaluation.", "published": "2018-10-31 22:57:51", "link": "http://arxiv.org/abs/1811.00146v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards End-to-End Code-Switching Speech Recognition", "abstract": "Code-switching speech recognition has attracted an increasing interest\nrecently, but the need for expert linguistic knowledge has always been a big\nissue. End-to-end automatic speech recognition (ASR) simplifies the building of\nASR systems considerably by predicting graphemes or characters directly from\nacoustic input. In the mean time, the need of expert linguistic knowledge is\nalso eliminated, which makes it an attractive choice for code-switching ASR.\nThis paper presents a hybrid CTC-Attention based end-to-end Mandarin-English\ncode-switching (CS) speech recognition system and studies the effect of hybrid\nCTC-Attention based models, different modeling units, the inclusion of language\nidentification and different decoding strategies on the task of code-switching\nASR. On the SEAME corpus, our system achieves a mixed error rate (MER) of\n34.24%.", "published": "2018-10-31 03:44:21", "link": "http://arxiv.org/abs/1810.13091v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Attentive Filtering Networks for Audio Replay Attack Detection", "abstract": "An attacker may use a variety of techniques to fool an automatic speaker\nverification system into accepting them as a genuine user. Anti-spoofing\nmethods meanwhile aim to make the system robust against such attacks. The\nASVspoof 2017 Challenge focused specifically on replay attacks, with the\nintention of measuring the limits of replay attack detection as well as\ndeveloping countermeasures against them. In this work, we propose our replay\nattacks detection system - Attentive Filtering Network, which is composed of an\nattention-based filtering mechanism that enhances feature representations in\nboth the frequency and time domains, and a ResNet-based classifier. We show\nthat the network enables us to visualize the automatically acquired feature\nrepresentations that are helpful for spoofing detection. Attentive Filtering\nNetwork attains an evaluation EER of 8.99$\\%$ on the ASVspoof 2017 Version 2.0\ndataset. With system fusion, our best system further obtains a 30$\\%$ relative\nimprovement over the ASVspoof 2017 enhanced baseline system.", "published": "2018-10-31 00:23:16", "link": "http://arxiv.org/abs/1810.13048v1", "categories": ["eess.AS", "cs.CL", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Attention-based sequence-to-sequence model for speech recognition:\n  development of state-of-the-art system on LibriSpeech and its application to\n  non-native English", "abstract": "Recent research has shown that attention-based sequence-to-sequence models\nsuch as Listen, Attend, and Spell (LAS) yield comparable results to\nstate-of-the-art ASR systems on various tasks. In this paper, we describe the\ndevelopment of such a system and demonstrate its performance on two tasks:\nfirst we achieve a new state-of-the-art word error rate of 3.43% on the test\nclean subset of LibriSpeech English data; second on non-native English speech,\nincluding both read speech and spontaneous speech, we obtain very competitive\nresults compared to a conventional system built with the most updated Kaldi\nrecipe.", "published": "2018-10-31 03:10:37", "link": "http://arxiv.org/abs/1810.13088v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "End-to-End Feedback Loss in Speech Chain Framework via Straight-Through\n  Estimator", "abstract": "The speech chain mechanism integrates automatic speech recognition (ASR) and\ntext-to-speech synthesis (TTS) modules into a single cycle during training. In\nour previous work, we applied a speech chain mechanism as a semi-supervised\nlearning. It provides the ability for ASR and TTS to assist each other when\nthey receive unpaired data and let them infer the missing pair and optimize the\nmodel with reconstruction loss. If we only have speech without transcription,\nASR generates the most likely transcription from the speech data, and then TTS\nuses the generated transcription to reconstruct the original speech features.\nHowever, in previous papers, we just limited our back-propagation to the\nclosest module, which is the TTS part. One reason is that back-propagating the\nerror through the ASR is challenging due to the output of the ASR are discrete\ntokens, creating non-differentiability between the TTS and ASR. In this paper,\nwe address this problem and describe how to thoroughly train a speech chain\nend-to-end for reconstruction loss using a straight-through estimator (ST).\nExperimental results revealed that, with sampling from ST-Gumbel-Softmax, we\nwere able to update ASR parameters and improve the ASR performances by 11\\%\nrelative CER reduction compared to the baseline.", "published": "2018-10-31 05:05:37", "link": "http://arxiv.org/abs/1810.13107v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "On The Inductive Bias of Words in Acoustics-to-Word Models", "abstract": "Acoustics-to-word models are end-to-end speech recognizers that use words as\ntargets without relying on pronunciation dictionaries or graphemes. These\nmodels are notoriously difficult to train due to the lack of linguistic\nknowledge. It is also unclear how the amount of training data impacts the\noptimization and generalization of such models. In this work, we study the\noptimization and generalization of acoustics-to-word models under different\namounts of training data. In addition, we study three types of inductive bias,\nleveraging a pronunciation dictionary, word boundary annotations, and\nconstraints on word durations. We find that constraining word durations leads\nto the most improvement. Finally, we analyze the word embedding space learned\nby the model, and find that the space has a structure dominated by the\npronunciation of words. This suggests that the contexts of words, instead of\ntheir phonetic structure, should be the future focus of inductive bias in\nacoustics-to-word models.", "published": "2018-10-31 17:07:14", "link": "http://arxiv.org/abs/1810.13407v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An Interdisciplinary Comparison of Sequence Modeling Methods for\n  Next-Element Prediction", "abstract": "Data of sequential nature arise in many application domains in forms of, e.g.\ntextual data, DNA sequences, and software execution traces. Different research\ndisciplines have developed methods to learn sequence models from such datasets:\n(i) in the machine learning field methods such as (hidden) Markov models and\nrecurrent neural networks have been developed and successfully applied to a\nwide-range of tasks, (ii) in process mining process discovery techniques aim to\ngenerate human-interpretable descriptive models, and (iii) in the grammar\ninference field the focus is on finding descriptive models in the form of\nformal grammars. Despite their different focuses, these fields share a common\ngoal - learning a model that accurately describes the behavior in the\nunderlying data. Those sequence models are generative, i.e, they can predict\nwhat elements are likely to occur after a given unfinished sequence. So far,\nthese fields have developed mainly in isolation from each other and no\ncomparison exists. This paper presents an interdisciplinary experimental\nevaluation that compares sequence modeling techniques on the task of\nnext-element prediction on four real-life sequence datasets. The results\nindicate that machine learning techniques that generally have no aim at\ninterpretability in terms of accuracy outperform techniques from the process\nmining and grammar inference fields that aim to yield interpretable models.", "published": "2018-10-31 18:54:27", "link": "http://arxiv.org/abs/1811.00062v1", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Dirichlet Variational Autoencoder for Text Modeling", "abstract": "We introduce an improved variational autoencoder (VAE) for text modeling with\ntopic information explicitly modeled as a Dirichlet latent variable. By\nproviding the proposed model topic awareness, it is more superior at\nreconstructing input texts. Furthermore, due to the inherent interactions\nbetween the newly introduced Dirichlet variable and the conventional\nmultivariate Gaussian variable, the model is less prone to KL divergence\nvanishing. We derive the variational lower bound for the new model and conduct\nexperiments on four different data sets. The results show that the proposed\nmodel is superior at text reconstruction across the latent space and\nclassifications on learned representations have higher test accuracies.", "published": "2018-10-31 22:04:22", "link": "http://arxiv.org/abs/1811.00135v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "DOLORES: Deep Contextualized Knowledge Graph Embeddings", "abstract": "We introduce a new method DOLORES for learning knowledge graph embeddings\nthat effectively captures contextual cues and dependencies among entities and\nrelations. First, we note that short paths on knowledge graphs comprising of\nchains of entities and relations can encode valuable information regarding\ntheir contextual usage. We operationalize this notion by representing knowledge\ngraphs not as a collection of triples but as a collection of entity-relation\nchains, and learn embeddings for entities and relations using deep neural\nmodels that capture such contextual usage. In particular, our model is based on\nBi-Directional LSTMs and learn deep representations of entities and relations\nfrom constructed entity-relation chains. We show that these representations can\nvery easily be incorporated into existing models to significantly advance the\nstate of the art on several knowledge graph prediction tasks like link\nprediction, triple classification, and missing relation type prediction (in\nsome cases by at least 9.5%).", "published": "2018-10-31 22:59:57", "link": "http://arxiv.org/abs/1811.00147v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Weakly Supervised Grammatical Error Correction using Iterative Decoding", "abstract": "We describe an approach to Grammatical Error Correction (GEC) that is\neffective at making use of models trained on large amounts of weakly supervised\nbitext. We train the Transformer sequence-to-sequence model on 4B tokens of\nWikipedia revisions and employ an iterative decoding strategy that is tailored\nto the loosely-supervised nature of the Wikipedia training corpus. Finetuning\non the Lang-8 corpus and ensembling yields an F0.5 of 58.3 on the CoNLL'14\nbenchmark and a GLEU of 62.4 on JFLEG. The combination of weakly supervised\ntraining and iterative decoding obtains an F0.5 of 48.2 on CoNLL'14 even\nwithout using any labeled GEC data.", "published": "2018-10-31 01:31:10", "link": "http://arxiv.org/abs/1811.01710v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "dAIrector: Automatic Story Beat Generation through Knowledge Synthesis", "abstract": "dAIrector is an automated director which collaborates with humans\nstorytellers for live improvisational performances and writing assistance.\ndAIrector can be used to create short narrative arcs through contextual plot\ngeneration. In this work, we present the system architecture, a quantitative\nevaluation of design choices, and a case-study usage of the system which\nprovides qualitative feedback from a professional improvisational performer. We\npresent relevant metrics for the understudied domain of human-machine creative\ngeneration, specifically long-form narrative creation. We include, alongside\npublication, open-source code so that others may test, evaluate, and run the\ndAIrector.", "published": "2018-10-31 15:41:22", "link": "http://arxiv.org/abs/1811.03423v1", "categories": ["cs.CY", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Latent variable approach to diarization of audio recordings using ad-hoc\n  randomly placed mobile devices", "abstract": "Diarization of audio recordings from ad-hoc mobile devices using spatial\ninformation is considered in this paper. A two-channel synchronous recording is\nassumed for each mobile device, which is used to compute directional statistics\nseparately at each device in a frame-wise manner. The recordings across the\nmobile devices are asynchronous, but a coarse synchronization is performed by\naligning the signals using acoustic events, or real-time clock. Direction\nstatistics computed for all the devices, are then modeled jointly using a\nDirichlet mixture model, and the posterior probability over the mixture\ncomponents is used to derive the diarization information. Experiments on real\nlife recordings using mobile phones show a diarization error rate of less than\n14%.", "published": "2018-10-31 05:16:25", "link": "http://arxiv.org/abs/1810.13109v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Discriminatively Re-trained i-vector Extractor for Speaker Recognition", "abstract": "In this work we revisit discriminative training of the i-vector extractor\ncomponent in the standard speaker verification (SV) system. The motivation of\nour research lies in the robustness and stability of this large generative\nmodel, which we want to preserve, and focus its power towards any intended SV\ntask. We show that after generative initialization of the i-vector extractor,\nwe can further refine it with discriminative training and obtain i-vectors that\nlead to better performance on various benchmarks representing different\nacoustic domains.", "published": "2018-10-31 09:47:01", "link": "http://arxiv.org/abs/1810.13183v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On Single-Channel Speech Enhancement and On Non-Linear Modulation-Domain\n  Kalman Filtering", "abstract": "This report focuses on algorithms that perform single-channel speech\nenhancement. The author of this report uses modulation-domain Kalman filtering\nalgorithms for speech enhancement, i.e. noise suppression and dereverberation,\nin [1], [2], [3], [4] and [5]. Modulation-domain Kalman filtering can be\napplied for both noise and late reverberation suppression and in [2], [1], [3]\nand [4], various model-based speech enhancement algorithms that perform\nmodulation-domain Kalman filtering are designed, implemented and tested. The\nmodel-based enhancement algorithm in [2] estimates and tracks the speech phase.\nThe short-time-Fourier-transform-based enhancement algorithm in [5] uses the\nactive speech level estimator presented in [6]. This report describes how\ndifferent algorithms perform speech enhancement and the algorithms discussed in\nthis report are addressed to researchers interested in monaural speech\nenhancement. The algorithms are composed of different processing blocks and\ntechniques [7]; understanding the implementation choices made during the system\ndesign is important because this provides insights that can assist the\ndevelopment of new algorithms. Index Terms - Speech enhancement,\ndereverberation, denoising, Kalman filter, minimum mean squared error\nestimation.", "published": "2018-10-31 19:30:39", "link": "http://arxiv.org/abs/1811.00078v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio Source Separation Using Variational Autoencoders and Weak Class\n  Supervision", "abstract": "In this paper, we propose a source separation method that is trained by\nobserving the mixtures and the class labels of the sources present in the\nmixture without any access to isolated sources. Since our method does not\nrequire source class labels for every time-frequency bin but only a single\nlabel for each source constituting the mixture signal, we call this scenario as\nweak class supervision. We associate a variational autoencoder (VAE) with each\nsource class within a non-negative (compositional) model. Each VAE provides a\nprior model to identify the signal from its associated class in a sound\nmixture. After training the model on mixtures, we obtain a generative model for\neach source class and demonstrate our method on one-second mixtures of\nutterances of digits from 0 to 9. We show that the separation performance\nobtained by source class supervision is as good as the performance obtained by\nsource signal supervision.", "published": "2018-10-31 04:52:42", "link": "http://arxiv.org/abs/1810.13104v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Introducing SPAIN (SParse Audio INpainter)", "abstract": "A novel sparsity-based algorithm for audio inpainting is proposed. It is an\nadaptation of the SPADE algorithm by Kiti\\'c et al., originally developed for\naudio declipping, to the task of audio inpainting. The new SPAIN (SParse Audio\nINpainter) comes in synthesis and analysis variants. Experiments show that both\nA-SPAIN and S-SPAIN outperform other sparsity-based inpainting algorithms.\nMoreover, A-SPAIN performs on a par with the state-of-the-art method based on\nlinear prediction in terms of the SNR, and, for larger gaps, SPAIN is even\nslightly better in terms of the PEMO-Q psychoacoustic criterion.", "published": "2018-10-31 07:40:41", "link": "http://arxiv.org/abs/1810.13137v4", "categories": ["cs.SD", "eess.AS", "math.OC"], "primary_category": "cs.SD"}
{"title": "MULAN: A Blind and Off-Grid Method for Multichannel Echo Retrieval", "abstract": "This paper addresses the general problem of blind echo retrieval, i.e., given\nM sensors measuring in the discrete-time domain M mixtures of K delayed and\nattenuated copies of an unknown source signal, can the echo locations and\nweights be recovered? This problem has broad applications in fields such as\nsonars, seismol-ogy, ultrasounds or room acoustics. It belongs to the broader\nclass of blind channel identification problems, which have been intensively\nstudied in signal processing. Existing methods in the literature proceed in two\nsteps: (i) blind estimation of sparse discrete-time filters and (ii) echo\ninformation retrieval by peak-picking on filters. The precision of these\nmethods is fundamentally limited by the rate at which the signals are sampled:\nestimated echo locations are necessary on-grid, and since true locations never\nmatch the sampling grid, the weight estimation precision is impacted. This is\nthe so-called basis-mismatch problem in compressed sensing. We propose a\nradically different approach to the problem, building on the framework of\nfinite-rate-of-innovation sampling. The approach operates directly in the\nparameter-space of echo locations and weights, and enables near-exact blind and\noff-grid echo retrieval from discrete-time measurements. It is shown to\noutperform conventional methods by several orders of magnitude in precision.", "published": "2018-10-31 15:30:00", "link": "http://arxiv.org/abs/1810.13338v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Net Features for Complex Emotion Recognition", "abstract": "This paper investigates the influence of different acoustic features,\naudio-events based features and automatic speech translation based lexical\nfeatures in complex emotion recognition such as curiosity. Pretrained networks,\nnamely, AudioSet Net, VoxCeleb Net and Deep Speech Net trained extensively for\ndifferent speech based applications are studied for this objective. Information\nfrom deep layers of these networks are considered as descriptors and encoded\ninto feature vectors. Experimental results on the EmoReact dataset consisting\nof 8 complex emotions show the effectiveness, yielding highest F1 score of 0.85\nas against the baseline of 0.69 in the literature.", "published": "2018-10-31 04:52:18", "link": "http://arxiv.org/abs/1811.00003v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Low-Dimensional Bottleneck Features for On-Device Continuous Speech\n  Recognition", "abstract": "Low power digital signal processors (DSPs) typically have a very limited\namount of memory in which to cache data. In this paper we develop efficient\nbottleneck feature (BNF) extractors that can be run on a DSP, and retrain a\nbaseline large-vocabulary continuous speech recognition (LVCSR) system to use\nthese BNFs with only a minimal loss of accuracy. The small BNFs allow the DSP\nchip to cache more audio features while the main application processor is\nsuspended, thereby reducing the overall battery usage. Our presented system is\nable to reduce the footprint of standard, fixed point DSP spectral features by\na factor of 10 without any loss in word error rate (WER) and by a factor of 64\nwith only a 5.8% relative increase in WER.", "published": "2018-10-31 14:20:24", "link": "http://arxiv.org/abs/1811.00006v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "WaveGlow: A Flow-based Generative Network for Speech Synthesis", "abstract": "In this paper we propose WaveGlow: a flow-based network capable of generating\nhigh quality speech from mel-spectrograms. WaveGlow combines insights from Glow\nand WaveNet in order to provide fast, efficient and high-quality audio\nsynthesis, without the need for auto-regression. WaveGlow is implemented using\nonly a single network, trained using only a single cost function: maximizing\nthe likelihood of the training data, which makes the training procedure simple\nand stable. Our PyTorch implementation produces audio samples at a rate of more\nthan 500 kHz on an NVIDIA V100 GPU. Mean Opinion Scores show that it delivers\naudio quality as good as the best publicly available WaveNet implementation.\nAll code will be made publicly available online.", "published": "2018-10-31 03:22:25", "link": "http://arxiv.org/abs/1811.00002v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Modeling Melodic Feature Dependency with Modularized Variational\n  Auto-Encoder", "abstract": "Automatic melody generation has been a long-time aspiration for both AI\nresearchers and musicians. However, learning to generate euphonious melodies\nhas turned out to be highly challenging. This paper introduces 1) a new variant\nof variational autoencoder (VAE), where the model structure is designed in a\nmodularized manner in order to model polyphonic and dynamic music with domain\nknowledge, and 2) a hierarchical encoding/decoding strategy, which explicitly\nmodels the dependency between melodic features. The proposed framework is\ncapable of generating distinct melodies that sounds natural, and the\nexperiments for evaluating generated music clips show that the proposed model\noutperforms the baselines in human evaluation.", "published": "2018-10-31 23:59:04", "link": "http://arxiv.org/abs/1811.00162v1", "categories": ["cs.AI", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
