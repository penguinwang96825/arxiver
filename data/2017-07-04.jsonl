{"title": "Multilingual Hierarchical Attention Networks for Document Classification", "abstract": "Hierarchical attention networks have recently achieved remarkable performance\nfor document classification in a given language. However, when multilingual\ndocument collections are considered, training such models separately for each\nlanguage entails linear parameter growth and lack of cross-language transfer.\nLearning a single multilingual model with fewer parameters is therefore a\nchallenging but potentially beneficial objective. To this end, we propose\nmultilingual hierarchical attention networks for learning document structures,\nwith shared encoders and/or shared attention mechanisms across languages, using\nmulti-task learning and an aligned semantic space as input. We evaluate the\nproposed models on multilingual document classification with disjoint label\nsets, on a large dataset which we provide, with 600k news documents in 8\nlanguages, and 5k labels. The multilingual models outperform monolingual ones\nin low-resource as well as full-resource settings, and use fewer parameters,\nthus confirming their computational efficiency and the utility of\ncross-language transfer.", "published": "2017-07-04 10:28:04", "link": "http://arxiv.org/abs/1707.00896v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An empirical study on the effectiveness of images in Multimodal Neural\n  Machine Translation", "abstract": "In state-of-the-art Neural Machine Translation (NMT), an attention mechanism\nis used during decoding to enhance the translation. At every step, the decoder\nuses this mechanism to focus on different parts of the source sentence to\ngather the most useful information before outputting its target word. Recently,\nthe effectiveness of the attention mechanism has also been explored for\nmultimodal tasks, where it becomes possible to focus both on sentence parts and\nimage regions that they describe. In this paper, we compare several attention\nmechanism on the multimodal translation task (English, image to German) and\nevaluate the ability of the model to make use of images to improve translation.\nWe surpass state-of-the-art scores on the Multi30k data set, we nevertheless\nidentify and report different misbehavior of the machine while translating.", "published": "2017-07-04 13:57:04", "link": "http://arxiv.org/abs/1707.00995v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Visually Grounded Word Embeddings and Richer Visual Features for\n  Improving Multimodal Neural Machine Translation", "abstract": "In Multimodal Neural Machine Translation (MNMT), a neural model generates a\ntranslated sentence that describes an image, given the image itself and one\nsource descriptions in English. This is considered as the multimodal image\ncaption translation task. The images are processed with Convolutional Neural\nNetwork (CNN) to extract visual features exploitable by the translation model.\nSo far, the CNNs used are pre-trained on object detection and localization\ntask. We hypothesize that richer architecture, such as dense captioning models,\nmay be more suitable for MNMT and could lead to improved translations. We\nextend this intuition to the word-embeddings, where we compute both linguistic\nand visual representation for our corpus vocabulary. We combine and compare\ndifferent confi", "published": "2017-07-04 14:26:56", "link": "http://arxiv.org/abs/1707.01009v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Transfer Learning for Event Extraction", "abstract": "Most previous event extraction studies have relied heavily on features\nderived from annotated event mentions, thus cannot be applied to new event\ntypes without annotation effort. In this work, we take a fresh look at event\nextraction and model it as a grounding problem. We design a transferable neural\narchitecture, mapping event mentions and types jointly into a shared semantic\nspace using structural and compositional neural networks, where the type of\neach event mention can be determined by the closest of all candidate types . By\nleveraging (1)~available manual annotations for a small set of existing event\ntypes and (2)~existing event ontologies, our framework applies to new event\ntypes without requiring additional annotation. Experiments on both existing\nevent types (e.g., ACE, ERE) and new event types (e.g., FrameNet) demonstrate\nthe effectiveness of our approach. \\textit{Without any manual annotations} for\n23 new event types, our zero-shot framework achieved performance comparable to\na state-of-the-art supervised model which is trained from the annotations of\n500 event mentions.", "published": "2017-07-04 16:48:28", "link": "http://arxiv.org/abs/1707.01066v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Slot Filling Performance with Attentive Neural Networks on\n  Dependency Structures", "abstract": "Slot Filling (SF) aims to extract the values of certain types of attributes\n(or slots, such as person:cities\\_of\\_residence) for a given entity from a\nlarge collection of source documents. In this paper we propose an effective DNN\narchitecture for SF with the following new strategies: (1). Take a regularized\ndependency graph instead of a raw sentence as input to DNN, to compress the\nwide contexts between query and candidate filler; (2). Incorporate two\nattention mechanisms: local attention learned from query and candidate filler,\nand global attention learned from external knowledge bases, to guide the model\nto better select indicative contexts to determine slot type. Experiments show\nthat this framework outperforms state-of-the-art on both relation extraction\n(16\\% absolute F-score gain) and slot filling validation for each individual\nsystem (up to 8.5\\% absolute F-score gain).", "published": "2017-07-04 17:18:50", "link": "http://arxiv.org/abs/1707.01075v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Shakespearizing Modern Language Using Copy-Enriched Sequence-to-Sequence\n  Models", "abstract": "Variations in writing styles are commonly used to adapt the content to a\nspecific context, audience, or purpose. However, applying stylistic variations\nis still by and large a manual process, and there have been little efforts\ntowards automating it. In this paper we explore automated methods to transform\ntext from modern English to Shakespearean English using an end to end trainable\nneural model with pointers to enable copy action. To tackle limited amount of\nparallel data, we pre-train embeddings of words by leveraging external\ndictionaries mapping Shakespearean words to modern English words as well as\nadditional text. Our methods are able to get a BLEU score of 31+, an\nimprovement of ~6 points above the strongest baseline. We publicly release our\ncode to foster further research in this area.", "published": "2017-07-04 21:42:55", "link": "http://arxiv.org/abs/1707.01161v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CharManteau: Character Embedding Models For Portmanteau Creation", "abstract": "Portmanteaus are a word formation phenomenon where two words are combined to\nform a new word. We propose character-level neural sequence-to-sequence (S2S)\nmethods for the task of portmanteau generation that are end-to-end-trainable,\nlanguage independent, and do not explicitly use additional phonetic\ninformation. We propose a noisy-channel-style model, which allows for the\nincorporation of unsupervised word lists, improving performance over a standard\nsource-to-target model. This model is made possible by an exhaustive candidate\ngeneration strategy specifically enabled by the features of the portmanteau\ntask. Experiments find our approach superior to a state-of-the-art FST-based\nbaseline with respect to ground truth accuracy and human evaluation.", "published": "2017-07-04 23:11:52", "link": "http://arxiv.org/abs/1707.01176v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hidden-Markov-Model Based Speech Enhancement", "abstract": "The goal of this contribution is to use a parametric speech synthesis system\nfor reducing background noise and other interferences from recorded speech\nsignals. In a first step, Hidden Markov Models of the synthesis system are\ntrained.\n  Two adequate training corpora consisting of text and corresponding speech\nfiles have been set up and cleared of various faults, including inaudible\nutterances or incorrect assignments between audio and text data. Those are\ntested and compared against each other regarding e.g. flaws in the synthesized\nspeech, it's naturalness and intelligibility. Thus different voices have been\nsynthesized, whose quality depends less on the number of training samples used,\nbut much more on the cleanliness and signal-to-noise ratio of those.\nGeneralized voice models have been used for synthesis and the results greatly\ndiffer between the two speech corpora.\n  Tests regarding the adaptation to different speakers show that a resemblance\nto the original speaker is audible throughout all recordings, yet the\nsynthesized voices sound robotic and unnatural in smaller parts. The spoken\ntext, however, is usually intelligible, which shows that the models are working\nwell.\n  In a novel approach, speech is synthesized using side information of the\noriginal audio signal, particularly the pitch frequency. Results show an\nincrease of speech quality and intelligibility in comparison to speech\nsynthesized solely from text, up to the point of being nearly indistinguishable\nfrom the original.", "published": "2017-07-04 09:58:40", "link": "http://arxiv.org/abs/1707.01090v1", "categories": ["cs.SD", "cs.CL"], "primary_category": "cs.SD"}
{"title": "Complexity Metric for Code-Mixed Social Media Text", "abstract": "An evaluation metric is an absolute necessity for measuring the performance\nof any system and complexity of any data. In this paper, we have discussed how\nto determine the level of complexity of code-mixed social media texts that are\ngrowing rapidly due to multilingual interference. In general, texts written in\nmultiple languages are often hard to comprehend and analyze. At the same time,\nin order to meet the demands of analysis, it is also necessary to determine the\ncomplexity of a particular document or a text segment. Thus, in the present\npaper, we have discussed the existing metrics for determining the code-mixing\ncomplexity of a corpus, their advantages, and shortcomings as well as proposed\nseveral improvements on the existing metrics. The new index better reflects the\nvariety and complexity of a multilingual document. Also, the index can be\napplied to a sentence and seamlessly extended to a paragraph or an entire\ndocument. We have employed two existing code-mixed corpora to suit the\nrequirements of our study.", "published": "2017-07-04 23:29:31", "link": "http://arxiv.org/abs/1707.01183v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Automatic Generation of Natural Language Explanations", "abstract": "An important task for recommender system is to generate explanations\naccording to a user's preferences. Most of the current methods for explainable\nrecommendations use structured sentences to provide descriptions along with the\nrecommendations they produce. However, those methods have neglected the\nreview-oriented way of writing a text, even though it is known that these\nreviews have a strong influence over user's decision.\n  In this paper, we propose a method for the automatic generation of natural\nlanguage explanations, for predicting how a user would write about an item,\nbased on user ratings from different items' features. We design a\ncharacter-level recurrent neural network (RNN) model, which generates an item's\nreview explanations using long-short term memories (LSTM). The model generates\ntext reviews given a combination of the review and ratings score that express\nopinions about different factors or aspects of an item. Our network is trained\non a sub-sample from the large real-world dataset BeerAdvocate. Our empirical\nevaluation using natural language processing metrics shows the generated text's\nquality is close to a real user written review, identifying negation,\nmisspellings, and domain specific vocabulary.", "published": "2017-07-04 14:52:41", "link": "http://arxiv.org/abs/1707.01561v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DeepStory: Video Story QA by Deep Embedded Memory Networks", "abstract": "Question-answering (QA) on video contents is a significant challenge for\nachieving human-level intelligence as it involves both vision and language in\nreal-world settings. Here we demonstrate the possibility of an AI agent\nperforming video story QA by learning from a large amount of cartoon videos. We\ndevelop a video-story learning model, i.e. Deep Embedded Memory Networks\n(DEMN), to reconstruct stories from a joint scene-dialogue video stream using a\nlatent embedding space of observed data. The video stories are stored in a\nlong-term memory component. For a given question, an LSTM-based attention model\nuses the long-term memory to recall the best question-story-answer triplet by\nfocusing on specific words containing key information. We trained the DEMN on a\nnovel QA dataset of children's cartoon video series, Pororo. The dataset\ncontains 16,066 scene-dialogue pairs of 20.5-hour videos, 27,328 fine-grained\nsentences for scene description, and 8,913 story-related QA pairs. Our\nexperimental results show that the DEMN outperforms other QA models. This is\nmainly due to 1) the reconstruction of video stories in a scene-dialogue\ncombined form that utilize the latent embedding and 2) attention. DEMN also\nachieved state-of-the-art results on the MovieQA benchmark.", "published": "2017-07-04 07:42:05", "link": "http://arxiv.org/abs/1707.00836v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Sentiment Identification in Code-Mixed Social Media Text", "abstract": "Sentiment analysis is the Natural Language Processing (NLP) task dealing with\nthe detection and classification of sentiments in texts. While some tasks deal\nwith identifying the presence of sentiment in the text (Subjectivity analysis),\nother tasks aim at determining the polarity of the text categorizing them as\npositive, negative and neutral. Whenever there is a presence of sentiment in\nthe text, it has a source (people, group of people or any entity) and the\nsentiment is directed towards some entity, object, event or person. Sentiment\nanalysis tasks aim to determine the subject, the target and the polarity or\nvalence of the sentiment. In our work, we try to automatically extract\nsentiment (positive or negative) from Facebook posts using a machine learning\napproach.While some works have been done in code-mixed social media data and in\nsentiment analysis separately, our work is the first attempt (as of now) which\naims at performing sentiment analysis of code-mixed social media text. We have\nused extensive pre-processing to remove noise from raw text. Multilayer\nPerceptron model has been used to determine the polarity of the sentiment. We\nhave also developed the corpus for this task by manually labeling Facebook\nposts with their associated sentiments.", "published": "2017-07-04 23:29:44", "link": "http://arxiv.org/abs/1707.01184v1", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
