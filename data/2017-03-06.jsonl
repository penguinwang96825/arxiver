{"title": "Word forms - not just their lengths- are optimized for efficient\n  communication", "abstract": "The inverse relationship between the length of a word and the frequency of\nits use, first identified by G.K. Zipf in 1935, is a classic empirical law that\nholds across a wide range of human languages. We demonstrate that length is one\naspect of a much more general property of words: how distinctive they are with\nrespect to other words in a language. Distinctiveness plays a critical role in\nrecognizing words in fluent speech, in that it reflects the strength of\npotential competitors when selecting the best candidate for an ambiguous\nsignal. Phonological information content, a measure of a word's string\nprobability under a statistical model of a language's sound or character\nsequences, concisely captures distinctiveness. Examining large-scale corpora\nfrom 13 languages, we find that distinctiveness significantly outperforms word\nlength as a predictor of frequency. This finding provides evidence that\nlisteners' processing constraints shape fine-grained aspects of word forms\nacross languages.", "published": "2017-03-06 00:38:51", "link": "http://arxiv.org/abs/1703.01694v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Novel Comprehensive Approach for Estimating Concept Semantic\n  Similarity in WordNet", "abstract": "Computation of semantic similarity between concepts is an important\nfoundation for many research works. This paper focuses on IC computing methods\nand IC measures, which estimate the semantic similarities between concepts by\nexploiting the topological parameters of the taxonomy. Based on analyzing\nrepresentative IC computing methods and typical semantic similarity measures,\nwe propose a new hybrid IC computing method. Through adopting the parameter\ndhyp and lch, we utilize the new IC computing method and propose a novel\ncomprehensive measure of semantic similarity between concepts. An experiment\nbased on WordNet \"is a\" taxonomy has been designed to test representative\nmeasures and our measure on benchmark dataset R&G, and the results show that\nour measure can obviously improve the similarity accuracy. We evaluate the\nproposed approach by comparing the correlation coefficients between five\nmeasures and the artificial data. The results show that our proposal\noutperforms the previous measures.", "published": "2017-03-06 05:07:12", "link": "http://arxiv.org/abs/1703.01726v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Performing Stance Detection on Twitter Data using Computational\n  Linguistics Techniques", "abstract": "As humans, we can often detect from a persons utterances if he or she is in\nfavor of or against a given target entity (topic, product, another person,\netc). But from the perspective of a computer, we need means to automatically\ndeduce the stance of the tweeter, given just the tweet text. In this paper, we\npresent our results of performing stance detection on twitter data using a\nsupervised approach. We begin by extracting bag-of-words to perform\nclassification using TIMBL, then try and optimize the features to improve\nstance detection accuracy, followed by extending the dataset with two sets of\nlexicons - arguing, and MPQA subjectivity; next we explore the MALT parser and\nconstruct features using its dependency triples, finally we perform analysis\nusing Scikit-learn Random Forest implementation.", "published": "2017-03-06 18:44:49", "link": "http://arxiv.org/abs/1703.02019v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "English Conversational Telephone Speech Recognition by Humans and\n  Machines", "abstract": "One of the most difficult speech recognition tasks is accurate recognition of\nhuman to human communication. Advances in deep learning over the last few years\nhave produced major speech recognition improvements on the representative\nSwitchboard conversational corpus. Word error rates that just a few years ago\nwere 14% have dropped to 8.0%, then 6.6% and most recently 5.8%, and are now\nbelieved to be within striking range of human performance. This then raises two\nissues - what IS human performance, and how far down can we still drive speech\nrecognition error rates? A recent paper by Microsoft suggests that we have\nalready achieved human performance. In trying to verify this statement, we\nperformed an independent set of human performance measurements on two\nconversational tasks and found that human performance may be considerably\nbetter than what was earlier reported, giving the community a significantly\nharder goal to achieve. We also report on our own efforts in this area,\npresenting a set of acoustic and language modeling techniques that lowered the\nword error rate of our own English conversational telephone LVCSR system to the\nlevel of 5.5%/10.3% on the Switchboard/CallHome subsets of the Hub5 2000\nevaluation, which - at least at the writing of this paper - is a new\nperformance milestone (albeit not at what we measure to be human performance!).\nOn the acoustic side, we use a score fusion of three models: one LSTM with\nmultiple feature inputs, a second LSTM trained with speaker-adversarial\nmulti-task learning and a third residual net (ResNet) with 25 convolutional\nlayers and time-dilated convolutions. On the language modeling side, we use\nword and character LSTMs and convolutional WaveNet-style language models.", "published": "2017-03-06 22:37:43", "link": "http://arxiv.org/abs/1703.02136v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sound-Word2Vec: Learning Word Representations Grounded in Sounds", "abstract": "To be able to interact better with humans, it is crucial for machines to\nunderstand sound - a primary modality of human perception. Previous works have\nused sound to learn embeddings for improved generic textual similarity\nassessment. In this work, we treat sound as a first-class citizen, studying\ndownstream textual tasks which require aural grounding. To this end, we propose\nsound-word2vec - a new embedding scheme that learns specialized word embeddings\ngrounded in sounds. For example, we learn that two seemingly (semantically)\nunrelated concepts, like leaves and paper are similar due to the similar\nrustling sounds they make. Our embeddings prove useful in textual tasks\nrequiring aural reasoning like text-based sound retrieval and discovering foley\nsound effects (used in movies). Moreover, our embedding space captures\ninteresting dependencies between words and onomatopoeia and outperforms prior\nwork on aurally-relevant word relatedness datasets such as AMEN and ASLex.", "published": "2017-03-06 04:30:12", "link": "http://arxiv.org/abs/1703.01720v4", "categories": ["cs.CL", "cs.AI", "cs.SD"], "primary_category": "cs.CL"}
{"title": "Cats and Captions vs. Creators and the Clock: Comparing Multimodal\n  Content to Context in Predicting Relative Popularity", "abstract": "The content of today's social media is becoming more and more rich,\nincreasingly mixing text, images, videos, and audio. It is an intriguing\nresearch question to model the interplay between these different modes in\nattracting user attention and engagement. But in order to pursue this study of\nmultimodal content, we must also account for context: timing effects, community\npreferences, and social factors (e.g., which authors are already popular) also\naffect the amount of feedback and reaction that social-media posts receive. In\nthis work, we separate out the influence of these non-content factors in\nseveral ways. First, we focus on ranking pairs of submissions posted to the\nsame community in quick succession, e.g., within 30 seconds, this framing\nencourages models to focus on time-agnostic and community-specific content\nfeatures. Within that setting, we determine the relative performance of author\nvs. content features. We find that victory usually belongs to \"cats and\ncaptions,\" as visual and textual features together tend to outperform\nidentity-based features. Moreover, our experiments show that when considered in\nisolation, simple unigram text features and deep neural network visual features\nyield the highest accuracy individually, and that the combination of the two\nmodalities generally leads to the best accuracies overall.", "published": "2017-03-06 04:56:19", "link": "http://arxiv.org/abs/1703.01725v1", "categories": ["cs.SI", "cs.CL", "cs.CV", "physics.soc-ph"], "primary_category": "cs.SI"}
{"title": "Generative and Discriminative Text Classification with Recurrent Neural\n  Networks", "abstract": "We empirically characterize the performance of discriminative and generative\nLSTM models for text classification. We find that although RNN-based generative\nmodels are more powerful than their bag-of-words ancestors (e.g., they account\nfor conditional dependencies across words in a document), they have higher\nasymptotic error rates than discriminatively trained RNN models. However we\nalso find that generative models approach their asymptotic error rate more\nrapidly than their discriminative counterparts---the same pattern that Ng &\nJordan (2001) proved holds for linear classification models that make more\nnaive conditional independence assumptions. Building on this finding, we\nhypothesize that RNN-based generative classification models will be more robust\nto shifts in the data distribution. This hypothesis is confirmed in a series of\nexperiments in zero-shot and continual learning settings that show that\ngenerative models substantially outperform discriminative models.", "published": "2017-03-06 14:40:09", "link": "http://arxiv.org/abs/1703.01898v2", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
