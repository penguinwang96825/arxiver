{"title": "Connotation Frames: A Data-Driven Investigation", "abstract": "Through a particular choice of a predicate (e.g., \"x violated y\"), a writer\ncan subtly connote a range of implied sentiments and presupposed facts about\nthe entities x and y: (1) writer's perspective: projecting x as an\n\"antagonist\"and y as a \"victim\", (2) entities' perspective: y probably dislikes\nx, (3) effect: something bad happened to y, (4) value: y is something valuable,\nand (5) mental state: y is distressed by the event. We introduce connotation\nframes as a representation formalism to organize these rich dimensions of\nconnotation using typed relations. First, we investigate the feasibility of\nobtaining connotative labels through crowdsourcing experiments. We then present\nmodels for predicting the connotation frames of verb predicates based on their\ndistributional word representations and the interplay between different types\nof connotative relations. Empirical results confirm that connotation frames can\nbe induced from various data sources that reflect how people use language and\ngive rise to the connotative meanings. We conclude with analytical results that\nshow the potential use of connotation frames for analyzing subtle biases in\nonline news media.", "published": "2015-06-09 00:58:51", "link": "http://arxiv.org/abs/1506.02739v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Textual Features for Best Answer Prediction in\n  Community-based Question Answering", "abstract": "This paper addresses the problem of determining the best answer in\nCommunity-based Question Answering (CQA) websites by focussing on the content.\nIn particular, we present a system, ACQUA [http://acqua.kmi.open.ac.uk], that\ncan be installed onto the majority of browsers as a plugin. The service offers\na seamless and accurate prediction of the answer to be accepted. Previous\nresearch on this topic relies on the exploitation of community feedback on the\nanswers, which involves rating of either users (e.g., reputation) or answers\n(e.g. scores manually assigned to answers). We propose a new technique that\nleverages the content/textual features of answers in a novel way. Our approach\ndelivers better results than related linguistics-based solutions and manages to\nmatch rating-based approaches. More specifically, the gain in performance is\nachieved by rendering the values of these features into a discretised form. We\nalso show how our technique manages to deliver equally good results in\nreal-time settings, as opposed to having to rely on information not always\nreadily available, such as user ratings and answer scores. We ran an evaluation\non 21 StackExchange websites covering around 4 million questions and more than\n8 million answers. We obtain 84% average precision and 70% recall, which shows\nthat our technique is robust, effective, and widely applicable.", "published": "2015-06-09 08:09:34", "link": "http://arxiv.org/abs/1506.02816v2", "categories": ["cs.CL", "cs.IR", "H.3.1"], "primary_category": "cs.CL"}
{"title": "An Ensemble method for Content Selection for Data-to-text Systems", "abstract": "We present a novel approach for automatic report generation from time-series\ndata, in the context of student feedback generation. Our proposed methodology\ntreats content selection as a multi-label classification (MLC) problem, which\ntakes as input time-series data (students' learning data) and outputs a summary\nof these data (feedback). Unlike previous work, this method considers all data\nsimultaneously using ensembles of classifiers, and therefore, it achieves\nhigher accuracy and F- score compared to meaningful baselines.", "published": "2015-06-09 14:17:06", "link": "http://arxiv.org/abs/1506.02922v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WordRank: Learning Word Embeddings via Robust Ranking", "abstract": "Embedding words in a vector space has gained a lot of attention in recent\nyears. While state-of-the-art methods provide efficient computation of word\nsimilarities via a low-dimensional matrix embedding, their motivation is often\nleft unclear. In this paper, we argue that word embedding can be naturally\nviewed as a ranking problem due to the ranking nature of the evaluation\nmetrics. Then, based on this insight, we propose a novel framework WordRank\nthat efficiently estimates word representations via robust ranking, in which\nthe attention mechanism and robustness to noise are readily achieved via the\nDCG-like ranking losses. The performance of WordRank is measured in word\nsimilarity and word analogy benchmarks, and the results are compared to the\nstate-of-the-art word embedding techniques. Our algorithm is very competitive\nto the state-of-the- arts on large corpora, while outperforms them by a\nsignificant margin when the training set is limited (i.e., sparse and noisy).\nWith 17 million tokens, WordRank performs almost as well as existing methods\nusing 7.2 billion tokens on a popular word similarity benchmark. Our multi-node\ndistributed implementation of WordRank is publicly available for general usage.", "published": "2015-06-09 03:08:06", "link": "http://arxiv.org/abs/1506.02761v4", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Scheduled Sampling for Sequence Prediction with Recurrent Neural\n  Networks", "abstract": "Recurrent Neural Networks can be trained to produce sequences of tokens given\nsome input, as exemplified by recent results in machine translation and image\ncaptioning. The current approach to training them consists of maximizing the\nlikelihood of each token in the sequence given the current (recurrent) state\nand the previous token. At inference, the unknown previous token is then\nreplaced by a token generated by the model itself. This discrepancy between\ntraining and inference can yield errors that can accumulate quickly along the\ngenerated sequence. We propose a curriculum learning strategy to gently change\nthe training process from a fully guided scheme using the true previous token,\ntowards a less guided scheme which mostly uses the generated token instead.\nExperiments on several sequence prediction tasks show that this approach yields\nsignificant improvements. Moreover, it was used successfully in our winning\nentry to the MSCOCO image captioning challenge, 2015.", "published": "2015-06-09 20:33:47", "link": "http://arxiv.org/abs/1506.03099v3", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
