{"title": "Defaultable bond liquidity spread estimation: an option-based approach", "abstract": "This paper extends an option-theoretic approach to estimate liquidity spreads\nfor corporate bonds. Inspired by Longstaff's equity market framework and\nsubsequent work by Koziol and Sauerbier on risk-free zero-coupon bonds, the\nmodel views liquidity as a look-back option. The model accounts for the\ninterplay of risk-free rate volatility and credit risk. A numerical analysis\nhighlights the impact of these factors on the liquidity spread, particularly\nfor bonds with different maturities and credit ratings. The methodology is\napplied to estimate the liquidity spread for unquoted bonds, with a specific\ncase study on the Republic of Italy's debt, leveraging market data to calibrate\nmodel parameters and classify liquid versus illiquid emissions. This approach\nprovides a robust tool for pricing illiquid bonds, emphasizing the importance\nof marketability in debt security valuation.", "published": "2025-01-20 11:56:09", "link": "http://arxiv.org/abs/2501.11427v1", "categories": ["q-fin.PR", "q-fin.CP"], "primary_category": "q-fin.PR"}
{"title": "Mean-Field Limits for Nearly Unstable Hawkes Processes", "abstract": "In this paper, we establish general scaling limits for nearly unstable Hawkes\nprocesses in a mean-field regime by extending the method introduced by Jaisson\nand Rosenbaum. Under a mild asymptotic criticality condition on the\nself-exciting kernels $\\{\\phi^n\\}$, specifically $\\|\\phi^n\\|_{L^1} \\to 1$, we\nfirst show that the scaling limits of these Hawkes processes are necessarily\nstochastic Volterra diffusions of affine type. Moreover, we establish a\npropagation of chaos result for Hawkes systems with mean-field interactions,\nhighlighting three distinct regimes for the limiting processes, which depend on\nthe asymptotics of $n(1-\\|\\phi^n\\|_{L^1})^2$. These results provide a\nsignificant generalization of the findings by Delattre, Fournier and Hoffmann.", "published": "2025-01-20 18:27:27", "link": "http://arxiv.org/abs/2501.11648v1", "categories": ["math.PR", "q-fin.ST", "60F05, 60G55, 60G22, 60F17, 60G57"], "primary_category": "math.PR"}
{"title": "Embedding-Driven Diversity Sampling to Improve Few-Shot Synthetic Data\n  Generation", "abstract": "Accurate classification of clinical text often requires fine-tuning\npre-trained language models, a process that is costly and time-consuming due to\nthe need for high-quality data and expert annotators. Synthetic data generation\noffers an alternative, though pre-trained models may not capture the syntactic\ndiversity of clinical notes. We propose an embedding-driven approach that uses\ndiversity sampling from a small set of real clinical notes to guide large\nlanguage models in few-shot prompting, generating synthetic text that better\nreflects clinical syntax. We evaluated this method using the CheXpert dataset\non a classification task, comparing it to random few-shot and zero-shot\napproaches. Using cosine similarity and a Turing test, our approach produced\nsynthetic notes that more closely align with real clinical text. Our pipeline\nreduced the data needed to reach the 0.85 AUC cutoff by 40% for AUROC and 30%\nfor AUPRC, while augmenting models with synthetic data improved AUROC by 57%\nand AUPRC by 68%. Additionally, our synthetic data was 0.9 times as effective\nas real data, a 60% improvement in value.", "published": "2025-01-20 00:16:57", "link": "http://arxiv.org/abs/2501.11199v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can xLLMs Understand the Structure of Dialog? Exploring Multilingual\n  Response Generation in Complex Scenarios", "abstract": "Multilingual research has garnered increasing attention, especially in the\ndomain of dialogue systems. The rapid advancements in large language models\n(LLMs) have fueled the demand for high-performing multilingual models. However,\ntwo major challenges persist: the scarcity of high-quality multilingual\ndatasets and the limited complexity of existing datasets in capturing realistic\ndialogue scenarios. To address these gaps, we introduce XMP, a high-quality\nparallel Multilingual dataset sourced from Multi-party Podcast dialogues. Each\nsample in the dataset features at least three participants discussing a wide\nrange of topics, including society, culture, politics, and\nentertainment.Through extensive experiments, we uncover significant limitations\nin previously recognized multilingual capabilities of LLMs when applied to such\ncomplex dialogue scenarios. For instance, the widely accepted multilingual\ncomplementary ability of LLMs is notably impacted. By conducting further\nexperiments, we explore the mechanisms of LLMs in multilingual environments\nfrom multiple perspectives, shedding new light on their performance in\nreal-world, diverse conversational contexts.", "published": "2025-01-20 04:33:03", "link": "http://arxiv.org/abs/2501.11269v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-round, Chain-of-thought Post-editing for Unfaithful Summaries", "abstract": "Recent large language models (LLMs) have demonstrated a remarkable ability to\nperform natural language understanding and generation tasks. In this work, we\ninvestigate the use of LLMs for evaluating faithfulness in news summarization,\nfinding that it achieves a strong correlation with human judgments. We further\ninvestigate LLMs' capabilities as a faithfulness post-editor, experimenting\nwith different chain-of-thought prompts for locating and correcting factual\ninconsistencies between a generated summary and the source news document and\nare able to achieve a higher editing success rate than was reported in prior\nwork. We perform both automated and human evaluations of the post-edited\nsummaries, finding that prompting LLMs using chain-of-thought reasoning about\nfactual error types is an effective faithfulness post-editing strategy,\nperforming comparably to fine-tuned post-editing models. We also demonstrate\nthat multiple rounds of post-editing, which has not previously been explored,\ncan be used to gradually improve the faithfulness of summaries whose errors\ncannot be fully corrected in a single round.", "published": "2025-01-20 04:55:43", "link": "http://arxiv.org/abs/2501.11273v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Advancing Multi-Party Dialogue Systems with Speaker-ware Contrastive\n  Learning", "abstract": "Dialogue response generation has made significant progress, but most research\nhas focused on dyadic dialogue. In contrast, multi-party dialogues involve more\nparticipants, each potentially discussing different topics, making the task\nmore complex. Current methods often rely on graph neural networks to model\ndialogue context, which helps capture the structural dynamics of multi-party\nconversations. However, these methods are heavily dependent on intricate graph\nstructures and dataset annotations, and they often overlook the distinct\nspeaking styles of participants. To address these challenges, we propose CMR, a\nContrastive learning-based Multi-party dialogue Response generation model. CMR\nuses self-supervised contrastive learning to better distinguish \"who says\nwhat.\" Additionally, by comparing speakers within the same conversation, the\nmodel captures differences in speaking styles and thematic transitions. To the\nbest of our knowledge, this is the first approach to apply contrastive learning\nin multi-party dialogue generation. Experimental results show that CMR\nsignificantly outperforms state-of-the-art models in multi-party dialogue\nresponse tasks.", "published": "2025-01-20 06:28:22", "link": "http://arxiv.org/abs/2501.11292v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RACCOON: A Retrieval-Augmented Generation Approach for Location\n  Coordinate Capture from News Articles", "abstract": "Geocoding involves automatic extraction of location coordinates of incidents\nreported in news articles, and can be used for epidemic intelligence or\ndisaster management. This paper introduces Retrieval-Augmented Coordinate\nCapture Of Online News articles (RACCOON), an open-source geocoding approach\nthat extracts geolocations from news articles. RACCOON uses a\nretrieval-augmented generation (RAG) approach where candidate locations and\nassociated information are retrieved in the form of context from a location\ndatabase, and a prompt containing the retrieved context, location mentions and\nnews articles is fed to an LLM to generate the location coordinates. Our\nevaluation on three datasets, two underlying LLMs, three baselines and several\nablation tests based on the components of RACCOON demonstrate the utility of\nRACCOON. To the best of our knowledge, RACCOON is the first RAG-based approach\nfor geocoding using pre-trained LLMs.", "published": "2025-01-20 12:26:39", "link": "http://arxiv.org/abs/2501.11440v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Curiosity-Driven Reinforcement Learning from Human Feedback", "abstract": "Reinforcement learning from human feedback (RLHF) has proven effective in\naligning large language models (LLMs) with human preferences, but often at the\ncost of reduced output diversity. This trade-off between diversity and\nalignment quality remains a significant challenge. Drawing inspiration from\ncuriosity-driven exploration in reinforcement learning, we introduce\ncuriosity-driven RLHF (CD-RLHF), a framework that incorporates intrinsic\nrewards for novel states, alongside traditional sparse extrinsic rewards, to\noptimize both output diversity and alignment quality. We demonstrate the\neffectiveness of CD-RLHF through extensive experiments on a range of tasks,\nincluding text summarization and instruction following. Our approach achieves\nsignificant gains in diversity on multiple diversity-oriented metrics while\nmaintaining alignment with human preferences comparable to standard RLHF. We\nmake our code publicly available at https://github.com/ernie-research/CD-RLHF.", "published": "2025-01-20 12:51:40", "link": "http://arxiv.org/abs/2501.11463v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Whose Boat Does it Float? Improving Personalization in Preference Tuning\n  via Inferred User Personas", "abstract": "LLMs are tuned to follow instructions (aligned) by learning which of two\noutputs users prefer for a prompt. However, this preference data format does\nnot convey why users prefer responses that are chosen or rejected, so LLMs\ntrained on these datasets cannot tailor responses to varied user needs. To\nsurface these parameters of personalization, we apply abductive reasoning to\npreference data, inferring needs and interests of users, i.e. personas, that\nmay prefer each output. We test this idea in two steps: Persona Inference\n(PI)-abductively inferring personas of users who prefer chosen or rejected\noutputs-and Persona Tailoring (PT)-training models to tailor responses to\npersonas from PI. We find: 1) LLMs infer personas accurately explaining why\ndifferent users may prefer both chosen or rejected outputs; 2) Training on\npreference data augmented with PI personas via PT boosts personalization,\nenabling models to support user-written personas; and 3) Rejected response\npersonas form harder personalization evaluations, showing PT better aids users\nwith uncommon preferences versus typical alignment methods. We argue for an\nabductive view of preferences for personalization, asking not only which\nresponse is better but when, why, and for whom.", "published": "2025-01-20 15:38:43", "link": "http://arxiv.org/abs/2501.11549v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PIKE-RAG: sPecIalized KnowledgE and Rationale Augmented Generation", "abstract": "Despite notable advancements in Retrieval-Augmented Generation (RAG) systems\nthat expand large language model (LLM) capabilities through external retrieval,\nthese systems often struggle to meet the complex and diverse needs of\nreal-world industrial applications. The reliance on retrieval alone proves\ninsufficient for extracting deep, domain-specific knowledge performing in\nlogical reasoning from specialized corpora. To address this, we introduce\nsPecIalized KnowledgE and Rationale Augmentation Generation (PIKE-RAG),\nfocusing on extracting, understanding, and applying specialized knowledge,\nwhile constructing coherent rationale to incrementally steer LLMs toward\naccurate responses. Recognizing the diverse challenges of industrial tasks, we\nintroduce a new paradigm that classifies tasks based on their complexity in\nknowledge extraction and application, allowing for a systematic evaluation of\nRAG systems' problem-solving capabilities. This strategic approach offers a\nroadmap for the phased development and enhancement of RAG systems, tailored to\nmeet the evolving demands of industrial applications. Furthermore, we propose\nknowledge atomizing and knowledge-aware task decomposition to effectively\nextract multifaceted knowledge from the data chunks and iteratively construct\nthe rationale based on original query and the accumulated knowledge,\nrespectively, showcasing exceptional performance across various benchmarks.", "published": "2025-01-20 15:39:39", "link": "http://arxiv.org/abs/2501.11551v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "YouLeQD: Decoding the Cognitive Complexity of Questions and Engagement\n  in Online Educational Videos from Learners' Perspectives", "abstract": "Questioning is a fundamental aspect of education, as it helps assess\nstudents' understanding, promotes critical thinking, and encourages active\nengagement. With the rise of artificial intelligence in education, there is a\ngrowing interest in developing intelligent systems that can automatically\ngenerate and answer questions and facilitate interactions in both virtual and\nin-person education settings. However, to develop effective AI models for\neducation, it is essential to have a fundamental understanding of questioning.\nIn this study, we created the YouTube Learners' Questions on Bloom's Taxonomy\nDataset (YouLeQD), which contains learner-posed questions from YouTube lecture\nvideo comments. Along with the dataset, we developed two RoBERTa-based\nclassification models leveraging Large Language Models to detect questions and\nanalyze their cognitive complexity using Bloom's Taxonomy. This dataset and our\nfindings provide valuable insights into the cognitive complexity of\nlearner-posed questions in educational videos and their relationship with\ninteraction metrics. This can aid in the development of more effective AI\nmodels for education and improve the overall learning experience for students.", "published": "2025-01-20 19:54:38", "link": "http://arxiv.org/abs/2501.11712v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reasoning Language Models: A Blueprint", "abstract": "Reasoning language models (RLMs), also known as Large Reasoning Models\n(LRMs), such as OpenAI's o1 and o3, DeepSeek-V3, and Alibaba's QwQ, have\nredefined AI's problem-solving capabilities by extending LLMs with advanced\nreasoning mechanisms. Yet, their high costs, proprietary nature, and complex\narchitectures - uniquely combining Reinforcement Learning (RL), search\nheuristics, and LLMs - present accessibility and scalability challenges. To\naddress these, we propose a comprehensive blueprint that organizes RLM\ncomponents into a modular framework, based on a survey and analysis of all RLM\nworks. This blueprint incorporates diverse reasoning structures (chains, trees,\ngraphs, and nested forms), reasoning strategies (e.g., Monte Carlo Tree Search,\nBeam Search), RL concepts (policy, value models and others), supervision\nschemes (Outcome-Based and Process-Based Supervision), and other related\nconcepts (e.g., Test-Time Compute, Retrieval-Augmented Generation, agent\ntools). We also provide detailed mathematical formulations and algorithmic\nspecifications to simplify RLM implementation. By showing how schemes like\nLLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as special cases,\nwe demonstrate the blueprint's versatility and unifying potential. To\nillustrate its utility, we introduce x1, a modular implementation for rapid RLM\nprototyping and experimentation. Using x1 and a literature review, we provide\nkey insights, such as multi-phase training for policy and value models, and the\nimportance of familiar training distributions. Finally, we discuss scalable RLM\ncloud deployments and we outline how RLMs can integrate with a broader LLM\necosystem. Our work demystifies RLM construction, democratizes advanced\nreasoning capabilities, and fosters innovation, aiming to mitigate the gap\nbetween \"rich AI\" and \"poor AI\" by lowering barriers to RLM design and\nexperimentation.", "published": "2025-01-20 02:16:19", "link": "http://arxiv.org/abs/2501.11223v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Question-to-Question Retrieval for Hallucination-Free Knowledge Access:\n  An Approach for Wikipedia and Wikidata Question Answering", "abstract": "This paper introduces an approach to question answering over knowledge bases\nlike Wikipedia and Wikidata by performing \"question-to-question\" matching and\nretrieval from a dense vector embedding store. Instead of embedding document\ncontent, we generate a comprehensive set of questions for each logical content\nunit using an instruction-tuned LLM. These questions are vector-embedded and\nstored, mapping to the corresponding content. Vector embedding of user queries\nare then matched against this question vector store. The highest similarity\nscore leads to direct retrieval of the associated article content, eliminating\nthe need for answer generation. Our method achieves high cosine similarity ( >\n0.9 ) for relevant question pairs, enabling highly precise retrieval. This\napproach offers several advantages including computational efficiency, rapid\nresponse times, and increased scalability. We demonstrate its effectiveness on\nWikipedia and Wikidata, including multimedia content through structured fact\nretrieval from Wikidata, opening up new pathways for multimodal question\nanswering.", "published": "2025-01-20 07:05:15", "link": "http://arxiv.org/abs/2501.11301v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Few-shot Policy (de)composition in Conversational Question Answering", "abstract": "The task of policy compliance detection (PCD) is to determine if a scenario\nis in compliance with respect to a set of written policies. In a conversational\nsetting, the results of PCD can indicate if clarifying questions must be asked\nto determine compliance status. Existing approaches usually claim to have\nreasoning capabilities that are latent or require a large amount of annotated\ndata. In this work, we propose logical decomposition for policy compliance\n(LDPC): a neuro-symbolic framework to detect policy compliance using large\nlanguage models (LLMs) in a few-shot setting. By selecting only a few exemplars\nalongside recently developed prompting techniques, we demonstrate that our\napproach soundly reasons about policy compliance conversations by extracting\nsub-questions to be answered, assigning truth values from contextual\ninformation, and explicitly producing a set of logic statements from the given\npolicies. The formulation of explicit logic graphs can in turn help answer\nPCDrelated questions with increased transparency and explainability. We apply\nthis approach to the popular PCD and conversational machine reading benchmark,\nShARC, and show competitive performance with no task-specific finetuning. We\nalso leverage the inherently interpretable architecture of LDPC to understand\nwhere errors occur, revealing ambiguities in the ShARC dataset and highlighting\nthe challenges involved with reasoning for conversational question answering.", "published": "2025-01-20 08:40:15", "link": "http://arxiv.org/abs/2501.11335v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Neural Contextual Reinforcement Framework for Logical Structure Language\n  Generation", "abstract": "The Neural Contextual Reinforcement Framework introduces an innovative\napproach to enhancing the logical coherence and structural consistency of text\ngenerated by large language models. Leveraging reinforcement learning\nprinciples, the framework integrates custom reward functions and dynamic\ncontext alignment mechanisms to address challenges inherent in maintaining\nlong-range dependencies across extended sequences. The architecture\nincorporates multi-head attention layers and hierarchical encoding modules,\nenabling the model to produce outputs that align closely with human\nexpectations of logical structure and semantic flow. Quantitative evaluations\nacross diverse datasets demonstrate substantial improvements in coherence\nmetrics, perplexity reduction, and semantic alignment, showcasing the\nframework's ability to outperform baseline models in both general and\ndomain-specific tasks. Qualitative analyses further highlight the framework's\ncapacity to generate text with improved narrative clarity and reduced\nredundancy, reflecting its effectiveness in balancing fluency with structural\nprecision. In addition to its performance gains, the framework exhibits\nrobustness in handling noisy input data and scalability across varying model\nsizes, reinforcing its versatility in practical applications. Experimental\nresults reveal that optimal context window sizes significantly influence\ncoherence outcomes, showing the importance of architectural flexibility in\nadapting to diverse linguistic structures. Cross-lingual performance\nevaluations affirm the framework's adaptability to multiple languages,\nextending its utility beyond monolingual contexts. Resource efficiency analyses\nindicate a reduction in computational overhead compared to traditional\napproaches, emphasizing the practicality of the framework for large-scale\ndeployment.", "published": "2025-01-20 11:34:28", "link": "http://arxiv.org/abs/2501.11417v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Ontology Matching with Large Language Models and Prioritized Depth-First\n  Search", "abstract": "Ontology matching (OM) plays a key role in enabling data interoperability and\nknowledge sharing, but it remains challenging due to the need for large\ntraining datasets and limited vocabulary processing in machine learning\napproaches. Recently, methods based on Large Language Model (LLMs) have shown\ngreat promise in OM, particularly through the use of a retrieve-then-prompt\npipeline. In this approach, relevant target entities are first retrieved and\nthen used to prompt the LLM to predict the final matches. Despite their\npotential, these systems still present limited performance and high\ncomputational overhead. To address these issues, we introduce MILA, a novel\napproach that embeds a retrieve-identify-prompt pipeline within a prioritized\ndepth-first search (PDFS) strategy. This approach efficiently identifies a\nlarge number of semantic correspondences with high accuracy, limiting LLM\nrequests to only the most borderline cases. We evaluated MILA using the\nbiomedical challenge proposed in the 2023 and 2024 editions of the Ontology\nAlignment Evaluation Initiative. Our method achieved the highest F-Measure in\nfour of the five unsupervised tasks, outperforming state-of-the-art OM systems\nby up to 17%. It also performed better than or comparable to the leading\nsupervised OM systems. MILA further exhibited task-agnostic performance,\nremaining stable across all tasks and settings, while significantly reducing\nLLM requests. These findings highlight that high-performance LLM-based OM can\nbe achieved through a combination of programmed (PDFS), learned (embedding\nvectors), and prompting-based heuristics, without the need of domain-specific\nheuristics or fine-tuning.", "published": "2025-01-20 12:29:09", "link": "http://arxiv.org/abs/2501.11441v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "SR-FoT: A Syllogistic-Reasoning Framework of Thought for Large Language\n  Models Tackling Knowledge-based Reasoning Tasks", "abstract": "Deductive reasoning is a crucial logical capability that assists us in\nsolving complex problems based on existing knowledge. Although augmented by\nChain-of-Thought prompts, Large Language Models (LLMs) might not follow the\ncorrect reasoning paths. Enhancing the deductive reasoning abilities of LLMs,\nand leveraging their extensive built-in knowledge for various reasoning tasks,\nremains an open question. Attempting to mimic the human deductive reasoning\nparadigm, we propose a multi-stage Syllogistic-Reasoning Framework of Thought\n(SR-FoT) that enables LLMs to perform syllogistic deductive reasoning to handle\ncomplex knowledge-based reasoning tasks. Our SR-FoT begins by interpreting the\nquestion and then uses the interpretation and the original question to propose\na suitable major premise. It proceeds by generating and answering minor premise\nquestions in two stages to match the minor premises. Finally, it guides LLMs to\nuse the previously generated major and minor premises to perform syllogistic\ndeductive reasoning to derive the answer to the original question. Extensive\nand thorough experiments on knowledge-based reasoning tasks have demonstrated\nthe effectiveness and advantages of our SR-FoT.", "published": "2025-01-20 17:00:41", "link": "http://arxiv.org/abs/2501.11599v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Trojan Detection Through Pattern Recognition for Large Language Models", "abstract": "Trojan backdoors can be injected into large language models at various\nstages, including pretraining, fine-tuning, and in-context learning, posing a\nsignificant threat to the model's alignment. Due to the nature of causal\nlanguage modeling, detecting these triggers is challenging given the vast\nsearch space. In this study, we propose a multistage framework for detecting\nTrojan triggers in large language models consisting of token filtration,\ntrigger identification, and trigger verification. We discuss existing trigger\nidentification methods and propose two variants of a black-box trigger\ninversion method that rely on output logits, utilizing beam search and greedy\ndecoding respectively. We show that the verification stage is critical in the\nprocess and propose semantic-preserving prompts and special perturbations to\ndifferentiate between actual Trojan triggers and other adversarial strings that\ndisplay similar characteristics. The evaluation of our approach on the TrojAI\nand RLHF poisoned model datasets demonstrates promising results.", "published": "2025-01-20 17:36:04", "link": "http://arxiv.org/abs/2501.11621v1", "categories": ["cs.CL", "cs.LG", "68T10, 68T20", "I.2; I.5"], "primary_category": "cs.CL"}
{"title": "StAyaL | Multilingual Style Transfer", "abstract": "Stylistic text generation plays a vital role in enhancing communication by\nreflecting the nuances of individual expression. This paper presents a novel\napproach for generating text in a specific speaker's style across different\nlanguages. We show that by leveraging only 100 lines of text, an individuals\nunique style can be captured as a high-dimensional embedding, which can be used\nfor both text generation and stylistic translation. This methodology breaks\ndown the language barrier by transferring the style of a speaker between\nlanguages. The paper is structured into three main phases: augmenting the\nspeaker's data with stylistically consistent external sources, separating style\nfrom content using machine learning and deep learning techniques, and\ngenerating an abstract style profile by mean pooling the learned embeddings.\nThe proposed approach is shown to be topic-agnostic, with test accuracy and F1\nscores of 74.9% and 0.75, respectively. The results demonstrate the potential\nof the style profile for multilingual communication, paving the way for further\napplications in personalized content generation and cross-linguistic stylistic\ntransfer.", "published": "2025-01-20 18:13:18", "link": "http://arxiv.org/abs/2501.11639v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Advancing Language Model Reasoning through Reinforcement Learning and\n  Inference Scaling", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\ncomplex reasoning tasks. However, existing approaches mainly rely on imitation\nlearning and struggle to achieve effective test-time scaling. While\nreinforcement learning (RL) holds promise for enabling self-exploration and\nlearning from feedback, recent attempts yield only modest improvements in\ncomplex reasoning. In this paper, we present T1 to scale RL by encouraging\nexploration and understand inference scaling. We first initialize the LLM using\nsynthesized chain-of-thought data that integrates trial-and-error and\nself-verification. To scale RL training, we promote increased sampling\ndiversity through oversampling. We further employ an entropy bonus as an\nauxiliary loss, alongside a dynamic anchor for regularization to facilitate\nreward optimization. We demonstrate that T1 with open LLMs as its base exhibits\ninference scaling behavior and achieves superior performance on challenging\nmath reasoning benchmarks. For example, T1 with Qwen2.5-32B as the base model\noutperforms the recent Qwen QwQ-32B-Preview model on MATH500, AIME2024, and\nOmni-math-500. More importantly, we present a simple strategy to examine\ninference scaling, where increased inference budgets directly lead to T1's\nbetter performance without any additional verification. We will open-source the\nT1 models and the data used to train them at \\url{https://github.com/THUDM/T1}.", "published": "2025-01-20 18:33:33", "link": "http://arxiv.org/abs/2501.11651v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Explain-Query-Test: Self-Evaluating LLMs Via Explanation and\n  Comprehension Discrepancy", "abstract": "Large language models (LLMs) have demonstrated remarkable proficiency in\ngenerating detailed and coherent explanations of complex concepts. However, the\nextent to which these models truly comprehend the concepts they articulate\nremains unclear. To assess the level of comprehension of a model relative to\nthe content it generates, we implemented a self-evaluation pipeline where\nmodels: (i) given a topic generate an excerpt with information about the topic,\n(ii) given an excerpt generate question-answer pairs, and finally (iii) given a\nquestion generate an answer. We refer to this self-evaluation approach as\nExplain-Query-Test (EQT). Interestingly, the accuracy on generated questions\nresulting from running the EQT pipeline correlates strongly with the model\nperformance as verified by typical benchmarks such as MMLU-Pro. In other words,\nEQT's performance is predictive of MMLU-Pro's, and EQT can be used to rank\nmodels without the need for any external source of evaluation data other than\nlists of topics of interest. Moreover, our results reveal a disparity between\nthe models' ability to produce detailed explanations and their performance on\nquestions related to those explanations. This gap highlights fundamental\nlimitations in the internal knowledge representation and reasoning abilities of\ncurrent LLMs. We release the code at https://github.com/asgsaeid/EQT.", "published": "2025-01-20 20:07:18", "link": "http://arxiv.org/abs/2501.11721v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks", "abstract": "Smartphones have become indispensable in modern life, yet navigating complex\ntasks on mobile devices often remains frustrating. Recent advancements in large\nmultimodal model (LMM)-based mobile agents have demonstrated the ability to\nperceive and act in mobile environments. However, current approaches face\nsignificant limitations: they fall short in addressing real-world human needs,\nstruggle with reasoning-intensive and long-horizon tasks, and lack mechanisms\nto learn and improve from prior experiences. To overcome these challenges, we\nintroduce Mobile-Agent-E, a hierarchical multi-agent framework capable of\nself-evolution through past experience. By hierarchical, we mean an explicit\nseparation of high-level planning and low-level action execution. The framework\ncomprises a Manager, responsible for devising overall plans by breaking down\ncomplex tasks into subgoals, and four subordinate agents--Perceptor, Operator,\nAction Reflector, and Notetaker--which handle fine-grained visual perception,\nimmediate action execution, error verification, and information aggregation,\nrespectively. Mobile-Agent-E also features a novel self-evolution module which\nmaintains a persistent long-term memory comprising Tips and Shortcuts. Tips are\ngeneral guidance and lessons learned from prior tasks on how to effectively\ninteract with the environment. Shortcuts are reusable, executable sequences of\natomic operations tailored for specific subroutines. The inclusion of Tips and\nShortcuts facilitates continuous refinement in performance and efficiency.\nAlongside this framework, we introduce Mobile-Eval-E, a new benchmark featuring\ncomplex mobile tasks requiring long-horizon, multi-app interactions. Empirical\nresults show that Mobile-Agent-E achieves a 22% absolute improvement over\nprevious state-of-the-art approaches across three foundation model backbones.\nProject page: https://x-plug.github.io/MobileAgent.", "published": "2025-01-20 20:35:46", "link": "http://arxiv.org/abs/2501.11733v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Optimizing Pretraining Data Mixtures with LLM-Estimated Utility", "abstract": "Large Language Models improve with increasing amounts of high-quality\ntraining data. However, leveraging larger datasets requires balancing quality,\nquantity, and diversity across sources. After evaluating nine baseline methods\nunder both compute- and data-constrained scenarios, we find token-count\nheuristics outperform manual and learned mixes, indicating that simple\napproaches accounting for dataset size and diversity are surprisingly\neffective. Building on this insight, we propose two complementary approaches:\nUtiliMax, which extends token-based heuristics by incorporating utility\nestimates from reduced-scale ablations, achieving up to a 10.6x speedup over\nmanual baselines; and Model Estimated Data Utility (MEDU), which leverages LLMs\nto estimate data utility from small samples, matching ablation-based\nperformance while reducing computational requirements by $\\sim$200x. Together,\nthese approaches establish a new framework for automated, compute-efficient\ndata mixing that is robust across training regimes.", "published": "2025-01-20 21:10:22", "link": "http://arxiv.org/abs/2501.11747v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Benchmarking Large Language Models via Random Variables", "abstract": "Recent studies have raised concerns about the reliability of current\nmathematical benchmarks, highlighting issues such as simplistic design and\npotential data contamination. Therefore, creating a reliable benchmark that\neffectively evaluates the genuine capabilities of large language models (LLMs)\nin mathematical reasoning remains a significant challenge. To address this, we\npropose RV-Bench, a framework for Benchmarking LLMs via Random Variables in\nmathematical reasoning. Specifically, the background content of a random\nvariable question (RV question) mirrors the original problem in existing\nbenchmarks, but the variable combinations are randomized, making it \"unseen\" by\nthe LLMs. Models must completely understand the question pattern of the\noriginal problem to correctly answer RV questions with various variable values.\nAs a result, the LLM's genuine capability in mathematical reasoning is\nreflected by its accuracy and robustness on RV-Bench. We conducted extensive\nexperiments on over 30 representative LLMs across more than 1000 RV questions.\nOur findings suggest that LLMs exhibit an imbalance in proficiency between\nencountered and \"unseen\" data domains. Proficiency generalization across\nsimilar mathematical reasoning tasks is verified to be limited by accuracy and\nrobustness, but it can still be enhanced through test-time scaling.", "published": "2025-01-20 23:41:22", "link": "http://arxiv.org/abs/2501.11790v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MyGO Multiplex CoT: A Method for Self-Reflection in Large Language\n  Models via Double Chain of Thought Thinking", "abstract": "Recent advancements in large language models (LLMs) have demonstrated their\nimpressive abilities in various reasoning and decision-making tasks. However,\nthe quality and coherence of the reasoning process can still benefit from\nenhanced introspection and self-reflection. In this paper, we introduce\nMultiplex CoT (Chain of Thought), a method that enables LLMs to simulate a form\nof self-review while reasoning, by initiating double Chain of Thought (CoT)\nthinking. Multiplex CoT leverages the power of iterative reasoning, where the\nmodel generates an initial chain of thought and subsequently critiques and\nrefines this reasoning with a second round of thought generation. This\nrecursive approach allows for more coherent, logical, and robust answers,\nimproving the overall decision-making process. We demonstrate how this method\ncan be effectively implemented using simple prompt engineering in existing LLM\narchitectures, achieving an effect similar to that of the Learning-Refinement\nModel (LRM) without the need for additional training. Additionally, we present\na practical guide for implementing the method in Google Colab, enabling easy\nintegration into real-world applications.", "published": "2025-01-20 12:54:57", "link": "http://arxiv.org/abs/2501.13117v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Longitudinal Abuse and Sentiment Analysis of Hollywood Movie Dialogues\n  using LLMs", "abstract": "Over the past decades, there has been an increasing concern about the\nprevalence of abusive and violent content in Hollywood movies. This study uses\nLarge Language Models (LLMs) to explore the longitudinal abuse and sentiment\nanalysis of Hollywood Oscar and blockbuster movie dialogues from 1950 to 2024.\nBy employing fine-tuned LLMs, we analyze subtitles for over a thousand movies\ncategorised into four genres to examine the trends and shifts in emotional and\nabusive content over the past seven decades. Our findings reveal significant\ntemporal changes in movie dialogues, which reflect broader social and cultural\ninfluences. Overall, the emotional tendencies in the films are diverse, and the\ndetection of abusive content also exhibits significant fluctuations. The\nresults show a gradual rise in abusive content in recent decades, reflecting\nsocial norms and regulatory policy changes. Genres such as thrillers still\npresent a higher frequency of abusive content that emphasises the ongoing\nnarrative role of violence and conflict. At the same time, underlying positive\nemotions such as humour and optimism remain prevalent in most of the movies.\nFurthermore, the gradual increase of abusive content in movie dialogues has\nbeen significant over the last two decades, where Oscar-nominated movies\novertook the top ten blockbusters.", "published": "2025-01-20 00:44:38", "link": "http://arxiv.org/abs/2501.13948v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can OpenAI o1 Reason Well in Ophthalmology? A 6,990-Question\n  Head-to-Head Evaluation Study", "abstract": "Question: What is the performance and reasoning ability of OpenAI o1 compared\nto other large language models in addressing ophthalmology-specific questions?\n  Findings: This study evaluated OpenAI o1 and five LLMs using 6,990\nophthalmological questions from MedMCQA. O1 achieved the highest accuracy\n(0.88) and macro-F1 score but ranked third in reasoning capabilities based on\ntext-generation metrics. Across subtopics, o1 ranked first in ``Lens'' and\n``Glaucoma'' but second to GPT-4o in ``Corneal and External Diseases'',\n``Vitreous and Retina'' and ``Oculoplastic and Orbital Diseases''. Subgroup\nanalyses showed o1 performed better on queries with longer ground truth\nexplanations.\n  Meaning: O1's reasoning enhancements may not fully extend to ophthalmology,\nunderscoring the need for domain-specific refinements to optimize performance\nin specialized fields like ophthalmology.", "published": "2025-01-20 02:40:01", "link": "http://arxiv.org/abs/2501.13949v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Layered Multi-Expert Framework for Long-Context Mental Health\n  Assessments", "abstract": "Long-form mental health assessments pose unique challenges for large language\nmodels (LLMs), which often exhibit hallucinations or inconsistent reasoning\nwhen handling extended, domain-specific contexts. We introduce Stacked\nMulti-Model Reasoning (SMMR), a layered framework that leverages multiple LLMs\nand specialized smaller models as coequal 'experts'. Early layers isolate\nshort, discrete subtasks, while later layers integrate and refine these partial\noutputs through more advanced long-context models. We evaluate SMMR on the\nDAIC-WOZ depression-screening dataset and 48 curated case studies with\npsychiatric diagnoses, demonstrating consistent improvements over single-model\nbaselines in terms of accuracy, F1-score, and PHQ-8 error reduction. By\nharnessing diverse 'second opinions', SMMR mitigates hallucinations, captures\nsubtle clinical nuances, and enhances reliability in high-stakes mental health\nassessments. Our findings underscore the value of multi-expert frameworks for\nmore trustworthy AI-driven screening.", "published": "2025-01-20 03:22:19", "link": "http://arxiv.org/abs/2501.13951v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Dual-use Dilemma in LLMs: Do Empowering Ethical Capacities Make a\n  Degraded Utility?", "abstract": "Recent years have witnessed extensive efforts to enhance Large Language\nModels (LLMs) across various domains, alongside growing attention to their\nethical implications. However, a critical challenge remains largely overlooked:\nLLMs must balance between rejecting harmful requests for safety and\naccommodating legitimate ones for utility. This paper presents a Direct\nPreference Optimization (DPO) based alignment framework that achieves better\noverall performance by addressing this ethical-utility trade-off, using\nchemical domain applications as a proof-of-concept. Our alignment pipeline\nstarts with a GPT-assisted three-phase data generation scheme, in which we\ncreate LibraChemQA, a chemical question-answering dataset comprising 31.6k\ntriplet instances. By incorporating an innovative balanced seed in the data\ngeneration process, our framework systematically considers both legitimate and\nillegitimate requests. The framework also introduces a rephrasing mechanism for\nefficient data augmentation that enhances the model's chemical comprehension.\nWe further develop a novel hybrid evaluation scheme with LLM judges for precise\nassessment of both safety and utility. Experimental results demonstrate our\nmodel's substantial improvements in overall performance where both safety and\nutility are considered - the resulting model outperforms leading LLMs including\nClaude-3, GPT-4o, and LLaMA-3 by margins of 13.44%, 7.16%, and 7.10%\nrespectively on our released benchmark. At the end of this paper, we analyze\nexperimental results obtained from testing DeepSeek-R1 on our benchmark and\nreveal the critical ethical concerns raised by this highly acclaimed model. We\nhighlight that the long Chain-of-Thought (CoT) reasoning process employed by\nDeepSeek-R1, as well as other LLMs distilled from it, introduces significant\nethical vulnerabilities when exposed to users.", "published": "2025-01-20 06:35:01", "link": "http://arxiv.org/abs/2501.13952v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Redundancy Principles for MLLMs Benchmarks", "abstract": "With the rapid iteration of Multi-modality Large Language Models (MLLMs) and\nthe evolving demands of the field, the number of benchmarks produced annually\nhas surged into the hundreds. The rapid growth has inevitably led to\nsignificant redundancy among benchmarks. Therefore, it is crucial to take a\nstep back and critically assess the current state of redundancy and propose\ntargeted principles for constructing effective MLLM benchmarks. In this paper,\nwe focus on redundancy from three key perspectives: 1) Redundancy of benchmark\ncapability dimensions, 2) Redundancy in the number of test questions, and 3)\nCross-benchmark redundancy within specific domains. Through the comprehensive\nanalysis over hundreds of MLLMs' performance across more than 20 benchmarks, we\naim to quantitatively measure the level of redundancy lies in existing MLLM\nevaluations, provide valuable insights to guide the future development of MLLM\nbenchmarks, and offer strategies to refine and address redundancy issues\neffectively.", "published": "2025-01-20 08:09:42", "link": "http://arxiv.org/abs/2501.13953v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PlotEdit: Natural Language-Driven Accessible Chart Editing in PDFs via\n  Multimodal LLM Agents", "abstract": "Chart visualizations, while essential for data interpretation and\ncommunication, are predominantly accessible only as images in PDFs, lacking\nsource data tables and stylistic information. To enable effective editing of\ncharts in PDFs or digital scans, we present PlotEdit, a novel multi-agent\nframework for natural language-driven end-to-end chart image editing via\nself-reflective LLM agents. PlotEdit orchestrates five LLM agents: (1)\nChart2Table for data table extraction, (2) Chart2Vision for style attribute\nidentification, (3) Chart2Code for retrieving rendering code, (4) Instruction\nDecomposition Agent for parsing user requests into executable steps, and (5)\nMultimodal Editing Agent for implementing nuanced chart component modifications\n- all coordinated through multimodal feedback to maintain visual fidelity.\nPlotEdit outperforms existing baselines on the ChartCraft dataset across style,\nlayout, format, and data-centric edits, enhancing accessibility for visually\nchallenged users and improving novice productivity.", "published": "2025-01-20 02:31:52", "link": "http://arxiv.org/abs/2501.11233v1", "categories": ["cs.IR", "cs.CL", "cs.MA"], "primary_category": "cs.IR"}
{"title": "Irony in Emojis: A Comparative Study of Human and LLM Interpretation", "abstract": "Emojis have become a universal language in online communication, often\ncarrying nuanced and context-dependent meanings. Among these, irony poses a\nsignificant challenge for Large Language Models (LLMs) due to its inherent\nincongruity between appearance and intent. This study examines the ability of\nGPT-4o to interpret irony in emojis. By prompting GPT-4o to evaluate the\nlikelihood of specific emojis being used to express irony on social media and\ncomparing its interpretations with human perceptions, we aim to bridge the gap\nbetween machine and human understanding. Our findings reveal nuanced insights\ninto GPT-4o's interpretive capabilities, highlighting areas of alignment with\nand divergence from human behavior. Additionally, this research underscores the\nimportance of demographic factors, such as age and gender, in shaping emoji\ninterpretation and evaluates how these factors influence GPT-4o's performance.", "published": "2025-01-20 03:02:00", "link": "http://arxiv.org/abs/2501.11241v1", "categories": ["cs.CL", "cs.CV", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Code Readability in the Age of Large Language Models: An Industrial Case\n  Study from Atlassian", "abstract": "Programmers spend a significant amount of time reading code during the\nsoftware development process. This trend is amplified by the emergence of large\nlanguage models (LLMs) that automatically generate code. However, little is\nknown about the readability of the LLM-generated code and whether it is still\nimportant from practitioners' perspectives in this new era. In this paper, we\nconduct a survey to explore the practitioners' perspectives on code readability\nin the age of LLMs and investigate the readability of our LLM-based software\ndevelopment agents framework, HULA, by comparing its generated code with\nhuman-written code in real-world scenarios. Overall, the findings underscore\nthat (1) readability remains a critical aspect of software development; (2) the\nreadability of our LLM-generated code is comparable to human-written code,\nfostering the establishment of appropriate trust and driving the broad adoption\nof our LLM-powered software development platform.", "published": "2025-01-20 04:11:21", "link": "http://arxiv.org/abs/2501.11264v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "RedStar: Does Scaling Long-CoT Data Unlock Better Slow-Reasoning\n  Systems?", "abstract": "Can scaling transform reasoning? In this work, we explore the untapped\npotential of scaling Long Chain-of-Thought (Long-CoT) data to 1000k samples,\npioneering the development of a slow-thinking model, RedStar. Through extensive\nexperiments with various LLMs and different sizes, we uncover the ingredients\nfor specialization and scale for Long-CoT training. Surprisingly, even smaller\nmodels show significant performance gains with limited data, revealing the\nsample efficiency of Long-CoT and the critical role of sample difficulty in the\nlearning process. Our findings demonstrate that Long-CoT reasoning can be\neffectively triggered with just a few thousand examples, while larger models\nachieve unparalleled improvements. We also introduce reinforcement learning\n(RL)-scale training as a promising direction for advancing slow-thinking\nsystems. RedStar shines across domains: on the MATH-Hard benchmark,\nRedStar-code-math boosts performance from 66.2\\% to 81.6\\%, and on the USA Math\nOlympiad (AIME), it solves 46.7\\% of problems using only 21k mixed-code-math\ndatasets. In multimodal tasks like GeoQA and MathVista-GEO, RedStar-Geo\nachieves competitive results with minimal Long-CoT data, outperforming other\nslow-thinking systems like QvQ-Preview. Compared to QwQ, RedStar strikes the\nperfect balance between reasoning and generalizability. Our work highlights\nthat, with careful tuning, scaling Long-CoT can unlock extraordinary reasoning\ncapabilities-even with limited dataset and set a new standard for slow-thinking\nmodels across diverse challenges. Our data and models are released at\nhttps://huggingface.co/RedStar-Reasoning.", "published": "2025-01-20 05:44:01", "link": "http://arxiv.org/abs/2501.11284v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Verifying Cross-modal Entity Consistency in News using Vision-language\n  Models", "abstract": "The web has become a crucial source of information, but it is also used to\nspread disinformation, often conveyed through multiple modalities like images\nand text. The identification of inconsistent cross-modal information, in\nparticular entities such as persons, locations, and events, is critical to\ndetect disinformation. Previous works either identify out-of-context\ndisinformation by assessing the consistency of images to the whole document,\nneglecting relations of individual entities, or focus on generic entities that\nare not relevant to news. So far, only few approaches have addressed the task\nof validating entity consistency between images and text in news. However, the\npotential of large vision-language models (LVLMs) has not been explored yet. In\nthis paper, we propose an LVLM-based framework for verifying Cross-modal Entity\nConsistency~(LVLM4CEC), to assess whether persons, locations and events in news\narticles are consistent across both modalities. We suggest effective prompting\nstrategies for LVLMs for entity verification that leverage reference images\ncrawled from web. Moreover, we extend three existing datasets for the task of\nentity verification in news providing manual ground-truth data. Our results\nshow the potential of LVLMs for automating cross-modal entity verification,\nshowing improved accuracy in identifying persons and events when using evidence\nimages. Moreover, our method outperforms a baseline for location and event\nverification in documents. The datasets and source code are available on GitHub\nat https://github.com/TIBHannover/LVLM4CEC.", "published": "2025-01-20 11:06:05", "link": "http://arxiv.org/abs/2501.11403v2", "categories": ["cs.CL", "cs.IR", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Each Graph is a New Language: Graph Learning with LLMs", "abstract": "Recent efforts leverage Large Language Models (LLMs) for modeling\ntext-attributed graph structures in node classification tasks. These approaches\ndescribe graph structures for LLMs to understand or aggregate LLM-generated\ntextual attribute embeddings through graph structure. However, these approaches\nface two main limitations in modeling graph structures with LLMs. (i) Graph\ndescriptions become verbose in describing high-order graph structure. (ii)\nTextual attributes alone do not contain adequate graph structure information.\nIt is challenging to model graph structure concisely and adequately with LLMs.\nLLMs lack built-in mechanisms to model graph structures directly. They also\nstruggle with complex long-range dependencies between high-order nodes and\ntarget nodes.\n  Inspired by the observation that LLMs pre-trained on one language can achieve\nexceptional performance on another with minimal additional training, we propose\n\\textbf{G}raph-\\textbf{D}efined \\textbf{L}anguage for \\textbf{L}arge\n\\textbf{L}anguage \\textbf{M}odel (GDL4LLM). This novel framework enables LLMs\nto transfer their powerful language understanding capabilities to\ngraph-structured data. GDL4LLM translates graphs into a graph language corpus\ninstead of graph descriptions and pre-trains LLMs on this corpus to adequately\nunderstand graph structures. During fine-tuning, this corpus describes the\nstructural information of target nodes concisely with only a few tokens. By\ntreating graphs as a new language, GDL4LLM enables LLMs to model graph\nstructures adequately and concisely for node classification tasks. Extensive\nexperiments on three real-world datasets demonstrate that GDL4LLM outperforms\ndescription-based and textual attribute embeddings-based baselines by\nefficiently modeling different orders of graph structure with LLMs.", "published": "2025-01-20 13:20:41", "link": "http://arxiv.org/abs/2501.11478v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generative AI and Large Language Models in Language Preservation:\n  Opportunities and Challenges", "abstract": "Generative AI and large-scale language models (LLM) have emerged as powerful\ntools in language preservation, particularly for near-native and endangered\nlanguages. With the increasing reliance on technology for communication,\neducation, and cultural documentation, new opportunities have emerged to\nmitigate the dramatic decline of linguistic diversity worldwide. This paper\nexamines the role of generative AIs and LLMs in preserving endangered\nlanguages, highlighting the risks and challenges associated with their use. We\nanalyze the underlying technologies driving these models, including natural\nlanguage processing (NLP) and deep learning, and explore several cases where\nthese technologies have been applied to low-resource languages. Additionally,\nwe discuss ethical considerations, data scarcity issues, and technical\nchallenges while proposing solutions to enhance AI-driven language\npreservation.", "published": "2025-01-20 14:03:40", "link": "http://arxiv.org/abs/2501.11496v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50, 91F20", "I.2.7; I.2.6; J.5"], "primary_category": "cs.CL"}
{"title": "Dialect2SQL: A Novel Text-to-SQL Dataset for Arabic Dialects with a\n  Focus on Moroccan Darija", "abstract": "The task of converting natural language questions (NLQs) into executable SQL\nqueries, known as text-to-SQL, has gained significant interest in recent years,\nas it enables non-technical users to interact with relational databases. Many\nbenchmarks, such as SPIDER and WikiSQL, have contributed to the development of\nnew models and the evaluation of their performance. In addition, other\ndatasets, like SEDE and BIRD, have introduced more challenges and complexities\nto better map real-world scenarios. However, these datasets primarily focus on\nhigh-resource languages such as English and Chinese. In this work, we introduce\nDialect2SQL, the first large-scale, cross-domain text-to-SQL dataset in an\nArabic dialect. It consists of 9,428 NLQ-SQL pairs across 69 databases in\nvarious domains. Along with SQL-related challenges such as long schemas, dirty\nvalues, and complex queries, our dataset also incorporates the complexities of\nthe Moroccan dialect, which is known for its diverse source languages, numerous\nborrowed words, and unique expressions. This demonstrates that our dataset will\nbe a valuable contribution to both the text-to-SQL community and the\ndevelopment of resources for low-resource languages.", "published": "2025-01-20 14:06:40", "link": "http://arxiv.org/abs/2501.11498v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.DB"], "primary_category": "cs.SE"}
{"title": "Explainable Lane Change Prediction for Near-Crash Scenarios Using\n  Knowledge Graph Embeddings and Retrieval Augmented Generation", "abstract": "Lane-changing maneuvers, particularly those executed abruptly or in risky\nsituations, are a significant cause of road traffic accidents. However, current\nresearch mainly focuses on predicting safe lane changes. Furthermore, existing\naccident datasets are often based on images only and lack comprehensive sensory\ndata. In this work, we focus on predicting risky lane changes using the CRASH\ndataset (our own collected dataset specifically for risky lane changes), and\nsafe lane changes (using the HighD dataset). Then, we leverage KG and Bayesian\ninference to predict these maneuvers using linguistic contextual information,\nenhancing the model's interpretability and transparency. The model achieved a\n91.5% f1-score with anticipation time extending to four seconds for risky lane\nchanges, and a 90.0% f1-score for predicting safe lane changes with the same\nanticipation time. We validate our model by integrating it into a vehicle\nwithin the CARLA simulator in scenarios that involve risky lane changes. The\nmodel managed to anticipate sudden lane changes, thus providing automated\nvehicles with further time to plan and execute appropriate safe reactions.\nFinally, to enhance the explainability of our model, we utilize RAG to provide\nclear and natural language explanations for the given prediction.", "published": "2025-01-20 16:02:26", "link": "http://arxiv.org/abs/2501.11560v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Training-free Ultra Small Model for Universal Sparse Reconstruction in\n  Compressed Sensing", "abstract": "Pre-trained large models attract widespread attention in recent years, but\nthey face challenges in applications that require high interpretability or have\nlimited resources, such as physical sensing, medical imaging, and\nbioinformatics. Compressed Sensing (CS) is a well-proved theory that drives\nmany recent breakthroughs in these applications. However, as a typical\nunder-determined linear system, CS suffers from excessively long sparse\nreconstruction times when using traditional iterative methods, particularly\nwith large-scale data. Current AI methods like deep unfolding fail to\nsubstitute them because pre-trained models exhibit poor generality beyond their\ntraining conditions and dataset distributions, or lack interpretability.\nInstead of following the big model fervor, this paper proposes ultra-small\nartificial neural models called coefficients learning (CL), enabling\ntraining-free and rapid sparse reconstruction while perfectly inheriting the\ngenerality and interpretability of traditional iterative methods, bringing new\nfeature of incorporating prior knowledges. In CL, a signal of length $n$ only\nneeds a minimal of $n$ trainable parameters. A case study model called CLOMP is\nimplemented for evaluation. Experiments are conducted on both synthetic and\nreal one-dimensional and two-dimensional signals, demonstrating significant\nimprovements in efficiency and accuracy. Compared to representative iterative\nmethods, CLOMP improves efficiency by 100 to 1000 folds for large-scale data.\nTest results on eight diverse image datasets indicate that CLOMP improves\nstructural similarity index by 292%, 98%, 45% for sampling rates of 0.1, 0.3,\n0.5, respectively. We believe this method can truly usher CS reconstruction\ninto the AI era, benefiting countless under-determined linear systems that rely\non sparse solution.", "published": "2025-01-20 16:50:59", "link": "http://arxiv.org/abs/2501.11592v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Biomedical Knowledge Graph: A Survey of Domains, Tasks, and Real-World\n  Applications", "abstract": "Biomedical knowledge graphs (BKGs) have emerged as powerful tools for\norganizing and leveraging the vast and complex data found across the biomedical\nfield. Yet, current reviews of BKGs often limit their scope to specific domains\nor methods, overlooking the broader landscape and the rapid technological\nprogress reshaping it. In this survey, we address this gap by offering a\nsystematic review of BKGs from three core perspectives: domains, tasks, and\napplications. We begin by examining how BKGs are constructed from diverse data\nsources, including molecular interactions, pharmacological datasets, and\nclinical records. Next, we discuss the essential tasks enabled by BKGs,\nfocusing on knowledge management, retrieval, reasoning, and interpretation.\nFinally, we highlight real-world applications in precision medicine, drug\ndiscovery, and scientific research, illustrating the translational impact of\nBKGs across multiple sectors. By synthesizing these perspectives into a unified\nframework, this survey not only clarifies the current state of BKG research but\nalso establishes a foundation for future exploration, enabling both innovative\nmethodological advances and practical implementations.", "published": "2025-01-20 18:02:03", "link": "http://arxiv.org/abs/2501.11632v2", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Is logical analysis performed by transformers taking place in\n  self-attention or in the fully connected part?", "abstract": "Transformers architecture apply self-attention to tokens represented as\nvectors, before a fully connected (neuronal network) layer. These two parts can\nbe layered many times. Traditionally, self-attention is seen as a mechanism for\naggregating information before logical operations are performed by the fully\nconnected layer. In this paper, we show, that quite counter-intuitively, the\nlogical analysis can also be performed within the self-attention. For this we\nimplement a handcrafted single-level encoder layer which performs the logical\nanalysis within self-attention. We then study the scenario in which a one-level\ntransformer model undergoes self-learning using gradient descent. We\ninvestigate whether the model utilizes fully connected layers or self-attention\nmechanisms for logical analysis when it has the choice. Given that gradient\ndescent can become stuck at undesired zeros, we explicitly calculate these\nunwanted zeros and find ways to avoid them. We do all this in the context of\npredicting grammatical category pairs of adjacent tokens in a text. We believe\nthat our findings have broader implications for understanding the potential\nlogical operations performed by self-attention.", "published": "2025-01-20 21:58:35", "link": "http://arxiv.org/abs/2501.11765v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T30", "I.2.4"], "primary_category": "cs.CL"}
{"title": "The Value of Nothing: Multimodal Extraction of Human Values Expressed by\n  TikTok Influencers", "abstract": "Societal and personal values are transmitted to younger generations through\ninteraction and exposure. Traditionally, children and adolescents learned\nvalues from parents, educators, or peers. Nowadays, social platforms serve as a\nsignificant channel through which youth (and adults) consume information, as\nthe main medium of entertainment, and possibly the medium through which they\nlearn different values. In this paper we extract implicit values from TikTok\nmovies uploaded by online influencers targeting children and adolescents. We\ncurated a dataset of hundreds of TikTok movies and annotated them according to\nthe Schwartz Theory of Personal Values. We then experimented with an array of\nMasked and Large language model, exploring how values can be detected.\nSpecifically, we considered two pipelines -- direct extraction of values from\nvideo and a 2-step approach in which videos are first converted to elaborated\nscripts and then values are extracted.\n  Achieving state-of-the-art results, we find that the 2-step approach performs\nsignificantly better than the direct approach and that using a trainable Masked\nLanguage Model as a second step significantly outperforms a few-shot\napplication of a number of Large Language Models. We further discuss the impact\nof fine-tuning and compare the performance of the different models on\nidentification of values present or contradicted in the TikTok. Finally, we\nshare the first values-annotated dataset of TikTok videos. Our results pave the\nway to further research on influence and value transmission in video-based\nsocial platforms.", "published": "2025-01-20 22:21:18", "link": "http://arxiv.org/abs/2501.11770v1", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Synthetic Data Can Mislead Evaluations: Membership Inference as Machine\n  Text Detection", "abstract": "Recent work shows membership inference attacks (MIAs) on large language\nmodels (LLMs) produce inconclusive results, partly due to difficulties in\ncreating non-member datasets without temporal shifts. While researchers have\nturned to synthetic data as an alternative, we show this approach can be\nfundamentally misleading. Our experiments indicate that MIAs function as\nmachine-generated text detectors, incorrectly identifying synthetic data as\ntraining samples regardless of the data source. This behavior persists across\ndifferent model architectures and sizes, from open-source models to commercial\nones such as GPT-3.5. Even synthetic text generated by different, potentially\nlarger models is classified as training data by the target model. Our findings\nhighlight a serious concern: using synthetic data in membership evaluations may\nlead to false conclusions about model memorization and data leakage. We caution\nthat this issue could affect other evaluations using model signals such as loss\nwhere synthetic or machine-generated translated data substitutes for real-world\nsamples.", "published": "2025-01-20 23:19:15", "link": "http://arxiv.org/abs/2501.11786v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multilinguality in LLM-Designed Reward Functions for Restless Bandits:\n  Effects on Task Performance and Fairness", "abstract": "Restless Multi-Armed Bandits (RMABs) have been successfully applied to\nresource allocation problems in a variety of settings, including public health.\nWith the rapid development of powerful large language models (LLMs), they are\nincreasingly used to design reward functions to better match human preferences.\nRecent work has shown that LLMs can be used to tailor automated allocation\ndecisions to community needs using language prompts. However, this has been\nstudied primarily for English prompts and with a focus on task performance\nonly. This can be an issue since grassroots workers, especially in developing\ncountries like India, prefer to work in local languages, some of which are\nlow-resource. Further, given the nature of the problem, biases along population\ngroups unintended by the user are also undesirable. In this work, we study the\neffects on both task performance and fairness when the DLM algorithm, a recent\nwork on using LLMs to design reward functions for RMABs, is prompted with\nnon-English language commands. Specifically, we run the model on a synthetic\nenvironment for various prompts translated into multiple languages. The prompts\nthemselves vary in complexity. Our results show that the LLM-proposed reward\nfunctions are significantly better when prompted in English compared to other\nlanguages. We also find that the exact phrasing of the prompt impacts task\nperformance. Further, as prompt complexity increases, performance worsens for\nall languages; however, it is more robust with English prompts than with\nlower-resource languages. On the fairness side, we find that low-resource\nlanguages and more complex prompts are both highly likely to create unfairness\nalong unintended dimensions.", "published": "2025-01-20 18:14:37", "link": "http://arxiv.org/abs/2501.13120v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Chat3GPP: An Open-Source Retrieval-Augmented Generation Framework for\n  3GPP Documents", "abstract": "The 3rd Generation Partnership Project (3GPP) documents is key standards in\nglobal telecommunications, while posing significant challenges for engineers\nand researchers in the telecommunications field due to the large volume and\ncomplexity of their contents as well as the frequent updates. Large language\nmodels (LLMs) have shown promise in natural language processing tasks, but\ntheir general-purpose nature limits their effectiveness in specific domains\nlike telecommunications. To address this, we propose Chat3GPP, an open-source\nretrieval-augmented generation (RAG) framework tailored for 3GPP\nspecifications. By combining chunking strategies, hybrid retrieval and\nefficient indexing methods, Chat3GPP can efficiently retrieve relevant\ninformation and generate accurate responses to user queries without requiring\ndomain-specific fine-tuning, which is both flexible and scalable, offering\nsignificant potential for adapting to other technical standards beyond 3GPP. We\nevaluate Chat3GPP on two telecom-specific datasets and demonstrate its superior\nperformance compared to existing methods, showcasing its potential for\ndownstream tasks like protocol generation and code automation.", "published": "2025-01-20 11:38:42", "link": "http://arxiv.org/abs/2501.13954v1", "categories": ["cs.CL", "cs.AI", "cs.DC", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Guided Persona-based AI Surveys: Can we replicate personal mobility\n  preferences at scale using LLMs?", "abstract": "This study explores the potential of Large Language Models (LLMs) to generate\nartificial surveys, with a focus on personal mobility preferences in Germany.\nBy leveraging LLMs for synthetic data creation, we aim to address the\nlimitations of traditional survey methods, such as high costs, inefficiency and\nscalability challenges. A novel approach incorporating \"Personas\" -\ncombinations of demographic and behavioural attributes - is introduced and\ncompared to five other synthetic survey methods, which vary in their use of\nreal-world data and methodological complexity. The MiD 2017 dataset, a\ncomprehensive mobility survey in Germany, serves as a benchmark to assess the\nalignment of synthetic data with real-world patterns. The results demonstrate\nthat LLMs can effectively capture complex dependencies between demographic\nattributes and preferences while offering flexibility to explore hypothetical\nscenarios. This approach presents valuable opportunities for transportation\nplanning and social science research, enabling scalable, cost-efficient and\nprivacy-preserving data generation.", "published": "2025-01-20 15:11:03", "link": "http://arxiv.org/abs/2501.13955v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Zep: A Temporal Knowledge Graph Architecture for Agent Memory", "abstract": "We introduce Zep, a novel memory layer service for AI agents that outperforms\nthe current state-of-the-art system, MemGPT, in the Deep Memory Retrieval (DMR)\nbenchmark. Additionally, Zep excels in more comprehensive and challenging\nevaluations than DMR that better reflect real-world enterprise use cases. While\nexisting retrieval-augmented generation (RAG) frameworks for large language\nmodel (LLM)-based agents are limited to static document retrieval, enterprise\napplications demand dynamic knowledge integration from diverse sources\nincluding ongoing conversations and business data. Zep addresses this\nfundamental limitation through its core component Graphiti -- a\ntemporally-aware knowledge graph engine that dynamically synthesizes both\nunstructured conversational data and structured business data while maintaining\nhistorical relationships. In the DMR benchmark, which the MemGPT team\nestablished as their primary evaluation metric, Zep demonstrates superior\nperformance (94.8% vs 93.4%). Beyond DMR, Zep's capabilities are further\nvalidated through the more challenging LongMemEval benchmark, which better\nreflects enterprise use cases through complex temporal reasoning tasks. In this\nevaluation, Zep achieves substantial results with accuracy improvements of up\nto 18.5% while simultaneously reducing response latency by 90% compared to\nbaseline implementations. These results are particularly pronounced in\nenterprise-critical tasks such as cross-session information synthesis and\nlong-term context maintenance, demonstrating Zep's effectiveness for deployment\nin real-world applications.", "published": "2025-01-20 16:52:48", "link": "http://arxiv.org/abs/2501.13956v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Conversation Routines: A Prompt Engineering Framework for Task-Oriented\n  Dialog Systems", "abstract": "This study introduces Conversation Routines (CR), a structured prompt\nengineering framework for developing task-oriented dialog systems using Large\nLanguage Models (LLMs). While LLMs demonstrate remarkable natural language\nunderstanding capabilities, engineering them to reliably execute complex\nbusiness workflows remains challenging. The proposed CR framework enables the\ndevelopment of Conversation Agentic Systems (CAS) through natural language\nspecifications, embedding task-oriented logic within LLM prompts. This approach\nprovides a systematic methodology for designing and implementing complex\nconversational workflows while maintaining behavioral consistency. We\ndemonstrate the framework's effectiveness through two proof-of-concept\nimplementations: a Train Ticket Booking System and an Interactive\nTroubleshooting Copilot. These case studies validate CR's capability to encode\nsophisticated behavioral patterns and decision logic while preserving natural\nconversational flexibility. Results show that CR enables domain experts to\ndesign conversational workflows in natural language while leveraging custom\nfunctions (tools) developed by software engineers, creating an efficient\ndivision of responsibilities where developers focus on core API implementation\nand domain experts handle conversation design. While the framework shows\npromise in accessibility and adaptability, we identify key challenges including\ncomputational overhead, non-deterministic behavior, and domain-specific logic\noptimization. Future research directions include CR evaluation methods based on\nprompt engineering frameworks driven by goal-oriented grading criteria,\nimproving scalability for complex multi-agent interactions, and enhancing\nsystem robustness to address the identified limitations across diverse business\napplications.", "published": "2025-01-20 17:19:02", "link": "http://arxiv.org/abs/2501.11613v7", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.HC", "cs.PL"], "primary_category": "cs.CL"}
{"title": "SEF-PNet: Speaker Encoder-Free Personalized Speech Enhancement with\n  Local and Global Contexts Aggregation", "abstract": "Personalized speech enhancement (PSE) methods typically rely on pre-trained\nspeaker verification models or self-designed speaker encoders to extract target\nspeaker clues, guiding the PSE model in isolating the desired speech. However,\nthese approaches suffer from significant model complexity and often\nunderutilize enrollment speaker information, limiting the potential performance\nof the PSE model. To address these limitations, we propose a novel Speaker\nEncoder-Free PSE network, termed SEF-PNet, which fully exploits the information\npresent in both the enrollment speech and noisy mixtures. SEF-PNet incorporates\ntwo key innovations: Interactive Speaker Adaptation (ISA) and Local-Global\nContext Aggregation (LCA). ISA dynamically modulates the interactions between\nenrollment and noisy signals to enhance the speaker adaptation, while LCA\nemploys advanced channel attention within the PSE encoder to effectively\nintegrate local and global contextual information, thus improving feature\nlearning. Experiments on the Libri2Mix dataset demonstrate that SEF-PNet\nsignificantly outperforms baseline models, achieving state-of-the-art PSE\nperformance.", "published": "2025-01-20 05:09:40", "link": "http://arxiv.org/abs/2501.11274v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "LLM supervised Pre-training for Multimodal Emotion Recognition in\n  Conversations", "abstract": "Emotion recognition in conversations (ERC) is challenging due to the\nmultimodal nature of the emotion expression. In this paper, we propose to\npretrain a text-based recognition model from unsupervised speech transcripts\nwith LLM guidance. These transcriptions are obtained from a raw speech dataset\nwith a pre-trained ASR system. A text LLM model is queried to provide\npseudo-labels for these transcripts, and these pseudo-labeled transcripts are\nsubsequently used for learning an utterance level text-based emotion\nrecognition model. We use the utterance level text embeddings for emotion\nrecognition in conversations along with speech embeddings obtained from a\nrecently proposed pre-trained model. A hierarchical way of training the\nspeech-text model is proposed, keeping in mind the conversational nature of the\ndataset. We perform experiments on three established datasets, namely, IEMOCAP,\nMELD, and CMU- MOSI, where we illustrate that the proposed model improves over\nother benchmarks and achieves state-of-the-art results on two out of these\nthree datasets.", "published": "2025-01-20 12:56:02", "link": "http://arxiv.org/abs/2501.11468v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A2SB: Audio-to-Audio Schrodinger Bridges", "abstract": "Audio in the real world may be perturbed due to numerous factors, causing the\naudio quality to be degraded. The following work presents an audio restoration\nmodel tailored for high-res music at 44.1kHz. Our model, Audio-to-Audio\nSchrodinger Bridges (A2SB), is capable of both bandwidth extension (predicting\nhigh-frequency components) and inpainting (re-generating missing segments).\nCritically, A2SB is end-to-end without need of a vocoder to predict waveform\noutputs, able to restore hour-long audio inputs, and trained on permissively\nlicensed music data. A2SB is capable of achieving state-of-the-art bandwidth\nextension and inpainting quality on several out-of-distribution music test\nsets. Our demo website is https: //research.nvidia.com/labs/adlr/A2SB/.", "published": "2025-01-20 07:28:41", "link": "http://arxiv.org/abs/2501.11311v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Investigation of Whisper ASR Hallucinations Induced by Non-Speech Audio", "abstract": "Hallucinations of deep neural models are amongst key challenges in automatic\nspeech recognition (ASR). In this paper, we investigate hallucinations of the\nWhisper ASR model induced by non-speech audio segments present during\ninference. By inducting hallucinations with various types of sounds, we show\nthat there exists a set of hallucinations that appear frequently. We then study\nhallucinations caused by the augmentation of speech with such sounds. Finally,\nwe describe the creation of a bag of hallucinations (BoH) that allows to remove\nthe effect of hallucinations through the post-processing of text\ntranscriptions. The results of our experiments show that such post-processing\nis capable of reducing word error rate (WER) and acts as a good safeguard\nagainst problematic hallucinations.", "published": "2025-01-20 10:14:52", "link": "http://arxiv.org/abs/2501.11378v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Uncertainty Estimation in the Real World: A Study on Music Emotion\n  Recognition", "abstract": "Any data annotation for subjective tasks shows potential variations between\nindividuals. This is particularly true for annotations of emotional responses\nto musical stimuli. While older approaches to music emotion recognition systems\nfrequently addressed this uncertainty problem through probabilistic modeling,\nmodern systems based on neural networks tend to ignore the variability and\nfocus only on predicting central tendencies of human subjective responses. In\nthis work, we explore several methods for estimating not only the central\ntendencies of the subjective responses to a musical stimulus, but also for\nestimating the uncertainty associated with these responses. In particular, we\ninvestigate probabilistic loss functions and inference-time random sampling.\nExperimental results indicate that while the modeling of the central tendencies\nis achievable, modeling of the uncertainty in subjective responses proves\nsignificantly more challenging with currently available approaches even when\nempirical estimates of variations in the responses are available.", "published": "2025-01-20 16:19:19", "link": "http://arxiv.org/abs/2501.11570v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Noise-Agnostic Multitask Whisper Training for Reducing False Alarm\n  Errors in Call-for-Help Detection", "abstract": "Keyword spotting is often implemented by keyword classifier to the encoder in\nacoustic models, enabling the classification of predefined or open vocabulary\nkeywords. Although keyword spotting is a crucial task in various applications\nand can be extended to call-for-help detection in emergencies, however, the\nprevious method often suffers from scalability limitations due to retraining\nrequired to introduce new keywords or adapt to changing contexts. We explore a\nsimple yet effective approach that leverages off-the-shelf pretrained ASR\nmodels to address these challenges, especially in call-for-help detection\nscenarios. Furthermore, we observed a substantial increase in false alarms when\ndeploying call-for-help detection system in real-world scenarios due to noise\nintroduced by microphones or different environments. To address this, we\npropose a novel noise-agnostic multitask learning approach that integrates a\nnoise classification head into the ASR encoder. Our method enhances the model's\nrobustness to noisy environments, leading to a significant reduction in false\nalarms and improved overall call-for-help performance. Despite the added\ncomplexity of multitask learning, our approach is computationally efficient and\nprovides a promising solution for call-for-help detection in real-world\nscenarios.", "published": "2025-01-20 18:01:42", "link": "http://arxiv.org/abs/2501.11631v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Task and Perception-aware Distributed Source Coding for Correlated\n  Speech under Bandwidth-constrained Channels", "abstract": "Emerging wireless AR/VR applications require real-time transmission of\ncorrelated high-fidelity speech from multiple resource-constrained devices over\nunreliable, bandwidth-limited channels. Existing autoencoder-based speech\nsource coding methods fail to address the combination of the following - (1)\ndynamic bitrate adaptation without retraining the model, (2) leveraging\ncorrelations among multiple speech sources, and (3) balancing downstream task\nloss with realism of reconstructed speech. We propose a neural distributed\nprincipal component analysis (NDPCA)-aided distributed source coding algorithm\nfor correlated speech sources transmitting to a central receiver. Our method\nincludes a perception-aware downstream task loss function that balances\nperceptual realism with task-specific performance. Experiments show significant\nPSNR improvements under bandwidth constraints over naive autoencoder methods in\ntask-agnostic (19%) and task-aware settings (52%). It also approaches the\ntheoretical upper bound, where all correlated sources are sent to a single\nencoder, especially in low-bandwidth scenarios. Additionally, we present a\nrate-distortion-perception trade-off curve, enabling adaptive decisions based\non application-specific realism needs.", "published": "2025-01-20 04:57:29", "link": "http://arxiv.org/abs/2501.17879v1", "categories": ["cs.IT", "cs.AI", "cs.SD", "eess.AS", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
