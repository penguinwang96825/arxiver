{"title": "\"Bilingual Expert\" Can Find Translation Errors", "abstract": "Recent advances in statistical machine translation via the adoption of neural\nsequence-to-sequence models empower the end-to-end system to achieve\nstate-of-the-art in many WMT benchmarks. The performance of such machine\ntranslation (MT) system is usually evaluated by automatic metric BLEU when the\ngolden references are provided for validation. However, for model inference or\nproduction deployment, the golden references are prohibitively available or\nrequire expensive human annotation with bilingual expertise. In order to\naddress the issue of quality evaluation (QE) without reference, we propose a\ngeneral framework for automatic evaluation of translation output for most WMT\nquality evaluation tasks. We first build a conditional target language model\nwith a novel bidirectional transformer, named neural bilingual expert model,\nwhich is pre-trained on large parallel corpora for feature extraction. For QE\ninference, the bilingual expert model can simultaneously produce the joint\nlatent representation between the source and the translation, and real-valued\nmeasurements of possible erroneous tokens based on the prior knowledge learned\nfrom parallel data. Subsequently, the features will further be fed into a\nsimple Bi-LSTM predictive model for quality evaluation. The experimental\nresults show that our approach achieves the state-of-the-art performance in the\nquality estimation track of WMT 2017/2018.", "published": "2018-07-25 04:31:21", "link": "http://arxiv.org/abs/1807.09433v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Novel ILP Framework for Summarizing Content with High Lexical Variety", "abstract": "Summarizing content contributed by individuals can be challenging, because\npeople make different lexical choices even when describing the same events.\nHowever, there remains a significant need to summarize such content. Examples\ninclude the student responses to post-class reflective questions, product\nreviews, and news articles published by different news agencies related to the\nsame events. High lexical diversity of these documents hinders the system's\nability to effectively identify salient content and reduce summary redundancy.\nIn this paper, we overcome this issue by introducing an integer linear\nprogramming-based summarization framework. It incorporates a low-rank\napproximation to the sentence-word co-occurrence matrix to intrinsically group\nsemantically-similar lexical items. We conduct extensive experiments on\ndatasets of student responses, product reviews, and news documents. Our\napproach compares favorably to a number of extractive baselines as well as a\nneural abstractive summarization system. The paper finally sheds light on when\nand why the proposed framework is effective at summarizing content with high\nlexical variety.", "published": "2018-07-25 15:42:01", "link": "http://arxiv.org/abs/1807.09671v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distinctive-attribute Extraction for Image Captioning", "abstract": "Image captioning, an open research issue, has been evolved with the progress\nof deep neural networks. Convolutional neural networks (CNNs) and recurrent\nneural networks (RNNs) are employed to compute image features and generate\nnatural language descriptions in the research. In previous works, a caption\ninvolving semantic description can be generated by applying additional\ninformation into the RNNs. In this approach, we propose a distinctive-attribute\nextraction (DaE) which explicitly encourages significant meanings to generate\nan accurate caption describing the overall meaning of the image with their\nunique situation. Specifically, the captions of training images are analyzed by\nterm frequency-inverse document frequency (TF-IDF), and the analyzed semantic\ninformation is trained to extract distinctive-attributes for inferring\ncaptions. The proposed scheme is evaluated on a challenge data, and it improves\nan objective performance while describing images in more detail.", "published": "2018-07-25 04:34:17", "link": "http://arxiv.org/abs/1807.09434v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Differentiable Perturb-and-Parse: Semi-Supervised Parsing with a\n  Structured Variational Autoencoder", "abstract": "Human annotation for syntactic parsing is expensive, and large resources are\navailable only for a fraction of languages. A question we ask is whether one\ncan leverage abundant unlabeled texts to improve syntactic parsers, beyond just\nusing the texts to obtain more generalisable lexical features (i.e. beyond word\nembeddings). To this end, we propose a novel latent-variable generative model\nfor semi-supervised syntactic dependency parsing. As exact inference is\nintractable, we introduce a differentiable relaxation to obtain approximate\nsamples and compute gradients with respect to the parser parameters. Our method\n(Differentiable Perturb-and-Parse) relies on differentiable dynamic programming\nover stochastically perturbed edge scores. We demonstrate effectiveness of our\napproach with experiments on English, French and Swedish.", "published": "2018-07-25 21:42:55", "link": "http://arxiv.org/abs/1807.09875v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Judging a Book by its Description : Analyzing Gender Stereotypes in the\n  Man Bookers Prize Winning Fiction", "abstract": "The presence of gender stereotypes in many aspects of society is a well-known\nphenomenon. In this paper, we focus on studying and quantifying such\nstereotypes and bias in the Man Bookers Prize winning fiction. We consider 275\nbooks shortlisted for Man Bookers Prize between 1969 and 2017. The gender bias\nis analyzed by semantic modeling of book descriptions on Goodreads. This\nreveals the pervasiveness of gender bias and stereotype in the books on\ndifferent features like occupation, introductions and actions associated to the\ncharacters in the book.", "published": "2018-07-25 08:36:02", "link": "http://arxiv.org/abs/1807.10615v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing keyword correlation for event detection in social networks\n  using SVD and k-means: Twitter case study", "abstract": "Extracting textual features from tweets is a challenging process due to the\nnoisy nature of the content and the weak signal of most of the words used. In\nthis paper, we propose using singular value decomposition (SVD) with clustering\nto enhance the signals of the textual features in the tweets to improve the\ncorrelation with events. The proposed technique applies SVD to the time series\nvector for each feature to factorize the matrix of feature/day counts, in order\nto ensure the independence of the feature vectors. Afterwards, the k-means\nclustering is applied to build a look-up table that maps members of each\ncluster to the cluster-centroid. The lookup table is used to map each feature\nin the original data to the centroid of its cluster, then we calculate the sum\nof the term frequency vectors of all features in each cluster to the\nterm-frequency-vector of the cluster centroid. To test the technique we\ncalculated the correlations of the cluster centroids with the golden standard\nrecord (GSR) vector before and after summing the vectors of the cluster members\nto the centroid-vector. The proposed method is applied to multiple correlation\ntechniques including the Pearson, Spearman, distance correlation and Kendal\nTao. The experiments have also considered the different word forms and lengths\nof the features including keywords, n-grams, skip-grams and bags-of-words. The\ncorrelation results are enhanced significantly as the highest correlation\nscores have increased from 0.3 to 0.6, and the average correlation scores have\nincreased from 0.3 to 0.4.", "published": "2018-07-25 12:56:25", "link": "http://arxiv.org/abs/1807.09561v1", "categories": ["cs.SI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Repartitioning of the ComplexWebQuestions Dataset", "abstract": "Recently, Talmor and Berant (2018) introduced ComplexWebQuestions - a dataset\nfocused on answering complex questions by decomposing them into a sequence of\nsimpler questions and extracting the answer from retrieved web snippets. In\ntheir work the authors used a pre-trained reading comprehension (RC) model\n(Salant and Berant, 2018) to extract the answer from the web snippets. In this\nshort note we show that training a RC model directly on the training data of\nComplexWebQuestions reveals a leakage from the training set to the test set\nthat allows to obtain unreasonably high performance. As a solution, we\nconstruct a new partitioning of ComplexWebQuestions that does not suffer from\nthis leakage and publicly release it. We also perform an empirical evaluation\non these two datasets and show that training a RC model on the training data\nsubstantially improves state-of-the-art performance.", "published": "2018-07-25 14:15:40", "link": "http://arxiv.org/abs/1807.09623v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Finding Better Subword Segmentation for Neural Machine Translation", "abstract": "For different language pairs, word-level neural machine translation (NMT)\nmodels with a fixed-size vocabulary suffer from the same problem of\nrepresenting out-of-vocabulary (OOV) words. The common practice usually\nreplaces all these rare or unknown words with a <UNK> token, which limits the\ntranslation performance to some extent. Most of recent work handled such a\nproblem by splitting words into characters or other specially extracted subword\nunits to enable open-vocabulary translation. Byte pair encoding (BPE) is one of\nthe successful attempts that has been shown extremely competitive by providing\neffective subword segmentation for NMT systems. In this paper, we extend the\nBPE style segmentation to a general unsupervised framework with three\nstatistical measures: frequency (FRQ), accessor variety (AV) and description\nlength gain (DLG). We test our approach on two translation tasks: German to\nEnglish and Chinese to English. The experimental results show that AV and DLG\nenhanced systems outperform the FRQ baseline in the frequency weighted schemes\nat different significant levels.", "published": "2018-07-25 14:43:46", "link": "http://arxiv.org/abs/1807.09639v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A multi-device dataset for urban acoustic scene classification", "abstract": "This paper introduces the acoustic scene classification task of DCASE 2018\nChallenge and the TUT Urban Acoustic Scenes 2018 dataset provided for the task,\nand evaluates the performance of a baseline system in the task. As in previous\nyears of the challenge, the task is defined for classification of short audio\nsamples into one of predefined acoustic scene classes, using a supervised,\nclosed-set classification setup. The newly recorded TUT Urban Acoustic Scenes\n2018 dataset consists of ten different acoustic scenes and was recorded in six\nlarge European cities, therefore it has a higher acoustic variability than the\nprevious datasets used for this task, and in addition to high-quality binaural\nrecordings, it also includes data recorded with mobile devices. We also present\nthe baseline system consisting of a convolutional neural network and its\nperformance in the subtasks using the recommended cross-validation setup.", "published": "2018-07-25 20:27:55", "link": "http://arxiv.org/abs/1807.09840v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
