{"title": "Source Code is a Graph, Not a Sequence: A Cross-Lingual Perspective on\n  Code Clone Detection", "abstract": "Source code clone detection is the task of finding code fragments that have\nthe same or similar functionality, but may differ in syntax or structure. This\ntask is important for software maintenance, reuse, and quality assurance (Roy\net al. 2009). However, code clone detection is challenging, as source code can\nbe written in different languages, domains, and styles. In this paper, we argue\nthat source code is inherently a graph, not a sequence, and that graph-based\nmethods are more suitable for code clone detection than sequence-based methods.\nWe compare the performance of two state-of-the-art models: CodeBERT (Feng et\nal. 2020), a sequence-based model, and CodeGraph (Yu et al. 2023), a\ngraph-based model, on two benchmark data-sets: BCB (Svajlenko et al. 2014) and\nPoolC (PoolC no date). We show that CodeGraph outperforms CodeBERT on both\ndata-sets, especially on cross-lingual code clones. To the best of our\nknowledge, this is the first work to demonstrate the superiority of graph-based\nmethods over sequence-based methods on cross-lingual code clone detection.", "published": "2023-12-27 09:30:31", "link": "http://arxiv.org/abs/2312.16488v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Make BERT-based Chinese Spelling Check Model Enhanced by Layerwise\n  Attention and Gaussian Mixture Model", "abstract": "BERT-based models have shown a remarkable ability in the Chinese Spelling\nCheck (CSC) task recently. However, traditional BERT-based methods still suffer\nfrom two limitations. First, although previous works have identified that\nexplicit prior knowledge like Part-Of-Speech (POS) tagging can benefit in the\nCSC task, they neglected the fact that spelling errors inherent in CSC data can\nlead to incorrect tags and therefore mislead models. Additionally, they ignored\nthe correlation between the implicit hierarchical information encoded by BERT's\nintermediate layers and different linguistic phenomena. This results in\nsub-optimal accuracy. To alleviate the above two issues, we design a\nheterogeneous knowledge-infused framework to strengthen BERT-based CSC models.\nTo incorporate explicit POS knowledge, we utilize an auxiliary task strategy\ndriven by Gaussian mixture model. Meanwhile, to incorporate implicit\nhierarchical linguistic knowledge within the encoder, we propose a novel form\nof n-gram-based layerwise self-attention to generate a multilayer\nrepresentation. Experimental results show that our proposed framework yields a\nstable performance boost over four strong baseline models and outperforms the\nprevious state-of-the-art methods on two datasets.", "published": "2023-12-27 16:11:07", "link": "http://arxiv.org/abs/2312.16623v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM Factoscope: Uncovering LLMs' Factual Discernment through Inner\n  States Analysis", "abstract": "Large Language Models (LLMs) have revolutionized various domains with\nextensive knowledge and creative capabilities. However, a critical issue with\nLLMs is their tendency to produce outputs that diverge from factual reality.\nThis phenomenon is particularly concerning in sensitive applications such as\nmedical consultation and legal advice, where accuracy is paramount. In this\npaper, we introduce the LLM factoscope, a novel Siamese network-based model\nthat leverages the inner states of LLMs for factual detection. Our\ninvestigation reveals distinguishable patterns in LLMs' inner states when\ngenerating factual versus non-factual content. We demonstrate the LLM\nfactoscope's effectiveness across various architectures, achieving over 96%\naccuracy in factual detection. Our work opens a new avenue for utilizing LLMs'\ninner states for factual detection and encourages further exploration into\nLLMs' inner workings for enhanced reliability and transparency.", "published": "2023-12-27 01:44:47", "link": "http://arxiv.org/abs/2312.16374v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automating Knowledge Acquisition for Content-Centric Cognitive Agents\n  Using LLMs", "abstract": "The paper describes a system that uses large language model (LLM) technology\nto support the automatic learning of new entries in an intelligent agent's\nsemantic lexicon. The process is bootstrapped by an existing non-toy lexicon\nand a natural language generator that converts formal, ontologically-grounded\nrepresentations of meaning into natural language sentences. The learning method\ninvolves a sequence of LLM requests and includes an automatic quality control\nstep. To date, this learning method has been applied to learning multiword\nexpressions whose meanings are equivalent to those of transitive verbs in the\nagent's lexicon. The experiment demonstrates the benefits of a hybrid learning\narchitecture that integrates knowledge-based methods and resources with both\ntraditional data analytics and LLMs.", "published": "2023-12-27 02:31:51", "link": "http://arxiv.org/abs/2312.16378v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transfer and Alignment Network for Generalized Category Discovery", "abstract": "Generalized Category Discovery is a crucial real-world task. Despite the\nimproved performance on known categories, current methods perform poorly on\nnovel categories. We attribute the poor performance to two reasons: biased\nknowledge transfer between labeled and unlabeled data and noisy representation\nlearning on the unlabeled data. To mitigate these two issues, we propose a\nTransfer and Alignment Network (TAN), which incorporates two knowledge transfer\nmechanisms to calibrate the biased knowledge and two feature alignment\nmechanisms to learn discriminative features. Specifically, we model different\ncategories with prototypes and transfer the prototypes in labeled data to\ncorrect model bias towards known categories. On the one hand, we pull instances\nwith known categories in unlabeled data closer to these prototypes to form more\ncompact clusters and avoid boundary overlap between known and novel categories.\nOn the other hand, we use these prototypes to calibrate noisy prototypes\nestimated from unlabeled data based on category similarities, which allows for\nmore accurate estimation of prototypes for novel categories that can be used as\nreliable learning targets later. After knowledge transfer, we further propose\ntwo feature alignment mechanisms to acquire both instance- and category-level\nknowledge from unlabeled data by aligning instance features with both augmented\nfeatures and the calibrated prototypes, which can boost model performance on\nboth known and novel categories with less noise. Experiments on three benchmark\ndatasets show that our model outperforms SOTA methods, especially on novel\ncategories. Theoretical analysis is provided for an in-depth understanding of\nour model in general. Our code and data are available at\nhttps://github.com/Lackel/TAN.", "published": "2023-12-27 08:35:47", "link": "http://arxiv.org/abs/2312.16467v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "S2M: Converting Single-Turn to Multi-Turn Datasets for Conversational\n  Question Answering", "abstract": "Supplying data augmentation to conversational question answering (CQA) can\neffectively improve model performance. However, there is less improvement from\nsingle-turn datasets in CQA due to the distribution gap between single-turn and\nmulti-turn datasets. On the other hand, while numerous single-turn datasets are\navailable, we have not utilized them effectively. To solve this problem, we\npropose a novel method to convert single-turn datasets to multi-turn datasets.\nThe proposed method consists of three parts, namely, a QA pair Generator, a QA\npair Reassembler, and a question Rewriter. Given a sample consisting of context\nand single-turn QA pairs, the Generator obtains candidate QA pairs and a\nknowledge graph based on the context. The Reassembler utilizes the knowledge\ngraph to get sequential QA pairs, and the Rewriter rewrites questions from a\nconversational perspective to obtain a multi-turn dataset S2M. Our experiments\nshow that our method can synthesize effective training resources for CQA.\nNotably, S2M ranks 1st place on the QuAC leaderboard at the time of submission\n(Aug 24th, 2022).", "published": "2023-12-27 10:41:18", "link": "http://arxiv.org/abs/2312.16511v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Large Language Model-based Computational Approach to Improve\n  Identity-Related Write-Ups", "abstract": "Creating written products is essential to modern life, including writings\nabout one's identity and personal experiences. However, writing is often a\ndifficult activity that requires extensive effort to frame the central ideas,\nthe pursued approach to communicate the central ideas, e.g., using analogies,\nmetaphors, or other possible means, the needed presentation structure, and the\nactual verbal expression. Large Language Models, a recently emerged approach in\nMachine Learning, can offer a significant help in reducing the effort and\nimproving the quality of written products. This paper proposes a new\ncomputational approach to explore prompts that given as inputs to a Large\nLanguage Models can generate cues to improve the considered written products.\nTwo case studies on improving write-ups, one based on an analogy and one on a\nmetaphor, are also presented in the paper.", "published": "2023-12-27 18:08:50", "link": "http://arxiv.org/abs/2312.16659v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Some things are more CRINGE than others: Iterative Preference\n  Optimization with the Pairwise Cringe Loss", "abstract": "Practitioners commonly align large language models using pairwise\npreferences, i.e., given labels of the type response A is preferred to response\nB for a given input. Perhaps less commonly, methods have also been developed\nfor binary feedback, i.e. training models given labels of type response A is\ngood or bad. We show how an existing performant binary feedback method, the\nCringe Loss (Adolphs et al., 2022), can be generalized to the pairwise\npreference setting using a simple soft margin extension. Pairwise Cringe Loss\nis straightforward to implement and efficient to train, and we find it\noutperforms state-of-the-art preference optimization algorithms such as PPO and\nDPO on the AlpacaFarm benchmark. We show that iterations of training of our\nmodel are important for improved results, and that we can generalize DPO to\nIterative DPO in the same way.", "published": "2023-12-27 18:53:09", "link": "http://arxiv.org/abs/2312.16682v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Reversible Perspective on Petri Nets and Event Structures", "abstract": "Event structures have emerged as a foundational model for concurrent\ncomputation, explaining computational processes by outlining the events and the\nrelationships that dictate their execution. They play a pivotal role in the\nstudy of key aspects of concurrent computation models, such as causality and\nindependence, and have found applications across a broad range of languages and\nmodels, spanning realms like persistence, probabilities, and quantum computing.\nRecently, event structures have been extended to address reversibility, where\ncomputational processes can undo previous computations. In this context,\nreversible event structures provide abstract representations of processes\ncapable of both forward and backward steps in a computation. Since their\nintroduction, event structures have played a crucial role in bridging\noperational models, traditionally exemplified by Petri nets and process\ncalculi, with denotational ones, i.e., algebraic domains. In this context, we\nrevisit the standard connection between Petri nets and event structures under\nthe lenses of reversibility. Specifically, we introduce a subset of contextual\nPetri nets, dubbed reversible causal nets, that precisely correspond to\nreversible prime event structures. The distinctive feature of reversible causal\nnets lies in deriving causality from inhibitor arcs, departing from the\nconventional dependence on the overlap between the post and preset of\ntransitions. In this way, we are able to operationally explain the full model\nof reversible prime event structures.", "published": "2023-12-27 20:47:48", "link": "http://arxiv.org/abs/2312.16714v1", "categories": ["cs.CL", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Conversational Question Answering with Reformulations over Knowledge\n  Graph", "abstract": "Conversational question answering (convQA) over knowledge graphs (KGs)\ninvolves answering multi-turn natural language questions about information\ncontained in a KG. State-of-the-art methods of ConvQA often struggle with\ninexplicit question-answer pairs. These inputs are easy for human beings to\nunderstand given a conversation history, but hard for a machine to interpret,\nwhich can degrade ConvQA performance. To address this problem, we propose a\nreinforcement learning (RL) based model, CornNet, which utilizes question\nreformulations generated by large language models (LLMs) to improve ConvQA\nperformance. CornNet adopts a teacher-student architecture where a teacher\nmodel learns question representations using human writing reformulations, and a\nstudent model to mimic the teacher model's output via reformulations generated\nby LLMs. The learned question representation is then used by an RL model to\nlocate the correct answer in a KG. Extensive experimental results show that\nCornNet outperforms state-of-the-art convQA models.", "published": "2023-12-27 00:03:05", "link": "http://arxiv.org/abs/2312.17269v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PanGu-$\u03c0$: Enhancing Language Model Architectures via Nonlinearity\n  Compensation", "abstract": "The recent trend of large language models (LLMs) is to increase the scale of\nboth model size (\\aka the number of parameters) and dataset to achieve better\ngenerative ability, which is definitely proved by a lot of work such as the\nfamous GPT and Llama. However, large models often involve massive computational\ncosts, and practical applications cannot afford such high prices. However, the\nmethod of constructing a strong model architecture for LLMs is rarely\ndiscussed. We first analyze the state-of-the-art language model architectures\nand observe the feature collapse problem. Based on the theoretical analysis, we\npropose that the nonlinearity is also very important for language models, which\nis usually studied in convolutional neural networks for vision tasks. The\nseries informed activation function is then introduced with tiny calculations\nthat can be ignored, and an augmented shortcut is further used to enhance the\nmodel nonlinearity. We then demonstrate that the proposed approach is\nsignificantly effective for enhancing the model nonlinearity through carefully\ndesigned ablations; thus, we present a new efficient model architecture for\nestablishing modern, namely, PanGu-$\\pi$. Experiments are then conducted using\nthe same dataset and training strategy to compare PanGu-$\\pi$ with\nstate-of-the-art LLMs. The results show that PanGu-$\\pi$-7B can achieve a\ncomparable performance to that of benchmarks with about 10\\% inference\nspeed-up, and PanGu-$\\pi$-1B can achieve state-of-the-art performance in terms\nof accuracy and efficiency. In addition, we have deployed PanGu-$\\pi$-7B in the\nhigh-value domains of finance and law, developing an LLM named YunShan for\npractical application. The results show that YunShan can surpass other models\nwith similar scales on benchmarks.", "published": "2023-12-27 11:49:24", "link": "http://arxiv.org/abs/2312.17276v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language Models for Conducting Advanced Text Analytics Information\n  Systems Research", "abstract": "The exponential growth of digital content has generated massive textual\ndatasets, necessitating the use of advanced analytical approaches. Large\nLanguage Models (LLMs) have emerged as tools that are capable of processing and\nextracting insights from massive unstructured textual datasets. However, how to\nleverage LLMs for text analytics Information Systems (IS) research is currently\nunclear. To assist the IS community in understanding how to operationalize\nLLMs, we propose a Text Analytics for Information Systems Research (TAISR)\nframework. Our proposed framework provides detailed recommendations grounded in\nIS and LLM literature on how to conduct meaningful text analytics IS research\nfor design science, behavioral, and econometric streams. We conducted three\nbusiness intelligence case studies using our TAISR framework to demonstrate its\napplication in several IS research contexts. We also outline the potential\nchallenges and limitations of adopting LLMs for IS. By offering a systematic\napproach and evidence of its utility, our TAISR framework contributes to future\nIS research streams looking to incorporate powerful LLMs for text analytics.", "published": "2023-12-27 19:49:00", "link": "http://arxiv.org/abs/2312.17278v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Stateful Conformer with Cache-based Inference for Streaming Automatic\n  Speech Recognition", "abstract": "In this paper, we propose an efficient and accurate streaming speech\nrecognition model based on the FastConformer architecture. We adapted the\nFastConformer architecture for streaming applications through: (1) constraining\nboth the look-ahead and past contexts in the encoder, and (2) introducing an\nactivation caching mechanism to enable the non-autoregressive encoder to\noperate autoregressively during inference. The proposed model is thoughtfully\ndesigned in a way to eliminate the accuracy disparity between the train and\ninference time which is common for many streaming models. Furthermore, our\nproposed encoder works with various decoder configurations including\nConnectionist Temporal Classification (CTC) and RNN-Transducer (RNNT) decoders.\nAdditionally, we introduced a hybrid CTC/RNNT architecture which utilizes a\nshared encoder with both a CTC and RNNT decoder to boost the accuracy and save\ncomputation. We evaluate the proposed model on LibriSpeech dataset and a\nmulti-domain large scale dataset and demonstrate that it can achieve better\naccuracy with lower latency and inference time compared to a conventional\nbuffered streaming model baseline. We also showed that training a model with\nmultiple latencies can achieve better accuracy than single latency models while\nit enables us to support multiple latencies with a single model. Our\nexperiments also showed the hybrid architecture would not only speedup the\nconvergence of the CTC decoder but also improves the accuracy of streaming\nmodels compared to single decoder models.", "published": "2023-12-27 21:04:26", "link": "http://arxiv.org/abs/2312.17279v3", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Gemini Pro Defeated by GPT-4V: Evidence from Education", "abstract": "This study compared the classification performance of Gemini Pro and GPT-4V\nin educational settings. Employing visual question answering (VQA) techniques,\nthe study examined both models' abilities to read text-based rubrics and then\nautomatically score student-drawn models in science education. We employed both\nquantitative and qualitative analyses using a dataset derived from\nstudent-drawn scientific models and employing NERIF (Notation-Enhanced Rubrics\nfor Image Feedback) prompting methods. The findings reveal that GPT-4V\nsignificantly outperforms Gemini Pro in terms of scoring accuracy and Quadratic\nWeighted Kappa. The qualitative analysis reveals that the differences may be\ndue to the models' ability to process fine-grained texts in images and overall\nimage classification performance. Even adapting the NERIF approach by further\nde-sizing the input images, Gemini Pro seems not able to perform as well as\nGPT-4V. The findings suggest GPT-4V's superior capability in handling complex\nmultimodal educational tasks. The study concludes that while both models\nrepresent advancements in AI, GPT-4V's higher performance makes it a more\nsuitable tool for educational applications involving multimodal data\ninterpretation.", "published": "2023-12-27 02:56:41", "link": "http://arxiv.org/abs/2401.08660v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Adapting Large Language Models for Education: Foundational Capabilities,\n  Potentials, and Challenges", "abstract": "Online education platforms, leveraging the internet to distribute education\nresources, seek to provide convenient education but often fall short in\nreal-time communication with students. They often struggle to address the\ndiverse obstacles students encounter throughout their learning journey. Solving\nthe problems encountered by students poses a significant challenge for\ntraditional deep learning models, as it requires not only a broad spectrum of\nsubject knowledge but also the ability to understand what constitutes a\nstudent's individual difficulties. It's challenging for traditional machine\nlearning models, as they lack the capacity to comprehend students' personalized\nneeds. Recently, the emergence of large language models (LLMs) offers the\npossibility for resolving this issue by comprehending individual requests.\nAlthough LLMs have been successful in various fields, creating an LLM-based\neducation system is still challenging for the wide range of educational skills\nrequired. This paper reviews the recently emerged LLM research related to\neducational capabilities, including mathematics, writing, programming,\nreasoning, and knowledge-based question answering, with the aim to explore\ntheir potential in constructing the next-generation intelligent education\nsystem. Specifically, for each capability, we focus on investigating two\naspects. Firstly, we examine the current state of LLMs regarding this\ncapability: how advanced they have become, whether they surpass human\nabilities, and what deficiencies might exist. Secondly, we evaluate whether the\ndevelopment methods for LLMs in this area are generalizable, that is, whether\nthese methods can be applied to construct a comprehensive educational\nsupermodel with strengths across various capabilities, rather than being\neffective in only a singular aspect.", "published": "2023-12-27 14:37:32", "link": "http://arxiv.org/abs/2401.08664v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Understanding News Creation Intents: Frame, Dataset, and Method", "abstract": "As the disruptive changes in the media economy and the proliferation of\nalternative news media outlets, news intent has progressively deviated from\nethical standards that serve the public interest. News intent refers to the\npurpose or intention behind the creation of a news article. While the\nsignificance of research on news intent has been widely acknowledged, the\nabsence of a systematic news intent understanding framework hinders further\nexploration of news intent and its downstream applications. To bridge this gap,\nwe propose News INTent (NINT) frame, the first component-aware formalism for\nunderstanding the news creation intent based on research in philosophy,\npsychology, and cognitive science. Within this frame, we define the news intent\nidentification task and provide a benchmark dataset with fine-grained labels\nalong with an efficient benchmark method. Experiments demonstrate that NINT is\nbeneficial in both the intent identification task and downstream tasks that\ndemand a profound understanding of news. This work marks a foundational step\ntowards a more systematic exploration of news creation intents.", "published": "2023-12-27 09:35:23", "link": "http://arxiv.org/abs/2312.16490v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "A proposed new metric for the conceptual diversity of a text", "abstract": "A word may contain one or more hidden concepts. While the \"animal\" word\nevokes many images in our minds and encapsulates many concepts (birds, dogs,\ncats, crocodiles, etc.), the `parrot' word evokes a single image (a colored\nbird with a short, hooked beak and the ability to mimic sounds). In spoken or\nwritten texts, we use some words in a general sense and some in a detailed way\nto point to a specific object. Until now, a text's conceptual diversity value\ncannot be determined using a standard and precise technique. This research\ncontributes to the natural language processing field of AI by offering a\nstandardized method and a generic metric for evaluating and comparing concept\ndiversity in different texts and domains. It also contributes to the field of\nsemantic research of languages. If we give examples for the diversity score of\ntwo sentences, \"He discovered an unknown entity.\" has a high conceptual\ndiversity score (16.6801), and \"The endoplasmic reticulum forms a series of\nflattened sacs within the cytoplasm of eukaryotic cells.\" sentence has a low\nconceptual diversity score which is 3.9068.", "published": "2023-12-27 12:19:06", "link": "http://arxiv.org/abs/2312.16548v1", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT", "I.2.4; I.2.7"], "primary_category": "cs.CL"}
{"title": "How Robust are LLMs to In-Context Majority Label Bias?", "abstract": "In the In-Context Learning (ICL) setup, various forms of label biases can\nmanifest. One such manifestation is majority label bias, which arises when the\ndistribution of labeled examples in the in-context samples is skewed towards\none or more specific classes making Large Language Models (LLMs) more prone to\npredict those labels. Such discrepancies can arise from various factors,\nincluding logistical constraints, inherent biases in data collection methods,\nlimited access to diverse data sources, etc. which are unavoidable in a\nreal-world industry setup. In this work, we study the robustness of in-context\nlearning in LLMs to shifts that occur due to majority label bias within the\npurview of text classification tasks. Prior works have shown that in-context\nlearning with LLMs is susceptible to such biases. In our study, we go one level\ndeeper and show that the robustness boundary varies widely for different models\nand tasks, with certain LLMs being highly robust (~90%) to majority label bias.\nAdditionally, our findings also highlight the impact of model size and the\nrichness of instructional prompts contributing towards model robustness. We\nrestrict our study to only publicly available open-source models to ensure\ntransparency and reproducibility.", "published": "2023-12-27 12:20:12", "link": "http://arxiv.org/abs/2312.16549v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Relationship between auditory and semantic entrainment using Deep Neural\n  Networks (DNN)", "abstract": "The tendency of people to engage in similar, matching, or synchronized\nbehaviour when interacting is known as entrainment. Many studies examined\nlinguistic (syntactic and lexical structures) and paralinguistic (pitch,\nintensity) entrainment, but less attention was given to finding the\nrelationship between them. In this study, we utilized state-of-the-art DNN\nembeddings such as BERT and TRIpLet Loss network (TRILL) vectors to extract\nfeatures for measuring semantic and auditory similarities of turns within\ndialogues in two comparable spoken corpora of two different languages. We found\npeople's tendency to entrain on semantic features more when compared to\nauditory features. Additionally, we found that entrainment in semantic and\nauditory linguistic features are positively correlated. The findings of this\nstudy might assist in implementing the mechanism of entrainment in\nhuman-machine interaction (HMI).", "published": "2023-12-27 14:50:09", "link": "http://arxiv.org/abs/2312.16599v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Rethinking Tabular Data Understanding with Large Language Models", "abstract": "Large Language Models (LLMs) have shown to be capable of various tasks, yet\ntheir capability in interpreting and reasoning over tabular data remains an\nunderexplored area. In this context, this study investigates from three core\nperspectives: the robustness of LLMs to structural perturbations in tables, the\ncomparative analysis of textual and symbolic reasoning on tables, and the\npotential of boosting model performance through the aggregation of multiple\nreasoning pathways. We discover that structural variance of tables presenting\nthe same content reveals a notable performance decline, particularly in\nsymbolic reasoning tasks. This prompts the proposal of a method for table\nstructure normalization. Moreover, textual reasoning slightly edges out\nsymbolic reasoning, and a detailed error analysis reveals that each exhibits\ndifferent strengths depending on the specific tasks. Notably, the aggregation\nof textual and symbolic reasoning pathways, bolstered by a mix self-consistency\nmechanism, resulted in achieving SOTA performance, with an accuracy of 73.6% on\nWIKITABLEQUESTIONS, representing a substantial advancement over previous\nexisting table processing paradigms of LLMs.", "published": "2023-12-27 19:58:52", "link": "http://arxiv.org/abs/2312.16702v1", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Selective-Memory Meta-Learning with Environment Representations for\n  Sound Event Localization and Detection", "abstract": "Environment shifts and conflicts present significant challenges for\nlearning-based sound event localization and detection (SELD) methods. SELD\nsystems, when trained in particular acoustic settings, often show restricted\ngeneralization capabilities for diverse acoustic environments. Furthermore,\nobtaining annotated samples for spatial sound events is notably costly.\nDeploying a SELD system in a new environment requires extensive time for\nre-training and fine-tuning. To overcome these challenges, we propose\nenvironment-adaptive Meta-SELD, designed for efficient adaptation to new\nenvironments using minimal data. Our method specifically utilizes\ncomputationally synthesized spatial data and employs Model-Agnostic\nMeta-Learning (MAML) on a pre-trained, environment-independent model. The\nmethod then utilizes fast adaptation to unseen real-world environments using\nlimited samples from the respective environments. Inspired by the\nLearning-to-Forget approach, we introduce the concept of selective memory as a\nstrategy for resolving conflicts across environments. This approach involves\nselectively memorizing target-environment-relevant information and adapting to\nthe new environments through the selective attenuation of model parameters. In\naddition, we introduce environment representations to characterize different\nacoustic settings, enhancing the adaptability of our attenuation approach to\nvarious environments. We evaluate our proposed method on the development set of\nthe Sony-TAu Realistic Spatial Soundscapes 2023 (STARSS23) dataset and\ncomputationally synthesized scenes. Experimental results demonstrate the\nsuperior performance of the proposed method compared to conventional supervised\nlearning methods, particularly in localization.", "published": "2023-12-27 06:02:50", "link": "http://arxiv.org/abs/2312.16422v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Frame-level emotional state alignment method for speech emotion\n  recognition", "abstract": "Speech emotion recognition (SER) systems aim to recognize human emotional\nstate during human-computer interaction. Most existing SER systems are trained\nbased on utterance-level labels. However, not all frames in an audio have\naffective states consistent with utterance-level label, which makes it\ndifficult for the model to distinguish the true emotion of the audio and\nperform poorly. To address this problem, we propose a frame-level emotional\nstate alignment method for SER. First, we fine-tune HuBERT model to obtain a\nSER system with task-adaptive pretraining (TAPT) method, and extract embeddings\nfrom its transformer layers to form frame-level pseudo-emotion labels with\nclustering. Then, the pseudo labels are used to pretrain HuBERT. Hence, the\neach frame output of HuBERT has corresponding emotional information. Finally,\nwe fine-tune the above pretrained HuBERT for SER by adding an attention layer\non the top of it, which can focus only on those frames that are emotionally\nmore consistent with utterance-level label. The experimental results performed\non IEMOCAP indicate that our proposed method performs better than\nstate-of-the-art (SOTA) methods.", "published": "2023-12-27 03:07:52", "link": "http://arxiv.org/abs/2312.16383v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Online Similarity-and-Independence-Aware Beamformer for Low-latency\n  Target Sound Extraction", "abstract": "This study introduces an online target sound extraction (TSE) process using\nthe similarity-and-independence-aware beamformer (SIBF) derived from an\niterative batch algorithm. The study aimed to reduce latency while maintaining\nextraction accuracy. The SIBF, which is a linear method, provides more accurate\nestimates of the target than an approximate magnitude spectrogram reference.\nThe transition to an online algorithm reduces latency but presents challenges.\nFirst, contrary to the conventional assumption, deriving the online algorithm\nmay degrade accuracy as compared to the batch algorithm using a sliding window.\nSecond, conventional post-processing methods intended for scaling the estimated\ntarget may widen the accuracy gap between the two algorithms. This study adopts\nan approach that addresses these challenges and minimizes the accuracy gap\nduring post-processing. It proposes a novel scaling method based on the\nsingle-channel Wiener filter (SWF-based scaling). To further improve accuracy,\nthe study introduces a modified version of the time-frequency-varying variance\ngeneralized Gaussian distribution as a source model to represent the joint\nprobability between the target and reference. Experimental results using the\nCHiME-3 dataset demonstrate several key findings: 1) SWF-based scaling\neffectively eliminates the gap between the two algorithms and improves\naccuracy. 2) The new source model achieves optimal accuracy, corresponding to\nthe Laplacian model. 3) Our online SIBF outperforms conventional linear TSE\nmethods, including independent vector extraction and minimum mean square error\nbeamforming. These findings can contribute to the fields of beamforming and\nblind source separation.", "published": "2023-12-27 07:27:09", "link": "http://arxiv.org/abs/2312.16449v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "AE-Flow: AutoEncoder Normalizing Flow", "abstract": "Recently normalizing flows have been gaining traction in text-to-speech (TTS)\nand voice conversion (VC) due to their state-of-the-art (SOTA) performance.\nNormalizing flows are unsupervised generative models. In this paper, we\nintroduce supervision to the training process of normalizing flows, without the\nneed for parallel data. We call this training paradigm AutoEncoder Normalizing\nFlow (AE-Flow). It adds a reconstruction loss forcing the model to use\ninformation from the conditioning to reconstruct an audio sample. Our goal is\nto understand the impact of each component and find the right combination of\nthe negative log-likelihood (NLL) and the reconstruction loss in training\nnormalizing flows with coupling blocks. For that reason we will compare\nflow-based mapping model trained with: (i) NLL loss, (ii) NLL and\nreconstruction losses, as well as (iii) reconstruction loss only. Additionally,\nwe compare our model with SOTA VC baseline. The models are evaluated in terms\nof naturalness, speaker similarity, intelligibility in many-to-many and\nmany-to-any VC settings. The results show that the proposed training paradigm\nsystematically improves speaker similarity and naturalness when compared to\nregular training methods of normalizing flows. Furthermore, we show that our\nmethod improves speaker similarity and intelligibility over the\nstate-of-the-art.", "published": "2023-12-27 12:29:21", "link": "http://arxiv.org/abs/2312.16552v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-supervised Pretraining for Robust Personalized Voice Activity\n  Detection in Adverse Conditions", "abstract": "In this paper, we propose the use of self-supervised pretraining on a large\nunlabelled data set to improve the performance of a personalized voice activity\ndetection (VAD) model in adverse conditions. We pretrain a long short-term\nmemory (LSTM)-encoder using the autoregressive predictive coding (APC)\nframework and fine-tune it for personalized VAD. We also propose a denoising\nvariant of APC, with the goal of improving the robustness of personalized VAD.\nThe trained models are systematically evaluated on both clean speech and speech\ncontaminated by various types of noise at different SNR-levels and compared to\na purely supervised model. Our experiments show that self-supervised\npretraining not only improves performance in clean conditions, but also yields\nmodels which are more robust to adverse conditions compared to purely\nsupervised learning.", "published": "2023-12-27 15:36:17", "link": "http://arxiv.org/abs/2312.16613v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "68T10", "I.2.6"], "primary_category": "cs.SD"}
