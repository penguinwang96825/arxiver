{"title": "Ethical Challenges in Data-Driven Dialogue Systems", "abstract": "The use of dialogue systems as a medium for human-machine interaction is an\nincreasingly prevalent paradigm. A growing number of dialogue systems use\nconversation strategies that are learned from large datasets. There are well\ndocumented instances where interactions with these system have resulted in\nbiased or even offensive conversations due to the data-driven training process.\nHere, we highlight potential ethical issues that arise in dialogue systems\nresearch, including: implicit biases in data-driven systems, the rise of\nadversarial examples, potential sources of privacy violations, safety concerns,\nspecial considerations for reinforcement learning systems, and reproducibility\nconcerns. We also suggest areas stemming from these issues that deserve further\ninvestigation. Through this initial survey, we hope to spur research leading to\nrobust, safe, and ethically sound dialogue systems.", "published": "2017-11-24 17:14:34", "link": "http://arxiv.org/abs/1711.09050v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Exploration of Word Embedding Initialization in Deep-Learning Tasks", "abstract": "Word embeddings are the interface between the world of discrete units of text\nprocessing and the continuous, differentiable world of neural networks. In this\nwork, we examine various random and pretrained initialization methods for\nembeddings used in deep networks and their effect on the performance on four\nNLP tasks with both recurrent and convolutional architectures. We confirm that\npretrained embeddings are a little better than random initialization,\nespecially considering the speed of learning. On the other hand, we do not see\nany significant difference between various methods of random initialization, as\nlong as the variance is kept reasonably low. High-variance initialization\nprevents the network to use the space of embeddings and forces it to use other\nfree parameters to accomplish the task. We support this hypothesis by observing\nthe performance in learning lexical relations and by the fact that the network\ncan learn to perform reasonably in its task even with fixed random embeddings.", "published": "2017-11-24 22:31:01", "link": "http://arxiv.org/abs/1711.09160v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Continuous Semantic Topic Embedding Model Using Variational Autoencoder", "abstract": "This paper proposes the continuous semantic topic embedding model (CSTEM)\nwhich finds latent topic variables in documents using continuous semantic\ndistance function between the topics and the words by means of the variational\nautoencoder(VAE). The semantic distance could be represented by any symmetric\nbell-shaped geometric distance function on the Euclidean space, for which the\nMahalanobis distance is used in this paper. In order for the semantic distance\nto perform more properly, we newly introduce an additional model parameter for\neach word to take out the global factor from this distance indicating how\nlikely it occurs regardless of its topic. It certainly improves the problem\nthat the Gaussian distribution which is used in previous topic model with\ncontinuous word embedding could not explain the semantic relation correctly and\nhelps to obtain the higher topic coherence. Through the experiments with the\ndataset of 20 Newsgroup, NIPS papers and CNN/Dailymail corpus, the performance\nof the recent state-of-the-art models is accomplished by our model as well as\ngenerating topic embedding vectors which makes possible to observe where the\ntopic vectors are embedded with the word vectors in the real Euclidean space\nand how the topics are related each other semantically.", "published": "2017-11-24 05:37:35", "link": "http://arxiv.org/abs/1711.08870v1", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Self-Supervised Vision-Based Detection of the Active Speaker as Support\n  for Socially-Aware Language Acquisition", "abstract": "This paper presents a self-supervised method for visual detection of the\nactive speaker in a multi-person spoken interaction scenario. Active speaker\ndetection is a fundamental prerequisite for any artificial cognitive system\nattempting to acquire language in social settings. The proposed method is\nintended to complement the acoustic detection of the active speaker, thus\nimproving the system robustness in noisy conditions. The method can detect an\narbitrary number of possibly overlapping active speakers based exclusively on\nvisual information about their face. Furthermore, the method does not rely on\nexternal annotations, thus complying with cognitive development. Instead, the\nmethod uses information from the auditory modality to support learning in the\nvisual domain. This paper reports an extensive evaluation of the proposed\nmethod using a large multi-person face-to-face interaction dataset. The results\nshow good performance in a speaker dependent setting. However, in a speaker\nindependent setting the proposed method yields a significantly lower\nperformance. We believe that the proposed method represents an essential\ncomponent of any artificial cognitive system or robotic platform engaging in\nsocial interactions.", "published": "2017-11-24 14:45:06", "link": "http://arxiv.org/abs/1711.08992v2", "categories": ["cs.CV", "cs.CL", "cs.HC", "cs.LG", "stat.ML", "I.2; I.4; I.5"], "primary_category": "cs.CV"}
{"title": "Interactive Robot Learning of Gestures, Language and Affordances", "abstract": "A growing field in robotics and Artificial Intelligence (AI) research is\nhuman-robot collaboration, whose target is to enable effective teamwork between\nhumans and robots. However, in many situations human teams are still superior\nto human-robot teams, primarily because human teams can easily agree on a\ncommon goal with language, and the individual members observe each other\neffectively, leveraging their shared motor repertoire and sensorimotor\nresources. This paper shows that for cognitive robots it is possible, and\nindeed fruitful, to combine knowledge acquired from interacting with elements\nof the environment (affordance exploration) with the probabilistic observation\nof another agent's actions.\n  We propose a model that unites (i) learning robot affordances and word\ndescriptions with (ii) statistical recognition of human gestures with vision\nsensors. We discuss theoretical motivations, possible implementations, and we\nshow initial results which highlight that, after having acquired knowledge of\nits surrounding environment, a humanoid robot can generalize this knowledge to\nthe case when it observes another agent (human partner) performing the same\nmotor actions previously executed during training.", "published": "2017-11-24 17:34:32", "link": "http://arxiv.org/abs/1711.09055v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Deep Cross-Modal Correlation Learning for Audio and Lyrics in Music\n  Retrieval", "abstract": "Little research focuses on cross-modal correlation learning where temporal\nstructures of different data modalities such as audio and lyrics are taken into\naccount. Stemming from the characteristic of temporal structures of music in\nnature, we are motivated to learn the deep sequential correlation between audio\nand lyrics. In this work, we propose a deep cross-modal correlation learning\narchitecture involving two-branch deep neural networks for audio modality and\ntext modality (lyrics). Different modality data are converted to the same\ncanonical space where inter modal canonical correlation analysis is utilized as\nan objective function to calculate the similarity of temporal structures. This\nis the first study on understanding the correlation between language and music\naudio through deep architectures for learning the paired temporal correlation\nof audio and lyrics. Pre-trained Doc2vec model followed by fully-connected\nlayers (fully-connected deep neural network) is used to represent lyrics. Two\nsignificant contributions are made in the audio branch, as follows: i)\npre-trained CNN followed by fully-connected layers is investigated for\nrepresenting music audio. ii) We further suggest an end-to-end architecture\nthat simultaneously trains convolutional layers and fully-connected layers to\nbetter learn temporal structures of music audio. Particularly, our end-to-end\ndeep architecture contains two properties: simultaneously implementing feature\nlearning and cross-modal correlation learning, and learning joint\nrepresentation by considering temporal structures. Experimental results, using\naudio to retrieve lyrics or using lyrics to retrieve audio, verify the\neffectiveness of the proposed deep correlation learning architectures in\ncross-modal music retrieval.", "published": "2017-11-24 14:21:46", "link": "http://arxiv.org/abs/1711.08976v2", "categories": ["cs.IR", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
