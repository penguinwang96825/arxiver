{"title": "Paraphrasing via Ranking Many Candidates", "abstract": "We present a simple and effective way to generate a variety of paraphrases\nand find a good quality paraphrase among them. As in previous studies, it is\ndifficult to ensure that one generation method always generates the best\nparaphrase in various domains. Therefore, we focus on finding the best\ncandidate from multiple candidates, rather than assuming that there is only one\ncombination of generative models and decoding options. Our approach shows that\nit is easy to apply in various domains and has sufficiently good performance\ncompared to previous methods. In addition, our approach can be used for data\naugmentation that extends the downstream corpus, showing that it can help\nimprove performance in English and Korean datasets.", "published": "2021-07-20 06:24:01", "link": "http://arxiv.org/abs/2107.09274v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sequence Model with Self-Adaptive Sliding Window for Efficient Spoken\n  Document Segmentation", "abstract": "Transcripts generated by automatic speech recognition (ASR) systems for\nspoken documents lack structural annotations such as paragraphs, significantly\nreducing their readability. Automatically predicting paragraph segmentation for\nspoken documents may both improve readability and downstream NLP performance\nsuch as summarization and machine reading comprehension. We propose a sequence\nmodel with self-adaptive sliding window for accurate and efficient paragraph\nsegmentation. We also propose an approach to exploit phonetic information,\nwhich significantly improves robustness of spoken document segmentation to ASR\nerrors. Evaluations are conducted on the English Wiki-727K document\nsegmentation benchmark, a Chinese Wikipedia-based document segmentation dataset\nwe created, and an in-house Chinese spoken document dataset. Our proposed model\noutperforms the state-of-the-art (SOTA) model based on the same BERT-Base,\nincreasing segmentation F1 on the English benchmark by 4.2 points and on\nChinese datasets by 4.3-10.1 points, while reducing inference time to less than\n1/6 of inference time of the current SOTA.", "published": "2021-07-20 06:44:13", "link": "http://arxiv.org/abs/2107.09278v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BoningKnife: Joint Entity Mention Detection and Typing for Nested NER\n  via prior Boundary Knowledge", "abstract": "While named entity recognition (NER) is a key task in natural language\nprocessing, most approaches only target flat entities, ignoring nested\nstructures which are common in many scenarios. Most existing nested NER methods\ntraverse all sub-sequences which is both expensive and inefficient, and also\ndon't well consider boundary knowledge which is significant for nested\nentities. In this paper, we propose a joint entity mention detection and typing\nmodel via prior boundary knowledge (BoningKnife) to better handle nested NER\nextraction and recognition tasks. BoningKnife consists of two modules,\nMentionTagger and TypeClassifier. MentionTagger better leverages boundary\nknowledge beyond just entity start/end to improve the handling of nesting\nlevels and longer spans, while generating high quality mention candidates.\nTypeClassifier utilizes a two-level attention mechanism to decouple different\nnested level representations and better distinguish entity types. We jointly\ntrain both modules sharing a common representation and a new dual-info\nattention layer, which leads to improved representation focus on entity-related\ninformation. Experiments over different datasets show that our approach\noutperforms previous state of the art methods and achieves 86.41, 85.46, and\n94.2 F1 scores on ACE2004, ACE2005, and NNE, respectively.", "published": "2021-07-20 11:44:36", "link": "http://arxiv.org/abs/2107.09429v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Seed Words Based Data Selection for Language Model Adaptation", "abstract": "We address the problem of language model customization in applications where\nthe ASR component needs to manage domain-specific terminology; although current\nstate-of-the-art speech recognition technology provides excellent results for\ngeneric domains, the adaptation to specialized dictionaries or glossaries is\nstill an open issue. In this work we present an approach for automatically\nselecting sentences, from a text corpus, that match, both semantically and\nmorphologically, a glossary of terms (words or composite words) furnished by\nthe user. The final goal is to rapidly adapt the language model of an hybrid\nASR system with a limited amount of in-domain text data in order to\nsuccessfully cope with the linguistic domain at hand; the vocabulary of the\nbaseline model is expanded and tailored, reducing the resulting OOV rate. Data\nselection strategies based on shallow morphological seeds and semantic\nsimilarity viaword2vec are introduced and discussed; the experimental setting\nconsists in a simultaneous interpreting scenario, where ASRs in three languages\nare designed to recognize the domain-specific terms (i.e. dentistry). Results\nusing different metrics (OOV rate, WER, precision and recall) show the\neffectiveness of the proposed techniques.", "published": "2021-07-20 12:08:27", "link": "http://arxiv.org/abs/2107.09433v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "More Parameters? No Thanks!", "abstract": "This work studies the long-standing problems of model capacity and negative\ninterference in multilingual neural machine translation MNMT. We use network\npruning techniques and observe that pruning 50-70% of the parameters from a\ntrained MNMT model results only in a 0.29-1.98 drop in the BLEU score.\nSuggesting that there exist large redundancies even in MNMT models. These\nobservations motivate us to use the redundant parameters and counter the\ninterference problem efficiently. We propose a novel adaptation strategy, where\nwe iteratively prune and retrain the redundant parameters of an MNMT to improve\nbilingual representations while retaining the multilinguality. Negative\ninterference severely affects high resource languages, and our method\nalleviates it without any additional adapter modules. Hence, we call it\nparameter-free adaptation strategy, paving way for the efficient adaptation of\nMNMT. We demonstrate the effectiveness of our method on a 9 language MNMT\ntrained on TED talks, and report an average improvement of +1.36 BLEU on high\nresource pairs. Code will be released here.", "published": "2021-07-20 17:04:15", "link": "http://arxiv.org/abs/2107.09622v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning ULMFiT and Self-Distillation with Calibration for Medical\n  Dialogue System", "abstract": "A medical dialogue system is essential for healthcare service as providing\nprimary clinical advice and diagnoses. It has been gradually adopted and\npracticed in medical organizations in the form of a conversational bot, largely\ndue to the advancement of NLP. In recent years, the introduction of\nstate-of-the-art deep learning models and transfer learning techniques like\nUniversal Language Model Fine Tuning (ULMFiT) and Knowledge Distillation (KD)\nlargely contributes to the performance of NLP tasks. However, some deep neural\nnetworks are poorly calibrated and wrongly estimate the uncertainty. Hence the\nmodel is not trustworthy, especially in sensitive medical decision-making\nsystems and safety tasks. In this paper, we investigate the well-calibrated\nmodel for ULMFiT and self-distillation (SD) in a medical dialogue system. The\ncalibrated ULMFiT (CULMFiT) is obtained by incorporating label smoothing (LS),\na commonly used regularization technique to achieve a well-calibrated model.\nMoreover, we apply the technique to recalibrate the confidence score called\ntemperature scaling (TS) with KD to observe its correlation with network\ncalibration. To further understand the relation between SD and calibration, we\nuse both fixed and optimal temperatures to fine-tune the whole model. All\nexperiments are conducted on the consultation backpain dataset collected by\nexperts then further validated using a large publicly medial dialogue corpus.\nWe empirically show that our proposed methodologies outperform conventional\nmethods in terms of accuracy and robustness.", "published": "2021-07-20 17:11:24", "link": "http://arxiv.org/abs/2107.09625v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TLA: Twitter Linguistic Analysis", "abstract": "Linguistics has been instrumental in developing a deeper understanding of\nhuman nature. Words are indispensable to bequeath the thoughts, emotions, and\npurpose of any human interaction, and critically analyzing these words can\nelucidate the social and psychological behavior and characteristics of these\nsocial animals. Social media has become a platform for human interaction on a\nlarge scale and thus gives us scope for collecting and using that data for our\nstudy. However, this entire process of collecting, labeling, and analyzing this\ndata iteratively makes the entire procedure cumbersome. To make this entire\nprocess easier and structured, we would like to introduce TLA(Twitter\nLinguistic Analysis). In this paper, we describe TLA and provide a basic\nunderstanding of the framework and discuss the process of collecting, labeling,\nand analyzing data from Twitter for a corpus of languages while providing\ndetailed labeled datasets for all the languages and the models are trained on\nthese datasets. The analysis provided by TLA will also go a long way in\nunderstanding the sentiments of different linguistic communities and come up\nwith new and innovative solutions for their problems based on the analysis.", "published": "2021-07-20 18:25:48", "link": "http://arxiv.org/abs/2107.09710v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Abstructions: Abstractions that Support Construction for Grounded\n  Language Learning", "abstract": "Although virtual agents are increasingly situated in environments where\nnatural language is the most effective mode of interaction with humans, these\nexchanges are rarely used as an opportunity for learning. Leveraging language\ninteractions effectively requires addressing limitations in the two most common\napproaches to language grounding: semantic parsers built on top of fixed object\ncategories are precise but inflexible and end-to-end models are maximally\nexpressive, but fickle and opaque. Our goal is to develop a system that\nbalances the strengths of each approach so that users can teach agents new\ninstructions that generalize broadly from a single example. We introduce the\nidea of neural abstructions: a set of constraints on the inference procedure of\na label-conditioned generative model that can affect the meaning of the label\nin context. Starting from a core programming language that operates over\nabstructions, users can define increasingly complex mappings from natural\nlanguage to actions. We show that with this method a user population is able to\nbuild a semantic parser for an open-ended house modification task in Minecraft.\nThe semantic parser that results is both flexible and expressive: the\npercentage of utterances sourced from redefinitions increases steadily over the\ncourse of 191 total exchanges, achieving a final value of 28%.", "published": "2021-07-20 07:01:15", "link": "http://arxiv.org/abs/2107.09285v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Audio2Head: Audio-driven One-shot Talking-head Generation with Natural\n  Head Motion", "abstract": "We propose an audio-driven talking-head method to generate photo-realistic\ntalking-head videos from a single reference image. In this work, we tackle two\nkey challenges: (i) producing natural head motions that match speech prosody,\nand (ii) maintaining the appearance of a speaker in a large head motion while\nstabilizing the non-face regions. We first design a head pose predictor by\nmodeling rigid 6D head movements with a motion-aware recurrent neural network\n(RNN). In this way, the predicted head poses act as the low-frequency holistic\nmovements of a talking head, thus allowing our latter network to focus on\ndetailed facial movement generation. To depict the entire image motions arising\nfrom audio, we exploit a keypoint based dense motion field representation.\nThen, we develop a motion field generator to produce the dense motion fields\nfrom input audio, head poses, and a reference image. As this keypoint based\nrepresentation models the motions of facial regions, head, and backgrounds\nintegrally, our method can better constrain the spatial and temporal\nconsistency of the generated videos. Finally, an image generation network is\nemployed to render photo-realistic talking-head videos from the estimated\nkeypoint based motion fields and the input reference image. Extensive\nexperiments demonstrate that our method produces videos with plausible head\nmotions, synchronized facial expressions, and stable backgrounds and\noutperforms the state-of-the-art.", "published": "2021-07-20 07:22:42", "link": "http://arxiv.org/abs/2107.09293v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Improving Sentence-Level Relation Extraction through Curriculum Learning", "abstract": "Sentence-level relation extraction mainly aims to classify the relation\nbetween two entities in a sentence. The sentence-level relation extraction\ncorpus often contains data that are difficult for the model to infer or noise\ndata. In this paper, we propose a curriculum learning-based relation extraction\nmodel that splits data by difficulty and utilizes them for learning. In the\nexperiments with the representative sentence-level relation extraction\ndatasets, TACRED and Re-TACRED, the proposed method obtained an F1-score of\n75.0% and 91.4% respectively, which are the state-of-the-art performance.", "published": "2021-07-20 08:44:40", "link": "http://arxiv.org/abs/2107.09332v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Toward Collaborative Reinforcement Learning Agents that Communicate\n  Through Text-Based Natural Language", "abstract": "Communication between agents in collaborative multi-agent settings is in\ngeneral implicit or a direct data stream. This paper considers text-based\nnatural language as a novel form of communication between multiple agents\ntrained with reinforcement learning. This could be considered first steps\ntoward a truly autonomous communication without the need to define a limited\nset of instructions, and natural collaboration between humans and robots.\nInspired by the game of Blind Leads, we propose an environment where one agent\nuses natural language instructions to guide another through a maze. We test the\nability of reinforcement learning agents to effectively communicate through\ndiscrete word-level symbols and show that the agents are able to sufficiently\ncommunicate through natural language with a limited vocabulary. Although the\ncommunication is not always perfect English, the agents are still able to\nnavigate the maze. We achieve a BLEU score of 0.85, which is an improvement of\n0.61 over randomly generated sequences while maintaining a 100% maze completion\nrate. This is a 3.5 times the performance of the random baseline using our\nreference set.", "published": "2021-07-20 09:19:29", "link": "http://arxiv.org/abs/2107.09356v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Dataset", "abstract": "We present a new dataset of Wikipedia articles each paired with a knowledge\ngraph, to facilitate the research in conditional text generation, graph\ngeneration and graph representation learning. Existing graph-text paired\ndatasets typically contain small graphs and short text (1 or few sentences),\nthus limiting the capabilities of the models that can be learned on the data.\nOur new dataset WikiGraphs is collected by pairing each Wikipedia article from\nthe established WikiText-103 benchmark (Merity et al., 2016) with a subgraph\nfrom the Freebase knowledge graph (Bollacker et al., 2008). This makes it easy\nto benchmark against other state-of-the-art text generative models that are\ncapable of generating long paragraphs of coherent text. Both the graphs and the\ntext data are of significantly larger scale compared to prior graph-text paired\ndatasets. We present baseline graph neural network and transformer model\nresults on our dataset for 3 tasks: graph -> text generation, graph -> text\nretrieval and text -> graph retrieval. We show that better conditioning on the\ngraph provides gains in generation and retrieval quality but there is still\nlarge room for improvement.", "published": "2021-07-20 15:18:30", "link": "http://arxiv.org/abs/2107.09556v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generative Video Transformer: Can Objects be the Words?", "abstract": "Transformers have been successful for many natural language processing tasks.\nHowever, applying transformers to the video domain for tasks such as long-term\nvideo generation and scene understanding has remained elusive due to the high\ncomputational complexity and the lack of natural tokenization. In this paper,\nwe propose the Object-Centric Video Transformer (OCVT) which utilizes an\nobject-centric approach for decomposing scenes into tokens suitable for use in\na generative video transformer. By factoring the video into objects, our fully\nunsupervised model is able to learn complex spatio-temporal dynamics of\nmultiple interacting objects in a scene and generate future frames of the\nvideo. Our model is also significantly more memory-efficient than pixel-based\nmodels and thus able to train on videos of length up to 70 frames with a single\n48GB GPU. We compare our model with previous RNN-based approaches as well as\nother possible video transformer baselines. We demonstrate OCVT performs well\nwhen compared to baselines in generating future frames. OCVT also develops\nuseful representations for video reasoning, achieving start-of-the-art\nperformance on the CATER task.", "published": "2021-07-20 03:08:39", "link": "http://arxiv.org/abs/2107.09240v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "StreamBlocks: A compiler for heterogeneous dataflow computing (technical\n  report)", "abstract": "To increase performance and efficiency, systems use FPGAs as reconfigurable\naccelerators. A key challenge in designing these systems is partitioning\ncomputation between processors and an FPGA. An appropriate division of labor\nmay be difficult to predict in advance and require experiments and\nmeasurements. When an investigation requires rewriting part of the system in a\nnew language or with a new programming model, its high cost can retard the\nstudy of different configurations. A single-language system with an appropriate\nprogramming model and compiler that targets both platforms simplifies this\nexploration to a simple recompile with new compiler directives.\n  This work introduces StreamBlocks, an open-source compiler and runtime that\nuses the CAL dataflow programming language to partition computations across\nheterogeneous (CPU/accelerator) platforms. Because of the dataflow model's\nsemantics and the CAL language, StreamBlocks can exploit both thread\nparallelism in multi-core CPUs and the inherent parallelism of FPGAs.\nStreamBlocks supports exploring the design space with a profile-guided tool\nthat helps identify the best hardware-software partitions.", "published": "2021-07-20 08:46:47", "link": "http://arxiv.org/abs/2107.09333v1", "categories": ["cs.AR", "cs.CL", "cs.PF", "C.5; D.1.3; D.3.0; I.6.5; B.6.0; B.8.2; B.4.0"], "primary_category": "cs.AR"}
{"title": "On Prosody Modeling for ASR+TTS based Voice Conversion", "abstract": "In voice conversion (VC), an approach showing promising results in the latest\nvoice conversion challenge (VCC) 2020 is to first use an automatic speech\nrecognition (ASR) model to transcribe the source speech into the underlying\nlinguistic contents; these are then used as input by a text-to-speech (TTS)\nsystem to generate the converted speech. Such a paradigm, referred to as\nASR+TTS, overlooks the modeling of prosody, which plays an important role in\nspeech naturalness and conversion similarity. Although some researchers have\nconsidered transferring prosodic clues from the source speech, there arises a\nspeaker mismatch during training and conversion. To address this issue, in this\nwork, we propose to directly predict prosody from the linguistic representation\nin a target-speaker-dependent manner, referred to as target text prediction\n(TTP). We evaluate both methods on the VCC2020 benchmark and consider different\nlinguistic representations. The results demonstrate the effectiveness of TTP in\nboth objective and subjective evaluations.", "published": "2021-07-20 13:30:23", "link": "http://arxiv.org/abs/2107.09477v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "QVHighlights: Detecting Moments and Highlights in Videos via Natural\n  Language Queries", "abstract": "Detecting customized moments and highlights from videos given natural\nlanguage (NL) user queries is an important but under-studied topic. One of the\nchallenges in pursuing this direction is the lack of annotated data. To address\nthis issue, we present the Query-based Video Highlights (QVHIGHLIGHTS) dataset.\nIt consists of over 10,000 YouTube videos, covering a wide range of topics,\nfrom everyday activities and travel in lifestyle vlog videos to social and\npolitical activities in news videos. Each video in the dataset is annotated\nwith: (1) a human-written free-form NL query, (2) relevant moments in the video\nw.r.t. the query, and (3) five-point scale saliency scores for all\nquery-relevant clips. This comprehensive annotation enables us to develop and\nevaluate systems that detect relevant moments as well as salient highlights for\ndiverse, flexible user queries. We also present a strong baseline for this\ntask, Moment-DETR, a transformer encoder-decoder model that views moment\nretrieval as a direct set prediction problem, taking extracted video and query\nrepresentations as inputs and predicting moment coordinates and saliency scores\nend-to-end. While our model does not utilize any human prior, we show that it\nperforms competitively when compared to well-engineered architectures. With\nweakly supervised pretraining using ASR captions, MomentDETR substantially\noutperforms previous methods. Lastly, we present several ablations and\nvisualizations of Moment-DETR. Data and code is publicly available at\nhttps://github.com/jayleicn/moment_detr", "published": "2021-07-20 16:42:58", "link": "http://arxiv.org/abs/2107.09609v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Different kinds of cognitive plausibility: why are transformers better\n  than RNNs at predicting N400 amplitude?", "abstract": "Despite being designed for performance rather than cognitive plausibility,\ntransformer language models have been found to be better at predicting metrics\nused to assess human language comprehension than language models with other\narchitectures, such as recurrent neural networks. Based on how well they\npredict the N400, a neural signal associated with processing difficulty, we\npropose and provide evidence for one possible explanation - their predictions\nare affected by the preceding context in a way analogous to the effect of\nsemantic facilitation in humans.", "published": "2021-07-20 17:33:13", "link": "http://arxiv.org/abs/2107.09648v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "What Do You Get When You Cross Beam Search with Nucleus Sampling?", "abstract": "We combine beam search with the probabilistic pruning technique of nucleus\nsampling to create two deterministic nucleus search algorithms for natural\nlanguage generation. The first algorithm, p-exact search, locally prunes the\nnext-token distribution and performs an exact search over the remaining space.\nThe second algorithm, dynamic beam search, shrinks and expands the beam size\naccording to the entropy of the candidate's probability distribution. Despite\nthe probabilistic intuition behind nucleus search, experiments on machine\ntranslation and summarization benchmarks show that both algorithms reach the\nsame performance levels as standard beam search.", "published": "2021-07-20 18:59:14", "link": "http://arxiv.org/abs/2107.09729v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Checkovid: A COVID-19 misinformation detection system on Twitter using\n  network and content mining perspectives", "abstract": "During the COVID-19 pandemic, social media platforms were ideal for\ncommunicating due to social isolation and quarantine. Also, it was the primary\nsource of misinformation dissemination on a large scale, referred to as the\ninfodemic. Therefore, automatic debunking misinformation is a crucial problem.\nTo tackle this problem, we present two COVID-19 related misinformation datasets\non Twitter and propose a misinformation detection system comprising\nnetwork-based and content-based processes based on machine learning algorithms\nand NLP techniques. In the network-based process, we focus on social\nproperties, network characteristics, and users. On the other hand, we classify\nmisinformation using the content of the tweets directly in the content-based\nprocess, which contains text classification models (paragraph-level and\nsentence-level) and similarity models. The evaluation results on the\nnetwork-based process show the best results for the artificial neural network\nmodel with an F1 score of 88.68%. In the content-based process, our novel\nsimilarity models, which obtained an F1 score of 90.26%, show an improvement in\nthe misinformation classification results compared to the network-based models.\nIn addition, in the text classification models, the best result was achieved\nusing the stacking ensemble-learning model by obtaining an F1 score of 95.18%.\nFurthermore, we test our content-based models on the Constraint@AAAI2021\ndataset, and by getting an F1 score of 94.38%, we improve the baseline results.\nFinally, we develop a fact-checking website called Checkovid that uses each\nprocess to detect misinformative and informative claims in the domain of\nCOVID-19 from different perspectives.", "published": "2021-07-20 20:58:23", "link": "http://arxiv.org/abs/2107.09768v1", "categories": ["cs.LG", "cs.CL", "cs.SI", "68T05, 68T07", "I.2; I.5"], "primary_category": "cs.LG"}
{"title": "Neural Variational Learning for Grounded Language Acquisition", "abstract": "We propose a learning system in which language is grounded in visual percepts\nwithout specific pre-defined categories of terms. We present a unified\ngenerative method to acquire a shared semantic/visual embedding that enables\nthe learning of language about a wide range of real-world objects. We evaluate\nthe efficacy of this learning by predicting the semantics of objects and\ncomparing the performance with neural and non-neural inputs. We show that this\ngenerative approach exhibits promising results in language grounding without\npre-specifying visual categories under low resource settings. Our experiments\ndemonstrate that this approach is generalizable to multilingual, highly varied\ndatasets.", "published": "2021-07-20 20:55:02", "link": "http://arxiv.org/abs/2107.14593v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Robust Deep Learning Frameworks for Acoustic Scene and Respiratory Sound\n  Classification", "abstract": "This thesis focuses on dealing with the task of acoustic scene classification\n(ASC), and then applied the techniques developed for ASC to a real-life\napplication of detecting respiratory disease. To deal with ASC challenges, this\nthesis addresses three main factors that directly affect the performance of an\nASC system. Firstly, this thesis explores input features by making use of\nmultiple spectrograms (log-mel, Gamma, and CQT) for low-level feature\nextraction to tackle the issue of insufficiently discriminative or descriptive\ninput features. Next, a novel Encoder network architecture is introduced. The\nEncoder firstly transforms each low-level spectrogram into high-level\nintermediate features, or embeddings, and thus combines these high-level\nfeatures to form a very distinct composite feature. The composite or combined\nfeature is then explored in terms of classification performance, with different\nDecoders such as Random Forest (RF), Multilayer Perception (MLP), and Mixture\nof Experts (MoE). By using this Encoder-Decoder framework, it helps to reduce\nthe computation cost of the reference process in ASC systems which make use of\nmultiple spectrogram inputs. Since the proposed techniques applied for general\nASC tasks were shown to be highly effective, this inspired an application to a\nspecific real-life application. This was namely the 2017 Internal Conference on\nBiomedical Health Informatics (ICBHI) respiratory sound dataset. Building upon\nthe proposed ASC framework, the ICBHI tasks were tackled with a deep learning\nframework, and the resulting system shown to be capable at detecting\nrespiratory anomaly cycles and diseases.", "published": "2021-07-20 05:41:00", "link": "http://arxiv.org/abs/2107.09268v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Joint Echo Cancellation and Noise Suppression based on Cascaded\n  Magnitude and Complex Mask Estimation", "abstract": "Acoustic echo and background noise can seriously degrade the intelligibility\nof speech. In practice, echo and noise suppression are usually treated as two\nseparated tasks and can be removed with various digital signal processing (DSP)\nand deep learning techniques. In this paper, we propose a new cascaded model,\nmagnitude and complex temporal convolutional neural network (MC-TCN), to\njointly perform acoustic echo cancellation and noise suppression with the help\nof adaptive filters. The MC-TCN cascades two separation cores, which are used\nto extract robust magnitude spectra feature and to enhance magnitude and phase\nsimultaneously. Experimental results reveal that the proposed method can\nachieve superior performance by removing both echo and noise in real-time. In\nterms of DECMOS, the subjective test shows our method achieves a mean score of\n4.41 and outperforms the INTERSPEECH2021 AEC-Challenge baseline by 0.54.", "published": "2021-07-20 07:27:06", "link": "http://arxiv.org/abs/2107.09298v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "PERSA+: A Deep Learning Front-End for Context-Agnostic Audio\n  Classification", "abstract": "Deep learning has been applied to diverse audio semantics tasks, enabling the\nconstruction of models that learn hierarchical levels of features from\nhigh-dimensional raw data, delivering state-of-the-art performance. But do\nthese algorithms perform similarly in real-world conditions, or just at the\nbenchmark, where their high learning capability assures the complete\nmemorization of the employed datasets? This work presents a deep learning\nfront-end, aiming at discarding detrimental information before entering the\nmodeling stage, bringing the learning process closer to the point, anticipating\nthe development of robust and context-agnostic classification algorithms.", "published": "2021-07-20 08:03:04", "link": "http://arxiv.org/abs/2107.09311v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Assessment of Self-Attention on Learned Features For Sound Event\n  Localization and Detection", "abstract": "Joint sound event localization and detection (SELD) is an emerging audio\nsignal processing task adding spatial dimensions to acoustic scene analysis and\nsound event detection. A popular approach to modeling SELD jointly is using\nconvolutional recurrent neural network (CRNN) models, where CNNs learn\nhigh-level features from multi-channel audio input and the RNNs learn temporal\nrelationships from these high-level features. However, RNNs have some\ndrawbacks, such as a limited capability to model long temporal dependencies and\nslow training and inference times due to their sequential processing nature.\nRecently, a few SELD studies used multi-head self-attention (MHSA), among other\ninnovations in their models. MHSA and the related transformer networks have\nshown state-of-the-art performance in various domains. While they can model\nlong temporal dependencies, they can also be parallelized efficiently. In this\npaper, we study in detail the effect of MHSA on the SELD task. Specifically, we\nexamined the effects of replacing the RNN blocks with self-attention layers. We\nstudied the influence of stacking multiple self-attention blocks, using\nmultiple attention heads in each self-attention block, and the effect of\nposition embeddings and layer normalization. Evaluation on the DCASE 2021 SELD\n(task 3) development data set shows a significant improvement in all employed\nmetrics compared to the baseline CRNN accompanying the task.", "published": "2021-07-20 10:12:39", "link": "http://arxiv.org/abs/2107.09388v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Music Tempo Estimation via Neural Networks -- A Comparative Analysis", "abstract": "This paper presents a comparative analysis on two artificial neural networks\n(with different architectures) for the task of tempo estimation. For this\npurpose, it also proposes the modeling, training and evaluation of a B-RNN\n(Bidirectional Recurrent Neural Network) model capable of estimating tempo in\nbpm (beats per minutes) of musical pieces, without using external auxiliary\nmodules. An extensive database (12,550 pieces in total) was curated to conduct\na quantitative and qualitative analysis over the experiment. Percussion-only\ntracks were also included in the dataset. The performance of the B-RNN is\ncompared to that of state-of-the-art models. For further comparison, a\nstate-of-the-art CNN was also retrained with the same datasets used for the\nB-RNN training. Evaluation results for each model and datasets are presented\nand discussed, as well as observations and ideas for future research. Tempo\nestimation was more accurate for the percussion only dataset, suggesting that\nthe estimation can be more accurate for percussion-only tracks, although\nfurther experiments (with more of such datasets) should be made to gather\nstronger evidence.", "published": "2021-07-20 00:29:28", "link": "http://arxiv.org/abs/2107.09208v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "I.2; I.5.4; J.5"], "primary_category": "cs.SD"}
{"title": "A Real-time Speaker Diarization System Based on Spatial Spectrum", "abstract": "In this paper we describe a speaker diarization system that enables\nlocalization and identification of all speakers present in a conversation or\nmeeting. We propose a novel systematic approach to tackle several long-standing\nchallenges in speaker diarization tasks: (1) to segment and separate\noverlapping speech from two speakers; (2) to estimate the number of speakers\nwhen participants may enter or leave the conversation at any time; (3) to\nprovide accurate speaker identification on short text-independent utterances;\n(4) to track down speakers movement during the conversation; (5) to detect\nspeaker change incidence real-time. First, a differential directional\nmicrophone array-based approach is exploited to capture the target speakers'\nvoice in far-field adverse environment. Second, an online speaker-location\njoint clustering approach is proposed to keep track of speaker location. Third,\nan instant speaker number detector is developed to trigger the mechanism that\nseparates overlapped speech. The results suggest that our system effectively\nincorporates spatial information and achieves significant gains.", "published": "2021-07-20 08:25:23", "link": "http://arxiv.org/abs/2107.09321v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SVSNet: An End-to-end Speaker Voice Similarity Assessment Model", "abstract": "Neural evaluation metrics derived for numerous speech generation tasks have\nrecently attracted great attention. In this paper, we propose SVSNet, the first\nend-to-end neural network model to assess the speaker voice similarity between\nconverted speech and natural speech for voice conversion tasks. Unlike most\nneural evaluation metrics that use hand-crafted features, SVSNet directly takes\nthe raw waveform as input to more completely utilize speech information for\nprediction. SVSNet consists of encoder, co-attention, distance calculation, and\nprediction modules and is trained in an end-to-end manner. The experimental\nresults on the Voice Conversion Challenge 2018 and 2020 (VCC2018 and VCC2020)\ndatasets show that SVSNet outperforms well-known baseline systems in the\nassessment of speaker similarity at the utterance and system levels.", "published": "2021-07-20 10:19:46", "link": "http://arxiv.org/abs/2107.09392v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Streaming End-to-End ASR based on Blockwise Non-Autoregressive Models", "abstract": "Non-autoregressive (NAR) modeling has gained more and more attention in\nspeech processing. With recent state-of-the-art attention-based automatic\nspeech recognition (ASR) structure, NAR can realize promising real-time factor\n(RTF) improvement with only small degradation of accuracy compared to the\nautoregressive (AR) models. However, the recognition inference needs to wait\nfor the completion of a full speech utterance, which limits their applications\non low latency scenarios. To address this issue, we propose a novel end-to-end\nstreaming NAR speech recognition system by combining blockwise-attention and\nconnectionist temporal classification with mask-predict (Mask-CTC) NAR. During\ninference, the input audio is separated into small blocks and then processed in\na blockwise streaming way. To address the insertion and deletion error at the\nedge of the output of each block, we apply an overlapping decoding strategy\nwith a dynamic mapping trick that can produce more coherent sentences.\nExperimental results show that the proposed method improves online ASR\nrecognition in low latency conditions compared to vanilla Mask-CTC. Moreover,\nit can achieve a much faster inference speed compared to the AR attention-based\nmodels. All of our codes will be publicly available at\nhttps://github.com/espnet/espnet.", "published": "2021-07-20 11:42:26", "link": "http://arxiv.org/abs/2107.09428v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Canonical Polyadic Decomposition and Deep Learning for Machine Fault\n  Detection", "abstract": "Acoustic monitoring for machine fault detection is a recent and expanding\nresearch path that has already provided promising results for industries.\nHowever, it is impossible to collect enough data to learn all types of faults\nfrom a machine. Thus, new algorithms, trained using data from healthy\nconditions only, were developed to perform unsupervised anomaly detection. A\nkey issue in the development of these algorithms is the noise in the signals,\nas it impacts the anomaly detection performance. In this work, we propose a\npowerful data-driven and quasi non-parametric denoising strategy for spectral\ndata based on a tensor decomposition: the Non-negative Canonical Polyadic (CP)\ndecomposition. This method is particularly adapted for machine emitting\nstationary sound. We demonstrate in a case study, the Malfunctioning Industrial\nMachine Investigation and Inspection (MIMII) baseline, how the use of our\ndenoising strategy leads to a sensible improvement of the unsupervised anomaly\ndetection. Such approaches are capable to make sound-based monitoring of\nindustrial processes more reliable.", "published": "2021-07-20 14:06:50", "link": "http://arxiv.org/abs/2107.09519v1", "categories": ["stat.ML", "cs.LG", "eess.AS"], "primary_category": "stat.ML"}
{"title": "Human Perception of Audio Deepfakes", "abstract": "The recent emergence of deepfakes has brought manipulated and generated\ncontent to the forefront of machine learning research. Automatic detection of\ndeepfakes has seen many new machine learning techniques, however, human\ndetection capabilities are far less explored. In this paper, we present results\nfrom comparing the abilities of humans and machines for detecting audio\ndeepfakes used to imitate someone's voice. For this, we use a web-based\napplication framework formulated as a game. Participants were asked to\ndistinguish between real and fake audio samples. In our experiment, 472 unique\nusers competed against a state-of-the-art AI deepfake detection algorithm for\n14912 total of rounds of the game. We find that humans and deepfake detection\nalgorithms share similar strengths and weaknesses, both struggling to detect\ncertain types of attacks. This is in contrast to the superhuman performance of\nAI in many application areas such as object detection or face recognition.\nConcerning human success factors, we find that IT professionals have no\nadvantage over non-professionals but native speakers have an advantage over\nnon-native speakers. Additionally, we find that older participants tend to be\nmore susceptible than younger ones. These insights may be helpful when\ndesigning future cybersecurity training for humans as well as developing better\ndetection algorithms.", "published": "2021-07-20 09:19:42", "link": "http://arxiv.org/abs/2107.09667v7", "categories": ["cs.HC", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
