{"title": "Lingxi: A Diversity-aware Chinese Modern Poetry Generation System", "abstract": "Poetry generation has been a difficult task in natural language processing.\nUnlike plain neural text generation tasks, poetry has a high requirement for\nnovelty, since an easily-understood sentence with too many high frequency words\nmight not be considered as poetic, while adequately ambiguous sentences with\nlow frequency words can possibly be novel and creative. Inspired by this, we\npresent Lingxi, a diversity-aware Chinese modern poetry generation system. We\npropose nucleus sampling with randomized head (NS-RH) algorithm, which\nrandomizes the high frequency part (\"head\") of the predicted distribution, in\norder to emphasize on the \"comparatively low frequency\" words. The proposed\nalgorithm can significantly increase the novelty of generated poetry compared\nwith traditional sampling methods. The permutation of distribution is\ncontrollable by tuning the filtering parameter that determines the \"head\" to\npermutate, achieving diversity-aware sampling. We find that even when a large\nportion of filtered vocabulary is randomized, it can actually generate fluent\npoetry but with notably higher novelty. We also propose a\nsemantic-similarity-based rejection sampling algorithm, which creates longer\nand more informative context on the basis of the short input poetry title while\nmaintaining high semantic similarity to the title, alleviating the off-topic\nissue.", "published": "2021-08-27 03:33:28", "link": "http://arxiv.org/abs/2108.12108v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Generation of Accurate \\& Fluent Medical X-ray Reports", "abstract": "Our paper focuses on automating the generation of medical reports from chest\nX-ray image inputs, a critical yet time-consuming task for radiologists. Unlike\nexisting medical re-port generation efforts that tend to produce human-readable\nreports, we aim to generate medical reports that are both fluent and clinically\naccurate. This is achieved by our fully differentiable and end-to-end paradigm\ncontaining three complementary modules: taking the chest X-ray images and\nclinical his-tory document of patients as inputs, our classification module\nproduces an internal check-list of disease-related topics, referred to as\nenriched disease embedding; the embedding representation is then passed to our\ntransformer-based generator, giving rise to the medical reports; meanwhile, our\ngenerator also pro-duces the weighted embedding representation, which is fed to\nour interpreter to ensure consistency with respect to disease-related\ntopics.Our approach achieved promising results on commonly-used metrics\nconcerning language fluency and clinical accuracy. Moreover, noticeable\nperformance gains are consistently ob-served when additional input information\nis available, such as the clinical document and extra scans of different views.", "published": "2021-08-27 05:47:28", "link": "http://arxiv.org/abs/2108.12126v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Secoco: Self-Correcting Encoding for Neural Machine Translation", "abstract": "This paper presents Self-correcting Encoding (Secoco), a framework that\neffectively deals with input noise for robust neural machine translation by\nintroducing self-correcting predictors. Different from previous robust\napproaches, Secoco enables NMT to explicitly correct noisy inputs and delete\nspecific errors simultaneously with the translation decoding process. Secoco is\nable to achieve significant improvements over strong baselines on two\nreal-world test sets and a benchmark WMT dataset with good interpretability. We\nwill make our code and dataset publicly available soon.", "published": "2021-08-27 07:04:45", "link": "http://arxiv.org/abs/2108.12137v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Offensive Language Identification in Low-resourced Code-mixed Dravidian\n  languages using Pseudo-labeling", "abstract": "Social media has effectively become the prime hub of communication and\ndigital marketing. As these platforms enable the free manifestation of thoughts\nand facts in text, images and video, there is an extensive need to screen them\nto protect individuals and groups from offensive content targeted at them. Our\nwork intends to classify codemixed social media comments/posts in the Dravidian\nlanguages of Tamil, Kannada, and Malayalam. We intend to improve offensive\nlanguage identification by generating pseudo-labels on the dataset. A custom\ndataset is constructed by transliterating all the code-mixed texts into the\nrespective Dravidian language, either Kannada, Malayalam, or Tamil and then\ngenerating pseudo-labels for the transliterated dataset. The two datasets are\ncombined using the generated pseudo-labels to create a custom dataset called\nCMTRA. As Dravidian languages are under-resourced, our approach increases the\namount of training data for the language models. We fine-tune several recent\npretrained language models on the newly constructed dataset. We extract the\npretrained language embeddings and pass them onto recurrent neural networks. We\nobserve that fine-tuning ULMFiT on the custom dataset yields the best results\non the code-mixed test sets of all three languages. Our approach yields the\nbest results among the benchmarked models on Tamil-English, achieving a\nweighted F1-Score of 0.7934 while scoring competitive weighted F1-Scores of\n0.9624 and 0.7306 on the code-mixed test sets of Malayalam-English and\nKannada-English, respectively.", "published": "2021-08-27 08:43:08", "link": "http://arxiv.org/abs/2108.12177v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Translation Error Detection as Rationale Extraction", "abstract": "Recent Quality Estimation (QE) models based on multilingual pre-trained\nrepresentations have achieved very competitive results when predicting the\noverall quality of translated sentences. Predicting translation errors, i.e.\ndetecting specifically which words are incorrect, is a more challenging task,\nespecially with limited amounts of training data. We hypothesize that, not\nunlike humans, successful QE models rely on translation errors to predict\noverall sentence quality. By exploring a set of feature attribution methods\nthat assign relevance scores to the inputs to explain model predictions, we\nstudy the behaviour of state-of-the-art sentence-level QE models and show that\nexplanations (i.e. rationales) extracted from these models can indeed be used\nto detect translation errors. We therefore (i) introduce a novel\nsemi-supervised method for word-level QE and (ii) propose to use the QE task as\na new benchmark for evaluating the plausibility of feature attribution, i.e.\nhow interpretable model explanations are to humans.", "published": "2021-08-27 09:35:14", "link": "http://arxiv.org/abs/2108.12197v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Partition Filter Network for Joint Entity and Relation Extraction", "abstract": "In joint entity and relation extraction, existing work either sequentially\nencode task-specific features, leading to an imbalance in inter-task feature\ninteraction where features extracted later have no direct contact with those\nthat come first. Or they encode entity features and relation features in a\nparallel manner, meaning that feature representation learning for each task is\nlargely independent of each other except for input sharing. We propose a\npartition filter network to model two-way interaction between tasks properly,\nwhere feature encoding is decomposed into two steps: partition and filter. In\nour encoder, we leverage two gates: entity and relation gate, to segment\nneurons into two task partitions and one shared partition. The shared partition\nrepresents inter-task information valuable to both tasks and is evenly shared\nacross two tasks to ensure proper two-way interaction. The task partitions\nrepresent intra-task information and are formed through concerted efforts of\nboth gates, making sure that encoding of task-specific features is dependent\nupon each other. Experiment results on six public datasets show that our model\nperforms significantly better than previous approaches. In addition, contrary\nto what previous work has claimed, our auxiliary experiments suggest that\nrelation prediction is contributory to named entity prediction in a\nnon-negligible way. The source code can be found at\nhttps://github.com/Coopercoppers/PFN.", "published": "2021-08-27 09:44:23", "link": "http://arxiv.org/abs/2108.12202v8", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Capacity of a Large-scale Masked Language Model to\n  Recognize Grammatical Errors", "abstract": "In this paper, we explore the capacity of a language model-based method for\ngrammatical error detection in detail. We first show that 5 to 10% of training\ndata are enough for a BERT-based error detection method to achieve performance\nequivalent to a non-language model-based method can achieve with the full\ntraining data; recall improves much faster with respect to training data size\nin the BERT-based method than in the non-language model method while precision\nbehaves similarly. These suggest that (i) the BERT-based method should have a\ngood knowledge of grammar required to recognize certain types of error and that\n(ii) it can transform the knowledge into error detection rules by fine-tuning\nwith a few training samples, which explains its high generalization ability in\ngrammatical error detection. We further show with pseudo error data that it\nactually exhibits such nice properties in learning rules for recognizing\nvarious types of error. Finally, based on these findings, we explore a\ncost-effective method for detecting grammatical errors with feedback comments\nexplaining relevant grammatical rules to learners.", "published": "2021-08-27 10:37:14", "link": "http://arxiv.org/abs/2108.12216v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ProtoInfoMax: Prototypical Networks with Mutual Information Maximization\n  for Out-of-Domain Detection", "abstract": "The ability to detect Out-of-Domain (OOD) inputs has been a critical\nrequirement in many real-world NLP applications. For example, intent\nclassification in dialogue systems. The reason is that the inclusion of\nunsupported OOD inputs may lead to catastrophic failure of systems. However, it\nremains an empirical question whether current methods can tackle such problems\nreliably in a realistic scenario where zero OOD training data is available. In\nthis study, we propose ProtoInfoMax, a new architecture that extends\nPrototypical Networks to simultaneously process in-domain and OOD sentences via\nMutual Information Maximization (InfoMax) objective. Experimental results show\nthat our proposed method can substantially improve performance up to 20% for\nOOD detection in low resource settings of text classification. We also show\nthat ProtoInfoMax is less prone to typical overconfidence errors of Neural\nNetworks, leading to more reliable prediction results.", "published": "2021-08-27 11:55:34", "link": "http://arxiv.org/abs/2108.12229v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tree Decomposition Attention for AMR-to-Text Generation", "abstract": "Text generation from AMR requires mapping a semantic graph to a string that\nit annotates. Transformer-based graph encoders, however, poorly capture vertex\ndependencies that may benefit sequence prediction. To impose order on an\nencoder, we locally constrain vertex self-attention using a graph's tree\ndecomposition. Instead of forming a full query-key bipartite graph, we restrict\nattention to vertices in parent, subtree, and same-depth bags of a vertex. This\nhierarchical context lends both sparsity and structure to vertex state updates.\nWe apply dynamic programming to derive a forest of tree decompositions,\nchoosing the most structurally similar tree to the AMR. Our system outperforms\na self-attentive baseline by 1.6 BLEU and 1.8 chrF++.", "published": "2021-08-27 14:24:25", "link": "http://arxiv.org/abs/2108.12300v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Latent Tree Decomposition Parsers for AMR-to-Text Generation", "abstract": "Graph encoders in AMR-to-text generation models often rely on neighborhood\nconvolutions or global vertex attention. While these approaches apply to\ngeneral graphs, AMRs may be amenable to encoders that target their tree-like\nstructure. By clustering edges into a hierarchy, a tree decomposition\nsummarizes graph structure. Our model encodes a derivation forest of tree\ndecompositions and extracts an expected tree. From tree node embeddings, it\nbuilds graph edge features used in vertex attention of the graph encoder.\nEncoding TD forests instead of shortest-pairwise paths in a self-attentive\nbaseline raises BLEU by 0.7 and chrF++ by 0.3. The forest encoder also\nsurpasses a convolutional baseline for molecular property prediction by 1.92%\nROC-AUC.", "published": "2021-08-27 14:30:35", "link": "http://arxiv.org/abs/2108.12304v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Train Short, Test Long: Attention with Linear Biases Enables Input\n  Length Extrapolation", "abstract": "Since the introduction of the transformer model by Vaswani et al. (2017), a\nfundamental question has yet to be answered: how does a model achieve\nextrapolation at inference time for sequences that are longer than it saw\nduring training? We first show that extrapolation can be enabled by simply\nchanging the position representation method, though we find that current\nmethods do not allow for efficient extrapolation. We therefore introduce a\nsimpler and more efficient position method, Attention with Linear Biases\n(ALiBi). ALiBi does not add positional embeddings to word embeddings; instead,\nit biases query-key attention scores with a penalty that is proportional to\ntheir distance. We show that this method trains a 1.3 billion parameter model\non input sequences of length 1024 that extrapolates to input sequences of\nlength 2048, achieving the same perplexity as a sinusoidal position embedding\nmodel trained on inputs of length 2048 but training 11% faster and using 11%\nless memory. ALiBi's inductive bias towards recency also leads it to outperform\nmultiple strong position methods on the WikiText-103 benchmark.", "published": "2021-08-27 17:35:06", "link": "http://arxiv.org/abs/2108.12409v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Pivots to Graphs: Augmented CycleDensity as a Generalization to One\n  Time InverseConsultation", "abstract": "This paper describes an approach used to generate new translations using raw\nbilingual dictionaries as part of the 4th Task Inference Across Dictionaries\n(TIAD 2021) shared task. We propose Augmented Cycle Density (ACD) as a\nframework that combines insights from two state of the art methods that require\nno sense information and parallel corpora: Cycle Density (CD) and One Time\nInverse Consultation (OTIC). The task results show that across 3 unseen\nlanguage pairs, ACD's predictions, has more than double (74%) the coverage of\nOTIC at almost the same precision (76%). ACD combines CD's scalability -\nleveraging rich multilingual graphs for better predictions, and OTIC's data\nefficiency - producing good results with the minimum possible resource of one\npivot language.", "published": "2021-08-27 19:00:08", "link": "http://arxiv.org/abs/2108.12459v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Opinions are Made to be Changed: Temporally Adaptive Stance\n  Classification", "abstract": "Given the rapidly evolving nature of social media and people's views, word\nusage changes over time. Consequently, the performance of a classifier trained\non old textual data can drop dramatically when tested on newer data. While\nresearch in stance classification has advanced in recent years, no effort has\nbeen invested in making these classifiers have persistent performance over\ntime. To study this phenomenon we introduce two novel large-scale, longitudinal\nstance datasets. We then evaluate the performance persistence of stance\nclassifiers over time and demonstrate how it decays as the temporal gap between\ntraining and testing data increases. We propose a novel approach to mitigate\nthis performance drop, which is based on temporal adaptation of the word\nembeddings used for training the stance classifier. This enables us to make use\nof readily available unlabelled data from the current time period instead of\nexpensive annotation efforts. We propose and compare several approaches to\nembedding adaptation and find that the Incremental Temporal Alignment (ITA)\nmodel leads to the best results in reducing performance drop over time.", "published": "2021-08-27 19:47:31", "link": "http://arxiv.org/abs/2108.12476v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Few-Shot Table-to-Text Generation with Prototype Memory", "abstract": "Neural table-to-text generation models have achieved remarkable progress on\nan array of tasks. However, due to the data-hungry nature of neural models,\ntheir performances strongly rely on large-scale training examples, limiting\ntheir applicability in real-world applications. To address this, we propose a\nnew framework: Prototype-to-Generate (P2G), for table-to-text generation under\nthe few-shot scenario. The proposed framework utilizes the retrieved\nprototypes, which are jointly selected by an IR system and a novel prototype\nselector to help the model bridging the structural gap between tables and\ntexts. Experimental results on three benchmark datasets with three\nstate-of-the-art models demonstrate that the proposed framework significantly\nimproves the model performance across various evaluation metrics.", "published": "2021-08-27 22:16:30", "link": "http://arxiv.org/abs/2108.12516v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TweetBLM: A Hate Speech Dataset and Analysis of Black Lives\n  Matter-related Microblogs on Twitter", "abstract": "In the past few years, there has been a significant rise in toxic and hateful\ncontent on various social media platforms. Recently Black Lives Matter movement\ncame into the picture, causing an avalanche of user generated responses on the\ninternet. In this paper, we have proposed a Black Lives Matter related tweet\nhate speech dataset TweetBLM. Our dataset comprises 9165 manually annotated\ntweets that target the Black Lives Matter movement. We annotated the tweets\ninto two classes, i.e., HATE and NONHATE based on their content related to\nracism erupted from the movement for the black community. In this work, we also\ngenerated useful statistical insights on our dataset and performed a systematic\nanalysis of various machine learning models such as Random Forest, CNN, LSTM,\nBiLSTM, Fasttext, BERTbase, and BERTlarge for the classification task on our\ndataset. Through our work, we aim at contributing to the substantial efforts of\nthe research community for the identification and mitigation of hate speech on\nthe internet. The dataset is publicly available.", "published": "2021-08-27 22:47:02", "link": "http://arxiv.org/abs/2108.12521v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Query-Focused Extractive Summarisation for Finding Ideal Answers to\n  Biomedical and COVID-19 Questions", "abstract": "This paper presents Macquarie University's participation to the BioASQ\nSynergy Task, and BioASQ9b Phase B. In each of these tasks, our participation\nfocused on the use of query-focused extractive summarisation to obtain the\nideal answers to medical questions. The Synergy Task is an end-to-end question\nanswering task on COVID-19 where systems are required to return relevant\ndocuments, snippets, and answers to a given question. Given the absence of\ntraining data, we used a query-focused summarisation system that was trained\nwith the BioASQ8b training data set and we experimented with methods to\nretrieve the documents and snippets. Considering the poor quality of the\ndocuments and snippets retrieved by our system, we observed reasonably good\nquality in the answers returned. For phase B of the BioASQ9b task, the relevant\ndocuments and snippets were already included in the test data. Our system split\nthe snippets into candidate sentences and used BERT variants under a sentence\nclassification setup. The system used the question and candidate sentence as\ninput and was trained to predict the likelihood of the candidate sentence being\npart of the ideal answer. The runs obtained either the best or second best\nROUGE-F1 results of all participants to all batches of BioASQ9b. This shows\nthat using BERT in a classification setup is a very strong baseline for the\nidentification of ideal answers.", "published": "2021-08-27 09:19:42", "link": "http://arxiv.org/abs/2108.12189v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Evaluating the Robustness of Neural Language Models to Input\n  Perturbations", "abstract": "High-performance neural language models have obtained state-of-the-art\nresults on a wide range of Natural Language Processing (NLP) tasks. However,\nresults for common benchmark datasets often do not reflect model reliability\nand robustness when applied to noisy, real-world data. In this study, we design\nand implement various types of character-level and word-level perturbation\nmethods to simulate realistic scenarios in which input texts may be slightly\nnoisy or different from the data distribution on which NLP systems were\ntrained. Conducting comprehensive experiments on different NLP tasks, we\ninvestigate the ability of high-performance language models such as BERT,\nXLNet, RoBERTa, and ELMo in handling different types of input perturbations.\nThe results suggest that language models are sensitive to input perturbations\nand their performance can decrease even when small changes are introduced. We\nhighlight that models need to be further improved and that current benchmarks\nare not reflecting model robustness well. We argue that evaluations on\nperturbed inputs should routinely complement widely-used benchmarks in order to\nyield a more realistic understanding of NLP systems robustness.", "published": "2021-08-27 12:31:17", "link": "http://arxiv.org/abs/2108.12237v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deep learning models are not robust against noise in clinical text", "abstract": "Artificial Intelligence (AI) systems are attracting increasing interest in\nthe medical domain due to their ability to learn complicated tasks that require\nhuman intelligence and expert knowledge. AI systems that utilize\nhigh-performance Natural Language Processing (NLP) models have achieved\nstate-of-the-art results on a wide variety of clinical text processing\nbenchmarks. They have even outperformed human accuracy on some tasks. However,\nperformance evaluation of such AI systems have been limited to accuracy\nmeasures on curated and clean benchmark datasets that may not properly reflect\nhow robustly these systems can operate in real-world situations. In order to\naddress this challenge, we introduce and implement a wide variety of\nperturbation methods that simulate different types of noise and variability in\nclinical text data. While noisy samples produced by these perturbation methods\ncan often be understood by humans, they may cause AI systems to make erroneous\ndecisions. Conducting extensive experiments on several clinical text processing\ntasks, we evaluated the robustness of high-performance NLP models against\nvarious types of character-level and word-level noise. The results revealed\nthat the NLP models performance degrades when the input contains small amounts\nof noise. This study is a significant step towards exposing vulnerabilities of\nAI models utilized in clinical text processing systems. The proposed\nperturbation methods can be used in performance evaluation tests to assess how\nrobustly clinical NLP models can operate on noisy data, in real-world settings.", "published": "2021-08-27 12:47:19", "link": "http://arxiv.org/abs/2108.12242v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CAPE: Context-Aware Private Embeddings for Private Language Learning", "abstract": "Deep learning-based language models have achieved state-of-the-art results in\na number of applications including sentiment analysis, topic labelling, intent\nclassification and others. Obtaining text representations or embeddings using\nthese models presents the possibility of encoding personally identifiable\ninformation learned from language and context cues that may present a risk to\nreputation or privacy. To ameliorate these issues, we propose Context-Aware\nPrivate Embeddings (CAPE), a novel approach which preserves privacy during\ntraining of embeddings. To maintain the privacy of text representations, CAPE\napplies calibrated noise through differential privacy, preserving the encoded\nsemantic links while obscuring sensitive information. In addition, CAPE employs\nan adversarial training regime that obscures identified private variables.\nExperimental results demonstrate that the proposed approach reduces private\ninformation leakage better than either single intervention.", "published": "2021-08-27 14:50:12", "link": "http://arxiv.org/abs/2108.12318v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automatic Text Evaluation through the Lens of Wasserstein Barycenters", "abstract": "A new metric \\texttt{BaryScore} to evaluate text generation based on deep\ncontextualized embeddings e.g., BERT, Roberta, ELMo) is introduced. This metric\nis motivated by a new framework relying on optimal transport tools, i.e.,\nWasserstein distance and barycenter. By modelling the layer output of deep\ncontextualized embeddings as a probability distribution rather than by a vector\nembedding; this framework provides a natural way to aggregate the different\noutputs through the Wasserstein space topology. In addition, it provides\ntheoretical grounds to our metric and offers an alternative to available\nsolutions e.g., MoverScore and BertScore). Numerical evaluation is performed on\nfour different tasks: machine translation, summarization, data2text generation\nand image captioning. Our results show that \\texttt{BaryScore} outperforms\nother BERT based metrics and exhibits more consistent behaviour in particular\nfor text summarization.", "published": "2021-08-27 19:08:52", "link": "http://arxiv.org/abs/2108.12463v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Code-switched inspired losses for generic spoken dialog representations", "abstract": "Spoken dialog systems need to be able to handle both multiple languages and\nmultilinguality inside a conversation (\\textit{e.g} in case of code-switching).\nIn this work, we introduce new pretraining losses tailored to learn\nmultilingual spoken dialog representations. The goal of these losses is to\nexpose the model to code-switched language. To scale up training, we\nautomatically build a pretraining corpus composed of multilingual conversations\nin five different languages (French, Italian, English, German and Spanish) from\n\\texttt{OpenSubtitles}, a huge multilingual corpus composed of 24.3G tokens. We\ntest the generic representations on \\texttt{MIAM}, a new benchmark composed of\nfive dialog act corpora on the same aforementioned languages as well as on two\nnovel multilingual downstream tasks (\\textit{i.e} multilingual mask utterance\nretrieval and multilingual inconsistency identification). Our experiments show\nthat our new code switched-inspired losses achieve a better performance in both\nmonolingual and multilingual settings.", "published": "2021-08-27 19:15:00", "link": "http://arxiv.org/abs/2108.12465v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ReGen: Reinforcement Learning for Text and Knowledge Base Generation\n  using Pretrained Language Models", "abstract": "Automatic construction of relevant Knowledge Bases (KBs) from text, and\ngeneration of semantically meaningful text from KBs are both long-standing\ngoals in Machine Learning. In this paper, we present ReGen, a bidirectional\ngeneration of text and graph leveraging Reinforcement Learning (RL) to improve\nperformance. Graph linearization enables us to re-frame both tasks as a\nsequence to sequence generation problem regardless of the generative direction,\nwhich in turn allows the use of Reinforcement Learning for sequence training\nwhere the model itself is employed as its own critic leading to Self-Critical\nSequence Training (SCST). We present an extensive investigation demonstrating\nthat the use of RL via SCST benefits graph and text generation on WebNLG+ 2020\nand TekGen datasets. Our system provides state-of-the-art results on WebNLG+\n2020 by significantly improving upon published results from the WebNLG 2020+\nChallenge for both text-to-graph and graph-to-text generation tasks.", "published": "2021-08-27 19:37:12", "link": "http://arxiv.org/abs/2108.12472v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Energy-Based Approximate Inference Networks for Structured\n  Applications in NLP", "abstract": "Structured prediction in natural language processing (NLP) has a long\nhistory. The complex models of structured application come at the difficulty of\nlearning and inference. These difficulties lead researchers to focus more on\nmodels with simple structure components (e.g., local classifier). Deep\nrepresentation learning has become increasingly popular in recent years. The\nstructure components of their method, on the other hand, are usually relatively\nsimple. We concentrate on complex structured models in this dissertation. We\nprovide a learning framework for complicated structured models as well as an\ninference method with a better speed/accuracy/search error trade-off. The\ndissertation begins with a general introduction to energy-based models. In NLP\nand other applications, an energy function is comparable to the concept of a\nscoring function. In this dissertation, we discuss the concept of the energy\nfunction and structured models with different energy functions. Then, we\npropose a method in which we train a neural network to do argmax inference\nunder a structured energy function, referring to the trained networks as\n\"inference networks\" or \"energy-based inference networks\". We then develop ways\nof jointly learning energy functions and inference networks using an\nadversarial learning framework. Despite the inference and learning difficulties\nof energy-based models, we present approaches in this thesis that enable\nenergy-based models more easily to be applied in structured NLP applications.", "published": "2021-08-27 22:48:20", "link": "http://arxiv.org/abs/2108.12522v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Web Scale Entity Extraction System", "abstract": "Understanding the semantic meaning of content on the web through the lens of\nentities and concepts has many practical advantages. However, when building\nlarge-scale entity extraction systems, practitioners are facing unique\nchallenges involving finding the best ways to leverage the scale and variety of\ndata available on internet platforms. We present learnings from our efforts in\nbuilding an entity extraction system for multiple document types at large scale\nusing multi-modal Transformers. We empirically demonstrate the effectiveness of\nmulti-lingual, multi-task and cross-document type learning. We also discuss the\nlabel collection schemes that help to minimize the amount of noise in the\ncollected data.", "published": "2021-08-27 16:37:37", "link": "http://arxiv.org/abs/2110.00423v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "4-bit Quantization of LSTM-based Speech Recognition Models", "abstract": "We investigate the impact of aggressive low-precision representations of\nweights and activations in two families of large LSTM-based architectures for\nAutomatic Speech Recognition (ASR): hybrid Deep Bidirectional LSTM - Hidden\nMarkov Models (DBLSTM-HMMs) and Recurrent Neural Network - Transducers\n(RNN-Ts). Using a 4-bit integer representation, a na\\\"ive quantization approach\napplied to the LSTM portion of these models results in significant Word Error\nRate (WER) degradation. On the other hand, we show that minimal accuracy loss\nis achievable with an appropriate choice of quantizers and initializations. In\nparticular, we customize quantization schemes depending on the local properties\nof the network, improving recognition performance while limiting computational\ntime. We demonstrate our solution on the Switchboard (SWB) and CallHome (CH)\ntest sets of the NIST Hub5-2000 evaluation. DBLSTM-HMMs trained with 300 or\n2000 hours of SWB data achieves $<$0.5% and $<$1% average WER degradation,\nrespectively. On the more challenging RNN-T models, our quantization strategy\nlimits degradation in 4-bit inference to 1.3%.", "published": "2021-08-27 00:59:52", "link": "http://arxiv.org/abs/2108.12074v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS", "I.2.6"], "primary_category": "cs.CL"}
{"title": "Harms of Gender Exclusivity and Challenges in Non-Binary Representation\n  in Language Technologies", "abstract": "Gender is widely discussed in the context of language tasks and when\nexamining the stereotypes propagated by language models. However, current\ndiscussions primarily treat gender as binary, which can perpetuate harms such\nas the cyclical erasure of non-binary gender identities. These harms are driven\nby model and dataset biases, which are consequences of the non-recognition and\nlack of understanding of non-binary genders in society. In this paper, we\nexplain the complexity of gender and language around it, and survey non-binary\npersons to understand harms associated with the treatment of gender as binary\nin English language technologies. We also detail how current language\nrepresentations (e.g., GloVe, BERT) capture and perpetuate these harms and\nrelated challenges that need to be acknowledged and addressed for\nrepresentations to equitably encode gender information.", "published": "2021-08-27 01:58:58", "link": "http://arxiv.org/abs/2108.12084v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving callsign recognition with air-surveillance data in air-traffic\n  communication", "abstract": "Automatic Speech Recognition (ASR) can be used as the assistance of speech\ncommunication between pilots and air-traffic controllers. Its application can\nsignificantly reduce the complexity of the task and increase the reliability of\ntransmitted information. Evidently, high accuracy predictions are needed to\nminimize the risk of errors. Especially, high accuracy is required in\nrecognition of key information, such as commands and callsigns, used to\nnavigate pilots. Our results prove that the surveillance data containing\ncallsigns can help to considerably improve the recognition of a callsign in an\nutterance when the weights of probable callsign n-grams are reduced per\nutterance. In this paper, we investigate two approaches: (1) G-boosting, when\ncallsigns weights are adjusted at language model level (G) and followed by the\ndynamic decoder with an on-the-fly composition, and (2) lattice rescoring when\ncallsign information is introduced on top of lattices generated using a\nconventional decoder. Boosting callsign n-grams with the combination of two\nmethods allowed us to gain 28.4% of absolute improvement in callsign\nrecognition accuracy and up to 74.2% of relative improvement in WER of callsign\nrecognition.", "published": "2021-08-27 07:56:47", "link": "http://arxiv.org/abs/2108.12156v1", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Grammar Based Speaker Role Identification for Air Traffic Control Speech\n  Recognition", "abstract": "Automatic Speech Recognition (ASR) for air traffic control is generally\ntrained by pooling Air Traffic Controller (ATCO) and pilot data into one set.\nThis is motivated by the fact that pilot's voice communications are more scarce\nthan ATCOs. Due to this data imbalance and other reasons (e.g., varying\nacoustic conditions), the speech from ATCOs is usually recognized more\naccurately than from pilots. Automatically identifying the speaker roles is a\nchallenging task, especially in the case of the noisy voice recordings\ncollected using Very High Frequency (VHF) receivers or due to the\nunavailability of the push-to-talk (PTT) signal, i.e., both audio channels are\nmixed. In this work, we propose to (1) automatically segment the ATCO and pilot\ndata based on an intuitive approach exploiting ASR transcripts and (2)\nsubsequently consider an automatic recognition of ATCOs' and pilots' voice as\ntwo separate tasks. Our work is performed on VHF audio data with high noise\nlevels, i.e., signal-to-noise (SNR) ratios below 15 dB, as this data is\nrecognized to be helpful for various speech-based machine-learning tasks.\nSpecifically, for the speaker role identification task, the module is\nrepresented by a simple yet efficient knowledge-based system exploiting a\ngrammar defined by the International Civil Aviation Organization (ICAO). The\nsystem accepts text as the input, either manually verified annotations or\nautomatically generated transcripts. The developed approach provides an average\naccuracy in speaker role identification of about 83%. Finally, we show that\ntraining an acoustic model for ASR tasks separately (i.e., separate models for\nATCOs and pilots) or using a multitask approach is well suited for the noisy\ndata and outperforms the traditional ASR system where all data is pooled\ntogether.", "published": "2021-08-27 08:40:08", "link": "http://arxiv.org/abs/2108.12175v2", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Injecting Text in Self-Supervised Speech Pretraining", "abstract": "Self-supervised pretraining for Automated Speech Recognition (ASR) has shown\nvaried degrees of success. In this paper, we propose to jointly learn\nrepresentations during pretraining from two different modalities: speech and\ntext. The proposed method, tts4pretrain complements the power of contrastive\nlearning in self-supervision with linguistic/lexical representations derived\nfrom synthesized speech, effectively learning from untranscribed speech and\nunspoken text. Lexical learning in the speech encoder is enforced through an\nadditional sequence loss term that is coupled with contrastive loss during\npretraining. We demonstrate that this novel pretraining method yields Word\nError Rate (WER) reductions of 10% relative on the well-benchmarked,\nLibrispeech task over a state-of-the-art baseline pretrained with wav2vec2.0\nonly. The proposed method also serves as an effective strategy to compensate\nfor the lack of transcribed speech, effectively matching the performance of\n5000 hours of transcribed speech with just 100 hours of transcribed speech on\nthe AMI meeting transcription task. Finally, we demonstrate WER reductions of\nup to 15% on an in-house Voice Search task over traditional pretraining.\nIncorporating text into encoder pretraining is complimentary to rescoring with\na larger or in-domain language model, resulting in additional 6% relative\nreduction in WER.", "published": "2021-08-27 11:36:40", "link": "http://arxiv.org/abs/2108.12226v1", "categories": ["cs.CL", "cs.SD", "eess.AS", "68T10", "I.2.7"], "primary_category": "cs.CL"}
{"title": "DomiKnowS: A Library for Integration of Symbolic Domain Knowledge in\n  Deep Learning", "abstract": "We demonstrate a library for the integration of domain knowledge in deep\nlearning architectures. Using this library, the structure of the data is\nexpressed symbolically via graph declarations and the logical constraints over\noutputs or latent variables can be seamlessly added to the deep models. The\ndomain knowledge can be defined explicitly, which improves the models'\nexplainability in addition to the performance and generalizability in the\nlow-data regime. Several approaches for such an integration of symbolic and\nsub-symbolic models have been introduced; however, there is no library to\nfacilitate the programming for such an integration in a generic way while\nvarious underlying algorithms can be used. Our library aims to simplify\nprogramming for such an integration in both training and inference phases while\nseparating the knowledge representation from learning algorithms. We showcase\nvarious NLP benchmark tasks and beyond. The framework is publicly available at\nGithub(https://github.com/HLR/DomiKnowS).", "published": "2021-08-27 16:06:42", "link": "http://arxiv.org/abs/2108.12370v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2"], "primary_category": "cs.LG"}
{"title": "Predicting the Factuality of Reporting of News Media Using Observations\n  About User Attention in Their YouTube Channels", "abstract": "We propose a novel framework for predicting the factuality of reporting of\nnews media outlets by studying the user attention cycles in their YouTube\nchannels. In particular, we design a rich set of features derived from the\ntemporal evolution of the number of views, likes, dislikes, and comments for a\nvideo, which we then aggregate to the channel level. We develop and release a\ndataset for the task, containing observations of user attention on YouTube\nchannels for 489 news media. Our experiments demonstrate both complementarity\nand sizable improvements over state-of-the-art textual representations.", "published": "2021-08-27 22:43:00", "link": "http://arxiv.org/abs/2108.12519v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.SI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Speech Representations and Phoneme Classification for Preserving the\n  Endangered Language of Ladin", "abstract": "A vast majority of the world's 7,000 spoken languages are predicted to become\nextinct within this century, including the endangered language of Ladin from\nthe Italian Alps. Linguists who work to preserve a language's phonetic and\nphonological structure can spend hours transcribing each minute of speech from\nnative speakers. To address this problem in the context of Ladin, our paper\npresents the first analysis of speech representations and machine learning\nmodels for classifying 32 phonemes of Ladin. We experimented with a novel\ndataset of the Fascian dialect of Ladin, collected from native speakers in\nItaly. We created frame-level and segment-level speech feature extraction\napproaches and conducted extensive experiments with 8 different classifiers\ntrained on 9 different speech representations. Our speech representations\nranged from traditional features (MFCC, LPC) to features learned with deep\nneural network models (autoencoders, LSTM autoencoders, and WaveNet). Our\nhighest-performing classifier, trained on MFCC representations of speech\nsignals, achieved an 86% average accuracy across all Ladin phonemes. We also\nobtained average accuracies above 77% for all Ladin phoneme subgroups examined.\nOur findings contribute insights for learning discriminative Ladin phoneme\nrepresentations and demonstrate the potential for leveraging machine learning\nand speech signal processing to preserve Ladin and other endangered languages.", "published": "2021-08-27 23:51:59", "link": "http://arxiv.org/abs/2108.12531v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Separable Temporal Convolution plus Temporally Pooled Attention for\n  Lightweight High-performance Keyword Spotting", "abstract": "Keyword spotting (KWS) on mobile devices generally requires a small memory\nfootprint. However, most current models still maintain a large number of\nparameters in order to ensure good performance. In this paper, we propose a\ntemporally pooled attention module which can capture global features better\nthan the AveragePool. Besides, we design a separable temporal convolution\nnetwork which leverages depthwise separable and temporal convolution to reduce\nthe number of parameter and calculations. Finally, taking advantage of\nseparable temporal convolution and temporally pooled attention, a efficient\nneural network (ST-AttNet) is designed for KWS system. We evaluate the models\non the publicly available Google speech commands data sets V1. The number of\nparameters of proposed model (48K) is 1/6 of state-of-the-art TC-ResNet14-1.5\nmodel (305K). The proposed model achieves a 96.6% accuracy, which is comparable\nto the TC-ResNet14-1.5 model (96.6%).", "published": "2021-08-27 07:27:33", "link": "http://arxiv.org/abs/2108.12146v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploring Retraining-Free Speech Recognition for Intra-sentential\n  Code-Switching", "abstract": "In this paper, we present our initial efforts for building a code-switching\n(CS) speech recognition system leveraging existing acoustic models (AMs) and\nlanguage models (LMs), i.e., no training required, and specifically targeting\nintra-sentential switching. To achieve such an ambitious goal, new mechanisms\nfor foreign pronunciation generation and language model (LM) enrichment have\nbeen devised. Specifically, we have designed an automatic approach to obtain\nhigh quality pronunciation of foreign language (FL) words in the native\nlanguage (NL) phoneme set using existing acoustic phone decoders and an\nLSTM-based grapheme-to-phoneme (G2P) model. Improved accented pronunciations\nhave thus been obtained by learning foreign pronunciations directly from data.\nFurthermore, a code-switching LM was deployed by converting the original NL LM\ninto a CS LM using translated word pairs and borrowing statistics for the NL\nLM. Experimental evidence clearly demonstrates that our approach better deals\nwith accented foreign pronunciations than techniques based on human labeling.\nMoreover, our best system achieves a 55.5% relative word error rate reduction\nfrom 34.4%, obtained with a conventional monolingual ASR system, to 15.3% on an\nintra-sentential CS task without harming the monolingual recognition accuracy.", "published": "2021-08-27 19:15:16", "link": "http://arxiv.org/abs/2109.00921v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Full Attention Bidirectional Deep Learning Structure for Single Channel\n  Speech Enhancement", "abstract": "As the cornerstone of other important technologies, such as speech\nrecognition and speech synthesis, speech enhancement is a critical area in\naudio signal processing. In this paper, a new deep learning structure for\nspeech enhancement is demonstrated. The model introduces a \"full\" attention\nmechanism to a bidirectional sequence-to-sequence method to make use of latent\ninformation after each focal frame. This is an extension of the previous\nattention-based RNN method. The proposed bidirectional attention-based\narchitecture achieves better performance in terms of speech quality (PESQ),\ncompared with OM-LSA, CNN-LSTM, T-GSA and the unidirectional attention-based\nLSTM baseline.", "published": "2021-08-27 03:19:07", "link": "http://arxiv.org/abs/2108.12105v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Task-aware Warping Factors in Mask-based Speech Enhancement", "abstract": "This paper proposes the use of two task-aware warping factors in mask-based\nspeech enhancement (SE). One controls the balance between speech-maintenance\nand noise-removal in training phases, while the other controls SE power applied\nto specific downstream tasks in testing phases. Our intention is to alleviate\nthe problem that SE systems trained to improve speech quality often fail to\nimprove other downstream tasks, such as automatic speaker verification (ASV)\nand automatic speech recognition (ASR), because they do not share the same\nobjects. It is easy to apply the proposed dual-warping factors approach to any\nmask-based SE method, and it allows a single SE system to handle multiple tasks\nwithout task-dependent training. The effectiveness of our proposed approach has\nbeen confirmed on the SITW dataset for ASV evaluation and the LibriSpeech\ndataset for ASR and speech quality evaluations of 0-20dB. We show that\ndifferent warping values are necessary for a single SE to achieve optimal\nperformance w.r.t. the three tasks. With the use of task-dependent warping\nfactors, speech quality was improved by an 84.7% PESQ increase, ASV had a 22.4%\nEER reduction, and ASR had a 52.2% WER reduction, on 0dB speech. The\neffectiveness of the task-dependent warping factors were also cross-validated\non VoxCeleb-1 test set for ASV and LibriSpeech dev-clean set for ASV and\nquality evaluations. The proposed method is highly effective and easy to apply\nin practice.", "published": "2021-08-27 05:57:37", "link": "http://arxiv.org/abs/2108.12128v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Music Composition with Deep Learning: A Review", "abstract": "Generating a complex work of art such as a musical composition requires\nexhibiting true creativity that depends on a variety of factors that are\nrelated to the hierarchy of musical language. Music generation have been faced\nwith Algorithmic methods and recently, with Deep Learning models that are being\nused in other fields such as Computer Vision. In this paper we want to put into\ncontext the existing relationships between AI-based music composition models\nand human musical composition and creativity processes. We give an overview of\nthe recent Deep Learning models for music composition and we compare these\nmodels to the music composition process from a theoretical point of view. We\nhave tried to answer some of the most relevant open questions for this task by\nanalyzing the ability of current Deep Learning models to generate music with\ncreativity or the similarity between AI and human composition processes, among\nothers.", "published": "2021-08-27 13:53:53", "link": "http://arxiv.org/abs/2108.12290v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
