{"title": "Variational Transformers for Diverse Response Generation", "abstract": "Despite the great promise of Transformers in many sequence modeling tasks\n(e.g., machine translation), their deterministic nature hinders them from\ngeneralizing to high entropy tasks such as dialogue response generation.\nPrevious work proposes to capture the variability of dialogue responses with a\nrecurrent neural network (RNN)-based conditional variational autoencoder\n(CVAE). However, the autoregressive computation of the RNN limits the training\nefficiency. Therefore, we propose the Variational Transformer (VT), a\nvariational self-attentive feed-forward sequence model. The VT combines the\nparallelizability and global receptive field of the Transformer with the\nvariational nature of the CVAE by incorporating stochastic latent variables\ninto Transformers. We explore two types of the VT: 1) modeling the\ndiscourse-level diversity with a global latent variable; and 2) augmenting the\nTransformer decoder with a sequence of fine-grained latent variables. Then, the\nproposed models are evaluated on three conversational datasets with both\nautomatic metric and human evaluation. The experimental results show that our\nmodels improve standard Transformers and other baselines in terms of diversity,\nsemantic relevance, and human judgment.", "published": "2020-03-28 07:48:02", "link": "http://arxiv.org/abs/2003.12738v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HIN: Hierarchical Inference Network for Document-Level Relation\n  Extraction", "abstract": "Document-level RE requires reading, inferring and aggregating over multiple\nsentences. From our point of view, it is necessary for document-level RE to\ntake advantage of multi-granularity inference information: entity level,\nsentence level and document level. Thus, how to obtain and aggregate the\ninference information with different granularity is challenging for\ndocument-level RE, which has not been considered by previous work. In this\npaper, we propose a Hierarchical Inference Network (HIN) to make full use of\nthe abundant information from entity level, sentence level and document level.\nTranslation constraint and bilinear transformation are applied to target entity\npair in multiple subspaces to get entity-level inference information. Next, we\nmodel the inference between entity-level information and sentence\nrepresentation to achieve sentence-level inference information. Finally, a\nhierarchical aggregation approach is adopted to obtain the document-level\ninference information. In this way, our model can effectively aggregate\ninference information from these three different granularities. Experimental\nresults show that our method achieves state-of-the-art performance on the\nlarge-scale DocRED dataset. We also demonstrate that using BERT representations\ncan further substantially boost the performance.", "published": "2020-03-28 09:32:31", "link": "http://arxiv.org/abs/2003.12754v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Countering Language Drift with Seeded Iterated Learning", "abstract": "Pretraining on human corpus and then finetuning in a simulator has become a\nstandard pipeline for training a goal-oriented dialogue agent. Nevertheless, as\nsoon as the agents are finetuned to maximize task completion, they suffer from\nthe so-called language drift phenomenon: they slowly lose syntactic and\nsemantic properties of language as they only focus on solving the task. In this\npaper, we propose a generic approach to counter language drift called Seeded\niterated learning (SIL). We periodically refine a pretrained student agent by\nimitating data sampled from a newly generated teacher agent. At each time step,\nthe teacher is created by copying the student agent, before being finetuned to\nmaximize task completion. SIL does not require external syntactic constraint\nnor semantic knowledge, making it a valuable task-agnostic finetuning protocol.\nWe evaluate SIL in a toy-setting Lewis Game, and then scale it up to the\ntranslation game with natural language. In both settings, SIL helps counter\nlanguage drift as well as it improves the task completion compared to\nbaselines.", "published": "2020-03-28 03:45:31", "link": "http://arxiv.org/abs/2003.12694v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Mining Implicit Entity Preference from User-Item Interaction Data for\n  Knowledge Graph Completion via Adversarial Learning", "abstract": "The task of Knowledge Graph Completion (KGC) aims to automatically infer the\nmissing fact information in Knowledge Graph (KG). In this paper, we take a new\nperspective that aims to leverage rich user-item interaction data (user\ninteraction data for short) for improving the KGC task. Our work is inspired by\nthe observation that many KG entities correspond to online items in application\nsystems. However, the two kinds of data sources have very different intrinsic\ncharacteristics, and it is likely to hurt the original performance using simple\nfusion strategy. To address this challenge, we propose a novel adversarial\nlearning approach by leveraging user interaction data for the KGC task. Our\ngenerator is isolated from user interaction data, and serves to improve the\nperformance of the discriminator. The discriminator takes the learned useful\ninformation from user interaction data as input, and gradually enhances the\nevaluation capacity in order to identify the fake samples generated by the\ngenerator. To discover implicit entity preference of users, we design an\nelaborate collaborative learning algorithms based on graph neural networks,\nwhich will be jointly optimized with the discriminator. Such an approach is\neffective to alleviate the issues about data heterogeneity and semantic\ncomplexity for the KGC task. Extensive experiments on three real-world datasets\nhave demonstrated the effectiveness of our approach on the KGC task.", "published": "2020-03-28 05:47:33", "link": "http://arxiv.org/abs/2003.12718v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Unsupervised feature learning for speech using correspondence and\n  Siamese networks", "abstract": "In zero-resource settings where transcribed speech audio is unavailable,\nunsupervised feature learning is essential for downstream speech processing\ntasks. Here we compare two recent methods for frame-level acoustic feature\nlearning. For both methods, unsupervised term discovery is used to find pairs\nof word examples of the same unknown type. Dynamic programming is then used to\nalign the feature frames between each word pair, serving as weak top-down\nsupervision for the two models. For the correspondence autoencoder (CAE),\nmatching frames are presented as input-output pairs. The Triamese network uses\na contrastive loss to reduce the distance between frames of the same predicted\nword type while increasing the distance between negative examples. For the\nfirst time, these feature extractors are compared on the same discrimination\ntasks using the same weak supervision pairs. We find that, on the two datasets\nconsidered here, the CAE outperforms the Triamese network. However, we show\nthat a new hybrid correspondence-Triamese approach (CTriamese), consistently\noutperforms both the CAE and Triamese models in terms of average precision and\nABX error rates on both English and Xitsonga evaluation data.", "published": "2020-03-28 14:31:01", "link": "http://arxiv.org/abs/2003.12799v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Orchestrating NLP Services for the Legal Domain", "abstract": "Legal technology is currently receiving a lot of attention from various\nangles. In this contribution we describe the main technical components of a\nsystem that is currently under development in the European innovation project\nLynx, which includes partners from industry and research. The key contribution\nof this paper is a workflow manager that enables the flexible orchestration of\nworkflows based on a portfolio of Natural Language Processing and Content\nCuration services as well as a Multilingual Legal Knowledge Graph that contains\nsemantic information and meaningful references to legal documents. We also\ndescribe different use cases with which we experiment and develop prototypical\nsolutions.", "published": "2020-03-28 22:10:48", "link": "http://arxiv.org/abs/2003.12900v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Serialized Output Training for End-to-End Overlapped Speech Recognition", "abstract": "This paper proposes serialized output training (SOT), a novel framework for\nmulti-speaker overlapped speech recognition based on an attention-based\nencoder-decoder approach. Instead of having multiple output layers as with the\npermutation invariant training (PIT), SOT uses a model with only one output\nlayer that generates the transcriptions of multiple speakers one after another.\nThe attention and decoder modules take care of producing multiple\ntranscriptions from overlapped speech. SOT has two advantages over PIT: (1) no\nlimitation in the maximum number of speakers, and (2) an ability to model the\ndependencies among outputs for different speakers. We also propose a simple\ntrick that allows SOT to be executed in $O(S)$, where $S$ is the number of the\nspeakers in the training sample, by using the start times of the constituent\nsource utterances. Experimental results on LibriSpeech corpus show that the SOT\nmodels can transcribe overlapped speech with variable numbers of speakers\nsignificantly better than PIT-based models. We also show that the SOT models\ncan accurately count the number of speakers in the input audio.", "published": "2020-03-28 02:37:09", "link": "http://arxiv.org/abs/2003.12687v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Streaming On-Device End-to-End Model Surpassing Server-Side\n  Conventional Model Quality and Latency", "abstract": "Thus far, end-to-end (E2E) models have not been shown to outperform\nstate-of-the-art conventional models with respect to both quality, i.e., word\nerror rate (WER), and latency, i.e., the time the hypothesis is finalized after\nthe user stops speaking. In this paper, we develop a first-pass Recurrent\nNeural Network Transducer (RNN-T) model and a second-pass Listen, Attend, Spell\n(LAS) rescorer that surpasses a conventional model in both quality and latency.\nOn the quality side, we incorporate a large number of utterances across varied\ndomains to increase acoustic diversity and the vocabulary seen by the model. We\nalso train with accented English speech to make the model more robust to\ndifferent pronunciations. In addition, given the increased amount of training\ndata, we explore a varied learning rate schedule. On the latency front, we\nexplore using the end-of-sentence decision emitted by the RNN-T model to close\nthe microphone, and also introduce various optimizations to improve the speed\nof LAS rescoring. Overall, we find that RNN-T+LAS offers a better WER and\nlatency tradeoff compared to a conventional model. For example, for the same\nlatency, RNN-T+LAS obtains a 8% relative improvement in WER, while being more\nthan 400-times smaller in model size.", "published": "2020-03-28 05:00:33", "link": "http://arxiv.org/abs/2003.12710v2", "categories": ["cs.CL", "cs.LG", "cs.SD"], "primary_category": "cs.CL"}
{"title": "Predicting the Popularity of Micro-videos with Multimodal Variational\n  Encoder-Decoder Framework", "abstract": "As an emerging type of user-generated content, micro-video drastically\nenriches people's entertainment experiences and social interactions. However,\nthe popularity pattern of an individual micro-video still remains elusive among\nthe researchers. One of the major challenges is that the potential popularity\nof a micro-video tends to fluctuate under the impact of various external\nfactors, which makes it full of uncertainties. In addition, since micro-videos\nare mainly uploaded by individuals that lack professional techniques, multiple\ntypes of noise could exist that obscure useful information. In this paper, we\npropose a multimodal variational encoder-decoder (MMVED) framework for\nmicro-video popularity prediction tasks. MMVED learns a stochastic Gaussian\nembedding of a micro-video that is informative to its popularity level while\npreserves the inherent uncertainties simultaneously. Moreover, through the\noptimization of a deep variational information bottleneck lower-bound (IBLBO),\nthe learned hidden representation is shown to be maximally expressive about the\npopularity target while maximally compressive to the noise in micro-video\nfeatures. Furthermore, the Bayesian product-of-experts principle is applied to\nthe multimodal encoder, where the decision for information keeping or\ndiscarding is made comprehensively with all available modalities. Extensive\nexperiments conducted on a public dataset and a dataset we collect from Xigua\ndemonstrate the effectiveness of the proposed MMVED framework.", "published": "2020-03-28 06:08:16", "link": "http://arxiv.org/abs/2003.12724v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Modulating Bottom-Up and Top-Down Visual Processing via\n  Language-Conditional Filters", "abstract": "How to best integrate linguistic and perceptual processing in multi-modal\ntasks that involve language and vision is an important open problem. In this\nwork, we argue that the common practice of using language in a top-down manner,\nto direct visual attention over high-level visual features, may not be optimal.\nWe hypothesize that the use of language to also condition the bottom-up\nprocessing from pixels to high-level features can provide benefits to the\noverall performance. To support our claim, we propose a U-Net-based model and\nperform experiments on two language-vision dense-prediction tasks: referring\nexpression segmentation and language-guided image colorization. We compare\nresults where either one or both of the top-down and bottom-up visual branches\nare conditioned on language. Our experiments reveal that using language to\ncontrol the filters for bottom-up visual processing in addition to top-down\nattention leads to better results on both tasks and achieves competitive\nperformance. Our linguistic analysis suggests that bottom-up conditioning\nimproves segmentation of objects especially when input text refers to low-level\nvisual concepts. Code is available at https://github.com/ilkerkesen/bvpr.", "published": "2020-03-28 07:54:03", "link": "http://arxiv.org/abs/2003.12739v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Mining Coronavirus (COVID-19) Posts in Social Media", "abstract": "World Health Organization (WHO) characterized the novel coronavirus\n(COVID-19) as a global pandemic on March 11th, 2020. Before this and in late\nJanuary, more specifically on January 27th, while the majority of the infection\ncases were still reported in China and a few cruise ships, we began crawling\nsocial media user postings using the Twitter search API. Our goal was to\nleverage machine learning and linguistic tools to better understand the impact\nof the outbreak in China. Unlike our initial expectation to monitor a local\noutbreak, COVID-19 rapidly spread across the globe. In this short article we\nreport the preliminary results of our study on automatically detecting the\npositive reports of COVID-19 from social media user postings using\nstate-of-the-art machine learning models.", "published": "2020-03-28 23:38:50", "link": "http://arxiv.org/abs/2004.06778v1", "categories": ["cs.CL", "cs.LG", "stat.ML", "I.2.7"], "primary_category": "cs.CL"}
