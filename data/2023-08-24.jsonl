{"title": "Rational Decision-Making Agent with Internalized Utility Judgment", "abstract": "Large language models (LLMs) have demonstrated remarkable advancements and\nhave attracted significant efforts to develop LLMs into agents capable of\nexecuting intricate multi-step decision-making tasks beyond traditional NLP\napplications. Existing approaches to LLM-based decision-making predominantly\nbuild upon the manually-designed external performance metrics to guide the\ndecision-making process. However, reliance on the external performance metrics\nas prior is problematic in real-world scenarios, where such prior may be\nunavailable, flawed, or even erroneous. For genuine autonomous decision making,\nit is imperative for the agent to develop its rationality from its posterior\nexperiences to judge decisions independently. Central to the development of\nrationality is the construction of an internalized utility judgment, capable of\nassigning numerical utilities to each decision. This paper proposes RadAgent\n(Rational Decision-Making Agent), which fosters the development of its\nrationality through an iterative framework involving Experience Exploration and\nUtility Learning. Within this framework, Elo-based Utility Construction is\ndevised to assign Elo scores to individual decision steps to judge their\nutilities via pairwise comparisons. Consequently, these Elo scores guide the\ndecision-making process to derive optimal outcomes. Experimental results on the\nToolBench dataset demonstrate RadAgent's superiority over baselines, achieving\nover 10% improvement in Pass Rate on diverse tasks. It offers higher-quality\nsolutions and reduces costs (ChatGPT API calls), highlighting its effectiveness\nand efficiency.", "published": "2023-08-24 03:11:45", "link": "http://arxiv.org/abs/2308.12519v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CARE: Co-Attention Network for Joint Entity and Relation Extraction", "abstract": "Joint entity and relation extraction is the fundamental task of information\nextraction, consisting of two subtasks: named entity recognition and relation\nextraction. However, most existing joint extraction methods suffer from issues\nof feature confusion or inadequate interaction between the two subtasks.\nAddressing these challenges, in this work, we propose a Co-Attention network\nfor joint entity and Relation Extraction (CARE). Our approach includes adopting\na parallel encoding strategy to learn separate representations for each\nsubtask, aiming to avoid feature overlap or confusion. At the core of our\napproach is the co-attention module that captures two-way interaction between\nthe two subtasks, allowing the model to leverage entity information for\nrelation prediction and vice versa, thus promoting mutual enhancement. Through\nextensive experiments on three benchmark datasets for joint entity and relation\nextraction (NYT, WebNLG, and SciERC), we demonstrate that our proposed model\noutperforms existing baseline models. Our code will be available at\nhttps://github.com/kwj0x7f/CARE.", "published": "2023-08-24 03:40:54", "link": "http://arxiv.org/abs/2308.12531v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Small and Fast BERT for Chinese Medical Punctuation Restoration", "abstract": "In clinical dictation, utterances after automatic speech recognition (ASR)\nwithout explicit punctuation marks may lead to the misunderstanding of dictated\nreports. To give a precise and understandable clinical report with ASR,\nautomatic punctuation restoration is required. Considering a practical\nscenario, we propose a fast and light pre-trained model for Chinese medical\npunctuation restoration based on 'pretraining and fine-tuning' paradigm. In\nthis work, we distill pre-trained models by incorporating supervised\ncontrastive learning and a novel auxiliary pre-training task (Punctuation Mark\nPrediction) to make it well-suited for punctuation restoration. Our experiments\non various distilled models reveal that our model can achieve 95% performance\nwhile 10% model size relative to state-of-the-art Chinese RoBERTa.", "published": "2023-08-24 05:15:43", "link": "http://arxiv.org/abs/2308.12568v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probabilistic Method of Measuring Linguistic Productivity", "abstract": "In this paper I propose a new way of measuring linguistic productivity that\nobjectively assesses the ability of an affix to be used to coin new complex\nwords and, unlike other popular measures, is not directly dependent upon token\nfrequency. Specifically, I suggest that linguistic productivity may be viewed\nas the probability of an affix to combine with a random base. The advantages of\nthis approach include the following. First, token frequency does not dominate\nthe productivity measure but naturally influences the sampling of bases.\nSecond, we are not just counting attested word types with an affix but rather\nsimulating the construction of these types and then checking whether they are\nattested in the corpus. Third, a corpus-based approach and randomised design\nassure that true neologisms and words coined long ago have equal chances to be\nselected. The proposed algorithm is evaluated both on English and Russian data.\nThe obtained results provide some valuable insights into the relation of\nlinguistic productivity to the number of types and tokens. It looks like\nburgeoning linguistic productivity manifests itself in an increasing number of\ntypes. However, this process unfolds in two stages: first comes the increase in\nhigh-frequency items, and only then follows the increase in low-frequency\nitems.", "published": "2023-08-24 08:36:28", "link": "http://arxiv.org/abs/2308.12643v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Chatter to Matter: Addressing Critical Steps of Emotion Recognition\n  Learning in Task-oriented Dialogue", "abstract": "Emotion recognition in conversations (ERC) is a crucial task for building\nhuman-like conversational agents. While substantial efforts have been devoted\nto ERC for chit-chat dialogues, the task-oriented counterpart is largely left\nunattended. Directly applying chit-chat ERC models to task-oriented dialogues\n(ToDs) results in suboptimal performance as these models overlook key features\nsuch as the correlation between emotions and task completion in ToDs. In this\npaper, we propose a framework that turns a chit-chat ERC model into a\ntask-oriented one, addressing three critical aspects: data, features and\nobjective. First, we devise two ways of augmenting rare emotions to improve ERC\nperformance. Second, we use dialogue states as auxiliary features to\nincorporate key information from the goal of the user. Lastly, we leverage a\nmulti-aspect emotion definition in ToDs to devise a multi-task learning\nobjective and a novel emotion-distance weighted loss function. Our framework\nyields significant improvements for a range of chit-chat ERC models on EmoWOZ,\na large-scale dataset for user emotion in ToDs. We further investigate the\ngeneralisability of the best resulting model to predict user satisfaction in\ndifferent ToD datasets. A comparison with supervised baselines shows a strong\nzero-shot capability, highlighting the potential usage of our framework in\nwider scenarios.", "published": "2023-08-24 08:46:30", "link": "http://arxiv.org/abs/2308.12648v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Harnessing the Power of David against Goliath: Exploring Instruction\n  Data Generation without Using Closed-Source Models", "abstract": "Instruction tuning is instrumental in enabling Large Language Models~(LLMs)\nto follow user instructions to complete various open-domain tasks. The success\nof instruction tuning depends on the availability of high-quality instruction\ndata. Owing to the exorbitant cost and substandard quality of human annotation,\nrecent works have been deeply engaged in the exploration of the utilization of\npowerful closed-source models to generate instruction data automatically.\nHowever, these methods carry potential risks arising from the usage\nrequirements of powerful closed-source models, which strictly forbid the\nutilization of their outputs to develop machine learning models. To deal with\nthis problem, in this work, we explore alternative approaches to generate\nhigh-quality instruction data that do not rely on closed-source models. Our\nexploration includes an investigation of various existing instruction\ngeneration methods, culminating in the integration of the most efficient\nvariant with two novel strategies to enhance the quality further. Evaluation\nresults from two benchmarks and the GPT-4 model demonstrate the effectiveness\nof our generated instruction data, which can outperform Alpaca, a method\nreliant on closed-source models. We hope that more progress can be achieved in\ngenerating high-quality instruction data without using closed-source models.", "published": "2023-08-24 11:07:47", "link": "http://arxiv.org/abs/2308.12711v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Similarity from Image Contents using Statistical and Semantic\n  Analysis Techniques", "abstract": "Plagiarism detection is one of the most researched areas among the Natural\nLanguage Processing(NLP) community. A good plagiarism detection covers all the\nNLP methods including semantics, named entities, paraphrases etc. and produces\ndetailed plagiarism reports. Detection of Cross Lingual Plagiarism requires\ndeep knowledge of various advanced methods and algorithms to perform effective\ntext similarity checking. Nowadays the plagiarists are also advancing\nthemselves from hiding the identity from being catch in such offense. The\nplagiarists are bypassed from being detected with techniques like paraphrasing,\nsynonym replacement, mismatching citations, translating one language to\nanother. Image Content Plagiarism Detection (ICPD) has gained importance,\nutilizing advanced image content processing to identify instances of plagiarism\nto ensure the integrity of image content. The issue of plagiarism extends\nbeyond textual content, as images such as figures, graphs, and tables also have\nthe potential to be plagiarized. However, image content plagiarism detection\nremains an unaddressed challenge. Therefore, there is a critical need to\ndevelop methods and systems for detecting plagiarism in image content. In this\npaper, the system has been implemented to detect plagiarism form contents of\nImages such as Figures, Graphs, Tables etc. Along with statistical algorithms\nsuch as Jaccard and Cosine, introducing semantic algorithms such as LSA, BERT,\nWordNet outperformed in detecting efficient and accurate plagiarism.", "published": "2023-08-24 15:06:04", "link": "http://arxiv.org/abs/2308.12842v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Code Llama: Open Foundation Models for Code", "abstract": "We release Code Llama, a family of large language models for code based on\nLlama 2 providing state-of-the-art performance among open models, infilling\ncapabilities, support for large input contexts, and zero-shot instruction\nfollowing ability for programming tasks. We provide multiple flavors to cover a\nwide range of applications: foundation models (Code Llama), Python\nspecializations (Code Llama - Python), and instruction-following models (Code\nLlama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are\ntrained on sequences of 16k tokens and show improvements on inputs with up to\n100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants\nsupport infilling based on surrounding content. Code Llama reaches\nstate-of-the-art performance among open models on several code benchmarks, with\nscores of up to 67% and 65% on HumanEval and MBPP, respectively. Notably, Code\nLlama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our\nmodels outperform every other publicly available model on MultiPL-E. We release\nCode Llama under a permissive license that allows for both research and\ncommercial use.", "published": "2023-08-24 17:39:13", "link": "http://arxiv.org/abs/2308.12950v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lexical Diversity in Kinship Across Languages and Dialects", "abstract": "Languages are known to describe the world in diverse ways. Across lexicons,\ndiversity is pervasive, appearing through phenomena such as lexical gaps and\nuntranslatability. However, in computational resources, such as multilingual\nlexical databases, diversity is hardly ever represented. In this paper, we\nintroduce a method to enrich computational lexicons with content relating to\nlinguistic diversity. The method is verified through two large-scale case\nstudies on kinship terminology, a domain known to be diverse across languages\nand cultures: one case study deals with seven Arabic dialects, while the other\none with three Indonesian languages. Our results, made available as browseable\nand downloadable computational resources, extend prior linguistics research on\nkinship terminology, and provide insight into the extent of diversity even\nwithin linguistically and culturally close communities.", "published": "2023-08-24 19:49:30", "link": "http://arxiv.org/abs/2308.13056v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards a Holistic Approach: Understanding Sociodemographic Biases in\n  NLP Models using an Interdisciplinary Lens", "abstract": "The rapid growth in the usage and applications of Natural Language Processing\n(NLP) in various sociotechnical solutions has highlighted the need for a\ncomprehensive understanding of bias and its impact on society. While research\non bias in NLP has expanded, several challenges persist that require attention.\nThese include the limited focus on sociodemographic biases beyond race and\ngender, the narrow scope of analysis predominantly centered on models, and the\ntechnocentric implementation approaches. This paper addresses these challenges\nand advocates for a more interdisciplinary approach to understanding bias in\nNLP. The work is structured into three facets, each exploring a specific aspect\nof bias in NLP.", "published": "2023-08-24 21:19:48", "link": "http://arxiv.org/abs/2308.13089v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentence Embedding Models for Ancient Greek Using Multilingual Knowledge\n  Distillation", "abstract": "Contextual language models have been trained on Classical languages,\nincluding Ancient Greek and Latin, for tasks such as lemmatization,\nmorphological tagging, part of speech tagging, authorship attribution, and\ndetection of scribal errors. However, high-quality sentence embedding models\nfor these historical languages are significantly more difficult to achieve due\nto the lack of training data. In this work, we use a multilingual knowledge\ndistillation approach to train BERT models to produce sentence embeddings for\nAncient Greek text. The state-of-the-art sentence embedding approaches for\nhigh-resource languages use massive datasets, but our distillation approach\nallows our Ancient Greek models to inherit the properties of these models while\nusing a relatively small amount of translated sentence data. We build a\nparallel sentence dataset using a sentence-embedding alignment method to align\nAncient Greek documents with English translations, and use this dataset to\ntrain our models. We evaluate our models on translation search, semantic\nsimilarity, and semantic retrieval tasks and investigate translation bias. We\nmake our training and evaluation datasets freely available at\nhttps://github.com/kevinkrahn/ancient-greek-datasets .", "published": "2023-08-24 23:38:44", "link": "http://arxiv.org/abs/2308.13116v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "GPTEval: A Survey on Assessments of ChatGPT and GPT-4", "abstract": "The emergence of ChatGPT has generated much speculation in the press about\nits potential to disrupt social and economic systems. Its astonishing language\nability has aroused strong curiosity among scholars about its performance in\ndifferent domains. There have been many studies evaluating the ability of\nChatGPT and GPT-4 in different tasks and disciplines. However, a comprehensive\nreview summarizing the collective assessment findings is lacking. The objective\nof this survey is to thoroughly analyze prior assessments of ChatGPT and GPT-4,\nfocusing on its language and reasoning abilities, scientific knowledge, and\nethical considerations. Furthermore, an examination of the existing evaluation\nmethods is conducted, offering several recommendations for future research in\nevaluating large language models.", "published": "2023-08-24 01:17:16", "link": "http://arxiv.org/abs/2308.12488v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Mind vs. Mouth: On Measuring Re-judge Inconsistency of Social Bias in\n  Large Language Models", "abstract": "Recent researches indicate that Pre-trained Large Language Models (LLMs)\npossess cognitive constructs similar to those observed in humans, prompting\nresearchers to investigate the cognitive aspects of LLMs. This paper focuses on\nexplicit and implicit social bias, a distinctive two-level cognitive construct\nin psychology. It posits that individuals' explicit social bias, which is their\nconscious expression of bias in the statements, may differ from their implicit\nsocial bias, which represents their unconscious bias. We propose a two-stage\napproach and discover a parallel phenomenon in LLMs known as \"re-judge\ninconsistency\" in social bias. In the initial stage, the LLM is tasked with\nautomatically completing statements, potentially incorporating implicit social\nbias. However, in the subsequent stage, the same LLM re-judges the biased\nstatement generated by itself but contradicts it. We propose that this re-judge\ninconsistency can be similar to the inconsistency between human's unaware\nimplicit social bias and their aware explicit social bias. Experimental\ninvestigations on ChatGPT and GPT-4 concerning common gender biases examined in\npsychology corroborate the highly stable nature of the re-judge inconsistency.\nThis finding may suggest that diverse cognitive constructs emerge as LLMs'\ncapabilities strengthen. Consequently, leveraging psychological theories can\nprovide enhanced insights into the underlying mechanisms governing the\nexpressions of explicit and implicit constructs in LLMs.", "published": "2023-08-24 05:35:58", "link": "http://arxiv.org/abs/2308.12578v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PromptMRG: Diagnosis-Driven Prompts for Medical Report Generation", "abstract": "Automatic medical report generation (MRG) is of great research value as it\nhas the potential to relieve radiologists from the heavy burden of report\nwriting. Despite recent advancements, accurate MRG remains challenging due to\nthe need for precise clinical understanding and disease identification.\nMoreover, the imbalanced distribution of diseases makes the challenge even more\npronounced, as rare diseases are underrepresented in training data, making\ntheir diagnostic performance unreliable. To address these challenges, we\npropose diagnosis-driven prompts for medical report generation (PromptMRG), a\nnovel framework that aims to improve the diagnostic accuracy of MRG with the\nguidance of diagnosis-aware prompts. Specifically, PromptMRG is based on\nencoder-decoder architecture with an extra disease classification branch. When\ngenerating reports, the diagnostic results from the classification branch are\nconverted into token prompts to explicitly guide the generation process. To\nfurther improve the diagnostic accuracy, we design cross-modal feature\nenhancement, which retrieves similar reports from the database to assist the\ndiagnosis of a query image by leveraging the knowledge from a pre-trained CLIP.\nMoreover, the disease imbalanced issue is addressed by applying an adaptive\nlogit-adjusted loss to the classification branch based on the individual\nlearning status of each disease, which overcomes the barrier of text decoder's\ninability to manipulate disease distributions. Experiments on two MRG\nbenchmarks show the effectiveness of the proposed method, where it obtains\nstate-of-the-art clinical efficacy performance on both datasets. The code is\navailable at https://github.com/jhb86253817/PromptMRG.", "published": "2023-08-24 07:10:31", "link": "http://arxiv.org/abs/2308.12604v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Improving Translation Faithfulness of Large Language Models via\n  Augmenting Instructions", "abstract": "Large Language Models (LLMs) present strong general capabilities, and a\ncurrent compelling challenge is stimulating their specialized capabilities,\nsuch as machine translation, through low-cost instruction tuning. The standard\ninstruction-following data is sequentially organized as the concatenation of an\ninstruction, an input, and a response. As the attention mechanism of LLMs has\nlimitations on local focus, LLMs tend to focus more on the words or sentences\nnearby at each position. This leads to a high risk of instruction forgetting\nduring decoding. To alleviate the above issues, We propose SWIE\n(Segment-Weighted Instruction Embedding) and an instruction-following dataset\nOVERMISS. SWIE improves the model instruction understanding by adding a global\ninstruction representation on the following input and response representations.\nOVERMISS improves model faithfulness by comparing over-translation and\nmiss-translation results with the correct translation. We apply our methods to\ntwo main-stream open-source LLMs, BLOOM and LLaMA. The experimental results\ndemonstrate significant improvements in translation performance with SWIE based\non BLOOMZ-3b, particularly in zero-shot and long text translations due to\nreduced instruction forgetting risk. Additionally, OVERMISS outperforms the\nbaseline in translation performance (e.g. an increase in BLEU scores from 0.69\nto 3.12 and an average improvement of 0.48 percentage comet scores for\nLLaMA-7b) with further enhancements seen in models combining OVERMISS and SWIE\n(e.g. the BLUE scores increase up to 0.56 from English to German across three\ndifferent backbones), and both exhibit improvements in the faithfulness metric\nbased on word alignment.", "published": "2023-08-24 09:32:29", "link": "http://arxiv.org/abs/2308.12674v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and\n  Vulnerabilities", "abstract": "Spurred by the recent rapid increase in the development and distribution of\nlarge language models (LLMs) across industry and academia, much recent work has\ndrawn attention to safety- and security-related threats and vulnerabilities of\nLLMs, including in the context of potentially criminal activities.\nSpecifically, it has been shown that LLMs can be misused for fraud,\nimpersonation, and the generation of malware; while other authors have\nconsidered the more general problem of AI alignment. It is important that\ndevelopers and practitioners alike are aware of security-related problems with\nsuch models. In this paper, we provide an overview of existing - predominantly\nscientific - efforts on identifying and mitigating threats and vulnerabilities\narising from LLMs. We present a taxonomy describing the relationship between\nthreats caused by the generative capabilities of LLMs, prevention measures\nintended to address such threats, and vulnerabilities arising from imperfect\nprevention measures. With our work, we hope to raise awareness of the\nlimitations of LLMs in light of such security concerns, among both experienced\ndevelopers and novel users of such technologies.", "published": "2023-08-24 14:45:50", "link": "http://arxiv.org/abs/2308.12833v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Inducing Causal Structure for Abstractive Text Summarization", "abstract": "The mainstream of data-driven abstractive summarization models tends to\nexplore the correlations rather than the causal relationships. Among such\ncorrelations, there can be spurious ones which suffer from the language prior\nlearned from the training corpus and therefore undermine the overall\neffectiveness of the learned model. To tackle this issue, we introduce a\nStructural Causal Model (SCM) to induce the underlying causal structure of the\nsummarization data. We assume several latent causal factors and non-causal\nfactors, representing the content and style of the document and summary.\nTheoretically, we prove that the latent factors in our SCM can be identified by\nfitting the observed training data under certain conditions. On the basis of\nthis, we propose a Causality Inspired Sequence-to-Sequence model (CI-Seq2Seq)\nto learn the causal representations that can mimic the causal factors, guiding\nus to pursue causal information for summary generation. The key idea is to\nreformulate the Variational Auto-encoder (VAE) to fit the joint distribution of\nthe document and summary variables from the training corpus. Experimental\nresults on two widely used text summarization datasets demonstrate the\nadvantages of our approach.", "published": "2023-08-24 16:06:36", "link": "http://arxiv.org/abs/2308.12888v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models Vote: Prompting for Rare Disease Identification", "abstract": "The emergence of generative Large Language Models (LLMs) emphasizes the need\nfor accurate and efficient prompting approaches. LLMs are often applied in\nFew-Shot Learning (FSL) contexts, where tasks are executed with minimal\ntraining data. FSL has become popular in many Artificial Intelligence (AI)\nsubdomains, including AI for health. Rare diseases affect a small fraction of\nthe population. Rare disease identification from clinical notes inherently\nrequires FSL techniques due to limited data availability. Manual data\ncollection and annotation is both expensive and time-consuming. In this paper,\nwe propose Models-Vote Prompting (MVP), a flexible prompting approach for\nimproving the performance of LLM queries in FSL settings. MVP works by\nprompting numerous LLMs to perform the same tasks and then conducting a\nmajority vote on the resulting outputs. This method achieves improved results\nto any one model in the ensemble on one-shot rare disease identification and\nclassification tasks. We also release a novel rare disease dataset for FSL,\navailable to those who signed the MIMIC-IV Data Use Agreement (DUA).\nFurthermore, in using MVP, each model is prompted multiple times, substantially\nincreasing the time needed for manual annotation, and to address this, we\nassess the feasibility of using JSON for automating generative LLM evaluation.", "published": "2023-08-24 16:09:13", "link": "http://arxiv.org/abs/2308.12890v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Qwen-VL: A Versatile Vision-Language Model for Understanding,\n  Localization, Text Reading, and Beyond", "abstract": "In this work, we introduce the Qwen-VL series, a set of large-scale\nvision-language models (LVLMs) designed to perceive and understand both texts\nand images. Starting from the Qwen-LM as a foundation, we endow it with visual\ncapacity by the meticulously designed (i) visual receptor, (ii) input-output\ninterface, (iii) 3-stage training pipeline, and (iv) multilingual multimodal\ncleaned corpus. Beyond the conventional image description and\nquestion-answering, we implement the grounding and text-reading ability of\nQwen-VLs by aligning image-caption-box tuples. The resulting models, including\nQwen-VL and Qwen-VL-Chat, set new records for generalist models under similar\nmodel scales on a broad range of visual-centric benchmarks (e.g., image\ncaptioning, question answering, visual grounding) and different settings (e.g.,\nzero-shot, few-shot). Moreover, on real-world dialog benchmarks, our\ninstruction-tuned Qwen-VL-Chat also demonstrates superiority compared to\nexisting vision-language chatbots. Code, demo and models are available at\nhttps://github.com/QwenLM/Qwen-VL.", "published": "2023-08-24 17:59:17", "link": "http://arxiv.org/abs/2308.12966v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Causal Parrots: Large Language Models May Talk Causality But Are Not\n  Causal", "abstract": "Some argue scale is all what is needed to achieve AI, covering even causal\nmodels. We make it clear that large language models (LLMs) cannot be causal and\ngive reason onto why sometimes we might feel otherwise. To this end, we define\nand exemplify a new subgroup of Structural Causal Model (SCM) that we call meta\nSCM which encode causal facts about other SCM within their variables. We\nconjecture that in the cases where LLM succeed in doing causal inference,\nunderlying was a respective meta SCM that exposed correlations between causal\nfacts in natural language on whose data the LLM was ultimately trained. If our\nhypothesis holds true, then this would imply that LLMs are like parrots in that\nthey simply recite the causal knowledge embedded in the data. Our empirical\nanalysis provides favoring evidence that current LLMs are even weak `causal\nparrots.'", "published": "2023-08-24 20:23:13", "link": "http://arxiv.org/abs/2308.13067v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Formal specification terminology for demographic agent-based models of\n  fixed-step single-clocked simulations", "abstract": "This document presents adequate formal terminology for the mathematical\nspecification of a subset of Agent Based Models (ABMs) in the field of\nDemography. The simulation of the targeted ABMs follows a fixedstep\nsingle-clocked pattern. The proposed terminology further improves the model\nunderstanding and can act as a stand-alone protocol for the specification and\noptionally the documentation of a significant set of (demographic) ABMs.\nNevertheless, it is imaginable the this terminology can serve as an inspiring\nbasis for further improvement to the largely-informal widely-used model\ndocumentation and communication O.D.D. protocol [Grimm and et al., 2020,\nAmouroux et al., 2010] to reduce many sources of ambiguity which hinder model\nreplications by other modelers. A published demographic model documentation,\nlargely simplified version of the Lone Parent Model [Gostoli and Silverman,\n2020] is separately published in [Elsheikh, 2023c] as illustration for the\nformal terminology presented here. The model was implemented in the Julia\nlanguage [Elsheikh, 2023b] based on the Agents.jl julia package [Datseris et\nal., 2022].", "published": "2023-08-24 20:57:07", "link": "http://arxiv.org/abs/2308.13081v3", "categories": ["cs.CL", "math.DS"], "primary_category": "cs.CL"}
{"title": "American Stories: A Large-Scale Structured Text Dataset of Historical\n  U.S. Newspapers", "abstract": "Existing full text datasets of U.S. public domain newspapers do not recognize\nthe often complex layouts of newspaper scans, and as a result the digitized\ncontent scrambles texts from articles, headlines, captions, advertisements, and\nother layout regions. OCR quality can also be low. This study develops a novel,\ndeep learning pipeline for extracting full article texts from newspaper images\nand applies it to the nearly 20 million scans in Library of Congress's public\ndomain Chronicling America collection. The pipeline includes layout detection,\nlegibility classification, custom OCR, and association of article texts\nspanning multiple bounding boxes. To achieve high scalability, it is built with\nefficient architectures designed for mobile phones. The resulting American\nStories dataset provides high quality data that could be used for pre-training\na large language model to achieve better understanding of historical English\nand historical world knowledge. The dataset could also be added to the external\ndatabase of a retrieval-augmented language model to make historical information\n- ranging from interpretations of political events to minutiae about the lives\nof people's ancestors - more widely accessible. Furthermore, structured article\ntexts facilitate using transformer-based methods for popular social science\napplications like topic classification, detection of reproduced content, and\nnews story clustering. Finally, American Stories provides a massive silver\nquality dataset for innovating multimodal layout analysis models and other\nmultimodal applications.", "published": "2023-08-24 00:24:42", "link": "http://arxiv.org/abs/2308.12477v1", "categories": ["cs.CL", "cs.CV", "econ.GN", "q-fin.EC"], "primary_category": "cs.CL"}
{"title": "MultiPA: A Multi-task Speech Pronunciation Assessment Model for Open\n  Response Scenarios", "abstract": "Pronunciation assessment models designed for open response scenarios enable\nusers to practice language skills in a manner similar to real-life\ncommunication. However, previous open-response pronunciation assessment models\nhave predominantly focused on a single pronunciation task, such as\nsentence-level accuracy, rather than offering a comprehensive assessment in\nvarious aspects. We propose MultiPA, a Multitask Pronunciation Assessment model\nthat provides sentence-level accuracy, fluency, prosody, and word-level\naccuracy assessment for open responses. We examined the correlation between\ndifferent pronunciation tasks and showed the benefits of multi-task learning.\nOur model reached the state-of-the-art performance on existing in-domain data\nsets and effectively generalized to an out-of-domain dataset that we newly\ncollected. The experimental results demonstrate the practical utility of our\nmodel in real-world applications.", "published": "2023-08-24 01:24:09", "link": "http://arxiv.org/abs/2308.12490v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "CALM : A Multi-task Benchmark for Comprehensive Assessment of Language\n  Model Bias", "abstract": "As language models (LMs) become increasingly powerful and widely used, it is\nimportant to quantify them for sociodemographic bias with potential for harm.\nPrior measures of bias are sensitive to perturbations in the templates designed\nto compare performance across social groups, due to factors such as low\ndiversity or limited number of templates. Also, most previous work considers\nonly one NLP task. We introduce Comprehensive Assessment of Language Models\n(CALM) for robust measurement of two types of universally relevant\nsociodemographic bias, gender and race. CALM integrates sixteen datasets for\nquestion-answering, sentiment analysis and natural language inference. Examples\nfrom each dataset are filtered to produce 224 templates with high diversity\n(e.g., length, vocabulary). We assemble 50 highly frequent person names for\neach of seven distinct demographic groups to generate 78,400 prompts covering\nthe three NLP tasks. Our empirical evaluation shows that CALM bias scores are\nmore robust and far less sensitive than previous bias measurements to\nperturbations in the templates, such as synonym substitution, or to random\nsubset selection of templates. We apply CALM to 20 large language models, and\nfind that for 2 language model series, larger parameter models tend to be more\nbiased than smaller ones. The T0 series is the least biased model families, of\nthe 20 LLMs investigated here. The code is available at\nhttps://github.com/vipulgupta1011/CALM.", "published": "2023-08-24 03:53:55", "link": "http://arxiv.org/abs/2308.12539v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Advancing Hungarian Text Processing with HuSpaCy: Efficient and Accurate\n  NLP Pipelines", "abstract": "This paper presents a set of industrial-grade text processing models for\nHungarian that achieve near state-of-the-art performance while balancing\nresource efficiency and accuracy. Models have been implemented in the spaCy\nframework, extending the HuSpaCy toolkit with several improvements to its\narchitecture. Compared to existing NLP tools for Hungarian, all of our\npipelines feature all basic text processing steps including tokenization,\nsentence-boundary detection, part-of-speech tagging, morphological feature\ntagging, lemmatization, dependency parsing and named entity recognition with\nhigh accuracy and throughput. We thoroughly evaluated the proposed\nenhancements, compared the pipelines with state-of-the-art tools and\ndemonstrated the competitive performance of the new models in all text\npreprocessing steps. All experiments are reproducible and the pipelines are\nfreely available under a permissive license.", "published": "2023-08-24 08:19:51", "link": "http://arxiv.org/abs/2308.12635v1", "categories": ["cs.CL", "cs.AI", "stat.ML", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "WavMark: Watermarking for Audio Generation", "abstract": "Recent breakthroughs in zero-shot voice synthesis have enabled imitating a\nspeaker's voice using just a few seconds of recording while maintaining a high\nlevel of realism. Alongside its potential benefits, this powerful technology\nintroduces notable risks, including voice fraud and speaker impersonation.\nUnlike the conventional approach of solely relying on passive methods for\ndetecting synthetic data, watermarking presents a proactive and robust defence\nmechanism against these looming risks. This paper introduces an innovative\naudio watermarking framework that encodes up to 32 bits of watermark within a\nmere 1-second audio snippet. The watermark is imperceptible to human senses and\nexhibits strong resilience against various attacks. It can serve as an\neffective identifier for synthesized voices and holds potential for broader\napplications in audio copyright protection. Moreover, this framework boasts\nhigh flexibility, allowing for the combination of multiple watermark segments\nto achieve heightened robustness and expanded capacity. Utilizing 10 to\n20-second audio as the host, our approach demonstrates an average Bit Error\nRate (BER) of 0.48\\% across ten common attacks, a remarkable reduction of over\n2800\\% in BER compared to the state-of-the-art watermarking tool. See\nhttps://aka.ms/wavmark for demos of our work.", "published": "2023-08-24 13:17:35", "link": "http://arxiv.org/abs/2308.12770v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Beyond Document Page Classification: Design, Datasets, and Challenges", "abstract": "This paper highlights the need to bring document classification benchmarking\ncloser to real-world applications, both in the nature of data tested ($X$:\nmulti-channel, multi-paged, multi-industry; $Y$: class distributions and label\nset variety) and in classification tasks considered ($f$: multi-page document,\npage stream, and document bundle classification, ...). We identify the lack of\npublic multi-page document classification datasets, formalize different\nclassification tasks arising in application scenarios, and motivate the value\nof targeting efficient multi-page document representations. An experimental\nstudy on proposed multi-page document classification datasets demonstrates that\ncurrent benchmarks have become irrelevant and need to be updated to evaluate\ncomplete documents, as they naturally occur in practice. This reality check\nalso calls for more mature evaluation methodologies, covering calibration\nevaluation, inference complexity (time-memory), and a range of realistic\ndistribution shifts (e.g., born-digital vs. scanning noise, shifting page\norder). Our study ends on a hopeful note by recommending concrete avenues for\nfuture improvements.}", "published": "2023-08-24 16:16:47", "link": "http://arxiv.org/abs/2308.12896v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Can Linguistic Knowledge Improve Multimodal Alignment in Vision-Language\n  Pretraining?", "abstract": "The multimedia community has shown a significant interest in perceiving and\nrepresenting the physical world with multimodal pretrained neural network\nmodels, and among them, the visual-language pertaining (VLP) is, currently, the\nmost captivating topic. However, there have been few endeavors dedicated to the\nexploration of 1) whether essential linguistic knowledge (e.g., semantics and\nsyntax) can be extracted during VLP, and 2) how such linguistic knowledge\nimpact or enhance the multimodal alignment. In response, here we aim to\nelucidate the impact of comprehensive linguistic knowledge, including semantic\nexpression and syntactic structure, on multimodal alignment. Specifically, we\ndesign and release the SNARE, the first large-scale multimodal alignment\nprobing benchmark, to detect the vital linguistic components, e.g., lexical,\nsemantic, and syntax knowledge, containing four tasks: Semantic structure,\nNegation logic, Attribute ownership, and Relationship composition. Based on our\nproposed probing benchmarks, our holistic analyses of five advanced VLP models\nillustrate that the VLP model: i) shows insensitivity towards complex syntax\nstructures and relies on content words for sentence comprehension; ii)\ndemonstrates limited comprehension of combinations between sentences and\nnegations; iii) faces challenges in determining the presence of actions or\nspatial relationships within visual information and struggles with verifying\nthe correctness of triple combinations. We make our benchmark and code\navailable at \\url{https://github.com/WangFei-2019/SNARE/}.", "published": "2023-08-24 16:17:40", "link": "http://arxiv.org/abs/2308.12898v2", "categories": ["cs.MM", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.MM"}
{"title": "Considerations for health care institutions training large language\n  models on electronic health records", "abstract": "Large language models (LLMs) like ChatGPT have excited scientists across\nfields; in medicine, one source of excitement is the potential applications of\nLLMs trained on electronic health record (EHR) data. But there are tough\nquestions we must first answer if health care institutions are interested in\nhaving LLMs trained on their own data; should they train an LLM from scratch or\nfine-tune it from an open-source model? For healthcare institutions with a\npredefined budget, what are the biggest LLMs they can afford? In this study, we\ntake steps towards answering these questions with an analysis on dataset sizes,\nmodel sizes, and costs for LLM training using EHR data. This analysis provides\na framework for thinking about these questions in terms of data scale, compute\nscale, and training budgets.", "published": "2023-08-24 00:09:01", "link": "http://arxiv.org/abs/2309.12339v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Real-time Detection of AI-Generated Speech for DeepFake Voice Conversion", "abstract": "There are growing implications surrounding generative AI in the speech domain\nthat enable voice cloning and real-time voice conversion from one individual to\nanother. This technology poses a significant ethical threat and could lead to\nbreaches of privacy and misrepresentation, thus there is an urgent need for\nreal-time detection of AI-generated speech for DeepFake Voice Conversion. To\naddress the above emerging issues, the DEEP-VOICE dataset is generated in this\nstudy, comprised of real human speech from eight well-known figures and their\nspeech converted to one another using Retrieval-based Voice Conversion.\nPresenting as a binary classification problem of whether the speech is real or\nAI-generated, statistical analysis of temporal audio features through t-testing\nreveals that there are significantly different distributions. Hyperparameter\noptimisation is implemented for machine learning models to identify the source\nof speech. Following the training of 208 individual machine learning models\nover 10-fold cross validation, it is found that the Extreme Gradient Boosting\nmodel can achieve an average classification accuracy of 99.3% and can classify\nspeech in real-time, at around 0.004 milliseconds given one second of speech.\nAll data generated for this study is released publicly for future research on\nAI speech detection.", "published": "2023-08-24 12:26:15", "link": "http://arxiv.org/abs/2308.12734v1", "categories": ["cs.SD", "cs.CL", "cs.HC", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Financial News Analytics Using Fine-Tuned Llama 2 GPT Model", "abstract": "The paper considers the possibility to fine-tune Llama 2 GPT large language\nmodel (LLM) for the multitask analysis of financial news. For fine-tuning, the\nPEFT/LoRA based approach was used. In the study, the model was fine-tuned for\nthe following tasks: analysing a text from financial market perspectives,\nhighlighting main points of a text, summarizing a text and extracting named\nentities with appropriate sentiments. The obtained results show that the\nfine-tuned Llama 2 model can perform a multitask financial news analysis with a\nspecified structure of response, part of response can be a structured text and\nanother part of data can have JSON format for further processing. Extracted\nsentiments for named entities can be considered as predictive features in\nsupervised machine learning models with quantitative target variables.", "published": "2023-08-24 18:58:10", "link": "http://arxiv.org/abs/2308.13032v2", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Naaloss: Rethinking the objective of speech enhancement", "abstract": "Reducing noise interference is crucial for automatic speech recognition (ASR)\nin a real-world scenario. However, most single-channel speech enhancement (SE)\ngenerates \"processing artifacts\" that negatively affect ASR performance. Hence,\nin this study, we suggest a Noise- and Artifacts-aware loss function, NAaLoss,\nto ameliorate the influence of artifacts from a novel perspective. NAaLoss\nconsiders the loss of estimation, de-artifact, and noise ignorance, enabling\nthe learned SE to individually model speech, artifacts, and noise. We examine\ntwo SE models (simple/advanced) learned with NAaLoss under various input\nscenarios (clean/noisy) using two configurations of the ASR system\n(with/without noise robustness). Experiments reveal that NAaLoss significantly\nimproves the ASR performance of most setups while preserving the quality of SE\ntoward perception and intelligibility. Furthermore, we visualize artifacts\nthrough waveforms and spectrograms, and explain their impact on ASR.", "published": "2023-08-24 07:29:31", "link": "http://arxiv.org/abs/2308.12615v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Whombat: An open-source annotation tool for machine learning development\n  in bioacoustics", "abstract": "1. Automated analysis of bioacoustic recordings using machine learning (ML)\nmethods has the potential to greatly scale biodiversity monitoring efforts. The\nuse of ML for high-stakes applications, such as conservation research, demands\na data-centric approach with a focus on utilizing carefully annotated and\ncurated evaluation and training data that is relevant and representative.\nCreating annotated datasets of sound recordings presents a number of\nchallenges, such as managing large collections of recordings with associated\nmetadata, developing flexible annotation tools that can accommodate the diverse\nrange of vocalization profiles of different organisms, and addressing the\nscarcity of expert annotators.\n  2. We present Whombat a user-friendly, browser-based interface for managing\naudio recordings and annotation projects, with several visualization,\nexploration, and annotation tools. It enables users to quickly annotate,\nreview, and share annotations, as well as visualize and evaluate a set of\nmachine learning predictions on a dataset. The tool facilitates an iterative\nworkflow where user annotations and machine learning predictions feedback to\nenhance model performance and annotation quality.\n  3. We demonstrate the flexibility of Whombat by showcasing two distinct use\ncases: an project aimed at enhancing automated UK bat call identification at\nthe Bat Conservation Trust (BCT), and a collaborative effort among the USDA\nForest Service and Oregon State University researchers exploring bioacoustic\napplications and extending automated avian classification models in the Pacific\nNorthwest, USA.\n  4. Whombat is a flexible tool that can effectively address the challenges of\nannotation for bioacoustic research. It can be used for individual and\ncollaborative work, hosted on a shared server or accessed remotely, or run on a\npersonal computer without the need for coding skills.", "published": "2023-08-24 10:06:20", "link": "http://arxiv.org/abs/2308.12688v2", "categories": ["cs.SD", "eess.AS", "H.5.5; H.5.2; J.3; I.2.m"], "primary_category": "cs.SD"}
{"title": "Sparks of Large Audio Models: A Survey and Outlook", "abstract": "This survey paper provides a comprehensive overview of the recent\nadvancements and challenges in applying large language models to the field of\naudio signal processing. Audio processing, with its diverse signal\nrepresentations and a wide range of sources--from human voices to musical\ninstruments and environmental sounds--poses challenges distinct from those\nfound in traditional Natural Language Processing scenarios. Nevertheless,\n\\textit{Large Audio Models}, epitomized by transformer-based architectures,\nhave shown marked efficacy in this sphere. By leveraging massive amount of\ndata, these models have demonstrated prowess in a variety of audio tasks,\nspanning from Automatic Speech Recognition and Text-To-Speech to Music\nGeneration, among others. Notably, recently these Foundational Audio Models,\nlike SeamlessM4T, have started showing abilities to act as universal\ntranslators, supporting multiple speech tasks for up to 100 languages without\nany reliance on separate task-specific systems. This paper presents an in-depth\nanalysis of state-of-the-art methodologies regarding \\textit{Foundational Large\nAudio Models}, their performance benchmarks, and their applicability to\nreal-world scenarios. We also highlight current limitations and provide\ninsights into potential future research directions in the realm of\n\\textit{Large Audio Models} with the intent to spark further discussion,\nthereby fostering innovation in the next generation of audio-processing\nsystems. Furthermore, to cope with the rapid development in this area, we will\nconsistently update the relevant repository with relevant recent articles and\ntheir open-source implementations at\nhttps://github.com/EmulationAI/awesome-large-audio-models.", "published": "2023-08-24 13:47:16", "link": "http://arxiv.org/abs/2308.12792v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Attention-Based Acoustic Feature Fusion Network for Depression Detection", "abstract": "Depression, a common mental disorder, significantly influences individuals\nand imposes considerable societal impacts. The complexity and heterogeneity of\nthe disorder necessitate prompt and effective detection, which nonetheless,\nposes a difficult challenge. This situation highlights an urgent requirement\nfor improved detection methods. Exploiting auditory data through advanced\nmachine learning paradigms presents promising research directions. Yet,\nexisting techniques mainly rely on single-dimensional feature models,\npotentially neglecting the abundance of information hidden in various speech\ncharacteristics. To rectify this, we present the novel Attention-Based Acoustic\nFeature Fusion Network (ABAFnet) for depression detection. ABAFnet combines\nfour different acoustic features into a comprehensive deep learning model,\nthereby effectively integrating and blending multi-tiered features. We present\na novel weight adjustment module for late fusion that boosts performance by\nefficaciously synthesizing these features. The effectiveness of our approach is\nconfirmed via extensive validation on two clinical speech databases, CNRAC and\nCS-NRAC, thereby outperforming previous methods in depression detection and\nsubtype classification. Further in-depth analysis confirms the key role of each\nfeature and highlights the importance of MFCCrelated features in speech-based\ndepression detection.", "published": "2023-08-24 00:31:51", "link": "http://arxiv.org/abs/2308.12478v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "UNISOUND System for VoxCeleb Speaker Recognition Challenge 2023", "abstract": "This report describes the UNISOUND submission for Track1 and Track2 of\nVoxCeleb Speaker Recognition Challenge 2023 (VoxSRC 2023). We submit the same\nsystem on Track 1 and Track 2, which is trained with only VoxCeleb2-dev.\nLarge-scale ResNet and RepVGG architectures are developed for the challenge. We\npropose a consistency-aware score calibration method, which leverages the\nstability of audio voiceprints in similarity score by a Consistency Measure\nFactor (CMF). CMF brings a huge performance boost in this challenge. Our final\nsystem is a fusion of six models and achieves the first place in Track 1 and\nsecond place in Track 2 of VoxSRC 2023. The minDCF of our submission is 0.0855\nand the EER is 1.5880%.", "published": "2023-08-24 03:30:38", "link": "http://arxiv.org/abs/2308.12526v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Hybrid noise shaping for audio coding using perfectly overlapped window", "abstract": "In recent years, audio coding technology has been standardized based on\nseveral frameworks that incorporate linear predictive coding (LPC). However,\ncoding the transient signal using frequency-domain LP residual signals remains\na challenge. To address this, temporal noise shaping (TNS) can be adapted,\nalthough it cannot be effectively operated since the estimated temporal\nenvelope in the modified discrete cosine transform (MDCT) domain is accompanied\nby the time-domain aliasing (TDA) terms. In this study, we propose the\nmodulated complex lapped transform-based coding framework integrated with\ntransform coded excitation (TCX) and complex LPC-based TNS (CTNS). Our approach\nuses a 50\\% overlap window and switching scheme for the CTNS to improve the\ncoding efficiency. Additionally, an adaptive calculation of the target bits for\nthe sub-bands using the frequency envelope information based on the quantized\nLPC coefficients is proposed. To minimize the quantization mismatch between\nboth modes, an integrated quantization for real and complex values and a TDA\naugmentation method that compensates for the artificially generated TDA\ncomponents during switching operations are proposed. The proposed coding\nframework shows a superior performance in both objective metrics and subjective\nlistening tests, thereby demonstrating its low bit-rate audio coding.", "published": "2023-08-24 05:12:33", "link": "http://arxiv.org/abs/2308.12566v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Exploiting Time-Frequency Conformers for Music Audio Enhancement", "abstract": "With the proliferation of video platforms on the internet, recording musical\nperformances by mobile devices has become commonplace. However, these\nrecordings often suffer from degradation such as noise and reverberation, which\nnegatively impact the listening experience. Consequently, the necessity for\nmusic audio enhancement (referred to as music enhancement from this point\nonward), involving the transformation of degraded audio recordings into\npristine high-quality music, has surged to augment the auditory experience. To\naddress this issue, we propose a music enhancement system based on the\nConformer architecture that has demonstrated outstanding performance in speech\nenhancement tasks. Our approach explores the attention mechanisms of the\nConformer and examines their performance to discover the best approach for the\nmusic enhancement task. Our experimental results show that our proposed model\nachieves state-of-the-art performance on single-stem music enhancement.\nFurthermore, our system can perform general music enhancement with multi-track\nmixtures, which has not been examined in previous work.", "published": "2023-08-24 06:56:54", "link": "http://arxiv.org/abs/2308.12599v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Emotion-Aligned Contrastive Learning Between Images and Music", "abstract": "Traditional music search engines rely on retrieval methods that match natural\nlanguage queries with music metadata. There have been increasing efforts to\nexpand retrieval methods to consider the audio characteristics of music itself,\nusing queries of various modalities including text, video, and speech. While\nmost approaches aim to match general music semantics to the input queries, only\na few focus on affective qualities. In this work, we address the task of\nretrieving emotionally-relevant music from image queries by learning an\naffective alignment between images and music audio. Our approach focuses on\nlearning an emotion-aligned joint embedding space between images and music.\nThis embedding space is learned via emotion-supervised contrastive learning,\nusing an adapted cross-modal version of the SupCon loss. We evaluate the joint\nembeddings through cross-modal retrieval tasks (image-to-music and\nmusic-to-image) based on emotion labels. Furthermore, we investigate the\ngeneralizability of the learned music embeddings via automatic music tagging.\nOur experiments show that the proposed approach successfully aligns images and\nmusic, and that the learned embedding space is effective for cross-modal\nretrieval applications.", "published": "2023-08-24 07:20:47", "link": "http://arxiv.org/abs/2308.12610v3", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Towards Automated Animal Density Estimation with Acoustic Spatial\n  Capture-Recapture", "abstract": "Passive acoustic monitoring can be an effective way of monitoring wildlife\npopulations that are acoustically active but difficult to survey visually.\nDigital recorders allow surveyors to gather large volumes of data at low cost,\nbut identifying target species vocalisations in these data is non-trivial.\nMachine learning (ML) methods are often used to do the identification. They can\nprocess large volumes of data quickly, but they do not detect all vocalisations\nand they do generate some false positives (vocalisations that are not from the\ntarget species). Existing wildlife abundance survey methods have been designed\nspecifically to deal with the first of these mistakes, but current methods of\ndealing with false positives are not well-developed. They do not take account\nof features of individual vocalisations, some of which are more likely to be\nfalse positives than others. We propose three methods for acoustic spatial\ncapture-recapture inference that integrate individual-level measures of\nconfidence from ML vocalisation identification into the likelihood and hence\nintegrate ML uncertainty into inference. The methods include a mixture model in\nwhich species identity is a latent variable. We test the methods by simulation\nand find that in a scenario based on acoustic data from Hainan gibbons, in\nwhich ignoring false positives results in 17% positive bias, our methods give\nnegligible bias and coverage probabilities that are close to the nominal 95%\nlevel.", "published": "2023-08-24 15:29:24", "link": "http://arxiv.org/abs/2308.12859v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ME"], "primary_category": "cs.SD"}
{"title": "A Survey of AI Music Generation Tools and Models", "abstract": "In this work, we provide a comprehensive survey of AI music generation tools,\nincluding both research projects and commercialized applications. To conduct\nour analysis, we classified music generation approaches into three categories:\nparameter-based, text-based, and visual-based classes. Our survey highlights\nthe diverse possibilities and functional features of these tools, which cater\nto a wide range of users, from regular listeners to professional musicians. We\nobserved that each tool has its own set of advantages and limitations. As a\nresult, we have compiled a comprehensive list of these factors that should be\nconsidered during the tool selection process. Moreover, our survey offers\ncritical insights into the underlying mechanisms and challenges of AI music\ngeneration.", "published": "2023-08-24 00:49:08", "link": "http://arxiv.org/abs/2308.12982v1", "categories": ["cs.SD", "cs.AI", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Generalizable Zero-Shot Speaker Adaptive Speech Synthesis with\n  Disentangled Representations", "abstract": "While most research into speech synthesis has focused on synthesizing\nhigh-quality speech for in-dataset speakers, an equally essential yet unsolved\nproblem is synthesizing speech for unseen speakers who are out-of-dataset with\nlimited reference data, i.e., speaker adaptive speech synthesis. Many studies\nhave proposed zero-shot speaker adaptive text-to-speech and voice conversion\napproaches aimed at this task. However, most current approaches suffer from the\ndegradation of naturalness and speaker similarity when synthesizing speech for\nunseen speakers (i.e., speakers not in the training dataset) due to the poor\ngeneralizability of the model in out-of-distribution data. To address this\nproblem, we propose GZS-TV, a generalizable zero-shot speaker adaptive\ntext-to-speech and voice conversion model. GZS-TV introduces disentangled\nrepresentation learning for both speaker embedding extraction and timbre\ntransformation to improve model generalization and leverages the representation\nlearning capability of the variational autoencoder to enhance the speaker\nencoder. Our experiments demonstrate that GZS-TV reduces performance\ndegradation on unseen speakers and outperforms all baseline models in multiple\ndatasets.", "published": "2023-08-24 18:13:10", "link": "http://arxiv.org/abs/2308.13007v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Component attention network for multimodal dance improvisation\n  recognition", "abstract": "Dance improvisation is an active research topic in the arts. Motion analysis\nof improvised dance can be challenging due to its unique dynamics. Data-driven\ndance motion analysis, including recognition and generation, is often limited\nto skeletal data. However, data of other modalities, such as audio, can be\nrecorded and benefit downstream tasks. This paper explores the application and\nperformance of multimodal fusion methods for human motion recognition in the\ncontext of dance improvisation. We propose an attention-based model, component\nattention network (CANet), for multimodal fusion on three levels: 1) feature\nfusion with CANet, 2) model fusion with CANet and graph convolutional network\n(GCN), and 3) late fusion with a voting strategy. We conduct thorough\nexperiments to analyze the impact of each modality in different fusion methods\nand distinguish critical temporal or component features. We show that our\nproposed model outperforms the two baseline methods, demonstrating its\npotential for analyzing improvisation in dance.", "published": "2023-08-24 15:04:30", "link": "http://arxiv.org/abs/2310.05938v1", "categories": ["cs.CV", "cs.AI", "cs.SD", "eess.AS", "I.2; I.5.4"], "primary_category": "cs.CV"}
