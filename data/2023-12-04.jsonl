{"title": "Explaining with Contrastive Phrasal Highlighting: A Case Study in\n  Assisting Humans to Detect Translation Differences", "abstract": "Explainable NLP techniques primarily explain by answering \"Which tokens in\nthe input are responsible for this prediction?''. We argue that for NLP models\nthat make predictions by comparing two input texts, it is more useful to\nexplain by answering \"What differences between the two inputs explain this\nprediction?''. We introduce a technique to generate contrastive highlights that\nexplain the predictions of a semantic divergence model via\nphrase-alignment-guided erasure. We show that the resulting highlights match\nhuman rationales of cross-lingual semantic differences better than popular\npost-hoc saliency techniques and that they successfully help people detect\nfine-grained meaning differences in human translations and critical machine\ntranslation errors.", "published": "2023-12-04 02:40:28", "link": "http://arxiv.org/abs/2312.01582v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Expand BERT Representation with Visual Information via Grounded Language\n  Learning with Multimodal Partial Alignment", "abstract": "Language models have been supervised with both language-only objective and\nvisual grounding in existing studies of visual-grounded language learning.\nHowever, due to differences in the distribution and scale of visual-grounded\ndatasets and language corpora, the language model tends to mix up the context\nof the tokens that occurred in the grounded data with those that do not. As a\nresult, during representation learning, there is a mismatch between the visual\ninformation and the contextual meaning of the sentence. To overcome this\nlimitation, we propose GroundedBERT - a grounded language learning method that\nenhances the BERT representation with visually grounded information.\nGroundedBERT comprises two components: (i) the original BERT which captures the\ncontextual representation of words learned from the language corpora, and (ii)\na visual grounding module which captures visual information learned from\nvisual-grounded datasets. Moreover, we employ Optimal Transport (OT),\nspecifically its partial variant, to solve the fractional alignment problem\nbetween the two modalities. Our proposed method significantly outperforms the\nbaseline language models on various language tasks of the GLUE and SQuAD\ndatasets.", "published": "2023-12-04 03:16:48", "link": "http://arxiv.org/abs/2312.01592v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Voice-Based Smart Assistant System for Vehicles using RASA", "abstract": "Conversational AIs, or chatbots, mimic human speech when conversing. Smart\nassistants facilitate the automation of several tasks that needed human\nintervention earlier. Because of their accuracy, absence of dependence on human\nresources, and accessibility around the clock, chatbots can be employed in\nvehicles too. Due to people's propensity to divert their attention away from\nthe task of driving while engaging in other activities like calling, playing\nmusic, navigation, and getting updates on the weather forecast and latest news,\nroad safety has declined and accidents have increased as a result. It would be\nadvantageous to automate these tasks using voice commands rather than carrying\nthem out manually. This paper focuses on the development of a voice-based smart\nassistance application for vehicles based on the RASA framework. The smart\nassistant provides functionalities like navigation, communication via calls,\ngetting weather forecasts and the latest news updates, and music that are\ncompletely voice-based in nature.", "published": "2023-12-04 05:48:18", "link": "http://arxiv.org/abs/2312.01642v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Retrieval-augmented Multi-modal Chain-of-Thoughts Reasoning for Large\n  Language Models", "abstract": "The advancement of Large Language Models (LLMs) has brought substantial\nattention to the Chain of Thought (CoT) approach, primarily due to its ability\nto enhance the capability of LLMs on complex reasoning tasks. Moreover, the\nsignificance of CoT approaches extends to the application of LLMs for\nmulti-modal tasks. However, the selection of optimal CoT demonstration examples\nin multi-modal reasoning remains less explored for LLMs due to the inherent\ncomplexity of multi-modal examples. In this paper, we introduce a novel\napproach that addresses this challenge by using retrieval mechanisms to\ndynamically and automatically select demonstration examples based on\ncross-modal and intra-modal similarities. Furthermore, we employ a Stratified\nSampling method of categorising demonstration examples into groups based on\ntheir types and then retrieving examples from different groups respectively to\npromote the diversity of demonstration examples. Through a series of\nexperiments on two popular benchmark datasets: ScienceQA and MathVista, we\ndemonstrate that our approach significantly improves the performance of GPT-4\nby 6% on ScienceQA and 12.9% on MathVista, and enhances the performance of\nGPT-4V on two datasets by 2.7%, substantially improving the performance of the\nmost advanced LLMs and LMMs for complex multi-modal reasoning tasks.", "published": "2023-12-04 08:07:21", "link": "http://arxiv.org/abs/2312.01714v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exchange-of-Thought: Enhancing Large Language Model Capabilities through\n  Cross-Model Communication", "abstract": "Large Language Models (LLMs) have recently made significant strides in\ncomplex reasoning tasks through the Chain-of-Thought technique. Despite this\nprogress, their reasoning is often constrained by their intrinsic\nunderstanding, lacking external insights. To address this, we propose\nExchange-of-Thought (EoT), a novel framework that enables cross-model\ncommunication during problem-solving. Drawing inspiration from network\ntopology, EoT integrates four unique communication paradigms: Memory, Report,\nRelay, and Debate. This paper delves into the communication dynamics and volume\nassociated with each paradigm. To counterbalance the risks of incorrect\nreasoning chains, we implement a robust confidence evaluation mechanism within\nthese communications. Our experiments across diverse complex reasoning tasks\ndemonstrate that EoT significantly surpasses established baselines,\nunderscoring the value of external insights in enhancing LLM performance.\nFurthermore, we show that EoT achieves these superior results in a\ncost-effective manner, marking a promising advancement for efficient and\ncollaborative AI problem-solving.", "published": "2023-12-04 11:53:56", "link": "http://arxiv.org/abs/2312.01823v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompting Disentangled Embeddings for Knowledge Graph Completion with\n  Pre-trained Language Model", "abstract": "Both graph structures and textual information play a critical role in\nKnowledge Graph Completion (KGC). With the success of Pre-trained Language\nModels (PLMs) such as BERT, they have been applied for text encoding for KGC.\nHowever, the current methods mostly prefer to fine-tune PLMs, leading to huge\ntraining costs and limited scalability to larger PLMs. In contrast, we propose\nto utilize prompts and perform KGC on a frozen PLM with only the prompts\ntrained. Accordingly, we propose a new KGC method named PDKGC with two prompts\n-- a hard task prompt which is to adapt the KGC task to the PLM pre-training\ntask of token prediction, and a disentangled structure prompt which learns\ndisentangled graph representation so as to enable the PLM to combine more\nrelevant structure knowledge with the text information. With the two prompts,\nPDKGC builds a textual predictor and a structural predictor, respectively, and\ntheir combination leads to more comprehensive entity prediction. Solid\nevaluation on three widely used KGC datasets has shown that PDKGC often\noutperforms the baselines including the state-of-the-art, and its components\nare all effective. Our codes and data are available at\nhttps://github.com/genggengcss/PDKGC.", "published": "2023-12-04 12:20:25", "link": "http://arxiv.org/abs/2312.01837v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Dependencies in Fact Editing for Language Models: Specificity\n  and Implication Awareness", "abstract": "The potential of using a large language model (LLM) as a knowledge base (KB)\nhas sparked significant interest. To manage the knowledge acquired by LLMs, we\nneed to ensure that the editing of learned facts respects internal logical\nconstraints, which are known as dependency of knowledge. Existing work on\nediting LLMs has partially addressed the issue of dependency, when the editing\nof a fact should apply to its lexical variations without disrupting irrelevant\nones. However, they neglect the dependency between a fact and its logical\nimplications. We propose an evaluation protocol with an accompanying\nquestion-answering dataset, DepEdit, that provides a comprehensive assessment\nof the editing process considering the above notions of dependency. Our\nprotocol involves setting up a controlled environment in which we edit facts\nand monitor their impact on LLMs, along with their implications based on\nIf-Then rules. Extensive experiments on DepEdit show that existing knowledge\nediting methods are sensitive to the surface form of knowledge, and that they\nhave limited performance in inferring the implications of edited facts.", "published": "2023-12-04 12:45:30", "link": "http://arxiv.org/abs/2312.01858v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero- and Few-Shots Knowledge Graph Triplet Extraction with Large\n  Language Models", "abstract": "In this work, we tested the Triplet Extraction (TE) capabilities of a variety\nof Large Language Models (LLMs) of different sizes in the Zero- and Few-Shots\nsettings. In detail, we proposed a pipeline that dynamically gathers contextual\ninformation from a Knowledge Base (KB), both in the form of context triplets\nand of (sentence, triplets) pairs as examples, and provides it to the LLM\nthrough a prompt. The additional context allowed the LLMs to be competitive\nwith all the older fully trained baselines based on the Bidirectional Long\nShort-Term Memory (BiLSTM) Network architecture. We further conducted a\ndetailed analysis of the quality of the gathered KB context, finding it to be\nstrongly correlated with the final TE performance of the model. In contrast,\nthe size of the model appeared to only logarithmically improve the TE\ncapabilities of the LLMs.", "published": "2023-12-04 15:12:04", "link": "http://arxiv.org/abs/2312.01954v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring Distributional Shifts in Text: The Advantage of Language\n  Model-Based Embeddings", "abstract": "An essential part of monitoring machine learning models in production is\nmeasuring input and output data drift. In this paper, we present a system for\nmeasuring distributional shifts in natural language data and highlight and\ninvestigate the potential advantage of using large language models (LLMs) for\nthis problem. Recent advancements in LLMs and their successful adoption in\ndifferent domains indicate their effectiveness in capturing semantic\nrelationships for solving various natural language processing problems. The\npower of LLMs comes largely from the encodings (embeddings) generated in the\nhidden layers of the corresponding neural network. First we propose a\nclustering-based algorithm for measuring distributional shifts in text data by\nexploiting such embeddings. Then we study the effectiveness of our approach\nwhen applied to text embeddings generated by both LLMs and classical embedding\nalgorithms. Our experiments show that general-purpose LLM-based embeddings\nprovide a high sensitivity to data drift compared to other embedding methods.\nWe propose drift sensitivity as an important evaluation metric to consider when\ncomparing language models. Finally, we present insights and lessons learned\nfrom deploying our framework as part of the Fiddler ML Monitoring platform over\na period of 18 months.", "published": "2023-12-04 20:46:48", "link": "http://arxiv.org/abs/2312.02337v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "New Evaluation Metrics Capture Quality Degradation due to LLM\n  Watermarking", "abstract": "With the increasing use of large-language models (LLMs) like ChatGPT,\nwatermarking has emerged as a promising approach for tracing machine-generated\ncontent. However, research on LLM watermarking often relies on simple\nperplexity or diversity-based measures to assess the quality of watermarked\ntext, which can mask important limitations in watermarking. Here we introduce\ntwo new easy-to-use methods for evaluating watermarking algorithms for LLMs: 1)\nevaluation by LLM-judger with specific guidelines; and 2) binary classification\non text embeddings to distinguish between watermarked and unwatermarked text.\nWe apply these methods to characterize the effectiveness of current\nwatermarking techniques. Our experiments, conducted across various datasets,\nreveal that current watermarking methods are detectable by even simple\nclassifiers, challenging the notion of watermarking subtlety. We also found,\nthrough the LLM judger, that watermarking impacts text quality, especially in\ndegrading the coherence and depth of the response. Our findings underscore the\ntrade-off between watermark robustness and text quality and highlight the\nimportance of having more informative metrics to assess watermarking quality.", "published": "2023-12-04 22:56:31", "link": "http://arxiv.org/abs/2312.02382v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context\n  Learning", "abstract": "The alignment tuning process of large language models (LLMs) typically\ninvolves instruction learning through supervised fine-tuning (SFT) and\npreference tuning via reinforcement learning from human feedback (RLHF). A\nrecent study, LIMA (Zhou et al. 2023), shows that using merely 1K examples for\nSFT can achieve significant alignment performance as well, suggesting that the\neffect of alignment tuning might be \"superficial.\" This raises questions about\nhow exactly the alignment tuning transforms a base LLM.\n  We analyze the effect of alignment tuning by examining the token distribution\nshift between base LLMs and their aligned counterpart. Our findings reveal that\nbase LLMs and their alignment-tuned versions perform nearly identically in\ndecoding on the majority of token positions. Most distribution shifts occur\nwith stylistic tokens. These direct evidence strongly supports the Superficial\nAlignment Hypothesis suggested by LIMA.\n  Based on these findings, we rethink the alignment of LLMs by posing the\nresearch question: how effectively can we align base LLMs without SFT or RLHF?\nTo address this, we introduce a simple, tuning-free alignment method, URIAL.\nURIAL achieves effective alignment purely through in-context learning (ICL)\nwith base LLMs, requiring as few as three constant stylistic examples and a\nsystem prompt. We conduct a fine-grained and interpretable evaluation on a\ndiverse set of examples, named JUST-EVAL-INSTRUCT. Results demonstrate that\nbase LLMs with URIAL can match or even surpass the performance of LLMs aligned\nwith SFT or SFT+RLHF. We show that the gap between tuning-free and tuning-based\nalignment methods can be significantly reduced through strategic prompting and\nICL. Our findings on the superficial nature of alignment tuning and results\nwith URIAL suggest that deeper analysis and theoretical understanding of\nalignment is crucial to future LLM research.", "published": "2023-12-04 00:46:11", "link": "http://arxiv.org/abs/2312.01552v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Challenging Multimodal Video Summary: Simultaneously Extracting and\n  Generating Keyframe-Caption Pairs from Video", "abstract": "This paper proposes a practical multimodal video summarization task setting\nand a dataset to train and evaluate the task. The target task involves\nsummarizing a given video into a predefined number of keyframe-caption pairs\nand displaying them in a listable format to grasp the video content quickly.\nThis task aims to extract crucial scenes from the video in the form of images\n(keyframes) and generate corresponding captions explaining each keyframe's\nsituation. This task is useful as a practical application and presents a highly\nchallenging problem worthy of study. Specifically, achieving simultaneous\noptimization of the keyframe selection performance and caption quality\nnecessitates careful consideration of the mutual dependence on both preceding\nand subsequent keyframes and captions. To facilitate subsequent research in\nthis field, we also construct a dataset by expanding upon existing datasets and\npropose an evaluation framework. Furthermore, we develop two baseline systems\nand report their respective performance.", "published": "2023-12-04 02:17:14", "link": "http://arxiv.org/abs/2312.01575v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "ChatGPT as a Math Questioner? Evaluating ChatGPT on Generating\n  Pre-university Math Questions", "abstract": "Mathematical questioning is crucial for assessing students problem-solving\nskills. Since manually creating such questions requires substantial effort,\nautomatic methods have been explored. Existing state-of-the-art models rely on\nfine-tuning strategies and struggle to generate questions that heavily involve\nmultiple steps of logical and arithmetic reasoning. Meanwhile, large language\nmodels(LLMs) such as ChatGPT have excelled in many NLP tasks involving logical\nand arithmetic reasoning. Nonetheless, their applications in generating\neducational questions are underutilized, especially in the field of\nmathematics. To bridge this gap, we take the first step to conduct an in-depth\nanalysis of ChatGPT in generating pre-university math questions. Our analysis\nis categorized into two main settings: context-aware and context-unaware. In\nthe context-aware setting, we evaluate ChatGPT on existing math\nquestion-answering benchmarks covering elementary, secondary, and ternary\nclasses. In the context-unaware setting, we evaluate ChatGPT in generating math\nquestions for each lesson from pre-university math curriculums that we crawl.\nOur crawling results in TopicMath, a comprehensive and novel collection of\npre-university math curriculums collected from 121 math topics and 428 lessons\nfrom elementary, secondary, and tertiary classes. Through this analysis, we aim\nto provide insight into the potential of ChatGPT as a math questioner.", "published": "2023-12-04 06:23:37", "link": "http://arxiv.org/abs/2312.01661v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "STADEE: STAtistics-based DEEp Detection of Machine Generated Text", "abstract": "We present STADEE, a \\textbf{STA}tistics-based \\textbf{DEE}p detection method\nto identify machine-generated text, addressing the limitations of current\nmethods that rely heavily on fine-tuning pre-trained language models (PLMs).\nSTADEE integrates key statistical text features with a deep classifier,\nfocusing on aspects like token probability and cumulative probability, crucial\nfor handling nucleus sampling. Tested across diverse datasets and scenarios\n(in-domain, out-of-domain, and in-the-wild), STADEE demonstrates superior\nperformance, achieving an 87.05% F1 score in-domain and outperforming both\ntraditional statistical methods and fine-tuned PLMs, especially in\nout-of-domain and in-the-wild settings, highlighting its effectiveness and\ngeneralizability.", "published": "2023-12-04 06:45:47", "link": "http://arxiv.org/abs/2312.01672v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Data Management For Training Large Language Models: A Survey", "abstract": "Data plays a fundamental role in training Large Language Models (LLMs).\nEfficient data management, particularly in formulating a well-suited training\ndataset, is significant for enhancing model performance and improving training\nefficiency during pretraining and supervised fine-tuning stages. Despite the\nconsiderable importance of data management, the underlying mechanism of current\nprominent practices are still unknown. Consequently, the exploration of data\nmanagement has attracted more and more attention among the research community.\nThis survey aims to provide a comprehensive overview of current research in\ndata management within both the pretraining and supervised fine-tuning stages\nof LLMs, covering various aspects of data management strategy design. Looking\ninto the future, we extrapolate existing challenges and outline promising\ndirections for development in this field. Therefore, this survey serves as a\nguiding resource for practitioners aspiring to construct powerful LLMs through\nefficient data management practices. The collection of the latest papers is\navailable at https://github.com/ZigeW/data_management_LLM.", "published": "2023-12-04 07:42:16", "link": "http://arxiv.org/abs/2312.01700v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mitigating Fine-Grained Hallucination by Fine-Tuning Large\n  Vision-Language Models with Caption Rewrites", "abstract": "Large language models (LLMs) have shown remarkable performance in natural\nlanguage processing (NLP) tasks. To comprehend and execute diverse human\ninstructions over image data, instruction-tuned large vision-language models\n(LVLMs) have been introduced. However, LVLMs may suffer from different types of\nobject hallucinations. Nevertheless, LVLMs are evaluated for coarse-grained\nobject hallucinations only (i.e., generated objects non-existent in the input\nimage). The fine-grained object attributes and behaviors non-existent in the\nimage may still be generated but not measured by the current evaluation\nmethods. In this paper, we thus focus on reducing fine-grained hallucinations\nof LVLMs. We propose \\textit{ReCaption}, a framework that consists of two\ncomponents: rewriting captions using ChatGPT and fine-tuning the\ninstruction-tuned LVLMs on the rewritten captions. We also propose a\nfine-grained probing-based evaluation method named \\textit{Fine-Grained Object\nHallucination Evaluation} (\\textit{FGHE}). Our experiment results demonstrate\nthat ReCaption effectively reduces fine-grained object hallucination for\ndifferent LVLM options and improves their text generation quality. The code can\nbe found at https://github.com/Anonymousanoy/FOHE.", "published": "2023-12-04 07:43:02", "link": "http://arxiv.org/abs/2312.01701v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Developing Linguistic Patterns to Mitigate Inherent Human Bias in\n  Offensive Language Detection", "abstract": "With the proliferation of social media, there has been a sharp increase in\noffensive content, particularly targeting vulnerable groups, exacerbating\nsocial problems such as hatred, racism, and sexism. Detecting offensive\nlanguage use is crucial to prevent offensive language from being widely shared\non social media. However, the accurate detection of irony, implication, and\nvarious forms of hate speech on social media remains a challenge. Natural\nlanguage-based deep learning models require extensive training with large,\ncomprehensive, and labeled datasets. Unfortunately, manually creating such\ndatasets is both costly and error-prone. Additionally, the presence of\nhuman-bias in offensive language datasets is a major concern for deep learning\nmodels. In this paper, we propose a linguistic data augmentation approach to\nreduce bias in labeling processes, which aims to mitigate the influence of\nhuman bias by leveraging the power of machines to improve the accuracy and\nfairness of labeling processes. This approach has the potential to improve\noffensive language classification tasks across multiple languages and reduce\nthe prevalence of offensive content on social media.", "published": "2023-12-04 10:20:36", "link": "http://arxiv.org/abs/2312.01787v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian\n  Perspective", "abstract": "This paper proposes an interpretation of RLAIF as Bayesian inference by\nintroducing distilled Self-Critique (dSC), which refines the outputs of a LLM\nthrough a Gibbs sampler that is later distilled into a fine-tuned model. Only\nrequiring synthetic data, dSC is exercised in experiments regarding safety,\nsentiment, and privacy control, showing it can be a viable and cheap\nalternative to align LLMs. Code released at\n\\url{https://github.com/vicgalle/distilled-self-critique}.", "published": "2023-12-04 15:16:12", "link": "http://arxiv.org/abs/2312.01957v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Know Your Audience: Do LLMs Adapt to Different Age and Education Levels?", "abstract": "Large language models (LLMs) offer a range of new possibilities, including\nadapting the text to different audiences and their reading needs. But how well\ndo they adapt? We evaluate the readability of answers generated by four\nstate-of-the-art LLMs (commercial and open-source) to science questions when\nprompted to target different age groups and education levels. To assess the\nadaptability of LLMs to diverse audiences, we compare the readability scores of\nthe generated responses against the recommended comprehension level of each age\nand education group. We find large variations in the readability of the answers\nby different LLMs. Our results suggest LLM answers need to be better adapted to\nthe intended audience demographics to be more comprehensible. They underline\nthe importance of enhancing the adaptability of LLMs in education settings to\ncater to diverse age and education levels. Overall, current LLMs have set\nreadability ranges and do not adapt well to different audiences, even when\nprompted. That limits their potential for educational purposes.", "published": "2023-12-04 17:19:53", "link": "http://arxiv.org/abs/2312.02065v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Competition-Level Problems are Effective LLM Evaluators", "abstract": "Large language models (LLMs) have demonstrated impressive reasoning\ncapabilities, yet there is ongoing debate about these abilities and the\npotential data contamination problem recently. This paper aims to evaluate the\nreasoning capacities of LLMs, specifically in solving recent competition-level\nprogramming problems in Codeforces, which are expert-crafted and unique,\nrequiring deep understanding and robust reasoning skills. We first provide a\ncomprehensive evaluation of GPT-4's peiceived zero-shot performance on this\ntask, considering various aspects such as problems' release time, difficulties,\nand types of errors encountered. Surprisingly, the peiceived performance of\nGPT-4 has experienced a cliff like decline in problems after September 2021\nconsistently across all the difficulties and types of problems, which shows the\npotential data contamination, as well as the challenges for any existing LLM to\nsolve unseen complex reasoning problems. We further explore various approaches\nsuch as fine-tuning, Chain-of-Thought prompting and problem description\nsimplification, unfortunately none of them is able to consistently mitigate the\nchallenges. Through our work, we emphasis the importance of this excellent data\nsource for assessing the genuine reasoning capabilities of LLMs, and foster the\ndevelopment of LLMs with stronger reasoning abilities and better generalization\nin the future.", "published": "2023-12-04 18:58:57", "link": "http://arxiv.org/abs/2312.02143v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Multimodal Sentiment Analysis: Supervised Angular Margin-based\n  Contrastive Learning for Enhanced Fusion Representation", "abstract": "The effectiveness of a model is heavily reliant on the quality of the fusion\nrepresentation of multiple modalities in multimodal sentiment analysis.\nMoreover, each modality is extracted from raw input and integrated with the\nrest to construct a multimodal representation. Although previous methods have\nproposed multimodal representations and achieved promising results, most of\nthem focus on forming positive and negative pairs, neglecting the variation in\nsentiment scores within the same class. Additionally, they fail to capture the\nsignificance of unimodal representations in the fusion vector. To address these\nlimitations, we introduce a framework called Supervised Angular-based\nContrastive Learning for Multimodal Sentiment Analysis. This framework aims to\nenhance discrimination and generalizability of the multimodal representation\nand overcome biases in the fusion vector's modality. Our experimental results,\nalong with visualizations on two widely used datasets, demonstrate the\neffectiveness of our approach.", "published": "2023-12-04 02:58:19", "link": "http://arxiv.org/abs/2312.02227v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Recursive Visual Programming", "abstract": "Visual Programming (VP) has emerged as a powerful framework for Visual\nQuestion Answering (VQA). By generating and executing bespoke code for each\nquestion, these methods demonstrate impressive compositional and reasoning\ncapabilities, especially in few-shot and zero-shot scenarios. However, existing\nVP methods generate all code in a single function, resulting in code that is\nsuboptimal in terms of both accuracy and interpretability. Inspired by human\ncoding practices, we propose Recursive Visual Programming (RVP), which\nsimplifies generated routines, provides more efficient problem solving, and can\nmanage more complex data structures. RVP is inspired by human coding practices\nand approaches VQA tasks with an iterative recursive code generation approach,\nallowing decomposition of complicated problems into smaller parts. Notably, RVP\nis capable of dynamic type assignment, i.e., as the system recursively\ngenerates a new piece of code, it autonomously determines the appropriate\nreturn type and crafts the requisite code to generate that output. We show\nRVP's efficacy through extensive experiments on benchmarks including VSR, COVR,\nGQA, and NextQA, underscoring the value of adopting human-like recursive and\nmodular programming techniques for solving VQA tasks through coding.", "published": "2023-12-04 17:27:24", "link": "http://arxiv.org/abs/2312.02249v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Fine-tuning pre-trained extractive QA models for clinical document\n  parsing", "abstract": "Electronic health records (EHRs) contain a vast amount of high-dimensional\nmulti-modal data that can accurately represent a patient's medical history.\nUnfortunately, most of this data is either unstructured or semi-structured,\nrendering it unsuitable for real-time and retrospective analyses. A remote\npatient monitoring (RPM) program for Heart Failure (HF) patients needs to have\naccess to clinical markers like EF (Ejection Fraction) or LVEF (Left\nVentricular Ejection Fraction) in order to ascertain eligibility and\nappropriateness for the program. This paper explains a system that can parse\nechocardiogram reports and verify EF values. This system helps identify\neligible HF patients who can be enrolled in such a program. At the heart of\nthis system is a pre-trained extractive QA transformer model that is fine-tuned\non custom-labeled data. The methods used to prepare such a model for deployment\nare illustrated by running experiments on a public clinical dataset like\nMIMIC-IV-Note. The pipeline can be used to generalize solutions to similar\nproblems in a low-resource setting. We found that the system saved over 1500\nhours for our clinicians over 12 months by automating the task at scale.", "published": "2023-12-04 19:52:56", "link": "http://arxiv.org/abs/2312.02314v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GNN2R: Weakly-Supervised Rationale-Providing Question Answering over\n  Knowledge Graphs", "abstract": "Most current methods for multi-hop question answering (QA) over knowledge\ngraphs (KGs) only provide final conclusive answers without explanations, such\nas a set of KG entities that is difficult for normal users to review and\ncomprehend. This issue severely limits the application of KG-based QA in\nreal-world scenarios. However, it is non-trivial to solve due to two\nchallenges: First, annotations of reasoning chains of multi-hop questions,\nwhich could serve as supervision for explanation generation, are usually\nlacking. Second, it is difficult to maintain high efficiency when explicit KG\ntriples need to be retrieved to generate explanations. In this paper, we\npropose a novel Graph Neural Network-based Two-Step Reasoning model (GNN2R) to\nsolve this issue. GNN2R can provide both final answers and reasoning subgraphs\nas a rationale behind final answers efficiently with only weak supervision that\nis available through question-final answer pairs. We extensively evaluated\nGNN2R with detailed analyses in experiments. The results demonstrate that, in\nterms of effectiveness, efficiency, and quality of generated explanations,\nGNN2R outperforms existing state-of-the-art methods that are applicable to this\ntask. Our code and pre-trained models are available at\nhttps://github.com/ruijie-wang-uzh/GNN2R.", "published": "2023-12-04 19:58:07", "link": "http://arxiv.org/abs/2312.02317v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Revisiting Topic-Guided Language Models", "abstract": "A recent line of work in natural language processing has aimed to combine\nlanguage models and topic models. These topic-guided language models augment\nneural language models with topic models, unsupervised learning methods that\ncan discover document-level patterns of word use. This paper compares the\neffectiveness of these methods in a standardized setting. We study four\ntopic-guided language models and two baselines, evaluating the held-out\npredictive performance of each model on four corpora. Surprisingly, we find\nthat none of these methods outperform a standard LSTM language model baseline,\nand most fail to learn good topics. Further, we train a probe of the neural\nlanguage model that shows that the baseline's hidden states already encode\ntopic information. We make public all code used for this study.", "published": "2023-12-04 20:33:24", "link": "http://arxiv.org/abs/2312.02331v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Evaluation Framework for Mapping News Headlines to Event Classes in a\n  Knowledge Graph", "abstract": "Mapping ongoing news headlines to event-related classes in a rich knowledge\nbase can be an important component in a knowledge-based event analysis and\nforecasting solution. In this paper, we present a methodology for creating a\nbenchmark dataset of news headlines mapped to event classes in Wikidata, and\nresources for the evaluation of methods that perform the mapping. We use the\ndataset to study two classes of unsupervised methods for this task: 1)\nadaptations of classic entity linking methods, and 2) methods that treat the\nproblem as a zero-shot text classification problem. For the first approach, we\nevaluate off-the-shelf entity linking systems. For the second approach, we\nexplore a) pre-trained natural language inference (NLI) models, and b)\npre-trained large generative language models. We present the results of our\nevaluation, lessons learned, and directions for future work. The dataset and\nscripts for evaluation are made publicly available.", "published": "2023-12-04 20:42:26", "link": "http://arxiv.org/abs/2312.02334v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Stock Movement and Volatility Prediction from Tweets, Macroeconomic\n  Factors and Historical Prices", "abstract": "Predicting stock market is vital for investors and policymakers, acting as a\nbarometer of the economic health. We leverage social media data, a potent\nsource of public sentiment, in tandem with macroeconomic indicators as\ngovernment-compiled statistics, to refine stock market predictions. However,\nprior research using tweet data for stock market prediction faces three\nchallenges. First, the quality of tweets varies widely. While many are filled\nwith noise and irrelevant details, only a few genuinely mirror the actual\nmarket scenario. Second, solely focusing on the historical data of a particular\nstock without considering its sector can lead to oversight. Stocks within the\nsame industry often exhibit correlated price behaviors. Lastly, simply\nforecasting the direction of price movement without assessing its magnitude is\nof limited value, as the extent of the rise or fall truly determines\nprofitability. In this paper, diverging from the conventional methods, we\npioneer an ECON. The framework has following advantages: First, ECON has an\nadept tweets filter that efficiently extracts and decodes the vast array of\ntweet data. Second, ECON discerns multi-level relationships among stocks,\nsectors, and macroeconomic factors through a self-aware mechanism in semantic\nspace. Third, ECON offers enhanced accuracy in predicting substantial stock\nprice fluctuations by capitalizing on stock price movement. We showcase the\nstate-of-the-art performance of our proposed model using a dataset,\nspecifically curated by us, for predicting stock market movements and\nvolatility.", "published": "2023-12-04 22:27:43", "link": "http://arxiv.org/abs/2312.03758v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "APoLLo: Unified Adapter and Prompt Learning for Vision Language Models", "abstract": "The choice of input text prompt plays a critical role in the performance of\nVision-Language Pretrained (VLP) models such as CLIP. We present APoLLo, a\nunified multi-modal approach that combines Adapter and Prompt learning for\nVision-Language models. Our method is designed to substantially improve the\ngeneralization capabilities of VLP models when they are fine-tuned in a\nfew-shot setting. We introduce trainable cross-attention-based adapter layers\nin conjunction with vision and language encoders to strengthen the alignment\nbetween the two modalities. We enforce consistency between the respective\nencoder branches (receiving augmented inputs) to prevent overfitting in\ndownstream tasks. Our method is evaluated on three representative tasks:\ngeneralization to novel classes, cross-dataset evaluation, and unseen domain\nshifts. In practice, APoLLo achieves a relative gain up to 6.03% over MaPLe\n(SOTA) on novel classes for 10 diverse image recognition datasets.", "published": "2023-12-04 01:42:09", "link": "http://arxiv.org/abs/2312.01564v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Characterizing Large Language Model Geometry Helps Solve Toxicity\n  Detection and Generation", "abstract": "Large Language Models (LLMs) drive current AI breakthroughs despite very\nlittle being known about their internal representations. In this work, we\npropose to shed the light on LLMs inner mechanisms through the lens of\ngeometry. In particular, we develop in closed form $(i)$ the intrinsic\ndimension in which the Multi-Head Attention embeddings are constrained to exist\nand $(ii)$ the partition and per-region affine mappings of the feedforward\n(MLP) network of LLMs' layers. Our theoretical findings further enable the\ndesign of novel principled solutions applicable to state-of-the-art LLMs.\nFirst, we show that, through our geometric understanding, we can bypass LLMs'\nRLHF protection by controlling the embedding's intrinsic dimension through\ninformed prompt manipulation. Second, we derive interpretable geometrical\nfeatures that can be extracted from any (pre-trained) LLM, providing a rich\nabstract representation of their inputs. We observe that these features are\nsufficient to help solve toxicity detection, and even allow the identification\nof various types of toxicity. Our results demonstrate how, even in large-scale\nregimes, exact theoretical results can answer practical questions in LLMs.\nCode: https://github.com/RandallBalestriero/SplineLLM", "published": "2023-12-04 06:01:32", "link": "http://arxiv.org/abs/2312.01648v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Jellyfish: A Large Language Model for Data Preprocessing", "abstract": "This paper explores the utilization of LLMs for data preprocessing (DP), a\ncrucial step in the data mining pipeline that transforms raw data into a clean\nformat conducive to easy processing. Whereas the use of LLMs has sparked\ninterest in devising universal solutions to DP, recent initiatives in this\ndomain typically rely on GPT APIs, raising inevitable data breach concerns.\nUnlike these approaches, we consider instruction-tuning local LLMs (7 -- 13B\nmodels) as universal DP task solvers that operate on a local, single, and\nlow-priced GPU, ensuring data security and enabling further customization. We\nselect a collection of datasets across four representative DP tasks and\nconstruct instruction tuning data using data configuration, knowledge\ninjection, and reasoning data distillation techniques tailored to DP. By tuning\nMistral-7B, Llama 3-8B, and OpenOrca-Platypus2-13B, our models, namely,\nJellyfish-7B/8B/13B, deliver competitiveness compared to GPT-3.5/4 models and\nstrong generalizability to unseen tasks while barely compromising the base\nmodels' abilities in NLP tasks. Meanwhile, Jellyfish offers enhanced reasoning\ncapabilities compared to GPT-3.5.\n  Our models are available at: https://huggingface.co/NECOUDBFM/Jellyfish .\n  Our instruction dataset is available at:\nhttps://huggingface.co/datasets/NECOUDBFM/Jellyfish-Instruct .", "published": "2023-12-04 07:01:54", "link": "http://arxiv.org/abs/2312.01678v6", "categories": ["cs.AI", "cs.CL", "cs.DB", "cs.LG"], "primary_category": "cs.AI"}
{"title": "A Machine Learning Approach Towards SKILL Code Autocompletion", "abstract": "As Moore's Law continues to increase the complexity of electronic systems,\nElectronic Design Automation (EDA) must advance to meet global demand. An\nimportant example of an EDA technology is SKILL, a scripting language used to\ncustomize and extend EDA software. Recently, code generation models using the\ntransformer architecture have achieved impressive results in academic settings\nand have even been used in commercial developer tools to improve developer\nproductivity. To the best of our knowledge, this study is the first to apply\ntransformers to SKILL code autocompletion towards improving the productivity of\nhardware design engineers. In this study, a novel, data-efficient methodology\nfor generating SKILL code is proposed and experimentally validated. More\nspecifically, we propose a novel methodology for (i) creating a high-quality\nSKILL dataset with both unlabeled and labeled data, (ii) a training strategy\nwhere T5 models pre-trained on general programming language code are fine-tuned\non our custom SKILL dataset using unsupervised and supervised learning, and\n(iii) evaluating synthesized SKILL code. We show that models trained using the\nproposed methodology outperform baselines in terms of human-judgment score and\nBLEU score. A major challenge faced was the extremely small amount of available\nSKILL code data that can be used to train a transformer model to generate SKILL\ncode. Despite our validated improvements, the extremely small dataset available\nto us was still not enough to train a model that can reliably autocomplete\nSKILL code. We discuss this and other limitations as well as future work that\ncould address these limitations.", "published": "2023-12-04 14:29:28", "link": "http://arxiv.org/abs/2312.01921v2", "categories": ["cs.SE", "cs.CL", "cs.PL", "I.2.2"], "primary_category": "cs.SE"}
{"title": "TimeChat: A Time-sensitive Multimodal Large Language Model for Long\n  Video Understanding", "abstract": "This work proposes TimeChat, a time-sensitive multimodal large language model\nspecifically designed for long video understanding. Our model incorporates two\nkey architectural contributions: (1) a timestamp-aware frame encoder that binds\nvisual content with the timestamp of each frame, and (2) a sliding video\nQ-Former that produces a video token sequence of varying lengths to accommodate\nvideos of various durations. Additionally, we construct an instruction-tuning\ndataset, encompassing 6 tasks and a total of 125K instances, to further enhance\nTimeChat's instruction-following performance. Experiment results across various\nvideo understanding tasks, such as dense captioning, temporal grounding, and\nhighlight detection, demonstrate TimeChat's strong zero-shot temporal\nlocalization and reasoning capabilities. For example, it achieves +9.2 F1 score\nand +2.8 CIDEr on YouCook2, +5.8 HIT@1 on QVHighlights, and +27.5 R@1 (IoU=0.5)\non Charades-STA, compared to state-of-the-art video large language models,\nholding the potential to serve as a versatile video assistant for long-form\nvideo comprehension tasks and satisfy realistic user requirements.", "published": "2023-12-04 17:09:52", "link": "http://arxiv.org/abs/2312.02051v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A Glitch in the Matrix? Locating and Detecting Language Model Grounding\n  with Fakepedia", "abstract": "Large language models (LLMs) have an impressive ability to draw on novel\ninformation supplied in their context. Yet the mechanisms underlying this\ncontextual grounding remain unknown, especially in situations where contextual\ninformation contradicts factual knowledge stored in the parameters, which LLMs\nalso excel at recalling. Favoring the contextual information is critical for\nretrieval-augmented generation methods, which enrich the context with\nup-to-date information, hoping that grounding can rectify outdated or noisy\nstored knowledge. We present a novel method to study grounding abilities using\nFakepedia, a novel dataset of counterfactual texts constructed to clash with a\nmodel's internal parametric knowledge. In this study, we introduce Fakepedia, a\ncounterfactual dataset designed to evaluate grounding abilities when the\ninternal parametric knowledge clashes with the contextual information. We\nbenchmark various LLMs with Fakepedia and conduct a causal mediation analysis\nof LLM components when answering Fakepedia queries, based on our Masked Grouped\nCausal Tracing (MGCT) method. Through this analysis, we identify distinct\ncomputational patterns between grounded and ungrounded responses. We finally\ndemonstrate that distinguishing grounded from ungrounded responses is\nachievable through computational analysis alone. Our results, together with\nexisting findings about factual recall mechanisms, provide a coherent narrative\nof how grounding and factual recall mechanisms interact within LLMs.", "published": "2023-12-04 17:35:42", "link": "http://arxiv.org/abs/2312.02073v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "When it Rains, it Pours: Modeling Media Storms and the News Ecosystem", "abstract": "Most events in the world receive at most brief coverage by the news media.\nOccasionally, however, an event will trigger a media storm, with voluminous and\nwidespread coverage lasting for weeks instead of days. In this work, we develop\nand apply a pairwise article similarity model, allowing us to identify story\nclusters in corpora covering local and national online news, and thereby create\na comprehensive corpus of media storms over a nearly two year period. Using\nthis corpus, we investigate media storms at a new level of granularity,\nallowing us to validate claims about storm evolution and topical distribution,\nand provide empirical support for previously hypothesized patterns of influence\nof storms on media coverage and intermedia agenda setting.", "published": "2023-12-04 18:49:06", "link": "http://arxiv.org/abs/2312.02118v1", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Magicoder: Empowering Code Generation with OSS-Instruct", "abstract": "We introduce Magicoder, a series of fully open-source (code, weights, and\ndata) Large Language Models (LLMs) for code that significantly closes the gap\nwith top code models while having no more than 7B parameters. Magicoder models\nare trained on 75K synthetic instruction data using OSS-Instruct, a novel\napproach to enlightening LLMs with open-source code snippets to generate\ndiverse instruction data for code. Our main motivation is to mitigate the\ninherent bias of the synthetic data generated by LLMs through the wealth of\nopen-source references for the production of more realistic and controllable\ndata. The orthogonality of OSS-Instruct and other data generation methods like\nEvol-Instruct further enables us to build an enhanced MagicoderS. Both\nMagicoder and MagicoderS substantially outperform state-of-the-art code models\nwith similar or even larger sizes on a wide range of coding benchmarks.\nNotably, MagicoderS-CL-7B based on CodeLlama even surpasses the prominent\nChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1 ). Overall, OSS-Instruct opens a\nnew direction for crafting diverse synthetic instruction data for code using\nabundant open-source references.", "published": "2023-12-04 18:50:35", "link": "http://arxiv.org/abs/2312.02120v2", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "TPPoet: Transformer-Based Persian Poem Generation using Minimal Data and\n  Advanced Decoding Techniques", "abstract": "Recent advances in language models (LMs), have demonstrated significant\nefficacy in tasks related to the arts and humanities. While LMs have exhibited\nexceptional performance across a wide range of natural language processing\ntasks, there are notable challenges associated with their utilization on small\ndatasets and their ability to replicate more creative human capacities. In this\nstudy, we aim to address these challenges by training a Persian classical\npoetry generation model using a transformer architecture on a specialized\ndataset with no pretraining. Additionally, we propose a novel decoding method\nto enhance coherence and meaningfulness in the generated poetry, effectively\nmanaging the tradeoff between diversity and quality. Furthermore, the results\nof our training approach and the proposed decoding method are evaluated through\ncomprehensive set of automatic and human evaluations and showed its superior\ncapability to generate coherent and meaningful poetry in compare to other\ndecoding methods and an existing Persian large language model (LLM).", "published": "2023-12-04 18:52:26", "link": "http://arxiv.org/abs/2312.02125v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generative Powers of Ten", "abstract": "We present a method that uses a text-to-image model to generate consistent\ncontent across multiple image scales, enabling extreme semantic zooms into a\nscene, e.g., ranging from a wide-angle landscape view of a forest to a macro\nshot of an insect sitting on one of the tree branches. We achieve this through\na joint multi-scale diffusion sampling approach that encourages consistency\nacross different scales while preserving the integrity of each individual\nsampling process. Since each generated scale is guided by a different text\nprompt, our method enables deeper levels of zoom than traditional\nsuper-resolution methods that may struggle to create new contextual structure\nat vastly different scales. We compare our method qualitatively with\nalternative techniques in image super-resolution and outpainting, and show that\nour method is most effective at generating consistent multi-scale content.", "published": "2023-12-04 18:59:25", "link": "http://arxiv.org/abs/2312.02149v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.GR"], "primary_category": "cs.CV"}
{"title": "Fine-Tuning Language Models for Context-Specific SQL Query Generation", "abstract": "The ability to generate SQL queries from natural language has significant\nimplications for making data accessible to non-specialists. This paper presents\na novel approach to fine-tuning open-source large language models (LLMs) for\nthe task of transforming natural language into SQL queries within the retail\ndomain. We introduce models specialized in generating SQL queries, trained on\nsynthetic datasets tailored to the Snowflake SQL and GoogleSQL dialects. Our\nmethodology involves generating a context-specific dataset using GPT-4, then\nfine-tuning three open-source LLMs(Starcoder Plus, Code-Llama, and Mistral)\nemploying the LoRa technique to optimize for resource constraints. The\nfine-tuned models demonstrate superior performance in zero-shot settings\ncompared to the baseline GPT-4, with Code-Llama achieving the highest accuracy\nrates, at 81.58% for Snowflake SQL and 82.66% for GoogleSQL. These results\nunderscore the effectiveness of fine-tuning LLMs on domain-specific tasks and\nsuggest a promising direction for enhancing the accessibility of relational\ndatabases through natural language interfaces.", "published": "2023-12-04 18:04:27", "link": "http://arxiv.org/abs/2312.02251v1", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.DB"}
{"title": "LLMs Accelerate Annotation for Medical Information Extraction", "abstract": "The unstructured nature of clinical notes within electronic health records\noften conceals vital patient-related information, making it challenging to\naccess or interpret. To uncover this hidden information, specialized Natural\nLanguage Processing (NLP) models are required. However, training these models\nnecessitates large amounts of labeled data, a process that is both\ntime-consuming and costly when relying solely on human experts for annotation.\nIn this paper, we propose an approach that combines Large Language Models\n(LLMs) with human expertise to create an efficient method for generating ground\ntruth labels for medical text annotation. By utilizing LLMs in conjunction with\nhuman annotators, we significantly reduce the human annotation burden, enabling\nthe rapid creation of labeled datasets. We rigorously evaluate our method on a\nmedical information extraction task, demonstrating that our approach not only\nsubstantially cuts down on human intervention but also maintains high accuracy.\nThe results highlight the potential of using LLMs to improve the utilization of\nunstructured clinical data, allowing for the swift deployment of tailored NLP\nsolutions in healthcare.", "published": "2023-12-04 19:26:13", "link": "http://arxiv.org/abs/2312.02296v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "VaQuitA: Enhancing Alignment in LLM-Assisted Video Understanding", "abstract": "Recent advancements in language-model-based video understanding have been\nprogressing at a remarkable pace, spurred by the introduction of Large Language\nModels (LLMs). However, the focus of prior research has been predominantly on\ndevising a projection layer that maps video features to tokens, an approach\nthat is both rudimentary and inefficient. In our study, we introduce a\ncutting-edge framework, VaQuitA, designed to refine the synergy between video\nand textual information. At the data level, instead of sampling frames\nuniformly, we implement a sampling method guided by CLIP-score rankings, which\nenables a more aligned selection of frames with the given question. At the\nfeature level, we integrate a trainable Video Perceiver alongside a\nVisual-Query Transformer (abbreviated as VQ-Former), which bolsters the\ninterplay between the input question and the video features. We also discover\nthat incorporating a simple prompt, \"Please be critical\", into the LLM input\ncan substantially enhance its video comprehension capabilities. Our\nexperimental results indicate that VaQuitA consistently sets a new benchmark\nfor zero-shot video question-answering tasks and is adept at producing\nhigh-quality, multi-turn video dialogues with users.", "published": "2023-12-04 19:48:02", "link": "http://arxiv.org/abs/2312.02310v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Explore, Select, Derive, and Recall: Augmenting LLM with Human-like\n  Memory for Mobile Task Automation", "abstract": "The advent of large language models (LLMs) has opened up new opportunities in\nthe field of mobile task automation. Their superior language understanding and\nreasoning capabilities allow users to automate complex and repetitive tasks.\nHowever, due to the inherent unreliability and high operational cost of LLMs,\ntheir practical applicability is quite limited. To address these issues, this\npaper introduces MobileGPT, an innovative LLM-based mobile task automator\nequipped with a human-like app memory. MobileGPT emulates the cognitive process\nof humans interacting with a mobile app -- explore, select, derive, and recall.\nThis approach allows for a more precise and efficient learning of a task's\nprocedure by breaking it down into smaller, modular sub-tasks that can be\nre-used, re-arranged, and adapted for various objectives. We implement\nMobileGPT using online LLMs services (GPT-3.5 and GPT-4) and evaluate its\nperformance on a dataset of 185 tasks across 18 mobile apps. The results\nindicate that MobileGPT can automate and learn new tasks with 82.7% accuracy,\nand is able to adapt them to different contexts with near perfect (98.75%)\naccuracy while reducing both latency and cost by 62.5% and 68.8%, respectively,\ncompared to the GPT-4 powered baseline.", "published": "2023-12-04 06:13:35", "link": "http://arxiv.org/abs/2312.03003v3", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Near-real-time Earthquake-induced Fatality Estimation using Crowdsourced\n  Data and Large-Language Models", "abstract": "When a damaging earthquake occurs, immediate information about casualties is\ncritical for time-sensitive decision-making by emergency response and aid\nagencies in the first hours and days. Systems such as Prompt Assessment of\nGlobal Earthquakes for Response (PAGER) by the U.S. Geological Survey (USGS)\nwere developed to provide a forecast within about 30 minutes of any significant\nearthquake globally. Traditional systems for estimating human loss in disasters\noften depend on manually collected early casualty reports from global media, a\nprocess that's labor-intensive and slow with notable time delays. Recently,\nsome systems have employed keyword matching and topic modeling to extract\nrelevant information from social media. However, these methods struggle with\nthe complex semantics in multilingual texts and the challenge of interpreting\never-changing, often conflicting reports of death and injury numbers from\nvarious unverified sources on social media platforms. In this work, we\nintroduce an end-to-end framework to significantly improve the timeliness and\naccuracy of global earthquake-induced human loss forecasting using\nmulti-lingual, crowdsourced social media. Our framework integrates (1) a\nhierarchical casualty extraction model built upon large language models, prompt\ndesign, and few-shot learning to retrieve quantitative human loss claims from\nsocial media, (2) a physical constraint-aware, dynamic-truth discovery model\nthat discovers the truthful human loss from massive noisy and potentially\nconflicting human loss claims, and (3) a Bayesian updating loss projection\nmodel that dynamically updates the final loss estimation using discovered\ntruths. We test the framework in real-time on a series of global earthquake\nevents in 2021 and 2022 and show that our framework streamlines casualty data\nretrieval, achieving speed and accuracy comparable to manual methods by USGS.", "published": "2023-12-04 17:09:58", "link": "http://arxiv.org/abs/2312.03755v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LineConGraphs: Line Conversation Graphs for Effective Emotion\n  Recognition using Graph Neural Networks", "abstract": "Emotion Recognition in Conversations (ERC) is a critical aspect of affective\ncomputing, and it has many practical applications in healthcare, education,\nchatbots, and social media platforms. Earlier approaches for ERC analysis\ninvolved modeling both speaker and long-term contextual information using graph\nneural network architectures. However, it is ideal to deploy\nspeaker-independent models for real-world applications. Additionally, long\ncontext windows can potentially create confusion in recognizing the emotion of\nan utterance in a conversation. To overcome these limitations, we propose novel\nline conversation graph convolutional network (LineConGCN) and graph attention\n(LineConGAT) models for ERC analysis. These models are speaker-independent and\nbuilt using a graph construction strategy for conversations -- line\nconversation graphs (LineConGraphs). The conversational context in\nLineConGraphs is short-term -- limited to one previous and future utterance,\nand speaker information is not part of the graph. We evaluate the performance\nof our proposed models on two benchmark datasets, IEMOCAP and MELD, and show\nthat our LineConGAT model outperforms the state-of-the-art methods with an\nF1-score of 64.58% and 76.50%. Moreover, we demonstrate that embedding\nsentiment shift information into line conversation graphs further enhances the\nERC performance in the case of GCN models.", "published": "2023-12-04 19:36:58", "link": "http://arxiv.org/abs/2312.03756v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Intelligent Virtual Assistants with LLM-based Process Automation", "abstract": "While intelligent virtual assistants like Siri, Alexa, and Google Assistant\nhave become ubiquitous in modern life, they still face limitations in their\nability to follow multi-step instructions and accomplish complex goals\narticulated in natural language. However, recent breakthroughs in large\nlanguage models (LLMs) show promise for overcoming existing barriers by\nenhancing natural language processing and reasoning capabilities. Though\npromising, applying LLMs to create more advanced virtual assistants still faces\nchallenges like ensuring robust performance and handling variability in\nreal-world user commands. This paper proposes a novel LLM-based virtual\nassistant that can automatically perform multi-step operations within mobile\napps based on high-level user requests. The system represents an advance in\nassistants by providing an end-to-end solution for parsing instructions,\nreasoning about goals, and executing actions. LLM-based Process Automation\n(LLMPA) has modules for decomposing instructions, generating descriptions,\ndetecting interface elements, predicting next actions, and error checking.\nExperiments demonstrate the system completing complex mobile operation tasks in\nAlipay based on natural language instructions. This showcases how large\nlanguage models can enable automated assistants to accomplish real-world tasks.\nThe main contributions are the novel LLMPA architecture optimized for app\nprocess automation, the methodology for applying LLMs to mobile apps, and\ndemonstrations of multi-step task completion in a real-world environment.\nNotably, this work represents the first real-world deployment and extensive\nevaluation of a large language model-based virtual assistant in a widely used\nmobile application with an enormous user base numbering in the hundreds of\nmillions.", "published": "2023-12-04 07:51:58", "link": "http://arxiv.org/abs/2312.06677v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Tree of Attacks: Jailbreaking Black-Box LLMs Automatically", "abstract": "While Large Language Models (LLMs) display versatile functionality, they\ncontinue to generate harmful, biased, and toxic content, as demonstrated by the\nprevalence of human-designed jailbreaks. In this work, we present Tree of\nAttacks with Pruning (TAP), an automated method for generating jailbreaks that\nonly requires black-box access to the target LLM. TAP utilizes an attacker LLM\nto iteratively refine candidate (attack) prompts until one of the refined\nprompts jailbreaks the target. In addition, before sending prompts to the\ntarget, TAP assesses them and prunes the ones unlikely to result in jailbreaks,\nreducing the number of queries sent to the target LLM. In empirical\nevaluations, we observe that TAP generates prompts that jailbreak\nstate-of-the-art LLMs (including GPT4-Turbo and GPT4o) for more than 80% of the\nprompts. This significantly improves upon the previous state-of-the-art\nblack-box methods for generating jailbreaks while using a smaller number of\nqueries than them. Furthermore, TAP is also capable of jailbreaking LLMs\nprotected by state-of-the-art guardrails, e.g., LlamaGuard.", "published": "2023-12-04 18:49:23", "link": "http://arxiv.org/abs/2312.02119v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "SEFGAN: Harvesting the Power of Normalizing Flows and GANs for Efficient\n  High-Quality Speech Enhancement", "abstract": "This paper proposes SEFGAN, a Deep Neural Network (DNN) combining maximum\nlikelihood training and Generative Adversarial Networks (GANs) for efficient\nspeech enhancement (SE). For this, a DNN is trained to synthesize the enhanced\nspeech conditioned on noisy speech using a Normalizing Flow (NF) as generator\nin a GAN framework. While the combination of likelihood models and GANs is not\ntrivial, SEFGAN demonstrates that a hybrid adversarial and maximum likelihood\ntraining approach enables the model to maintain high quality audio generation\nand log-likelihood estimation. Our experiments indicate that this approach\nstrongly outperforms the baseline NF-based model without introducing additional\ncomplexity to the enhancement network. A comparison using computational metrics\nand a listening experiment reveals that SEFGAN is competitive with other\nstate-of-the-art models.", "published": "2023-12-04 09:10:08", "link": "http://arxiv.org/abs/2312.01744v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A text-dependent speaker verification application framework based on\n  Chinese numerical string corpus", "abstract": "Researches indicate that text-dependent speaker verification (TD-SV) often\noutperforms text-independent verification (TI-SV) in short speech scenarios.\nHowever, collecting large-scale fixed text speech data is challenging, and as\nspeech length increases, factors like sentence rhythm and pauses affect TDSV's\nsensitivity to text sequence. Based on these factors, We propose the hypothesis\nthat strategies such as more fine-grained pooling methods on time scales and\ndecoupled representations of speech speaker embedding and text embedding are\nmore suitable for TD-SV. We have introduced an end-to-end TD-SV system based on\na dataset comprising longer Chinese numerical string texts. It contains a text\nembedding network, a speaker embedding network, and back-end fusion. First, we\nrecorded a dataset consisting of long Chinese numerical text named SHAL, which\nis publicly available on the Open-SLR website. We addressed the issue of\ndataset scarcity by augmenting it using Tacotron2 and HiFi-GAN. Next, we\nintroduced a dual representation of speech with text embedding and speaker\nembedding. In the text embedding network, we employed an enhanced Transformer\nand introduced a triple loss that includes text classification loss, CTC loss,\nand decoder loss. For the speaker embedding network, we enhanced a sliding\nwindow attentive statistics pooling (SWASP), combined with attentive statistics\npooling (ASP) to create a multi-scale pooling method. Finally, we fused text\nembedding and speaker embedding. Our pooling methods achieved an equal error\nrate (EER) performance improvement of 49.2% on Hi-Mia and 75.0% on SHAL,\nrespectively.", "published": "2023-12-04 05:52:59", "link": "http://arxiv.org/abs/2312.01645v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Head Orientation Estimation with Distributed Microphones Using Speech\n  Radiation Patterns", "abstract": "Determining the head orientation of a talker is not only beneficial for\nvarious speech signal processing applications, such as source localization or\nspeech enhancement, but also facilitates intuitive voice control and\ninteraction with smart environments or modern car assistants. Most approaches\nfor head orientation estimation are based on visual cues. However, this\nrequires camera systems which often are not available. We present an approach\nwhich purely uses audio signals captured with only a few distributed\nmicrophones around the talker. Specifically, we propose a novel method that\ndirectly incorporates measured or modeled speech radiation patterns to infer\nthe talker's orientation during active speech periods based on a cosine\nsimilarity measure. Moreover, an automatic gain adjustment technique is\nproposed for uncalibrated, irregular microphone setups, such as ad-hoc sensor\nnetworks. In experiments with signals recorded in both anechoic and reverberant\nenvironments, the proposed method outperforms state-of-the-art approaches,\nusing either measured or modeled speech radiation patterns.", "published": "2023-12-04 11:14:55", "link": "http://arxiv.org/abs/2312.01808v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Building Ears for Robots: Machine Hearing in the Age of Autonomy", "abstract": "This study explores the significance of robot hearing systems, emphasizing\ntheir importance for robots operating in diverse and uncertain environments. It\nintroduces the hardware design principles using robotaxis as an example, where\nexterior microphone arrays are employed to detect sound events such as sirens.\nThe challenges, goals, and test methods are discussed, focusing on achieving a\nsuitable signal-to-noise ratio (SNR). Additionally, it presents a preliminary\nsoftware framework rooted in probabilistic robotics theory, advocating for the\nintegration of robot hearing into the broader context of perception and\ndecision-making. It discusses various models, including Bayes filters,\npartially observable Markov decision processes (POMDP), and multiagent systems,\nhighlighting the multifaceted roles that robot hearing can play. In conclusion,\nas service robots continue to evolve, robot hearing research will expand,\noffering new perspectives and challenges for future development beyond simple\nsound event classification.", "published": "2023-12-04 00:52:18", "link": "http://arxiv.org/abs/2312.01554v2", "categories": ["cs.SD", "cs.RO", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multimodal Speech Emotion Recognition Using Modality-specific\n  Self-Supervised Frameworks", "abstract": "Emotion recognition is a topic of significant interest in assistive robotics\ndue to the need to equip robots with the ability to comprehend human behavior,\nfacilitating their effective interaction in our society. Consequently,\nefficient and dependable emotion recognition systems supporting optimal\nhuman-machine communication are required. Multi-modality (including speech,\naudio, text, images, and videos) is typically exploited in emotion recognition\ntasks. Much relevant research is based on merging multiple data modalities and\ntraining deep learning models utilizing low-level data representations.\nHowever, most existing emotion databases are not large (or complex) enough to\nallow machine learning approaches to learn detailed representations. This paper\nexplores modalityspecific pre-trained transformer frameworks for\nself-supervised learning of speech and text representations for data-efficient\nemotion recognition while achieving state-of-the-art performance in recognizing\nemotions. This model applies feature-level fusion using nonverbal cue data\npoints from motion capture to provide multimodal speech emotion recognition.\nThe model was trained using the publicly available IEMOCAP dataset, achieving\nan overall accuracy of 77.58% for four emotions, outperforming state-of-the-art\napproaches", "published": "2023-12-04 01:49:24", "link": "http://arxiv.org/abs/2312.01568v1", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Exploring the Viability of Synthetic Audio Data for Audio-Based Dialogue\n  State Tracking", "abstract": "Dialogue state tracking plays a crucial role in extracting information in\ntask-oriented dialogue systems. However, preceding research are limited to\ntextual modalities, primarily due to the shortage of authentic human audio\ndatasets. We address this by investigating synthetic audio data for audio-based\nDST. To this end, we develop cascading and end-to-end models, train them with\nour synthetic audio dataset, and test them on actual human speech data. To\nfacilitate evaluation tailored to audio modalities, we introduce a novel\nPhonemeF1 to capture pronunciation similarity. Experimental results showed that\nmodels trained solely on synthetic datasets can generalize their performance to\nhuman voice data. By eliminating the dependency on human speech data\ncollection, these insights pave the way for significant practical advancements\nin audio-based DST. Data and code are available at\nhttps://github.com/JihyunLee1/E2E-DST.", "published": "2023-12-04 12:25:46", "link": "http://arxiv.org/abs/2312.01842v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Synthetic Data Generation Techniques for Developing AI-based Speech\n  Assessments for Parkinson's Disease (A Comparative Study)", "abstract": "Changes in speech and language are among the first signs of Parkinson's\ndisease (PD). Thus, clinicians have tried to identify individuals with PD from\ntheir voices for years. Doctors can leverage AI-based speech assessments to\nspot PD thanks to advancements in artificial intelligence (AI). Such AI systems\ncan be developed using machine learning classifiers that have been trained\nusing individuals' voices. Although several studies have shown reasonable\nresults in developing such AI systems, these systems would need more data\nsamples to achieve promising performance. This paper explores using deep\nlearning-based data generation techniques on the accuracy of machine learning\nclassifiers that are the core of such systems.", "published": "2023-12-04 03:12:09", "link": "http://arxiv.org/abs/2312.02229v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
