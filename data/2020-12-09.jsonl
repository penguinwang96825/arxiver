{"title": "On an Unknown Ancestor of Burrows' Delta Measure", "abstract": "This article points out some surprising similarities between a 1944 study by\nGeorgy Udny Yule and modern approaches to authorship attribution.", "published": "2020-12-09 00:10:57", "link": "http://arxiv.org/abs/2012.04796v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fusing Context Into Knowledge Graph for Commonsense Question Answering", "abstract": "Commonsense question answering (QA) requires a model to grasp commonsense and\nfactual knowledge to answer questions about world events. Many prior methods\ncouple language modeling with knowledge graphs (KG). However, although a KG\ncontains rich structural information, it lacks the context to provide a more\nprecise understanding of the concepts. This creates a gap when fusing knowledge\ngraphs into language modeling, especially when there is insufficient labeled\ndata. Thus, we propose to employ external entity descriptions to provide\ncontextual information for knowledge understanding. We retrieve descriptions of\nrelated concepts from Wiktionary and feed them as additional input to\npre-trained language models. The resulting model achieves state-of-the-art\nresult in the CommonsenseQA dataset and the best result among non-generative\nmodels in OpenBookQA.", "published": "2020-12-09 00:57:49", "link": "http://arxiv.org/abs/2012.04808v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Relation Extraction by Leveraging Knowledge Graph Link\n  Prediction", "abstract": "Relation extraction (RE) aims to predict a relation between a subject and an\nobject in a sentence, while knowledge graph link prediction (KGLP) aims to\npredict a set of objects, O, given a subject and a relation from a knowledge\ngraph. These two problems are closely related as their respective objectives\nare intertwined: given a sentence containing a subject and an object o, a RE\nmodel predicts a relation that can then be used by a KGLP model together with\nthe subject, to predict a set of objects O. Thus, we expect object o to be in\nset O. In this paper, we leverage this insight by proposing a multi-task\nlearning approach that improves the performance of RE models by jointly\ntraining on RE and KGLP tasks. We illustrate the generality of our approach by\napplying it on several existing RE models and empirically demonstrate how it\nhelps them achieve consistent performance gains.", "published": "2020-12-09 01:08:13", "link": "http://arxiv.org/abs/2012.04812v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Complex Relation Extraction: Challenges and Opportunities", "abstract": "Relation extraction aims to identify the target relations of entities in\ntexts. Relation extraction is very important for knowledge base construction\nand text understanding. Traditional binary relation extraction, including\nsupervised, semi-supervised and distant supervised ones, has been extensively\nstudied and significant results are achieved. In recent years, many complex\nrelation extraction tasks, i.e., the variants of simple binary relation\nextraction, are proposed to meet the complex applications in practice. However,\nthere is no literature to fully investigate and summarize these complex\nrelation extraction works so far. In this paper, we first report the recent\nprogress in traditional simple binary relation extraction. Then we summarize\nthe existing complex relation extraction tasks and present the definition,\nrecent progress, challenges and opportunities for each task.", "published": "2020-12-09 02:05:00", "link": "http://arxiv.org/abs/2012.04821v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emotional Conversation Generation with Heterogeneous Graph Neural\n  Network", "abstract": "The successful emotional conversation system depends on sufficient perception\nand appropriate expression of emotions. In a real-life conversation, humans\nfirstly instinctively perceive emotions from multi-source information,\nincluding the emotion flow hidden in dialogue history, facial expressions,\naudio, and personalities of speakers. Then, they convey suitable emotions\naccording to their personalities, but these multiple types of information are\ninsufficiently exploited in emotional conversation fields. To address this\nissue, in this paper, we propose a heterogeneous graph-based model for\nemotional conversation generation. Firstly, we design a Heterogeneous\nGraph-Based Encoder to represent the conversation content (i.e., the dialogue\nhistory, its emotion flow, facial expressions, audio, and speakers'\npersonalities) with a heterogeneous graph neural network, and then predict\nsuitable emotions for feedback. Secondly, we employ an\nEmotion-Personality-Aware Decoder to generate a response relevant to the\nconversation context as well as with appropriate emotions, through taking the\nencoded graph representations, the predicted emotions by the encoder and the\npersonality of the current speaker as inputs. Experiments on both automatic and\nhuman evaluation show that our method can effectively perceive emotions from\nmulti-source knowledge and generate a satisfactory response. Furthermore, based\non the up-to-date text generator BART, our model still can achieve consistent\nimprovement, which significantly outperforms some existing state-of-the-art\nmodels.", "published": "2020-12-09 06:09:31", "link": "http://arxiv.org/abs/2012.04882v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating semantic maps through multidimensional scaling: linguistic\n  applications and theory", "abstract": "This paper reports on the state-of-the-art in application of multidimensional\nscaling (MDS) techniques to create semantic maps in linguistic research. MDS\nrefers to a statistical technique that represents objects (lexical items,\nlinguistic contexts, languages, etc.) as points in a space so that close\nsimilarity between the objects corresponds to close distances between the\ncorresponding points in the representation. We focus on the use of MDS in\ncombination with parallel corpus data as used in research on cross-linguistic\nvariation.\n  We first introduce the mathematical foundations of MDS and then give an\nexhaustive overview of past research that employs MDS techniques in combination\nwith parallel corpus data. We propose a set of terminology to succinctly\ndescribe the key parameters of a particular MDS application. We then show that\nthis computational methodology is theory-neutral, i.e. it can be employed to\nanswer research questions in a variety of linguistic theoretical frameworks.\nFinally, we show how this leads to two lines of future developments for MDS\nresearch in linguistics.", "published": "2020-12-09 10:02:09", "link": "http://arxiv.org/abs/2012.04946v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Breeding Gender-aware Direct Speech Translation Systems", "abstract": "In automatic speech translation (ST), traditional cascade approaches\ninvolving separate transcription and translation steps are giving ground to\nincreasingly competitive and more robust direct solutions. In particular, by\ntranslating speech audio data without intermediate transcription, direct ST\nmodels are able to leverage and preserve essential information present in the\ninput (e.g. speaker's vocal characteristics) that is otherwise lost in the\ncascade framework. Although such ability proved to be useful for gender\ntranslation, direct ST is nonetheless affected by gender bias just like its\ncascade counterpart, as well as machine translation and numerous other natural\nlanguage processing applications. Moreover, direct ST systems that exclusively\nrely on vocal biometric features as a gender cue can be unsuitable and\npotentially harmful for certain users. Going beyond speech signals, in this\npaper we compare different approaches to inform direct ST models about the\nspeaker's gender and test their ability to handle gender translation from\nEnglish into Italian and French. To this aim, we manually annotated large\ndatasets with speakers' gender information and used them for experiments\nreflecting different possible real-world scenarios. Our results show that\ngender-aware direct ST solutions can significantly outperform strong - but\ngender-unaware - direct ST models. In particular, the translation of\ngender-marked words can increase up to 30 points in accuracy while preserving\noverall translation quality.", "published": "2020-12-09 10:18:03", "link": "http://arxiv.org/abs/2012.04955v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Knowledge Distillation for Direct Speech Translation", "abstract": "Direct speech translation (ST) has shown to be a complex task requiring\nknowledge transfer from its sub-tasks: automatic speech recognition (ASR) and\nmachine translation (MT). For MT, one of the most promising techniques to\ntransfer knowledge is knowledge distillation. In this paper, we compare the\ndifferent solutions to distill knowledge in a sequence-to-sequence task like\nST. Moreover, we analyze eventual drawbacks of this approach and how to\nalleviate them maintaining the benefits in terms of translation quality.", "published": "2020-12-09 10:33:13", "link": "http://arxiv.org/abs/2012.04964v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tracking Interaction States for Multi-Turn Text-to-SQL Semantic Parsing", "abstract": "The task of multi-turn text-to-SQL semantic parsing aims to translate natural\nlanguage utterances in an interaction into SQL queries in order to answer them\nusing a database which normally contains multiple table schemas. Previous\nstudies on this task usually utilized contextual information to enrich\nutterance representations and to further influence the decoding process. While\nthey ignored to describe and track the interaction states which are determined\nby history SQL queries and are related with the intent of current utterance. In\nthis paper, two kinds of interaction states are defined based on schema items\nand SQL keywords separately. A relational graph neural network and a non-linear\nlayer are designed to update the representations of these two states\nrespectively. The dynamic schema-state and SQL-state representations are then\nutilized to decode the SQL query corresponding to current utterance.\nExperimental results on the challenging CoSQL dataset demonstrate the\neffectiveness of our proposed method, which achieves better performance than\nother published methods on the task leaderboard.", "published": "2020-12-09 11:59:58", "link": "http://arxiv.org/abs/2012.04995v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Word Sense Disambiguation using mBERT Embeddings with\n  Syntactic Dependencies", "abstract": "Cross-lingual word sense disambiguation (WSD) tackles the challenge of\ndisambiguating ambiguous words across languages given context. The pre-trained\nBERT embedding model has been proven to be effective in extracting contextual\ninformation of words, and have been incorporated as features into many\nstate-of-the-art WSD systems. In order to investigate how syntactic information\ncan be added into the BERT embeddings to result in both semantics- and\nsyntax-incorporated word embeddings, this project proposes the concatenated\nembeddings by producing dependency parse tress and encoding the relative\nrelationships of words into the input embeddings. Two methods are also proposed\nto reduce the size of the concatenated embeddings. The experimental results\nshow that the high dimensionality of the syntax-incorporated embeddings\nconstitute an obstacle for the classification task, which needs to be further\naddressed in future studies.", "published": "2020-12-09 20:22:11", "link": "http://arxiv.org/abs/2012.05300v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Normalization of Different Swedish Dialects Spoken in Finland", "abstract": "Our study presents a dialect normalization method for different Finland\nSwedish dialects covering six regions. We tested 5 different models, and the\nbest model improved the word error rate from 76.45 to 28.58. Contrary to\nresults reported in earlier research on Finnish dialects, we found that\ntraining the model with one word at a time gave best results. We believe this\nis due to the size of the training data available for the model. Our models are\naccessible as a Python package. The study provides important information about\nthe adaptability of these methods in different contexts, and gives important\nbaselines for further study.", "published": "2020-12-09 20:59:31", "link": "http://arxiv.org/abs/2012.05318v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Speech Recognition for Endangered and Extinct Samoyedic languages", "abstract": "Our study presents a series of experiments on speech recognition with\nendangered and extinct Samoyedic languages, spoken in Northern and Southern\nSiberia. To best of our knowledge, this is the first time a functional ASR\nsystem is built for an extinct language. We achieve with Kamas language a Label\nError Rate of 15\\%, and conclude through careful error analysis that this\nquality is already very useful as a starting point for refined human\ntranscriptions. Our results with related Nganasan language are more modest,\nwith best model having the error rate of 33\\%. We show, however, through\nexperiments where Kamas training data is enlarged incrementally, that Nganasan\nresults are in line with what is expected under low-resource circumstances of\nthe language. Based on this, we provide recommendations for scenarios in which\nfurther language documentation or archive processing activities could benefit\nfrom modern ASR technology. All training data and processing scripts haven been\npublished on Zenodo with clear licences to ensure further work in this\nimportant topic.", "published": "2020-12-09 21:41:40", "link": "http://arxiv.org/abs/2012.05331v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Label Confusion Learning to Enhance Text Classification Models", "abstract": "Representing a true label as a one-hot vector is a common practice in\ntraining text classification models. However, the one-hot representation may\nnot adequately reflect the relation between the instances and labels, as labels\nare often not completely independent and instances may relate to multiple\nlabels in practice. The inadequate one-hot representations tend to train the\nmodel to be over-confident, which may result in arbitrary prediction and model\noverfitting, especially for confused datasets (datasets with very similar\nlabels) or noisy datasets (datasets with labeling errors). While training\nmodels with label smoothing (LS) can ease this problem in some degree, it still\nfails to capture the realistic relation among labels. In this paper, we propose\na novel Label Confusion Model (LCM) as an enhancement component to current\npopular text classification models. LCM can learn label confusion to capture\nsemantic overlap among labels by calculating the similarity between instances\nand labels during training and generate a better label distribution to replace\nthe original one-hot label vector, thus improving the final classification\nperformance. Extensive experiments on five text classification benchmark\ndatasets reveal the effectiveness of LCM for several widely used deep learning\nclassification models. Further experiments also verify that LCM is especially\nhelpful for confused or noisy datasets and superior to the label smoothing\nmethod.", "published": "2020-12-09 11:34:35", "link": "http://arxiv.org/abs/2012.04987v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Intrinsically Motivated Compositional Language Emergence", "abstract": "Recently, there has been a great deal of research in emergent communication\non artificial agents interacting in simulated environments. Recent studies have\nrevealed that, in general, emergent languages do not follow the\ncompositionality patterns of natural language. To deal with this, existing\nworks have proposed a limited channel capacity as an important constraint for\nlearning highly compositional languages. In this paper, we show that this is\nnot a sufficient condition and propose an intrinsic reward framework for\nimproving compositionality in emergent communication. We use a reinforcement\nlearning setting with two agents -- a \\textit{task-aware} Speaker and a\n\\textit{state-aware} Listener that are required to communicate to perform a set\nof tasks. Through our experiments on three different referential game setups,\nincluding a novel environment gComm, we show intrinsic rewards improve\ncompositionality scores by $\\approx \\mathbf{1.5-2}$ times that of existing\nframeworks that use limited channel capacity.", "published": "2020-12-09 12:47:20", "link": "http://arxiv.org/abs/2012.05011v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generative Adversarial Networks for Annotated Data Augmentation in Data\n  Sparse NLU", "abstract": "Data sparsity is one of the key challenges associated with model development\nin Natural Language Understanding (NLU) for conversational agents. The\nchallenge is made more complex by the demand for high quality annotated\nutterances commonly required for supervised learning, usually resulting in\nweeks of manual labor and high cost. In this paper, we present our results on\nboosting NLU model performance through training data augmentation using a\nsequential generative adversarial network (GAN). We explore data generation in\nthe context of two tasks, the bootstrapping of a new language and the handling\nof low resource features. For both tasks we explore three sequential GAN\narchitectures, one with a token-level reward function, another with our own\nimplementation of a token-level Monte Carlo rollout reward, and a third with\nsentence-level reward. We evaluate the performance of these feedback models\nacross several sampling methodologies and compare our results to upsampling the\noriginal data to the same scale. We further improve the GAN model performance\nthrough the transfer learning of the pretrained embeddings. Our experiments\nreveal synthetic data generated using the sequential generative adversarial\nnetwork provides significant performance boosts across multiple metrics and can\nbe a major benefit to the NLU tasks.", "published": "2020-12-09 20:38:17", "link": "http://arxiv.org/abs/2012.05302v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Simple or Complex? Learning to Predict Readability of Bengali Texts", "abstract": "Determining the readability of a text is the first step to its\nsimplification. In this paper, we present a readability analysis tool capable\nof analyzing text written in the Bengali language to provide in-depth\ninformation on its readability and complexity. Despite being the 7th most\nspoken language in the world with 230 million native speakers, Bengali suffers\nfrom a lack of fundamental resources for natural language processing.\nReadability related research of the Bengali language so far can be considered\nto be narrow and sometimes faulty due to the lack of resources. Therefore, we\ncorrectly adopt document-level readability formulas traditionally used for U.S.\nbased education system to the Bengali language with a proper age-to-age\ncomparison. Due to the unavailability of large-scale human-annotated corpora,\nwe further divide the document-level task into sentence-level and experiment\nwith neural architectures, which will serve as a baseline for the future works\nof Bengali readability prediction. During the process, we present several\nhuman-annotated corpora and dictionaries such as a document-level dataset\ncomprising 618 documents with 12 different grade levels, a large-scale\nsentence-level dataset comprising more than 96K sentences with simple and\ncomplex labels, a consonant conjunct count algorithm and a corpus of 341 words\nto validate the effectiveness of the algorithm, a list of 3,396 easy words, and\nan updated pronunciation dictionary with more than 67K words. These resources\ncan be useful for several other tasks of this low-resource language. We make\nour Code & Dataset publicly available at\nhttps://github.com/tafseer-nayeem/BengaliReadability} for reproduciblity.", "published": "2020-12-09 01:41:35", "link": "http://arxiv.org/abs/2012.07701v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SongMASS: Automatic Song Writing with Pre-training and Alignment\n  Constraint", "abstract": "Automatic song writing aims to compose a song (lyric and/or melody) by\nmachine, which is an interesting topic in both academia and industry. In\nautomatic song writing, lyric-to-melody generation and melody-to-lyric\ngeneration are two important tasks, both of which usually suffer from the\nfollowing challenges: 1) the paired lyric and melody data are limited, which\naffects the generation quality of the two tasks, considering a lot of paired\ntraining data are needed due to the weak correlation between lyric and melody;\n2) Strict alignments are required between lyric and melody, which relies on\nspecific alignment modeling. In this paper, we propose SongMASS to address the\nabove challenges, which leverages masked sequence to sequence (MASS)\npre-training and attention based alignment modeling for lyric-to-melody and\nmelody-to-lyric generation. Specifically, 1) we extend the original\nsentence-level MASS pre-training to song level to better capture long\ncontextual information in music, and use a separate encoder and decoder for\neach modality (lyric or melody); 2) we leverage sentence-level attention mask\nand token-level attention constraint during training to enhance the alignment\nbetween lyric and melody. During inference, we use a dynamic programming\nstrategy to obtain the alignment between each word/syllable in lyric and note\nin melody. We pre-train SongMASS on unpaired lyric and melody datasets, and\nboth objective and subjective evaluations demonstrate that SongMASS generates\nlyric and melody with significantly better quality than the baseline method\nwithout pre-training or alignment constraint.", "published": "2020-12-09 16:56:59", "link": "http://arxiv.org/abs/2012.05168v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Topological Planning with Transformers for Vision-and-Language\n  Navigation", "abstract": "Conventional approaches to vision-and-language navigation (VLN) are trained\nend-to-end but struggle to perform well in freely traversable environments.\nInspired by the robotics community, we propose a modular approach to VLN using\ntopological maps. Given a natural language instruction and topological map, our\napproach leverages attention mechanisms to predict a navigation plan in the\nmap. The plan is then executed with low-level actions (e.g. forward, rotate)\nusing a robust controller. Experiments show that our method outperforms\nprevious end-to-end approaches, generates interpretable navigation plans, and\nexhibits intelligent behaviors such as backtracking.", "published": "2020-12-09 20:02:03", "link": "http://arxiv.org/abs/2012.05292v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Towards Coinductive Models for Natural Language Understanding. Bringing\n  together Deep Learning and Deep Semantics", "abstract": "This article contains a proposal to add coinduction to the computational\napparatus of natural language understanding. This, we argue, will provide a\nbasis for more realistic, computationally sound, and scalable models of natural\nlanguage dialogue, syntax and semantics. Given that the bottom up, inductively\nconstructed, semantic and syntactic structures are brittle, and seemingly\nincapable of adequately representing the meaning of longer sentences or\nrealistic dialogues, natural language understanding is in need of a new\nfoundation. Coinduction, which uses top down constraints, has been successfully\nused in the design of operating systems and programming languages. Moreover,\nimplicitly it has been present in text mining, machine translation, and in some\nattempts to model intensionality and modalities, which provides evidence that\nit works. This article shows high level formalizations of some of such uses.\n  Since coinduction and induction can coexist, they can provide a common\nlanguage and a conceptual model for research in natural language understanding.\nIn particular, such an opportunity seems to be emerging in research on\ncompositionality. This article shows several examples of the joint appearance\nof induction and coinduction in natural language processing. We argue that the\nknown individual limitations of induction and coinduction can be overcome in\nempirical settings by a combination of the the two methods. We see an open\nproblem in providing a theory of their joint use.", "published": "2020-12-09 03:10:36", "link": "http://arxiv.org/abs/2012.05715v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Mapping the Space of Chemical Reactions Using Attention-Based Neural\n  Networks", "abstract": "Organic reactions are usually assigned to classes containing reactions with\nsimilar reagents and mechanisms. Reaction classes facilitate the communication\nof complex concepts and efficient navigation through chemical reaction space.\nHowever, the classification process is a tedious task. It requires the\nidentification of the corresponding reaction class template via annotation of\nthe number of molecules in the reactions, the reaction center, and the\ndistinction between reactants and reagents. This work shows that\ntransformer-based models can infer reaction classes from non-annotated, simple\ntext-based representations of chemical reactions. Our best model reaches a\nclassification accuracy of 98.2%. We also show that the learned representations\ncan be used as reaction fingerprints that capture fine-grained differences\nbetween reaction classes better than traditional reaction fingerprints. The\ninsights into chemical reaction space enabled by our learned fingerprints are\nillustrated by an interactive reaction atlas providing visual clustering and\nsimilarity searching.", "published": "2020-12-09 10:25:30", "link": "http://arxiv.org/abs/2012.06051v1", "categories": ["physics.chem-ph", "cs.CL", "cs.LG"], "primary_category": "physics.chem-ph"}
{"title": "Automating Document Classification with Distant Supervision to Increase\n  the Efficiency of Systematic Reviews", "abstract": "Objective: Systematic reviews of scholarly documents often provide complete\nand exhaustive summaries of literature relevant to a research question.\nHowever, well-done systematic reviews are expensive, time-demanding, and\nlabor-intensive. Here, we propose an automatic document classification approach\nto significantly reduce the effort in reviewing documents. Methods: We first\ndescribe a manual document classification procedure that is used to curate a\npertinent training dataset and then propose three classifiers: a keyword-guided\nmethod, a cluster analysis-based refined method, and a random forest approach\nthat utilizes a large set of feature tokens. As an example, this approach is\nused to identify documents studying female sex workers that are assumed to\ncontain content relevant to either HIV or violence. We compare the performance\nof the three classifiers by cross-validation and conduct a sensitivity analysis\non the portion of data utilized in training the model. Results: The random\nforest approach provides the highest area under the curve (AUC) for both\nreceiver operating characteristic (ROC) and precision/recall (PR). Analyses of\nprecision and recall suggest that random forest could facilitate manually\nreviewing 20\\% of the articles while containing 80\\% of the relevant cases.\nFinally, we found a good classifier could be obtained by using a relatively\nsmall training sample size. Conclusions: In sum, the automated procedure of\ndocument classification presented here could improve both the precision and\nefficiency of systematic reviews, as well as facilitating live reviews, where\nreviews are updated regularly.", "published": "2020-12-09 22:45:40", "link": "http://arxiv.org/abs/2012.07565v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DeepTalk: Vocal Style Encoding for Speaker Recognition and Speech\n  Synthesis", "abstract": "Automatic speaker recognition algorithms typically characterize speech audio\nusing short-term spectral features that encode the physiological and anatomical\naspects of speech production. Such algorithms do not fully capitalize on\nspeaker-dependent characteristics present in behavioral speech features. In\nthis work, we propose a prosody encoding network called DeepTalk for extracting\nvocal style features directly from raw audio data. The DeepTalk method\noutperforms several state-of-the-art speaker recognition systems across\nmultiple challenging datasets. The speaker recognition performance is further\nimproved by combining DeepTalk with a state-of-the-art physiological speech\nfeature-based speaker recognition system. We also integrate DeepTalk into a\ncurrent state-of-the-art speech synthesizer to generate synthetic speech. A\ndetailed analysis of the synthetic speech shows that the DeepTalk captures F0\ncontours essential for vocal style modeling. Furthermore, DeepTalk-based\nsynthetic speech is shown to be almost indistinguishable from real speech in\nthe context of speaker recognition.", "published": "2020-12-09 14:39:40", "link": "http://arxiv.org/abs/2012.05084v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Semantic Communications for Speech Signals", "abstract": "We consider a semantic communication system for speech signals, named\nDeepSC-S. Motivated by the breakthroughs in deep learning (DL), we make an\neffort to recover the transmitted speech signals in the semantic communication\nsystems, which minimizes the error at the semantic level rather than the bit\nlevel or symbol level as in the traditional communication systems.\nParticularly, based on an attention mechanism employing squeeze-and-excitation\n(SE) networks, we design the transceiver as an end-to-end (E2E) system, which\nlearns and extracts the essential speech information. Furthermore, in order to\nfacilitate the proposed DeepSC-S to work well on dynamic practical\ncommunication scenarios, we find a model yielding good performance when coping\nwith various channel environments without retraining process. The simulation\nresults demonstrate that our proposed DeepSC-S is more robust to channel\nvariations and outperforms the traditional communication systems, especially in\nthe low signal-to-noise (SNR) regime.", "published": "2020-12-09 23:43:59", "link": "http://arxiv.org/abs/2012.05369v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Conjugate Mixture Models for Clustering Multimodal Data", "abstract": "The problem of multimodal clustering arises whenever the data are gathered\nwith several physically different sensors. Observations from different\nmodalities are not necessarily aligned in the sense there there is no obvious\nway to associate or to compare them in some common space. A solution may\nconsist in considering multiple clustering tasks independently for each\nmodality. The main difficulty with such an approach is to guarantee that the\nunimodal clusterings are mutually consistent. In this paper we show that\nmultimodal clustering can be addressed within a novel framework, namely\nconjugate mixture models. These models exploit the explicit transformations\nthat are often available between an unobserved parameter space (objects) and\neach one of the observation spaces (sensors). We formulate the problem as a\nlikelihood maximization task and we derive the associated conjugate\nexpectation-maximization algorithm. The convergence properties of the proposed\nalgorithm are thoroughly investigated. Several local/global optimization\ntechniques are proposed in order to increase its convergence speed. Two\ninitialization strategies are proposed and compared. A consistent\nmodel-selection criterion is proposed. The algorithm and its variants are\ntested and evaluated within the task of 3D localization of several speakers\nusing both auditory and visual data.", "published": "2020-12-09 10:13:22", "link": "http://arxiv.org/abs/2012.04951v1", "categories": ["stat.ML", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "stat.ML"}
