{"title": "Speciesist Language and Nonhuman Animal Bias in English Masked Language\n  Models", "abstract": "Various existing studies have analyzed what social biases are inherited by\nNLP models. These biases may directly or indirectly harm people, therefore\nprevious studies have focused only on human attributes. However, until recently\nno research on social biases in NLP regarding nonhumans existed. In this paper,\nwe analyze biases to nonhuman animals, i.e. speciesist bias, inherent in\nEnglish Masked Language Models such as BERT. We analyzed speciesist bias\nagainst 46 animal names using template-based and corpus-extracted sentences\ncontaining speciesist (or non-speciesist) language. We found that pre-trained\nmasked language models tend to associate harmful words with nonhuman animals\nand have a bias toward using speciesist language for some nonhuman animal\nnames. Our code for reproducing the experiments will be made available on\nGitHub.", "published": "2022-03-10 03:32:29", "link": "http://arxiv.org/abs/2203.05140v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Accurate Unsupervised Method for Joint Entity Alignment and Dangling\n  Entity Detection", "abstract": "Knowledge graph integration typically suffers from the widely existing\ndangling entities that cannot find alignment cross knowledge graphs (KGs). The\ndangling entity set is unavailable in most real-world scenarios, and manually\nmining the entity pairs that consist of entities with the same meaning is\nlabor-consuming. In this paper, we propose a novel accurate Unsupervised method\nfor joint Entity alignment (EA) and Dangling entity detection (DED), called\nUED. The UED mines the literal semantic information to generate pseudo entity\npairs and globally guided alignment information for EA and then utilizes the EA\nresults to assist the DED. We construct a medical cross-lingual knowledge graph\ndataset, MedED, providing data for both the EA and DED tasks. Extensive\nexperiments demonstrate that in the EA task, UED achieves EA results comparable\nto those of state-of-the-art supervised EA baselines and outperforms the\ncurrent state-of-the-art EA methods by combining supervised EA data. For the\nDED task, UED obtains high-quality results without supervision.", "published": "2022-03-10 04:08:53", "link": "http://arxiv.org/abs/2203.05147v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Faithfulness in Natural Language Generation: A Systematic Survey of\n  Analysis, Evaluation and Optimization Methods", "abstract": "Natural Language Generation (NLG) has made great progress in recent years due\nto the development of deep learning techniques such as pre-trained language\nmodels. This advancement has resulted in more fluent, coherent and even\nproperties controllable (e.g. stylistic, sentiment, length etc.) generation,\nnaturally leading to development in downstream tasks such as abstractive\nsummarization, dialogue generation, machine translation, and data-to-text\ngeneration. However, the faithfulness problem that the generated text usually\ncontains unfaithful or non-factual information has become the biggest\nchallenge, which makes the performance of text generation unsatisfactory for\npractical applications in many real-world scenarios. Many studies on analysis,\nevaluation, and optimization methods for faithfulness problems have been\nproposed for various tasks, but have not been organized, compared and discussed\nin a combined manner. In this survey, we provide a systematic overview of the\nresearch progress on the faithfulness problem of NLG, including problem\nanalysis, evaluation metrics and optimization methods. We organize the\nevaluation and optimization methods for different tasks into a unified taxonomy\nto facilitate comparison and learning across tasks. Several research trends are\ndiscussed further.", "published": "2022-03-10 08:28:32", "link": "http://arxiv.org/abs/2203.05227v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Look Backward and Forward: Self-Knowledge Distillation with\n  Bidirectional Decoder for Neural Machine Translation", "abstract": "Neural Machine Translation(NMT) models are usually trained via unidirectional\ndecoder which corresponds to optimizing one-step-ahead prediction. However,\nthis kind of unidirectional decoding framework may incline to focus on local\nstructure rather than global coherence. To alleviate this problem, we propose a\nnovel method, Self-Knowledge Distillation with Bidirectional Decoder for Neural\nMachine Translation(SBD-NMT). We deploy a backward decoder which can act as an\neffective regularization method to the forward decoder. By leveraging the\nbackward decoder's information about the longer-term future, distilling\nknowledge learned in the backward decoder can encourage auto-regressive NMT\nmodels to plan ahead. Experiments show that our method is significantly better\nthan the strong Transformer baselines on multiple machine translation data\nsets.", "published": "2022-03-10 09:21:28", "link": "http://arxiv.org/abs/2203.05248v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Connecting Neural Response measurements & Computational Models of\n  language: a non-comprehensive guide", "abstract": "Understanding the neural basis of language comprehension in the brain has\nbeen a long-standing goal of various scientific research programs. Recent\nadvances in language modelling and in neuroimaging methodology promise\npotential improvements in both the investigation of language's neurobiology and\nin the building of better and more human-like language models. This survey\ntraces a line from early research linking Event Related Potentials and\ncomplexity measures derived from simple language models to contemporary studies\nemploying Artificial Neural Network models trained on large corpora in\ncombination with neural response recordings from multiple modalities using\nnaturalistic stimuli.", "published": "2022-03-10 11:24:54", "link": "http://arxiv.org/abs/2203.05300v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SATLab at SemEval-2022 Task 4: Trying to Detect Patronizing and\n  Condescending Language with only Character and Word N-grams", "abstract": "A logistic regression model only fed with character and word n-grams is\nproposed for the SemEval-2022 Task 4 on Patronizing and Condescending Language\nDetection (PCL). It obtained an average level of performance, well above the\nperformance of a system that tries to guess without using any knowledge about\nthe task, but much lower than the best teams. As the proposed model is very\nsimilar to the one that performed well on a task requiring to automatically\nidentify hate speech and offensive content, this paper confirms the difficulty\nof PCL detection.", "published": "2022-03-10 13:09:48", "link": "http://arxiv.org/abs/2203.05355v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Faking Fake News for Real Fake News Detection: Propaganda-loaded\n  Training Data Generation", "abstract": "Despite recent advances in detecting fake news generated by neural models,\ntheir results are not readily applicable to effective detection of\nhuman-written disinformation. What limits the successful transfer between them\nis the sizable gap between machine-generated fake news and human-authored ones,\nincluding the notable differences in terms of style and underlying intent. With\nthis in mind, we propose a novel framework for generating training examples\nthat are informed by the known styles and strategies of human-authored\npropaganda. Specifically, we perform self-critical sequence training guided by\nnatural language inference to ensure the validity of the generated articles,\nwhile also incorporating propaganda techniques, such as appeal to authority and\nloaded language. In particular, we create a new training dataset, PropaNews,\nwith 2,256 examples, which we release for future use. Our experimental results\nshow that fake news detectors trained on PropaNews are better at detecting\nhuman-written disinformation by 3.62 - 7.69% F1 score on two public datasets.", "published": "2022-03-10 14:24:19", "link": "http://arxiv.org/abs/2203.05386v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextualized Sensorimotor Norms: multi-dimensional measures of\n  sensorimotor strength for ambiguous English words, in context", "abstract": "Most large language models are trained on linguistic input alone, yet humans\nappear to ground their understanding of words in sensorimotor experience. A\nnatural solution is to augment LM representations with human judgments of a\nword's sensorimotor associations (e.g., the Lancaster Sensorimotor Norms), but\nthis raises another challenge: most words are ambiguous, and judgments of words\nin isolation fail to account for this multiplicity of meaning (e.g., \"wooden\ntable\" vs. \"data table\"). We attempted to address this problem by building a\nnew lexical resource of contextualized sensorimotor judgments for 112 English\nwords, each rated in four different contexts (448 sentences total). We show\nthat these ratings encode overlapping but distinct information from the\nLancaster Sensorimotor Norms, and that they also predict other measures of\ninterest (e.g., relatedness), above and beyond measures derived from BERT.\nBeyond shedding light on theoretical questions, we suggest that these ratings\ncould be of use as a \"challenge set\" for researchers building grounded language\nmodels.", "published": "2022-03-10 21:23:00", "link": "http://arxiv.org/abs/2203.05648v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Internet-augmented language models through few-shot prompting for\n  open-domain question answering", "abstract": "In this work, we aim to capitalize on the unique few-shot capabilities of\nlarge-scale language models (LSLMs) to overcome some of their challenges with\nrespect to grounding to factual and up-to-date information. Motivated by\nsemi-parametric language models (LMs), which ground their decisions in external\nretrieved evidence, we use few-shot prompting to learn to condition LMs on\ninformation returned from the web using Google Search, a broad and constantly\nupdated knowledge source. Our approach does not involve fine-tuning or learning\nadditional parameters, thus making it applicable to any LM, offering therefore\na strong baseline. Indeed, we find that LMs conditioned on the web surpass\nperformance of closed-book models of similar, or even larger, model sizes in\nopen-domain question answering. Finally, we find that increasing the\ninference-time compute of models, achieved via using multiple retrieved\nevidences to generate multiple answers followed by a reranking stage that uses\nscores generated by the same LMs, leads to better performance and alleviates\nlower performance of smaller few-shot LMs. All in all, our findings suggest\nthat it might be beneficial to slow down the race towards the biggest model and\ninstead shift attention towards finding more effective ways to use models,\nincluding but not limited to, better prompting or increasing inference-time\ncompute.", "published": "2022-03-10 02:24:14", "link": "http://arxiv.org/abs/2203.05115v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "StyleBabel: Artistic Style Tagging and Captioning", "abstract": "We present StyleBabel, a unique open access dataset of natural language\ncaptions and free-form tags describing the artistic style of over 135K digital\nartworks, collected via a novel participatory method from experts studying at\nspecialist art and design schools. StyleBabel was collected via an iterative\nmethod, inspired by `Grounded Theory': a qualitative approach that enables\nannotation while co-evolving a shared language for fine-grained artistic style\nattribute description. We demonstrate several downstream tasks for StyleBabel,\nadapting the recent ALADIN architecture for fine-grained style similarity, to\ntrain cross-modal embeddings for: 1) free-form tag generation; 2) natural\nlanguage description of artistic style; 3) fine-grained text search of style.\nTo do so, we extend ALADIN with recent advances in Visual Transformer (ViT) and\ncross-modal representation learning, achieving a state of the art accuracy in\nfine-grained style retrieval.", "published": "2022-03-10 12:15:55", "link": "http://arxiv.org/abs/2203.05321v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Knowledge-enriched Attention Network with Group-wise Semantic for Visual\n  Storytelling", "abstract": "As a technically challenging topic, visual storytelling aims at generating an\nimaginary and coherent story with narrative multi-sentences from a group of\nrelevant images. Existing methods often generate direct and rigid descriptions\nof apparent image-based contents, because they are not capable of exploring\nimplicit information beyond images. Hence, these schemes could not capture\nconsistent dependencies from holistic representation, impairing the generation\nof reasonable and fluent story. To address these problems, a novel\nknowledge-enriched attention network with group-wise semantic model is\nproposed. Three main novel components are designed and supported by substantial\nexperiments to reveal practical advantages. First, a knowledge-enriched\nattention network is designed to extract implicit concepts from external\nknowledge system, and these concepts are followed by a cascade cross-modal\nattention mechanism to characterize imaginative and concrete representations.\nSecond, a group-wise semantic module with second-order pooling is developed to\nexplore the globally consistent guidance. Third, a unified one-stage story\ngeneration model with encoder-decoder structure is proposed to simultaneously\ntrain and infer the knowledge-enriched attention network, group-wise semantic\nmodule and multi-modal story generation decoder in an end-to-end fashion.\nSubstantial experiments on the popular Visual Storytelling dataset with both\nobjective and subjective evaluation metrics demonstrate the superior\nperformance of the proposed scheme as compared with other state-of-the-art\nmethods.", "published": "2022-03-10 12:55:47", "link": "http://arxiv.org/abs/2203.05346v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "KSoF: The Kassel State of Fluency Dataset -- A Therapy Centered Dataset\n  of Stuttering", "abstract": "Stuttering is a complex speech disorder that negatively affects an\nindividual's ability to communicate effectively. Persons who stutter (PWS)\noften suffer considerably under the condition and seek help through therapy.\nFluency shaping is a therapy approach where PWSs learn to modify their speech\nto help them to overcome their stutter. Mastering such speech techniques takes\ntime and practice, even after therapy. Shortly after therapy, success is\nevaluated highly, but relapse rates are high. To be able to monitor speech\nbehavior over a long time, the ability to detect stuttering events and\nmodifications in speech could help PWSs and speech pathologists to track the\nlevel of fluency. Monitoring could create the ability to intervene early by\ndetecting lapses in fluency. To the best of our knowledge, no public dataset is\navailable that contains speech from people who underwent stuttering therapy\nthat changed the style of speaking. This work introduces the Kassel State of\nFluency (KSoF), a therapy-based dataset containing over 5500 clips of PWSs. The\nclips were labeled with six stuttering-related event types: blocks,\nprolongations, sound repetitions, word repetitions, interjections, and -\nspecific to therapy - speech modifications. The audio was recorded during\ntherapy sessions at the Institut der Kasseler Stottertherapie. The data will be\nmade available for research purposes upon request.", "published": "2022-03-10 14:17:07", "link": "http://arxiv.org/abs/2203.05383v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "OneRel:Joint Entity and Relation Extraction with One Module in One Step", "abstract": "Joint entity and relation extraction is an essential task in natural language\nprocessing and knowledge graph construction. Existing approaches usually\ndecompose the joint extraction task into several basic modules or processing\nsteps to make it easy to conduct. However, such a paradigm ignores the fact\nthat the three elements of a triple are interdependent and indivisible.\nTherefore, previous joint methods suffer from the problems of cascading errors\nand redundant information. To address these issues, in this paper, we propose a\nnovel joint entity and relation extraction model, named OneRel, which casts\njoint extraction as a fine-grained triple classification problem. Specifically,\nour model consists of a scoring-based classifier and a relation-specific horns\ntagging strategy. The former evaluates whether a token pair and a relation\nbelong to a factual triple. The latter ensures a simple but effective decoding\nprocess. Extensive experimental results on two widely used datasets demonstrate\nthat the proposed method performs better than the state-of-the-art baselines,\nand delivers consistent performance gain on complex scenarios of various\noverlapping patterns and multiple triples.", "published": "2022-03-10 15:09:59", "link": "http://arxiv.org/abs/2203.05412v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Semantic Norm Recognition and its application to Portuguese Law", "abstract": "Being able to clearly interpret legal texts and fully understanding our\nrights, obligations and other legal norms has become progressively more\nimportant in the digital society. However, simply giving citizens access to the\nlaws is not enough, as there is a need to provide meaningful information that\ncater to their specific queries and needs. For this, it is necessary to extract\nthe relevant semantic information present in legal texts. Thus, we introduce\nthe SNR (Semantic Norm Recognition) system, an automatic semantic information\nextraction system trained on a domain-specific (legal) text corpus taken from\nPortuguese Consumer Law. The SNR system uses the Portuguese Bert (BERTimbau)\nand was trained on a legislative Portuguese corpus. We demonstrate how our\nsystem achieved good results (81.44\\% F1-score) on this domain-specific corpus,\ndespite existing noise, and how it can be used to improve downstream tasks such\nas information retrieval.", "published": "2022-03-10 15:28:05", "link": "http://arxiv.org/abs/2203.05425v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "IndicNLG Benchmark: Multilingual Datasets for Diverse NLG Tasks in Indic\n  Languages", "abstract": "Natural Language Generation (NLG) for non-English languages is hampered by\nthe scarcity of datasets in these languages. In this paper, we present the\nIndicNLG Benchmark, a collection of datasets for benchmarking NLG for 11 Indic\nlanguages. We focus on five diverse tasks, namely, biography generation using\nWikipedia infoboxes, news headline generation, sentence summarization,\nparaphrase generation and, question generation. We describe the created\ndatasets and use them to benchmark the performance of several monolingual and\nmultilingual baselines that leverage pre-trained sequence-to-sequence models.\nOur results exhibit the strong performance of multilingual language-specific\npre-trained models, and the utility of models trained on our dataset for other\nrelated NLG tasks. Our dataset creation methods can be easily applied to\nmodest-resource languages as they involve simple steps such as scraping news\narticles and Wikipedia infoboxes, light cleaning, and pivoting through machine\ntranslation data. To the best of our knowledge, the IndicNLG Benchmark is the\nfirst NLG benchmark for Indic languages and the most diverse multilingual NLG\ndataset, with approximately 8M examples across 5 tasks and 11 languages. The\ndatasets and models are publicly available at\nhttps://ai4bharat.iitm.ac.in/indicnlg-suite.", "published": "2022-03-10 15:53:58", "link": "http://arxiv.org/abs/2203.05437v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A new approach to calculating BERTScore for automatic assessment of\n  translation quality", "abstract": "The study of the applicability of the BERTScore metric was conducted to\ntranslation quality assessment at the sentence level for English -> Russian\ndirection. Experiments were performed with a pre-trained Multilingual BERT as\nwell as with a pair of Monolingual BERT models. To align monolingual\nembeddings, an orthogonal transformation based on anchor tokens was used. It\nwas demonstrated that such transformation helps to prevent mismatching issue\nand shown that this approach gives better results than using embeddings of the\nMultilingual model. To improve the token matching process it is proposed to\ncombine all incomplete WorkPiece tokens into meaningful words and use simple\naveraging of corresponding vectors and to calculate BERTScore based on anchor\ntokens only. Such modifications allowed us to achieve a better correlation of\nthe model predictions with human judgments. In addition to evaluating machine\ntranslation, several versions of human translation were evaluated as well, the\nproblems of this approach were listed.", "published": "2022-03-10 19:25:16", "link": "http://arxiv.org/abs/2203.05598v4", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "State of the Art in Artificial Intelligence applied to the Legal Domain", "abstract": "While Artificial Intelligence applied to the legal domain is a topic with\norigins in the last century, recent advances in Artificial Intelligence are\nposed to revolutionize it. This work presents an overview and contextualizes\nthe main advances on the field of Natural Language Processing and how these\nadvances have been used to further the state of the art in legal text analysis.", "published": "2022-03-10 16:20:36", "link": "http://arxiv.org/abs/2204.07047v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Librarian-in-the-Loop: A Natural Language Processing Paradigm for\n  Detecting Informal Mentions of Research Data in Academic Literature", "abstract": "Data citations provide a foundation for studying research data impact.\nCollecting and managing data citations is a new frontier in archival science\nand scholarly communication. However, the discovery and curation of research\ndata citations is labor intensive. Data citations that reference unique\nidentifiers (i.e. DOIs) are readily findable; however, informal mentions made\nto research data are more challenging to infer. We propose a natural language\nprocessing (NLP) paradigm to support the human task of identifying informal\nmentions made to research datasets. The work of discovering informal data\nmentions is currently performed by librarians and their staff in the\nInter-university Consortium for Political and Social Research (ICPSR), a large\nsocial science data archive that maintains a large bibliography of data-related\nliterature. The NLP model is bootstrapped from data citations actively\ncollected by librarians at ICPSR. The model combines pattern matching with\nmultiple iterations of human annotations to learn additional rules for\ndetecting informal data mentions. These examples are then used to train an NLP\npipeline. The librarian-in-the-loop paradigm is centered in the data work\nperformed by ICPSR librarians, supporting broader efforts to build a more\ncomprehensive bibliography of data-related literature that reflects the\nscholarly communities of research data users.", "published": "2022-03-10 02:11:30", "link": "http://arxiv.org/abs/2203.05112v1", "categories": ["cs.DL", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.DL"}
{"title": "Compilable Neural Code Generation with Compiler Feedback", "abstract": "Automatically generating compilable programs with (or without) natural\nlanguage descriptions has always been a touchstone problem for computational\nlinguistics and automated software engineering. Existing deep-learning\napproaches model code generation as text generation, either constrained by\ngrammar structures in decoder, or driven by pre-trained language models on\nlarge-scale code corpus (e.g., CodeGPT, PLBART, and CodeT5). However, few of\nthem account for compilability of the generated programs. To improve\ncompilability of the generated programs, this paper proposes COMPCODER, a\nthree-stage pipeline utilizing compiler feedback for compilable code\ngeneration, including language model fine-tuning, compilability reinforcement,\nand compilability discrimination. Comprehensive experiments on two code\ngeneration tasks demonstrate the effectiveness of our proposed approach,\nimproving the success rate of compilation from 44.18 to 89.18 in code\ncompletion on average and from 70.3 to 96.2 in text-to-code generation,\nrespectively, when comparing with the state-of-the-art CodeGPT.", "published": "2022-03-10 03:15:17", "link": "http://arxiv.org/abs/2203.05132v1", "categories": ["cs.CL", "cs.AI", "cs.PL"], "primary_category": "cs.CL"}
{"title": "TextConvoNet:A Convolutional Neural Network based Architecture for Text\n  Classification", "abstract": "In recent years, deep learning-based models have significantly improved the\nNatural Language Processing (NLP) tasks. Specifically, the Convolutional Neural\nNetwork (CNN), initially used for computer vision, has shown remarkable\nperformance for text data in various NLP problems. Most of the existing\nCNN-based models use 1-dimensional convolving filters n-gram detectors), where\neach filter specialises in extracting n-grams features of a particular input\nword embedding. The input word embeddings, also called sentence matrix, is\ntreated as a matrix where each row is a word vector. Thus, it allows the model\nto apply one-dimensional convolution and only extract n-gram based features\nfrom a sentence matrix. These features can be termed as intra-sentence n-gram\nfeatures. To the extent of our knowledge, all the existing CNN models are based\non the aforementioned concept. In this paper, we present a CNN-based\narchitecture TextConvoNet that not only extracts the intra-sentence n-gram\nfeatures but also captures the inter-sentence n-gram features in input text\ndata. It uses an alternative approach for input matrix representation and\napplies a two-dimensional multi-scale convolutional operation on the input. To\nevaluate the performance of TextConvoNet, we perform an experimental study on\nfive text classification datasets. The results are evaluated by using various\nperformance metrics. The experimental results show that the presented\nTextConvoNet outperforms state-of-the-art machine learning and deep learning\nmodels for text classification purposes.", "published": "2022-03-10 06:09:56", "link": "http://arxiv.org/abs/2203.05173v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "MORE: Multi-Order RElation Mining for Dense Captioning in 3D Scenes", "abstract": "3D dense captioning is a recently-proposed novel task, where point clouds\ncontain more geometric information than the 2D counterpart. However, it is also\nmore challenging due to the higher complexity and wider variety of inter-object\nrelations contained in point clouds. Existing methods only treat such relations\nas by-products of object feature learning in graphs without specifically\nencoding them, which leads to sub-optimal results. In this paper, aiming at\nimproving 3D dense captioning via capturing and utilizing the complex relations\nin the 3D scene, we propose MORE, a Multi-Order RElation mining model, to\nsupport generating more descriptive and comprehensive captions. Technically,\nour MORE encodes object relations in a progressive manner since complex\nrelations can be deduced from a limited number of basic ones. We first devise a\nnovel Spatial Layout Graph Convolution (SLGC), which semantically encodes\nseveral first-order relations as edges of a graph constructed over 3D object\nproposals. Next, from the resulting graph, we further extract multiple triplets\nwhich encapsulate basic first-order relations as the basic unit, and construct\nseveral Object-centric Triplet Attention Graphs (OTAG) to infer multi-order\nrelations for every target object. The updated node features from OTAG are\naggregated and fed into the caption decoder to provide abundant relational\ncues, so that captions including diverse relations with context objects can be\ngenerated. Extensive experiments on the Scan2Cap dataset prove the\neffectiveness of our proposed MORE and its components, and we also outperform\nthe current state-of-the-art method. Our code is available at\nhttps://github.com/SxJyJay/MORE.", "published": "2022-03-10 07:26:15", "link": "http://arxiv.org/abs/2203.05203v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A Closer Look at Debiased Temporal Sentence Grounding in Videos:\n  Dataset, Metric, and Approach", "abstract": "Temporal Sentence Grounding in Videos (TSGV), which aims to ground a natural\nlanguage sentence in an untrimmed video, has drawn widespread attention over\nthe past few years. However, recent studies have found that current benchmark\ndatasets may have obvious moment annotation biases, enabling several simple\nbaselines even without training to achieve SOTA performance. In this paper, we\ntake a closer look at existing evaluation protocols, and find both the\nprevailing dataset and evaluation metrics are the devils that lead to\nuntrustworthy benchmarking. Therefore, we propose to re-organize the two\nwidely-used datasets, making the ground-truth moment distributions different in\nthe training and test splits, i.e., out-of-distribution (OOD) test. Meanwhile,\nwe introduce a new evaluation metric \"dR@n,IoU@m\" that discounts the basic\nrecall scores to alleviate the inflating evaluation caused by biased datasets.\nNew benchmarking results indicate that our proposed evaluation protocols can\nbetter monitor the research progress. Furthermore, we propose a novel\ncausality-based Multi-branch Deconfounding Debiasing (MDD) framework for\nunbiased moment prediction. Specifically, we design a multi-branch deconfounder\nto eliminate the effects caused by multiple confounders with causal\nintervention. In order to help the model better align the semantics between\nsentence queries and video moments, we enhance the representations during\nfeature encoding. Specifically, for textual information, the query is parsed\ninto several verb-centered phrases to obtain a more fine-grained textual\nfeature. For visual information, the positional information has been decomposed\nfrom moment features to enhance representations of moments with diverse\nlocations. Extensive experiments demonstrate that our proposed approach can\nachieve competitive results among existing SOTA approaches and outperform the\nbase model with great gains.", "published": "2022-03-10 08:58:18", "link": "http://arxiv.org/abs/2203.05243v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "AIFB-WebScience at SemEval-2022 Task 12: Relation Extraction First --\n  Using Relation Extraction to Identify Entities", "abstract": "In this paper, we present an end-to-end joint entity and relation extraction\napproach based on transformer-based language models. We apply the model to the\ntask of linking mathematical symbols to their descriptions in LaTeX documents.\nIn contrast to existing approaches, which perform entity and relation\nextraction in sequence, our system incorporates information from relation\nextraction into entity extraction. This means that the system can be trained\neven on data sets where only a subset of all valid entity spans is annotated.\nWe provide an extensive evaluation of the proposed system and its strengths and\nweaknesses. Our approach, which can be scaled dynamically in computational\ncomplexity at inference time, produces predictions with high precision and\nreaches 3rd place in the leaderboard of SemEval-2022 Task 12. For inputs in the\ndomain of physics and math, it achieves high relation extraction macro F1\nscores of 95.43% and 79.17%, respectively. The code used for training and\nevaluating our models is available at: https://github.com/nicpopovic/RE1st", "published": "2022-03-10 12:19:44", "link": "http://arxiv.org/abs/2203.05325v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LoopITR: Combining Dual and Cross Encoder Architectures for Image-Text\n  Retrieval", "abstract": "Dual encoders and cross encoders have been widely used for image-text\nretrieval. Between the two, the dual encoder encodes the image and text\nindependently followed by a dot product, while the cross encoder jointly feeds\nimage and text as the input and performs dense multi-modal fusion. These two\narchitectures are typically modeled separately without interaction. In this\nwork, we propose LoopITR, which combines them in the same network for joint\nlearning. Specifically, we let the dual encoder provide hard negatives to the\ncross encoder, and use the more discriminative cross encoder to distill its\npredictions back to the dual encoder. Both steps are efficiently performed\ntogether in the same model. Our work centers on empirical analyses of this\ncombined architecture, putting the main focus on the design of the distillation\nobjective. Our experimental results highlight the benefits of training the two\nencoders in the same network, and demonstrate that distillation can be quite\neffective with just a few hard negative examples. Experiments on two standard\ndatasets (Flickr30K and COCO) show our approach achieves state-of-the-art dual\nencoder performance when compared with approaches using a similar amount of\ndata.", "published": "2022-03-10 16:41:12", "link": "http://arxiv.org/abs/2203.05465v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Model soups: averaging weights of multiple fine-tuned models improves\n  accuracy without increasing inference time", "abstract": "The conventional recipe for maximizing model accuracy is to (1) train\nmultiple models with various hyperparameters and (2) pick the individual model\nwhich performs best on a held-out validation set, discarding the remainder. In\nthis paper, we revisit the second step of this procedure in the context of\nfine-tuning large pre-trained models, where fine-tuned models often appear to\nlie in a single low error basin. We show that averaging the weights of multiple\nmodels fine-tuned with different hyperparameter configurations often improves\naccuracy and robustness. Unlike a conventional ensemble, we may average many\nmodels without incurring any additional inference or memory costs -- we call\nthe results \"model soups.\" When fine-tuning large pre-trained models such as\nCLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides\nsignificant improvements over the best model in a hyperparameter sweep on\nImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on\nImageNet, achieved a new state of the art. Furthermore, we show that the model\nsoup approach extends to multiple image classification and natural language\nprocessing tasks, improves out-of-distribution performance, and improves\nzero-shot performance on new downstream tasks. Finally, we analytically relate\nthe performance similarity of weight-averaging and logit-ensembling to flatness\nof the loss and confidence of the predictions, and validate this relation\nempirically. Code is available at https://github.com/mlfoundations/model-soups.", "published": "2022-03-10 17:03:49", "link": "http://arxiv.org/abs/2203.05482v3", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Conditional Prompt Learning for Vision-Language Models", "abstract": "With the rise of powerful pre-trained vision-language models like CLIP, it\nbecomes essential to investigate ways to adapt these models to downstream\ndatasets. A recently proposed method named Context Optimization (CoOp)\nintroduces the concept of prompt learning -- a recent trend in NLP -- to the\nvision domain for adapting pre-trained vision-language models. Specifically,\nCoOp turns context words in a prompt into a set of learnable vectors and, with\nonly a few labeled images for learning, can achieve huge improvements over\nintensively-tuned manual prompts. In our study we identify a critical problem\nof CoOp: the learned context is not generalizable to wider unseen classes\nwithin the same dataset, suggesting that CoOp overfits base classes observed\nduring training. To address the problem, we propose Conditional Context\nOptimization (CoCoOp), which extends CoOp by further learning a lightweight\nneural network to generate for each image an input-conditional token (vector).\nCompared to CoOp's static prompts, our dynamic prompts adapt to each instance\nand are thus less sensitive to class shift. Extensive experiments show that\nCoCoOp generalizes much better than CoOp to unseen classes, even showing\npromising transferability beyond a single dataset; and yields stronger domain\ngeneralization performance as well. Code is available at\nhttps://github.com/KaiyangZhou/CoOp.", "published": "2022-03-10 18:59:41", "link": "http://arxiv.org/abs/2203.05557v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Parameter-Free Attentive Scoring for Speaker Verification", "abstract": "This paper presents a novel study of parameter-free attentive scoring for\nspeaker verification. Parameter-free scoring provides the flexibility of\ncomparing speaker representations without the need of an accompanying\nparametric scoring model. Inspired by the attention component in Transformer\nneural networks, we propose a variant of the scaled dot product attention\nmechanism to compare enrollment and test segment representations. In addition,\nthis work explores the effect on performance of (i) different types of\nnormalization, (ii) independent versus tied query/key estimation, (iii) varying\nthe number of key-value pairs and (iv) pooling multiple enrollment utterance\nstatistics. Experimental results for a 4 task average show that a simple\nparameter-free attentive scoring mechanism can improve the average EER by 10%\nover the best cosine similarity baseline.", "published": "2022-03-10 21:11:37", "link": "http://arxiv.org/abs/2203.05642v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "NELA-GT-2022: A Large Multi-Labelled News Dataset for The Study of\n  Misinformation in News Articles", "abstract": "In this paper, we present the fifth installment of the NELA-GT datasets,\nNELA-GT-2022. The dataset contains 1,778,361 articles from 361 outlets between\nJanuary 1st, 2022 and December 31st, 2022. Just as in past releases of the\ndataset, NELA-GT-2022 includes outlet-level veracity labels from Media\nBias/Fact Check and tweets embedded in collected news articles. The\nNELA-GT-2022 dataset can be found at: https://doi.org/10.7910/DVN/AMCV2H", "published": "2022-03-10 21:58:33", "link": "http://arxiv.org/abs/2203.05659v2", "categories": ["cs.CL", "cs.CY", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for\n  Conversational Gestures Synthesis", "abstract": "Achieving realistic, vivid, and human-like synthesized conversational\ngestures conditioned on multi-modal data is still an unsolved problem due to\nthe lack of available datasets, models and standard evaluation metrics. To\naddress this, we build Body-Expression-Audio-Text dataset, BEAT, which has i)\n76 hours, high-quality, multi-modal data captured from 30 speakers talking with\neight different emotions and in four different languages, ii) 32 millions\nframe-level emotion and semantic relevance annotations. Our statistical\nanalysis on BEAT demonstrates the correlation of conversational gestures with\nfacial expressions, emotions, and semantics, in addition to the known\ncorrelation with audio, text, and speaker identity. Based on this observation,\nwe propose a baseline model, Cascaded Motion Network (CaMN), which consists of\nabove six modalities modeled in a cascaded architecture for gesture synthesis.\nTo evaluate the semantic relevancy, we introduce a metric, Semantic Relevance\nGesture Recall (SRGR). Qualitative and quantitative experiments demonstrate\nmetrics' validness, ground truth data quality, and baseline's state-of-the-art\nperformance. To the best of our knowledge, BEAT is the largest motion capture\ndataset for investigating human gestures, which may contribute to a number of\ndifferent research fields, including controllable gesture synthesis,\ncross-modality analysis, and emotional gesture recognition. The data, code and\nmodel are available on https://pantomatrix.github.io/BEAT/.", "published": "2022-03-10 11:19:52", "link": "http://arxiv.org/abs/2203.05297v5", "categories": ["cs.CV", "cs.CL", "cs.GR", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Echo-enabled Direction-of-Arrival and range estimation of a mobile\n  source in Ambisonic domain", "abstract": "Range estimation of a far field sound source in a reverberant environment is\nknown to be a notoriously difficult problem, hence most localization methods\nare only capable of estimating the source's Direction-of-Arrival (DoA). In an\nearlier work, we have demonstrated that, under certain restrictive acoustic\nconditions and given the orientation of a reflecting surface, one can exploit\nthe dominant acoustic reflection to evaluate the DoA \\emph{and} the distance to\na static sound source in Ambisonic domain. In this article, we leverage the\nrecently presented Generalized Time-domain Velocity Vector (GTVV)\nrepresentation to estimate these quantities for a moving sound source without\nan a priori knowledge of reflectors' orientations. We show that the\ntrajectories of a moving source and the corresponding reflections are spatially\nand temporally related, which can be used to infer the absolute delay of the\npropagating source signal and, therefore, approximate the microphone-to-source\ndistance. Experiments on real sound data confirm the validity of the proposed\napproach.", "published": "2022-03-10 09:53:52", "link": "http://arxiv.org/abs/2203.05265v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "EACELEB: An East Asian Language Speaking Celebrity Dataset for Speaker\n  Recognition", "abstract": "Large datasets are very useful for training speaker recognition systems, and\nvarious research groups have constructed several over the years. Voxceleb is a\nlarge dataset for speaker recognition that is extracted from Youtube videos.\nThis paper presents an audio-visual method for acquiring audio data from\nYoutube given the speaker's name as input. The system follows a pipeline\nsimilar to that of the Voxceleb data acquisition method. However, our work\nfocuses on fast data acquisition by using face-tracking in subsequent frames\nonce a face has been detected -- this is preferable over face detection for\nevery frame considering its computational cost. We show that applying audio\ndiarization to our data after acquiring it can yield equal error rates\ncomparable to Voxceleb. A secondary set of experiments showed that we could\nfurther decrease the error rate by fine-tuning a pre-trained x-vector system\nwith the acquired data. Like Voxceleb, the work here focuses primarily on\ndeveloping audio for celebrities. However, unlike Voxceleb, our target audio\ndata is from celebrities in East Asian countries. Finally, we set up a speaker\nverification task to evaluate the accuracy of our acquired data. After\ndiarization and fine-tuning, we achieved an equal error rate of approximately\n4\\% across our entire dataset.", "published": "2022-03-10 12:29:35", "link": "http://arxiv.org/abs/2203.05333v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Audio-Visual Attention Based Multimodal Network for Fake Talking Face\n  Videos Detection", "abstract": "DeepFake based digital facial forgery is threatening the public media\nsecurity, especially when lip manipulation has been used in talking face\ngeneration, the difficulty of fake video detection is further improved. By only\nchanging lip shape to match the given speech, the facial features of identity\nis hard to be discriminated in such fake talking face videos. Together with the\nlack of attention on audio stream as the prior knowledge, the detection failure\nof fake talking face generation also becomes inevitable. Inspired by the\ndecision-making mechanism of human multisensory perception system, which\nenables the auditory information to enhance post-sensory visual evidence for\ninformed decisions output, in this study, a fake talking face detection\nframework FTFDNet is proposed by incorporating audio and visual representation\nto achieve more accurate fake talking face videos detection. Furthermore, an\naudio-visual attention mechanism (AVAM) is proposed to discover more\ninformative features, which can be seamlessly integrated into any audio-visual\nCNN architectures by modularization. With the additional AVAM, the proposed\nFTFDNet is able to achieve a better detection performance on the established\ndataset (FTFDD). The evaluation of the proposed work has shown an excellent\nperformance on the detection of fake talking face videos, which is able to\narrive at a detection rate above 97%.", "published": "2022-03-10 06:16:11", "link": "http://arxiv.org/abs/2203.05178v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Attacks as Defenses: Designing Robust Audio CAPTCHAs Using Attacks on\n  Automatic Speech Recognition Systems", "abstract": "Audio CAPTCHAs are supposed to provide a strong defense for online resources;\nhowever, advances in speech-to-text mechanisms have rendered these defenses\nineffective. Audio CAPTCHAs cannot simply be abandoned, as they are\nspecifically named by the W3C as important enablers of accessibility.\nAccordingly, demonstrably more robust audio CAPTCHAs are important to the\nfuture of a secure and accessible Web. We look to recent literature on attacks\non speech-to-text systems for inspiration for the construction of robust,\nprinciple-driven audio defenses. We begin by comparing 20 recent attack papers,\nclassifying and measuring their suitability to serve as the basis of new\n\"robust to transcription\" but \"easy for humans to understand\" CAPTCHAs. After\nshowing that none of these attacks alone are sufficient, we propose a new\nmechanism that is both comparatively intelligible (evaluated through a user\nstudy) and hard to automatically transcribe (i.e., $P({\\rm transcription}) = 4\n\\times 10^{-5}$). Finally, we demonstrate that our audio samples have a high\nprobability of being detected as CAPTCHAs when given to speech-to-text systems\n($P({\\rm evasion}) = 1.77 \\times 10^{-4}$). In so doing, we not only\ndemonstrate a CAPTCHA that is approximately four orders of magnitude more\ndifficult to crack, but that such systems can be designed based on the insights\ngained from attack papers using the differences between the ways that humans\nand computers process audio.", "published": "2022-03-10 15:04:15", "link": "http://arxiv.org/abs/2203.05408v1", "categories": ["cs.CR", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
