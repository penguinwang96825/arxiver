{"title": "Revisiting the Centroid-based Method: A Strong Baseline for\n  Multi-Document Summarization", "abstract": "The centroid-based model for extractive document summarization is a simple\nand fast baseline that ranks sentences based on their similarity to a centroid\nvector. In this paper, we apply this ranking to possible summaries instead of\nsentences and use a simple greedy algorithm to find the best summary.\nFurthermore, we show possi- bilities to scale up to larger input docu- ment\ncollections by selecting a small num- ber of sentences from each document prior\nto constructing the summary. Experiments were done on the DUC2004 dataset for\nmulti-document summarization. We ob- serve a higher performance over the orig-\ninal model, on par with more complex state-of-the-art methods.", "published": "2017-08-25 11:21:44", "link": "http://arxiv.org/abs/1708.07690v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SPARQL as a Foreign Language", "abstract": "In the last years, the Linked Data Cloud has achieved a size of more than 100\nbillion facts pertaining to a multitude of domains. However, accessing this\ninformation has been significantly challenging for lay users. Approaches to\nproblems such as Question Answering on Linked Data and Link Discovery have\nnotably played a role in increasing information access. These approaches are\noften based on handcrafted and/or statistical models derived from data\nobservation. Recently, Deep Learning architectures based on Neural Networks\ncalled seq2seq have shown to achieve state-of-the-art results at translating\nsequences into sequences. In this direction, we propose Neural SPARQL Machines,\nend-to-end deep architectures to translate any natural language expression into\nsentences encoding SPARQL queries. Our preliminary results, restricted on\nselected DBpedia classes, show that Neural SPARQL Machines are a promising\napproach for Question Answering on Linked Data, as they can deal with known\nproblems such as vocabulary mismatch and perform graph pattern composition.", "published": "2017-08-25 06:41:55", "link": "http://arxiv.org/abs/1708.07624v2", "categories": ["cs.CL", "cs.DB", "68T99", "I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "$k$-Nearest Neighbor Augmented Neural Networks for Text Classification", "abstract": "In recent years, many deep-learning based models are proposed for text\nclassification. This kind of models well fits the training set from the\nstatistical point of view. However, it lacks the capacity of utilizing\ninstance-level information from individual instances in the training set. In\nthis work, we propose to enhance neural network models by allowing them to\nleverage information from $k$-nearest neighbor (kNN) of the input text. Our\nmodel employs a neural network that encodes texts into text embeddings.\nMoreover, we also utilize $k$-nearest neighbor of the input text as an external\nmemory, and utilize it to capture instance-level information from the training\nset. The final prediction is made based on features from both the neural\nnetwork encoder and the kNN memory. Experimental results on several standard\nbenchmark datasets show that our model outperforms the baseline model on all\nthe datasets, and it even beats a very deep neural network model (with 29\nlayers) in several datasets. Our model also shows superior performance when\ntraining instances are scarce, and when the training set is severely\nunbalanced. Our model also leverages techniques such as semi-supervised\ntraining and transfer learning quite well.", "published": "2017-08-25 19:04:25", "link": "http://arxiv.org/abs/1708.07863v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Nationality Classification Using Name Embeddings", "abstract": "Nationality identification unlocks important demographic information, with\nmany applications in biomedical and sociological research. Existing name-based\nnationality classifiers use name substrings as features and are trained on\nsmall, unrepresentative sets of labeled names, typically extracted from\nWikipedia. As a result, these methods achieve limited performance and cannot\nsupport fine-grained classification.\n  We exploit the phenomena of homophily in communication patterns to learn name\nembeddings, a new representation that encodes gender, ethnicity, and\nnationality which is readily applicable to building classifiers and other\nsystems. Through our analysis of 57M contact lists from a major Internet\ncompany, we are able to design a fine-grained nationality classifier covering\n39 groups representing over 90% of the world population. In an evaluation\nagainst other published systems over 13 common classes, our F1 score (0.795) is\nsubstantial better than our closest competitor Ethnea (0.580). To the best of\nour knowledge, this is the most accurate, fine-grained nationality classifier\navailable.\n  As a social media application, we apply our classifiers to the followers of\nmajor Twitter celebrities over six different domains. We demonstrate stark\ndifferences in the ethnicities of the followers of Trump and Obama, and in the\nsports and entertainments favored by different groups. Finally, we identify an\nanomalous political figure whose presumably inflated following appears largely\nincapable of reading the language he posts in.", "published": "2017-08-25 22:03:09", "link": "http://arxiv.org/abs/1708.07903v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
