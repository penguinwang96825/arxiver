{"title": "Improving Text-based Early Prediction by Distillation from Privileged\n  Time-Series Text", "abstract": "Modeling text-based time-series to make prediction about a future event or\noutcome is an important task with a wide range of applications. The standard\napproach is to train and test the model using the same input window, but this\napproach neglects the data collected in longer input windows between the\nprediction time and the final outcome, which are often available during\ntraining. In this study, we propose to treat this neglected text as privileged\ninformation available during training to enhance early prediction modeling\nthrough knowledge distillation, presented as Learning using Privileged\ntIme-sEries Text (LuPIET). We evaluate the method on clinical and social media\ntext, with four clinical prediction tasks based on clinical notes and two\nmental health prediction tasks based on social media posts. Our results show\nLuPIET is effective in enhancing text-based early predictions, though one may\nneed to consider choosing the appropriate text representation and windows for\nprivileged text to achieve optimal performance. Compared to two other methods\nusing transfer learning and mixed training, LuPIET offers more stable\nimprovements over the baseline, standard training. As far as we are concerned,\nthis is the first study to examine learning using privileged information for\ntime-series in the NLP context.", "published": "2023-01-26 01:07:02", "link": "http://arxiv.org/abs/2301.10887v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Causal Reasoning of Entities and Events in Procedural Texts", "abstract": "Entities and events are crucial to natural language reasoning and common in\nprocedural texts. Existing work has focused either exclusively on entity state\ntracking (e.g., whether a pan is hot) or on event reasoning (e.g., whether one\nwould burn themselves by touching the pan), while these two tasks are often\ncausally related. We propose CREPE, the first benchmark on causal reasoning of\nevent plausibility and entity states. We show that most language models,\nincluding GPT-3, perform close to chance at .35 F1, lagging far behind human at\n.87 F1. We boost model performance to .59 F1 by creatively representing events\nas programming languages while prompting language models pretrained on code. By\ninjecting the causal relations between entities and events as intermediate\nreasoning steps in our representation, we further boost the performance to .67\nF1. Our findings indicate not only the challenge that CREPE brings for language\nmodels, but also the efficacy of code-like prompting combined with\nchain-of-thought prompting for multihop event reasoning.", "published": "2023-01-26 01:43:17", "link": "http://arxiv.org/abs/2301.10896v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NLP as a Lens for Causal Analysis and Perception Mining to Infer Mental\n  Health on Social Media", "abstract": "Interactions among humans on social media often convey intentions behind\ntheir actions, yielding a psychological language resource for Mental Health\nAnalysis (MHA) of online users. The success of Computational Intelligence\nTechniques (CIT) for inferring mental illness from such social media resources\npoints to NLP as a lens for causal analysis and perception mining. However, we\nargue that more consequential and explainable research is required for optimal\nimpact on clinical psychology practice and personalized mental healthcare. To\nbridge this gap, we posit two significant dimensions: (1) Causal analysis to\nillustrate a cause and effect relationship in the user generated text; (2)\nPerception mining to infer psychological perspectives of social effects on\nonline users intentions. Within the scope of Natural Language Processing (NLP),\nwe further explore critical areas of inquiry associated with these two\ndimensions, specifically through recent advancements in discourse analysis.\nThis position paper guides the community to explore solutions in this space and\nadvance the state of practice in developing conversational agents for inferring\nmental health from social media. We advocate for a more explainable approach\ntoward modeling computational psychology problems through the lens of language\nas we observe an increased number of research contributions in dataset and\nproblem formulation for causal relation extraction and perception enhancements\nwhile inferring mental states.", "published": "2023-01-26 09:26:01", "link": "http://arxiv.org/abs/2301.11004v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Paraphrase Acquisition from Image Captions", "abstract": "We propose to use image captions from the Web as a previously underutilized\nresource for paraphrases (i.e., texts with the same \"message\") and to create\nand analyze a corresponding dataset. When an image is reused on the Web, an\noriginal caption is often assigned. We hypothesize that different captions for\nthe same image naturally form a set of mutual paraphrases. To demonstrate the\nsuitability of this idea, we analyze captions in the English Wikipedia, where\neditors frequently relabel the same image for different articles. The paper\nintroduces the underlying mining technology, the resulting Wikipedia-IPC\ndataset, and compares known paraphrase corpora with respect to their syntactic\nand semantic paraphrase similarity to our new resource. In this context, we\nintroduce characteristic maps along the two similarity dimensions to identify\nthe style of paraphrases coming from different sources. An annotation study\ndemonstrates the high reliability of the algorithmically determined\ncharacteristic maps.", "published": "2023-01-26 10:54:51", "link": "http://arxiv.org/abs/2301.11030v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SemSup-XC: Semantic Supervision for Zero and Few-shot Extreme\n  Classification", "abstract": "Extreme classification (XC) involves predicting over large numbers of classes\n(thousands to millions), with real-world applications like news article\nclassification and e-commerce product tagging. The zero-shot version of this\ntask requires generalization to novel classes without additional supervision.\nIn this paper, we develop SemSup-XC, a model that achieves state-of-the-art\nzero-shot and few-shot performance on three XC datasets derived from legal,\ne-commerce, and Wikipedia data. To develop SemSup-XC, we use automatically\ncollected semantic class descriptions to represent classes and facilitate\ngeneralization through a novel hybrid matching module that matches input\ninstances to class descriptions using a combination of semantic and lexical\nsimilarity. Trained with contrastive learning, SemSup-XC significantly\noutperforms baselines and establishes state-of-the-art performance on all three\ndatasets considered, gaining up to 12 precision points on zero-shot and more\nthan 10 precision points on one-shot tests, with similar gains for recall@10.\nOur ablation studies highlight the relative importance of our hybrid matching\nmodule and automatically collected class descriptions.", "published": "2023-01-26 18:49:02", "link": "http://arxiv.org/abs/2301.11309v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LoRaLay: A Multilingual and Multimodal Dataset for Long Range and\n  Layout-Aware Summarization", "abstract": "Text Summarization is a popular task and an active area of research for the\nNatural Language Processing community. By definition, it requires to account\nfor long input texts, a characteristic which poses computational challenges for\nneural models. Moreover, real-world documents come in a variety of complex,\nvisually-rich, layouts. This information is of great relevance, whether to\nhighlight salient content or to encode long-range interactions between textual\npassages. Yet, all publicly available summarization datasets only provide plain\ntext content. To facilitate research on how to exploit visual/layout\ninformation to better capture long-range dependencies in summarization models,\nwe present LoRaLay, a collection of datasets for long-range summarization with\naccompanying visual/layout information. We extend existing and popular English\ndatasets (arXiv and PubMed) with layout information and propose four novel\ndatasets -- consistently built from scholar resources -- covering French,\nSpanish, Portuguese, and Korean languages. Further, we propose new baselines\nmerging layout-aware and long-range models -- two orthogonal approaches -- and\nobtain state-of-the-art results, showing the importance of combining both lines\nof research.", "published": "2023-01-26 18:50:54", "link": "http://arxiv.org/abs/2301.11312v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Task formulation for Extracting Social Determinants of Health from\n  Clinical Narratives", "abstract": "Objective: The 2022 n2c2 NLP Challenge posed identification of social\ndeterminants of health (SDOH) in clinical narratives. We present three systems\nthat we developed for the Challenge and discuss the distinctive task\nformulation used in each of the three systems. Materials and Methods: The first\nsystem identifies target pieces of information independently using machine\nlearning classifiers. The second system uses a large language model (LLM) to\nextract complete structured outputs per document. The third system extracts\ncandidate phrases using machine learning and identifies target relations with\nhand-crafted rules. Results: The three systems achieved F1 scores of 0.884,\n0.831, and 0.663 in the Subtask A of the Challenge, which are ranked third,\nseventh, and eighth among the 15 participating teams. The review of the\nextraction results from our systems reveals characteristics of each approach\nand those of the SODH extraction task. Discussion: Phrases and relations\nannotated in the task is unique and diverse, not conforming to the conventional\nevent extraction task. These annotations are difficult to model with limited\ntraining data. The system that extracts information independently, ignoring the\nannotated relations, achieves the highest F1 score. Meanwhile, LLM with its\nversatile capability achieves the high F1 score, while respecting the annotated\nrelations. The rule-based system tackling relation extraction obtains the low\nF1 score, while it is the most explainable approach. Conclusion: The F1 scores\nof the three systems vary in this challenge setting, but each approach has\nadvantages and disadvantages in a practical application. The selection of the\napproach depends not only on the F1 score but also on the requirements in the\napplication.", "published": "2023-01-26 20:00:54", "link": "http://arxiv.org/abs/2301.11386v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Beyond Arabic: Software for Perso-Arabic Script Manipulation", "abstract": "This paper presents an open-source software library that provides a set of\nfinite-state transducer (FST) components and corresponding utilities for\nmanipulating the writing systems of languages that use the Perso-Arabic script.\nThe operations include various levels of script normalization, including visual\ninvariance-preserving operations that subsume and go beyond the standard\nUnicode normalization forms, as well as transformations that modify the visual\nappearance of characters in accordance with the regional orthographies for\neleven contemporary languages from diverse language families. The library also\nprovides simple FST-based romanization and transliteration. We additionally\nattempt to formalize the typology of Perso-Arabic characters by providing\none-to-many mappings from Unicode code points to the languages that use them.\nWhile our work focuses on the Arabic script diaspora rather than Arabic itself,\nthis approach could be adopted for any language that uses the Arabic script,\nthus providing a unified framework for treating a script family used by close\nto a billion people.", "published": "2023-01-26 20:37:03", "link": "http://arxiv.org/abs/2301.11406v1", "categories": ["cs.CL", "I.2.7; I.7.2; I.7.1"], "primary_category": "cs.CL"}
{"title": "Neural-Symbolic Inference for Robust Autoregressive Graph Parsing via\n  Compositional Uncertainty Quantification", "abstract": "Pre-trained seq2seq models excel at graph semantic parsing with rich\nannotated data, but generalize worse to out-of-distribution (OOD) and long-tail\nexamples. In comparison, symbolic parsers under-perform on population-level\nmetrics, but exhibit unique strength in OOD and tail generalization. In this\nwork, we study compositionality-aware approach to neural-symbolic inference\ninformed by model confidence, performing fine-grained neural-symbolic reasoning\nat subgraph level (i.e., nodes and edges) and precisely targeting subgraph\ncomponents with high uncertainty in the neural parser. As a result, the method\ncombines the distinct strength of the neural and symbolic approaches in\ncapturing different aspects of the graph prediction, leading to well-rounded\ngeneralization performance both across domains and in the tail. We empirically\ninvestigate the approach in the English Resource Grammar (ERG) parsing problem\non a diverse suite of standard in-domain and seven OOD corpora. Our approach\nleads to 35.26% and 35.60% error reduction in aggregated Smatch score over\nneural and symbolic approaches respectively, and 14% absolute accuracy gain in\nkey tail linguistic categories over the neural model, outperforming prior\nstate-of-art methods that do not account for compositionality or uncertainty.", "published": "2023-01-26 23:11:03", "link": "http://arxiv.org/abs/2301.11459v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How poor is the stimulus? Evaluating hierarchical generalization in\n  neural networks trained on child-directed speech", "abstract": "When acquiring syntax, children consistently choose hierarchical rules over\ncompeting non-hierarchical possibilities. Is this preference due to a learning\nbias for hierarchical structure, or due to more general biases that interact\nwith hierarchical cues in children's linguistic input? We explore these\npossibilities by training LSTMs and Transformers - two types of neural networks\nwithout a hierarchical bias - on data similar in quantity and content to\nchildren's linguistic input: text from the CHILDES corpus. We then evaluate\nwhat these models have learned about English yes/no questions, a phenomenon for\nwhich hierarchical structure is crucial. We find that, though they perform well\nat capturing the surface statistics of child-directed speech (as measured by\nperplexity), both model types generalize in a way more consistent with an\nincorrect linear rule than the correct hierarchical rule. These results suggest\nthat human-like generalization from text alone requires stronger biases than\nthe general sequence-processing biases of standard neural network\narchitectures.", "published": "2023-01-26 23:24:17", "link": "http://arxiv.org/abs/2301.11462v2", "categories": ["cs.CL", "J.4; I.2.7"], "primary_category": "cs.CL"}
{"title": "Break It Down: Evidence for Structural Compositionality in Neural\n  Networks", "abstract": "Though modern neural networks have achieved impressive performance in both\nvision and language tasks, we know little about the functions that they\nimplement. One possibility is that neural networks implicitly break down\ncomplex tasks into subroutines, implement modular solutions to these\nsubroutines, and compose them into an overall solution to a task - a property\nwe term structural compositionality. Another possibility is that they may\nsimply learn to match new inputs to learned templates, eliding task\ndecomposition entirely. Here, we leverage model pruning techniques to\ninvestigate this question in both vision and language across a variety of\narchitectures, tasks, and pretraining regimens. Our results demonstrate that\nmodels often implement solutions to subroutines via modular subnetworks, which\ncan be ablated while maintaining the functionality of other subnetworks. This\nsuggests that neural networks may be able to learn compositionality, obviating\nthe need for specialized symbolic mechanisms.", "published": "2023-01-26 00:53:11", "link": "http://arxiv.org/abs/2301.10884v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt\n  Tuning", "abstract": "Dialogue state tracking (DST) is an important step in dialogue management to\nkeep track of users' beliefs. Existing works fine-tune all language model (LM)\nparameters to tackle the DST task, which requires significant data and\ncomputing resources for training and hosting. The cost grows exponentially in\nthe real-world deployment where dozens of fine-tuned LM are used for different\ndomains and tasks. To reduce parameter size and better utilize cross-task\nshared information, we propose to use soft prompt token embeddings to learn\ntask properties. Without tuning LM parameters, our method drastically reduces\nthe number of parameters needed to less than 0.5% of prior works while achieves\nbetter low-resource DST performance.", "published": "2023-01-26 03:01:59", "link": "http://arxiv.org/abs/2301.10915v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cross Modal Global Local Representation Learning from Radiology Reports\n  and X-Ray Chest Images", "abstract": "Deep learning models can be applied successfully in real-work problems;\nhowever, training most of these models requires massive data. Recent methods\nuse language and vision, but unfortunately, they rely on datasets that are not\nusually publicly available. Here we pave the way for further research in the\nmultimodal language-vision domain for radiology. In this paper, we train a\nrepresentation learning method that uses local and global representations of\nthe language and vision through an attention mechanism and based on the\npublicly available Indiana University Radiology Report (IU-RR) dataset.\nFurthermore, we use the learned representations to diagnose five lung\npathologies: atelectasis, cardiomegaly, edema, pleural effusion, and\nconsolidation. Finally, we use both supervised and zero-shot classifications to\nextensively analyze the performance of the representation learning on the IU-RR\ndataset. Average Area Under the Curve (AUC) is used to evaluate the accuracy of\nthe classifiers for classifying the five lung pathologies. The average AUC for\nclassifying the five lung pathologies on the IU-RR test set ranged from 0.85 to\n0.87 using the different training datasets, namely CheXpert and CheXphoto.\nThese results compare favorably to other studies using UI-RR. Extensive\nexperiments confirm consistent results for classifying lung pathologies using\nthe multimodal global local representations of language and vision information.", "published": "2023-01-26 06:02:28", "link": "http://arxiv.org/abs/2301.10951v1", "categories": ["cs.CV", "cs.CL", "I.2.7"], "primary_category": "cs.CV"}
{"title": "Neural Dynamic Focused Topic Model", "abstract": "Topic models and all their variants analyse text by learning meaningful\nrepresentations through word co-occurrences. As pointed out by Williamson et\nal. (2010), such models implicitly assume that the probability of a topic to be\nactive and its proportion within each document are positively correlated. This\ncorrelation can be strongly detrimental in the case of documents created over\ntime, simply because recent documents are likely better described by new and\nhence rare topics. In this work we leverage recent advances in neural\nvariational inference and present an alternative neural approach to the dynamic\nFocused Topic Model. Indeed, we develop a neural model for topic evolution\nwhich exploits sequences of Bernoulli random variables in order to track the\nappearances of topics, thereby decoupling their activities from their\nproportions. We evaluate our model on three different datasets (the UN general\ndebates, the collection of NeurIPS papers, and the ACL Anthology dataset) and\nshow that it (i) outperforms state-of-the-art topic models in generalization\ntasks and (ii) performs comparably to them on prediction tasks, while employing\nroughly the same number of parameters, and converging about two times faster.\nSource code to reproduce our experiments is available online.", "published": "2023-01-26 08:37:34", "link": "http://arxiv.org/abs/2301.10988v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A benchmark for toxic comment classification on Civil Comments dataset", "abstract": "Toxic comment detection on social media has proven to be essential for\ncontent moderation. This paper compares a wide set of different models on a\nhighly skewed multi-label hate speech dataset. We consider inference time and\nseveral metrics to measure performance and bias in our comparison. We show that\nall BERTs have similar performance regardless of the size, optimizations or\nlanguage used to pre-train the models. RNNs are much faster at inference than\nany of the BERT. BiLSTM remains a good compromise between performance and\ninference time. RoBERTa with Focal Loss offers the best performance on biases\nand AUROC. However, DistilBERT combines both good AUROC and a low inference\ntime. All models are affected by the bias of associating identities. BERT, RNN,\nand XLNet are less sensitive than the CNN and Compact Convolutional\nTransformers.", "published": "2023-01-26 14:25:09", "link": "http://arxiv.org/abs/2301.11125v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Characterizing the Entities in Harmful Memes: Who is the Hero, the\n  Villain, the Victim?", "abstract": "Memes can sway people's opinions over social media as they combine visual and\ntextual information in an easy-to-consume manner. Since memes instantly turn\nviral, it becomes crucial to infer their intent and potentially associated\nharmfulness to take timely measures as needed. A common problem associated with\nmeme comprehension lies in detecting the entities referenced and characterizing\nthe role of each of these entities. Here, we aim to understand whether the meme\nglorifies, vilifies, or victimizes each entity it refers to. To this end, we\naddress the task of role identification of entities in harmful memes, i.e.,\ndetecting who is the 'hero', the 'villain', and the 'victim' in the meme, if\nany. We utilize HVVMemes - a memes dataset on US Politics and Covid-19 memes,\nreleased recently as part of the CONSTRAINT@ACL-2022 shared-task. It contains\nmemes, entities referenced, and their associated roles: hero, villain, victim,\nand other. We further design VECTOR (Visual-semantic role dEteCToR), a robust\nmulti-modal framework for the task, which integrates entity-based contextual\ninformation in the multi-modal representation and compare it to several\nstandard unimodal (text-only or image-only) or multi-modal (image+text) models.\nOur experimental results show that our proposed model achieves an improvement\nof 4% over the best baseline and 1% over the best competing stand-alone\nsubmission from the shared-task. Besides divulging an extensive experimental\nsetup with comparative analyses, we finally highlight the challenges\nencountered in addressing the complex task of semantic role labeling within\nmemes.", "published": "2023-01-26 16:55:15", "link": "http://arxiv.org/abs/2301.11219v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Understanding Finetuning for Factual Knowledge Extraction from Language\n  Models", "abstract": "Language models (LMs) pretrained on large corpora of text from the web have\nbeen observed to contain large amounts of various types of knowledge about the\nworld. This observation has led to a new and exciting paradigm in knowledge\ngraph construction where, instead of manual curation or text mining, one\nextracts knowledge from the parameters of an LM. Recently, it has been shown\nthat finetuning LMs on a set of factual knowledge makes them produce better\nanswers to queries from a different set, thus making finetuned LMs a good\ncandidate for knowledge extraction and, consequently, knowledge graph\nconstruction. In this paper, we analyze finetuned LMs for factual knowledge\nextraction. We show that along with its previously known positive effects,\nfinetuning also leads to a (potentially harmful) phenomenon which we call\nFrequency Shock, where at the test time the model over-predicts rare entities\nthat appear in the training set and under-predicts common entities that do not\nappear in the training set enough times. We show that Frequency Shock leads to\na degradation in the predictions of the model and beyond a point, the harm from\nFrequency Shock can even outweigh the positive effects of finetuning, making\nfinetuning harmful overall. We then consider two solutions to remedy the\nidentified negative effect: 1- model mixing and 2- mixture finetuning with the\nLM's pre-training task. The two solutions combined lead to significant\nimprovements compared to vanilla finetuning.", "published": "2023-01-26 18:29:50", "link": "http://arxiv.org/abs/2301.11293v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability\n  Curvature", "abstract": "The increasing fluency and widespread usage of large language models (LLMs)\nhighlight the desirability of corresponding tools aiding detection of\nLLM-generated text. In this paper, we identify a property of the structure of\nan LLM's probability function that is useful for such detection. Specifically,\nwe demonstrate that text sampled from an LLM tends to occupy negative curvature\nregions of the model's log probability function. Leveraging this observation,\nwe then define a new curvature-based criterion for judging if a passage is\ngenerated from a given LLM. This approach, which we call DetectGPT, does not\nrequire training a separate classifier, collecting a dataset of real or\ngenerated passages, or explicitly watermarking generated text. It uses only log\nprobabilities computed by the model of interest and random perturbations of the\npassage from another generic pre-trained language model (e.g., T5). We find\nDetectGPT is more discriminative than existing zero-shot methods for model\nsample detection, notably improving detection of fake news articles generated\nby 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline\nto 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code,\ndata, and other project information.", "published": "2023-01-26 18:44:06", "link": "http://arxiv.org/abs/2301.11305v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multimodal Event Transformer for Image-guided Story Ending Generation", "abstract": "Image-guided story ending generation (IgSEG) is to generate a story ending\nbased on given story plots and ending image. Existing methods focus on\ncross-modal feature fusion but overlook reasoning and mining implicit\ninformation from story plots and ending image. To tackle this drawback, we\npropose a multimodal event transformer, an event-based reasoning framework for\nIgSEG. Specifically, we construct visual and semantic event graphs from story\nplots and ending image, and leverage event-based reasoning to reason and mine\nimplicit information in a single modality. Next, we connect visual and semantic\nevent graphs and utilize cross-modal fusion to integrate different-modality\nfeatures. In addition, we propose a multimodal injector to adaptive pass\nessential information to decoder. Besides, we present an incoherence detection\nto enhance the understanding context of a story plot and the robustness of\ngraph modeling for our model. Experimental results show that our method\nachieves state-of-the-art performance for the image-guided story ending\ngeneration.", "published": "2023-01-26 19:10:07", "link": "http://arxiv.org/abs/2301.11357v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Improving Cross-modal Alignment for Text-Guided Image Inpainting", "abstract": "Text-guided image inpainting (TGII) aims to restore missing regions based on\na given text in a damaged image. Existing methods are based on a strong vision\nencoder and a cross-modal fusion model to integrate cross-modal features.\nHowever, these methods allocate most of the computation to visual encoding,\nwhile light computation on modeling modality interactions. Moreover, they take\ncross-modal fusion for depth features, which ignores a fine-grained alignment\nbetween text and image. Recently, vision-language pre-trained models (VLPM),\nencapsulating rich cross-modal alignment knowledge, have advanced in most\nmultimodal tasks. In this work, we propose a novel model for TGII by improving\ncross-modal alignment (CMA). CMA model consists of a VLPM as a vision-language\nencoder, an image generator and global-local discriminators. To explore\ncross-modal alignment knowledge for image restoration, we introduce cross-modal\nalignment distillation and in-sample distribution distillation. In addition, we\nemploy adversarial training to enhance the model to fill the missing region in\ncomplicated structures effectively. Experiments are conducted on two popular\nvision-language datasets. Results show that our model achieves state-of-the-art\nperformance compared with other strong competitors.", "published": "2023-01-26 19:18:27", "link": "http://arxiv.org/abs/2301.11362v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Style-Aware Contrastive Learning for Multi-Style Image Captioning", "abstract": "Existing multi-style image captioning methods show promising results in\ngenerating a caption with accurate visual content and desired linguistic style.\nHowever, existing methods overlook the relationship between linguistic style\nand visual content. To overcome this drawback, we propose style-aware\ncontrastive learning for multi-style image captioning. First, we present a\nstyle-aware visual encoder with contrastive learning to mine potential visual\ncontent relevant to style. Moreover, we propose a style-aware triplet contrast\nobjective to distinguish whether the image, style and caption matched. To\nprovide positive and negative samples for contrastive learning, we present\nthree retrieval schemes: object-based retrieval, RoI-based retrieval and\ntriplet-based retrieval, and design a dynamic trade-off function to calculate\nretrieval scores. Experimental results demonstrate that our approach achieves\nstate-of-the-art performance. In addition, we conduct an extensive analysis to\nverify the effectiveness of our method.", "published": "2023-01-26 19:21:39", "link": "http://arxiv.org/abs/2301.11367v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "LAGAN: Deep Semi-Supervised Linguistic-Anthropology Classification with\n  Conditional Generative Adversarial Neural Network", "abstract": "Education is a right of all, however, every individual is different than\nothers. Teachers in post-communism era discover inherent individualism to\nequally train all towards job market of fourth industrial revolution. We can\nconsider scenario of ethnic minority education in academic practices. Ethnic\nminority group has grown in their own culture and would prefer to be taught in\ntheir native way. We have formulated such linguistic anthropology(how people\nlearn)based engagement as semi-supervised problem. Then, we have developed an\nconditional deep generative adversarial network algorithm namely LA-GAN to\nclassify linguistic ethnographic features in student engagement. Theoretical\njustification proves the objective, regularization and loss function of our\nsemi-supervised adversarial model. Survey questions are prepared to reach some\nform of assumptions about z-generation and ethnic minority group, whose\nlearning style, learning approach and preference are our main area of interest.", "published": "2023-01-26 05:25:34", "link": "http://arxiv.org/abs/2301.13853v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Affective Faces for Goal-Driven Dyadic Communication", "abstract": "We introduce a video framework for modeling the association between verbal\nand non-verbal communication during dyadic conversation. Given the input speech\nof a speaker, our approach retrieves a video of a listener, who has facial\nexpressions that would be socially appropriate given the context. Our approach\nfurther allows the listener to be conditioned on their own goals,\npersonalities, or backgrounds. Our approach models conversations through a\ncomposition of large language models and vision-language models, creating\ninternal representations that are interpretable and controllable. To study\nmultimodal communication, we propose a new video dataset of unscripted\nconversations covering diverse topics and demographics. Experiments and\nvisualizations show our approach is able to output listeners that are\nsignificantly more socially appropriate than baselines. However, many\nchallenges remain, and we release our dataset publicly to spur further\nprogress. See our website for video results, data, and code:\nhttps://realtalk.cs.columbia.edu.", "published": "2023-01-26 05:00:09", "link": "http://arxiv.org/abs/2301.10939v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Semi-Supervised Image Captioning by Adversarially Propagating Labeled\n  Data", "abstract": "We present a novel data-efficient semi-supervised framework to improve the\ngeneralization of image captioning models. Constructing a large-scale labeled\nimage captioning dataset is an expensive task in terms of labor, time, and\ncost. In contrast to manually annotating all the training samples, separately\ncollecting uni-modal datasets is immensely easier, e.g., a large-scale image\ndataset and a sentence dataset. We leverage such massive unpaired image and\ncaption data upon standard paired data by learning to associate them. To this\nend, our proposed semi-supervised learning method assigns pseudo-labels to\nunpaired samples in an adversarial learning fashion, where the joint\ndistribution of image and caption is learned. Our method trains a captioner to\nlearn from a paired data and to progressively associate unpaired data. This\napproach shows noticeable performance improvement even in challenging scenarios\nincluding out-of-task data (i.e., relational captioning, where the target task\nis different from the unpaired data) and web-crawled data. We also show that\nour proposed method is theoretically well-motivated and has a favorable global\noptimal property. Our extensive and comprehensive empirical results both on (1)\nimage-based and (2) dense region-based captioning datasets followed by\ncomprehensive analysis on the scarcely-paired COCO dataset demonstrate the\nconsistent effectiveness of our semisupervised learning method with unpaired\ndata compared to competing methods.", "published": "2023-01-26 15:25:43", "link": "http://arxiv.org/abs/2301.11174v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Domain-Agnostic Molecular Generation with Chemical Feedback", "abstract": "The generation of molecules with desired properties has become increasingly\npopular, revolutionizing the way scientists design molecular structures and\nproviding valuable support for chemical and drug design. However, despite the\npotential of language models in molecule generation, they face challenges such\nas generating syntactically or chemically flawed molecules, having narrow\ndomain focus, and struggling to create diverse and feasible molecules due to\nlimited annotated data or external molecular databases. To tackle these\nchallenges, we introduce MolGen, a pre-trained molecular language model\ntailored specifically for molecule generation. Through the reconstruction of\nover 100 million molecular SELFIES, MolGen internalizes structural and\ngrammatical insights. This is further enhanced by domain-agnostic molecular\nprefix tuning, fostering robust knowledge transfer across diverse domains.\nImportantly, our chemical feedback paradigm steers the model away from\nmolecular hallucinations, ensuring alignment between the model's estimated\nprobabilities and real-world chemical preferences. Extensive experiments on\nwell-known benchmarks underscore MolGen's optimization capabilities in\nproperties such as penalized logP, QED, and molecular docking. Additional\nanalyses confirm its proficiency in accurately capturing molecule\ndistributions, discerning intricate structural patterns, and efficiently\nexploring the chemical space. Code is available at\nhttps://github.com/zjunlp/MolGen.", "published": "2023-01-26 17:52:56", "link": "http://arxiv.org/abs/2301.11259v6", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Detecting Pump&Dump Stock Market Manipulation from Online Forums", "abstract": "The intersection of social media, low-cost trading platforms, and naive\ninvestors has created an ideal situation for information-based market\nmanipulations, especially pump&dumps. Manipulators accumulate small-cap stocks,\ndisseminate false information on social media to inflate their price, and sell\nat the peak. We collect a dataset of stocks whose price and volume profiles\nhave the characteristic shape of a pump&dump, and social media posts for those\nsame stocks that match the timing of the initial price rises. From these we\nbuild predictive models for pump&dump events based on the language used in the\nsocial media posts.\n  There are multiple difficulties: not every post will cause the intended\nmarket reaction, some pump&dump events may be triggered by posts in other\nforums, and there may be accidental confluences of post timing and market\nmovements. Nevertheless, our best model achieves a prediction accuracy of 85%\nand an F1-score of 62%. Such a tool can provide early warning to investors and\nregulators that a pump&dump may be underway.", "published": "2023-01-26 20:31:27", "link": "http://arxiv.org/abs/2301.11403v1", "categories": ["cs.SI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "On granularity of prosodic representations in expressive text-to-speech", "abstract": "In expressive speech synthesis it is widely adopted to use latent prosody\nrepresentations to deal with variability of the data during training. Same text\nmay correspond to various acoustic realizations, which is known as a\none-to-many mapping problem in text-to-speech. Utterance, word, or\nphoneme-level representations are extracted from target signal in an\nauto-encoding setup, to complement phonetic input and simplify that mapping.\nThis paper compares prosodic embeddings at different levels of granularity and\nexamines their prediction from text. We show that utterance-level embeddings\nhave insufficient capacity and phoneme-level tend to introduce instabilities\nwhen predicted from text. Word-level representations impose balance between\ncapacity and predictability. As a result, we close the gap in naturalness by\n90% between synthetic speech and recordings on LibriTTS dataset, without\nsacrificing intelligibility.", "published": "2023-01-26 22:24:21", "link": "http://arxiv.org/abs/2301.11446v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A simple model for pink noise from amplitude modulations", "abstract": "We propose a simple model for the origin of pink noise (or 1/f fluctuation)\nbased on the beat of cooperative waves. These cooperative waves arise\nspontaneously in a system with synchronization, resonance, and infrared\ndivergence. Many cooperative waves with close frequencies can produce signals\nof arbitrary small frequencies from a system of small size. This beat mechanism\ncan be understood as amplitude modulation. The pink noise can appear after the\ndemodulation process, which produces a variety of pink noise in many fields.\nThe pink noise thus formed from the beat has nothing to do with dissipation or\nlong-time memory. We also suggest new ways of looking at pink noise in shallow\nearthquakes, solar flares, and stellar activities.", "published": "2023-01-26 15:33:19", "link": "http://arxiv.org/abs/2301.11176v1", "categories": ["eess.AS", "cs.SD", "physics.class-ph"], "primary_category": "eess.AS"}
{"title": "MusicLM: Generating Music From Text", "abstract": "We introduce MusicLM, a model generating high-fidelity music from text\ndescriptions such as \"a calming violin melody backed by a distorted guitar\nriff\". MusicLM casts the process of conditional music generation as a\nhierarchical sequence-to-sequence modeling task, and it generates music at 24\nkHz that remains consistent over several minutes. Our experiments show that\nMusicLM outperforms previous systems both in audio quality and adherence to the\ntext description. Moreover, we demonstrate that MusicLM can be conditioned on\nboth text and a melody in that it can transform whistled and hummed melodies\naccording to the style described in a text caption. To support future research,\nwe publicly release MusicCaps, a dataset composed of 5.5k music-text pairs,\nwith rich text descriptions provided by human experts.", "published": "2023-01-26 18:58:53", "link": "http://arxiv.org/abs/2301.11325v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
